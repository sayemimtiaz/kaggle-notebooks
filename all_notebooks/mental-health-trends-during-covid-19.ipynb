{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\nSocial interaction plays a key role in maintaining mental health [1]. However, according to a 2018 national survey by Cigna, nearly a half of 20,000 U.S. adults reported feeling sometimes or always feeling alone [2]. This increased isolation has a devastating impact on mental health, leading to a slower functioning society. Research shows that isolation and loneliness can heighten or introduce health risks such as smoking up to 15 cigarettes a day or alcohol use disorder. Additionally, social isolation is twice as harmful to physical and mental health when compared to obesity [3]. Other studies have linked loneliness to a 30% increase in risk and development of coronary heart disease [4] and to a 40% increase in risk of dementia [5]. A main reason for loneliness pre COVID-19 was dissatisfication with family life; according to a poll conducted by the Pew Research Center, 28% of adults polled were dissatisfied with familial life [6].\n\nAfter COVID-19, mental health has been a paramount issue, as quarantine has increased the social isolation many Americans already faced. A poll conducted in late March show that over a third (36%) of Americans say the coronavirus has had a serious impact on mental health [7]. Another poll conducted in late April reports the number to be even higher, with nearly half of Americans reporting that COVID-19 negatively impacted their mental health [8].\n\nThis paper delves into the impact that quarantine has had on the mental health of Americans. However, instead of using a traditional approach of polling subjects, this paper uses data mining and big data techniques to accurately sketch trends in mental health after quarantine. Traditional polling on public opinion runs into many issues and biases, many of which are implicit and hidden. Such issues include: 1. Slightly changing the wording of a question can change the results entirely, even if the researchers intended for the questions to be neutral; 2. Poll questions are often black and white, and may be inaccurate towards obtaining the full picture of a subject; 3. When subjects are polled, they are consciously aware that they are being polled, thus leading to results that may be subjective of the subject in the moment. We instead use the Google Trends API to gather our data, and then analyze it with python.\n\nThe keywords we will use are \"depression,\" \"anxiety,\" \"panic attack,\" \"insomnia,\" and \"loneliness\". These words should encompass the majority of depressive attitudes. Note, other words could definitely be used, and the results would thus most likely change. This would be an area of future study.\n\n**Aims:**\n1. Is there a long-term trend (e.g. seasonal) that we should account for?\n2. Is there a difference in the relevance of key search terms during COVID-19?\n3. Do certain demographic factors play a role in determining whether someone is more likely to search an indicative term? Demographic factors would include race, political affilation, and socio-economic status.\n4. Do people in states with more coronavirus cases tend to search for these indicators more?\n5. Do Americans tend to search these mental health keywords more or less than people from other countries?\n6. Given the results found, what are possible solutions that America (and other countries) can implement?\n7. What are areas for improvement/future study?\n\n**Sources:**\n\n[1] “Social Connection and Healthy Activities Are Important to Mental Health.” Lifeskills South Florida, 2 May 2019, [www.lifeskillssouthflorida.com/mental-health-blog/social-connection-and-healthy-activities-are-important-to-mental-health/](http://www.lifeskillssouthflorida.com/mental-health-blog/social-connection-and-healthy-activities-are-important-to-mental-health/).\n\n[2] “Loneliness At Epidemic Levels In America.” Cigna Surveys Loneliness in America | Cigna Newsroom, [www.cigna.com/about-us/newsroom/studies-and-reports/loneliness-epidemic-america](http://www.cigna.com/about-us/newsroom/studies-and-reports/loneliness-epidemic-america).\n\n[3] Holt-Lunstad, J., Smith, T. B., Baker, M., Harris, T., & Stephenson, D. (2015). Loneliness and Social Isolation as Risk Factors for Mortality: A Meta-Analytic Review. Perspectives on Psychological Science, 10(2), 227–237. [https://doi.org/10.1177/1745691614568352](https://doi.org/10.1177/1745691614568352)\n\n[4] Valtorta NK, Kanaan M, Gilbody S, et alLoneliness and social isolation as risk factors for coronary heart disease and stroke: systematic review and meta-analysis of longitudinal observational studiesHeart 2016;102:1009-1016. [https://heart.bmj.com/content/102/13/1009](https://heart.bmj.com/content/102/13/1009)\n\n[5] Julene K Johnson, PhD, Anita L Stewart, PhD, Michael Acree, PhD, Anna M Nápoles, PhD, Jason D Flatt, PhD, Wendy B Max, PhD, Steven E Gregorich, PhD, A Community Choir Intervention to Promote Well-Being Among Diverse Older Adults: Results From the Community of Voices Trial, The Journals of Gerontology: Series B, Volume 75, Issue 3, March 2020, Pages 549–559, [https://doi.org/10.1093/geronb/gby132](https://doi.org/10.1093/geronb/gby132)\n\n[6] Bialik, Kristen. “Americans Unhappy with Family, Social or Financial Life Are More Likely to Say They Feel Lonely.” Pew Research Center, Pew Research Center, 30 May 2020, [www.pewresearch.org/fact-tank/2018/12/03/americans-unhappy-with-family-social-or-financial-life-are-more-likely-to-say-they-feel-lonely/](http://www.pewresearch.org/fact-tank/2018/12/03/americans-unhappy-with-family-social-or-financial-life-are-more-likely-to-say-they-feel-lonely/).\n\n[7] New Poll: COVID-19 Impacting Mental Well-Being: Americans Feeling Anxious, Especially for Loved Ones; Older Adults Are Less Anxious, 25 Mar. 2020, [www.psychiatry.org/newsroom/news-releases/new-poll-covid-19-impacting-mental-well-being-americans-feeling-anxious-especially-for-loved-ones-older-adults-are-less-anxious](http://www.psychiatry.org/newsroom/news-releases/new-poll-covid-19-impacting-mental-well-being-americans-feeling-anxious-especially-for-loved-ones-older-adults-are-less-anxious).\n\n[8] “Poll: Here's COVID-19 Toll on Americans' Mental Health.” Hartford HealthCare, 24 Apr. 2020, [hartfordhealthcare.org/about-us/news-press/news-detail?articleid=25741&publicId=395](http://hartfordhealthcare.org/about-us/news-press/news-detail?articleid=25741&publicId=395).[1] “Social Connection and Healthy Activities Are Important to Mental Health.” Lifeskills South Florida, 2 May 2019, [www.lifeskillssouthflorida.com/mental-health-blog/social-connection-and-healthy-activities-are-important-to-mental-health/](http://www.lifeskillssouthflorida.com/mental-health-blog/social-connection-and-healthy-activities-are-important-to-mental-health/)."},{"metadata":{},"cell_type":"markdown","source":"# Before We Begin\n\nBefore we begin analyzing the effect of COVID-19 on the relevance of mental health search terms, let's analyze the five year trend to see if there is any factors (like seasonality) that we should take into consideration.\n\nFirst, we will be using PyTrends, an unofficial API that connects to Google Trends."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pytrends","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have our list of five keywords, \"depression\", \"anxiety\", \"panic attack\", \"insomnia\", \"loneliness\", and we also have \"covid\" as a graphical marker for when covid first came into the public's attention.\n\nIn the code below, we build the dataframe from individual components. This has two advantages: first, it allows the data to be relative to itself, instead of being relative to the most popular word (which is what searching these terms on Google Trends would give you). Second, it allows the user to add as many keywords as they want, as opposed to the maximum of 5 keywords with a traditional method.\n\nFor reference, this is the code for how the datasets used here were created. I have already created the dataset, which can be found on my github repository for this project, linked here: https://github.com/IronicNinja/covid19api. The same code can also be found on my github repository under the \"create.py\" file."},{"metadata":{"trusted":true},"cell_type":"code","source":"# import pandas as pd\n# from pytrends.request import TrendReq\n\n# search_period = 'start_date end_date'\n\n# pytrends = TrendReq(hl='en-US', tz=420)\n# kw_list = [\"depression\", \"anxiety\", \"panic attack\", \"insomnia\", \"loneliness\", \"covid\"] #list of keywords\n\n# ### Building the dataframe\n# df = pd.DataFrame({})\n# for id in kw_list:\n#     pytrends.build_payload([id], geo='US', timeframe = search_period)\n#     df[id] = pytrends.interest_over_time()[id]\n\n# df.to_excel('file_name')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will also import all the other libraries we will use in this kernel."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nfrom datetime import date\nimport datetime\nimport scipy.stats as st\nimport plotly.graph_objects as go\nimport statsmodels.api as sm\nimport statistics\nfrom scipy import integrate\nfrom pytrends.request import TrendReq","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will import the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"fname = 'https://raw.githubusercontent.com/IronicNinja/covid19api/master/5_year_period.xlsx'\ndf = pd.read_excel(fname)\n\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Plot the figure\n\nkw_list = [\"depression\", \"anxiety\", \"panic attack\", \"insomnia\", \"loneliness\"] #Initialize keywords (without 'covid')\nword_color_list = ['red', 'blue', 'orange', 'yellow', 'gray'] #Initialize colors\n\nplt.figure(figsize=(20,10))\nfor pos in range(len(kw_list)):\n    plt.plot(df['date'], df[kw_list[pos]], color = word_color_list[pos], linewidth=2)\n\nplt.plot(df['date'], df['covid'], color = 'black')\n\nplt.xlim(date(2015, 5, 31).toordinal(), date(2020, 5, 31).toordinal())\nplt.legend([\"depression\", \"anxiety\", \"panic attack\", \"insomnia\", \"loneliness\", \"covid\"])\nplt.xlabel('Time')\nplt.ylabel('Search Interest')\nplt.title('Search Interest of Mental Health in the US - 5 year period')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is unclear what is happening in this graph. To better understand if there is a trend here, let's plot the trend using seasonal_decompose from the stats_models library."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_5 = df.copy() #Make a copy of the dataframe\n\nstart_5 = date(2020, 5, 31)\n\n### Converts the indices of the dataframe into dates - helps with the graphing. We will use this alot later\ndef change_axis_time(df, start):\n    temp_list = []\n    for time in range(len(df)):\n        d0 = start-datetime.timedelta(days=7*time)\n        d1 = d0.strftime(\"%Y-%m-%d\")\n        temp_list.append(d1)\n\n    temp_list.sort()\n    df.index = temp_list\n\nchange_axis_time(df_5, start_5)\ndf_5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nfor pos in range(len(kw_list)):\n    fig = sm.tsa.seasonal_decompose(df_5[kw_list[pos]], period=26) #Period is every half year\n    plt.plot(fig.trend, color=word_color_list[pos], linewidth=3)\n    plt.plot(df[kw_list[pos]], color = word_color_list[pos], linewidth=0.5) #blur the original\n    \nlegend_list = []\nfor keyword in kw_list:\n    legend_list.append(keyword + ' trend')\n    legend_list.append(keyword + ' org') #Org for original\n    \nplt.xticks([26*n for n in range(math.ceil(len(df_5)/26))])\nplt.xlim(0, 261)\nplt.ylim(-5, 105)\n\nplt.legend(legend_list)\nplt.xlabel('Time')\nplt.ylabel('Search Interest')\nplt.title('Search Interest of Mental Health in the US - 5 year period, Trend')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's clear there is some kind of seasonal trend, as the relevance for our search terms peaks around March. This is definitely something to take into consideration when analyzing pre and post-COVID 19 trends, as the coronavirus hit worst around March.\n\nTo get a better picture of the overall trend, let's analyze the average relevance of these five keywords over time (excluding 'covid')."},{"metadata":{"trusted":true},"cell_type":"code","source":"### Getting the average search interest at each date\n\ndf['total'] = float(0)\n\n### There will be a warning because we are overwritting a value in the DataFrame. \npd.set_option('mode.chained_assignment', None) #Turns off warning - if you fork, probably turn this back on\n\nfor row in range(len(df)):\n  c = 0\n  for keyword in kw_list:\n    c += df[keyword][row]\n  df['total'][row] = (c/5)\n\ndf_5 = df.copy()\nchange_axis_time(df_5, start_5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\n\nfig = sm.tsa.seasonal_decompose(df_5['total'], period=26)\nplt.plot(fig.trend, color='red', linewidth=3)\nplt.plot(df_5['total'], color='red', linewidth=0.5)\nplt.plot(df_5['covid'], color = 'black', linewidth=0.4)\n\nplt.xticks([26*n for n in range(math.ceil(len(df_5)/26))])\n\nplt.xlim(0, 261)\nplt.ylim(-5, 105)\n\nplt.legend(['average trend', 'average org', 'covid'])\nplt.xlabel('Time')\nplt.ylabel('Search Interest')\nplt.title('Search Interest of Mental Health in the US - 5 year period, Average Trend')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, the seasonal trend becomes a little less obvious. We instead see that these keywords are being searched more over time, suggesting that American mental health is deteriorating slightly every year, or perhaps that destigmatization of mental health encourages google searches of terms related to mental health. To show that more explicitly, let's graph the linear regression on our graph, as well as printing out some key values like the r-value and p-value."},{"metadata":{"trusted":true},"cell_type":"code","source":"### Linear Regression\nx5 = [n for n in range(262)] #262 weeks over the 5 year period\ny5 = [df_5['total'][row] for row in range(262)]\ncoef5 = np.polyfit(x5,y5,1)\nfn5 = np.poly1d(coef5) \n\n### Figuring out statistics of linear regression\nstat = st.linregress(x5, y5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('rvalue - %.4f' % stat[2])\nprint('pvalue - %.4f' % stat[3])\nprint('+%.4f relevance per year' % (stat[0]*52))\n\nplt.figure(figsize=(20,10))\nfig = sm.tsa.seasonal_decompose(df_5['total'], period=26)\nplt.plot(fig.trend, color='red', linewidth=3)\nplt.plot(df_5['total'], color='red', linewidth=0.5)\nplt.plot(df_5['covid'], color = 'black', linewidth=0.4)\nplt.plot(x5, fn5(x5), linestyle='solid', color='blue', linewidth=2)\n\nplt.annotate('+%.4f relevance per week' % stat[0], (100, 85), fontsize=15)\n\nplt.xticks([26*n for n in range(math.ceil(len(df_5)/26))])\n\nplt.xlim(0, 261)\nplt.ylim(-5, 105)\n\nplt.legend(['total trend', 'total org', 'covid', 'line of best fit'])\nplt.xlabel('Time')\nplt.ylabel('Search Interest')\nplt.title('Search Interest of Mental Health in the US - 5 year period, Average Trend with Line of Best Fit')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a clear positive trend with our linear regression, especially considering the high r-value and the p-value of ~0. We must keep this in mind when comparing pre and post quarantine. However, as seen in the above graph as well, the average search interest of these keywords peaks around COVID, seeming to reach almost 90 at its peak."},{"metadata":{},"cell_type":"markdown","source":"# Pre and Post Quarantine\n\nNow that we have analyzed the past 5 years and the trends, let's limit our search to only the past year. I created another dataset for this, which we will load in below."},{"metadata":{"trusted":true},"cell_type":"code","source":"fname = 'https://raw.githubusercontent.com/IronicNinja/covid19api/master/1_year_period.xlsx'\ndf1 = pd.read_excel(fname)\n\ndf1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This dataset starts from 2019-06-02 and goes to 2020-05-31. The main goal of this section is to analyze the first wave of COVID cases. For further study, the second wave, which started around early-mid June, could also be analyzed by loading in more recent data."},{"metadata":{"trusted":true},"cell_type":"code","source":"start_1 = date(2020, 5, 31)\n\ndf_1 = df1.copy()\nchange_axis_time(df_1, start_1)\n\n### Plot the figure\n\nplt.figure(figsize=(20,10))\n\nfor pos in range(len(kw_list)):\n    plt.plot(df_1[kw_list[pos]], color = word_color_list[pos], linewidth=2)\n\nplt.plot(df_1['covid'], color = 'black')\n\nplt.xticks([7.4*n for n in range(math.ceil(len(df_1)/7.4))]) #Every 2 months\n\nplt.xlim(0, 52)\nplt.legend([\"depression\", \"anxiety\", \"panic attack\", \"insomnia\", \"loneliness\", \"covid\"])\nplt.xlabel('Time')\nplt.ylabel('Search Interest')\nplt.title('Search Interest of Mental Health in the US - 1 year period')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems like there's a positive correlation between these search terms and the covid crisis, but it's quite unclear. Instead, let's look at a graph that computes the average search interest of each term and compare that with covid's search interest."},{"metadata":{"trusted":true},"cell_type":"code","source":"### Getting the average search interest at each date\n\ndf1['total'] = float(0)\nfor row in range(len(df1)):\n  c = 0\n  for keyword in kw_list:\n    c += df1[keyword][row]\n  df1['total'][row] = (c/5)\n\ndf_1 = df1.copy()\nchange_axis_time(df_1, start_1)\n\n### Plotting the figure\n\nplt.figure(figsize=(20,10))\nplt.plot(df_1['total'], color = 'red', linewidth=2)\nplt.plot(df_1['covid'], color = 'black', linewidth=0.5)\n\nplt.xticks([7.4*n for n in range(math.ceil(len(df_1)/7.4))])\nplt.xlim(0, 52)\n\nplt.legend([\"total\", \"covid\"])\nplt.xlabel('Time')\nplt.ylabel('Search Interest')\nplt.title('Search Interest of Mental Health in the US, 1 year period - Average')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To better visualize if the coronavirus had a direct impact on search queries, let's add some vertical lines to the graph, each of which will represent an important event."},{"metadata":{"trusted":true},"cell_type":"code","source":"begin = date(2019, 6, 2)\nplt.figure(figsize=(20,10))\nplt.plot(df_1['total'], color = 'red', linewidth=2)\nplt.plot(df_1['covid'], color = 'black', linewidth=0.5)\n\nplt.legend([\"total\", \"covid\"], loc='upper left')\nplt.xticks([7.4*n for n in range(math.ceil(len(df_1)/7.4))])\nplt.xlim(0, 52)\nplt.xlabel('Time')\nplt.ylabel('Search Interest')\nplt.title('Search Interest of Mental Health in the US - Average, with events')\n\nplt.annotate(\"Covid leadup\", (13, 15), fontsize=20)\nplt.annotate(\"Before peak\", (34.5, 15), fontsize=20)\nplt.annotate(\"After peak\", (45, 15), fontsize=20)\n\nplt.plot((date(2020, 2, 29)-begin).days/7, 86.1, 'yo', markersize=10)\nplt.plot((date(2020, 3, 20)-begin).days/7, 87.3, 'yo', markersize=10)\nplt.plot((date(2020, 5, 25)-begin).days/7, 75.2, 'yo', markersize=10)\n\nplt.axvline(x = (date(2020, 1, 5)-begin).days/7, color = 'green') #WHO publishes a report on covid\nplt.axvline(x = (date(2020, 3, 29)-begin).days/7, color = 'green') #Supposed peak of the virus popularity\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Legend:**\n\nFirst Green Line (1/5/2020) - WHO publishes first coronavirus report, we denote this as the start of the covid pandemic being global news\n\nSecond Green Line (3/29/2020) - Covid searches spike\n\nEnd of Search (5/31/2020) - Covid searches at a low since start of outbreak\n\nFirst Yellow Point (2/29/2020) - Surgeon general says masks are not useful towards combatting the spread of the disease\n\nSecond Yellow Point (3/20/2020) - Most American public schools have closed down\n\nThird Yellow Point (5/25/2020) - George Floyd dies, sends America into a frenzy with the BLM movement"},{"metadata":{},"cell_type":"markdown","source":"Now that we have split our graph into three sections (lead up to covid, before peak, after peak), we will perform three linear regressions on each section."},{"metadata":{"trusted":true},"cell_type":"code","source":"### Linear Regression using numpy\n\nstart_date = date(2019, 6, 2)\npoint1 = int(((date(2020, 1, 5) - start_date).days)/7)\npoint2 = int(((date(2020, 3, 29) - start_date).days)/7)\npoint3 = int(((date(2020, 5, 31) - start_date).days)/7)\n\n### Section 1\nx1 = [n for n in range(point1+1)]\ny1 = [df1['total'][row] for row in range(point1+1)]\ncoef1 = np.polyfit(x1,y1,1)\nfn1 = np.poly1d(coef1) \n\n### Section 2\nx2 = [n for n in range(point1, point2+1)]\ny2 = [df1['total'][row] for row in range(point1, point2+1)]\ncoef2 = np.polyfit(x2,y2,1)\nfn2 = np.poly1d(coef2) \n\n### Section 3\nx3 = [n for n in range(point2, point3+1)]\ny3 = [df1['total'][row] for row in range(point2, point3+1)]\ncoef3 = np.polyfit(x3,y3,1)\nfn3 = np.poly1d(coef3) \n\ndf_1 = df1.copy()\nchange_axis_time(df_1, start_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.plot(df_1['total'], color = 'red', linewidth=2)\nplt.plot(df_1['covid'], color = 'black', linewidth=0.5)\nplt.plot(x1, fn1(x1), linestyle='solid', color='blue', linewidth=1.5)\nplt.plot(x2, fn2(x2), linestyle='solid', color='blue', linewidth=1.5)\nplt.plot(x3, fn3(x3), linestyle='solid', color='blue', linewidth=1.5)\n\nplt.xticks([8*n for n in range(math.ceil(len(df_1)/8))])\nplt.legend([\"average\", \"covid\", \"line of best fit\"], loc='upper left')\nplt.xlim(0, 52)\nplt.xlabel('Time')\nplt.ylabel('Search Interest')\nplt.title('Search Interest of Mental Health in the US - Average, with Labels')\n\nplt.annotate(\"Covid leadup\", (13, 15), fontsize=20)\nplt.annotate(\"Before peak\", (34.5, 15), fontsize=20)\nplt.annotate(\"After peak\", (45, 15), fontsize=20)\n\nplt.axvline(x = point1, color = 'green') #WHO publishes a report on covid\nplt.axvline(x = point2, color = 'green') #Supposed peak of the virus popularity\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can use the scipy.stats library to figure out the slope, r-value, and p-value for each section's linear regression."},{"metadata":{"trusted":true},"cell_type":"code","source":"st_list = [st.linregress(x1, y1), st.linregress(x2, y2), st.linregress(x3, y3)]\nheaders_list = ['', 'slope', 'rvalue', 'pvalue']\nvalues_list = [['Covid leadup', 'Before peak', 'After peak']]\n\nfor pos in range(5):\n    if pos != 1 and pos != 4:\n        tmp_list = []\n        for line in st_list:\n            tmp_list.append(round(line[pos], 4))\n        values_list.append(tmp_list)\n        \nlayout = go.Layout(\n    title=go.layout.Title(\n        text=\"Statistical Values for Average Over 1 year period\",\n        x=0.5\n    ),\n  margin=go.layout.Margin(\n        l=0, #left margin\n        r=50, #right margin\n        b=0, #bottom margin\n        t=40  #top margin\n    ), \n  height = 130\n)\n\nfig = go.Figure(data=[go.Table(header=dict(values=headers_list), cells=dict(values=values_list))], layout = layout)\n\n#How to add title/caption?\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the results show, there is positive correlation between total search results and before the peak of COVID-19, and negative correlation between total search results and after the peak of COVID-19. The data leading up to COVID is unclear, and the fluctuations are most likely due to seasonal variances and/or the slightly increasing trend over time. That is fine though, since we are mainly focused on before the peak of COVID and after the peak of COVID, and the COVID leadup section provides a nice baseline for our analysis.\n\nWe can also compare each graph individually against \"covid\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"c = 0\nfig, ax = plt.subplots(5, figsize=(15,30))\nst_list2 = []\n\nfor keyword in kw_list:\n    ### Section 1\n    x1 = [n for n in range(point1+1)]\n    y1 = [df_1[keyword][row] for row in range(point1+1)]\n    coef1 = np.polyfit(x1,y1,1)\n    fn1 = np.poly1d(coef1) \n\n    ### Section 2\n    x2 = [n for n in range(point1, point2+1)]\n    y2 = [df_1[keyword][row] for row in range(point1, point2+1)]\n    coef2 = np.polyfit(x2,y2,1)\n    fn2 = np.poly1d(coef2) \n\n    ### Section 3\n    x3 = [n for n in range(point2, point3+1)]\n    y3 = [df_1[keyword][row] for row in range(point2, point3+1)]\n    coef3 = np.polyfit(x3,y3,1)\n    fn3 = np.poly1d(coef3) \n\n    df1_tmp = df_1.copy()\n\n\n    ### Plot Figure\n\n    ax[c].plot(df_1[keyword], color = 'red', linewidth=2)\n    ax[c].plot(df_1['covid'], color = 'black', linewidth=0.5)\n    ax[c].plot(x1, fn1(x1), color='blue', linestyle='solid', linewidth=1.5)\n    ax[c].plot(x2, fn2(x2), color='blue', linestyle='solid', linewidth=1.5)\n    ax[c].plot(x3, fn3(x3), color='blue', linestyle='solid', linewidth=1.5)\n    ax[c].legend([keyword, \"covid\"], loc='upper left')\n    ax[c].title.set_text('Search Interest of Mental Health in the US, 1 year period with Linear Regressions - ' \n                         + keyword)\n    ax[c].axvline(x = point1, color = 'green') #WHO publishes a report on covid\n    ax[c].axvline(x = point2, color = 'green') #Supposed peak of the virus popularity\n    ax[c].set_xlim([0, 52])\n    ax[c].set_xlabel('Time')\n    ax[c].set_ylabel('Search Interest')\n    ax[c].set_xticks([8*n for n in range(math.ceil(len(df_1)/8))])\n    ax[c].annotate(\"Covid leadup\", (13, 15), fontsize=20)\n    ax[c].annotate(\"Before peak\", (34.5, 15), fontsize=20)\n    ax[c].annotate(\"After peak\", (45, 15), fontsize=20)\n\n    st_list2.append([st.linregress(x1, y1), st.linregress(x2, y2), st.linregress(x3, y3)])\n    c += 1\n\nfig.tight_layout(pad=3)\n    \nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headers_list = ['', 'slope', 'rvalue', 'pvalue']\nhed_list = ['Covid leadup', 'Before peak', 'After peak']\nfig_list = []\ndata_list = []\n\nfor pos in range(3):\n    values_list = []\n    index_list = []\n    for keyword in kw_list:\n        index_list.append(hed_list[pos] + ' - ' + keyword)\n    values_list.append(index_list)\n\n    for count in range(5):\n        if count != 1 and count != 4:\n            tmp_list = []\n            for nested in st_list2:\n                tmp_list.append(round(nested[pos][count], 4))\n            values_list.append(tmp_list)\n            \n    ### table layout\n    layout = go.Layout(\n        title=go.layout.Title(\n            text=\"Statistical Values of Each Keyword, %s\" % hed_list[pos],\n            x=0.5\n        ),\n          margin=go.layout.Margin(\n                l=0, #left margin\n                r=50, #right margin\n                b=0, #bottom margin\n                t=40  #top margin\n            ), \n          height = 180\n        )\n    \n    data_list.append(values_list)\n    fig_list.append(go.Figure(data=[go.Table(header=dict(values=headers_list), cells=dict(values=values_list))], \n                              layout=layout))\n\nfor fig in fig_list:\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can better visualize these numbers by color coding them."},{"metadata":{"trusted":true},"cell_type":"code","source":"### r value analysis based off https://www.dummies.com/education/math/statistics/how-to-interpret-a-correlation-coefficient-r/\n\nheaders_list = ['', 'Covid leadup', 'Before peak', 'After peak']\nvalues_list = [['depression', 'anxiety', 'panic attack', 'insomnia', 'loneliness']]\ncolor_list = [['rgb(100,149,237)']]\n\nfor nested in data_list:\n    tmp_list = []\n    tmp_color_list = []\n    for pos in range(5):\n        ### p value, p < 0.05 else null\n        if(nested[3][pos] < 0.05):\n            ### r values\n            correlation = \"\"\n            color = \"\"\n            if(nested[2][pos] >= 0.7):\n                correlation = \"Strongly Positive\"\n                color = '(124,252,0)'\n            elif(nested[2][pos] >= 0.5):\n                correlation = \"Moderately Positive\"\n                color = '(154,205,50)'\n            elif(nested[2][pos] >= 0.3):\n                correlation = \"Weakly Positive\"\n                color = '(189, 183, 107)'\n            elif(nested[2][pos] > -0.3):\n                correlation = \"No Relationship\"\n                color = '(238, 232, 170)'\n            elif(nested[2][pos] > -0.5):\n                correlation = \"Weakly Negative\"\n                color = '(255, 165, 0)'\n            elif(nested[2][pos] > -0.7):\n                correlation = \"Moderately Negative\"\n                color = '(255, 140, 0)'\n            else:\n                correlation = \"Strongly Negative\"\n                color = '(255, 69, 0)'\n            tmp_list.append(correlation)\n            tmp_color_list.append('rgb'+color)\n        else:\n            tmp_list.append('Null')\n            tmp_color_list.append('rgb(47, 79, 79)')\n    values_list.append(tmp_list)\n    color_list.append(tmp_color_list)\n    \nlayout = go.Layout(\n        title = go.layout.Title(\n            text = \"Statistical Values of Each Keyword, Color Coded\",\n            x=0.5\n        ),\n          margin=go.layout.Margin(\n                l=0, #left margin\n                r=50, #right margin\n                b=0, #bottom margin\n                t=40  #top margin\n            ), \n          height = 180\n        )\n\ntrace = dict(header=dict(values=headers_list, \n                font = dict(color=['rgb(255,255,255)'], size=12),\n             fill=dict(color='rgb(70,130,180)')),\n        cells=dict(values=values_list,\n                   font = dict(color=['rgb(255,255,255)'], size=12),\n                    fill = dict(color=color_list)\n                  )\n            )\n\n\nfig = go.Figure(data=[go.Table(trace)], layout=layout)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see from this chart, although the relevance of mental health search terms during the leadup to covid is unclear, the relevance of mental health search terms increase greatly before the peak of Covid 19, while having a strong decrease in relevance after the peak of Covid 19. This can be explained in a multitude of ways:\n1. A person who is in severe need of mental health help would probably search these terms in the developing stages of Covid 19. If they got their issue resolved or became more educated, they would probably not continue searching the terms. If they didn't get their issue resolved, then they still probably would not continue searching the term after their initial searches.\n2. With large events happening like BLM, the severe decrease in relevance of mental health terms can be attributed to greater focus and attention being put on social movements happening across the country.\n3. The difference could be attributed to the seasonal variation we observed in the last section, but this is unlikely since the search relevance of mental health search terms peaked during the peak of COVID.\n\n**Areas of Further Study:**\n* Is there any correlation between each of these search terms, or do they operate separately? In other words, is someone who searches for a word like \"depression\" more likely to search for a related word like \"anxiety\"?\n* Analysis comparing search relevance with BLM? What about other social movements in the past? Do social movements increase or decrease the relevance of these search terms?\n"},{"metadata":{},"cell_type":"markdown","source":"# Comparing With Income\n\nNow that we have looked at the relevance of mental health search terms before and after COVID's peak, let's take a look and see if socio-economic status (income) plays a role in determining whether someone would search a keyword more or not.\n\nFirst, we initialize our master dataframe with information ranging from population to poverty rate. You can find this dataset on my github repository (or download it by copy pasting the file name into your web browser)."},{"metadata":{"trusted":true},"cell_type":"code","source":"kw_list_avg = [\"depression\", \"anxiety\", \"panic attack\", \"insomnia\", \"loneliness\", \"avg\"]\n\nfname = \"https://raw.githubusercontent.com/IronicNinja/covid19api/master/states_info.xlsx\"\ndf_master = pd.read_excel(fname)\n\nstates_list = {\n'AL': 0, 'AK': 0, 'AZ': 0, 'AR': 0, 'CA': 0, 'CO': 0, 'CT': 0, 'DE': 0, 'FL': 0, 'GA': 0, 'HI': 0, 'ID': 0, 'IL': 0, 'IN': 0,\n'IA': 0, 'KS': 0, 'KY': 0, 'LA': 0, 'ME': 0, 'MD': 0, 'MA': 0, 'MI': 0, 'MN': 0, 'MS': 0, 'MO': 0, 'MT': 0, 'NE': 0, 'NV': 0,\n'NH': 0, 'NJ': 0, 'NM': 0, 'NY': 0, 'NC': 0, 'ND': 0, 'OH': 0, 'OK': 0, 'OR': 0, 'PA': 0, 'RI': 0, 'SC': 0, 'SD': 0, 'TN': 0,\n'TX': 0, 'UT': 0, 'VT': 0, 'VA': 0, 'WA': 0, 'WV': 0, 'WI': 0, 'WY': 0\n}\n\ndf_master.index = states_list #The indices of df_master are now state abbreviations instead of numbers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# state_df = {}\n\n# for state in states_list:\n#   df0 = pd.DataFrame({})\n#   for id in kw_list:\n#     pytrends.build_payload([id], geo='US' + '-' + state, timeframe = search_period)\n#     df0[id] = pytrends.interest_over_time()[id]\n#   state_df[state] = df0\n\n# state_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above code is what we would normally run to get the data. The issue is that, because there are 50 states and 6 operations given for each state, the amount of requests it sends to the Google Trends API can quickly surpass the rate limit (which is supposedly 1400). Instead, I used a script to convert each of the 50 states' dataframes into an excel file, which I loop through and save with the following code."},{"metadata":{"trusted":true},"cell_type":"code","source":"state_df = {}\nfor state in states_list:\n    fname = \"https://raw.githubusercontent.com/IronicNinja/covid19api/master/states/data_\"+state+\".xlsx\"\n    df_tmp = pd.read_excel(fname)\n    state_df[state] = df_tmp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Finding the average and appending it to the dataframe\n\nfor state in states_list:\n    tmp_list = []\n    for row in range(len(state_df[state])):\n        c = 0\n        for keyword in kw_list:\n            c += state_df[state][keyword][row]\n        tmp_list.append(c/5)\n    state_df[state]['avg'] = tmp_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a baseline, we first need to figure out the average of the states with no weights added."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_statetotal = pd.DataFrame(float(0), index=np.arange(len(state_df['AL'])), columns=kw_list_avg)\n\nfor state in states_list:\n    for row in range(53):\n        for keyword in kw_list:\n            value = state_df[state][keyword][row]/50\n            df_statetotal[keyword][row] += value\n            df_statetotal['avg'][row] += (value/5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"change_axis_time(df_statetotal, start_1)\nplt.figure(figsize=(20,10))\n\nfor pos in range(len(kw_list)):\n    plt.plot(df_statetotal[kw_list[pos]], color=word_color_list[pos], linewidth=2)\n\nplt.plot(df_statetotal['avg'], color='black', linewidth=2.5)\n\nplt.xlim(0, 52)\nplt.ylim(-5, 105)\nplt.xticks([7.4*n for n in range(math.ceil(len(df_statetotal)/7.4))])\nplt.legend([\"depression\", \"anxiety\", \"panic attack\", \"insomnia\", \"loneliness\", \"average\"])\nplt.xlabel('Time')\nplt.ylabel('Search Interest')\nplt.title('Search Interest of Mental Health in the US, Average of All States')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we can add our weights, which are average household income, median household income, and poverty rate. For simplicity sake, we are going to use only median household income, since it should be more accurate anyways. The average household income dataframe is still initialized here though."},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_inc = pd.DataFrame(float(0), index=np.arange(len(state_df['AL'])), columns=kw_list_avg)\nmed_inc = pd.DataFrame(float(0), index=np.arange(len(state_df['AL'])), columns=kw_list_avg)\npov_inc = pd.DataFrame(float(0), index=np.arange(len(state_df['AL'])), columns=kw_list_avg)\n\ntotal_inc1 = 0\ntotal_inc2 = 0\npoverty_rate = 0\nfor state in states_list:\n    inc1 = df_master['avg income'][state]\n    inc2 = df_master['median income'][state]\n    poverty = df_master['poverty'][state]\n    total_inc1 += inc1\n    total_inc2 += inc2\n    poverty_rate += poverty\n    for row in range(len(state_df[state])):\n        for col in kw_list:\n            relevance = state_df[state].iloc[row][col]\n            v1 = relevance*inc1\n            v2 = relevance*inc2\n            v3 = relevance*poverty\n            avg_inc.iloc[row][col] += v1\n            avg_inc.iloc[row]['avg'] += v1/5\n            med_inc.iloc[row][col] += v2\n            med_inc.iloc[row]['avg'] += v2/5\n            pov_inc.iloc[row][col] += v3\n            pov_inc.iloc[row]['avg'] += v3/5 \n\n### Normalize\nfor row in range(len(state_df[state])):\n    for col in kw_list_avg:\n        avg_inc.iloc[row][col] /= total_inc1\n        med_inc.iloc[row][col] /= total_inc2\n        pov_inc.iloc[row][col] /= poverty_rate","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can compare our weighted dataset with the average search relevance."},{"metadata":{"trusted":true},"cell_type":"code","source":"### Create subplots\n\nfig, ax = plt.subplots(6, 2, figsize=(15,20))\n\ndef compareplots(med_inc, title, c):\n    for pos in range(len(kw_list_avg)):\n        ax[pos][c].plot(med_inc[kw_list_avg[pos]], color = 'red', linewidth=2)\n        ax[pos][c].plot(df_statetotal[kw_list_avg[pos]], color = 'black', linewidth=2)\n        ax[pos][c].legend([\"median income\", \"average\"], loc='upper left')\n        ax[pos][c].title.set_text('%s vs Average Search Relevance - %s' % \n                                  (title, kw_list_avg[pos]))\n        ax[pos][c].set_xlim([0, 52])\n        ax[pos][c].set_xlabel('Time')\n        ax[pos][c].set_ylabel('Search Interest')\n        ax[pos][c].set_xticks([13*n for n in range(math.ceil(len(df_1)/13))])\n\ncompareplots(med_inc, 'Median Household Income', 0)\ncompareplots(pov_inc, 'Poverty Rate', 1)\n\nfig.tight_layout(pad=3)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, they are pretty similar. A better way to visualize the difference would be to split states into two groups, one that has a higher median household income/lower poverty rate and then another that has a lower median household income/higher poverty rate. It should be noted that the following data does not necessarily indicate individual behavior since we are using general data (relevance among states and state-level data). That is fine for the purposes of this project, as the goal is to analyze the overall trend for the relevance of these search terms; however, more in-depth analysis (through polls or region-level analysis) would be needed to confirm the following results. "},{"metadata":{"trusted":true},"cell_type":"code","source":"inc_list = [df_master['median income'][state] for state in states_list] #Initialize list of median incomes\ninc_list.sort()\n\nmedian_inc = statistics.median(inc_list) #Median of that list\n\ndef inc_conv(median_median):\n    c = 0\n    for states in states_list:\n        inc = df_master['median income'][states]\n        if inc > median_median:\n            c += 1\n\n    print(c) #count number of states above threshold\n\n    high_inc = pd.DataFrame(float(0), index=np.arange(len(state_df['AL'])), columns=kw_list_avg)\n    low_inc = pd.DataFrame(float(0), index=np.arange(len(state_df['AL'])), columns=kw_list_avg)\n\n    high_rate = 0\n    low_rate = 0\n    for states in states_list:\n        inc = df_master['median income'][states]\n        if inc > median_median:\n            high_rate += inc\n        else:\n            low_rate += inc\n        for row in range(len(state_df[states])):\n            for col in kw_list:\n                relevance = state_df[states].iloc[row][col]\n                if inc > median_median:\n                    v1 = relevance*inc\n                    high_inc.iloc[row][col] += v1\n                    high_inc.iloc[row]['avg'] += (v1/5)\n                else:\n                    v2 = relevance*inc\n                    low_inc.iloc[row][col] += v2\n                    low_inc.iloc[row]['avg'] += (v2/5)\n\n    change_axis_time(low_inc, start_1)\n    change_axis_time(high_inc, start_1)\n    \n    for row in range(len(state_df[states])):\n        for col in kw_list_avg:\n            high_inc.iloc[row][col] /= high_rate\n            low_inc.iloc[row][col] /= low_rate\n            \n    return high_inc, low_inc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"high_inc, low_inc = inc_conv(median_inc)\n\ndef plot_diff(low_inc, high_inc, title, legend_list):\n    fig, ax = plt.subplots(6, 2, figsize=(15,20))\n\n    for pos in range(len(kw_list_avg)):\n        ax[pos][0].plot(low_inc[kw_list_avg[pos]], color = 'red', linewidth=2)\n        ax[pos][0].plot(high_inc[kw_list_avg[pos]], color = 'green', linewidth=2)\n        ax[pos][0].legend(legend_list, loc='upper left')\n        ax[pos][0].title.set_text('%s vs %s looking at %s - %s' % \n                                  (legend_list[0], legend_list[1], title, kw_list_avg[pos]))\n        ax[pos][0].set_xlim([0, 52])\n        ax[pos][0].set_xlabel('Time')\n        ax[pos][0].set_ylabel('Search Interest')\n        ax[pos][0].set_xticks([13*n for n in range(math.ceil(len(df_1)/13))])\n\n        ### Percent Differences\n        tmp_list = [] \n        avg_diff = 0\n        for row in range(53):\n            diff = (low_inc[kw_list_avg[pos]][row] - high_inc[kw_list_avg[pos]][row])\n            tmp_list.append(diff)\n            avg_diff += diff\n        \n        avg_diff /= 53\n        \n        ### Change the dates to match so it's graphable\n        temp_list = []\n        for time in range(len(low_inc)):\n            d0 = start_1-datetime.timedelta(days=7*time)\n            d1 = d0.strftime(\"%Y-%m-%d\")\n            temp_list.append(d1)\n\n        temp_list.sort()\n\n        X = [temp_list[n] for n in range(53)]\n        ax[pos][1].bar(X, tmp_list)\n        ax[pos][1].title.set_text('Difference between %s and %s - %s' % \n                                  (legend_list[0], legend_list[1], kw_list_avg[pos]))\n        ax[pos][1].set_xlim([0, 52])\n        ax[pos][1].set_xlabel('Time')\n        ax[pos][1].set_ylabel('Difference')\n        ax[pos][1].set_xticks([13*n for n in range(math.ceil(len(df_1)/13))])\n        \n        max_num = max(tmp_list)\n        ax[pos][1].annotate('avg diff: %s%.2f' % ('+' if avg_diff>0 else '', avg_diff), (40, max_num-2))\n\n    fig.tight_layout(pad=3)\n\n    fig.show()\n      \n### Use a previous function to eliminate repeated code\nplot_diff(low_inc, high_inc, 'Median Household Income', [\"Low Income\", \"High Income\"]) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at median household incomes, it seems that lower income households search these terms a little more than higher income households. The results are still unclear and inconclusive though. Let's instead look at something similar, by using a state's poverty rate instead."},{"metadata":{"trusted":true},"cell_type":"code","source":"pov_list = [df_master['poverty'][state] for state in states_list] #Initialize poverty rates\npov_list.sort()\n\npov_median = statistics.median(pov_list)\n\ndef poverty_conv(pov_rate):\n    c = 0\n    for states in states_list:\n        poverty = df_master['poverty'][states]\n        if poverty > pov_rate:\n            c += 1\n\n    print(c) #Count number of states above threshold\n    \n    high_pov = pd.DataFrame(float(0), index=np.arange(len(state_df['AL'])), columns=kw_list_avg)\n    low_pov = pd.DataFrame(float(0), index=np.arange(len(state_df['AL'])), columns=kw_list_avg)\n\n    high_rate = 0\n    low_rate = 0\n    for states in states_list:\n        poverty = df_master['poverty'][states]\n        if poverty > pov_rate:\n            high_rate += poverty\n        else:\n            low_rate += poverty\n        for row in range(len(state_df[states])):\n            for col in kw_list:\n                relevance = state_df[states].iloc[row][col]\n                if poverty > pov_rate:\n                    v1 = relevance*poverty\n                    high_pov.iloc[row][col] += v1\n                    high_pov.iloc[row]['avg'] += (v1/5)\n                else:\n                    v2 = relevance*poverty\n                    low_pov.iloc[row][col] += v2\n                    low_pov.iloc[row]['avg'] += (v2/5)\n    \n    change_axis_time(high_pov, start_1)\n    change_axis_time(low_pov, start_1)\n    \n    for row in range(len(state_df[states])):\n        for col in kw_list_avg:\n            high_pov.iloc[row][col] /= high_rate\n            low_pov.iloc[row][col] /= low_rate\n   \n    return high_pov, low_pov","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"high_pov, low_pov = poverty_conv(pov_median)\n\nplot_diff(high_pov, low_pov, 'Poverty Rate', [\"High Poverty\", \"Low Poverty\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that on average, states with higher poverty rates search for these terms significantly more than states with lower poverty rates. Looking at the trend through the year, it doesn't seem like either group was more affected by COVID though.\n\nThese results are significant because, at the very least, it means that we should target online counseling, therapy, and other resources towards those in high poverty regions. It could also mean that we should target high poverty regions more with in-person resources and other movements to promote mental health awareness and destigmitize mental health issues.\n\n**Ideas for further study:**\n* Splitting the groups differently, e.g. looking at the top 10 states in terms of poverty rate versus the rest.\n* Looking at regional/county data and analyzing if the same disparity exists between search relevance and socio-economic status.\n* Weighing the data differently, e.g. using another weight like population or eliminating the poverty rate weight. From what I've seen, eliminating the poverty rate weight gives roughly the same results, while adding a weight like population distorts the data to where there aren't any real differences since states like California and Texas consist of over 1/5 of the weight.\n\nIn a later section, we will combine socio-economic status with race."},{"metadata":{},"cell_type":"markdown","source":"# Comparing by Race\n\nThe code for this section is roughly the same as when comparing with income, so there won't be much explanation. For simplicity sake, I split the data into people that are white and people who are not."},{"metadata":{"trusted":true},"cell_type":"code","source":"white_df = pd.DataFrame(float(0), index=np.arange(len(state_df['AL'])), columns=kw_list_avg)\nother_df = pd.DataFrame(float(0), index=np.arange(len(state_df['AL'])), columns=kw_list_avg)\n\nwhite_per = 0\nother_per = 0\nfor state in states_list:\n    white = df_master['white'][state]\n    hispanic = df_master['hispanic'][state]\n    black = df_master['black'][state]\n    asian = df_master['asian'][state]\n    american_indian = df_master['american indian'][state]\n    other = hispanic+black+asian+american_indian\n    white_per += white\n    other_per += other\n    for row in range(len(state_df[state])):\n        for col in kw_list:\n            relevance = state_df[state].iloc[row][col]\n            v1 = relevance*white\n            v2 = relevance*other\n            white_df.iloc[row][col] += v1\n            white_df.iloc[row]['avg'] += v1/5\n            other_df.iloc[row][col] += v2\n            other_df.iloc[row]['avg'] += v2/5 \n\n### Normalize\nfor row in range(len(state_df['AL'])):\n    for col in kw_list_avg:\n        white_df.iloc[row][col] /= white_per\n        other_df.iloc[row][col] /= other_per","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"change_axis_time(white_df, start_1)\nchange_axis_time(other_df, start_1)\n\n### Create subplots\n\nfig, ax = plt.subplots(6, 2, figsize=(15,20))\n\nfor pos in range(len(kw_list_avg)):\n    ax[pos][0].plot(white_df[kw_list_avg[pos]], color = 'grey', linewidth=2)\n    ax[pos][0].plot(other_df[kw_list_avg[pos]], color = 'black', linewidth=2)\n    ax[pos][0].legend([\"white\", \"other\"], loc='upper left')\n    ax[pos][0].title.set_text('Comparing Mental Health Between Races - %s' % kw_list_avg[pos])\n    ax[pos][0].set_xlim([0, 52])\n    ax[pos][0].set_xlabel('Time')\n    ax[pos][0].set_ylabel('Search Interest')\n    ax[pos][0].set_xticks([13*n for n in range(math.ceil(len(df_1)/13))])\n\n    ### Percent Differences\n    tmp_list = []\n    avg_diff = 0\n    for row in range(53):\n        diff = (other_df[kw_list_avg[pos]][row] - white_df[kw_list_avg[pos]][row])\n        tmp_list.append(diff)\n        avg_diff += diff\n    \n    avg_diff /= 53\n    \n    temp_list = []\n    for time in range(len(low_inc)):\n        d0 = start_1-datetime.timedelta(days=7*time)\n        d1 = d0.strftime(\"%Y-%m-%d\")\n        temp_list.append(d1)\n\n    temp_list.sort()\n    \n    X = [temp_list[n] for n in range(53)]\n    ax[pos][1].bar(X, tmp_list)\n    ax[pos][1].title.set_text('Differences between Other Races and White - %s' % kw_list_avg[pos])\n    ax[pos][1].set_xlim([0, 52])\n    ax[pos][1].set_xlabel('Time')\n    ax[pos][1].set_ylabel('Difference')\n    ax[pos][1].set_xticks([13*n for n in range(math.ceil(len(df_1)/13))])\n    max_num = max(tmp_list)\n    ax[pos][1].annotate('avg diff - %s%.2f' % ('+' if avg_diff>0 else '', avg_diff), (10, max_num-0.5))\n    \nfig.tight_layout(pad=3)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a pretty noticeable discrepancy here. Additionally, it seems as if the differences grew larger when COVID-19 hit."},{"metadata":{"trusted":true},"cell_type":"code","source":"### We will use the two temporary lists from the above code for simplicity sake\n\nplt.figure(figsize=(20,10))\n\n### Section 1\nx1 = [n for n in range(point1+1)]\ny1 = [tmp_list[n] for n in range(point1+1)]\ncoef1 = np.polyfit(x1,y1,1)\nfn1 = np.poly1d(coef1) \n\n### Section 2\nx2 = [n for n in range(point1, point2+1)]\ny2 = [tmp_list[n] for n in range(point1, point2+1)]\ncoef2 = np.polyfit(x2,y2,1)\nfn2 = np.poly1d(coef2) \n\n### Section 3\nx3 = [n for n in range(point2, point3+1)]\ny3 = [tmp_list[n] for n in range(point2, point3+1)]\ncoef3 = np.polyfit(x3,y3,1)\nfn3 = np.poly1d(coef3) \n\n### Plot Figure\n\nplt.plot(temp_list, tmp_list, color = 'red', linewidth=2)\nplt.plot(x1, fn1(x1), color='blue', linestyle='solid', linewidth=1.5)\nplt.plot(x2, fn2(x2), color='blue', linestyle='solid', linewidth=1.5)\nplt.plot(x3, fn3(x3), color='blue', linestyle='solid', linewidth=1.5)\nplt.legend([\"avg diff between races\", \"line of best fit\"], loc='upper left')\nplt.title('Differences between Races - avg')\nplt.axvline(x = point1, color = 'green') #WHO publishes a report on covid\nplt.axvline(x = point2, color = 'green') #Supposed peak of the virus popularity\nplt.xlim(0, 52)\nplt.ylim(bottom=0)\nplt.xlabel('Time')\nplt.ylabel('Difference')\nplt.xticks([7.4*n for n in range(math.ceil(len(df_1)/7.4))])\nplt.annotate(\"Covid leadup\", (13, 1), fontsize=20)\nplt.annotate(\"Before peak\", (34.5, 1), fontsize=20)\nplt.annotate(\"After peak\", (45, 1), fontsize=20)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"st_list = [st.linregress(x1, y1), st.linregress(x2, y2), st.linregress(x3, y3)]\nheaders_list = ['', 'slope', 'rvalue', 'pvalue']\nvalues_list = [['Covid leadup', 'Before peak', 'After peak']]\n\nfor pos in range(5):\n    if pos != 1 and pos != 4:\n        tmp_list = []\n        for line in st_list:\n              tmp_list.append(round(line[pos], 4))\n        values_list.append(tmp_list)\n        \nlayout = go.Layout(\n  margin=go.layout.Margin(\n        l=0, #left margin\n        r=50, #right margin\n        b=0, #bottom margin\n        t=0  #top margin\n    ), \n  height = 100\n)\n\nfig = go.Figure(data=[go.Table(header=dict(values=headers_list), cells=dict(values=values_list))], layout = layout)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is quite clear that people of non-white races were effected more heavily than people of the white race.\n\n**Areas for Further Study:**\n* Comparing races between each other. I didn't do that here as I believed the above would be the best visual representation of the discrepancy of search mental health search terms."},{"metadata":{},"cell_type":"markdown","source":"# Comparing Search Relevance to both Income and Race\n\nNow we can combine the two demographic factors."},{"metadata":{"trusted":true},"cell_type":"code","source":"white_df_high = pd.DataFrame(float(0), index=np.arange(len(state_df['AL'])), columns=kw_list_avg)\nother_df_high = pd.DataFrame(float(0), index=np.arange(len(state_df['AL'])), columns=kw_list_avg)\nother_df_low = pd.DataFrame(float(0), index=np.arange(len(state_df['AL'])), columns=kw_list_avg)\nwhite_df_low = pd.DataFrame(float(0), index=np.arange(len(state_df['AL'])), columns=kw_list_avg)\n\nwhite_per_high = 0\nwhite_per_low = 0\nother_per_high = 0\nother_per_low = 0\nmed_inc = statistics.median(inc_list)\n\nfor state in states_list:\n    white = df_master['white'][state]\n    hispanic = df_master['hispanic'][state]\n    black = df_master['black'][state]\n    asian = df_master['asian'][state]\n    american_indian = df_master['american indian'][state]\n    other = hispanic+black+asian+american_indian\n    inc = df_master['median income'][state]\n    if inc > med_inc:\n        high_pov += poverty\n        white_per_high += white\n        other_per_high += other\n    else:\n        low_pov += poverty\n        white_per_low += white\n        other_per_low += other\n        \n    for row in range(len(state_df[state])):\n        for col in kw_list:\n            relevance = state_df[state].iloc[row][col]\n            v1 = relevance*white\n            v2 = relevance*other\n            if inc > med_inc:\n                white_df_high.iloc[row][col] += v1\n                white_df_high.iloc[row]['avg'] += v1/5\n                other_df_high.iloc[row][col] += v2\n                other_df_high.iloc[row]['avg'] += v2/5 \n            else:\n                white_df_low.iloc[row][col] += v1\n                white_df_low.iloc[row]['avg'] += v1/5\n                other_df_low.iloc[row][col] += v2\n                other_df_low.iloc[row]['avg'] += v2/5 \n                \n### Normalize\nfor row in range(len(state_df['AL'])):\n    for col in kw_list_avg:\n        white_df_high.iloc[row][col] /= (white_per_high)\n        white_df_low.iloc[row][col] /= (white_per_low)\n        other_df_high.iloc[row][col] /= (other_per_high)\n        other_df_low.iloc[row][col] /= (other_per_low)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"change_axis_time(white_df_high, start_1)\nchange_axis_time(white_df_low, start_1)\nchange_axis_time(other_df_high, start_1)\nchange_axis_time(other_df_low, start_1)\n\n### Create subplots\n\nfig, ax = plt.subplots(6, figsize=(15,30))\n\nfor pos in range(len(kw_list_avg)):\n    ax[pos].plot(white_df_high[kw_list_avg[pos]], color = 'grey', linewidth=2)\n    ax[pos].plot(white_df_low[kw_list_avg[pos]], color = 'green', linewidth=2)\n    ax[pos].plot(other_df_high[kw_list_avg[pos]], color = 'black', linewidth=2)\n    ax[pos].plot(other_df_low[kw_list_avg[pos]], color = 'yellow', linewidth=2)\n    ax[pos].legend([\"white, high poverty\", \"white, low poverty\", \"other, high poverty\", \"other, low poverty\"], \n                   loc='upper left')\n    ax[pos].title.set_text('Search Relevance Looking at both Race and Socio-economic Status - %s' % kw_list_avg[pos])\n    ax[pos].set_xlim([0, 52])\n    ax[pos].set_xlabel('Time')\n    ax[pos].set_ylabel('Search Interest')\n    ax[pos].set_xticks([7.4*n for n in range(math.ceil(len(df_1)/7.4))])\n    \nfig.tight_layout(pad=3)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data is somewhat inconclusive, so we will also look at using poverty rate."},{"metadata":{"trusted":true},"cell_type":"code","source":"white_df_high = pd.DataFrame(float(0), index=np.arange(len(state_df['AL'])), columns=kw_list_avg)\nother_df_high = pd.DataFrame(float(0), index=np.arange(len(state_df['AL'])), columns=kw_list_avg)\nother_df_low = pd.DataFrame(float(0), index=np.arange(len(state_df['AL'])), columns=kw_list_avg)\nwhite_df_low = pd.DataFrame(float(0), index=np.arange(len(state_df['AL'])), columns=kw_list_avg)\n\nwhite_per_high = 0\nwhite_per_low = 0\nother_per_high = 0\nother_per_low = 0\npov_rate = 0.131\n\nfor state in states_list:\n    white = df_master['white'][state]\n    hispanic = df_master['hispanic'][state]\n    black = df_master['black'][state]\n    asian = df_master['asian'][state]\n    american_indian = df_master['american indian'][state]\n    other = hispanic+black+asian+american_indian\n    poverty = df_master['poverty'][state]\n    if poverty > pov_rate:\n        white_per_high += white\n        other_per_high += other\n    else:\n        white_per_low += white\n        other_per_low += other\n        \n    for row in range(len(state_df[state])):\n        for col in kw_list:\n            relevance = state_df[state].iloc[row][col]\n            v1 = relevance*white\n            v2 = relevance*other\n            if poverty > pov_rate:\n                white_df_high.iloc[row][col] += v1\n                white_df_high.iloc[row]['avg'] += v1/5\n                other_df_high.iloc[row][col] += v2\n                other_df_high.iloc[row]['avg'] += v2/5 \n            else:\n                white_df_low.iloc[row][col] += v1\n                white_df_low.iloc[row]['avg'] += v1/5\n                other_df_low.iloc[row][col] += v2\n                other_df_low.iloc[row]['avg'] += v2/5 \n                \n### Normalize\nfor row in range(len(state_df['AL'])):\n    for col in kw_list_avg:\n        white_df_high.iloc[row][col] /= (white_per_high)\n        white_df_low.iloc[row][col] /= (white_per_low)\n        other_df_high.iloc[row][col] /= (other_per_high)\n        other_df_low.iloc[row][col] /= (other_per_low)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"change_axis_time(white_df_high, start_1)\nchange_axis_time(white_df_low, start_1)\nchange_axis_time(other_df_high, start_1)\nchange_axis_time(other_df_low, start_1)\n\n### Create subplots\n\nfig, ax = plt.subplots(6, figsize=(15,30))\n\nfor pos in range(len(kw_list_avg)):\n    ax[pos].plot(white_df_high[kw_list_avg[pos]], color = 'grey', linewidth=2)\n    ax[pos].plot(white_df_low[kw_list_avg[pos]], color = 'green', linewidth=2)\n    ax[pos].plot(other_df_high[kw_list_avg[pos]], color = 'black', linewidth=2)\n    ax[pos].plot(other_df_low[kw_list_avg[pos]], color = 'yellow', linewidth=2)\n    ax[pos].legend([\"white, high poverty\", \"white, low poverty\", \"other, high poverty\", \"other, low poverty\"], \n                   loc='upper left')\n    ax[pos].title.set_text('Search Interest of Mental Health in the US, Poverty vs Average - %s' % kw_list_avg[pos])\n    ax[pos].set_xlim([0, 52])\n    ax[pos].set_xlabel('Time')\n    ax[pos].set_ylabel('Search Interest')\n    ax[pos].set_xticks([7.4*n for n in range(math.ceil(len(df_1)/7.4))])\n    \nfig.tight_layout(pad=3)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are a couple of caviats to this analysis. The first is that the results should be expected; if people with lower income search these terms more, while people who are not wite also search these terms more, it should no surprise that when we consider both income and race, the people with lower income and of a non-white race search these terms more. This analysis of combining the two categories, however, is merely correlative. In fact, given the current results, the differences could be much, much higher. We did not take into consideration that minorities may have lower average income than people who are white. "},{"metadata":{},"cell_type":"markdown","source":"# Comparing Republicans vs Democrats\n\nWe can see a clear upwards trend of Americans searching terms for mental health after covid 19. The next question we ask ourselves is: is there a difference in the relevance of these search terms between republican and democrat states? If so, what might be a reason for the difference? Let's explore this question in depth."},{"metadata":{"trusted":true},"cell_type":"code","source":"kw_listextra = [\"depression\", \"anxiety\", \"panic attack\", \"insomnia\", \"loneliness\", \"covid\"]\n\n### Plotting function which we initialize now    \n\ndef graph(repub_df, democrat_df):\n    change_axis_time(repub_df, start_1)\n    change_axis_time(democrat_df, start_1)\n\n    ### republicans\n    plt.figure(figsize=(20,10))\n    for pos in range(len(kw_list)):\n        plt.plot(repub_df[kw_list[pos]], color = word_color_list[pos], linewidth=2)\n\n    plt.plot(repub_df['avg'], color = 'black', linewidth=2)\n    \n    ### list comprehension where each date is spaced out for every 8 weeks\n    plt.xticks([8*n for n in range(math.ceil(len(repub_df)/8))])\n    plt.legend([\"depression\", \"anxiety\", \"panic attack\", \"insomnia\", \"loneliness\", \"average\"])\n    plt.xlabel('Time')\n    plt.ylabel('Search Interest')\n    plt.title('Search Interest of Mental Health in the US - Republicans')\n\n    ### democrats\n    plt.figure(figsize=(20,10))\n    for pos in range(len(kw_list)):\n        plt.plot(democrat_df[kw_list[pos]], color = word_color_list[pos], linewidth=2)\n        \n    plt.plot(democrat_df['avg'], color = 'black')\n\n    plt.xticks([7.4*n for n in range(math.ceil(len(repub_df)/7.4))])\n    plt.legend([\"depression\", \"anxiety\", \"panic attack\", \"insomnia\", \"loneliness\", \"average\"])\n    plt.xlabel('Time')\n    plt.ylabel('Search Interest')\n    plt.title('Search Interest of Mental Health in the US - Democrats')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are multiple algorithms we use, each of which covers a different weighing method. This section will use an algorithm that weighs population and the percentage of republicans vs democrats. "},{"metadata":{"trusted":true},"cell_type":"code","source":"repub_df = pd.DataFrame(float(0), index=np.arange(len(state_df['AL'])), columns=kw_list_avg)\ndemocrat_df = pd.DataFrame(float(0), index=np.arange(len(state_df['AL'])), columns=kw_list_avg)\n\nrepub_total = 0\ndemocrat_total = 0\nfor state in states_list:\n    population = df_master['population'][state]\n    repub_p = df_master['repub'][state]\n    democrat_p = df_master['democrat'][state]\n    partisan_p = repub_p+democrat_p\n    repub_total += population*repub_p\n    democrat_total += population*democrat_p\n    for row in range(len(state_df[state])):\n        for col in kw_list:\n            relevance = state_df[state].iloc[row][col]\n            v1 = relevance*population*repub_p/partisan_p\n            v2 = relevance*population*democrat_p/partisan_p\n            repub_df.iloc[row][col] += v1\n            repub_df.iloc[row]['avg'] += v1/5\n            democrat_df.iloc[row][col] += v2\n            democrat_df.iloc[row]['avg'] += v2/5\n\nrepub_dfc = repub_df.copy()\ndemocrat_dfc = democrat_df.copy()\n\n### Normalize the data by dividing it by the total population\n\nfor row in range(len(state_df['AL'])):\n    for col in kw_list_avg:\n        repub_dfc.iloc[row][col] *= (democrat_total/(democrat_total+repub_total)/100)\n        democrat_dfc.iloc[row][col] *= (repub_total/(democrat_total+repub_total)/100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def weighted_analysis(repub_dfc, democrat_dfc, algotype):\n    c = 0\n    fig, ax = plt.subplots(6, figsize=(15,30))\n\n    for keyword in kw_list_avg:\n        ax[c].plot(repub_dfc[keyword], color = 'red')\n        ax[c].plot(democrat_dfc[keyword], color = 'blue')\n        ax[c].legend([\"republican\", \"democrat\"], loc='upper left')\n        ax[c].title.set_text('Search Interest of Mental Health in the US, Republican vs Democrat, %s - %s' % (algotype, keyword))\n        ax[c].set_xlim([0, 52])\n        ax[c].set_xlabel('Time')\n        ax[c].set_ylabel('Search Interest')\n        ax[c].set_xticks([7.4*n for n in range(math.ceil(len(df_1)/7.4))])\n\n        c += 1\n\n    fig.show()\n    \nchange_axis_time(repub_dfc, start_1)\nchange_axis_time(democrat_dfc, start_1)\nweighted_analysis(repub_dfc, democrat_dfc, 'weighted')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although this algorithm should give us the most accurate representation since it takes into consideration the percentage of both republicans and democrats, it doesn't give show us the true differences between republican states and democrat states. As seen from the graphs, the trends generally have the same shape for both states and the search interests aren't much different.\n\nWe can also look at a graph that is more black & white, where a state can either be considered republican, democrat, or neither (if the percentages are the same)."},{"metadata":{"trusted":true},"cell_type":"code","source":"### by political party, weighted by population and repub vs democrat percentage // weighted, black & white\n\nrepub_df2 = pd.DataFrame(float(0), index=np.arange(len(state_df['AL'])), columns=kw_list_avg)\ndemocrat_df2 = pd.DataFrame(float(0), index=np.arange(len(state_df['AL'])), columns=kw_list_avg)\n\npopulation_total = 0\nrepub_population = 0\ndemocrat_population = 0\nfor state in states_list:\n    population = df_master['population'][state]\n    repub_p = df_master['repub'][state]\n    democrat_p = df_master['democrat'][state]\n    if repub_p > democrat_p:\n        repub_population += population\n    elif democrat_p > repub_p:\n        democrat_population += population\n    \n    for row in range(len(state_df[state])):\n        for col in kw_list:\n            relevance = state_df[state].iloc[row][col]\n            v1 = relevance*population\n            ### conditional statement\n            if repub_p > democrat_p:\n                repub_df2.iloc[row][col] += v1\n                repub_df2.iloc[row]['avg'] += v1/5\n            elif democrat_p > repub_p:\n                democrat_df2.iloc[row][col] += v1\n                democrat_df2.iloc[row]['avg'] += v1/5\n\n### Normalizing the data as the population of democrat states is higher than of republican states\n\nfor row in range(len(state_df['AL'])):\n    for col in kw_list_avg:\n        repub_df2.iloc[row][col] /= repub_population\n        democrat_df2.iloc[row][col] /= democrat_population\n\ngraph(repub_df2, democrat_df2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weighted_analysis(repub_df2, democrat_df2, 'divided')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In these series of graphs, we have normalized the search interest by dividing by the total population of each state. We can still see these terms are searched more in democrat states than republican states.\n\nWith the final algorithm, I decided to take a black & white approach but where larger states don't get weighted as heavily. I settled upon using the electoral college system, where the largest states have no more than 20 times the votes of the smallest states. Compare that with the other algorithms, where the largest states could have over 50 times the population compared to the smallest states. All data is pulled from https://www.270towin.com/."},{"metadata":{"trusted":true},"cell_type":"code","source":"### by electoral college\n\nrepub_df3 = pd.DataFrame(float(0), index=np.arange(len(state_df['AL'])), columns=kw_list_avg)\ndemocrat_df3 = pd.DataFrame(float(0), index=np.arange(len(state_df['AL'])), columns=kw_list_avg)\nind_df3 = pd.DataFrame(float(0), index=np.arange(len(state_df['AL'])), columns=kw_list_avg)\n\nrepub_votes = 0\ndemocrat_votes = 0\nind_votes = 0\npopulation_total = 0\nfor state in states_list:\n    votes = df_master['votes'][state]\n    party = df_master['party'][state]\n    if party == 'R':\n        repub_votes += votes\n    elif party == 'D':\n        democrat_votes += votes\n    else:\n        ind_votes += votes\n        \n    for row in range(len(state_df[state])):\n        for col in kw_list:\n            relevance = state_df[state].iloc[row][col]\n            v1 = relevance*votes\n            if party == 'R':\n                repub_df3.iloc[row][col] += v1\n                repub_df3.iloc[row]['avg'] += v1/5\n            elif party == 'D':\n                democrat_df3.iloc[row][col] += v1\n                democrat_df3.iloc[row]['avg'] += v1/5\n            else:\n                ind_df3.iloc[row][col] += v1\n                ind_df3.iloc[row]['avg'] += v1/5\n                    \nfor row in range(len(state_df['AL'])):\n    for col in kw_list_avg:\n        repub_df3.iloc[row][col] /= repub_votes\n        democrat_df3.iloc[row][col] /= democrat_votes\n        ind_df3.iloc[row][col] /= ind_votes\n\nchange_axis_time(repub_df3, start_1)\nchange_axis_time(democrat_df3, start_1)\nchange_axis_time(ind_df3, start_1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I created a category for the states where there was not enough political distinction for the website to differentiate, which is denoted as \"neither\". There are only 5 states are in this category though, and they represent 69 out of 538 of the total votes."},{"metadata":{"trusted":true},"cell_type":"code","source":"c = 0\nfig, ax = plt.subplots(6, figsize=(15,30))\n\nfor keyword in kw_list_avg:\n    ax[c].plot(repub_df3[keyword], color = 'red')\n    ax[c].plot(democrat_df3[keyword], color = 'blue')\n    ax[c].plot(ind_df3[keyword], color = 'green')\n    ax[c].legend([\"republican\", \"democrat\", \"neither\"], loc='upper left')\n    ax[c].title.set_text('Search Interest of Mental Health in the US, Republican vs Democrat, Electoral - ' + keyword)\n    ax[c].set_xlim([0, 52])\n    ax[c].set_xlabel('Time')\n    ax[c].set_ylabel('Search Interest')\n    ax[c].set_xticks([7.4*n for n in range(math.ceil(len(df_1)/7.4))])\n\n    c += 1\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's compare the difference between republican states and democrat states with each of the three algorithms."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"algo_list = []\ndef sumdiff(repub_dfc, democrat_dfc):\n    tmp_list = []\n    df_perc = pd.DataFrame(0, index=np.arange(len(state_df['AL'])), columns=kw_list)\n    for row in range(len(repub_dfc)):\n        c = 0\n        for keyword in kw_list:\n            diff = (democrat_dfc[keyword][row] - repub_dfc[keyword][row])\n            df_perc[keyword][row] += diff\n            c += diff\n        tmp_list.append(c/5)\n    df_perc['total'] = tmp_list\n    change_axis_time(df_perc, start_1)\n    algo_list.append(df_perc)\n\nsumdiff(repub_dfc, democrat_dfc) #algo 1\nsumdiff(repub_df2, democrat_df2) #algo 2\nsumdiff(repub_df3, democrat_df3) #algo 3\n\nkey_list = [keyword for keyword in kw_list]\nkey_list.append('total')\n\nc = 0\nfig, ax = plt.subplots(5, figsize=(15,30))\n\nfor keyword in kw_list:\n    comb = [algo_list[0][keyword][n]+algo_list[1][keyword][n]+algo_list[2][keyword][n] for n in range(53)]\n    ax[c].plot(algo_list[0][keyword], color = 'orange')\n    ax[c].plot(algo_list[1][keyword], color = 'blue')\n    ax[c].plot(algo_list[2][keyword], color = 'green')\n    ax[c].plot(comb, color='red', linewidth=2)\n    ax[c].hlines(0, 0, 53)\n    \n    ax[c].legend([\"weighted\", \"divided\", \"electoral\", \"combined\"], loc='upper left')\n    ax[c].title.set_text('Search Interest of Mental Health in the US, Difference - ' + keyword)\n    ax[c].set_xlim([0, 52])\n    ax[c].set_xlabel('Time')\n    ax[c].set_ylabel('Difference Between Democrats and Republicans')\n    ax[c].set_xticks([7.4*n for n in range(math.ceil(len(df_1)/7.4))])\n    \n    X = [n for n in range(53)]\n    y_int = integrate.cumtrapz(comb, X, initial=0) #Integrate\n    \n    max_num = max(comb)\n    ax[c].annotate('Avg Diff - +%.4f' % (y_int[52]/52), (8.5, max_num-5), fontsize=15)\n    \n    c += 1\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There seems to be a significant difference, where on average, democrats search mental health search terms far more than republicans. We can also look at the total percentage difference to get a better idea of the cumulative average over all five search terms."},{"metadata":{"trusted":true},"cell_type":"code","source":"total_list = pd.DataFrame({})\n\nfor row in range(53):\n    tmp_list = [0, 0, 0, 0] #weighted divided electoral combined\n    for keyword in kw_list:\n        comb = [algo_list[0][keyword][n]+algo_list[1][keyword][n]+algo_list[2][keyword][n] for n in range(53)]\n        for pos in range(3):\n            tmp_list[pos] += (algo_list[pos][keyword][row]/5)\n        tmp_list[3] += (comb[row]/5)\n    total_list[row] = tmp_list\n\ntotal_list = total_list.T\ntotal_listc = total_list.copy()\nchange_axis_time(total_listc, start_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.plot(total_listc[0], color = 'orange')\nplt.plot(total_listc[1], color = 'blue')\nplt.plot(total_listc[2], color = 'green')\nplt.plot(total_listc[3], color = 'red', linewidth=2)\n\nplt.xticks([7.4*n for n in range(math.ceil(len(df_1)/7.4))])\nplt.legend([\"weighted\", \"divided\", \"electoral\", \"combined\"], loc='upper left')\nplt.hlines(0, 0, 53)\n\nplt.xlim([0, 52])\nplt.xlabel('Time')\nplt.ylabel('Difference')\nplt.title('Search Interest of Mental Health in the US, Republicans vs Democrats, Total Difference')\n\n    \nX = [n for n in range(53)]\ny_int = integrate.cumtrapz(total_listc[3], X, initial=0) #Integrate\n\nplt.annotate('Avg Diff - +%.4f' % (y_int[52]/52), (7.5, 23), fontsize=15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once again, on average, democrat states search these terms more than republican states. It is hard to make a definitive judgment when comparing democrat states and the states in the \"neither\" category, most likely because of the small sample size in the \"neither\" category. \n\nThe discrepancy between democrat states and republican states' search interest of these terms cannot be underlooked. This discrepancy may arise from factors that would suggest that, on average, COVID19 and the events surrounding it has worsened mental health problems more for democrats than republicans. There are other variables, such as BLM, that could suggest such a trend. \n\nOne possible non-mental health related explanation for this discrepancy was the possibility that democrat states use google more than republican states; however, for Google Trends, \"Each data point is divided by the total searches of the geography and time range it represents\", according to https://ahrefs.com/blog/how-to-use-google-trends-for-keyword-research. \n\nWe will also perform a series of linear regressions to determine whether Democrats or Republicans were more heavily affected by COVID-19."},{"metadata":{"trusted":true},"cell_type":"code","source":"### Linear Regression using numpy\n\nstart_date = date(2019, 6, 2)\npoint1 = int(((date(2020, 1, 5) - start_date).days)/7)\npoint2 = int(((date(2020, 3, 29) - start_date).days)/7)\npoint3 = int(((date(2020, 5, 31) - start_date).days)/7)\n\n### Section 1\nx1 = [n for n in range(point1+1)]\ny1 = [total_list[3][row] for row in range(point1+1)]\ncoef1 = np.polyfit(x1,y1,1)\nfn1 = np.poly1d(coef1) \n\n### Section 2\nx2 = [n for n in range(point1, point2+1)]\ny2 = [total_list[3][row] for row in range(point1, point2+1)]\ncoef2 = np.polyfit(x2,y2,1)\nfn2 = np.poly1d(coef2) \n\n### Section 3\nx3 = [n for n in range(point2, point3+1)]\ny3 = [total_list[3][row] for row in range(point2, point3+1)]\ncoef3 = np.polyfit(x3,y3,1)\nfn3 = np.poly1d(coef3) \n\ntotal_listc2 = total_list.copy()\n\nchange_axis_time(total_listc2, start_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.plot(total_listc2[3], color = 'red', linewidth=2)\nplt.plot(x1, fn1(x1), linestyle='solid', color='blue', linewidth=1.5)\nplt.plot(x2, fn2(x2), linestyle='solid', color='blue', linewidth=1.5)\nplt.plot(x3, fn3(x3), linestyle='solid', color='blue', linewidth=1.5)\n\nplt.xticks([8*n for n in range(math.ceil(len(df_1)/8))])\nplt.legend([\"total\", \"line of best fit\"], loc='upper left')\nplt.xlim(0, 52)\nplt.xlabel('Time')\nplt.ylabel('Percentage Difference')\nplt.title('Search Interest of Mental Health in the US - Democrat vs Republican, Linear Regressions')\n\nplt.annotate(\"Covid leadup\", (13, 2), fontsize=20)\nplt.annotate(\"Before peak\", (34.5, 2), fontsize=20)\nplt.annotate(\"After peak\", (45, 2), fontsize=20)\n\nplt.axvline(x = point1, color = 'green') #WHO publishes a report on covid\nplt.axvline(x = point2, color = 'green') #Supposed peak of the virus popularity\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"st_list = [st.linregress(x1, y1), st.linregress(x2, y2), st.linregress(x3, y3)]\nheaders_list = ['', 'slope', 'rvalue', 'pvalue']\nvalues_list = [['Covid leadup', 'Before peak', 'After peak']]\n\nfor pos in range(5):\n    if pos != 1 and pos != 4:\n        tmp_list = []\n        for line in st_list:\n            tmp_list.append(round(line[pos], 4))\n        values_list.append(tmp_list)\n        \nlayout = go.Layout(\n  margin=go.layout.Margin(\n        l=0, #left margin\n        r=50, #right margin\n        b=0, #bottom margin\n        t=0  #top margin\n    ), \n  height = 100\n)\n\nfig = go.Figure(data=[go.Table(header=dict(values=headers_list), cells=dict(values=values_list))], layout = layout)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can clearly see that democrats were harmed more mentally than republicans due to COVID 19, while the data for the COVID leadup and after the peak of COVID is inconclusive since p > 0.05."},{"metadata":{},"cell_type":"markdown","source":"# Comparing Mental Health Searches with Covid Numbers by State\n\nI decided that the best way to continue getting data was to webscrape it off of the worldometers covid counter, but because they don't publicly store their data, I had to get the missing data by accessing a dataset from https://usafacts.org/visualizations/coronavirus-covid-19-spread-map/. For the spreadsheet and the web scraping procedures, check out my github repository at https://github.com/IronicNinja/covid19api.\n\nFor this data, our search period will be from 1/22 (when the first US COVID cases were recorded) to 7/5. The algorithm weighs the relevance using the COVID population of a state and then normalizes the data by dividing by the number of COVID cases on that day. This will allow us to look at a trend from the beginning of the pandemic to the end without needing to do much more math to take into account the exponential growth of COVID cases."},{"metadata":{"trusted":true},"cell_type":"code","source":"state_df1 = {}\nfor state in states_list:\n    fname = \"https://raw.githubusercontent.com/IronicNinja/covid19api/master/states_covid/data_\"+state+\".xlsx\"\n    df_tmp = pd.read_excel(fname)\n    state_df1[state] = df_tmp\n\nfname = \"https://raw.githubusercontent.com/IronicNinja/covid19api/master/state_covid19.xlsx\"\ndf2 = pd.read_excel(fname)\nstart_2 = date(2020, 7, 5)\n\n\ndays = 8 #Start on the first sunday, which has an ID of 8\nrow = 0\nc = 0\n\nday_df = pd.DataFrame({})\nweek_df = pd.DataFrame({})\ntmp_list_week = {\n    \"depression\": 0, \"anxiety\": 0, \"panic attack\": 0, \"insomnia\": 0, \"loneliness\": 0, \"avg\": 0\n}\n### Search up to the week of 7/5\nwhile days < 170:\n    if (days-1)%7 == 0 and days != 8:\n        s = pd.Series(tmp_list_week)\n        week_df[(days-8)/7] = s\n        for keyword in tmp_list_week:\n            tmp_list_week[keyword] = 0\n            \n    tmp_list = {\n        \"depression\": 0, \"anxiety\": 0, \"panic attack\": 0, \"insomnia\": 0, \"loneliness\": 0, \"avg\": 0\n    }\n    \n    total_population = 0\n    ### There's a row I added which is 'total', if you delete that row then just use len(df2) for the range\n    for row in range(len(df2)-1):\n        population_tmp = df2.iloc[row][days]\n        total_population += population_tmp\n        for keyword in kw_list:\n            v1 = (state_df1[df2.iloc[row]['states']][keyword][days-8])*population_tmp\n            tmp_list[keyword] += v1\n            tmp_list['avg'] += v1/5\n    for keyword in tmp_list:\n        tmp_list[keyword] /= total_population\n        tmp_list_week[keyword] += tmp_list[keyword]/7\n        \n    s = pd.Series(tmp_list)\n    day_df[days-8] = s\n    days += 1\n\n\n### Transpose the week_df dataframe so the dates are the rows\n\ndf3 = day_df.T\n\ndef change_days(df3, start):\n    temp_list = []\n    for time in range(len(df3)):\n        d0 = start-datetime.timedelta(days=time)\n        d1 = d0.strftime(\"%Y-%m-%d\")\n        temp_list.append(d1)\n\n    temp_list.sort()\n    df3.index = temp_list\n\nchange_days(df3, start_2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will initialize the average search relevance to compare with."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_state_covid = pd.DataFrame(float(0), index=np.arange(len(state_df1['AL'])), columns=kw_list_avg)\n\nfor state in states_list:\n    for row in range(len(state_df1[state])):\n        for keyword in kw_list:\n            v1 = (state_df1[state][keyword][row]/50)\n            df_state_covid[keyword][row] += v1\n            df_state_covid['avg'][row] += v1/5\n\nchange_days(df_state_covid, start_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(6, 2, figsize=(15,30))\n\nc = 0\nfor keyword in kw_list_avg:\n    fig1 = sm.tsa.seasonal_decompose(df3[keyword], period=7) #Period is weekly\n    fig2 = sm.tsa.seasonal_decompose(df_state_covid[keyword], period=7)\n    ax[c][0].plot(fig1.trend, color='red', linewidth=3)\n    ax[c][0].plot(df3[keyword], color = 'red', linewidth=0.5)\n    ax[c][0].plot(fig2.trend, color='black', linewidth=3)\n    ax[c][0].plot(df_state_covid[keyword], color = 'black', linewidth=0.5)\n    ax[c][0].legend([\"weighted with covid cases, trend\", \"covid org\", \"average trend\", \"average org\"], loc='upper left')\n    \n    ax[c][0].title.set_text('Comparing COVID Cases with the Average - ' + keyword)\n    ax[c][0].set_xlim([0, 161])\n    ax[c][0].set_ylim([0, 105])\n    ax[c][0].set_xlabel('Time')\n    ax[c][0].set_ylabel('Search Interest')\n    ax[c][0].set_xticks([24*n for n in range(math.ceil(len(df3)/24))])\n    \n    tmp_list = [] \n    avg_diff = 0\n    for row in range(len(df3)):\n        diff = (df3[keyword][row] - df_state_covid[keyword][row])\n        tmp_list.append(diff)\n        avg_diff += diff\n\n    avg_diff /= len(df3)\n        \n    temp_list = []\n    for time in range(len(df3)):\n        d0 = start_1-datetime.timedelta(days=7*time)\n        d1 = d0.strftime(\"%Y-%m-%d\")\n        temp_list.append(d1)\n\n    temp_list.sort()\n\n    X = [temp_list[n] for n in range(len(df3))]\n    ax[c][1].bar(X, tmp_list)\n    ax[c][1].title.set_text('Difference between COVID Cases and Average - %s' % keyword)\n    ax[c][1].set_xlim([0, 161])\n    ax[c][1].set_xlabel('Time')\n    ax[c][1].set_ylabel('Difference')\n    ax[c][1].set_xticks([24*n for n in range(math.ceil(len(df3)/24))])\n\n    max_num = max(tmp_list)\n    ax[c][1].annotate('avg diff: %s%.2f' % ('+' if avg_diff>0 else '', avg_diff), (120, max_num-2))\n    \n    c += 1\n\nfig.tight_layout(pad=3)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the daily data, it seems like the more COVID cases in a state, the more people in that state that search these mental health keywords. This can mean that we should prioritize mental health resources towards states & regions that have more COVID cases.\n\n**Areas of Further Study:**\n* Look at past flu outbreaks / pandemics like H1N1 and do the same analysis there to see if this is a consistent trend."},{"metadata":{},"cell_type":"markdown","source":"# America vs Rest of World\n\nTo conclude, we can quickly compare the relevance of these search terms in America with the rest of the world."},{"metadata":{"trusted":true},"cell_type":"code","source":"fname = 'https://raw.githubusercontent.com/IronicNinja/covid19api/master/worldwide.xlsx'\ndf_world = pd.read_excel(fname)\n\ndf_world","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_world['total'] = float(0)\nfor row in range(len(df_world)):\n    c = 0\n    for keyword in kw_list:\n        c += df_world[keyword][row]\n    df_world['total'][row] = (c/5)\n\nchange_axis_time(df_world, start_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.plot(df1['total'], color = 'red', linewidth=2)\nplt.plot(df_world['total'], color = 'black', linewidth=2)\n\nplt.xticks([7.4*n for n in range(math.ceil(len(df_1)/7.4))])\nplt.legend([\"US\", \"worldwide\"], loc='upper left')\nplt.xlim(0, 52)\nplt.ylim(-5, 105)\nplt.xlabel('Time')\nplt.ylabel('Search Interest')\nplt.title('Search Interest of Mental Health in the US - Average, with Labels')\n\nplt.annotate(\"Covid leadup\", (13, 15), fontsize=20)\nplt.annotate(\"Before peak\", (34.5, 15), fontsize=20)\nplt.annotate(\"After peak\", (45, 15), fontsize=20)\n\nplt.axvline(x = point1, color = 'green') #WHO publishes a report on covid\nplt.axvline(x = point2, color = 'green') #Supposed peak of the virus popularity\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c = 0\nfig, ax = plt.subplots(5, figsize=(15,30))\n\nfor keyword in kw_list:\n    ax[c].plot(df1[keyword], color='red', linewidth=2)\n    ax[c].plot(df_world[keyword], color='black', linewidth=2)\n    ax[c].legend([\"US\", \"worldwide\"])\n    ax[c].title.set_text(\"America vs the Rest of the World - %s\" % keyword)\n    ax[c].set_xlim([0, 52])\n    ax[c].set_xlabel('Time')\n    ax[c].set_ylabel('Search Interest')\n    ax[c].set_xticks([7.4*n for n in range(math.ceil(len(df_world)/7.4))])\n    \n    c += 1\n    \n    \nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There doesn't seem to be much of a difference here."},{"metadata":{},"cell_type":"markdown","source":"# Solutions\n\nBefore I give my personal solutions, let's take a look at the search relevance of potential solutions that an individual could search, like \"rehab\" and \"online therapy\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"fname = 'https://raw.githubusercontent.com/IronicNinja/covid19api/master/treatment.xlsx'\ntreatment_df = pd.read_excel(fname)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kw_list_treatment = [\"rehab\", \"meditation\", \"online therapy\", \"herbs\", \"online counseling\"]\nc = 0\nfig, ax = plt.subplots(5, figsize=(15,30))\nst_list2 = []\n\nfor keyword in kw_list_treatment:\n    ### Section 1\n    x1 = [n for n in range(point1+1)]\n    y1 = [treatment_df[keyword][row] for row in range(point1+1)]\n    coef1 = np.polyfit(x1,y1,1)\n    fn1 = np.poly1d(coef1) \n\n    ### Section 2\n    x2 = [n for n in range(point1, point2+1)]\n    y2 = [treatment_df[keyword][row] for row in range(point1, point2+1)]\n    coef2 = np.polyfit(x2,y2,1)\n    fn2 = np.poly1d(coef2) \n\n    ### Section 3\n    x3 = [n for n in range(point2, point3+1)]\n    y3 = [treatment_df[keyword][row] for row in range(point2, point3+1)]\n    coef3 = np.polyfit(x3,y3,1)\n    fn3 = np.poly1d(coef3) \n\n    df1_tmp = df_1.copy()\n\n\n    ### Plot Figure\n    ax[c].plot(treatment_df[keyword], color = 'red', linewidth=2)\n    ax[c].plot(df_1['covid'], color = 'black', linewidth=0.5)\n    ax[c].plot(x1, fn1(x1), color='blue', linestyle='solid', linewidth=1.5)\n    ax[c].plot(x2, fn2(x2), color='blue', linestyle='solid', linewidth=1.5)\n    ax[c].plot(x3, fn3(x3), color='blue', linestyle='solid', linewidth=1.5)\n    ax[c].legend([keyword, \"covid\"], loc='upper left')\n    ax[c].title.set_text('Search Interest of Mental Health in the US, 1 year period - ' + keyword)\n    ax[c].axvline(x = point1, color = 'green') #WHO publishes a report on covid\n    ax[c].axvline(x = point2, color = 'green') #Supposed peak of the virus popularity\n    ax[c].set_xlim([0, 52])\n    ax[c].set_xlabel('Time')\n    ax[c].set_ylabel('Search Interest')\n    ax[c].set_xticks([7.4*n for n in range(math.ceil(len(treatment_df)/7.4))])\n    ax[c].annotate(\"Covid leadup\", (13, 15), fontsize=20)\n    ax[c].annotate(\"Before peak\", (34.5, 15), fontsize=20)\n    ax[c].annotate(\"After peak\", (45, 15), fontsize=20)\n\n    st_list2.append([st.linregress(x1, y1), st.linregress(x2, y2), st.linregress(x3, y3)])\n    c += 1\n\n    fig.tight_layout(pad=3)\n\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headers_list = ['', 'slope', 'rvalue', 'pvalue']\nhed_list = ['Covid leadup', 'Before peak', 'After peak']\nfig_list = []\ndata_list = []\n\nfor pos in range(3):\n    values_list = []\n    index_list = []\n    for keyword in kw_list:\n        index_list.append(hed_list[pos] + ' - ' + keyword)\n    values_list.append(index_list)\n\n    for count in range(5):\n        if count != 1 and count != 4:\n            tmp_list = []\n            for nested in st_list2:\n                tmp_list.append(round(nested[pos][count], 4))\n            values_list.append(tmp_list)\n            \n    ### table layout\n    layout = go.Layout(\n        title = go.layout.Title(\n            text=\"Treatment Search Relevance - %s\" % hed_list[pos], \n            x=0.5\n        ),\n          margin=go.layout.Margin(\n                l=0, #left margin\n                r=50, #right margin\n                b=0, #bottom margin\n                t=40  #top margin\n            ), \n          height = 180\n        )\n        \n    data_list.append(values_list)\n    fig_list.append(go.Figure(data=[go.Table(header=dict(values=headers_list), cells=dict(values=values_list))], \n                              layout=layout))\n\nfor fig in fig_list:\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"headers_list = ['', 'Covid leadup', 'Before peak', 'After peak']\nvalues_list = [kw_list_treatment]\ncolor_list = [['rgb(100,149,237)']]\n\nfor nested in data_list:\n    tmp_list = []\n    tmp_color_list = []\n    for pos in range(5):\n        ### p value, p < 0.05 else null\n        if(nested[3][pos] < 0.05):\n            ### r values\n            correlation = \"\"\n            color = \"\"\n            if(nested[2][pos] >= 0.7):\n                correlation = \"Strongly Positive\"\n                color = '(124,252,0)'\n            elif(nested[2][pos] >= 0.5):\n                correlation = \"Moderately Positive\"\n                color = '(154,205,50)'\n            elif(nested[2][pos] >= 0.3):\n                correlation = \"Weakly Positive\"\n                color = '(189, 183, 107)'\n            elif(nested[2][pos] > -0.3):\n                correlation = \"No Relationship\"\n                color = '(238, 232, 170)'\n            elif(nested[2][pos] > -0.5):\n                correlation = \"Weakly Negative\"\n                color = '(255, 165, 0)'\n            elif(nested[2][pos] > -0.7):\n                correlation = \"Moderately Negative\"\n                color = '(255, 140, 0)'\n            else:\n                correlation = \"Strongly Negative\"\n                color = '(255, 69, 0)'\n            tmp_list.append(correlation)\n            tmp_color_list.append('rgb'+color)\n        else:\n            tmp_list.append('Null')\n            tmp_color_list.append('rgb(47, 79, 79)')\n    values_list.append(tmp_list)\n    color_list.append(tmp_color_list)\n    \nlayout = go.Layout(\n        title=go.layout.Title(\n            text=\"Treatment Relevance Color Coded\",\n            x=0.5\n        ),\n          margin=go.layout.Margin(\n                l=0, #left margin\n                r=50, #right margin\n                b=0, #bottom margin\n                t=0  #top margin\n            ), \n          height = 150\n        )\n\ntrace = dict(header=dict(values=headers_list, font = dict(color=['rgb(255,255,255)'], size=12),\n             fill=dict(color='rgb(70,130,180)')),\n        cells=dict(values=values_list,\n                   font = dict(color=['rgb(255,255,255)'], size=12),\n                    fill = dict(color=color_list)\n                  )\n            )\n\n\nfig = go.Figure(data=[go.Table(trace)], layout=layout)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aside from \"herbs\" and perhaps \"online therapy\", it's unclear whether COVID had a positive impact on the relevance of these treatment search terms. However, from the previous sections, we can come up with some solutions to combat mental health issues during COVID-19.\n\nFirst, it is imperative that we know who to target, which was the main purpose of this kernel. From our analyses, we concluded that minorities in poorer regions should be targetted with more online resources. For example, mental health advocates/people who want to spread awareness can focus on and target their advertisements to people in these regions.\n\nSecond, it is important to destigmatize mental health issues, especially in the areas mentioned above. A reason people may google these terms more in low income, minority areas is because mental health be more stigmatized there. For example, if you are a member of the \"hood\", then an individual with mental health issues would potentially not speak out in fear of being called \"soft\".\n\nThird and perhaps most importantly, we need to make sure more people are safe and don't put others in danger of the coronavirus, as the number one factor that is leading to more depressive thoughts and actions would be quarantine (social isolation). The Trump administration needs to do a better job in mitigating the spread of the coronavirus, and according to the data, republicans need to become more educated and aware of the impact social isolation has on mental health so that we as a society can work towards stopping this pandemic."},{"metadata":{},"cell_type":"markdown","source":"# Important Notes\n\nThere are some important caviats with this analysis. First, the five keywords that we chose do not have equal relevance, even though in the analysis we treat them like they do. This is because in the analysis, we are looking for upwards trends, and not necessarily judging which word is the most relevant or popular. The visual below shows how comparatively relevant each word is."},{"metadata":{"trusted":true},"cell_type":"code","source":"pytrends = TrendReq(hl='en-US', tz=420)\npytrends.build_payload(kw_list, geo='US', timeframe = '2019-6-2 2020-5-31')\ndf1 = pytrends.interest_over_time()\ndf2 = df1.drop(columns = ['isPartial'])\n\ny = []\n\nfor keyword in kw_list:\n  tmp_list = []\n  for n in range(53): #Analyzing 53 weeks\n    tmp_list.append(df2.iloc[n][keyword]/2.16) #March 29 relevance sum divided by 100\n  y.append(tmp_list)\n\nx = [date(2019, 6, 2)+datetime.timedelta(days=7*n) for n in range(53)]\n\nplt.figure(figsize=(20,10))\nplt.stackplot(x,y, labels=kw_list)\nplt.plot(x, df_1['covid'], color='black')\nplt.xlabel('Time')\nplt.ylabel('Search Interest')\nplt.legend(loc='upper left')\nplt.xlim(date(2019, 6, 2), date(2020, 5, 31))\nplt.title('Combined Search Interest of Mental Health in the US')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Same thing with the relevance of terms that signify mental health treatment."},{"metadata":{"trusted":true},"cell_type":"code","source":"pytrends = TrendReq(hl='en-US', tz=420)\npytrends.build_payload(kw_list_treatment, geo='US', timeframe = '2019-6-2 2020-5-31')\ndf1 = pytrends.interest_over_time()\ndf2 = df1.drop(columns = ['isPartial'])\n\ny = []\n\nfor keyword in kw_list_treatment:\n    tmp_list = []\n    for n in range(53): #Analyzing 53 weeks\n        tmp_list.append(df2.iloc[n][keyword]/2.16) #March 29 relevance sum divided by 100\n    y.append(tmp_list)\n\nx = [date(2019, 6, 2)+datetime.timedelta(days=7*n) for n in range(53)]\n\nplt.figure(figsize=(20,10))\nplt.stackplot(x,y, labels=kw_list_treatment)\nplt.plot(x, df_1['covid'], color='black')\nplt.xlabel('Time')\nplt.ylabel('Search Interest')\nplt.legend(loc='upper left')\nplt.xlim(date(2019, 6, 2), date(2020, 5, 31))\nplt.title('Combined Search Interest of Mental Health Treatment in the US')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another important note is that people who search these terms may not necessarily be depressed or even exhibit depressive attitudes; they may instead be educating themselves on these issues. However, we can infer that there are constantly people trying to educate themselves on mental health, and so the sudden increase in mental health search term relevance can be attributed to COVID itself.\n\nIf you made it down here, thank you for reading! I really enjoyed putting everything together, from the datasets to the graphs themselves. If you have any questions, feel free to comment or message me, and if you enjoyed this kernel, it would be greatly appreciated if you gave it an upvote!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}