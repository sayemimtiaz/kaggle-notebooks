{"cells":[{"metadata":{},"cell_type":"markdown","source":"# REGRESSION From Scratch With SALES PREDICTION"},{"metadata":{},"cell_type":"markdown","source":"<img src='https://drive.google.com/uc?id=1-6z0sZc9YrK_czjy8mBQuxBj3wdD01-V' width=800 >"},{"metadata":{},"cell_type":"markdown","source":"#### In this Notebook we will Learn:-\n* Basic EDA.\n* Feature Engineering\n* Dealing with missing values.\n* Aplly Scaling on Feature matrix.\n* Dealing with Categorical Dataset.\n* Dimensionality Reduction (PCA) .\n* K-Cross validation to check accuracy.\n* Multi-linear Regression\n* Random Forest Regressor\n* Polynomial Regression\n* Prediction on new Values."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly.offline import init_notebook_mode, download_plotlyjs, iplot\nimport cufflinks as cf\ninit_notebook_mode(connected=True)\ncf.go_offline()\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nprint()\nprint(\"The files in the dataset are:-\")\nfrom subprocess import check_output\nprint(check_output(['ls','../input']).decode('utf'))\n\n# Any results you write to the current directory are saved as output.\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Importing the datasets\ndf_train = pd.read_csv('../input/Train.csv')\ndf_test = pd.read_csv('../input/Test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BASIC ANALYSIS AND FEATURES ENGINEERING"},{"metadata":{},"cell_type":"markdown","source":"#### 1). Removing Unwanted Columns/Features."},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    df_train.drop(labels=['Item_Identifier', 'Outlet_Identifier', 'Outlet_Establishment_Year'], axis=1, inplace=True)\n    df_test.drop(labels=['Item_Identifier', 'Outlet_Identifier', 'Outlet_Establishment_Year'], axis=1, inplace=True)\nexcept Exception as e:\n    pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2). Getting Information about Null values,"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df = df_train.isnull().sum().reset_index()\ntemp_df['Percentage'] = (temp_df[0]/len(df_train))*100\ntemp_df.columns = ['Column Name', 'Number of null values', 'Null values in percentage']\nprint(f\"The length of dataset is \\t {len(df_train)}\")\ntemp_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* So it is clear that we do not have to remove null values, as they 28% and 17% in the Outlet_Size and Item_Weight Columns respectively.\n* Null values are in less quantity.\n* We will replace them later with thier mean or mode values."},{"metadata":{},"cell_type":"markdown","source":"#### 3). Making Correction in 'Item_Fat_Content' column."},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert(x):\n    if x in ['low fat', 'LF']: \n        return 'Low Fat'\n    elif x=='reg':\n        return 'Regular'\n    else:\n        return x\n\ndf_train['Item_Fat_Content'] = df_train['Item_Fat_Content'].apply(convert)\ndf_test['Item_Fat_Content'] = df_train['Item_Fat_Content'].apply(convert)\n\nprint(f\"Now Unique values in this column in Train Set are\\t  {df_train['Item_Fat_Content'].unique()} \")\nprint(f\"Now Unique values in this column in Test Set are\\t  {df_test['Item_Fat_Content'].unique()} \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4). Dealing with the Missing Values in Categorical type column i.e. 'Outlet_Size'"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Counting the values\ncount = df_train['Outlet_Size'].value_counts().reset_index()\ncount.iplot(kind='bar', color='deepskyblue', x='index', y='Outlet_Size', title='High VS Mediun VS Small',\n           xTitle='Size', yTitle='Frequency')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We will remove the missing values from 'Medium' in both Training set and Test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Outlet_Size'].fillna(value='Medium', inplace= True)\ndf_test['Outlet_Size'].fillna(value='Medium', inplace= True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ==============================================================================="},{"metadata":{},"cell_type":"markdown","source":"# PREDICTION WITH REGRESSION MODELS."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us Import the Important Libraries  to train our Model for Machine Learning \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder # To deal with Categorical Data in Target Vector.\nfrom sklearn.model_selection import train_test_split  # To Split the dataset into training data and testing data.\nfrom sklearn.model_selection import cross_val_score   # To check the accuracy of the model.\nfrom sklearn.preprocessing import Imputer   # To deal with the missing values\nfrom sklearn.preprocessing import StandardScaler   # To appy scaling on the dataset.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us create feature matrix and Target Vector.\nx_train = df_train.iloc[:, :-1].values    # Features Matrix\ny_train = df_train.iloc[:,-1].values   # Target Vector\nx_test = df_test.values    # Features Matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1). Dealing with Missing data."},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer = Imputer()\nx_train[:,[0]] = imputer.fit_transform(x_train[:,[0]])\nx_test[:,[0]] = imputer.fit_transform(x_test[:,[0]])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2). Dealing With the Categorical Values in Features/Columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"labelencoder_x = LabelEncoder()\nx_train[:, 1 ] = labelencoder_x.fit_transform(x_train[:,1 ])\nx_train[:, 3 ] = labelencoder_x.fit_transform(x_train[:,3 ])\nx_train[:, 5 ] = labelencoder_x.fit_transform(x_train[:,5 ])\nx_train[:, 6 ] = labelencoder_x.fit_transform(x_train[:,6 ])\nx_train[:, 7 ] = labelencoder_x.fit_transform(x_train[:,7 ])\n\n\n#this is need to done when we have more than two categorical values.\nonehotencoder_x = OneHotEncoder(categorical_features=[3,5,6,7]) \nx_train = onehotencoder_x.fit_transform(x_train).toarray()\n\n# Let's apply same concept on test set.\nx_test[:, 1 ] = labelencoder_x.fit_transform(x_test[:,1 ])\nx_test[:, 3 ] = labelencoder_x.fit_transform(x_test[:,3 ])\nx_test[:, 5 ] = labelencoder_x.fit_transform(x_test[:,5 ])\nx_test[:, 6 ] = labelencoder_x.fit_transform(x_test[:,6 ])\nx_test[:, 7 ] = labelencoder_x.fit_transform(x_test[:,7 ])\n\n\n#this is need to done when we have more than two categorical values.\nonehotencoder_x = OneHotEncoder(categorical_features=[3,5,6,7]) \nx_test = onehotencoder_x.fit_transform(x_test).toarray()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3). Now time to Apply Feature Scaling on Feature matrix ."},{"metadata":{"trusted":true},"cell_type":"code","source":"sc_X=StandardScaler()\nx_train=sc_X.fit_transform(x_train)\nx_test = sc_X.fit_transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4). DIMENSIONALITY REDUCTION\n* We are doing this to reduce the number of dimensions/features in the dataset.\n* The features which have less effect on the prediction , we will remove those features.\n* It also boosts the process.\n* It saves time.\n* Here we will use Principal Component Analysis (PCA) with 'rbf' kernel."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=None)\nx_train = pca.fit_transform(x_train)\nx_test = pca.fit_transform(x_test)\nexplained_variance = pca.explained_variance_ratio_\nexplained_variance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Here we will take n_component = 24."},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=25)\nx_train = pca.fit_transform(x_train)\nx_test = pca.fit_transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5). Apply Multi-Linear Regression Model, Polynomial Regression and Random Forest Model and compare thier accuracy and pick the best one."},{"metadata":{},"cell_type":"markdown","source":"#### Multi-Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Multi-linear regression Model.\nregressor_multi = LinearRegression()\nregressor_multi.fit(x_train,y_train)\n\n# Let us check the accuray\naccuracy = cross_val_score(estimator=regressor_multi, X=x_train, y=y_train,cv=10)\nprint(f\"The accuracy of the Multi-linear Regressor Model is \\t {accuracy.mean()}\")\nprint(f\"The deviation in the accuracy is \\t {accuracy.std()}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Random Forest Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"# Random Forest Model.\nregressor_random = RandomForestRegressor(n_estimators=100,)\nregressor_random.fit(x_train,y_train)\n\n# Let us check the accuray\naccuracy = cross_val_score(estimator=regressor_random, X=x_train, y=y_train,cv=10)\nprint(f\"The accuracy of the Random Forest Model is \\t {accuracy.mean()}\")\nprint(f\"The deviation in the accuracy is \\t {accuracy.std()}\") \"\"\"\n\nprint(\"Here accuray is 53% with deviation of 3%.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Polynomial regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n# Fitting polynomial regression to dataset\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_reg=PolynomialFeatures(degree=4) #These 3 steps are to convert X matrix into X polynomial\nx_poly=poly_reg.fit_transform(x_train) #matrix. \nregressor_poly=LinearRegression()\nregressor_poly.fit(x_poly,y_train)\n\n# Let us check the accuray\naccuracy = cross_val_score(estimator=regressor_poly, X=x_train, y=y_train,cv=10)\nprint(f\"The accuracy of the Polynomial Regression Model is \\t {accuracy.mean()}\")\nprint(f\"The deviation in the accuracy is \\t {accuracy.std()}\")\n\"\"\"\nprint(\"Here accuracy is 55% with deviation of 2%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### observation:-\n* As the accuracy of Multi-linear regression Model is the best one.\n* Multi-linear Regression Model takes less time as compare to Random forest and Polynomial regression Models.\n* We will choose Multi-linear regression Model.\n* Here we are getting the accuracy of 55% and deviation of 2%, means in future if we mak eprediction on new values then we will get the accuracy in range 53% to 57%.\n* We are getting low accuracy due to less quantity of data."},{"metadata":{},"cell_type":"markdown","source":"### Let us make Prediction on test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = regressor_multi.predict(x_test)\n\ny_pred[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ============================================================================\n### ============================================================================\n### ============================================================================\n### ============================================================================"},{"metadata":{},"cell_type":"markdown","source":"# IF THIS KERNEL IS HELPFUL, THEN PLEASE UPVOTE.\n<img src='https://drive.google.com/uc?id=1LBdaJj2pTM0cq9PY6k70RaGfUFDakUzG' width=500 >"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}