{"cells":[{"metadata":{},"cell_type":"markdown","source":"# This is a simplistic attempt to get basic baseline from different models\n## Models used include\n* XGBoost\n* Keras\n* RandomForest\n* DecisionTrees\n* AdaBoost\n* VotingClassifier (Hard + Soft)\n* Others tried but discarded include MLP, Gaussian, NB \n\n## New Feature were created as well\n* Current code only keeps the features which provide a spearman correlation of +/-1 sigma to churn. Everything else is dropped. You can experiment with all features or change the sigma threshold in the code (I did not get any major difference in results)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom inspect import signature\nfrom sklearn.metrics import *\nfrom sklearn.utils.multiclass import unique_labels\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras import optimizers\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"file1 = \"/kaggle/input/telecom-customer/Telecom_customer churn.csv\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(file1)\ndf_o = df.copy() #I just made a copy for easier experimentation during code writing.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Perform any data exploration below."},{"metadata":{"trusted":true},"cell_type":"code","source":"## all experiments for exploration are discarded as mainly it was a combination of excel and tableau before I decided to use all features.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We will now reset the df and start the feature processing. (Exploration work will be unsaved)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df_o.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=True,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    #classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax\n\ndef plot_roc(y_true, y_pred, title):\n    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n    #print(fpr, tpr, prec, rec)\n    plt.plot(fpr, tpr)\n    plt.plot(fpr,tpr,linestyle = \"dotted\",\n             color = \"royalblue\",linewidth = 2,\n             label = \"AUC = \" + str(np.around(roc_auc_score(y_true,y_pred),3)))\n    plt.legend(loc='best')\n    plt.plot([0,1], [0,1])\n    plt.xticks(np.arange(0,1.1,0.1))\n    plt.yticks(np.arange(0,1.1,0.1))\n    plt.grid(b=True, which='both')\n    plt.title(title)\n    plt.xticks(np.arange(0, 1.1, 0.1))\n    plt.yticks(np.arange(0, 1.1, 0.1))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# first of all, lets try to see the non-numeric fields.\n# list them down and explore 1 by 1 please\ndf.columns[df.dtypes.values == \"object\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categ_nominal = ['new_cell', 'asl_flag', 'prizm_social_one', 'area', 'dualband', 'refurb_new', 'hnd_webcap',\n                 'ownrent', 'dwlltype', 'marital', 'infobase', 'HHstatin', 'dwllsize', 'ethnic',\n                 'kid0_2', 'kid3_5', 'kid6_10', 'kid11_15', 'kid16_17', 'creditcd']\n\ncateg_ordinal_ordered = ['crclscod']  #order this as alphabet sort and then assign numbers.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pd.get_dummies(df['new_cell'], prefix=\"new_cell\", dummy_na=True)\n\n#df_p = df.copy()\n\nfor i in categ_nominal:\n    df = pd.concat([df, pd.get_dummies(df[i], prefix=i, dummy_na=True)], sort=False, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(categ_nominal, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in categ_ordinal_ordered:\n    s_a = sorted(df[i].unique())\n    s_a_dict = {i:x for x,i in enumerate(s_a)}\n    df[i] = df[i].map(s_a_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns[df.dtypes.values == \"object\"] #recheck if there are any more object types remaining in the dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Below are the new features we are building"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['vce_blk_rate'] = 0\ndf.loc[ df['plcd_vce_Mean'] > 0, 'vce_blk_rate'] = df['blck_vce_Mean'] / df['plcd_vce_Mean']\n\ndf['vce_drp_rate'] = 0\ndf.loc[ df['plcd_vce_Mean'] > 0, 'vce_drp_rate'] = df['drop_vce_Mean'] / df['plcd_vce_Mean']\n\ndf['dat_blk_rate'] = 0\ndf.loc[ df['plcd_dat_Mean'] > 0, 'dat_blk_rate'] = df['blck_dat_Mean'] / df['plcd_dat_Mean']\n\ndf['dat_drp_rate'] = 0\ndf.loc[ df['plcd_dat_Mean'] > 0, 'dat_drp_rate'] = df['drop_dat_Mean'] / df['plcd_dat_Mean']\n\ndf['vce_cmpt_rate'] = 0\ndf.loc[ df['plcd_vce_Mean'] > 0, 'vce_cmpt_rate'] = df['comp_vce_Mean'] / df['plcd_vce_Mean']\n\ndf['dat_cmpt_rate'] = 0\ndf.loc[ df['plcd_dat_Mean'] > 0, 'dat_cmpt_rate'] = df['comp_dat_Mean'] / df['plcd_dat_Mean']\n\ndf['tot_cmpt_rate'] = 0\ndf.loc[ df['attempt_Mean'] > 0, 'tot_cmpt_rate'] = df['complete_Mean'] / df['attempt_Mean']\n\ndf['tot_drp_blk_rate'] = 0\ndf.loc[ df['attempt_Mean'] > 0, 'tot_drp_blk_rate'] = df['drop_blk_Mean'] / df['attempt_Mean']\n\ndf['vce_dat_ratio'] = 0\ndf.loc[ (df['plcd_vce_Mean'] + df['plcd_dat_Mean']) > 0, 'tot_drp_blk_rate'] = df['plcd_vce_Mean'] /  (df['plcd_vce_Mean'] + df['plcd_dat_Mean'])\n\ndf['diff_3mon_overall_mou'] = 0\ndf.loc[ (df['avgmou'] == df['avgmou']) & (df['avg3mou'] == df['avg3mou']), 'diff_3mon_overall_mou'] = (df['avg3mou'] - df['avgmou']) / df['avgmou']\n\ndf['diff_3mon_overall_qty'] = 0\ndf.loc[ (df['avgqty'] == df['avgqty']) & (df['avg3qty'] == df['avg3qty']), 'diff_3mon_overall_qty'] = (df['avg3qty'] - df['avgqty']) / df['avgqty']\n\ndf['diff_3mon_overall_rev'] = 0\ndf.loc[ (df['avgrev'] == df['avgrev']) & (df['avg3rev'] == df['avg3rev']), 'diff_3mon_overall_rev'] = (df['avg3rev'] - df['avgrev']) / df['avgrev']\n\ndf['diff_6mon_overall_mou'] = 0\ndf.loc[ (df['avgmou'] == df['avgmou']) & (df['avg6mou'] == df['avg6mou']), 'diff_6mon_overall_mou'] = (df['avg6mou'] - df['avgmou']) / df['avgmou']\n\ndf['diff_6mon_overall_qty'] = 0\ndf.loc[ (df['avgqty'] == df['avgqty']) & (df['avg6qty'] == df['avg6qty']), 'diff_6mon_overall_qty'] = (df['avg6qty'] - df['avgqty']) / df['avgqty']\n\ndf['diff_6mon_overall_rev'] = 0\ndf.loc[ (df['avgrev'] == df['avgrev']) & (df['avg6rev'] == df['avg6rev']), 'diff_6mon_overall_rev'] = (df['avg6rev'] - df['avgrev']) / df['avgrev']\n\ndf['total_nulls'] = 0\ndf.loc[:, 'total_nulls'] = np.sum(pd.isnull(df), axis=1)\n\ndf['eqpdays_digitized'] = np.digitize(df['eqpdays'], bins=[-10, 30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330, 360, 390, 5000])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_mat = df.corr(method='spearman')\n# nothing\n#sorted(corr_mat['churn'], reverse=True)\n\n# nothing\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_mat.loc[ corr_mat['churn'] == 1, 'churn'] = np.nan\ndev = 1\ns_6 = corr_mat['churn'].mean() + corr_mat['churn'].std() * dev\ns__6 = corr_mat['churn'].mean() - corr_mat['churn'].std() * dev\nprint(s_6, s__6)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduced_features = corr_mat[(corr_mat['churn'] >= s_6) | (corr_mat['churn'] <= s__6)].index.values.tolist()\nreduced_features.extend(['churn'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[reduced_features].copy() #reducing the features based on standard deviation on the spearman correlation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_o = df.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **ML Starting below**"},{"metadata":{"trusted":true},"cell_type":"code","source":"RANDOM_STATE = 91\nTEST_SIZE = 0.3\nmodels = dict() #trained models will be kept in this dict as \"ModelName\": Model\n\nclassifiers = {\n    \"Decision trees\" : DecisionTreeClassifier(max_depth=5),\n    \"Random Forest\" : RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    \"Adaboost\" : AdaBoostClassifier(n_estimators=200, learning_rate=0.01),\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df_o.copy() #restore the worked on dataset for multiple running of only below cells\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n#Check different NA handling options.\n\nOPTION = 4\nTECH = 'median' #technique to apply for OPTION 2 setting\n\nif OPTION == 1:\n    #Option 1, drop na\n    df = df.dropna(axis=1)\nelif OPTION == 2:\n    #Take a fill approach\n    #NA handling using different techniques just to see the impact.\n    if TECH == 'median':\n        df = df.fillna(value=df.median())\n        print(\"Applying median to all nan values.\")\n    elif TECH == 'mean':\n        df = df.fillna(value=df.mean())\n        print(\"Applying mean to all nan values.\")\n    elif TECH == 'mode':\n        df = df.fillna(value=df.mode())\n        print(\"Applying mode to all nan values.\")\n    else:\n        df = df.fillna(value=0)\n        print(\"Applying 0 to all nan values.\")\nelif OPTION == 3: #impute\n    my_imputer = SimpleImputer()\n    df_i = my_imputer.fit_transform(df)\n    df = pd.DataFrame(df_i, columns=df.columns.values)\nelif OPTION == 4:\n    # make copy to avoid changing original data (when Imputing)\n    new_data = df.copy()\n\n    # make new columns indicating what will be imputed\n    cols_with_missing = (col for col in new_data.columns if new_data[col].isnull().any())\n    for col in cols_with_missing:\n        new_data[col + '_was_missing'] = new_data[col].isnull()\n    # Imputation\n    my_imputer = SimpleImputer(strategy=TECH)\n    #print(\"new_data now \", new_data.shape, \" columns \", new_data.columns.values)\n    new_data = pd.DataFrame(my_imputer.fit_transform(new_data), columns=new_data.columns)\n    #print(\"new_data now \", new_data.shape, \" columns \", new_data.columns.values)\n    #new_data.columns = df.columns\n    df = new_data.copy()\n    #print(\"DF shape now \", df.shape, \" columns \", df.columns.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature scaling. Simple approach to MinMaxScaler choosen here as the distribution of all the columns is almost similar with limited outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler()\n\ndf_X = sc.fit_transform(df)\ndf = pd.DataFrame(df_X, columns=df.columns.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Separate the target label from the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df['churn']\n# make sure that Customer_ID is not present in the data set as ideally this should not be a feature to predict churn if data was well randomized\n# so we will drop the churn column and the Customer_ID (if exists) from the dataframe for training purposes.\ndf = df.drop([x for x in ['churn', 'Customer_ID'] if x in df.columns], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Optional PCA plot below (2-d). tSNE is too time consuming."},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=2, svd_solver='full')\ndf_pca = pca.fit_transform(df)\n\n\n\ndf_o[\"pca1\"] = df_pca[:, 0]\ndf_o[\"pca2\"] = df_pca[:, 1]\n\nplt.subplots(figsize=(10,8))\nsns.scatterplot(data=df_o, x=\"pca1\", y=\"pca2\", hue=\"churn\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train test split. (For now no stratified splitting is happening)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = \\\n        train_test_split(df, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost Below"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\n\nparam = {'max_depth': 9, 'objective':'binary:hinge', 'grow_policy' : 'lossguide', 'predictor' : 'gpu_predictor',\n        'booster' : 'dart', 'rate_drop' : 0.02, 'tree_method' : 'gpu_hist'}\n\nparam_2 = {'booster': 'dart',\n         'max_depth': 8, 'learning_rate': 0.1,\n         'objective': 'binary:logistic',\n         'sample_type': 'weighted',#select dropped trees based on weight\n         'normalize_type': 'tree',\n         'rate_drop': 0.1,\n         'skip_drop': 0.5}\n\nwatchlist = [(dtest, 'eval'), (dtrain, 'train')]\nnum_round = 200\n\n\nbst = xgb.train(param_2, dtrain, num_round, watchlist)\nmodels['XGB'] = bst\nplot_roc(y_test, bst.predict(dtest), \"XGB\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Keras Model Below"},{"metadata":{},"cell_type":"markdown","source":"#### Warning, this is a pretty big model for a simple task. But, no matter the configurations tried, I was unable to make it perform better than the XGB. Please if someone can make a better DNN for this classification, kindly let me know."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Warning, this is a pretty big model for a simple task. But, no matter the configurations tried, I was unable to make it perform better than the XGB.\nmodel = Sequential()\nmodel.add(Dense(512, input_shape=(X_train.shape[1],), activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compile model\nmodel.compile(loss='binary_crossentropy', optimizer=optimizers.RMSprop(lr=0.0001), metrics=['accuracy'], )\n\nmodel.fit(X_train, y_train, epochs=20, batch_size=20, validation_data=(X_test, y_test))\nmodels['Keras'] = model\n\nplot_roc(y_test, model.predict(X_test), title=\"Keras \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RandomForest, DecisionTrees, Adaboost below"},{"metadata":{"trusted":true},"cell_type":"code","source":"incorrect_indexes = []\n\n\nfor name in classifiers.keys():\n    print(\"Starting with \", name)\n    clf = classifiers[name]\n    clf.fit(X_train, y_train)\n    score = clf.score(X_test, y_test)\n    y_pred = clf.predict(X_test)\n    prec = precision_score(y_test, y_pred)\n    rec = recall_score(y_test, y_pred)\n    CM = confusion_matrix(y_test, y_pred)\n    print(X_test[y_pred != y_test].index.values)\n    print(\"Name {}, Score {}, Precision {}, Recall {}\".format(name, score, prec, rec))\n    print(CM)\n    np.set_printoptions(precision=2)\n\n    # Plot non-normalized confusion matrix\n    plot_confusion_matrix(y_test, y_pred, classes=[\"loyal\", \"churn\"],\n                          title='Confusion matrix, with normalization', normalize=True)\n    plt.show()\n    \n    models[name] = clf\n    \n    plot_roc(y_test, y_pred, title=\"Clf:\" + name)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let us try a Voting Classifier as well"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import VotingClassifier\n\nclflr = LogisticRegression(solver='lbfgs', multi_class='multinomial',\n                          random_state=1)\n\nclfrf = RandomForestClassifier(n_estimators=300, random_state=1)\n\nclfgb = GradientBoostingClassifier(learning_rate=0.05)\n\nclfad = AdaBoostClassifier(n_estimators=300)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eclf1 = VotingClassifier(estimators=[\n        ('lr', clflr), ('rf', clfrf), ('gnb', clfgb), ('adab', clfad)], voting='soft')\n\neclf1 = eclf1.fit(X_train, y_train)\n\ny_pred = eclf1.predict(X_test)\n\nmodels['VotingSoft'] = eclf1\nplot_roc(y_test, y_pred, \"Voting - Soft\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eclf2 = VotingClassifier(estimators=[\n        ('lr', clflr), ('rf', clfrf), ('gnb', clfgb), ('adab', clfad)], voting='hard')\n\neclf2 = eclf1.fit(X_train, y_train)\n\ny_pred = eclf1.predict(X_test)\n\nmodels['VotingHard'] = eclf1\nplot_roc(y_test, y_pred, \"Voting - Hard\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finally the end. We will now try to plot the ROC curves for all the classifiers in one place"},{"metadata":{"trusted":true},"cell_type":"code","source":"for mname in models.keys():\n    print(mname)\n    if mname.lower().find(\"xgb\") > -1:\n        plot_roc(y_test, models[mname].predict(dtest), title=mname) #xgb has a different dtest\n    else:\n        plot_roc(y_test, models[mname].predict(X_test), title=mname)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### I believe that seeing the PCA plots, the data correlation for churn and non-churn is too high for models to pick up better patterns. Another possibility is that I am missing some clear opportunities to make the model better. (feature engineering maybe??)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}