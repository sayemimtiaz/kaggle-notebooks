{"cells":[{"cell_type":"markdown","source":"**Author:** Raoul Malm  \n**Description:** Given is a training set of samples listing passengers who survived or did not survive the Titanic disaster. The goal is to construct a model that can predict from a test dataset not containing the survival information if these passengers in the test dataset survived or not. This is a supervised classification task. The individual steps for the solution are:\n- Analyze data\n- Manipulate data: complete, convert, create, delete features\n- Model data: kNN, SVC, Decision Tree, Random Forest, Neural Networks\n\n**Results:** Using a split of 90%/10% on the labeled training data this implementation, training on data of 801 passengers, achieves a 86% accuracy on the validation set of 90 passengers.  \n**Reference:** [Titanic Data Science Solutions by Manav Sehgal](https://www.kaggle.com/startupsci/titanic-data-science-solutions?scriptVersionId=1145136)\n\n","metadata":{"_uuid":"17aaaacd8c9701d4aecc4fcd650770fcc93e2147","_cell_guid":"4d0f1227-9788-4cf8-a9cf-21bbfad4db3a"}},{"cell_type":"markdown","source":"# Libraries and Settings","metadata":{"_uuid":"266d8130e738c87f2149790be96ffab9ca99ce94","_cell_guid":"6bacf1cb-9713-450b-9107-60190cc8c767"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\nimport sklearn.linear_model\nimport sklearn.svm\nimport sklearn.ensemble\nimport sklearn.neighbors\nimport sklearn.naive_bayes\nimport sklearn.tree\nimport sklearn.neural_network\nfrom subprocess import check_output\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport tensorflow as tf\n%matplotlib inline\n\ntrain_set_size = 891;\nvalid_set_size = 0;\n\n#display parent directory and working directory\nprint(os.path.dirname(os.getcwd())+':', os.listdir(os.path.dirname(os.getcwd())));\nprint(os.getcwd()+':', os.listdir(os.getcwd()));","metadata":{"_uuid":"5cfd2df9914284894f08d8f702efed041897fbac","scrolled":false,"_cell_guid":"811372be-a4a3-4b03-a1dd-66a36f96a834"}},{"cell_type":"markdown","source":"# Analyze Data\n\nThe train/test sets have 891/418 rows with 12/11 columns. The features are:\n- Survived: 0 = No, 1 = Yes \n- Pclass: Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd \n- Name: Name of the passenger\n- Sex: male, female \n- Age: Age in years. Is fractional if less than 1. If the age is estimated, it is in the form of xx.5.\n- SibSp: # of siblings / spouses aboard the Titanic (Sibling = brother, sister, stepbrother, stepsister, Spouse = husband, wife). Mistresses and fianc√©s were ignored\n- Parch: # of parents / children aboard the Titanic (Parent = mother, father, Child = daughter, son, stepdaughter, stepson). Some children travelled only with a nanny, therefore Parch=0 for them.\n- Ticket: Ticket number \n- Fare: Passenger fare \n- Cabin: Cabin number \n- Embarked: Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton\n\nThe features can be characterized by different types:\n- numerical: Age (continuous, float64), Fare (continuous, float64), SibSp (discrete, int64), Parch (discrete, int64)\n- categorial: Sex (string), Pclass (int64), Embarked (character), Survived (int64), Ticket (alphanumeric, string), Cabin (alphanumeric, string), Name (string)\n","metadata":{"_uuid":"76c019d3fd8b019a37badd2ca90dd68df746d3aa","_cell_guid":"e35a0af9-f2f5-4c9d-b1ab-cd55d34dc368"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# read data and have a first look at it\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\ncombine = [train_df, test_df]\ntrain_df.info()\nprint('_'*40)\ntest_df.info()","metadata":{"_uuid":"ad8140a6b4de1b76e866685fd9c615e3b0a1b22d","_cell_guid":"f696b514-65b0-4c68-afc9-24d9c7ad5b76"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# look at the first five rows\ntrain_df.head()","metadata":{"_uuid":"fe63088da87c3e8c058d98759343029bf687867c","_cell_guid":"c6a07f39-00ee-44e7-8c63-b8b1ab325425"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# look at the first five rows\ntest_df.head() ","metadata":{"_uuid":"38c3f85679cf49944fd1bfc722d8088121dde7ae","_cell_guid":"4e0f38e6-35f2-46dc-9c34-d0c88ac0d311"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# describe numerical data\ntrain_df.describe()","metadata":{"_uuid":"788e866d0c76a0a37c90b8beea35c2cbf4b0face","_cell_guid":"463d0a51-b3a1-4727-9468-74c3f0e45758"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# describe numerical data\ntest_df.describe()","metadata":{"_uuid":"47a2deaa936890dfb617a31e7db56b5dc0c8e715","_cell_guid":"956858f4-7cda-4c5e-be06-e9a45caa708e"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# describe object data\ntrain_df.describe(include=['O'])","metadata":{"_uuid":"48c9d13d71f64ce0280025b93540353ee703778d","_cell_guid":"596013e4-944c-4c44-9536-3753fc029e7d"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# describe object data\ntest_df.describe(include=['O'])","metadata":{"_uuid":"9a01e53cae1125459c83d26367ad58f5bbd3038f","scrolled":true,"_cell_guid":"27528cc9-a2b9-4bc4-a391-e30b38425905"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# check Pclass - Survived correlation\ntrain_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","metadata":{"_uuid":"2818d95b9ce19d3f1fa55895fb99abaca87fca49","_cell_guid":"8b52a7ab-7b4b-4074-9461-66c3c0e27e1d"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# check Sex - Survived correlation\ntrain_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","metadata":{"_uuid":"a13fda47b1f19675626428444e0d654674c6a6a4","_cell_guid":"073264d9-9ee9-430a-a789-d7a9ea8ce819"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# check SibSp - Survived correlation\ntrain_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","metadata":{"_uuid":"493254854cddf43d35ae88ecccd39fdf2fc32d61","_cell_guid":"f06e19f9-41c7-48f5-99af-3131ebc1bcb8"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# check Parch - Survived correlation\ntrain_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","metadata":{"_uuid":"5b62f8794633932930bc14526dbe428ab9c4ed61","_cell_guid":"f11b085e-12ab-4ae4-a30e-6f7fa84cdec1"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# Age histograms depending on Survived\ngrid = sns.FacetGrid(train_df, col='Survived');\ngrid.map(plt.hist, 'Age', bins=20);","metadata":{"_uuid":"25ea5d350a8282b17fa20723bfd421495659c671","_cell_guid":"b9e3c595-52fc-4ce1-bca8-3d3c569d1155"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# Age histograms depending on Survived, Pclass\ngrid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","metadata":{"_uuid":"71efb8aa83cda4bb1a9e668d204fa4f180429433","_cell_guid":"9a71b19e-956e-4852-8b85-76aa31822268"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# Survived values depending on Embarked, Sex\ngrid = sns.FacetGrid(train_df, row='Embarked', size=2.2, aspect=1.6);\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep');\ngrid.add_legend();","metadata":{"_uuid":"489e7932e751bf8e946d3c1b8aab1e233785d4a9","_cell_guid":"537c62c3-1f43-4dbb-83a8-f7f0f0cf60d5"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# Fare depending on Embarked, Survived, Sex\ngrid = sns.FacetGrid(train_df, row='Embarked', col='Survived', size=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()","metadata":{"_uuid":"da065d1b2edd2423f6dd6212848a9e200213cc79","_cell_guid":"a83cdbab-e89e-4590-b449-170f3aef58e8"}},{"cell_type":"markdown","source":"# Manipulate Data\n\nBy having analyzed the data we will perform the following steps:\n\n- create new feature: Title\n- delete features: Ticket, Cabin, Name, PassengerId\n- convert features: Sex\n- complete and convert feature: Age\n- create new features: IsAlone, Age*Class\n- complete and convert feature: Embarked \n- complete and convert feature: Fare","metadata":{"_uuid":"bd727fbb9d8a43457f301c93cf0dc8fe815e7579","_cell_guid":"f5e6bedd-9ad3-4324-93d6-b6d8836c0c49"}},{"cell_type":"markdown","source":"### Create new feature: Title","metadata":{"_uuid":"ff612dee502f55a7352ad74f1a1c1bc5e1ac3897","_cell_guid":"ede3c3c4-dc9f-4800-b439-50b73ab93ca5"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# extract title from Name and then create new feature: Title  \nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_df['Title'], train_df['Sex'])\n\n# reduce the number of titles\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\n                                                 'Don', 'Dr', 'Major', 'Rev', 'Sir',\n                                                 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \n#train_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\n#print('train_df.shape=',train_df.shape)\n\n# map the title to int64\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_df.head()","metadata":{"_uuid":"048ae6c19ad3a4dc573453f50d34bc1e26f58dee","_cell_guid":"3c6470f3-e3fe-46de-8482-1e1036eb99ab"}},{"cell_type":"markdown","source":" ### Delete features: Ticket, Cabin, Name, PassengerId","metadata":{"_uuid":"30c909298fc876c03b542b7eb36f26997aa3282f","_cell_guid":"02a9e9f6-c72d-49b8-9f03-407934ac5dda"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# delete columns: Ticket, Cabin, Name, PassengerId\ntrain_df = train_df.drop(['Name', 'PassengerId', 'Ticket', 'Cabin'], axis=1)\ntest_df = test_df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\ncombine = [train_df, test_df]\nprint(\"train_df = \", train_df.shape)\nprint(\"test_df = \", test_df.shape)","metadata":{"_uuid":"5b0b89370e7b7362db6acb6915eb6c9feae05d66","_cell_guid":"339881dc-a267-4553-b4df-5ac1c34d09de"}},{"cell_type":"markdown","source":" ### Convert features: Sex","metadata":{"_uuid":"30c5a5ca5805e67eb96d87edae088b9a0501a67e","_cell_guid":"6915c616-8fbf-479c-9350-e966102c1ab1"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# convert variable 'Sex' into type int64\nfor dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ntrain_df.head()","metadata":{"_uuid":"152ea9309d211154ce6cc0a94c649f7e0d1cb506","_cell_guid":"4bad8e4c-27f5-4ec7-b5f1-5fa9a51f4c0a"}},{"cell_type":"markdown","source":"### Complete and convert feature: Age","metadata":{"_uuid":"7e9cddf2d40b2eedc7ffa33d9e1d64b1f1300862","_cell_guid":"dd33c23a-35a6-4614-b032-89e4ba55787d"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# complete missing age entries by using information on Sex, Pclass\nguess_ages = np.zeros((2,3));\n\nfor dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & \n                               (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_df.median()\n            guess_ages[i,j] = int(age_guess/0.5 + 0.5 ) * 0.5\n            #print(age_guess)\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[(dataset.Age.isnull()) & (dataset.Sex == i) & \n                        (dataset.Pclass == j+1),'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain_df.head()","metadata":{"_uuid":"f1ba3ac193e6a7f7bf4ae3ed2633eee18a663d9b","_cell_guid":"92666ab6-92e2-496c-983d-9809331c2199"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# create new feature AgeBand\ntrain_df['AgeBand'] = pd.cut(train_df['Age'], 5)\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","metadata":{"_uuid":"f32868c32eeb97af1bdf95d6516cfbcb6b23e949","_cell_guid":"c41b12f4-62de-4ea4-b085-f1ea982b45fe"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# Replace Age with ordinals based on the bands in AgeBand\nfor dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ntrain_df.head()","metadata":{"_uuid":"1b7a96abfbd2d6a933883ddf870cd7d9f21818aa","_cell_guid":"80329730-e5f2-4e99-b975-bd4dfa108991"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# remove AgeBand\ntrain_df = train_df.drop(['AgeBand'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.head()","metadata":{"_uuid":"471a7f63edbe0b7be514fa5ce77c4038ad3bd7f6","scrolled":true,"_cell_guid":"27b9268a-c1cf-4cba-93f8-59c2205718b7"}},{"cell_type":"markdown","source":"### Create new features: IsAlone, Age*Class","metadata":{"_uuid":"ba8c386ddd84948b992f2b4adc69cc233c675d0a","_cell_guid":"0ac53154-9ff5-4c1c-b5f3-2c9b5d91fbdd"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# create new feature FamilySize\nfor dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","metadata":{"_uuid":"064ba062be5d0cc303081b20e7e5b85c39b15894","_cell_guid":"2d076f61-8c88-4509-bd19-828d536cf926"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# create new feature IsAlone\nfor dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()\n","metadata":{"_uuid":"ef1d0f1c394bb86a81d13c0ac1d7bf8858fd2212","_cell_guid":"8b10a488-878d-4460-a642-131568285796"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# remove features: Parch, SibSp, FamilySize\n#train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\n#test_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntrain_df = train_df.drop(['Parch', 'SibSp'], axis=1)\ntest_df = test_df.drop(['Parch', 'SibSp'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.head()","metadata":{"_uuid":"b5317852a9d312efac586a7c7ef8898354d5e294","_cell_guid":"c32dd512-ae87-4a0c-a5e1-5fb0bc997eda"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# create new feature Age*Class\nfor dataset in combine:\n    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n\n#train_df.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)\ntrain_df.head()","metadata":{"_uuid":"d7e4de39b1ac2addbb1c6d8bce5e13ecc5188c87","_cell_guid":"c9eaa4c9-c3af-4241-b117-8a5ea69fd3f1"}},{"cell_type":"markdown","source":"### Complete and convert feature: Embarked ","metadata":{"_uuid":"489a58dd294ce81096511b194f7f75f970c83e60","_cell_guid":"c44e4349-77e0-492c-8d8d-a0001c85bbfa"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# most frequent occurence of Embarked value\nfreq_port = train_df.Embarked.dropna().mode()[0]\nprint(freq_port);\n\n# replace na entries with most frequent value of Embarked\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ntrain_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","metadata":{"_uuid":"d1c424c98e58b98e3582013d8fdfd3ed47f2d378","_cell_guid":"09ca0df6-2880-474c-bc23-19e94a766281"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# map Embarked values to integer values\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain_df.head()","metadata":{"_uuid":"0f18ea6356be3e906ab5592cdce6e910932b8a85","_cell_guid":"924e0324-b852-42b8-a934-a7e06bfb852c"}},{"cell_type":"markdown","source":"### Complete and convert feature: Fare","metadata":{"_uuid":"ef90aa54ac2e6b864bba561be4392a27d1c8051b","_cell_guid":"4d5a0773-53de-4c7e-ab7c-709e0ef16efd"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# complete feature Fare\ntest_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\ntest_df.head()","metadata":{"_uuid":"961a2c418877d7c17280a90189a772476dd15315","_cell_guid":"335505b5-fe55-49d6-99df-a42703b6a00f"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# create feature FareBand\ntrain_df['FareBand'] = pd.qcut(train_df['Fare'], 6)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","metadata":{"_uuid":"b87bd5292ed86a448dcbd999e6b534cfced94cd5","_cell_guid":"7c18595a-fff1-4ad0-91e9-db32a018cedf"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# replace feature Fare by ordinals based on FareBand\nfor dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain_df = train_df.drop(['FareBand'], axis=1)\ncombine = [train_df, test_df]\n    \ntrain_df.head()","metadata":{"_uuid":"a2b444fb5267852f76935e0e995a3ce7a05e0878","_cell_guid":"e6d4340f-b368-4c2b-b84d-792178e7d03d"}},{"cell_type":"markdown","source":"\n# Model Data\n\nSupervised learning plus cassification limits the number of machine learning algorithms to: \n    - Logistic Regression\n    - kNN (k-Nearest Neighbors)\n    - SVM (Support Vector Machine) with different kernels\n    - Gaussian Naive Bayes\n    - Decision Tree\n    - Random Forrest\n    - Perceptron\n    - Multi-layer Perceptron\n    - Deep Neural Network","metadata":{"_uuid":"cc3e41c2e9b3bbdd2272991cf9489439a21aeac4","_cell_guid":"20ffee26-bb39-4a33-8e22-c71a5c8f8427"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# create subsets for training, validation and testing\nX_train = train_df.drop(\"Survived\", axis=1)[0:train_set_size]\nY_train = train_df[\"Survived\"][0:train_set_size]\nif valid_set_size > 0:\n    X_valid = train_df.drop(\"Survived\", axis=1)[train_set_size:train_set_size+valid_set_size]\n    Y_valid = train_df[\"Survived\"][train_set_size:train_set_size+valid_set_size]\nelse:\n    X_valid = train_df.drop(\"Survived\", axis=1)[801:]\n    Y_valid = train_df[\"Survived\"][801:]\n    \nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\n\nprint(\"training data: \", 'X_train.shape = ', X_train.shape, 'Y_train.shape = ', Y_train.shape)\nprint(\"validation data: \", 'X_valid.shape = ', X_valid.shape, 'Y_valid.shape = ', Y_valid.shape)\nprint(\"test data: \", 'X_test.shape = ', X_test.shape)","metadata":{"_uuid":"1bd3e2715c87bcffa4a1f3a9efdc32a87e47f208","scrolled":true,"_cell_guid":"f67936b8-5a1b-4654-80aa-005b09274f15"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# normalize features\nX_train_norm = (X_train)/(X_train.max()-X_train.min());\nX_valid_norm = (X_valid)/(X_valid.max()-X_valid.min());\nX_test_norm = (X_test)/(X_test.max()-X_test.min());","metadata":{"collapsed":true,"_uuid":"a57c726fe307caec353d2c205b35c1009825db94","_cell_guid":"708802d3-9546-4ee9-9e69-b0a83621ca7e"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"\"\"\"\n# skip features\nfeature = 'Age*Class';\nif feature in X_train.columns:\n    X_train = X_train.drop(feature, axis=1)\nif feature in X_valid.columns:\n    X_valid = X_valid.drop(feature, axis=1)\nif feature in X_test.columns:\n    X_test = X_test.drop(feature, axis=1)\n\"\"\"   \nprint('X_train.columns = ', X_train.columns.values)\nprint('X_valid.columns = ', X_valid.columns.values)\nprint('X_test.columns = ', X_test.columns.values)\n","metadata":{"_uuid":"2229c780ad9c31786cabc945084bb49ebfbab772","_cell_guid":"10ba259a-00bf-4764-9df5-fcfa734ed8b8"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"## Logistic Regression as a benchmark model\n\nlogreg = sklearn.linear_model.LogisticRegression()\nlogreg.fit(X_train_norm, Y_train)\nY_log_pred = logreg.predict(X_test_norm)\nacc_log_train = np.round(logreg.score(X_train_norm, Y_train), 4)\nacc_log_valid = np.round(logreg.score(X_valid_norm, Y_valid), 4)\nprint('Logistic Regression: train/valid Acc = %.4f/%.4f'%(acc_log_train, acc_log_valid))\ncoeff_df = pd.DataFrame(X_train.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\ncoeff_df.sort_values(by='Correlation', ascending=False)","metadata":{"_uuid":"05c78c65f10ed33df9d76032b21ec03867647ed9","_cell_guid":"27f7f7f1-aed6-49e5-9f6c-498d64e40ca7"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"## Further Machine Learning Algorithms\n\n# support vector machine with rbf kernel\nsvc_rbf = sklearn.svm.SVC(kernel='rbf')\nsvc_rbf.fit(X_train_norm, Y_train)\nY_pred_svc_rbf = svc_rbf.predict(X_test_norm)\nacc_svc_rbf_train = np.round(svc_rbf.score(X_train_norm, Y_train), 4)\nacc_svc_rbf_valid = np.round(svc_rbf.score(X_valid_norm, Y_valid), 4)\nprint('SVC rbf kernel: train/valid Acc = %.4f/%.4f'%(acc_svc_rbf_train, acc_svc_rbf_valid))\n\n# support vector machine with linear kernel\nsvc_linear = sklearn.svm.SVC(kernel='linear')\nsvc_linear.fit(X_train_norm, Y_train)\nY_pred_svc_linear = svc_linear.predict(X_test_norm)\nacc_svc_linear_train = np.round(svc_linear.score(X_train_norm, Y_train), 4)\nacc_svc_linear_valid = np.round(svc_linear.score(X_valid_norm, Y_valid), 4)\nprint('SVC linear kernel: train/valid Acc = %.4f/%.4f'%(acc_svc_linear_train, acc_svc_linear_valid))\n\n# k-Nearest-Neighbour Algorithm\nknn = sklearn.neighbors.KNeighborsClassifier(n_neighbors = 5)\nknn.fit(X_train_norm, Y_train)\nY_pred_knn = knn.predict(X_test_norm)\nacc_knn_train = np.round(knn.score(X_train_norm, Y_train), 4)\nacc_knn_valid = np.round(knn.score(X_valid_norm, Y_valid), 4)\nprint('kNN: train/valid Acc = %.4f/%.4f'%(acc_knn_train, acc_knn_valid))\n\n# Gaussian Naive Bayes\ngaussianNB = sklearn.naive_bayes.GaussianNB()\ngaussianNB.fit(X_train_norm, Y_train)\nY_pred_gaussianNB = gaussianNB.predict(X_test_norm)\nacc_gaussianNB_train = np.round(gaussianNB.score(X_train_norm, Y_train), 4)\nacc_gaussianNB_valid = np.round(gaussianNB.score(X_valid_norm, Y_valid), 4)\nprint('Gaussian Naive Bayes: train/valid Acc = %.4f/%.4f'%(acc_gaussianNB_train, acc_gaussianNB_valid))\n\n# Decision Tree\ndecision_tree = sklearn.tree.DecisionTreeClassifier()\ndecision_tree.fit(X_train_norm, Y_train)\nY_pred_decision_tree = decision_tree.predict(X_test_norm)\nacc_decision_tree_train = np.round(decision_tree.score(X_train_norm, Y_train), 4)\nacc_decision_tree_valid = np.round(decision_tree.score(X_valid_norm, Y_valid), 4)\nprint('Decision Tree: train/valid Acc = %.4f/%.4f'%(acc_decision_tree_train, acc_decision_tree_valid))\n\n# Random Forest\nrandom_forest = sklearn.ensemble.RandomForestClassifier(n_estimators=10)\nrandom_forest.fit(X_train_norm, Y_train)\nY_pred_random_forest = random_forest.predict(X_test_norm)\nrandom_forest.score(X_train_norm, Y_train)\nacc_random_forest_train = np.round(random_forest.score(X_train_norm, Y_train), 4)\nacc_random_forest_valid = np.round(random_forest.score(X_valid_norm, Y_valid), 4)\nprint('Random Forest: train/valid Acc = %.4f/%.4f'%(acc_random_forest_train, acc_random_forest_valid))\n\n# Perceptron\nperceptron = sklearn.linear_model.Perceptron(max_iter = 10000, tol = 1e-6, shuffle = True)\nperceptron.fit(X_train_norm, Y_train)\nY_pred_perceptron = perceptron.predict(X_test_norm)\nacc_perceptron_train = np.round(perceptron.score(X_train_norm, Y_train), 4)\nacc_perceptron_valid = np.round(perceptron.score(X_valid_norm, Y_valid), 4)\nprint('Perceptron: train/valid Acc = %.4f/%.4f'%(acc_perceptron_train, acc_perceptron_valid))\n\n# Multi Layer Perceptron\nmlp = sklearn.neural_network.MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',\n                                           max_iter = 10000, tol = 1e-6, shuffle = False, \n                                           hidden_layer_sizes=(64,32,16), solver ='adam',\n                                           learning_rate = 'adaptive',\n                                           learning_rate_init=0.001, verbose=False);\nmlp.fit(X_train_norm, Y_train)\nY_pred_mlp = mlp.predict(X_test_norm)\nacc_mlp_train = np.round(mlp.score(X_train_norm, Y_train), 4)\nacc_mlp_valid = np.round(mlp.score(X_valid_norm, Y_valid), 4)\nprint('MLP: train/valid Acc = %.4f/%.4f'%(acc_mlp_train, acc_mlp_valid))","metadata":{"_uuid":"e5d76b8e14e7d077a2f3759fd7af371188202d85","scrolled":true,"_cell_guid":"25d15df0-e81a-4f3e-a6e1-0a9e6728bf6e"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"## Deep Neural Network\n\nx_size = X_train_norm.shape[1]; # number of features\ny_size = 1; # binary variable\nn_n_fc1 = 128; # number of neurons of first layer\nn_n_fc2 = 64; # number of neurons of second layer\nn_n_fc3 = 32; # number of neurons of third layer\n\n# variables for input and output \nx_data = tf.placeholder('float', shape=[None, x_size])\ny_data = tf.placeholder('float', shape=[None, y_size])\n\n# 1.layer: fully connected\nW_fc1 = tf.Variable(tf.truncated_normal(shape = [x_size, n_n_fc1], stddev = 0.1))\nb_fc1 = tf.Variable(tf.constant(0.1, shape = [n_n_fc1]))  \nh_fc1 = tf.nn.relu(tf.matmul(x_data, W_fc1) + b_fc1)\n\n# dropout\ntf_keep_prob = tf.placeholder('float')\nh_fc1_drop = tf.nn.dropout(h_fc1, tf_keep_prob)\n\n# 2.layer: fully connected\nW_fc2 = tf.Variable(tf.truncated_normal(shape = [n_n_fc1, n_n_fc2], stddev = 0.1)) \nb_fc2 = tf.Variable(tf.constant(0.1, shape = [n_n_fc2]))  \nh_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) \n\n# dropout\nh_fc2_drop = tf.nn.dropout(h_fc2, tf_keep_prob)\n\n# 3.layer: fully connected\nW_fc3 = tf.Variable(tf.truncated_normal(shape = [n_n_fc2, n_n_fc3], stddev = 0.1)) \nb_fc3 = tf.Variable(tf.constant(0.1, shape = [n_n_fc3]))  \nh_fc3 = tf.nn.relu(tf.matmul(h_fc2_drop, W_fc3) + b_fc3) \n\n# dropout\nh_fc3_drop = tf.nn.dropout(h_fc3, tf_keep_prob)\n\n# 3.layer: fully connected\nW_fc4 = tf.Variable(tf.truncated_normal(shape = [n_n_fc3, y_size], stddev = 0.1)) \nb_fc4 = tf.Variable(tf.constant(0.1, shape = [y_size]))  \nz_pred = tf.matmul(h_fc3_drop, W_fc4) + b_fc4  \n\n# cost function\ncross_entropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_data, logits=z_pred));\n\n# optimisation function\ntf_learn_rate = tf.placeholder(dtype='float', name=\"tf_learn_rate\")\ntrain_step = tf.train.AdamOptimizer(tf_learn_rate).minimize(cross_entropy)\n\n# evaluation\ny_pred = tf.nn.sigmoid(z_pred);\ny_pred_class = tf.cast(tf.greater(y_pred, 0.5),'float')\naccuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred_class, y_data ), 'float'))\n\n# start TensorFlow session and initialize global variables\nsess = tf.InteractiveSession() \nsess.run(tf.global_variables_initializer())  \n\nkeep_prob = 0.2; # dropout regularization with keeping probability\nlearn_rate_range = [0.01,0.005,0.0025,0.001];\nlearn_rate_step = 500;\n\nx_train_batch = X_train_norm.iloc[:,:].values.astype('float');\ny_train_batch = Y_train.iloc[:].values.reshape(Y_train.shape[0],1).astype('float');\n\nx_valid_batch = X_valid_norm.iloc[:,:].values.astype('float');\ny_valid_batch = Y_valid.iloc[:].values.reshape(Y_valid.shape[0],1).astype('float');\n\nx_test_batch = X_test_norm.iloc[:,:].values.astype('float');\n\nn_epoch = 1000; # number of epochs\ntrain_loss, train_acc, valid_loss, valid_acc = np.array([]), np.array([]), np.array([]), np.array([]);\nn_step = -1;\n\n# training model\nfor i in range(0,n_epoch):\n    \n    if i%learn_rate_step == 0:\n        n_step += 1;\n        learn_rate = learn_rate_range[n_step];\n        print('set learnrate = ', learn_rate)\n        \n    sess.run(train_step, feed_dict={x_data: x_train_batch, y_data: y_train_batch, tf_keep_prob: keep_prob, \n                                    tf_learn_rate: learn_rate})\n    \n    if i%100==0:\n        train_loss = np.append(train_loss, sess.run(cross_entropy, feed_dict={x_data: x_train_batch, \n                                                                              y_data: y_train_batch, \n                                                                              tf_keep_prob: 1.0}));\n    \n        train_acc = np.append(train_acc, accuracy.eval(feed_dict={x_data: x_train_batch, \n                                                                  y_data: y_train_batch, \n                                                                  tf_keep_prob: 1.0}));      \n    \n        valid_loss = np.append(valid_loss, sess.run(cross_entropy, feed_dict={x_data: x_valid_batch, \n                                                                              y_data: y_valid_batch, \n                                                                              tf_keep_prob: 1.0}));\n    \n        valid_acc = np.append(valid_acc, accuracy.eval(feed_dict={x_data: x_valid_batch, \n                                                                  y_data: y_valid_batch, \n                                                                  tf_keep_prob: 1.0}));      \n    \n        print('%d epoch: train/val loss = %.4f/%.4f, train/val acc = %.4f/%.4f'%(i+1,\n                                                                                 train_loss[-1],\n                                                                                 valid_loss[-1],\n                                                                                 train_acc[-1],\n                                                                                 valid_acc[-1]))\n\nacc_DNN_train = train_acc[-1];\nacc_DNN_valid = valid_acc[-1];\n# prediction for test set\nY_pred_DNN = y_pred_class.eval(feed_dict={x_data: x_test_batch,tf_keep_prob: 1.0}).astype('int').flatten()\n\nsess.close();","metadata":{"_uuid":"1ec800285f3b259c739b42da52c5c8ad48e3b12f","_cell_guid":"2873867e-926f-471e-8787-e4453679d4ac"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# model summary\nmodels = pd.DataFrame({\n    'Model': ['SVC with rbf kernel', 'kNN', 'Logistic Regression', \n              'Random Forest', 'Gaussian Naive Bayes', 'Perceptron', \n              'MLP', 'SVC with linear kernel', 'Decision Tree', 'Deep Neural Network'],\n    'Train Acc': [acc_svc_rbf_train, acc_knn_train, acc_log_train, \n              acc_random_forest_train, acc_gaussianNB_train, acc_perceptron_train, \n              acc_mlp_train, acc_svc_linear_train, acc_decision_tree_train, acc_DNN_train],\n    'Valid Acc': [acc_svc_rbf_valid, acc_knn_valid, acc_log_valid, \n              acc_random_forest_valid, acc_gaussianNB_valid, acc_perceptron_valid, \n              acc_mlp_valid, acc_svc_linear_valid, acc_decision_tree_valid, acc_DNN_valid]})\nmodels.sort_values(by='Valid Acc', ascending=False)","metadata":{"_uuid":"9476d3d481c3306eb4a71945801fbd734a15e77c","scrolled":true,"_cell_guid":"2c24065a-4aed-4bb7-a7db-f14a0a56716c"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# submit the best results\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred_DNN\n    })\n\n#if not os.path.exists(os.path.dirname(os.getcwd())+'/output'): \n#    print('create directory ', os.path.dirname(os.getcwd())+'/output')\n#    os.makedirs(os.path.dirname(os.getcwd())+'/output')\n#submission.to_csv('../output/submission.csv', index=False)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"collapsed":true,"_uuid":"e44f786a96daaf149f8744f163748831ba8f1576","_cell_guid":"a9812ec5-e121-469e-ae19-c90efecf2896"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"","metadata":{"collapsed":true,"_uuid":"b2948a1583a04172cc2d363d83930646b1444e07","_cell_guid":"d4e36479-4084-4e58-aa57-6813ca201148"}}],"nbformat":4,"nbformat_minor":1,"metadata":{"language_info":{"version":"3.6.3","pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","name":"python"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}}}