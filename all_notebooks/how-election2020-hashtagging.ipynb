{"cells":[{"metadata":{},"cell_type":"markdown","source":"# How people hashtagging\n### During election 2020\n\n> A hack on [election tweets dataset](https://www.kaggle.com/manchunhui/us-election-2020-tweets), please upvote the dataset for his dedicated effort to collect the data\n\n![data image](https://storage.googleapis.com/kaggle-datasets-images/935914/1582537/4fc3f5fa3b371aad3c097e7fb892305b/dataset-cover.jpg?t=2020-10-23-13-26-22)\n\nThe election is at some extent all over, but what worries me is the ever so separated perception of media, stories, even the basic definition of things that leads to a devided society.  As the problem described so vividly by the Netflix documentary [The Social Delimma](https://www.imdb.com/title/tt11464826/). \n\nThis notebook tries to look closely at the diversity forms of tweet texts and how online society devides. And I hope the data science community can move toward the goal of bringing down segregation/news integrity in one day"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -q forgebox","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from forgebox.imports import *\nfrom forgebox.df import PandasDisplay\n\nfrom datetime import datetime\nfrom typing import List, Tuple\nimport random\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport plotly.express as px\nfrom wordcloud import WordCloud\nfrom forgebox.html import DOM\nfrom forgebox.images.widgets import image_dom\nfrom itertools import chain\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the tweets are either hash tagging D.J.Trump or J.R.Bidenï¼Œ saved separately in 2 csv files"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"DATA = Path(\"/kaggle/input/us-election-2020-tweets/\")\n\ntrump_df = pd.read_csv(\n    DATA/\"hashtag_donaldtrump.csv\",\n    lineterminator='\\n', error_bad_lines=False)\nbiden_df = pd.read_csv(DATA/\"hashtag_joebiden.csv\", error_bad_lines=False)\n# Remove the error lines (last column nan)\ntrump_df = trump_df[~trump_df.collected_at.isna()]\nbiden_df = biden_df[~biden_df.collected_at.isna()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove the error lines (last column nan)"},{"metadata":{"trusted":true},"cell_type":"code","source":"trump_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"us_countries = ['United States of America','United States']\n\ndef compare_vc(col, filter_query=\"tweet_id!=''\"):\n    \"\"\"\n    Compare value counts of 2 candidate\n    \"\"\"\n    DOM(f\"Value counts under 2 candidates on column {col.upper()}\", \"h3\")()\n    return biden_df.query(filter_query).vc(col).rename(columns={col: \"biden\"}).join(\n        trump_df.query(filter_query).vc(col).rename(columns={col: \"trump\"})\n    ).head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_vc(\"country\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_vc(\"state\",f\"(country in {us_countries})\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_vc(\"source\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But to be honest, the hashtag distribution among countries, states etc demonstrate nothing meaning full, we still don't know what kind of information is pro-trump, what is anti-trump"},{"metadata":{},"cell_type":"markdown","source":"## Combine the tweet data into one"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_df = pd.concat([trump_df, biden_df])\n\nall_df = all_df.query(\"retweet_count>2\").reset_index(drop=True)\n\ndef replace_us(x): return \"US\" if x in us_countries else x\n\nall_df[\"country\"]= all_df[\"country\"].apply(replace_us)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Find the hash_tag(#) and the at(@)\n\nWith regex ```r\"@(\\w+)\"``` for @ and ```r\"#(\\w+)\"``` for hash tag"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport re\n\ndef find_at(text):\n    return re.findall(r\"@(\\w+)\",text)\n\ndef find_hashtag(text):\n    return re.findall(r\"#(\\w+)\",text)\n\nall_df[\"at_s\"] = all_df.tweet.apply(find_at)\nall_df[\"hash_tags\"] = all_df.tweet.apply(find_hashtag)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Frequent Hash Tag"},{"metadata":{"trusted":true},"cell_type":"code","source":"p_display=PandasDisplay(max_colwidth = 0,max_rows=120)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here are the top mentioned hashtags "},{"metadata":{"trusted":true},"cell_type":"code","source":"all_hashtags = chain(*all_df[\"hash_tags\"])\nhashtag_stat = pd.DataFrame({\"hashtag\":all_hashtags}).vc(\"hashtag\")\nwith p_display:\n    display(hashtag_stat.head(100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Manual review\n> I mannually review 12 tweets sampled from each hashtag, of the top frequent hashtags. And put the hashtag into \"Pro Trump\" or \"Pro Biden\" list to the best of my judgement.(eg if some hashtag looks like pro-trump, but the sample tweets hashtagged this phrase isn't always pro trump, I won't label it pro trump)\n\n> To be clear I'm a coder/ Chinese citizen who's hooked to the US 2020 election rallies news/ tweets/ posts/ conspiracy theory non-stop, all of those from both sides, so my judgement might not be very accurate, besides, there are lots of non-english expression. I seriously hoped someone can try and relabel that"},{"metadata":{"trusted":true},"cell_type":"code","source":"def start_review(hashtag_stat):\n    gen = iter(hashtag_stat.index)\n    def next_ht():\n        \"\"\"\n        Review hashtags one by one\n        \"\"\"\n        ht = next(gen)\n        DOM(f\"#{ht}\",\"h3\")()\n        has_ht = all_df[\"hash_tags\"].apply(lambda x: ht in x)\n        total = has_ht.sum()\n        DOM(f\"Total count: {total}, {int(total*100/len(all_df))}%\",\"h4\")()\n        frame = DOM(\"\",\"div\",{\"class\":\"row\"})\n        for tweet in all_df[has_ht].sample(12).tweet:\n            frame.append(DOM(tweet.replace(ht, f\"<span class='text-danger'>{ht}</span>\"),\n                             \"div\",{\"class\":\"col-sm-4\"}))\n        frame()\n    return next_ht","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PRO_TRUMP = [\"Trump2020\",\"MAGA\",\"HunterBiden\", \"China\", \"MAGA2020\", \"KAG\", \"Trump2020Landslide\", \"Trump2020LandslideVictory\",\n             \"TRUMP2020ToSaveAmerica\",\"maga\", \"HunterBidenLaptop\", \"BidenCrimeFamiily\", \"PresidentTrump\",\n             \"ccp\",\"hunter\", \"HunterBidenEmails\", \"4MoreYears\",\"TrumpPence2020\"\n            ]\nPRO_BIDEN = [ \"TrumpOut\", \"TrumpGenocide\", \"Biden2020\", \"TrumpMeltdown\",\"BlueWave\",\"BidenHarris2020\", \"BidenHaris2020\",\n             \"ByeByeTrump\", \"TrumpIsLosing\",\"Resist\", \"VoteHimOut\", \"PresidentElectJoe\", \"Truth\",\n             \"TrumpVirus\",\"VoteBidenHarris2020\", \"VoteBlue\",\"CountEveryVote\", \"BidenPresident\",\n             \"BidenHarrisToSaveAmerica\",\"TrumpOut\", \"TrumpCrimeFamily\", \"BidenHarrisLandslide2020\",\n             \"TrumpIsANationalDisgrace\",\"TrumpCollapse\",\"PresidentElect\", \"ByeByeTrump\",\"VoteBidenHarrisToSaveAmerica\",\n             \"VoteBlueToSaveAmerica\", \"VoteBiden\", \"DumpTrump\",\"JoeBiden2020\"\n            ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"next_ht = start_review(hashtag_stat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I repeated run next_ht to review the tweet message ðŸ˜­"},{"metadata":{"trusted":true},"cell_type":"code","source":"next_ht()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Display some samples"},{"metadata":{"trusted":true},"cell_type":"code","source":"with p_display:\n    display(\n        all_df[[\"tweet\", \"user_name\", \"retweet_count\", \"at_s\", \"hash_tags\"]].sample(20))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## All ats and hashtags"},{"metadata":{},"cell_type":"markdown","source":"## Learn the graph"},{"metadata":{"trusted":true},"cell_type":"code","source":"class HashTagGraph(Dataset):\n    @classmethod\n    def from_df(cls, df: pd.DataFrame):\n        # count unique hash tag numbers in a row\n        df[\"ht_count\"] =  df.hash_tags.apply(lambda x:len(set(x)))\n        # we use the lower case\n        df[\"hash_tags_lower\"] = df.hash_tags.apply(lambda x:list(i.lower() for i in x))\n        # we use the group with more than 1 hash tag\n        hashtag_groups = list(df.query(\"ht_count>1\")[\"hash_tags_lower\"])\n        return cls(hashtag_groups)\n        \n    def __init__(self, hashtag_groups: List[str]):\n        self.hashtag_groups = hashtag_groups\n        self.all_hashtags = list(chain(*self.hashtag_groups))\n        self.hashtag_stat = pd.DataFrame(\n            pd.DataFrame({\"hashtags\": self.all_hashtags})[\"hashtags\"].value_counts()\n        )\n        self.total_pool = np.arange(len(self.hashtag_stat))\n        self.i2c = np.array(self.hashtag_stat.index)\n        self.c2i = dict((v,k) for k,v in enumerate(self.hashtag_stat.index))\n        \n    def __len__(self): return len(self.hashtag_groups)\n        \n    def __getitem__(self, idx: int) -> Tuple[int]:\n        group = self.hashtag_groups[idx]\n        choices = random.sample(group, 2 )\n        return tuple([\n            self.c2i[choices[0]],\n            self.c2i[choices[1]],\n            random.choice(self.total_pool),\n        ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"ds = HashTagGraph.from_df(all_df)\nds[5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_dl(df, batch_size=256, num_workers=4):\n    ds = HashTagGraph.from_df(df)\n    return DataLoader(\n        ds, shuffle=True,\n        batch_size=batch_size,\n        num_workers=num_workers)\n\ndef get_gen(df, **kwargs):\n    return iter(get_dl(df, **kwargs))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"gen = get_gen(all_df)\n\nfor i in range(30):\n    x1,x2,z = next(gen)\nprint(\",\".join(map(str,map(lambda x:x.shape, [x1, x2, z]))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train a graph"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pytorch_lightning as pl\npl.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class GraphData(pl.LightningDataModule):\n    def __init__(self, df, batch_size=256):\n        super().__init__()\n        self.df=df\n        self.batch_size=batch_size\n        self.ds = HashTagGraph.from_df(self.df)\n        self.c2i=ds.c2i\n        self.i2c=ds.i2c\n        \n    def train_dataloader(self):\n        return DataLoader(self.ds, shuffle=True, batch_size=self.batch_size, num_workers=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"graph_data = GraphData(all_df)\nnum_cates = len(graph_data.i2c)\nnum_cates","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loss Function\n> Cosine Embedding Loss\n\n$\\text{loss}(x, y) =\n    \\begin{cases}\n    1 - \\cos(x_1, x_2), & \\text{if } y = 1 \\\\\n    \\max(0, \\cos(x_1, x_2) - \\text{margin}), & \\text{if } y = -1\n    \\end{cases}\n$"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Graph(pl.LightningModule):\n    def __init__(\n        self,\n        num_cates: int=num_cates,\n        margin: float=32.\n    ):\n        super().__init__()\n        self.num_cates = num_cates\n        self.emb = nn.Embedding(num_cates, 32)\n        self.crit = nn.CosineEmbeddingLoss(margin)\n        \n    def forward(\n        self, x1, x2, z):\n        x_1 = torch.cat([x1,x1])\n        x_2 = torch.cat([x2,z])\n        y = torch.cat([\n            torch.ones_like(x2),\n            -torch.zeros_like(z),\n        ])\n        return self.crit(self.emb(x_1), self.emb(x_2), y)\n        \n    def configure_optimizers(self): return torch.optim.Adam(self.parameters())\n        \n    def training_step(self, batch, batch_idx):\n        x1, x2, z = batch\n        loss = self(x1, x2, z)\n        return {\"loss\":loss}\n    \ngraph = Graph()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer = pl.Trainer(max_epochs=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer.fit(graph, graph_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vecs = graph.emb.weight.data.numpy()\nvecs.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Find closest hashtag"},{"metadata":{"trusted":true},"cell_type":"code","source":"class CosineSim:\n    def __init__(self, names: List[str], vecs: np.array, top_k: int=20):\n        self.names = np.array(names)\n        self.n2i = dict((v,k) for k,v in enumerate(self.names))\n        self.top_k = top_k\n        self.vecs = vecs\n        self.l2 = np.sqrt(np.power(vecs,2).sum(-1))\n        self.vecs_normed = vecs/(self.l2[:,None])\n        \n    def __repr__(self):return f\"Cosine Similarity Engine:\\n\\t\"+\",\".join(self.names[:10])+\",...\"\n    \n    def __call__(self, name):\n        i = self.n2i.get(name)\n        \n        if i is None: raise ValueError(f\"[ERROR] name '{name}' not found\")\n        \n        sim_map = (self.vecs_normed[i,None]*self.vecs_normed).sum(-1)\n        df = pd.DataFrame({\"names\":self.names,\"sim\":sim_map}).sort_values(by = \"sim\", ascending=False)\n        return df.head(self.top_k)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CosineSim(graph_data.ds.hashtag_stat.index, vecs)(\"trump2020\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_hashtags = graph_data.ds.hashtag_stat.head(150)\ntop_ids = np.array(list(graph_data.ds.c2i[i] for i in top_hashtags.index))\n\ntsne=TSNE(2, metric=\"cosine\", )\nemb_coords = tsne.fit_transform(graph.emb.weight.data.numpy()[top_ids])\n\n# pca=PCA(2)\n# emb_coords = pca.fit_transform(graph.emb.weight.data.numpy()[top_ids])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualize manifold\n\n> We visualize the the TSNE of the top 150 mentioned hashtags, and marked the blue or red for clear preference of such hashtag\n\n> You can see the clustering effect of the graph isn't that impressive ðŸ¥¶, but some similar words are groupped up together"},{"metadata":{"trusted":true},"cell_type":"code","source":"pro_trump = list(map(lambda x:x.lower(),PRO_TRUMP))\npro_biden = list(map(lambda x:x.lower(),PRO_BIDEN))\n\ncolor_list =list((\n    \"1 pro_trump\" if i in pro_trump else \n    (\"0 pro_biden\" if i in pro_biden else\n     \"2 no_preference\")) for i in top_hashtags.index)\nfig = px.scatter(\n    emb_coords,\n    x=0, y=1,\n    hover_name=top_hashtags.index,\n    color=color_list,\n    color_discrete_map={\"0 pro_biden\":\"blue\",\"1 pro_trump\":\"red\" }\n)\nfig.update_traces(marker_size=8)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Words Ownership"},{"metadata":{},"cell_type":"markdown","source":"> Okay let's try words ownership by country, states, some words just appearing a lot, like trump biden, but they are in almost evey tweet, but some words are only owned by certain category of data"},{"metadata":{"trusted":true},"cell_type":"code","source":"hashtags = list(chain(*all_df[\"hash_tags_lower\"]))\nhashtag_stat = pd.DataFrame({\"hashtags\": hashtags}).vc(\"hashtags\")\n\nh2count = dict(zip(hashtag_stat.index, hashtag_stat.hashtags))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_owner(df, col):\n    keys = list(df.vc(col).index)[:20]\n    result = []\n    for k in keys:\n        tags = list(chain(*df.query(f\"{col}=='{k}'\").hash_tags_lower))\n        k_df = pd.DataFrame({\"ht\": tags}).vc(\"ht\").reset_index()\n        k_df[f\"{col}\"] = k\n        k_df = k_df.rename(columns={\"index\": \"hashtag\", \"ht\":\"ct\"})\n        result.append(k_df)\n    return_df = pd.concat(result)[[col, \"hashtag\", \"ct\", ]]\n    return_df[\"total\"] = return_df[\"hashtag\"].apply(lambda x:h2count[x])\n    return_df[\"ownership\"] = return_df[\"ct\"] / return_df[\"total\"]\n    return return_df\n\ndef clean_onwership(df, col):\n    col_df = calc_owner(df, col)\n    rt = []\n    wc_painter = WordCloud(\n            background_color=\"white\",\n            height=500, width=800\n            )\n    for k in col_df[col].unique():\n        k_df = col_df.query(f\"{col}=='{k}'\").query(f\"ct>3\")\n        k_df = k_df.sort_values(by = [\"ownership\",\"total\"], ascending=False).reset_index(drop=2).head(50)\n        wc = wc_painter.generate(\" \".join(k_df.hashtag))\n        DOM(f\"{col}:{k}\", \"h3\")()\n        image_dom(wc.to_image())()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Word cloud hashtag by country"},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_onwership(all_df, \"country\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Word cloud hashtag by state\n\n> See what's on eash states' mind"},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_onwership(all_df.query(\"country=='US'\"), \"state\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}