{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction and Agenda\n\nHi! Today I am working on the IBM Attrition Dataset. I want to show different methods to develop different models for prediction and discuss different ways to calculate feature importance of different models.\n\n**If you like my work, please UPVOTE :)**\n\n\n**Agenda:**\n\n1. Loading Data\n2. Standard EDA\n3. Feature Selection\n\n    3.1.     Preparing Likert-scale features\n    \n    3.2.     Dummy-Encoding\n    \n    3.3.     Elimination of Multicolinearity\n    \n4. Oversampling\n5. Training of various Models with CV and Parameter Tuning\n6. Testing the Models and Feature Importance\n7. Short Conclusion\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 1. Loading the Data","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ndf = pd.read_csv(\"../input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv\")\ndf.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 2. Standard EDA\n\nWe can see that some features dont have any variance and thus no value for our ML, this will be confirmed with the next cod","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of rows:{} \\nNumber of columns:{}\".format(df.shape[0], df.shape[1]))\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I like somthing like that, so I have a rough insight about the variation in information, type of data (continuous, numerical, categorical), ect.\nfor i in df:\n    print(i)\n    print(df[i].unique())\n    print(\"#\"*40)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some columns will be dropped because no they lack variation and thus also information.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# you can do similar automated things with codes like,\n    #from sklearn.feature_selection import VarianceThreshold\n    #VarianceThreshold(0.1).fit_transform(X)\n\ndf.drop(columns = {\"Over18\", \"EmployeeCount\", \"EmployeeNumber\", \"StandardHours\"}, inplace = True)\nprint(\"Number of variables after VarianceThreshold:\", df.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# no columns with high ration of missing values so no column to drop or to fill in (:\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# seperate categorical from numerical features\n\nnumerical = df.dtypes[df.dtypes != \"object\"].index\nprint(\"numerical features:\\n\", numerical)\nprint(\"+\"*80)\ncategorical = df.dtypes[df.dtypes== \"object\"].index\nprint(\"categorical features:\\n\", categorical)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we have a highly imbalanced dependant variable and skewness will be high as well\n\ndf[\"Attrition\"].value_counts().plot(kind = \"bar\", x = df[\"Attrition\"])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I think a very simple vizualisation of features can be helpful at times.\nThough, in this case, it is a very hard to interpret some features especcially the Rates (there is a disscussion on Kaggle about the meaning of those).\nLets take \"Overtime\". I think it is not clear what that means. I interpret this feature as \"Is making overtime a regular basis in your job?\". If so it seems to me that of attrition-group, the overtime-rate is much higher\nSingles seem to be more often affected by attrition.\nHealthcare Representatives seem to suffer less often under attrition due to unknown reasons.\nSales employees seem to suffer more often from attrition.\nEmployees who travel often seem to be affected more often from attrition.\nOther Features as \"Gender\" and \"Educational Field\" is not conclusive just by eyesight from my graph, which does not mean that this does not play a role in the models to come.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# simple Visulazation of the categorical features\nCat = categorical.tolist()\ntype(Cat)\nprint(Cat)\n\nfor x in Cat:\n    if x != \"Attrition\":\n        df[categorical].groupby([\"Attrition\", x])[x].count().unstack().plot(kind = \"bar\", stacked = True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The visualization of the numerical features is not as easy as I first thought.\n# Some features are in reality true categorical features with no intervall or true zero (Education etc.) which makes it harder to interpret\n# Other numerical features like \"YearsSinceLastPromotion\" are not as conclusive as I would have suspected\n# Younger people seem to suffer more under attrition.\n# People with greater \"DistanceFromHome\" is a candidate to influence attrition.\n# A higher Attrition-rate seems to be connected to lower \"JobLevel\" and lower \"JobSatisfaction\" and lower \"MonthlyIncome\" and so on.\n# The lack on possible StockOptions and a lower \"MonthlyIncome\" seem to be good indicators, which is quite reasonable.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nNum = numerical.tolist()\n\nfor j in Num:\n    fig, ax = plt.subplots()\n    ax = sns.boxplot(y = df[j], x = df[\"Attrition\"], data = df)\n    median = df.groupby(['Attrition'])[j].median().values\n    median_labels = [str(np.round(s, 2)) for s in median]\n    pos = range(len(median))\n    for i in pos:\n        ax.text(pos[i-1], median[i], median_labels[i], \n        horizontalalignment='center', size='x-small', color='w', weight='semibold')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 3. Feature Selection\n\nIn this section, I will first map all numerical features wich are true categorical features back to categorical features.\nSecond, I will dummy_encode the old and new categorical features.\nThird, I create an artificial dataset and upsample it, so the skewness will not dimish the picture of correlation between the target and other features.\nFouth, I create a heatmap and eliminate features with high correlation to each other as a feature selection measure.\nThe further Feature Selectoin will be done by correlation between the features.\n\nhttps://towardsdatascience.com/having-an-imbalanced-dataset-here-is-how-you-can-solve-it-1640568947eb","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 3.1 Preparing Likert-scale features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Education_dict = {1: 'Below College', 2: 'College', 3: 'Bachelor', 4: 'Master', 5: 'Doctor'}\n\nEnvironmentSatisfaction_dict = {1: 'Low', 2: 'Medium', 3: 'High', 4: 'Very High'}\n\nJobInvolvement_dict = {1: 'Low', 2: 'Medium', 3: 'High', 4: 'Very High'}\n\nJobSatisfaction_dict = {1: 'Low', 2: 'Medium', 3: 'High', 4: 'Very High'}\n\nPerformanceRating_dict = {1: 'Low', 2: 'Good', 3: 'Excellent', 4: 'Outstanding'}\n\nRelationshipSatisfaction_dict = {1: 'Low', 2: 'Medium', 3: 'High', 4: 'Very High'}\n\nWorkLifeBalance_dict = {1:'Bad', 2: 'Good', 3: 'Better', 4: 'Best'}\n\ndf[\"Education\"] = df[\"Education\"].map(Education_dict)\ndf[\"EnvironmentSatisfaction\"] = df[\"EnvironmentSatisfaction\"].map(EnvironmentSatisfaction_dict)\ndf[\"JobInvolvement\"] = df[\"JobInvolvement\"].map(JobInvolvement_dict)\ndf[\"JobSatisfaction\"] = df[\"JobSatisfaction\"].map(JobSatisfaction_dict)\ndf[\"PerformanceRating\"] = df[\"PerformanceRating\"].map(PerformanceRating_dict)\ndf[\"RelationshipSatisfaction\"] = df[\"RelationshipSatisfaction\"].map(RelationshipSatisfaction_dict)\ndf[\"WorkLifeBalance\"] = df[\"WorkLifeBalance\"].map(WorkLifeBalance_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2 Dummy-Encoding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.get_dummies(data = df, columns = ['Attrition', 'BusinessTravel', 'Department', 'EducationField', 'Gender','JobRole', 'MaritalStatus', 'OverTime', \"Education\", \"EnvironmentSatisfaction\", \"JobInvolvement\", \"JobSatisfaction\", \"PerformanceRating\", \"RelationshipSatisfaction\", \"WorkLifeBalance\"], drop_first = True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# creation of the artificial dataset and heatmap\n\nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE(sampling_strategy = \"minority\", random_state = 10)\nX_art, y_art = sm.fit_sample(df.drop(columns = {\"Attrition_Yes\"}), df[\"Attrition_Yes\"])\nartificial_df = pd.concat([pd.DataFrame(X_art), pd.DataFrame(y_art)], axis = 1)\nartificial_df.columns = [df.columns]\n\n\nfig, ax = plt.subplots(figsize = (10,10))\nax = sns.heatmap(artificial_df.corr())\nax.set_title(\"Correlation after SMOTE\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 3.3 Elimination of Multicolinearity\n\nTo gather sense of a heatmap with so many columns is rather dificult.\nTo make it easier I will write a code that eliminate mulitcolinearity.\nI will use a threshold of 0.7 which indiates that two columns contain quite similar information.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = artificial_df.corr()\n\nprint(\"before Multi-check:\", corr.shape)\nfor vars in corr:\n    mask = (corr[[vars]] > 0.7) & (corr[[vars]] < 1) | (corr[[vars]] < -0.7) \n    corr[mask] = np.nan\ncorr.dropna(inplace = True)\nprint(\"after Multi-check:\", corr.shape)\n# 9x rows got eliminated\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dont know why df[corr.index] does not work? any explanation is welcome!\n# anyway, now we got a dataframe ready!\ndf = df[corr.index.get_level_values(0)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## 4. Oversampling\nNow after cleaning the dataset from features with variance = zero, relabeling some \"numerical\" features back to true categorical features, dummy-encoding the categorical features (with n-1 new columns) and minimizing the problem of multicollinearity, for the next part, I want to oversample my dataset.\nIt is important to note that oversampling must be used carefully to avoid data leakage!\nIf you guys think you found something foul, contact me!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import (classification_report, recall_score)\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n\nX = df.drop(columns = {\"Attrition_Yes\"})\ny = df[\"Attrition_Yes\"]\n\nX_trainval, X_test, y_trainval, y_test = train_test_split(X, y, random_state = 7, stratify = y, test_size = 0.15)\n\nX_trainval_over, y_trainval_over = sm.fit_sample(X_trainval, y_trainval)\n\nprint(\"Size of trainval:{}\\nSize of test:{}\".format(X_trainval_over.shape, X_test.shape))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Training of various Models with CV and Parameter Tuning\nI tried various boundaries for the parameters in my algorithms, so I will only show you the intervals of interst","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Gradient Boosting Classifier\n* The Baseline Model Recall is 0.87 and highly overfitting. Lets regulate the model a little bit.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"all_scores = []\nlearning_rate = [0.025, 0.05, 0.1, 0.15, 0.2]\nfor lr in learning_rate:\n    GBM = GradientBoostingClassifier(random_state = 10, learning_rate = lr, max_features = \"sqrt\")\n    score = cross_val_score(GBM, X_trainval_over, y_trainval_over, cv = 5, scoring = \"recall\").mean()\n    all_scores.append(score)\nfig = plt.figure()\n\nplt.scatter(x = learning_rate, y = all_scores)\nplt.xlabel(\"learning_rate\")\nplt.ylabel(\"Recall-Score\")\nplt.title(\"GBM with different learning rate\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_scores = []\nn_estimators = [100, 200, 300, 400, 500]\nfor est in n_estimators:\n    GBM = GradientBoostingClassifier(random_state = 10, n_estimators = est, max_features = \"sqrt\")\n    score = cross_val_score(GBM, X_trainval_over, y_trainval_over, cv = 5, scoring = \"recall\").mean()\n    all_scores.append(score)\nfig = plt.figure()\n\nplt.scatter(x = n_estimators, y = all_scores)\nplt.xlabel(\"n_estimators\")\nplt.ylabel(\"Recall-Score\")\nplt.title(\"GBM with different numbers of estimators\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate = [0.2, 0.15, 0.1, 0.5, 0.025]\nn_estimators = [100, 200, 300, 400, 500]\nbest_score = 0\nfor est in n_estimators:\n    for lr in learning_rate:\n        GBM = GradientBoostingClassifier(random_state = 10, n_estimators = est, learning_rate = lr, max_features = \"sqrt\")\n        score = cross_val_score(GBM, X_trainval_over, y_trainval_over, cv = 5, scoring = \"recall\").mean()\n        if score > best_score:\n            best_score = score\n            best_parameters = {\"n_estimator\": est, \"learning_rate\": lr}\nprint(\"best recall:{} with best parameters:{}\".format(best_score,best_parameters))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RandomForest\nPartially, I will take other tuning parameters than with the DecisionTree. Baseline-Score with CV is 82.9. \nFor more info on Parameter Tuning (or in this case Pruning), you can check out : https://medium.com/all-things-ai/in-depth-parameter-tuning-for-random-forest-d67bb7e920d","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"all_scores = []\nestimators = range(400,900,100)\nfor est in estimators:\n    RF = RandomForestClassifier(random_state = 10, n_jobs = -1, n_estimators = est)\n    score = cross_val_score(RF, X_trainval_over, y_trainval_over, cv = 5, scoring = \"recall\").mean()\n    all_scores.append(score)\nfig = plt.figure()\n\nplt.scatter(x = estimators, y = all_scores)\nplt.xlabel(\"estimators\")\nplt.ylabel(\"Recall-Score\")\nplt.title(\"RandomForests with different numbers of estimators\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_scores = []\nmax_depth = range(1,10,1)\nfor depth in max_depth:\n    RF = RandomForestClassifier(random_state = 10, n_jobs = -1, max_depth = depth)\n    scores = cross_val_score(RF, X_trainval_over, y_trainval_over, cv = 5, scoring = \"recall\").mean()\n    all_scores.append(scores)\nfig = plt.figure()\nplt.scatter (x = max_depth, y = all_scores)\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"Recall-Score\")\nplt.title(\"RandomForests with different tree depth\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_depth = range(1,10,1)\nestimators = range(400,900,100)\nbest_score = 0\nfor depth in max_depth:\n    for est in estimators:\n        RF = RandomForestClassifier(random_state = 10, max_depth = depth, n_estimators = est, n_jobs = -1)\n        scores = cross_val_score(RF, X_trainval_over, y_trainval_over, cv = 5, scoring = \"recall\").mean()\n        if scores > best_score:\n            best_score = scores\n            best_parameters = {\"depth\": depth, \"estimators\": est}\nprint(\"best recall:{} with best parameters:{}\".format(best_score,best_parameters))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LogisticRegression\nBase Line Model Score is 79.6","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#the liblinear-solver (defualt solver for LR), can handle both MAE and MSE penalty\n\npenalty = [\"l1\", \"l2\"]\nall_score = []\nfor p in penalty:\n    LR = LogisticRegression(penalty = p)\n    scores = cross_val_score(LR, X_trainval_over, y_trainval_over,cv = 5, scoring = \"recall\").mean()\n    all_score.append(scores)\nfig = plt.figure()\nplt.scatter(x = penalty, y = all_score)\nplt.xlabel(\"penalty\")\nplt.ylabel(\"Recall-Score\")\nplt.title(\"LogisticRegression with MAE and MSE as penalty\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"C_value = [0.001, 0.01, 0.1, 1, 10, 100]                       \nall_score = []\nfor C in C_value:\n    LR = LogisticRegression(C = C, n_jobs = -1)\n    scores = cross_val_score(LR, X_trainval_over, y_trainval_over,cv = 5, scoring = \"recall\").mean()\n    all_score.append(scores)\nfig = plt.figure()\nplt.scatter (x = C_value, y = all_score)\nplt.xlabel(\"C\")\nplt.ylabel(\"Recall-Score\")\nplt.title(\"LogisticRegression with different regularization Values for C\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"C_value = [0.001, 0.01, 0.1, 1, 10, 100] \npenalty = [\"l1\", \"l2\"]\nbest_score = 0\n\nfor C in C_value:\n    for p in penalty:\n        LR = LogisticRegression(C = C, penalty = p, n_jobs = -1)\n        scores = cross_val_score(LR, X_trainval_over, y_trainval_over, cv = 5, scoring = \"recall\").mean()\n        if scores > best_score:\n            best_score = scores\n            best_parameters = {\"p\": p, \"C\": C}\nprint(\"best recall_score:\\n{}\".format(best_score))\nprint(\"with given parameters:\\n{}\".format(best_parameters))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Support Vector Machine \nBase Line Model Score is 0.792.\nSVMÂ´s perform much better when the dataset is scaled, so I used the MinMaxScaler to achive a better result.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(X_trainval_over)\nX_train_scaled = scaler.transform(X_trainval_over)\nSVM = SVC(random_state = 10, kernel = \"rbf\")\nscore = cross_val_score(SVM, X_train_scaled, y_trainval_over, cv = 5, scoring = \"recall\").mean()\nprint(\"BaseLineScore:\", score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# low values for C means that values far away from a imaginary decision boundry will be included in the calculation\n# vice versa, high values for C ignore these far away values and focus only on values near the imaginary decision boundry\nall_scores = []\nC_values = [0.001, 0.01, 0.1, 1, 10, 100]\nfor C in C_values:\n    SVM = SVC(random_state = 10, C = C, kernel = \"rbf\")\n    score = cross_val_score(SVM, X_train_scaled, y_trainval_over, cv = 5, scoring = \"recall\").mean()\n    all_scores.append(score)\nfig = plt.figure()\nplt.scatter(x = C_values, y = all_scores)\nplt.xlabel(\"Parameter C\")\nplt.ylabel(\"Recall_score\")\nplt.title(\"SVM with different C values\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_scores = []\ngamma = [0.001, 0.01, 0.1, 1, 10, 100]\nfor g in gamma:\n    SVM = SVC(random_state = 10, gamma = g, kernel = \"rbf\")\n    score = cross_val_score(SVM, X_train_scaled, y_trainval_over, cv = 5, scoring = \"recall\").mean()\n    all_scores.append(score)\nfig = plt.figure()\nplt.scatter(x = gamma, y = all_scores)\nplt.xlabel(\"Parameter Gamma\")\nplt.ylabel(\"Recall_score\")\nplt.title(\"SVM with different C values\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"C_Values = [0.001, 0.01, 0.1, 1, 10, 100]\nGamma = [0.001, 0.01, 0.1, 1, 10, 100]\nbest_score = 0\nfor C in C_Values:\n    for g in Gamma:\n        SVM = SVC(random_state = 10, C = C, gamma=g, kernel = \"rbf\")\n        score = cross_val_score(SVM, X_train_scaled, y_trainval_over, cv = 5, scoring = \"recall\").mean()\n        all_scores.append(score)\n        if score > best_score:\n            best_score = score\n            best_parameters = {\"Gamma\": g, \"C\": C}\nprint(\"best recall:{} with best parameters:{}\".format(best_score,best_parameters))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Testing the models and Feature Importance\n\nI will now test the models and (for me) more important, evaluate the feature importance of the various models.\nNow, somethings in advance:\nThe measured feature importances for the different models are not directly comparable, because of the method of measurement or different scale. But, we can for example compare the feature importance between the GradientBoostingClassifer and the RandomForestClassifier but not with LogisticRegression.\nWhat I think we can do, is to compare which features reappear in the top10 in the models, although this gives us also limited information.\nI try to explain what feature importance means in the various cases.\n\nhttps://blog.minitab.com/blog/adventures-in-statistics-2/how-to-identify-the-most-important-predictor-variables-in-regression-models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Gradient Boosting Classifier\nFrom what I got from the docs, the feature importance should be calculated the very similar as with the other tree based ensembles as forest. Basically, the more often a feature is used in for split, the more important it is. The aggregated Gini Importance (aka weighted impurity decrease) of a feature is averaged over the number of trees. \n\nhttps://sklearn.org/modules/ensemble.html","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"GBM = GradientBoostingClassifier(random_state = 10, n_estimators = 500, learning_rate = 0.2, max_features = \"sqrt\")\nGBM.fit(X_trainval_over, y_trainval_over)\nGBM_predict = GBM.predict(X_test)\nprint(round(recall_score(y_test, GBM_predict, average = \"micro\"),2))\nprint(classification_report(y_test, GBM_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance_GBM = pd.DataFrame(dict(Column = np.array(X.columns), Importance = GBM.feature_importances_)).sort_values(by = \"Importance\", ascending = False)\nfeature_importance_GBM\nfig = plt.figure(figsize = (14,4))\nplt.bar(x = feature_importance_GBM.iloc[:10, 0], height = feature_importance_GBM.iloc[:10, 1])\nplt.xticks(rotation = 75)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RandomForest\n\nA tree uses the Gini-Impurity as its measure to decide which characteristic to use for every split. For every feature, respectivly for every value of the feature, the model sums the decrease in Gini-Impurity whenever a Node uses this value or feature. The calculated sum is then divided by the number of trees in the forest that use it. Luckily, a scale is not necessary for this calculation.\n\nFrom the docs we get: \"The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature.\" Also the formula for the Gini Importance (aka weighted impurity decrease) is: \n\"N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity)\nN is the total number of samples, N_t is the number of samples at the current node, N_t_L is the number of samples in the left child, and N_t_R is the number of samples in the right child.\"\nhttps://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier https://stackoverflow.com/questions/49170296/scikit-learn-feature-importance-calculation-in-decision-trees https://towardsdatascience.com/model-based-feature-importance-d4f6fb2ad403\n\nThe feature importance of such ensemble algorithms do not tell us something about the magnitude or the direction of the relationship. All we can tell is, that featureX was helpful (or not) in predicting the outcome.\nI find several things interesting when we compare the importance of the features between the forest and the GBM.\nWithin the top10 features we find an intersection of 9 features, which I interpret as some sort of \"confirmation\" in the consistency of my models.\n2 monetary motives are in the intersection (Monthly_Rate, Stockoption) and the forest takes also \"DailyRate\" as another important feature. Medics and Life_Science are relevant to predict whether a person suffers under attrition. Further interviews are needed here to conclude the direction of impact, if no other data is provided. \nThe private life of the employees seem to play an important role in both models (Work_Life_Balance, Martial_Status) which is not suprising. From this point on, I am a little suprised that a classical attribute for attrition like \"Overtime\" is not in the top10 of both models.\nOther soft indicators such \"Jobsatisfaction\" or \"Environmentsatisfaction\" seem to play a role too.\n ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"RF = RandomForestClassifier(random_state = 10, n_jobs = -1, n_estimators = 400, max_depth = 9)\nRF.fit(X_trainval_over, y_trainval_over)\nRF_predict = RF.predict(X_test)\nprint(round(recall_score(y_test, RF_predict, average = \"micro\"), 2))\nprint(classification_report(y_test, RF_predict))\n#print(RF_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance_RF = pd.DataFrame(dict(Column = np.array(X.columns), Importance = RF.feature_importances_)).sort_values(by = \"Importance\", ascending = False)\nfeature_importance_RF\nfig = plt.figure(figsize = (14,4))\nplt.bar(x = feature_importance_RF.iloc[:10,0] , height = feature_importance_RF.iloc[:10,1])\nplt.xticks(rotation = 75)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LogisticRegression\nFeature importance with logistic regression is different than with trees and forests.\nHere, the feature-importance is measured by the regression-coefficient. For categorical features it is a little easier to interpret. The effect of \"OverTime_Yes\" on the target \"Attrition\" is more than twice as big as \"JobInvolement_Low\". \nFor numeric data, it is a little more complicated:\nIn the Model, the coefficient for a numeric predictor variable shows the effect on the target variable with a 1-unit change in the predictor variable.\nStatistically, a positive sign (positive coefficient) for a feature means that all else being equal, the chances for attrition are higher, e.g.: A Person working overtime is more likely to result in attrition. In contrast, a person with an educational background in Medical or LifeScience or good WorkLifeBalance or high JobSatisfaction seem to be less likely to suffer under attrition.\nIn my opinion, the easily understandable coefficients aka feature importance of a linear model makes it a wonderful candidate to chose, if comparing the features and researching the impact of features is important to the task.\n\nFor more I highly recommend the work of Tim Bock:\nhttps://www.displayr.com/how-to-interpret-logistic-regression-coefficients/?utm_referrer=https%3A%2F%2Fwww.google.com%2F\n\nInterestingly the 5 most negative coefficients have a way higher impact than the 5 most positive coefficients.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"LR = LogisticRegression(C = 0.1, penalty = \"l2\", n_jobs = -1)\nLR.fit(X_trainval_over, y_trainval_over)\nLR_predict = LR.predict(X_test)\nprint(round(recall_score(y_test, LR_predict, average = \"micro\"), 2))\nprint(classification_report(y_test, LR_predict))\n#print(LR_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# nice hack for first n rows and last n rows in a graph\nfeature_importance_LR = pd.DataFrame(dict(Column = np.hstack(np.array([X.columns])), Importance = np.hstack(LR.coef_))).sort_values(by = \"Importance\", ascending = False)\nfeature_importance_LR\nfig = plt.figure(figsize = (14,4))\nplt.bar(x = pd.concat([feature_importance_LR.iloc[:5,0],feature_importance_LR.iloc[-5:,0]]), height = pd.concat([feature_importance_LR.iloc[:5,1],feature_importance_LR.iloc[-5:,1]]))\nplt.xticks(rotation = 75)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Support Vector Machine\nI used a SVC with a Radial Basis Function, not only because it often gives better results but also to show another way to compute feature importance.\n\nThe main idea is to measure the drop in accuracy when shuffling a single column of the test-set. A copy-dataset will ensure not to waste the original test-set. By shuffling, we detach the affected feature from the target variable and hence decrease its information value. This process is ofcourse iterated for all features. Before all of that a benchmark with all features intact is created \n\nInterestingly the calculated feature importances are pretty small. I only can imagine that there is still much multicolinearity left among the features, although I eliminated everything with correlation > 0.7 and < -0.7. Many features have no importance for the rbf kernel at all. \nAs with feature importance from Forests or Gradient Boosters, there is no way to say something about the direction or magnitude of certain features. \nBy the way, this is also a nice method for feature selection and thus faster models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"SVM = SVC(random_state = 10,gamma = 0.001, C = 0.001, kernel = \"rbf\")\nSVM.fit(X_trainval_over, y_trainval_over)\nscaler.fit(X_trainval_over)\nX_test_scaled = scaler.transform(X_test)\nSVM_predict = SVM.predict(X_test_scaled)\n#print(SVM_predict)\nclean_score = round(recall_score(y_test, SVM_predict, average = \"micro\"), 5)\nprint(\"Recall without Shuffling:\",clean_score)\nprint(classification_report(y_test, SVM_predict))\n\nall_scores = []\nfor i in range(X_test.shape[1]):\n    X_test_noisy = X_test_scaled.copy()\n    np.random.shuffle(X_test_noisy[:, i])                 \n    noisy_predict = SVM.predict(X_test_noisy)\n    noisy_score = round(recall_score(y_test, noisy_predict, average = \"micro\"), 5)\n    feature_imp = clean_score-noisy_score\n    all_scores.append(feature_imp)\n#print(all_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance_SVM = pd.DataFrame(dict(Column = np.array(X.columns), Importance = np.array(all_scores))).sort_values(by = \"Importance\", ascending = False)\nfig = plt.figure(figsize=(16,4))\nplt.bar(x = feature_importance_SVM.iloc[:11, 0], height = feature_importance_SVM.iloc[:11, 1])\nplt.title(\"Feature Importance for SVM with rfb Kernel\")\nplt.xticks(rotation = 75)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## 7. Short Conclusion\n\nHope you enjoyed some parts of my work.\nFrom what I took from the various models, I think I would stick with the GBClassifier. \nNot only where its results regarding feature importance very consistent with the RandomForest, but it also had the highest recall score on correctly predicted attrition cases (see at the classification_report).\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}