{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Problem statement:**\nDevelopment of a system for the automatic search for answers and the formation of the context of documents\n\n# **Problems:**\nThe main problem of information retrieval is that the developed system should not only find sentences with similar words that appear in the request, but also take into account the context.\n\n# **Solution:**\nTo solve the Question-Answer problem, a pre-trained model of the BERT neural network on the squad dataset was used, and to obtain the main context of many articles, the TF-IDF algorithm with the KMeans clusterizer was used to visualize the data."},{"metadata":{},"cell_type":"markdown","source":"# **STEP 1.** Read the data on json format from biorxiv, comm_use_subset, noncomm_use_subset.\nWe form text documents that store the text of the papers, annotation, information about the authors and the title\n\nIf the paper does not contain a title, abstract or authors, the field is replaced with the value None"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport json\nimport time, sys\nimport re\n\nMainDirectory = \"../input/CORD-19-research-challenge/\" # Path to coronavirus dataset\nFoldersPath = [\"biorxiv_medrxiv/biorxiv_medrxiv/pdf_json/\", \"comm_use_subset/comm_use_subset/pdf_json/\",\n               \"noncomm_use_subset/noncomm_use_subset/pdf_json/\"]\n\nTitles = []\nAuthors = []\nAbstracts = []\nTexts = []\n\nfor CurrentDirectory in FoldersPath:\n    print(\"Current Directory\")\n    print(CurrentDirectory)\n   \n    Directory = MainDirectory + CurrentDirectory\n    Files = os.listdir(Directory)\n    print(\"Number files in directory\")\n    print(len(Files))\n    \n    print(\"Status Preprocessing, %:\")\n    Counter = 0\n    for CurrentFileName in Files:\n        \n        FileStream = open(Directory + str(CurrentFileName), \"r\")\n        CurrentFile = json.load(FileStream)\n        FileStream.close()\n        \n        sys.stdout.write(str(Counter/len(Files) * 100))\n        sys.stdout.flush()\n        sys.stdout.write(\"\\r\")\n        Counter += 1\n\n        Author = \"\"\n        AllKeys = CurrentFile.keys()\n        AllKeys = list(AllKeys)\n        MetaDataKeys = CurrentFile[AllKeys[1]]\n\n        Title = MetaDataKeys[\"title\"]\n\n        if len(MetaDataKeys[\"authors\"]) > 0:\n            Author += MetaDataKeys[\"authors\"][0][\"first\"] + \" \" + MetaDataKeys[\"authors\"][0][\"last\"]\n        else:\n            Author = \"None\"\n\n        if len(CurrentFile[AllKeys[2]]) > 0:\n            Abstract = CurrentFile[AllKeys[2]][0][\"text\"]\n        else:\n            Abstract = \"None\"\n\n        if len(CurrentFile[AllKeys[3]]) > 0:\n            Text = CurrentFile[AllKeys[3]][0][\"text\"]\n        else:\n            Text = \"None\"\n\n\n        if len(Title) > 0:\n            Titles.append(Title)\n        else:\n            Titles.append(\"None\")\n\n        if len(Author) > 0:\n            Authors.append(Author)\n        else:\n            Authors.append(\"None\")\n\n        if len(Abstract) > 0:\n            Abstracts.append(Abstract)\n        else:\n            Abstracts.append(\"None\")\n\n        if len(Text) > 0:\n            Texts.append(Text)\n        else:\n            Texts.append(\"None\")\nprint(\"\\n\")\n\n\nNewTexts = []\nfor CurrentPaper in Texts:\n    Result = re.split(\"\\s[a-z]+\\s\", CurrentPaper)\n    if len(Result) > 20:\n        NewTexts.append(CurrentPaper)\n        \nTexts = NewTexts\nprint(\"Number Papers\")\nprint(len(Texts))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Attention!!! The code block discussed below will take about 40 minutes to complete on a full dataset.**\n\n# **STEP 2.** Retrieving the context from the dataset.\n* The first step is to clean the dataset from the \"garbage\". Delete all characters that are not letters of the alphabet or numbers.\n\n* We will form a text dataframe that includes the main text of the paper. Using the spacy library we bring all verbs to the correct form and all nouns to the singular. This approach allows you to reduce the sentences space of proposals and better understand the context.\n* To get the area of text representations the word bag approach was used with the countVectorizer library implemented in sklearn. However, this approach did not give satisfactory results, since the classes turned out to be significantly unbalanced. This means that the algorithm did a poor job.\n* The TF-IDF frequency coding approach implemented in the sklearn Tfidfvectorizer library, which together with KMeans showed much better results, was also used. Clasters turned out to be more balanced than using the CountVectorizer approach.\n* For clustering, we used the KMeans method with 10 clusters.\n* To form a context, the most frequently encountered words that have high weight in the presentation area were selected from the resulting cluster.\n* PCA, t-SNE, NMF algorithms are used to reduce the feature space"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import spacy\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport re\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import NMF\nimport pylab\nfrom mpl_toolkits.mplot3d import Axes3D\nimport time, sys\n\nclass DocumentsClusterisation:\n    def DataPreprocessing(self):\n        print(\"Begin Data Preprocessing Method\")\n\n        self.TextDF = pd.DataFrame()\n        self.TextDF[\"Text\"] = Texts\n        self.TextDF[\"Text\"] = self.TextDF[\"Text\"].str.replace(r\"\\W+\", \" \") #Replace all non alphabetic symbols\n        self.TextDF[\"Text\"] = self.TextDF[\"Text\"].str.replace(r\"[0-9]+\", \" \") #Replace all numbers\n        self.TextDF[\"Text\"] = self.TextDF[\"Text\"].str.replace(r\"\\s\\w\\s\", \" \") #Replace all single words\n        self.TextDF[\"Text\"] = self.TextDF[\"Text\"].str.replace(r\"\\s+\", \" \") #Replace long spaces to single space \n        self.TextDF[\"Text\"] = self.TextDF[\"Text\"].str.lower()\n\n        self.Regular = r\"[a-z]+\"\n        for index, CurrentDocument in enumerate(self.TextDF[\"Text\"].tolist()):\n            NewCurrentDocument = \"\"\n            AllWords = re.findall(self.Regular, CurrentDocument)\n            for CurrentWord in AllWords:\n                NewCurrentDocument += CurrentWord + \" \"\n            self.TextDF[\"Text\"].iloc[index] = NewCurrentDocument\n\n        #Use spacy to delete stopwords and correct nouns and verbs: can`t ~ can not, dogs ~ dog\n        self.NLP = spacy.load(\"en_core_web_sm\") \n\n        Counter = 0\n        self.TextList = self.TextDF[\"Text\"].tolist()\n        print(\"Preprocessing Papers. Status %:\")\n        for CurrentDocument in self.TextList:\n        \n            sys.stdout.write(str(Counter/len(self.TextList) * 100))\n            sys.stdout.flush()\n            sys.stdout.write(\"\\r\")\n            \n            CurrentText = \"\"\n            Document = self.NLP(CurrentDocument)\n            for CurrentToken in Document:\n                if CurrentToken.dep_ != \"punct\" and CurrentToken.is_stop != True:\n                    CurrentText += CurrentToken.lemma_ + \" \"\n\n            self.TextDF[\"Text\"].iloc[Counter] = CurrentText\n            Counter += 1\n\n        self.Text = self.TextDF[\"Text\"].tolist()\n\n        print(\"End Data Preprocessing Method\")\n\n    #Use Bag of words\n    def CountVectorizerTokenizator(self):\n        print(\"Begin CountVectorizer Model\")\n        self.CountVectorizerModel = CountVectorizer(stop_words=\"english\")\n        self.Features = self.CountVectorizerModel.fit_transform(self.Text)\n        print(\"Shape \")\n        print(self.Features.shape)\n        self.FeaturesExtended = self.Features\n        self.FeaturesExtended = self.FeaturesExtended.toarray()\n        print(\"End CountVectorizer Model\")\n\n    def TFIDFTokenizator(self):\n        print(\"Begin TFIDF Model\")\n        self.TfidfVectorizerModel = TfidfVectorizer(stop_words=\"english\")\n        self.Features = self.TfidfVectorizerModel.fit_transform(self.Text)\n        self.Vocabulary = self.TfidfVectorizerModel.vocabulary_\n        self.VocabularyKeys = self.Vocabulary.keys()\n        self.VocabularyItems = []\n\n        self.FeaturesExtended = self.Features\n        self.FeaturesExtended = self.FeaturesExtended.toarray()\n\n        for i in self.VocabularyKeys:\n            self.VocabularyItems.append(self.Vocabulary[i])\n\n        self.VocabularyKeys = list(self.VocabularyKeys)\n        print(\"End TFIDF Model\")\n\n    #10 clusters for 10 tasks \n    def ClusteringKMeans(self):\n        print(\"Begin KMeans Model\")\n        self.KMeansModel = KMeans(n_clusters=10)\n        self.KMeansModel.fit(self.Features)\n        self.Lables = self.KMeansModel.labels_\n\n        self.CountLables = []\n        self.UniqueLables = np.unique(self.Lables)\n        print(\"Number Unique Items per Cluster\")\n        for i in self.UniqueLables:\n            Counter = 0\n            for j in self.Lables:\n                if i==j:\n                    Counter += 1\n            self.CountLables.append(Counter)\n            print(\"Lable \" + str(i) + \" - \" + str(Counter))\n\n        #Main KeyWords in cluster\n        self.ClusterDescription = []\n        self.MostFreqValues = []\n        print(\"Current Cluster:\")\n        for indexI, i in enumerate(self.UniqueLables):\n            \n            sys.stdout.write(str(indexI))\n            sys.stdout.flush()\n            sys.stdout.write(\"\\r\")\n            \n            self.CurrentClusterDescription = []\n\n            self.GroupedFeatures = np.zeros((self.CountLables[indexI],self.FeaturesExtended.shape[1]), dtype=\"float32\")\n            Counter = 0\n            self.Positions = []\n            for indexJ, j in enumerate(self.Lables):\n                if i == j:\n                    self.GroupedFeatures[Counter] = self.FeaturesExtended[indexJ]\n                    Counter += 1\n\n            for k in range(10):\n                self.PositionMaxValue = np.unravel_index(np.argmax(self.GroupedFeatures, axis=None), self.GroupedFeatures.shape)\n                self.PositionMaxValue = list(self.PositionMaxValue)\n                self.Positions.append(self.PositionMaxValue)\n                self.GroupedFeatures[self.PositionMaxValue[0]][self.PositionMaxValue[1]] = 0\n\n            Sum = []\n\n            for k in range(self.GroupedFeatures.shape[1]):\n                Counter = 0\n                for l in range(self.GroupedFeatures.shape[0]):\n                    if self.GroupedFeatures[l][k] != 0:\n                        Counter += 1\n\n                Sum.append(Counter)\n\n            Sum = np.array(Sum)\n\n            #Most frequently words in cluster\n            self.MostFreq = []\n            CurrentMostFreqValues = []\n            for k in range(10):\n                ArgMax = np.argmax(Sum)\n                CurrrentValue = Sum[ArgMax]\n                CurrentMostFreqValues.append(CurrrentValue)\n                self.MostFreq.append(ArgMax)\n                Sum[ArgMax] = 0\n\n            for CurrentPosition in self.MostFreq:\n                for indexJ, CurrentItem in enumerate(self.VocabularyItems):\n                    if CurrentPosition == CurrentItem:\n                        self.CurrentClusterDescription.append(self.VocabularyKeys[indexJ])\n                        \n            \n\n            self.ClusterDescription.append(self.CurrentClusterDescription)\n            self.MostFreqValues.append(CurrentMostFreqValues)\n\n        for id, CurrentDescription in enumerate(self.ClusterDescription):\n          print(\"Cluster \" + str(id) + \" KeyWords\")\n          print(CurrentDescription)\n          Fig, Axis = plt.subplots(figsize=(25,10))\n          plt.rc('xtick', labelsize=20)\n          plt.rc('ytick', labelsize=20)\n          plt.rc('axes', titlesize=20)\n          plt.rc('axes', labelsize=20)\n          plt.rc('legend', fontsize=20)\n          plt.rc('figure', titlesize=20)\n          plt.rc('font', size=20)\n          Axis.grid()\n          Axis.bar(CurrentDescription,self.MostFreqValues[id])\n          Axis.set_xlabel(\"Most frequently words\")\n          Axis.set_ylabel(\"Frequency\")\n          Title = \"Most frequently words \" + str(id) + \" Cluster\"\n          plt.title(Title)\n          plt.show()\n            \n            \n        \n        print(\"\\n\")\n        print(\"End KMeans Model\")\n\n    def PCADecomposition2D(self):\n        print(\"PCA Decomposition 2D\")\n        self.PCAModel = PCA(n_components=2)\n        self.Result = self.PCAModel.fit_transform(self.FeaturesExtended)\n        print(self.Result)\n\n    def PCADecomposition3D(self):\n        print(\"PCA Decomposition 3D\")\n        self.PCAModel = PCA(n_components=3)\n        self.Result = self.PCAModel.fit_transform(self.FeaturesExtended)\n        print(self.Result)\n\n    def TSNEDecomposition(self):\n        print(\"t-SNE Decomposition 2D\")\n        self.TSNEModel =TSNE(n_components=2, n_jobs=4)\n        self.Result = self.TSNEModel.fit_transform(self.FeaturesExtended)\n\n        print(self.Result)\n\n    def NMFDecomposition(self):\n        print(\"NMF Decomposition\")\n        print(self.FeaturesExtended.shape)\n        self.NMFModel = NMF(n_components = 2)\n        self.Result = self.NMFModel.fit_transform(self.FeaturesExtended)\n        H = self.NMFModel.components_\n        H = np.array(H)\n        print(self.Result)\n\n\n    def SimilarityDocuments(self):\n        self.NLP = spacy.load(\"en_core_web_sm\")\n        PatternDocuments = [\"transmission, incubation, environmental, stability\",\n                            \"COVID-19 risk factors\"]\n\n\n        AllIndexDocuments = []\n        for CurrentPatternDocument in PatternDocuments:\n            SimilarityDocuments = []\n            CurrentPatternDocument = self.NLP(CurrentPatternDocument)\n            for IndexDocument, CurrentDoucumentI in enumerate(self.Text):\n                CurrentDoucumentI = self.NLP(CurrentDoucumentI)\n                Similarity = CurrentDoucumentI.similarity(CurrentPatternDocument)\n                print(Similarity)\n                if Similarity> 0.6:\n                    SimilarityDocuments.append(IndexDocument)\n            AllIndexDocuments.append(SimilarityDocuments)\n\n        print(\"Counter\")\n        for i in range(len(AllIndexDocuments) - 1):\n            Counter = 0\n            FirstIndexes = AllIndexDocuments[i]\n            SecondIndexes = AllIndexDocuments[i + 1]\n            for j in FirstIndexes:\n                if j in SecondIndexes:\n                    Counter += 1\n            print(Counter/len(AllIndexDocuments[0]))\n            print(Counter / len(AllIndexDocuments[1]))\n\n        Counter = 0\n\n\n    def Visualisation2D(self):\n        print(\"Visualisation\")\n        CplorList = [\"red\", \"blue\", \"black\", \"orange\", \"green\", \"yellow\", \"cyan\",\"magenta\", \"lime\",\"violet\"]\n        CurrentText = [i for i in range(1,11)]\n        Fig, Axis = plt.subplots(figsize=(25,10))\n        plt.rc('xtick', labelsize=18)\n        plt.rc('ytick', labelsize=18)\n        plt.rc('axes', titlesize=18)\n        plt.rc('axes', labelsize=18)\n        plt.rc('legend', fontsize=18)\n        plt.rc('figure', titlesize=30)\n        plt.rc('font', size=18)\n        for indexI, i in enumerate(self.UniqueLables):\n            CurrentX = []\n            CurrentY = []\n\n            for indexJ, j in enumerate(self.Lables):\n                if i == j:\n                    CurrentX.append(self.Result[indexJ,0])\n                    CurrentY.append(self.Result[indexJ,1])\n\n            Axis.plot(CurrentX, CurrentY, marker=\"o\", linewidth=0, color=CplorList[indexI], label =\"Cluster \" + str(CurrentText[indexI]))\n        Axis.grid()\n        Axis.set_xlabel(\"Component 1\")\n        Axis.set_ylabel(\"Component 2\")\n        plt.title(\"PCE Decomposition\")\n        Axis.legend()\n        plt.show()\n\n    def Visualisation3D(self):\n        CplorList = [\"red\", \"blue\", \"black\", \"orange\", \"green\", \"yellow\", \"cyan\",\"magenta\", \"lime\",\"violet\"]\n        CurrentText = [i for i in range(1,11)]\n        Fig = pylab.figure()\n        Axes = Axes3D(Fig)\n        for indexI, i in enumerate(self.UniqueLables):\n            CurrentX = []\n            CurrentY = []\n            CurrentZ = []\n\n            for indexJ, j in enumerate(self.Lables):\n                if i == j:\n                    CurrentX.append(self.Result[indexJ,0])\n                    CurrentY.append(self.Result[indexJ,1])\n                    CurrentZ.append(self.Result[indexJ,2])\n            Axes.scatter(CurrentX, CurrentY, CurrentZ, color=CplorList[indexI])\n\n        pylab.show()\n\n\n\nDocumentsClassterisationObj = DocumentsClusterisation()\nDocumentsClassterisationObj.DataPreprocessing()\n#Features = CountVectorizerTokenizator(Text=Text)\nDocumentsClassterisationObj.TFIDFTokenizator()\n#DocumentsClassterisationObj.SimilarityDocuments()\nDocumentsClassterisationObj.ClusteringKMeans()\nDocumentsClassterisationObj.PCADecomposition2D()\nDocumentsClassterisationObj.Visualisation2D()\n#DocumentsClassterisationObj.PCADecomposition3D()\n#DocumentsClassterisationObj.NMFDecomposition()\n#DocumentsClassterisationObj.Visualisation()\n#DocumentsClassterisationObj.Visualisation3D()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Using the obtained keywords one can formulate the context of the clusters.**\n\n* **Cluster 1 key words:** influenza, virus, pandemic, human, infection, cause, health, disease, respiratory, case\n\nFrom these tokens, it can be assumed that the papers of this class relate to the causes of transmission of the disease between people and the respiratory syndrome.\n* **Cluster 2 key words:** respiratory, infection, cause, virus, child, tract, disease, acute, human, viral\n\nFrom the data of tokens, it can be assumed that the papers of this class are related to the acute course of the disease in children, in particular diseases of the gastrointestinal tract\n\n* **Cluster 3 key words:** cov, respiratory, syndrome, coronavirus, east, mers, human, middle, infection, severe\n\nFrom these tokens, it can be assumed that the papers of this class relate to those about the place of occurrence of an infectious disease\n\n* **Cluster 4 key words:** cell, response, infection, immune, virus, viral, protein, include, host, receptor\n\nFrom these tokens, it can be assumed that the papers of this class relate to the structure of the coronavirus, its biological features and, possibly, the transmission method.\n\n* **Cluster 5 key words:** china, case, coronavirus, wuhan, novel, report, december, disease, sars, health\n\nFrom these tokens, it can be assumed that the papers of this class relate to topics about literature, possibly by Chinese scientists\n\n* **Cluster 6 key words:** al, et, virus, disease, human, infection, cause, include, viral, cell\n\nFrom these tokens, it can be assumed that the papers of this class relate to the topic of changing human cells, the functioning of the virus inside the cells. It can be seen that in this situation the words al, et. This suggests that perhaps a more thorough, clean text is required.\n\n* **Cluster 7 key words:** pedv, virus, diarrhea, porcine, epidemic, pig, piglet, cause, swine, mortality\n\nFrom these tokens, it can be assumed that the papers of this class relate to those on the methods of transmission of the virus through animals, in particular through pigs.\n\n* **Cluster 8 key words:** protein, virus, rna, viral, genome, cell, strand, include, single, family\n\nFrom these tokens, it can be assumed that the papers of this class relate to those on the methods of transmission of the virus through animals, in particular through pigs.\n\n* **Cluster 9 key words:** disease, infection, include, study, virus, cause, high, human, patient, result\n\nFrom these tokens, it can be assumed that the papers of this class relate to those about the study of ways to treat a person\n\n* **Cluster 10 key words:** disease, virus, health, human, infection, infectious, cause, outbreak, public, include\n\nFrom these tokens, it can be assumed that the papers of this cluster relate to topics on the methods of spreading and outbreaks of viral infection in society."},{"metadata":{},"cell_type":"markdown","source":"# **Step 3. Filtering the dataset**\nTo use the Question-Answer system, it is necessary to filter out articles whose context is not related to coronavirus"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run if you need to filter covid dataset\nprint(\"Filter Dataset\")\nimport re\nimport copy\nimport numpy as np\nKeyWords = [\"corona\", \"cov\", \"sars\", \"cord\", \"covid-19\"]\nHelpText = copy.copy(Texts)\n\n\nfor index, CurrentText in enumerate(HelpText):\n  CurrentText = CurrentText.lower()\n  HelpText[index] = CurrentText\n\nGeneralResult = []\nfor Word in KeyWords:\n  ResultList = []\n  for CurrentText in HelpText:\n    Result = re.findall(Word,CurrentText)\n    ResultList.append(Result)\n  GeneralResult.append(ResultList)\n\nIndexes = []\nfor i in range(len(GeneralResult[0])):\n  Sum = 0\n  for j in range(len(KeyWords)):\n    Sum += len(GeneralResult[j][i])\n  if Sum != 0:\n    Indexes.append(i)\n\nTexts = np.array(Texts)\nAbstracts = np.array(Abstracts)\nTitles = np.array(Titles)\nAuthors = np.array(Authors)\n\nTexts = Texts[Indexes]\nAbstracts = Abstracts[Indexes]\nTitles = Titles[Indexes]\nAuthors = Authors[Indexes]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Length Text, Abstract, Authors, Titles\")\nprint(len(Texts))\nprint(len(Abstracts))\nprint(len(Authors))\nprint(len(Titles))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Step 4. Using a BERT-based Question and Answer System**"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install cdqa\nimport pandas as pd\nfrom ast import literal_eval\nfrom cdqa.utils.filters import filter_paragraphs\nfrom cdqa.utils.download import download_model, download_bnpp_data\nfrom cdqa.pipeline.cdqa_sklearn import QAPipeline\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"download_model(model='bert-squad_1.1', dir='./models')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\n\nimport en_core_web_lg\n\n#Download large english words model\nprint(\"Download large model\")\nNLP = en_core_web_lg.load()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CDQA dataframe format. It is important to make dataframe which consist of columns\n#'date', 'title', 'category', 'link', 'abstract', 'paragraphs'\n\nDF = pd.DataFrame(columns=['date', 'title', 'category', 'link', 'abstract', 'paragraphs'])\n\nfor i in range(len(Texts)):\n  CurrentText = [Texts[i]]\n  CurrentList = [\"None\", Titles[i], \"None\", \"None\", Abstracts[i], CurrentText]\n  CurrentList = np.array(CurrentList)\n  CurrentList = CurrentList.reshape(1, CurrentList.shape[0])\n\n  CurrentList = pd.DataFrame(data = CurrentList, columns=['date', 'title', 'category', 'link', 'abstract', 'paragraphs'])\n\n  DF = pd.concat([DF, CurrentList], ignore_index=True)\n\nprint(DF.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DF = filter_paragraphs(DF)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CDQAPipeline = QAPipeline(reader='models/bert_qa.joblib')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CDQAPipeline.fit_retriever(df=DF)\nQuestion = 'How long does covid live outside the human body'\nQuestionList = [\"What is the incubation period for covid\", \"How long does a covid live on different surfaces?\",\n               \"How long has a person been ill with a covid?\", \"What covid transmission routes are known\",\n               \"How not to get infected with covid\"]\n\nfor CurrentQuestion in QuestionList:\n    Predict = CDQAPipeline.predict(query=CurrentQuestion)\n    print(\"Question: \" + CurrentQuestion)\n    print(\"Answer: \" + Predict[0])\n    print(\"Title: \" + Predict[1])\n    print(\"Current Paper: \" + Predict[2])\n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Step 5. BERT embedding. Designing a vector representation of papers using t5 google transformer**"},{"metadata":{},"cell_type":"markdown","source":"# **To analyze the content of papers, a new Google neural network is used. To install, you must run the following commands.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pytorch-pretrained-bert\n!pip install transformers==2.8.0\n!pip install torch==1.4.0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **The BERT neural network is used to obtain a vector representation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n\nimport logging\nlogging.basicConfig(level=logging.INFO)\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\nPapers = Texts\n\nimport re, sys\nimport numpy as np\n\nmodel = BertModel.from_pretrained('bert-base-uncased')\nmodel.eval()\n\nprint(\"Current Paper ID\")\nPaperEmbedding = []\n\nAllPaperWords = []\nAllPaperWordsEmbedding = []\n\nfor CurrentPaperID, CurrentPaper in enumerate(Papers):\n\n  sys.stdout.write(str(CurrentPaperID * 100/ len(Papers)))\n  sys.stdout.flush()\n  sys.stdout.write(\"\\r\")\n  \n  SentenceEmbeddingList = []\n  PaperSentences = re.split(\"\\.\\s\",CurrentPaper)\n\n  CurrentPaperWords = []\n  CurrentPaperWordsEmbedding = []\n\n  for CurrentSentence in PaperSentences:\n    CurrentSentence = \"[CLS] \" + CurrentSentence + \" [SEP]\"\n    TokenizedText = tokenizer.tokenize(CurrentSentence)\n\n    if len(TokenizedText) < 510:\n\n        IndexedTokens = tokenizer.convert_tokens_to_ids(TokenizedText)\n        SegmentsId = [1] * len(TokenizedText)\n        TokensTensor = torch.tensor([IndexedTokens])\n        SegmentsTensor = torch.tensor([SegmentsId])\n        with torch.no_grad():\n          EncodedLayer, _ = model(TokensTensor, SegmentsTensor)\n\n        TokenEmbedding = torch.stack(EncodedLayer, dim=0)\n        TokenEmbedding.size()\n        TokenEmbedding = torch.squeeze(TokenEmbedding, dim=1)\n        TokenEmbedding.size()\n        TokenEmbedding = TokenEmbedding.permute(1,0,2)\n        TokenEmbedding.size()\n        TokensVectors = []\n        for CurrentToken in TokenEmbedding:\n          SumVectors = torch.sum(CurrentToken[-4:], dim=0)\n          TokensVectors.append(SumVectors)\n\n\n        for CurrentToken in TokenizedText:\n          CurrentPaperWords.append(CurrentToken)\n\n        for CurrentWordVector in TokensVectors:\n          CurrentPaperWordsEmbedding.append(CurrentWordVector)\n\n        TV = EncodedLayer[11][0]\n\n        SentenceEmbedding = torch.mean(TV, dim=0)\n\n        SentenceEmbedding = np.array(SentenceEmbedding)\n        SentenceEmbeddingList.append(SentenceEmbedding)\n   \n  \n  AveragePaperEmbedding = np.mean(SentenceEmbeddingList, axis=0)\n  PaperEmbedding.append(AveragePaperEmbedding)\n\n  AllPaperWords.append(CurrentPaperWords)\n  AllPaperWordsEmbedding.append(CurrentPaperWordsEmbedding)\n\n\nPaperEmbedding = np.array(PaperEmbedding)\n\nprint(\"Shape Paper Embedding\")\nprint(PaperEmbedding.shape)\nprint(\"Length Word List\")\nprint(len(AllPaperWords))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nKMeansModel = KMeans(n_clusters=10)\nKMeansModel.fit(PaperEmbedding)\nLables = KMeansModel.labels_\nprint(Lables)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\nPCAModel = PCA(n_components=2)\nResult = PCAModel.fit_transform(PaperEmbedding)\n\n\nUniqueLables = np.unique(Lables)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Result.shape)\nimport matplotlib.pyplot as plt\n\nprint(\"Visualisation\")\nCplorList = [\"red\", \"blue\", \"black\", \"orange\", \"green\", \"yellow\", \"cyan\",\"magenta\", \"lime\",\"violet\"]\nCurrentText = [i for i in range(1,11)]\nFig, Axis = plt.subplots(figsize=(25,10))\nplt.rc('xtick', labelsize=18)\nplt.rc('ytick', labelsize=18)\nplt.rc('axes', titlesize=18)\nplt.rc('axes', labelsize=18)\nplt.rc('legend', fontsize=18)\nplt.rc('figure', titlesize=30)\nplt.rc('font', size=18)\nfor indexI, i in enumerate(UniqueLables):\n    CurrentX = []\n    CurrentY = []\n\n    for indexJ, j in enumerate(Lables):\n        if i == j:\n            CurrentX.append(Result[indexJ,0])\n            CurrentY.append(Result[indexJ,1])\n\n    Axis.plot(CurrentX, CurrentY, marker=\"o\", linewidth=0, color=CplorList[indexI], label =\"Cluster \" + str(CurrentText[indexI]))\nAxis.grid()\nAxis.set_xlabel(\"Component 1\")\nAxis.set_ylabel(\"Component 2\")\nplt.title(\"PCE Decomposition\")\nAxis.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **The main idea is to build a vector representation of papers and clustering it. Assumption: tokens that are contextually specific to a domain will have a similar vector representation, which means that the cosine distance will be close. The T5 transformer neural network is used to extract the context of the cluster.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\ntokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\ndevice = torch.device(\"cpu\")\n\nfor idCluster, CurrentUniqueLable in enumerate(UniqueLables):\n  CurrentText = \"\"\n  Counter = 0\n  for id, CurrentLable in enumerate(Lables):\n    if CurrentLable == CurrentUniqueLable:\n      if Counter > 2:\n        break\n      CurrentText += Papers[id]\n      Counter += 1\n\n  preprocess_text = CurrentText.strip().replace(\"\\n\",\"\")\n  t5_prepared_Text = \"summarize: \" + preprocess_text\n  tokenized_text = tokenizer.encode(t5_prepared_Text,return_tensors=\"pt\").to(device)\n\n  summary_ids = model.generate(tokenized_text,\n                                    num_beams=4,\n                                    no_repeat_ngram_size=2,\n                                    min_length=30,\n                                    max_length=100,\n                                    early_stopping=True)\n\n  output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n  print(\"Description \" + str(idCluster) + \" Cluster\")\n  print(output)\n  print(\"\\n\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **As can be seen from the figure, the clusters are quite well separable. Apparently, the last cluster contains uninformative articles that can be attributed to \"garbage\"**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}