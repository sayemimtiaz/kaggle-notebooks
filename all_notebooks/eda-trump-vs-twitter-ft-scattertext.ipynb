{"cells":[{"metadata":{},"cell_type":"markdown","source":"<center><img src=https://www.thetimes.co.uk/imageserver/image/%2Fmethode%2Ftimes%2Fprod%2Fweb%2Fbin%2F31d3727c-d0ea-11e5-b413-ac87650795f1.jpg?crop=1500%2C844%2C0%2C78&resize=1180>Image Source: https://www.thetimes.co.uk</center>"},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nDonald Trump Jr, is arguably the most controversial leader that the world has seen in a very long time. But regardless of what his haters might say, he was undeniably the largest source of Internet memes, since the Harlem Shake. And for good reason, too! President Trump has been an avid user of Twitter.com over the years and his words have made the headlines more times than one can count.\n\nThanks to the creator of this dataset, we now have a chance to analyse some of his words over time, and visualize for ourselves, how his sentiments, ideals and opinions changed over time and the global reach and impact they made. \n\n"},{"metadata":{},"cell_type":"markdown","source":"\n| ![space-1.jpg](https://media2.giphy.com/media/26uf2JHNV0Tq3ugkE/source.gif) | \n|:--:| \n| **When I heard that Using Gifs in notebooks gets you more upvotes** |\n"},{"metadata":{},"cell_type":"markdown","source":"If this is your first time, here, please note that this is a work in progress. I want to develop my notebooks over time, adding something new and interesting each time, as and when I think of it. If you like my notbook, you can let me know by upvoting it! it not only helps keep me motivated but also serves as an indicator for what I did right and what could be improved. \n\nThanks for stopping by!"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# used for identifying errors in tweets\n!pip install pyspellchecker","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#imports \nimport re\nfrom tqdm.notebook import tqdm\nimport pandas as pd \nimport numpy as np\nfrom datetime import datetime\n\nfrom spellchecker import SpellChecker\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n\nimport plotly.offline as pyo \nimport plotly.express as px\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nfrom plotly.subplots import make_subplots\nimport seaborn as sns \nimport scattertext as st\nfrom IPython.display import IFrame\nfrom wordcloud import WordCloud, ImageColorGenerator\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nimport random ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# intializate our tools \nsns.set_style('darkgrid')\n\n# for sentiment analysis \nsia = SIA() \n\n# to identify misspelled words\nspell = SpellChecker() \n\n# to display plotly graphs \npyo.init_notebook_mode() \n\n# duh\ndf = pd.read_csv(\"/kaggle/input/trumps-legacy/Trumps Legcy.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing\n\nHere, we will first clean our data and make it easier to work with. We perform the following steps to clean our data: \n1. **Convert time Strings to python datetime objects** : This makes it easier to extract Year, month, date, hour etc. \n2. **Remove mentions** : The strings that begin with \"@\" since these do not contribute to the content of the tweet itself. However, we will store these separately. Maybe Trump tends to mentions some accounts more than others? May be interesting to check. \n3. **Remove Hashtags** : Hashtags are often changing over time so I will not be analysing them in this notebook. \n4. **Remove URLs** : Who cares about URLs anyway? \n5. **Remove Special Characters** : Keep only english strings\n6. **Remove Single characters** : We will only be looking at full words and not individual characters \n7. **Replace multiple Spaces** : Replace a string of spaces with a single space \n\nPlease note that this part of the code has been borrowed from some popular notebooks on twitter data and it not my own. "},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"data = df.copy()\ndata['original_text'] = df['text']\ndata['date'] = df.date.apply(lambda x: datetime.strptime(x, \"%m/%d/%Y %H:%M\"))\nrt_mask = data.text.apply(lambda x: \"RT @\" in x)\n\n# standard tweet preprocessing \ndata.text =data.text.str.lower()\n#Remove twitter handlers\ndata.text = data.text.apply(lambda x:re.sub('@[^\\s]+','',x))\n#remove hashtags\ndata.text = data.text.apply(lambda x:re.sub(r'\\B#\\S+','',x))\n# Remove URLS\ndata.text = data.text.apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n# Remove all the special characters\ndata.text = data.text.apply(lambda x:' '.join(re.findall(r'\\w+', x)))\n#remove all single characters\ndata.text = data.text.apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))\n# Substituting multiple spaces with single space\ndata.text = data.text.apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Extraction\n\n\nHere, we will use the cleaned data to extract some new columsn that will aid us in generating visualizations later in this notebook. Some of the features we will extract are: \n\n1. **Errors** : The errors in the sentence, identified by the `pyspellchecker` Library. \n2. **Counts** : The counts for the words, errors and characters in the tweet text \n3. **Time and Date** : This will make grouping the rows more convenient later \n4. **Sentiment** : Sentiment Values as predicted by the python VADER package. It assigns a compound score betweem -1 and 1 to each sentence. On my previous notebook [here](https://www.kaggle.com/pawanbhandarkar/training-a-sith-lord/comments) I found that generally, values between -0.05 and 0.35 correspond to a \"neutral\" sentiment, less than -0.05 correspong to a neutral sentiment and anything above 0.35 can be labeled positive. These are the thresholds we will use in this notebook as well. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def label_sentiment(x:float):\n    if x < -0.05 : return 'negative'\n    if x > 0.35 : return 'positive'\n    return 'neutral'\n\n# Feature Extraction\ndata['words'] = data.text.apply(lambda x:re.findall(r'\\w+', x ))\ndata['errors'] = data.words.apply(spell.unknown)\ndata['errors_count'] = data.errors.apply(len)\ndata['words_count'] = data.words.apply(len)\ndata['sentence_length'] = data.text.apply(len)\ndata['hour'] = data.date.apply(lambda x: x.hour)\ndata['date'] = data.date.apply(lambda x: x.date())\ndata['month'] = data.date.apply(lambda x: x.month)\ndata['year'] = data.date.apply(lambda x: x.year)\n\n\n# Extract Sentiment Values for each tweet \ndata['sentiment'] = [sia.polarity_scores(x)['compound'] for x in tqdm(data['text'])]\ndata['overall_sentiment'] = data['sentiment'].apply(label_sentiment);\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting the Data\n\nThe Datset contains two types of tweets: \n1. **Original Content (OC)** -Tweets sent by Trump's official accounts \n2. **Retweets (RT)** - Tweets that one or more of his affiliated accounts retweeted. \n\nWhile I am interested in diving into some of his retweets, I will start my analysis by considering only his Original tweets. I might add some analysis of his Retweets in a future version, depending on how well this notebook is recieved. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split into Retweets and Original Content \nrt_df = data[rt_mask]\noc_df = data[~rt_mask]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's quickly take a look at our data so far. "},{"metadata":{"trusted":true},"cell_type":"code","source":"oc_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Visualizations\n\n### Trump's Rise in Popularity\n\nWhile Donald Trump was pretty well known for being a Billionaire and occasionally making cameos on TV shows and movies, it was only around 2016, when he ran for office of 45th President of the United States and subsequently won, that his popularity skyrocketed. As the plots below suggest, It was during the months of January 2015, (the start of his campaign) that his twitter activity started to climb in terms of Likes and Retweets. \n\n**Technical note**: \nThere is a LOT of data for this plot, since a lot can happen in 5 years. Plotting a simple Scatter Plot would make it very difficult to interpret. So to make the graph more readable, I define a helper function to convert a list of numbers into a cumulative list of exponentially weighted averages, that capture the trend in the data. \n\nNo one explains it better than the OG of Machine Learning, [Dr. Andrew Ng himself. ](https://www.youtube.com/watch?v=lAq96T8FkTw&t=145s)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Helper Function to get the running average \ndef get_weighted(series: pd.Series, beta=0.9):\n    weighted = pd.Series(dtype=float)\n    weighted[series.index[0]] = 0 \n    for i in range(1, len(series)):\n        current = series.iloc[i]\n        previous = weighted.iloc[i-1]\n        date = series.index[i]\n        weighted[date] = beta*previous + (1-beta)*current\n    return weighted \n\n# Get a two-line title for our plots\ndef get_multi_line_title(title:str, subtitle:str):\n    return f\"{title}<br><sub>{subtitle}</sub><br>\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"title = get_multi_line_title(\n    \"Trump's Rise In Popularity\", \n    \"Plotting the avergae number of likes and rewteets over the years\"\n)\n\nbeta = 0.99 #higher value -> smoother curve\n\nlikes = oc_df.groupby('date')['favorites'].mean()\nlikes_std = likes.std()\nlikes = likes[likes < 3*likes_std]\n\nretweets = oc_df.groupby('date')['retweets'].mean()\nretweets_std = retweets.std()\nretweets = retweets[retweets < 3*retweets_std]\n\nweighted_retweets = get_weighted(retweets, beta)\nweighted_likes = get_weighted(likes, beta)\n\n\nfig = go.Figure([\n    go.Scatter(\n        name=\"Daily Average Likes\",\n        x=likes.index, \n        y=likes.values,\n        mode=\"markers\",\n        opacity=0.3,\n        marker_color=\"salmon\"\n    ), \n    go.Scatter(\n        name=\"Weighted Average Likes\",\n        x=weighted_likes.index, \n        y=weighted_likes.values,\n        opacity=0.8,\n        marker_color='crimson'\n    ),\n    go.Scatter(\n        name=\"Daily Average Retweets\",\n        x=retweets.index, \n        y=retweets.values,\n        mode=\"markers\",\n        opacity=0.3,\n        marker_color=\"lightseagreen\"\n        \n    ), \n    go.Scatter(\n        name=\"Weighted Average Retweets\",\n        x=weighted_retweets.index, \n        y=weighted_retweets.values,\n        opacity=0.8,\n        marker_color='darkgreen'\n    )\n])\n\nfig.update_layout(\n    hovermode='x',\n    title=title,\n    xaxis_title=\"Time\",\n    yaxis_title=\"Average Likes per Tweet\",\n    template=\"ggplot2\",\n    legend_orientation = 'h'\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### How tech-savvy is Donald Trump?\n\nWith the advant of the internet, a lot of social platforms including twitter have gained massive momentum in the last decade alone. With technology advancing at such an overwhelming pace, it is only expected that politicians too would have to \"get with the times\". In this section, let us try to visualize what devices Trump used over the years in order to communicate with the masses."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"devices = pd.DataFrame(oc_df.groupby(['year', 'device'])['text'].count()).sort_values('year').reset_index()\n\ntitle = get_multi_line_title(\n    \"Devices Over the Years\", \n    \"What devices has Trump used over the years?\"\n)\n\nunique_devices = devices.device.unique().tolist()\n\ndevices = devices[devices['year'] != 2021] \nline_plots = []\nfor d in unique_devices:\n    device_data = devices[devices.device == d]\n    line_plots.append(go.Scatter(\n        name = d,\n        x = device_data.year,\n        y=device_data.text,\n    ))\n    \nfig = go.Figure(line_plots)\nfig.update_layout(\n    title =title,\n    template=\"ggplot2\",\n    hovermode='x',\n    legend_orientation = 'h'\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### What do we learn from this? \n\nIt Looks like ever since the start of his campaign, Trump has been almost exclusivley using iPhones for his tweets. What could be the reason behind this? The ease of use? The iOS privacy features? I'll let you know once the Pentagon returns my calls. "},{"metadata":{"trusted":true},"cell_type":"code","source":"title = get_multi_line_title(\"Sentence Length Distribution\", \"Distirbution of number of characters per tweet, by top-5 devices\")\ndata = oc_df[oc_df['text'].apply(len) != 0]\ntop_devices = data.groupby('device')['text'].count().sort_values(ascending=False)[:5].index.tolist()\ndata = data[data['device'].apply(lambda x: x in top_devices)]\nfig = px.histogram(data, x=\"sentence_length\", color=\"device\", opacity=0.75)\nfig.update_layout(hovermode='x', title=title)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### This graph is more interesting than you might think\nIt looks like almost all of trumps more verbose tweets came from an Iphone as we can see a sharp increase in the total counts. But the real reason for this is that trump started to use an iphone in 2016, around the same time that [Twitter DOUBLED it's maximum character count](https://www.washingtonpost.com/news/the-switch/wp/2017/11/07/twitter-is-officially-doubling-the-character-limit-to-280/). So it makes sense that since then pretty much all the tweets would have been over 140 characters (previous limit) and closer to 280 characters (new limit)"},{"metadata":{},"cell_type":"markdown","source":"### How Active is Trump? \n\nNot much to say about this section except that's it's a simple visualization of the total number of tweets by Donald Trump each Year."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"title = get_multi_line_title(\n    \"Activity over the years\", \n    \"How many Tweets has trump sent out over the years?\")\n\nannual_counts = pd.DataFrame(oc_df['year'].value_counts()).reset_index()\nannual_counts.columns = ['year', 'count']\nannual_counts = annual_counts[annual_counts['year'] != 2021]\n\nfig = go.Figure(go.Bar(\n    name=\"Annual Count\", \n    x=annual_counts.year, \n    y=annual_counts['count'], \n    marker_color=annual_counts['count'] \n))\nfig.update_layout(template='ggplot2', title=title)\nfig.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### What time of the Day was Donald most active? \nThere's a popular meme on the internet which states that Donald Trump used to tweet while on the toilet. So to humour this meme, I would like to visualize the distribution of Trump's tweets across the 24 hours on average. This is where the DateTime Processing we did earlier comes handy!\n\nFor now, we will only visualize the time of day for the tweets made from iphones, since these are the dominating devices\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"title = get_multi_line_title(\"Time of the day most Tweeted\", \"\")\ndef format_hour(h: int):\n    h = str(h)\n    if len(h) == 1: \n        h = '0'+h\n    h = h+ \":00\"\n    return h\n\noc = oc_df[oc_df['device'] == 'Twitter for iPhone']\nhourly = oc.groupby('hour')['text'].count()\nhourly = pd.DataFrame(hourly).reset_index()\nhourly.columns =['Hour of Day',\"Number of Tweets\"]\nhourly['Hour of Day'] = hourly['Hour of Day'].apply(format_hour)\n\n\nfig = px.line_polar(\n    data_frame=hourly,\n    r = 'Number of Tweets',\n    theta='Hour of Day',\n    line_close=True,\n    color_discrete_sequence=['crimson'],\n)\n\nfig.update_layout(\n    title=title, \n    template=\"ggplot2\",\n    title_x=0.5)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### What do we understand from this? \n\nIt looks like most of Trump's tweets are increasingly likely to be sent towards the noon and we see the peak at the 12th hour, with tweets getting less frequent after that. \n\nNOTE: This is **NOT** the visualization Trump's tweets in a single day but of his tweets over the course of many years. It would interesting to plot a similar polar grapph for the frequency of tweets hour each hour than just the raw count. I will add this in a future version. "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### How Positive is Trump? \n\nBeing the leader of the free world is no small thing. Every day your thoughts and actions affect the lives of Hundreds of thousands, if not millions of people. Therefore, it is critical that his messages do not convey too much negativity too often. One way to measure this would to quantify the \"sentiment\" values for his tweets and then visualise and that's exactly what we aim to do in this section!\n\n**Technical Note**:\nThe Sentiment Analysis for this task is a clustering problem - one that can be solved using a variety of approaches, depending on the desired outcomes. I decided to use the popular VADER package from NLTK since it is quite simple to use and relatively straightforward. You can search for \"sentiment Analysis\" on Kaggle to get some good alternative solutions by talented Kagglers."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"title = get_multi_line_title(\n    'Sentiment Distribution',\n    \"How positive is the leader of the free world?\"\n)\n\nsentiment_pie = pd.DataFrame(oc_df['overall_sentiment'].value_counts() / oc_df.shape[0]*100).reset_index()\nsentiment_pie.columns = ['Sentiment', 'Percentage']\nfig = px.pie(sentiment_pie, values='Percentage', names='Sentiment', title=title)\n\nfig.update_layout(\ntitle=title, title_x=0.48)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note**: The colors are probably not the best for the sentiments and I will fix it later. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sentiment_over_time = oc_df.sort_values('date')[['year', 'sentiment', 'overall_sentiment']]\nsentiment_over_time = sentiment_over_time[sentiment_over_time.year !=2021]\nannual_sentiment = pd.DataFrame(sentiment_over_time.groupby('year')['overall_sentiment'].value_counts())\nannual_sentiment.columns = ['Count']\nannual_sentiment = annual_sentiment.reset_index()\n\ntitle = get_multi_line_title('Annual Tweet Sentiment', \"How Trump's sentiments changed over the years\")\nyears = annual_sentiment.year.unique().tolist()\nsents = {'positive' : 'mediumseagreen', 'negative': 'crimson', 'neutral': 'royalblue'}\n\n\nsentiment_bars = [] \nfor s in sents.keys():\n    current_year = annual_sentiment[annual_sentiment.overall_sentiment == s]\n    sentiment_bars.append(\n        go.Bar(name=s, x=current_year.year, y=current_year.Count, marker_color = sents[s])\n    )\n    \n    \nfig = go.Figure(sentiment_bars)\nfig.update_layout(template='ggplot2', title=title)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Wordclouds\n\nWhat were the most frequently used words, based on the sentiment of the tweet? "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def flatten_list(l):\n    return [x for y in l for x in y]\n\n# color coding our wordclouds \ndef red_color_func(word, font_size, position, orientation, random_state=None,**kwargs):\n    return f\"hsl(0, 100%, {random.randint(25, 75)}%)\" \n\ndef green_color_func(word, font_size, position, orientation, random_state=None,**kwargs):\n    return f\"hsl({random.randint(90, 150)}, 100%, 30%)\" \n\ndef yellow_color_func(word, font_size, position, orientation, random_state=None,**kwargs):\n    return f\"hsl(42, 100%, {random.randint(25, 50)}%)\" \n\ndef generate_word_clouds(neg_doc, neu_doc, pos_doc):\n    # Display the generated image:\n    fig, axes = plt.subplots(1,3, figsize=(20,10))\n    \n    \n    wordcloud_neg = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(\" \".join(neg_doc))\n    axes[0].imshow(wordcloud_neg.recolor(color_func=red_color_func, random_state=3), interpolation='bilinear')\n    axes[0].set_title(\"Negative Tweets\")\n    axes[0].axis(\"off\")\n\n    wordcloud_neu = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(\" \".join(neu_doc))\n    axes[1].imshow(wordcloud_neu.recolor(color_func=yellow_color_func, random_state=3), interpolation='bilinear')\n    axes[1].set_title(\"Neutral Words\")\n    axes[1].axis(\"off\")\n\n    wordcloud_pos = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(\" \".join(pos_doc))\n    axes[2].imshow(wordcloud_pos.recolor(color_func=green_color_func, random_state=3), interpolation='bilinear')\n    axes[2].set_title(\"Positive Words\")\n    axes[2].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show();\n\nsentiment_sorted= data.sort_values('favorites', ascending=False)\npositive_top_100 = sentiment_sorted[sentiment_sorted['overall_sentiment'] == \"positive\"].iloc[:100]\nnegative_top_100 = sentiment_sorted[sentiment_sorted['overall_sentiment'] == \"negative\"].iloc[:100]\nneutral_top_100 = sentiment_sorted[sentiment_sorted['overall_sentiment'] == \"neutral\"].iloc[:100]\n\ncleanup = lambda x: [y for y in x.split() if y not in stopwords.words('english')]\nneg_doc = flatten_list(negative_top_100['text'].apply(cleanup))\npos_doc = flatten_list(positive_top_100['text'].apply(cleanup))\nneu_doc = flatten_list(neutral_top_100['text'].apply(cleanup))\n\ngenerate_word_clouds(neg_doc, neu_doc, pos_doc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ScatterText\n\nNow we come to my favourite part of this notebook: **ScatterText**\n\n## What is it? \nScatterText as the name suggests, is a scatterplot for text data. But unlike regular old BORING scatter graphs, ScatterText is ridiculously intuitive and quite frankly, I'm shocked that not more people use this. I learned about it on a [medium post](https://jamesopacich.medium.com/interpreting-scattertext-a-seductive-tool-for-plotting-text-2e94e5824858) here and I know right away that I had to use it and showcase it to the world.\n\nWhile I highly recommend that you give the above article a read, I will quickly go over some important features of this tool: \n1. It is particularly well suited when you want to see how words are distributed betweem two categorical variables. In our case, we will consider the \"Negative\" and \"Non-Negative\" sentiment as the categorical classes.\n2. Words closer to the  axes are said to have higher \"precision\" with respect to each axis and the ones farther away are said to have more 'Recall'\n3. The words in the top right corner of the graph have high recall in both classes and generally represent stop words\n4. The word across the diagonal represent words common to both classes  \n5. The Search bar can be used to highlight text by index (in this case, we use \"Date\") \n6. The graph is saved as HTML and therefore we use IFrame to display it \n\nIf you want to get started, I recommend taking a look at the article above and also the github repo readme [here](https://github.com/JasonKessler/scattertext)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data = oc_df.copy()\ndata['binary_sentiment'] = data['overall_sentiment'].apply(lambda x: x if x ==\"negative\" else \"non-negative\")\ndata['date'] = data['date'].apply(str)\n\ndf = data.assign(\n    parse=lambda df: df.original_text.apply(st.whitespace_nlp_with_sentences)\n)\n\ncorpus = st.CorpusFromParsedDocuments(\n    df, category_col='binary_sentiment', parsed_col='parse'\n).build().get_unigram_corpus().compact(st.AssociationCompactor(2000))\n\nhtml = st.produce_scattertext_explorer(\n    corpus,\n    category='negative', category_name='Negative', not_category_name='Neutral/Positive',\n    minimum_term_frequency=0, pmi_threshold_coefficient=0,\n    width_in_pixels=1000, metadata=corpus.get_df()['date'],\n    transform=st.Scalers.dense_rank\n)\n\nopen('./demo_compact.html', 'w').write(html)\nIFrame(src='./demo_compact.html', width=1200, height=700)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### What do we understand from this? \n\nWe see from the above graph, that words like \"strong\", \"maga\" and \"golf\" generally appear in tweets that convey a more neutral or positive sentiment, while words like \"hoax\", \"illegal\" and \"fake\" have often been used to convey negative sentiments. Whereas words like \"job\", \"obama\" \"country\" are equally as likely to appear in texts with either sentiment. \n\nIntuitive, no? "},{"metadata":{},"cell_type":"markdown","source":"# Conclusion \n\nIn conclusion I would like to conclude by saying that the notebook is now concluded. "},{"metadata":{},"cell_type":"markdown","source":"# Summary\n\nWork Completed as of 07-02-2021:\n\n* **Data Cleaning** : Preprocessing and date time cleanup.\n* **Feature Extraction** : Creating some useful features for visualizations \n* **Trump's Rise in Popularity** : How did his twitter fandom increase over time?\n* **Devices Over the Years** : How tech-savvy is Donal Trump? \n* **Activity Over the Years** : Visualize the Annual total tweets \n* **Time of Day** : What time of the day is Trump most active? \n* **Sentiment Analysis** : How positive is the leader of the free world? \n* **Word Clouds** : Visualize frequent words by sentiment\n* **ScatterText** : A powerful tool for text data visualization \n\nThanks for taking the time to read this notebook. If you liked it, an **UPVOTE** is massively encouraging! I will try to keep this notebook updated and add in more visualizations in the future so be sure to check back soon!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}