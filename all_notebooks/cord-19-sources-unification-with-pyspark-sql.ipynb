{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"<h1><center><font size=\"6\">COVID-19</font></center></h1>\n\n<h2><center><font size=\"5\">CORD-19 Sources unification with pyspark SQL</font></center></h2>\n\n\n# <a id='0'>Content</a>\n\n- <a href='#1'>1. Introduction</a> \n- <a href='#2'>2. Metadata</a>\n- <a href='#3'>3. Data preprocessing and unification</a>\n- <a href='#4'>4. Conclusion</a>\n    \n    \n# <a id='1'>1. Introduction</a>\n\nThe objective of this kernel is simple: to produce a single, complete and clean covid.csv files containing all papers from the four different sources. The output size will be about 370 MB.\n\nIf you, just like me, love digging into the data, you do not need to go through this kernel. Just start a new kernels and import the cleaned file:\n\n    pd.read_csv(\"/kaggle/input/single-complete-flattened-text-based-dataset/clean_covid.csv\")\n    \nI will use pyspark SQL to first load all JSON data into a single spark Dataframe. Subsequently, by using query selectors, I will filter out only the column with only relevant text data. This will be the selected column (new columns may be added when time goes by):\n\nDisclaimer: this is a working in progress. Please, share your valuable opinions in the comments box!"},{"metadata":{},"cell_type":"markdown","source":"# <a id='2'>2. Metadata</a>\n\nHere, we load the data and transform three columns to permits to easily work with columns in future.\n\n- `source_x` becomes `source`\n- `Microsoft Academic Paper ID` becomes `mic_id`\n- `WHO #Covidence` becomes `who_covidence`\n\nLet's visualize it:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"!pip install pyspark\n\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(color_codes=True)\n\nimport glob\nimport json\n\nfrom pathlib import Path\n\nroot_path = Path('/kaggle/input/CORD-19-research-challenge/2020-03-13')\nmetadata_path = root_path / Path('all_sources_metadata_2020-03-13.csv')\n\nmetadata = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\n\nmetadata.rename(columns={'source_x': 'source', 'Microsoft Academic Paper ID': 'mic_id', 'WHO #Covidence': 'who_covidence'}, inplace=True)\n\nprint(\"There are \", len(metadata), \" sources in the metadata file.\")\n\nmetadata.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"The dataset `metadata` contains 14 columns. Just by reading the different names and looking at the values we can already have a good understanding of the underline data."},{"metadata":{},"cell_type":"markdown","source":"# <a id='3'>3. Data preprocessing and unification</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_json = glob.glob( str(root_path) + '/**/*.json', recursive=True)\nprint(\"There are \", len(all_json), \"sources files.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark import SparkContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\n\nspark = (\n    SparkSession.builder.appName(\"covid\")\n    .master(\"local[*]\")\n    .config(\"spark.driver.memory\", \"16g\")\n    .config(\"spark.executor.memory\", \"16g\")\n    .config(\"spark.driver.maxResultSize\", \"4g\")\n    .getOrCreate()\n)\n\ndata = spark.read.json(all_json, multiLine=True)\ndata.createOrReplaceTempView(\"data\")\n\n#data.printSchema()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select text columns\ncovid_sql = spark.sql(\n        \"\"\"\n        SELECT\n            metadata.title AS title,\n            abstract.text AS abstract, \n            body_text.text AS body_text,\n            back_matter.text AS back_matter,\n            paper_id\n        FROM data\n        \"\"\")\n\n# Convert it to pandas and join all texts\ncovid_pd = covid_sql.toPandas()\ncovid_pd['abstract'] = covid_pd['abstract'].str.join(' ')\ncovid_pd['body_text'] = covid_pd['body_text'].str.join(' ')\ncovid_pd['back_matter'] = covid_pd['back_matter'].str.join(' ')\n\ncovid_pd.head()\ncovid_pd.to_csv('clean_covid.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='4'>4. Conclusion</a>\n\nIn the near future, I will also add into the database the authors as well as the bibliography columns. Feel free to leave a comment if you have questions or advices.\n\nThank you for reading, and ... stay healthy!\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}