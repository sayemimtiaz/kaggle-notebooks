{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a id=\"top\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#005097; border:0' role=\"tab\" aria-controls=\"home\"><center>Marketing Campaign Analysis </center></h1>"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np\nfrom numpy import isnan\nimport pandas as pd\nfrom sklearn.impute import KNNImputer\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nimport plotly.graph_objs as go\nimport plotly.express as px\nfrom scipy.stats import shapiro\nfrom scipy.stats import chi2_contingency\nfrom scipy.stats import chi2\nimport scipy.stats as stats\nfrom numpy import median\nfrom numpy import std\nfrom IPython.display import Image\nimport os\n!ls ../input/\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndata_folder = \"/kaggle/input/arketing-campaign/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"Image(\"/kaggle/input/marketing-campaign-image/What_is_the_Most_Important_Data_for_your_Marketing_Campaign-2695514.jpg\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Table of Contents\n\n* [Data Preprocessing](#section_1)\n    * [Data types and data completeness](#section_1_1)\n    * [Feature engineering](#section_1_2)\n    * [Statistical summary](#section_1_3)\n    * [Handling outliers](#section_1_4)\n    * [Handling missing values](#section_1_5)\n    \n    ___\n* [Visual Exploratory Data Analysis](#section_2)\n    * [Diploma distribution by income level](#section_2_1)\n    * [Average income by diploma](#section_2_2)\n    * [Spending by Income](#section_2_3)\n    * [Diploma distribution by marital situation](#section_2_4)\n    * [Income level by parental status](#section_2_5)\n    \n    ---\n* [Statistical Hypothesis Testing](#section_3) \n    * [Normality tests](#section_3_1)\n        * [Graphical Method : Histogram plot](#section_3_1_1)\n        * [Statistical Method : Shapiro test](#section_3_1_2)\n    * [Rank Significance Tests](#section_3_2)\n        * [Numerical variables : Mann-Whitney's test](#section_3_2_1)\n    * [Rank Correlation Tests](#section_3_3)\n        * [Numerical variables : Spearman test](#section_3_3_1)\n        * [Categorical variables : Chi-square test](#section_3_3_2)\n        * [Numerical & Categorical variables : Point-Biserial Correlation test](#section_3_3_3)\n        \n    ---"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"dataset=pd.read_csv(data_folder+'marketing_campaign.csv',header=0,sep=';') \ndataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Data Preprocessing <a class=\"anchor\" id=\"section_1\"></a>"},{"metadata":{},"cell_type":"markdown","source":"### A. Data types and data completeness <a class=\"anchor\" id=\"section_1_1\"></a>"},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"print(dataset.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the __*info function*__, we can pull the following information from our dataset :\n>- We have __3 categorical variables__ and __26 numerical variables__\n>- We have __missing values__ for the __*Income*__ variable"},{"metadata":{},"cell_type":"markdown","source":"### B. Feature Engineering <a class=\"anchor\" id=\"section_1_2\"></a>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"dataset.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Having a first look at the row data enables us to start thinking at some useful variables we could create in order to better understand our dataset and reveal precious information.  \n\nWe wrill create several variables :\n\n>- Variable __*Age*__ in replacement of the variable *Year_birth*\n>- Variable __*Spending*__ as the sum of the amount spent on the 6 product categories\n>- Variable __*Marital_Situation*__ to group the different marital status in only 2 comprehensive categories : In couple vs Alone\n>- Variable __*Has_child*__ as a binary variable equal to Yes if the customer has 1 child or more\n>- Variable __*Educationnal_years*__ as the total number of years of education the individual achieved according to its diploma\n\nWe will remove the unused variables for this analysis"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"dataset['Age']=2014-dataset['Year_Birth']\ndataset['Spending']=dataset['MntWines']+dataset['MntFruits']+dataset['MntMeatProducts']+dataset['MntFishProducts']+dataset['MntSweetProducts']+dataset['MntGoldProds']\ndataset['Marital_Situation']=dataset['Marital_Status'].replace({'Divorced':'Alone','Single':'Alone','Married':'In couple','Together':'In couple','Absurd':'Alone','Widow':'Alone','YOLO':'Alone'})\ndataset['Has_child'] = np.where(dataset.Kidhome+dataset.Teenhome > 0, 'Yes', 'No')\ndataset['Educational_years']=dataset['Education'].replace({'Basic':5,'2n Cycle':8,'Graduation':12,'Master':18,'PhD':21})\ndataset=dataset[['Age','Income','Spending','Marital_Situation','Has_child','Education','Educational_years']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### C. Statistical summary <a class=\"anchor\" id=\"section_1_3\"></a>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"pd.options.display.float_format = \"{:.2f}\".format\ndataset.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The __*describe function*__ generates for us the 5-Number summary, particularly useful as a first step in our preliminary investigation. Analyzing the statistical summary gives us insightful information in one look : \n> - Average income is __52247 dollars__ while median income is  __51300 dollars__. The distribution is right skewed with the possible presence of outliers\n     - The maximum value being equal to __666666 dollars__ and the 3rd quartile being only equal to __68522 dollars__ reinforce this hypothesis    \n> - Average spending in the last 2 years is __600 dollars__ while median spending is __396 dollars__  \n> - Average age is __45 years old__ and the oldest customer is 121 years old which is a pretty (and beautiful) rare event  \n> - Average number of years of education is __14.4 years__ which corresponds to a Bachelor degree"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"df = pd.DataFrame(data=dataset, columns=['Age','Income','Spending','Educational_years'])\n#Permet de tracer les courbes de distribution de toutes les variables\nnd = pd.melt(df, value_vars =df )\nn1 = sns.FacetGrid (nd, col='variable', col_wrap=4, sharex=False, sharey = False)\nn1 = n1.map(sns.distplot, 'value')\nn1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting the distribution is a complementary work to do when analyzing the statistical summary. It provides a visual help to better appropriate the dataset"},{"metadata":{},"cell_type":"markdown","source":"### D. Handling outliers <a class=\"anchor\" id=\"section_1_4\"></a>"},{"metadata":{},"cell_type":"markdown","source":"An outlier is an observation that differs significantly from other values.  \nOutliers can be detected using several methods such as statistical methods or graphical methods.\nWe will use the Box-Plot graphical method which enables us to vizualize the range of our data and plot the outliers. By using this technique, we first calculate the Interquartile Range (IQR) defined as follow :\n$$IQR= Q_3-Q_1$$\nwhere :\n- $Q_1$ is the first quartile\n- $Q3$ is the third quartile  \n\n> - Any value greater (lower) 1.5 times the IQR above (below) the third quartile (the first quartile) is defined as a __mild outlier__  \n>- Any value greater (lower) 3 times the IQR above (below) the third quartile (the first quartile) is defined as an __extreme outlier__"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df = dataset[['Age','Income','Spending','Educational_years']]\n\nfig = px.box(df.melt(), y=\"value\", facet_col=\"variable\",facet_col_wrap=2, boxmode=\"overlay\", color=\"variable\",height=1000, width=900)\nfig.update_yaxes(matches=None)\n\nfor i in range(len(fig[\"data\"])):\n    yaxis_name = 'yaxis' if i == 0 else f'yaxis{i + 1}'\n    fig.layout[yaxis_name].showticklabels = True\n\nfig.update_layout(showlegend=False)\nfig.update_xaxes(showline=True, linewidth=2, linecolor='grey')\nfig.update_yaxes(showline=True, linewidth=2, linecolor='grey')\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Outliers analysis must be done with care. From above we can see that several variables seem to have outliers :\n>- __*Age*__: 3 customers older than the Upper Fence set at 74 years old. We will not remove them\n>- __*Income*__ : Several value are greater than the Upper Fence of 113K. While having an income of 150k is not impossible, we will remove the customer who has an income of 666k (Moreover, this observation is defined as an extreme outlier based on our definition stated previously) \n>- __*Spending*__ : There is only one outlier which is at the limit of the Upper Fence. We will not remove it"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#We remove the only outlier in our dataset before handling missing values\ndataset = dataset.drop(dataset[dataset['Income']> 600000].index).reset_index(drop=True)\ndataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### E. Handling missing values <a class=\"anchor\" id=\"section_1_5\"></a>"},{"metadata":{},"cell_type":"markdown","source":"As seen earlier, the *Income* variable has 24 missing values  \nThere are several ways to handle null-values :\n\n- We can delete the entire column containing null-values\n- We can delete the rows containing null-values\n- We can impute the mean value\n- We can input the mean value of a specific population : in this case we would split by Education diploma \n- We can use a model to predict missing values \n    \nWith our dataset, we will go for the last option and use the K-Nearest Neighbor Imputation.  \nKNN Imputation works by __imputing the average income__ of the __k nearest neighbors__ found in the training set for each of the missing value.  \nWe will use *Education*, *Age* and *Income* to run the algorithm. KNNimputer will automatically normalize our data."},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer = KNNImputer()\nimputer = KNNImputer(n_neighbors=5,metric='nan_euclidean')\n# fit on the dataset\nimputer.fit(dataset[['Income','Age','Educational_years']])\n# transform the dataset\nX = imputer.transform(dataset[['Income','Age','Educational_years']])\nIncome_impute=pd.DataFrame(X,columns=['Income','Age','Educational_years'])\ndataset['Income']=Income_impute['Income'].reset_index(drop=True)\ncount_nan = len(dataset) - dataset.count()\nprint(count_nan)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We dont have any missing value anymore"},{"metadata":{},"cell_type":"markdown","source":"# 2. Visual Exploratory Data Analysis <a class=\"anchor\" id=\"section_2\"></a>"},{"metadata":{},"cell_type":"markdown","source":"* <h3>Diploma distribution by income level <a class=\"anchor\" id=\"section_2_1\"></a>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = px.histogram(dataset, x=\"Income\",color=\"Education\", nbins=10,histnorm='percent',barnorm='fraction',barmode='relative')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">- __Lower income__ are mainly represented by __Basic diploma owners__ and __2n Cycle owners__\n    - Income between 0 and 20k are represented at __68%__ by Basic diploma owners and at __14%__ by 2n Cycle diploma owners  \n>      \n>      \n>- __Higher income__ are mainly represented by __Master__ and __Phd owners__\n    - Income between 140k and 160K are represented at __48%__ by PhD and at __31%__ by Master owners"},{"metadata":{},"cell_type":"markdown","source":"* <h3>Average income by diploma <a class=\"anchor\" id=\"section_2_2\"></a>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df = dataset[['Income','Education']]\ncategory_orders={\"Education\":[\"Basic\",\"2n Cycle\",\"Graduation\",\"Master\",\"PhD\"]}\nfig = px.histogram(df, x=\"Education\",y=\"Income\",  histfunc='avg',category_orders=category_orders)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">- Average income is the highest for __PhD owners__ with __56161 dollars__  \n>- Average income is the lowest for __Basic diploma owners__ with __20306 dollars__  \n>- The better the diploma is, the higher the average salary  \n\nWe will verify later with a statistical test if the average salary of PhD owners is statistically different from Master owners ([go to section 3_2_1](#section_3_2_1))"},{"metadata":{},"cell_type":"markdown","source":"* <h3>Spending by Income<a class=\"anchor\" id=\"section_2_3\"></a>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"reg = LinearRegression().fit(np.vstack(dataset['Spending']), dataset['Income'])\ndf['bestfit'] = reg.predict(np.vstack(dataset['Spending']))\n\nfig = go.Figure(data=go.Scatter(name='observations',x=dataset['Spending'], y=dataset['Income'],mode='markers'))\nfig.add_trace(go.Scatter(name='line of best fit', x=dataset['Spending'], y=df['bestfit'], mode='lines'))\nfig.update_traces(hovertemplate='Spending: %{x} <br>Income: %{y}')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">- Spending seems to be __positively correlated__ with the income level\n\nWe will verify later with a statistical test if the correlation between the annual income and the amount of spending is statistically significant    \n([go to section 3_3_1](#section_3_3_1))"},{"metadata":{},"cell_type":"markdown","source":"* <h3>Diploma distribution by marital situation <a class=\"anchor\" id=\"section_2_4\"></a>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df = dataset[['Education','Marital_Situation']]\n\nfig = px.sunburst(df, path=['Marital_Situation','Education'],color_discrete_sequence=px.colors.diverging.Spectral)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">- The distribution of diploma category owned seems to be __identical__ for the two population _In couple_ and _Alone_ \n\nWe could be tempted to believe that there is __no correlation__ between the diploma owned and the marital status  \nWe will verify later if this hypothesis is true ([go to section 3_3_2](#section_3_3_2))"},{"metadata":{},"cell_type":"markdown","source":"* <h3>Income level by parental status <a class=\"anchor\" id=\"section_2_5\"></a>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Creating 4 quartiles to segment Income\ncut_labels_Income = ['Low income', 'low-Medium income', 'Medium-high income', 'High income']\ndataset['Income_bins'] = pd.qcut(dataset['Income'], q=4,labels=cut_labels_Income)\n\ndf = dataset[['Income_bins','Has_child']]\n\nfig = px.sunburst(df, path=['Has_child','Income_bins'],color_discrete_sequence=px.colors.diverging.Portland)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">- People with __high income__ are largely representing the population who has __no child__\n>- People having __at least 1 child__ are mainly represented by people with __low income__  \n\nWe created here 4 segments to cluster individuals by their income level based on quartiles\nWe could be tempted to believe that having a high income tend to not have a child.  \nWe will verify later if there is a correlation between the income and the fact to have at least one child ([go to section 3_3_3](#section_3_3_3))"},{"metadata":{},"cell_type":"markdown","source":"# 3. Statistical Hypothesis Testing <a class=\"anchor\" id=\"section_3\"></a>"},{"metadata":{},"cell_type":"markdown","source":"The data vizualization step enabled us to better understand our dataset and helped us formulate questions about the data. Statistical hypothesis testing enables us to provide confidence or likelihood about the answers.  \nEach time we will be testing a hypothesis, we will present our analysis in the following plan :\n1. Hypothesis statement\n2. Analysis plan formulation\n3. Analyze sample data\n4. Interpret the results"},{"metadata":{},"cell_type":"markdown","source":"### A. Normality tests <a class=\"anchor\" id=\"section_3_1\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Before running any hypothesis test, it's important to know which statistical method we should use.\nStatstical methods are divided in __two parts__ :  \n   - Parametric statistical methods\n   - Nonparametric statistical methods\n    \nTo know which one to use, normality tests must be done on our data. If our data have a known and specific distribution, such as the Gaussian distribution; parametric statistical methods must be used. On the contrary, if data are not Gaussian, nonparametric statistical methods should be used.\n\nThere are two main ways to know if our data are Gaussian :\n   - __Graphical methods__\n       - Histogram plot\n       - QQ plot\n   - __Statistical methods__\n       - Shapiro test\n       - D'Agostino and Pearson test\n       - Anderson-Darling test\n       - Kolmogorov-Smirnov test\n       \n__Graphical methods__ are mainly used for __qualifying__ deviations from normality  \n__Statistical methods__ are mainly used for __quantifying__ deviations from normality"},{"metadata":{},"cell_type":"markdown","source":"### a. Graphical Method<a class=\"anchor\" id=\"section_3_1_1\"></a>\n### Histogram plot"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(data=dataset, columns=['Age','Income','Spending','Educational_years'])\nnd = pd.melt(df, value_vars =df )\nn1 = sns.FacetGrid (nd, col='variable', col_wrap=4, sharex=False, sharey = False)\nn1 = n1.map(sns.distplot, 'value')\nn1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the graph, we can immediately see which variables seem to be Gaussian or Gaussian-like :\n>- Age and Income have __Gaussian-like distributions__\n>- Spending has a __Log-normal distribution__\n>- Educationnal_years has a __Multinomial distribution__\n\nWe can verify with a statistical method that none of our variables is Gaussian"},{"metadata":{},"cell_type":"markdown","source":"### b. Statistical Method<a class=\"anchor\" id=\"section_3_1_2\"></a>"},{"metadata":{},"cell_type":"markdown","source":"There are several tests to verify if a variable is Gaussian or not.\nEach of them will return two metrics :\n- Statstic value : metric used to calculate the p-value\n- p-value : metric used to interpret the test\n\nFor each of the tests, the conclusion mecanic is the same :\n- if p-value $ \t\\le \\alpha$ : We __reject__ the null hypothesis and conclude of a non Gaussian distribution\n- if p-value $>\\alpha$ : We __fail to reject__ the null hypothesis and conclude of a Gaussian distribution\n\nWith $\\alpha$ being the significance level"},{"metadata":{},"cell_type":"markdown","source":"### Shapiro test"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"X=['Age','Income','Spending','Educational_years']\n\ncolumn_dict= {elem : pd.DataFrame() for elem in X}\n\ndef shapiro_test(data):\n    stat, p = shapiro(data)\n    print('%s : Statistics=%.3f, p=%.3f' % (column,stat, p))\n    alpha = 0.05\n    if p > alpha:\n        print('Sample looks Gaussian (We fail to reject H0)')\n    else:\n        print('Sample does not look Gaussian (We reject H0)')\n\nfor column in X:\n    column_dict[column] = dataset[column]\n    shapiro_test(column_dict[column])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">All the p-values are __inferior__ to 0.05 :  \nWe __reject__ the null-hypothesis. Our variables are __not Gaussian__ at a 5% significance level\n\nFrom this point we have two options :\n1. Normalizing our data to use parametric statistical methods\n2. Using directly nonparametric statistical methods\n\nWe will go for the second option and use nonparametric statistical methods to test our hypotheses"},{"metadata":{},"cell_type":"markdown","source":"### B. Rank Significance Tests <a class=\"anchor\" id=\"section_3_2\"></a>"},{"metadata":{},"cell_type":"markdown","source":"### a. Numerical variables : Mann-Whitney's test<a class=\"anchor\" id=\"section_3_2_1\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Our first question was to find if the average income of PhD owners is statistically different from the average income of Master owners.\n\n#### 1. Hypothesis statement  \n* __$H_0$__ : The mean ranks of the two groups are equal  \n* __$H_a$__ : The mean ranks of the two groups are not equal\n\n#### 2. Analysis plan formulation  \n* __Significance level :__ We will test our hypothesis at a 5% significance level  \n  \n  \n* __Test method :__ We use the Mann-Whitney's test to determine whether one group has higher or lower income than the other group. Mann-Whitney U test is a nonparametric statistical significance test for determining whether two independent samples were drawn from a population with the same distribution. The default assumption or null hypothesis is that there is no difference between the distributions of the data samples. Rejection of this hypothesis suggests that there is likely some difference between the samples. More specifically, the test determines whether it is equally likely that any randomly selected observation from one sample will be greater or less than a sample in the other distribution. If violated, it suggests differing distributions. \nTherefore, if our assumption is correct, the result of the test should enable us to reject the null hypothesis."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Creation of the samples\nDiploma=dataset[['Education','Income']]\n\nPhd_graduate=Diploma[Diploma['Education']=='PhD']\nMaster_graduate=Diploma[Diploma['Education']=='Master']\nBasic_graduate=Diploma[Diploma['Education']=='Basic']\nSecond_cycle_graduate=Diploma[Diploma['Education']=='2n Cycle']\nGraduation_graduate=Diploma[Diploma['Education']=='Graduation']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3. Analyze Sample Data\n\n#####  Mann-Whitney U statistic  : \n\n$$U=min(U_1,U_2) \\quad  with \\quad  U_1 = n_1n_2+\\frac { n_1(n_1+1)}{2}-R_1 \\quad  and \\quad   U_2 = n_1n_2+\\frac { n_2(n_2+1)}{2}-R_2 $$\nwhere :  \n$R_1$ = Sum of the rank in group 1   \n$R_2$ = Sum of the rank in group 2\n\n#####  Z-score  (if n>20): \n\n$$z = \\frac {U-\\frac {n_1n_2}{2}}{\\sqrt{\\frac {n_1n_2(N+1)}{12}}}$$\nwhere :  \n$N$ = $n_1$ + $n_2$  \n\nNB: the two sample distributions should not be Gaussian. We will verify this requirement"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# normality tests of our two samples\nstat, p = shapiro(Phd_graduate.Income)\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n# interpret\nalpha = 0.05\nif p > alpha:\n\tprint('Sample looks Gaussian (fail to reject H0)')\nelse:\n\tprint('Sample does not look Gaussian (reject H0)')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"stat, p = shapiro(Master_graduate.Income)\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n# interpret\nalpha = 0.05\nif p > alpha:\n\tprint('Sample looks Gaussian (fail to reject H0)')\nelse:\n\tprint('Sample does not look Gaussian (reject H0)')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# summarize\nprint('PhD: median = %.0f stdv = %.1f' % (median(Phd_graduate.Income), std(Phd_graduate.Income)))\nprint('Master: median = %.0f stdv = %.1f' % (median(Master_graduate.Income), std(Master_graduate.Income)))\n\nprint(stats.mannwhitneyu(Phd_graduate.Income, Master_graduate.Income))\n\nif p > alpha:\n\tprint('Means are not statistically different (We fail to reject H0)')\nelse:\n\tprint('Means are statistically different (We reject H0)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4. Interpret the results\n\nThe p-value is __inferior__ than the significance level of 5%, we can __reject the null hypothesis__.<br>    \nWe can conclude that the average income of PhD owners is different from the average income of Master owners at a 95% confidence level"},{"metadata":{},"cell_type":"markdown","source":"### C. Rank Correlation Tests <a class=\"anchor\" id=\"section_3_3\"></a>"},{"metadata":{},"cell_type":"markdown","source":"### a. Numerical variables : Spearman Rank Correlation test<a class=\"anchor\" id=\"section_3_3_1\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Our second question was to find if there is a statistically significant correlation between the income and the spending amount.\n\n#### 1. Hypothesis statement  \n* __$H_0$__ : There is no monotonic association between income and spending amount  \n* __$H_a$__ : There is a monotonic association between income and spending amount\n\n#### 2. Analysis plan formulation  \n* __Significance level__ : We will test our hypothesis at 5% significance level  \n* __Test method__ : We use the Spearman rank correlation test to determine if our two variables are correlated. This statistical method quantifies the degree to which ranked variables are associated by a monotonic function, meaning an increasing or decreasing relationship. As a statistical hypothesis test, the method assumes that the samples are uncorrelated (fail to reject H0).  \nTherefore, if our assumption is correct, the result of the test should enable us to reject the null hypothesis."},{"metadata":{},"cell_type":"markdown","source":"#### 3.Analyze Sample Data\n\n#####  Correlation coefficient  : \n\n$$\\rho = 1- {\\frac {6 \\sum d_i^2}{n(n^2 - 1)}}$$\n\nwhere :  \n$d$ = the pairwise distances of the ranks of the variables $x_i$ and $y_i$  \n$n$ = the number of samples"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"Spending=dataset[['Spending','Income']]\n\ncor, pval = stats.spearmanr(Spending[['Spending']], Spending[['Income']])\nprint(\"Non-Parametric Spearman correlation test : correlation coefficient : %.4f, pval: %.4f\" % (cor, pval))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4. Interpret the results\nThe p-value close to 0 confirms that we can __reject the null hypothesis__ that the two variables are uncorrelated.\nWe can conclude that the correlation between the income and the spending amount is __statistically significant__ at a 95% confidence level.  \nThe high correlation coefficient induces a __strong positive relationship__ between the two variables, confirming our hypothesis."},{"metadata":{},"cell_type":"markdown","source":"### b. Categorical variables : Chi-square test for independance<a class=\"anchor\" id=\"section_3_3_2\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Our third question was to find if there is a statistically significant correlation between the diploma and the marital situation.\n\n\n#### 1. Hypothesis statement\n__$H_0$__ : Education and Marital_Situation are independant  \n__$H_a$__ : Education and Marital_Situation are not independant\n\n#### 2. Analysis plan formulation  \n* __Significance level__ : We will test our hypothesis at a 5% significance level  \n* __Test method__ : We use the Chi-square test for independence to determine whether there is a significant relationship between our two categorical variables."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Marital_Situation=dataset[['Education','Marital_Situation']]\ncrosstab = pd.crosstab(Marital_Situation[\"Education\"], Marital_Situation[\"Marital_Situation\"])\ncrosstab","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.Analyze Sample Data\n__Degrees of freedom :__ \n$$DF = (r-1)*(c-1)$$  \nwhere $r$ is the number of levels for one catagorical variable, and $c$ is the number of levels for the other categorical variable."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"stat, p, dof, expected = chi2_contingency(crosstab)\nprint('Degress of freedom = %d' % dof)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Expected frequencies :__  \nThe expected frequency counts are computed separately for each level of one categorical variable at each level of the other categorical variable.  \nWe compute $r * c$ expected frequencies, according to the following formula :  \n$$E_{r,c} = \\frac{(n_r * n_c)}{n}$$  \nWhere :\n- $E_{r,c}$ is the expected frequency count for level $r$ of Variable $A$ and level $c$ of Variable $B$ \n- $n_r$ is the total number of sample observations at level $r$ of Variable $A$\n- $n_c$ is the total number of sample observations at level $c$ of Variable $B$\n- $n$ is the total sample size"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(expected)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Test statistic :__  \nThe test statistic is a chi-square random variable ($\\chi^2$) defined by the following equation:   \n$$\\chi^2=\\sum_{} \\frac{(O_{r,c} - E_{r,c})^2}{E_{r,c}}$$\nWhere :\n- $O_{r,c}$ is the observed frequency count at level $r$ of Variable $A$ and level $c$ of Variable $B$\n- $E_{r,c}$ is the expected frequency count at level $r$ of Variable $A$ and level $c$ of Variable $B$"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"prob = 0.95\ncritical = chi2.ppf(prob, dof)\nprint('probability=%.3f, critical=%.3f, stat=%.3f' % (prob, critical, stat))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__P-value :__  \nThe P-value is the probability of observing a sample statistic as extreme as the test statistic. Since the test statistic is a Chi-square, we should use use the Chi-Square table to assess the probability associated with the test statistic by using the degrees of freedom we found above."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"if abs(stat) >= critical:\n\tprint('Dependent (We reject H0)')\nelse:\n\tprint('Independent (We fail to reject H0)')\nalpha = 1.0 - prob\nprint('significance=%.3f, p=%.3f' % (alpha, p))\nif p <= alpha:\n\tprint('Dependent (We reject H0)')\nelse:\n\tprint('Independent (We fail to reject H0)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4. Interpret the results\nWe can interpret the results by two ways :\n- The P-value is superior than the significance level of 5%, we __fail to reject the null hypothesis__.     \n- The Test-statistic is inferior than the critical value, we __fail to reject the null hypothesis__.     \n\n\nWe can conclude that the marital status is __independant__ from the diploma owned at a 95% confidence level"},{"metadata":{},"cell_type":"markdown","source":"### c. Numerical & Categorical variables : Point-Biserial Correlation test<a class=\"anchor\" id=\"section_3_3_3\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Our fourth question was to find if there is a correlation between the income and the fact to have at least one child.\n\n\n#### 1. Hypothesis statement\n* __$H_0$__ : Education and Has_child are independant  \n* __$H_a$__ : Education and Has_child are not independant\n\n#### 2. Analysis plan formulation  \n* __Significance level__ : We will test our hypothesis at 5% significance level  \n* __Test method__ : As our categorical variable is a binary variable and its categories don't have a natural ordering (it doesn't matter if *Yes* is coded 1 or 0), we can transform it in a continuous variable and use a special case of the Pearson Correlation named Point-Biserial Correlation.<br> Point-Biserial Correlation assumes continuous data within each group created by the binary variable to be Gaussian (Here the income)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"dataset['Has_child_bool'] = np.where(dataset.Has_child =='Yes', 1, 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3. Analyze Sample Data\n\n__Correlation coefficient  :__    \n$$r_{xy} =\\frac{M_1 - M_0}{S_n}\\sqrt{xy}$$\nWhere :  \n    $M_1$ = Mean of the group that received the positive binary variable (i.e. the “1”)  \n    $M_0$ = Mean of the group that received the negative binary variable (i.e. the “0”)  \n    $S_n$ = Standard deviation for the entire test  \n    $x$ = Proportion of cases in the “0” group  \n    $y$ = Proportion of cases in the “1” group  \n    \n__t-statistic   :__   \n$$t_{xy} =\\frac{r_{xy}\\sqrt{n-2}}{\\sqrt{1-r_{xy}^2}}$$\n\n\nDegree of Freedom = $n$-2"},{"metadata":{},"cell_type":"markdown","source":"First step we have to __normalize__ *Income* variable. For this we will use the __Box-Cox transformation__\n\n`Nota Bene : Here we use the continuous variable Income and not the categorical variable Income_bins created earlier as it's nearly always a bad idea to categorize continuous variable to test independance `"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Income_child=dataset[['Has_child_bool','Income']]\n\nfrom sklearn.preprocessing import PowerTransformer\npt = PowerTransformer(method='box-cox', standardize=True) \nrescaler = pt.fit_transform(Income_child[['Income']])\n\nIncome_child[['Income']]=rescaler","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"nd = pd.melt(Income_child, value_vars =Income_child[['Income']] )\nn1 = sns.FacetGrid (nd, col='variable')\nn1 = n1.map(sns.distplot, 'value')\nn1","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# D'Agostino and Pearson's Test\nfrom scipy.stats import normaltest\n\n# normality test\nstat, p = normaltest(Income_child[['Income']])\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n# interpret\nalpha = 0.05\nif p > alpha:\n\tprint('Sample looks Gaussian (fail to reject H0)')\nelse:\n\tprint('Sample does not look Gaussian (reject H0)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Income_child['Income'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">Both drawing our normalized distribution and running the d'Agostino and Pearson's test confirm us our _Income_   variable is __Gaussian__. The variable has a 0 mean and a standard deviation of 1.  \nWe can now run the Point-Biserial Correlation test"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(stats.pointbiserialr(Income_child.Has_child_bool, Income_child.Income))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4. Interpret the results\nThe p-value close to 0 confirms that we can __reject the null hypothesis__ that the two variables are uncorrelated.\nWe can conclude that the correlation between the income and the binary variable Has_child is __statistically significant__ at a 95% confidence level.  \nThe negative correlation coefficient confirms our thought on the fact that there is a __moderate negative relationship__ between the income and the fact to have at least one child"},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"This dataset is very interesting to master data preprocessing tasks such as handling null-values, detecting outliers, and feature creation. The demographic variables are an inspiring source for expressing a multitude of  statistical hypotheses. Next objectives will be the implementation of clustering algorithm and customer personnas modeling based on the demographic and behavioural data we have."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}