{"cells":[{"metadata":{},"cell_type":"markdown","source":"# PySpark DataFrame Preprocessing for CORD-19\n\nSQL is a useful tool for querying data. [Apache Spark](https://spark.apache.org/) is a framework that allows for map-reduce workloads with a SQL-interface through the `pyspark.sql` module. The data provided by CORD-19 is semi-structured and contains many nested fields that can be tricky to work with. \n\nThis notebook contains starter code for pre-processing the raw JSON documents into a structured and strongly-typed [Spark DataFrame](https://spark.apache.org/docs/latest/sql-programming-guide.html) that can be queried using Spark SQL. I'll provide a cell that can be used as the starting point for exploration into the dataset. I'll also provide a few example queries for interacting with nested data.\n\n\n### Handy References\n\n* [`spark.sql` module documentation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html)\n* [Databricks `select` documentation on lateral view](https://docs.databricks.com/spark/latest/spark-sql/language-manual/select.html#lateral-view)\n* [Spark Data Types reference](https://spark.apache.org/docs/latest/sql-reference.html)\n\n\n### Notes on the environment\n\nTo being, make sure the notebook has access to the internet. If you are running any Spark code locally, I suggest setting the `SPARK_HOME` variable so it is pointing to the local python site packages.\n\n```bash\n# in bash or zsh on MacOS or Linux\nSPARK_HOME=$(python -c \"import pyspark; print(pyspark.__path__[0])\")\n\n# in powershell on Windows\n$env:SPARK_HOME = $(python -c \"import pyspark; print(pyspark.__path__[0])\")\n```"},{"metadata":{},"cell_type":"markdown","source":"# Starter Code\n\nSpark can be installed via the Python package manager, `pip`."},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"! pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql.functions import lit\nfrom pyspark.sql.types import (\n    ArrayType,\n    IntegerType,\n    MapType,\n    StringType,\n    StructField,\n    StructType,\n)\n\n\ndef generate_cord19_schema():\n    \"\"\"Generate a Spark schema based on the semi-textual description of CORD-19 Dataset.\n\n    This captures most of the structure from the crawled documents, and has been\n    tested with the 2020-03-13 dump provided by the CORD-19 Kaggle competition.\n    The schema is available at [1], and is also provided in a copy of the\n    challenge dataset.\n\n    One improvement that could be made to the original schema is to write it as\n    JSON schema, which could be used to validate the structure of the dumps. I\n    also noticed that the schema incorrectly nests fields that appear after the\n    `metadata` section e.g. `abstract`.\n    \n    [1] https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-03-13/json_schema.txt\n    \"\"\"\n\n    # shared by `metadata.authors` and `bib_entries.[].authors`\n    author_fields = [\n        StructField(\"first\", StringType()),\n        StructField(\"middle\", ArrayType(StringType())),\n        StructField(\"last\", StringType()),\n        StructField(\"suffix\", StringType()),\n    ]\n\n    authors_schema = ArrayType(\n        StructType(\n            author_fields\n            + [\n                # Uncomment to cast field into a JSON string. This field is not\n                # well-specified in the source.\n                StructField(\n                    \"affiliation\",\n                    StructType(\n                        [\n                            StructField(\"laboratory\", StringType()),\n                            StructField(\"institution\", StringType()),\n                            StructField(\n                                \"location\",\n                                StructType(\n                                    [\n                                        StructField(\"settlement\", StringType()),\n                                        StructField(\"country\", StringType()),\n                                    ]\n                                ),\n                            ),\n                        ]\n                    ),\n                ),\n                StructField(\"email\", StringType()),\n            ]\n        )\n    )\n\n    # used in `section_schema` for citations, references, and equations\n    spans_schema = ArrayType(\n        StructType(\n            [\n                # character indices of inline citations\n                StructField(\"start\", IntegerType()),\n                StructField(\"end\", IntegerType()),\n                StructField(\"text\", StringType()),\n                StructField(\"ref_id\", StringType()),\n            ]\n        )\n    )\n\n    # A section of the paper, which includes the abstract, body, and back matter.\n    section_schema = ArrayType(\n        StructType(\n            [\n                StructField(\"text\", StringType()),\n                StructField(\"cite_spans\", spans_schema),\n                StructField(\"ref_spans\", spans_schema),\n                # While equations don't appear in the abstract, but appear here\n                # for consistency\n                StructField(\"eq_spans\", spans_schema),\n                StructField(\"section\", StringType()),\n            ]\n        )\n    )\n\n    bib_schema = MapType(\n        StringType(),\n        StructType(\n            [\n                StructField(\"ref_id\", StringType()),\n                StructField(\"title\", StringType()),\n                StructField(\"authors\", ArrayType(StructType(author_fields))),\n                StructField(\"year\", IntegerType()),\n                StructField(\"venue\", StringType()),\n                StructField(\"volume\", StringType()),\n                StructField(\"issn\", StringType()),\n                StructField(\"pages\", StringType()),\n                StructField(\n                    \"other_ids\",\n                    StructType([StructField(\"DOI\", ArrayType(StringType()))]),\n                ),\n            ]\n        ),\n        True,\n    )\n\n    # Can be one of table or figure captions\n    ref_schema = MapType(\n        StringType(),\n        StructType(\n            [\n                StructField(\"text\", StringType()),\n                # Likely equation spans, not included in source schema, but\n                # appears in JSON\n                StructField(\"latex\", StringType()),\n                StructField(\"type\", StringType()),\n            ]\n        ),\n    )\n\n    return StructType(\n        [\n            StructField(\"paper_id\", StringType()),\n            StructField(\n                \"metadata\",\n                StructType(\n                    [\n                        StructField(\"title\", StringType()),\n                        StructField(\"authors\", authors_schema),\n                    ]\n                ),\n                True,\n            ),\n            StructField(\"abstract\", section_schema),\n            StructField(\"body_text\", section_schema),\n            StructField(\"bib_entries\", bib_schema),\n            StructField(\"ref_entries\", ref_schema),\n            StructField(\"back_matter\", section_schema),\n        ]\n    )\n\n\ndef extract_dataframe_kaggle(spark):\n    \"\"\"Extract a structured DataFrame from the semi-structured document dump.\n\n    It should be fairly straightforward to modify this once there are new\n    documents available. The date of availability (`crawl_date`) and `source`\n    are available as metadata.\n    \"\"\"\n    base = \"/kaggle/input/CORD-19-research-challenge\"\n    crawled_date = \"2020-03-13\"\n    sources = [\n        \"noncomm_use_subset\",\n        \"comm_use_subset\",\n        \"biorxiv_medrxiv\",\n        \"pmc_custom_license\",\n    ]\n\n    dataframe = None\n    for source in sources:\n        path = f\"{base}/{crawled_date}/{source}/{source}\"\n        df = (\n            spark.read.json(path, schema=generate_cord19_schema(), multiLine=True)\n            .withColumn(\"crawled_date\", lit(crawled_date))\n            .withColumn(\"source\", lit(source))\n        )\n        if not dataframe:\n            dataframe = df\n        else:\n            dataframe = dataframe.union(df)\n    return dataframe\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Example Usage and Exploration\n\nNow that we've defined the helper functions, lets start to take a look at the data. First we define a new `SparkSession`, which will create or reuse an existing session. [By default](https://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts), this will utilize all cores and `total_memory - 1GB` of memory.\n\n## Extracting the Data\n\nTake note of the schema, which is heavy nested and repeated."},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\n\nspark = SparkSession.builder.getOrCreate()\ndf = extract_dataframe_kaggle(spark)\ndf.printSchema()\n\ndf.createOrReplaceTempView(\"cord19\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we register the DataFrame as a temporary table so we can run SQL. Caching can also help significantly, if there is enough memory available.\n\n## DataFrame API vs Spark SQL\n\nThese APIs are interchangable, since there is a query planner that figures out the best way to accomplish the query. Having a declarative API is helpful before you dump the data in the flattened form that suits your application.\n\nI will be showing off both the Spark DataFrame interface which can be used programmatically and the SQL interface which can be adapted for use on BigQuery.\n\n#### Group By: How many papers are there in each source?\n\nOne example of a source is `biorxiv`."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Using the Spark DataFrame interface...\")\ndf.groupBy(\"source\").agg(F.countDistinct(\"paper_id\")).show()\n\nprint(\"Using the Spark SQL interface...\")\nquery = \"\"\"\nSELECT\n    source,\n    COUNT(DISTINCT paper_id)\nFROM\n    cord19\nGROUP BY\n    source\n\"\"\"\nspark.sql(query).show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Flatten: Who has written the most papers?\n\nHere, lets take a look at our first nested field. Each paper can have many authors. \n\nThe `COLUMN.*` notation will extract all the columns from a struct into the scope of the `SELECT` clause."},{"metadata":{"trusted":true},"cell_type":"code","source":"authors = df.select(\"paper_id\", F.explode(\"metadata.authors\").alias(\"author\")).select(\"paper_id\", \"author.*\")\nauthors.select(\"first\", \"middle\", \"last\", \"email\").where(\"email <> ''\").show(n=5)\nauthors.printSchema()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now count the number of distinct papers for each author."},{"metadata":{"trusted":true},"cell_type":"code","source":"(\n    authors.groupBy(\"first\", \"middle\", \"last\")\n    .agg(F.countDistinct(\"paper_id\").alias(\"n_papers\"))\n    .orderBy(F.desc(\"n_papers\"))\n).show(n=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like the [German virologist Christian Drosten](https://en.wikipedia.org/wiki/Christian_Drosten) has quite a bit to say on the matter. There also seems to be a few data quality issues, since there are quite a few papers authored by \"â€ \".\n\nWe can also express the same query, but in Spark-flavored SQL. The `LATERAL VIEW` will be used throughout with this DataFrame for unnesting."},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"\"\"\nWITH authors AS (\n    SELECT\n        paper_id,\n        author.*\n    FROM\n        cord19\n    LATERAL VIEW\n        explode(metadata.authors) AS author\n)\nSELECT\n    first,\n    last,\n    COUNT(DISTINCT paper_id) as n_papers\nFROM\n    authors\nGROUP BY\n    first,\n    last\nORDER BY\n    n_papers DESC\n\"\"\"\n\nspark.sql(query).show(n=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Array Aggregate: Generating full abstracts\n\nOne last useful trick for handling nested fields are [`array` aggregate functions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.array). We'll take a look at `pyspark.sql.functions.array_join` for generating full abstracts.\n\nThe first way involves exploding the DataFrame with the array position, and then concatenating all of the rows belonging to particular paper. This can be translated directly into SQL. The second way involves the use of User Defined Functions, which can work on data row at a time."},{"metadata":{"trusted":true},"cell_type":"code","source":"# based on https://stackoverflow.com/a/50668635\nfrom pyspark.sql import Window\n\nabstract = (\n    df.select(\"paper_id\", F.posexplode(\"abstract\").alias(\"pos\", \"value\"))\n    .select(\"paper_id\", \"pos\", \"value.text\")\n    .withColumn(\"ordered_text\", F.collect_list(\"text\").over(Window.partitionBy(\"paper_id\").orderBy(\"pos\")))\n    .groupBy(\"paper_id\")\n    .agg(F.max(\"ordered_text\").alias(\"sentences\"))\n    .select(\"paper_id\", F.array_join(\"sentences\", \" \").alias(\"abstract\"))\n    .withColumn(\"words\", F.size(F.split(\"abstract\", \"\\s+\")))\n)\n\nabstract.show(n=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you're curious how this is represented under the hood, you can take a look at the query planner. The performance is not too bad in return for ergonomics. "},{"metadata":{"trusted":true},"cell_type":"code","source":"abstract.explain()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now for the SQL analogue. This may be a bit convoluted, but if you're following along, you should be ready for any sort of data processing."},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"\"\"\nWITH abstract AS (\n    SELECT\n        paper_id,\n        pos,\n        value.text as text\n    FROM\n        cord19\n    LATERAL VIEW\n        posexplode(abstract) AS pos, value\n),\ncollected AS (\n    SELECT\n        paper_id,\n        collect_list(text) OVER (PARTITION BY paper_id ORDER BY pos) as sentences\n    FROM\n        abstract\n),\nsentences AS (\n    SELECT\n        paper_id,\n        max(sentences) as sentences\n    FROM\n        collected\n    GROUP BY\n        paper_id\n)\nSELECT\n    paper_id,\n    array_join(sentences, \" \") as abstract,\n    -- make sure the regex is being escaped properly\n    size(split(array_join(sentences, \" \"), \"\\\\\\s+\")) as words\nFROM\n    sentences\n\"\"\"\n\nspark.sql(query).show(n=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we can use a User Defined Function written in Python. This is versatile, and is similar to a `pandas.Dataframe.apply`"},{"metadata":{"trusted":true},"cell_type":"code","source":"@F.udf(\"string\")\ndef join_abstract(rows) -> str:\n    return \" \".join([row.text for row in rows])\n\n(\n    df.select(\"paper_id\", join_abstract(\"abstract\").alias(\"abstract\"))\n    .where(\"abstract <> ''\")\n    # mix and match SQL using `pyspark.sql.functions.expr` or `DataFrame.selectExpr`\n    .withColumn(\"words\", F.expr(\"size(split(abstract, '\\\\\\s+'))\"))\n).show(n=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can also be registered to use in SQL."},{"metadata":{"trusted":true},"cell_type":"code","source":"spark.udf.register(\"join_abstract\", join_abstract)\n\nquery = \"\"\"\nSELECT\n    paper_id,\n    join_abstract(abstract) as abstract,\n    size(split(join_abstract(abstract), '\\\\\\s+')) as words\nFROM\n    cord19\nWHERE\n    size(abstract) > 1\n\"\"\"\n\nspark.sql(query).show(n=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# What next?\n\nTake this notebook, cut all of the extra cells out, and begin processing your data for text mining. Spark has an excellent [feature-extraction](https://spark.apache.org/docs/latest/ml-features) library that can be used to transform data in all sorts of ways. For example, the extracted abstracts from above can be tokenized and turned into weighted term-frequency vectors for similarity searches.\n\nHopefully I'll be able to follow up with some interesting analysis involving recommendation systems. I'm particularly interested in the literature around the non-pharmaceudical  interventions, and I hope to curate a sensible list of the approaches that people around the world have taken to combat COVID-19. \n\nIf you need help with anything related to Spark or SQL related to this notebook, feel free to reach out. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}