{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is a very small dataset, but I decided to try to do some computer vision anyway.\n\nThe goal of the model is to take in a grayscale image of a pokemon it has never seen before and then correctly color. It will be fun to see how close it gets to the right colors. Since I only have one image of every pokemon, you cannot expect it to be too accurate. \n\nI did a bit of data fusion and used the pokemon type data (fire,grass,etc) to improve the model slightly (about 12% in MSE). This is my first model fusing text and image data and my first Kaggle kernel.\n\nI do not believe these are the best architectures to color images, but I chose it because it is an interesting architecture inspired by some papers I have read and I wanted to explore it.\n\n\nThere are three models. 1) A simple u-net, 2) A u-net that has a dense network feed the pokemon type into the bottle neck layer, and 3) A u-net with an autoencoder on the pokemon type data. I built the third model as a method of forcing the second model to be sure and use the type information.\n\nChoose the model you want to run by changing the variable modelnum in the first code box."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfrom PIL import Image\nimgs = []\ngray_imgs = []\ngray_imgs3chan = []\none_hot_list = []\n\n# Choose the model you want to run\nmodelnum = 2 # 1 for U-Net, 2 for U-Net with dense branch attached, 3 for U-Net with autoencoder attached\n\n# Read in the csv file which contains pokemon types\ndf = pd.read_csv('/kaggle/input/pokemon-images-and-types/pokemon.csv')\n\n# One hot encode the pokemon's primary type\none_hot = pd.get_dummies(df['Type1'])\n\n# Go through add grab every image and save them as numpy arrays in a list\nfor dirname, _, filenames in os.walk('/kaggle/input/pokemon-images-and-types/images/'):\n    for filename in filenames:\n        # split the file name string to find what type of pokemon the image contains\n        pokemonname = filename.split('/')[-1].split('.')[0]\n        rownum = df.loc[df['Name'] == pokemonname].index[0]\n        encoded =np.array(one_hot.iloc[rownum,:]).astype(np.float32)\n        one_hot_list.append(encoded)\n        \n        # Open the image with PIL and save it as grayscale color and grayscale but in RGB format\n        img = Image.open(os.path.join(dirname, filename)).convert('RGB') # Some are RGBA so we convert them\n        gray_img = img.convert('L') # Save a gray scale version too\n        imgs.append(np.array(img))\n        gray_imgs.append(np.array(gray_img))\n        gray_imgs3chan.append(np.array(gray_img.convert('RGB'))) # This is a 3 channel version of the gray scale image.\n\n# Convert lists to numpy arrays for tensorflow processing\none_hot_list = np.array(one_hot_list)\nimgs = np.array(imgs)\ngray_imgs = np.array(gray_imgs)\ngray_imgs = np.expand_dims(gray_imgs, axis=-1)\ngray_imgs3chan = np.array(gray_imgs3chan)\nprint(imgs.shape, gray_imgs.shape, one_hot_list.shape)\n\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I just used these commands for data exploration. I haven't used much Pandas before\ndf = pd.read_csv('/kaggle/input/pokemon-images-and-types/pokemon.csv')\ndf.head()\ndf.Type2.value_counts()\ndf.Type2.isna().sum()\n#len(df.Type1.value_counts()) # there are 18 different pokemon types\ndf.head()\n\n\ndf.loc[df['Name'] == 'charizard'].Type1.iloc[0] == 'Fire'\n#df['Name'].where(df['Name'] == 'charizard')\none_hot = pd.get_dummies(df['Type1'])\nnp.array(one_hot.iloc[2,:]).astype(np.float32)\nprint(\"ignore this\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"Let's visualize some of the data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(10,2) \nf.subplots_adjust(0,0,3,3)\nfor i in range(0,10,1):\n    ax[i,1].imshow(gray_imgs[i,:,:,0], cmap=plt.get_cmap('gray'))\n    ax[i,0].imshow(imgs[i,:,:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# break the data into a training and testing partition\n\n\ntestdatasize = 30\ntrain_gray = gray_imgs[:-testdatasize]/255\ntest_gray = gray_imgs[-testdatasize:]/255\ntrain_color = imgs[:-testdatasize]/255\ntest_color = imgs[-testdatasize:]/255\ngray_imgs3chan = gray_imgs3chan[:-testdatasize]/255 # I can use this for pretraining\ntrain_oh = one_hot_list[:-testdatasize]\ntest_oh = one_hot_list[-testdatasize:]\n\nprint(train_gray.shape, train_color.shape, train_oh.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data augmentation by rotation (not currently using this)\ndef D4aug(arr):\n    r1 = np.rot90(arr,k=1,axes=(1,2))\n    r2 = np.rot90(arr,k=2,axes=(1,2))\n    r3 = np.rot90(arr,k=3,axes=(1,2))\n    return np.concatenate((arr,r1,r2,r3),axis=0)\n\n#train_gray = D4aug(train_gray)\n#train_color = D4aug(train_color)\n#gray_imgs3chan = D4aug(gray_imgs3chan)\n#print(train_gray.shape,gray_imgs3chan.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import tensorflow \nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Concatenate, Conv2DTranspose, SpatialDropout2D, Dense, Add, Flatten","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the something like the U-Net Model\ndef unet():\n    X = tf.keras.Input(shape=(120,120,1))\n    l1 = Conv2D(64, (3,3), padding='same', activation='relu')(X)\n    l2 = Conv2D(64, (3,3), padding='same', activation='relu')(l1)\n    \n    MP1 = MaxPooling2D((2,2),strides=(2,2))(l2)\n    MP1 = SpatialDropout2D(.1)(MP1)\n    \n    l3 = Conv2D(128, (3,3), padding='same', activation='relu')(MP1)\n    l4 = Conv2D(128, (3,3), padding='same', activation='relu')(l3)\n    MP2 = MaxPooling2D((2,2),strides=(2,2))(l4)\n    MP2 = SpatialDropout2D(.2)(MP2)\n    \n    l5 = Conv2D(128, (3,3), padding='same', activation='relu')(MP2)\n    l6 = Conv2D(128, (3,3), padding='same', activation='relu')(l5)\n    MP3 = MaxPooling2D((2,2),strides=(2,2))(l6)\n    MP3 = SpatialDropout2D(.2)(MP3)\n    \n    bn1 = Conv2D(256, (3,3), padding='same', activation='relu')(MP3)\n    bn2 = Conv2D(256, (3,3), padding='same', activation='relu')(bn1)\n    bn2 = SpatialDropout2D(.2)(bn2)\n    \n    \n    \n    u1 = Conv2DTranspose(64,(3,3),strides=(2,2), padding='same', activation='relu')(bn2)\n    conc1 = Concatenate()([u1,l6])\n    c1 = Conv2D(3,(3,3),padding='same', activation='relu')(conc1)\n    \n    u2 = Conv2DTranspose(64,(3,3),strides=(2,2), padding='same', activation='relu')(conc1)\n    conc2 = Concatenate()([u2,l4])\n    c2 = Conv2D(3,(3,3),padding='same', activation='relu')(conc2)\n    \n    u3 = Conv2DTranspose(64,(3,3),strides=(2,2), padding='same', activation='relu')(conc2)\n    conc3 = Concatenate()([u3,l2])\n    #conc3 = SpatialDropout2D(.15)(conc3)\n    c3 = Conv2D(3,(3,3),padding='same', activation='sigmoid')(conc3)\n    \n    model = tf.keras.Model(X,c3)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# You can use this callback if you want\n# if you don't let the model overfit then the pokemon are not very colorful, which is sad\nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', min_delta=0, patience=15, verbose=0, mode='auto',\n    baseline=None, restore_best_weights=True)\n# Below is code to pretrain. You are simply training the model to output the same grayscale image\n# I did not observe any benefit from pretraining, which is what you would expect when using ReLU activations\n# and skip connection. It is easy to learn the identity map\n# I tried pretraining because the dataset is so small\n# Un-comment the next line to pretrain\n#model.fit(train_gray,gray_imgs3chan,40,20,validation_split=.05,callbacks=[es])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def unet_with_type():\n    \n    type_in = tf.keras.Input(shape=(18,))\n    d1 = Dense(2**9, activation='relu')(type_in)\n    d2 = Dense(2**8, activation='relu')(d1)\n    \n    d2 = Dense(15*15,activation='relu')(d2)\n    d2 = tf.keras.layers.Reshape((15,15,1))(d2)\n    \n    \n    # Below is the unet\n    X = tf.keras.Input(shape=(120,120,1))\n    l1 = Conv2D(64, (3,3), padding='same', activation='relu')(X)\n    l2 = Conv2D(64, (3,3), padding='same', activation='relu')(l1)\n    \n    MP1 = MaxPooling2D((2,2),strides=(2,2))(l2)\n    MP1 = SpatialDropout2D(.1)(MP1)\n    \n    l3 = Conv2D(128, (3,3), padding='same', activation='relu')(MP1)\n    l4 = Conv2D(128, (3,3), padding='same', activation='relu')(l3)\n    MP2 = MaxPooling2D((2,2),strides=(2,2))(l4)\n    MP2 = SpatialDropout2D(.2)(MP2)\n    \n    l5 = Conv2D(128, (3,3), padding='same', activation='relu')(MP2)\n    l6 = Conv2D(128, (3,3), padding='same', activation='relu')(l5)\n    MP3 = MaxPooling2D((2,2),strides=(2,2))(l6)\n    MP3 = SpatialDropout2D(.2)(MP3)\n    \n    bn1 = Conv2D(256, (3,3), padding='same', activation='relu')(MP3)\n    bn1 = Add()([bn1,d2])\n    bn2 = Conv2D(256, (3,3), padding='same', activation='relu')(bn1)\n    bn2 = SpatialDropout2D(.2)(bn2)\n    \n    \n    \n    u1 = Conv2DTranspose(64,(3,3),strides=(2,2), padding='same', activation='relu')(bn2)\n    conc1 = Concatenate()([u1,l6])\n    c1 = Conv2D(3,(3,3),padding='same', activation='relu')(conc1)\n    \n    u2 = Conv2DTranspose(64,(3,3),strides=(2,2), padding='same', activation='relu')(conc1)\n    conc2 = Concatenate()([u2,l4])\n    c2 = Conv2D(3,(3,3),padding='same', activation='relu')(conc2)\n    \n    u3 = Conv2DTranspose(64,(3,3),strides=(2,2), padding='same', activation='relu')(conc2)\n    conc3 = Concatenate()([u3,l2])\n    #conc3 = SpatialDropout2D(.15)(conc3)\n    c3 = Conv2D(3,(3,3),padding='same', activation='sigmoid')(conc3)\n    \n    model = tf.keras.Model([X,type_in],c3)\n    return model\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def auto_with_unet():\n    type_in = tf.keras.Input(shape=(18,))\n    d1 = Dense(2**9, activation='relu')(type_in)\n    d2 = Dense(2**8, activation='relu')(d1)\n    \n    d2 = Dense(15*15,activation='relu')(d2)\n    d2 = tf.keras.layers.Reshape((15,15,1))(d2)\n    \n    \n    # Below is the unet\n    X = tf.keras.Input(shape=(120,120,1))\n    l1 = Conv2D(64, (3,3), padding='same', activation='relu')(X)\n    l2 = Conv2D(64, (3,3), padding='same', activation='relu')(l1)\n    \n    MP1 = MaxPooling2D((2,2),strides=(2,2))(l2)\n    MP1 = SpatialDropout2D(.1)(MP1)\n    \n    l3 = Conv2D(128, (3,3), padding='same', activation='relu')(MP1)\n    l4 = Conv2D(128, (3,3), padding='same', activation='relu')(l3)\n    MP2 = MaxPooling2D((2,2),strides=(2,2))(l4)\n    MP2 = SpatialDropout2D(.2)(MP2)\n    \n    l5 = Conv2D(128, (3,3), padding='same', activation='relu')(MP2)\n    l6 = Conv2D(128, (3,3), padding='same', activation='relu')(l5)\n    MP3 = MaxPooling2D((2,2),strides=(2,2))(l6)\n    MP3 = SpatialDropout2D(.2)(MP3)\n    \n    bn1 = Conv2D(256, (3,3), padding='same', activation='relu')(MP3)\n    bn1 = Add()([bn1,d2])\n    bn2 = Conv2D(256, (3,3), padding='same', activation='relu')(bn1)\n    bn2 = SpatialDropout2D(.2)(bn2)\n    \n    \n    \n    u1 = Conv2DTranspose(64,(3,3),strides=(2,2), padding='same', activation='relu')(bn2)\n    conc1 = Concatenate()([u1,l6])\n    c1 = Conv2D(3,(3,3),padding='same', activation='relu')(conc1)\n    \n    u2 = Conv2DTranspose(64,(3,3),strides=(2,2), padding='same', activation='relu')(conc1)\n    conc2 = Concatenate()([u2,l4])\n    c2 = Conv2D(3,(3,3),padding='same', activation='relu')(conc2)\n    \n    u3 = Conv2DTranspose(64,(3,3),strides=(2,2), padding='same', activation='relu')(conc2)\n    conc3 = Concatenate()([u3,l2])\n    #conc3 = SpatialDropout2D(.15)(conc3)\n    c3 = Conv2D(3,(3,3),padding='same', activation='sigmoid')(conc3)\n    \n    # output of autoencoder\n    e0 = Flatten()(bn2)\n    e1 = Dense(15*15,activation='relu')(e0)\n    e2 = Dense(2**8,activation='relu')(e1)\n    e3 = Dense(2**9,activation='relu')(e2)\n    eout = Dense(18,activation='sigmoid')(e3)\n    model = tf.keras.Model([X,type_in],[c3,eout])\n    return model\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"myadam = tf.keras.optimizers.Adam(learning_rate=0.001/3.0, beta_1=0.9, beta_2=0.999, amsgrad=False)\nif modelnum == 1:\n    model = unet()\n    model.compile('adam', loss='MSE') # for training unet and auto_with_unet\nelif modelnum == 2:\n    model = unet_with_type()\n    model.compile('adam', loss='MSE') # for training unet and auto_with_unet\nelse:\n    model = auto_with_unet()\n    model.compile(myadam,loss=['MSE','categorical_crossentropy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train with types\nif modelnum == 2:\n    model.fit([train_gray,train_oh],train_color,10,150,validation_split=.05,callbacks=None)\n# train unet\nelif modelnum == 1:\n    model.fit(train_gray, train_color,10,150,validation_split=.05,callbacks=None)\nelse:\n    model.fit([train_gray,train_oh],[train_color,train_oh],10,150,validation_split=.05,callbacks=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following table contains the MSE of the runs\n\n|Run Num|U-Net|U-Net with type  | U-Net with AE|\n|---|---|---|---|\n|1|0.0027  |0.002419|0.002484|\n|2|0.002521|0.002371|0.002468|\n|3|0.002801|0.002517|0.002581|\n|4|0.003058|0.002242|0.002399|\n|---|---|---|---|\n|Avg|.00277|.002387|.002483|\n\nSo we got about a 12% improvement by including the type information in prediction over the standard U-Net.\n\nThe model with the lowest MSE in a single run is U-Net with the dense input branch with an MSE of 0.002242 "},{"metadata":{"trusted":true},"cell_type":"code","source":"# examine results when trained with types\nif modelnum == 2:\n    print('MSE: ',model.evaluate([test_gray,test_oh],test_color))\n    preds = model.predict([test_gray,test_oh])\n\n# examine results when trained w/o types\nelif modelnum == 1:\n    print(\"MSE loss: \", model.evaluate(test_gray,test_color))\n    preds = model.predict(test_gray)\n\n# examine results of auto_with_unet\nelse:\n    print(model.evaluate([test_gray,test_oh],[test_color,test_oh]))\n    preds, garb = model.predict([test_gray,test_oh])\nnumimgs = 10\nf,ax = plt.subplots(numimgs,2,figsize=[6.4*2,4.8*2]) \nf.subplots_adjust(0,0,2,2)\nfor i in range(0,numimgs,1):\n    ax[i,0].imshow(Image.fromarray( (preds[i]*255).astype(np.uint8)))\n    ax[i,1].imshow(Image.fromarray( (test_color[i]*255).astype(np.uint8)))\n    \nx = [axi.set_axis_off() for axi in ax.ravel()]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}