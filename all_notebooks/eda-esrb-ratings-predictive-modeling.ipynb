{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction:\n\nIn this Notebook we tend to perform an Expolatory data Analysis on ESRB Ratings.\nAnd will develope an essemble model for Predicting these ratings. Before developing the ensemble model we will also be training various sklearn classifiers and will see their performance on the dataset as well.\n\n# What is ESRB and what are the various Categories?\nESRB stands for Entertainment Software Rating Board. It is an American self-regulatory organization for assigning \ncontent ratings to consumer Video games. It was established in 1994 in response to criticism of contoversial video\ngames like mortal Kombat \"Fatality, hmm...\". ESRB has the following labels:\n* RP => Ratings Pending (1994-present) This symbol is used in promotional materials for games which have not yet been assigned a final rating by the ESRB.\n* EC => Early Childhood (1994-2018) Games with this rating contain content which is aimed towards a preschool audience. They do not contain content that parents would find objectionable to this audience.No longer used as of 2018 due to few titles using this, and all titles with this rating are replaced with the E rating.\n* E => Everyone (1994-present) Games with this rating contain content which the ESRB believes is \"generally suitable for all ages\".They can contain content such as infrequent use of \"mild\"/cartoon violence and mild language.This rating was known as Kids to Adults (K-A) until 1998, when it was renamed \"Everyone\".\n* E10+ => Everyone 10+ (2005-present) Games with this rating contain content which the ESRB believes is generally suitable for those aged 10 years and older. They can contain content with an impact higher than the \"Everyone\" rating can accommodate, but still not as high as to warrant a \"Teen\" rating, such as a larger amount of violence, mild language, crude humor, or suggestive content.\n* T => Teen (1994-present) Games with this rating contain content which the ESRB believes is generally suitable for those aged 13 years and older; they can contain content such as moderate amounts of violence (including small amounts of blood), mild to moderate use of language or suggestive themes, sexual content, partial nudity and crude humor.\n* M17+ => Mature 17+ (1994-present) Games with this rating contain content which the ESRB believes is generally suitable for those aged 17 years and older; they can contain content with an impact higher than the \"Teen\" rating can accommodate, such as intense and/or realistic portrayals of violence (including blood, gore, mutilation, and depictions of death), strong sexual themes and content, nudity, and more frequent use of strong language.\n* A => Adults (1994-present) Games with this rating contain content which the ESRB believes is only suitable for those aged 18 years and older; they contain content with an impact higher than the \"Mature\" rating can accommodate, such as graphic sexual themes and content, extreme portrayals of violence, or unsimulated gambling with real currency. The majority of AO-rated titles are pornographic adult video games; the ESRB has seldom issued the AO rating solely for violence.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's import some Essential libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sb\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\n\n#lets load data into dataframes\ntrainFrame = pd.read_csv('/kaggle/input/video-games-rating-by-esrb/Video_games_esrb_rating.csv')\ntestFrame  = pd.read_csv('/kaggle/input/video-games-rating-by-esrb/test_esrb.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# doing sanity checks on train Frame and test Frame\ntrainFrame[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's do some EDA\n# Plotting distribution of games Platformwise\n\npsExclusives = len(trainFrame[ trainFrame['console'] == 0 ])\nxBoxEclusives = len(trainFrame[ trainFrame['console'] == 1 ])\navailableOnBoth = len(trainFrame[ trainFrame['console'] == 2 ])\nother =len(trainFrame[ trainFrame['console'] == 3 ])\n\nnameList = ['Play Station Exclusives', 'XBox Exclusives', 'Available on both', 'Others']\nnameValues = [psExclusives, xBoxEclusives, availableOnBoth, other]\n\nplt.figure(figsize=(17, 5))\nplt.subplot(1,2,1)\nplt.bar(nameList, nameValues, color = 'green')\nplt.title('Bar graph: Game distribution Platformwise [Train set]')\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.pie(nameValues, labels = nameList,autopct='%1.2f%%')\nplt.title('Pie chart: For platforwise game distribution [Train set]')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# doing same for test set\npsExclusives = len(testFrame[ testFrame['console'] == 0 ])\nxBoxEclusives = len(testFrame[ testFrame['console'] == 1 ])\navailableOnBoth = len(testFrame[ testFrame['console'] == 2 ])\nother =len(testFrame[ testFrame['console'] == 3 ])\n\nnameList = ['Play Station Exclusives', 'XBox Exclusives', 'Available on both', 'Others']\nnameValues = [psExclusives, xBoxEclusives, availableOnBoth, other]\n\nplt.figure(figsize=(17, 5))\nplt.subplot(1,2,1)\nplt.bar(nameList, nameValues, color = 'green')\nplt.title('Bar graph: Game distribution Platformwise [Test set]')\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.pie(nameValues, labels = nameList,autopct='%1.2f%%')\nplt.title('Pie chart: For platforwise game distribution [Test set]')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note : It seems in the given data set there are no Xbox exclusives and other platform games. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now lets do the same for label distribution  but lets write a \n# function to do so\n\ndef DistributionPlotter(dataFrame, featureName, setName):\n    uniqueVals = set(dataFrame[featureName])\n    countArr =  []\n    \n    for mem in uniqueVals:\n        countArr.append(len(dataFrame[dataFrame[featureName] == mem]))\n    \n    plt.figure(figsize=(17, 5))\n    plt.subplot(1,2,1)\n    plt.bar(list(uniqueVals), countArr, color = 'orange')\n    plt.title('Bar graph: ' + str(setName))\n    plt.grid(True)\n\n    plt.subplot(1, 2, 2)\n    plt.pie(countArr, labels = list(uniqueVals),autopct='%1.2f%%')\n    plt.title('Pie chart:  ' + str(setName))\n    plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DistributionPlotter(trainFrame, 'esrb_rating', 'Rating distribution [Train set]')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DistributionPlotter(testFrame, 'esrb_rating', 'Rating distribution [Test set]')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also this seems in the given set only 4 ratings namely E, ET+, T and mature are present.\nlets try to do 2 more things:\n* finding correlation between features and ratings\n* Visualize data in 2d hyperspace.\n\nTo do this we must first map rating values to numerical values. I will be following the below mapping (based on severity of rating):\n\n{ 'E' : 0,\n  'ET': 1,\n  'T' : 2,\n  'M' : 3,\n}"},{"metadata":{"trusted":true},"cell_type":"code","source":"mapp = { 'E' : 0,\n  'ET': 1,\n  'T' : 2,\n  'M' : 3,\n}\n\ntestFrame['esrb_rating'] = testFrame['esrb_rating'].map(mapp)\ntrainFrame['esrb_rating'] = trainFrame['esrb_rating'].map(mapp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (19, 11))\nfeaturesRequired = trainFrame.columns[2:]\nsb.heatmap(testFrame[featuresRequired].corr(), annot = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some Conclusions that can be drawn from here: \n* features blood_gore and blood has correlations of 0.5 and 0.41 respectively, more of these more sever will be the rating.\n* feature voilence has a correlation 0.47 and hence follows the above statement.\n\nPretty much what we expected.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# moving with task 2 visualizing rating in hyperspace(here 2D space)\nfeaturesRequired = trainFrame.columns[2:-1]\nfeatueVector = np.array(trainFrame[featuresRequired])\n\nfrom sklearn.preprocessing import StandardScaler\nstdSc = StandardScaler()\nfeatueVector = stdSc.fit_transform(featueVector)\n\npca = PCA(n_components=2)\npca.fit(featueVector)\nfeatueVector = pca.transform(featueVector)\n\ndimReducedDataFrame = pd.DataFrame(featueVector)\ndimReducedDataFrame['targets'] = trainFrame['esrb_rating'].map({0 : 'E', 1 : 'ET', 2 : 'T', 3 : 'M'})\ndimReducedDataFrame = dimReducedDataFrame.rename(columns = {0:'V1', 1 : 'V2'})\n\nplt.figure(figsize=(10, 10))\nsb.scatterplot(data=dimReducedDataFrame, x='V1', y='V2', hue = 'targets')\nplt.grid(True)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# doing same for testset\n# moving with task 2 visualizing rating in hyperspace(here 2D space)\nfeaturesRequired = testFrame.columns[2:-1]\nfeatueVector = np.array(testFrame[featuresRequired])\n\nfrom sklearn.preprocessing import StandardScaler\nstdSc = StandardScaler()\nfeatueVector = stdSc.fit_transform(featueVector)\n\npca = PCA(n_components=2)\npca.fit(featueVector)\nfeatueVector = pca.transform(featueVector)\n\ndimReducedDataFrame = pd.DataFrame(featueVector)\ndimReducedDataFrame['targets'] = testFrame['esrb_rating'].map({0 : 'E', 1 : 'ET', 2 : 'T', 3 : 'M'})\ndimReducedDataFrame = dimReducedDataFrame.rename(columns = {0:'V1', 1 : 'V2'})\n\nplt.figure(figsize=(10, 10))\nsb.scatterplot(data=dimReducedDataFrame, x='V1', y='V2', hue = 'targets')\nplt.grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above 2 figures it is clear that there exists a decission boundary\nand hence our algos will work well !!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets define a utility function for visualizing the performance of our classifiers\ndef Vizutil(trueVals, predictedVals, classifierName):\n    plt.figure(figsize=(7, 7))\n    plt.scatter(trueVals, predictedVals, color = 'green')\n    plt.xlabel('True Values')\n    plt.ylabel('Predicted Values')\n    plt.grid(True)\n    \n    from sklearn.metrics import accuracy_score\n    acc = accuracy_score(trueVals, predictedVals)\n    \n    plt.title('performance of ' + str(classifierName) + 'Acc :' + str(acc*100))\n    plt.plot([0,1,2,3], [0,1,2,3], color = 'blue')\n    plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we split data into test - train sets\nxTrain = np.array (trainFrame[trainFrame.columns[2:-1]])\nxTest = np.array (testFrame[testFrame.columns[2:-1]])\n\nyTrain = np.array (trainFrame[trainFrame.columns[-1]])\nyTest = np.array (testFrame[testFrame.columns[-1]])\n\n# Kindly note we dont need to apply standard scalling here \n# as features are binary in nature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classifier 1.) Decission Tree classifier\n\nfrom sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(xTrain, yTrain)\n\npredVal1 = dtc.predict(xTest)\nVizutil(yTest, predVal1, 'Decission Tree Classifier')\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classifier 2.) Random Forest classifier\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nrfc.fit(xTrain, yTrain)\n\npredVal2 = rfc.predict(xTest)\nVizutil(yTest, predVal2, 'Radom forest  Classifier')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classifier 3.) gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\ngbc = GradientBoostingClassifier()\ngbc.fit(xTrain, yTrain)\n\npredVal3 = gbc.predict(xTest)\nVizutil(yTest, predVal3, 'Gradient Boosting  Classifier')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classifier 4.) bagging classifier\nfrom sklearn.ensemble import BaggingClassifier\nbgc = BaggingClassifier()\n\nbgc.fit(xTrain, yTrain)\npredVal4 = bgc.predict(xTest)\nVizutil(yTest, predVal4, 'Bagging Classifier')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classifier 5 ensebled Classifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nestimators = [\n    ('dt', DecisionTreeClassifier()),\n    ('rf', RandomForestClassifier()),\n    ('bg', BaggingClassifier()),\n    ('rf2', RandomForestClassifier(n_estimators = 100, max_depth= 12))\n    \n]\n\nclf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\nclf.fit(xTrain, yTrain)\n\npredVal5 = clf.predict(xTest)\nVizutil(yTest, predVal5, 'Stacked/Ensembled Classifier')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above we can see that:\n\n* Decission Tree has an acc score of 83 % on test set\n* Random Forest has an acc score of 85.4 % on test set\n* Gradient Boosting classifier has an accuracy of 79.6 % (this is unusual as gbc in many cases outperforms the above 2)\n* Bagging Classifier has an accuracy of 83.8 % on test set\n* Ensembled Classifier has an accuracy of 85.6 % on test\n\n\nTodo: Implementing tensorflow model, but then again it's performance will be around 80%\nbecause neural networks tends have similar performance as that of gradient boosting classifier\n\nWell if you have reached this far, Thanks for reading.........."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}