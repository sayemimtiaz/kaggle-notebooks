{"cells":[{"metadata":{},"cell_type":"markdown","source":"# An optimization approach to the design and analysis of COVID-19 literature search engines"},{"metadata":{},"cell_type":"markdown","source":"# Summary of the methodology"},{"metadata":{},"cell_type":"markdown","source":"Contributions\n\nThe main contributions of this work are summarized as follows. \n\n1. Framework : A quantitative framework to evaluate the quality of all Covid-19 literature search engines\n\n2. Algorithm : An algorithm to mine relevant documents and texts given any query which is designed according to the framework in (1)\n\n3. Retrieval : Evaluation using the algorithm in (2)\n"},{"metadata":{},"cell_type":"markdown","source":"Framework\n\nGiven any text, let $f$ be an encoding function such that $f(text)$ is a $R^n$ compact representation of the text. This is also known as feature vector.  \n\n\nGiven any two feature vectors $f_1, f_2$ for two sets of text, we define a similarity function $s(f_1, f_2) \\in R^{[0,1]}$ such that it represents how similar two sets of text are. The more similar two sets of text are, the closer $s(f_1, f_2)$ is to 1.\n\nGiven a set of ground truth data ($T_1$, $T_2$, $p$) where $T_1$ and $T_2$ are text and $p\\in R^{[0,1]} $ represents how relevent $T_1$ and $T_2$ are. $p$ is 0 if $T_1$ and $T_2$ are not totally not related and $1$ if they are the same.\n\nGiven $N$ ground truth data ($T^{(k)}_1$, $T^{(k)}_2$, $p^{(k)}$) $_{k = 1 : N}$, our quantitative framework seeks to find feature vector function $f$ and similarity function $s$ such that it minimize the following loss, which is also known as the cross-entropy loss. \n\n$$ \\min _{s,f} - \\frac{1}{N} [\\sum_{k=1}^{N} p^{(k)} \\log s(f(T^{(k)}_1), T^{(k)}_2))  +  (1- p^{(k)}) \\log (1- s(f(T^{(k)}_1), T^{(k)}_2))) ]  $$\n\nIt is noted that under this framework, one may take $f_1$ using the topic model vector from Latent Dirichlet Allocation and take $s_1$ as consine similarity function. That represents one valid solution under this framework.\n\nAlternatively, one may take $f_2$ as the bag of words vector and use $s_2$ as a weighted sum of indicator function on individual important words. That represents another valid solution under this framework. \n\nOne may also concatenate $f_1$ and $f_2$ as $f_3 = (f_1, f_2) $ and take average of $s_1$ and $s_2$ to form $s_3(f_3(T_1), f_3(T_2)) = 0.5*(s_1(f_1(T_1), f_1(T_2)) + s_2(f_2(T_1), f_2(T_2)))$. That also represent a valid solution under this framework. \n\nThe nice thing of this optimization approach is that we can compare all these methods using **one** number, the value of the loss function. \n"},{"metadata":{},"cell_type":"markdown","source":"Algorithm\n\nTo concretely construct the algorithm, there are 3 different challenges that need to be addressed.\n\n1. How do we obtain the ground truth data ? \n\n2. How do we parametrize feature vector $f$ ?\n\n3. How do we parametrize similarity function $s$ ? \n\nFor (1), we randomly sample words from the texts to construct pseudo documents. If the text are originally from the same document, then we consider them to be highly similar and assign a value of 1. If the psedo documents originate from different sources, we assign similarity as 0. We remark that this step can be improved when we have human labelled data.\n\nFor (2), we use a concatenation of bag of words vector, TFIDF vector, LDA vector and LSI vector. We first do some prerpocessing like filtering common words, filtering words of low frequency, and perform stemming. Then we construct LDA, LSI, TFIDF and BOW models from the data. Finally, we apply the model on each text and concatentate the output to form the final feature vector.\n\nFor (3), we use a neural network. We have also tried using random forest. But it turns out that neural network gives better performance than random forest. We also remark that one may do some feature engineering before feeding them feature vector  into the random forest. But we take a simpler approach of letting the neural network figure out the best way to do similarity calcualation.\n\nThe algorithm is summarized as follows. \n\n\na. Find feature vector generation function $f$ by concatenating the bag of words vector, TFIDF vector, LDA vector and LSI vector. \n\nb. Use sampling to generate ground truth data\n\nc. Find similarity function $s$ by training a neural network.\n \n\n\n"},{"metadata":{},"cell_type":"markdown","source":"Retrieval\n\nOnce we obtain the functions $f$ and $s$, there are multiple ways for us to perform retrieval, depending on the size of the documents available or whether we have user input.\n\nA simple approach is to do retrieval is simply compute the similarity of the query $Q$ and the documents and find the one that is most relevant as follows. That is the approach that we implemented.\n\n$$\\max_{T} s(f(Q), f(T))$$\n\n\nHowever, we also would like to mention a few other methods that use the same $f$ and $s$ but will be more appropriate depending on how the application is constructed.\n\nIf we have huge amount of documents, we need a fast way for retrieval.\n\nTo speed up computation,  we can build a graph and perform graph search in real time on that pre-constructed graph. We may want to use keyword matches to quickly find the initial nodes that are relevant.\n\nIf we have input from the user who is manually looking through the documents, we would like to use information from his choices in real time to improve accuracy. \n\nFor example, when the user only clicks on documents set $D_1$ but not document set $D_2$ in any given time, we can use that information to improve accuracy by doing nearest neighbor search.\n\n$$ \\max_{T} \\sum_{x\\in D_1} s(f(x), f(T))   -  \\lambda \\sum_{x\\in D_2} s(f(x), f(T)) $$ with $\\lambda$ as a regularization constant parameter.\n\nOur evaluation results will be presented in the later section of this notebook. "},{"metadata":{},"cell_type":"markdown","source":"Pros/Cons of the current approach:\n\nPros : This approach gives a quantitative framework for us to compare different search engines. The framework is broad enough that it covers multiple standard approaches. Also, with single parameter to optimize, the approach is compatible with other advanced approaches, like hyperparameter search using reinforcment learning. \n\nCons : The machine learning model need significant amount of data to perform well. If there is lack of data, it might be better to use heuristic based method. Also, the sampling based method to obtain ground truth can be noisy. Ideally, we would like to have some human labelled data to supplement that. We also remark that the current version does not include many features(e.g. same features but for bi-grams, same feature set but for the citation references, etc) we would like to have due to limitation of time."},{"metadata":{},"cell_type":"markdown","source":"# Implementation of a Covid-19 literature search engine prototype"},{"metadata":{},"cell_type":"markdown","source":"We use IS_FAST mode to generate the output. For more detailed results using more data, please use **IS_FAST = False**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nnltk.download('punkt')\n\nIS_FAST = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data cleaning\nimport json\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom collections import defaultdict\nfrom gensim import corpora\nfrom gensim.models.phrases import Phrases, Phraser\nimport glob\n\nclass dataCleaningRobot():\n\tdef __init__(self, path, max_count_arg = 10000000):\n\t\tself.max_count = max_count_arg\n\t\tself.paths_to_files =  glob.glob(path + \"/*.json\")\n\t\tself.path = path\n\n\tdef getText(self):\n\t\tself.getDocDicFromPath()\n\t\tself.filterTokens()\n\n\tdef getDocDicFromPath(self):\n\t\tself.dicOfTexts = {}\n\t\tmycount = 0\n\t\tfor filename in self.paths_to_files:\n\t\t\twith open(filename) as f:\n\t\t\t\tdata = json.load(f)\n\n\t\t\twordList = []\n\t\t\tpaperID = data['paper_id']\n\n\t\t\tfor eachTextBlk in data['abstract'] + data['body_text']:\n\t\t\t\twordList += word_tokenize(eachTextBlk['text'])\n\n\t\t\tself.dicOfTexts[paperID] = wordList\n\t\t\tmycount +=1 \n\t\t\tif mycount > self.max_count:\n\t\t\t\tbreak\n\n\tdef filterTokens(self):\n\t\tself.dicOfFilterTexts = {}\n\n\t\t### Token-based filtering   \n\t\tnewStopWords = set(['preprint', 'copyright', 'doi', 'http', 'licens', 'biorxiv', 'medrxiv'])\n\t\tstopWords = set( stopwords.words('english'))\n\n\t\tporter = PorterStemmer()\n\t\tself.wordCtFreq = defaultdict(int)\n\n\t\tfor eachText in self.dicOfTexts:\n\t\t\tfiltered = []\n\t\t\tfor word in self.dicOfTexts[eachText] :\n\t\t\t\tif word.isalpha() and len(word) > 2 :\n\t\t\t\t\ttoken = word.lower()\n\t\t\t\t\tif token in stopWords:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\ttoken = porter.stem(token)\n\t\t\t\t\tif token in newStopWords:\n\t\t\t\t\t\tcontinue\n\n\t\t\t\t\tfiltered.append(token)\n\t\t\t\t\tself.wordCtFreq[token] += 1\n\n\t\t\tself.dicOfFilterTexts[eachText] = filtered\n\t\t\n\t\t### Count-based filtering\n\t\tfor eachText in self.dicOfFilterTexts:\n\t\t\tfiltered = []\n\n\t\t\tfor word in self.dicOfFilterTexts[eachText] :\n\t\t\t\tif self.wordCtFreq[word] > 10 :\n\t\t\t\t\tfiltered.append(word)\n\n\t\t\tself.dicOfFilterTexts[eachText] = filtered\n\n\tdef getDicCorpus(self):\n\t\ttexts = [ self.dicOfFilterTexts[eachitem] for eachitem in self.dicOfFilterTexts ] \n\t\tself.dictionary = corpora.Dictionary(texts)\n\t\tself.corpus = [self.dictionary.doc2bow(text) for text in texts]\n\n\tdef getSingleWordCount(self):\n\t\treturn self.getCountInfo(self.wordCtFreq)\n\n\tdef getBigramCount(self):\n\t\treturn self.getCountInfo(self.bigramCtFreq)\n\n\tdef getCountInfo(self, ctFreqDic):\n\n\t\tword_freq_list = []\n\t\tfor each_token in ctFreqDic: \n\t\t\tword_freq_list.append([ctFreqDic[each_token], each_token])\n\t\tword_freq_list.sort(reverse= True)\n\n\t\t# print(word_freq_list[0:5])\n\t\twordList = []\n\t\tcountList = []\n\n\t\tfor eachitem in word_freq_list:\n\t\t\twordList.append(eachitem[1])\n\t\t\tcountList.append(eachitem[0])\n\n\t\treturn wordList, countList, word_freq_list\n\n\n\tdef getBigramData(self):\n\t\ttexts = [ self.dicOfFilterTexts[eachitem] for eachitem in self.dicOfFilterTexts ] \n\t\tself.bigram = Phrases(texts)\n\t\tself.bigram_model = Phraser(self.bigram)\n\t\tbigram_texts = [self.bigram_model[self.dicOfFilterTexts[eachitem]] for eachitem in self.dicOfFilterTexts]\n\n\t\tself.bigramCtFreq = defaultdict(int)\n\t\tfor eachText in bigram_texts:\n\t\t\tfor word in eachText :\n\t\t\t\ttmp_array = word.split('_') \n\n\t\t\t\t### Only extract two words\n\t\t\t\tif len(tmp_array) > 1 :\n\t\t\t\t\tself.bigramCtFreq[word] += 1\n\n\n\t\tself.bigram_dictionary = corpora.Dictionary(bigram_texts)\n\t\tself.bigram_corpus = [self.bigram_dictionary.doc2bow(text) for text in bigram_texts]\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature generation\nfrom gensim import models\nimport numpy as np\nfrom gensim.test.utils import datapath\nfrom nltk.stem.porter import PorterStemmer\nfrom gensim.corpora import Dictionary\n\nclass featureGenRobot():\n\tdef __init__(self):\n\t\tassert(True)\n\n\tdef getModels(self):\n\t\tself.num_lsi_topics = 2\n\t\tself.num_lda_topics = 10\n\t\tself.tfidf_model = models.TfidfModel(self.corpus)\n\t\tself.corpus_tfidf = self.tfidf_model[self.corpus]\n\t\tself.lsi_model = models.LsiModel(self.corpus_tfidf, id2word=self.dictionary, num_topics=self.num_lsi_topics)  \n\t\tself.lda_model = models.LdaModel(self.corpus,id2word=self.dictionary, num_topics=self.num_lda_topics, iterations=1500, passes=20, minimum_probability=0.0)\n\n\tdef saveModelsDict(self):\n\t\tprefix = '/kaggle/working/'\n\t\tself.lda_model.save(datapath(prefix + \"lda_debug.model\"))\n\t\tself.lsi_model.save(datapath(prefix + \"lsi_debug.model\"))\n\t\tself.tfidf_model.save(datapath(prefix + \"tfidf_model_debug.model\"))\n\t\tself.dictionary.save_as_text(prefix + \"dictionary_debug.txt\")\n\n\tdef loadModelsDict(self):\n\t\tprefix = '/kaggle/working/'\n\n\t\tself.num_lsi_topics = 2 \n\t\tself.num_lda_topics = 10\n\n\t\tself.tfidf_model = models.TfidfModel.load(prefix + \"tfidf_model_debug.model\")\n\t\tself.lsi_model = models.LsiModel.load(prefix + \"lsi_debug.model\")\n\t\tself.lda_model = models.LdaModel.load(prefix + \"lda_debug.model\")\n\n\t\tself.dictionary = Dictionary.load_from_text(prefix+ \"dictionary_debug.txt\")\n\n\tdef getFeaVec(self, text):\n\t\tporter = PorterStemmer()\n\t\tmyList = text.lower().split()\n\t\tmyList2 = [ porter.stem(word.lower()) for word in myList ]\n\t\tbow_vec = self.dictionary.doc2bow(myList2)\n\t\treturn self.getFeaVecFromBow(bow_vec)\n\n\tdef getFeaVecFromBow(self, bow_vec):\n\t\t### Computer BOW, TFIDF , LSI, LDA values . models topic division as dense features. \n\t\tvector_tfidf = self.tfidf_model[bow_vec]\n\t\tvector_lsi = self.lsi_model[bow_vec]\n\t\tvector_lda = self.lda_model[bow_vec]\n\t\t\n\t\t### Convert LSI, LDA values as dense vectors. \n\t\tlsi_topic = self.num_lsi_topics \n\t\tlda_topic = self.num_lda_topics\n\n\t\tN = lsi_topic + lda_topic\n\t\tdenseVector = np.zeros(N)\n\t\t\n\t\tbase = 0 \n\t\tfor i in range(len(vector_lsi)):\n\t\t\tidx = vector_lsi[i][0]\n\t\t\tdenseVector[base + idx] = vector_lsi[i][1]\n\n\t\tbase = len(vector_lsi)\n\t\tfor i in range(len(vector_lda)):\n\t\t\tidx = vector_lda[i][0]\n\t\t\tdenseVector[base + idx] = vector_lda[i][1]\n\n\t\t### Convert BOW, IFIDF as sparse vectors.\n\t\tm1 = len(self.dictionary)\n\t\tm2 = len(self.dictionary)\n\t\tsparseVec = np.zeros(m1 + m2)\n\t\tbase = 0 \n\t\tfor eachitem in bow_vec:\n\t\t\tidx = eachitem[0]\n\t\t\tval = eachitem[1]\n\t\t\tsparseVec[base + idx] = val\n\n\t\tbase = m1\n\t\tfor eachitem in vector_tfidf:\n\t\t\tidx = eachitem[0]\n\t\t\tval = eachitem[1]\n\t\t\tsparseVec[base + idx] = val\n\n\t\t### Convert dense and sparse feature vectors.\n\t\tcombined = np.concatenate((sparseVec, denseVector))\n\t\t#combined = denseVector\n\t\treturn denseVector, sparseVec, combined\n\n\tdef genFeaVecMap(self, dictionary_arg, corpus_arg):\n\t\tself.dictionary = dictionary_arg\n\t\tself.corpus = corpus_arg\n\t\tself.getModels()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example generation\nimport random\nimport numpy as np \n\n\nclass exampleGenRobot():\n\tdef __init__(self, dicOfFilterTexts, dictionary, featureRobot):\n\t\tassert(True)\n\t\tself.dicOfFilterTexts = dicOfFilterTexts\n\t\tself.dictionary = dictionary\n\t\tself.featureRobot= featureRobot\n\n\tdef getIJPair(self):\t\n\t\tself.ijPairs = []\n\t\tN = len(self.dicOfFilterTexts)\n\t\tposNum = 100\n\t\tnegNum = 100\n\t\tdocRatio = 0.01\n\n\t\tkeyList = list(self.dicOfFilterTexts)\n\n\t\tfor id_text in self.dicOfFilterTexts:\n\t\t\twholeText = self.dicOfFilterTexts[id_text]\n\t\t\tsentArr = [ [] for i in range(posNum)  ]\n\t\t\t### Pos eg\n\t\t\tfor word in wholeText:\n\t\t\t\tfor i in range(posNum):\n\t\t\t\t\tif random.random() < docRatio:\n\t\t\t\t\t\tsentArr[i].append(word)\n\t\t\t\n\t\t\tfor i in range(int(posNum/2)):\n\t\t\t\tbow_vec1 = self.dictionary.doc2bow(sentArr[2*i])\n\t\t\t\tbow_vec2 = self.dictionary.doc2bow(sentArr[2*i+1])\n\t\t\t\tdenseVector1, sparseVec1, combined1 = self.featureRobot.getFeaVecFromBow(bow_vec1)\n\t\t\t\tdenseVector2, sparseVec2, combined2 = self.featureRobot.getFeaVecFromBow(bow_vec2)\n\t\t\t\tself.ijPairs.append([combined1, combined2, 1])\n\n\t\t\t### Neg eg\n\t\t\tfor j in range(negNum):\n\n\t\t\t\tk = random.choice(keyList)\n\t\t\t\twhile ( k == id_text):\n\t\t\t\t\tk = random.choice(keyList)\n\n\t\t\t\tnegSent = []\n\t\t\t\tfor word in self.dicOfFilterTexts[k]:\n\t\t\t\t\tif random.random() < docRatio:\n\t\t\t\t\t\tnegSent.append(word)\n\n\t\t\t\tbow_vecneg = self.dictionary.doc2bow(negSent)\n\t\t\t\tbow_vec1 = self.dictionary.doc2bow(sentArr[j])\n\n\t\t\t\tdenseVector1, sparseVec1, combined1 = self.featureRobot.getFeaVecFromBow(bow_vec1)\n\t\t\t\tdenseVectorneg, sparseVecneg, combinedneg = self.featureRobot.getFeaVecFromBow(bow_vecneg)\n\t\t\t\tself.ijPairs.append([combined1, combinedneg, 0])\n\t\t\t\t\n\t\ttrain_X = np.zeros((len(self.ijPairs),  self.ijPairs[0][0].shape[0] + self.ijPairs[0][1].shape[0] ))\n\t\t\n\t\ty = np.zeros(len(self.ijPairs))\n\n\t\tfor i in range(len(self.ijPairs)) :\n\t\t\ttrain_X[i] = np.concatenate((self.ijPairs[i][0], self.ijPairs[i][1]))\n\t\t\ty[i] = self.ijPairs[i][2]\n\n\t\treturn train_X, y \n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model training\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\n\nclass simGenerator():\n\tdef __init__(self,train_eg, train_labels):\n\t\tself.train_eg = train_eg\n\t\tself.train_labels = train_labels\n\n\tdef trainModel(self):\n\t\ttrain_examples, test_examples, train_labels, test_labels = train_test_split(self.train_eg, self.train_labels, test_size=0.33, random_state=42)\n\t\ttrain_dataset = tf.data.Dataset.from_tensor_slices((train_examples, train_labels))\n\t\ttest_dataset = tf.data.Dataset.from_tensor_slices((test_examples, test_labels))\n\t\t\n\t\tBATCH_SIZE = 4\n\t\tSHUFFLE_BUFFER_SIZE = 16\n\n\t\ttrain_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n\t\ttest_dataset = test_dataset.batch(BATCH_SIZE)\n\n\t\tprint(train_examples.shape[1])\n\n\t\tmodel = tf.keras.Sequential()\n\t\tmodel.add(tf.keras.layers.Dense(10, input_dim=train_examples.shape[1], activation='relu'))\n\t\tmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n\n\t\tmodel.compile(optimizer=tf.keras.optimizers.RMSprop(),\n\t\t\tloss=tf.keras.losses.BinaryCrossentropy(),\n\t\t\tmetrics=['binary_accuracy'])\n\n\t\tmodel.fit(train_dataset, epochs=10)\n\t\tprint(model.evaluate(test_dataset))\n\n\n\t\tmodel_json = model.to_json()\n\t\twith open(\"/kaggle/working/kmodel.js\", \"w\") as json_file:\n\t\t\tjson_file.write(model_json)\n\n\t\tmodel.save_weights(\"/kaggle/working/kmodel.h5\")\n\t\tprint(\"Saved model to disk\")\n\n\t\tself.trained_model = model\n\n\t\t","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Retrieval \n\nimport numpy as np\n\nclass retrievalMethod():\n\tdef __init__(self, sim_model,dicOfFilterTexts, corpus, featureBot):\n\t\tself.dicOfFilterTexts = dicOfFilterTexts\n\t\tself.sim_model = sim_model\n\t\tself.corpus = corpus\n\t\tself.featureBot= featureBot\n\n\tdef findClosest(self, searchtext, limit=10, offset=0):\n\t\tdenseVector, sparseVec, combined = self.featureBot.getFeaVec(searchtext)\n\t\ttexts = [(eachitem, self.dicOfFilterTexts[eachitem]) for eachitem in self.dicOfFilterTexts] \n\n\t\trelevantList = []\n\t\tfor i in range(len(self.corpus)):\n\t\t\teach_bow = self.corpus[i]\n\t\t\tdenseVector1, sparseVec1, combined1 = self.featureBot.getFeaVecFromBow(each_bow)\n\t\t\tmodelVec = np.concatenate((combined1, combined))\n\t\t\tmodelVec = modelVec.reshape((1, modelVec.shape[0]))\n\t\t\tprob = self.sim_model.predict(modelVec)\n\t\t\trelevantList.append((prob, i))\n\n\t\trelevantList.sort(reverse=True)\n\n\t\treturn [{\n\t\t\"prob\": r[0],\n\t\t\"paper_id\": texts[r[1]][0]\n\t\t} for r in relevantList[offset:offset + limit]]\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example usage \n\nprint(\"Data cleaning\")\nfileList = \"/kaggle/input/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv/pdf_json\"\nif IS_FAST:\n    cleaner_bot = dataCleaningRobot(fileList, 70)\nelse:\n    cleaner_bot = dataCleaningRobot(fileList)\n\ncleaner_bot.getText()\ncleaner_bot.getDicCorpus()\n\nprint(\"Feature generation\")\nfeat_bot = featureGenRobot()\nfeat_bot.genFeaVecMap(cleaner_bot.dictionary, cleaner_bot.corpus)\nfeat_bot.saveModelsDict()\n\nprint(\"Ground truth generation\")\nfileList = \"/kaggle/input/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv/pdf_json\"\ncleaner_bot_small = dataCleaningRobot(fileList, 70)\ncleaner_bot_small.getText()\ncleaner_bot_small.getDicCorpus()\n\neg_bot = exampleGenRobot(cleaner_bot_small.dicOfFilterTexts, cleaner_bot.dictionary, feat_bot)\ntrain_eg, train_labels = eg_bot.getIJPair()\n\nprint(\"Model training\")\nsim_bot = simGenerator(train_eg , train_labels)\nmodel = sim_bot.trainModel()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"retrieve_bot = retrievalMethod(sim_bot.trained_model, cleaner_bot.dicOfFilterTexts, cleaner_bot.corpus, feat_bot)\nretrieve_bot.findClosest(\"vaccine and drug\") ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data exploration and visualization"},{"metadata":{},"cell_type":"markdown","source":"The library code defined above can also be used to do data exploration and visualization."},{"metadata":{"trusted":true},"cell_type":"code","source":"### Single word count visualization\nimport glob\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport numpy as np\n\n\nwordList, countList, word_freq_list = cleaner_bot.getSingleWordCount()\n\nplt.plot(range(len(countList)), countList)  \nplt.xlabel(\"Top i vacab\")\nplt.ylabel(\"Word count\")\nplt.show()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"offset = 0 \nnumItems = 10\nplt.barh(wordList[offset:offset+ numItems], countList[offset:offset+ numItems])\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"offset = 50 \nnumItems = 10\nplt.barh(wordList[offset:offset+ numItems], countList[offset:offset+ numItems])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Bigram word count visualization\n\ncleaner_bot.getBigramData()\nwordList_bi, countList_bi, word_freq_list_bi = cleaner_bot.getBigramCount()\nplt.xlabel(\"Top i bigram\")\nplt.ylabel(\"Bigram count\")\nplt.plot(range(len(countList_bi)), countList_bi)  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\noffset = 0 \nnumItems = 10\nplt.barh(wordList_bi[offset:offset+ numItems], countList_bi[offset:offset+ numItems])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"offset = 20 \nnumItems = 10\nplt.barh(wordList_bi[offset:offset+ numItems], countList_bi[offset:offset+ numItems])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Word cloud visualization\n\ntexts = \"\"\nfor eachDoc in cleaner_bot_small.dicOfFilterTexts:\n  for eachword in cleaner_bot_small.dicOfFilterTexts[eachDoc]:\n    if cleaner_bot_small.wordCtFreq[eachword] > countList[-30] and cleaner_bot_small.wordCtFreq[eachword] < countList[30]:\n      texts = texts + \" \" + eachword\n\n\nwordcloud = WordCloud(width = 800, height = 800, \n      background_color ='white', \n      min_font_size = 10).generate(texts)\n\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation"},{"metadata":{},"cell_type":"markdown","source":"We first test the simplest method using the function $f$ and $s$ to do retrieval, namely, directly use apply $f$ on the query and the documents and use $s$ to find relevant documents. But we found that when we use a small dataset to train the model, the performance is not good. We suspect that the size of the dataset is one of the culpit and we also note that the inherent noise in this unsupervised approach may be hard to deal with if dataset is not big enough. In particular, it gave the same set of document for a variety of query here"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.core.display import display, HTML\nfrom jinja2 import Template\nimport glob\n\nclass evaluatorRobot():\n\tdef __init__(self):\n\t\t\n\t\tself.table_template = Template('''\n\t\t<table>\n\t\t<thead>\n\t\t<tr>\n\t\t<th>Title</th>\n\t\t<th>Authors</th>\n\t\t<th>Abstract</th>\n\t\t<th>Paper ID</th>\n\t\t</tr>\n\t\t</thead>\n\t\t<tbody>\n\t\t{% for paper in papers %}\n\t\t<tr>\n\t\t<td>{{ paper.title }}</td>\n\t\t<td>{{ paper.authors }}</td>\n\t\t<td>\n\t\t{% for paragraph in paper.abstract %}\n\t\t<p>{{ paragraph }}</p>\n\t\t{% endfor %}\n\t\t</td>\n\t\t<td>{{ paper.paper_id }}</td>\n\t\t</tr>\n\t\t{% endfor %}\n\t\t</tbody>\n\t\t</table>\n\t\t''')\n\n\tdef loadPath(self, path):\n\t\tself.pathtofiles = path\n\n\tdef load_paper(self, paper_id):\n\t\tmatches = glob.glob(self.pathtofiles + \"/\" + f'{paper_id}.json', recursive=True)\n\t\tfilename = matches[0]\n\t\twith open(filename) as f:\n\t\t\tdata = json.load(f)\n\t\treturn data\n\n\tdef formatPaper(self, raw_paper):\n\t\tpaper = self.load_paper(raw_paper['paper_id'])\n\t\tauthors = [f'{author[\"first\"]} {author[\"last\"]}' for author in paper['metadata']['authors']]\n\t\tabstract_paragraphs = [paragraph['text'][:100] + '...' for paragraph in paper['abstract']]\n\t\n\t\treturn {\n\t\t\t'title': paper['metadata']['title'],\n\t\t\t'authors': ', '.join(authors),\n\t\t\t'abstract': abstract_paragraphs,\n\t\t\t'paper_id': paper['paper_id'],\n\t\t\t\"prob\": raw_paper['prob']\n\t\t}\n\n\tdef presentResults(self, results):\n\t\tpapers = [self.formatPaper(r) for r in results]\n\t\trender = self.table_template.render(papers=papers)\n\t\tdisplay(HTML(render))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"retrieve_bot = retrievalMethod(sim_bot.trained_model, cleaner_bot.dicOfFilterTexts, cleaner_bot.corpus, feat_bot)\nresults = retrieve_bot.findClosest(\"drug and vaccines\", limit=5, offset=0)\n\neval_bot = evaluatorRobot()\neval_bot.loadPath(cleaner_bot.path)\neval_bot.presentResults(results)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We then try to use another approach to leverage the $f$ and $s$ that we first use key word search, then cosine similarity search, then search using the similarity function learnt. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Retrieval \n\nimport numpy as np\nfrom scipy import spatial\nfrom nltk.stem.porter import PorterStemmer\n\nclass retrievalMethod2():\n\tdef __init__(self, sim_model,dicOfFilterTexts, corpus, featureBot):\n\t\tself.dicOfFilterTexts = dicOfFilterTexts\n\t\tself.sim_model = sim_model\n\t\tself.corpus = corpus\n\t\tself.featureBot= featureBot\n        \n\tdef findClosestWithSeed(self,  searchtext, limit=10, offset=0):\n\t\t# = [\"therapeutic\"]\n\t\t#key_words_2  = [\"animal\" ,\"model\"]\n\t\ttexts = [ [eachitem , self.dicOfFilterTexts[eachitem]] for eachitem in self.dicOfFilterTexts ] \n\t\trelevantList = []\n\t\talreadyFoundDic = {}\n\n\t\t### Key word match first : score range 0.7 to 1 \n\t\tkey_words_1 = searchtext.split()\n\t\tporter = PorterStemmer()\n\n\t\tkw_stem = []\n\t\tfor word in key_words_1:\n\t\t\tkw_stem.append(porter.stem(word))\n\n\t\tkw_matched_list = []\n\n\t\tindex = 0\n\t\tfor doc_id in self.dicOfFilterTexts:\n\t\t\tN = len(self.dicOfFilterTexts[doc_id])\n\t\t\tcount  = 0 \n\t\t\ttotal_count = 0\n\t\t\tfor i in range(N - len(kw_stem)):\n\t\t\t\ttotal_count += 1\n\t\t\t\tif self.dicOfFilterTexts[doc_id][i:i+ len(kw_stem)] == kw_stem:\n\t\t\t\t\tcount += 1 \n\n\t\t\tif count > 0:\n\t\t\t\tkw_matched_list.append([ count*1.0/total_count, count , doc_id, index])\n\n\t\t\tindex += 1 \n\n\t\tkw_matched_list.sort(reverse=True)\n\t\tkw_matched_score_list= []\n\t\tfor j in range(len(kw_matched_list)):\n\t\t\tindex = kw_matched_list[j][-1]\n\t\t\t#print(\"d1\", j)\n\t\t\trelevantList.append([1 - (1-0.7)*j/len(kw_matched_list), index])\n\t\t\tkw_matched_score_list.append([1 - (1-0.7)*j/len(kw_matched_list), index])\n\t\t\talreadyFoundDic[index] = True\n\n\t\t### Cosine distance match : score range 0.3 to 0.7 \n\t\tcos_sim_matched_list = []\n\n\t\tfor each_already_rel in kw_matched_score_list:\n\t\t\tscore, kk = each_already_rel[0],  each_already_rel[1]\n\t\t\tdenseVector, sparseVec, combined = self.featureBot.getFeaVecFromBow(self.corpus[kk])\n\n\t\t\tfor i  in range(len(self.corpus )):\n\t\t\t\tif  i in alreadyFoundDic:\n\t\t\t\t\tcontinue\n\t\t\t\teach_bow = self.corpus[i]\n\t\t\t\tdenseVector1, sparseVec1, combined1 = self.featureBot.getFeaVecFromBow(each_bow)\n\t\t\t\tsimilarity = 1 - spatial.distance.cosine(denseVector, denseVector1)\n\t\t\t\tif similarity > 0.9 :\n\t\t\t\t\tcos_sim_matched_list.append([score*similarity, i ] )\n\n\n\t\tcos_sim_matched_list.sort(reverse=True)\n\t\tfor j in range(len(cos_sim_matched_list)):\n\t\t\tindex = cos_sim_matched_list[j][-1]\n\t\t\t#print(\"d2\", j)\n\t\t\trelevantList.append([0.7 - (0.7-0.3)*j/len(cos_sim_matched_list), index])\n\t\t\talreadyFoundDic[index] = True\n\n\n\t\t### s distance match using doc  : score range 0.1 to 0.3 \n\t\t### Cosine distance match : score range 0.3 to 0.7 \n\t\tdoc_sim_matched_list = []\n\n\t\tfor each_already_rel in kw_matched_score_list:\n\t\t\tscore, kk = each_already_rel[0],  each_already_rel[1]\n\t\t\tdenseVector, sparseVec, combined = self.featureBot.getFeaVecFromBow(self.corpus[kk])\n\t\t\tfor i  in range(len(self.corpus )):\n\t\t\t\tif  i in alreadyFoundDic:\n\t\t\t\t\tcontinue\n\t\t\t\teach_bow = self.corpus[i]\n\t\t\t\tdenseVector1, sparseVec1, combined1 = self.featureBot.getFeaVecFromBow(each_bow)\n\n\t\t\t\tmodelVec = np.concatenate((combined1, combined))\n\t\t\t\tmodelVec= modelVec.reshape((1, modelVec.shape[0]))\n\t\t\t\tprob = self.sim_model.predict(modelVec)\n\t\t\t\t\n\t\t\t\tif prob > 0.9:\n\t\t\t\t\tdoc_sim_matched_list.append([score*prob, i ] )\n\n\n\t\tdoc_sim_matched_list.sort(reverse=True)\n\t\tfor j in range(len(doc_sim_matched_list)):\n\t\t\tindex = doc_sim_matched_list[j][-1]\n\t\t\t#print(\"d3\", j)\n\t\t\trelevantList.append([0.3 - (0.3-0.1)*j/len(doc_sim_matched_list), index])\n\t\t\talreadyFoundDic[index] = True\n\n\t\trelevantList.sort(reverse = True)\n\n\t\treturn [{\n\t\t\t\"prob\": r[0],\n\t\t\t\"paper_id\": texts[r[1]][0]\n\t\t} for r in relevantList[offset:offset + limit]]\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"retrieve_bot = retrievalMethod2(sim_bot.trained_model, cleaner_bot.dicOfFilterTexts, cleaner_bot.corpus, feat_bot)\nresults = retrieve_bot.findClosestWithSeed(\"vaccine\") \n\neval_bot = evaluatorRobot()\neval_bot.loadPath(cleaner_bot.path)\neval_bot.presentResults(results)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"retrieve_bot = retrievalMethod2(sim_bot.trained_model, cleaner_bot.dicOfFilterTexts, cleaner_bot.corpus, feat_bot)\nresults = retrieve_bot.findClosestWithSeed(\"drug\") \n\neval_bot = evaluatorRobot()\neval_bot.loadPath(cleaner_bot.path)\neval_bot.presentResults(results)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We further categorize each subquestions in this task and identify important keywords to start the search. Here are what we used. \nCure (Drug, vaccine and preventive measures):\n\n[Keyword: therapeutic]\n  1. Effectiveness of drugs being developed and tried to treat COVID-19 patients. Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication. \n\n  4. Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.\n\n[Keyword: universal vaccine]\n  6. Efforts targeted at a universal coronavirus vaccine. \n\n[Keyword: prophylaxis]\n  8. Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers \n\n\nAnimal model:\n\n[Keyword: animal model]\n\n    3. Exploration of use of best animal models and their predictive value for a human vaccine.\n\n    7. Efforts to develop animal models and standardize challenge studies\n\n    10. Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models [in conjunction with therapeutics]\n\n\nSide effects:\n\n[Keyword: enhanced disease]\n\n    2. Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.\n\n    9. Approaches to evaluate risk for enhanced disease after vaccination\n\n\nDistribution method:\n\n[Keyword: prioritize]\n\n    10. Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up. This could include identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"retrieve_bot = retrievalMethod2(sim_bot.trained_model, cleaner_bot.dicOfFilterTexts, cleaner_bot.corpus, feat_bot)\nresults = retrieve_bot.findClosestWithSeed(\"therapeutic\") \n\neval_bot = evaluatorRobot()\neval_bot.loadPath(cleaner_bot.path)\neval_bot.presentResults(results)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"retrieve_bot = retrievalMethod2(sim_bot.trained_model, cleaner_bot.dicOfFilterTexts, cleaner_bot.corpus, feat_bot)\nresults = retrieve_bot.findClosestWithSeed(\"universal vaccine\") \n\neval_bot = evaluatorRobot()\neval_bot.loadPath(cleaner_bot.path)\neval_bot.presentResults(results)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"retrieve_bot = retrievalMethod2(sim_bot.trained_model, cleaner_bot.dicOfFilterTexts, cleaner_bot.corpus, feat_bot)\nresults = retrieve_bot.findClosestWithSeed(\"prophylaxis\") \n\neval_bot = evaluatorRobot()\neval_bot.loadPath(cleaner_bot.path)\neval_bot.presentResults(results)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"retrieve_bot = retrievalMethod2(sim_bot.trained_model, cleaner_bot.dicOfFilterTexts, cleaner_bot.corpus, feat_bot)\nresults = retrieve_bot.findClosestWithSeed(\"animal model\") \n\neval_bot = evaluatorRobot()\neval_bot.loadPath(cleaner_bot.path)\neval_bot.presentResults(results)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"retrieve_bot = retrievalMethod2(sim_bot.trained_model, cleaner_bot.dicOfFilterTexts, cleaner_bot.corpus, feat_bot)\nresults = retrieve_bot.findClosestWithSeed(\"enhanced disease\") \n\neval_bot = evaluatorRobot()\neval_bot.loadPath(cleaner_bot.path)\neval_bot.presentResults(results)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"retrieve_bot = retrievalMethod2(sim_bot.trained_model, cleaner_bot.dicOfFilterTexts, cleaner_bot.corpus, feat_bot)\nresults = retrieve_bot.findClosestWithSeed(\"prioritize\") \n\neval_bot = evaluatorRobot()\neval_bot.loadPath(cleaner_bot.path)\neval_bot.presentResults(results)\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}