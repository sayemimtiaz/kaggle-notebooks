{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Modelling Boston Airbnb prices\n\nMaybe you have a place that you're fortunate enough to be able to potentially place on Airbnb but you're not sure if it's worth the hassle. With Airbnb making host listing data publicly available, we can break down the prices to understand what factors influence how much you can charge. We'll go through that here, providing an easy guide to get a rough idea for how much to charge if you have a place in Boston you can rent out.\n\nNote: Airbnb also gives pricing tips based on much more sophisticated models than I can accomplish here ([more details](https://www.vrmintel.com/inside-airbnbs-algorithm/)). Use this kernel though for a very rough guide to get you started.\n\nAlternatively, the findings from this kernel apply to those looking to understand which filters to play around with when trying to find a cheap (or expensive?) place, other than using the price filter on the search page ¯\\_(ツ)_/¯."},{"metadata":{},"cell_type":"markdown","source":"# What data are we working with?\n\nWe're given three datasets:\n\n- calendar.csv - listings with dates detailing whether they're available or not and how much they cost to rent on each date.\n- listings.csv - lots of information about each listing including text descriptions, host details, number of bedrooms, bathrooms, location and more.\n- reviews.csv - full text reviews for those listings that have been reviewed\n\nWe will mostly focus on `listings.csv` and pull date-sensitive prices from `calendar.csv`."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n\n# Show maximum 500 columns when displaying dataframes\npd.set_option('display.max_columns', 500)\n\n# Use Seaborn standard design palette for plots\nsns.set()\n\ncalendar = pd.read_csv('../input/boston/calendar.csv')\nlistings = pd.read_csv('../input/boston/listings.csv')\n\nfor dataset in [calendar, listings]:\n    display(dataset.sample(3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Listings data clean up"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'The dataset contains {len(listings.id.unique())} from {len(listings.host_id.unique())} hosts.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import missingno as msno\n\nmsno.matrix(listings);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the missingno matrix we see that we have a few features with only null values and some with mostly null values. We'll drop these along with any columns containing only one unique value and columns containing IDs or URL links."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Note number of columns\nbefore = len(listings.columns)\n\n# Drop columns with all null values\nall_null = len(listings.columns[listings.isnull().mean() == 1])\nlistings = listings.dropna(how='all', axis=1)\n\n# Drop columns with more than 50% null values\nmore_than_50_null = listings.columns[listings.isnull().mean() > 0.5]\nlistings = listings.drop(more_than_50_null, axis=1)\n\n# Drop columns with only one value\none_value_columns = [\n    column for column in listings.columns if len(listings[column].unique()) == 1\n]\nlistings.drop(one_value_columns, axis=1, inplace=True)\n\n# Drop url, ID (except for 'id') and name columns\nurl_id_columns = listings.columns[listings.columns.str.contains('url|_id|name')]\nlistings = listings.drop(url_id_columns, axis=1)\n\nprint(\n    '{} columns dropped:\\\n    \\n\\t{} columns with only null values\\\n    \\n\\t{} columns with more than 50% null values\\\n    \\n\\t{} columns with only one unique value\\\n    \\n\\t{} URL/ID/name columns'.format(\n        before - len(listings.columns), all_null, len(more_than_50_null),\n        len(one_value_columns), len(url_id_columns)\n    )\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will drop a bunch of features for various reasons:\n- they're covered by other features\n- they are likely to add variance to the model by being colinear to our target variable\n- they are almost completely one value"},{"metadata":{"trusted":true},"cell_type":"code","source":"to_drop = [\n    'host_neighbourhood', 'host_listings_count', 'host_total_listings_count',\n    'host_verifications', 'host_has_profile_pic', 'street', 'neighbourhood',\n    'city', 'zipcode', 'market', 'smart_location', 'latitude', 'longitude',\n    'is_location_exact', 'cleaning_fee', 'guests_included', 'extra_people',\n    'minimum_nights', 'maximum_nights', 'calendar_updated', 'availability_30',\n    'bed_type', 'availability_60', 'availability_90', 'availability_365',\n    'first_review', 'last_review', 'review_scores_rating', \n    'review_scores_accuracy', 'review_scores_cleanliness', \n    'review_scores_checkin', 'review_scores_communication',\n    'review_scores_location', 'require_guest_profile_picture', \n    'require_guest_phone_verification'\n]\n\nlistings = listings.drop(to_drop, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are now going to convert text features to numerical ones.\n\n- Summary and description features will be converted to a character count to see if a longer description helps.\n- We will create a boolean local feature based on whether the host is from Boston or not\n- `host_since` will be converted to a timedelta between an individual host's sign up date and the most recent host sign up\n- `host_response_time` will be converted to a dummy variable\n- % and $ signs will be stripped\n- `amenities` column will be converted to a count of amenities listed\n- Convert boolean t/f features to 1 or 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define description features\ndescription_features = ['summary', 'space', 'description', 'neighborhood_overview', 'transit', 'access', 'interaction', 'house_rules', 'host_about']\n\n# Convert null values to empty strings\nlistings[description_features] = listings[description_features].apply(lambda col: col.fillna(''), axis=1)\n\n# Convert description features to character counts\nfor column in listings[description_features]:\n    listings[column] = listings[column].apply(lambda x: len(x))\n\n# Convert host_since to datetime and create host_since_days timedelta feature\nlistings.host_since = pd.to_datetime(listings.host_since, yearfirst=True)\nlistings['host_since_days'] = (listings.host_since - listings.host_since.min()).dt.days\n\n# Create is_local feature based on host_location\nlocal_destination = 'Boston, Massachusetts, United States'\nlistings['is_local'] = listings.host_location.apply(\n    lambda location: 1 if location==local_destination else 0\n)\n\n# Drop converted features\nlistings = listings.drop(['host_since', 'host_location'], axis=1)\n\n# Map host_response_time values to numerical values\nresponse_map = {\n    np.nan: 0,\n    'a few days or more': 1,\n    'within a day': 2,\n    'within a few hours': 3,\n    'within an hour': 4\n}\nlistings.host_response_time = listings.host_response_time.replace(response_map)\n\n# Remove ['$', ',', '%'] and convert to float\nstr_to_float_columns = ['host_response_rate', 'host_acceptance_rate', 'price']\nfor column in str_to_float_columns:\n#     listings[column] = listings[column].str.replace('$', '').str.replace('%', '').str.replace(',', '').astype(float)\n    listings[column] = listings[column].apply(lambda value: re.sub(r'\\$|,|%', '', str(value))).astype(float)\n    \n# Convert boolean t/f columns to 1/0 columns\nboolean_columns = ['host_is_superhost', 'instant_bookable']\nfor column in boolean_columns:\n    listings[column] = listings[column].apply(lambda val: True if val=='t' else False)\n    \n# Convert amenities to amenities_count\nlistings['amenities_count'] = listings.amenities.str.count(',')+1\nlistings = listings.drop('amenities', axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dropping listings without reviews\n\nWe want to understand what price to put for our new listing, we want to model this off \"successful\" listings. Therefore we're going to remove listings without a review. This also has the handy side effect of removing some listings that might define as outliers. Outliers can greatly impact a model's ability to predict pricing and account for variance but should not be removed for this reason alone. Data points may look like outliers but in our case, unless if they're an obvious mistake (e.g. a typo or other), then they are valid."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_without_reviews = len(listings[listings.number_of_reviews==0])\npercent_gone = num_without_reviews/len(listings)\n\nprint(f'{num_without_reviews} rows or {percent_gone:.2%} of rows dropped as having no reviews')\n\nlistings = listings[listings.number_of_reviews!=0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Calendar data clean up\n\nFirst off, with `price` as our target variable, we will drop any rows where the price is null. A quick glance at the data shows that this value is null when the listing is not availble for a given date.\n\nTherefore we drop rows where price is null and drop the `available` column as this then contains only one value."},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar.sample(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop any rows without the predictor value\ncalendar.dropna(subset=['price'], inplace=True)\n\n# Convert price into a float\ncalendar.price = calendar.price.apply(\n    lambda value: re.sub(r'\\$|,', '', value)\n).astype(float)\n\n# Extract month from date string and rewrite values\ncalendar['month'] = calendar.date.apply(lambda value: value.split('-')[1])\ncalendar['month'] = calendar['month'].replace({\n    '01': 'Jan',\n    '02': 'Feb',\n    '03': 'Mar',\n    '04': 'Apr',\n    '05': 'May',\n    '06': 'Jun',\n    '07': 'Jul',\n    '08': 'Aug',\n    '09': 'Sep',\n    '10': 'Oct',\n    '11': 'Nov',\n    '12': 'Dec'\n})\n\n# Drop available and date columns\ncalendar = calendar.drop(['available', 'date'], axis=1)\n\ncalendar.sample(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Merge datasets\n\nWith both datasets cleaned up and transformed where needed, we can now merge the two. This will give us two `price` columns so we'll keep only the time-sensitive one coming from the calendar dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.merge(\n    listings, calendar, how='left',\n    left_on='id', right_on='listing_id',\n)\n\ndf = df.drop(['price_x', 'listing_id' ], axis=1)\ndf = df.rename({'price_y': 'price'}, axis=1)\n\n# Drop any remaining rows without a price value\ndf = df.dropna(subset=['price'])\n\ndf.sample(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring the data for trends\n\nWith some basic cleaning and feature engineering done, let's explore our dataset.\n\nIt's clear from the basic plots below that we have some informative features in our dataset."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, axs = plt.subplots(2, 2, figsize=(15, 10), dpi=80)\n\n# Neighbourhoods\nneighbourhoods = listings.groupby('neighbourhood_cleansed').agg(\n    {\n        'price': np.mean\n    }\n).reset_index()\nsns.barplot(\n    x='price', y='neighbourhood_cleansed',\n    data=neighbourhoods.sort_values('price'),\n    orient='h',\n    palette='Blues',\n    ax=axs[0, 0]\n)\naxs[0, 0].set_title('Neighbourhood')\naxs[0, 0].set_xlabel('Mean Price ($)')\naxs[0, 0].set_ylabel('')\n\n# Room type\n# The easiest way to plot this would be using histplot available in seaborn >= 0.11.0\n# sns.histplot(x='price', hue='room_type', data=listings, ax=axs[0, 1])\n# Instead we use distplot\nsns.distplot(listings[listings.room_type == 'Private room']['price'],\n             kde=False, ax=axs[0, 1], label='Private room')\nsns.distplot(listings[listings.room_type == 'Shared room']['price'],\n             kde=False, ax=axs[0, 1], label='Shared room')\nsns.distplot(listings[listings.room_type == 'Entire home/apt']['price'],\n             kde=False, ax=axs[0, 1], label='Entire home/apt')\naxs[0, 1].set_xlim(0, 600)\naxs[0, 1].set_title('Room Type')\naxs[0, 1].set_xlabel('Price ($)')\naxs[0, 1].legend()\n\n# Cancellation Policy\nsns.boxplot(x='price', y='cancellation_policy', fliersize=1, linewidth=0.75,\n            data=listings, palette='Blues', ax=axs[1, 0],\n            order=['flexible', 'moderate', 'strict', 'super_strict_30'])\naxs[1, 0].set_xlim(0, 600)\naxs[1, 0].set_title('Cancellation Policy')\naxs[1, 0].set_xlabel('Price ($)')\naxs[1, 0].set_ylabel('')\n\n# Property type\nsns.boxplot(x='price', y='property_type', fliersize=1, linewidth=0.75,\n            data=listings, palette='Blues', ax=axs[1, 1])\naxs[1, 1].set_xlim(0, 600)\naxs[1, 1].set_title('Property Type')\naxs[1, 1].set_xlabel('Price ($)')\naxs[1, 1].set_ylabel('')\n\nplt.tight_layout()\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There's a particularly strong relationship here between price and the number of bedrooms. The month plot is on quite a narrow scale suggesting that it's impact is not so great. We can also see a potentially non-linear relationship between bathrooms and price here."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, axs = plt.subplots(1, 3, figsize=(20, 5))\n\n# Month plot\nmonths = df.groupby('month').agg({'price': 'mean'}).reset_index()\n\n# Converting to category to be able to set the order\nmonths.month = months.month.astype('category')\nsorter = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\nmonths.month.cat.set_categories(sorter, inplace=True)\n\nsns.lineplot(\n    x='month', y='price',\n    data=months,\n#     orient='v',\n    color='#2F7FBC',\n    ax=axs[0]\n)\naxs[0].set_title('Month of the Year')\naxs[0].set_xlabel('')\n\n# Bedrooms\nbedrooms = df.groupby('bedrooms').agg({'price': 'mean'}).reset_index()\nsns.barplot(\n    x='bedrooms', y='price',\n    data=bedrooms,\n    orient='v',\n    color='#2F7FBC',\n    ax=axs[1]\n)\naxs[1].set_title('Number of Bedrooms')\naxs[1].set_xlabel('')\naxs[1].set_ylabel('')\n\n# Bathrooms\nbathrooms = df.groupby('bathrooms').agg({'price': 'mean'}).reset_index()\nsns.barplot(\n    x='bathrooms', y='price',\n    data=bathrooms,\n    orient='v',\n    color='#2F7FBC',\n    ax=axs[2]\n)\naxs[2].set_title('Number of Bathrooms')\naxs[2].set_xlabel('')\naxs[2].set_ylabel('')\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the numerical description features we created from the text summaries about the listing, neighbourhood, etc. We transformed text descriptions into character counts."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"description_features = ['summary', 'space', 'description',\n                        'neighborhood_overview', 'transit', 'access',\n                        'interaction', 'house_rules', 'host_about']\n\nfig, axs = plt.subplots(3, 3, figsize=(15, 15))\n\nfor feature, ax in zip(description_features, axs.reshape(-1)):\n    sns.regplot(x=feature, y='price', data=df, ax=ax, ci=None, line_kws={'color': 'orange'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With no evidence of any strong relationships here, we'll drop these features to reduce complexity. It's also simple enough to run the regression models with or without these features to see that they have no impact."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(description_features, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing step"},{"metadata":{},"cell_type":"markdown","source":"As a final bit of preprocessing, we'll deal with missing values, encode categorical features and drop the `id` column.\n\n5% of the rows contain missing data in one of the columns, mostly the `host_response_rate` and `host_acceptance_rate` columns. Even though it seems lazy, I see no reason why we shouldn't impute these missing values with the median rather than drop this 5% of the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the extremely small subset of rows with no property_type\ndf = df.dropna(subset=['property_type']).copy()\n\n# Imputing the mean for the remaining columns with null values\ncolumns_with_null = df.columns[df.isnull().any()]\nfor column in columns_with_null:\n    df[column] = df[column].fillna(df[column].median())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally we create dummies and drop the id column."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.get_dummies(df)\ndf = df.drop(['id'], axis=1)\ndf.sample(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling"},{"metadata":{},"cell_type":"markdown","source":"We'll fit the data to a few different regression methods but the trusty Linear Regression may suit our needs best. The data may contain non-linear relationships that Linear Regression will not be able to capture but it does have the significant benefit of being easily interpretable and the coefficients provided by the model will allow us to easily get a rough idea for how much we should be charging for listings.\n\nWe'll then use a few tree-based ensemble methods. Tree-based methods give us great flexibility in their ability to describe non-linear relationships but they also tend to be very sensitive to small variations in the training data and, unconstrained, can lead to overfitting. This is why we'll use a few ensemble methods to reduce this tendency to overfit. Further, tree-based methods allow for measuring the importance of features in prediction."},{"metadata":{},"cell_type":"markdown","source":"## Linear Regression\n\nWith a moderate amount of features, we'll need to keep an eye on overfitting, scoring the model on both the training and test sets. If we were to detect any bias/variance then we can look at one of the regularisation methods."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def print_scores(model):\n    \"\"\"Print the R-squared and RMSE scores for the train and test set\n    \n    Parameters\n        model: fitted regression model\n    \"\"\"\n    \n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n\n    r2_train = r2_score(y_train, y_pred_train)\n    r2_test = r2_score(y_test, y_pred_test)\n    rmse_train = (mean_squared_error(y_train, y_pred_train))**0.5\n    rmse_test = (mean_squared_error(y_test, y_pred_test))**0.5\n\n    print(\n        'Train R-squared: {:.3f}\\tTrain RMSE: ${:.2f}\\\n        \\nTest R-squared: {:.3f}\\tTest RMSE: ${:.2f}'\n        .format(r2_train, rmse_train, r2_test, rmse_test)\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nX = df.drop('price', axis=1)\ny = df.price\n\n# Split into train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n                                                    random_state=42)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Use previously created function to output metric scores\nprint_scores(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our model spits out an R-squared score of 60% meaning that we can account for around 60% of the variance using our features using Linear Regression. We also need to bear in mind the significant standard deviation of the residuals represented by the $97 root-mean-squared-error. Predictions made with the model have a large error margin.\n\nNow let's have a look at the coefficients."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"feature_importance = pd.DataFrame(\n    {'features': X.columns, 'coefficients': model.coef_}\n).sort_values(by='coefficients')\nfeature_importance['features'] = feature_importance['features']\n\nimport plotly.express as px\nfig = px.bar(x='features', y='coefficients',\n             data_frame=feature_importance, height=600)\nfig.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With this birds eye view of the coefficients we can see quite a few features having little to no impact on the price according to our linear regression model. Notable features that don't influence the price here:\n\n- details about the host such as their response time/rate and number of listings\n- number of reviews or the average rating given to a listing\n\nBelow we pull out the features deemed relevant by the model in predicting the price. The neighbourhood, property type and room type, alongside the number of rooms and properties stand out as key features in deciding the price.\n\nThe way to read this would be to go through the different features and add/subtract the coefficients together to get a rough idea for what price to put your Airbnb up for. To this you then need to add the **'intercept' of the model**, in our case $-\\$86$. That means you need to subtract $\\$86$ from the final price. The intercept is negative because we have certain features such as bedrooms which start at 1 instead of 0.\n\nFor example, say I have a two bed apartment in South End that I want to rent out in January. I would calculate\n\n- $+\\$50$ for the South End neighbourhood\n- $+\\$20$ because it's an apartment\n- $-\\$20$ since January is a cheap month\n- $+\\$0$ because I want to use a moderate cancellation policy\n- $+\\$42$ since I'll rent out the whole apartment\n- $+2\\times\\$63$ for the two bedrooms\n- $+\\$35$ for the one bathroom\n- $+3\\times\\$6$ for the three beds (there's a sofa bed)\n- $+6\\times\\$6$ since the place accommodates 6 people\n- $-\\$86$ for the intercept\n\nTherefore the model says I should put the place up for a price around $\\$221$. Again, bearing in mind the potential error margin here, I might think that my apartment is in a particularly nice part of South End and want to push the price up a bit. Maybe it's a bit of a cheek calling it a two-bed and knock a few dollars off.\n\nFeel free to go through the features here to get a rough idea of how much you might price your property."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def filter_coefficients(keyword, coefficients_df):\n    \"\"\"\n    Filters a dataframe of coefficients for specific features\n    \n    Parameters:\n        keyword (str): The keyword to filter the features by\n        coefficients_df (DataFrame): the coefficients df to filter by\n    \n    Returns:\n        df (DataFrame): a keyword filtered dataframe of coefficients\n    \"\"\"\n    \n    df = coefficients_df[coefficients_df.\\\n                         features.str.contains(keyword)].copy()\n    df.features = df.features.str.lstrip(keyword+'_')\n    \n    return df\n\ndef plot_coefficients(coefficients_df, ax=None, palette='icefire', xlabel='Coefficients', title=None):\n    \"\"\"Plots a horizontal barplot of the coefficients\"\"\"\n    \n    sns.barplot(\n        y='features', x='coefficients',\n        orient='h', data=coefficients_df,\n        palette=palette,\n        ax = ax\n    )\n    if ax:\n        ax.set_xlabel(xlabel)\n        ax.set_ylabel('')\n        ax.set_title(title)\n    else:\n        plt.xlabel(xlabel)\n        plt.ylabel('')\n        plt.title(title)\n        \n# Create the subplot grid\nfig, axs = plt.subplots(2, 3, figsize=(14, 8), dpi=100,\n                        gridspec_kw = {'height_ratios': [4, 2]})\n\n# Create lists to loop over for simple plots\nkeywords = ['neighbourhood_cleansed', 'property_type', 'cancellation_policy', 'room_type']\ntitles = ['Neighbourhood', 'Property Type', 'Cancellation Policy', 'Room Type']\naxes = [axs[0, 0], axs[0, 1], axs[1, 0], axs[1, 1]]\n\nfor kw, title, axis in zip(keywords, titles, axes):\n    plot_coefficients(\n        filter_coefficients(kw, feature_importance),\n        ax=axis,\n        title=title\n    )\n\n# Filter and sort months\nmonth = filter_coefficients('month', feature_importance)\nmonth.features = month.features.astype('category')\nsorter = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\nmonth['features'].cat.set_categories(sorter, inplace=True)\nplot_coefficients(month, ax=axs[0, 2], title='Month of the Year')\n\n# Pick final few features\nother = feature_importance[feature_importance.features.isin(\n    ['bedrooms', 'bathrooms', 'beds', 'accommodates',\n     'host_is_superhost', 'is_local']\n)].copy()\nplot_coefficients(other, ax=axs[1, 2], title='Other Features')\n\nfig.tight_layout()\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are some clear drawbacks to this model. For one thing, it's possible to plug in data to return a negative listing price implying that you should pay someone to come and stay in your dorm in West Roxbury in December. However, whilst Linear Regression in this case may not be the best method to accurately predict listing prices, it does provide an easily interpeted model that can be used (at least in some form) without even needing to use anything other than a pen and paper.\n\nWe'll now look at some ensemble methods to see if we can better account for the variation in the data."},{"metadata":{},"cell_type":"markdown","source":"## Ensemble methods\n\nAs mentioned, ensembled methods allow us leverage the flexibility of (in this case) tree-based models whilst reducing their tendency to memorise noise."},{"metadata":{},"cell_type":"markdown","source":"### Random Forests\n\nRandom Forests is one of the most powerful Machine Learning algorithms, despite its simplicity. It is an ensemble of Decision Trees, taking the average prediction from multiple individual trees all trained on a different random subset of the training data. Let's see how it does on our data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(n_estimators=20, n_jobs=-1)\nrf.fit(X_train, y_train)\nprint_scores(rf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Over 90% of the variance accounted for using Random Forests and with no sign of overfitting! Note though the sizable residual error standard deviation. There is stil a room for error even with so much of the variability in our dataset accounted for.\n\nWe'll look at the key features driving the prediction in a moment. First we'll look at a second ensemble method."},{"metadata":{},"cell_type":"markdown","source":"### XGBoost\n\nXGBoost stands for Extreme Gradient Boosting, a \"boosting\" ensemble method that works by sequentially adding predictors to an ensemble, each one correcting its predecessor. Another popular boosting method is AdaBoost, which stands for Adaptive Boosting."},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\n\nxgb = XGBRegressor()\nxgb.fit(X_train, y_train)\nprint_scores(xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We've chosen tree-based models here not only for their flexibility but also their ability to assess feature importance. Feature importance is measured by how much the tree nodes use a particular feature to predict prices. Scikit-Learn scales the results so that the sum of all importances is equal to 1.\n\nLet's see the top most important features according to the two ensemble methods."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_feature_importances(model):\n    \"\"\"Return sorted model feature importances\"\"\"\n    \n    df = pd.DataFrame(\n        {'features': X.columns, 'coefficients': model.feature_importances_}\n    ).sort_values(by='coefficients', ascending=False)[:15]\n    return df\n        \nfig, axs = plt.subplots(1, 2, figsize=(15, 6), dpi=80)\nplot_coefficients(\n    get_feature_importances(rf), ax=axs[0], xlabel='Importance',\n    title='Random Forest Feature Importance', palette='Blues_r')\nplot_coefficients(\n    get_feature_importances(xgb), ax=axs[1], xlabel='Importance',\n    title='XGBoost Feature Importance', palette='Blues_r')\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both ensemble methods are aligned that the number of bedrooms, bathrooms and whether the listing is an entire home/apt or not are the most important features. For both models, these features account for 57% of the importance. The rest of the features attribute small importances to the models."},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\n## Avoiding complexity\n\nThrough the models chosen and with the task at hand, we've managed to avoid some extra steps here in making our predictions. We did not need to scale our data since we used Linear Regression rather than one of the regularized methods and scaling did not impact the ensemble methods. Further, since we are only providing guidelines, we did not need to optimize our models with hyperparameter tuning. Our ensemble methods scored well enough by most standards without the added complexity.\n\n\n## That _Je Ne Sais Quoi_\n\nThe models have given us some sensible guidelines for choosing how much to put a place up for on Airbnb but in each case, there was a sizeable error margin.\n\nWhile our models do a great job at generalizing prices based on features, each listing is unique and what makes a traveller decide that your place is worth the money comes down to many features we won't be able to capture, although Airbnb do their best with their model.\n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}