{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Less is more - using neural textual Question Answering for CORD19\n\n## Task: What do we know about vaccines and therapeutics?\n\n## Our goal\n\nOur goal is to build a system which tries to fight information overload. For this purpose the system returns short replies, and only answers when the quality of retrieval and answers is satisfactory. The user could access the whole paragraphs and documents if further details were needed. We use neural textual Question Answering (QA) techniques to directly find specific answers to the scientific questions listed in [COVID-19 Open Research Dataset Challenge](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge). We adapted the scientific questions given in each task description to be more amenable towards current technology. The system is not tailored towards specific questions, and can be readily used to answer any other question. \n\n\n\n## Approach\n\nWe only use the freely available [CORD-19 dataset](https://pages.semanticscholar.org/coronavirus-research), which contains metadata of over 51,000 scientific papers (full text is also available for around 40,000 of them) about COVID-19, SARS-CoV-2, and related coronaviruses.\n\nAs we are mostly interested in papers related to COVID-19 (and not other coronaviruses), we filter out papers that are about coronaviruses other than COVID-19 (for example, SARS-CoV and MERS).\n\nThe implemented system has three main components. The first component is an Information Retrieval system (IR), based on the classical BM25F search algorithm. This system indexes abstracts and paragraphs on the full text of the papers.\n\nThe second component of the system is a QA system that, given a question in natural language and a paragraph returns the answer to the question in the paragraph or “I don’t know” otherwise. The implemented system is based on neural network techniques. More specifically, we have used the [SciBERT language representation model](https://arxiv.org/abs/1903.10676), which is a pretrained language model based on [BERT](https://arxiv.org/abs/1810.04805), but trained on a large corpus of scientific text, including text from biomedical domain. BERT has shown successful results in many NLP tasks, such as QA, but SciBERT is more adapted to our current domain. We explored several option to fine-tune SciBERT for QA for this challenge: (1) the [SQuAD2.0 dataset](https://rajpurkar.github.io/SQuAD-explorer/), which is a reading comprehension dataset widely used in the QA research community; (2) the [pre-processed](https://github.com/dmis-lab/bioasq-biobert) version of [BioASQ](http://bioasq.org/) 6b/7b - Phase B datasets that were proposed for a  large-scale biomedical semantic indexing and question answering competition; (3) the [QuAC dataset](http://quac.ai/), which is a conversational QA dataset containing a higher rate of non-factoid questions than previous ones. On inspection of the produced answers we observed that: SQuAD2.0 produced good quality for questions seeking short answers, and a combination of SQuAD2.0 and QuAC produced better quality for questions which require longer answers; fine tuning on BioASQ does not seem to be effective. We thus used the SciBERT model that has been fine-tuned first on SQuAD 2.0 and then on QuAC.\n\nFinally, the third component aggregates the results of the QA system. For each task, it runs the IR and QA systems over all questions. The IR system returns top-20 paragraphs for each query, so there are 20 answers per question. In order to fight information overload, we took the following decisions: (A) discard paragraphs where the QA system returns “I don’t know”; (B) return the best answer for each of the best five paragraphs, that is, five answers per question (but we highlight the best 3 answers in the text); (C) discard questions which don’t seem to be amenable for the current technology, estimated as those questions which receive more than %85 of “I don’t know” answers.  \n\nNote that the system is identical for all the tasks. Given a set of questions related to a task, returns answers for those questions without any additional tuning.\n\n\n## Pros and cons\n\nPositive aspects of the system:\n* The system tries to fight information overload: A) returns specific answers to the questions. B) returns answers only when relevant, trying to avoid low quality answers. \n* Given the questions, it is completely automatic and does not need any tuning. The better the questions quality, the better the answers the system provides. \n* The system can be used to easily explore other tasks and information needs, as it directly returns answers to the document collection via new questions.\n* We experimented with different fine-tuning strategies according to two broad types of questions. \n* The system is complementary to labor-intensive information extraction techniques that try to find answers to specific tasks using hand-annotation or manually built rules.\n\nLimitations and possible improvements (cons):\n* The interface could be richer, allowing more in-depth exploration in cases where the user would like to explore additional documents and answers. \n* Currently the system relies only on the information available in the metadata file and full texts of the CORD19 dataset. We have not used any external source or other related dataset.\n* The speed can easily be improved. It is limited by the 5Gb of storage space available, which makes the system slow in getting abstracts and full documents. Producing larger and richer indices will speed up the system considerably.\n* The system can be easily improved with a more sophisticated Information Retrieval module (see to-dos)\n* The system can be easily improved by incorporating domain-specific annotated development data (see to-dos) and a continuous learning component to keep learning thanks to the feedback of some hand-selected expert users (see to-dos)\n* We also plan to improve the system with a confidence measure in the answers. In the future we would like to introduce an improved confidence measure that combines the IR and QA scores into a unified measure that automatically assesses the quality of the answers (see to-dos).\n\nSome of the limitations are software-engineering tasks which do not add to the technical and scientific part of our system. We thus will focus on the more challenging and hopefully effective improvements of the next to-dos:\n* We will test more sophisticated IR modules for paragraph retrieval (we plan to evaluate on the [TREC-COVID challenge](https://ir.nist.gov/covidSubmit/))\n* We plan to collaborate with third parties to exploit domain-specific development data ([COVID-QA project](https://github.com/deepset-ai/COVID-QA/tree/master/data/question-answering)) \n* We plan to add a continuous learning component to keep learning thanks to the feedback of hand-selected expert users, using Human In The Loop strategies.\n* We also plan to improve the system with a confidence measure in the answers.\n\n\n## Steps\n1. [Install packages and load libraries](#libraries)\n2. [Load info from metadata file](#files)\n3. [Create an index for the paper retrieval system](#index)\n4. [Define a function to query the index and retrieve relevant papers](#retrieval)\n5. [Define a function to extract answers from the retrieved papers](#qa)\n6. [Define questions for all the tasks](#questions)\n7. [Show results for a task](#results)"},{"metadata":{},"cell_type":"markdown","source":"## 1. Install packages and load libraries<a class=\"anchor\" id=\"libraries\"></a>\n\nIn this section we will install all the packages and load all the libraries needed to run the code below."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install Whoosh # search engine library","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os.path # pathname manipulations\nimport codecs # base classes for standard Python codecs, like text encodings (UTF-8,...)\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport json # JSON file manipulations\nfrom IPython.core.display import display, HTML # object displaying in different formats\nfrom whoosh.index import * # whoosh: full-text indexing and searching\nfrom whoosh.fields import *\nfrom whoosh import qparser\nimport torch # optimized tensor library for deep learning using GPUs and CPUs\nfrom transformers import BertTokenizer, BertForQuestionAnswering, BasicTokenizer # transformers: large-scale transformer models like BERT, and usage scripts for them\nfrom transformers.data.metrics.squad_metrics import _get_best_indexes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Load info from metadata file<a class=\"anchor\" id=\"files\"></a>\n\nCORD19-dataset includes a metadata file (CSV file) of research papers related to coronavirus and COVID-19. In this section we first load the info in the metadata file into a dataframe object. As we are not interested in all the metadata info, we will select just some of the columns of the CSV file, such as title, publish time, abstract or journal.\n\nNote: this version of the notebook takes v7 of the dataset (from 2020-04-10).\n\nCORD-19.v7 includes info of 51,078 papers, but some of them are repeated (they have the same *cord_uid*). Thus, we filter out the repeated ones."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define path for CORD-19 dataset and its metadata file\npath_dataset = '/kaggle/input/CORD-19-research-challenge/'\npath_mdata = path_dataset + 'metadata.csv'\n\n# Select interesting fields from metadata file\nfields = ['cord_uid','title', 'publish_time', 'abstract', 'journal','url', 'has_pdf_parse', 'has_pmc_xml_parse', 'pmcid', 'full_text_file', 'sha']\n\n# Extract selected fields from metadata file into dataframe\ndf_mdata = pd.read_csv(path_mdata, skipinitialspace=True, index_col='cord_uid',usecols=fields)\n\n# WARNING: cord_uid is described as unique, but some of them are repeated. So we keep just the first one\n# Repeated cord_uids in v7 of data: 0klupmep, 0z5wacxs, 21htepa1, 21qu87oh, 2maferew, 30duqivi, 3ury4hnv, \n#   4fbr8fx8, 4hlvrfeh, 5ei7iwu0, 5kzx5hgg, 6hdoap81, 6z5f2gz3, 79mzwv1c, 7y8fd521, 8fwa2c24, 940au47y,\n#   adygntbe, brz1fn2h, c4u0gxp5, e9pwguwm, eich19nx, hox2xwjg, j7swau26, laq5ze8o, m6q8kbjg, mmls866r,\n#   o4r34pff, qhftb6d7, sv9mdgek, vp5358rr, vqbreyna, xjpev4jw\ndf_mdata = df_mdata.loc[~df_mdata.index.duplicated(keep='first')]\n\n# Sanity check\nprint(\"Number of papers loaded from metadata (after filtering out the repeated ones):\", len(df_mdata))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we are mostly interested in papers related to COVID-19 (and not other coronaviruses) in this challenge, we want to filter out papers that are about coronaviruses other than COVID-19 (for example, SARS-CoV and MERS). For that purpose, we created a list of synonyms of COVID-19 (see the list in the code below; we based mostly on this [article of WHO](https://www.who.int/emergencies/diseases/novel-coronavirus-2019/technical-guidance/naming-the-coronavirus-disease-%28covid-2019%29-and-the-virus-that-causes-it) to create the list), and we check if a synonym appears in the title or the abstract of a paper. In that way, we filter out those papers that do not include any of the synonyms. From now on, we will consider only the papers that we keep after filtering. \n\nThis filtering is also necessary for another reason. We are going to index the papers in the next step, and in order to store the index that includes all the papers in the metadata, we would need more than 5GB of disk space that we have available for this notebook.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of COVID-19 synonyms\nsynonyms = [\n    'coronavirus 2019',\n    'coronavirus disease 19',\n    'cov2',\n    'cov-2',\n    'covid',\n    'ncov 2019',\n    '2019ncov',\n    '2019-ncov',\n    '2019 ncov',\n    'novel coronavirus',\n    'sarscov2',\n    'sars-cov-2',\n    'sars cov 2',\n    'severe acute respiratory syndrome coronavirus 2',\n    'wuhan coronavirus',\n    'wuhan pneumonia',\n    'wuhan virus'\n]\n\n# Create a filter with 'False' values\nindex_list =  list(df_mdata.index.values) \nfilter = pd.Series([False] * len(index_list))\nfilter.index = index_list\n  \n# Update the filter using the synonym list, checking if a synonym appears in the title or the abstract of a paper\nfor s in synonyms:\n    # Check if a synonym is in title or abstract\n    filter = filter | df_mdata.title.str.lower().str.contains(s) | df_mdata.abstract.str.lower().str.contains(s)\n\n# Filter out papers in metadata dataframe using the above filter\ndf_mdata = df_mdata[filter]\n\n# Sanity check\nprint(\"After filtering, number of papers in metadata related to 'COVID-19':\", len(df_mdata))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Create an index for the paper retrieval system <a class=\"anchor\" id=\"index\"></a>\n\nThe first component of the system that we are going to develop in our approach is the information retrieval system. An information retrieval system is a tool that searches for  documents that are relevant to an information need from a collection of documents. This system has two main modules: the indexing system and the query system. \n\nThe first module is in charge of creating the primary data structure for the system, which is the index. The second component is the one with which users interact submitting a query based on their information need, and based on this query and using the index, retrieves documents. In this section we will create an index, and in the next section, we will develop the query system. For the implementation of these modules, we will use [Whoosh library](https://pypi.org/project/Whoosh/), which contains functions for indexing text and then searching the index.\n\nThe index is a data structure that makes it possible to search for information in a document collection in a very efficient way. In short, it lists, for every word, all documents that contain it.\n\nIn order to create an index, we must define the schema of the index. The schema lists the fields in the index. A field is a piece of information for each document in the index, for example, id, path of the document, title and text. We define the type of these last two fields as “TEXT”, which means that they will be searchable. As it is common practice, we also define to apply the Stemming Analyzer to these text fields. Applying this analyzer all the text will be tokenized, then all the tokens will be converted to lowercase, a stopword filter will be applied in order to remove too common words, and finally, a stemming algorithm will be applied."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Schema definition:\n# - id: type ID, unique, stored; cord_uid + \"##abs\" for abstract, and \"##pmc-N\" or \"##pdf-N\" for paragraphs in body text (Nth paragraph)\n# - path: type ID, stored; path to the JSON file (only for papers with full text)\n# - title: type TEXT processed by StemmingAnalyzer; not stored; title of the paper\n# - text: type TEXT processed by StemmingAnalyzer; not stored; content of the abstract section or the paragraph\nschema = Schema(id = ID(stored=True,unique=True),\n                path = ID(stored=True),\n                title = TEXT(analyzer=analysis.StemmingAnalyzer()),\n                text = TEXT(analyzer=analysis.StemmingAnalyzer())\n               )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once we have the schema, we can create an index. The created index will be stored in the “kaggle/working” directory of our output workspace.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create an index\nif not os.path.exists(\"index\"):\n    os.mkdir(\"index\")\n\nix = create_in(\"index\", schema)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we will add documents to the index. We will index the papers related to COVID-19, not only the abstracts that are in the metadata file, but also the full text provided in PMC or PDF JSON format. As having shorter documents is better for the answering system that we will develop later, we will not index the whole text in a paper together. Instead, the indexing unit will be an abstract or each of the paragraphs of the full text (as marked in JSON files).\n\nThis could take several minutes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add papers to the index, iterating through each row in the metadata dataframe\nwriter = ix.writer()\n\n\nnot_indexed = []\nindexed_sha = []\n\nfor ind in df_mdata.index: \n    indexed = False\n    \n    # If paper has an abstract, index the abstract\n    if not pd.isnull(df_mdata.loc[ind,'abstract']):\n        if pd.isnull(df_mdata.loc[ind,'title']):\n            df_mdata.at[ind,'title'] = \"\"\n        # Add document to the index\n        writer.add_document(id=ind+\"##abs\", title=df_mdata['title'][ind], text=df_mdata['abstract'][ind])\n        indexed = True\n    \n    # If paper has PMC or PDF full text, access its JSON file and index each paragraph separately\n    # First check if paper has PMC xml\n    if df_mdata['has_pmc_xml_parse'][ind] == True:\n        if pd.isnull(df_mdata.loc[ind,'title']):\n            df_mdata.at[ind,'title'] = \"\"\n        \n        # Find JSON file: path specified in 'full_text_file', file name specidfied in 'pmcid'\n        path_json = path_dataset + df_mdata['full_text_file'][ind] + '/' + df_mdata['full_text_file'][ind] + '/pmc_json/' + df_mdata['pmcid'][ind] + '.xml.json'\n        with open(path_json, 'r') as j:\n            jsondata = json.load(j)\n            \n            ## Iterate through paragraphs of body_text\n            for p, paragraph in enumerate(jsondata['body_text']):  \n                # Add document to the index\n                writer.add_document(id=ind+\"##pmc-\" + str(p), path = path_json, title=df_mdata['title'][ind], text=paragraph['text'])\n                indexed = True\n    \n    # As current paper does not have PMC, check if it has JSON PDF\n    elif df_mdata['has_pdf_parse'][ind] == True:\n        if pd.isnull(df_mdata.loc[ind,'title']):\n            df_mdata.at[ind,'title'] = \"\"\n        \n        # Find JSON file: path specified in 'full_text_file', file name specidfied in 'sha'\n        # There could be more than one reference in 'sha' separated by ;\n        shas = df_mdata['sha'][ind].split(';')\n        for sha in shas:\n            sha = sha.strip()\n            # Check if paper with this sha has been indexed already\n            if sha not in indexed_sha:\n                indexed_sha.append(sha)\n                path_json = path_dataset + df_mdata['full_text_file'][ind] + '/' + df_mdata['full_text_file'][ind] + '/pdf_json/' + sha + '.json'\n                with open(path_json, 'r') as j:\n                    jsondata = json.load(j)\n            \n                    ## iterate through paragraphs of body_text\n                    for p, paragraph in enumerate(jsondata['body_text']):  \n                        # Add document to the index\n                        writer.add_document(id=ind+\"##pdf-\" + str(p), path = path_json, title=df_mdata['title'][ind], text=paragraph['text'])\n                        indexed = True\n    \n    if not indexed:\n        not_indexed.append(ind)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we will save the added documents to the index."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the added documents\nwriter.commit()\nprint(\"Index successfully created\")\n\n# Sanity check\nprint(\"Number of documents (abstracts and paragraphs of papers) in the index: \", ix.doc_count())\nprint(\"Number of papers not indexed (because they don't have neither the abstract nor full text): \", len(not_indexed))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Define a function to query the index and retrieve relevant papers <a class=\"anchor\" id=\"retrieval\"></a>\n\nIn this section we will define a function that given a question, a dataframe that contains metadata info and a maximum number of documents as input, it uses this query to retrieve relevant papers that were indexed in the previous section.\n\nIn this function we set the algorithm used for scoring (we will be using the default BM25F algorithm), and we  also set the query parser to use, defining the default field to search (in our case '*text*’ field). Then, we run the query and get the most relevant documents on the index (*n_docs* documents at maximum). We create a new dataframe with the results.\n\nThe output of the function is this dataframe, where the following data is stored for each relevant paragraph: id, date, journal, title, text and score."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input: Question, dataframe that contains metadata info, maximum number of documents to retrieve\ndef retrieve_docs(qstring, df_metadata, n_docs):\n\n    # Open the searcher for reading the index. The default BM25F algorithm will be used for scoring\n    with ix.searcher() as searcher:\n        searcher = ix.searcher()\n        \n        # Define the query parser ('text' will be the default field to search), and set the input query\n        q = qparser.QueryParser(\"text\", ix.schema, group=qparser.OrGroup).parse(qstring)\n    \n        # Search using the query q, and get the n_docs documents, sorted with the highest-scoring documents first\n        results = searcher.search(q, limit=n_docs)\n        # results is a list of dictionaries where each dictionary is the stored fields of the document (id, path). 'title' and text' are not stored\n    \n    # Create columns (id, date, journal, title, text and score) for a new dataframe which will be used to store the results\n    ids = []\n    dates = []\n    journals = []\n    titles = []\n    texts = []\n    scores = [] \n    # Iterate over the retrieved documents to fill in the new dataframe\n    for hit in results:\n        # Add id to the new dataframe\n        ids.append(hit['id'])\n        \n        # As year, title and text are not stored in the index, they are not returned in results object. They have to be extracted from metadata\n        # Get paper id and type of section (abstract, full text paragraph from pmc or pdf)\n        pid,sect = hit['id'].split(\"##\") # id examples: 'vho70jcx##pmc-1', a5x5ga60##abs\n        \n        # Add year to the new dataframe\n        if pd.isnull(df_metadata.loc[pid,'publish_time']):\n            dates.append(\"\")\n        else:\n            dates.append(df_metadata['publish_time'][pid])\n            \n        # Add journal to the new dataframe\n        if pd.isnull(df_metadata.loc[pid,'journal']):\n            journals.append(\"unknown journal\")\n        else:\n            journals.append(df_metadata['journal'][pid])\n        \n        # Add title (with link to the doi, if exists) to the new dataframe \n        if pd.isnull(df_metadata.loc[pid,'url']):\n            titles.append(df_metadata['title'][pid])\n        else:\n            titles.append(\"<a target=blank href=\\\"\" + df_metadata['url'][pid] + \"\\\">\" + df_metadata['title'][pid] + \"</a>\")\n        \n        # Add text to the new dataframe\n        if sect == 'abs': # get text of the abstract (reading from metadata)\n            texts.append(df_metadata['abstract'][pid])\n        else: # get text of the paragraph (reading from a JSON file)\n            # get pmc or pdf, and the number of paragraph in body full text\n            json_type,nsect = sect.split(\"-\") # sect examples: 'pmc-1', 'pdf-5'\n    \n            # path of the JSON file whether text has been extracted from PMC or PDF\n            #if json_type == 'pmc':\n            #    path_json = path_dataset + df_metadata['full_text_file'][pid] + '/' + df_metadata['full_text_file'][pid] + '/pmc_json/' + df_metadata['pmcid'][pid] + '.xml.json'    \n            #else: \n            #    path_json = path_dataset + df_metadata['full_text_file'][pid] + '/' + df_metadata['full_text_file'][pid] + '/pdf_json/' + df_metadata['sha'][pid] + '.json'\n            with open(hit['path'], 'r') as j:\n                jsondata = json.load(j)\n                texts.append(jsondata['body_text'][int(nsect)]['text'])\n        \n        # Add score to the new dataframe\n        scores.append(hit.score)\n    \n    # Create a dataframe of results with the columns\n    df_results = pd.DataFrame()\n    df_results['id'] = ids\n    df_results['date'] = dates\n    df_results['journal'] = journals\n    df_results['title'] = titles\n    df_results['text'] = texts\n    df_results['score'] = scores\n    \n    \n    return df_results\n    # Output: Dataframe where each line is a relevant paragraph, and the columns are the following:\n    #         id, date, journal, title, text, score\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Define a function to extract answers from the retrieved papers<a class=\"anchor\" id=\"qa\"></a>\n\nThe second main component of the system that we are going to develop in our approach is the QA system. Given a question in natural language and a paragraph, this system returns the answer to the question in the paragraph or “I don’t know” otherwise. Our implementation for such a system will be based on neural networks techniques. The implementation details will be given below.\n\nIn this section we will first define the main function that given a question, a dataframe with the relevant paragraphs (returned by the *retrieve_docs()* function), maximum number of answers to extract and maximum length of the answer, it extracts specific answers from all  the relevant paragraphs. \n\nThis function returns the dataframe with relevant paragraphs, but with additional data. The best answers are added for each paragraph, specifying the answer itself (text), the score, and the start and end index that define the position of the answer in the paragraph."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input: Question, dataframe that returns the retrieve_docs() function, maximum number of answers to extract, maximum length of the answer \ndef extract_answers(qstring, df_results, n_answers, max_answer_len):\n    \n    # Set tokenizer to lower case the paragraph\n    basic_tokenizer = BasicTokenizer(do_lower_case=False)\n    \n    answers = []\n    # Iterate over the paragraphs\n    for i, context in enumerate(df_results['text']):\n        context = ' '.join(basic_tokenizer.tokenize(context))\n        # Add for QuAC\n        context += ' CANNOTANSWER'\n        # Call a function to extract answers from a paragraph (context)\n        answers.append(run_qa(qstring, context, n_answers, max_answer_len))  \n        # Remove it from context\n        context = context.replace(' CANNOTANSWER', '')\n        df_results['text'][i] = context\n    # Add answer to the results dataframe\n    df_results['qa_answers'] = answers\n    \n    return df_results\n    # Output: Dataframe where each line is a relevant paragraph, and the columns are the following:\n    #         id, date, journal, title, text, score, qa_answers.\n    #         qa_answers is a dictionary containing 'text' (answer), 'score', 'start_index' and 'end_index' (positions of the answer in the paragraph)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You will find below the functions that are needed for the implementation of the main function. The aim of these functions is to extract answers from a specific paragraph.\n\nFor the implementation of these functions we took the [SciBERT language representation model](https://arxiv.org/abs/1903.10676) and we fine tuned for QA using [SQuAD2.0](https://arxiv.org/abs/1806.03822) and [QuAC](https://arxiv.org/abs/1808.07036) datasets. We performed this fine tuning externally. Thus, we made this [model publicly available in Kaggle](https://www.kaggle.com/jonander95/bertsquadquac), and we just need to load it here."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the SciBERT model fine tuned for QA with SQuAD 2.0 and QuAC\ntokenizer = BertTokenizer.from_pretrained('/kaggle/input/bertsquadquac/checkpoint-42000/')\nbasic_tokenizer = BasicTokenizer(do_lower_case=False)\nmodel = BertForQuestionAnswering.from_pretrained('/kaggle/input/bertsquadquac/checkpoint-42000/')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Following the usual reading comprehension method we use BERT as a pointer network. This kind of networks select an answer start and end index given a question and a context. In order to extract the correct answer span we get the highest probability pairs of start and end indexes in the code below. As the input length for the BERT model is fixed, we use a sliding window approach for sequences that are longer than 384 subtokens. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input: Question, paragraph, maximum number of answers to extract, maximum length of the answer \ndef run_qa(question, context, nbest, max_answer_len):\n    #Simple sliding window approach for max context cases\n    tokenizer_dict = tokenizer.encode_plus(text=question, text_pair=context, max_length=384, stride=120,\n                                           return_overflowing_tokens=True, truncation_strategy='only_second')\n    input_ids = [tokenizer_dict['input_ids']]\n    input_type_ids = [tokenizer_dict['token_type_ids']]\n    \n    while 'overflowing_tokens' in tokenizer_dict.keys():\n        tokenizer_dict = tokenizer.encode_plus(text=tokenizer.encode(question, add_special_tokens=False), text_pair=tokenizer_dict['overflowing_tokens'], \n                                               max_length=384, stride=120, return_overflowing_tokens=True, truncation_strategy='only_second', \n                                               is_pretokenized=True, pad_to_max_length=True)\n        input_ids.append(tokenizer_dict['input_ids'])\n        input_type_ids.append(tokenizer_dict['token_type_ids'])    \n        \n    outputs = model(torch.tensor(input_ids), token_type_ids = torch.tensor(input_type_ids)) \n    answers = []\n    \n    for i in range(len(input_ids)):\n        start_logits, end_logits = [output[i].detach().cpu().tolist() for output in outputs] \n        answers += compute_predictions(start_logits, end_logits, input_ids[i], context.lower(), nbest, max_answer_len)\n    \n    answers.sort(key = lambda x: x['score'], reverse=True)\n    return answers[0:nbest]\n    # Output: List of dictionaries containing 'text' (answer), 'score', 'start_index' and 'end_index' (positions of the answer in the paragraph)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input: start and end logits for the model, ids, paragraph, maximum number of answers to extract, maximum length of the answer \ndef compute_predictions(start_logits, end_logits, input_ids, context, nbest, max_answer_length):\n    start_indexes = _get_best_indexes(start_logits, nbest + 10)\n    end_indexes = _get_best_indexes(end_logits, nbest + 10)\n    answers = []\n    for start_index in start_indexes:\n        for end_index in end_indexes:\n            #Avoid invalid predictions\n            answer_len = end_index - start_index + 1\n            if end_index < start_index:\n                continue\n            if max_answer_length < answer_len:\n                continue\n            text = tokenizer.decode(input_ids[start_index:start_index + answer_len], clean_up_tokenization_spaces=False)\n            try:\n                original_start_index = context.index(text)\n                original_end_index = original_start_index + len(text)\n            except:\n                #If there is any problem when looking for the answer in the text\n                #For example:\n                # System says answer in is question\n                # Or special tokens in answer [SEP] [PAD]\n                continue   \n            #When answer contains text and cannotanswer remove the cannotanswer part \n            if text != 'cannotanswer':\n                text = text.replace(' cannotanswer', '')\n            answer = {'text': text.capitalize(),\n                     'score': start_logits[start_index] + end_logits[end_index],\n                     'start_index': original_start_index,\n                     'end_index': original_end_index}\n            answers.append(answer)  \n    return answers\n    # Output: List of dictionaries containing 'text' (answer), 'score', 'start_index' and 'end_index' (positions of the answer in the paragraph)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Define questions for all the tasks<a class=\"anchor\" id=\"questions\"></a>\n\nIn this section we define the questions that will be used as an input for the QA system implemented in the previous section.\n\nAs some of the subquestions for each of the tasks defined by the organizers are too complex for the QA system, we refined them."},{"metadata":{"trusted":true},"cell_type":"code","source":"tasks = [\n    {\n        'task': \"Task1 - What is known about transmission, incubation, and environmental stability?\",\n        'questions': [\n            \"Range of incubation periods for the disease in humans\",\n            \"Range of incubation periods for the disease in humans depending on age\",\n            \"Range of incubation periods for the disease in humans depending on health status\",\n            \"How long individuals are contagious?\",\n            \"Prevalence of asymptomatic shedding and transmission\",\n            \"Prevalence of asymptomatic shedding and transmission in children\",\n            \"Seasonality of transmission\",\n            \"Charge distribution\",\n            \"Adhesion to hydrophilic/phobic surfaces\",\n            \"Environmental survival to inform decontamination efforts for affected areas\",\n            \"Viral shedding\",\n            \"Persistence and stability on nasal discharge\",\n            \"Persistence and stability on sputum\",\n            \"Persistence and stability on urine\",\n            \"Persistence and stability on fecal matter\",\n            \"Persistence and stability on blood\",\n            \"Persistence of virus on surfaces of different materials\",\n            \"Persistence of virus on copper\",\n            \"Persistence of virus on stainless steel\",\n            \"Persistence of virus on plastic\",\n            \"Natural history of the virus\",\n            \"Shedding the virus from an infected person\",\n            \"Implementation of diagnostics to improve clinical processes\",\n            \"Implementation of products to improve clinical processes\",\n            \"Disease models, including animal models for infection, disease and transmission\",\n            \"Tools to monitor phenotypic change and potential adaptation of the virus\",\n            \"Studies to monitor phenotypic change and potential adaptation of the virus\",\n            \"Immune response and immunity\",\n            \"Effectiveness of movement control strategies to prevent secondary transmission in health care and community settings\",\n            \"Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings\",\n            \"Role of the environment in transmission\"\n         ]\n    },\n    {\n        'task': \"Task 2 - What do we know about COVID-19 risk factors?\",\n        'questions': [\n            \"Which are the main risk factors?\",\n            \"Does smoking increase risk for COVID-19?\",\n            \"Is a pre-existing pulmonary disease a risk factor for COVID-19?\",\n            \"Do co-infections increase risk for COVID-19?\",\n            \"Does a respiratory or viral infection increase risk for COVID-19?\",\n            \"Are neonates at increased risk of COVID-19?\",\n            \"Are pregnant women at increased risk of COVID-19?\",\n            \"Is there any socio-economic factor associated with increased risk for COVID-19?\",\n            \"Is there any behavioral factor associated with increased risk for COVID-19?\",\n            \"What is the basic reproductive number?\",\n            \"What is the incubation period?\",\n            \"What are the modes of transmission?\",\n            \"What are the environmental factors?\",\n            \"Risk of fatality among symptomatic hospitalized patients\",\n            \"Risk of fatality among high-risk patient groups\",\n            \"Susceptibility of populations\",\n            \"Public health mitigation measures that could be effective for control\"\n        ]\n    },\n    {\n        'task': \"Task 3 - What do we know about virus genetics, origin, and evolution?\",\n        'questions': [\n            \"Real-time tracking of whole genomes to inform the development of diagnostics\",\n            \"Real-time tracking of whole genomes to inform the development of therapeutics\",\n            \"Real-time tracking of whole genomes to track variations of the virus over time\",\n            \"Mechanism for coordinating the rapid dissemination of whole genomes to inform the development of diagnostics\",\n            \"Mechanism for coordinating the rapid dissemination of whole genomes to inform the development of therapeutics\",\n            \"Mechanism for coordinating the rapid dissemination of whole genomes to track variations of the virus over time\",\n            \"Which geographic and temporal diverse sample sets are accessed to understand geographic differences?\",\n            \"Which geographic and temporal diverse sample sets are accessed to understand genomic differences?\",\n            \"Is there more than one strain in circulation?\",\n            \"Is any multi-lateral agreement leveraged such as the Nagoya Protocol?\",\n            \"Is there evidence that livestock could be infected and serve as a reservoir after the epidemic appears to be over?\",\n            \"Has there been any field surveillance to show that livestock could be infected?\",\n            \"Has there been any genetic sequencing to show that livestock could be infected?\",\n            \"Has there been any receptor binding to show that livestock could be infected?\",\n            \"Is there evidence that farmers are infected?\",\n            \"Is there evidence that farmers could have played a role in the origin?\",\n            \"What are the results of the surveillance of mixed wildlife-livestock farms for SARS-CoV-2 and other coronaviruses in Southeast Asia?\",\n            \"What are the results of the experimental infections to test host range for this pathogen?\",\n            \"Which are the animal hosts?\",\n            \"Is there evidence of continued spill-over to humans from animals?\",\n            \"Which are the socioeconomic and behavioral risk factors for the spill-over to humans from animals?\",\n            \"Sustainable risk reduction strategies\"\n        ]\n    },\n    {\n        'task': \"Task 4 - What do we know about vaccines and therapeutics?\",\n        'questions': [\n            \"What is known about the effectiveness of drugs being developed to treat COVID-19 patients?\",\n            \"What is known about the effectiveness of drugs tried to treat COVID-19 patients?\",\n            \"Show results of clinical and bench trials to investigate less common viral inhibitors against COVID-19\",\n            \"Show results of clinical and bench trials to investigate naproxen against COVID-19\",\n            \"Show results of clinical and bench trials to investigate clarithromycin against COVID-19\",\n            \"Show results of clinical and bench trials to investigate Minocyclinethat against COVID-19\",\n            \"Which are the methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients?\",\n            \"What is known about the use of best animal models and their predictive value for a human vaccine?\",\n            \"Capabilities to discover a therapeutic for the disease\",\n            \"Clinical effectiveness studies to discover therapeutics, to include antiviral agents\",\n            \"Which are the models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics?\",\n            \"Efforts targeted at a universal coronavirus vaccine\",\n            \"Efforts to develop animal models and standardize challenge studies\",\n            \"Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers\",\n            \"Approaches to evaluate risk for enhanced disease after vaccination\",\n            \"Assays to evaluate vaccine immune response\",\n            \"Process development for vaccines, alongside suitable animal models\"\n        ]\n    },\n    {\n        'task': \"Task 5 - What has been published about medical care?\",\n        'questions': [\n            \"Resources to support skilled nursing facilities\",\n            \"Resources to support long term care facilities\",\n            \"Mobilization of surge medical staff to address shortages in overwhelmed communities\",\n            \"Age-adjusted mortality data for Acute Respiratory Distress Syndrome (ARDS)\",\n            \"Age-adjusted mortality data for Acute Respiratory Distress Syndrome (ARDS) for viral etiologies\",\n            \"What are the outcomes of Extracorporeal membrane oxygenation (ECMO) of COVID-19 patients?\",\n            \"What are the outcomes for COVID-19 after mechanical ventilation adjusted for age?\",\n            \"What is known of the frequency, manifestations, and course of extrapulmonary manifestations of COVID-19?\",\n            \"What is known of the frequency, manifestations, and course of cardiomyopathy?\",\n            \"What is known of the frequency, manifestations, and course of cardiac arrest?\",\n            \"Application of regulatory standards (e.g., EUA, CLIA)\",\n            \"Ability to adapt care to crisis standards of care level\",\n            \"Approaches for encouraging and facilitating the production of elastomeric respirators, which can save thousands of N95 masks\",\n            \"Which are the best telemedicine practices?\",\n            \"Which are the facilitators to expand the telemedicine practices?\",\n            \"Which are the specific actions to expand the telemedicine practices?\",\n            \"Guidance on the simple things people can do at home to take care of sick people and manage disease\",\n            \"Which are the oral medications that might potentially work?\",\n            \"Use of artificial intelligence in real-time health care delivery to evaluate interventions\",\n            \"Use of artificial intelligence in real-time health care delivery to evaluate risk factors\",\n            \"Use of artificial intelligence in real-time health care delivery to evaluate outcomes\",\n            \"Which are the challenges, solutions and technologies in hospital flow and organization?\",\n            \"Which are the challenges, solutions and technologies in workforce protection?\",\n            \"Which are the challenges, solutions and technologies in workforce allocation?\",\n            \"Which are the challenges, solutions and technologies in community-based support resources?\",\n            \"Which are the challenges, solutions and technologies in payment?\",\n            \"Which are the challenges, solutions and technologies in supply chain management to enhance capacity, efficiency, and outcomes?\",\n            \"Efforts to define the natural history of disease to inform clinical care, public health interventions, infection prevention control, transmission, and clinical trials\",\n            \"What has been done to develop a core clinical outcome set to maximize usability of data across a range of trials?\",\n            \"Can adjunctive or supportive intervention (e.g. steroids, high flow oxygen)  improve the clinical outcomes of infected patients?\"\n        ]\n    },\n    {\n        'task': \"Task 6 - What do we know about non-pharmaceutical interventions?\",\n        'questions': [\n            \"Which is the best way to scale up NPIs in a more coordinated way to give us time to enhance our health care delivery system capacity to respond to an increase in cases?\",\n            \"Which is the best way to mobilize resources to geographic areas where critical shortfalls are identified?\",\n            \"Rapid design and execution of experiments to examine and compare NPIs currently being implemented\",\n            \"What is known about the efficacy of school closures?\",\n            \"What is known about the efficacy of travel bans?\",\n            \"What is known about the efficacy of bans on mass gatherings?\",\n            \"What is known about the efficacy of social distancing approaches?\",\n            \"Which are the methods to control the spread in communities?\",\n            \"Models of potential interventions to predict costs and benefits depending on race\",\n            \"Models of potential interventions to predict costs and benefits depending on income\",\n            \"Models of potential interventions to predict costs and benefits depending on disability\",\n            \"Models of potential interventions to predict costs and benefits depending on age\",\n            \"Models of potential interventions to predict costs and benefits depending on geographic location\",\n            \"Models of potential interventions to predict costs and benefits depending on immigration status\",\n            \"Models of potential interventions to predict costs and benefits depending on housing status\",\n            \"Models of potential interventions to predict costs and benefits depending on employment status\",\n            \"Models of potential interventions to predict costs and benefits depending on health insurance status\",\n            \"Policy changes necessary to enable the compliance of individuals with limited resources and the underserved with NPIs\",\n            \"Why do people fail to comply with public health advice?\",\n            \"Which is the economic impact of any pandemic?\",\n            \"How can we mitigate risks to critical government services in a pandemic?\",\n            \"Alternatives for food distribution and supplies in a pandemic\",\n            \"Alternatives for household supplies in a pandemic\",\n            \"Alternatives for health diagnoses, treatment, and needed care in a pandemic\"\n        ]\n    },\n    {\n        'task': \"Task 7 - What do we know about diagnostics and surveillance?\",\n        'questions': [\n            \"Which are the sampling methods to determine asymptomatic disease?\",\n            \"What can we do for early detection of disease?\",\n            \"Is the use of screening of neutralizing antibodies such as ELISAs valid for early detection of disease?\",\n            \"Which are the existing diagnostic platforms?\",\n            \"Which are the existing surveillance platforms?\",\n            \"Recruitment, support, and coordination of local expertise and capacity\",\n            \"How states might leverage universities and private laboratories for testing purposes?\",\n            \"Which are the best ways for communications to public health officials and the public?\",\n            \"What is the speed, accessibility, and accuracy of a point-of-care test?\",\n            \"What is the speed, accessibility, and accuracy of rapid bed-side tests?\",\n            \"Rapid design and execution of targeted surveillance experiments calling for all potential testers using PCR in a defined area to start testing and report to a specific entity\",\n            \"Separation of assay development issues from instruments\",\n            \"Which is the role of the private sector to help quickly migrate assays?\",\n            \"What has been done to track the evolution of the virus?\",\n            \"Latency issues and when there is sufficient viral load to detect the pathogen\",\n            \"What is needed in terms of biological and environmental sampling?\",\n            \"Use of diagnostics such as host response markers (e.g., cytokines) to detect early disease or predict severe disease progression\",\n            \"Policies and protocols for screening and testing\",\n            \"Policies to mitigate the effects on supplies associated with mass testing, including swabs and reagents\",\n            \"Technology roadmap for diagnostics\",\n            \"Which are the barriers to developing and scaling up new diagnostic tests?\",\n            \"How future coalition and accelerator models could provide critical funding for diagnostics?\",\n            \"How future coalition and accelerator models could provide critical funding for opportunities for a streamlined regulatory environment?\",\n            \"New platforms and technology (CRISPR) to improve response times\",\n            \"New platforms and technology to employ more holistic approaches\",\n            \"Coupling genomics and diagnostic testing on a large scale\",\n            \"What is needed for rapid sequencing and bioinformatics to target regions of the genome that will allow specificity for a particular variant?\",\n            \"What is needed for sequencing with advanced analytics for unknown pathogens?\",\n            \"What is needed for distinguishing naturally-occurring pathogens from intentional?\",\n            \"What is known about One Health surveillance of humans and potential sources of future spillover or ongoing exposure for this organism and future pathogens?\"\n        ]\n    },\n    {\n        'task': \"Task 8 - Help us understand how geography affects virality\",\n        'questions': [\n            \"Are there geographic variations in the rate of COVID-19 spread?\",\n            \"Are there geographic variations in the mortality rate of COVID-19?\",\n            \"Is there any evidence to suggest geographic based virus mutations?\"\n        ]\n    },\n    {\n        'task': \"Task 9 - What has been published about ethical and social science considerations?\",\n        'questions': [\n            \"Articulate and translate existing ethical principles and standards to salient issues in COVID-2019\",\n            \"Embed ethics across all thematic areas, engage with novel ethical issues that arise and coordinate to minimize duplication of oversight\",\n            \"Support sustained education, access, and capacity building in the area of ethics\",\n            \"Establish a team at WHO that will be integrated within multidisciplinary research and operational platforms and that will connect with existing and expanded global networks of social sciences\",\n            \"Develop qualitative assessment frameworks to systematically collect information related to local barriers and enablers for the uptake and adherence to public health measures for prevention and control\",\n            \"How the burden of responding to the outbreak and implementing public health measures affects the physical and psychological health of those providing care for Covid-19 patients?\",\n            \"Identify the underlying drivers of fear, anxiety and stigma that fuel misinformation and rumor, particularly through social media\"\n        ]\n    },\n    {\n        'task': \"Task 10 - What has been published about information sharing and inter-sectoral collaboration?\",\n        'questions': [\n            \"Which are the methods for coordinating data-gathering with standardized nomenclature?\",\n            \"Sharing response information among planners, providers, and others\",\n            \"Understanding and mitigating barriers to information-sharing\",\n            \"How to recruit, support, and coordinate local expertise and capacity relevant to public health emergency response?\",\n            \"Integration of federal/state/local public health surveillance systems\",\n            \"Value of investments in baseline public health response infrastructure preparedness\",\n            \"Modes of communicating with target high-risk populations (elderly, health care workers)\",\n            \"Risk communication and guidelines that are easy to understand and follow\",\n            \"Communication that indicates potential risk of disease to all population groups\",\n            \"Misunderstanding around containment and mitigation\",\n            \"Action plan to mitigate gaps and problems of inequity in the Nation’s public health capability, capacity, and funding to ensure all citizens in need are supported and can access information, surveillance, and treatment\",\n            \"Measures to reach marginalized and disadvantaged populations\",\n            \"Data systems and research priorities and agendas incorporate attention to the needs and circumstances of disadvantaged populations and underrepresented minorities\",\n            \"Mitigating threats to incarcerated people from COVID-19, assuring access to information, prevention, diagnosis, and treatment\",\n            \"Understanding coverage policies (barriers and opportunities) related to testing, treatment, and care\"\n        ]\n    }\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Show results for a task<a class=\"anchor\" id=\"results\"></a>\n\nIn this last section, we want to show the results for the task. For that purpose, we will run the above functions to first retrieve relevant paragraphs from the papers, and then extract specific answers from them. \n\nWe set to 20 the maximum number of paragraphs that the IR system returns, but we discard paragraphs where the QA system returns “I don’t know”. Moreover, we decided not to show any results for the questions which receive more than %85 of “I don’t know” answers. For the rest of the questions, we show the best answer string for each of the best five paragraphs, that is, five specific answers per question. Additionally, next to each answer, we show some extra information: the title of the paper from where the answer was extracted (with a link to access online version on the web), the journal and the date of the publication. Moreover, under the answer we show the paragraph from which the answer was extracted. In this paragraph the best 3 answers are highlighted, using different lightness of color (the darker the better the answer)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creates the HTML code to show all the answers colored gradually in the paragraph\ndef color_snippet(text,marks):\n    # Set colors for answers\n    colors = ['#ffebcc', '#ffc266','#ff9900']\n    \n    # Create HTML code to show the colored paragraph\n    html = '<blockquote>'\n    current_mark = 0\n    for i,mark in enumerate(marks):\n        if current_mark != mark:\n            if current_mark != 0:\n                html += '</span>'\n            if mark > 0:\n                html += '<span style=\"background-color: {}\">'.format(colors[mark-1])\n            current_mark = mark\n        html += text[i]\n    if current_mark != 0:\n        html += '</span>'\n    html += '</blockquote>' \n    return html\n\n\n# Set number of this task\nntask = 4\n\n# Show title of the task\ntask_title = tasks[ntask-1]['task']\nhtml = html = \"<p><h1>\" + task_title + \"</h1></p><br>\"\n\n# Set input parameters of the functions above\n# Maximum number of documents to retrieve\nmax_n_docs = 20\n# Maximum number of answers to extract\nmax_n_answers = 3\n# Maximum answer length\nmax_answer_length = 30\n# Amount of Cannotanswers to declare answers as not suitable\nthreshold = 17\n\n# Iterate over all the questions in a task and call the functions above\nfor nq,question in enumerate(tasks[ntask-1]['questions']):\n    # Call the function to retrieve relevant paragraphs of papers\n    df_ir_results = retrieve_docs(question, df_mdata, max_n_docs)\n    # Call the function to extract answers from paragraphs\n    df_qa_results = extract_answers(question, df_ir_results, max_n_answers, max_answer_length)\n\n    # Show the question\n    html += '<br><p><font color=\"#683E00\"><h2>{}</h2></font>'.format(question)\n    \n    # Count how many non-null answers are extracted for a question\n    n_cannotanswer = 0\n    for ind in df_qa_results.index:\n        answer = df_qa_results['qa_answers'][ind][0] \n        #Take SQuAD and QuAC cases into account\n        if answer['text'] == 'Cannotanswer' or len(answer['text'])==0:\n            n_cannotanswer += 1\n            \n    if n_cannotanswer < threshold:\n        # Set maximum number of results to show\n        max_n_results = 5\n        n_results = 0\n        for ind in df_qa_results.index:\n            if n_results == max_n_results:\n                break\n            answers = df_qa_results['qa_answers'][ind]\n            # If the first answer is non-null, show the answer\n            #if answers[0]['text'] != 'CANNOTANSWER':\n            \n            if answers[0]['text'] != 'Cannotanswer' and len(answers[0]['text']) != 0:\n                answer_string = answers[0]['text']\n                html += '<br><b>{}</b> <span style=\"background-color: #dddddd\"> [{}, <i>{}</i>, {}]</span><br>'.format(answer_string, df_qa_results['title'][ind], df_qa_results['journal'][ind], df_qa_results['date'][ind])\n            \n                # Color the paragraph to highlight the answers\n                marks = [0] * len(df_qa_results['text'][ind])\n               \n                for n_ans, answer in enumerate(answers):\n                    if answer['text'] != 'Cannotanswer':\n                        level = max_n_answers - n_ans\n                        start = answer['start_index']\n                        if answer['end_index'] >= len(marks):\n                            end = len(marks)-1\n                        else:\n                            end = answer['end_index']\n                       \n                        for i in range(start,end):\n                            if marks[i] < level:\n                                marks[i] = level\n                html += color_snippet(df_qa_results['text'][ind], marks)\n                n_results += 1        \n        html += '<hr>'\n    else:\n        html += '<br><i>No suitable answers found.</i><br>'\n        html += '<hr>'\n    \n\n# Display the HTML string that contains all the answers\ndisplay(HTML(html))\n\n# Save the HTML code of the answers into a file\nif not os.path.exists(\"html\"):\n    os.mkdir(\"html\")\nhtml_file = codecs.open(\"/kaggle/working/html/task\" + str(ntask) + \".html\",\"w\",\"utf-8\")\nhtml_file.write(html)\nhtml_file.close()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}