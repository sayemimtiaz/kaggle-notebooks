{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Table of content\n\n1. [Introduction](#Introduction)\n2. [Data overview](#Data-overview)\n3. [Data wrangling](#Data-wrangling)\n4. [Factor analysis](#Factor-analysis)\n    1. [Factor interpretation](#Factor-interpretation)\n5. [Exploratory analysis](#Exploratory-analysis) \n    1. [Box-Cox power transformation](#Box-Cox-power-transformation)\n    2. [Linearity and normality](#Linearity-and-normality)\n6. [Multiple linear regression](#Multiple-linear-regression)\n    1. [Model interpretation](#Model-interpretation)\n7. [Final remarks](#Final-remarks)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nFor this analysis, my research question would be what factors are associated with COVID-19 infection rates in the United States counties. I will be utilizing various datasets concerning county health information, county COVID-19 infection rates, county population densities, and state political affiliation. In turn, I will attempt to identify and evaluate risk factors connected to infection rates by doing multiple regression analysis. The resultant linear regression model would be used mainly for interpretation. The outcome of the study could be used to better understand a population's vulnerability to COVID-19 based on the community's characteristics from the reference point of the American counties.  ","execution_count":null},{"metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"# Kaggle specific code\n\n!pip install factor_analyzer\n\n# https://www.kaggle.com/general/63534#672910\n!pip install altair vega_datasets notebook vega # needs internet in settings (right panel)\n\n# https://www.kaggle.com/jakevdp/altair-kaggle-renderer\n# Define and register a kaggle renderer for Altair\n\nimport json\nimport altair as alt\nfrom IPython.display import HTML\n\nKAGGLE_HTML_TEMPLATE = \"\"\"\n<style>\n.vega-actions a {{\n    margin-right: 12px;\n    color: #757575;\n    font-weight: normal;\n    font-size: 13px;\n}}\n.error {{\n    color: red;\n}}\n</style>\n<div id=\"{output_div}\"></div>\n<script>\nrequirejs.config({{\n    \"paths\": {{\n        \"vega\": \"{base_url}/vega@{vega_version}?noext\",\n        \"vega-lib\": \"{base_url}/vega-lib?noext\",\n        \"vega-lite\": \"{base_url}/vega-lite@{vegalite_version}?noext\",\n        \"vega-embed\": \"{base_url}/vega-embed@{vegaembed_version}?noext\",\n    }}\n}});\nfunction showError(el, error){{\n    el.innerHTML = ('<div class=\"error\">'\n                    + '<p>JavaScript Error: ' + error.message + '</p>'\n                    + \"<p>This usually means there's a typo in your chart specification. \"\n                    + \"See the javascript console for the full traceback.</p>\"\n                    + '</div>');\n    throw error;\n}}\nrequire([\"vega-embed\"], function(vegaEmbed) {{\n    const spec = {spec};\n    const embed_opt = {embed_opt};\n    const el = document.getElementById('{output_div}');\n    vegaEmbed(\"#{output_div}\", spec, embed_opt)\n      .catch(error => showError(el, error));\n}});\n</script>\n\"\"\"\n\nclass KaggleHtml(object):\n    def __init__(self, base_url='https://cdn.jsdelivr.net/npm'):\n        self.chart_count = 0\n        self.base_url = base_url\n        \n    @property\n    def output_div(self):\n        return \"vega-chart-{}\".format(self.chart_count)\n        \n    def __call__(self, spec, embed_options=None, json_kwds=None):\n        # we need to increment the div, because all charts live in the same document\n        self.chart_count += 1\n        embed_options = embed_options or {}\n        json_kwds = json_kwds or {}\n        html = KAGGLE_HTML_TEMPLATE.format(\n            spec=json.dumps(spec, **json_kwds),\n            embed_opt=json.dumps(embed_options),\n            output_div=self.output_div,\n            base_url=self.base_url,\n            vega_version=alt.VEGA_VERSION,\n            vegalite_version=alt.VEGALITE_VERSION,\n            vegaembed_version=alt.VEGAEMBED_VERSION\n        )\n        return {\"text/html\": html}\n    \nalt.renderers.register('kaggle', KaggleHtml())\nprint(\"Define and register the kaggle renderer. Enable with\\n\\n\"\n      \"    alt.renderers.enable('kaggle')\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"alt.renderers.enable('kaggle')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import necessary libraries \n\nimport pandas as pd\nimport numpy as np \nimport altair as alt\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm  \nfrom factor_analyzer import FactorAnalyzer\nimport matplotlib.pyplot as plt\nfrom factor_analyzer.factor_analyzer import calculate_kmo\nfrom scipy import stats\nimport itertools\nfrom statsmodels.graphics.gofplots import qqplot\nfrom matplotlib import pyplot as plt\nfrom scipy.stats import levene, normaltest","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data overview\n\nLet's take a look at the datasets this analysis will be using. A few of them  are put together by crawling wiki pages. The rest are from what are provided officially.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://github.com/nytimes/covid-19-data\n# Cumulative counts of coronavirus cases in the US at the county level\ncounty_infection = pd.read_csv('../input/county-covid-related/us-counties.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"county_infection.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's sort the counties first by date\n\ncounty_infection['date'] = pd.to_datetime(county_infection['date'])\ncounty_infection = county_infection.sort_values(by='date')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"county_infection.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> The latest date of the data is May 13th, 2020.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"county_infection[(county_infection['state'] == 'Illinois') & (county_infection['county'] == 'Cook')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"county_infection[(county_infection['state'] == 'California') & (county_infection['county'] == 'Santa Clara')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> It seems like each county's data starts with the first case of infection and then contains each subsequent day's cumulative count.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://en.wikipedia.org/wiki/County_(United_States)\n# County population and density\ncounty_population = pd.read_csv('../input/county-covid-related/county-population.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"county_population.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://en.wikipedia.org/wiki/Political_party_strength_in_U.S._states\n# https://en.wikipedia.org/wiki/List_of_United_States_governors\n# State party affiliation based on house representation\nstate_party_line = pd.read_csv('../input/county-covid-related/state_party_line.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"state_party_line.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://www.countyhealthrankings.org/\n# Access: https://app.namara.io/#/data_sets/579ee1c6-8f66-418c-9df9-d7b5b618c774?organizationId=5ea77ea08fb3bf000c9879a1\n# County health information\ncounty_health = pd.read_csv('../input/uncover/UNCOVER/county_health_rankings/county_health_rankings/us-county-health-rankings-2020.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"county_health.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"county_health.columns[:75]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> This dataset contains extensive information about a county's attributes, including the rankings, quantiles, rates, and percentages of numerous demographic as well as health qualities. Of the many measurements of each quality, we probably only need one or two to avoid duplication. In addition, I will do a **factor analysis** on the columns to see if it makes sense.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For more information about these columns, please visit this [info](https://app.namara.io/#/data_sets/579ee1c6-8f66-418c-9df9-d7b5b618c774/info?organizationId=5ea77ea08fb3bf000c9879a1) page","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data wrangling\n\nIn this section, we want to prepare our data for further exploration and analysis. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Aggregate data related to county infection and basic characteristics\ncounty = county_infection.merge(\n    county_population, left_on=['county', 'state'], right_on=['county', 'state']\n).merge(\n    state_party_line, left_on=['state'], right_on=['state']\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"county.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the statistics of the counted days for the counties","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count the number of days each county data has\ndef count_days(series):\n    time_series = pd.to_datetime(series)\n    first_date = time_series.iloc[0]\n    last_date = time_series.iloc[-1]\n    \n    return (last_date - first_date).days + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped_county = county.groupby(['state', 'county']).agg(days_counted=('date', count_days))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped_county.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped_county.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We have 2758 counties in the data. The minimum amount of days counted for a county is only one, while the maximum is about almost four months. I am happy that the median is 50 days. Ideally, I want all counties in the analysis to have at least two months worth of data so that any of its heath characteristics can have a decent chance of exerting its influence if there could be any at all. With the current data and analysis, I will only include counties with 50 day worth of data to maximize the representativeness of the eventual infection picture and not exclude too much data. Please understand that I'm not a domain expert. I apologize that this cutoff point seems rather arbitrary, but I hope the rationale makes sense domain-wise.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"With that said, for the next step, we want to group the infection data by counties and create a bunch of aggregated columns including counted days, confirmed infection in the percentage of county population, death rate, and raw infection counts. We will also calculate those columns for the cutoff point of 50 days so that we can do the analysis without accounting for the number of days for model simplicity. This is also where we will exclude counties that have less than 50 days of data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the value at the 50 day mark\ndef county_cumulative_days(series, days = 50):\n    # This may not be 100% accurate because perhaps some days are missing, \n    # but that seems to happen rarely. So this should be accurate enough.\n    if len(series) < days:\n        return series.iloc[-1]\n    else:\n        return series.iloc[days - 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group our data in terms of county and aggregate some columns to show overall infection rate \n# and death rate as well as at the 50 day mark\ndef group_county_data(data):\n    grouped_data = data.groupby(['state', 'county']).agg(\n        population=('population', lambda x: x.iloc[-1]),\n        density_km=('density_km', lambda x: x.iloc[-1]),\n        state_house_blue_perc=('state_house_blue_perc', lambda x: x.iloc[-1]),\n        state_governor_party=('state_governor_party', lambda x: x.iloc[-1]),\n        days_counted=('date', count_days),\n        case_sum=('cases', lambda x: x.iloc[-1]),\n        death_sum=('deaths', lambda x: x.iloc[-1]),\n        case_count_50_days=('cases', county_cumulative_days),\n        death_count_50_days=('deaths', county_cumulative_days)\n    )\n    \n    grouped_data = grouped_data[grouped_data['days_counted'] >= 50]\n    grouped_data['infection_rate'] = grouped_data['case_sum']/grouped_data['population']*100\n    grouped_data['death_rate'] = grouped_data['death_sum']/grouped_data['case_sum']*100\n    grouped_data = grouped_data[grouped_data['infection_rate'] != float(\"inf\")]\n    grouped_data['infection_rate_50_days'] = grouped_data['case_count_50_days']/grouped_data['population']*100\n    grouped_data['death_rate_50_days'] = grouped_data['death_count_50_days']/grouped_data['case_count_50_days']*100\n    \n    return grouped_data.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped_county = group_county_data(county)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped_county","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We end up with 1463 counties.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped_county.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, let's tackle county health data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove state total rows first\ncounty_health = county_health.dropna(subset=['county'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Take a quick look over the data again.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"county_health.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"county_health.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"county_health.columns[:100]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> There are 507 columns. To reiterate my proposed course of action, we want to first get rid of many different measurements of the same quality and only keep the rates. We also want to remove some redundant columns such as population. The purpose is to hopefully keep the complexity under a managable level, while maintaining the values of the information.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"excluded_column_words = [\n    'quartile',\n    'ci_high',\n    'ci_low',\n    'fips',\n    'num',\n    'denominator',\n    'ratio',\n    'population',\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filtered_columns = county_health.columns[~county_health.columns.str.contains('|'.join(excluded_column_words))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(str(len(filtered_columns)) + ' columns remain!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filtered_county_health = county_health[filtered_columns]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, let's merge the health data into the infection data, and check out the merged data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"county = grouped_county.merge(\n    filtered_county_health, left_on=['county', 'state'], right_on=['county', 'state']\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"county","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We still have a lot of columns. Perhaps a lot of them have missing data for more than half of the data. We have no reasonable and accessible way of dealing with missing data here. We could fill in missing values from nearby counties, but that could be both erroneous and difficult. As a result, we will simply get rid of missing data in terms of columns and rows. Let's deal with columns first because we want to keep as many as rows as possible.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the columns at near 90% cutoff points\ncounty.dropna(thresh=1300, axis=1).info(max_cols=200)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> At the 90% row number cutoff point, we have a decent amount of columns. Most of the columns seem important, so we will try to keep most of them by setting the cutoff point at 1370 rows to keep the indexes related to suicide.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"county.dropna(thresh=1370, axis=1).dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are keeping a good amount of data. Let's go ahead with that decision.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"county = county.dropna(thresh=1370, axis=1).dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Factor analysis\n\nAfter data wrangling, we are still dealing with a large number of columns. If we continue with our anaylsis as is, it might suffer from the curse of dimensionality. Also, if we are to include interaction terms, the number of parameters could get close to the number of rows. Furthermore, there is a high chance that we will run into multicollinearity. For all these reaons, I have decided to run factor anaylsis as the next step to reduce dimensionality and find independant latent variables. Please refer to its [wiki](https://en.wikipedia.org/wiki/Factor_analysis) for more information on the technique itself.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exclude columns that won't be used as explanatory variables and can't used in factor analysis\nexcluded_columns = [\n    'state',\n    'county', \n    'population',\n    'state_house_blue_perc',\n    'state_governor_party',\n    'days_counted', \n    'case_sum', \n    'death_sum', \n    'case_count_50_days',\n    'death_count_50_days', \n    'infection_rate', \n    'death_rate',\n    'infection_rate_50_days', \n    'death_rate_50_days',\n    'presence_of_water_violation'\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"county_non_factor = county[excluded_columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"county_factor = county.drop(excluded_columns, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(county_factor.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"county_factor.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check whether factor anaylsis is appropriate first. We will be using [Leveneâ€™s test](https://en.wikipedia.org/wiki/Levene%27s_test) and [Kaiser-Meyer-Olkin Test](https://www.statisticshowto.com/kaiser-meyer-olkin/). The former is used to assess whether or not the variables have homoscedasticity for samples that might not have perfectly normal distributions. The latter measures the suitability of data for factor analysis.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To assess which function of the data to use in the Levene test, we need to look at the normality of the columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = county_factor.hist(\n    column=county_factor.columns, \n    xlabelsize=0.1, \n    ylabelsize=0.1, \n    layout=(11, 7), \n    figsize=(10, 10),\n    bins=50\n)  \n[x.title.set_size(0) for x in fig.ravel()]\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Seems like most columns have tailed distributions. Only a few have non-normal distributions. As a result, we will look at the test results for both `mean` and `trimmed` functions. I do acknowledge that there seems to be no perfect test to account for the variety of distributions here.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"levene(*county_factor.to_numpy(), center='trimmed')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"levene(*county_factor.to_numpy(), center='mean')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> The tests were both statistically significant, indicating that there is likely no homoscedasticity among the variables. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"kmo_all, kmo_model = calculate_kmo(county_factor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmo_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> This score indicates that the data is excellent for factor analysis.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's check out all the original eigenvalues first.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fa = FactorAnalyzer()\n\n# Using the varimax rotation because it makes it easier to identify each variable with a single factor.\nfa.set_params(rotation='varimax')\nfa.fit(county_factor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ev, v = fa.get_eigenvalues()\nev[:30]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(range(1, len(ev)+1), ev)\nplt.plot(range(1, len(ev)+1), ev)\nplt.title('Scree plot')\nplt.xlabel('Factor')\nplt.ylabel('Eigenvalue')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> It seems like we have 14 factors that are significant (eigenvalue >= 1)..","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fa = FactorAnalyzer()\nfa.set_params(n_factors=14, rotation='varimax')\nfa.fit(county_factor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"factor_loading = pd.DataFrame(fa.loadings_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"factor_loading.index = county_factor.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"factor_loading.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"factor_loading","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Factor interpretation\n\nNow that we have collected all the significant factors, let's interpret them one by one. We will look at columns that have decent loadings(>|.3|) for easier interpretations.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def filter_decent_loadings(factor):\n    return factor[(factor > 0.3) | (factor < -0.3)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for factor in factor_loading.columns:\n    print('Factor ' + str(factor + 1) + ' loadings: ')\n    print()\n    print(filter_decent_loadings(factor_loading[factor]))\n    print()\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. The first factor seems to encompass a lot of indexes related to general well-being. Some of its most substantial positive loadings(> .8) are for the years of potential life lost rate, the percentage of fair or poor health, the percentage of smokers, the percentage of physical or mental distress, and child poverty percentage. Some of its most substantial negative loadings(< -.7) are for median household income, life expectancy, general income, and college percentage. It explains a lot of variables connected to welfare in high coefficients. And its positive direction is towards poor welfare. Based on the loadings, I can identify this factor as **poor general well-being** with decent confidence.\n\n2. The second factor has apparent connections to variables related to housing issues. Its biggest loadings(> .8) are for the percentage of severe housing problems, severe housing cost burden, and the percentage of severe housing cost burden. The percentage of homeowners has the lowest loadings for this factor. It is convincing that this factor is for **housing burden**.\n\n3. The third factor seems to be connected to the prevalence of hispanic population. Its most substantial positive loadings(> 0.8) are for the percentage of hispanic population and the percentage of people not proficient in English. Its most vigorous negative factor loading is for the percentage of non-hispanic white population(~-.52). Other variables, such as housing problems and youth population, with lower factor loadings also seem to make sense for hispanic population prevalence. As a result, I would determine this factor to be **hispanic relative population size**.  \n\n4. The fourth factor is mostly about suicide rates in the opposite direction, so we could interpret this to be **inverse suicide rate**.\n\n5. The fifth factor is for **uninsured rate** because that's all its concerns with high loadings.\n\n6. The sixth factor has mostly to do with care provider rates such as dentist and mental health(> .5). It seems to be inversely connected with rural percentage and long commute. Although its loadings are relatively weaker, we can probably conclude that it is for **care provider accessibility**.\n\n7. The seventh factor seems to be about the population age as the extreme youth percentage has a positive loading(> .55), and the senior percentage has a very negative loading(< -.86). We can somewhat conclude that this factor is for **population youth**.\n\n8. The eighth factor seems to be mostly about the crime rate and its contributing factors, so we will determine this as **crime risk**.\n\n9. The ninth factor should have weak loadings overall. The theme seems to be about the overall income as it includes median household income, 80th & 20th percentile income, long commute, and white household income. We will loosely define this factor to be about **overall income level**.\n\n10. The tenth factor seems to be highly related to population density. Its highest loadings are for density in km(\\~.66), traffic volumn(\\~.4), and Asian population(\\~.48), while its lowest loadings are related to lone drive to work. It is clear that this factor is about **population density level**.\n\n11. The ninth factor should be somewhat apparent, with its two biggest loadings being the percentage of American Indian Alaska Native(.75) and inadequate kitchen or plumbing facilities(.71). We will determine this factor to be about **native relative population size**.\n\n12. The twelveth factor seems to be mainly connected to black population. Its most significant loading is for the percentage of black population(~.44). A lot of its other loadings are seemingly problems more common in black communities. Some examples are low high school graduation rate, crime rate, and single parent households. One of its negative loadings is the percentage of non-hispanic white population. I think we can determine that this factor is for **black relative population size**.\n\n13. The thirteenth factor seems to be about urbanization with its most negative loading being about rural percentage and its most positive one about access to exercise opportunities. The [latter](https://www.countyhealthrankings.org/explore-health-rankings/measures-data-sources/county-health-rankings-model/health-factors/health-behaviors/diet-exercise/access-to-exercise-opportunities) is defined to be specifically about facilities, which are more plentiful in an urban environment. As a result, we will determine this factor to be about **urbanization level**.\n\n14. The fourteenth factor is also straightforward as its loadings are all about food. We will determine it to be about **poor food environment index** due to its related variables and their directions.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> Please note that these interpreations are purely subjective and could be done better with more domain knowledge.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fa.get_factor_variance()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Together the 14 factors explain about 74% of the total variance.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now with the factors interpreted, let's transform the original columns into factor scores, and append `_fa_score` to the factor names. Then, we will merge the data back.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fa_score_columns = [\n    'poor_general_wellbeing_fa_score',\n    'housing_burden_fa_score',\n    'hispanic_relative_population_fa_score',\n    'inverse_sucicde_rate_fa_score',\n    'uninsured_rate_fa_score',\n    'care_provider_accessibility_fa_score',\n    'population_youth_fa_score',\n    'crime_risk_fa_score',\n    'overall_income_fa_score',\n    'population_density_fa_score',\n    'native_relative_population_fa_score',\n    'black_relative_population_fa_score',\n    'urbanization_level_fa_score',\n    'poor_food_environment_fa_score',\n]\n\ntransformed_county_factor = pd.DataFrame(\n    fa.transform(county_factor),\n    columns=fa_score_columns\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"county = county_non_factor.reset_index(drop=True).join(transformed_county_factor)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory analysis\n\nIn this section, we want to explore some factors' distribution and their relationships with the response variable. We will also compare the counties by their state governor parties. For the rest of the analysis, we will set the response variable as the infection rate at 50 days since the first case of a county.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove some columns we are interested in for sure\ncounty = county.drop(columns=[\n    'population', \n    'state_house_blue_perc', \n    'days_counted',\n    'case_sum',\n    'death_sum',\n    'case_count_50_days',\n    'death_count_50_days',\n    'presence_of_water_violation'\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"county.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"county.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alt.Chart(county).mark_bar().encode(\n    alt.X(\"infection_rate_50_days\", bin=alt.Bin(extent=[0, 3], step=0.02)),\n    y='count()',\n).properties(\n    width=800,\n    height=400,\n    title='Infection rate at 50 days since first case'\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> It looks like we have a right skewness for the response variable distribution. Alternatively and maybe more accurately, we are looking at a Gamma distribution here.  Most counties' infection rates seem to lie below 0.8.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Box-Cox power transformation\n\nIn any case, we will fix the skewness and achieve a normal distribution by transforming the response variable as opposed to building a generalized linear model with Gamma distribution for easier interpretation and better intuition, from my perspective. We will use the [Box-Cox](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html) power transformation because it tends to be more powerful than log transformation and taking the root in solving skewness.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"infection_rate_50_days_boxcox, lmbda = stats.boxcox(county['infection_rate_50_days'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lmbda","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let the transformed rate be `x` and the old rate be `y`. \n\nThe formula of their relationship is `(lmbda * x + 1)^(1/lambda) = y`, which in this case is approximately `(0.044x + 1)^22.73 = y`. Let's visualize what that means.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"county['infection_rate_50_days_boxcox'] = infection_rate_50_days_boxcox","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alt.Chart(county).mark_line().encode(\n    x='infection_rate_50_days_boxcox',\n    y='infection_rate_50_days'\n).properties(\n    title='Infection rate Boxcox transformation relationship'\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Basically, as the Box-Cox transformed infection rate increase, the real infection rate increases exponentially, meaning the coefficients have exponential impacts on the real infection rate.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"alt.Chart(county).mark_bar().encode(\n    alt.X(\"infection_rate_50_days_boxcox\", bin=alt.Bin(extent=[-5, 2], step=0.1)),\n    y='count()',\n).properties(\n    width=800,\n    height=400,\n    title='Boxcox infection rate at 50 days since first case'\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> The new response variable's distribution looks normal.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Next, let's look at the correlatons with the transformed rate.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"county[county['state_governor_party'] == 'blue'].corr(method='pearson')['infection_rate_50_days_boxcox']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> For a county in a blue state, the more prominent positively correlated factors(>.25) are **urbanization**, **income level**, and **inverse suicide**. The more prominent negatively correlated factors are **crime**(-.2) and **care provider accessibility**(-.1).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"county[county['state_governor_party'] == 'red'].corr(method='pearson')['infection_rate_50_days_boxcox']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> For a county in a red state, the more prominent positively correlated factors(>.2) are **black population** and **inverse suicide**. The more prominent negatively correlated factors are **crime**(-.13) and **hispanic population**(-.11).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"county.corr(method='pearson')['infection_rate_50_days_boxcox']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Overall, the more prominent positively correlated factors(>.2) are **income level** and **inverse suicide**. The more prominent negatively correlated factors are **crime**(-.18) and **care provider accessibility**(-.085).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Linearity and normality\n\nNext, we will explore some of the more prominent explanatory variables and visualize their relationships with the infection rate as well as their distributions. We will look at common relatively significant and positively correlated explanatory variables first.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"alt_y = alt.Y(\n    'infection_rate_50_days_boxcox', \n    axis=alt.Axis(values=list(np.linspace(-6, 2, 81))),\n    scale=alt.Scale(domain=(-5, 2), clamp=True)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alt.Chart(county).mark_point(filled=True, size=22).encode(\n    x='inverse_sucicde_rate_fa_score',\n    y=alt_y,\n    color='state_governor_party'\n).properties(\n    width=800,\n    height=400,\n    title='Inverse suicide factor score vs Boxcox infection rate'\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alt.Chart(county).mark_bar().encode(\n    alt.X(\"inverse_sucicde_rate_fa_score\", bin=alt.Bin(extent=[-3, 3], step=0.1)),\n    y='count()',\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alt.Chart(county).mark_point(filled=True, size=22).encode(\n    x='overall_income_fa_score',\n    y=alt_y,\n    color='state_governor_party'\n).properties(\n    width=800,\n    height=400,\n    title='Overall income factor score vs Boxcox infection rate'\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alt.Chart(county).mark_bar().encode(\n    alt.X(\"overall_income_fa_score\", bin=alt.Bin(extent=[-3, 3], step=0.1)),\n    y='count()',\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> As expected, though normally distributed, the above factor scores, with relatively high positive coefficients regardless of state party, have weak positive linearity relationships with the transformed infection rate.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will look at some positively correlated explanatory variables that are relatively significant only to one party next.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"alt.Chart(county).mark_point(filled=True, size=22).encode(\n    x='urbanization_level_fa_score',\n    y=alt_y,\n    color='state_governor_party'\n).properties(\n    width=800,\n    height=400,\n    title='Urbanization factor score vs Boxcox infection rate'\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Blue counties exhibit stronger linearity here.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"alt.Chart(county).mark_bar().encode(\n    alt.X(\"urbanization_level_fa_score\", bin=alt.Bin(extent=[-3, 3], step=0.1)),\n    y='count()',\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alt.Chart(county).mark_point(filled=True, size=22).encode(\n    x='black_relative_population_fa_score',\n    y=alt_y,\n    color='state_governor_party'\n).properties(\n    width=800,\n    height=400,\n    title='Black population factor score vs Boxcox infection rate'\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Red counties exhibit slightly stronger linearity here.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"alt.Chart(county).mark_bar().encode(\n    alt.X(\"black_relative_population_fa_score\", bin=alt.Bin(extent=[-3, 3], step=0.1)),\n    y='count()',\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Here we are looking at two factors that have different effects on counties in states with different governor parties. The urbanization factor has a pretty obvious but weak linearity relationship with the transformed infection rate in blue counties, but that relationship cannot be found with red counties. On the other hand, the black population factor has a very weak linearity relationship with the transformed infection rate only for counties in red states. Overall, these relationships are difficult to spot because they are not especially strong.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In conclusion, although linearity seems to be weak, it does exist for some explanatary variables with the response variable so that we can be confident on finding a somewhat useful linear equation. We can be fairly assured that decent normality is ensured and multicollinearity is alleviated with the factors. Moreover, based on the relationship graphs, we see no obvious pattern with the residuals, so we can be somewhat confident with homoscedasticity as well. With that said, we can proceed with the regression.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Multiple linear regression\n\nFirst, We will devise all combinations of model formulas and build them. Then, we will identify and explore the models with the highest adjusted R-squared, lowest AIC, and BIC scores. Finally, we will evaluate assumptions again and interpret the models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"interaction_term = 'state_governor_party'\nresponse_variable = 'infection_rate_50_days_boxcox'\nexplanatory_variables = fa_score_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explanatory_variables","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"variable_combinations = []\n\nfor variable in explanatory_variables:\n    variable_combinations.append([variable, variable + '*' + interaction_term])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"formula_combinations = list(itertools.product(*variable_combinations))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There are ' + str(len(formula_combinations)) + ' combinations.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"models = []\nrsquared_adjs = []\nformulas = []\naics = []\nbics = []\n\nfor combo in formula_combinations:\n    explanatory_variable_part = ' + '.join(combo)\n    formula = ' '.join([\n        'infection_rate_50_days_boxcox ~',\n        explanatory_variable_part\n    ])\n    \n    mod = smf.ols(formula=formula, data=county)\n    res = mod.fit()\n\n    models.append(res)\n    formulas.append(formula)\n    rsquared_adjs.append(res.rsquared_adj)\n    aics.append(res.aic)\n    bics.append(res.bic)\n    \n    if len(models)%1600 == 0:\n        print(str(len(models)) + ' models finished so far!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = pd.DataFrame({\n    'formula': formulas,\n    'rsquared_adj': rsquared_adjs,\n    'aic': aics,\n    'bic': bics,\n    'model': models\n})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.sort_values(by='rsquared_adj', ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.sort_values(by='aic').head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.sort_values(by='bic').head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Looks like optimizing for AIC and adjusted R-squared gives the same model, while minimizing BIC results in a different model. We will focus on these two models for the rest of the analysis.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"aic_res = result.iloc[result.sort_values(by='aic').iloc[0].name]['model']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bic_res = result.iloc[result.sort_values(by='bic').iloc[0].name]['model']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we will confirm whether the two models' residuals look random and are somewhat normally distributed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_df = pd.DataFrame({\n    'aic_model_residual': county['infection_rate_50_days_boxcox'].values - aic_res.fittedvalues,\n    'bic_model_residual': county['infection_rate_50_days_boxcox'].values - bic_res.fittedvalues,\n    'real_val': county['infection_rate_50_days_boxcox'],\n    'aic_model_pred': aic_res.fittedvalues,\n    'bic_model_pred': bic_res.fittedvalues,\n})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# QQ plot for the AIC model residuals\nplt.show(qqplot(model_df['aic_model_residual'], line='s'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# QQ plot for the BIC model residuals\nplt.show(qqplot(model_df['bic_model_residual'], line='s'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alt.Chart(model_df).mark_point().encode(\n    x='aic_model_pred',\n    y='aic_model_residual',\n) | alt.Chart(model_df).mark_point().encode(\n    x='bic_model_pred',\n    y='bic_model_residual',\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> The QQ plots would indicate that our residuals are normal, but their distributions do have slight heavy right tails. I think this would indicate that when the models underestimate, they tend to underestimate more compared to overestimation. The residual plots seem to display no obvious pattern.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Model interpretation\n\nNext, we will explore the models in depth. As we go through the models, the following sections will be relevant to keep in mind.\n\n- [Boxcox transformation](#Box-Cox-power-transformation) for the relationship between the Boxcox transformed rate and real infection rate\n\n- [Factor interpretation](#Factor-interpretation) for understanding what each factor encompasses in more detail\n\nWe will discuss model differences first and then explore what they have in common.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(aic_res.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Although this model is less parsimonious, it does provide some useful information regarding the party-specific effects for some factors, which the other model lacks. We will look at the significant ones(< .05 p) here. The poor general well-being(.18) and housing burden(.13) factor scores in blue counties seems to have a moderate effect on the transformed infection rate. The native relative population factor score seems to have exactly opposite effects on the rate between blues(-.10) and reds(.10) counties. The hispanic relative population factor score has surprisingly a negative effect(-.11) on the infection rate for red counties.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(bic_res.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> This model is a lot simpler with less interaction terms. Let's look at the differing significant coefficients. The poor general well-being, the housing burden, and the black relative population factor scores are simplified to have an overall positive effect(.13, .095 & .12) on the transformed infection rate. On the other hand, the hispanic relative factor score is generalized to have a slight negative effect(-.086) on the infection rate.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The two models agree on a lot as well. Both have landed on a relatively substantial positive coefficient(.31) for the inverse suicide factor score as well as a relatively more negative coefficient(-.18) for the crime risk factor score. For the smaller effects, both identify the poor food environment(.036), black relative population(.12), care provider accessibility(-0.11), and population density(.083) factor scores. For insignificant factors, both include the population youth and uninsured rate factor scores. In terms of party differences, both identify the overall income(.33 & -.20) and the urbanization(.31 & -.13) factor scores to have the opposite effects on blue and red counties respectively in terms of their transformed infection rate.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Final remarks\n\nI want to touch on some limitations of my analysis. First, I am not a domain expert. As a result, there is probably a lot of room for improvement throughout the analysis that would make more sense domain-wise. For instance, there might be a more reasonable response variable given our data. Another example would be the factor analysis. I am confident that a domain expert would likely have more domain-specific insights for the factors, translating to a more robust model interpretation.\n\nWhere assumptions or simplifications were made, I have erred on the side of caution. For instance, in my model interpretation, I was merely describing the coefficients without going into detail about what they imply for the general public. However, I think a more thorough investigation of the model would prove to be extremely valuable.\n\nI want to thank the New York Times and County Health Rankings for the data. Thank you for reading. Stay safe!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}