{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Emotion Classification NLP","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"from pprint import pprint\n!pip install emoji\n\n# Setup\nimport os\nimport re\nimport time\n\nimport emoji\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport scipy.stats as ss\nimport sklearn\nfrom keras_preprocessing.sequence import pad_sequences\nfrom keras_preprocessing.text import Tokenizer\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.model_selection import cross_val_score, RandomizedSearchCV, GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nfrom sklearn.utils.validation import check_is_fitted\n\n%matplotlib inline\nsns.set_style('white')\nsns.set_context('notebook')\n\nrandom_state = 143","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"}}},{"cell_type":"markdown","source":"## Frame the problem and look at the big picture\nFrom the information in several texts to determine the **emotion** that each of these convey.\n\n* How will your solution be used?\nIt will be used to categorise other tweets and other trending information regarding a specific\ntopic to determine the emotion behind.\n\n* What are the current solutions/workarounds (if any)?\nCurrent solutions use natural language processing by creating, or using readily available, embeddings\nof the words. These embeddings are vectors that represent several *dimensions* of the word and allow them\nto be compared with each other. Words with similar embeddings tend to have a similar meaning or used\nin similar contexts. (Similar work)\n\n* How should you frame this problem (supervised/unsupervised, online/offline, etc.)\nData is classified, so a supervised algorithm might be better suited. Embeddings can be tested using\nwords within the data or using other databases in english.\n\n* How should performance be measured?\nPerformance is measured in accuracy, though ideally it should be able to output the probability of it\nbeing from several classes (e.g. happy and angry).\n\n## Data loading","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"def load_emotion_data(dataset):\n    \"\"\"\n    Loads the required dataset from the emotions input\n    :param dataset:\n    :return:\n    \"\"\"\n    base_path = '../input/emotion-classification-nlp/'\n    dataset_name = 'emotion-labels-%s.csv' % dataset\n\n    data = pd.read_csv(os.path.join(base_path, dataset_name))\n    return data['text'].values, data['label'].values\n\ntrain_X, train_y = load_emotion_data('train')\nval_X, val_y = load_emotion_data('val')\ntest_X, test_y = load_emotion_data('test')\n\n\n\nprint(train_X)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"}}},{"cell_type":"markdown","source":"## Data Exploration\n\nColumns in the text\n\n| Column name | Description |\n|-|-|\n| text | Piece of text |\n| label | Assigned emotion to the text |","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# Labels\nsns.displot(train_y, height=5, aspect=2)\nplt.title('Ratio of different emotion labels in text')\nplt.show()\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"}}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"vectorizer = CountVectorizer()\nvectorizer.fit(train_X)\n\nvocabulary = pd.DataFrame(vectorizer.vocabulary_.items(), columns=['word', 'count'])\nvocabulary['length'] = vocabulary.apply(lambda x: len(x[0]), axis=1)\n\n\nprint(vocabulary.head())\n\nplt.figure(figsize=(10, 10))\nsns.scatterplot(data=vocabulary, x='length', y='count')\nplt.show()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"}}},{"cell_type":"markdown","source":"## Baseline\nDo a simple ML approach to see how much accuracy we can get.","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"def test_classifiers(classifiers, X, y, output=True):\n    best_classifier = None\n    best_score = 0\n    for _clf in classifiers:\n        try:\n            clf = sklearn.clone(_clf)\n            start = time.time()\n            scores = cross_val_score(clf, X, y)\n\n            if output:\n                print('Using classifier:', clf)\n                print('\\ttime:', time.time() - start)\n                print('\\tscores:', scores)\n                print('\\tbest:', np.max(scores))\n                print('\\taverage:', np.average(scores))\n\n            max_score = np.average(scores)\n            if max_score > best_score:\n                best_score = max_score\n                best_classifier = clf\n        except Exception as e:\n            print('** FAILED ** classifier', _clf)\n            print(e)\n\n    print('Fitting best classifier')\n    best_classifier.fit(X, y)\n    y_pred = best_classifier.predict(X)\n\n    conf_matrix = confusion_matrix(y, y_pred, normalize='true')\n\n    plt.figure(figsize=(10, 5))\n    sns.heatmap(conf_matrix, annot=True, fmt='.4f')\n    plt.show()\n\n    return best_classifier, best_score\n\n\nclassifiers = [\n    KNeighborsClassifier(),\n    LinearSVC(max_iter=10000, random_state=random_state),\n    RandomForestClassifier(random_state=random_state, n_jobs=8),\n    # MLPClassifier(random_state=random_state), # Takes a long time\n]\nprep_pipeline = Pipeline([\n    ('vectorizer', CountVectorizer())\n])\n\ntrain_X_transformed = prep_pipeline.fit_transform(train_X)\nprint(train_X[0])\nprint(train_X_transformed[0])\n\nbest_clf, best_score = test_classifiers(classifiers, train_X_transformed, train_y)\nprint('Best classifier is:', best_clf, best_score)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"}}},{"cell_type":"markdown","source":"## Text tokenizer comparison\n\nThe CountTokenizer creates vector per phrase with n columns, where n is the amount of different words\nfound in the text corpus. On the other hand, Keras Tokenizer converts each sentence to the numbers that\nrepresent the word in the dictionary, maintaining the order of the words.","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# Test using Keras tokenizer\ntokenizer = Tokenizer(oov_token='<OOV>')\ntokenizer.fit_on_texts(train_X)\n\ntrain_X_transformed = tokenizer.texts_to_sequences(train_X)\ntrain_X_padded = pad_sequences(train_X_transformed)\nprint(train_X_padded)\n\nprint(train_X[0])\nprint(train_X_transformed[0])\n\nbest_clf, best_score = test_classifiers(classifiers, train_X_padded, train_y)\nprint('Best classifier is:', best_clf, best_score)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"}}},{"cell_type":"markdown","source":"## Preprocessing\n\nWhile it didn't give better results, it would be interesting to check with other classifiers\nor go directly with an RNN. Before that I'll be trying several preprocessing techniques\nto see if filtering the text might help. Among these are:\n- Removing handles (@name) as I don't want the algorithm to associate a person with sentiment.\n- Remove coded values (e.g. &amp;).\n- Convert emojis to tokens (as those are good representation of sentiment yet are filtered by the\ntokenizers).\n- Review if there is other extraneous input like URL's or other\n\n### Remove handles","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"class RemoveHandles(TransformerMixin, BaseEstimator):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        values = pd.Series(X)\n        return values.replace(r'@\\w+', '', regex=True).values\n\nremove_handles = RemoveHandles()\nprint(remove_handles.fit_transform(train_X[:10]))","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"}}},{"cell_type":"markdown","source":"### Remove coded value","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"class RemoveCodedValues(TransformerMixin, BaseEstimator):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        values = pd.Series(X)\n        return values.replace(r'&\\w+;', '', regex=True).values\n\nremove_coded = RemoveCodedValues()\n\npat = re.compile(r'&\\w+;')\nfor f in remove_coded.fit_transform(train_X):\n    found = pat.findall(f)\n    if len(found) > 0:\n        print(pat.findall(f))","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"}}},{"cell_type":"markdown","source":"### Convert emojis to tokens","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"class TokenizeEmoji(TransformerMixin, BaseEstimator):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return np.array([emoji.demojize(text) for text in X])\n\ntokenize_emoji = TokenizeEmoji()\ntest = tokenize_emoji.fit_transform(train_X[:10])\n\nvectorizer.fit(test)\nprint(vectorizer.vocabulary_)\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"}}},{"cell_type":"markdown","source":"### Remove URLs or other input","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"class Emojifier(TransformerMixin, BaseEstimator):\n    \"\"\"\n    Converts characters like :) :( :/ to a unique value\n    \"\"\"\n    def __init__(self, emoji_pattern=r'[:;Xx][)(\\/D]|[)(\\/D][:;]'):\n        self.emoji_pattern = emoji_pattern\n\n    def fit(self, X, y=None):\n        emoji_list = set()\n        pattern = re.compile(self.emoji_pattern)\n\n        for line in X:\n            emoji_list.update(pattern.findall(line))\n\n\n        self.found_emojis_ = {}\n        for i, emoji in enumerate(emoji_list):\n            self.found_emojis_[emoji] = '<EMOJI_%d>' % i\n\n        return self\n\n    def transform(self, X):\n        # Validate\n        check_is_fitted(self, ['found_emojis_'])\n\n        # Transform\n        data = pd.Series(X)\n        for emoji, name in self.found_emojis_.items():\n            data = data.str.replace(emoji, name, regex=False)\n\n        return data.values\n\nemojifier = Emojifier()\nemojifier.fit(train_X)\nprint(emojifier.found_emojis_)\n\nemojified_X = emojifier.transform(train_X)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"}}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"for i, val in enumerate(train_X[:100]):\n    m = re.findall(r'[:;Xx][)(\\/D]|[)(\\/D][:;x]', val)\n    if len(m) > 0:\n        print(set(m))\n        print(i, val, train_y[i])\n        print(i, emojified_X[i])\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"}}},{"cell_type":"markdown","source":"## Test full pipeline with current changes","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"pipeline = Pipeline([\n    ('handles', RemoveHandles()),\n    ('coded', RemoveCodedValues()),\n    ('emoji', TokenizeEmoji()),\n    ('emojify', Emojifier()),\n    ('counts', CountVectorizer())\n])\n\nprepared_train_X = pipeline.fit_transform(train_X)\n\nbest_clf, score = test_classifiers(classifiers, prepared_train_X, train_y)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"}}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# Try best classifier in the validation dataset\nval_X_transformed = pipeline.transform(val_X)\nval_y_predicted = best_clf.predict(val_X_transformed)\n\naccuracy = accuracy_score(val_y, val_y_predicted)\nprint('Accuracy on validation:', accuracy)\n\nconf_matrix = confusion_matrix(val_y, val_y_predicted, normalize='true')\nconf_matrix = pd.DataFrame(conf_matrix, columns=best_clf.classes_, index=best_clf.classes_)\n# sums = np.sum(conf_matrix, axis=1, keepdims=True)\n\n\nplt.figure(figsize=(10, 5))\nsns.heatmap(conf_matrix, annot=True, fmt='.5f')\nplt.show()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"}}},{"cell_type":"markdown","source":"## Error analysis\n\nLook at the errors that the classifier is doing to get where the problem could be\narising.","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# Plot the previous heatmap, but with emphasis in the errors\n\nnp.fill_diagonal(conf_matrix.values, 0)\n\nplt.figure(figsize=(10, 5))\nsns.heatmap(conf_matrix, annot=True, fmt='.4f')\nplt.show()\n\nprint(classification_report(val_y, val_y_predicted))\nprint(best_clf.classes_)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"}}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# Check some examples of errors\n\nerrors = val_y != val_y_predicted\nerrors_to_show = 10\n\nfor text, label, predicted in zip(val_X[errors], val_y[errors], val_y_predicted[errors]):\n    print('%s -> %s\\n\\t%s' % (label, predicted, text))\n\n    errors_to_show -= 1\n    if errors_to_show == 0:\n        break","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"}}},{"cell_type":"markdown","source":"## Hyper-parameter tweaking\n\nTry to get the most of the classifiers by tweaking hyperparameters.","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"pipeline = Pipeline([\n    ('handles', RemoveHandles()),\n    ('coded', RemoveCodedValues()),\n    ('emoji', TokenizeEmoji()),\n    ('emojify', Emojifier()),\n    ('counts', CountVectorizer()),\n    ('clf', RandomForestClassifier(random_state=random_state))\n])\n\ngrid_params = [{\n    # 'handles': ['passthrough', RemoveHandles()], # Eliminate bias per author or mentioned people\n    'coded': ['passthrough', RemoveCodedValues()],\n    'emoji': ['passthrough', TokenizeEmoji()],\n    'emojify': ['passthrough', Emojifier()],\n    # 'clf__n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 5)],\n    # 'clf__max_features': ['auto', 'sqrt'],\n    'clf__max_depth': [int(x) for x in np.linspace(10, 100, num=4)] + [None,],\n    'clf__bootstrap': [True, False]\n}]\n\ngrid_search = GridSearchCV(pipeline, grid_params, scoring='accuracy',\n                           refit=True, cv=5, n_jobs=16,\n                           verbose=2)\ngrid_search.fit(train_X, train_y)\n\nprint(grid_search.best_score_, grid_search.best_estimator_)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"}}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"}}}]}