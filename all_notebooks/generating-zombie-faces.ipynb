{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Creating New Faces - Generative Adversarial Network Tutorial\n\n\n[Edward Toth, PhD, University of Sydney]\n\n- e-mail: eddie_toth@hotmail.com\n- Add me on: https://www.linkedin.com/in/edward-toth/ \n- Join the community: https://www.meetup.com/Get-Singapore-Meetup-Group/\n- Data Avenger: https://data-avenger.mailchimpsites.com/\n\n\n### In Today's Tutorial, you'll learn about:\n    - Discriminative and Generative Models\n    - Deep Convolutional Generative adversarial networks (DCGANs)\n    - Generating Faces \n\nOriginal Paper for DEEP CONVOLUTIONAL\nGENERATIVE ADVERSARIAL NETWORKS (DCGANs):\n https://arxiv.org/pdf/1511.06434.pdf\n\n<a id = \"0\"></a><br>\n1. [Load Libraries](#1)  \n1. [Data Preparation](#2)\n     * [View Images](#2a) \n1. [Discriminative vs. Generative Models](#3)\n1. [Discriminative Model](#4)\n   [Generative Model](#5)\n1. [Generative Adversarial Networks (GANs)](#6)   \n- [Steps for Generating Zombie Faces](#7)\n    ","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"markdown","source":"<a id = \"1\"></a><br>\n## Load Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"2\"></a><br>\n## Data Preparation\n- Check image sizes \n- Resize images to $120 \\times 120$ pixels\n- Normalize images (divide by 255)\n- Convert to tensorflow compatible format\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dir = \"/kaggle/input/faces-data-new/images/\"\ndef load_faces():\n    data = []\n    sizes = []\n    for i in os.listdir(dir):   \n        if '.jpg' in i:\n            img = Image.open(dir + i)\n            size = np.array(img,dtype = \"float32\").shape\n            sizes.append(size)\n            img = img.resize((120,120))\n            # Convert to tf format   pixels = tf.keras.preprocessing.image.img_to_array(img)\n            # Normalize data\n            pixels = np.array(img,dtype = \"float32\")/255\n            data.append(pixels)\n        else:\n            print(\"\")\n    return sizes, np.stack(data)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check image sizes and load data\nsizes, dataset = load_faces()\nprint(\"Number of images:\", len(sizes))\nprint(\"Unique Shapes of images:\", pd.Series(sizes).unique() )\ndataset.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"2a\"></a><br>\n### Image Selection\n- View Images\n- Pick Images with green background","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,15))\nfor i in range(10):\n    plt.subplot(5,5,i+1)\n    plt.axis(\"off\")\n    plt.imshow(dataset[i])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" #RGB\n# np.unique(dataset[2][:,:,1])\n\nimport numpy as np\ngreen_back = [] \nfor d in dataset:\n    a = d[:,:,1]*255 #red\n    b =d[:,:,0]*255 #blue\n    if (pd.Series(a.flat).mean() > 70) & (pd.Series(a.flat).std() < 40):\n    #pd.Series(a.flat).value_counts().index[0] > 60:\n        green_back.append(d)\n    else:\n       \"Not green\"\n\nlen(green_back)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = np.stack(green_back)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize = (15,15))\nfor i in range(50):\n    plt.subplot(10,7,i+1)\n    plt.axis(\"off\")\n    plt.imshow(dataset[i])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"3\"></a><br>\n## Discriminative vs. Generative Models\n\nDiscriminative machine learning:\n- Typically, model parameters are learnt by maximizing the conditional probability P(Y/X).\n- Classify data into a certain category/label. \n- E.g. Neural networks with output layer with \"sigmoid\" activation function.   \n\nGenerative machine learning:\n- Train model to learn parameters by maximizing the joint probability of P(X,Y).\n- Calculates probability that data belongs to a certain category/label. \n- E.g. Neural networks with output layer with \"softmax\" probabilities.   \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = \"4\"></a><br>\n## Discriminative Model\n\n### Convolutional Neural Network\n\nRecommendations from original DCGAN paper https://arxiv.org/pdf/1511.06434.pdf:\n- Replace Max-pooling layers with strided convolutions in CNN model.\n- kernel initializor: Initial weights were generated from a Normal distribution with mean 0 and standard deviation 0.02. \n- Adam(0.0002, beta_1=0.5): Follows from DCGAN paper, where learning rate is 0.0002 and momentum is 0.5 (accumulates the gradient of the past  epochs to determine the direction to go). \n- Use LeakyReLU activation in the discriminator for all layers.\n\nFurther Explanations: \n- `LeakyRelu`: Leaky ReLU allows the pass of a small gradient signal for negative values while ReLu only ignores negative values. This enables more gradients from the discriminator to flow into the generator.\n- Number of convolutional filters in layers: $(128,128,64,64)$\n- Kernel size: E.g. 3 or (3,3)  is the sizes of the convolutional filters that are moved throughout the images. Lose information because of the image borders as a (3,3) square can only (N,N) image, N-3 times. \n- Padding = \"same\": size of output features has the same length as the original input. Without this, the output size  is reduced depending on kernel size.\n- Strides = E.g. (2,2) or 2 move the convolutional filters throughout the images by 2 pixels at a time. Since you move 2 pixels at a time and with padding = ‘same’, this means the size of the input tensor is halved in both height and width. \n\nAnother suggestion: \n- batch normalization (batchnorm): Directly applying batchnorm to all layers however, results model instability (even excluding the generator output layer and the discriminator input layer). We achieve better results for our image generation without batchnorm.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# CNN model 3*[Conv2D -> Leaky] -> Conv2D -> Dropout -> Flatten -> Dense + Sigmoid\n\ndef discriminator(inp_shape = (120,120,3)):\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Conv2D(128, kernel_size = 3, strides = 2,  padding=\"same\", \n               input_shape = inp_shape, kernel_initializer = tf.random_normal_initializer(mean=0, stddev=0.02) ),\n        tf.keras.layers.LeakyReLU(0.2),\n        \n        tf.keras.layers.Conv2D(128, kernel_size = 3, strides = 2, padding=\"same\"),\n        tf.keras.layers.LeakyReLU(0.2),\n        \n        tf.keras.layers.Conv2D(64, kernel_size = 3, strides = 2, padding=\"same\"),\n        tf.keras.layers.LeakyReLU(0.2),\n        \n        tf.keras.layers.Conv2D(64, kernel_size = 3, strides = 2, padding = \"same\"),\n        tf.keras.layers.Flatten(),      \n        tf.keras.layers.Dense(1, activation = \"sigmoid\")\n    ],\n        name=\"discriminator\")\n    model.compile(optimizer = tf.keras.optimizers.Adam(0.0002, beta_1=0.5), loss = \"binary_crossentropy\", metrics = ['acc'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# View model layers \nd_model = discriminator()\ntf.keras.utils.plot_model(d_model, show_shapes = True)\n# d_model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"5\"></a><br>\n## Generative Model  \n\n\nArchitecture guidelines for stable Deep Convolutional GANs from https://arxiv.org/pdf/1511.06434.pdf: \n- `Conv2DTranspose`: fractional-strided convolutions or transposed convolutional layer uses transformations that reverse the direction of normal convolutions. In a way, we want to unfold what happened with the discriminator. \n\n- `latent_dim`: Represents the dimension of an unseen (hidden) space in the generator. The size of this initial latent space is paramount to allow for an accurate generation of images. Manually selected and should see how this parameter changes the results. \n\n- (15,15,128): [Or shape with 128 * 15 * 15] represents   128 generations of images with $15 \\times 15$ pixels.\n- Use ReLU activation in generator for all layers except for the output, which uses Tanh.\n\n\n<!-- - DCGANs Directly applying batchnorm to all layers however, resulted in sample oscillation and model instability. This was avoided by not applying\nbatchnorm to the generator output layer and the discriminator input layer. -->\n\nDifferent models:\n- Change `relu` activation function to `LeakyRelu`\n- Add Batch Normalization after each convolutional layer except the output layer. \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generative Model with BatchNormalization and LeakyRelu \ndef generator(latent_dim = 100):\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Dense(128 * 15 * 15, input_dim = latent_dim,\n                              kernel_initializer = tf.random_normal_initializer(mean=0, stddev=0.02) ),\n        tf.keras.layers.LeakyReLU(0.2),\n        tf.keras.layers.Reshape((15,15,128)),\n\n        # First transpose convolutional filter \n        tf.keras.layers.Conv2DTranspose(128, kernel_size = 3, strides = 2,  padding = \"same\"),\n#         tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.LeakyReLU(0.2),\n\n        # Second transpose convolutional filter \n        tf.keras.layers.Conv2DTranspose(128, kernel_size = 3, strides = 2,  padding = \"same\"),\n#         tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.LeakyReLU(0.2),\n\n         # Third transpose convolutional filter \n        tf.keras.layers.Conv2DTranspose(64, kernel_size = 3, strides = 2,  padding = \"same\"),\n#         tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.LeakyReLU(0.2),\n        \n        tf.keras.layers.Conv2D(3,  kernel_size = 3, padding = \"same\", activation = \"tanh\") #\"sigmoid\")\n        \n    ])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g_model = generator()\ntf.keras.utils.plot_model(g_model, show_shapes = True)\n# g_model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"6\"></a><br>\n## Deep Convolutional Generative Adversarial Networks (DCGANs) \n\nDCGANs is one of the popular and successful network design for GAN. It mainly composes of convolution layers without max pooling or fully connected layers. It uses convolutional stride and transposed convolution for the downsampling and the upsampling. The figure below is the network design for the generator.\n\nOther notebooks that use DCGANs: \nhttps://www.kaggle.com/akshat0007/generating-new-simpsons-character-using-dcgan\n\n[Back to Top](#0)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def gan(g_model, d_model):\n    d_model.trainable = False\n    model = tf.keras.models.Sequential([\n        g_model,\n        d_model\n    ],\n        name=\"DCGANs\")\n    model.compile(optimizer = tf.keras.optimizers.Adam(0.0002, beta_1=0.5), loss = \"binary_crossentropy\")\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gan_model = gan(g_model, d_model)\ntf.keras.utils.plot_model(gan_model, show_shapes = True)\n# gan_model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"New functions: \n- generate_real_samples: Select a random batch of real images\n- generate_latent_space: Take a point from the latent space as input and generates a new image using DCGAN. \n- generate_fake_examples:  Generate a batch of fake images \n- plot_samples: Show 49 generated images \n- summarize_performance: returns accuracy of discriminator's ability to classify an image as fake or real. Also uses plot_samples to show generated images.\n\nFunction inputs: \n- n_size = 200 is the batch size of images that are selected and generated. \n\nnp.ones((batch_size, 1))","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def summarize_performance(g_model, dataset, n_size = 128):\n    X_real, y_real = generate_real_samples(dataset)\n    _,accr = d_model.evaluate(X_real, y_real)\n    \n    X_fake, y_fake = generate_fake_examples(g_model)\n    _, accf = d_model.evaluate(X_fake, y_fake)\n    \n    print(\"Real samples Acc: {}\".format(accr*100))\n    print(\"Fake samples Acc: {}\".format(accf*100))\n    \n#     plot_samples(X_fake)\n    plt.figure(figsize = (15,15))\n    for i in range(7*7):\n        plt.subplot(7,7,i+1)\n        plt.axis(\"off\")\n        plt.imshow(Xfake[i])\n    plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"7\"></a><br>\n# Steps for Generating Zombie Faces:\n1. Select a random batch of real images\n- Randomly select points from the latent space and generate fake images  \n- Track discriminator loss (combined loss from real and fake batches)\n- Track loss from DCGANs over the number of epoches\n- Generate fake images using generator model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef train(g_model, d_model, gan_model, dataset, iterations=2000, batch_size=200, latent_dim=100, sample_interval=200):\n losses = []\n accuracies = []\n # Labels for real and fake examples\n real = np.ones((batch_size, 1))\n fake = np.zeros((batch_size, 1))\n\n for iteration in range(iterations):\n\n    # -------------------------\n    #  Train the Discriminator\n    # -------------------------\n    # Select a random batch of real images\n    ind = np.random.randint(0, dataset.shape[0], batch_size)\n    imgs = dataset[ind]\n\n    # Generate points from the latent space \n    z = np.random.normal(0, 1, (batch_size, latent_dim))\n    # Generate a batch of fake images\n    gen_imgs = g_model.predict(z)\n\n    # Discriminator loss function \n    d_loss_real = d_model.train_on_batch(imgs, real) # real = 0 label\n    d_loss_fake = d_model.train_on_batch(gen_imgs, fake) # fake = 1 label\n    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n    # ---------------------\n    #  Train the Generator\n    # ---------------------\n    # Generate point from a latent space \n    z = np.random.normal(0, 1, (batch_size, latent_dim))\n\n    # GANs loss\n    g_loss = gan_model.train_on_batch(z, real)\n\n    # Generate a batch of fake images\n    gen_imgs = g_model.predict(z)\n\n\n    if iteration % sample_interval == 0:\n\n    # Output training progress\n        print (\"%d [D loss: %f, Acc.: %.2f%%] [G loss: %f]\" %\n                  (iteration, d_loss[0], 100*d_loss[1], g_loss))\n\n        # Save losses and accuracies to be plotted after training\n        losses.append((d_loss[0], g_loss))\n        accuracies.append(100*d_loss[1])\n\n        # Output generated images\n        #          summarize_performance(g_model, dataset)\n        plt.figure(figsize = (15,15))\n        for i in range(7*7):\n            plt.subplot(7,7,i+1)\n            plt.axis(\"off\")\n            plt.imshow(gen_imgs[i])\n        plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train(g_model, d_model, gan_model, dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Our results are consistent with the findings from https://machinelearningmastery.com/practical-guide-to-gan-failure-modes/, \n- A stable GAN will have a discriminator loss (D loss) around 0.5 (maybe as high 0.8).\n- The accuracy (acc) of the discriminator for classifying real and generated images will not be 50%, but should  hover around 70% to 80%.\n- The generator loss (G loss) is typically larger than D loss.\n\nHow to improve results?\n- Tune parameters or use different architectures \n- Need images that are centered with white backgrounds\n- Require additional interations or epochs? \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### THE END\n# If you like this tutorial, add an UPVOTE or COMMENT! \n[Back to Top](#0)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}