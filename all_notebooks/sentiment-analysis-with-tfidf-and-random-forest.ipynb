{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport nltk\n%matplotlib inline\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sentiment Analysis with TFIDF and Random Forest\n\nThis notebook creates a Random Forest Classifier for Sentiment Analysis using NLP techniques and TFIDF features. The dataset used consists of movie reviews from IMDB that have been labelled with a positive (1) or negative(0) label.\n\nThis notebook includes the following:\n1. Dataset splitting\n2. WordClouds\n3. Text pre-processing and cleaning\n4. Vectorizing with Tfidf\n5. Random Forest classification\n6. Hyperparameter tuning using GridSearchCV\n7. Evaluation of Validation and Test Set\n8. Most significant features for prediction\n\nFirst, we will see what's in our dataset."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"imdb=pd.read_csv('/kaggle/input/imdb-dataset-sentiment-analysis-in-csv-format/Train.csv')\nprint(imdb.info())\nprint(imdb.shape)\nprint(imdb.head(10))\n\nimdb['label'].value_counts().plot.pie(figsize=(6,6),title=\"Distribution of reviews per sentiment\",labels=['',''],autopct='%1.1f%%')\nlabels=[\"Positive\",\"Negative\"]\nplt.legend(labels,loc=3)\nplt.gca().set_aspect('equal')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see there are 40000 entries and there's not a single null value. The dataset comprises movie reviews with a positive (1) or negative (0) value equally distributed. SInce there are too many examples for our computation capacity, we'll split the dataset and reduce it. \n\nFirst, we'll reduce the data set to 10% and then we'll keep 50% for the training set and 25% both for the validation and the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nfeatures = imdb.drop(\"label\",axis=1)\nlabels = imdb[\"label\"]\n\nX_train, X_test, y_train, y_test = train_test_split(features,labels,test_size = 0.90, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X_train,y_train,test_size = 0.5, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_test,y_test,test_size = 0.5, random_state=42)\n\nprint(\"Data distribution:\\n- Train: {} \\n- Validation: {} \\n- Test: {}\".format(len(y_train),len(y_val),len(y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nNow let's create a function that will pre-process and clean the data. It does the following:\n* Tokenize sentences\n* Remove capital letters\n* Remove stopwords\n* Remove non-alphanumeric characters\n* Lemmatize the tokens"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean(text):\n    wn = nltk.WordNetLemmatizer()\n    stopword = nltk.corpus.stopwords.words('english')\n    tokens = nltk.word_tokenize(text)\n    lower = [word.lower() for word in tokens]\n    no_stopwords = [word for word in lower if word not in stopword]\n    no_alpha = [word for word in no_stopwords if word.isalpha()]\n    lemm_text = [wn.lemmatize(word) for word in no_alpha]\n    clean_text = lemm_text\n    return clean_text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we've cleaned our data, let's explore it.\n\nWe create lists for positive and negative words and then visualize the most common ones with WordClouds."},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nfrom collections import Counter\n\ndef generate_wordcloud(words,sentiment):\n    plt.figure(figsize=(16,13))\n    wc = WordCloud(background_color=\"white\", max_words=100, max_font_size=50)\n    wc.generate(words)\n    plt.title(\"Most common {} words\".format(sentiment), fontsize=20)\n    plt.imshow(wc.recolor(colormap='Pastel2', random_state=17), alpha=0.98)\n    plt.axis('off')\n    \nimdb=imdb.head(1000)\nprint(\"Processing data...\")\nimdb['clean']=imdb['text'].map(clean)\nimdb['clean_text']=imdb['clean'].apply(lambda x: \" \".join([str(word) for word in x]))\n\nprint(\"Creating word clouds...\")\npositive_words=\" \".join(imdb[imdb.label==1]['clean_text'].values)\nnegative_words=\" \".join(imdb[imdb.label==0]['clean_text'].values)\n\ngenerate_wordcloud(positive_words,\"positive\")\ngenerate_wordcloud(negative_words,\"negative\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we explored our data, this function will turn it into a matrix for our analysis by using the TfidfVectorizer."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ndef vectorize(data,tfidf_vect_fit):\n    X_tfidf = tfidf_vect_fit.transform(data)\n    words = tfidf_vect_fit.get_feature_names()\n    X_tfidf_df = pd.DataFrame(X_tfidf.toarray())\n    X_tfidf_df.columns = words\n    return(X_tfidf_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_vect = TfidfVectorizer(analyzer=clean)\ntfidf_vect_fit=tfidf_vect.fit(X_train['text'])\nX_train=vectorize(X_train['text'],tfidf_vect_fit)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We explore first results of a Random Forest Classifier without any hyperparameter tuning using 5-cross-fold validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\nrf = RandomForestClassifier()\nscores = cross_val_score(rf,X_train,y_train.values.ravel(),cv=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(scores)\nscores.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we'll do some hyperparameter tuning using GridSearchCV."},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_results(results):\n    print('BEST PARAMS: {}\\n'.format(results.best_params_))\n\n    means = results.cv_results_['mean_test_score']\n    stds = results.cv_results_['std_test_score']\n    for mean, std, params in zip(means, stds, results.cv_results_['params']):\n        print('{} (+/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nrf = RandomForestClassifier()\nparameters = {\n    'n_estimators': [5,50,100],\n    'max_depth': [2,10,20,None]\n}\n\ncv = GridSearchCV(rf,parameters)\ncv.fit(X_train,y_train.values.ravel())\nprint_results(cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the evaluation of the validation set, first we need to transform it using the same features as the training set. Then, we can evaluate our best models from CV on it."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_val=vectorize(X_val['text'],tfidf_vect_fit)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf1 = RandomForestClassifier(n_estimators=100,max_depth=20)\nrf1.fit(X_train, y_train.values.ravel())\nrf2 = RandomForestClassifier(n_estimators=100,max_depth=None)\nrf2.fit(X_train, y_train.values.ravel())\nrf3 = RandomForestClassifier(n_estimators=5,max_depth=None)\nrf3.fit(X_train, y_train.values.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score,precision_score,recall_score\n\nfor mdl in [rf1,rf2,rf3]:\n    y_pred = mdl.predict(X_val)\n    accuracy = round(accuracy_score(y_val,y_pred), 3)\n    precision = round(precision_score(y_val,y_pred), 3)\n    recall = round(recall_score(y_val,y_pred), 3)\n    print('MAX DEPTH: {} / # OF EST: {} -- A: {} / P: {} / R: {}'.format(mdl.max_depth,\n                                                                         mdl.n_estimators,\n                                                                         accuracy,\n                                                                         precision,\n                                                                         recall))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we can see the the best model is the second one:\n\n**MAX DEPTH: None / # OF EST: 100**\n\nSo now we will evaluate the best model on the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test=vectorize(X_test['text'],tfidf_vect_fit)\n\ny_pred = rf2.predict(X_test)\naccuracy = round(accuracy_score(y_test,y_pred), 3)\nprecision = round(precision_score(y_test,y_pred), 3)\nrecall = round(recall_score(y_test,y_pred), 3)\nprint('MAX DEPTH: {} / # OF EST: {} -- A: {} / P: {} / R: {}'.format(rf3.max_depth,\n                                                                     rf3.n_estimators,\n                                                                     accuracy,\n                                                                     precision,\n                                                                     recall))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can explore and see what are the most important words to predict the correct results."},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_importances = pd.Series(rf2.feature_importances_, index=X_train.columns)\nfeat_importances.nlargest(20).plot(kind='bar',figsize=(10,10))\nplt.title(\"Top 20 important features\")\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}