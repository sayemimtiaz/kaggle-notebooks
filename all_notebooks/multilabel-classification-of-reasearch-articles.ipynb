{"cells":[{"metadata":{},"cell_type":"markdown","source":"Please check this [article](https://medium.com/analytics-vidhya/an-introduction-to-multi-label-text-classification-b1bcb7c7364c?sk=8a30075009552cfd4a7534663edaed7e) for detailed explanation."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nimport re\nfrom gensim.models import Word2Vec\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom wordcloud import WordCloud,STOPWORDS\n\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import hamming_loss\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.model_selection import train_test_split\nfrom skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.naive_bayes import MultinomialNB\nfrom skmultilearn.problem_transform import ClassifierChain\nfrom skmultilearn.problem_transform import LabelPowerset","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/topic-modeling-for-research-articles/train.csv')\ntest_data = pd.read_csv('/kaggle/input/topic-modeling-for-research-articles/test.csv')\ntrain_data.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.shape)\nprint(test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=train_data.iloc[:,3:].sum()\nrowsums=train_data.iloc[:,2:].sum(axis=1)\nno_label_count = 0\nfor sum in rowsums.items():\n    if sum==0:\n        no_label_count +=1\n\nprint(\"Total number of articles = \",len(train_data))\nprint(\"Total number of articles without label = \",no_label_count)\nprint(\"Total labels = \",x.sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Check for missing values in Train dataset\")\nprint(train_data.isnull().sum().sum())\nprint(\"Check for missing values in Test dataset\")\nnull_check=test_data.isnull().sum()\nprint(null_check)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=train_data.iloc[:,3:].sum()\n#plot\nplt.figure(figsize=(12,12))\nax= sns.barplot(x.index, x.values, alpha=0.8)\nplt.title(\"Class counts\")\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.xlabel('Label ', fontsize=12)\n\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the dataset is imbalanced, MLSMOTE technique can be used for sampling. As of now, I am not using any sampling techniques."},{"metadata":{"trusted":true},"cell_type":"code","source":"x=rowsums.value_counts()\n\n#plot\nplt.figure(figsize=(12,12))\nax = sns.barplot(x.index, x.values, alpha=0.8)\nplt.title(\"Multiple tags per article\")\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.xlabel('# of Labels ', fontsize=12)\n\n#adding the text labels\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['Text']=train_data['TITLE']+' '+train_data['ABSTRACT']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.drop(columns=['TITLE','ABSTRACT'], inplace=True)\ntrain_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\n#text = description_category.description.values\ncloud = WordCloud(stopwords=STOPWORDS, background_color='black', collocations=False, width=2500, height=1800).generate(\" \".join(train_data['Text']))\nplt.axis('off')\nplt.imshow(cloud)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['Text'][5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have to remove symbols and punctuations"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remove Stopwords\nstop_words = set(stopwords.words('english'))\n\n# function to remove stopwords\ndef remove_stopwords(text):\n    no_stopword_text = [w for w in text.split() if not w in stop_words]\n    return ' '.join(no_stopword_text)\n\ntrain_data['Text'] = train_data['Text'].apply(lambda x: remove_stopwords(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nstemmer = SnowballStemmer(\"english\")\ndef stemming(sentence):\n    stemSentence = \"\"\n    for word in sentence.split():\n        stem = stemmer.stem(word)\n        stemSentence += stem\n        stemSentence += \" \"\n    stemSentence = stemSentence.strip()\n    return stemSentence\n\ntrain_data['Text'] = train_data['Text'].apply(stemming)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['Text'][5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categories=['Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance'] \ntrain_data[categories].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split the data\n\nx_train, x_test, y_train, y_test = train_test_split(train_data['Text'], train_data[categories], test_size=0.2, random_state=40, shuffle=True)\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MultiLabel Classification"},{"metadata":{},"cell_type":"markdown","source":"# Binary Relevace"},{"metadata":{"trusted":true},"cell_type":"code","source":"# using binary relevance\n\n\n\npipeline = Pipeline([\n                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n                ('clf', BinaryRelevance(MultinomialNB())),\n            ])\n\n# train\npipeline.fit(x_train, y_train)\n\n# predict\npredictions = pipeline.predict(x_test)\n\n\nfrom sklearn.metrics import accuracy_score\nprint('Accuracy = ', accuracy_score(y_test,predictions))\nprint('F1 score is ',f1_score(y_test, predictions, average=\"micro\"))\nprint('Hamming Loss is ', hamming_loss(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using binary relevance with Logistic Regression\n\npipeline = Pipeline([\n                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n                ('clf', BinaryRelevance(LogisticRegression(solver='sag'))),\n            ])\n\n# train\npipeline.fit(x_train, y_train)\n\n# predict\npredictions = pipeline.predict(x_test)\n\n\nfrom sklearn.metrics import accuracy_score\nprint('Accuracy = ', accuracy_score(y_test,predictions))\nprint('F1 score is ',f1_score(y_test, predictions, average=\"micro\"))\nprint('Hamming Loss is ', hamming_loss(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classifier Chains"},{"metadata":{"trusted":true},"cell_type":"code","source":"# using classifier chains with MultinomialNB\n\n\n# initialize classifier chains multi-label classifier\n# with a gaussian naive bayes base classifier\npipeline = Pipeline([\n                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n                ('clf', ClassifierChain(MultinomialNB())),\n            ])\n# ClassifierChain(GaussianNB())\n\n# train\npipeline.fit(x_train, y_train)\n\n# predict\npredictions = pipeline.predict(x_test)\n\nprint('Accuracy = ', accuracy_score(y_test,predictions))\nprint('F1 score is ',f1_score(y_test, predictions, average=\"micro\"))\nprint('Hamming Loss is ', hamming_loss(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = Pipeline([\n                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n                ('clf', ClassifierChain(LogisticRegression(solver='sag'))),\n            ])\n# ClassifierChain(GaussianNB())\n\n# train\npipeline.fit(x_train, y_train)\n\n# predict\npredictions = pipeline.predict(x_test)\n\nprint('Accuracy = ', accuracy_score(y_test,predictions))\nprint('F1 score is ',f1_score(y_test, predictions, average=\"micro\"))\nprint('Hamming Loss is ', hamming_loss(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Label Powerset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# using Label Powerset\n\n# initialize label powerset multi-label classifier\npipeline = Pipeline([\n                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n                ('clf',  LabelPowerset(LogisticRegression())),\n            ])\n#classifier = LabelPowerset(LogisticRegression())\n# train\npipeline.fit(x_train, y_train)\n# predict\npredictions = pipeline.predict(x_test)\n# accuracy\nprint(\"Accuracy = \",accuracy_score(y_test,predictions))\nprint('F1 score is ',f1_score(y_test, predictions, average=\"micro\"))\nprint('Hamming Loss is ', hamming_loss(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# References\n\n* https://www.analyticsvidhya.com/blog/2017/08/introduction-to-multi-label-classification/\n\n* https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}