{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Autoencoder Introduction \n<hr>\n\n![Poster-YouTubeModel-based Deep Convolutional Face Autoencoder for Unsupervised Monocular Reconstruction](https://i.ytimg.com/vi/uIMpHZYB8fI/maxresdefault.jpg)\n> *Image [source](http://gvv.mpi-inf.mpg.de/projects/MZ/Papers/arXiv2017_FA/page.html)*\n\n> An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal ‚Äúnoise‚Äù. Source: [wiki](https://en.wikipedia.org/wiki/Autoencoder#:~:text=An%20autoencoder%20is%20a%20type,to%20ignore%20signal%20%E2%80%9Cnoise%E2%80%9D.)\n\n<hr>\n\n### **NOTE:**\n**From now I will be updating my notebook on @tarunk04 account. Old account is now closed due to some unavoidable reasons. I have lost most of my work but I still have a copy of most of my popular notebook and I am going to reupload it on this account. I hope I will get the same response on this account. Thanks for your support üôè.** \n\n**<span style = \"color:#cc1616\">Update log (on old account):</span>**\n* <font style=\"color: rgba(107, 61, 35, 0.92) \"><b>02 July 2020 03:15 AM IST (Version 1) :</b> Initial Version </font>\n* <font style=\"color: rgba(107, 61, 35, 0.92) \"><b>02 July 2020 04:17 PM IST (Version 4) :</b> Few fixes. </font>\n* <font style=\"color: rgba(107, 61, 35, 0.92) \"><b>02 July 2020 09:15 PM IST (Version 5) :</b> New section added <a href=\"#Denoising-Cifar10-Data\">Denoising Cifar10 Data.</a> </font>\n* <font style=\"color: rgba(107, 61, 35, 0.92) \"><b>20 September 2020 10:50 PM IST (Version 16) :</b> Latest Update. </font>\n\n**<span style = \"color:#cc1616\">Update log:</span>**\n* <font style=\"color: #cc1616 \"><b>06 Feb 2021 07:10 PM IST (Version 1) :</b> Latest Update. </font>\n\n<hr>\n\n<font style=\"color:Red;font-size:18px\">If you find this notebook helpful, Please UPVOTE.</font>\n\n### Follow me:\n\n* <a href=\"https://bit.ly/tarungithub\"><img src=\"data:image/svg+xml;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCAyNCAyNCIgaGVpZ2h0PSI1MTIiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjUxMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtMTIgLjVjLTYuNjMgMC0xMiA1LjI4LTEyIDExLjc5MiAwIDUuMjExIDMuNDM4IDkuNjMgOC4yMDUgMTEuMTg4LjYuMTExLjgyLS4yNTQuODItLjU2NyAwLS4yOC0uMDEtMS4wMjItLjAxNS0yLjAwNS0zLjMzOC43MTEtNC4wNDItMS41ODItNC4wNDItMS41ODItLjU0Ni0xLjM2MS0xLjMzNS0xLjcyNS0xLjMzNS0xLjcyNS0xLjA4Ny0uNzMxLjA4NC0uNzE2LjA4NC0uNzE2IDEuMjA1LjA4MiAxLjgzOCAxLjIxNSAxLjgzOCAxLjIxNSAxLjA3IDEuODAzIDIuODA5IDEuMjgyIDMuNDk1Ljk4MS4xMDgtLjc2My40MTctMS4yODIuNzYtMS41NzctMi42NjUtLjI5NS01LjQ2Ni0xLjMwOS01LjQ2Ni01LjgyNyAwLTEuMjg3LjQ2NS0yLjMzOSAxLjIzNS0zLjE2NC0uMTM1LS4yOTgtLjU0LTEuNDk3LjEwNS0zLjEyMSAwIDAgMS4wMDUtLjMxNiAzLjMgMS4yMDkuOTYtLjI2MiAxLjk4LS4zOTIgMy0uMzk4IDEuMDIuMDA2IDIuMDQuMTM2IDMgLjM5OCAyLjI4LTEuNTI1IDMuMjg1LTEuMjA5IDMuMjg1LTEuMjA5LjY0NSAxLjYyNC4yNCAyLjgyMy4xMiAzLjEyMS43NjUuODI1IDEuMjMgMS44NzcgMS4yMyAzLjE2NCAwIDQuNTMtMi44MDUgNS41MjctNS40NzUgNS44MTcuNDIuMzU0LjgxIDEuMDc3LjgxIDIuMTgyIDAgMS41NzgtLjAxNSAyLjg0Ni0uMDE1IDMuMjI5IDAgLjMwOS4yMS42NzguODI1LjU2IDQuODAxLTEuNTQ4IDguMjM2LTUuOTcgOC4yMzYtMTEuMTczIDAtNi41MTItNS4zNzMtMTEuNzkyLTEyLTExLjc5MnoiIGZpbGw9IiMyMTIxMjEiLz48L3N2Zz4=\" width=\"22\" align=\"left\" style=\"margin-right:10px\"/> GitHub</a>\n* <a href=\"https://bit.ly/tarnkr-youtube\"><img src=\"data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iaXNvLTg4NTktMSI/Pg0KPCEtLSBHZW5lcmF0b3I6IEFkb2JlIElsbHVzdHJhdG9yIDE5LjAuMCwgU1ZHIEV4cG9ydCBQbHVnLUluIC4gU1ZHIFZlcnNpb246IDYuMDAgQnVpbGQgMCkgIC0tPg0KPHN2ZyB2ZXJzaW9uPSIxLjEiIGlkPSJMYXllcl8xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB4PSIwcHgiIHk9IjBweCINCgkgdmlld0JveD0iMCAwIDQ2MS4wMDEgNDYxLjAwMSIgc3R5bGU9ImVuYWJsZS1iYWNrZ3JvdW5kOm5ldyAwIDAgNDYxLjAwMSA0NjEuMDAxOyIgeG1sOnNwYWNlPSJwcmVzZXJ2ZSI+DQo8cGF0aCBzdHlsZT0iZmlsbDojRjYxQzBEOyIgZD0iTTM2NS4yNTcsNjcuMzkzSDk1Ljc0NEM0Mi44NjYsNjcuMzkzLDAsMTEwLjI1OSwwLDE2My4xMzd2MTM0LjcyOA0KCWMwLDUyLjg3OCw0Mi44NjYsOTUuNzQ0LDk1Ljc0NCw5NS43NDRoMjY5LjUxM2M1Mi44NzgsMCw5NS43NDQtNDIuODY2LDk1Ljc0NC05NS43NDRWMTYzLjEzNw0KCUM0NjEuMDAxLDExMC4yNTksNDE4LjEzNSw2Ny4zOTMsMzY1LjI1Nyw2Ny4zOTN6IE0zMDAuNTA2LDIzNy4wNTZsLTEyNi4wNiw2MC4xMjNjLTMuMzU5LDEuNjAyLTcuMjM5LTAuODQ3LTcuMjM5LTQuNTY4VjE2OC42MDcNCgljMC0zLjc3NCwzLjk4Mi02LjIyLDcuMzQ4LTQuNTE0bDEyNi4wNiw2My44ODFDMzA0LjM2MywyMjkuODczLDMwNC4yOTgsMjM1LjI0OCwzMDAuNTA2LDIzNy4wNTZ6Ii8+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8L3N2Zz4NCg==\" width=\"22\" align=\"left\" style=\"margin-right:10px\"/>Youtube<a/>\n* <a href=\"https://medium.com/@codeeasy\"><img src=\"data:image/svg+xml;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCAyNCAyNCIgaGVpZ2h0PSI1MTIiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjUxMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtMjIuMDg1IDQuNzMzIDEuOTE1LTEuODMydi0uNDAxaC02LjYzNGwtNC43MjggMTEuNzY4LTUuMzc5LTExLjc2OGgtNi45NTZ2LjQwMWwyLjIzNyAyLjY5M2MuMjE4LjE5OS4zMzIuNDkuMzAzLjc4M3YxMC41ODNjLjA2OS4zODEtLjA1NS43NzMtLjMyMyAxLjA1bC0yLjUyIDMuMDU0di4zOTZoNy4xNDV2LS40MDFsLTIuNTItMy4wNDljLS4yNzMtLjI3OC0uNDAyLS42NjMtLjM0Ny0xLjA1di05LjE1NGw2LjI3MiAxMy42NTloLjcyOWw1LjM5My0xMy42NTl2MTAuODgxYzAgLjI4NyAwIC4zNDYtLjE4OC41MzRsLTEuOTQgMS44Nzd2LjQwMmg5LjQxMnYtLjQwMWwtMS44Ny0xLjgzMWMtLjE2NC0uMTI0LS4yNDktLjMzMi0uMjE0LS41MzR2LTEzLjQ2N2MtLjAzNS0uMjAzLjA0OS0uNDExLjIxMy0uNTM0eiIgZmlsbD0iIzIxMjEyMSIvPjwvc3ZnPg==\"  width=\"22\" align=\"left\" style=\"margin-right:10px\"/> Medium</a>\n* <a href=\"https://www.linkedin.com/in/tarun-kumar-iit-ism/\"><img src=\"data:image/svg+xml;base64,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\" width=\"22\" align=\"left\" style=\"margin-right:10px\"/>Linkedin</a>\n\n\n\n<hr>"},{"metadata":{},"cell_type":"markdown","source":"# Content:\n\n<hr>\n\n* [Autoencoder Introduction](#Autoencoder-Introduction)\n* [What is Auotencoder?](#What-is-Auotencoder?)\n* [Applications](#Applications)\n* [Image Denoising](#Image-Denoising)\n* [Required Imports](#Required-Imports)\n* [Load/Prepare MNIST data](#Load/Prepare-MNIST-data)\n* [Autoencoder Model](#Autoencoder-Model)\n* [Performance/ Visualise Results](#Performance/-Visualise-Results)\n* **[Denoising Cifar10 Data](#Denoising-Cifar10-Data)** ***<span style=\"color:red\">New</span>***\n    * [Deconvolution (Conv2DTranspose)](#Deconvolution-(Conv2DTranspose))\n    * [Skip Connection](#Skip-Connection)\n* [Conclusion](#Conclusion)\n* [Valuable Feedback](#Valuable-Feedback)\n\n"},{"metadata":{},"cell_type":"markdown","source":"# What is Auotencoder?\nAutoencoder is an unsupervised learning technique that can efficiently learn to compress the data and then reconstruct it from the compressed version of the data. The reconstructed data is close to the original data with minimum reconstruction loss as possible. In other words, autoencoders can learn to encode the essential features of the input data needed to reconstruct the data.\n\nAutoencoders can be considered as a data compression algorithm, where compression and decompression are specific to data and learned automatically from data. Autoencoders are data specific means it can only compress data for which it has been trained. Unlike other compression algorithms such as JPEG can compress any image input, this is not true for autoencoders. If autoencoder is trained on the MNIST dataset, then it can only compress MNIST data. \n\n**Components of Autoencoder:**\n* Enoder: Learns to reduce the data into low dimension\n* Decoder: Takes an encoded version of input and regenerate the data\n* Bottleneck: Compressed version of data\n\n![Autoencode](https://lilianweng.github.io/lil-log/assets/images/denoising-autoencoder-architecture.png)\n\n<hr>\n\n# Applications\n* **Dimensional Reduction:** One of the earliest applications of autoencoder was dimensionality reduction. PCA is a well-known technique to reduce the dimension and can give good results but has limitations as PCA uses linear algebra transformations. In contrast, the neural network can perform non-linear transformations (non-linear activation function). Also, it is much efficient with several hidden layers to train than one transformation with PCA. Autoencoders can significantly well when data is complex.\n![PCA vs Autoencoder](https://www.jeremyjordan.me/content/images/2018/03/Screen-Shot-2018-03-07-at-8.52.21-AM.png)\n\n* **Anomaly Detection:** An well-trained autoencoder can reconstruct the data input data with minimum reconstruction error. Now, if any outliner or anomaly is passed through a trained autoencoder, then the output is quite different from that of input and has a significant error term, representing an anomaly.\n\n* **Data Denoising:** Autoencoders has been proved excellent in denoising task. Layers of autoencoders can easily learn to ignore the noise in the encoded images (bottleneck) and hence can regenerate the denoised image. In this tutorial, I am going to implement this idea on the MNIST dataset. ![Noise Detection](https://miro.medium.com/max/2000/1*sHOPK4Mm5kl5-fju9kLByg.png)\n\n* **Image Super-Resolution:** There are several algorithms for increasing the resolution of images such as bicubic, bilinear, etc., but all are interpolation algorithms and has limitations. Autoencoders are quite impressive in this task.\n![super-resolution image](https://miro.medium.com/max/700/1*5wzZbWyKt9v_vWVmdHmBxA.png)\n\n**Autoencoders can also be used with other techniques to get even better results**\n> For 2D visualization specifically, t-SNE (pronounced \"tee-snee\") is probably the best algorithm around, but it typically requires relatively low-dimensional data. So a good strategy for visualizing similarity relationships in high-dimensional data is to start by using an autoencoder to compress your data into a low-dimensional space (e.g. 32 dimensional), then use t-SNE for mapping the compressed data to a 2D plane. Note that a nice parametric implementation of t-SNE in Keras was developed by Kyle McDonald and is available on Github. Otherwise scikit-learn also has a simple and practical implementation.\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Image Denoising\n![Denoisng](https://miro.medium.com/max/5160/1*SxwRp9i23OM0Up4sEze1QQ@2x.png)\n<br>\nNoise reduction or denoising is the process of removal of noise form any signal or data input. We can train our autoencoder to remove the noise from the data. Data can be images or audio. Let's see how to build an autoencoder. I am going to use the MNIST dataset to keep things simple and easy to understand. But you can implement this idea to build your custom autoencoder.   \n<br>\n\nSteps involved:\n\n* Prepare input data by adding noise to MNIST dataset \n* Build a CNN Autoencoder Network\n* Train the network\n* Test the performance of Autoencoder \n\n**Note: If you want to Learn More about training classifier for MNIST dataset, you can check my [Notebook](https://www.kaggle.com/tarunkr/digit-recognition-tutorial-cnn-99-67-accuracy)**"},{"metadata":{},"cell_type":"markdown","source":"# Required Imports\n\n**Imports:**\n* numpy\n* Matplotlib - for visualization \n* Keras - Building and Training CNN autoencoder"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\n\nimport matplotlib.pyplot as plt \n\nfrom keras.layers import Conv2D, Input, Dense, Dropout, MaxPool2D, UpSampling2D\nfrom keras.models import Model\nfrom keras.datasets import mnist, cifar10\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load/Prepare MNIST data \n\nDownload MNIST data from the Keras dataset. There are several other datasets available at Keras dataset such as CIFAR10, Fashion MNIST, IMDB movie review, etc. [READ MORE](https://keras.io/api/datasets/)\n\nAfter downloading the dataset, reshape the train and test images to the required model input format `[samples, 28, 28, 1]`, where `1` represents the number of channels. And scale the images to `[0,1]` by dividing with `255`."},{"metadata":{"trusted":true},"cell_type":"code","source":"(train, _), (test, _) = mnist.load_data()\n\n# scaling input data\ntrain = train.reshape([-1,28,28,1]) / 255\ntest = test.reshape([-1,28,28,1]) / 255","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding Noise\nWe need to add noise to generate the noisy images. To add noise we can generate array with same dimension of our images with random values between `[0,1]` using normal distribution with `mean = 0` and standard `deviation = 1`.\n<br><br>\nTo generate normal distribution, we can use [np.random.normal(loc,scale,size)](https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html). Then scale the noise by some factor, here I am using `0.3`. After adding noise, pixel values can be out of range `[0,1]`, so we need to clip the values using [np.clip(arr, arr_min, arr_max )](https://numpy.org/doc/1.18/reference/generated/numpy.clip.html)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding noise to data\nnoise = 0.3\ntrain_noise = train + noise * np.random.normal(0, 1, size=train.shape)\ntest_noise = test + noise * np.random.normal(0, 1, size=test.shape)\n\ntrain_noise = np.clip(train_noise, 0, 1)\ntest_noise = np.clip(test_noise, 0, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualise Training data\n\nLet's see how our training data looks like\n#### Input (Noisy Images)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample noisy image\n\nrows = 5 # defining no. of rows in figure\ncols = 6 # defining no. of colums in figure\nsubplot_size = 2\n\nf = plt.figure(figsize=(subplot_size*cols,subplot_size*rows)) # defining a figure \n\nfor i in range(rows*cols): \n    f.add_subplot(rows,cols,i+1) # adding sub plot to figure on each iteration\n    plt.imshow(train_noise[i].reshape([28,28]),cmap=\"Reds\") \n    plt.axis(\"off\")\nplt.savefig(\"digits_noise.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Original Images"},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample original image\n\nrows = 5 # defining no. of rows in figure\ncols = 6 # defining no. of colums in figure\nsubplot_size = 2\nf = plt.figure(figsize=(subplot_size*cols, subplot_size*rows)) # defining a figure \n\nfor i in range(rows*cols): \n    f.add_subplot(rows,cols,i+1) # adding sub plot to figure on each iteration\n    plt.imshow(train[i].reshape([28,28]),cmap=\"Reds\") \n    plt.axis(\"off\")\nplt.savefig(\"digits_original.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Autoencoder Model\nI am using basic CNN architecture to build the model (CNN works well with images). CNN can improve the reconstruction quality.\n<br>\n\nAutoencoder consists of two parts:\n1. **Encoder:**  In Encoder, I am using 2 Conv2D layers and 2 MaxPool2D layers. The output of the 2nd MaxPool2D layer is the encoded features or the input to the Decoder.\n2. **Decoder:** Decoder takes the encoder output as input. Here I used Conv2D and UpSampling2D layers. UpSampling2D layer increases the dimension, opposite of MaxPool which reduces the dimension. Output of decoder has same dimension as the input of the encoder.\n\n**How UpSampling2D works:**\nThe input image of shape 2x2 will be 4x4, like the example below.\n```\nInput = [\n         [1, 2],\n         [3, 4]\n        ]\n\nOutput =  [\n           [1, 1, 2, 2],\n           [1, 1, 2, 2],\n           [3, 3, 4, 4],\n           [3, 3, 4, 4]\n          ]\n```\n\nI am using a functional API of Keras. If you have not familiar functional API, you may find syntax weird. But this is a feature of Python where you can define `__call__()` function to make objects of the class callable. This enables the instance of the class behave as a function.\nSee the example below:\n```\nclass A:\n    def __call__(self):\n        print(\"This is a call function!!\")\n\nobj = A()\nobj()\n\nOutput:\nThis is a call function!!\n```\n\nIf you look carefully, you are doing the same thing in functional API.\n\nRead More:\n* [Functioanl API](https://keras.io/guides/functional_api/)\n* `__call__()` python [Read More1](https://www.geeksforgeeks.org/callable-in-python/) [Read More2](https://www.geeksforgeeks.org/__call__-in-python/)\n* [UpSampling2D](#https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D)"},{"metadata":{},"cell_type":"markdown","source":"### Encoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoder \ninputs = Input(shape=(28,28,1))\n\nx = Conv2D(32, 3, activation='relu', padding='same')(inputs)\nx = MaxPool2D()(x)\nx = Dropout(0.3)(x)\nx = Conv2D(32, 3, activation='relu', padding='same')(x)\nencoded = MaxPool2D()(x)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decoder\nActivation of our output layer is sigmoid to make every value between `[0,1]`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decoder\n\nx = Conv2D(32, 3, activation='relu', padding='same')(encoded)\nx = UpSampling2D()(x)\nx = Dropout(0.3)(x)\nx = Conv2D(32, 3, activation='relu', padding='same')(x)\nx = UpSampling2D()(x)\ndecoded = Conv2D(1, 3, activation='sigmoid', padding='same')(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create and compile model"},{"metadata":{"trusted":true},"cell_type":"code","source":"autoencoder = Model(inputs, decoded)\nautoencoder.compile(optimizer='rmsprop', loss='binary_crossentropy')\n\nautoencoder.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training\nI am training for `50` epochs with batch size `256`. Batch size may vary for your system. If you are using Kaggle or Colab, then `256` will work. Also, remember if you are using other dataset, it may be required to change the number of epochs and batch size."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"epochs = 50\nbatch_size = 256\n\nhistory = autoencoder.fit(train_noise,\n                train,\n                epochs=epochs,\n                batch_size=batch_size,\n                shuffle=True,\n                validation_data=(test_noise, test)\n               )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Performance/ Visualise Results\nTraining seems to be great. Loss and validation loss has decreased as expected."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining Figure\nf = plt.figure(figsize=(10,7))\nf.add_subplot()\n\n#Adding Subplot\nplt.plot(history.epoch, history.history['loss'], label = \"loss\") # Loss curve for training set\nplt.plot(history.epoch, history.history['val_loss'], label = \"val_loss\") # Loss curve for validation set\n\nplt.title(\"Loss Curve\",fontsize=18)\nplt.xlabel(\"Epochs\",fontsize=15)\nplt.ylabel(\"Loss\",fontsize=15)\nplt.grid(alpha=0.3)\nplt.legend()\nplt.savefig(\"Loss_curve.png\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sample few test images"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select few random test images\nnum_imgs = 16\nrand = np.random.randint(1, 100)\n\ntest_images = test_noise[rand:rand+num_imgs] # slicing\ntest_desoided = autoencoder.predict(test_images) # predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize test images with their denoised images\n\nrows = 2 # defining no. of rows in figure\ncols = 8 # defining no. of colums in figure\n\nf = plt.figure(figsize=(2*cols,2*rows*2)) # defining a figure \n\nfor i in range(rows):\n    for j in range(cols): \n        f.add_subplot(rows*2,cols, (2*i*cols)+(j+1)) # adding sub plot to figure on each iteration\n        plt.imshow(test_images[i*cols + j].reshape([28,28]),cmap=\"Reds\") \n        plt.axis(\"off\")\n        \n    for j in range(cols): \n        f.add_subplot(rows*2,cols,((2*i+1)*cols)+(j+1)) # adding sub plot to figure on each iteration\n        plt.imshow(test_desoided[i*cols + j].reshape([28,28]),cmap=\"Reds\") \n        plt.axis(\"off\")\n        \nf.suptitle(\"Autoencoder Results\",fontsize=18)\nplt.savefig(\"test_results.png\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Denoising Cifar10 Data\n\nAfter MNIST dataset, let's try the idea on Cifar10 dataset. If you want to know more about ciraf dataset [Read Here](https://www.cs.toronto.edu/~kriz/cifar.html). You can easly get Cifar10 dataset from keras dataset.\n\n### Loading/ Preparing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"(cifar_train, _), (cifar_test, _) = cifar10.load_data()\n\nsize = 32\nchannel = 3\n# scaling input data\ncifar_train = cifar_train / 255\ncifar_test = cifar_test / 255\n\n# Adding noise mean = 0, std = 0.3\nnoise = 0.3\ncifar_train_noise = cifar_train + noise * np.random.normal(0, 0.3, size=cifar_train.shape) \ncifar_test_noise = cifar_test + noise * np.random.normal(0, 0.3, size=cifar_test.shape)\n\ncifar_train_noise = np.clip(cifar_train_noise, 0, 1)\ncifar_test_noise = np.clip(cifar_test_noise, 0, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sample few noisy and original images"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize few training images with their noisy images\n\nrows = 2 # defining no. of rows in figure\ncols = 8 # defining no. of colums in figure\n\nf = plt.figure(figsize=(2*cols,2*rows*2)) # defining a figure \n\nfor i in range(rows):\n    for j in range(cols): \n        f.add_subplot(rows*2,cols, (2*i*cols)+(j+1)) # adding sub plot to figure on each iteration\n        plt.imshow(cifar_train_noise[i*cols + j]) \n        plt.axis(\"off\")\n        \n    for j in range(cols): \n        f.add_subplot(rows*2,cols,((2*i+1)*cols)+(j+1)) # adding sub plot to figure on each iteration\n        plt.imshow(cifar_train[i*cols + j]) \n        plt.axis(\"off\")\n        \nf.suptitle(\"Sample Training Data\",fontsize=18)\nplt.savefig(\"Cifar-trian.png\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model\nHere I am using more complicated architecture. What's different from last Model Architecture:\n* Conv2DTranspose layer\n* No UpSampling2D layer\n* Skip connection from the encoder to the decoder\n* 3 Conv2D layers followed by BatchNormalization and MaxPool2D\n\n## Deconvolution (Conv2DTranspose)\n**Conv2DTranspose** layer performs the inverse of that of **Conv2D**. It performs deconvolution, and it is much better than **UpSampling**. **UpSampling** layer copies the values to the upscaled dimension. But deconvolution layer can combine the upsampling and convolution in one layer. It fills the value by interpreting the input. But there one disadvantage also, deconvolution can lead to the Checkerboard Artifacts. You can see the artifact in below image. [Read More about Checkerboard Artifacts](https://distill.pub/2016/deconv-checkerboard/)\n> Checkerboard Artifacts\n![Checkerboard Artifacts](https://distill.pub/2016/deconv-checkerboard/assets/deepdream_full_gitter_8x8.png) \n\n> Deconvolution operation\n![deconv](https://miro.medium.com/max/1972/1*kOThnLR8Fge_AJcHrkR3dg.gif)\n\n\n\n>The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution\n‚Äî [A Guide To Convolution Arithmetic For Deep Learning, 2016.](https://arxiv.org/abs/1603.07285)\n\nRead More:\n* [Conv2DTranspose layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose)\n* [UpSampling2D vs Conv2DTranspose](https://machinelearningmastery.com/upsampling-and-transpose-convolution-layers-for-generative-adversarial-networks/)\n* [Deconvolution and Checkerboard Artifacts](https://distill.pub/2016/deconv-checkerboard/)\n\n## Skip Connection\nSkip connections are very useful when working with any network where convolutions and deconvolution operations are performed. It helps in restoring the pieces of information which can be lost during convolution and deconvolutions.\n\n![Skip connection](https://www.researchgate.net/profile/Muzammal_Naseer/publication/323694671/figure/fig5/AS:603205645910017@1520826841252/A-basic-autoencoder-architecture.png)\n\n<br>\n\nYou can go through this research paper [Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections](https://arxiv.org/pdf/1606.08921.pdf).\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Conv2DTranspose, BatchNormalization, add, LeakyReLU\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoder \ninputs = Input(shape=(size,size,channel))\n\nx = Conv2D(32, 3, activation='relu', padding='same')(inputs)\nx = BatchNormalization()(x)\nx = MaxPool2D()(x)\nx = Dropout(0.5)(x)\nskip = Conv2D(32, 3, padding='same')(x) # skip connection for decoder\nx = LeakyReLU()(skip)\nx = BatchNormalization()(x)\nx = MaxPool2D()(x)\nx = Dropout(0.5)(x)\nx = Conv2D(64, 3, activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nencoded = MaxPool2D()(x)\n\n# Decoder\nx = Conv2DTranspose(64, 3,activation='relu',strides=(2,2), padding='same')(encoded)\nx = BatchNormalization()(x)\nx = Dropout(0.5)(x)\nx = Conv2DTranspose(32, 3, activation='relu',strides=(2,2), padding='same')(x)\nx = BatchNormalization()(x)\nx = Dropout(0.5)(x)\nx = Conv2DTranspose(32, 3, padding='same')(x)\nx = add([x,skip]) # adding skip connection\nx = LeakyReLU()(x)\nx = BatchNormalization()(x)\ndecoded = Conv2DTranspose(3, 3, activation='sigmoid',strides=(2,2), padding='same')(x)\n\nautoencoder = Model(inputs, decoded)\nautoencoder.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy')\nautoencoder.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Training\nepochs = 25\nbatch_size = 256\n\nhistory = autoencoder.fit(cifar_train_noise,\n                cifar_train,\n                epochs=epochs,\n                batch_size=batch_size,\n                shuffle=True,\n                validation_data=(cifar_test_noise, cifar_test)\n               )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining Figure\nf = plt.figure(figsize=(10,7))\nf.add_subplot()\n\n#Adding Subplot\nplt.plot(history.epoch, history.history['loss'], label = \"loss\") # Loss curve for training set\nplt.plot(history.epoch, history.history['val_loss'], label = \"val_loss\") # Loss curve for validation set\n\nplt.title(\"Loss Curve\",fontsize=18)\nplt.xlabel(\"Epochs\",fontsize=15)\nplt.ylabel(\"Loss\",fontsize=15)\nplt.grid(alpha=0.3)\nplt.legend()\nplt.savefig(\"Loss_curve_cifar10.png\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select few random test images\nnum_imgs = 48\nrand = np.random.randint(1, cifar_test_noise.shape[0]-48) \n\ncifar_test_images = cifar_test_noise[rand:rand+num_imgs] # slicing\ncifar_test_desoided = autoencoder.predict(cifar_test_images) # predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize test images with their denoised images\n\nrows = 4 # defining no. of rows in figure\ncols = 12 # defining no. of colums in figure\ncell_size = 1.5\nf = plt.figure(figsize=(cell_size*cols,cell_size*rows*2)) # defining a figure \nf.tight_layout()\nfor i in range(rows):\n    for j in range(cols): \n        f.add_subplot(rows*2,cols, (2*i*cols)+(j+1)) # adding sub plot to figure on each iteration\n        plt.imshow(cifar_test_images[i*cols + j]) \n        plt.axis(\"off\")\n        \n    for j in range(cols): \n        f.add_subplot(rows*2,cols,((2*i+1)*cols)+(j+1)) # adding sub plot to figure on each iteration\n        plt.imshow(cifar_test_desoided[i*cols + j]) \n        plt.axis(\"off\")\n\nf.suptitle(\"Autoencoder Results - Cifar10\",fontsize=18)\nplt.savefig(\"test_results_cifar10.png\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### That all for now! You can build your autoencoders üòéüòé. Explore more datasets and have fun training your own autoencoders."},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nAutoencoders are powerful and can do a lot more. Here I introduced you with 2 simple examples, and you can see how well our model performed on the denoising task. There are other uses as well, such as using autoencoder for sequential data. Variational autoencoder (VAE) is a slightly more advanced and modern approach. It can be used to generate the images. In the future, I will try to cover more uses of autoencoder with code implementation. <br>\n\n![faces](https://miro.medium.com/max/1400/1*BaZPg3SRgZGVigguQCmirA.png)\n> Face images generated with a Variational Autoencoder (source: [Wojciech Mormul on Github](https://github.com/WojciechMormul/vae))."},{"metadata":{},"cell_type":"markdown","source":"# About Me\n<hr>\nI am Tarun Kumar from India.<br>\nSoftware Developer at Toppr.<br>\nHobbyist Artist. <br>\nPE Undergrad from IIT ISM Dhanbad. <br>\nDeep Learning, Reinforcement Learning, and Data Science. \n<br>\n<br>\n\n### Follow me:\n\n* <a href=\"https://bit.ly/tarungithub\"><img src=\"data:image/svg+xml;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCAyNCAyNCIgaGVpZ2h0PSI1MTIiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjUxMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtMTIgLjVjLTYuNjMgMC0xMiA1LjI4LTEyIDExLjc5MiAwIDUuMjExIDMuNDM4IDkuNjMgOC4yMDUgMTEuMTg4LjYuMTExLjgyLS4yNTQuODItLjU2NyAwLS4yOC0uMDEtMS4wMjItLjAxNS0yLjAwNS0zLjMzOC43MTEtNC4wNDItMS41ODItNC4wNDItMS41ODItLjU0Ni0xLjM2MS0xLjMzNS0xLjcyNS0xLjMzNS0xLjcyNS0xLjA4Ny0uNzMxLjA4NC0uNzE2LjA4NC0uNzE2IDEuMjA1LjA4MiAxLjgzOCAxLjIxNSAxLjgzOCAxLjIxNSAxLjA3IDEuODAzIDIuODA5IDEuMjgyIDMuNDk1Ljk4MS4xMDgtLjc2My40MTctMS4yODIuNzYtMS41NzctMi42NjUtLjI5NS01LjQ2Ni0xLjMwOS01LjQ2Ni01LjgyNyAwLTEuMjg3LjQ2NS0yLjMzOSAxLjIzNS0zLjE2NC0uMTM1LS4yOTgtLjU0LTEuNDk3LjEwNS0zLjEyMSAwIDAgMS4wMDUtLjMxNiAzLjMgMS4yMDkuOTYtLjI2MiAxLjk4LS4zOTIgMy0uMzk4IDEuMDIuMDA2IDIuMDQuMTM2IDMgLjM5OCAyLjI4LTEuNTI1IDMuMjg1LTEuMjA5IDMuMjg1LTEuMjA5LjY0NSAxLjYyNC4yNCAyLjgyMy4xMiAzLjEyMS43NjUuODI1IDEuMjMgMS44NzcgMS4yMyAzLjE2NCAwIDQuNTMtMi44MDUgNS41MjctNS40NzUgNS44MTcuNDIuMzU0LjgxIDEuMDc3LjgxIDIuMTgyIDAgMS41NzgtLjAxNSAyLjg0Ni0uMDE1IDMuMjI5IDAgLjMwOS4yMS42NzguODI1LjU2IDQuODAxLTEuNTQ4IDguMjM2LTUuOTcgOC4yMzYtMTEuMTczIDAtNi41MTItNS4zNzMtMTEuNzkyLTEyLTExLjc5MnoiIGZpbGw9IiMyMTIxMjEiLz48L3N2Zz4=\" width=\"22\" align=\"left\" style=\"margin-right:10px\"/> GitHub</a>\n* <a href=\"https://bit.ly/tarnkr-youtube\"><img src=\"data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iaXNvLTg4NTktMSI/Pg0KPCEtLSBHZW5lcmF0b3I6IEFkb2JlIElsbHVzdHJhdG9yIDE5LjAuMCwgU1ZHIEV4cG9ydCBQbHVnLUluIC4gU1ZHIFZlcnNpb246IDYuMDAgQnVpbGQgMCkgIC0tPg0KPHN2ZyB2ZXJzaW9uPSIxLjEiIGlkPSJMYXllcl8xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB4PSIwcHgiIHk9IjBweCINCgkgdmlld0JveD0iMCAwIDQ2MS4wMDEgNDYxLjAwMSIgc3R5bGU9ImVuYWJsZS1iYWNrZ3JvdW5kOm5ldyAwIDAgNDYxLjAwMSA0NjEuMDAxOyIgeG1sOnNwYWNlPSJwcmVzZXJ2ZSI+DQo8cGF0aCBzdHlsZT0iZmlsbDojRjYxQzBEOyIgZD0iTTM2NS4yNTcsNjcuMzkzSDk1Ljc0NEM0Mi44NjYsNjcuMzkzLDAsMTEwLjI1OSwwLDE2My4xMzd2MTM0LjcyOA0KCWMwLDUyLjg3OCw0Mi44NjYsOTUuNzQ0LDk1Ljc0NCw5NS43NDRoMjY5LjUxM2M1Mi44NzgsMCw5NS43NDQtNDIuODY2LDk1Ljc0NC05NS43NDRWMTYzLjEzNw0KCUM0NjEuMDAxLDExMC4yNTksNDE4LjEzNSw2Ny4zOTMsMzY1LjI1Nyw2Ny4zOTN6IE0zMDAuNTA2LDIzNy4wNTZsLTEyNi4wNiw2MC4xMjNjLTMuMzU5LDEuNjAyLTcuMjM5LTAuODQ3LTcuMjM5LTQuNTY4VjE2OC42MDcNCgljMC0zLjc3NCwzLjk4Mi02LjIyLDcuMzQ4LTQuNTE0bDEyNi4wNiw2My44ODFDMzA0LjM2MywyMjkuODczLDMwNC4yOTgsMjM1LjI0OCwzMDAuNTA2LDIzNy4wNTZ6Ii8+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8L3N2Zz4NCg==\" width=\"22\" align=\"left\" style=\"margin-right:10px\"/>Youtube<a/>\n* <a href=\"https://medium.com/@codeeasy\"><img src=\"data:image/svg+xml;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCAyNCAyNCIgaGVpZ2h0PSI1MTIiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjUxMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtMjIuMDg1IDQuNzMzIDEuOTE1LTEuODMydi0uNDAxaC02LjYzNGwtNC43MjggMTEuNzY4LTUuMzc5LTExLjc2OGgtNi45NTZ2LjQwMWwyLjIzNyAyLjY5M2MuMjE4LjE5OS4zMzIuNDkuMzAzLjc4M3YxMC41ODNjLjA2OS4zODEtLjA1NS43NzMtLjMyMyAxLjA1bC0yLjUyIDMuMDU0di4zOTZoNy4xNDV2LS40MDFsLTIuNTItMy4wNDljLS4yNzMtLjI3OC0uNDAyLS42NjMtLjM0Ny0xLjA1di05LjE1NGw2LjI3MiAxMy42NTloLjcyOWw1LjM5My0xMy42NTl2MTAuODgxYzAgLjI4NyAwIC4zNDYtLjE4OC41MzRsLTEuOTQgMS44Nzd2LjQwMmg5LjQxMnYtLjQwMWwtMS44Ny0xLjgzMWMtLjE2NC0uMTI0LS4yNDktLjMzMi0uMjE0LS41MzR2LTEzLjQ2N2MtLjAzNS0uMjAzLjA0OS0uNDExLjIxMy0uNTM0eiIgZmlsbD0iIzIxMjEyMSIvPjwvc3ZnPg==\"  width=\"22\" align=\"left\" style=\"margin-right:10px\"/> Medium</a>\n* <a href=\"https://www.linkedin.com/in/tarun-kumar-iit-ism/\"><img src=\"data:image/svg+xml;base64,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\" width=\"22\" align=\"left\" style=\"margin-right:10px\"/>Linkedin</a>\n\n\n<hr>\n\n# Feedback\n* **Your feedback is much appreciated**\n* **Please UPVOTE if you LIKE this notebook**\n* **Comment if you have any doubts or you found any errors in the notebook**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}