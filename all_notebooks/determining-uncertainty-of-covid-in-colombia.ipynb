{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Defeating under reporting: neural networks for estimation of the true scale of Covid-19 in Colombia\n\nSir Thomas Bayes said that statistics is not the study of outcomes, but of uncertainty. We are living in uncertain times, so it might be a good time to start listening. One of the most powerful tools we have in the fight is information. Every hour of every day, public and private entities are collecting data with the hope that with enough of it, we may plot a course ahead. Although this approach is founden in sound principles, most treatments assume something misleading: we have complete information.\n\nNot only there is a period of incubation of the disease before starting to show simptoms, but those simptoms can be misinterpreted. That, and other factors such as healthcare access and intentional misreporting may skew the data and prevent the through analysis that can be done.\n\nIn this kernel, I propose a method for estimating the uncertainty of statistical reports of Covid-19 cases in the case study of Colombia between the months of March and April of 2020. It begins with an exploration of the behavior of the countries relative to each other based on the historic day by day increase in cases and deaths. This exploration was done via different clustering techniques.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport matplotlib.pyplot as plt\nimport sklearn.cluster\nfrom matplotlib.lines import Line2D\nimport umap\nimport torch\nimport sklearn.neural_network\n\n%matplotlib inline\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data ordering\nIn order to better understand the temporal evolution of the cases, we manipulated the data to counstruct an evolution vector, whose components are the increases in cases and deaths per date.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"eddc=pd.read_csv('/kaggle/input/uncover/UNCOVER/ECDC/current-data-on-the-geographic-distribution-of-covid-19-cases-worldwide.csv')\neddc=eddc.drop('daterep',axis='columns')\neddc['casesNorm']=eddc['cases']/eddc['popdata2018']\neddc['deathsNorm']=eddc['deaths']/eddc['popdata2018']\neddc=eddc.dropna(axis=0)\neddc=eddc.set_index(eddc['countriesandterritories'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can visualize the data as is and look for any patterns. Regional patterns, for instance, may give us insight into colective behavior.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"april4th=(eddc[(eddc['day']==28) & (eddc['month']==4)])\nplt.figure(figsize=(14,7))\n\nasia=april4th[april4th['continentexp']=='Asia']\namerica=april4th[april4th['continentexp']=='America']\neurope=april4th[april4th['continentexp']=='Europe']\noceania=april4th[april4th['continentexp']=='Oceania']\nafrica=april4th[april4th['continentexp']=='Africa']\n\nplt.subplot(1,2,1)\n\nplt.plot(asia['casesNorm'],asia['deathsNorm'],'.',label='Asia',markersize=10,color='red')\nplt.plot(america['casesNorm'],america['deathsNorm'],'.',label='America',markersize=10,color='blue')\nplt.plot(europe['casesNorm'],europe['deathsNorm'],'.',label='Europe',markersize=10,color='green')\nplt.plot(oceania['casesNorm'],oceania['deathsNorm'],'.',label='Oceania',markersize=10,color='black')\nplt.plot(africa['casesNorm'],africa['deathsNorm'],'.',label='Africa',markersize=10,color='cyan')\n\n#for index,row in april4th.iterrows():\n#    plt.annotate(s=row['countriesandterritories'],xy=(row['cases'],row['deaths']))\nplt.xlabel('Cases')\nplt.ylabel('Deaths')\nplt.legend()\n\nplt.subplot(1,2,2)\n\nplt.plot(asia['casesNorm'],asia['deathsNorm'],'.',label='Asia',markersize=10,color='red')\nplt.plot(america['casesNorm'],america['deathsNorm'],'.',label='America',markersize=10,color='blue')\nplt.plot(europe['casesNorm'],europe['deathsNorm'],'.',label='Europe',markersize=10,color='green')\nplt.plot(oceania['casesNorm'],oceania['deathsNorm'],'.',label='Oceania',markersize=10,color='black')\nplt.plot(africa['casesNorm'],africa['deathsNorm'],'.',label='Africa',markersize=10,color='cyan')\nplt.title('Deaths and cases')\n#for index,row in april4th.iterrows():\n#    plt.annotate(s=row['countriesandterritories'],xy=(row['cases'],row['deaths']))\nplt.xscale('log')\nplt.yscale('log')\nplt.xlabel('Cases per capita')\nplt.ylabel('Deaths per capita')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Besides a european cluster, nothing jumps out from a simple death per cases analysis, not even a range. This points towards an heavy dependence of the particular country conditions. We might try and study the temporal evolution vectors and look for similarities.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne = sklearn.manifold.TSNE(perplexity=100)\npredictors=['casesNorm','deathsNorm']\ntsne.fit(april4th[predictors])\nembedding = tsne.embedding_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,7))\nplt.subplot(1,2,1)\ni=0\nfor index,row in april4th.iterrows():\n    continent=row['continentexp']\n    if continent=='Asia':\n        plt.scatter(embedding[i,0], embedding[i,1], s=3.0, color='red')\n    if continent=='America':\n        plt.scatter(embedding[i,0], embedding[i,1], s=3.0, color='blue')\n    if continent=='Europe':\n        plt.scatter(embedding[i,0], embedding[i,1], s=3.0, color='green')\n    if continent=='Africa':\n        plt.scatter(embedding[i,0], embedding[i,1], s=3.0, color='cyan')\n    if continent=='Oceania':\n        plt.scatter(embedding[i,0], embedding[i,1], s=3.0, color='black')\n    i=i+1\nlegend_elements = [Line2D([0], [0], color='w', marker='o', markerfacecolor='red',label='Asia'),\n                   Line2D([0], [0], color='w', marker='o', markerfacecolor='blue',label='America'),\n                  Line2D([0], [0], color='w', marker='o', markerfacecolor='green',label='Europe'),\n                  Line2D([0], [0], color='w', marker='o', markerfacecolor='black',label='Oceania'),\n                  Line2D([0], [0], color='w', marker='o', markerfacecolor='cyan',label='Africa')]\nplt.legend(handles=legend_elements)\n\nplt.subplot(1,2,2)\n\ni=0\nfor index,row in april4th.iterrows():\n    #plt.annotate(s=row['countriesandterritories'],xy=(embedding[i,0],[i,1]))\n    continent=row['continentexp']\n    if continent=='Asia':\n        plt.scatter(embedding[i,0], embedding[i,1], s=3.0, color='red')\n    if continent=='America':\n        plt.scatter(embedding[i,0], embedding[i,1], s=3.0, color='blue')\n    if continent=='Europe':\n        plt.scatter(embedding[i,0], embedding[i,1], s=3.0, color='green')\n    if continent=='Africa':\n        plt.scatter(embedding[i,0], embedding[i,1], s=3.0, color='cyan')\n    if continent=='Oceania':\n        plt.scatter(embedding[i,0], embedding[i,1], s=3.0, color='black')\n    labels=['Colombia','Peru','Ecuador','Venezuela','Canada','Germany','Singapore','Brazil','China','France']\n    for label in labels:\n        if row['countriesandterritories']==label:\n            plt.annotate(s=label,xy=(embedding[i,0],embedding[i,1])) \n    i=i+1\nlegend_elements = [Line2D([0], [0], color='w', marker='o', markerfacecolor='red',label='Asia'),\n                   Line2D([0], [0], color='w', marker='o', markerfacecolor='blue',label='America'),\n                  Line2D([0], [0], color='w', marker='o', markerfacecolor='green',label='Europe'),\n                  Line2D([0], [0], color='w', marker='o', markerfacecolor='black',label='Oceania'),\n                  Line2D([0], [0], color='w', marker='o', markerfacecolor='cyan',label='Africa')]\nplt.legend(handles=legend_elements)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This crescent moon shape with a lasso at the end is quite sugestive, the countries in the lower end of the crest are countries with high reporting metrics, independent of their success mitigating policies. Now, we can use dimension reducing clustering to study similarities between temporal evolutions for different countries.\n\nFirst, we create the temporal evolution vectors for each country.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"time_evol=pd.DataFrame(index=list(eddc.countriesandterritories.unique()))\ncountries=eddc.countriesandterritories.unique()\ndates=eddc.drop_duplicates(['month','day'])\ndates=np.vstack(((np.array(dates['day'])),(np.array(dates['month']))))\ndates=dates.T\ndates=dates[:45]\nnan_col=np.empty(len(countries))\nnan_col[:]=np.NaN\ntime_evol['Continent']=nan_col\n\nfor date in dates:\n        \n    #date=eddc[(eddc['day']==day) & (eddc['month']==month)]\n    #if eddc_it['countriesandterritories']==country:\n    day=date[0]\n    month=date[1]\n    #print('Month: ',month,'Day: ',day)\n    labelCases='casesNorm '+str(day)+'-'+str(month)\n    labelDeaths='deathsNorm '+str(day)+'-'+str(month)\n    time_evol[labelCases]=nan_col\n    time_evol[labelDeaths]=nan_col\n    time_evol['Cases']=nan_col\n    time_evol['Deaths']=nan_col\n    cases=0\n    deaths=0\n    temp =eddc[(eddc['day']==day) & (eddc['month']==month)]\n\n    for country,row in temp.iterrows():\n        casesNorm=row['casesNorm']\n        deathsNorm=row['deathsNorm']\n        continent=row['continentexp']\n        time_evol.loc[country,labelCases]=casesNorm\n        cases+=casesNorm\n        time_evol.loc[country,labelDeaths]=deathsNorm\n        deaths+=deathsNorm\n        time_evol.loc[country,'Continent']=continent\n        \n        time_evol.loc[country,'Cases']=cases\n        time_evol.loc[country,'Deaths']=deaths\n#time_evol=time_evol.drop(0,axis='columns')\ntime_evol=time_evol.dropna(axis=0)\ntime_evol","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then, we apply t-SNE clustering to search for local similarities.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne = sklearn.manifold.TSNE(perplexity=100)\ntsne.fit(time_evol.drop(['Continent','Cases','Deaths'],axis='columns'))\ntsne_embedding = tsne.embedding_\n\nplt.figure(figsize=(10,7))\ni=0\nfor index,row in time_evol.iterrows():\n    #plt.annotate(s=row['countriesandterritories'],xy=(tsne_embedding[i,0],[i,1]))\n    continent=row['Continent']\n    if continent=='Asia':\n        plt.scatter(tsne_embedding[i,0], tsne_embedding[i,1], s=3.0, color='red')\n    if continent=='America':\n        plt.scatter(tsne_embedding[i,0], tsne_embedding[i,1], s=3.0, color='blue')\n    if continent=='Europe':\n        plt.scatter(tsne_embedding[i,0], tsne_embedding[i,1], s=3.0, color='green')\n    if continent=='Africa':\n        plt.scatter(tsne_embedding[i,0], tsne_embedding[i,1], s=3.0, color='cyan')\n    if continent=='Oceania':\n        plt.scatter(tsne_embedding[i,0], tsne_embedding[i,1], s=3.0, color='black')\n    labels=['Colombia','Peru','Ecuador','Venezuela','Canada','Germany','Singapore','France','United_States_of_America']\n    for label in labels:\n        if index==label:\n            plt.annotate(s=label,xy=(tsne_embedding[i,0],tsne_embedding[i,1])) \n    i=i+1\nlegend_elements = [Line2D([0], [0], color='w', marker='o', markerfacecolor='red',label='Asia'),\n                   Line2D([0], [0], color='w', marker='o', markerfacecolor='blue',label='America'),\n                  Line2D([0], [0], color='w', marker='o', markerfacecolor='green',label='Europe'),\n                  Line2D([0], [0], color='w', marker='o', markerfacecolor='black',label='Oceania'),\n                  Line2D([0], [0], color='w', marker='o', markerfacecolor='cyan',label='Africa')]\nplt.legend(handles=legend_elements)\nplt.title('t-SNE clustering',size=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we see a comet-like clustering, with a good portion of African countries near the core and well reported territories towards the tail. May be a more global aproach can be used. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"reducer = umap.UMAP(n_neighbors=5)\nreducer.fit(time_evol.drop(['Continent','Deaths','Cases'],axis='columns'))\numap_embedding = reducer.transform(time_evol.drop(['Continent','Deaths','Cases'],axis='columns'))\nplt.figure(figsize=(10,7))\ni=0\nfor index,row in time_evol.iterrows():\n    #plt.annotate(s=row['countriesandterritories'],xy=(umap_embedding[i,0],[i,1]))\n    continent=row['Continent']\n    if continent=='Asia':\n        plt.scatter(umap_embedding[i,0], umap_embedding[i,1], s=3.0, color='red')\n    if continent=='America':\n        plt.scatter(umap_embedding[i,0], umap_embedding[i,1], s=3.0, color='blue')\n    if continent=='Europe':\n        plt.scatter(umap_embedding[i,0], umap_embedding[i,1], s=3.0, color='green')\n    if continent=='Africa':\n        plt.scatter(umap_embedding[i,0], umap_embedding[i,1], s=3.0, color='cyan')\n    if continent=='Oceania':\n        plt.scatter(umap_embedding[i,0], umap_embedding[i,1], s=3.0, color='black')\n    labels=['Colombia','Peru','Ecuador','Venezuela','Canada','Germany','Singapore','United_States_of_America','Chile','China']\n    for label in labels:\n        if index==label:\n            plt.annotate(s=label,xy=(umap_embedding[i,0],umap_embedding[i,1])) \n    i=i+1\nlegend_elements = [Line2D([0], [0], color='w', marker='o', markerfacecolor='red',label='Asia'),\n                   Line2D([0], [0], color='w', marker='o', markerfacecolor='blue',label='America'),\n                  Line2D([0], [0], color='w', marker='o', markerfacecolor='green',label='Europe'),\n                  Line2D([0], [0], color='w', marker='o', markerfacecolor='black',label='Oceania'),\n                  Line2D([0], [0], color='w', marker='o', markerfacecolor='cyan',label='Africa')]\nplt.legend(handles=legend_elements)\nplt.title('UMAP clustering',size=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When global features are studied, we can clearly see a sharpening of our suspictions, with the well reported cases closer to the (mainly) european cluster and the under reported ones to the (mainly) african one. Colombia is closer to the latter, suggesting a lower level of reporting and a need for estimation of the uncertainty of the reports. To estimate said uncertainty, I propose a neural network to predict the number of cases per capita based on the passed history of cases and deaths in each country.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"net = torch.nn.Sequential(\n                torch.nn.Linear(90, 40),\n                torch.nn.ReLU(),\n                torch.nn.Linear(40, 20),\n                torch.nn.ReLU(),\n                torch.nn.Linear(20, 10),\n                torch.nn.ReLU(),\n                torch.nn.Linear(10, 5),\n                torch.nn.ReLU(),\n                torch.nn.Linear(5, 1)\n)\n#Dado que no es un problema de clasificación, debemos utilizar un criterio diferente. MSELoss mide la distancia entre la\n#predicción y el valor verdadero\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.SGD(net.parameters(), lr=0.02) #lr: learning rate","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this neural network, the mean squared error (MSE) metric was used to compare the prediction of the network with the most recent values of cases. Given that this square root of this metric is in units of the cases per capita, its square root can be interpreted as a standard deviation from the actual value. For training the network, we pass the temporal evolution for every country and determine the absolute deviation between the oredicted and true values via the loss criterion.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 100\n\nexcep_deaths=[s for s in time_evol.keys() if 'deaths' in s]\nonly_cases=time_evol.drop(excep_deaths,axis='columns')\ntarget_cases=only_cases['Cases']\nonly_cases=time_evol.drop(['Continent','Cases','Deaths'],axis='columns')\nloss_values = np.zeros((len(only_cases),epochs))\n\nit=0\nonly_cases_countries=[]\nfor index,row in only_cases.iterrows():\n    inputs = torch.autograd.Variable(torch.Tensor(row.values).float())\n    targets = torch.autograd.Variable(torch.Tensor(row.values).float())\n    for epoch in range(epochs):\n\n        optimizer.zero_grad()\n        out = net(inputs)\n        loss = criterion(out, targets)\n        loss.backward()\n        optimizer.step()\n\n        loss_values[it,epoch] = loss.item()\n    only_cases_countries.append(index)\n    it+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nplt.imshow(loss_values.T)\nplt.colorbar()\nplt.xlabel('Countries',size=20)\nplt.ylabel('Epochs',size=20)\nplt.title('Loss criterion for training country by country for 100 epochs',size=24)\nplt.xticks(np.arange(len(only_cases_countries)),only_cases_countries,rotation=90,size=9)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although, there is improvement the more we train the network, 100  epochs is a reasonably high computation parameter for a study of the general features of the desviation for every country. It only rests to study the deviation for the particular case of Colombia.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"casesCol =time_evol.loc['Colombia',[s for s in time_evol.keys() if 'cases' in s]]\ncasesCol= casesCol[::-1]\ni=1\nwhile i<45:\n    casesCol[i]=casesCol[i]+casesCol[i-1]\n    i+=1\ndeathsCol =time_evol.loc['Colombia',[s for s in time_evol.keys() if 'deaths' in s]]\ndeathsCol= deathsCol[::-1]\ni=1\nwhile i<45:\n    deathsCol[i]=deathsCol[i]+deathsCol[i-1]\n    i+=1\n\nstrDates=[]\nfor date in dates:\n    strDates.append(str(date[0])+'/'+str(date[1]))\nstrDates= strDates[::-1]\n\nplt.figure(figsize=(14,10))\nUncertCol=np.array(casesCol+12*(loss_values[99,only_cases_countries.index('Colombia')])**0.5)\nDowncertCol=np.array(casesCol-12*(loss_values[99,only_cases_countries.index('Colombia')])**0.5)\n\ndias=np.array(np.arange(len(strDates)))\nplt.plot(dias,casesCol,'.')\n#plt.plot(dias,UncertCol,'.')\nplt.fill_between(x=dias,y1=UncertCol.astype(float),y2=DowncertCol.astype(float),alpha=0.2)\nplt.xticks(dias,strDates,rotation=45)\nplt.xlabel('Date',size=20)\nplt.ylabel('Covid cases per capita',size=20)\nplt.title('Confiedence interval for Covid cases in Colombia',size=24)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see a high level of variability and a wide range of values avaliable of the possible evolution of the system. This is a rough estimate based on the similarity of the data in every country and to minimize the difference between a prediction and the real value.\n\n## Limitations\n\nIt bears to say that the method exposed here only applies for determination of uncertanty based in previous data and does not aim to predict the evolution of the disease or prove the effectivity of public policies. A high dispersion may also affect the stability of the network, as it would happen in a country with a poor healthcare system that reports the cases in batches, such as week per week.\n\n## Outlook and final remarks\n\nThis is a first approximation to the problem of determining uncertainty and it is based on the data alone. Proposing a model for the evolution of the virus may help in improving the calculation of uncertainty via logistic regression, for instance. This project has been quite fun to develop and has showed me a new perspective on how we read and interpret data. I eagerly await for comments and ideas in how I can improve my estimations.\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}