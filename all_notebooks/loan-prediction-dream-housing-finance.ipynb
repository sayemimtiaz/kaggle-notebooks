{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.axes\n\nfrom sklearn.svm import SVC,LinearSVC, SVR\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.externals import joblib\n\nfrom sklearn.metrics import roc_auc_score, precision_recall_fscore_support, classification_report \nfrom sklearn.metrics import precision_recall_curve, confusion_matrix, roc_curve, auc\n\nfrom scipy.stats import skew, kurtosis\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\n#plt.style.use('dark_background')\ncurrent_palette = sns.color_palette('colorblind')\nsns.palplot(current_palette)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### import data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_set = pd.read_csv('../input/loan_data_set.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_set.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_set.LoanAmount = data_set.LoanAmount*1000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_set.Loan_Status.value_counts(normalize = True).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(data = data_set.Loan_Status.value_counts(normalize = True).reset_index(),\n            x = 'index',\n            y = 'Loan_Status',\n            palette=current_palette)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- we can see the data is not balanced, 69% approved for load while 31% where not\n- lets check the proportion in each feature between Y and N"},{"metadata":{"trusted":true},"cell_type":"code","source":"def prop_check(data):\n    f, axes = plt.subplots(6,2,figsize= (12,20))\n    plt.suptitle('Train data, count vs proportion of each object feature vs Loan_Status', size =16, y = 0.9)\n    col = data.columns[1:data.shape[1]-1]\n    r = 0\n    for i in col:\n        if (data.dtypes == 'object')[i]:        \n            data_prop = (data['Loan_Status']\n                          .groupby(data[i])\n                          .value_counts(normalize = True)\n                          .rename('prop')\n                          .reset_index())\n            sns.countplot(data = data, \n                          x ='Loan_Status', \n                          hue = i, \n                          ax = axes[r,0], \n                          hue_order=data_prop[i].unique(), \n                          palette=current_palette)\n            sns.barplot(data = data_prop, \n                        x = 'Loan_Status', \n                        y = 'prop',\n                        hue = i,\n                        ax = axes[r,1],\n                        palette=current_palette)\n            r = r+1\nprop_check(data_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- as the graphs indicate, they are mostly distributed evenly between the features\n- we can also learn there is no ovious correlation jumping out at this stage\n- lets make the Loan_ID column to be an index for further analysis (we could also delete it)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_index(df):\n    df.set_index('Loan_ID', inplace=True)\n    return df\ndata_set = make_index(data_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- drop all NAs\n- previous attempts to fill the gaps showed that most frequent dummy classifier bring the best results, although increases the bias within the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_set.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data_set, hue = 'Loan_Status', palette=current_palette)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- we can see here first sign to some correlation with both the incomes features and the loan amount\n- lets work them out to be more models friendly (normaly distributed and with 0 mean)\n"},{"metadata":{},"cell_type":"markdown","source":"## Categorizing numerically object columns by column name\n- each column translated to binary value\n- multi variables columns slpitted with dummy columns\n- drop the duplications in the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def categorize(df):\n    df.Gender.replace({'Male': 1, 'Female': 0}, inplace = True)\n    df.Married.replace({'Yes': 1, 'No': 0}, inplace = True)\n    df.Education.replace({'Graduate': 1, 'Not Graduate': 0}, inplace = True)\n    df.Self_Employed.replace({'Yes': 1, 'No': 0}, inplace = True)\n    df = df.join(pd.get_dummies(df.Dependents, prefix='Dependents'))\n    df.drop(columns= ['Dependents', 'Dependents_3+'], inplace=True)\n    df = df.join(pd.get_dummies(df.Property_Area, prefix='Property_Area'))\n    df.drop(columns= ['Property_Area', 'Property_Area_Rural'], inplace=True)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_set = categorize(data_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_set.Loan_Status.replace({'Y': 1, 'N':0}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model selection"},{"metadata":{},"cell_type":"markdown","source":"> ### Adding new features:\n- naive estimation of monthly loan return (LoanAmount/Loan_Amount_Term) normalized and with ln() let us get the distribution as closer as can be to normal distribution\n- total income (ApplicantIncome + CoaplicantIncome) normlized and with ln() let us get the distribution as closer as can be to normal distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_feat(df):\n    ln_monthly_return = np.log(df.LoanAmount/df.Loan_Amount_Term)\n    df['ln_monthly_return'] = (ln_monthly_return - np.mean(ln_monthly_return))/(np.std(ln_monthly_return)/np.sqrt(len(ln_monthly_return)))\n    \n    ln_total_monthly_income = np.log(df.ApplicantIncome + df.CoapplicantIncome)\n    df['ln_total_income'] = (ln_total_monthly_income - np.mean(ln_total_monthly_income))/(np.std(ln_total_monthly_income)/np.sqrt(len(ln_total_monthly_income)))\n    \n    ln_LoanAmount = np.log(1000*df.LoanAmount)\n    df['ln_LoanAmount'] = (ln_LoanAmount - np.mean(ln_LoanAmount))/(np.std(ln_LoanAmount)/np.sqrt(len(ln_LoanAmount)))\n    \n    \n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_set = add_feat(data_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- lets check the new features distributions"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef norm_plt(df):\n    f, axes = plt.subplots(3,2,figsize= (12,15),squeeze=False)\n\n    ######total income########\n    sns.distplot(df.ln_total_income\n                 ,ax=axes[0,0]).set_title('ln(total_income) norm distribution')\n    #axes[0,0].set_xlim(-100,100)\n    axes[0,0].text(0.03, 0.85,\n                   'skew: {0:0.2}\\nkurtosis: {1:0.2f}'\n                   .format(skew(df.ln_total_income),\n                                          kurtosis(df.ln_total_income)),\n                   horizontalalignment='left',\n                   verticalalignment='bottom',\n                   transform=axes[0,0].transAxes,\n                   bbox={'facecolor': 'white'})\n    sns.distplot((df.ApplicantIncome+df.CoapplicantIncome),\n                 ax=axes[0,1]).set_title('total_income distribution')\n    axes[0,1].text(0.7, 0.85,\n                   'skew: {0:0.2f}\\nkurtosis: {1:0.2f}'\n                   .format(skew(df.ApplicantIncome+df.CoapplicantIncome),\n                           kurtosis(df.ApplicantIncome+df.CoapplicantIncome)),\n                   horizontalalignment='left',\n                   verticalalignment='bottom',\n                   transform=axes[0,1].transAxes,\n                   bbox={'facecolor': 'white'})\n\n    #######monthly return###########\n    sns.distplot(df.ln_monthly_return,\n                 ax=axes[1,0]).set_title('ln(monthly_return) norm distribution')\n    #axes[1,0].set_xlim(-100,100)\n    axes[1,0].text(0.03, 0.85,\n                   'skew: {0:0.2}\\nkurtosis: {1:0.2f}'\n                   .format(skew(df.ln_monthly_return),\n                           kurtosis(df.ln_monthly_return)),\n                   horizontalalignment='left',\n                   verticalalignment='bottom',\n                   transform=axes[1,0].transAxes,\n                   bbox={'facecolor': 'white'})\n\n    sns.distplot((1000*df.LoanAmount/df.Loan_Amount_Term),\n                 ax=axes[1,1]).set_title('monthly_return distribution')\n    axes[1,1].text(0.7, 0.85,\n                   'skew: {0:0.2f}\\nkurtosis: {1:0.2f}'\n                   .format(skew(df.LoanAmount/df.Loan_Amount_Term),\n                           kurtosis(df.LoanAmount/df.Loan_Amount_Term)),\n                   horizontalalignment='left',\n                   verticalalignment='bottom',\n                   transform=axes[1,1].transAxes,\n                   bbox={'facecolor': 'white'})\n\n    ######norm ln_LoanAmount########\n    sns.distplot(df.ln_LoanAmount\n                 ,ax=axes[2,0]).set_title('ln(LoanAmount) norm distribution')\n    #axes[2,0].set_xlim(-100,100)\n    axes[2,0].text(0.03, 0.85,\n                   'skew: {0:0.2}\\nkurtosis: {1:0.2f}'\n                   .format(skew(df.ln_LoanAmount),\n                                          kurtosis(df.ln_LoanAmount)),\n                   horizontalalignment='left',\n                   verticalalignment='bottom',\n                   transform=axes[2,0].transAxes,\n                   bbox={'facecolor': 'white'})\n    sns.distplot((df.LoanAmount),\n                 ax=axes[2,1]).set_title('LoanAmount distribution')\n    axes[2,1].text(0.7, 0.85,\n                   'skew: {0:0.2f}\\nkurtosis: {1:0.2f}'\n                   .format(skew(df.LoanAmount),\n                           kurtosis(df.LoanAmount)),\n                   horizontalalignment='left',\n                   verticalalignment='bottom',\n                   transform=axes[2,1].transAxes,\n                   bbox={'facecolor': 'white'})\n    \n    \n    ####### adding grid to the graph#########\n    for i in range(3):\n        for j in range(2):\n            axes[i,j].grid(b=True, which='both', axis='both', color='grey', linestyle = '--', linewidth = '0.3')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"norm_plt(data_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- the skew and the kurtosis are much closer to 0, as expected from normalized normal distribution\n- now lets drop the neglectable features those are dependeds of the new features created above\n- i found in previous runs the Married and Dependents are neglectable too"},{"metadata":{"trusted":true},"cell_type":"code","source":"dropit=['LoanAmount', \n        'Loan_Amount_Term', \n        'ApplicantIncome',\n        'CoapplicantIncome',\n        'Married',\n        'Dependents_0',\n        'Dependents_1',\n        'Dependents_2']\ndata_set.drop(columns=dropit, \n           inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- the following is a sanity check for the distribution between Y and N in the target column\n- the distribution between the values didn't change much (less than 0.5%)"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_set['Loan_Status'].value_counts(normalize=True)\nsns.barplot(data = data_set.Loan_Status.value_counts(normalize = True).reset_index(),\n            x = 'index',\n            y = 'Loan_Status',\n            palette=current_palette)\nplt.grid(b=True, which='both', axis='both', color='grey', linestyle = '--', linewidth = '0.3')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Classifiers optimization"},{"metadata":{},"cell_type":"markdown","source":"- In our case: binary classification, the \"cross_val_score\" function uses StratifiedKFold cross validation to reduce the bias effect in imbalanced data.\n- it is a good approach for the cases the target column distribution is biased"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef cv_check(X,y, CV):\n    models = [\n        RandomForestClassifier(criterion='gini',\n                               n_estimators=50,\n                               max_depth=11,\n                               max_features=6,\n                               random_state=42,\n                               class_weight='balanced_subsample',\n                               n_jobs=4),\n        SVC(C=1, kernel='rbf', gamma='auto',random_state=42,class_weight='balanced'),\n        LogisticRegression(solver='lbfgs',\n                           multi_class='ovr',\n                           max_iter=500,\n                           C=1,\n                           random_state=42,\n                           class_weight='balanced'),\n        GaussianNB(),\n        #LinearSVC(C=1, \n        #         max_iter=500,\n        #          random_state=0),\n        DummyClassifier(strategy='most_frequent',random_state=42)\n    ]\n\n    entries = []\n    \n    for model in models:\n        model_name = model.__class__.__name__\n        print (\"Currently fitting: {}\".format(model_name))\n        accuracies = cross_val_score(model,\n                                     X,\n                                     y, \n                                     scoring='roc_auc', cv=CV, n_jobs=4)\n        for fold_idx, accuracy in enumerate(accuracies):\n            entries.append((model_name, fold_idx, accuracy))\n        cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'roc_auc'])\n        \n    return cv_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cv_bp(cv_df, title, axes):\n    axes.grid(b=True, \n              which='both', \n              axis='both', \n              color='grey', \n              linestyle = '--', \n              linewidth = '0.3')    \n    sns.boxplot(x='model_name', \n                y='roc_auc', \n                data=cv_df, \n                width = 0.5, \n                ax=axes,\n                palette=current_palette).set_title(title)\n    sns.stripplot(x='model_name', \n                  y='roc_auc',\n                  data=cv_df, \n                  size=5, jitter=True, \n                  edgecolor=\"grey\", \n                  linewidth=1, \n                  ax=axes)\n    plt.ylim(0.2,1)\n    plt.savefig('{}.png'.format(title), format='png')\n    #plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(1,1,figsize= (20,8),squeeze=False, sharey=True)\ncv_bp(cv_check(data_set.drop(['Loan_Status'],axis=1),\n               data_set.Loan_Status,10), '{} without NAs'.format('train'),axes[0,0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- using grid search cross validation we scanned to find the optimal values for each model variables\n- the values chosen after several runs to get the optimum variables within the optimal range for the roc_auc as closer we can get to 1"},{"metadata":{},"cell_type":"markdown","source":"### RandomForest"},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_score(train, model, grid_values, scorers_list):\n    X_train = train.drop(columns=['Loan_Status'])\n    y_train = train['Loan_Status']\n    \n    clf_dict = {}\n    \n    for i, scorer in enumerate(scorers_list):\n        clf_eval = GridSearchCV(model, param_grid=grid_values, scoring=scorer, cv=5, iid=False)\n        clf_eval.fit(X_train,y_train)\n        print('Grid best parameters for {0}: {1} scoring: {2}'\n              .format(scorer, clf_eval.best_params_, round(clf_eval.best_score_,3)))\n        clf_dict[scorer] = clf_eval\n    return clf_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_values = {'max_features': [4, 5, 6, 7],\n              'max_depth': [3, 7, 11, 13]}\nscorers_list = ['accuracy','roc_auc','precision','recall', 'f1']\n\nrf_cv = model_score(data_set,\n            RandomForestClassifier(random_state=42, \n                                   n_jobs=4, \n                                   class_weight='balanced_subsample', \n                                   n_estimators=50), \n            grid_values, \n            scorers_list)\n\ntemp_df1 = pd.DataFrame()\nfor i in scorers_list:\n      temp_df1[i]=rf_cv[i].cv_results_['mean_test_score'][rf_cv[i].cv_results_['param_max_features']==4]\ntemp_df1['max_depth'] = rf_cv['roc_auc'].cv_results_['param_max_depth'][rf_cv['roc_auc'].cv_results_['param_max_features']==4]\ntemp_df1.set_index('max_depth', inplace=True)\nprint('4:\\n')\ntemp_df1\n\ntemp_df2 = pd.DataFrame()\nfor i in scorers_list:\n      temp_df2[i]=rf_cv[i].cv_results_['mean_test_score'][rf_cv[i].cv_results_['param_max_features']==6]\ntemp_df2['max_depth'] = rf_cv['roc_auc'].cv_results_['param_max_depth'][rf_cv['roc_auc'].cv_results_['param_max_features']==6]\ntemp_df2.set_index('max_depth', inplace=True)\nprint('6:\\n')\ntemp_df2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LogisticRegression"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_values = {'C': [0.01, 0.1, 1, 10, 100],\n              'penalty': ['l1', 'l2']}\nscorers_list = ['accuracy','roc_auc','precision','recall', 'f1']\n\n\nlr_cv = model_score(data_set,\n                    LogisticRegression(solver='liblinear',random_state=42, max_iter=500,\n                                      class_weight='balanced'),\n                    grid_values,\n                    scorers_list)\n\n\ntemp_df1 = pd.DataFrame()\nfor i in scorers_list:\n      temp_df1[i]=lr_cv[i].cv_results_['mean_test_score'][lr_cv[i].cv_results_['param_penalty']=='l1']\ntemp_df1['C'] = lr_cv['roc_auc'].cv_results_['param_C'][lr_cv['roc_auc'].cv_results_['param_penalty']=='l1']\ntemp_df1.set_index('C', inplace=True)\nprint('l1:\\n')\ntemp_df1\n\ntemp_df2 = pd.DataFrame()\nfor i in scorers_list:\n      temp_df2[i]=lr_cv[i].cv_results_['mean_test_score'][lr_cv[i].cv_results_['param_penalty']=='l2']\ntemp_df2['C'] = lr_cv['roc_auc'].cv_results_['param_C'][lr_cv['roc_auc'].cv_results_['param_penalty']=='l2']\ntemp_df2.set_index('C', inplace=True)\nprint('l2:\\n')\ntemp_df2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVC"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_values = {'C': [1, 10],\n              'gamma': [0.5, 0.7, 0.9, 0.95]}\nscorers_list = ['accuracy','roc_auc','precision','recall', 'f1']\n\n\nsvc_cv = model_score(data_set,\n                    SVC(random_state=42, class_weight='balanced',kernel='rbf'),\n                    grid_values,\n                    scorers_list)\n\n\ntemp_df1 = pd.DataFrame()\nfor i in scorers_list:\n      temp_df1[i]=svc_cv[i].cv_results_['mean_test_score'][svc_cv[i].cv_results_['param_C']==1]\ntemp_df1['gamma'] = svc_cv['roc_auc'].cv_results_['param_gamma'][svc_cv['roc_auc'].cv_results_['param_C']==1]\ntemp_df1.set_index('gamma', inplace=True)\nprint('C=1:\\n')\ntemp_df1\n\ntemp_df2 = pd.DataFrame()\nfor i in scorers_list:\n      temp_df2[i]=svc_cv[i].cv_results_['mean_test_score'][svc_cv[i].cv_results_['param_C']==10]\ntemp_df2['gamma'] = svc_cv['roc_auc'].cv_results_['param_gamma'][svc_cv['roc_auc'].cv_results_['param_C']==10]\ntemp_df2.set_index('gamma', inplace=True)\nprint('C=10:\\n')\ntemp_df2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The following are the main functions to run each model evaluation in the chosen best values"},{"metadata":{"trusted":true},"cell_type":"code","source":"def mod_eval(df,predictions, predprob, y_test, title):\n    # prints confusion matrix heatmap    \n    cm = confusion_matrix(df.Loan_Status[y_test.index], predictions)\n    sns.heatmap(cm, annot=True, fmt='.3g', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes']).set_title(title)\n    plt.xlabel('Real')\n    plt.ylabel('Predict')\n    \n    print(classification_report(df.Loan_Status[y_test.index], predictions))\n    \n    f, axes = plt.subplots(1,2,figsize= (20,6),squeeze=False)\n\n    fpr, tpr, _ = roc_curve(df.Loan_Status[y_test.index], predprob[:,1])\n    roc_auc = auc(fpr,tpr)\n    axes[0,0].plot(fpr, tpr, lw=3)\n    axes[0,0].set_title('{} ROC curve (area = {:0.2f})'.format(title, roc_auc))\n    axes[0,0].set(xlabel='False Positive Rate',ylabel='True Positive Rate')\n    axes[0,0].grid(b=True, which='both', axis='both', color='grey', linestyle = '--', linewidth = '0.3')\n\n    precision, recall, thresholds = precision_recall_curve(y_test, predprob[:,1])\n    best_index = np.argmin(np.abs(precision-recall)) # set the best index to be the minimum delta between precision and recall\n    axes[0,1].plot(precision,recall)\n    axes[0,1].set_title('{} Precision-Recall Curve'.format(title))\n    axes[0,1].set(xlabel='Precision', ylabel='Recall', xlim=(0.4,1.05))\n    axes[0,1].plot(precision[best_index],recall[best_index],'o',color='r')\n    axes[0,1].grid(b=True, which='both', axis='both', color='grey', linestyle = '--', linewidth = '0.3')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_training(classifier,df):\n    clf = classifier\n    t=df.drop(columns=['Loan_Status'])\n    X_train, X_test, y_train, y_tests = train_test_split(t,\n                                                         df['Loan_Status'],\n                                                         test_size=ts,\n                                                         stratify=df['Loan_Status'])\n    clf.fit(X_train, y_train)\n    return clf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Chosen values per model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#RandomForest\nmax_depth=11\nmax_features=6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LogisticRegression\nlr_C=0.1\npenalty='l1'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SVC\nsvc_C=1\ngamma=0.9","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Test Size\nts = 0.333","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## RandomForest"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = model_training(RandomForestClassifier(random_state=42, \n                                           n_jobs=4, \n                                           n_estimators=50, \n                                           max_depth=max_depth,\n                                           max_features=max_features),data_set)\n\nt=data_set.drop(columns=['Loan_Status'])\nX_train, X_test, y_train, y_test = train_test_split(t,\n                                                     data_set['Loan_Status'],\n                                                     test_size=ts,\n                                                     stratify=data_set['Loan_Status'])\n\nmod_eval(data_set, rf.predict(X_test), rf.predict_proba(X_test), y_test, 'RandomForest')\nfi_df = pd.DataFrame({'fi': rf.feature_importances_},index=t.columns).sort_values(by='fi', ascending=False)\nfi_df\nplt.show()\nplt.figure(figsize=(12,5))\nplt.xticks(rotation='vertical')\nsns.barplot(x=fi_df.index, y=fi_df['fi'], palette=current_palette)\nplt.grid(b=True, which='both', axis='both', color='grey', linestyle = '--', linewidth = '0.3')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LogisticRegression"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = model_training(LogisticRegression(C=lr_C, \n                                       penalty=penalty,\n                                       solver='liblinear',\n                                       max_iter=1000),data_set)\n\nt=data_set.drop(columns=['Loan_Status'])\nX_train, X_test, y_train, y_test = train_test_split(t,\n                                                    data_set['Loan_Status'],\n                                                    test_size=ts,\n                                                    random_state = 42, stratify=data_set['Loan_Status'])\n\nt = 0.71\npredprob = lr.predict_proba(X_test)\n\npred_y = [np.ceil(x) if x>=t else np.floor(x) for x in predprob[:,1]]\n\n#pred_y = lr.predict(X_test)\nmod_eval(data_set, pred_y, lr.predict_proba(X_test), y_test, 'LogisticRegressin') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## NaiveBayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"gnb = model_training(GaussianNB(),data_set)\n\nt=data_set.drop(columns=['Loan_Status'])\nX_train, X_test, y_train, y_test = train_test_split(t,\n                                                    data_set['Loan_Status'],\n                                                    test_size=ts,\n                                                    random_state = 42, stratify=data_set['Loan_Status'])\n\n\nt = 0.75\npredprob = gnb.predict_proba(X_test)\n\npred_y = [np.ceil(x) if x>=t else np.floor(x) for x in predprob[:,1]]\n#pred_y = gnb.predict(X_test)\nmod_eval(data_set,pred_y, gnb.predict_proba(X_test), y_test, 'GaussianNB')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SVC"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = model_training(SVC(kernel='linear',\n                         C=1, \n                         gamma='auto',\n                         class_weight='balanced',\n                         probability=True),data_set)\n\n\nt=data_set.drop(columns=['Loan_Status'])\nX_train, X_test, y_train, y_test = train_test_split(t,\n                                                    data_set['Loan_Status'],\n                                                    test_size=ts,\n                                                    random_state = 42, stratify=data_set['Loan_Status'])\n\nt=0.75\nprint('t={}'.format(t))\npredprob = svc.predict_proba(X_test)\n\npred_y = [np.ceil(x) if x>=t else np.floor(x) for x in predprob[:,1]]\n#pred_y = svc.predict(X_test)\n\nmod_eval(data_set,pred_y, svc.predict_proba(X_test), y_test, 'SVC')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dummy"},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy = model_training(DummyClassifier(strategy='stratified'),data_set)\n\nt=data_set.drop(columns=['Loan_Status'])\nX_train, X_test, y_train, y_test = train_test_split(t,\n                                                    data_set['Loan_Status'],\n                                                    test_size=ts,\n                                                    random_state = 42, stratify=data_set['Loan_Status'])\n\nt = 0.5\npredprob = dummy.predict_proba(X_test)\n\npred_y = [np.ceil(x) if x>=t else np.floor(x) for x in predprob[:,1]]\n#pred_y = dummy.predict(X_test)\n\nmod_eval(data_set, pred_y, dummy.predict_proba(X_test), y_test, 'Dummy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Summary\n- The RandomForest classifier brings the best results\n- Although, it might seems a bit overfitted according the CrossValidation session before the last fitting\n- Dummy classifier is essensial for sanity check along the analysis"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}