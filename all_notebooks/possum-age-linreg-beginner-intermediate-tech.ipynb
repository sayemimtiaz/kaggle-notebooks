{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:37.268394Z","iopub.execute_input":"2021-08-17T11:25:37.268818Z","iopub.status.idle":"2021-08-17T11:25:37.276399Z","shell.execute_reply.started":"2021-08-17T11:25:37.268774Z","shell.execute_reply":"2021-08-17T11:25:37.27511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import required library\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:37.278749Z","iopub.execute_input":"2021-08-17T11:25:37.279073Z","iopub.status.idle":"2021-08-17T11:25:38.197726Z","shell.execute_reply.started":"2021-08-17T11:25:37.279045Z","shell.execute_reply":"2021-08-17T11:25:38.196533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/openintro-possum/possum.csv')\ndf","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:38.200011Z","iopub.execute_input":"2021-08-17T11:25:38.200443Z","iopub.status.idle":"2021-08-17T11:25:38.240062Z","shell.execute_reply.started":"2021-08-17T11:25:38.200397Z","shell.execute_reply":"2021-08-17T11:25:38.238769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()\n#some null values","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:38.242263Z","iopub.execute_input":"2021-08-17T11:25:38.242741Z","iopub.status.idle":"2021-08-17T11:25:38.266755Z","shell.execute_reply.started":"2021-08-17T11:25:38.242674Z","shell.execute_reply":"2021-08-17T11:25:38.265273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Basic EDA\n\n- See age distribution per values in each categorical column\n- See correlation of different numerical columns with age column","metadata":{}},{"cell_type":"code","source":"#categorical columns\n\ncat = ['sex','Pop','site']\n\nfor col in cat:\n    print(f'In {col}: {df[col].unique()}')","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:38.268829Z","iopub.execute_input":"2021-08-17T11:25:38.269317Z","iopub.status.idle":"2021-08-17T11:25:38.284154Z","shell.execute_reply.started":"2021-08-17T11:25:38.269267Z","shell.execute_reply":"2021-08-17T11:25:38.282591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax=plt.subplots(3, figsize=(10,10))\nax=ax.ravel()\n\nfor index, col in enumerate(cat):\n    sns.boxplot(x='age',y=col,data=df, ax=ax[index])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-08-17T11:25:38.286391Z","iopub.execute_input":"2021-08-17T11:25:38.286944Z","iopub.status.idle":"2021-08-17T11:25:38.927023Z","shell.execute_reply.started":"2021-08-17T11:25:38.286892Z","shell.execute_reply":"2021-08-17T11:25:38.925582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Categorical findings\n\n- female has higher median age than males\n\n- Vic has more variations in age\n\n- site 2 has significantly lower median age than other sites","metadata":{}},{"cell_type":"code","source":"df['site'] = df['site'].apply(lambda x:str(x))\ndf.info() #change site into string","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:38.930634Z","iopub.execute_input":"2021-08-17T11:25:38.930996Z","iopub.status.idle":"2021-08-17T11:25:38.949735Z","shell.execute_reply.started":"2021-08-17T11:25:38.930965Z","shell.execute_reply":"2021-08-17T11:25:38.948204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax=plt.subplots(figsize=(15,15))\nsns.heatmap(df.corr(),annot=True)\nplt.show()\n#to a limited degree, body dimensions are to some degree correlated with age","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:38.951952Z","iopub.execute_input":"2021-08-17T11:25:38.95226Z","iopub.status.idle":"2021-08-17T11:25:39.982144Z","shell.execute_reply.started":"2021-08-17T11:25:38.952231Z","shell.execute_reply":"2021-08-17T11:25:39.98104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Numerical findings\n\n- to a limited degree, body dimensions are to some degree correlated with age","metadata":{}},{"cell_type":"markdown","source":"## Distribution, skewness & log-transform\n\n- Some regression techniques don't work well skewed data, so we are doing this to detech is that's the case.  Therefore, we will inspect each numerical columns and see if log-transform is appropriate\n\n- We can also experiment with log-transforming the outomce (age)","metadata":{}},{"cell_type":"code","source":"num = ['hdlngth',\n 'skullw',\n 'totlngth',\n 'taill',\n 'footlgth',\n 'earconch',\n 'eye',\n 'chest',\n 'belly']\n\n#numerical columns EDA\n\nfig,ax=plt.subplots(3,3, figsize=(10,10),constrained_layout=True)\nax=ax.ravel()\n\nfor index, col in enumerate(num):\n    sns.histplot(x=col,data=df,ax=ax[index],\n               kde=True)\n    ax[index].set_title(f'Skewness:{df[col].skew(axis = 0)}')\n    \n#Some regression techniques don't work well skewed data, so we are doing this to detech is that's the case","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:39.983625Z","iopub.execute_input":"2021-08-17T11:25:39.983985Z","iopub.status.idle":"2021-08-17T11:25:42.640258Z","shell.execute_reply.started":"2021-08-17T11:25:39.983952Z","shell.execute_reply":"2021-08-17T11:25:42.639134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num = ['hdlngth',\n 'skullw',\n 'totlngth',\n 'taill',\n 'footlgth',\n 'earconch',\n 'eye',\n 'chest',\n 'belly']\n\n#numerical columns EDA\n\nfig,ax=plt.subplots(3,3, figsize=(10,10),constrained_layout=True)\nax=ax.ravel()\n\nfor index, col in enumerate(num):\n    log = (f'{col}_log')\n    df[log] = df[col].apply(lambda x:np.log(x+1))\n    sns.histplot(x=f'{col}_log',data=df,ax=ax[index],\n               kde=True)\n    ax[index].set_title(f'Skewness:{df[log].skew(axis = 0)}')\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:42.641844Z","iopub.execute_input":"2021-08-17T11:25:42.642223Z","iopub.status.idle":"2021-08-17T11:25:45.342735Z","shell.execute_reply.started":"2021-08-17T11:25:42.64219Z","shell.execute_reply":"2021-08-17T11:25:45.34164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#target variable\nsns.histplot(x='age',data=df,kde=True)\nplt.title(f'Skewness:{df.age.skew(axis = 0)}')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:45.343933Z","iopub.execute_input":"2021-08-17T11:25:45.34423Z","iopub.status.idle":"2021-08-17T11:25:45.536895Z","shell.execute_reply.started":"2021-08-17T11:25:45.3442Z","shell.execute_reply":"2021-08-17T11:25:45.535547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#log transform\ndf['age_log'] = df['age'].apply(lambda x:np.log(x+1))\nsns.histplot(x='age_log',data=df,kde=True)\nplt.title(f'Skewness:{df.age_log.skew(axis = 0)}')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:45.538417Z","iopub.execute_input":"2021-08-17T11:25:45.538741Z","iopub.status.idle":"2021-08-17T11:25:45.739844Z","shell.execute_reply.started":"2021-08-17T11:25:45.538688Z","shell.execute_reply":"2021-08-17T11:25:45.7386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Log-transform findings\n\n- Skewness improves the best for skullw, taill, earconch, eye, chest\n\n- We can inverse this later post-prediction with **np.exp(x) - 1**","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:45.741432Z","iopub.execute_input":"2021-08-17T11:25:45.74178Z","iopub.status.idle":"2021-08-17T11:25:45.780068Z","shell.execute_reply.started":"2021-08-17T11:25:45.741749Z","shell.execute_reply":"2021-08-17T11:25:45.77898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Handling missing data","metadata":{}},{"cell_type":"code","source":"#handling missing date - dropping them since only a few rows are missing\ndf.dropna(axis=0,inplace=True)\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:45.781616Z","iopub.execute_input":"2021-08-17T11:25:45.782046Z","iopub.status.idle":"2021-08-17T11:25:45.809339Z","shell.execute_reply.started":"2021-08-17T11:25:45.782013Z","shell.execute_reply":"2021-08-17T11:25:45.808178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## All about Linear Regression\n\nIn the book \"Introduction to Machine Learning\", there are many techniques to improve a basic Linear Regression technique, including the following:\n\n- comparing results with just log, no log and mix of log and no log on Linear Regression\n\n- trying binning and discretization\n\n- trying polynomial features\n\nIn the next section of the notebook, we will try each of the strategy and see how the Linear Regression performs","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:45.810948Z","iopub.execute_input":"2021-08-17T11:25:45.811269Z","iopub.status.idle":"2021-08-17T11:25:46.087316Z","shell.execute_reply.started":"2021-08-17T11:25:45.81124Z","shell.execute_reply":"2021-08-17T11:25:46.086135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:46.089035Z","iopub.execute_input":"2021-08-17T11:25:46.089542Z","iopub.status.idle":"2021-08-17T11:25:46.101284Z","shell.execute_reply.started":"2021-08-17T11:25:46.089472Z","shell.execute_reply":"2021-08-17T11:25:46.099965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#different column variations\n\nX_log = ['site', 'Pop', 'sex','hdlngth_log','skullw_log', 'totlngth_log', 'taill_log', 'footlgth_log',\n         'earconch_log', 'eye_log', 'chest_log', 'belly_log']\n\nX_regular = ['site', 'Pop', 'sex', 'hdlngth', 'skullw', 'totlngth', 'taill',\n       'footlgth', 'earconch', 'eye', 'chest', 'belly']\n\nX_mix = ['skullw_log', 'taill_log', 'earconch_log', 'eye_log', 'chest_log','totlngth', 'taill',\n       'footlgth','site', 'Pop', 'sex', 'belly']\n\ny_regular = 'age'\ny_log = 'age_log'","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:46.102861Z","iopub.execute_input":"2021-08-17T11:25:46.103234Z","iopub.status.idle":"2021-08-17T11:25:46.110373Z","shell.execute_reply.started":"2021-08-17T11:25:46.10319Z","shell.execute_reply":"2021-08-17T11:25:46.109321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#base case\nX = df[X_regular]\ny = df[y_regular]","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:46.112015Z","iopub.execute_input":"2021-08-17T11:25:46.112426Z","iopub.status.idle":"2021-08-17T11:25:46.124875Z","shell.execute_reply.started":"2021-08-17T11:25:46.112381Z","shell.execute_reply":"2021-08-17T11:25:46.123817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = pd.get_dummies(X)\nX","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:46.129178Z","iopub.execute_input":"2021-08-17T11:25:46.129538Z","iopub.status.idle":"2021-08-17T11:25:46.173392Z","shell.execute_reply.started":"2021-08-17T11:25:46.129484Z","shell.execute_reply":"2021-08-17T11:25:46.172434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.33, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:46.177432Z","iopub.execute_input":"2021-08-17T11:25:46.177751Z","iopub.status.idle":"2021-08-17T11:25:46.186634Z","shell.execute_reply.started":"2021-08-17T11:25:46.17772Z","shell.execute_reply":"2021-08-17T11:25:46.185237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LinearRegression()","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:46.188834Z","iopub.execute_input":"2021-08-17T11:25:46.189516Z","iopub.status.idle":"2021-08-17T11:25:46.195865Z","shell.execute_reply.started":"2021-08-17T11:25:46.189269Z","shell.execute_reply":"2021-08-17T11:25:46.194771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\nprint(f'RMSE:{np.sqrt(mean_squared_error(y_test, y_pred))}')\nprint(f'Standard Deviation of Age:{df.age.std()}')","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:46.197219Z","iopub.execute_input":"2021-08-17T11:25:46.19755Z","iopub.status.idle":"2021-08-17T11:25:46.260354Z","shell.execute_reply.started":"2021-08-17T11:25:46.197521Z","shell.execute_reply":"2021-08-17T11:25:46.259122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Base Case results\n\n- RMSE = 1.89\n\n- We will explore how each strategy worsens or improves the model","metadata":{}},{"cell_type":"code","source":"#log\nX = df[X_log].copy()\ny = df[y_log].copy()\n\nX = pd.get_dummies(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.33, random_state=42)\nlr = LinearRegression()\n\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\n\ny_test = np.exp(y_test)-1\ny_pred = np.exp(y_pred)-1\n\nprint(f'RMSE:{np.sqrt(mean_squared_error(y_test, y_pred))}')\nprint(f'Standard Deviation of Age:{df.age.std()}')","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:46.261652Z","iopub.execute_input":"2021-08-17T11:25:46.261999Z","iopub.status.idle":"2021-08-17T11:25:46.288834Z","shell.execute_reply.started":"2021-08-17T11:25:46.26197Z","shell.execute_reply":"2021-08-17T11:25:46.287868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mix\nX = df[X_mix].copy()\ny = df[y_log].copy()\n\nX = pd.get_dummies(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.33, random_state=42)\nlr = LinearRegression()\n\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\n\ny_test = np.exp(y_test)-1\ny_pred = np.exp(y_pred)-1\n\nprint(f'RMSE:{np.sqrt(mean_squared_error(y_test, y_pred))}')\nprint(f'Standard Deviation of Age:{df.age.std()}')","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:46.290419Z","iopub.execute_input":"2021-08-17T11:25:46.290732Z","iopub.status.idle":"2021-08-17T11:25:46.315486Z","shell.execute_reply.started":"2021-08-17T11:25:46.29069Z","shell.execute_reply":"2021-08-17T11:25:46.314752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mix v2\n\nX = df[X_mix].copy()\ny = df[y_regular].copy()\n\nX = pd.get_dummies(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.33, random_state=42)\nlr = LinearRegression()\n\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\n\nprint(f'RMSE:{np.sqrt(mean_squared_error(y_test, y_pred))}')\nprint(f'Standard Deviation of Age:{df.age.std()}')","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:46.31663Z","iopub.execute_input":"2021-08-17T11:25:46.317123Z","iopub.status.idle":"2021-08-17T11:25:46.338837Z","shell.execute_reply.started":"2021-08-17T11:25:46.317082Z","shell.execute_reply":"2021-08-17T11:25:46.338106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## log vs not-log X\n\n- So far, in this dataset, log does not help at all.  However, that may not be the case in other dataset.  \n\n- Data science, IMO, is all about trial and error.  Considering the computing resources required by Linear Regression, feel free to recommend or try more combinations!","metadata":{}},{"cell_type":"code","source":"#recall that distribution of 'footlgth' & 'earconch' are like 2 bell curves joined together, let's see if binning helps with performance","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:46.339921Z","iopub.execute_input":"2021-08-17T11:25:46.340355Z","iopub.status.idle":"2021-08-17T11:25:46.353605Z","shell.execute_reply.started":"2021-08-17T11:25:46.340316Z","shell.execute_reply":"2021-08-17T11:25:46.352291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Trying binning\n\n- recall that distribution of 'footlgth' & 'earconch' are like 2 bell curves joined together, let's see if binning helps with performance","metadata":{}},{"cell_type":"code","source":"num #columns of numerical values","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:46.355004Z","iopub.execute_input":"2021-08-17T11:25:46.355298Z","iopub.status.idle":"2021-08-17T11:25:46.363864Z","shell.execute_reply.started":"2021-08-17T11:25:46.355272Z","shell.execute_reply":"2021-08-17T11:25:46.362796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#'footlgth', 'earconch' columns to be binned\nX=df[X_regular].copy()\ny=df[y_regular].copy()","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:46.364867Z","iopub.execute_input":"2021-08-17T11:25:46.365133Z","iopub.status.idle":"2021-08-17T11:25:46.378365Z","shell.execute_reply.started":"2021-08-17T11:25:46.365108Z","shell.execute_reply":"2021-08-17T11:25:46.376675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#base case of 2 bins per column\n\nto_bin = ['footlgth', 'earconch']\n\nfor col in to_bin:\n    bins = np.linspace(X[col].min(),X[col].max(),2)\n    foot_bin = np.digitize(X[col], bins=bins)\n    X[f'{col}_binned'] = foot_bin\n    X[f'{col}_binned'] = X[f'{col}_binned'].apply(lambda x:str(x))\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:46.380408Z","iopub.execute_input":"2021-08-17T11:25:46.380967Z","iopub.status.idle":"2021-08-17T11:25:46.395369Z","shell.execute_reply.started":"2021-08-17T11:25:46.380848Z","shell.execute_reply":"2021-08-17T11:25:46.393977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = pd.get_dummies(X)\nX","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:46.397507Z","iopub.execute_input":"2021-08-17T11:25:46.39805Z","iopub.status.idle":"2021-08-17T11:25:46.453051Z","shell.execute_reply.started":"2021-08-17T11:25:46.398002Z","shell.execute_reply":"2021-08-17T11:25:46.451933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.drop(to_bin,axis=1,inplace=True) #dropping original columns\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.33, random_state=42)\nlr = LinearRegression()\n\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\n\nprint(f'RMSE:{np.sqrt(mean_squared_error(y_test, y_pred))}')\nprint(f'Standard Deviation of Age:{df.age.std()}')","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:46.45476Z","iopub.execute_input":"2021-08-17T11:25:46.455117Z","iopub.status.idle":"2021-08-17T11:25:46.477769Z","shell.execute_reply.started":"2021-08-17T11:25:46.455086Z","shell.execute_reply":"2021-08-17T11:25:46.476656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Base-case binning results\n\n- Base case of 2 bins do not provide much difference in performance\n\n- Let's see if more bins can help.","metadata":{}},{"cell_type":"code","source":"bin_num = [2,3,4,5,6,7,8,9,10,11,12,13,14,15] #testing number of bins\nto_bin = ['footlgth', 'earconch'] #columns to bin\n\nfor bin_n in bin_num:\n    \n    X=df[X_regular].copy()\n    y=df[y_regular].copy()\n\n    for col in to_bin:\n        \n        bins = np.linspace(X[col].min(),X[col].max(),bin_n)\n        foot_bin = np.digitize(X[col], bins=bins)\n        X[f'{col}_binned'] = foot_bin\n        X[f'{col}_binned'] = X[f'{col}_binned'].apply(lambda x:str(x))\n        \n        \n    X.drop(to_bin,axis=1,inplace=True) #dropping original columns\n    X = pd.get_dummies(X)\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.33, random_state=42)\n    lr = LinearRegression()\n\n    lr.fit(X_train,y_train)\n    y_pred = lr.predict(X_test)\n\n    print(f'At bin = {bin_n}, RMSE:{np.sqrt(mean_squared_error(y_test, y_pred))}')\n    print(f'Standard Deviation of Age:{df.age.std()}\\n')\n    \n#default RMSE:1.8887116003974755\n#seems at bin = 4, we have the most improvements","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:46.479445Z","iopub.execute_input":"2021-08-17T11:25:46.479766Z","iopub.status.idle":"2021-08-17T11:25:46.845777Z","shell.execute_reply.started":"2021-08-17T11:25:46.479737Z","shell.execute_reply":"2021-08-17T11:25:46.844399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Binning conclusion\n\n- Recall base-case default RMSE = 1.8887116003974755\n\n- At bin = 4, we have the most improvements\n\n- Binning is a powerful tool to improve performance of Linear Regression","metadata":{}},{"cell_type":"markdown","source":"## Binning and interactions\n\n- In the above, we dropped the original data when we binned their columns\n\n- However, we can include the original data back in after getting dummies\n\n- Without adding back, we predict a value for each bin.  However, we may also want to capture the slope for each bin\n\n- Besides adding back, we can compute a product of the bin dummies and the original data so we can capture a unique slope for each of the bin","metadata":{}},{"cell_type":"code","source":"#adding back\n\nto_bin = ['footlgth', 'earconch']\n\n\nX=df[X_regular].copy()\ny=df[y_regular].copy()\n\n\nfor col in to_bin:\n\n    bins = np.linspace(X[col].min(),X[col].max(),4)\n    foot_bin = np.digitize(X[col], bins=bins)\n    X[f'{col}_binned'] = foot_bin\n    X[f'{col}_binned'] = X[f'{col}_binned'].apply(lambda x:str(x))\n\nX = pd.get_dummies(X)\n\n#don't drop to_bin this time\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                test_size=0.33, random_state=42)\nlr = LinearRegression()\n\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\n\nprint(f'RMSE: {np.sqrt(mean_squared_error(y_test, y_pred))}')\nprint(f'Standard Deviation of Age: {df.age.std()}\\n')\n\n#seems to perform worse","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:46.847501Z","iopub.execute_input":"2021-08-17T11:25:46.847966Z","iopub.status.idle":"2021-08-17T11:25:46.888328Z","shell.execute_reply.started":"2021-08-17T11:25:46.847912Z","shell.execute_reply":"2021-08-17T11:25:46.887501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#product of the bin dummies and the original data \n\nto_bin = ['footlgth', 'earconch']\n\n\nX=df[X_regular].copy()\ny=df[y_regular].copy()\n\n\nfor col in to_bin:\n\n    bins = np.linspace(X[col].min(),X[col].max(),4)\n    foot_bin = np.digitize(X[col], bins=bins)\n    X[f'{col}_binned'] = foot_bin\n    X[f'{col}_binned'] = X[f'{col}_binned'].apply(lambda x:str(x))\n\nX = pd.get_dummies(X)\nX[['footlgth_binned_1', 'footlgth_binned_2', 'footlgth_binned_3',\n    'footlgth_binned_4', 'earconch_binned_1', 'earconch_binned_2',\n    'earconch_binned_3', 'earconch_binned_4']]","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:46.889664Z","iopub.execute_input":"2021-08-17T11:25:46.890066Z","iopub.status.idle":"2021-08-17T11:25:46.931149Z","shell.execute_reply.started":"2021-08-17T11:25:46.890032Z","shell.execute_reply":"2021-08-17T11:25:46.929732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dummy = [['footlgth_binned_1', 'footlgth_binned_2', 'footlgth_binned_3',\n    'footlgth_binned_4'], ['earconch_binned_1', 'earconch_binned_2',\n    'earconch_binned_3', 'earconch_binned_4']]\n\noriginal = ['footlgth', 'earconch']\n\nfor o in range(0,len(original)):\n    for d in range(0,len(dummy[o])):\n        col_name = f'{original[o]}*{[dummy[o][d]]}'\n        X[col_name] = (X[original[o]]*X[dummy[o][d]])","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:46.932919Z","iopub.execute_input":"2021-08-17T11:25:46.933357Z","iopub.status.idle":"2021-08-17T11:25:46.948529Z","shell.execute_reply.started":"2021-08-17T11:25:46.933297Z","shell.execute_reply":"2021-08-17T11:25:46.947261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.drop(original,axis=1,inplace=True)\nX","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:46.95157Z","iopub.execute_input":"2021-08-17T11:25:46.951949Z","iopub.status.idle":"2021-08-17T11:25:47.004358Z","shell.execute_reply.started":"2021-08-17T11:25:46.951917Z","shell.execute_reply":"2021-08-17T11:25:47.002623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                test_size=0.33, random_state=42)\nlr = LinearRegression()\n\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\n\nprint(f'RMSE: {np.sqrt(mean_squared_error(y_test, y_pred))}')\nprint(f'Standard Deviation of Age: {df.age.std()}\\n')\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:47.005928Z","iopub.execute_input":"2021-08-17T11:25:47.006265Z","iopub.status.idle":"2021-08-17T11:25:47.024517Z","shell.execute_reply.started":"2021-08-17T11:25:47.006229Z","shell.execute_reply":"2021-08-17T11:25:47.023245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Interaction Findings\n\n- Just adding back does not improve performance, worsens it in fact\n\n- However, using a product of original data and dummy variables do improve upon baseline model.  However, not to the degree of just binning","metadata":{}},{"cell_type":"markdown","source":"## Polynomial Features\n\n- we can also use polynomial to expand the features we have.  Let's see if that improves the performance","metadata":{}},{"cell_type":"code","source":"#given what we've learned so far, use bin 4 with dropping original data going forward\n\nto_bin = ['footlgth', 'earconch']\n\n\nX=df[X_regular].copy()\ny=df[y_regular].copy()\n\n\nfor col in to_bin:\n\n    bins = np.linspace(X[col].min(),X[col].max(),4)\n    foot_bin = np.digitize(X[col], bins=bins)\n    X[f'{col}_binned'] = foot_bin\n    X[f'{col}_binned'] = X[f'{col}_binned'].apply(lambda x:str(x))\n    \nX.drop(to_bin,axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:47.026178Z","iopub.execute_input":"2021-08-17T11:25:47.026491Z","iopub.status.idle":"2021-08-17T11:25:47.045342Z","shell.execute_reply.started":"2021-08-17T11:25:47.026461Z","shell.execute_reply":"2021-08-17T11:25:47.043852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\n\n#baseline = x^(2)\npoly = PolynomialFeatures(degree=2, include_bias=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:47.047204Z","iopub.execute_input":"2021-08-17T11:25:47.047944Z","iopub.status.idle":"2021-08-17T11:25:47.053718Z","shell.execute_reply.started":"2021-08-17T11:25:47.047886Z","shell.execute_reply":"2021-08-17T11:25:47.052247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:47.055582Z","iopub.execute_input":"2021-08-17T11:25:47.056096Z","iopub.status.idle":"2021-08-17T11:25:47.095328Z","shell.execute_reply.started":"2021-08-17T11:25:47.05603Z","shell.execute_reply":"2021-08-17T11:25:47.093851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_transform = ['hdlngth',\n 'skullw',\n 'totlngth',\n 'taill',\n 'eye',\n 'chest',\n 'belly']\n\npoly.fit(X[to_transform])\nX_poly = pd.DataFrame(poly.transform(X[to_transform]),\n                      columns=poly.get_feature_names(X[to_transform].columns))\nX_poly","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:47.097149Z","iopub.execute_input":"2021-08-17T11:25:47.097471Z","iopub.status.idle":"2021-08-17T11:25:47.156818Z","shell.execute_reply.started":"2021-08-17T11:25:47.097442Z","shell.execute_reply":"2021-08-17T11:25:47.155539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# adding back categorical data from before\n\nadd_back = ['site','Pop','sex','footlgth_binned','earconch_binned']\nX_poly[add_back] = X[add_back]\nX_poly","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:47.158293Z","iopub.execute_input":"2021-08-17T11:25:47.158701Z","iopub.status.idle":"2021-08-17T11:25:47.212168Z","shell.execute_reply.started":"2021-08-17T11:25:47.158649Z","shell.execute_reply":"2021-08-17T11:25:47.210983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_poly = pd.get_dummies(X_poly) \n\nX_train, X_test, y_train, y_test = train_test_split(X_poly, y, \n                                                test_size=0.33, random_state=42)\nlr = LinearRegression()\n\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\n\nprint(f'RMSE: {np.sqrt(mean_squared_error(y_test, y_pred))}')\nprint(f'Standard Deviation of Age: {df.age.std()}\\n')","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:47.214822Z","iopub.execute_input":"2021-08-17T11:25:47.215316Z","iopub.status.idle":"2021-08-17T11:25:47.243748Z","shell.execute_reply.started":"2021-08-17T11:25:47.215263Z","shell.execute_reply":"2021-08-17T11:25:47.242423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"degrees = [2,3,4,5,6,7,8,9,10]\n\nto_transform = ['hdlngth',\n                'skullw',\n                'totlngth',\n                'taill',\n                'eye',\n                'chest',\n                'belly']\n\nadd_back = ['site','Pop','sex','footlgth_binned','earconch_binned']\n\nX_preprocessed = X.copy() #with the bins included\n\nfor d in degrees:\n    \n    X_preprocessed = X.copy() #with the bins included\n    \n    poly = PolynomialFeatures(degree=d, include_bias=False)\n    \n    poly.fit(X_preprocessed[to_transform])\n    \n    X_poly = pd.DataFrame(poly.transform(X_preprocessed[to_transform]),\n                          columns=poly.get_feature_names(X_preprocessed[to_transform].columns))\n    \n    X_poly = pd.get_dummies(X_poly) \n\n    X_train, X_test, y_train, y_test = train_test_split(X_poly, y, \n                                                    test_size=0.33, random_state=42)\n    lr = LinearRegression()\n\n    lr.fit(X_train,y_train)\n    y_pred = lr.predict(X_test)\n\n    print(f'At degree = {d}, RMSE: {np.sqrt(mean_squared_error(y_test, y_pred))}')\n    print(f'Standard Deviation of Age: {df.age.std()}\\n')","metadata":{"execution":{"iopub.status.busy":"2021-08-17T11:25:47.245471Z","iopub.execute_input":"2021-08-17T11:25:47.24595Z","iopub.status.idle":"2021-08-17T11:25:49.827474Z","shell.execute_reply.started":"2021-08-17T11:25:47.245904Z","shell.execute_reply":"2021-08-17T11:25:49.826136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Polynomial Findings\n\n- In this case, adding features that scale up the original data by degrees do not seem to help with performance.  However, that may not be the case for other dataset","metadata":{}},{"cell_type":"markdown","source":"## Conclusion\n\n**In this notebook, we have done:**\n\n- Implementining a basic Linear Regression to predict Age of possums\n\n- Using binning, binning-interaction and polynomials techniques to improve upon a basic Linear Regression technique\n\n- We learned that binning is a powerful technique to boost performance for Linear Regression in this dataset\n\n- Most of the techniques in the notebook have been inspired by the book *\"Introduction to Machine Learning\" by Andreas C. MÃ¼ller and Sarah Guido* ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}