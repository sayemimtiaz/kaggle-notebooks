{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install sklearn --upgrade --quiet --disable-pip-version-check","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction\nThe dataset used in this notebook is publicly available and was created by Dr. William H. Wolberg, physician at the University Of Wisconsin Hospital at Madison, Wisconsin, USA. The original source, UCI Machine Learning Repository for breast cancer dataset can be found [here](http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29).\n\nTo create the dataset Dr. Wolberg used digitized fluid samples images, taken from patients with solid breast masses and a computer program, which is capable of perform the analysis of cytological features  of the cell nuclei based on such digital scan. The program uses a curve-fitting algorithm, to compute ten features from each one of the cells in the sample, than it calculates the mean value, standard error and extreme (worst) value of each feature for the image, returning a 30 real-valuated vector.\n\nMore advanced information on the mathematical principles and alghorithm implementation can be found in the literature below:\n\n[1] K. P. Bennett, \"Decision Tree Construction Via Linear Programming\" Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society, 1992\n\n[2] K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992"},{"metadata":{},"cell_type":"markdown","source":"# Dataset\nAttribute Information:\n\n1) ID number\n\n2) Diagnosis (M = malignant, B = benign)\n\n3-32) Ten real-valued features are computed for each cell nucleus:\n\n* a) radius (mean of distances from center to points on the perimeter)\n* b) texture (standard deviation of gray-scale values)\n* c) perimeter\n* d) area\n* e) smoothness (local variation in radius lengths)\n* f) compactness (perimeter^2 / area - 1.0)\n* g) concavity (severity of concave portions of the contour)\n* h) concave points (number of concave portions of the contour)\n* i) symmetry\n* j) fractal dimension (\"coastline approximation\" - 1)\n\nThe mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features.\nFor instance, field 3 is Mean Radius, field 13 is Radius Standard Error, field 23 is Worst Radius."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# save filepath to variable for easier access\nfile_path = '../input/breast-cancer-wisconsin-data/data.csv'\n\n# read the data and store data in DataFrame\ndata = pd.read_csv(file_path)\ndata.dropna(axis=1, inplace=True)\n\n# summary of data types and null counts \ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print some examples of the training data\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Right away we notice that there is absolutely no need for data cleaning since this extremely curated dataset does not contain any missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['diagnosis'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Furthermore, of the 569 entries in the dataset, 357 are classified as benign while 212 as malignant. This means a 1.68 : 1 proportion that we must keep in mind for later, namely when creating the different splits that will be used, so that our results are not influenced by sampling bias. "},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install seaborn --upgrade --quiet --disable-pip-version-check\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# histograms for numerical variable 1: Radius\nplt.figure(figsize=(15,5));\n\nplt.subplot(131)\nsns.distplot(data.iloc[:, 2], kde=True);\n\nplt.subplot(132)\nsns.distplot(data.iloc[:, 12], kde=True);\n\nplt.subplot(133)\nsns.distplot(data.iloc[:, 22], kde=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# histograms for numerical variable 1: Radius\nfig = plt.figure(figsize=(15,5));\n\nax1 = fig.add_subplot(131)\nsns.kdeplot(data=data, x=\"radius_mean\", hue=\"diagnosis\", ax=ax1);\n\nax2 = fig.add_subplot(132)\nsns.kdeplot(data=data, x=\"radius_se\", hue=\"diagnosis\", ax=ax2);\n\nax3 = fig.add_subplot(133)\nsns.kdeplot(data=data, x=\"radius_worst\", hue=\"diagnosis\", ax=ax3);\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# histograms for numerical variable 2: Texture\nplt.figure(figsize=(15,5));\n\nplt.subplot(131)\nsns.distplot(data.iloc[:, 3], kde=True);\n\nplt.subplot(132)\nsns.distplot(data.iloc[:, 13], kde=True);\n\nplt.subplot(133)\nsns.distplot(data.iloc[:, 23], kde=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# histograms for numerical variable 2: Textrue\nfig = plt.figure(figsize=(15,5));\n\nax1 = fig.add_subplot(131)\nsns.kdeplot(data=data, x=\"texture_mean\", hue=\"diagnosis\", ax=ax1);\n\nax2 = fig.add_subplot(132)\nsns.kdeplot(data=data, x=\"texture_se\", hue=\"diagnosis\", ax=ax2);\n\nax3 = fig.add_subplot(133)\nsns.kdeplot(data=data, x=\"texture_worst\", hue=\"diagnosis\", ax=ax3);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# histograms for numerical variable 3: Perimeter\nplt.figure(figsize=(15,5));\n\nplt.subplot(131)\nsns.distplot(data.iloc[:, 4], kde=True);\n\nplt.subplot(132)\nsns.distplot(data.iloc[:, 14], kde=True);\n\nplt.subplot(133)\nsns.distplot(data.iloc[:, 24], kde=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# histograms for numerical variable 4: Area\nplt.figure(figsize=(15,5));\n\nplt.subplot(131)\nsns.distplot(data.iloc[:, 5], kde=True);\n\nplt.subplot(132)\nsns.distplot(data.iloc[:, 15], kde=True);\n\nplt.subplot(133)\nsns.distplot(data.iloc[:, 25], kde=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# histograms for numerical variable 5: Smoothness\nplt.figure(figsize=(15,5));\n\nplt.subplot(131)\nsns.distplot(data.iloc[:, 6], kde=True);\n\nplt.subplot(132)\nsns.distplot(data.iloc[:, 16], kde=True);\n\nplt.subplot(133)\nsns.distplot(data.iloc[:, 26], kde=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# histograms for numerical variable 6: Compactness\nplt.figure(figsize=(15,5));\n\nplt.subplot(131)\nsns.distplot(data.iloc[:, 7], kde=True);\n\nplt.subplot(132)\nsns.distplot(data.iloc[:, 17], kde=True);\n\nplt.subplot(133)\nsns.distplot(data.iloc[:, 27], kde=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# histograms for numerical variable 7: Concavity\nplt.figure(figsize=(15,5));\n\nplt.subplot(131)\nsns.distplot(data.iloc[:, 8], kde=True);\n\nplt.subplot(132)\nsns.distplot(data.iloc[:, 18], kde=True);\n\nplt.subplot(133)\nsns.distplot(data.iloc[:, 28], kde=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# histograms for numerical variable 8: Concave Points\nplt.figure(figsize=(15,5));\n\nplt.subplot(131)\nsns.distplot(data.iloc[:, 9], kde=True);\n\nplt.subplot(132)\nsns.distplot(data.iloc[:, 19], kde=True);\n\nplt.subplot(133)\nsns.distplot(data.iloc[:, 29], kde=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# histograms for numerical variable 9: Symmetry\nplt.figure(figsize=(15,5));\n\nplt.subplot(131)\nsns.distplot(data.iloc[:, 10], kde=True);\n\nplt.subplot(132)\nsns.distplot(data.iloc[:, 20], kde=True);\n\nplt.subplot(133)\nsns.distplot(data.iloc[:, 30], kde=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# histograms for numerical variable 10: Fractal Dimension\nplt.figure(figsize=(15,5));\n\nplt.subplot(131)\nsns.distplot(data.iloc[:, 11], kde=True);\n\nplt.subplot(132)\nsns.distplot(data.iloc[:, 21], kde=True);\n\nplt.subplot(133)\nsns.distplot(data.iloc[:, 31], kde=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The histogram distribution is fairly normal (simil-gaussian) for many of the numerical fields, for instance **symmetry_mean**. However some of them, like **area_se**, are skewed towards the origin of the axis, so we may consider normalizing it later before proceeding further with our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One thing that is for sure needed is scaling: our dataset contains features highly varying in magnitudes and range but since most of the machine learning algorithms use some type of distance between two data points in their computations we need to bring all features to the same level of magnitude. In practice, this means we must transform the available data so that it fits within a specific scale, like 0â€“1.\n\nThe best idea is to try a distance-based algorithm, like for instance SVM, and see which type of processing yields the best result."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,8))\nplt.title('Correlation heatmap for mean attributes')\nsns.heatmap(data.iloc[:, 2:12].corr(), annot=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This matrix is a table is showing correlation coefficients between variables, we can see that columns with high absolute values are indeed logically linked just like we expect.\n\nFor example, columns **radius_mean**, **perimeter_mean** and **area_mean** have close to 1 values, just like we expect since both perimeter and area are function of the radius. Same goes for columns **concavity_mean** and **concave points_mean**, representing the severity and count of the concave portions of the countour respectively."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,8))\nplt.title('Correlation heatmap for se attributes')\nsns.heatmap(data.iloc[:, 12:22].corr(), annot=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,8))\nplt.title('Correlation heatmap for worst attributes')\nsns.heatmap(data.iloc[:, 22:32].corr(), annot=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The same patterns are present also among standard error and worst correlation heatmaps. Ultimately if we consider all the 30 real-valued features of the dataset the correlation values between features are standard, excluding the ones which are directly derived from each other (like **radius**, **perimeter** and **area**)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute the correlation matrix\ncorr = data.iloc[:, 2:32].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(24, 12))\n\n# Select colormap\ncmap = sns.color_palette('mako', as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nplt.title('Correlation heatmap')\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5});","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data=data.iloc[:, np.r_[1, 22:32]], hue=\"diagnosis\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Intuitively, features that show noticeable differences in characteristics between the two target classification categories (malignant and benign) are to be kept while features that do not show such differentiation should be eliminated. \n\nThe above plot is colored by each entry classification value. For each of the features in the pair plot, it can be noticed that the differences in values between the two target classification categories can be noticed in some cases, but not all of them. Intuitively we can say that **area_worst** is a more interesting property with respect to **compactness_worst**, but objectively, there is no way to tell for sure."},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing\n\nBefore proceding further with our analysis, it is best to transform our data in a way that will be more understandable by the mathematical models later. \nLet us begin by transforming the target column **diagnosis** to a boolean representation, benign entries will be mapped to 0 (False) and malignant entries to 1 (True).\n\nWe will use Label Encoder to label the categorical data. Label Encoder is used to convert categorical data, or object fields, into numbers, which our predictive models can better understand."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nencoder = LabelEncoder()\ndata.iloc[:, 1] = encoder.fit_transform(data.iloc[:, 1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Moreover, as suggested before, we are now going to scale and normalize all of the real-valued features. In this manner we should be able to avoid issues between different features in distance-based algorithms, and all of them wiil equally contribute to the result of our models.\n\nIt is crucial that we split our dataset now, before performing any kind of scaling otherwise data leakage will happen and decrease our model performance. Train-test contamination leads to high performance on the training set (and possibly even the validation data), but the model will perform poorly in production.\n\nIn other words, leakage causes a model to look accurate until you start making decisions with the model, and then the model becomes very inaccurate."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = data.iloc[:, 2:32]\ny = data.iloc[:, 1]\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use parameter stratify to mantain the target class proportion nearly the same in both sets.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.concat([X_train, y_train], axis=1, sort=False)\ntrain['diagnosis'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.concat([X_test, y_test], axis=1, sort=False)\ntest['diagnosis'].value_counts() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Standardization is another scaling technique where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\nX_test_scaled = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)\n\nX_train_scaled.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normalization is a scaling technique in which values are shifted and rescaled so that they end up ranging between 0 and 1, for this purpose we will use MinMaxScaler. Usually, it is good to use when you know that the distribution of your data does not follow a Gaussian distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nX_train_norm = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\nX_test_norm = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)\n\nX_train_norm.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\ndef print_scores(title, y_hat, y_pred):\n    print(title)\n    print(f\"Accuracy: {metrics.accuracy_score(y_test, y_hat)*100}\")\n    print(f\"F1: {metrics.f1_score(y_test, y_hat)*100}\")\n    print(f\"ROC AUC: {metrics.roc_auc_score(y_test, y_hat)*100}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection\n\nIt is now quite clear that some features will not be useful to the machine learning model. Therefore, we should now select the most useful of them. Either we straight up eliminate features that are highly correlated with each other or we find a way to select them.\n\nSince we are working with only 569 entries blindly eliminating some information based only on feature correlation could be a serious loss for the performance of our models. \nLet us insted try to filter features using the _feature_importances_ attribute of a baseline Random Forest model, and see if it ultimately boost the accuracy of the final model. \n\nTo create such a filter, train a default LightGBM binary classifier on the existing feature set and determine the importance of each of the features by referencing the _feature_importances_ attribute of the tree based model. Setting a threshold for features with low importance, all features with an importance less than the importance threshold will be eliminated from the data set.\n\nLightGBM is a gradient boosting framework that uses tree based learning algorithm. In this model trees grows vertically while other algorithm trees grows horizontally, meaning that LightGBM grows tree leaf-wise while other algorithm grows level-wise. It is exceptionally fast and accurate, matching state-of-the-art machine learning algorithms like XGBoost and CatBoost."},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\n\ngbm = lgb.LGBMClassifier()\ngbm.fit(X_train, y_train)\n\ny_hat = gbm.predict(X_test)\nprint_scores('LightLGB baseline scores', y_hat, y_test)\nlgb.plot_importance(gbm.booster_);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us now use recursive feature elimination to repeat the above computation and take the top 10 important attributes over 20 separate steps."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\n\nrfe = RFE(gbm, 10)\nrfe = rfe.fit(X_train, y_train)\n\n# summarize the selection of the attributes\nimportant_features = X_train.columns[rfe.support_].tolist()\nprint(important_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can immediately see that our LightGBM random forest model regards **texture** and **concave points** are far and away the most important features, making out 5 of our top 10 values. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,8))\nplt.title('Correlation heatmap for important attributes')\nsns.heatmap(data[important_features].corr(), annot=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbm = lgb.LGBMClassifier()\ngbm.fit(X_train[important_features], y_train)\n\ny_hat = gbm.predict(X_test[important_features])\nprint_scores('LightLGB scores with feature selection', y_hat, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interestingly we did not improve our score by filtering out 20 of the 30 real-valued features, but at the same time we did not lose any accuracy at all.\nWe can keep going forward with both and see which kind of model thrive with more or less attributes."},{"metadata":{},"cell_type":"markdown","source":"# Model Building"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf_scores = {}\nrf = RandomForestClassifier()\n\nrf_scores['X_train'] = cross_val_score(rf, X_train, y_train, cv=5)\nrf_scores['X_train_scaled'] = cross_val_score(rf, X_train_scaled, y_train, cv=5)\nrf_scores['X_train_norm'] = cross_val_score(rf, X_train_norm, y_train, cv=5)\nrf_scores['X_train_imp_feat'] = cross_val_score(rf, X_train[important_features], y_train, cv=5)\nrf_scores['X_train_scaled_imp_feat'] = cross_val_score(rf, X_train_scaled[important_features], y_train, cv=5)\nrf_scores['X_train_norm_imp_feat'] = cross_val_score(rf, X_train_norm[important_features], y_train, cv=5)\n\nscores = pd.DataFrame(rf_scores)\nprint('Random Forest Classifier cross validation scores:')\nprint(scores.mean(axis = 0)*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected the Random Forest Classifier does not care about scaled or normalized features, reducing the feature set is also not relevant."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nsv_scores = {}\nsv = SVC()\n\nsv_scores['X_train'] = cross_val_score(sv, X_train, y_train, cv=5)\nsv_scores['X_train_scaled'] = cross_val_score(sv, X_train_scaled, y_train, cv=5)\nsv_scores['X_train_norm'] = cross_val_score(sv, X_train_norm, y_train, cv=5)\nsv_scores['X_train_imp_feat'] = cross_val_score(sv, X_train[important_features], y_train, cv=5)\nsv_scores['X_train_scaled_imp_feat'] = cross_val_score(sv, X_train_scaled[important_features], y_train, cv=5)\nsv_scores['X_train_norm_imp_feat'] = cross_val_score(sv, X_train_norm[important_features], y_train, cv=5)\n\nscores = pd.DataFrame(sv_scores)\nprint('Support Vector Classifier cross validation scores:')\nprint(scores.mean(axis = 0)*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Support Vector Classifier, our first distance-based algorithm, clearly benefits from scaled and normalized data with an average increase in accuracy of 5 percentage points. The reduced feature sets does not benefit the model this time, losing a percentage point."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr_scores = {}\nlr = LogisticRegression()\n\nlr_scores['X_train'] = cross_val_score(lr, X_train, y_train, cv=5)\nlr_scores['X_train_scaled'] = cross_val_score(lr, X_train_scaled, y_train, cv=5)\nlr_scores['X_train_norm'] = cross_val_score(lr, X_train_norm, y_train, cv=5)\nlr_scores['X_train_imp_feat'] = cross_val_score(lr, X_train[important_features], y_train, cv=5)\nlr_scores['X_train_scaled_imp_feat'] = cross_val_score(lr, X_train_scaled[important_features], y_train, cv=5)\nlr_scores['X_train_norm_imp_feat'] = cross_val_score(lr, X_train_norm[important_features], y_train, cv=5)\n\nscores = pd.DataFrame(lr_scores)\nprint('Logistic Regression Classifier cross validation scores:')\nprint(scores.mean(axis = 0)*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression Classifier presents a less marked difference, the best result is with scaled data but only slighty over the others."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nkn_scores = {}\nkn = KNeighborsClassifier()\n\nkn_scores['X_train'] = cross_val_score(kn, X_train, y_train, cv=5)\nkn_scores['X_train_scaled'] = cross_val_score(kn, X_train_scaled, y_train, cv=5)\nkn_scores['X_train_norm'] = cross_val_score(kn, X_train_norm, y_train, cv=5)\nkn_scores['X_train_imp_feat'] = cross_val_score(kn, X_train[important_features], y_train, cv=5)\nkn_scores['X_train_scaled_imp_feat'] = cross_val_score(kn, X_train_scaled[important_features], y_train, cv=5)\nkn_scores['X_train_norm_imp_feat'] = cross_val_score(kn, X_train_norm[important_features], y_train, cv=5)\n\nscores = pd.DataFrame(kn_scores)\nprint('KNeighbors Classifier cross validation scores:')\nprint(scores.mean(axis = 0)*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"KNeighbors Classifier benefits from scaled (and normalized) data as much as SVC. \n\nAs a final consideration we can now for sure say that feature selection was not very useful since all 5 different models now perform equally as good or better when working with all 30 features. For this reason we will continue from now on with the entire feature set."},{"metadata":{},"cell_type":"markdown","source":"# Model Tuning\nThe top scorer up to now is the Support Vector Classifier on _X\\_train\\_scaled_ with regard to cross validation score. Lets now see if some parameter changes can help us reach the more powerful LightGBM Classifier accuracy on the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_hat = rf.predict(X_test)\nprint_scores('Random Forest Classifier baseline scores', y_hat, y_test)\n\nrf = RandomForestClassifier()\nparam_grid =  {\n    'n_estimators': [50, 100, 150],\n    'criterion': ['gini', 'entropy'],\n    'bootstrap': [True],\n    'max_depth': [15, 20, 25],\n    'max_features': ['auto','sqrt', 10],\n    'min_samples_split': [2, 3] }      \n\nclf_rf = GridSearchCV(rf, param_grid=param_grid, cv=5, n_jobs=-1)\nbest_rf = clf_rf.fit(X_train, y_train)\nprint(f'Best Parameters: {str(best_rf.best_params_)}')\n\ny_hat = best_rf.predict(X_test)\nprint_scores('Random Forest Classifier tuned scores', y_hat, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, [ax_1, ax_2] = plt.subplots(1, 2, figsize=(13, 5))\nmetrics.plot_confusion_matrix(best_rf, X_test, y_test, ax=ax_1, cmap=plt.cm.Blues);\nmetrics.plot_confusion_matrix(best_rf, X_test, y_test, ax=ax_2, cmap=plt.cm.Blues, normalize='true');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sv = SVC(probability = True)\nsv.fit(X_train_scaled, y_train)\ny_hat = sv.predict(X_test_scaled)\nprint_scores('Support Vector Classifier baseline scores', y_hat, y_test)\n\nsv = SVC(probability = True)\nparam_grid = [ \n    {'kernel': ['rbf'], 'gamma': [.1, .5, 1, 2, 5, 10], 'C': [.1, 1, 10, 100, 1000]},\n    {'kernel': ['linear'], 'C': [.1, 1, 10, 100, 1000]},\n    {'kernel': ['poly'], 'degree' : [2, 3, 4, 5], 'C': [.1, 1, 10, 100, 1000]} ]\n\nclf_sv = GridSearchCV(sv, param_grid = param_grid, cv = 5, n_jobs = -1)\nbest_sv = clf_sv.fit(X_train_scaled, y_train)\nprint(f'Best Parameters: {str(best_sv.best_params_)}')\n\ny_hat = clf_sv.predict(X_test_scaled)\nprint_scores('Support Vector Classifier tuned scores', y_hat, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, [ax_1, ax_2] = plt.subplots(1, 2, figsize=(13, 5))\nmetrics.plot_confusion_matrix(best_sv, X_test_scaled, y_test, ax=ax_1, cmap=plt.cm.Blues);\nmetrics.plot_confusion_matrix(best_sv, X_test_scaled, y_test, ax=ax_2, cmap=plt.cm.Blues, normalize='true');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(X_train_scaled, y_train)\ny_hat = lr.predict(X_test_scaled)\nprint_scores('Logistic Regression Classifier baseline scores', y_hat, y_test)\n\nlr = LogisticRegression()\nparam_grid = { \n    'penalty' : ['l1', 'l2'],\n    'C': [.1, 1, 10, 100, 1000],\n    'solver' : ['liblinear', 'saga', 'lbfgs'] }\n\nclf_lr = GridSearchCV(lr, param_grid=param_grid, cv=5, n_jobs=-1)\nbest_lr = clf_lr.fit(X_train_scaled, y_train)\nprint(f'Best Parameters: {str(best_lr.best_params_)}')\n\ny_hat = clf_lr.predict(X_test_scaled)\nprint_scores('Logistic Regression Classifier tuned scores', y_hat, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, [ax_1, ax_2] = plt.subplots(1, 2, figsize=(13, 5))\nmetrics.plot_confusion_matrix(best_lr, X_test_scaled, y_test, ax=ax_1, cmap=plt.cm.Blues);\nmetrics.plot_confusion_matrix(best_lr, X_test_scaled, y_test, ax=ax_2, cmap=plt.cm.Blues, normalize='true');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kn = KNeighborsClassifier()\nkn.fit(X_train_norm, y_train)\ny_hat = kn.predict(X_test_norm)\nprint_scores('KNeighbors Classifier baseline scores', y_hat, y_test)\n\nkn = KNeighborsClassifier()\nparam_grid = { \n    'n_neighbors' : [3, 5, 7, 9],\n    'weights' : ['uniform', 'distance'],\n    'algorithm' : ['auto', 'ball_tree','kd_tree'],\n    'p' : [1, 2] }\n\nclf_kn = GridSearchCV(kn, param_grid=param_grid, cv=5, n_jobs=-1)\nbest_kn = clf_kn.fit(X_train_norm, y_train)\nprint(f'Best Parameters: {str(best_kn.best_params_)}')\n\ny_hat = clf_kn.predict(X_test_norm)\nprint_scores('KNeighbors Classifier tuned scores', y_hat, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, [ax_1, ax_2] = plt.subplots(1, 2, figsize=(13, 5))\nmetrics.plot_confusion_matrix(best_kn, X_test_norm, y_test, ax=ax_1, cmap=plt.cm.Blues);\nmetrics.plot_confusion_matrix(best_kn, X_test_norm, y_test, ax=ax_2, cmap=plt.cm.Blues, normalize='true');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizations"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, [ax_roc, ax_det] = plt.subplots(1, 2, figsize=(12, 5))\n\nmetrics.plot_roc_curve(best_rf, X_test, y_test, ax=ax_roc, name='Random Forest')\nmetrics.plot_roc_curve(best_sv, X_test_scaled, y_test, ax=ax_roc, name='Support Vector')\nmetrics.plot_roc_curve(best_lr, X_test_scaled, y_test, ax=ax_roc, name='Logistic Regression')\nmetrics.plot_roc_curve(best_kn, X_test_norm, y_test, ax=ax_roc, name='KNeighbors')\n\n#metrics.plot_det_curve(clf, X_test, y_test, ax=ax_det, name=name)\n\nax_roc.set_title('Receiver Operating Characteristic (ROC) curves')\nax_det.set_title('Detection Error Tradeoff (DET) curves')\n\nax_roc.grid(linestyle='--')\nax_det.grid(linestyle='--')\n\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\ndef plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Generate 3 plots: the test and training learning curve, the training\n    samples vs fit times curve, the fit times vs score curve.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        An estimator instance implementing `fit` and `predict` methods which\n        will be cloned for each validation.\n\n    title : str\n        Title for the chart.\n\n    X : array-like of shape (n_samples, n_features)\n        Training vector, where ``n_samples`` is the number of samples and\n        ``n_features`` is the number of features.\n\n    y : array-like of shape (n_samples) or (n_samples, n_features)\n        Target relative to ``X`` for classification or regression;\n        None for unsupervised learning.\n\n    axes : array-like of shape (3,), default=None\n        Axes to use for plotting the curves.\n\n    ylim : tuple of shape (2,), default=None\n        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).\n\n    cv : int, cross-validation generator or an iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n          - None, to use the default 5-fold cross-validation,\n          - integer, to specify the number of folds.\n          - :term:`CV splitter`,\n          - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : int or None, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    train_sizes : array-like of shape (n_ticks,)\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the ``dtype`` is float, it is regarded\n        as a fraction of the maximum size of the training set (that is\n        determined by the selected validation method), i.e. it has to be within\n        (0, 1]. Otherwise it is interpreted as absolute sizes of the training\n        sets. Note that for classification the number of samples usually have\n        to be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n    \"\"\"\n    if axes is None:\n        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes,\n                       return_times=True)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n\n    # Plot n_samples vs fit_times\n    axes[1].grid()\n    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n                         fit_times_mean + fit_times_std, alpha=0.1)\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"fit_times\")\n    axes[1].set_title(\"Scalability of the model\")\n\n    # Plot fit_time vs score\n    axes[2].grid()\n    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1)\n    axes[2].set_xlabel(\"fit_times\")\n    axes[2].set_ylabel(\"Score\")\n    axes[2].set_title(\"Performance of the model\")\n\n    return plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(3, 4, figsize=(24, 15))\n\ntitle = \"Learning Curves (Random Forest)\"\ncv = ShuffleSplit(n_splits=5, test_size=0.2)\nestimator = RandomForestClassifier(**best_rf.best_params_)\nplot_learning_curve(estimator, title, X_train, y_train, axes=axes[:, 0], ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n\ntitle = \"Learning Curves (SVC)\"\ncv = ShuffleSplit(n_splits=5, test_size=0.2)\nestimator = SVC(**best_sv.best_params_)\nplot_learning_curve(estimator, title, X_train_scaled, y_train, axes=axes[:, 1], ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n\ntitle = \"Learning Curves (Logistic Regression)\"\ncv = ShuffleSplit(n_splits=5, test_size=0.2)\nestimator = LogisticRegression(**best_lr.best_params_)\nplot_learning_curve(estimator, title, X_train_scaled, y_train, axes=axes[:, 2], ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n\ntitle = \"Learning Curves (KNeighbors)\"\ncv = ShuffleSplit(n_splits=5, test_size=0.2)\nestimator = KNeighborsClassifier(**best_kn.best_params_)\nplot_learning_curve(estimator, title, X_train_norm, y_train, axes=axes[:, 3], ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}