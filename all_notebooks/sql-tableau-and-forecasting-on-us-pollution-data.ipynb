{"cells":[{"metadata":{},"cell_type":"markdown","source":"# SQL, Tableau, and Forecasting on US Pollution Data\n---\n**Keywords**:<br>\nSQL, Tableau, Timeseries, Forecasting, Pandas, Environment\n\n## Introduction\n<p>\n    This dataset was obtained on <a href=\"https://www.kaggle.com/sogun3/uspollution\">kaggle</a>. The link contains the complete description but all you need to know to understand this analysis will be defined here. \n</p>\n\n<p>\n    I put a few comments on throughout the analysis to either clarify some points or give my opinion on a subject. Those are presented in blockquotes colored in dark blue. <br>\n<blockquote>\n    <font color=\"darkblue\">This is a comment! I hope you enjoy this work and can get some useful insight or piece of code from it.</font>\n</blockquote>\n</p>\n\n\n\n### Contents\n\n![](https://i.imgur.com/aptQbQK.png)\n\n### Objectives\n- Pre-process the data using PostgreSQL \n- Query the data needed for this analysis: last 6 years of the dataset\n- Build dashboards for exploration using Tableau\n- Analyse the time components of each pollutant\n- Build Naive Models as a comparison\n- Forecast using SARIMA models\n\n### TLDR Version <font color=\"firebrick\">(Under construction)</font>\n\n..."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# 1 - The Dataset\n---\n## 1.1 - Introducing the Data"},{"metadata":{},"cell_type":"markdown","source":"### General Information\n- Original format: csv\n- Dataset shape: 1.746.661 x 30 (rows x columns)\n- Granularity: Combination of State-County-City-Site_num-Date_local features\n- Usability: Many nulls and duplicates. See chapter 1.3.\n\n### Features in the dataset\n\n<ul>\n    <li><b>Id</b> - Not unique integer</li>\n    <li><b>State_code</b></li>\n    <li><b>Country_code</b></li>\n    <li><b>Site_num</b>- Number of measurement site</li>\n    <li><b>Address, State_name, County, City</b></li>\n    <li><b>date_local</b> - Date of the measurement</li>\n    <li><b>(*4x) Unit</b> - Unit of the measurement</li>\n    <li><b>(4x) Mean</b></li>\n    <li><b>(4x) Max_value</b></li>\n    <li><b>(4x) Max_hour</b> - Hour of the max value</li>\n    <li><b>(4x) AQI</b> - <a href='https://www.weather.gov/safety/airquality-aqindex'>Air Quality Index</a>. A standard measure for air quality that can be used to compare pollutants. <u>Since AQI allows us to compare different pollutants, we won't use the other pollutant features.</u> There are AQI bands according to risk to health. The bands are defined on the image below.</li> \n\n</ul>\n\n![](http://sparetheair.com/assets/aqi/PM2017.png)\n\n<p>\n    <b>*</b> Each feature with the 4x indicator means they repeat for the four pollutants in our dataset: <b>NO2, O3, SO2 and CO.</b>\n</p>\n\n### The Pollutants\nAs mentioned, we have four pollutants. This image describes them briefly (Source: wikipedia).\n![](https://i.imgur.com/oyhrJKd.png)\n"},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"## 1.2 -  Importing Libraries"},{"metadata":{},"cell_type":"markdown","source":"We need only the basic tools for an EDA for now."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nsns.set(style='whitegrid', rc={'axes.grid': False})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr/>"},{"metadata":{},"cell_type":"markdown","source":"## 1.3 - Data Pre-Processing\nThe tool used for reading and preprocessing this dataset was PostgreAdmin together with PostgreSQL queries. My resources are limited and demand SQL data optimization for larger datasets.\n\nAs mentioned, the inital dataset has over 1.7 million lines, but as soon as you take your first look at it you notice that there are many problems going on. To illustrate this I took what should supposedly be a single row of data from SQL using the following SQL query:\n\n```SQL\nselect \n\tstate_name, city, date_local,\n\tno2_mean, no2_max_value, no2_aqi,\n\to3_mean, o3_max_value, o3_aqi,\n\tso2_mean, so2_max_value, so2_aqi,\n\tco_mean, co_max_value, co_aqi \nfrom us_pollution\nwhere\n\tstate_name = 'Illinois' and\n\tcounty = 'Cook' and\n\tcity = 'Northbrook' and\n\tsite_num = 4201 and\n\tdate_local = date('2007-05-15');\n```\nThe data from this query is presented below."},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_data = pd.read_csv('../input/sample-query/sample_query.csv')\nsample_data.iloc[:,3:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For what should be a single measure we have 16. I've decided to explore only the AQI columns and that means I can drop the nulls and just group the data and get the average AQI values. From 1.7mi now we have something around 400k rows.\n\n\n<blockquote>\n    <font color='darkblue'>\n        <b>If we wanted to use the other features</b>, we should use a MAX( ) aggregation function for the max values. To get the hour for this max value things would get a more complicated. There is no way to aggregate the hour values in a way that picks the right information so you would probably want to create a query without the hour values and join to the original table.\n    </font>\n    </blockquote>"},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"## 1.4 - Querying\nAs stated in the **objectives**, we will use on the last six years of this dataset (from 2011 to 2016). Another important thing is to look if the cities are present in all years. Having cities joining and leaving the data throughout the time period will mess with any analysis. A SQL query can expose this problem:\n```SQL\nselect \n\tstate_name, county, \n\tcity, site_num,\n\tcount(distinct(extract(year from date_local))) years\nfrom \n\tus_pollution\nwhere \n\tdate_local > date('2010-12-31')\ngroup by \n\tstate_name, county,\n\tcity, site_num\nhaving \n    years = 6;\n```\n\nThe output of this query is 51 lines of locations that are present in the six years of our analysis - applying those as a filter results in approximately 88k rows.\n\nThat's it for the pre-processing and we're ready to explore."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/cleaned-us-pollution/us_pollution_cleaned_v3.csv')\ndataset.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"# 2 - Exploratory Data Analysis\nI've decided for this kernel to do the exploratory visualization on Tableau and build dashboards so you can do the data exploration as well. \n\n**Dashboard One** shows an overview of all cities in the dataset, which is to help you choose which one you want to further explore in **Dashboard Two**."},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"## 2.1 - US Dashboard"},{"metadata":{},"cell_type":"markdown","source":"This dashboard shows a ranking of cities and an interactive map. You can choose which pollutant to look and if you want to look for Average values or Maximum values of AQI. Below the ranking there is an interactive map, showing the same results.\n\nThe idea is to have an overall visualization on this first dashboard, pick a city and look it up on the second dashboard."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%HTML\n<div class='tableauPlaceholder' id='viz1564931468161' style='position: relative'><noscript><a href='#'><img alt=' ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;4R&#47;4RXN929JT&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='path' value='shared&#47;4RXN929JT' /> <param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;4R&#47;4RXN929JT&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1564931468161');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='100%';vizElement.style.maxWidth='750px';vizElement.style.height=(divElement.offsetWidth*0.85)+'px';vizElement.style.maxHeight='1187px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"## 2.2 - City Dashboard\nThe second dashboard is ment to look at each individual city for a deeper analysis on the data. You can see how each city progresses over the time period using the time filter. Once more, you can choose the pollutant and the city you would like to see in the interactive filters."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%HTML\n<div class='tableauPlaceholder' id='viz1564066138024' style='position: relative'><noscript><a href='#'><img alt=' ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;US&#47;US_pollution&#47;CityDashboard&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='US_pollution&#47;CityDashboard' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;US&#47;US_pollution&#47;CityDashboard&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1564066138024');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='100%';vizElement.style.maxWidth='750px';vizElement.style.height=(divElement.offsetWidth*0.85)+'px';vizElement.style.maxHeight='1187px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"## 2.3 - Grouped Data\nHow is the overall air quality index? We can answer this question grouping by dates. This is how our grouped data looks like:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pollutants = ['no2_aqi', 'o3_aqi', 'so2_aqi', 'co_aqi']\n\ngrouped_data = dataset.groupby('date_local').mean().drop('site_num', axis=1)\ngrouped_data.index = pd.to_datetime(grouped_data.index)\ngrouped_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we have a daily data of the AQI averaged between cities across the US. With our grouped data we can plot all the pollutants indexes between 2011 and 2016. The graph is presented below - once more using Tableau. \n\nHowever, it is also easy to do it using pandas. Simply using <code>grouped_data.plot()</code> would result in a similar graph using the default settings."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%HTML\n<div class='tableauPlaceholder' id='viz1564414530161' style='position: relative'><noscript><a href='#'><img alt=' ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;US&#47;US_pollution_grouped&#47;DB2&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='US_pollution_grouped&#47;DB2' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;US&#47;US_pollution_grouped&#47;DB2&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1564414530161');                    var vizElement = divElement.getElementsByTagName('object')[0];                  \nvizElement.style.width='100%';\nvizElement.style.height='620px'\nvizElement.style.maxHeight='620px';\nvar scriptElement = document.createElement('script');\nscriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    \nvizElement.parentNode.insertBefore(scriptElement, vizElement);  \n</script>","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights from the plot:**\n- On a first look, CO, NO2 and O3 apparently have yearly patterns, while SO2 looks like quite noisy (we will explore that later);\n- Ozone is produced by sunlight and is correlated to traffic volumes and the july and august peaks are observed accordingly;  \n- Curiously, Ozone's pattern behaves in the opposite way of CO and NO2 and I have no explanation for that. \n\nWe mustn't forget that these are averages and we don't know how distributed the values are. A way to visualize how clear these patterns are is by plotting a confidence interval for each time series. We will plot each pollutant separatedly to avoid making the graph too visually polluted (pun not intended). \n\n<hr>\n\n## 2.4 - Confidence Bands\n\nThe graph below presents the daily average and a confidence band of 95% (-2/+2 standard deviations) measured on the 6 years of data for each pollutant. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%HTML\n<div class='tableauPlaceholder' id='viz1564414741872' style='position: relative'><noscript><a href='#'><img alt=' ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;US&#47;US_pollution_grouped&#47;DB1&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='US_pollution_grouped&#47;DB1' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;US&#47;US_pollution_grouped&#47;DB1&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1564414741872');                    var vizElement = divElement.getElementsByTagName('object')[0];               \nvizElement.style.width='100%';\nvizElement.style.height='620px'\nvizElement.style.maxHeight='620px';\nvar scriptElement = document.createElement('script');    \nscriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';   \nvizElement.parentNode.insertBefore(scriptElement, vizElement);       \n</script>","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**There is a large confidence interval for O3 and NO2. I can think of three reasons for that:**\n1. The cities vary a lot on those two pollutants and different cities behave differently\n2. The data is simply very disperse for those pollutants\n3. Both cities and data are disperse (1 and 2 together).  \n\nLet's look at two cities - Dallas and New York - and see if we can figure out which option is it. The code below queries for both cities and groups the data like we did in the plot just above for Ozone to illustrate how we could reproduce this graph with simple Python."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"## Dallas Data\n# Querying the city\ndallas = dataset.query(\"city == 'Dallas'\").copy()\n\n# Creating a day of year column using pandas datetime properties\ndallas.loc[:, 'date'] = pd.to_datetime(dallas.loc[:, 'date_local'])\ndallas.loc[:, 'day_of_year'] = dallas['date'].apply(lambda x: x.dayofyear)\n\n# Grouping the dataset with day of year and creating upper and lower limits for the \n# confidence interval using two standard deviations\ngrouped_dallas = dallas[['day_of_year', 'o3_aqi']].groupby('day_of_year').aggregate(['mean', 'std'])\ngrouped_dallas['upper'] = grouped_dallas['o3_aqi', 'mean'] + 2 * grouped_dallas['o3_aqi', 'std']\ngrouped_dallas['lower'] = grouped_dallas['o3_aqi', 'mean'] - 2 * grouped_dallas['o3_aqi', 'std']\n\n# New York Data\n# Querying the city\nnyork = dataset.query(\"city == 'New York'\").copy()\n\n# Creating a day of year column using pandas datetime properties\nnyork.loc[:, 'date'] = pd.to_datetime(nyork.loc[:, 'date_local'])\nnyork.loc[:, 'day_of_year'] = nyork.loc[:, 'date'].apply(lambda x: x.dayofyear)\n\n# Grouping the dataset with day of year and creating upper and lower limits for the \n# confidence interval using two standard deviations\ngrouped_nyork = nyork[['day_of_year', 'o3_aqi']].groupby('day_of_year').aggregate(['mean', 'std'])\ngrouped_nyork['upper'] = grouped_nyork['o3_aqi', 'mean'] + 2 * grouped_nyork['o3_aqi', 'std']\ngrouped_nyork['lower'] = grouped_nyork['o3_aqi', 'mean'] - 2 * grouped_nyork['o3_aqi', 'std']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, axes = plt.subplots(2, 1, figsize=(14,10), sharex=True)\ngrouped_dallas.plot(y=('o3_aqi', 'mean'), kind='line', ax=axes[0], stacked=False, color='black')\ngrouped_dallas.plot(y='upper', kind='area', ax=axes[0], stacked=False, color='red', alpha=0.1)\ngrouped_dallas.plot(y='lower', kind='area', ax=axes[0], stacked=False, color='white', alpha=1)\naxes[0].set_ylim(0, 120)\naxes[0].legend().set_visible(False)\naxes[0].set_title(\"Dallas' AQI with 95% Confidence Band\")\n\ngrouped_nyork.plot(y=('o3_aqi', 'mean'), kind='line', ax=axes[1], stacked=False, color='black')\ngrouped_nyork.plot(y='upper', kind='area', ax=axes[1], stacked=False, color='red', alpha=0.1)\ngrouped_nyork.plot(y='lower', kind='area', ax=axes[1], stacked=False, color='white', alpha=1)\naxes[1].set_ylim(0, 120)\naxes[1].legend().set_visible(False)\naxes[1].set_title(\"New York's AQI with 95% Confidence Band\")\nplt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the peaks are not necessarily alligned for both cities - **Reason 1** stated before. For instance, at day 200 we have normal values in Dallas and a peak in New York.\n\nWe can also notice that within 6 years the data has a large confidence interval (i.e. data itself is spread out - **Reason 2**). So apparently both are correct for Ozone. Due to Ozone's high correlation with sun irradiation and sunlight exposure it is plausible that peaks don't allign.\n\nAlso, due to the fact that peaks don't happen simultaneously on different cities, we have the large confidence band we see on the last Tableau plot."},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"## 2.5 - Time Decomposition\nFor the last part of our exploratory data analysis we will decompose our grouped data into **Trend, Seasonal and Residual**. \n\nI will borrow the definition of Trend and Seasonality from the amazing free book <b><a href='https://otexts.com/fpp2/'>Forecasting: Principles and Practice</a></b> by Rob J Hyndman and George Athanasopoulos:\n<br><br>\n<p><font color='darkgreen'>\n    <b>\"Trend</b>: <i>A trend exists when there is a long-term increase or decrease in the data. It does not have to be linear. Sometimes we will refer to a trend as “changing direction”, when it might go from an increasing trend to a decreasing trend.\"</i>\n</font></p>\n<p><font color='darkgreen'>\n    <b>\"Seasonality</b>: <i>A seasonal pattern occurs when a time series is affected by seasonal factors such as the time of the year or the day of the week. Seasonality is always of a fixed and known frequency.\"</i>\n</font></p>\n<br>\nA trend should tell us if the average values are changing over the years. The seasonality in our case happens yearly, as we've seen in the previous plots.\n\nThe **Residual** component is whatever is not explained by the other two. The residual plot should be with mean 0 and constant variation through time. If that's not the case there might be something not being accounted for in the other two components.\n\n**Statsmodels** provides the function <code>seasonal_decompose()</code> that does this decomposition for us. \n\n<blockquote>\n    <font color='darkblue'>\nOur data's granularity is in days and your seasonal patterns are yearly so we must set the <b>frequency to 365</b>."},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\nno2_dec = seasonal_decompose(grouped_data['no2_aqi'], model='additive', freq=365)\no3_dec = seasonal_decompose(grouped_data['o3_aqi'], model='additive', freq=365)\nso2_dec = seasonal_decompose(grouped_data['so2_aqi'], model='additive', freq=365)\nco_dec = seasonal_decompose(grouped_data['co_aqi'], model='additive', freq=365)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I've plotted two groups: NO2/O3 and SO2/CO because the range of values is similar and I can use the same Y-limits. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, axes = plt.subplots(3,2, figsize=(14,8), sharex=True, sharey='row', dpi=80)\n\naxes[0][0].set_title('NO2\\n\\nTrend')\naxes[1][0].set_title('Seasonal')\naxes[2][0].set_title('Residual')\nno2_dec.trend.plot(ax=axes[0][0], ylim=(0, 40), c='C0')\nno2_dec.seasonal.plot(ax=axes[1][0], ylim=(-15, 15), c='C0')\nno2_dec.resid.plot(ax=axes[2][0], ylim=(-15, 15), c='C0')\n\naxes[0][1].set_title('O3\\n\\nTrend')\naxes[1][1].set_title('Seasonal')\naxes[2][1].set_title('Residual')\no3_dec.trend.plot(ax=axes[0][1], ylim=(0, 40), c='C1')\no3_dec.seasonal.plot(ax=axes[1][1], ylim=(-15, 15), c='C1')\no3_dec.resid.plot(ax=axes[2][1], ylim=(-15, 15), c='C1')\nfig.subplots_adjust(wspace=0.05);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, axes = plt.subplots(3,2, figsize=(14,8), sharex=True, sharey='row', dpi=80)\n\naxes[0][0].set_title('SO2\\n\\nTrend')\naxes[1][0].set_title('Seasonal')\naxes[2][0].set_title('Residual')\nso2_dec.trend.plot(ax=axes[0][0], ylim=(0, 8), c='C2')\nso2_dec.seasonal.plot(ax=axes[1][0], ylim=(-5, 5), c='C2')\nso2_dec.resid.plot(ax=axes[2][0], ylim=(-3, 3), c='C2')\n\naxes[0][1].set_title('CO\\n\\nTrend')\naxes[1][1].set_title('Seasonal')\naxes[2][1].set_title('Residual')\nco_dec.trend.plot(ax=axes[0][1], ylim=(0, 8), c='C3')\nco_dec.seasonal.plot(ax=axes[1][1], ylim=(-5, 5), c='C3')\nco_dec.resid.plot(ax=axes[2][1], ylim=(-3, 3), c='C3')\nfig.subplots_adjust(wspace=0.05)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Trends:**\n- Ozone is slightly increasing\n- SO2 is crearly decreasing\n- CO and NO2 are hard to tell but seem to be slightly decreasing\n\n**Seasonal Patterns**\n- All four pollutants seem to have seasonal patterns\n- CO has the less noisy pattern\n- O3 has the highest pattern amplitude\n\n**Residuals**:\n- All residuals are really high compared to the trend and seasonal components. \n- The mean is approximately zero for all residuals\n- The variance is also not constant through time"},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"## 2.6 - Train and Test sets\nWe will be trying to train a model to predict six months into the future. Therefore, our test set should be of at least six months. \n\nThat said, our test set will be the last six months in our data and the rest will be used as training (this means test interval is from october 2015 to march 2016). From now on we won't touch the data from the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = grouped_data[:'2015-10-01']\ntest = grouped_data['2015-10-01':'2016-03-31']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(14,5))\nplt.plot(train.index, train['no2_aqi'], label='Train')\nplt.plot(test.index, test['no2_aqi'], color='C1', label='Test')\nplt.legend()\nplt.title('Train/Test Sets');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"# 3 - Naive Models\nThe first models we are going to use to forecast will be three naive models for two reasons:\n1. Some times a simpler solution is good enough\n2. We will have something to compare our other higher complexity models\n\nThe naive models we are going to use are:\n- **Average Method**: The prediction is simply the average of the past values\n- **Naive Method**: The prediction is simply the last value on the training data\n- **Seasonal Naive Method**: The prediction is the same value in the last seasonal period (i.e. last year for our data)\n\nWe will be measuring the performance with **root mean squared error** and **mean absolute error**. Let's start forecasting."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, mean_absolute_error\n\n# Creating a dataframe to store scores\nmindex = pd.MultiIndex.from_product(iterables=(['RMSE', 'MAE'], \\\n                                               ['Average', 'Naive',\n                                                'Seasonal_Naive']))\nfc_scores = pd.DataFrame(index=mindex, columns=test.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The cell belows defines a function to save the values in our scores table and another function plot the forecasts for each method. We will be using them frequently."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def store_scores(method, predictions):\n    for pol in pollutants:\n        mse = mean_squared_error(test[pol], predictions[pol])\n        mae = mean_absolute_error(test[pol], predictions[pol])\n        fc_scores.loc[('RMSE', method), pol] = np.sqrt(mse)\n        fc_scores.loc[('MAE', method), pol] = mae\n        \n        \ndef plot_forecast(predictions):\n    fig, axes = plt.subplots(2, 2, figsize=(15,8), dpi=80, sharex=True)\n    fig.subplots_adjust(wspace=0.1, hspace=0.1)\n\n    axlist = [(0, 0), (0, 1), (1, 0), (1, 1)]\n\n    for (i, j), pol in zip(axlist, pollutants):\n        axes[i][j].plot(train.loc['2015':, pol], lw=0.75, label='Train')\n        axes[i][j].plot(test[pol], lw=0.75, label='Test', c='darkgray')\n        axes[i][j].plot(predictions[pol], c='red', lw=2, ls='--', label='Forecast')\n        axes[i][j].set_title(pol)\n\n    axes[0][0].legend(bbox_to_anchor=[1.35, 1.15], ncol=3);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"## 3.1 - Average Method\nFor the average method, we just need to take the average of the training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_predictions = pd.DataFrame(index=test.index, columns=test.columns)\nfor i in range(4):\n    avg_predictions.iloc[:,i] = train.iloc[:,i].mean()\n    \nstore_scores('Average', avg_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_forecast(avg_predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"## 3.2- Naive Method\nThe naive method consists of picking the last value and using it as a forecast. Another one-liner."},{"metadata":{"trusted":true},"cell_type":"code","source":"naive_predictions = pd.DataFrame(index=test.index, columns=test.columns)\nfor i in range(4):\n    naive_predictions.iloc[:,i] = train.iloc[-1,i]\n    \nstore_scores('Naive', naive_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_forecast(naive_predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"## 3.3 - Seasonal Naive Method\nSimilar to the naive method but with a seasonal offset. On this method, in order to forecast march 21st we use the value of march 21st of the previous year. Unfortunately, february in our test data has 29 days so we will just repeat the value of february 28th (a two-liner, thanks to the leap year)."},{"metadata":{"trusted":true},"cell_type":"code","source":"sn_predictions = pd.DataFrame(index=test.index, columns=test.columns)\nfor pol in pollutants:\n    sn_predictions.loc['2015-10-01':'2016-02-28',pol] = \\\n        train.loc['2014-10-01':'2015-02-28',pol].values\n    sn_predictions.loc['2016-02-29':'2016-03-31',pol] = \\\n        train.loc['2015-02-28':'2015-03-31',pol].values\n    \n# Storing scores\nstore_scores('Seasonal_Naive', sn_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_forecast(sn_predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"## 3.4 - Scores\nNow that we have our three naive models ready, let's see which ones are best suited for each pollutant. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize=(15,8), dpi=80, sharex=True)\nfig.subplots_adjust(wspace=0.1, hspace=0.1)\n\naxlist = [(0, 0), (0, 1), (1, 0), (1, 1)]\n\nfor (i, j), pol in zip(axlist, pollutants):\n    axes[i][j].plot(test[pol], lw=1.5, label='True', c='black')    \n    axes[i][j].plot(sn_predictions[pol], c='darkorange', lw=1, ls='-', label='Seasonal')\n    axes[i][j].plot(avg_predictions[pol], c='red', lw=2.5, ls='--', label='Average')\n    axes[i][j].plot(naive_predictions[pol], c='blue', lw=4, ls=':', label='Naive')   \n    axes[i][j].set_title(pol)\n\naxes[0][0].legend(bbox_to_anchor=[1.5, 1.15], ncol=4);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fc_scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights from results:**\n- **RMSE and MAE** had different values but agreed on which models were the best for all for pollutants\n- **NO2** performed best with the average method - this might suggest the overall values haven't changed much over the years \n- **SO2** had the lowest error on the Naive method. Maybe the past values shouldn't have much influence on it. We can also see on the result plot that the seasonal naive method was higher overall.\n- **CO and O3** have clear seasonal patterns for the test period and that is reflected on the performances. Both got the lowest values using Seasonal Naive Method.\n\nThat's it for the naive models, now we will use a more sofisticated technqiue."},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"# 4 - SARIMAX\nThe model we are building to compare with our naive models is the SARIMAX. \n\n**S** stands for Seasonal, **AR** is auto-regression, <b>I</b> is just integrated, **MA** means moving average and **X** stands for exogenous (i.e. features other than the value itself).\n\nThe problem with SARIMAX is that it doesn't support high values of seasonal frequency (ours is 365). One way around that is by using Fourier Terms to add an exogenous variable that has the proper frequency we need. <a href='https://otexts.com/fpp2/useful-predictors.html'>Chapter 5.4 of Forecasting: Principles and Practice</a> has a nice explanation on this methodology. \n\nAnother great but shorter read for using fourier terms is the blog post <a href='https://medium.com/intive-developers/forecasting-time-series-with-multiple-seasonalities-using-tbats-in-python-398a00ac0e8a'>Forecasting Time Series with Multiple Seasonalities using TBATS in Python</a>.\n\nThat said, the first thing we need to do is to create our **Fourier Terms**. "},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"## 4.1 - Fourier Terms"},{"metadata":{},"cell_type":"markdown","source":"Fourier terms are basically combinations of *sines* and *cosines* using the frequency of our data's seasonality. We will be using the first four terms, which means two sines and 2 cosines - which are defined in the code below. The code cell below creates the data and splits into train and test."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fourier terms\nfourier = pd.DataFrame(index=grouped_data[:'2016-03-31'].index)\n\n# Frequency is being set to 365.25 because we have leap years\nfourier['sin_1'] = np.sin(2 * np.pi * fourier.index.dayofyear / 365.25)\nfourier['cos_1'] = np.cos(2 * np.pi * fourier.index.dayofyear / 365.25)\nfourier['sin_2'] = np.sin(4 * np.pi * fourier.index.dayofyear / 365.25)\nfourier['cos_2'] = np.cos(4 * np.pi * fourier.index.dayofyear / 365.25)\n\nfourier_train = fourier.iloc[:train.shape[0], :]\nfourier_test = fourier.iloc[train.shape[0]-1:, :]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"## 4.2 - AutoArima"},{"metadata":{},"cell_type":"markdown","source":"There are many parameters to choose for an SARIMA model, so we will use pmdarima's <code>auto_arima</code> function that searches for the best sets of parameters. To use our fourier terms we need to feed the <code>exogenous</code> parameter with our fourier terms data.\n\nAnother way of choosing the parameters without doing a grid search is to analyse the ACF and PACF plots to get an idea of the level of AR and MA components, but I will leave that to another study.\n\n<blockquote>\n    <font color='darkblue'>\n        <b>I wasn't able to install the pmdarima module here</b>. Nevertheless I will post the code on markdown cells and the results as images. Sorry for that.\n    </font>\n    </blockquote>\n    \n```python\nfrom pmdarima import auto_arima\n```\n\n```python\nno2_arima = auto_arima(train['no2_aqi'],\n                      exogenous=fourier_train\n                      , start_p=1, start_q=0,\n                      stepwise=True,\n                      suppress_warnings=True,\n                      error_action='ignore')\n\no3_arima = auto_arima(train['o3_aqi'],\n                      exogenous=fourier_train\n                      , start_p=1, start_q=0,\n                      stepwise=True,\n                      suppress_warnings=True)\n\nso2_arima = auto_arima(train['so2_aqi'],\n                       exogenous=fourier_train,\n                       start_p=1, start_q=0, \n                       stepwise=True, \n                       suppress_warnings=True,\n                       error_action='ignore')\n\nco_arima =  auto_arima(train['co_aqi'],\n                       exogenous=fourier_train,\n                       start_p=1, start_q=0, \n                       stepwise=True, \n                       suppress_warnings=True)\n```"},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"## 4.3 - Analysing Results\nNow that we have our models trained, we can predict the test values simply calling the <code>.predict</code> method.\n\n```python\nno2_predictions = no2_arima.predict(n_periods=test.shape[0], exogenous=fourier_test)\no3_predictions  = o3_arima.predict(n_periods=test.shape[0], exogenous=fourier_test)\nso2_predictions = so2_arima.predict(n_periods=test.shape[0], exogenous=fourier_test)\nco_predictions  = co_arima.predict(n_periods=test.shape[0], exogenous=fourier_test)\nsarima_pred = pd.DataFrame(np.c_[no2_predictions,\n                                 o3_predictions,\n                                 so2_predictions,\n                                 co_predictions],\n                           columns=test.columns,\n                           index=test.index)\n```"},{"metadata":{},"cell_type":"markdown","source":"<blockquote>\n    <font color='darkblue'>\n        The code below stores the Sarima scores and groups them with the naive models for comparison\n    </font>\n    </blockquote>\n        \n```python\ns_ix = pd.MultiIndex.from_tuples([('RMSE', 'SARIMA'), ('MAE', 'SARIMA')])\n\nsarima_scores = pd.DataFrame(index=s_ix, columns=pollutants)\n\nfor pol in pollutants:\n    pred = sarima_pred[pol]\n    rmse = np.sqrt(mean_squared_error(test[pol], pred))\n    mae = mean_absolute_error(test[pol], pred)\n    sarima_scores.loc[:,pol] = [rmse, mae]\n    \ngrouped_scores = pd.concat([fc_scores[:3], sarima_scores[:1],\n                            fc_scores[3:], sarima_scores[1:]])\n```\n\n```python\nplot_forecast(sarima_pred)\n```\n\n```python\ngrouped_scores\n```"},{"metadata":{},"cell_type":"markdown","source":"![](https://i.imgur.com/fAUpSBd.png)\n![](https://i.imgur.com/2fxYGrg.png)\n\n**Insights from the results**\n- Visually the SARIMA model has learned well the seasonality of our pollutants;\n- This is reflected on the results. **The SARIMA model got better scores for all pollutants, except SO2**.\n\nOn <u>Chapter Three</u>, **SO2** was the only model that scored best with the Naive predictions and we noticed that both the Average and the Seasonal Naive had higher values than the actual values for the test period. I argued that this could mean that the SO2 test values are lower than the historical data. \n\nSARIMA predictions do catch some seasonality for SO2 (see the 3rd graph above) but it is, once more, higher than the true values. This reinforces the possibility of having atypical low values for the test period."},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# 5. Conclusion\nIn **Chapter One** we started introducing this study and reading the data. We've found many nulls and duplicates and removed them. By the end of chapter one we queried for years 2011 to 2016 and selected cities.\n\nIn **Chapter Two**, exploring our data, we have two dashboards to visualize the data. We've seen that the confidence bands are pretty high for O3 and NO2 and attributed that to a very disperse dataset and the sensibility of the AQI index to weather conditions. We also saw that the Residual component of the time series decomposition is pretty high, meaning, again, a very disperse dataset.\n\n**Chapter Three** was about building naive models. We've build three of them using the average method, the naive method and the seasonal naive method. O3 and CO had the best results using the seasonal naive. NO2 had the lowest error using the average method and, curiously, SO2 had the best results with the naive method. This might suggest that the values in the test set are lower than the data for the other years.\n\nFinally, **Chapter Four** built SARIMA models using Fourier Terms. The results were pretty satisfactory, with all models having lowered the errors - except for SO2. This reinforces the assumption made on chapter three about this pollutant.   \n\nPlease leave your thoughts or questions in the comments. Any feedback is welcome.\n\n**If you've enjoyed it, let me know by UPVOTING! This way I will get motivated to make more Kernels for you.**"},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"<hr>"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}