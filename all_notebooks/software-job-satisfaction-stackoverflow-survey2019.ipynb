{"cells":[{"metadata":{},"cell_type":"markdown","source":"# What StackOverflow 2019 Survey Says About Job Satisfaction?\n\n### Questions of interest\n\n1. What are the best predictors of Annual Income?\n2. What are the best predictors of Job Satisfaction?\n3. What are the occupations of the most satisfied professionals?\n4. What are the methods the most satisfied professionals use to learn?"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import r2_score, mean_squared_error, fbeta_score, accuracy_score\nimport re\nimport seaborn as sns\nimport sys\nfrom time import time\n%matplotlib inline\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reload_questions(verbose=False):\n    \"\"\" Method used to load and modify the schema.\n    \n    The file scheme contains the question used in the survey.\n    The original question were edited so they can fit properly in the graphics.\"\"\"\n    \n    ds_scheme = pd.read_csv('/kaggle/input/stack-overflow-developer-survey-results-2019/survey_results_schema.csv')\n    \n    question_replacements = {\n        r'MainBranch': r'Which of the following options best describes you today?',\n        r'EduOther': r'Which type of non-degree education have you used or participated in?',\n        r'DevType': r'Which of the following describe you?',\n        r'WorkChallenge': r'Of these options, what are your greatest challenges to productivity as a developer?',\n        r'LanguageWorkedWith': r'Which of the following programming, scripting, and markup languages have you worked in the last year?',\n        r'PlatformWorkedWith': r'Which platform have you worked with in the past year?',\n        r'PlatformDesireNextYear': r'Which platform you want to work in the next year?',\n        r'MiscTechWorkedWith': r'Which frameworks, libraries, and tools have you worked to work with in the past year?',\n        r'MiscTechDesireNextYear': r'Which frameworks, libraries, and tools you want to work next in the year?',\n        r'Age': r'What is your age (in years)?',\n        r'Ethnicity': r'Which of the following do you identify as?'\n    }\n    for column_name, after in question_replacements.items():\n        if verbose:\n            print(\"\\nBEFORE: \", ds_scheme.loc[ds_scheme['Column'] == column_name, 'QuestionText'].values[0])\n        \n        ds_scheme.loc[ds_scheme['Column'] == column_name, 'QuestionText'] = after\n        \n        if verbose:\n            print(\"AFTER: \", ds_scheme.loc[ds_scheme['Column'] == column_name, 'QuestionText'].values[0])\n    \n    return ds_scheme","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_questions_text = reload_questions(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_first = pd.read_csv('/kaggle/input/stack-overflow-developer-survey-results-2019/survey_results_public.csv')\n\ndf_first.drop(columns='Respondent', inplace=True)\n#Drop Respondent because it is a index and Pandas already create another index.\n\ndf_first.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_first.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_first.EdLevel.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_first.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_first.nunique().sort_values(ascending=True).head(20)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_first.nunique().sort_values(ascending=False).head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that some features have much more unique values than expected. LanguageDesireNextYear for instance has 27259 unique values. I guess there is not enough languages in the world for that."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_first['LanguageDesireNextYear'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is the problem! Some features have multiple selection responses stored in the same column. The different values are separeted by a semicolon.\n\nLet's check what columns have values separeted by semicolon:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"for col in df_first.sort_index(axis=1).columns:\n    # Look at any string\n    if df_first[col].dtype == 'object':\n        \n        counts = df_first[col].value_counts()\n\n        value_samples = []\n        counter = 0\n        for index, value in zip(counts.index, counts.values):\n            if ';' in index: # Check if is separeted by semicolon\n                value_samples.append(index)\n                counter = counter + 1\n                \n                # Print only 5 values, some colums have thousands of uniques with ;\n                if counter > 5: break\n        if len(value_samples) > 0:\n            print(col)\n            print(value_samples, '\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"WorkPlan looks different than the others. It has only one values separated by semicolon."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_first.WorkPlan.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"WorkPlan is an exception. It has a semicolon in \"There's no schedule or spec; I work on what seems most important or urgent\", but it is not a multiple selection feature, its grammar.\n\nLet's define all the columns that must be expanded."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Columns to be expanded\nmultiple_choice = ['Containers',\n                  'DatabaseDesireNextYear',\n                  'DatabaseWorkedWith',\n                  'DevEnviron',\n                  'DevType',\n                  'EduOther',\n                  'Ethnicity',\n                  'Gender',\n                  'JobFactors',\n                  'LanguageDesireNextYear',\n                  'LanguageWorkedWith',\n                  'LastInt',\n                  'MiscTechDesireNextYear',\n                  'MiscTechWorkedWith',\n                  'PlatformDesireNextYear',\n                  'PlatformWorkedWith',\n                  'SONewContent',\n                  'SOVisitTo',\n                  'Sexuality',\n                  'WebFrameDesireNextYear',\n                  'WebFrameWorkedWith',\n                  'WorkChallenge']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def expand_categories(df, column, sep=\";\", template_name=\"{0}_{1}\", replace=False):\n    \"\"\" Expand the aggregated categories joined by semicolun into different columns.\n    \n    Parameters:\n    df\n        The dataframe containing the aggregated columns.\n    column\n        The column with aggregated values.\n    template_name\n        The template the name the new columns for expanded values.\n    replace\n        A boolean indicating if the new columns must replace the old aggregated columns.\n        \n    Returns\n    A dataframe with the new columns\n    A list of the new columns\n    \"\"\"\n    feat = df[column]\n\n    sys.stdout.write(\"\\r[%-20s] %d%%\" % ('='*0, 0))\n\n    # Find all unique values separeted by semicolon\n    unique_values = set()\n    for i in feat:\n        if type(i) == str:\n            vals = i.split(sep)\n            for v in vals:\n                unique_values.add(v.strip())\n\n    def contains(target, term):\n        return int(term.casefold() in str(target).casefold())\n\n    progress=0\n    total_uniques = len(unique_values)\n    expanded = {}\n    added_names = []\n    for uvalue in unique_values: # Creates a new column for each unique value\n        # Replace unwanted characters\n        new_feat_name = uvalue.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"'\", \"\")\n        new_feat_name = re.sub(r'[)(-,.:]', '', new_feat_name)\n        \n        # Define the new name\n        new_feat_name = template_name.format(column, new_feat_name)\n        added_names.append(new_feat_name)\n\n        # Create the new column according to the content of the old column\n        result = pd.Series(feat.apply(contains, args=[uvalue]), index=feat.index)\n        expanded[new_feat_name] = result\n\n        if replace:\n            df[new_feat_name] = result\n\n        # Print progress\n        sys.stdout.write(\"\\r[%-20s] %d%%\" % ('='*(progress*20//total_uniques), progress*100//total_uniques))\n        sys.stdout.flush()\n        progress += 1\n\n    if replace:\n        df.drop(columns=column, inplace=True)\n\n    # Print progress\n    sys.stdout.write(\"\\r[%-20s] %d%%\" % ('='*(20), 100))\n    print()\n    return pd.DataFrame(expanded), added_names\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List with all expanded new features\nexpanded_feats = []\n\ndf_expanded = df_first.drop(columns=multiple_choice) # Remove the multiple choice columns\n\nfor column in multiple_choice: # Expand each multiple-choice column\n    print(column)\n    new_expanded, new_feats = expand_categories(df_first, column)\n    \n    # Merge the result with the dataframe withou multiple-choice columns\n    df_expanded = pd.merge(df_expanded, new_expanded, left_index=True, right_index=True, how='outer')\n    expanded_feats.extend(new_feats) # Update the list","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Checking what is new\nexpanded_feats","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_expanded.YearsCode.value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's define what feaures must be numerical and what must be dummy variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nto_dummy_without_na = ['Hobbyist']\nto_dummy_with_na = ['BetterLife',\n                    'BlockchainIs',\n                    'BlockchainOrg',\n                    'CodeRev',\n                    'CompFreq',\n                    'Country',\n                    'CurrencyDesc',\n                    'CurrencySymbol',\n                    'Dependents',\n                    'Employment',\n                    'EntTeams',\n                    'Extraversion',\n                    'FizzBuzz',\n                    'ITperson',\n                    'MainBranch',\n                    'OffOn',\n                    'OpSys',\n                    'PurchaseHow',\n                    'ResumeUpdate',\n                    'ScreenName',\n                    'SocialMedia',\n                    'Trans',\n                    'UndergradMajor',\n                    'UnitTests',\n                    'WorkLoc']\n\n# Categorical values representing intervals (replace by average)\ninterval_to_numerical = ['CareerSat',\n                         'EdLevel',\n                         'ImpSyn',\n                         'JobSat',\n                         'JobSeek',\n                         'LastHireDate',\n                         'MgrMoney',\n                         'MgrWant',\n                         'MgrIdiot',\n                         'OpenSource',\n                         'OpenSourcer',\n                         'OrgSize',\n                         'PurchaseWhat',\n                         'SOAccount',\n                         'SOComm',\n                         'SOFindAnswer',\n                         'SOHowMuchTime',\n                         'SOJobs',\n                         'SOPartFreq',\n                         'SOTimeSaved',\n                         'SOVisitFreq',\n                         'Student',\n                         'SurveyEase',\n                         'SurveyLength',\n                         'WelcomeChange',\n                         'WorkPlan',\n                         'WorkRemote']\n\n# Columns with dtype object holding numbers (convert string to number)\nobject_to_num = ['Age1stCode', 'SOVisit1st', 'YearsCode', 'YearsCodePro']\n\n# Numerical columns (nothing to be done)\nnumerical = ['Age',\n             'CodeRevHrs',\n             'CompTotal',\n             'ConvertedComp',\n             'WorkWeekHrs']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_uniques(df, columns):\n    \"\"\" Prints the unique values of the column.\n    \n    It prints in a way to facilitates the create of a dictionary to trasnforme are unique value.\n    \"\"\"\n    for c in df.sort_index(axis=1).columns:\n        if c in columns :\n            \n            print(\"\\n=========\", c, \"=========\")\n            print(\"(\", df[c].dtype, \")\")\n            \n            for i in df[c].value_counts(dropna=False).index:\n                if i is np.NaN :\n                    print(\"np.nan : np.nan,\".format(i))#Keep nans as nans\n                else:\n                    print(\"'{}' : ,\".format(i))# Prints an empty entry","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_uniques(df_expanded, to_dummy_with_na)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print_uniques(df_expanded, interval_to_numerical)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's create dictionaries to transforme each categorical feature that can be transformed into numbers."},{"metadata":{"trusted":true},"cell_type":"code","source":"replacemens_interval_num = {\n'CareerSat': {\n    np.nan:np.nan,\n    'Very satisfied':4,\n    'Slightly satisfied':3,\n    'Neither satisfied nor dissatisfied':2,\n    'Slightly dissatisfied':1,\n    'Very dissatisfied':0\n},\n'EdLevel' : {\n    np.nan : np.nan,\n    'Other doctoral degree (Ph.D, Ed.D., etc.)' : 8,\n    'Master’s degree (MA, MS, M.Eng., MBA, etc.)' : 7,\n    'Bachelor’s degree (BA, BS, B.Eng., etc.)' : 6,\n    'Professional degree (JD, MD, etc.)' : 5,\n    'Associate degree' : 4,\n    'Some college/university study without earning a degree' : 3,\n    'Secondary school (e.g. American high school, German Realschule or Gymnasium, etc.)' : 2,\n    'Primary/elementary school' : 1,\n    'I never completed any formal education' : 0,\n},\n'ImpSyn' : {\n    np.nan : np.nan,\n    'Far above average' : 2,\n    'A little above average' : 1,\n    'Average' : 0,\n    'A little below average' : -1,\n    'Far below average' : -2,\n},\n'JobSeek' : {\n    np.nan : np.nan,\n    'I am actively looking for a job' : 2,\n    'I’m not actively looking, but I am open to new opportunities' : 1,\n    'I am not interested in new job opportunities' : 0,\n},\n\n'JobSat' :  {\n    np.nan:np.nan,\n    'Very satisfied' : 4,\n    'Slightly satisfied' : 3,\n    'Neither satisfied nor dissatisfied' : 2,\n    'Slightly dissatisfied' : 1,\n    'Very dissatisfied' : 0\n},\n'LastHireDate' : {\n    np.nan : np.nan,\n    'More than 4 years ago' : 6, # 6 years\n    '3-4 years ago' : 3.5, # 3.5 years\n    '1-2 years ago' : 1.5, #1.5 years\n    'Less than a year ago' : 0.5, # half year\n    'I\\'ve never had a job' : 0,\n    'NA - I am an independent contractor or self employed' : -1,# Create a boolean feature for this -1\n},\n'MgrIdiot' : {\n    np.nan : np.nan,\n    'Very confident' : 3,\n    'Somewhat confident' : 2,\n    'Not at all confident' : 1,\n    'I don\\'t have a manager' : -1,# Create a boolean feature for this -1\n},\n'MgrMoney' :{\n    'Yes' : 1,\n    np.nan : 0.5,\n    'Not sure' : 0.5,\n    'No' : 0,\n},\n'MgrWant' : {\n    'Yes' : 1,\n    np.nan : 0.5,\n    'Not sure' : 0.5,\n    'No' : 0,\n    'I am already a manager' : -1,# Create a boolean feature for this -1\n},\n'OpenSource' : {\n    np.nan : np.nan,\n    'OSS is, on average, of HIGHER quality than proprietary / closed source software' : 1,\n    'The quality of OSS and closed source software is about the same' : 0,\n    'OSS is, on average, of LOWER quality than proprietary / closed source software' : -1,\n},    \n'OpenSourcer' : {\n    'Once a month or more often' : 18/12,# 18 times per yers\n    'Less than once a month but more than once per year' : 6/12, # 6 times per yers\n    'Less than once per year' : 0.5/12, # less than 1 per year\n    'Never' : 0,\n},\n'OrgSize' : {\n    np.nan : np.nan, # keep as nan\n    '10,000 or more employees':(10000 * 10)/2, # between 10'000 and 100'000\n    '5,000 to 9,999 employees' :(5000 + 9999)/2,# mean values...\n    '1,000 to 4,999 employees':(1000 + 4999)/2,\n    '500 to 999 employees':(500 + 999)/2,\n    '100 to 499 employees' : (100 + 499)/2,\n    '20 to 99 employees' : (99+20)/2,\n    '10 to 19 employees':(10 + 19)/2,\n    '2-9 employees':(2 + 9)/2,\n    'Just me - I am a freelancer, sole proprietor, etc.': 1\n},\n'PurchaseWhat' : {\n    np.nan : np.nan,\n    'I have a great deal of influence' : 3,\n    'I have some influence' : 1,\n    'I have little or no influence' : 0,\n},\n'SOAccount' : {\n    'Yes' : 1,\n    'Not sure / can\\'t remember' : 0.5,\n    np.nan : 0.5,\n    'No' : 0,\n},    \n'SOComm' : {\n    np.nan : np.nan,\n    'Yes, definitely' : 4,\n    'Yes, somewhat' : 3,\n    'Neutral' : 2,\n    'Not sure' : 2,\n    'No, not really' : 1,\n    'No, not at all' : 0,\n},\n'SOFindAnswer' : {\n    np.nan : np.nan,\n    'More than 10 times per week' : 15, # between 10 and 20 per week\n    '6-10 times per week' : 8, # 8 per week\n    '3-5 times per week' : 4, # 4 per week\n    '1-2 times per week' : 1.5, # 1.5 per week\n    'Less than once per week' : 0.5, # 0.5 per week\n},\n'SOHowMuchTime' : {\n    np.nan : np.nan,\n    '60+ minutes' : 90,\n    '31-60 minutes' : 45.5,\n    '11-30 minutes' : 20.5,\n    '0-10 minutes' : 5,\n},\n'SOJobs' : {\n    np.nan : np.nan,\n    'Yes' : 1,\n    'No, I knew that Stack Overflow had a job board but have never used or visited it' : 0.5,\n    'No, I didn\\'t know that Stack Overflow had a job board' : 0,\n},\n'SOPartFreq' : {\n    np.nan : np.nan,\n    'Multiple times per day' : 3,\n    'Daily or almost daily' : 1,\n    'A few times per week' : 10/30,\n    'A few times per month or weekly' : 4/30,\n    'Less than once per month or monthly' :1/30 ,\n    'I have never participated in Q&A on Stack Overflow' : 0\n},\n'SOTimeSaved' : {\n    np.nan : np.nan,\n    'Stack Overflow was much faster' : 2,\n    'Stack Overflow was slightly faster' : 1,\n    'They were about the same' : 0,\n    'The other resource was slightly faster' : -1,\n    'The other resource was much faster' : -2,\n},\n'SOVisitFreq' : {\n    np.nan : np.nan,\n    'Multiple times per day' : 3, # 3 times a day (=90/30)\n    'Daily or almost daily' : 1, # every day (=30/30)\n    'A few times per week' : 10/30, # 10 per month\n    'A few times per month or weekly' : 5/30,# 5 per month\n    'Less than once per month or monthly' : 0.5/30, # 0.5 per month\n    'I have never visited Stack Overflow (before today)' : 0,\n},\n'Student' : {\n    np.nan : np.nan,\n    'Yes, full-time' : 1,\n    'Yes, part-time' : 0.5,\n    'No' : 0,\n},\n'SurveyEase' : {\n    np.nan : np.nan,\n    'Easy' : 0,\n    'Neither easy nor difficult' : 0.5,\n    'Difficult' : 1,\n},\n'SurveyLength' : {\n    np.nan : np.nan,\n    'Too long' : 1,\n    'Appropriate in length' : 0.5,\n    'Too short' : 0,\n},\n'WelcomeChange' : {\n    np.nan : np.nan,\n    'Not applicable - I did not use Stack Overflow last year' : np.nan,\n    'A lot more welcome now than last year' : 2,\n    'Somewhat more welcome now than last year' : 1,\n    'Just as welcome now as I felt last year' : 0,\n    'Somewhat less welcome now than last year' : -1,\n    'A lot less welcome now than last year' : -2,\n},\n'WorkPlan' : {\n    np.nan : np.nan,\n    'There is a schedule and/or spec (made by me or by a colleague), and I follow it very closely' : 1,\n    'There is a schedule and/or spec (made by me or by a colleague), and my work somewhat aligns' : 0.5,\n    'There\\'s no schedule or spec; I work on what seems most important or urgent' : 0,\n},\n'WorkRemote' : {\n    np.nan : np.nan,\n    'It\\'s complicated' : np.nan,\n    'All or almost all the time (I\\'m full-time remote)' : 1,\n    'More than half, but not all, the time' : 0.75,\n    'About half the time' : 0.5,\n    'Less than half the time, but at least one day each week' : 0.4,\n    'A few days each month' : 0.2,\n    'Less than once per month / Never' : 0,\n}\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some features are almost numbers. In most cases they can be transformed by casting, but they also contain some strings that must be mapped in a dictionary."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print_uniques(df_expanded, object_to_num)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mapping the string values that need to be transformed into numbers\nreplacemens_str_num = {\n    np.nan : np.nan,\n    'I don\\'t remember' : np.nan,\n    'Less than 1 year' : 0.5,\n    'Younger than 5 years' : 4.0,\n    'More than 50 years' : 60.0,\n    'Older than 85' : 90.0,\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convertTo_num(df, map_str_num, map_inter_num, map_dummies_na, map_dummies_nona, verbose=False, drop_firt_dummy=True):\n    \"\"\" Convert all the strings in the dataframe into numbers.\n    \n    This is an impure function. It uses parameters and values defined in other cells.\n    \n    Parameteres:\n    df\n        The dataframe to be transformed\n    map_str_num\n        The dictionary mapping values from string to numbers.\n    map_dummies_na\n        The dictionary mapping feature to be transformed into dummies. These\n        features may contain NaNs.\n    map_dummies_nona\n        The dictionary mapping feature to be transformed into dummies. These\n        features do NOT contain NaNs.\n    verbose\n        Boolean to indicate if need to print logs.\n    drop_firt_dummy\n        Boolean indicating if the first dummy variable must be dropped.\n    \"\"\"\n    \n    def replace_float(value, mapping):\n        if value in mapping:\n            return mapping[value]\n        else:\n            return float(value)\n\n    df_new = df.copy()\n    for col_name, mapping in map_inter_num.items():\n\n        df_new[col_name] = df_new[col_name].apply(func = lambda value: mapping[value])\n        if verbose: print(\"{}: dtype{}\".format(col_name, df_new[col_name].dtype))\n\n    new_feat_self_emp = 'LastHireDate_self_employed'\n    df_new[new_feat_self_emp] = df_expanded['LastHireDate'] == (-1)\n    df_new['LastHireDate'].replace(-1, np.nan, inplace=True)\n    \n    new_feat_no_manager = 'MgrIdiot_dont_have_manager'\n    df_new[new_feat_no_manager] = df_expanded['MgrIdiot'] == (-1)\n    df_new['MgrIdiot'].replace(-1, np.nan, inplace=True)\n    \n    new_feat_Iam_manager = 'MgrWant_Iam_maneger'\n    df_new['MgrWant_Iam_maneger'] = df_expanded['MgrWant'] == (-1)\n    df_new['MgrWant'].replace(-1, np.nan, inplace=True)\n    \n    new_dummies = [new_feat_self_emp, new_feat_no_manager, new_feat_Iam_manager]\n\n    print()\n    for col_name in object_to_num:\n\n        df_new[col_name] = df_new[col_name].apply(func = lambda value: replace_float(value, map_str_num))\n        if verbose: print(\"{}: dtype{}\".format(col_name, df_new[col_name].dtype))\n\n\n    new_dummies = []\n    dummies = pd.get_dummies(df_new[map_dummies_na], drop_first=drop_firt_dummy)\n    new_dummies.extend(dummies.columns)\n\n    df_new = pd.merge(df_new, dummies, right_index=True, left_index=True, how='outer')\n    df_new.drop(columns = map_dummies_na, inplace=True)\n\n\n    dummies = pd.get_dummies(df_new[map_dummies_nona], drop_first=drop_firt_dummy)\n    new_dummies.extend(dummies.columns)\n\n    df_new = pd.merge(df_new, dummies, right_index=True, left_index=True, how='outer')\n    df_new.drop(columns = map_dummies_nona, inplace=True)\n    \n    return df_new, new_dummies\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert the values of the dataframe into numbers\ndf_numerical, new_dummies = convertTo_num(df_expanded,\n                                          replacemens_str_num,\n                                          replacemens_interval_num,\n                                          to_dummy_with_na,\n                                          to_dummy_without_na)\ndf_numerical.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_numerical.isna().sum().sort_values(ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_numerical.isna().sum().sort_values(ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef get_group_dummies(df, name):\n    \"\"\" This function look for columns starting with a name.\n    \n    It is very useful to find dummy variables named with a prefix.\n    \n    Parameters:\n    df\n        The dataframe to look for columns.\n    name\n        The prefix of the column.\n    \"\"\"\n    return [col for col in df.columns if col.startswith(name)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_dummies","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_numerical[new_dummies].sum().sort_values().head(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_numerical.SOTimeSaved.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_last = df_numerical\nfor c in df_last.sort_index(axis=1).columns:\n    #if c not in expanded_feats :\n        print(\"\\n\\n\\n============={}=============\".format(c))\n        print(df_last[c].dtype)\n        print(df_last[c].value_counts(dropna=False).head(30))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check how many unique values remain. DesiredLangague doesn't have a myriad of unique values anymore."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Check how many unique values remain\ndf_numerical.nunique().sort_values(ascending=False).head(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_numerical.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_numerical.isna().sum().sort_values()/df_numerical.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_feat_and_dummies(df, features):\n    \"\"\" Remove the feature informed and their originated dummies.\n    \n    Parameters:\n    df\n        The dataframe from which the features are going to be removed.\n    features\n        The prefix of the features to be removed. If there are conflic \n        of prefix between different features they all will be removed.\n    \"\"\"\n    correlated_columns = []\n    for feat in features:\n        correlated_columns.extend(get_group_dummies(df, feat))\n    df_corr = df.drop(columns=correlated_columns)\n    return df_corr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove unwanted highly correlated columns\n# Currency symbol and description are highly correlated to country\n# Total compensation and compensation frequency are values used to calculate converted compensatoin\n# Age, YearsCode and YearsCodePro are correlated features, let's drop the first two\n# Employment_Employed part-time is correlated to WorkWeekHrs\nfeats_corr = ['CurrencyDesc_', 'CurrencySymbol_', 'CompFreq_', 'CompTotal', 'YearsCode', 'Age', 'Employment_Employed part-time']\ndf_corr = remove_feat_and_dummies(df_numerical, features=feats_corr)\n\nfor fe in feats_corr:\n    assert(len(get_group_dummies(df_corr, fe)) == 0)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"print(\"NaNs %\")\ndf_corr.isna().mean().sort_values().tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ConvertedComp has many nans. If we remove columns with more than 30% nans this columns would be removed. Since this is the column being predicted, it can't be removed. We need to remove nans from this columns first."},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_data(df, dependent_var_name, nans_tresh=0.3, verbose=True):\n    df = df.dropna(axis=0, subset=[dependent_var_name])# Remove nans in the dependent variable\n    \n    df = df.dropna(how='all', axis=0) # Remove rows complete with nans\n    df = df.dropna(how='all', axis=1) # Remove columns complete with nans\n    \n    if verbose: print(\"removing {} columns by unique values\".format(( df.nunique() <= 1).sum()))\n    df = df.loc[:, df.nunique() > 1] # Keep columns with at least two unique values\n    \n    if verbose: print(\"removing {} columns by nans %\".format((df.isna().mean(0) >= nans_tresh).sum()))\n    df = df.loc[:, df.isna().mean(0) < nans_tresh] # Columns with nans% < nans_tresh\n            \n    \n    return df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = clean_data(df_corr, 'ConvertedComp')\n\ndf_last = df_clean\ndf_last.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_corr.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"previous_length = len(df_corr)\nnans_in_dependent = df_corr.ConvertedComp.isnull().sum()\nprint(\"The dataframe had {} rows.\".format(previous_length))\nprint(\"There were {} NaNs in the dependet variable.\".format(nans_in_dependent))\nprint(\"These rows with NaNs were removed and {} remains.\".format(previous_length-nans_in_dependent))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def drop_all_nans(df, column_treshold = 0.1, verbose=True):\n    \"\"\" Remove columns with a certain threshold of nans and then remove all nans\n    \n    Parameters:\n    df\n        The dataframe to remove nans\n    column_treshold\n        The limit to drop the column if it has more nans than expected.\n    verbose\n        Boolean indicating if must print information\n    \"\"\"\n    \n    if verbose:\n        print(\"Removing {} columns by nans %\".format((df.isna().mean() >= column_treshold).sum()))\n        \n    # Keep columns with nans% < column_treshold\n    df_new = df.loc[:, (df.isna().mean() < column_treshold)]\n\n    # Drop nans\n    if verbose:\n        removing_rows = (df_new.isna().sum(1) > 0).sum()\n        previous_rows = len(df_new)\n        print(\"Removing {} rows with NaNs values from the total {} rows.\".format(removing_rows, previous_rows))\n    \n    df_new = df_new.dropna(axis=0, how='any')\n    \n    if verbose:\n        print(\"Final shape {}\".format(df_new.shape))\n    \n    return df_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop nans\ndf_dense = drop_all_nans(df_clean)\n\ndf_last = df_dense\ndf_last.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The function **clean_data** removes rows with NaNs in the depended variable. There were more than 30% NaNs in this variable. If we had used the function **drop_all_nans** before **clean_data**, the columns with the dependend variable would have been removed, because this method firstly removes all columns with high percentage of NaNs and than removes all rows with ant NaNs. As a consequence, less rows are removed because because there are many rows with NaNs in the dependent variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_all_nans(df_corr).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_cuttoff(df, cutoff, verbose=False):\n    \"\"\" Drop columns with a certaing amout of zeros.\n    \n    Parameters:\n    df\n        The dataframe to drop columns\n    cutoff\n        The limit (%) of zeros allowed in the columns.\n    \n    Returns:\n    The dataframe with columns removed.\n    \"\"\"\n    \n    reduce_X = df.loc[:, (df == 0).mean(0) < cutoff]\n    for col_name in to_dummy_with_na:\n        dummies = get_group_dummies(reduce_X, col_name)\n        \n        non_zeros = (reduce_X[dummies] != 0).sum().sum()\n        if len(dummies) >= 2 and non_zeros > (reduce_X.shape[0] * .99):\n            # Drop first dummy if non-zeros values represent more than 99% or rows\n            if verbose:\n                print(\"Dropping dummy {}\".format(dummies[0]))\n                \n            reduce_X = reduce_X.drop(columns=dummies[0])\n            \n    return reduce_X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_cuttoffs(X, y, cutoffs, test_size = 0.3, random_state=50, plot=True):\n    '''\n    Parameters:\n    X\n        The dataframe of independent variables (predictors).\n    y\n        The dataframe of dependent variable to be predicted.\n    cutoffs\n        List of floats of percentage of zeros allowed in the columns.\n    test_size\n        Float between 0 and 1, default 0.3, determines the proportion of data as test data\n    random_state\n        Int, default 50, controls random state for train_test_split\n    plot\n        Boolean, True to plot result\n\n    Returns:\n    scores\n        the scores of all trains\n    lm_model\n        The best linear regression\n    data_split\n        The data split of the best model\n    best_cutoff_st\n        The best cutoff\n    '''\n    r2_scores_test, r2_scores_train, num_feats, results = [], [], [], dict()\n    for cutoff in cutoffs:\n        print(\"Cutoff:\", cutoff)\n\n        reduce_X = reduce_cuttoff(X, cutoff, verbose=False)\n        print(\"num_feats: {}\".format(reduce_X.shape[1]))\n        num_feats.append(reduce_X.shape[1])\n\n        #split the data into train and test\n        X_train, X_test, y_train, y_test = train_test_split(reduce_X, y, test_size = test_size, random_state=random_state)\n\n        #fit the model and obtain pred response\n        lm_model = LinearRegression(normalize=True)\n        lm_model.fit(X_train, y_train)\n        \n        y_test_preds = lm_model.predict(X_test)\n        y_train_preds = lm_model.predict(X_train)\n        \n        r2_test = r2_score(y_test, y_test_preds)\n        r2_train = r2_score(y_train, y_train_preds)\n        mse_test = mean_squared_error(y_test, y_test_preds)\n        mse_train = mean_squared_error(y_train, y_train_preds)\n\n        scores = {\n            \"r2_scores_test\" : r2_test,\n            \"r2_scores_train\" : r2_train,\n            \"mse_scores_test\" : mse_test,\n            \"mse_scores_train\" : mse_train,\n        }\n        \n        print(scores)\n\n        #append the r2 value from the test set\n        r2_scores_test.append(r2_test)\n        r2_scores_train.append(r2_train)\n        results[str(cutoff)] = r2_score(y_test, y_test_preds)\n\n    best_cutoff_st = max(results, key=results.get)\n    print(\"\\nBest cutoff:\", best_cutoff_st)\n\n    if plot:\n        #plt.plot(num_feats, r2_scores_test, label=\"Test\", alpha=.5, )\n        #plt.plot(num_feats, r2_scores_train, label=\"Train\", alpha=.5)\n        plt.plot(cutoffs, r2_scores_test, label=\"Test\", alpha=.5, )\n        plt.plot(cutoffs, r2_scores_train, label=\"Train\", alpha=.5)\n        plt.xlabel('Cutoff')\n        plt.ylabel('Rsquared')\n        plt.title('Rsquared by Number of Features')\n        plt.legend(loc=1)\n        plt.grid(True)\n        plt.show()\n\n    reduce_X = reduce_cuttoff(X, float(best_cutoff_st))\n    num_feats.append(reduce_X.shape[1])\n\n    #split the data into train and test\n    X_train, X_test, y_train, y_test = train_test_split(reduce_X, y, test_size = test_size, random_state=random_state)\n    \n    data_split = {\n        \"X_train\":X_train,\n        \"X_test\": X_test,\n        \"y_train\": y_train,\n        \"y_test\": y_test,\n    }\n\n    #fit the model\n    lm_model = LinearRegression(normalize=True)\n    lm_model.fit(X_train, y_train)\n    \n    y_test_preds = lm_model.predict(X_test)\n    y_train_preds = lm_model.predict(X_train)\n    \n    r2_test = r2_score(y_test, y_test_preds)\n    r2_train = r2_score(y_train, y_train_preds)\n    mse_test = mean_squared_error(y_test, y_test_preds)\n    mse_train = mean_squared_error(y_train, y_train_preds)\n\n    scores = {\n        \"r2_scores_test\" : r2_test,\n        \"r2_scores_train\" : r2_train,\n        \"mse_scores_test\" : mse_test,\n        \"mse_scores_train\" : mse_train,\n    }\n    \n\n    return scores, lm_model, data_split, float(best_cutoff_st)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dense.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compensation"},{"metadata":{},"cell_type":"markdown","source":"## Training Random Forests"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\ndef fit_RF(df, y_colum, max_depth=5, n_estimators=100, test_size = 0.3, random_state=0):\n    \"\"\"Trains a RandomForest.\n    df\n        The dataframe with independent variables to be trained (predictors).\n    y_colum\n        The dataframe with dependent variables to be predicted.\n    max_depth\n        default=5\n        Mas depth of the tress in the RandomForests\n    n_estimators \n        default=100\n        The number of tress to be trained.\n    test_size\n        default=0.3\n        The percentage of date used to test the model (between 0 and 1).\n    random_state\n        default=0\n        Controls random state for train_test_split\n    \"\"\"\n    \n    y = df[y_colum] # define y\n    X = df.drop(columns=[y_colum]) # remove y from dataset\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state=random_state)\n    data_split = {\n        'X_train' : X_train,\n        'X_test' : X_test,\n        'y_train' : y_train,\n        'y_test' : y_test\n    }\n\n    regr = RandomForestRegressor(max_depth=max_depth, random_state=random_state,\n                                 n_estimators=n_estimators)\n    regr.fit(X_train, y_train)\n    \n    y_test_preds = regr.predict(X_test)\n    y_train_preds = regr.predict(X_train)\n    \n    preds = {\n        'y_test_preds' : y_test_preds,\n        'y_train_preds' : y_train_preds\n    }\n\n    #append the r2 value from the test set\n    r2_scores_test = r2_score(y_test, y_test_preds)\n    r2_scores_train = r2_score(y_train, y_train_preds)\n    mse_scores_test = mean_squared_error(y_test, y_test_preds)\n    mse_scores_train = mean_squared_error(y_train, y_train_preds)\n    \n    scores = {\n        \"r2_scores_test\" : r2_scores_test,\n        \"r2_scores_train\" : r2_scores_train,\n        \"mse_scores_test\" : mse_scores_test,\n        \"mse_scores_train\" : mse_scores_train,\n    }\n\n    print(scores)\n\n    print(regr.feature_importances_)\n    \n    return regr, scores, data_split, preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the RandomForest\nregr_comp_RF, scores_comp_RF, data_split_comp_RF, preds_comp_RF = fit_RF(df_dense,\n                                                                         y_colum = 'ConvertedComp',\n                                                                         max_depth=5,\n                                                                         n_estimators=50,\n                                                                         random_state=10)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature importance for RandomForest"},{"metadata":{"trusted":true},"cell_type":"code","source":"def DF_feature_importance(regr, X_train):\n    \"\"\" Defines the feature importance according to the trained model.\n    \n    Parameters:\n    regr\n        The trained RandomForest regressor.\n    X_train\n        The dataframe in which the regressor was trained.\n    \n    Returns\n    A datafram with the importance of each feature.\n    \"\"\"\n    coefs_df_RF = pd.DataFrame()\n    coefs_df_RF['Feature'] = X_train.columns\n    coefs_df_RF['Importance'] = regr.feature_importances_\n    return coefs_df_RF","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coefs_df_comp_RF = DF_feature_importance(regr_comp_RF, data_split_comp_RF['X_train'])\nplot_df = coefs_df_comp_RF.sort_values(by='Importance', ascending=False).head(5)\nprint(plot_df)\nplot_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def retrieve_question(df, question_col, df_question):\n    \"\"\" Retrives the text of the questions.\n    \n    Parameters:\n    df\n        The dataframe containing the columns to recover the question.\n    question_col\n        What columns are going to be used to recover the text.\n    df_question\n        The dataframe with the text of questions.\n    \n    Returns:\n    A list of questions.\n    \"\"\"\n    questions = []\n    \n    for name in df[question_col]:\n        #print(name)\n        print(name)\n        name_split = name.split('_')\n        text = df_question.loc[df_question.Column == name_split[0]]\n        if len(text) > 0:\n            quest = text['QuestionText'].values[0] + \"\\n\" + \" \".join(name_split[1:])\n            questions.append(quest)\n            print(quest)\n        print()\n    \n    return questions\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_questions_text = reload_questions()\nplot_df['Questions'] = retrieve_question(plot_df, \"Feature\", df_questions_text)\nsns.set(font_scale=2.3)\nf, ax = plt.subplots(figsize=(16, 8))\n\nax = sns.barplot(x='Importance', y='Questions', data=plot_df, color=\"r\")\nax.legend(ncol=2, loc=\"lower right\", frameon=True, )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression for Compensation"},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression with the most importat feats of RandomForests\n\nHere we will train a linear regression to understand the contribuition of each important feature from the random forest."},{"metadata":{"trusted":true},"cell_type":"code","source":"def coef_weights(coefficients, X_train):\n    ''' Retrive the coefficient weights of each feature.\n    \n    Parameters:\n    coefficients\n        the coefficients of the linear model \n    X_train\n        the training data, so the column names can be used\n    \n    Returns:\n    coefs_df\n        a dataframe holding the coefficient, estimate, and abs(estimate)\n    '''\n    coefs_df = pd.DataFrame()\n    coefs_df['Feature'] = X_train.columns\n    coefs_df['Coefficient'] = coefficients\n    coefs_df['Coefficient (absolut)'] = np.abs(coefficients)\n    coefs_df = coefs_df.sort_values('Coefficient (absolut)', ascending=False)\n    return coefs_df","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"cutoffs = [1, 0.8] #Percentage of zeros allowed in the columns\n\ny_colum = 'ConvertedComp' # Dependent variable\ny = df_dense[y_colum] # define y\nX = df_dense[plot_df.Feature.values] # Only used the important features from the RandomForest\n\nscores, lm_model, data_split, best_cutoff = train_cuttoffs(X, y, cutoffs, test_size=0.3, random_state=1000, plot=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Coefficients importance of linear regression (the most importat ones)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the coefficents\ncoef_df = coef_weights(lm_model.coef_, data_split['X_train'])\n\n# Normalize the coefficents\ncoef_df['Coefficient_norm'] = coef_df['Coefficient']/np.abs(coef_df['Coefficient'].sum())\ncoef_df.sort_values(by='Coefficient (absolut)', ascending=False).head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncoef_df['Coefficient'] = coef_df['Coefficient']/coef_df['Coefficient'].sum()\n\n\ndf_questions_text = reload_questions()\ncoef_df['Questions'] = retrieve_question(coef_df, \"Feature\", df_questions_text)\n\nsns.set(font_scale=2.3)\nf, ax = plt.subplots(figsize=(16, 8))\n#sns.set_color_codes(\"pastel\")\nax = sns.barplot(x='Coefficient_norm', y='Questions', data=coef_df.sort_index(), color=\"blue\")\nax.legend(ncol=2, loc=\"lower right\", frameon=True, )\nax.set_xlabel('Importance')\nax.set_ylabel('Coefficient')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"coef_df_merge = coef_df.merge(plot_df, on='Feature', left_index=True)\n\ndf_questions_text = reload_questions()\ncoef_df_merge['Questions'] = retrieve_question(coef_df_merge, \"Feature\", df_questions_text)\n\nsns.set(font_scale=2.3)\nf, ax = plt.subplots(figsize=(16, 8))\n#sns.set_color_codes(\"pastel\")\nax = sns.barplot(x='Coefficient', y='Questions', data=coef_df_merge, color=\"blue\")\nax = sns.barplot(x='Importance', y='Questions', data=coef_df_merge, color=\"r\")\n#ax.legend(ncol=2, loc=\"lower right\", frameon=True, )\n#ax.set_xlabel('totalCount')"},{"metadata":{},"cell_type":"markdown","source":"## Also train a LinearRegression with all features"},{"metadata":{"trusted":true},"cell_type":"code","source":"cutoffs = [0.99, 0.98, 0.95]\n\ny_colum = 'ConvertedComp'\ny = df_dense[y_colum] # define y\nX = df_dense.drop(columns=[y_colum]) # remove y from dataset\n\nscores, lm_model, data_split, best_cutoff = train_cuttoffs(X, y, cutoffs, test_size=0.3, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Coefficients importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Use the function\ncoef_df = coef_weights(lm_model.coef_, data_split['X_train'])\n\n#A quick look at the top results\ncoef_df_sort = coef_df.sort_values(by='Coefficient (absolut)', ascending=False).head(10)\ncoef_df_sort","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"\n#coef_df = coef_df[coef_df['Feature'].isin(plot_df['Feature'].values)]\n\ncoef_df_sort = coef_df_sort.head(5)\n\ndf_questions_text = reload_questions()\ncoef_df_sort['Questions'] = retrieve_question(coef_df_sort, \"Feature\", df_questions_text)\n\nsns.set(font_scale=2.3)\nf, ax = plt.subplots(figsize=(16, 8))\n#sns.set_color_codes(\"pastel\")\nax = sns.barplot(x='Coefficient', y='Questions', data=coef_df_sort, color=\"blue\")\nax.legend(ncol=2, loc=\"lower right\", frameon=False, )\n#ax.set_xlabel('totalCount')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Job Satisfaction"},{"metadata":{},"cell_type":"markdown","source":"## Train the RandomForest"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_colum_job = 'JobSat'\n\ndf_clean_job = clean_data(df_corr, y_colum_job)\n\n# JobSat is highly cross correlated to CareerSat\ndf_clean_job = df_clean_job.drop(columns='CareerSat')\n\ndf_last_job = df_clean_job\ndf_last_job.shape\n\ndf_dense_job = drop_all_nans(df_clean_job)\n\n\ndf_last_job = df_dense_job\ndf_last_job.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the RandomForest to the job satisfaction\nregr_job, scores_job, data_split_job, preds_job = fit_RF(df_dense_job,\n                                                                     y_colum = y_colum_job,\n                                                                     max_depth=5,\n                                                                     n_estimators=50,\n                                                                     random_state=200)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Look into the feature importance\ncoefs_df_RF_job = DF_feature_importance(regr_job, data_split_job['X_train'])\ncoefs_df_RF_job_sort = coefs_df_RF_job.sort_values(by='Importance', ascending=False).head(6)\n#print(coefs_df_RF_job_sort)\ncoefs_df_RF_job_sort","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the data about importance to plot\nplot_df = coefs_df_RF_job.sort_values(by='Importance', ascending=False).head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the feature importance about the job satisfaction.\ndf_questions_text = reload_questions()\nplot_df['Questions'] = retrieve_question(plot_df, \"Feature\", df_questions_text)\n\nsns.set(font_scale=2.3)\nf, ax = plt.subplots(figsize=(16, 8))\n\nax = sns.barplot(x='Importance', y='Questions', data=plot_df, color=\"r\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Coefficients importance (the most importat ones)"},{"metadata":{"trusted":true},"cell_type":"code","source":"cutoffs = [1, 0.8]\n\n\ny_colum_job = 'JobSat'\ny = df_dense_job[y_colum_job] # define y\nX = df_dense_job[plot_df.Feature.values] # remove y from dataset\n\nscores, lm_model, data_split, best_cutoff = train_cuttoffs(X, y, cutoffs, test_size=0.3, random_state=1000, plot=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Retrieve the weights of the coefficents\ncoef_df = coef_weights(lm_model.coef_, data_split['X_train'])\n\n\ncoef_df.sort_values(by='Coefficient (absolut)', ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"coef_df = coef_df[coef_df['Feature'].isin(plot_df['Feature'].values)]\n\ncoef_df['Coefficient_norm'] = coef_df['Coefficient']/np.abs(coef_df['Coefficient'].sum())\n\n\ndf_questions_text = reload_questions()\ncoef_df['Questions'] = retrieve_question(coef_df, \"Feature\", df_questions_text)\n\nsns.set(font_scale=2.3)\nf, ax = plt.subplots(figsize=(16, 8))\n#sns.set_color_codes(\"pastel\")\nax = sns.barplot(x='Coefficient_norm', y='Questions', data=coef_df.sort_index(), color=\"blue\")\n#ax.legend(ncol=2, loc=\"lower right\", frameon=True, )\nax.set_xlabel('Importance')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Occupation of people with high job satisfaction\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# What are the occupations of people with high job satisfaction?\ndf_numerical, new_dummies = convertTo_num(df_expanded,\n                                          replacemens_str_num,\n                                          replacemens_interval_num,\n                                          to_dummy_with_na,\n                                          to_dummy_without_na)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_corr(df, feats, feat_corrwith):\n    \"\"\"Calculates the correlation between a set of features and another feature.\n    \n    Parameters:\n    feats\n        A list with the names of features to calculate the correlation.\n    feat_corrwith\n        The name of the other feature to calculate the correlation\n        \n    Returns:\n    A dataframe with the correlation.\n    \"\"\"\n    df_corrwith = pd.DataFrame()\n    df_corrwith['corr'] = df[get_group_dummies(df, feats)].corrwith(df[feat_corrwith]).sort_values()\n    df_corrwith = df_corrwith.reset_index()\n    df_questions_text = reload_questions()\n    df_corrwith['Questions'] = retrieve_question(df_corrwith, \"index\", df_questions_text)\n    return df_corrwith","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Calculates the correlation between all the types of dev and the job satisfaction\ndf_corr_occupation = calculate_corr(df_dense, 'DevType', 'JobSat')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_corr_occupation","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Retrives the questions' text\ndf_questions_text = reload_questions()\ndf_corr_occupation['Questions'] = retrieve_question(df_corr_occupation, \"index\", df_questions_text)\n\ndf_corr_occupation['Questions2'] = df_corr_occupation['Questions'].apply(lambda x : x.rsplit('\\n')[1])\ndf_corr_occupation['Questions2']\ndf_corr_occupation","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Plot the correlation\nsns.set(font_scale=2.3)\nf, ax = plt.subplots(figsize=(16, 33))\n#sns.set_color_codes(\"pastel\") # dark deep\nax = sns.barplot(x='corr', y='Questions2', data=df_corr_occupation.sort_values(by='corr', ascending=False))#, color=\"blue\"\n#ax.legend(ncol=2, loc=\"lower right\", frameon=True, )\nax.set_xlabel('Correlation')\nax.set_ylabel('Occupation')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Relative Frequency - occupation by job satisfaction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the dummy variable of developer type \nfeats_dev_job = get_group_dummies(df_dense, 'DevType')\n\n# Add job satisfaction to the list\nfeats_dev_job.append('JobSat')\ndf_dense_corr = df_dense[feats_dev_job]\n\n# Group the dev types by job satisfaction\ngroup = df_dense_corr.groupby(['JobSat']).sum()/df_dense_corr.sum().drop('JobSat')*100\ngroup = group.reset_index()\ngroup = group.melt(id_vars = 'JobSat', var_name=\"variable\")\n\n#df_questions_text = reload_questions()\ngroup['Occupation'] = retrieve_question(group, \"variable\", df_questions_text)\n\ngroup['Occupation'] = retrieve_question(group, \"variable\", df_questions_text)\n\n# Creates another column only with the answers, the second element after \\n (break line)\ngroup['Occupation2'] = group['Occupation'].apply(lambda x : x.rsplit('\\n')[1])\ngroup['Occupation2']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Retrives equivalent text of the numerical job satisfaction\nmap_job_sat = {\n    4: 'Very satisfied',\n    3: 'Slightly satisfied',\n    2: 'Neither satisfied nor dissatisfied',\n    1: 'Slightly dissatisfied',\n    0: 'Very dissatisfied'\n}\ngroup['Job Satisfaction'] = group['JobSat'].apply(lambda x : map_job_sat[x])\ngroup.value.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"map_job_sat.values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Plot the relative frequencies\ngroup = group.sort_values(by=['JobSat', 'value'], ascending=False)\nsns.set(font_scale=2.3)\nf, ax = plt.subplots(figsize=(10, 40))\nsns.set_color_codes(\"pastel\") # dark deep\n\nax = sns.barplot(x='value', y='Occupation2', data=group, hue='Job Satisfaction', hue_order=map_job_sat.values(), palette=\"viridis_r\")#, color=colors)#, color=\"blue\"\n\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nax.xaxis.set_minor_locator(plt.MultipleLocator(5))\nax.grid(b=True, axis='x', which='major', color='#FFFFFF', linewidth=1.8)\nax.grid(b=True, axis='x', which='minor', color='#FFFFFF', linewidth=1.3, linestyle='--')# '-', '--', '-.'\n\n\nax.set_xlabel('Relative Frequency (%)')\nax.set_ylabel('Occupation')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Methods the most satisfied professionals use to learn (non-degree)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the names of the dummy variables of EduOther\nfeats_edu_job = get_group_dummies(df_dense, 'EduOther')\n\n# Add job satisfaction\nfeats_edu_job.append('JobSat')\ndf_dense_corr = df_dense[feats_edu_job]\n\n# Group the education by job satisfactoin\ngroup = df_dense_corr.groupby(['JobSat']).sum()/df_dense_corr.sum().drop('JobSat')*100\ngroup = group.reset_index()\ngroup = group.melt(id_vars = 'JobSat', var_name=\"variable\")\n\n# Retrive the text of the questions\ndf_questions_text = reload_questions()\ngroup['Education'] = retrieve_question(group, \"variable\", df_questions_text)\n\n# Define another columns with the answer\ngroup['Education_answer'] = group['Education'].apply(lambda x : x.rsplit('\\n')[1])\ngroup['Education_answer']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Retrieves the equivalent text of the numerical job satisfaction\nmap_job_sat = {\n    4: 'Very satisfied',\n    3: 'Slightly satisfied',\n    2: 'Neither satisfied nor dissatisfied',\n    1: 'Slightly dissatisfied',\n    0: 'Very dissatisfied'\n}\ngroup['Job Satisfaction'] = group['JobSat'].apply(lambda x : map_job_sat[x])\ngroup","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group[group[\"Education_answer\"] == 'Participated in a hackathon'].sum()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Plot the relative frequency of education by job satisfaction\ngroup = group.sort_values(by=['JobSat', 'value'], ascending=False)\nsns.set(font_scale=2.3)\nf, ax = plt.subplots(figsize=(10, 40))\nsns.set_color_codes(\"pastel\") # dark deep\n\n#colors = plt.cm.GnBu_r(5) #RdYlGn\nax = sns.barplot(x='value', y='Job Satisfaction', data=group, hue='Education_answer', palette=\"tab10\")#, color=colors)#, color=\"blue\"\n#ax.legend(ncol=1, loc=2, frameon=True, )#loc=\"lower right\"\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n#ax.grid(True, which='both', axis='x')\n\nax.xaxis.set_minor_locator(plt.MultipleLocator(5))\nax.grid(b=True, axis='x', which='major', color='#FFFFFF', linewidth=2.5)\nax.grid(b=True, axis='x', which='minor', color='#FFFFFF', linewidth=1.3, linestyle='--')# '-', '--', '-.'\n\n\nax.set_xlabel('Learning Method Relative Frequency (%)')\nax.set_ylabel('Job Satisfaction')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"}},"nbformat":4,"nbformat_minor":1}