{"cells":[{"metadata":{},"cell_type":"markdown","source":" # **Feature Selection, SVM with parameter tuning and proper use of the time variable**"},{"metadata":{},"cell_type":"markdown","source":"**Abstract:** In this exercise we will predict the heart failure of a patient. We will demonstrate the importance of understanding the features and use a Support Vector Machine (SVM) with parameter tuning to build a model. Most importantly we will construct a new feature from the *time* variable. This feature is strongly correlated with the response variable and in contrast to the original *time* variable it can be used properly as a feature."},{"metadata":{},"cell_type":"markdown","source":"# **1.) Introduction**\n\nI understand this exercise as \"Design a tool for the medical market\". The tool should help a doctor who has collected the required data of a patient that was diagnosed  with CVD to make proper decisions about further special treatment (e.g. medical surgery, medicine).\nOur aim is to use the data to train a machine learning model for classification. The response/target variable should be the *DEATH_EVENT*. \n\nDepending on the treatment a doctor might prefer a classifier with high recall or high presicion. Let's assume for a second that the treatment has a low risk for the patient but might indeed prevent death. In such a case false positives are more acceptable than false negatives and we might prefer a classifier with high recall over one with high precision. If the treatment has however a high risk to kill a patient who would have survivied without treatment, false negatives would be more acceptable and we might desire a model with higher precision. Hence the AUC score seems to be a good metric for measuring the performance of the classifier. \n\nNote that features have a medical and/or biophysical interpretation. Knowledge about the meaning and significance of the feature would be highly valuable.\n\nI should mention before hand that I view this notebook as an exercise and that a full fledged solution requires significantly more effort. Thoughts on this can be found in the conclusion section.\n\nLet's go."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import Normalize\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report, plot_roc_curve, accuracy_score, f1_score,roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The file is located at"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **2.) Preprocessing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv\")\n\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data is already cleaned. There are only some binary features that we might prefer to be booleans."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df[['sex','smoking','diabetes','high_blood_pressure','anaemia','DEATH_EVENT']].head(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's convert them into booleans."},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in ['sex','smoking','diabetes','high_blood_pressure','anaemia','DEATH_EVENT']:\n        df[feature]=df[feature].astype(bool)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **3.) Exploratory Data Analysis and Feature Selection**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us first count the number of death events."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df[\"DEATH_EVENT\"].describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 203 (68%) patients that survived and 96 (32%) patients that died. Hence, the data set is skewed."},{"metadata":{},"cell_type":"markdown","source":"Next I would like to remark that some features are equipped with units. Even if we do not have a medical interpretation at hand this information provides us at least with some physical information. We might for example try to construct new physically reasonable features via dimensional analysis. We don't care about the precise units at the moment since we will rescale the features anyways. A short explenation of the features (from a quick google search) and dimensions are\n\n**The features:**\n\n[age] = $T$\n\n**Creatine Phosphokinase** (a.k.a., creatine kinase, CPK, or CK) is an enzyme (a protein that helps to elicit chemical changes in your body) found in your heart, brain, and skeletal muscles. When muscle tissue is damaged, CPK leaks into your blood.\n\n[creatinine_phosphokinase] = $M L^{-3}$\n\n**Ejection fraction**, for example, refers to how well the left heart ventricle (or right ventricle) pumps blood with each heart beat. \n\n[ejection_fraction] = dimensionless\n\n\n**Platelets** form a platelet plug to stop bleeding from an injured blood vessel. In cardiovascular disease, abnormal clotting occurs that can result in heart attacks or stroke. Blood vessels injured by smoking, cholesterol, or high blood pressure develop cholesterol-rich build-ups (plaques) that line the blood vessel; these plaques can rupture and cause the platelets to form a clot.\n\n[platelets] = $L^{-3}$\n\nThe **serum creatinine** variable is a measure for kidney health. \nFrom this [paper](https://www.ahajournals.org/doi/full/10.1161/01.str.28.3.557):\n\"Stroke risk was significantly increased at levels above 116 Î¼mol/L (90th percentile) even after adjustment for a wide range of cardiovascular risk factors.\"\n\n[serum_creatinine] = $ML^{-3}$\n\n\n**Serum Sodium** variable is measure for sorium content in blood. From this paper [paper](https://pubmed.ncbi.nlm.nih.gov/29935992/):\n\"In the multivariate model, low-level serum sodium was associated with an increased risk of cardiovascular mortality (hazard ratio [HR], 1.10; 95% confidence interval [CI], 1.02-1.18 per standard deviation [SD]; P = 0.009), whereas a lower level of serum chloride was not (HR, 1.04; 95% CI, 0.97-1.12 per standard deviation; P = 0.278). Analyses with restrictive cubic splines yielded similar results.\"\n\n[serum_sodium] = $L^{-3}mol$\n\n\n\nMore on **time** later\n\n[time] = $T$ (surprise!)\n\n\n\nwhere $T,L,$ $M$ and $mol$ are time, length, mass and mole respectively.\n\nFrom Wikipedia: **Anemia** (also spelled anaemia) is a decrease in the total amount of red blood cells (RBCs) or hemoglobin in the blood,[3][4] or a lowered ability of the blood to carry oxygen.\nHere is a [medical paper](https://pubmed.ncbi.nlm.nih.gov/14531771/).\n\nThe remaining features should be clear.\n\n\n\nThe first 11 variables in [14] or combinations thereof can be used as feature variables. The variable *DEATH_EVENT* will be the response/target variable. The *time* variable requires deeper discussion and analysis.\n\n\n\n\n**The time variable**:\n\nThis variable appears to be the time which the patient already sees the doctor for treatment. \n\nIn the discussion section it was argued that time is a response/target variable and that it should not be used as a feature variable. I would say that this is indeed up to interpretation. One might for example construct a new binary feature from time. Let's say, by asking if the treatment of the patient took already longer than time $x$. The constructed variable is then a feature and might help a doctor decide if additional treatment is required. If such new feature is helpful for our training should will be analysed now."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.loc[df['DEATH_EVENT']==0]['time'].mean())\nprint(df.loc[df['DEATH_EVENT']==0]['time'].median())\nprint(df.loc[df['DEATH_EVENT']==1]['time'].mean()) \nprint(df.loc[df['DEATH_EVENT']==1]['time'].median()) \nprint(df.loc[df['DEATH_EVENT']==1]['time'].max())\nprint(df.loc[df['DEATH_EVENT']==1]['time'].max())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The surviving patients are an average of 158 days in treatment while those who died left this world already at around 71 days on average (we also computed the median because it's less sensitive to outliers). We can conclude that patients who already managed to keep alive for a certian time during treatment have a higher chance of survival. It is therefore reasonable to construct a new feature. For that purpose we use the intuitive idea to  search for a threshold which maximizes the absolute value of the linear correlation between the new variable and the variable *DEATH_EVENT*.\nThe new feature will be called *time_new*"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_correlation = 0\nfor threshold in np.arange(20,240,1):\n    df['time_new'] = (df['time'] >= threshold)\n    correlation = df[['time_new','DEATH_EVENT']].corr().to_numpy()[0,1]\n    if np.abs(correlation) >  np.abs(best_correlation):\n            best_correlation = df[['time_new','DEATH_EVENT']].corr().to_numpy()[0,1]\n            best_threshold = threshold\n                \nprint(\"Best threshold = \" + str(best_threshold) + ' days' )\nprint(\"linear correlation = \" + str(best_correlation) )\n\ndf['time_new'] = (df['time']>= best_threshold)\ndf = df.drop(['time'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The magic number is 74 days. The new feature is useful for a doctor and in addition it is strongly correlated with the response variable."},{"metadata":{},"cell_type":"markdown","source":"**Correlations:**\n\nLet us now turn to selecting the other features. For that purpose we plot the linear correlations of all features with the response DEATH_EVENT."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.corr()['DEATH_EVENT'])\nplt.barh(np.arange(len(df.corr()['DEATH_EVENT'])), df.corr()['DEATH_EVENT'],align = 'center',tick_label = df.columns)\nplt.xlim((-1,1))\nplt.grid(axis='x')\nplt.title('Pearson Correlation of features with response')\nplt.show()\n\nplt.clf()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The features that seem to provide the best potential to train a model are those that have highest linear correlation with response variable. We choose the following"},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_features = ['time_new','age','ejection_fraction','serum_creatinine','serum_sodium']\nprint(selected_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that throwing away features is potentially dangerous. Even if two features $x_i$ and $x_j$ are not strongly linearly correlated to the response $y$ there could exist some combination $f(x_i,x_j)$ that has a strong correlation with $y$. I am missing the medical knowledge for the construction of such features. \n\nNote that the Point-biserial correlation is a better measure for the correlation between a continous and a binary variable.\n\nThis is why I decide here to put some more effort into the feature selection."},{"metadata":{},"cell_type":"markdown","source":"**Random Forest for evaluation of feature importance:**\nWe can also use a Random Forest Classifier to evaluate the relevance of the features. Quick and dirty:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['DEATH_EVENT'],axis=1)\ny = df['DEATH_EVENT']\n\n\n\nsc = StandardScaler()\nX = sc.fit_transform(X)\n\n\nrfc = RandomForestClassifier(n_estimators=1000, random_state=0)\n\nrfc.fit(X, y)\n\n\nimportances = rfc.feature_importances_\n\nprint(df.drop('DEATH_EVENT', axis=1).columns)\n\nplt.barh(np.arange(len(df.drop('DEATH_EVENT', axis=1).columns)), importances,align = 'center',tick_label = df.drop('DEATH_EVENT', axis=1).columns)\nplt.xlim((0,0.5))\nplt.grid(axis='x')\nplt.title('Random Forest importances')\nplt.show()\n\nplt.clf()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above analysis provides a slightly different picture than our analysis of the correlation. The Random Forest assigns more importance to the features *creatine_phosphokinase* and *platelets*. I have two possible explenations:\n1. We used the wrong measure of correlation between continous and binary features \n2. A combination of several features including the above mentioned ones might be higher correlated with the response. This sounds reasonable since decision trees in the random forest might explicitely take such combinations into account."},{"metadata":{},"cell_type":"markdown","source":"I have trust in both the spirit of the forest and the medical papers we cited above and add the features to the feature list."},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_features.append('creatinine_phosphokinase')\nselected_features.append('platelets')\nprint(selected_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***smoking*, *high_blood_pressure*,*anaemia* and *sex***:\n\nThe remaining features *smoking*, *high_blood_pressure* and *anaemia* are known to increase the overall risk to be diagnosed with CVD. This does, however, not mean that these features necessarily increase the chance of a patient that has already been diagnosed to die.\n\nThere is also the *sex* feature that is not highly correlated with the *DEATH_EVENT*.\n\nThere might, however, be combinations of these features that have a higher correlation with the *DEATH_EVENT*. For example: What if a patient is a smoker that has high blood pressure at the same time? One should indeed check all such combinations. Note that two binary features $x_i$ and $x_j$ can be combined in several ways, we might, for example, choose the combination $x_i \\land x_j$ or $\\neg(x_i) \\land x_j$, where $\\neg$ is the negation operator.\n\nI am too busy at the moment to write an algorithm that checks all such combinations and I will only check a few that appear reasonable to me."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['new_feature']=df['smoking']&df['high_blood_pressure']\ncorrelation = df[['new_feature','DEATH_EVENT']].corr().to_numpy()[0,1]\nprint('smoking and high_blood_pressure correlation='+str(correlation))\n\ndf['new_feature']=df['anaemia']&df['sex']\ncorrelation = df[['new_feature','DEATH_EVENT']].corr().to_numpy()[0,1]\nprint('anaemia and male correlation='+str(correlation))\n\ndf['new_feature']=df['smoking']&df['sex']\ncorrelation = df[['new_feature','DEATH_EVENT']].corr().to_numpy()[0,1]\nprint('smoking and male='+str(correlation))\n\ndf['new_feature']=df['smoking']&(~df['sex'])\ncorrelation = df[['new_feature','DEATH_EVENT']].corr().to_numpy()[0,1]\nprint('smoking and female='+str(correlation))\n\ndf['new_feature']=df['high_blood_pressure']&df['anaemia']\ncorrelation = df[['new_feature','DEATH_EVENT']].corr().to_numpy()[0,1]\nprint('high_blood_pressure and anaemia correlation='+str(correlation))\n\ndf['new_feature']=df['smoking']&df['high_blood_pressure']&df['sex']\ncorrelation = df[['new_feature','DEATH_EVENT']].corr().to_numpy()[0,1]\nprint('smoking, high_blood_pressure and male correlation='+str(correlation))\n\ndf['new_feature']=df['smoking']&df['high_blood_pressure']&df['anaemia']\ncorrelation = df[['new_feature','DEATH_EVENT']].corr().to_numpy()[0,1]\nprint('smoking, high_blood_pressure and anaemia correlation='+str(correlation))\n\ndf['new_feature']=df['smoking']&df['high_blood_pressure']&df['anaemia']&df['sex']\ncorrelation = df[['new_feature','DEATH_EVENT']].corr().to_numpy()[0,1]\nprint('smoking, high_blood_pressure, anaemia and male correlation='+str(correlation))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The quick search by hand did not reveal anything too interesting. We therefore decide not to add any new features constructed from the features 'smoking, high_blood_pressure','sex' and 'anaemia'.\n\nWe might also check combinations of the features *smoking, high_blood_pressure,anaemia* and *sex* with those we already have in the selected feature list. We leave this task for future investigations. "},{"metadata":{},"cell_type":"markdown","source":"**Final plot of the correlation matrix:**\n\nLet us plot the correlation matrix for the features selected so far. The linear correlations among the selected features are not too strong. Thus there seems to be no reason to drop any of them. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.heatmap(df[selected_features].corr(), vmin=-1, vmax=1, cmap='seismic', annot=True)\nplt.title('Correlation matrix')\nplt.yticks(rotation=0)\nplt.show()\nplt.clf()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Some scatter plots**:\n\nWe provide some scatter plots to visualize the problem."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2d scatter plot\n\n\nplt.scatter(df.loc[df['DEATH_EVENT'] == 0]['age'],df.loc[df['DEATH_EVENT'] == 0]['ejection_fraction'], color='blue')\nplt.scatter(df.loc[df['DEATH_EVENT'] == 1]['age'],df.loc[df['DEATH_EVENT'] == 1]['ejection_fraction'], color='red')\nplt.legend(['DEATH_EVENT=1','DEATH_EVENT=0'])\nplt.xlabel('age')\nplt.ylabel('ejection_fraction')\nplt.show()\nplt.clf()\n\n# 3d scatter plot\n\nfig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, projection='3d')\n\nfor farbe,b in [('blue',0),('red',1)]:\n    xs = df.loc[df['DEATH_EVENT'] == b]['serum_creatinine']\n    ys = df.loc[df['DEATH_EVENT'] == b]['ejection_fraction']\n    zs = df.loc[df['DEATH_EVENT'] == b]['age']\n    ax.scatter(xs, ys, zs, color = farbe)\n\nax.set_xlabel('serum_creatinine')\nax.set_ylabel('ejection_fraction')\nax.set_zlabel('age')\n\nplt.show()\nplt.clf()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# # **4.) Parameter tuned SVM with Gaussian kernel**\nThe number of features and the size of the data set suggests to use a Support Vector Machine (SVM) with a Gaussian Kernel. The performance of the model can be controlled by the inverse regularization coefficient $C$ and the width of the Gaussian kernel which is regulated through the paramter $\\gamma$.\n\nSince the data is skewed and optimization of AUC seems to be numerically too expensive we will try to optimize the model for the F1 score. \nOur strategy: \n1. Get insights of the performance on the parameter space spanned by $C$ and $\\gamma$ by plotting the F1 score over a large but rough grid. For that we will use *sklearn.model_selection.GridSearchCV*.\n2. Use these insights to select a smaller portion of the parameter space and perform a finer search by using *sklearn.model_selection.RandomizedSearchCV*"},{"metadata":{},"cell_type":"markdown","source":"**4. a) Train-test split**\n\nWe choose the features selected in the third chapter and perform a random train test split. The method is not scale invariant therfore we also perform feature scaling by using *sc.fit_transform*."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df[selected_features]\ny = df['DEATH_EVENT']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 6)\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**4.b) Plot the grid** "},{"metadata":{},"cell_type":"markdown","source":"I found some of the code [here](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html#sphx-glr-auto-examples-svm-plot-rbf-parameters-py)."},{"metadata":{"trusted":true},"cell_type":"code","source":"C_range = np.logspace(-2,3,11)\ngamma_range = np.logspace(-4,1,11)\n\n\nparam_grid = dict(C=C_range,gamma=gamma_range)\n\n# initialize the grid\nsvc = SVC(kernel='rbf')\ngrid = GridSearchCV(svc, param_grid, cv=10, scoring = 'f1')\n\n\n\n#fit grid to training data\ngrid.fit(X_train,y_train)\n\ncv_results_df = pd.DataFrame.from_dict(grid.cv_results_)\n\n# ###########################################################################\n# Plot heatmap of F1 score\n# ###########################################################################\n# Utility function to move the midpoint of a colormap to be around\n# the values of interest.\n# ###########################################################################\nclass MidpointNormalize(Normalize):\n\n    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n        self.midpoint = midpoint\n        Normalize.__init__(self, vmin, vmax, clip)\n\n    def __call__(self, value, clip=None):\n        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n        return np.ma.masked_array(np.interp(value, x, y))\n# ############################################################################\nscores = grid.cv_results_['mean_test_score'].reshape(len(C_range),\n                                                     len(gamma_range))\n# ############################################################################\n# The parameters vmin and midpoint control the colorbar of the heatmap\nplt.figure(figsize=(8, 6))\nplt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\nplt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot,\n           norm=MidpointNormalize(vmin=0.2, midpoint=0.6))\nplt.xlabel('gamma')\nplt.ylabel('C')\nplt.colorbar()\n\nC_range_aux = np.logspace(-2,3,6)\ngamma_range_aux = np.logspace(-4,1,6)\nplt.xticks(np.arange(0,len(gamma_range),2), gamma_range_aux, rotation=45)\nplt.yticks(np.arange(0,len(C_range),2), C_range_aux)\nplt.title('Cross-validation F1 score')\nplt.show()\nplt.clf()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is a typical figure for the cross-validation F1 score."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Best F1 score = ' + str(grid.best_score_)+ 'at')\nprint(grid.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets see if we can do better than that."},{"metadata":{},"cell_type":"markdown","source":"**4.c) Random search on a smaller but finer grid**"},{"metadata":{"trusted":true},"cell_type":"code","source":"C_range = np.logspace(-1,4,200)\ngamma_range = np.logspace(-4, 0,200)\n\nparam_dist = dict(C=C_range,gamma=gamma_range)\n\nrand = RandomizedSearchCV(svc, param_dist, cv=10, scoring = 'f1',n_iter = 150 , random_state=42)\nrand.fit(X_train,y_train)\n\nprint('Best F1 score = ' + str(rand.best_score_) + 'at')\nprint(rand.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. # **5.) Model evaluation**\n\nWe evaluate the confusion matrix on the test set. "},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_svc = rand.predict(X_test)\nprint(confusion_matrix(y_test, pred_svc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The classification report:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, pred_svc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And the ROC curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve(rand, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Sanity Check**:\n\nSomething like this should be a standard on Kaggle in my opinion. \nIt proves that I did not fine tune the random state variables to optimize the scores."},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_best = SVC(kernel='rbf', C=0.4500557675700499 , gamma=0.017027691722258997)\n\nf1_scores = []\naccuracy_scores = []\nroc_auc_scores = []\n\nfor i in range(1, 200):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = i)\n    sc = StandardScaler()\n    X_train = sc.fit_transform(X_train)\n    X_test = sc.fit_transform(X_test)\n    svc_best.fit(X_train,y_train)\n    pred_svc = svc_best.predict(X_test)\n    f1_scores.append(f1_score(y_test,pred_svc))\n    accuracy_scores.append(accuracy_score(y_test,pred_svc))\n    roc_auc_scores.append(roc_auc_score(y_test,pred_svc))\n    \n    \nplt.hist(f1_scores, bins=15)\nplt.title('Distribution of F1 scores')\nplt.show()\nplt.clf()\n\n\nplt.hist(roc_auc_scores, bins=15)\nplt.title('Distribution of AUC scores')\nplt.show()\nplt.clf()\n\nplt.hist(accuracy_scores, bins=15)\nplt.title('Distribution of accuracy_scores')\nplt.show()\nplt.clf()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **6.) Diagnosis of SVM model**\n\nWe plot the relative error on the training and test set for an increasing number of data points. We might be able to see a general trend in the resulting diagram."},{"metadata":{"trusted":true},"cell_type":"code","source":"# randomly shuffle rows\ndf.sample(frac=1) \nX = df[selected_features].astype('float64').to_numpy()\ny = df['DEATH_EVENT'].astype('float64').to_numpy()\n\nerror_train = []\nerror_test = []\n\nI = range(50, 298)\n\nfor i in I:\n    X_train, X_test, y_train, y_test = train_test_split(X[0:i,:], y[0:i], test_size = 0.2, random_state = 42)\n    X_train = sc.fit_transform(X_train)\n    X_test = sc.fit_transform(X_test)\n    svc_best.fit(X_train,y_train)\n    pred_svc_train = svc_best.predict(X_train)\n    pred_svc_test  = svc_best.predict(X_test)\n    error_train.append(np.linalg.norm(pred_svc_train-y_train)/(np.shape(X_train)[0]))\n    error_test.append(np.linalg.norm(pred_svc_test-y_test)/(np.shape(X_test)[0]))\n\n    \nplt.plot(I,error_train, color='blue')\nplt.plot(I,error_test,color='red')\nplt.ylabel('Errors')\nplt.xlabel('Number of data points')\nplt.legend(['Error on training set','Error on test set'])\nplt.show()\nplt.clf()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **7.) Conclusion**\n\nThe AUC score looks actually quite good. I am happy with the result.\n\nWhat would help to improve the performance? Should we work more on the features or collect more data? The diagnosis suggests that we should work on the features instead of collecting more data.\nAdding new features such as height and weight might definetly be worth a try.\nI believe, however, that a true understanding, and hence, a better solution to the problem can be achived by acquiring deeper insights into the meaning and medical significance of the features. \nA quick google search and some skimming on wikipedia, however, reveals a rabbit hole which I don't want to enter at the moment. A proper solution, which might only be obtained after digging into the medical and biophysical processes, is beyond the scope of this simple exercise. \nIn other words: If I planned to put more effort into this exercise I would probably work on improving the features. If this were a real situation I would maybe look for consultation by a medical expert on CVD who knows the statistics of the problem.\n\nI also liked [Furkan Gulsen's idea](https://www.kaggle.com/codeblogger/step-by-step-support-vector-machine-svm). He uses SMOTE to generate new observations to \"un-skew\" the data.\n\nI would also like to mention that a medical doctor might prefer a model that outputs probabilities. The SVM is not the perfect choice if this was expected. After all we might prefer ensemble learning to build the ultimate classifier taylored to the problem at hand.\n\n\nThanks for taking the time to read through my first Kaggle notebook. I learned a valuable lesson from this exercise: The importance of understanding the problem and the features. Machine Learning is just a tool. Understanding the problem, on the other hand, might be the key to achieve a proper solution.\n\n\nConstructive feedback of any kind is highly appreciated. The problem in this notebook was certainly quite interesting and I might come back to it in the future."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}