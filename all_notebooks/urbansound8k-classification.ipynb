{"cells":[{"metadata":{},"cell_type":"markdown","source":"## URBANSOUND8K DATASET\n\n* This dataset contains 8732 labeled sound excerpts (<=4s) of urban sounds from 10 classes: air_conditioner, car_horn, children_playing, dog_bark, drilling, enginge_idling, gun_shot, jackhammer, siren, and street_music\n\n* The files are pre-sorted into ten folds (folders named fold1-fold10) to help in the reproduction of and comparison with the automatic classification results reported in the article above.\n\n* In addition to the sound excerpts, a CSV file containing metadata about each excerpt is also provided.\n\n* 8732 audio files of urban sounds (see description above) in WAV format. The sampling rate, bit depth, and number of channels are the same as those of the original file uploaded to Freesound (and hence may vary from file to file)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Lets look at the data\ndata = pd.read_csv(\"../input/urbansound8k/UrbanSound8K.csv\")\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The meta-data contains 8 columns.\n\n1. slice_file_name: name of the audio file\n1. fsID: FreesoundID of the recording where the excerpt is taken from\n1. start: start time of the slice\n1. end: end time of the slice\n1. salience: salience rating of the sound. 1 = foreground, 2 = background\n1. fold: The fold number (1–10) to which this file has been allocated\n1. classID:\n0 = air_conditioner\n1 = car_horn\n2 = children_playing\n3 = dog_bark\n4 = drilling\n5 = engine_idling\n6 = gun_shot\n7 = jackhammer\n8 = siren\n9 = street_music\n1. class: class name"},{"metadata":{},"cell_type":"markdown","source":"### The audio data has been already sliced and excerpted and even allocated to 10 different folds. Some of the excerpts are from the same original file but different slice. If one slice from a certain recording was in training data, and a different slice from the same recording was in test data, this might increase the accuracy of a final model falsely. Thanks to the original research, this has also been taken care of by allocating slices into folds such that all slices originating from the same Freesound recording go into the same fold."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets look at class distribution of each fold\nappended = []\nfor i in range(1,11):\n    appended.append(data[data.fold == i]['class'].value_counts())\n    \nclass_distribution = pd.DataFrame(appended)\nclass_distribution = class_distribution.reset_index()\nclass_distribution['index'] = [\"fold\"+str(x) for x in range(1,11)]\nclass_distribution","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dataset is not perfectly balanced\ndata['class'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see that Car Horn and Gun Shot class is unbalanced, for now we are not going to do any augmentation"},{"metadata":{},"cell_type":"markdown","source":"# We can use Librosa Python Library for extracting features\n\nLibrosa library can read audio files and convert them to there amplitude values for each sample of audio. \n\nLet us say there is an audio file of 4s and sampling rate of audio file is 22050 Hz. This means that audio file is made using amplitude samples such that 22050 samples of amplitudes are recorded in each second. Hence a 4s audio file with sampling rate 22050 can be expressed as an array of 4*22050=88200 size\n"},{"metadata":{},"cell_type":"markdown","source":"## Model 1: With MFCC features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pip._internal import main\nmain([\"install\",\"progressbar\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import python script to import audio files\n\nfrom shutil import copyfile\ncopyfile(src = \"../input/urbansound8k-import-data/import_data.py\", dst = \"../working/import_data.py\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing required Library\n\nimport progressbar\nimport time\nimport os\nimport struct\nimport matplotlib.pyplot as plt\nimport IPython.display as ipd\nimport pandas as pd\nimport numpy as np\nimport librosa # for sound processing.\nimport import_data as dc # a local module","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = np.zeros(shape = (data.shape[0],2),dtype = object)\ndataset.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extracting Feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nbar = progressbar.ProgressBar(maxval=data.shape[0], widgets=[progressbar.Bar('$', '||', '||'), ' ', progressbar.Percentage()])\nbar.start()\nfor i in range(data.shape[0]):\n    \n    fullpath, class_id = dc.path_class(data,data.slice_file_name[i])\n    try:\n        X, sample_rate = librosa.load(fullpath, res_type='kaiser_fast')\n        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0)\n    except Exception:\n        print(\"Error encountered while parsing file: \", file)\n        mfccs,class_id = None, None\n    feature = mfccs\n    label = class_id\n    dataset[i,0],dataset[i,1] = feature,label\n    \n    bar.update(i+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.save(\"dataset\",dataset,allow_pickle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l = np.load(\"dataset.npy\",allow_pickle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l[8730,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['class'][8730]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating MFCC based Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Pre-processing\n\ndata_mfcc = pd.DataFrame(np.load(\"dataset.npy\",allow_pickle= True))\ndata_mfcc.columns = ['feature', 'label']\ndata_mfcc['fold'] = data['fold']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_mfcc.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_mfcc[data_mfcc['fold'] != 5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom keras.utils import np_utils\nlb = LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Convolution2D, MaxPooling2D\nfrom keras.optimizers import Adam\nfrom keras.utils import np_utils\nfrom sklearn import metrics ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = np.array(data_mfcc.label.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = np.array(data_mfcc.label.tolist())\n\nfilter_size = 3\nlb = LabelEncoder()\ny = np_utils.to_categorical(lb.fit_transform(y))\n\nnum_labels = y.shape[1]\n\n# build model\nmodel = Sequential()\nmodel.add(Dense(512, input_shape=(40,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(256))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_labels))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted = []\nactual = []\nfor i in range(1,11):\n    validation_data = data_mfcc[data_mfcc['fold'] == i]\n    train_data = data_mfcc[data_mfcc['fold'] != i]\n    \n    X = np.array(train_data.feature.tolist())\n    y = np.array(train_data.label.tolist())\n    \n    x_val = np.array(validation_data.feature.tolist())\n    y_val = np.array(validation_data.label.tolist())\n    \n    y = np_utils.to_categorical(lb.fit_transform(y))\n    y_val = np_utils.to_categorical(lb.fit_transform(y_val))\n    \n    model.fit(X, y, batch_size=64, epochs=60, validation_data=(x_val, y_val))\n    pred = model.predict(x_val)\n    \n    predicted.append(pred)\n    actual.append(y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = []\nfor i in range(0,10):\n    predict_conv = np.argmax(predicted[i],axis=1)\n    actual_conv = np.argmax(actual[i],axis=1)\n    acc.append(accuracy_score(actual_conv,predict_conv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy for 10 fold cross validation\",np.mean(acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model 2 : Melspectrogram"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extracting feature from audio file\nbar = progressbar.ProgressBar(maxval=data.shape[0], widgets=[progressbar.Bar('$', '||', '||'), ' ', progressbar.Percentage()])\nbar.start()\nfor i in range(data.shape[0]):\n    \n    fullpath, class_id = dc.path_class(data,data.slice_file_name[i])\n    try:\n        X, sample_rate = librosa.load(fullpath, res_type='kaiser_fast')\n        mfccs = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T,axis=0)\n    except Exception:\n        print(\"Error encountered while parsing file: \", file)\n        mfccs,class_id = None, None\n    feature = mfccs\n    label = class_id\n    dataset[i,0],dataset[i,1] = feature,label\n    \n    bar.update(i+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.save(\"dataset_melspectrogram\",dataset,allow_pickle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Pre-processing\n\ndata_mal = pd.DataFrame(np.load(\"dataset_melspectrogram.npy\",allow_pickle= True))\ndata_mal.columns = ['feature', 'label']\ndata_mal['fold'] = data['fold']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_mal.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = np.array(data_mal.label.tolist())\n\nfilter_size = 3\nlb = LabelEncoder()\ny = np_utils.to_categorical(lb.fit_transform(y))\n\nnum_labels = y.shape[1]\n\n# build model\nmodel = Sequential()\nmodel.add(Dense(512, input_shape=(128,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(256))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_labels))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted = []\nactual = []\nfor i in range(1,11):\n    validation_data = data_mal[data_mal['fold'] == i]\n    train_data = data_mal[data_mal['fold'] != i]\n    \n    X = np.array(train_data.feature.tolist())\n    y = np.array(train_data.label.tolist())\n    \n    x_val = np.array(validation_data.feature.tolist())\n    y_val = np.array(validation_data.label.tolist())\n    \n    y = np_utils.to_categorical(lb.fit_transform(y))\n    y_val = np_utils.to_categorical(lb.fit_transform(y_val))\n    \n    \n    model.fit(X, y, batch_size=64, epochs=100, validation_data=(x_val, y_val))\n    pred = model.predict(x_val)\n    \n    predicted.append(pred)\n    actual.append(y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = []\nfor i in range(0,10):\n    predict_conv = np.argmax(predicted[i],axis=1)\n    actual_conv = np.argmax(actual[i],axis=1)\n    acc.append(accuracy_score(actual_conv,predict_conv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy for 10 fold cross validation\",np.mean(acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion:\n\n1. 10 fold cross validation as described in the original paper results in 96.7% accuracy with MFCC features an 80% accuracy with melspectrogram features.\n1. Due to short time we haven’t done hyper parameter tuning, we can do hyper-parameter tuning with reports like shown [Here](https://wandb.ai/buntyshah/XGBoost/reports/Chicago-Crimes-Datesets-XGBooster--VmlldzoxMzcxNzE?accessToken=xprku0os6i6np7ptv1knj9xhholi24b5u85qaqaq9rgm63mlnxqurb3ik0xqh7d4)\n1. Due to short time we haven’t looked into AUC or F1 score or Classification report parameter and improve the model\n1. We can do augmentation of input sound to be more accurate.\n1. Currently it uses a very basic DNN, we can improve the model to be more accurate.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}