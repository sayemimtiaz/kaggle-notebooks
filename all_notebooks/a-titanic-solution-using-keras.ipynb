{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Table of Contents\n1.  Load Data\n1.  Feature Engineering\n1.  Missing values\n1.  Preprocess\n1.  Processing\n1.  Conclusion \n1.  Sources ","metadata":{"id":"0B9oOmAqS8ZW"}},{"cell_type":"markdown","source":"# 1. Load and check data\nLoad libraries and data","metadata":{}},{"cell_type":"code","source":"!pip install plotnine","metadata":{"id":"dlcipAaBpCEE","outputId":"c2b55435-db4c-4b60-a2e9-003f7f7f5d9d","execution":{"iopub.status.busy":"2021-07-13T18:09:15.271044Z","iopub.execute_input":"2021-07-13T18:09:15.271488Z","iopub.status.idle":"2021-07-13T18:09:42.446392Z","shell.execute_reply.started":"2021-07-13T18:09:15.271452Z","shell.execute_reply":"2021-07-13T18:09:42.445426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom numpy import nan, asarray\nimport pandas as pd\nimport seaborn as sns\nimport itertools\nimport re\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom collections import Counter\n\nimport sklearn\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split #this is just to split training \nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy","metadata":{"id":"BF6Wl1psttqK","execution":{"iopub.status.busy":"2021-07-13T18:09:42.448794Z","iopub.execute_input":"2021-07-13T18:09:42.44928Z","iopub.status.idle":"2021-07-13T18:09:42.458127Z","shell.execute_reply.started":"2021-07-13T18:09:42.449226Z","shell.execute_reply":"2021-07-13T18:09:42.457157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/titanic/train.csv')\ntest = pd.read_csv('../input/titanic/test.csv')","metadata":{"id":"egYGqk2UxtR2","execution":{"iopub.status.busy":"2021-07-13T18:09:42.460515Z","iopub.execute_input":"2021-07-13T18:09:42.460844Z","iopub.status.idle":"2021-07-13T18:09:42.489973Z","shell.execute_reply.started":"2021-07-13T18:09:42.460813Z","shell.execute_reply":"2021-07-13T18:09:42.488898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"og_test = test\ntrain= pd.DataFrame(train) \ntrain.info()","metadata":{"id":"7I61NQcEzKJA","outputId":"c00ad3c2-07ea-443b-a738-2a02e0afe2e0","execution":{"iopub.status.busy":"2021-07-13T18:09:42.492977Z","iopub.execute_input":"2021-07-13T18:09:42.493346Z","iopub.status.idle":"2021-07-13T18:09:42.512059Z","shell.execute_reply.started":"2021-07-13T18:09:42.493314Z","shell.execute_reply":"2021-07-13T18:09:42.510875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Feature Engineering:\nManipulate data to re-organise table.\nTypes of maniplation and why:\n*  Name: name only has one substring of value and that would be title (Mr. Miss...) Remove the rest of the string or that column would be too sparce to use.\n*  Sibsp and Parch: these to columns offer information about how many siblings are with the guest and parents/children. This data can be maniplated in diffrent ways. for example would sibling survival be more likely if their parents or children were not there? But for my case, I have added the two columns together to show that people with smaller families are more likely to survive, ideally no family would be optimal.\n* I also dropped some columns mainly due to the modification of the table.\n","metadata":{"id":"Tnng7OdBT5lA"}},{"cell_type":"code","source":"#Adding title column \nname_array = train['Name']\ntitle_train = []\nfor name in name_array:\n  title = re.search(', (.+?). ', name).group(1)\n  title_train.append(title)\n\ntrain['Title'] = title_train\n\nname_ar = test['Name']\ntitle_test = []\nfor name in name_ar:\n  title = re.search(', (.+?). ', name).group(1)\n  title_test.append(title)\n\ntest['Title'] = title_test","metadata":{"id":"-9t4YYQ4mKwx","outputId":"bdc4bad1-39c3-4a3f-9949-8fe687781867","execution":{"iopub.status.busy":"2021-07-13T18:09:42.513836Z","iopub.execute_input":"2021-07-13T18:09:42.514212Z","iopub.status.idle":"2021-07-13T18:09:42.530198Z","shell.execute_reply.started":"2021-07-13T18:09:42.514178Z","shell.execute_reply":"2021-07-13T18:09:42.529232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Narrow down uncommon titles. \n\ntrain['Title'] = train['Title'].replace(['Dona', 'Lady', 'the Countess','Capt', 'Col', 'Don', \n                'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer'], value ='rare')\n\ntest['Title'] = test['Title'].replace(to_replace =['Dona', 'Lady', 'the Countess','Capt', 'Col', 'Don', \n                'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer'], value ='rare')\n\ntrain.loc[(train.Title == 'the Countess'),'Title']='rare'\ntrain.loc[(train.Title == 'Mlle'),'Title']='Miss'\ntrain.loc[(train.Title == 'Ms'),'Title']='Miss'\ntrain.loc[(train.Title == 'Mme'),'Title']='Mrs'\n\ntest.loc[(test.Title == 'Mlle'),'Title']='Miss'\ntest.loc[(test.Title == 'Ms'),'Title']='Miss'\ntest.loc[(test.Title == 'Mme'),'Title']='Mrs'\n","metadata":{"id":"42egT9kPh7qB","execution":{"iopub.status.busy":"2021-07-13T18:09:42.53139Z","iopub.execute_input":"2021-07-13T18:09:42.531732Z","iopub.status.idle":"2021-07-13T18:09:42.559354Z","shell.execute_reply.started":"2021-07-13T18:09:42.531704Z","shell.execute_reply":"2021-07-13T18:09:42.55829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Find unique values in title\nprint(\"Train titles: \\n \",train['Title'].unique())\nprint(\"Test titles: \\n\", test['Title'].unique())\n\n#Change them to int values for future processing\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"rare\": 5}\n\ntrain['Title'] = train['Title'].map(title_mapping)\ntrain['Title'] = train['Title'].fillna(0)\ntest['Title'] = test['Title'].map(title_mapping)\ntest['Title'] = test['Title'].fillna(0)\n\n","metadata":{"id":"jOM7bE5Ufr3h","outputId":"993eaf13-7f29-4bac-b0c6-43bac7452d1f","execution":{"iopub.status.busy":"2021-07-13T18:09:42.560722Z","iopub.execute_input":"2021-07-13T18:09:42.561007Z","iopub.status.idle":"2021-07-13T18:09:42.576638Z","shell.execute_reply.started":"2021-07-13T18:09:42.560979Z","shell.execute_reply":"2021-07-13T18:09:42.575947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.factorplot(\"Title\", col=\"Survived\", col_wrap=4,\n                    data=train[train.Survived.notnull()],\n                    kind=\"count\", height=10.5, aspect=1);","metadata":{"id":"xnnY_EkA_tfj","outputId":"9bca0978-99c7-441e-9abf-24e749229283","execution":{"iopub.status.busy":"2021-07-13T18:14:26.131466Z","iopub.execute_input":"2021-07-13T18:14:26.131995Z","iopub.status.idle":"2021-07-13T18:14:26.206757Z","shell.execute_reply.started":"2021-07-13T18:14:26.131909Z","shell.execute_reply":"2021-07-13T18:14:26.205357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.scatter(train, x=\"Fare\", y=\"Sex\", color=\"Survived\",\n                 title=\"Fare of people based on gender\"\n                )\nfig.show()","metadata":{"id":"wKGVeHj1cyXh","outputId":"08c1cfa4-fb2a-441b-e4f6-13f5d27fe0fb","execution":{"iopub.status.busy":"2021-07-13T18:09:43.088374Z","iopub.execute_input":"2021-07-13T18:09:43.088863Z","iopub.status.idle":"2021-07-13T18:09:43.173692Z","shell.execute_reply.started":"2021-07-13T18:09:43.088815Z","shell.execute_reply":"2021-07-13T18:09:43.17283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Family = Parch + SibSp\ntrain['Family'] = train['Parch'] + train['SibSp']\ntest['Family'] = test['Parch'] + test['SibSp']\n\n#Drop Tables\ntrain = train.drop(['Parch'], axis = 1)\ntrain = train.drop(['SibSp'], axis = 1)\ntest = test.drop(['Parch'], axis = 1)\ntest = test.drop(['SibSp'], axis = 1)\n\npd.DataFrame(train)","metadata":{"id":"_afVj3bCyoRr","outputId":"5a8bafce-91b7-46d0-fb3c-bde0382ea6c8","execution":{"iopub.status.busy":"2021-07-13T18:09:43.175447Z","iopub.execute_input":"2021-07-13T18:09:43.175768Z","iopub.status.idle":"2021-07-13T18:09:43.213069Z","shell.execute_reply.started":"2021-07-13T18:09:43.175737Z","shell.execute_reply":"2021-07-13T18:09:43.212451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#More tables to drop\ntrain = train.drop(['Ticket'], axis = 1)\ntrain = train.drop(['Cabin'], axis = 1)\ntrain = train.drop(['Name'], axis = 1)\ntrain = train.drop(['PassengerId'], axis = 1)\n\ntest = test.drop(['Ticket'], axis = 1)\ntest= test.drop(['Cabin'], axis = 1)\ntest = test.drop(['Name'], axis = 1)\ntest = test.drop(['PassengerId'], axis = 1)","metadata":{"id":"zN0lhcmozejT","execution":{"iopub.status.busy":"2021-07-13T18:09:43.214158Z","iopub.execute_input":"2021-07-13T18:09:43.214567Z","iopub.status.idle":"2021-07-13T18:09:43.226915Z","shell.execute_reply.started":"2021-07-13T18:09:43.214524Z","shell.execute_reply":"2021-07-13T18:09:43.225962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Missing values.\nAfter removing most of the columns with missing values there is only a handfull of columns still missing values. Most notibly Age.","metadata":{"id":"Nzu3_xryUdC6"}},{"cell_type":"markdown","source":"**Fast and cheap** = drop all missing data or single imputation\n\n**Good and cheap** = proper imputation or pattern submodel method (this is what I did)\n\n**Good and fast** = gather data in a complete manner\n\n\n-Start cleaning the data (names, age, embarked...)","metadata":{"id":"eFoi9u7kSnsj"}},{"cell_type":"code","source":"train.isnull().sum(axis = 0)","metadata":{"id":"DIrikvXjcAR3","outputId":"18b75094-7d6d-4631-92e7-195d75364d49","execution":{"iopub.status.busy":"2021-07-13T18:09:43.228268Z","iopub.execute_input":"2021-07-13T18:09:43.228876Z","iopub.status.idle":"2021-07-13T18:09:43.23917Z","shell.execute_reply.started":"2021-07-13T18:09:43.228832Z","shell.execute_reply":"2021-07-13T18:09:43.238299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['Embarked'] = train['Embarked'].replace(np.nan, 'C')\n\ntrain['Embarked'] = train['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntest['Embarked'] = test['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)","metadata":{"id":"evZIP742iI5F","execution":{"iopub.status.busy":"2021-07-13T18:09:43.24042Z","iopub.execute_input":"2021-07-13T18:09:43.241044Z","iopub.status.idle":"2021-07-13T18:09:43.255977Z","shell.execute_reply.started":"2021-07-13T18:09:43.241002Z","shell.execute_reply":"2021-07-13T18:09:43.255264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#There is one fare missing in test so I just put the median there\ntest['Fare'].fillna(test['Fare'].dropna().median(), inplace=True)","metadata":{"id":"LoSEOak3jdkJ","execution":{"iopub.status.busy":"2021-07-13T18:09:43.257222Z","iopub.execute_input":"2021-07-13T18:09:43.257804Z","iopub.status.idle":"2021-07-13T18:09:43.270125Z","shell.execute_reply.started":"2021-07-13T18:09:43.25776Z","shell.execute_reply":"2021-07-13T18:09:43.269254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n#Change female and male to 1 and 0 for processing\ntest['Sex'] = test['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\n#Make groups of ages instead of unique ages per person, I have tried both and this one\n#led to better accuracy.\nguess_ages = np.zeros((2,3))\nguess_ages\n\nfor i in range(0, 2):\n    for j in range(0, 3):\n      guess_df = test[(test['Sex'] == i) & (test['Pclass'] == j+1)]['Age'].dropna()\n      age_guess = int(guess_df.median())\n      guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5\n            \nfor i in range(0, 2):\n    for j in range(0, 3):\n        test.loc[ (test.Age.isnull()) & (test.Sex == i) & (test.Pclass == j+1),\\\n                 'Age'] = guess_ages[i,j]\n\ntest['Age'] = test['Age'].astype(int)\n\n#------train------\ntrain['Sex'] = train['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\nguess_ages = np.zeros((2,3))\nguess_ages\n\nfor i in range(0, 2):\n    for j in range(0, 3):\n      guess_df = train[(train['Sex'] == i) & (train['Pclass'] == j+1)]['Age'].dropna()\n      age_guess = int(guess_df.median())\n      guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5\n            \nfor i in range(0, 2):\n    for j in range(0, 3):\n        train.loc[ (train.Age.isnull()) & (train.Sex == i) & (train.Pclass == j+1),\\\n                 'Age'] = guess_ages[i,j]\n\ntrain['Age'] = train['Age'].astype(int)\n\ntrain.head()","metadata":{"id":"nxEiB6lhQjYA","outputId":"152cfd8a-7752-4dc7-83e0-45c4e1ea77c4","execution":{"iopub.status.busy":"2021-07-13T18:09:43.271504Z","iopub.execute_input":"2021-07-13T18:09:43.271912Z","iopub.status.idle":"2021-07-13T18:09:43.334907Z","shell.execute_reply.started":"2021-07-13T18:09:43.271868Z","shell.execute_reply":"2021-07-13T18:09:43.333863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['AgeBand'] = pd.cut(test['Age'], 5)\ntest.loc[ test['Age'] <= 16, 'Age'] = 0\ntest.loc[(test['Age'] > 16) & (test['Age'] <= 32), 'Age'] = 1\ntest.loc[(test['Age'] > 32) & (test['Age'] <= 48), 'Age'] = 2\ntest.loc[(test['Age'] > 48) & (test['Age'] <= 64), 'Age'] = 3\ntest.loc[ test['Age'] > 64, 'Age']\n\ntrain['AgeBand'] = pd.cut(train['Age'], 5)\ntrain[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)\n\ntrain.loc[ train['Age'] <= 16, 'Age'] = 0\ntrain.loc[(train['Age'] > 16) & (train['Age'] <= 32), 'Age'] = 1\ntrain.loc[(train['Age'] > 32) & (train['Age'] <= 48), 'Age'] = 2\ntrain.loc[(train['Age'] > 48) & (train['Age'] <= 64), 'Age'] = 3\ntrain.loc[ train['Age'] > 64, 'Age']","metadata":{"id":"uIF5k9MYaDfw","outputId":"8ce034e6-6037-45f7-df14-509616090a30","execution":{"iopub.status.busy":"2021-07-13T18:09:43.336025Z","iopub.execute_input":"2021-07-13T18:09:43.336301Z","iopub.status.idle":"2021-07-13T18:09:43.37508Z","shell.execute_reply.started":"2021-07-13T18:09:43.336274Z","shell.execute_reply":"2021-07-13T18:09:43.374141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test.drop(['AgeBand'], axis=1)\ntrain = train.drop(['AgeBand'], axis=1)\ntrain.head()","metadata":{"id":"IO7Zni8waL7N","outputId":"c932431b-9f8c-40af-badf-1711f751fac7","execution":{"iopub.status.busy":"2021-07-13T18:09:43.376346Z","iopub.execute_input":"2021-07-13T18:09:43.376646Z","iopub.status.idle":"2021-07-13T18:09:43.393188Z","shell.execute_reply.started":"2021-07-13T18:09:43.376617Z","shell.execute_reply":"2021-07-13T18:09:43.392211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Two large factors of survival was age and class, besides of course gender\n# Here I am putting more weight on this combination for the neraul net to factor.\ntrain['Age*Class'] = train.Age * train.Pclass\ntest['Age*Class'] = test.Age * test.Pclass\n\ntrain.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)","metadata":{"id":"PHzvijYqk5Li","outputId":"03dddaf9-94ab-4ac5-f4b9-dabb5a7516eb","execution":{"iopub.status.busy":"2021-07-13T18:09:43.394632Z","iopub.execute_input":"2021-07-13T18:09:43.394912Z","iopub.status.idle":"2021-07-13T18:09:43.41577Z","shell.execute_reply.started":"2021-07-13T18:09:43.394884Z","shell.execute_reply":"2021-07-13T18:09:43.414933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#no more empty values (yay).\ntrain.isnull().sum(axis = 0)","metadata":{"id":"VGjNTnR_g0-o","outputId":"1eb3411b-3960-4c23-c9a0-39c56a8392b3","execution":{"iopub.status.busy":"2021-07-13T18:09:43.416894Z","iopub.execute_input":"2021-07-13T18:09:43.417232Z","iopub.status.idle":"2021-07-13T18:09:43.432541Z","shell.execute_reply.started":"2021-07-13T18:09:43.417199Z","shell.execute_reply":"2021-07-13T18:09:43.431403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Preprocess.\n- When I was testing the accuracy I split the training set in to two, test and training. This is my final code so i left training alone. But if you were to split it, here would be ideal.\n- building the model. \n","metadata":{"id":"W9so8mPxoWKb"}},{"cell_type":"code","source":"#Break up the data in to test and training\nX_train = train.iloc[:,1:9]\ny_train= train.iloc[:,0]\nprint(test)","metadata":{"id":"BDt-Ika5Fg6Q","outputId":"23878cdc-3bfb-4247-9040-a3b6375a55f6","execution":{"iopub.status.busy":"2021-07-13T18:09:43.433649Z","iopub.execute_input":"2021-07-13T18:09:43.434139Z","iopub.status.idle":"2021-07-13T18:09:43.448029Z","shell.execute_reply.started":"2021-07-13T18:09:43.434104Z","shell.execute_reply":"2021-07-13T18:09:43.446781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = test\n# Feature Scaling\nsc = StandardScaler()\n\nX_train = np.array(sc.fit_transform(X_train))\nX_test = np.array(sc.transform(X_test))\n\ny_train= np.array(y_train)","metadata":{"id":"vN6eKrERGK96","outputId":"cb303620-944f-4e65-cb53-6b6b9083dc52","execution":{"iopub.status.busy":"2021-07-13T18:09:43.449477Z","iopub.execute_input":"2021-07-13T18:09:43.449871Z","iopub.status.idle":"2021-07-13T18:09:43.463724Z","shell.execute_reply.started":"2021-07-13T18:09:43.44983Z","shell.execute_reply":"2021-07-13T18:09:43.462695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Processing\n\nHere I used a sequential neural network because the problem was a classification type, the data is not complicated, and there is not alot of data. ","metadata":{"id":"kZrvzvnv-EGW"}},{"cell_type":"markdown","source":"\n\nRules I followed:\n *   If training is much better than the validation set, you are probably overfitting and you can use techniques like regularization.\n *   If training and validation are both low, you are probably underfitting and you can probably increase the capacity of your network and train more or longer.\n *   If there is an inflection point when training goes above the validation, you might be able to use early stopping.\n","metadata":{"id":"t7W2D8uZPgpb"}},{"cell_type":"code","source":"#I tried other types of activation layers (sigmoid, tanh, softplus)and got worse perfomance, \n#this was th most optimal number of layers and activation layers I tried.\nimport tensorflow as tf\nmodel = Sequential([\n                    Dense(units=16, input_shape=(8,), activation='relu'),\n                    #Dropout(0.5),#decreases because of drop out...\n                    Dense(units=32, activation='relu'),\n                    #Dropout(0.25),\n                    Dense(units=2, activation='softmax')\n                    ])\n","metadata":{"id":"8VLbXfd9HLpt","execution":{"iopub.status.busy":"2021-07-13T18:09:43.465112Z","iopub.execute_input":"2021-07-13T18:09:43.465534Z","iopub.status.idle":"2021-07-13T18:09:43.500976Z","shell.execute_reply.started":"2021-07-13T18:09:43.465502Z","shell.execute_reply":"2021-07-13T18:09:43.500237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The values here are base on trial and error.\nmodel.compile(optimizer=Adam(learning_rate=0.01), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\ntrained=model.fit(x=X_train, y= y_train,  batch_size=16, epochs=25, shuffle=True, verbose=2)","metadata":{"id":"bhP6Laua6XSa","outputId":"9e2add25-aeea-40f2-e517-d3ad71b8a5c8","execution":{"iopub.status.busy":"2021-07-13T18:09:43.502011Z","iopub.execute_input":"2021-07-13T18:09:43.502499Z","iopub.status.idle":"2021-07-13T18:09:45.185732Z","shell.execute_reply.started":"2021-07-13T18:09:43.502457Z","shell.execute_reply":"2021-07-13T18:09:45.18491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_acc = np.mean(trained.history['accuracy'])\nprint(\"\\n%s: %.2f%%\" % ('val_acc', val_acc*100))\ntest_shape = X_test.shape\nprint(test_shape)","metadata":{"id":"iNoX3iIuKvRE","outputId":"f14ffae0-aa8a-4b16-a4ce-db5155c2dabc","execution":{"iopub.status.busy":"2021-07-13T18:09:45.187021Z","iopub.execute_input":"2021-07-13T18:09:45.187296Z","iopub.status.idle":"2021-07-13T18:09:45.193145Z","shell.execute_reply.started":"2021-07-13T18:09:45.187268Z","shell.execute_reply":"2021-07-13T18:09:45.192009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = model.predict(x=X_test, batch_size=10, verbose=1)\nrounded_predictions =np.argmax(predictions, axis= -1)\nrounded_predictions=rounded_predictions.reshape((test_shape[0],1))\n\nsolution = pd.DataFrame(rounded_predictions)\nsolution.columns = ['Survived']\nid = pd.DataFrame(og_test['PassengerId'])\nid = id.join(solution)\nprint(id)\n\nid.to_csv(\"NN_sol_titanic1.csv\", index=False)","metadata":{"id":"qhGWRsgjII-s","outputId":"21ea207e-5de3-48eb-964b-ead03feef608","execution":{"iopub.status.busy":"2021-07-13T18:09:45.194572Z","iopub.execute_input":"2021-07-13T18:09:45.194851Z","iopub.status.idle":"2021-07-13T18:09:45.310457Z","shell.execute_reply.started":"2021-07-13T18:09:45.194824Z","shell.execute_reply":"2021-07-13T18:09:45.309496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Conclusion\nThis is my first attempt of neural networks. I am aware that other types of predictions might have worked better, such as random forests. My score was 0.756 accuracy which put me at top 85% on the leading board. Feel free to critique my work. I am always happy to learn something new.","metadata":{"id":"6tGwIxKKo84M"}},{"cell_type":"markdown","source":"# 7. Sources \nThese are all the sources I used to help me complete this project. I hope some of this maybe usefull to you.","metadata":{}},{"cell_type":"markdown","source":"Machine Learning A-Zâ„¢: Hands-On Python & R In Data Science \nhttps://www.udemy.com/course/machinelearning/\n\nKeras with TensorFlow Course - Python Deep Learning and Neural Networks for Beginners Tutorial\nhttps://www.youtube.com/watch?v=qFJeN9V1ZsI&t=131s\n\nA Comprehensive Guide to types of Neual Networks\nhttps://www.digitalvidya.com/blog/types-of-neural-networks/\n\nKeras\nhttps://keras.io/getting_started/\n\nDifference Between a Batch and an Epoch in a Neural Network\nhttps://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/\n\nDiscover Feature Engineering, How to Engineer Features and How to Get Good at It\nhttps://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/\n\nHow To Improve Deep Learning Performance\nhttps://machinelearningmastery.com/improve-deep-learning-performance/\n\nTitanic: Neural Network for Beginners\nhttps://www.kaggle.com/jamesleslie/titanic-neural-network-for-beginners\n\nTitanic: Machine Learning from Disaster\nhttps://www.kaggle.com/gokultalele/titanic-machine-learning-from-disaster","metadata":{}},{"cell_type":"markdown","source":"","metadata":{"id":"Ks5vnFa1QMBI"}}]}