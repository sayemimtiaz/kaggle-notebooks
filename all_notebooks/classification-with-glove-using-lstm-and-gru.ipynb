{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Loading the required libraries","metadata":{}},{"cell_type":"code","source":"#for data analysis and modeling\nimport tensorflow as tf\nfrom tensorflow.keras.layers import LSTM, GRU, Dense, Embedding, Dropout\nfrom tensorflow.keras.preprocessing import text, sequence \nfrom tensorflow.keras.models import Sequential\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n#for text cleaning\nimport string\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n#for visualization\nimport matplotlib.pyplot as plt\n","metadata":{"execution":{"iopub.status.busy":"2021-07-18T18:49:17.216242Z","iopub.execute_input":"2021-07-18T18:49:17.21657Z","iopub.status.idle":"2021-07-18T18:49:17.222734Z","shell.execute_reply.started":"2021-07-18T18:49:17.216524Z","shell.execute_reply":"2021-07-18T18:49:17.221756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading data and visualizing","metadata":{}},{"cell_type":"code","source":"true = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/True.csv')\ntrue.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-18T18:49:17.654278Z","iopub.execute_input":"2021-07-18T18:49:17.654617Z","iopub.status.idle":"2021-07-18T18:49:18.219036Z","shell.execute_reply.started":"2021-07-18T18:49:17.654585Z","shell.execute_reply":"2021-07-18T18:49:18.217931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/Fake.csv')\nfake.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-18T18:49:18.220827Z","iopub.execute_input":"2021-07-18T18:49:18.221237Z","iopub.status.idle":"2021-07-18T18:49:18.840456Z","shell.execute_reply.started":"2021-07-18T18:49:18.221185Z","shell.execute_reply":"2021-07-18T18:49:18.839569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Combine fake and true dataframes\nCreate a new column 'truth' showing whethere the news is fake or real. Then, concatenate two datasets into one dataframe. We can choose to use either 'title' or 'text' column or concatenated 'title+text' for training. But, for the sake of processing time, we'll only use 'title'.\n","metadata":{}},{"cell_type":"code","source":"true['truth'] = 1\nfake['truth'] = 0\ndf = pd.concat([true, fake], axis=0, ignore_index=True)\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-18T18:49:18.842178Z","iopub.execute_input":"2021-07-18T18:49:18.842585Z","iopub.status.idle":"2021-07-18T18:49:18.859275Z","shell.execute_reply.started":"2021-07-18T18:49:18.842526Z","shell.execute_reply":"2021-07-18T18:49:18.858351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text Cleaning\n\nWe need to clean the text first. If you start searching on the text cleaning domain, you realize there are many different techniques. But you may need just a few methods for the purpose of your NLP task. \n<br>\n<br>\nI looked up the following resources for the text cleaning that I used in this notebook: <br>\nhttps://machinelearningmastery.com/clean-text-machine-learning-python/ <br>\nhttps://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/<br>\n<br>\nHere are 5 steps that give decent text cleaning result for this task:<br>\n1.\t<b>Replace contractions</b>\n2.\t<b>Removing punctuation</b>\n3.\t<b>Splitting into words</b>\n4.\t<b>Removing stopwords</b>\n5.\t<b>Removing leftover punctuations</b>\n\n\nNormalizing by case is also common practice. But, since we are using keras tokenizer later, we can skip this step as tokenizer does this step by default. There are other preprocessing techniques of text like Stemming, and Lemmatization. However, in the realm of deep learning NLP they are not necessary anymore. \n","metadata":{}},{"cell_type":"code","source":"%%time\ndef clean_text(txt):\n    \"\"\"\"\"\n    cleans the input text in the following steps\n    1- replace contractions\n    2- removing punctuation\n    3- spliting into words\n    4- removing stopwords\n    5- removing leftover punctuations\n    \"\"\"\"\"\n    contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n    def _get_contractions(contraction_dict):\n        contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n        return contraction_dict, contraction_re\n\n    def replace_contractions(text):\n        contractions, contractions_re = _get_contractions(contraction_dict)\n        def replace(match):\n            return contractions[match.group(0)]\n        return contractions_re.sub(replace, text)\n\n    # replace contractions\n    txt = replace_contractions(txt)\n    \n    #remove punctuations\n    txt  = \"\".join([char for char in txt if char not in string.punctuation])\n    txt = re.sub('[0-9]+', '', txt)\n    \n    # split into words\n    words = word_tokenize(txt)\n    \n    # remove stopwords\n    stop_words = set(stopwords.words('english'))\n    words = [w for w in words if not w in stop_words]\n    \n    # removing leftover punctuations\n    words = [word for word in words if word.isalpha()]\n    \n    cleaned_text = ' '.join(words)\n    return cleaned_text\n    \ndf['data_cleaned'] = df['title'].apply(lambda txt: clean_text(txt))\n","metadata":{"execution":{"iopub.status.busy":"2021-07-18T18:49:19.993185Z","iopub.execute_input":"2021-07-18T18:49:19.993569Z","iopub.status.idle":"2021-07-18T18:49:39.255709Z","shell.execute_reply.started":"2021-07-18T18:49:19.993521Z","shell.execute_reply":"2021-07-18T18:49:39.254681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Preperation ( Train and Test)\n\nUse the usual train_test_split by sklearn to split the data. ","metadata":{}},{"cell_type":"code","source":"xtrain, xtest, ytrain, ytest = train_test_split(df['data_cleaned'], df['truth'], shuffle=True, test_size=0.2)\n# find the length of the largest sentence in training data\nmax_len = xtrain.apply(lambda x: len(x)).max()\nprint(f'Max number of words in a text in training data: {max_len}')","metadata":{"execution":{"iopub.status.busy":"2021-07-18T18:49:39.257508Z","iopub.execute_input":"2021-07-18T18:49:39.257881Z","iopub.status.idle":"2021-07-18T18:49:39.294948Z","shell.execute_reply.started":"2021-07-18T18:49:39.257844Z","shell.execute_reply":"2021-07-18T18:49:39.293808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenization\n\n \nThere are two distinct steps in tokenizing in this way:\n1. fit_on_texts: We'll fit the tokenizer on our training data to create the word indices\n2. texts_to_sequences: using the word index dictionary from step above, we take this step to transform both train and test data. \n\nHere, we set num_words to a limit such as 10000 words. num_words is a parameter that defines the maximum number of words to keep, based on the word frequency. Keras actually keeps (num_words-1) words. We can leave the num_words to 'None' and tokenizer will pick all the words in the vacabulary. \n\n## Padding and truncating input sequences","metadata":{}},{"cell_type":"code","source":"max_words = 10000\ntokenizer = text.Tokenizer(num_words = max_words)\n# create the vocabulary by fitting on x_train text\ntokenizer.fit_on_texts(xtrain)\n# generate the sequence of tokens\nxtrain_seq = tokenizer.texts_to_sequences(xtrain)\nxtest_seq = tokenizer.texts_to_sequences(xtest)\n\n# pad the sequences\nxtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\nxtest_pad = sequence.pad_sequences(xtest_seq, maxlen=max_len)\nword_index = tokenizer.word_index\n\nprint('Text Example:', xtrain[0])\nprint('Sequence of indices(before padding):', xtrain_seq[0])\nprint('Sequence of indices(after padding):', xtrain_pad[0])","metadata":{"execution":{"iopub.status.busy":"2021-07-18T18:49:48.251458Z","iopub.execute_input":"2021-07-18T18:49:48.251809Z","iopub.status.idle":"2021-07-18T18:49:49.722408Z","shell.execute_reply.started":"2021-07-18T18:49:48.251776Z","shell.execute_reply":"2021-07-18T18:49:49.720959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word embedding using pre-trained GloVe vectors\n\nNow that we have tokenized the text, we use GloVe pretrained vectors. Word embeddings is a way to represent words with similar meaning to have a similar representation. â€¯Using word embedding through GloVe, we can have a decent performance with models with even relatively small label training sets. \n\nYou can download GloVe pre-trained word vectors from the link below. There are different sizes of vocabulary and embedding dimension available. \n\nhttps://nlp.stanford.edu/projects/glove/\n","metadata":{}},{"cell_type":"markdown","source":"## Load the GloVe vectors","metadata":{}},{"cell_type":"code","source":"%%time\nembedding_vectors = {}\n# with open('/kaggle/input/glove6b100d/glove.6B.100d.txt','r',encoding='utf-8') as file:\nwith open('/kaggle/input/glove42b300dtxt/glove.42B.300d.txt','r',encoding='utf-8') as file:\n    for row in file:\n        values = row.split(' ')\n        word = values[0]\n        weights = np.asarray([float(val) for val in values[1:]])\n        embedding_vectors[word] = weights\nprint(f\"Size of vocabulary in GloVe: {len(embedding_vectors)}\")   \n","metadata":{"execution":{"iopub.status.busy":"2021-07-18T18:49:55.444611Z","iopub.execute_input":"2021-07-18T18:49:55.444966Z","iopub.status.idle":"2021-07-18T18:54:02.57354Z","shell.execute_reply.started":"2021-07-18T18:49:55.444934Z","shell.execute_reply":"2021-07-18T18:54:02.572015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create an embedding matrix with the GloVe vectors\n","metadata":{}},{"cell_type":"code","source":"#initialize the embedding_matrix with zeros\nemb_dim = 300\nif max_words is not None: \n    vocab_len = max_words \nelse:\n    vocab_len = len(word_index)+1\nembedding_matrix = np.zeros((vocab_len, emb_dim))\noov_count = 0\noov_words = []\nfor word, idx in word_index.items():\n    if idx < vocab_len:\n        embedding_vector = embedding_vectors.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[idx] = embedding_vector\n        else:\n            oov_count += 1 \n            oov_words.append(word)\n#print some of the out of vocabulary words\nprint(f'Some out of valubulary words: {oov_words[0:5]}')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-18T18:54:11.672921Z","iopub.execute_input":"2021-07-18T18:54:11.673252Z","iopub.status.idle":"2021-07-18T18:54:11.719752Z","shell.execute_reply.started":"2021-07-18T18:54:11.673218Z","shell.execute_reply":"2021-07-18T18:54:11.718821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'{oov_count} out of {vocab_len} words were OOV.')","metadata":{"execution":{"iopub.status.busy":"2021-07-18T18:54:11.918713Z","iopub.execute_input":"2021-07-18T18:54:11.919036Z","iopub.status.idle":"2021-07-18T18:54:11.923876Z","shell.execute_reply.started":"2021-07-18T18:54:11.919007Z","shell.execute_reply":"2021-07-18T18:54:11.923032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling\nNow, we can create a model and pre-train the embedding layer with the embedding matrix we just created based on GloVe vectors. You saw the [model overview](#model_overview) above. The first layer is Embedding. Embedding layer by Keras is a flexible layer that can be used also without any pre-trained weights. In that case, the Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset. In this exercise, we will set the weights of the Embedding layer to the embedding matrix from GloVe pre-trained vectors.This is a tranfer learning. \n\nAnother parameter in Embedding layer is \"trainable\" which can be set to True in case you want to fine-tune the word embedding or if you don't want the embedding weights to be updated you can set it to False. Here, we set it to False.<br>\n\nAfter the Embedding layer, we have a layer of LSTM or GRU and then a Dropout layer for regularization. \n\nThen we have a Dense layer with Sigmoid activation which transforms the output of previous layers to 0 or 1 (real or fake). \n","metadata":{}},{"cell_type":"markdown","source":"## LSTM\nLet's start with LSTM models.","metadata":{}},{"cell_type":"code","source":"lstm_model = Sequential()\nlstm_model.add(Embedding(vocab_len, emb_dim, trainable = False, weights=[embedding_matrix]))\nlstm_model.add(LSTM(128, return_sequences=False))\nlstm_model.add(Dropout(0.5))\nlstm_model.add(Dense(1, activation = 'sigmoid'))\nlstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(lstm_model.summary())","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-18T18:54:19.94296Z","iopub.execute_input":"2021-07-18T18:54:19.943334Z","iopub.status.idle":"2021-07-18T18:54:22.646195Z","shell.execute_reply.started":"2021-07-18T18:54:19.943302Z","shell.execute_reply":"2021-07-18T18:54:22.64527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nbatch_size = 256\nepochs  = 10\nhistory = lstm_model.fit(xtrain_pad, np.asarray(ytrain), validation_data=(xtest_pad, np.asarray(ytest)), batch_size = batch_size, epochs = epochs)\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-18T18:54:26.352788Z","iopub.execute_input":"2021-07-18T18:54:26.353106Z","iopub.status.idle":"2021-07-18T18:55:40.531142Z","shell.execute_reply.started":"2021-07-18T18:54:26.353077Z","shell.execute_reply":"2021-07-18T18:55:40.530248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LSTM - Evaluation\n\nLet's find the accuracy of training and testing dataset below:","metadata":{}},{"cell_type":"code","source":"#plot accuracy\nplt.figure(figsize=(15, 7))\nplt.plot(range(epochs), history.history['accuracy'])\nplt.plot(range(epochs), history.history['val_accuracy'])\nplt.legend(['training_acc', 'validation_acc'])\nplt.title('Accuracy')","metadata":{"execution":{"iopub.status.busy":"2021-07-18T18:55:51.979635Z","iopub.execute_input":"2021-07-18T18:55:51.979961Z","iopub.status.idle":"2021-07-18T18:55:52.196083Z","shell.execute_reply.started":"2021-07-18T18:55:51.979934Z","shell.execute_reply":"2021-07-18T18:55:52.195264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain_lstm_results = lstm_model.evaluate(xtrain_pad, np.asarray(ytrain), verbose=0, batch_size=256)\ntest_lstm_results = lstm_model.evaluate(xtest_pad, np.asarray(ytest), verbose=0, batch_size=256)\nprint(f'Train accuracy: {train_lstm_results[1]*100:0.2f}')\nprint(f'Test accuracy: {test_lstm_results[1]*100:0.2f}')","metadata":{"execution":{"iopub.status.busy":"2021-07-18T18:55:57.404951Z","iopub.execute_input":"2021-07-18T18:55:57.405275Z","iopub.status.idle":"2021-07-18T18:56:01.445334Z","shell.execute_reply.started":"2021-07-18T18:55:57.405246Z","shell.execute_reply":"2021-07-18T18:56:01.444502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## GRU","metadata":{}},{"cell_type":"code","source":"emb_dim = embedding_matrix.shape[1]\ngru_model = Sequential()\ngru_model.add(Embedding(vocab_len, emb_dim, trainable = False, weights=[embedding_matrix]))\ngru_model.add(GRU(128, return_sequences=False))\ngru_model.add(Dropout(0.5))\ngru_model.add(Dense(1, activation = 'sigmoid'))\ngru_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(gru_model.summary())","metadata":{"execution":{"iopub.status.busy":"2021-07-18T18:56:06.897604Z","iopub.execute_input":"2021-07-18T18:56:06.897933Z","iopub.status.idle":"2021-07-18T18:56:07.162586Z","shell.execute_reply.started":"2021-07-18T18:56:06.897905Z","shell.execute_reply":"2021-07-18T18:56:07.161818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nbatch_size = 256\nepochs  = 10\nhistory = gru_model.fit(xtrain_pad, np.asarray(ytrain), validation_data=(xtest_pad, np.asarray(ytest)), batch_size = batch_size, epochs = epochs)\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-18T18:56:09.867877Z","iopub.execute_input":"2021-07-18T18:56:09.868201Z","iopub.status.idle":"2021-07-18T18:57:17.871517Z","shell.execute_reply.started":"2021-07-18T18:56:09.868172Z","shell.execute_reply":"2021-07-18T18:57:17.870719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## GRU Evaluation\n\nLet's find the accuracy of training and testing dataset with GRU model below:","metadata":{}},{"cell_type":"code","source":"#plot accuracy\nplt.figure(figsize=(15, 7))\nplt.plot(range(epochs), history.history['accuracy'])\nplt.plot(range(epochs), history.history['val_accuracy'])\nplt.legend(['training_acc', 'validation_acc'])\nplt.title('Accuracy')","metadata":{"execution":{"iopub.status.busy":"2021-07-18T18:57:20.794319Z","iopub.execute_input":"2021-07-18T18:57:20.794658Z","iopub.status.idle":"2021-07-18T18:57:20.974215Z","shell.execute_reply.started":"2021-07-18T18:57:20.794628Z","shell.execute_reply":"2021-07-18T18:57:20.973451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_gru_results = gru_model.evaluate(xtrain_pad, np.asarray(ytrain), verbose=0, batch_size=256)\ntest_gru_results = gru_model.evaluate(xtest_pad, np.asarray(ytest), verbose=0, batch_size=256)\nprint(f'Train accuracy: {train_gru_results[1]*100:0.2f}')\nprint(f'Test accuracy: {test_gru_results[1]*100:0.2f}')","metadata":{"execution":{"iopub.status.busy":"2021-07-18T18:57:24.120798Z","iopub.execute_input":"2021-07-18T18:57:24.121112Z","iopub.status.idle":"2021-07-18T18:57:28.403992Z","shell.execute_reply.started":"2021-07-18T18:57:24.121085Z","shell.execute_reply":"2021-07-18T18:57:28.402519Z"},"trusted":true},"execution_count":null,"outputs":[]}]}