{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Celosia - Dynamic network update demo"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Imports\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.python.client import device_lib\nimport time\nimport seaborn as sns\nimport matplotlib.gridspec as gridspec\n\nfrom keras.layers import Input, Dense\nfrom keras import regularizers, Model\nfrom keras.models import Sequential, model_from_json\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.optimizers import RMSprop\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import accuracy_score, precision_recall_curve, classification_report, confusion_matrix, precision_score, average_precision_score, roc_curve, auc, recall_score, f1_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import benign and gafgyt combo dataset for a given device id (1-9)\ndef import_dataset_benign_gagfyt_combo(device_id):\n    normal = pd.read_csv('../input/nbaiot-dataset/{}.benign.csv'.format(device_id))\n    n_X = normal.iloc[:,]\n    n_X_scaled = MinMaxScaler().fit_transform(n_X.values)\n    n_y = np.ones(n_X.shape[0]) # 1 represents normal\n\n    anomalous = pd.read_csv('../input/nbaiot-dataset/{}.gafgyt.combo.csv'.format(device_id))\n    a_X = anomalous.iloc[:,]\n    a_X_scaled = MinMaxScaler().fit_transform(a_X.values)\n    a_y = np.zeros(a_X.shape[0]) # 0 represents anomalous\n\n    #normal.info()\n    #normal.describe()\n    #normal.head()\n\n    #anomalous.info()\n    #anomalous.describe()\n    #anomalous.head()\n\n    return (n_X_scaled, n_y, a_X_scaled, a_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(feature_count):\n    model = Sequential()\n    model.add(Dense(units=8, kernel_initializer=\"uniform\", activation=\"relu\", input_dim=feature_count)) # Hidden Layer 1 with 8 nodes\n    model.add(Dense(units=6, kernel_initializer=\"uniform\", activation=\"relu\"))  # Hidden Layer 2 with 6 nodes\n    model.add(Dense(units=1, kernel_initializer=\"uniform\", activation=\"sigmoid\")) # Output Layer\n    model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def change_model_feature_count(model, feature_count):\n    # replace input shape of first layer\n    model._layers[1].batch_input_shape = (None, feature_count)\n\n    # rebuild model architecture by exporting and importing via json\n    new_model = model_from_json(model.to_json())\n    #new_model.summary()\n\n    # copy weights from old model to new one\n    for layer in new_model.layers:\n        try:\n            layer.set_weights(model.get_layer(name=layer.name).get_weights())\n        except:\n            print(\"Could not transfer weights for layer {}\".format(layer.name))\n\n    new_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return new_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, X_train, y_train):\n    history = model.fit(X_train, y_train,\n                    batch_size=64,\n                    epochs=3) #,\n                    # We pass some validation for\n                    # monitoring validation loss and metrics\n                    # at the end of each epoch\n                    #validation_data=(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_model(model, X_test, y_test):\n    #model.summary()\n\n    y_pred = model.predict(X_test)\n    y_pred = y_pred.round()\n\n    #print (\"\")\n    #print (\"Classification Report: \")\n    #print (classification_report(y_test, y_pred.round()))\n\n    #print (\"\")\n    #print (\"Accuracy Score: \", accuracy_score(y_test, y_pred.round()))\n    #loss, accuracy, f1_score, precision, recall = model.evaluate(X_test, y_test, verbose=0)\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    return (accuracy, precision, recall, f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def limit_dataset_feature_count(feature_count, n_X_scaled, n_y, a_X_scaled, a_y):\n    X = np.append(n_X_scaled[:,:feature_count], a_X_scaled[:,:feature_count], axis = 0)\n    y = np.append(n_y, a_y)\n\n    return train_test_split(X, y, test_size=0.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_accuracy_metrices(title, metrices):\n    (accuracy, precision, recall, f1) = metrices\n    print (f\"{title}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dynamic_network_expansion(device_id):\n    devices_names = ['Danmini_Doorbell', 'Ecobee_Thermostat', 'Ennio_Doorbell', 'Philips_B120N10_Baby_Monitor', 'Provision_PT_737E_Security_Camera', 'Provision_PT_838_Security_Camera', 'Samsung_SNH_1011_N_Webcam', 'SimpleHome_XCS7_1002_WHT_Security_Camera', 'SimpleHome_XCS7_1003_WHT_Security_Camera']\n    (n_X_scaled, n_y, a_X_scaled, a_y) = import_dataset_benign_gagfyt_combo(device_id)\n    feature_count = 100\n    X_train, X_test, y_train, y_test = limit_dataset_feature_count(feature_count, n_X_scaled, n_y, a_X_scaled, a_y)\n\n    model = build_model(feature_count)\n\n    train_model(model, X_train, y_train)\n\n    evaluate_model(model, X_test, y_test)\n\n    new_feature_count = 115\n    X_train, X_test, y_train, y_test = limit_dataset_feature_count(new_feature_count, n_X_scaled, n_y, a_X_scaled, a_y)\n    new_model = change_model_feature_count(model, new_feature_count)\n\n    before = evaluate_model(new_model, X_test, y_test)\n    train_model(new_model, X_train, y_train)\n    after = evaluate_model(new_model, X_test, y_test)\n\n    return (devices_names[device_id - 1], before, after)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device_ids = [1, 2, 3, 4, 5, 6, 7, 8, 9]\nmetrices = []\nfor device_id in device_ids:\n    metrices.append(dynamic_network_expansion(device_id))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for metric in metrices:\n    (name, before, after) = metric\n    print(name)\n    print(\"\")\n    print_accuracy_metrices(f\"{name}-Before\", before)\n    print_accuracy_metrices(f\"{name}-After\", after)\n    print(\"-----------------\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}