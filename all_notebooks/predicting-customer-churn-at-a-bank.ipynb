{"cells":[{"metadata":{"_uuid":"cea54943ccde7a2bb67311b2a721da6451407719"},"cell_type":"markdown","source":"# Bank Customer Churn Prediction"},{"metadata":{"_uuid":"c3a6a53bae44fbd607737a30b4d442e9c2a3ee51"},"cell_type":"markdown","source":"In this exercice, we are going to build and train a model that predict which customers\nmay churn in future so that they can take steps to incentivise those customers to stay. \nWe will classify the predictions of those customers in either exited or stayed in binary classification (0 and 1)\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Reading the input directory files\nimport os\nprint(os.listdir(\"../input/\"))\n# Importing Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport seaborn as sns\nimport math\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c51c0bbd0ef2bd292d7aec74677e5b7d23f48a1"},"cell_type":"code","source":"# Reading the Bank Customers file using pandas function read.csv()\ncustomers_data = pd.read_csv('../input/Churn_Modelling.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e2677f447d9a4cc6e9cea87d8755aedc4167ff3"},"cell_type":"markdown","source":"## 1. Exploratory Analysis"},{"metadata":{"_uuid":"f64562b4e053d792da3861284bd8988da82b7a90"},"cell_type":"markdown","source":"In this phase 1, we will explore data, to have an understanding of its format, content and see if there is need to clean them before using them in our model prediction"},{"metadata":{"_uuid":"8710e8271cf0c88ac48065955c8c3de922521621","trusted":true},"cell_type":"code","source":"# Displaying the top rows of the dataset for a quick visualization of the data\nprint(customers_data.head())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# running a script on customers data  file: customers_data.describe() to run the descriptive statistics on the data\n#in order to screen outliers and potential bad data.\n\ncustomers_data.describe(include=\"all\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a52d2b9a1c83d71c3469e00a72e0d644d7cc8be0"},"cell_type":"code","source":"# analyzing the data, to know the number of rows and columns and see if there are any missing data\ncustomers_data.shape\nprint(\" The number of null values is: \" , customers_data.isnull().values.sum())\nprint(customers_data.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"596376128783d0ba6e381b667879705d92af1218"},"cell_type":"markdown","source":"From the results, we can see that there are no missing data "},{"metadata":{"trusted":true,"_uuid":"8ed2e955317372cf5d848461406b0f1720b993ad"},"cell_type":"code","source":"# Running customers_data.info () command to check if there are no missing values in any of the fields or NaN \n# and if all columns types were consistent with the data they contains. All were complete and consistent.\ncustomers_data.info () \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac3dc129ebf067478f0ef91f220c5aeccc143120"},"cell_type":"markdown","source":"#### From the above analysis from .info() we identify 3 columns with object dtype, in which two Geography and Gender are categorical features, the 3rd one surname is just a string data but not categorical\n#### And all data are complete, there are no missing values, as we have in all columns the total number of rows which is 10000"},{"metadata":{"trusted":true,"_uuid":"b85d2775c184da40f566ae2596ee53b37657b829"},"cell_type":"code","source":"#Creating helper functions to see visualy the distributon of the the different predictor variables\n\ndef visual_exploratory(x):\n    \n    for var in x.select_dtypes(include = [np.number]).columns :\n        print( var + ' : ')\n        x[var].plot('hist')\n        plt.show()\n        \nvisual_exploratory(customers_data)\n\n# ploting the box plot to visually inspect numeric data\n\ndef boxPlot_exploratory(x):\n    \n    for var in x.select_dtypes(include = [np.number]).columns :\n        print( var + ' : ')\n        x.boxplot(column = var)\n        plt.show()\n        \nboxPlot_exploratory(customers_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c54faecd7693431d8713334e1c9651dd800947ca"},"cell_type":"code","source":"#Creating a variable of Categorical features\n\ncat_df_customers = customers_data.select_dtypes(include = ['object']).copy()\nprint(cat_df_customers.head()) \nprint(\" The number of null values is: \" , cat_df_customers.isnull().values.sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c814d5418fcc7ee7334b1ff11a3b3fe82ecb85ce"},"cell_type":"markdown","source":"#### Plotting the distribution of the above categorical features"},{"metadata":{"trusted":true,"_uuid":"ac82f1cc3c601b135d853bd23cb30684aa046695"},"cell_type":"code","source":"#Plotting categorical features\n\n## 1. Plot for Geographical location\n\nlocation_count = cat_df_customers['Geography'].value_counts()\nsns.barplot(location_count.index, location_count.values)\nplt.title('Geographical location Distribution of Bank Customers')\nplt.ylabel('Frequency', fontsize=11)\nplt.xlabel('Geography', fontsize=11)\nplt.show()\n\n\n## 2. Plot for Gender \n\nlocation_count = cat_df_customers['Gender'].value_counts()\nsns.barplot(location_count.index, location_count.values)\nplt.title('Gender Distribution of Bank Customers')\nplt.ylabel('Frequency', fontsize=11)\nplt.xlabel('Gender', fontsize=11)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b157a320a678d2e306c7ca517d007f7895ffc04"},"cell_type":"markdown","source":"## 2. Building classification Model with Extreme Gradient Boosting(XGBoost) algorithm"},{"metadata":{"_uuid":"af290755abf67dfe347e437660d0da9bec81a06c"},"cell_type":"markdown","source":"We decided to use XGBoost as it s a strong model which tries to create a strong learner from an ensemble of weak learners (models)\nhence from the ensemble of weak models it learns from their error and combine all together to build a combination of them and keep only the parts where they performed well\nit has the advantages of combining different models into one and apply regularization, Penalisation of trees, performance, speed all in one model:hence has an inbuilt optimization\nit reduce the collinearity amongs features for a better performing model."},{"metadata":{"trusted":true,"_uuid":"018fd2623fbf07038e13264871b0526c7524bb9e"},"cell_type":"code","source":"#gradient boosting decision tree algorithm\nimport xgboost as xgb\nimport sklearn as skt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"608d35f60f35bd005f321c11cf093bc38081bdfa"},"cell_type":"code","source":"new_customers_data=customers_data.copy()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"176b0d6388785916f2bbaf36b298233421a0d884"},"cell_type":"markdown","source":"### 2.1. Label Encode string values in the dataset"},{"metadata":{"trusted":true,"_uuid":"c82fe3365f1b05630d6a084c5ccaa5c5fa2d252a"},"cell_type":"markdown","source":"Since XGBoost models takes only numeric values as input as it considers problems as regression modelling problem we will transform all string features values of gender into numerical value. Here we use label encoder as we have only two choices for gender which is ok as it will not create any wrong intrepretation of weighting."},{"metadata":{"trusted":true,"_uuid":"15c7457f619a1e686eee49fab04c95d1c513dc70"},"cell_type":"code","source":"# encode string class values as integers\n\nGender = new_customers_data['Gender']\nlabel_encoder = LabelEncoder()\nlabel_encoder = label_encoder.fit(Gender)\nlabel_encoded =label_encoder.transform(Gender)\nnew_customers_data['Gender']=label_encoded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4aaf09efd91c009420f8f1cdec4d858efe1e946"},"cell_type":"markdown","source":"### 2.2 Transform Geography string values with one-Hot Encoding"},{"metadata":{"trusted":true,"_uuid":"98d2c812c470f3463134567fabfe3d7f45657bb3"},"cell_type":"markdown","source":"Here one-hot encoding  convert each geographic name  into a new column and assign it a 1 or 0 and for this we will use \n.get_dummies(), a pandas method.This will make our model not interprete the values as weight since we will have only 1 and 0 instead of 0,1 and 2 for the case of label encoder"},{"metadata":{"trusted":true,"_uuid":"65b9cf31f42a4324a81830700ce48f5fc4f87bc8"},"cell_type":"code","source":"#print(new_customers_data.head())\n#Gend = new_customers_data['Gender']\n#print(Gend)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4ebfcf71c95dfe1080de583670018108c990c45"},"cell_type":"code","source":"temp_customers_data=new_customers_data.copy()\ntemp_customers_data = pd.get_dummies(temp_customers_data, columns=['Geography'], prefix = ['Geography'])\nprint(temp_customers_data.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f21535f5ed77627e98100fb1c71eeb01e0eb6791"},"cell_type":"code","source":"# Appending the new column to the new_customers_data dataframe\n\nnew_customers_data.insert(13, 'Geography_France' , temp_customers_data['Geography_France'])\nnew_customers_data.insert(14, 'Geography_Germany' , temp_customers_data['Geography_Germany'])\nnew_customers_data.insert(15, 'Geography_Spain' , temp_customers_data['Geography_Spain'])\nprint(new_customers_data.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d41a3c6e5764dc4a22bdd8de8e60a4b10c3a3222"},"cell_type":"markdown","source":"### 2.3 Creating new transformed features and adding them to the dataset"},{"metadata":{"trusted":true,"_uuid":"c669666e45ee28b396c82eb570435de2d9cb294b"},"cell_type":"code","source":"# Helper function that will create and add a new column tof credit score range the data frame\ndef creditscore(data):\n    score = data.CreditScore\n    score_range =[]\n    for i in range(len(score)) : \n        if (score[i] < 600) :  \n            score_range.append(1) # 'Very Bad Credit'\n        elif ( 600 <= score[i] < 650) :  \n            score_range.append(2) # 'Bad Credit'\n        elif ( 650 <= score[i] < 700) :  \n            score_range.append(3) # 'Good Credit'\n        elif ( 700 <= score[i] < 750) :  \n            score_range.append(4) # 'Very Good Credit'\n        elif score[i] >= 750 : \n            score_range.append(5) # 'Excellent Credit'\n    return score_range\n\n# converting the returned list into a dataframe\nCreditScore_category = pd.DataFrame({'CreditScore_range': creditscore(new_customers_data)})\n\n# Appending the new column to the new_customers_data dataframe\nnew_customers_data.insert(16, 'CreditScore_range' , CreditScore_category['CreditScore_range'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1ec371df9e238f4f444747c8270bb4c9a7df1c2"},"cell_type":"code","source":"# Helper function that will create and add a new column of age group to the data frame\ndef agegroup(data):\n    age = data.Age\n    age_range =[]\n    for i in range(len(age)) : \n        if (age[i] < 30) :  \n            age_range.append(1) # 'Between 18 and 30 year'   \n        elif ( 30 <= age[i] < 40) :  \n            age_range.append(2) # 'Between 30 and 40 year'\n        elif ( 40 <= age[i] < 50) :  \n            age_range.append(3) # 'Between 40 and 50 year'\n        elif ( 50 <= age[i] < 60) :  \n            age_range.append(4) # ''Between 50 and 60 year'\n        elif ( 60 <= age[i] < 70) :  \n            age_range.append(5) # 'Between 60 and 70 year'\n        elif ( 70 <= age[i] < 80) :  \n            age_range.append(6) # 'Between 70 and 80 year'\n        elif age[i] >= 80 : \n            age_range.append(7) # ''Above 80 year'\n    return age_range\n\n# converting the returned list into a dataframe\nAgeGroup_category = pd.DataFrame({'age_group': agegroup(new_customers_data)})\n\n# Appending the new column to the new_customers_data dataframe\nnew_customers_data.insert(17, 'age_group' , AgeGroup_category['age_group'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b147f8627075ac785bd3492b9aed9917e35aa3d"},"cell_type":"code","source":"print(new_customers_data.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63888930ba8a863ddf1ddbf6747d32d1b6ce5e3c"},"cell_type":"markdown","source":"### 2.4 Training and Building the XGBoost model"},{"metadata":{"_uuid":"59bf6546d70d4f3f6ae798852c9c118420e67366"},"cell_type":"markdown","source":"For training and testing the performance of our XGBoost model, we will base on the principle of using 67% of the data as training dataset and 33% as testing dataset."},{"metadata":{"trusted":true,"_uuid":"aa9fcea0c4ca6c21e6719d2bcffe23b83f787981"},"cell_type":"code","source":"new_customers_data_xgboost=new_customers_data.copy()\nTarget = 'Exited'\nSurname = 'Surname'\nGeography = 'Geography'\n#Gender= 'Gender'\nID= 'RowNumber'\nCustomerId = 'CustomerId'\n#Choose all predictors except Target, Surname, Geography, CustomerId & ID and also separate the response variable\nX = [x for x in new_customers_data_xgboost.columns if x not in [Surname,Geography, Target, ID, CustomerId]]\nY = new_customers_data_xgboost.iloc[:,-1]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1bf67c5bb130646e746daeb7b87c22d39af7bcdd"},"cell_type":"code","source":"predictors = new_customers_data_xgboost[X] #predictor variable\nresponse = Y # response variable\nprint(predictors.head())\nprint(response.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a69382e0d2642435be0ec890ce24a7fb837abd4a"},"cell_type":"markdown","source":"### 2.5 Training the model with train and test technique"},{"metadata":{"trusted":true,"_uuid":"cbef294f64e7e79e6cd7e9c3e517405e0b480054"},"cell_type":"code","source":"# split data into train and test sets\nseed = 7\ntest_size = 0.33\nX_train, X_test, y_train, y_test = train_test_split(predictors, response, test_size=test_size,\nrandom_state=seed)\n# fit model on training data\n\n## xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n##                max_depth = 5, alpha = 10, n_estimators = 10)\n\n#model = xgb.XGBClassifier(objective ='binary:logistic', colsample_bytree = 0.3, learning_rate = 0.1,\n                #max_depth = 5, alpha = 10, n_estimators = 10)\nmodel = xgb.XGBClassifier()\nmodel.fit(X_train, y_train)\n# make predictions for test data\npredictions = model.predict(X_test)\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\n\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2b8f9344bb903a61790d638ec20a6aa69e2b170"},"cell_type":"markdown","source":"#### 2.5.1 Inspecting the model parameters"},{"metadata":{"_uuid":"6fcb33a30c84b302bcb4376b64f8dfd91e6a3d5f"},"cell_type":"markdown","source":"#### 2.5.2 plotting important features identified by the model"},{"metadata":{"trusted":true,"_uuid":"b58b9cf3ddb01bc0f9e54dd4cb7781d2232feb65"},"cell_type":"code","source":"# plotting important features for a quick idea of which contribute to the model perfromance better\nimport matplotlib.pyplot as plt\nparams = {\"objective\":\"binary:logistic\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n                'max_depth': 5, 'alpha': 10}\nxgb.plot_importance(model)\nplt.rcParams['figure.figsize'] = [5, 5]\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c8a1d33f11ff1e8b52ed34c35e6e2ad321ba606"},"cell_type":"markdown","source":"### 2.6 Training the model with k-fold cross validation technique"},{"metadata":{"_uuid":"99a17534113b727ecd8341206b20d0ebe9014df6"},"cell_type":"markdown","source":"This split the data into folds. The algorithm is trained on k âˆ’ 1 folds with one held back and tested on the held back fold.\nThis is repeated so that each fold of the dataset is given a chance to be the held back test set\n\nit is more accurate because the algorithm is trained and evaluated multiple times on different data"},{"metadata":{"trusted":true,"_uuid":"306a958274fc5e4655d8a44852aa71ed1ece5ca0"},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold # to nforce the same distribution of classes in each fold\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"113815460159fe47bd98ceef07ce99540be6c45f"},"cell_type":"code","source":"# testing the cross validated model\nmodel2 = xgb.XGBClassifier()\nkfold = StratifiedKFold(n_splits=10, random_state=7)\nresults = cross_val_score(model2, predictors, response, cv=kfold)\nprint(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7fcf697a3c59d4ee8d52ebf85aeb03fed0ec0b8"},"cell_type":"code","source":"print(results) # These results represent the accuracy at each fold in the cross validated model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11b03a9e7c3bf569bf49a0d092235ceb2ba26cea"},"cell_type":"code","source":"print(model.feature_importances_) # the inbuild method from the model  display the importance score according to the input order of the predictors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17d2f05174ae3007c37a34ce6dc6168f1be20df3"},"cell_type":"code","source":"# plot feature importance\nfrom matplotlib import pyplot\nxgb.plot_importance(model)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc26533694b973c87dfdc6248ca3a6db52a3d0bd"},"cell_type":"markdown","source":"#### 2.6.1 Selecting features based on  their respective feature importance scores"},{"metadata":{"_uuid":"85420e6b3ee84b1d1455dff80c4a1fcc27ad25e7"},"cell_type":"markdown","source":"This is to avoid including redundant features in our training dataset as they do not  contribute to the improvemenet of the model"},{"metadata":{"trusted":true,"_uuid":"32a0cf51e5a907b9a36e47e736772dded789b7de"},"cell_type":"code","source":"print(np.sort(model.feature_importances_)) # Sorting them according to the importance order of the features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39446288d83ffb0f5820cc5561d90f97ff515e45"},"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nthresholds = np.sort(model.feature_importances_)\nfor thresh in thresholds:\n    selection = SelectFromModel(model, threshold=thresh, prefit=True)\n    select_X_train = selection.transform(X_train)\n    # train model\n    selection_model = xgb.XGBClassifier()\n    selection_model.fit(select_X_train, y_train)\n    # eval model\n    select_X_test = selection.transform(X_test)\n    y_pred = selection_model.predict(select_X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1],accuracy*100.0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d58d88a50f00916913eba70219685fee8f0905e"},"cell_type":"markdown","source":"### 2.7 Re-run the model and prediction with the optimal threshold of 0.04582651, taking only 7 predictors"},{"metadata":{"trusted":true,"_uuid":"fadb86f1fe3bdd516a81a91e81c84e9710b18eea"},"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\n# select features using threshold\nthresh= 0.04582651\n\nselection = SelectFromModel(model, threshold=thresh, prefit=True)\nselect_X_train = selection.transform(X_train)\n# train model\nselection_model = xgb.XGBClassifier()\nselection_model.fit(select_X_train, y_train)\n# eval model\nselect_X_test = selection.transform(X_test)\ny_pred = selection_model.predict(select_X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1],accuracy*100.0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7189836087162869a5bdffdbce6fb743e6305e29"},"cell_type":"code","source":"pred = pd.DataFrame(y_pred)\nprint (pred.head())\nwith open('churns_predict.csv', 'w') as f:\n    print( pred, file=f) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0509bafbadfb6c8e4cc51f3ea5738a3ccb050454"},"cell_type":"code","source":"predictions = [round(value) for value in y_pred]\nprint(predictions)\npreds = pd.DataFrame(predictions)\nprint(preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a94e5b3e8bb25984a65aba65d61eecefe8b38b77"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"624b865fe258ea4cffe020989701dad74e802a99"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}