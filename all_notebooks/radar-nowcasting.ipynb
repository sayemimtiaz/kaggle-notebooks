{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NOWCASTING RADAR RAINMAP","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The goal of this project is to use Radar Data from meteonet as an image and to predict the movement of a rainmap radar using deep learning. <br>\nIn the Meteonet data, for each radar data type, you will find one archive per month, each one sliced in periods of 10 or 11 days (each month is separated in 3 files). <br> For this notebook we will foreast one hour data and use a 15 minits step. <br> <br> So first our goal is to process those raw data and get in the order: <br> \n* Pick data for a 15 minits step\n* Sequence of 5 rainmap for learning + 4 rainmap as label\n* 256*256 map","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://weatheregg.com/wp-content/uploads/2018/01/doppler-radar-map-weathergov.png)","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# IMPORT THE LIBRARIES\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.optimizers import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LOCATION OF THE TRAINING DATA\ndirectory = '/kaggle/input/meteonet/'\nzone = \"NW\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fname_coords = directory + 'Radar_coords/Radar_coords/'+ f'radar_coords_{zone}.npz'\ncoords = np.load(fname_coords, allow_pickle=True)\n#it is about coordinates of the top left corner of pixels -> it is necessary to get the coordinates of the center of pixels\n#to perform a correct overlay of data\nresolution = 0.01 #spatial resolution of radar data (into degrees)\nlat = coords['lats']-resolution/2\nlon = coords['lons']+resolution/2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> LOAD THE USEFULL FUNCTIONS FOR ANALYSIS </h3>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# LOAD ANY RAINFALL RADAR PICKLE\ndef load_fichier(year,part_month,month):\n    directory = '/kaggle/input/meteonet/'\n    zone = \"NW\"\n    fname = directory + f'{zone}_rainfall_{str(year)}/{zone}_rainfall_{str(year)}/rainfall-{zone}-{str(year)}-{str(month).zfill(2)}/rainfall-{zone}-{str(year)}-{str(month).zfill(2)}/rainfall_{zone}_{str(year)}_{str(month).zfill(2)}.{str(part_month)}.npz'\n    pickle = np.load(fname, allow_pickle=True)\n    return pickle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert the given date of the pickle to the missing index.\ndef indice_miss_date(pickle):\n    indice_tab= np.zeros(pickle['miss_dates'].shape[0],dtype=int)\n    print(\"This is the missing date in the data: \",pickle['miss_dates'])\n    for miss_date in pickle['miss_dates']:\n        print(miss_date)\n        compteur=0\n        Diff_year = miss_date.year - pickle['dates'][0].year\n        Diff_Month = miss_date.month - pickle['dates'][0].month\n        Diff_Day = miss_date.day - pickle['dates'][0].day\n        Diff_Hour = miss_date.hour - pickle['dates'][0].hour\n        Diff_Minute = miss_date.minute - pickle['dates'][0].minute\n\n        indice = Diff_Day*24*12 + Diff_Hour*12 + Diff_Minute/5\n\n        indice_abs= int(indice) / indice\n        # We raise error because for now we don't handle that kind of situation\n        if Diff_year != 0 or Diff_Month != 0:\n            raise NameError('There is a difference in month or year !')\n        if indice_abs != 1:\n            raise NameError('Indice is not a integer')\n        else:\n            indice_tab[compteur]= int(indice)\n        compteur+=1\n    return indice_tab\n\n\n# cut the input data by the step we have consider\ndef cut_timestep(pickle,Step_minute):\n    Step_data = 5\n    Step = Step_minute // Step_data\n    print(\"We make a step every \",Step,\"indices\")\n    Val_selec=np.arange(0,len(pickle['dates']),Step) # Every 15 minits\n    return pickle['data'][Val_selec,:,:],pickle['dates'][Val_selec]\n\n\n# If Missing values.\ndef cut_timestep_miss(pickle,Echeance_minute,Step_minute) :\n    Step_data = 5\n    Step = Step_minute // Step_data\n    Entrainement = Echeance_minute//15 + 1\n    Prediction = Echeance_minute//15\n    Tot = Entrainement + Prediction\n    Qinit = (Step * Tot) - Step\n    Q1= Step * Tot\n  #  print('là',Q1)\n    indice_tab= indice_miss_date(pickle) # LOAD indice_miss_date function to get the missing indices \n    Tab_index=[]\n    \n    valeur_init=0\n    for indice in indice_tab:\n        Quotient= indice // Q1\n        Reste = indice % Q1\n        if Quotient > 1:\n            Index_end = Qinit + (Quotient - 1) * Q1\n            New_begin_index = Index_end + Q1 + Step - 1\n        elif Quotient == 1:\n            Index_end = Qinit * Quotient\n            New_begin_index = Index_end + Q1 + Step - 1\n        else :\n            Index_end = Qinit * Quotient\n            New_begin_index = Index_end + Q1 - 1\n        Sequence = np.arange(valeur_init , Index_end+1 , 3 , dtype=int)\n        Tab_index.append(Sequence[:])\n        valeur_init = New_begin_index\n     #   print(indice,Quotient,Reste,Index_end,Sequence,New_begin_index,Tab_index,valeur_init)\n    Sequence = np.arange(valeur_init ,pickle[\"dates\"].shape[0] , 3 , dtype=int)\n    Tab_index.append(Sequence[:])\n    return Tab_index\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cut_data_and_coord(data,Pixel,lat,lon,lat_edge=49.5,lon_edge=-2.5):\n    lat_only=lat[:,1]\n    lon_only=lon[1,:]\n    Temp_lat = np.where(lat_only < lat_edge)\n    Temp_lon = np.where(lon_only < lon_edge)\n    Lat_cut_index = Temp_lat[0][:Pixel]\n    Lon_cut_index = Temp_lon[0][:Pixel]\n    data_cut = data[:,Lat_cut_index]\n    data_cut= data_cut[:,:,Lon_cut_index]\n    return data_cut\n\ndef cut_timestep_miss2(pickle,Echeance_minute,Step_minute):\n    Tab = cut_timestep_miss(pickle,Echeance_minute,Step_minute)\n    shape = len(Tab)\n    Tab_tot = np.empty(0,dtype=int)\n    for i in range(shape):\n        Tab_tot=np.concatenate((Tab_tot,Tab[i]))\n        \n    return pickle['data'][Tab_tot,:,:],pickle['dates'][Tab_tot]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CODE TO CHANGE DATA BY 1 OR 0\ndef data_threshold(data_cut,rain_limit):\n    data_cut[data_cut > rain_limit ] = 1\n    data_cut[data_cut <= rain_limit ] = 0\n    return data_cut","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def XYTRAIN(data_process):\n    X_Train=np.zeros((0,256,256,5))\n    Y_Train=np.zeros((0,256,256,4))\n    DATA_X=np.zeros((1,256,256,5))\n    DATA_Y=np.zeros((1,256,256,4))\n    pas_X=0\n    pas_Y=4\n    print(\"Valeur max:\",int(data_process.shape[0]/9) )\n    for globale in range(int(data_process.shape[0]/9)):\n        for X in range(1,6):\n            DATA_X[0,:,:,X-1]=data_process[pas_X+X-1,:,:]\n        for Y in range(1,5):\n            DATA_Y[0,:,:,Y-1]=data_process[pas_Y+Y,:,:]\n        pas_X+=9\n        pas_Y+=9\n        X_Train= np.append(X_Train,DATA_X,axis=0)\n        Y_Train= np.append(Y_Train,DATA_Y,axis=0)\n        DATA_X=np.zeros((1,256,256,5))\n        DATA_Y=np.zeros((1,256,256,4))\n    return X_Train,Y_Train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def COUNT_RAIN_SITUATION(X_TRAIN,Y_TRAIN):\n    Count_X=0\n    Count_Y=0\n    Tab_delete=[]\n    Count=0\n    for X in X_TRAIN:\n        Temp_x = np.count_nonzero(X == 1)\n        if (Temp_x <= 200):\n            Count_X += 1\n            Tab_delete.append(Count)\n        Count +=1\n        \n    for Y in Y_TRAIN:\n        Temp_y = np.count_nonzero(Y == 1)\n        if Temp_y <= 200:\n            Count_Y += 1\n            \n    X_TRAIN = np.delete(X_TRAIN,Tab_delete,axis=0)\n    Y_TRAIN = np.delete(Y_TRAIN,Tab_delete,axis=0)\n        \n    return(X_TRAIN,Y_TRAIN,Count_X,Count_Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CONFIGURATION DATA\n# CHOOSE THE STEP AND THE ECHEANCE\nEcheance_minute = 60\nStep_minute = 15\nPixel=256\nrain_limit = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GLOBAL CODE FOR GIVEN THE DATASET TO LEARN THE MODEL\nmonth=[1,2,3]\npart=[1,2,3]\nyear=[2016,2017]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## LOOP FOR TRAIN DATA\ninit=0\nfor m in month:\n    for p in part:\n        pickle = load_fichier(2016,p,m)\n        print(\"This is the part \" ,p)\n    \n        if pickle[\"miss_dates\"].shape[0] == 0:\n            print(\"There is no missing data in this chunk\")\n            data_radar,dates_radar = cut_timestep(pickle,Step_minute)\n            print( data_radar.shape)\n            data_cut = cut_data_and_coord(data_radar,Pixel,lat,lon)\n            print( data_cut.shape)       \n            data_process = data_threshold(data_cut,rain_limit)\n            print(\"Data have been threshold\")\n            X_TRAIN,Y_TRAIN = XYTRAIN(data_process)\n            print(\"Shape of the temporary train data\",X_TRAIN.shape,Y_TRAIN.shape)\n        \n        else:\n            print(\"There is miss dates\")\n            data_radar,dates_radar = cut_timestep_miss2(pickle,Echeance_minute,Step_minute)\n            print(data_radar.shape)\n            data_cut = cut_data_and_coord(data_radar,Pixel,lat,lon)\n            print( data_cut.shape)\n            data_process = data_threshold(data_cut,rain_limit)\n            print(\"Data have been threshold\")\n            X_TRAIN,Y_TRAIN = XYTRAIN(data_process)\n            print(\"Shape of the temporary train data\",X_TRAIN.shape,Y_TRAIN.shape)\n        \n        if (init == 0):\n            X_TEMP=X_TRAIN\n            Y_TEMP=Y_TRAIN\n            print(\"STEP N°1: \",X_TEMP.shape,Y_TEMP.shape)\n        elif ( init == 1):\n            X_train= np.append(X_TEMP,X_TRAIN,axis=0)\n            Y_train= np.append(Y_TEMP,Y_TRAIN,axis=0)\n            print(\"STEP N°2: \",X_train.shape,Y_train.shape)\n        else:\n            X_train= np.append(X_train,X_TRAIN,axis=0)\n            Y_train= np.append(Y_train,Y_TRAIN,axis=0)\n            print(\"OTHER STEP : \",X_train.shape,Y_train.shape)\n        init += 1\n        print(\"----------------------------------------------------------------------\")\n    \ndel(X_TEMP,Y_TEMP,X_TRAIN,Y_TRAIN,data_cut,data_radar,data_process)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SHOW THE SITUATION WITHOUT METEO DATA\nprint(X_train.shape,Y_train.shape)\nX_train,Y_train,Count_X,Count_Y = COUNT_RAIN_SITUATION(X_train,Y_train)\nprint(X_train.shape,Y_train.shape,Count_X,Count_Y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# U_NET NETWORK","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = keras.Input(shape=(256,256,5))\n\nconv1 = layers.Conv2D(filters=32,kernel_size=(3,3),activation=\"relu\", padding=\"same\")(inputs)\nconv1 = layers.Conv2D(filters=32,kernel_size=(3,3),activation=\"relu\", padding=\"same\")(conv1)\n\nmaxpool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n\nconv2 = layers.Conv2D(filters=64,kernel_size=(3,3),activation=\"relu\", padding=\"same\")(maxpool1)\nconv2 = layers.Conv2D(filters=64,kernel_size=(3,3),activation=\"relu\", padding=\"same\")(conv2)\n\nmaxpool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n\nconv3 = layers.Conv2D(filters=128,kernel_size=(3,3),activation=\"relu\", padding=\"same\")(maxpool2)\nconv3 = layers.Conv2D(filters=128,kernel_size=(3,3),activation=\"relu\", padding=\"same\")(conv3)\n\nmaxpool3 = layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n\nconv4 = layers.Conv2D(filters=256,kernel_size=(3,3),activation=\"relu\", padding=\"same\")(maxpool3)\nconv4 = layers.Conv2D(filters=256,kernel_size=(3,3),activation=\"relu\", padding=\"same\")(conv4)\ndrop4 = layers.Dropout(0.5)(conv4)\n\nmaxpool4 = layers.MaxPooling2D(pool_size=(2, 2))(conv4)\n\nconv5 = layers.Conv2D(filters=512,kernel_size=(3,3),activation=\"relu\", padding=\"same\")(maxpool4)\nconv5 = layers.Conv2D(filters=512,kernel_size=(3,3),activation=\"relu\", padding=\"same\")(conv5)\ndrop5 = layers.Dropout(0.5)(conv5)\n\n\nup6 = layers.Conv2D(256, 2, activation = 'relu', padding=\"same\")(layers.UpSampling2D(size = (2,2))(drop5))\nmerge6 = layers.concatenate([drop4,up6], axis = 3)\n\nconv6 = layers.Conv2D(256, 3, activation = 'relu', padding=\"same\")(merge6)\nconv6 = layers.Conv2D(256, 3, activation = 'relu', padding=\"same\")(conv6)\n\n\n\nup7 = layers.Conv2D(128, 2, activation = 'relu', padding=\"same\")(layers.UpSampling2D(size = (2,2))(conv6))\nmerge7 = layers.concatenate([conv3,up7], axis = 3)\nconv7 = layers.Conv2D(128, 3, activation = 'relu', padding=\"same\")(merge7)\nconv7 = layers.Conv2D(128, 3, activation = 'relu', padding=\"same\")(conv7)\n\n\n\nup8 = layers.Conv2D(64, 2, activation = 'relu', padding=\"same\")(layers.UpSampling2D(size = (2,2))(conv7))\nmerge8 = layers.concatenate([conv2,up8], axis = 3)\nconv8 = layers.Conv2D(64, 3, activation = 'relu', padding=\"same\")(merge8)\nconv8 = layers.Conv2D(64, 3, activation = 'relu', padding=\"same\")(conv8)\n\nup9 = layers.Conv2D(32, 2, activation = 'relu', padding=\"same\")(layers.UpSampling2D(size = (2,2))(conv8))\nmerge9 = layers.concatenate([conv1,up9], axis = 3)\nconv9 = layers.Conv2D(32, 3, activation = 'relu', padding=\"same\")(merge9)\nconv9 = layers.Conv2D(32, 3, activation = 'relu' , padding=\"same\")(conv9)\nconv10 = layers.Conv2D(4, 3, activation = 'sigmoid', padding=\"same\")(conv9)\n\n\nmodel= keras.Model(inputs,conv10)\nmodel.compile(optimizer = \"Adam\", loss = 'binary_crossentropy', metrics = ['binary_accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model.fit(X_train,Y_train,batch_size=32, epochs=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MAKE PLACE IN MEMORY\ndel(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PLOT THE RESULTS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"epoch=[1,2,3,4,5,6,7,8,9,10]\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nax.plot(epoch,history.history['loss'],'o-',label=\"loss\")\nax.set_xlabel(\"Epochs\")\nax.set_ylabel(\"Cross_entropy loss\")\nax.legend()\nax.set_title('Loss plot for epoch iteration')\nplt.savefig(\"Loss_Train\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nax.plot(epoch,history.history['binary_accuracy'],'o-',label=\"accuracy\")\nax.set_xlabel(\"Epochs\")\nax.set_ylabel(\"Accuracy metrics\")\nax.legend()\nax.set_title('Accuracy plot for epoch iteration')\nplt.savefig(\"Accuracy_Train\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PREPARE TEST DATA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# VARIABLE FOR TEST DATA\nyear_test= 2018\nmonth_test = [1]\npart_test=[1,2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## LOOP FOR TEST DATA\ninit=0\nfor p in part_test:\n    pickle = load_fichier(year_test,p,1)\n    print(\"This is the part \" ,p)\n    \n    if pickle[\"miss_dates\"].shape[0] == 0:\n        print(\"There is no missing data in this chunk\")\n        data_radar,dates_radar = cut_timestep(pickle,Step_minute)\n        print( data_radar.shape)\n        data_cut = cut_data_and_coord(data_radar,Pixel,lat,lon)\n        print( data_cut.shape)       \n        data_process = data_threshold(data_cut,rain_limit)\n        print(\"Data have been threshold\")\n        X_TEST,Y_TEST = XYTRAIN(data_process)\n        print(\"Shape of the temporary train data\",X_TEST.shape,Y_TEST.shape)\n      #  X_TEST,Y_TEST = PERSISTANCE(data_process)\n        \n    else:\n        print(\"There is miss dates\")\n        data_radar,dates_radar = cut_timestep_miss2(pickle,Echeance_minute,Step_minute)\n        print(data_radar.shape)\n        data_cut = cut_data_and_coord(data_radar,Pixel,lat,lon)\n        print( data_cut.shape)\n        data_process = data_threshold(data_cut,rain_limit)\n        print(\"Data have been threshold\")\n        X_TEST,Y_TEST = XYTRAIN(data_process)\n        print(\"Shape of the temporary train data\",X_TEST.shape,Y_TEST.shape)\n     #   X_TEST,Y_TEST = PERSISTANCE(data_process)\n        \n    if (init == 0):\n        X_TEMP=X_TEST\n        Y_TEMP=Y_TEST\n        print(\"STEP N°1: \",X_TEMP.shape,Y_TEMP.shape)\n    elif ( init == 1):\n        X_test= np.append(X_TEMP,X_TEST,axis=0)\n        Y_test= np.append(Y_TEMP,Y_TEST,axis=0)\n        print(\"STEP N°2: \",X_test.shape,Y_test.shape)\n    else:\n        X_test= np.append(X_test,X_TEST,axis=0)\n        Y_test= np.append(Y_test,Y_TEST,axis=0)\n        print(\"OTHER STEP : \",X_test.shape,Y_test.shape)\n    init += 1\n    print(\"----------------------------------------------------------------------\")\n    \ndel(X_TEST,Y_TEST,data_cut,data_radar,data_process)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MODEL EVALUATION","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# EVALUATE THE MODEL ON TEST DATA.\nprint(\"Evaluate on test data\")\nresults = model.evaluate(X_test, Y_test, batch_size=20)\n# EVALUATE THE MODEL WITH THAT\nprint(\"test loss, test acc:\", results)\nY_predict = model.predict(X_test)\n#Y_predict.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import log_loss\n#accuracy = accuracy_score(Y_predict,Y_test)\nlog_loss= log_loss(np.ravel(Y_test[:,:,:,3]), np.ravel(Y_predict[:,:,:,3]))\nprint(log_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(log_loss)\nlog_loss_tab= [None,0.15,0.2,0.24,0.26]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del(X_test,Y_test)\n#del(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def LATLON_CUT(lat,lon):\n    lat_edge=49.5\n    lon_edge=-2.5\n    lat_only=lat[:,1]\n    lon_only=lon[1,:]\n    Temp_lat = np.where(lat_only < lat_edge)\n    Temp_lon = np.where(lon_only < lon_edge)\n    Lat_cut_index = Temp_lat[0][:Pixel]\n    Lon_cut_index = Temp_lon[0][:Pixel]\n    lat_format= lat[Lat_cut_index,Lon_cut_index]\n    lon_format= lon[Lat_cut_index,Lon_cut_index]\n    return lat_format,lon_format","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lat/Lon formating for plot.\nlat_format,lon_format = LATLON_CUT(lat,lon)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cartopy.crs as ccrs\nfrom cartopy.mpl.geoaxes import GeoAxes\nfrom cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import AxesGrid\nfrom matplotlib import colors\nimport cartopy.feature as cfeature\nimport numpy as np\n\nlllat = lat_format[-1]  #lower left latitude\nurlat = lat_format[0]  #upper right latitude\nlllon =  lon_format[0]  #lower left longitude\nurlon =lon_format[-1]  #upper right longitude\nextent = [lllon, urlon, lllat, urlat]\ncmap = colors.ListedColormap(['silver','#85F599','blue','#FFFF57','#FFC400','#FF2200'])\ncmap = colors.ListedColormap(['white', 'darkslateblue', \n                              'skyblue','cyan','lime','yellow',\n                              'orange','brown','red','plum'])\nbounds = [0,0.4,0.5,0.55,0.6,0.65,0.7,0.8,0.9,1]\nnorm = colors.BoundaryNorm(bounds, cmap.N)\n\n\nlats,lons = np.meshgrid(lat_format,lon_format)\n\nprojection = ccrs.PlateCarree()\naxes_class = (GeoAxes,dict(map_projection=projection))\n\nfig=plt.figure(figsize=(80,80))\naxgr= AxesGrid(fig, 222, axes_class=axes_class,\n                    nrows_ncols=(2, 9),\n                    axes_pad=0.2,\n                    cbar_location='right',\n                    cbar_mode= 'single',\n                    cbar_size='5%',\n                    label_mode='')\n                   # shared_all=True)  # note the empty label_mode\nPas= 3\nfor i, ax in enumerate(axgr):\n    ax.coastlines(resolution='50m', linewidth=2)\n    ax.gridlines()\n    ax.add_feature(cfeature.BORDERS.with_scale('50m'))\n\n    if i < 5:\n        p = ax.imshow(X_test[Pas,:,:,i],cmap=cmap,norm=norm, interpolation='none', origin='upper',extent=extent)\n    elif i >= 5 and i < 9:\n        p = ax.imshow(Y_test[Pas,:,:,i-5],cmap=cmap,norm=norm, interpolation='none', origin='upper',extent=extent)\n    elif i >=9 and i < 14:\n        p=  ax.imshow(X_test[Pas,:,:,i-9],cmap=cmap, interpolation='none', origin='upper',extent=extent)\n    elif i >=14 and i < 19:\n        p=  ax.imshow(Y_predict[Pas,:,:,i-14],cmap=cmap, interpolation='none', origin='upper',extent=extent)\naxgr.cbar_axes[0].colorbar(p)\nplt.show()\nfig.savefig(\"Echeance 3\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}