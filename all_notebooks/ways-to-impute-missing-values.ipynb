{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nData analyst and scientist spend a lot of time retreiving and cleaning data. It is then normal to understand and use different technique to accomplish this task. Dealing with missing value is part of the job. To many time, I see in various notebook the use of the mean, median or most frequent strategy to impute missing values when better stratey exist. The goal of this notebook is to demonstrate how to impute missing data using KNNImputer and compare result with the median and the mean strategy using SimpleImputer.We will see that the KNNImputer is a better choice when dealing with missing values. \n\nThis notbook will build a model to predict the median value of houses in the boston region. I did not tune the model or test other imputing technique. I might do it later."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import the dataset\nhousing = pd.read_csv('/kaggle/input/boston-housing-dataset/HousingData.csv')\n\nhousing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Explore it\nhousing.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#More exploration\nhousing.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can see here that around 20 variables are missing in 6 columns of the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Look at the sum of missing values in each columns\nhousing.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets see a correlation matrixs of the dataset\ncorrMatrix=housing.corr()\n\nplt.figure(figsize=(15,10))\n\nsns.heatmap(corrMatrix,annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(housing,corner=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Looking at the correlation between Median house values and all the other variables\ncorrMatrix['MEDV'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# IMPUTING AND MODELING"},{"metadata":{"trusted":true},"cell_type":"code","source":"# IMPORT MODULES\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import KNNImputer, SimpleImputer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data into X and y values\nX=housing.drop(['CHAS','MEDV'],axis=1) #CHAS variable does seems relevent for this task.\ny=housing['MEDV']\n\n#split train and test set\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=124)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#KNN IMPUTER \n\n# Impute with KNNImputer\nknn_impute=KNNImputer(n_neighbors=5,weights='distance')\n\n# transform the Na with the strategy\nX_train_filled_knn=knn_impute.fit_transform(X_train)\nX_test_filled_knn=knn_impute.transform(X_test)\n\n#Convert the arrays created back to Dataframe\nX_train_filled_knn= pd.DataFrame(X_train_filled_knn,columns=X_train.columns)\nX_test_filled_knn=pd.DataFrame(X_test_filled_knn,columns=X_test.columns)\n\n#Perform the model\nknn_imputed_model = LinearRegression()\nknn_imputed_model.fit(X_train_filled_knn,y_train)\ny_pred_knn=knn_imputed_model.predict(X_test_filled_knn)\n\n#Check the RMSE score\nRMSE_knn_model = np.sqrt(mean_squared_error(y_pred_knn,y_test))\nprint('This is the score of KNNImputer:', RMSE_knn_model)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MEAN IMPUTER\n\n# impute with the mean\nmean_imputer = SimpleImputer(strategy='mean')\n\n#transform the Na with the strategy\nX_train_filled_mean=mean_imputer.fit_transform(X_train)\nX_test_filled_mean = mean_imputer.transform(X_test)\n\n#Convert the arrays created back to Dataframe\nX_train_filled_mean = pd.DataFrame(X_train_filled_mean,columns=X_train.columns)\nX_test_filled_mean = pd.DataFrame(X_test_filled_mean,columns=X_test.columns)\n\n#Perform the model\nmean_imputed_model = LinearRegression()\nmean_imputed_model.fit(X_train_filled_mean,y_train)\ny_pred_mean= mean_imputed_model.predict(X_test_filled_mean)\n\n#Check the RMSE score\nRMSE_mean_model = np.sqrt(mean_squared_error(y_pred_mean,y_test))\nprint('This is the RMSE of the score with Nas impute with the mean: ',RMSE_mean_model)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MEDIAN IMPUTER\n\n#impute with the median\nmedian_imputer=SimpleImputer(strategy='median')\n\n# fill the Na with the strategy\nX_train_filled_median=median_imputer.fit_transform(X_train)\nX_test_filled_median=median_imputer.transform(X_test)\n\n#Convert the arrays created back to Dataframe\nX_train_filled_median=pd.DataFrame(X_train_filled_median,columns=X_train.columns)\nX_test_filled_median=pd.DataFrame(X_test_filled_median,columns=X_test.columns)\n\n#Perform the model\nmedian_imputed_model=LinearRegression()\nmedian_imputed_model.fit(X_train_filled_median,y_train)\ny_pred_median=median_imputed_model.predict(X_test_filled_median)\n\n#Check the RMSE score\nRMSE_median_model=np.sqrt(mean_squared_error(y_pred_median,y_test))\nprint('This is the score for the median imputed Nas: ',RMSE_median_model)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CONCLUSION\n\nThis was a quick overview of how to use imputer and why KNNImputer should be considered when dealing with missing data. The RMSE score was lower when the KNNImputer was used than the typical mean or median strategy and doesn't require much time to it. \n\nIf you liked this notebook give it an Upvote and feel free to comment !\n\nThank you !\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}