{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport sklearn as skl\nimport plotly as plt\nimport csv\n\nplt.offline.init_notebook_mode(connected=True)\npd.options.mode.chained_assignment = None\nnp.set_printoptions(linewidth = 95)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading and preprocessing the data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First, we load the data, doing some preprocessing, preserve the columns we are interested, and rename them in the process:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_path = \"/kaggle/input/pokemon-database/Pokemon Database.csv\"\n\ndf_raw = pd.read_csv(data_path, encoding='cp1252')\ndf_raw = df_raw.set_index('Pokemon Id')\ndf_raw.loc[df_raw['Original Pokemon ID'].notna(),'Legendary Type'] = \\\n    list(df_raw.loc()[df_raw[df_raw['Original Pokemon ID'].notna()]['Original Pokemon ID']]['Legendary Type'])\ndf_raw.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column_name_dict = {\n    'Pokedex Number': 'nid', \n    'Pokemon Name': 'name', \n    'Alternate Form Name': 'form',\n    'Legendary Type': 'legendary', \n    'Pokemon Height': 'height', \n    'Pokemon Weight': 'weight', \n    'Primary Type': 'type_1', \n    'Secondary Type': 'type_2',\n    'Health Stat': 'hp', \n    'Attack Stat': 'atk', \n    'Defense Stat': 'def', \n    'Special Attack Stat': 'satk', \n    'Special Defense Stat': 'sdef', \n    'Speed Stat': 'spd', \n    'EV Yield Total': 'ev_total', \n    'Pre-Evolution Pokemon Id': 'prev_id'\n}\n\ndf = df_raw[column_name_dict.keys()]\ndf.columns = column_name_dict.values()\ndf = df.fillna(value={'form': '', 'legendary': ''})\ndf.type_2[df.type_2.isna()] = df.type_1[df.type_2.isna()] \ndf = df[df.form==''].drop(columns=['form'])\ndf = df[df.prev_id.isin(df.index) | df.prev_id.isna()]\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the sake of simplicity I only keep the base forms i.e. no regional forms, alternate forms etc. and for I'm going to do later I also need to remove the Pokemon after those Pokemon, Sirfetch'd for example. \nBefore doing normalization, we calculate the statistics (especially min and max) to determine how to normalize the data:  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted(list(set(df.legendary)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SIZES = ['height', 'weight']\nTYPES = ['type_1', 'type_2']\nTYPE_LIST = sorted(list(set(df.type_1)))\nLEGENDARY_TYPE_LIST = sorted(list(set(df.legendary)))\nSTATS = ['hp', 'atk', 'def', 'satk', 'sdef', 'spd']\ndisplay(df[SIZES].describe().T.drop(columns=['count']).style.set_caption('Stats for heights and weights'))\ndisplay(df[STATS].describe().T.drop(columns=['count']).style.set_caption('Stats for base stats'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the height and weight have more log-like distribution, here I introduce some function for normalizing and reverse normalizing: ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def log_normalize(arr, mid, r_scale):\n    return np.log(arr/mid)/r_scale\n\ndef log_rev_normalize(arr, mid, r_scale):\n    return np.exp(arr*r_scale)*mid\n\ndisplay(log_normalize(df[['height']], 1,3).describe().T.drop(columns=['count']).style.set_caption('Stats for heights'))\ndisplay(log_normalize(df[['weight']],10,5).describe().T.drop(columns=['count']).style.set_caption('Stats for weights'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And then to make the names easier to deal with, I turn the name of all Pokemon to lowercase, but I still need to deal names with some non-alphabetic characters: ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndf.name = df.name.apply(lambda n: n.lower())\nname_is_special = [len(re.sub('[a-z]', '', n.lower())) > 0 for n in df.name]\ndf.name[name_is_special]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.name[name_is_special] = df.name[name_is_special].apply(lambda n: \n                                                          re.sub('_\\((.).+\\)', '_\\g<1>', \n                                                          re.sub(\"[-\\ 2:']+\", '_', \n                                                                 n.replace('.', '').replace('Ã©', 'e')))\n                                                         )\ndf.name[name_is_special]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After that, now we observe the length of the modified names: ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.name.reindex(df.name.str.len().sort_values(ascending=True).index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.name.str.len().describe().drop(['count'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 1: Autoencoder on stats, sizes, types, and legendary type","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data normalization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First, we make the functions that can go back and forth between the raw data and the training data for the network: ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def df_to_arrays(df_in): \n    stats = df_in[STATS]\n    types = df_in[TYPES]\n    legendary = df_in['legendary']\n    \n    height = log_normalize(df_in[['height']], 1,3)\n    weight = log_normalize(df_in[['weight']],10,5)\n    stats_norm = np.asarray(stats)/256\n    types_onehot = np.equal.outer(np.asarray(types), TYPE_LIST).astype(np.float)\n    legendaty_onehot = np.equal.outer(np.asarray(legendary), LEGENDARY_TYPE_LIST).astype(np.float)\n    \n    return (stats_norm, np.concatenate([height, weight], axis=1), \n            types_onehot[:,0,:], types_onehot[:,1,:],\n            legendaty_onehot)\n\ndef arrays_to_df(stats_norm, sizes, \n                 type1_onehot, type2_onehot, legendaty_onehot): \n    stats = np.round(stats_norm*256).astype(np.int)\n    type1 = np.array(TYPE_LIST)[np.argmax(type1_onehot,axis=1)]\n    type2 = np.array(TYPE_LIST)[np.argmax(type2_onehot,axis=1)]\n    height = log_rev_normalize(sizes[:,0], 1,3)\n    weight = log_rev_normalize(sizes[:,1],10,5)\n    legendary = np.array(LEGENDARY_TYPE_LIST)[np.argmax(legendaty_onehot,axis=1)]\n    \n    return pd.concat([pd.DataFrame(stats, columns=STATS),\n                      pd.DataFrame(np.stack([type1,type2],axis=1), columns=TYPES),\n                      pd.DataFrame(np.stack([height,weight],axis=1), columns=SIZES),\n                      pd.DataFrame(legendary, columns=['legendary']),\n                     ], \n                     axis=1)\n\ndf_in = df[df.name.isin(['skarmory', 'lugia', 'rayquaza'])]\ndisplay(df_in.style.set_caption('before normalization'))\n\narrays = df_to_arrays(df_in)\nprint('after normalization')\ndisplay(arrays)\nprint('shapes: ', [a.shape for a in arrays])\n\ndisplay(arrays_to_df(*arrays).style.set_caption('after reverse normalization'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data preparation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all_array = df_to_arrays(df)\ndisplay(data_all_array)\nprint([a.shape for a in data_all_array])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model definition","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Input\nfrom keras.layers import Dense, Concatenate\nfrom keras.models import Model\n\nstats_num = len(STATS)\nsizes_num = len(SIZES)\ntypes_num = len(TYPE_LIST)\nlegendary_num = len(LEGENDARY_TYPE_LIST)\nhidden_dim = [64, 32, 16]\nencode_dim = 8\n\n# encoder\ninput_stats = Input(shape=(stats_num,), name='input_st')\ninput_sizes = Input(shape=(sizes_num,), name='input_sz')\ninput_type1 = Input(shape=(types_num,), name='input_t1')\ninput_type2 = Input(shape=(types_num,), name='input_t2')\ninput_legend = Input(shape=(legendary_num,), name='input_lg')\n\ninputs = Concatenate(name='concat_in')([input_stats, input_sizes, input_type1, input_type2, input_legend])\n\nfor i, dim in enumerate(hidden_dim):\n    if i==0:\n        enc_hidden = Dense(dim, activation='elu', name='hidden_1_en')(inputs)\n    else:\n        enc_hidden = Dense(dim, activation='elu', name=f'hidden_{i+1}_en')(enc_hidden)\n\nenc_latent = Dense(encode_dim, activation='softsign', name='output_en')(enc_hidden)\n\nencoder_model = Model(inputs=(input_stats, input_sizes, \n                              input_type1, input_type2, input_legend), \n                      outputs=enc_latent, \n                      name='encoder')\nencoder_model.summary()\nprint()\n\n# decoder\ninput_latent = Input(shape=(encode_dim,), name='input_lt')\n\nfor i, dim in enumerate(hidden_dim[::-1]):\n    if i==0:\n        dec_hidden = Dense(dim, activation='elu', name='hidden_1_de')(input_latent)\n    else:\n        dec_hidden = Dense(dim, activation='elu', name=f'hidden_{i+1}_de')(dec_hidden)\n\ndec_stats = Dense(stats_num, activation='sigmoid', name='output_st')(dec_hidden)\ndec_sizes = Dense(sizes_num, activation='sigmoid', name='output_sz')(dec_hidden)\ndec_type1 = Dense(types_num, activation='softmax', name='output_t1')(dec_hidden)\ndec_type2 = Dense(types_num, activation='softmax', name='output_t2')(dec_hidden)\ndec_legend = Dense(legendary_num, activation='softmax', name='output_lg')(dec_hidden)\n\ndecoder_model = Model(inputs=input_latent,\n                      outputs=(dec_stats, dec_sizes, \n                               dec_type1, dec_type2, dec_legend), \n                      name='decoder')\ndecoder_model.summary()\nprint()\n\n# autoencoder\nautoencoder_model = Model(inputs=encoder_model.input, outputs=decoder_model(encoder_model.output), name='autonencoder')\nautoencoder_model.summary()\nprint()\n\noutput_names = ['stats', 'sizes', 'type1', 'type2', 'legendary']\noutput_names_dict = dict(zip(output_names, autoencoder_model.output_names))\noutput_names_rev_dict = dict(zip(autoencoder_model.output_names, output_names))\n\ndef output_name_map(output_names_dict, mapped):\n    return {output_names_dict[k]:mapped[k] for k in mapped.keys()}\n\nlosses = {'stats': 'mean_absolute_error',\n          'sizes': 'mean_absolute_error',\n          'type1': 'categorical_crossentropy',\n          'type2': 'categorical_crossentropy',\n          'legendary': 'categorical_crossentropy',}\nlosses = output_name_map(output_names_dict, losses)\n\nloss_weights = {'stats': 50.,\n                'sizes': 30.,\n                'type1': .5,\n                'type2': .5,\n                'legendary': .2}\nloss_weights = output_name_map(output_names_dict, loss_weights)\n\nmetrics = {'type1': 'categorical_accuracy',\n           'type2': 'categorical_accuracy',\n           'legendary': 'categorical_accuracy'}\nmetrics = output_name_map(output_names_dict, metrics)\n\nautoencoder_model.compile(optimizer='adam', \n                          loss=losses, loss_weights=loss_weights, \n                          metrics=metrics)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.keras import TqdmCallback\n\nload_if_avalible = True\nweight_path = '/kaggle/working/model_1_weight.h5'\n\nif (not load_if_avalible) or (not os.path.exists(weight_path)): \n    train_history = autoencoder_model.fit(data_all_array, data_all_array,\n                                          epochs=20_000+1, batch_size=512, shuffle=True, \n                                          verbose=0, callbacks=[TqdmCallback(verbose=0)])\n    autoencoder_model.save_weights(weight_path)\nelse: \n    autoencoder_model.load_weights(weight_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate the fitting result","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\n\nhistory = {}\nfor k in train_history.history.keys():\n    if 'loss' in k:\n        if k == 'loss':\n            history[k] = train_history.history[k]\n        else:\n            nk = output_names_rev_dict[k[:-len('_loss')]]+'_loss'\n            history[nk] = train_history.history[k]\n    if 'categorical_accuracy' in k:\n        nk = output_names_rev_dict[k[:-len('_categorical_accuracy')]]+'_categorical_accuracy'\n        history[nk] = train_history.history[k]\n\ndisplay(\n    px.line({m: history[m][::100] \n              for m in filter(lambda s: 'loss' in s, history.keys())})\\\n                .update_layout(xaxis_title='epoch/100', yaxis_title='loss')\n    )\n\ndisplay(\n    px.line({m: history[m][::100] \n              for m in filter(lambda s: 'accuracy' in s, history.keys())})\\\n                .update_layout(xaxis_title='epoch/100', yaxis_title='accuracy')\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_in = df[df.name.isin(['skarmory', 'lugia', 'rayquaza', 'archeops', 'latias', 'latios', 'arceus', 'keldeo'])]\n\ndisplay(df_in[STATS+TYPES+SIZES+['legendary']]\n        .style.set_caption('input data').format({'height': '{:.1f}', 'weight': '{:.1f}'}))\n\nlatent_vector = encoder_model.predict(df_to_arrays(df_in))\ndisplay(pd.DataFrame(latent_vector).style.set_caption('latent vector'))\n                                          \ndisplay(arrays_to_df(*decoder_model.predict(latent_vector))\n       .style.set_caption('output data').format({'height': '{:.1f}', 'weight': '{:.1f}'}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recons_df = arrays_to_df(*autoencoder_model.predict(data_all_array))\nrecons_df.index = df.index\nrecons_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inspect the latent space results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"latent_vector_all = encoder_model.predict(data_all_array)\n\nlatent_df = pd.DataFrame(latent_vector_all, \n                         columns=[f'lt{i}' for i in range(encode_dim)],\n                        index=df.index)\ndisplay(latent_df.describe().T.drop(columns=['count']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lt_df_plot = pd.concat([df[['nid', 'name']+STATS+SIZES+TYPES+['ev_total', 'legendary']], \n                        latent_df],\n                        axis=1).copy()\n\nlt_df_plot.legendary[lt_df_plot.legendary == ''] = 'None'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import ipywidgets as widgets\n\nTYPE_COLOR_MAP = {\n    'Bug': 'lightgreen', \n    'Dark': 'black', \n    'Dragon': 'blue', \n    'Electric': 'yellow', \n    'Fairy': 'fuchsia', \n    'Fighting': 'orange', \n    'Fire': 'red', \n    'Flying': 'skyblue', \n    'Ghost': 'midnightblue', \n    'Grass': 'green', \n    'Ground': 'brown', \n    'Ice': 'aqua', \n    'Normal': 'gray', \n    'Poison': 'purple', \n    'Psychic': 'violet', \n    'Rock': 'teal', \n    'Steel': 'silver', \n    'Water': 'navy', \n}\n\ndef show_pcs_fig(df):\n    def show_pcs_fig_df(x_axis, y_axis, color):\n        fig = px.scatter(df, x=x_axis, y=y_axis, \n                         color=color, size='ev_total', \n                         hover_data=['name','legendary'],\n                         size_max=6, \n                         color_discrete_map=TYPE_COLOR_MAP,\n                         category_orders={'type_1': TYPE_LIST,\n                                          'type_2': TYPE_LIST})\n        return fig\n    return show_pcs_fig_df\n\nlatent_str = [f'lt{i}' for i in range(encode_dim)]+STATS+SIZES\nlt_x_dropdown = widgets.Dropdown(options=latent_str, value=latent_str[0])\nlt_y_dropdown = widgets.Dropdown(options=latent_str, value=latent_str[1])\nclass_dropdown = widgets.Dropdown(options=['type_1', 'type_2', 'legendary'], value='type_1')\n\n_ = widgets.interact(show_pcs_fig(lt_df_plot), x_axis=lt_x_dropdown, y_axis=lt_y_dropdown, color=class_dropdown)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Apply PCA to the latent space vectors","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"latent_vector_all = encoder_model.predict(data_all_array)\n\nlatent_df = pd.DataFrame(latent_vector_all, columns=[f'lt{i}' for i in range(encode_dim)])\nlatent_df.describe().T.drop(columns=['count'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\ndef normalize(df, population=None):\n    if population is None:\n        population = df\n    df_desc = population.describe().loc()[['mean', 'std']]\n    return (df-df_desc.loc['mean'])/df_desc.loc['std']\n\ndef rev_normalize(df, population=None):\n    if population is None:\n        population = df\n    df_desc = population.describe().loc()[['mean', 'std']]\n    return (df*df_desc.loc['std'])+df_desc.loc['mean']\n\npca = PCA(random_state=227)\npca.fit(normalize(latent_df))\npcs = pca.components_\n\nlatent_var_r = pd.DataFrame(pca.explained_variance_ratio_[:,np.newaxis], columns=['var_r'])\nlatent_var_r.index = [f'pc{i}' for i in range(len(pcs))]\n\nlatent_pc = pd.DataFrame(pca.components_, columns=[f'lt{i}' for i in range(encode_dim)])\nlatent_pc.index = [f'pc{i}' for i in range(len(pcs))]\n\ndisplay(\n    pd.concat([latent_pc, latent_var_r],axis=1).style\\\n        .background_gradient(cmap='bwr_r', subset=[f'lt{i}' for i in range(encode_dim)], axis=0)\\\n        .background_gradient(cmap='Blues', subset=['var_r'], axis=0)\\\n        .format('{:.3}')\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pc_df_plot = pd.concat([df[['nid', 'name']+STATS+SIZES+TYPES+['ev_total', 'legendary']], \n                        pd.DataFrame(pca.transform(normalize(latent_df)), \n                                     columns=[f'pc{i}' for i in range(len(pcs))],\n                                     index=df.index)],\n                        axis=1).copy()\n\npc_df_plot.legendary[pc_df_plot.legendary == ''] = 'None'\n\npcs_str = [f'pc{i}' for i in range(len(pcs))]+STATS+SIZES\npc_x_dropdown = widgets.Dropdown(options=pcs_str, value=pcs_str[0])\npc_y_dropdown = widgets.Dropdown(options=pcs_str, value=pcs_str[1])\nclass_dropdown = widgets.Dropdown(options=['type_1', 'type_2', 'legendary'], value='type_1')\n\n_ = widgets.interact(show_pcs_fig(pc_df_plot), x_axis=pc_x_dropdown, y_axis=pc_y_dropdown, color=class_dropdown)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n\ndef pca_components_to_df(pca_latent_in):\n    latent_in = rev_normalize(pca.inverse_transform(pca_latent_in), latent_df)[np.newaxis, :]\n    return arrays_to_df(*decoder_model.predict(latent_in))\n\npca_latent_sliders = [widgets.FloatSlider(value=0, min=-4.0, max=4.0, step=0.01,\n                                          description=f'pc{i}:', orientation='vertical', continuous_update=False, \n                                          readout=True, readout_format='.2f', ) for i in range(encode_dim)]\npca_latent_hbox = widgets.HBox(pca_latent_sliders)\n\ndf_out = widgets.Output(layout={'border': '1px solid black', 'height': '80px'})\n\n\ndef on_slider_update(change):\n    df_out.clear_output()\n    slider_values = [slider.value for slider in pca_latent_sliders]\n    df_gen = pca_components_to_df(slider_values)\n    df_gen.insert(6, 'bst', df_gen[STATS].sum(axis=1))\n    \n    with df_out: \n        display(df_gen.style.format('{:.1f}', subset=SIZES))\n        \nfor slider in pca_latent_sliders:\n    slider.observe(on_slider_update, names='value')\n    \n\nrandomize_button = widgets.Button(description='Randomize')\n\ndef on_click_randomize(b):\n    for slider in pca_latent_sliders:\n        rand_norm = random.gauss(0, 1.0)\n        slider.value = round(min(4.0, max(-4.0, rand_norm)), 2)\n        \nrandomize_button.on_click(on_click_randomize)   \n\n\nset_as_button = widgets.Button(description='Set as: ')\nset_as_text = widgets.Text(value='',\n                           placeholder='Enter name', disabled=False)\n\ndef on_click_set_as(b):\n    if set_as_text.value not in list(pc_df_plot.name):\n        return\n    pcs_values = list(np.asarray(pc_df_plot[pc_df_plot.name == set_as_text.value][[f'pc{i}' for i in range(encode_dim)]]))[0]\n    for slider, value in zip(pca_latent_sliders, pcs_values):\n        slider.value = round(min(4.0, max(-4.0, value)), 2)\n        \nset_as_button.on_click(on_click_set_as)\n\nbuttons_hbox = widgets.HBox([randomize_button, set_as_button, set_as_text])\n\non_slider_update(_)\ndisplay(pca_latent_hbox, buttons_hbox, df_out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}