{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Exploring the twitterverse for feelings on NPIs in response to COVID-19 in Canada\n\n# Introduction\n\n**Question:** How is twitter responding to NPIs?\n\n**Motivation:** To understand the public’s response to non-pharmaceutical interventions (NPIs) across Canada, we may be able to leverage sentiment analysis of social media platforms. Which interventions were positively or negatively recieved?\n\n**Solution:** A very casual, exploratory sentiment analysis of tweets by intervention categories.\n\n**Take-aways:**\n1. A dataset of tweets related to the interventions in the CAN-NPI dataset\n2. Investigation on how sentiment analyses can go wrong\n3. A first-go at a workflow to roughly understand how people feel about interventions.\n\nI am no expert, so would love to get some feedback and any expertise, expecially regarding improvement of the sentiment analyses, pulling tweets, and data visualization :) \n\n\n# Method\n\n## Data\n\nI use the CAN-NPI dataset (`covid19-challenges/npi_canada.csv`), and use any tweets that contained any of the `source_urls` from this dataset which were pulled using [twint](https://github.com/twintproject/twint).\n\n\n## Overview\n0. **Set up:** Load packages, import modules, download data.\n1. **Data Preprocessing:** Clean the tweets\n2. **Data Analysis:** Comparison of sentiment analysis on text-only and a custom sentiment analysis that incorporates the sentiment of emojis.\n3. **Visualization:** Plot the proportion of positive, negative, and neutral tweets of intervention categories with \"sufficient\" tweet coverage."},{"metadata":{},"cell_type":"markdown","source":"## Set Up"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# download necessary packages\n!pip install langdetect\n!pip install emoji","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load modules\nimport pandas as pd\nfrom datetime import datetime, date, timedelta\nimport numpy as np\nimport re\nimport os\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport nltk \nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport gensim\nfrom sklearn.model_selection import cross_val_score, StratifiedShuffleSplit,train_test_split, GroupShuffleSplit\nfrom langdetect import detect\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom textblob import TextBlob\n\n\nos.environ['KMP_DUPLICATE_LIB_OK']='True'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load CAN-NPI dataset\nnpis_csv = \"/kaggle/input/covid19-challenges/npi_canada.csv\"\nraw_data = pd.read_csv(npis_csv,encoding = \"ISO-8859-1\")\n# remove any rows that don't have a start_date, region, or intervention_category\ndf = raw_data.dropna(how='any', subset=['start_date', 'region', 'intervention_category'])\ndf['region'] = df['region'].replace('Newfoundland', 'Newfoundland and Labrador')\nnum_rows_removed = len(raw_data)-len(df)\nprint(\"Number of rows removed: {}\".format(num_rows_removed))\n\n# get all regions\nregions = list(set(df.region.values))\nprint(\"Number of unique regions: {}\".format(len(regions)))\n\n# get all intervention categories\nnum_cats = list(set(df.intervention_category.values))\nnum_interventions = len(num_cats)\nprint(\"Number of unique intervention categories: {}\".format(len(num_cats)))\n\n# get earliest start date and latest start date\ndf['start_date'] = pd.to_datetime(df['start_date'], format='%Y-%m-%d')\nearliest_start_date = df['start_date'].min()\nlatest_start_date = df['start_date'].max()\nnum_days = latest_start_date - earliest_start_date\nprint(\"Analyzing from {} to {} ({} days)\".format(earliest_start_date.date(), latest_start_date.date(), num_days))\nprint(\"DONE READING DATA\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load tweets\nmerged_tweets_csv = '/kaggle/input/npi-twitterverse-april-30/tweets_to_intervention_category.source_urls.tsv'\ncolnames = [\"npi_record_id\", \"intervention_category\", \"oxford_government_response_category\", \"source_url\", \"id\", \"conversation_id\", \"created_at\", \"date\", \"time\", \"timezone\", \"user_id\", \"username\", \"name\", \"place\", \"tweet\", \"mentions\", \"urls\", \"photos\", \"replies_count\", \"retweets_count\", \"likes_count\", \"hashtags\", \"cashtags\", \"link\", \"retweet\", \"quote_url\", \"video\", \"near\", \"geo\", \"source\", \"user_rt_id\", \"user_rt\", \"retweet_id\", \"reply_to\", \"retweet_date\", \"translate\", \"trans_src\", \"trans_dest\"]\ntweets_df = pd.read_csv(merged_tweets_csv, encoding = \"utf-8\", error_bad_lines=False, engine='python', names=colnames)\n# drop any rows without tweets - aka any interventions supported by non-tweeted media urls\ntweets_df = tweets_df.dropna(how='any', subset=['npi_record_id', 'intervention_category', 'tweet'])\n\n# only get english tweets\ndata = []\nfor index, row in tweets_df.iterrows():\n    # detect only english tweets\n    tweet = row['tweet'].strip()\n    if tweet != \"\":\n        language =\"\"\n        try:\n            language = detect(tweet)\n        except:\n            language = \"error\"\n        if language == \"en\":\n            data.append([row['intervention_category'], tweet])\ntweets_df_en = pd.DataFrame(data, columns=[\"intervention_category\", \"tweet\"])\nprint(\"Number of non-english tweets = {}\".format(len(tweets_df) - len(tweets_df_en)))\nprint(\"Number of tweets collected = {}\".format(len(tweets_df_en)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing\n\nI performed some standard text preprocessing on the tweets. I masked any URLs, usernames, and removed any hashtags or non-alpabetical characters. Any words with repeated characters were shortened (for ex. \"hellooooooo\"-->\"hello\").\n\nSome non-standard practices I used, included removal of words that highly influenced the sentiment analysis but I found did not make much sense in this context. For example, words that such as \"first\", \"positive\", or \"confirmed\", seemed to drive the polarity scores positively. This makes sense intuitively in other contexts. However, in the CAN-NPI dataset, this unfairly skewed the sentiments of tweets related to \"First death announcements\" or \"General case announcements\".\n\n### Examples of the pitfalls when using off-the-shelf sentiment analyses in NPI sentiment analyses"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here's a few examples of First death announcements\nex1 = \"Here's a wrap of the latest coronavirus news in Canada: 77 cases, one death, an outbreak in a B.C. nursing home and Ottawa asks provinces about their critical supply gaps.  https://www.theglobeandmail.com/canada/article-bc-records-canadas-first-coronavirus-death/\"\nex2 = \"B.C. records Canada’s first coronavirus death  http://dlvr.it/RRZPGL  pic.twitter.com/pn8T4yumQJ\"\nprint(\"Example 1 = {}\".format(ex1))\nprint(\"Example 2 = {}\".format(ex2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are just announcements, pretty neutral but the scores are the following:"},{"metadata":{"trusted":true},"cell_type":"code","source":"ex1_tb = TextBlob(ex1)\nex1_ss = ex1_tb.sentiment[0]\nprint(\"Example 1 has score={}\".format(ex1_ss))\nex2_tb = TextBlob(ex2)\nex2_ss = ex2_tb.sentiment[0]\nprint(\"Example 2 has score={}\".format(ex2_ss))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Words like \"first\" in other contexts is pretty positive, but not in this case. What is the effect of removing this word on the sentiment score?"},{"metadata":{"trusted":true},"cell_type":"code","source":"ex = \"first coronavirus death\"\nex_tb = TextBlob(ex)\nex_ss = ex_tb.sentiment[0]\nprint(\"{} with score={}\".format(ex, ex_ss))\n\nex = \"coronavirus death\"\nex_tb = TextBlob(ex)\nex_ss = ex_tb.sentiment[0]\nprint(\"{} with score={}\".format(ex, ex_ss))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It got more positive with the word \"first\". Moving forward, I remove other words that show that same pattern such as \"confirm\", and \"positive\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"import re \nimport nltk\nnltk.download('punkt')\n\ndef tweet_preprocess(text):\n  '''Return tokenized text with \n  rsemoved URLs, usernames, hashtags, weird characters, repeated\n  characters, stop words, and numbers\n  '''\n  text = text.lower()\n  text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', text) # remove URLs\n  text = re.sub(r'@[A-Za-z0-9]+','USER',text) # removes any usernames in tweets\n  text = re.sub(r'#([^\\s]+)', r'\\1', text) # remove the # in #hashtag\n  text = re.sub('[^a-zA-Z0-9-*. ]', ' ', text) # remove any remaining weird characters\n  words = word_tokenize(text)  # remove repeated characters (helloooooooo into hello)\n  ignore = set(stopwords.words('english'))\n  more_ignore = {'at', 'and', 'also', 'or', \"http\", \"ca\", \"www\", \"https\", \"com\", \"twitter\", \"html\", \"news\", \"link\", \\\n                 \"positive\", \"first\", \"First\", \"confirmed\", \"confirm\", \"confirms\"}\n  ignore.update(more_ignore)\n  #porter = PorterStemmer()\n  #cleaned_words_tokens = [porter.stem(w) for w in words if w not in ignore]\n  cleaned_words_tokens = [w for w in words if w not in ignore]\n  cleaned_words_tokens = [w for w in cleaned_words_tokens if w.isalpha()]\n\n  return cleaned_words_tokens","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data analysis\n\n### Sentiment analysis (text-based only)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_sentiment_analysis(tweets_df):\n  tweets_df[\"sentiment\"] = 0\n  for index, row in tweets_df.iterrows():\n    tokens = tweet_preprocess(row['tweet'])\n    clean_text = ' '.join(tokens)\n    analysis = TextBlob(row['tweet'])\n    analysis_after_clean = TextBlob(clean_text)\n\n    print(\"{}: {} \\n before cleaning score={}, after cleaning score={}\".format(row['intervention_category'], row['tweet'], analysis.sentiment[0], analysis_after_clean.sentiment[0]))\n\n    if analysis.sentiment[0]>0:\n      print('Positive')\n    elif analysis.sentiment[0]<0:\n      print('Negative')\n    else:\n      print('Neutral')\n    print(\"======================================\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run_sentiment_analysis(tweets_df_en[:5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sentiment analysis (text+emoji)\n\nI found that some tweets that were clearly positive, were not being scored as such. \n\nFor example, this Public Announcement: \n> \"THANK YOU Government of #Canada ! ❤❤❤❤❤❤ Government of #Canada evacuating Canadians on board #DiamondPrincess cruise ship   https://bit.ly/2UVjHgx  #outbreak #COVID19 #SARSCoV2 #Coronavirus #nCoV2019 #COVIDー1\"\n\nHad a polarity score of 0.0. However, it's hard to argue that it's a neutral tweet.\n\nI wondered if there was a way to better account for the use of emojis. Previous work, seemed to show that incorporating emojis significantly improves polarity scores and often \"dominate[s] the sentiment conveyed by textual cues and forms a good proxy for the polarity of text\" [(Hogenboom et al., 2015)](https://personal.eur.nl/frasincar/papers/JWE2015/jwe2015.pdf). I introduce a *very rough* modified sentiment score using the emoji sentiment mapping and scoring scheme from [Novak et al., 2015](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0144296#pone.0144296.ref006). Here, they give a sentiment score for each emoji.\n\nDue to the lack of availability of labeled tweets, emojis are sometimes used to \"distantly\" label the sentiment of tweets [(Felbo et al., 2017)](https://arxiv.org/pdf/1708.00524.pdf). Given that previous work actually use the emojis as labels and often dominate the sentiment, I use a rule-based method, where emojis with a \"high\" score either negative or positive determine the overall sentiment of the tweet. For cases, where there are multiple emojis, I average the sentiment scores. While the emoji sentiment mappings provided previously are fairly comprehensive, in some cases, the emojis are not found in their set. In these cases, I do not take these emojis into account when averaging their scores.\n\nIf no emojis exist, I use the sentiment analysis on the preprocessed tweets. After manual inspection, I found that sometimes the sentiment scores did not make sense again. I determine a very strict threshold based on a subset of tweets, only classifying a tweet as positive if the sentiment scores are greater than 0.25 and negative if less than -0.25."},{"metadata":{"trusted":true},"cell_type":"code","source":"# download sentiment map\n!wget https://www.clarin.si/repository/xmlui/bitstream/handle/11356/1048/Emoji_Sentiment_Data_v1.0.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import emoji\n\n# get emoji sentiment map\nemoji_sent_csv = \"Emoji_Sentiment_Data_v1.0.csv\"\nemoji_data = pd.read_csv(emoji_sent_csv,encoding = \"ISO-8859-1\")\n\ndef extract_emojis(str):\n  return ''.join(c for c in str if c in emoji.UNICODE_EMOJI)\n\ndef calc_emoji_sent(e):\n    e_uc = '0x{:X}'.format(ord(e)).lower()\n    #print(e_uc)\n    count_pos =0\n    count_neg =0\n    count_neutral = 0\n    sr = emoji_data.loc[emoji_data[\"Unicode codepoint\"] == e_uc.lower()]\n    score = -100\n    if not sr.empty:\n        oc = int(sr[\"Occurrences\"].astype(int))\n        num_pos = int(sr[\"Positive\"].astype(int))\n        num_neut = int(sr[\"Neutral\"].astype(int))\n        num_neg = int(sr[\"Negative\"].astype(int))\n        score = 1*num_pos/oc + -1*num_neg/oc + 0*num_neut/oc\n    #print(\"{} with score={}\".format(e, score))\n    return score\n\ndef run_sentiment_analysis_mod(tweets_df):\n  tweets_df[\"sentiment_score\"] = 0.0\n  tweets_df[\"sentiment_class\"] = \"\"\n\n  for index, row in tweets_df.iterrows():\n    tokens = tweet_preprocess(row['tweet'])\n    clean_text = ' '.join(tokens)\n    analysis = TextBlob(row['tweet'])\n    analysis_after_clean = TextBlob(clean_text)\n    c_score = analysis_after_clean.sentiment[0]\n    \n    # add emojis in sentiment analysis\n    emojis_detected = extract_emojis(row['tweet'])\n    avg_emoji_sent_score = 0\n    emoji_counts = 0\n    if emojis_detected:\n        for e in emojis_detected:\n            em_sent_score = calc_emoji_sent(e)\n            if em_sent_score == -100:\n              continue\n            avg_emoji_sent_score += em_sent_score\n            emoji_counts += 1\n        if emoji_counts > 0:\n            avg_emoji_sent_score = avg_emoji_sent_score/emoji_counts\n        #print(avg_emoji_sent_score)\n\n\n    # final score calculations\n    score = 0.0\n    label = \"NEUTRAL\"\n    if avg_emoji_sent_score > 0.10:\n        score = avg_emoji_sent_score\n        label = \"POSITIVE\"\n    elif avg_emoji_sent_score < -0.10:\n        score = avg_emoji_sent_score\n        label = \"NEGATIVE\"\n    else:\n        score = analysis_after_clean.sentiment[0]\n        if score > 0.25:\n          label = \"POSITIVE\"\n        elif score < -0.25:\n          label = \"NEGATIVE\"\n    tweets_df.at[index, \"sentiment_score\"] = score\n    tweets_df.at[index, \"sentiment_class\"] = label \n    '''print(\"=============================\")\n    print(row[\"intervention_category\"] + \"\\n\")\n    print(row['tweet'])\n    print(clean_text)\n    print(\"Score (no clean) = {}\".format(analysis.sentiment[0]))\n    print(\"Score (clean) = {}\".format(c_score))\n    print(\"Final Score = {}\".format(score))\n    print(label)'''\n  return tweets_df\n\nmod_tweets_df = run_sentiment_analysis_mod(tweets_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results (Preliminary)\n\nLet's see the proportion of sentiment classes by intervention category for intervention categories with at least 50 tweets. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\nimport plotly\n\ndef split_data_by_class(tweets_df):\n    total_tweets_by_cat = tweets_df.groupby('intervention_category')[\"id\"].count().reset_index(name=\"count\").sort_values(\"intervention_category\", ascending=False)\n    counts = tweets_df.groupby(['intervention_category',\"sentiment_class\"])[\"id\"].count().reset_index(name=\"count\").sort_values(\"intervention_category\", ascending=False)\n    counts[\"proportion\"] = 0.0\n    for index, row in counts.iterrows():\n        total_tweets = int(total_tweets_by_cat.loc[total_tweets_by_cat[\"intervention_category\"] == row[\"intervention_category\"]][\"count\"].astype(int))\n        counts.at[index, \"proportion\"] = row[\"count\"]/total_tweets\n\n    y = counts[\"intervention_category\"].unique().tolist()\n\n    # fill gaps - some sentiment_class + intervention_category combinations are empty\n    # and it messes up my graphs :(\n    fill_data = []\n    for ic in y:\n      for sc in [\"POSITIVE\", \"NEUTRAL\", \"NEGATIVE\"]:\n        subset = counts[(counts.sentiment_class == sc) & (counts.intervention_category == ic)]\n        if subset.empty:\n          fill_data.append([ic, sc, 0, 0.0])\n    fill_data_df = pd.DataFrame(fill_data, columns=[\"intervention_category\", \"sentiment_class\", \"count\", \"proportion\"])\n    full_counts = counts.append(fill_data_df).sort_values(\"intervention_category\", ascending=False)\n\n    return full_counts, y\n\ndef plot(full_counts, y, measure):\n    # only plot intervention_category if it had \"sufficient\" number of tweets\n    THRESH = 50\n    total_tweets_by_cat = tweets_df.groupby('intervention_category')[\"id\"].count().reset_index(name=\"count\").sort_values(\"intervention_category\", ascending=False)\n    if measure == \"proportion\":\n      # find all intervention_category with enough tweets\n      y = total_tweets_by_cat[total_tweets_by_cat[\"count\"] > THRESH][\"intervention_category\"].unique().tolist()\n      full_counts = full_counts[full_counts.intervention_category.isin(y)]\n\n    # split up by sentiment_class\n    pos_counts = full_counts.loc[full_counts[\"sentiment_class\"] == \"POSITIVE\"]\n    neg_counts = full_counts.loc[full_counts[\"sentiment_class\"] == \"NEGATIVE\"]\n    neut_counts = full_counts.loc[full_counts[\"sentiment_class\"] == \"NEUTRAL\"]\n    print(\"Mean {} for positive class: {}\".format(measure, round(pos_counts[measure].mean(),2)))\n    print(\"Mean {} for negative class: {}\".format(measure, round(neg_counts[measure].mean(),2)))\n    print(\"Range {} for positive class: {}-{}\".format(measure, round(pos_counts[measure].min(),2), round(pos_counts[measure].max(),2)))\n    print(\"Range {}  for negative class: {}-{}\".format(measure, round(neg_counts[measure].min(),2), round(neg_counts[measure].max(),2)))\n    \n    fig = go.Figure()\n    fig.add_trace(go.Bar(\n        y=y,\n        x=pos_counts[measure],\n        name='Positive',\n        orientation='h',\n        marker=dict(\n            color='rgba(90, 191,165, 1.0)',\n            line=dict(color='rgba(255, 255, 255, 1.0)', width=1)\n        )\n    ))\n    fig.add_trace(go.Bar(\n        y=y,\n        x=neg_counts[measure],\n        name='Negative',\n        orientation='h',\n        marker=dict(\n            color='rgba(230, 130, 130, 1.0)',\n            line=dict(color='rgba(255, 255, 255, 1.0)', width=1)\n        )\n    ))\n    fig.add_trace(go.Bar(\n        y=y,\n        x=neut_counts[measure],\n        name='Neutral',\n        orientation='h',\n        marker=dict(\n            color='rgba(190, 203, 200, 1.0)',\n            line=dict(color='rgba(255, 255, 255, 1.0)', width=1)\n        )\n    ))\n\n\n    fig.update_layout(width=800, height=1200,barmode='stack', \n                      template='plotly_white',\n                      bargap=0.5, # gap between bars of adjacent location coordinates.\n                      #bargroupgap=0.5 # gap between bars of the same location coordinate.\n                     )\n    fig.show()\n    #plotly.offline.iplot(fig, filename='fig.png')\n\nfull_counts, y = split_data_by_class(mod_tweets_df)\nplot(full_counts,y, \"proportion\")\nplot(full_counts,y, \"count\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Main points\n\n* Majority of tweets are neutral\n* When not neutral, for the most part twitter is responding pretty positively to the NPIs, with a mean proportion of 0.17.\n* There was an order of magnitute difference in the number of tweets related to school closure with >20K tweets. The next closest intervention category was Emergency economic funding with 2650 tweets."},{"metadata":{},"cell_type":"markdown","source":"# Discussion\n\nThis work is in development still but it's interesting to see the pitfalls of sentiment analyses especially in the context of NPIs in response to COVID-19. \n\n## Future work\n* Pull tweets from the Oxford Government Response Tracker, and see how the feelings differ in different regions.\n* Sentiment analysis over time\n* Sentiment analysis by region\n* Use Twitter API to pull more tweets and replies. Tweepy is limited in the replies it pulls. I expect that most of the tweets here will be neutral as they are often coming from government officials or websites distributing news. I could possibly gain more of the public's perspective if I catch more replies.\n* Evaluating how the different preprocessing steps influence sentiment\n* Possibly develop a sentiment classifier. Off-the-shelf classfiers such as TextBlob do not seem to perform as well in cases such as described above with any general case announcements or first death announcements where words like \"positive\", \"confirmed\", and \"first\" are likely positive in sentiment in other contexts but not in the context of NPIs and COVID."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}