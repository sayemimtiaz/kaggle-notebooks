{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1><center>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</center></h1>\n<h1 style=\"font-family:verdana;\"> <center> Data Science - Final Assignment </center> </h1>\n<h1><center>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</center></h1>"},{"metadata":{},"cell_type":"markdown","source":"<center>\n<img src = \"https://comicvine1.cbsistatic.com/uploads/original/11135/111350799/6404749-4102173126-144401.jpeg\"></img>\n</center>"},{"metadata":{},"cell_type":"markdown","source":"<h1> Part I: <u>  <center>  Business & Data Understanding  </center> </u> </h1>"},{"metadata":{},"cell_type":"markdown","source":"## Introduction\nI present you my final assignment in the course \"Data Science\", studied at Ben Gurion University.\n\nFor my assignment I decided to explore and deepen my knowldege with **recommendation systems**, as I was very intrigued about this subject that we learned about in lesson 5, and even though we had a practical exercise back then, I felt like there's more that I want to try and experience for myself.\n\nAs a huge anime fan, I searched for a dataset that could bright my day and what could be a better collection of data other than over 85K user reviews of anime, scraped from the site https://myanimelist.net/, best site there is to all anime fans out there?!?!"},{"metadata":{},"cell_type":"markdown","source":"## Problem - \n\nThe problem we are dealing with today is actually a pretty well known problem - every business wants to recommend their clients products they would like to buy, or in our case - watch!\n\nBy doing so, people will stay longer in their sites, and the buisness will earn more money. \n\nIt's a win-win scenario where everybody's happy.\n\nOf Course, it's no secret that there are **countless** of ways to solve this kind of problem, but I think that by trying tools we didn't actually touched in the course, and by adding my own small improvments, this assignment would be uniqe enough in its own way.\n(I certaintly hope so!)\n\nIn lesson 5's exercise we saw how calculating simlarity score between users can help us predict a recommendation, but in the anime world - new animes are coming out regulary, and we won't have enough information to make a recommendation by using the same tech.\n\nWe should try new methods that we didn't try back then and see how they compare to each other while also using a smart filtering of the data for refining the results! "},{"metadata":{},"cell_type":"markdown","source":"To begin this exploratory analysis, first I import libraries and define functions for plotting the data using `matplotlib`."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade pip","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We currently have 3 files, lets check their names:"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's check the file: \"MAL Anime Reviews 85k.csv\"\n\nThis is the main file containing the most important data for this project.\nI'm also opening a file that gives extra details for the shows\n\n"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"nRowsRead = None # specify 'None' if want to read whole file\n# MAL Anime Top 10000 Details.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows\nanime_df = pd.read_csv('/kaggle/input/MAL Anime Reviews 85k.csv', delimiter=',', nrows = nRowsRead)\nanime_details_df = pd.read_csv('/kaggle/input/MAL Anime Top 10000 Details.csv', delimiter=',', nrows = nRowsRead)\nanime_df.dataframeName = 'MAL Anime Reviews 85k.csv'\nanime_details_df.dataframeName = 'MAL Anime Top 10000 Details.csv'\nnRow, nCol = anime_df.shape\nprint(f'There are {nRow} rows and {nCol} columns')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a quick look at what the data looks like:\nwe can see that we have data about users and their review for anime shows based on different criteria, its seems like a good start.\nlet's take a look on the user counts - users that reviewd more than once are important to create a decent model."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"anime_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_counts = anime_df['Username'].value_counts()\nuser_counts = user_counts.value_counts()\nuser_counts[0:15].plot(kind = 'area', title = 'number of users that rated X animes')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that most of the users in our data only reviewd 1 anime, and the numbers are getting smaller and smaller drastically.\n\nConsidered that there are thousands of different animes in our data, it makes a really poor data to make a personalized recommendation, so we are going to need thinking about a 'cold start' model that will help us give recommmendation for users. "},{"metadata":{},"cell_type":"markdown","source":"lets look at our anime sparse data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"anime_counts = anime_df['Anime Title'].value_counts()\nanime_counts = anime_counts.value_counts()\nanime_counts\nanime_counts[0:15].plot(kind = 'pie', title = 'number of anime that rated X times')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'll filter out users with less than 10 reviews for animes:"},{"metadata":{"trusted":true},"cell_type":"code","source":"counts = anime_df['Username'].value_counts()\nanime_df = anime_df[anime_df['Username'].isin(counts[counts > 10].index)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1> Part II: <u>  <center>  Data Preparation  </center> </u> </h1>"},{"metadata":{},"cell_type":"markdown","source":"Removing columns and duplications:\n\nThese columns are pretty weird. I don't think we need them in this project"},{"metadata":{"trusted":true},"cell_type":"code","source":"anime_df = anime_df.drop_duplicates()\nanime_df = anime_df.drop(['Anime URL','Review Date','Review'],axis = 1)\nanime_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that we have a pretty cool column named \"Episodes Watched\".\n\nIt tells us how many episodes of the anime the user watched before submitting the review.\n\nI don't know much about you, but I think a decent review should come from someone who watched as many episodes as possible from the show.\n(There are many great animes that starts bad but improve a lot with time)\n\nThis data could be great for us, but we can't really use it in its current state, we need to change it."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(anime_df['Episodes Watched'].dtypes)\nprint(anime_df['Episodes Watched'][0])\n\nprint(anime_df.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets build a function that will transform this column to numerical one. \ninstead of a line stating how many episodes the user watched I want to know about it in percents."},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_completion_percent(line):\n    array = [int(s) for s in line.split() if s.isdigit()]\n    if len(array) == 2 and array[1] != 0:\n        return array[0]/array[1]\n    else:\n        return -1\n        \n    \nprint(\"Test: \" + str(show_completion_percent(\"64 of 64 episodes seen\") * 100) + \"%\")\nanime_df['Episodes Watched'] = [show_completion_percent(i) for i in anime_df['Episodes Watched']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets see how many records we are going to lose after cleaning the users that didn't give a show a proper chance:"},{"metadata":{"trusted":true},"cell_type":"code","source":"anime_df.head()\ncount = 0\nfor i in anime_df['Episodes Watched'].values:\n    if i <= 0.5:\n        count += 1\nprint(count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"anime_df = anime_df[anime_df['Episodes Watched'] > 0.5]\nanime_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I want to create a target column of rating which will be the mean value of the columns - Story, Animation, Sound, Character and enjoyment. \n\nThat will be the new \"Overall Rating\" of a user to an anime.\nI really think that the overall rating for a show supposed to be consisted of the other measures.\n\nIt doesn't make sense giving a show a score of 10/10 for story, animation, sound, character and enjoyment and then say that your **overall rating is 0**.\n\nThat would be super silly don't you agree with me?\nI'm pretty sure that there are few records like that in this huge dataset and we don't want any of that misleading information in our data.\n\nAlso, I'm changing possible scores of 0 to be 0.1, because later we are going to use 0 as indication for that the user didn't rate an anime at all"},{"metadata":{"trusted":true},"cell_type":"code","source":"rating_average = anime_df.iloc[:,6:11].mean(axis=1)\nrating_average = rating_average\nfor i in rating_average:\n    if i == 0:\n        i = 0.1\n        \nrating_average\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"anime_df = anime_df.copy()\nanime_df.loc[0:,'Average Overall Rating'] = rating_average\nanime_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are going to rid of anyone who have a difference of |Average Overall Rating - Overall Rating| > 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"anime_df = anime_df[abs(anime_df['Overall Rating'] - anime_df['Average Overall Rating']) <= 2]\nanime_df.drop(['Overall Rating'], axis = 1)\nanime_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, I want to use the \"Wisdom of the crowd\" to create features for anime shows. "},{"metadata":{"trusted":true},"cell_type":"code","source":"anime_features_df = anime_df.groupby(['Anime Title','Anime Rank'])[['Story Rating','Animation Rating','Sound Rating','Character Rating','Enjoyment Rating']].mean()\nanime_features_df['Mean'] = anime_features_df.mean(axis = 1)\nanime_features_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, lets leave the features table aside for a moment and create a matrix table of Users X Anime with the values of their respective Overall score."},{"metadata":{"trusted":true},"cell_type":"code","source":"anime_sparse_table = anime_df.pivot_table(index='Username',columns='Anime Title',values='Average Overall Rating').fillna(0)\nanime_sparse_table","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can see that this table is very **very** sparsed!\nSimple glimpse into that table right now shows no values at all, thats because there are so many animes(=items) and most of the people didn't even watch them. That's an example for a reality challenge.\n\nI'll split the data into train & test,\nour test will be 15% of the table "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(anime_sparse_table, test_size = 0.15)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1> Part III: <u>  <center> Modeling  </center> </u> </h1>"},{"metadata":{},"cell_type":"markdown","source":"# **Collaborative Filtering - Item Based**\n\nIn item based model we are trying to recommend an item to a user based on **similar items** instead of user - simlarity like we have done in lesson 5.\n\nOur data will usually contain many more users than items, so computing a recommendation that way should be faster and somewhat more powerful.\n\nFirst I am calculating the pearson correlation between the columns in the matrix (the animes)\nthat way we can check most similiar animes to a specific anime."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_between_anime = train.corr(method='pearson')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_between_anime","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets check what are the most similiar animes to 'Dragon Ball':"},{"metadata":{},"cell_type":"markdown","source":"<center>\n<img src = \"https://wallpapercave.com/wp/wp6392800.jpg\"></img>\n</center>"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_between_anime['Dragon Ball Z'].sort_values(ascending = False).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it's time to create the prediction function:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def top_k_similiar_anime(anime, k = 5):\n    temp = corr_between_anime[anime].sort_values(ascending = False)\n    top_similiar = []\n    for i in range(1,6):\n        top_similiar.append(temp.index[i])\n    return top_similiar\n\ndef get_anime_user_rated(user,df):\n    anime_user_rated = []\n    count = 0\n    for i in df.loc[user]:\n        if i > 0:\n            anime_user_rated.append(df.loc[user].index[count])\n        count += 1\n    return anime_user_rated\n\ndef is_nan(x):\n    return (x != x)\n        \ndef predict_score(user, anime, df, k = 6):\n    anime_user_rated = get_anime_user_rated(user,df)\n    prediction = None\n    if len(anime_user_rated) >= 6:\n        corr_table = corr_between_anime[anime].loc[anime_user_rated].sort_values(ascending = False)\n        mone = 0\n        mehane = 0\n        for i in range(1,k):\n            mone += corr_table[i] * df[corr_table.index[i]].loc[user]\n            mehane += corr_table[i]\n    \n        prediction = mone / mehane\n    \n    if prediction is None or is_nan(prediction) or prediction < 0:\n        prediction = anime_features_df['Mean'][anime].values[0]    \n    return prediction\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"My prediction function makes a prediction based on the ratings the user gave to top K most similiar animes = (Neareast Neighbors),\nIf there isn't enough information I'm using the global average score of the anime as the prediction - and that would be beneficial for the 'cold start' problem."},{"metadata":{},"cell_type":"markdown","source":"Now, in order to save time I will predict the score for 100 users in the train data, only on animes they did review."},{"metadata":{"trusted":true},"cell_type":"code","source":"hundred_users_with_over_ten_reviews = []\ncount = 0\nfor user in train.index:\n    if count != 100:\n        animes_user_rated = get_anime_user_rated(user,train)\n        if len(animes_user_rated) >= 10:\n            hundred_users_with_over_ten_reviews.append(user)\n            count += 1\n\nhundred_users_with_over_ten_reviews","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"real_scores_train = []\npredicted_scores_train = []\nfor anime in train.columns:\n    for user in hundred_users_with_over_ten_reviews:\n        if train[anime][user] > 0:\n            real_scores_train.append(train[anime][user])\n            pred = predict_score(user,anime,train)\n            predicted_scores_train.append(pred)\n            print('Username: ' + str(user) + ' Rated Anime: ' + str(anime) + ' score: ' + str(train[anime][user]))\n            print('The Predicted Score however is: ' + str(pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Time to check the error margins:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nprint('MSE: = ' + str(mean_squared_error(real_scores_train, predicted_scores_train)))\nprint('RMSE: = ' + str(mean_squared_error(real_scores_train, predicted_scores_train,squared = False)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I'll do the same with the test: (While using the similiarity score of the train)"},{"metadata":{"trusted":true},"cell_type":"code","source":"users_test = []\nfor user in test.index:\n        animes_user_rated = get_anime_user_rated(user,test)\n        if len(animes_user_rated) >= 10:\n            users_test.append(user)\n\nusers_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"real_scores_test = []\npredicted_scores_test = []\nfor anime in test.columns:\n    for user in users_test:\n        if test[anime][user] > 0:\n            real_scores_test.append(test[anime][user])\n            pred = predict_score(user,anime,test)\n            predicted_scores_test.append(pred)\n            print('Username: ' + str(user) + ' Rated Anime: ' + str(anime) + ' score: ' + str(test[anime][user]))\n            print('The Predicted Score however is: ' + str(pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('MSE: = ' + str(mean_squared_error(real_scores_test, predicted_scores_test)))\nprint('RMSE: = ' + str(mean_squared_error(real_scores_test, predicted_scores_test,squared = False)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>  CTR: DeepFM - Model </h1>"},{"metadata":{},"cell_type":"markdown","source":"Time to merge our anime features table from above with a table that contains more features about each anime title in this list."},{"metadata":{"trusted":true},"cell_type":"code","source":"anime_details_df.info()\nfusion_table = pd.merge(anime_details_df,anime_features_df,left_on='Ranking',right_on='Anime Rank')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fusion_table.info()\nfusion_table = fusion_table.iloc[:,[0,12,14,23,24,25,26,27]]\nfusion_table","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"anime_df = anime_df.drop(['Animation Rating','Sound Rating','Character Rating','Enjoyment Rating', 'Story Rating'],axis = 1)\nfusion_table = pd.merge(fusion_table,anime_df,on='Anime Title')\nfusion_table","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fusion_table.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_columns_name = ['Anime_Title', 'Studios', 'Genres', 'Story_Rating', 'Animation_Rating',\n       'Sound_Rating', 'Character_Rating', 'Enjoyment_Rating', 'Anime_Rank',\n       'Username', 'Episodes_Watched', 'Review_Likes', 'Overall_Rating',\n       'Average_Overall_Rating']\nfusion_table.columns = new_columns_name\nfusion_table","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fusion_table[\"Genres\"]\ndummies = fusion_table['Genres'].str.get_dummies(sep=',')\n#dummies\nfinal_fusion_table = pd.concat([fusion_table,dummies],axis=1)\nfinal_fusion_table = final_fusion_table.rename(columns={\"Martial Arts\": \"Martial_Arts\", \"Slice of Life\": \"Slice_of_Life\", \"Super Power\": \"Super_Power\"})\ncolumns = final_fusion_table.columns\n\n#columns.delete(['Average_Overall_Rating','Overall_Rating'])\ncolumns_array = []\nfor i in columns:\n    if i[0] != ' ':\n        columns_array.append(i)\ncolumns_array.remove('Average_Overall_Rating')\ncolumns_array.remove('Overall_Rating')\ncolumns_array.remove('Studios')\ncolumns_array.remove('Genres')\ncolumns_array\ngenres = columns_array[10:]\ncolumns_array\n\n#fusion_table = pd.get_dummies(fusion_table, columns = ['Studios'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install git+https://github.com/shenweichen/DeepCTR","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom deepctr.models import DeepFM\nfrom deepctr.feature_column import SparseFeat,get_feature_names\n\ndata = final_fusion_table\nsparse_features = columns_array\ntarget = ['Average_Overall_Rating']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Label Encoding for sparse features,and do simple Transformation for dense features"},{"metadata":{"trusted":true},"cell_type":"code","source":"for feat in sparse_features:\n    lbe = LabelEncoder()\n    data[feat] = lbe.fit_transform(data[feat])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. count #unique features for each sparse field"},{"metadata":{"trusted":true},"cell_type":"code","source":"fixlen_feature_columns = [SparseFeat(feat, data[feat].nunique(),embedding_dim=4)\n                          for feat in sparse_features]\nlinear_feature_columns = fixlen_feature_columns\ndnn_feature_columns = fixlen_feature_columns\nfeature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. splitting the data to train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = train_test_split(data, test_size=0.2, random_state=2020)\ntrain_model_input = {name:train[name].values for name in feature_names}\ntest_model_input = {name:test[name].values for name in feature_names}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 4. Define Model,train, and predict\nTime to see the DeepFM model in action:"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = DeepFM(linear_feature_columns, dnn_feature_columns, task='regression')\nmodel.compile(\"adam\", \"mse\", metrics=['mse'], )\n\nhistory = model.fit(train_model_input, train[target].values,\n                    batch_size=256, epochs=10, verbose=2, validation_split=0.2, )\npred_ans = model.predict(test_model_input, batch_size=256)\npred_ans","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1> Part IV: <u>  <center> Evaluation [MSE]  </center> </u> </h1>"},{"metadata":{},"cell_type":"markdown","source":"we can use the deepFM model MSE calculator to evaluate the Model."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"test MSE\", round(mean_squared_error(test[target].values, pred_ans), 4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\nToday, we looked into 2 more ways to build a recommender system.\nwe have seen that real life data can be very poor and challenging to work with and that's why we need models that can hybrid between static info of animes and the personalized data of the users.\n\nIn terms of evaluation it seems like both models did pretty good job with predicting user scores for anime,\nit seems like the DeepFM has the upperhand however, it used much more data in the calculation combining factorization machine (the ratings of the users and their seprated features)."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}