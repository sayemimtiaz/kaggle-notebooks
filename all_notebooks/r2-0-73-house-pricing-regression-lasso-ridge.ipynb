{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\n\n### Approach\n- **Models**: Linear regression, Lasso and Ridge methods\n- **Feature Selection**: Based on correlation with target feature and p-value<=0.05\n\n### Definitions of the Variables\n- **id** - Unique ID for each home sold\n- **date** - Date of the home sale\n- **price** - Price of each home sold\n- **bedrooms** - Number of bedrooms\n- **bathrooms** - Number of bathrooms, where .5 accounts for a room with a toilet but no shower\n- **sqft_living** - Square footage of the apartments interior living space\n- **sqft_lot** - Square footage of the land space\n- **floors** - Number of floors\n- **waterfront** - A dummy variable for whether the apartment was overlooking the waterfront or not\n- **view** - An index from 0 to 4 of how good the view of the property was\n- **condition** - An index from 1 to 5 on the condition of the apartment,\n- **grade** - An index from 1 to 13, where 1-3 falls short of building construction and design, 7 has an average level of construction and design, and 11-13 have a high quality level of construction and design.\n- **sqft_above** - The square footage of the interior housing space that is above ground level\n- **sqft_basement** - The square footage of the interior housing space that is below ground level\n- **yr_built** - The year the house was initially built\n- **yr_renovated** - The year of the houseâ€™s last renovation\n- **zipcode** - What zipcode area the house is in\n- **lat** - Lattitude\n- **long** - Longitude\n- **sqft_living15** - The square footage of interior housing living space for the nearest 15 neighbors\n- **sqft_lot15** - The square footage of the land lots of the nearest 15 neighbors\n\n*Thanks to user Nova19 for the column definitions; https://www.kaggle.com/harlfoxem/housesalesprediction/discussion/207885*","metadata":{}},{"cell_type":"markdown","source":"## Package & Data Imports","metadata":{}},{"cell_type":"code","source":"# Programming\nimport pandas as pd\nimport numpy as np\nimport warnings\n\n# Modeling\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import Pipeline\nfrom scipy import stats\n\n# Visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing Dataset\ndf = pd.read_csv('/kaggle/input/housesalesprediction/kc_house_data.csv').drop('id', axis=1)\npd.set_option('display.max_columns', None)\n\n# Settings\npd.set_option('display.max_columns', None)\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA DataFrame\n- No missing values, no inputing step will be needed on the pipelines I'll create down the line\n- Mix of categorical and continuous variables","metadata":{}},{"cell_type":"code","source":"print('# Observations: {}'.format(df.shape[0]))\nprint('# Variables: {}'.format(df.shape[1]))\nprint('')\nprint(df.info())\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA Continuous Variables\n- Continuous features selected based on ***Pearson Correlation Coefficients (>=0.3)*** and ***p-values (<0.05)***:   ['sqft_living', 'sqft_above', 'sqft_basement', 'sqft_living15', 'lat']\n- p-values of selected features being 0.0 provide a high degree of confidence that the selected features are significantly correlated with price","metadata":{}},{"cell_type":"code","source":"# Creating a Cont Df\ncont_var = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'sqft_living15', 'sqft_lot15', 'zipcode', 'lat', 'long', 'price']\ndf_cont = df[cont_var]\n\n# Correlation Heatmap\nsns.set(rc={'figure.figsize':(20,10)})\nsns.heatmap(df_cont.corr(), vmin=-1, vmax=1, cmap=\"Spectral\", annot=True)\nplt.show()\nplt.close()\n\n# Pearson Corr Coe & p-Values\ndef pearsoncorr_pval(feature, target):\n    '''Computes the Pearson Coefficient and p-value of a given pair of arrays'''\n    '''Funtion requires: from scipy import stats'''\n    pearson_coeff, p_value = stats.pearsonr(feature, target)\n    return pearson_coeff, p_value\n# Printing the results\nprint('')\nsqft_liv = pearsoncorr_pval(df.sqft_living, df.price)\nprint('sqft_living | Pearson Coeff: {} |'.format(sqft_liv[0]), 'p-value: {}'.format(sqft_liv[1]))\nsq_abo = pearsoncorr_pval(df.sqft_above, df.price)\nprint('sqft_above | Pearson Coeff: {} |'.format(sq_abo[0]), 'p-value: {}'.format(sq_abo[1]))\nsq_base = pearsoncorr_pval(df.sqft_basement, df.price)\nprint('sqft_basement | Pearson Coeff: {} |'.format(sq_base[0]), 'p-value: {}'.format(sq_base[1]))\nsq_liv15 = pearsoncorr_pval(df.sqft_living15, df.price)\nprint('sqft_living15 | Pearson Coeff: {} |'.format(sq_liv15[0]), 'p-value: {}'.format(sq_liv15[1]))\nla = pearsoncorr_pval(df.lat, df.price)\nprint('lat | Pearson Coeff: {} |'.format(la[0]), 'p-value: {}'.format(la[1]))\nprint('')\n\n# Final Continuous Variables list\ncont_var_selected = ['sqft_living', 'sqft_above', 'sqft_living15', 'sqft_basement', 'lat']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA Categorical Variables\n- Categorical features selected:   ['bedrooms', 'bathrooms', 'waterfront', 'view', 'grade']","metadata":{}},{"cell_type":"code","source":"cat_var = ['bedrooms', 'bathrooms', 'floors', 'waterfront', 'view', 'condition', 'grade', 'yr_built', 'yr_renovated', 'price']\ndf_cat = df[cat_var]\n\n# Pairplot\nsns.set(rc={'figure.figsize':(20,10)})\nsns.pairplot(df_cat)\nplt.show()\nplt.close()\n\n# Boxplots\nfor features in cat_var:\n    sns.set(rc={'figure.figsize':(20,10)})\n    sns.boxplot(x=features, y='price', data=df_cat)\n    plt.show()\n    plt.close()\n    \n# Final categorical variables list\ncat_var_selected = ['bedrooms', 'bathrooms', 'waterfront', 'view', 'grade']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model\n\n#### Model/Hold-Out and Training/Test Splits\n- Step 0: Select only the desired variables based on EDA\n- Step 1: Arrange all features and labels as numpy arrays\n- Step 2: Create *Model* and *Hold-Out* sets. Having a *Hold-Out* set (data never seen by the model) will ensure the best test on model performance on unseen data","metadata":{}},{"cell_type":"code","source":"# Features based on EDA\nfeatures_selected = cont_var_selected + cat_var_selected\n\n# Features/Target in numpy arrays\nfeatures = df[features_selected].values\ntarget = df.price.values\n    # Model/Hold-out sets\nmodel_features, holdout_features, model_target, holdout_target = \\\n    train_test_split(features, target, test_size=0.2, random_state=11)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Lasso Regression\n- Step 0: Create a pipeline that scales the data, adds polynomial features (helps explaining non-linear relationships in a linear method) and runs the desired model\n- Step 1: Use GridSearchCV to tune the hyperparamter alpha and apply cross-validation (5-fold)\n- Step 2: Obtain scores for training (model) and test (holdout) data and plot the results","metadata":{}},{"cell_type":"code","source":"# Pipeline for Lasso\npipe_steps = [('Scaling', StandardScaler()), ('Polynomial Features', PolynomialFeatures(include_bias=False)), ('Lasso Linear Regression', Lasso())]\npipeline = Pipeline(pipe_steps)\n# Tuning\n    # Used print(pipeline.get_params().keys()) to get the name for the parameter to tune\n    # Started testing the model with the following list of options to quickly check the ballpark of the optimal hyperparamter -> [0.0001, 0.001, 0.01, 0.1, 0, 1, 10, 100, 1000, 10000]\nparam_grid = {'Lasso Linear Regression__alpha': np.arange(1000,1500, step=10)}\nlasso_grid = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, scoring='r2')\n# Fitting\nlasso_grid.fit(model_features, model_target)\nalpha = lasso_grid.best_params_\nr2 = lasso_grid.best_score_ \nr2_unseen = lasso_grid.score(holdout_features, holdout_target)\nprediction = lasso_grid.predict(holdout_features)\n# Priniting Results\nprint('LASSO PERFORMANCE')\nprint('Optimal Hyperparameter Alpha: {}'.format(alpha['Lasso Linear Regression__alpha']))\nprint('r2 Score on Training Data: {}'.format(r2.round(5)))\nprint('r2 Score on Hold-Out Data: {}'.format(r2_unseen.round(5)))\n#Plotting Results\nsns.set(rc={'figure.figsize':(10,5)})\n_ = sns.kdeplot(holdout_target)\n_ = sns.kdeplot(prediction)\n_.legend(['target', 'prediction'])\nplt.show()\nplt.close()\n# Deleting Variables\ndel pipe_steps, pipeline, param_grid, alpha, r2, r2_unseen, prediction","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Ridge Regression\n- Step 0: Create a pipeline that scales the data, adds polynomial features (helps explaining non-linear relationships in a linear method) and runs the desired model\n- Step 1: Use GridSearchCV to tune the hyperparamter alpha and apply cross-validation (5-fold)\n- Step 2: Obtain scores for training (model) and test (holdout) data and plot the results","metadata":{}},{"cell_type":"code","source":"# Pipeline for Ridge\npipe_steps = [('Scaling', StandardScaler()), ('Polynomial Features', PolynomialFeatures(include_bias=False)), ('Ridge Linear Regression', Ridge())]\npipeline = Pipeline(pipe_steps)\n# Tuning\n    # Used print(pipeline.get_params().keys()) to get the name for the parameter to tune\n    # Started testing the model with the following list of options to quickly check the ballpark of the optimal hyperparamter -> [0.0001, 0.001, 0.01, 0.1, 0, 1, 10, 100, 1000, 10000]\nparam_grid = {'Ridge Linear Regression__alpha': np.arange(1000,1500, step=10)}\nridge_grid = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, scoring='r2')\n# Fitting\nridge_grid.fit(model_features, model_target)\nalpha = ridge_grid.best_params_\nr2 = ridge_grid.best_score_\nr2_unseen = ridge_grid.score(holdout_features, holdout_target)\nprediction = ridge_grid.predict(holdout_features)\n# Priniting Results\nprint('RIDGE PERFORMANCE')\nprint('Optimal Hyperparameter Alpha: {}'.format(alpha['Ridge Linear Regression__alpha']))\nprint('r2 Score on Training Data: {}'.format(r2.round(5)))\nprint('r2 Score on Hold-Out Data: {}'.format(r2_unseen.round(5)))\n#Plotting Results\nsns.set(rc={'figure.figsize':(10,5)})\n_ = sns.kdeplot(holdout_target)\n_ = sns.kdeplot(prediction)\n_.legend(['target', 'prediction'])\nplt.show()\nplt.close()\n# Deleting Variables\ndel pipe_steps, pipeline, param_grid, alpha, r2, r2_unseen, prediction","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}