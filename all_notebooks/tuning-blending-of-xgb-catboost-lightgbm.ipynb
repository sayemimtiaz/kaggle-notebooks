{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Refrences: \n1. https://www.kaggle.com/abhishek/code : Optimization, Blending etc\n2. https://www.kaggle.com/kingoffitpredict/time-optimization#Featuretools : Feature Tools","metadata":{}},{"cell_type":"code","source":"# Data Preprocessing and Model Building Libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import OrdinalEncoder\nimport lightgbm as lgb\nimport optuna\nfrom catboost import CatBoostRegressor\n\n# Libraries for Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.rcParams['figure.figsize'] = (16, 8)\nplt.style.use('fivethirtyeight')\n\n# Fearture Tools: For Feature Engineering\nimport featuretools as ft\nimport optuna\nfrom optuna.visualization import plot_optimization_history, plot_param_importances\n\n# For Not displaying Warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-01T09:56:09.497887Z","iopub.execute_input":"2021-09-01T09:56:09.498217Z","iopub.status.idle":"2021-09-01T09:56:13.042326Z","shell.execute_reply.started":"2021-09-01T09:56:09.498186Z","shell.execute_reply":"2021-09-01T09:56:13.041496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/30days-folds/train_folds.csv\")\ndf_test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\nuseful_features = [c for c in df_train.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\nnumerical_cols = [col for col in useful_features if col.startswith(\"cont\")]\ndf_test = df_test[useful_features]\n\nf, ax = plt.subplots(nrows=4, ncols=4, figsize=(12, 12))\nf.suptitle('Distribution of Numerical Features', fontsize=16)\nsns.distplot(df_train['cont0'], ax=ax[0, 0])\nsns.distplot(df_train['cont1'], ax=ax[0, 1])\nsns.distplot(df_train['cont2'], ax=ax[0, 2])\nsns.distplot(df_train['cont3'], ax=ax[0, 3])\n\nsns.distplot(df_train['cont4'], ax=ax[1, 0])\nsns.distplot(df_train['cont5'], ax=ax[1, 1])\nsns.distplot(df_train['cont6'], ax=ax[1, 2])\nsns.distplot(df_train['cont7'], ax=ax[1, 3])\n\nsns.distplot(df_train['cont8'], ax=ax[2, 0])\nsns.distplot(df_train['cont9'], ax=ax[2, 1])\nsns.distplot(df_train['cont10'], ax=ax[2, 2])\nsns.distplot(df_train['cont11'], ax=ax[2, 3])\n\nsns.distplot(df_train['cont12'], ax=ax[3, 0])\nsns.distplot(df_train['cont13'], ax=ax[3, 1])\nf.delaxes(ax[3, 2])\nf.delaxes(ax[3, 3])\nplt.tight_layout()\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-09-01T09:56:13.043862Z","iopub.execute_input":"2021-09-01T09:56:13.0442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(nrows=3, ncols=3, figsize=(12, 12))\nf.suptitle('Distribution of Categorical Features', fontsize=16)\nsns.countplot(df_train['cat0'], ax=ax[0, 0])\nsns.countplot(df_train['cat1'], ax=ax[0, 1])\nsns.countplot(df_train['cat2'], ax=ax[0, 2])\nsns.countplot(df_train['cat3'], ax=ax[1, 0])\nsns.countplot(df_train['cat4'], ax=ax[1, 1])\nsns.countplot(df_train['cat5'], ax=ax[1, 2])\nsns.countplot(df_train['cat6'], ax=ax[2, 0])\nsns.countplot(df_train['cat7'], ax=ax[2, 1])\nsns.countplot(df_train['cat8'], ax=ax[2, 2])\n\nplt.tight_layout()\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-09-01T09:55:58.089429Z","iopub.status.idle":"2021-09-01T09:55:58.090138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/30days-folds/train_folds.csv\")\ndf_test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\nnumerical_cols = [col for col in useful_features if col.startswith(\"cont\")]\ndf_test = df_test[useful_features]\n\n######## For Training set\n# initial set up of FeatureTool\nes = ft.EntitySet(id = 'data')\nes.entity_from_dataframe(entity_id = 'original_train_data', \n                         dataframe = df[numerical_cols], \n                         index='id') \n\n#creating separate dataframe with feature Engineered columns\nfeature_matrix_train, feature_defs_train = ft.dfs(entityset = es,                                          \n                                      target_entity = 'original_train_data',                               \n                                      trans_primitives = ['add_numeric', 'multiply_numeric'],  \n                                      verbose=1)  \n\n# adding new features into original dataframe\nfor i in feature_matrix_train.iloc[:,14:].columns:\n    df[i] = feature_matrix_train[i]\n\n\n######## Using same technique on Test Set\n\n# initial set up of FeatureTool\nes = ft.EntitySet(id = 'data')\nes.entity_from_dataframe(entity_id = 'original_test_data', \n                         dataframe = df_test[numerical_cols], \n                         index='id') \n\n#creating separate dataframe with feature Engineered columns\nfeature_matrix_test, feature_defs_test = ft.dfs(entityset = es,                                          \n                                      target_entity = 'original_test_data',                               \n                                      trans_primitives = ['add_numeric', 'multiply_numeric'],  \n                                      verbose=1)  \n\n# adding new features into original dataframe\nfor i in feature_matrix_test.iloc[:,14:].columns:\n    df_test[i] = feature_matrix_test[i]\n\n#adding new features to numerical columns list\nnumerical_cols = list(feature_matrix_train.columns)\nuseful_features = object_cols + numerical_cols\n\n# Model 1: XGB using FeatureTool\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])    \n\n\n    model = XGBRegressor(\n        random_state=fold,\n        tree_method=\"gpu_hist\",\n        gpu_id=0,\n        predictor=\"gpu_predictor\",\n        n_estimators=11000,\n        learning_rate=0.03875678489649957,\n        reg_lambda=0.000899770960119882,\n        reg_alpha=0.00026578249068599785,\n        subsample=0.9532248864902615,\n        colsample_bytree=0.16561394428070153,\n        max_depth=3,\n    )\n    \n    \n    model.fit(xtrain, ytrain)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_XGB\"]\nfinal_valid_predictions.to_csv(\"train_pred_XGB.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_XGB\"]\nsample_submission.to_csv(\"test_pred_XGB.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T09:55:58.091321Z","iopub.status.idle":"2021-09-01T09:55:58.091928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model 2: Light GBM**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/30days-folds/train_folds.csv\")\ndf_test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if col.startswith(\"cat\")]\ndf_test = df_test[useful_features]\n\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    param = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        \"n_estimators\": 10000,\n        \"early_stopping_round\": 300,\n        \"device\": \"gpu\",\n        \"gpu_platform_id\": 0,\n        \"gpu_device_id\": 0,\n    }\n    \n    param2 = {\n        'lambda_l1': 0.00472279780583036, \n        'lambda_l2': 2.9095205689488508e-05, \n        'num_leaves': 158, \n        'feature_fraction': 0.7386878356648194, \n        'bagging_fraction': 0.8459744550725283, \n        'bagging_freq': 2, \n        'max_depth': 2, \n        'max_bin': 249, \n        'learning_rate': 0.044738463593017294,\n        'min_child_samples': 13\n    }\n    param.update(param2)\n    \n    lgb_train = lgb.Dataset(xtrain, ytrain)\n    lgb_valid = lgb.Dataset(xvalid, yvalid, reference=lgb_train)\n\n    model = lgb.train(param, lgb_train, valid_sets=[lgb_valid], verbose_eval=1000)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n    \n    \nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_LGB\"]\nfinal_valid_predictions.to_csv(\"train_pred_LGB.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_LGB\"]\nsample_submission.to_csv(\"test_pred_LGB.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T09:55:58.093215Z","iopub.status.idle":"2021-09-01T09:55:58.093827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model 3: XGB + Target Encoding**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/30days-folds/train_folds.csv\")\ndf_test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if col.startswith(\"cat\")]\ndf_test = df_test[useful_features]\n\nfor col in object_cols:\n    temp_df = []\n    temp_test_feat = None\n    for fold in range(5):\n        xtrain =  df[df.kfold != fold].reset_index(drop=True)\n        xvalid = df[df.kfold == fold].reset_index(drop=True)\n        feat = xtrain.groupby(col)[\"target\"].agg(\"mean\")\n        feat = feat.to_dict()\n        xvalid.loc[:, f\"tar_enc_{col}\"] = xvalid[col].map(feat)\n        temp_df.append(xvalid)\n        if temp_test_feat is None:\n            temp_test_feat = df_test[col].map(feat)\n        else:\n            temp_test_feat += df_test[col].map(feat)\n    \n    temp_test_feat /= 5\n    df_test.loc[:, f\"tar_enc_{col}\"] = temp_test_feat\n    df = pd.concat(temp_df)\n    \n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if col.startswith(\"cat\")]\ndf_test = df_test[useful_features]\n\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    params = {'learning_rate': 0.07853392035787837, 'reg_lambda': 1.7549293092194938e-05,\n              'reg_alpha': 14.68267919457715, 'subsample': 0.8031450486786944, 'colsample_bytree': 0.170759104940733, \n              'max_depth': 3}\n    \n    model = XGBRegressor(\n        random_state=0, \n        tree_method='gpu_hist',\n        gpu_id=0,\n        predictor=\"gpu_predictor\",\n        n_estimators=5000,\n        **params\n    )\n    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_XGB1\"]\nfinal_valid_predictions.to_csv(\"train_pred_XGB1.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_XGB1\"]\nsample_submission.to_csv(\"test_pred_XGB1.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T09:55:58.094965Z","iopub.status.idle":"2021-09-01T09:55:58.095528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **XGB3: Ordinal Encoding**\n\n","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/30days-folds/train_folds.csv\")\ndf_test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\ndf_test = df_test[useful_features]\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    params = {\n        'random_state': 1, \n        'booster': 'gbtree',\n        'n_estimators': 10000,\n        'learning_rate': 0.03628302216953097,\n        'reg_lambda': 0.0008746338866473539,\n        'reg_alpha': 23.13181079976304,\n        'subsample': 0.7875490025178415,\n        'colsample_bytree': 0.11807135201147481,\n        'max_depth': 3\n    }\n    \n    model = XGBRegressor(\n        n_jobs=-1,\n        **params\n    )\n    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_XGB3\"]\nfinal_valid_predictions.to_csv(\"train_pred_XGB3.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_XGB3\"]\nsample_submission.to_csv(\"test_pred_XGB3.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T09:55:58.160822Z","iopub.execute_input":"2021-09-01T09:55:58.161063Z","iopub.status.idle":"2021-09-01T09:55:58.1842Z","shell.execute_reply.started":"2021-09-01T09:55:58.161039Z","shell.execute_reply":"2021-09-01T09:55:58.182292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **CatBoost using Target Encoding**\n### **Model Tuning**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/30days-folds/train_folds.csv\")\ndf_test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\ndf_test = df_test[useful_features]  \n\nfor col in object_cols:\n    temp_df = []\n    temp_test_feat = None\n    for fold in range(5):\n        xtrain =  df[df.kfold != fold].reset_index(drop=True)\n        xvalid = df[df.kfold == fold].reset_index(drop=True)\n        feat = xtrain.groupby(col)[\"target\"].agg(\"mean\")\n        feat = feat.to_dict()\n        xvalid.loc[:, f\"tar_enc_{col}\"] = xvalid[col].map(feat)\n        temp_df.append(xvalid)\n        if temp_test_feat is None:\n            temp_test_feat = df_test[col].map(feat)\n        else:\n            temp_test_feat += df_test[col].map(feat)\n    \n    temp_test_feat /= 5\n    df_test.loc[:, f\"tar_enc_{col}\"] = temp_test_feat\n    df = pd.concat(temp_df)\n    \n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if col.startswith(\"cat\")]\ndf_test = df_test[useful_features]\n    \ndef run(trial):\n    for fold in range(5):\n        xtrain =  df[df.kfold != fold].reset_index(drop=True)\n        xvalid = df[df.kfold == fold].reset_index(drop=True)\n        xtest = df_test.copy()\n\n        ytrain = xtrain['target']\n        yvalid = xvalid['target']\n\n        xtrain = xtrain[useful_features]\n        xvalid = xvalid[useful_features]\n        \n        ordinal_encoder = preprocessing.OrdinalEncoder()\n        xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n        xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n        \n        \n        cat_parameters_1 = {'iterations':trial.suggest_int(\"iterations\", 5000, 8000),\n             'learning_rate':trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True),\n                            'l2_leaf_reg':trial.suggest_int(\"l2_leaf_reg\", 5, 200),\n             'random_strength':trial.suggest_float(\"random_strength\", 0.1, 5),'grow_policy':'Depthwise',\n                        'leaf_estimation_method':'Newton', 'od_type':'Iter',\n             'bootstrap_type':'Bayesian','thread_count':-1,'verbose':False,'loss_function':'RMSE','eval_metric':'RMSE'}\n        \n        \n        \n        \n        model = CatBoostRegressor(**cat_parameters_1, task_type = 'GPU')\n        model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n        \n        preds_valid = model.predict(xvalid)\n        rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n        \n    return rmse\n\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(run, n_trials=10)\n\nstudy.best_params","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-09-01T09:55:58.185241Z","iopub.status.idle":"2021-09-01T09:55:58.185629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting Important Parameters\nplot_param_importances(study)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T09:55:58.187027Z","iopub.status.idle":"2021-09-01T09:55:58.187878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting history of Study\nplot_optimization_history(study)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T09:55:58.189061Z","iopub.status.idle":"2021-09-01T09:55:58.189916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Model building using tuned Parameters**","metadata":{}},{"cell_type":"code","source":"# CAtBoost\n\ndf = pd.read_csv(\"../input/30days-folds/train_folds.csv\")\ndf_test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\nnumerical_cols = [col for col in useful_features if col.startswith(\"cont\")]\ndf_test = df_test[useful_features]\n\n\nfor col in object_cols:\n    temp_df = []\n    temp_test_feat = None\n    for fold in range(5):\n        xtrain =  df[df.kfold != fold].reset_index(drop=True)\n        xvalid = df[df.kfold == fold].reset_index(drop=True)\n        feat = xtrain.groupby(col)[\"target\"].agg(\"mean\")\n        feat = feat.to_dict()\n        xvalid.loc[:, f\"tar_enc_{col}\"] = xvalid[col].map(feat)\n        temp_df.append(xvalid)\n        if temp_test_feat is None:\n            temp_test_feat = df_test[col].map(feat)\n        else:\n            temp_test_feat += df_test[col].map(feat)\n    \n    temp_test_feat /= 5\n    df_test.loc[:, f\"tar_enc_{col}\"] = temp_test_feat\n    df = pd.concat(temp_df)\n    \n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if col.startswith(\"cat\")]\ndf_test = df_test[useful_features]\n\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    \n    params = {'od_type':'Iter','iterations': 5376,\n 'learning_rate': 0.023320327820924816,\n 'l2_leaf_reg': 181,\n 'random_strength': 3.391745124598448,'grow_policy':'Lossguide',\n                        'leaf_estimation_method':'Newton', \n             'bootstrap_type':'Bayesian','thread_count':-1,'verbose':False,'loss_function':'RMSE','eval_metric':'RMSE'}\n\n    \n    model = CatBoostRegressor(**params, task_type = 'GPU')\n    model.fit(xtrain, ytrain, verbose =500)\n    \n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_CatB\"]\nfinal_valid_predictions.to_csv(\"train_pred_CatB.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_CatB\"]\nsample_submission.to_csv(\"test_pred_CatB.csv\", index=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-01T09:55:58.191109Z","iopub.status.idle":"2021-09-01T09:55:58.191935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Blending All Models**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/30days-folds/train_folds.csv\")\ndf_test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\ndf1 = pd.read_csv(\"./train_pred_XGB.csv\")\ndf2 = pd.read_csv(\"./train_pred_LGB.csv\")\ndf3 = pd.read_csv(\"./train_pred_XGB1.csv\")\ndf4 = pd.read_csv(\"./train_pred_XGB3.csv\")\ndf5 = pd.read_csv(\"./train_pred_CatB.csv\")\n\n\ndf_test1 = pd.read_csv(\"./test_pred_XGB.csv\")\ndf_test2 = pd.read_csv(\"./test_pred_LGB.csv\")\ndf_test3 = pd.read_csv(\"./test_pred_XGB1.csv\")\ndf_test4 = pd.read_csv(\"./test_pred_XGB3.csv\")\ndf_test5 = pd.read_csv(\"./test_pred_CatB.csv\")\n\ndf = df.merge(df1, on=\"id\", how=\"left\")\ndf = df.merge(df2, on=\"id\", how=\"left\")\ndf = df.merge(df3, on=\"id\", how=\"left\")\ndf = df.merge(df4, on=\"id\", how=\"left\")\ndf = df.merge(df5, on=\"id\", how=\"left\")\n\ndf_test = df_test.merge(df_test1, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test2, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test3, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test4, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test5, on=\"id\", how=\"left\")\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T09:55:58.193227Z","iopub.status.idle":"2021-09-01T09:55:58.19397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Final Model 1: Tuning XGB**","metadata":{}},{"cell_type":"code","source":"useful_features = [\"pred_XGB\",\"pred_XGB3\", \"pred_XGB1\", \"pred_LGB\",\"pred_CatB\"] \n    \ndef run(trial):\n    for fold in range(5):\n        xtrain =  df[df.kfold != fold].reset_index(drop=True)\n        xvalid = df[df.kfold == fold].reset_index(drop=True)\n        xtest = df_test.copy()\n\n        ytrain = xtrain['target']\n        yvalid = xvalid['target']\n\n        xtrain = xtrain[useful_features]\n        xvalid = xvalid[useful_features]\n        \n        \n        learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n        reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0)\n        reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0)\n        subsample = trial.suggest_float(\"subsample\", 0.1, 1.0)\n        colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n        max_depth = trial.suggest_int(\"max_depth\", 1, 7)\n        \n        model = XGBRegressor(\n        random_state=fold,\n        tree_method=\"gpu_hist\",\n        gpu_id=1,\n        predictor=\"gpu_predictor\",\n        n_estimators=12000,\n        learning_rate=learning_rate,\n        reg_lambda=reg_lambda,\n        reg_alpha=reg_alpha,\n        subsample=subsample,\n        colsample_bytree=colsample_bytree,\n        max_depth=max_depth)\n        \n        \n        model.fit(xtrain, ytrain, early_stopping_rounds=300,eval_set=[(xvalid, yvalid)],  verbose=1000)\n        \n        preds_valid = model.predict(xvalid)\n        rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n        \n    return rmse\n\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(run, n_trials=10)\n\nstudy.best_params","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-09-01T09:55:58.195123Z","iopub.status.idle":"2021-09-01T09:55:58.195873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting Important Parameters\nplot_param_importances(study)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T09:55:58.196946Z","iopub.status.idle":"2021-09-01T09:55:58.197674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting history of Study\nplot_optimization_history(study)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T09:55:58.392098Z","iopub.execute_input":"2021-09-01T09:55:58.392468Z","iopub.status.idle":"2021-09-01T09:55:58.403635Z","shell.execute_reply.started":"2021-09-01T09:55:58.392399Z","shell.execute_reply":"2021-09-01T09:55:58.402318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Final Model 2: Tuning CatBoost**","metadata":{}},{"cell_type":"code","source":"useful_features = [\"pred_XGB\",\"pred_XGB3\", \"pred_XGB1\", \"pred_LGB\",\"pred_CatB\"]\n   \n    \ndef run(trial):\n    for fold in range(5):\n        xtrain =  df[df.kfold != fold].reset_index(drop=True)\n        xvalid = df[df.kfold == fold].reset_index(drop=True)\n        xtest = df_test.copy()\n\n        ytrain = xtrain['target']\n        yvalid = xvalid['target']\n\n        xtrain = xtrain[useful_features]\n        xvalid = xvalid[useful_features]\n        \n        \n        cat_parameters_1 = {'iterations':trial.suggest_int(\"iterations\", 5000, 8000),\n             'learning_rate':trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True),\n                            'l2_leaf_reg':trial.suggest_int(\"l2_leaf_reg\", 5, 200),\n             'random_strength':trial.suggest_float(\"random_strength\", 0.1, 5),'grow_policy':'Depthwise',\n                        'leaf_estimation_method':'Newton', 'od_type':'Iter',\n             'bootstrap_type':'Bayesian','thread_count':-1,'verbose':False,'loss_function':'RMSE','eval_metric':'RMSE'}\n        \n        \n        \n        \n        model = CatBoostRegressor(**cat_parameters_1)\n        model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n        \n        preds_valid = model.predict(xvalid)\n        rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    return rmse\n\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(run, n_trials=10)\n\nstudy.best_params","metadata":{"scrolled":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-09-01T09:55:59.540244Z","iopub.execute_input":"2021-09-01T09:55:59.54058Z","iopub.status.idle":"2021-09-01T09:55:59.560029Z","shell.execute_reply.started":"2021-09-01T09:55:59.540548Z","shell.execute_reply":"2021-09-01T09:55:59.55868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting Important Parameters\nplot_param_importances(study)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting history of Study\nplot_optimization_history(study)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Final Model XGB**","metadata":{}},{"cell_type":"code","source":"useful_features = [\"pred_XGB\",\"pred_XGB1\", \"pred_XGB3\", \"pred_LGB\"]\ndf_test = df_test[useful_features]\n\n\nfinal_test_predictions = []\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n        \n    model = XGBRegressor(\n        random_state=fold,\n        tree_method=\"gpu_hist\",\n        gpu_id=1,\n        predictor=\"gpu_predictor\",\n        n_estimators=12000,\n        learning_rate=0.024882747879237756,\n        reg_lambda= 0.000827524788778563,\n        reg_alpha=8.633187860723039e-07,\n        subsample=0.16151014613960882,\n        colsample_bytree=0.6057298386505088,\n        max_depth=1)\n        \n    model.fit(xtrain, ytrain, early_stopping_rounds=300,eval_set=[(xvalid, yvalid)],  verbose=1000)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n    \nprint(np.mean(scores), np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2021-09-01T07:16:05.649999Z","iopub.execute_input":"2021-09-01T07:16:05.650363Z","iopub.status.idle":"2021-09-01T07:16:05.721475Z","shell.execute_reply.started":"2021-09-01T07:16:05.650282Z","shell.execute_reply":"2021-09-01T07:16:05.719782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Final Model CatBoost**","metadata":{}},{"cell_type":"code","source":"useful_features = [\"pred_XGB\",\"pred_XGB1\", \"pred_XGB3\", \"pred_LGB\"]\ndf_test = df_test[useful_features]\n\n\nfinal_test_predictions = []\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]    \n    \n    cat_parameters_1 = {'iterations': 6487,\n 'learning_rate': 0.040692021622714805,\n 'l2_leaf_reg': 99,\n 'random_strength': 2.0689175540262017,'grow_policy':'Depthwise',\n                    'leaf_estimation_method':'Newton', 'od_type':'Iter',\n         'bootstrap_type':'Bayesian','thread_count':-1,'verbose':False,'loss_function':'RMSE','eval_metric':'RMSE'}\n\n    model = CatBoostRegressor(**cat_parameters_1)\n    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n    \nprint(np.mean(scores), np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2021-08-31T13:39:41.226998Z","iopub.execute_input":"2021-08-31T13:39:41.227361Z","iopub.status.idle":"2021-08-31T13:41:24.89285Z","shell.execute_reply.started":"2021-08-31T13:39:41.227331Z","shell.execute_reply":"2021-08-31T13:41:24.891864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Select the best of two**","metadata":{}},{"cell_type":"code","source":"sample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T09:54:03.02543Z","iopub.execute_input":"2021-09-01T09:54:03.025849Z","iopub.status.idle":"2021-09-01T09:54:03.093191Z","shell.execute_reply.started":"2021-09-01T09:54:03.025765Z","shell.execute_reply":"2021-09-01T09:54:03.091688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Thank you**","metadata":{}}]}