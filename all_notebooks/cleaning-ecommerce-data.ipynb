{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nThis notebook will follow my methodology for Exploratory Data Analysis (EDA) and attempt at data cleaning. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno\nimport warnings\nimport datetime\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline    \nprint(\"Dependencies Loaded.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following code is a one size fits all basic EDA template by Jiahao Weng I use to save time. I'll call it later."},{"metadata":{"trusted":true},"cell_type":"code","source":"def time_series_plot(df):\n    \"\"\"Given dataframe, generate times series plot of numeric data by daily, monthly and yearly frequency\"\"\"\n    print(\"\\nTo check time series of numeric data  by daily, monthly and yearly frequency\")\n    if len(df.select_dtypes(include='datetime64').columns)>0:\n        for col in df.select_dtypes(include='datetime64').columns:\n            for p in ['D', 'M', 'Y']:\n                if p=='D':\n                    print(\"Plotting daily data\")\n                elif p=='M':\n                    print(\"Plotting monthly data\")\n                else:\n                    print(\"Plotting yearly data\")\n                for col_num in df.select_dtypes(include=np.number).columns:\n                    __ = df.copy()\n                    __ = __.set_index(col)\n                    __T = __.resample(p).sum()\n                    ax = __T[[col_num]].plot()\n                    ax.set_ylim(bottom=0)\n                    ax.get_yaxis().set_major_formatter(\n                    matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n                    plt.show()\n\n                    \ndef numeric_eda(df, hue=None):\n    \"\"\"Given dataframe, generate EDA of numeric data\"\"\"\n    print(\"\\nTo check: \\nDistribution of numeric data\")\n    display(df.describe().T)\n    columns = df.select_dtypes(include=np.number).columns\n    figure = plt.figure(figsize=(20, 10))\n    figure.add_subplot(1, len(columns), 1)\n    for index, col in enumerate(columns):\n        if index > 0:\n            figure.add_subplot(1, len(columns), index + 1)\n        sns.boxplot(y=col, data=df, boxprops={'facecolor': 'None'})\n    figure.tight_layout()\n    plt.show()\n    \n    if len(df.select_dtypes(include='category').columns) > 0:\n        for col_num in df.select_dtypes(include=np.number).columns:\n            for col in df.select_dtypes(include='category').columns:\n                fig = sns.catplot(x=col, y=col_num, kind='violin', data=df, height=5, aspect=2)\n                fig.set_xticklabels(rotation=90)\n                plt.show()\n    \n    # Plot the pairwise joint distributions\n    print(\"\\nTo check pairwise joint distribution of numeric data\")\n    if hue==None:\n        sns.pairplot(df.select_dtypes(include=np.number))\n    else:\n        sns.pairplot(df.select_dtypes(include=np.number).join(df[[hue]]), hue=hue)\n    plt.show()\n\n\ndef top5(df):\n    \"\"\"Given dataframe, generate top 5 unique values for non-numeric data\"\"\"\n    columns = df.select_dtypes(include=['object', 'category']).columns\n    for col in columns:\n        print(\"Top 5 unique values of \" + col)\n        print(df[col].value_counts().reset_index().rename(columns={\"index\": col, col: \"Count\"})[\n              :min(5, len(df[col].value_counts()))])\n        print(\" \")\n    \n    \ndef categorical_eda(df, hue=None):\n    \"\"\"Given dataframe, generate EDA of categorical data\"\"\"\n    print(\"\\nTo check: \\nUnique count of non-numeric data\\n\")\n    print(df.select_dtypes(include=['object', 'category']).nunique())\n    top5(df)\n    # Plot count distribution of categorical data\n    for col in df.select_dtypes(include='category').columns:\n        fig = sns.catplot(x=col, kind=\"count\", data=df, hue=hue)\n        fig.set_xticklabels(rotation=90)\n        plt.show()\n    \n\ndef eda(df):\n    \"\"\"Given dataframe, generate exploratory data analysis\"\"\"\n    # check that input is pandas dataframe\n    if type(df) != pd.core.frame.DataFrame:\n        raise TypeError(\"Only pandas dataframe is allowed as input\")\n        \n    # replace field that's entirely space (or empty) with NaN\n    df = df.replace(r'^\\s*$', np.nan, regex=True)\n\n    print(\"Preview of data:\")\n    display(df.head(3))\n\n    print(\"\\nTo check: \\n (1) Total number of entries \\n (2) Column types \\n (3) Any null values\\n\")\n    print(df.info())\n\n    # generate preview of entries with null values\n    if len(df[df.isnull().any(axis=1)] != 0):\n        print(\"\\nPreview of data with null values:\")\n        display(df[df.isnull().any(axis=1)].head(3))\n        missingno.matrix(df)\n        plt.show()\n\n    # generate count statistics of duplicate entries\n    if len(df[df.duplicated()]) > 0:\n        print(\"\\n***Number of duplicated entries: \", len(df[df.duplicated()]))\n        display(df[df.duplicated(keep=False)].sort_values(by=list(df.columns)).head())\n    else:\n        print(\"\\nNo duplicated entries found\")\n\n    # EDA of categorical data\n    categorical_eda(df)\n    \n    # EDA of numeric data\n    numeric_eda(df)\n        \n    # Plot time series plot of numeric data\n    time_series_plot(df)\n    \nprint('Template loaded.')    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading in the data\necommerce_data_path = \"../input/ecommerce-bookings-data/ecommerce_data.csv\"\necom_data = pd.read_csv(ecommerce_data_path)\n\n# Working with a copy to avoid modifying the original\ned = ecom_data.copy()\n\n# Get a quick preview of the data\nprint(ed.head(),ed.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preliminary Data Processing\nBased on the results of the ```.info()``` method, I can see that firstly there are no null fields, so nothing to drop or impute right off the bat (thankfully) and also a few changes will have to be made to get the data read for analysis, namely the dtypes have to be fixed as follows:\n* ```date``` must be changed to ```datetime64```\n* ```product_id``` and ```city_id```must be changed to ```category``` (even though thy appear to be numbers, they are not interpreted in the same way ```orders``` is, as they have fixed finite values are are not meant to be involved in arithmetic calculations. Imagine imputing ```city_id``` and ending up with an ID for a city that doesnt exist.)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correcting datatypes\ned['date'] = ed['date'].astype('datetime64')\ned['product_id'] = ed['product_id'].astype('category')\ned['city_id'] = ed['city_id'].astype('category')\nprint('dtypes updated')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ed.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis\nOkay, dtypes all looking good! Time to begin EDA by calling the function mentioned earlier."},{"metadata":{"trusted":true},"cell_type":"code","source":"eda(ed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cleaning\nThe first and most alarming trend to jump out at me is the presence of outliers in orders, and based on the box graph, they lie as close as ~10,000 and go out as far as 126919, to identify and remove these, I'm going to use the zscore method, this method re-scales and centers the data then looks for points too far away from 0, in this case I'll use the fairly standard threshold of max=3 min=-3."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\n\n# Generates the Z score for each entry in the 'orders' column\nz=np.abs(stats.zscore(ed.orders))\n\n# Print the first 20 Z scores\nprint(z[:20])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Immmediately upon peeking at the first 20 Z scores, 2 obvious outliers are present, but since I can't manually scrape through all 50,000+ rows, I'll single out all the outliers and more importantly, figure out their location as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group together all outliers\noutls = [i for i in z  if i>3 or i<-3]\nprint(outls[:20])\n\n# Find index of outliers\noutls_loc = np.where((z>3) |(z<-3))[0]\nprint(outls_loc[:20])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next step from here is to decide whether to drop or impute the outliers, depending on how many there are."},{"metadata":{"trusted":true},"cell_type":"code","source":"len(outls_loc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1585 outliers pallors in comparison to the behemoth 523684 total entries, so it is safe to drop without worrying about losing vital info."},{"metadata":{"trusted":true},"cell_type":"code","source":"ed = ed.drop(outls_loc)\ned.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA: Round 2\nI'll do less exhaustive, manual EDA pass with the updated DataFrame which should yield more accurate data now for possible further cleaning.\n\nI have 2 main tasks in mind:\n\n1. Get a more detailed idea of the spread on ```orders``` and how it relates to or is affected by the nature of the data.\n2. I'll plot line charts to detect any oddies on the city and product level individually as well as a heatmap to help differenciate regular rising and falling trends from abnormal spikes."},{"metadata":{"trusted":true},"cell_type":"code","source":"ed.orders.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(figsize=(18,7))\nsns.scatterplot(y=ed.orders,x=ed.date,size=1)\nax.set_xlim([datetime.date(2018,7,10),datetime.date(2019,12,16)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Immediately it hops out at me that all the outliers (or a least the vast majority) were concentrated over late August 2018 to around th beginning of October with the exception of 2 days. This now raises the concern that I made a mistake previously and some of those weren't outliers but a seasonal spike, in which case would lead to the question of what triggered it, but for now I'll just generate the heatmap for both the edited and unedited data."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,12))\nheatdf = pd.DataFrame({'Orders':ed.orders,'City_ID':ed.city_id,'Date':ed.date})\nresult = heatdf.pivot_table(index='Date',columns='City_ID',values='Orders')\nsns.heatmap(data=result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Now I see that the very high order days I dropped were saturated not only mainly over a single period (July - October 2018), but seem to be almost exculsivle concentrated in city 15.\nI'm now not entirely sure that dropping the outliers so broadly was a good idea, perhaps if I lowered the threshold or tried to detect a sesonal trend to remove true outliers. Another point of note is at some point between November 11 and 19 2018, almost half of the cities had a spike, which again may be a recording errorthat needs to be dropped or it can also be as a result of a common stimulus. There is evidently far more to this data than meets the eye, but this is as far as my limited expertise will take me for now. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}