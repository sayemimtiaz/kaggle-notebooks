{"cells":[{"metadata":{},"cell_type":"markdown","source":"# SBERT+BERT for Cord-19 Data\n## Introduction\n\nThe development of **Question Answering** (QA) system is necessary for rapidly emerging domains, such as the ongoing **coronavirus disease of 2019** (COVID-19) pandemic. In particular, when no suitable domain-specific resources are likely available at the starting. \nTo respond to the needs of medical experts to quickly and accurately receive answers to their scientific questions related to coronaviruses, we can develop QA systems based on articles related to COVID-19. Thus, the Kaggle opened this competition named the **COVID-19 Open Research Dataset (CORD-19)** and proposed the **CORD-19** dataset that encompasses 120K articles about coronaviruses and other diseases. The competition offered more than ten tasks to cover some fundamental questions related to COVID 19 and provided the chance for the ML community to develop QA systems and employ them on the CORD-19 dataset.  In this notebook, we have been focused on finding answers for **What is the efficacy of novel therapeutics being tested currently?**. \n\" \n\n## Approaches\n\nTo do so, we implemented two frameworks (SBERT+BERT and LDA+ALBERT) that are based on advanced NLP and ML tools. This notebook is focusing on SBERT+BERT. In another notebook, we are using [LDA+ALBERT](https://www.kaggle.com/parkyoona/lda-albert-for-cord-19-data). \n\n### SBERT+BERT\nThe first framework based on [BERT](https://arxiv.org/abs/1810.04805) model. BERT (Bidirectional Encoder Representations from Transformers) is a well-known language representation model that is designed to pre-train deep bidirectional representations from the unlabeled text by jointly conditioning on both left and right context in all layers. The pre-trained BERT model can be fine-tuned with just adding one additional output layer and can be used for a wide range of tasks, such as question answering and language inference. \nIn this notebook, we are using Sentence-BERT ([SBERT](https://arxiv.org/abs/1908.10084)) as well. SBER is an enhancement of the BERT and is useful in developing a semantic-based sentence embedding.\n\n### LDA+ALBERT\n\nThe second framework is based on [ALBERT](https://arxiv.org/abs/1909.11942). \nALBERT has similar architecture as other BERT models, but it is based on a transformer encoder with Gaussian Error Linear Units (GELU) nonlinearities. ALBERT uses a different embedding method than BERT. In more detail, ALBERT uses two-step word embedding that first projects a word into a lower-dimensional embedding space and then extends it to the hidden space. Furthermore, ALBERT uses a cross-layer parameter sharing to improve parameter efficiency; it only uses feed-forward network (FFN) parameters across layers to share attention parameters. Another difference between ALBER and BERT is that ALBERT uses a sentence-order prediction (SOP) loss to avoid topic prediction and focus on modeling inter-sentence coherence. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nImage('../input/system-diagram/BERT-Based QA System Diagram.jpg')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The QA system diagram is shown in Figure 1. The articles were first filtered using a keyword search which filters out articles that do not contain COVID-19 keywords in the titles. This reduced the number of scholarly articles from 120,464 to 31,450. Once filtered, the top n articles were extracted by embedding the query and the article titles using [Sentence-BERT](https://arxiv.org/abs/1908.10084), measuring the cosine similarity between the query embedding and each article title embedding, and sorting the articles by the cosine similarity score. Only the top n articles with the highest cosine similarity scores were kept. In this experiment, we set n as 100.  We used a pre-trained Sentence-BERT model, ['bert-base-nli-stsb-mean-tokens'](https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/sts-models.md). This model is a BERT-base model with mean-tokens trained on AllNLI and then on STS benchmark training set.\n\nOnce the top articles were extracted, the primary endpoint of each article was extracted using QA BERT. To do this, we set the question as the article title, the context as the article conclusion, and the answer as the primary endpoint of the article. We used a pre-trained QA BERT-large-uncased model with whole word masking fine-tuned on SQuAD, ['bert-large-uncased-whole-word-masking-finetuned-squad'](https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad#). This whole process from input to output took 532.13 seconds.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Figure 1: BERT-based QA System Diagram","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Step 0. Install and Import All Required Libraries ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# turn the internet on for this to install properly\n! pip install -U sentence-transformers\nimport scipy.spatial\nimport numpy as np\nimport os, json\nimport glob\nimport re\nimport torch\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 0-1. Read Input Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.spatial\nimport numpy as np\nimport os, json\nimport glob\nimport re\nimport torch\nimport pandas as pd\n\ndata_dirs = ['../input/CORD-19-research-challenge/document_parses/pmc_json', \n             '../input/CORD-19-research-challenge/document_parses/pdf_json'\n            ]\njson_article_paths = []\nfor data_dir in data_dirs:\n    json_article_paths = json_article_paths + glob.glob(os.path.join(data_dir, \"*.json\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 1. The First Filtering Method\n\nThe Cord-19 dataset encompasses around 120K articles. We use keyword such as keywords **RNA virus, SARS, coronavirus, COVID, SARS-Cov-2, -Co, 2019-nCoV, vaccine, Antibody-Dependent Enhancement, therapeutic, prophylaxis clinical, naproxen, clarithromycin, minocyclinethat** to filter articles that are not related to the the following question:  \n1. What is the efficacy of novel therapeutics being tested currently?\n\nAs the results of our filtering method we find the titles and paper ids of paper that are relevent to our questions. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(json_article_paths))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are initially 120,464 scholarly articles in the CORD-19 dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os, json\n\nif not os.path.exists('../input/filtered-data/filtered_df.csv'):\n    # synonyms to COVID-19 according to wikipedia\n    keywords = ['persistence','decontamination','RNA virus',' SARS','coronavirus', 'COVID', 'SARS-Cov-2', '-CoV', '2019-nCoV','coronavirus vaccine','Antibody-Dependent Enhancement','therapeutic','prophylaxis clinical','naproxen','clarithromycin','minocyclinethat']\n\n    #keywords\n    titles = []\n    paper_ids = []\n    #json_article_paths[0:10000]: #TODO Change to all articles when done\n    for json_file in json_article_paths: \n        # read json file into doc\n        doc = json.load(open(json_file))\n\n        # clean title\n        title = doc['metadata']['title']  \n        title = re.sub(r'[^\\x00-\\x7F]',' ', title)\n\n        # append article only if it contains any of the keywords in its title\n        if title != '' and any(keyword.lower() in title.lower() for keyword in keywords):\n            titles.append(title)\n            paper_ids.append(doc['paper_id'])\n\n    print(len(paper_ids))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After filtering out articles that do not contain COVID-19 keywords in the titles, the number of scholarly articles was reduced from 120,464 to 31.450.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"if os.path.exists('../input/filtered-data/filtered_df.csv'):\n    filtered_df = pd.read_csv('../input/filtered-data/filtered_df.csv')\n\nelse:\n    keyword_articles_df = pd.DataFrame({\n        'title': titles, \n        'paper_id': paper_ids\n    })\n\n    meta_df = pd.read_csv('../input/CORD-19-research-challenge/metadata.csv')\n\n    filtered_df = pd.merge(meta_df, keyword_articles_df)\n    filtered_df = filtered_df.drop_duplicates(subset='title')\n    filtered_df = filtered_df.dropna(subset=['abstract'])\n    \nfiltered_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Out of the filtered articles, only 11,468 articles contain metadata, have unique titles, and contain abstracts.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"titles = list(filtered_df.title)\npaper_ids = list(filtered_df.paper_id)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2. The First Filtering Method Extract Answers by Finding Semantically Similar PubMed Articles Using SBERT\n\nWe are using [SBERT](http://https://pypi.org/project/sentence-transformers/) to derive sentence embeddings from our query set and the titles that have been collected in the pervious step. We use computes the cosine similarity between the sentence embeddings. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom transformers import BertForQuestionAnswering","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_n_similar_articles_df(question, titles):\n    embedder = SentenceTransformer('bert-base-nli-stsb-mean-tokens')\n    query_embeddings = embedder.encode([question])\n\n    # list of article titles\n    title_embeddings = embedder.encode(titles)\n\n    # get top 50 article titles based on cosine similarity\n    closest_n = 50\n    distances = scipy.spatial.distance.cdist(query_embeddings, title_embeddings, \"cosine\")[0]\n\n    results = zip(range(len(distances)), distances)\n    results = sorted(results, key=lambda x: x[1])\n\n    # save similar articles info\n    top_paper_ids = []\n    top_titles = []\n    top_similarity_scores = []\n    top_abstracts = []\n    abstracts = list(filtered_df.abstract)\n\n    print('Query: ' + question + '\\n')\n\n    # Find the closest 50 article titles for each query sentence based on cosine similarity\n    for idx, distance in results[0:closest_n]:\n        top_paper_ids.append(paper_ids[idx])\n        top_titles.append(titles[idx])\n        top_similarity_scores.append(round((1-distance), 4))\n        top_abstracts.append(abstracts[idx])\n        print('Paper ID: ' + paper_ids[idx])\n        print('PubMed Article Title: ' + titles[idx])\n        print('Similarity Score: ' + \"%.4f\" % (1-distance))\n        print('\\n')\n        \n    top_50_similar_articles_df = pd.DataFrame({\n        'paper_id': top_paper_ids,\n        'cosine_similarity': top_similarity_scores,\n        'title': top_titles,\n        'abstract': top_abstracts\n    })\n    \n    return top_50_similar_articles_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2-1. Get Top Articles for Question 1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# research question\nqueries = ['What is the efficacy of novel therapeutics being tested currently?', \n           'What is the best method to combat the hypercoagulable state seen in COVID-19?']\n\nif os.path.exists('../input/top-50-similar-articles/top_50_similar_articles_df.csv'):\n    q1_top_50_similar_articles_df = pd.read_csv('../input/top-50-similar-articles/top_50_similar_articles_df.csv')\n\nelse:\n    q1_top_50_similar_articles_df = get_top_n_similar_articles_df(queries[0], titles)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"q1_top_50_similar_articles_df[['cosine_similarity', 'title']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2-2. Get Top Articles for Question 2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# research question\nqueries = ['What is the efficacy of novel therapeutics being tested currently?', \n           'What is the best method to combat the hypercoagulable state?']\n\nif os.path.exists('../input/q2-top-50-similar-articles-df/q2_top_50_similar_articles_df.csv'):\n    q2_top_50_similar_articles_df = pd.read_csv('../input/q2-top-50-similar-articles-df/q2_top_50_similar_articles_df.csv')\n\nelse:\n    q2_top_50_similar_articles_df = get_top_n_similar_articles_df(queries[1], titles)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"q2_top_50_similar_articles_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 3. Extract Excerpt From Abstracts of Relevant Articles Using QA BERT\nThe article title is the question and the excerpt is the answer to the question. The answer to the question is extracted from the abstract of the article. We used the pre-trained QA BERT model \"bert-large-uncased-whole-word-masking-finetuned-squad\". ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_answer_from_text(question, text):\n    # Apply the tokenizer to the input text, treating them as a text-pair.\n    input_ids = tokenizer.encode(question, text)\n    input_ids = input_ids[0:512]\n    \n    # Search the input_ids for the first instance of the `[SEP]` token.\n    sep_index = input_ids.index(tokenizer.sep_token_id)\n\n    # The number of segment A tokens includes the [SEP] token istelf.\n    num_seg_a = sep_index + 1\n\n    # The remainder are segment B.\n    num_seg_b = len(input_ids) - num_seg_a\n\n    # Construct the list of 0s and 1s.\n    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n\n    # There should be a segment_id for every input token.\n    assert len(segment_ids) == len(input_ids)\n    \n    # Run our embeddings through the model\n    start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n                                 token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from texts\n    \n    # BERT only needs the token IDs, but for the purpose of inspecting the \n    # tokenizer's behavior, let's also get the token strings and display them.\n    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n\n    # Find the tokens with the highest `start` and `end` scores.\n    answer_start = torch.argmax(start_scores)\n    answer_end = torch.argmax(end_scores)\n    \n    if answer_start <= 0 or answer_end <= 0 or answer_end <= answer_start:\n        answer = \"Not Relevant\"\n        score = float('-inf')\n    \n    else:\n        # Start with the first token.\n        answer = tokens[answer_start]\n\n        # Select the remaining answer tokens and join them with whitespace.\n        for i in range(answer_start + 1, answer_end + 1):\n\n            # If it's a subword token, then recombine it with the previous token.\n            if tokens[i][0:2] == '##':\n                answer += tokens[i][2:]\n\n            # Otherwise, add a space then the token.\n            else:\n                answer += ' ' + tokens[i]\n\n        # extract answer\n        answer = answer.replace('[CLS]', '')\n        answer = answer.replace('[SEP]', '').strip()\n\n        # extract score\n        score = (start_scores.max() + end_scores.max()) / 2\n        score = score.item()\n\n    return answer, score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_excerpts_and_scores(question, abstracts):\n    excerpts = []\n    scores = []\n\n    for index, abstract in enumerate(abstracts):\n        # extract excerpt from abstract\n        excerpt, score = extract_answer_from_text(question, abstract)\n        excerpts.append(excerpt)\n        scores.append(score)\n\n    return excerpts, scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import colorama\nimport re \n\ndef print_top_n_articles(question, top_n_articles, top_n_similar_articles_df, scores, excerpts, top_indices):\n    print(\"Prediction highlighted in red....\")\n    print(\"========  \" + question + \"  ======== \\n\")\n\n    for i, top_idx in enumerate(top_indices):\n        print(\"Rank: \" + str(i+1))\n        \n        # get top 50 articles\n        data = top_n_similar_articles_df.iloc[top_idx]\n        \n        print(\"Title : \" + data['title'])\n        print(\"Confidence: \" + str(scores[top_idx]))\n        \n        abstract = data['abstract']\n        \n        # clearn excerpt\n        excerpt = excerpts[top_idx]\n        excerpt = re.sub(' -', '-', excerpt)\n        excerpt = re.sub('- ', '-', excerpt)\n        excerpt = re.sub(' ,', ',', excerpt)\n        excerpt = re.sub(r'\\s([?.!\"](?:\\s|$))', r'\\1', excerpt)\n        excerpt = re.sub('\\( ', '(', excerpt)\n        excerpt = re.sub(' \\)', ')', excerpt)\n        \n        # put excerpt in red font\n        insensitive_excerpt = re.compile(re.escape(excerpt), re.IGNORECASE)\n        highlighted_txt = insensitive_excerpt.sub('\\033[31m' + excerpt + '\\033[39m', abstract)\n        print(\"Abstract: \" + highlighted_txt)\n        print('\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 3-1. Print Top 10 Answers to Question 1\nThe answers are the excerpts of the abstracts highlighted in red.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# get excerpts and scores for question 1\nq1_excerpts, q1_scores = get_excerpts_and_scores(queries[0], q1_top_50_similar_articles_df.abstract[0:40])\nn = 10\nq1_top_indices = [i[0] for i in sorted(enumerate(q1_scores), key=lambda x:-x[1])][0:n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_top_n_articles(queries[0], n, q1_top_50_similar_articles_df, q1_scores, q1_excerpts, q1_top_indices)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 3-1. Print Top 10 Answers to Question 2\nThe answers are the excerpts of the abstracts highlighted in red.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# get excerpts and scores for question 1\nq2_excerpts, q2_scores = get_excerpts_and_scores(queries[1], q2_top_50_similar_articles_df.abstract[0:40])\nn = 10\nq2_top_indices = [i[0] for i in sorted(enumerate(q2_scores), key=lambda x:-x[1])][0:n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_top_n_articles(queries[1], n, q2_top_50_similar_articles_df, q2_scores, q2_excerpts, q2_top_indices)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}