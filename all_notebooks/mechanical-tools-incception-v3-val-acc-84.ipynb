{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install tensorflow\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining each of these directories"},{"metadata":{"trusted":true},"cell_type":"code","source":"#base_dir = '../input/mechanical-tools-dataset/Mechanical Tools Image dataset'\n\ntrain_dir = os.path.join('../input/mechanical-tools-dataset/train_data_V2/train_data_V2')\nvalidation_dir = os.path.join('../input/mechanical-tools-dataset/validation_data_V2/validation_data_V2')\n\n# Directory with our training screwdriver/wrench pictures\ntrain_screwdriver_dir = os.path.join('../input/mechanical-tools-dataset/train_data_V2/train_data_V2/screwdriver')\ntrain_wrench_dir = os.path.join('../input/mechanical-tools-dataset/train_data_V2/train_data_V2/wrench')\ntrain_hammer_dir = os.path.join('../input/mechanical-tools-dataset/train_data_V2/train_data_V2/hammer')\n\n# Directory with our validation screwdriver/wrench pictures\nvalidation_screwdriver_dir = os.path.join('../input/mechanical-tools-dataset/validation_data_V2/validation_data_V2/screwdriver')\nvalidation_wrench_dir = os.path.join('../input/mechanical-tools-dataset/validation_data_V2/validation_data_V2/wrench')\nvalidation_hammer_dir = os.path.join('../input/mechanical-tools-dataset/validation_data_V2/validation_data_V2/hammer')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now, let's see what the filenames look like in the training directories:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_screwdriver_fnames = os.listdir(train_screwdriver_dir )\ntrain_wrench_fnames = os.listdir( train_wrench_dir)\ntrain_hammer_fnames = os.listdir( train_hammer_dir )\n\nprint(train_screwdriver_fnames[:20])\nprint(train_wrench_fnames[:20])\nprint(train_hammer_fnames[:20])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's find out the number of wrench and pliers images in the directory**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('total training screwdriver images :', len(os.listdir(train_screwdriver_dir)))\nprint('total training wrench images :', len(os.listdir(train_wrench_dir)))\nprint('total training hammer images :', len(os.listdir(train_hammer_dir)))\n\n\nprint('total validation screwdriver images :', len(os.listdir( validation_screwdriver_dir ) ))\nprint('total validation wrench images :', len(os.listdir( validation_wrench_dir) ))\nprint('total validation hammer images :', len(os.listdir( validation_hammer_dir) ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now let's take a look at a few pictures to get a better sense of what they look like. First, configure the matplot parameters:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# Parameters for our graph; we'll output images in a 10x10 configuration\nnrows = 10\nncols = 10\n\n# Index for iterating over images\npic_index = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now, display a batch of 50 wrench and 50 pliers pictures. You can rerun the cell to see a fresh batch each time:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up matplotlib fig, and size it to fit 4x4 pics\nfig = plt.gcf()\nfig.set_size_inches(ncols * 4, nrows * 4)\n\npic_index += 10\nnext_screwdriver_pix = [os.path.join(train_screwdriver_dir, fname) \n                for fname in train_screwdriver_fnames[pic_index-10:pic_index]]\nnext_wrench_pix = [os.path.join(train_wrench_dir, fname) \n                for fname in train_wrench_fnames[pic_index-10:pic_index]]\nnext_hammer_pix = [os.path.join(train_hammer_dir, fname) \n                for fname in train_hammer_fnames[pic_index-10:pic_index]]\n\nfor i, img_path in enumerate(next_screwdriver_pix+next_wrench_pix+next_hammer_pix):\n  # Set up subplot; subplot indices start at 1\n  sp = plt.subplot(nrows, ncols, i + 1)\n  sp.axis('Off') # Don't show axes (or gridlines)\n\n  img = mpimg.imread(img_path)\n  plt.imshow(img)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The \"output shape\" column shows how the size of your feature map evolves in each successive layer. The convolution layers reduce the size of the feature maps by a bit due to padding, and each pooling layer halves the dimensions.**"},{"metadata":{},"cell_type":"markdown","source":"# Pre-Trained Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\n\nlocal_weights_file = '../input/inceptionv3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n\n\npre_trained_model = InceptionV3(input_shape = (300,300, 3), \n                                include_top = False, \n                                weights = None)\n\npre_trained_model.load_weights(local_weights_file)\n\nfor layer in pre_trained_model.layers:\n  layer.trainable = False\n  \n# pre_trained_model.summary()\n\nlast_layer = pre_trained_model.get_layer('mixed7')\nprint('last layer output shape: ', last_layer.output_shape)\nlast_output = last_layer.output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# All images will be rescaled by 1./255.\ntest_datagen  = ImageDataGenerator( rescale = 1.0/255. )\n\n# Add our data-augmentation parameters to ImageDataGenerator\ntrain_datagen = ImageDataGenerator(rescale = 1./255.,\n                                   rotation_range = 40,\n                                   width_shift_range = 0.2,\n                                   height_shift_range = 0.2,\n                                   shear_range = 0.2,\n                                   zoom_range = 0.2,\n                                   horizontal_flip = True)\n\ntest_datagen = train_datagen = ImageDataGenerator(rescale = 1./255.,\n                                   rotation_range = 40,\n                                   width_shift_range = 0.2,\n                                   height_shift_range = 0.2,\n                                   shear_range = 0.2,\n                                   zoom_range = 0.2,\n                                   horizontal_flip = True)\n\n# --------------------\n# Flow training images in batches of 20 using train_datagen generator\n# --------------------\ntrain_generator = train_datagen.flow_from_directory(train_dir,\n                                                    batch_size=30,\n                                                    class_mode='categorical',\n                                                    target_size=(300,300))     \n# --------------------\n# Flow validation images in batches of 20 using test_datagen generator\n# --------------------\nvalidation_generator =  test_datagen.flow_from_directory(validation_dir,\n                                                         batch_size=30,\n                                                         class_mode  = 'categorical',\n                                                         target_size = (300,300))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We then add convolutional layers as in the previous example, and flatten the final result to feed into the densely connected layers.**\n**Note that because we are facing a two-class classification problem, i.e. a binary classification problem, we will end our network with a sigmoid activation, so that the output of our network will be a single scalar between 0 and 1, encoding the probability that the current image is class 1 (as opposed to class 0).**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.optimizers import RMSprop\n\n# Flatten the output layer to 1 dimension\nx = layers.Flatten()(last_output)\n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nx = layers.Dense(128, activation='relu')(x)\n# Add a dropout rate of 0.2\nx = layers.Dropout(0.2)(x)                  \n# Add a final sigmoid layer for classification\nx = layers.Dense(3, activation='softmax')(x)           \n\nmodel = Model( pre_trained_model.input, x) \n\nmodel.compile(optimizer = RMSprop(lr=0.0001), \n              loss = 'categorical_crossentropy', \n              metrics = ['accuracy'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train_generator,\n                              validation_data=validation_generator,\n                              steps_per_epoch=20,\n                              epochs=40,\n                              validation_steps=10,\n                              verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\nacc      = history.history[     'accuracy' ]\nval_acc  = history.history[ 'val_accuracy' ]\nloss     = history.history[    'loss' ]\nval_loss = history.history['val_loss' ]\n\nepochs   = range(len(acc)) # Get number of epochs\n\n#------------------------------------------------\n# Plot training and validation accuracy per epoch\n#------------------------------------------------\nplt.plot  ( epochs,     acc )\nplt.plot  ( epochs, val_acc )\nplt.title ('Training and validation accuracy')\nplt.figure()\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.plot  ( epochs,     loss )\nplt.plot  ( epochs, val_loss )\nplt.title ('Training and validation loss'  )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport cv2\nimg = cv2.imread('../input/mechanical-tools-dataset/test_data/test_data/Screwdriver (1415).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict(img)\nprint(classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.imread('../input/mechanical-tools-dataset/test_data/test_data/Screwdriver (1401).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict(img)\nprint(classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.imread('../input/mechanical-tools-dataset/test_data/test_data/Screwdriver (1402).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict(img)\nprint(classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.imread('../input/mechanical-tools-dataset/test_data/test_data/Wrench (286).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict(img)\nprint(classes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualizing Intermediate Representations\nTo get a feel for what kind of features our convnet has learned, one fun thing to do is to visualize how an input gets transformed as it goes through the convnet.\nLet's pick a random image from the training set, and then generate a figure where each row is the output of a layer, and each image in the row is a specific filter in that output feature map. Rerun this cell to generate intermediate representations for a variety of training images.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport random\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img\n\n# Let's define a new Model that will take an image as input, and will output\n# intermediate representations for all layers in the previous model after\n# the first.\nsuccessive_outputs = [layer.output for layer in model.layers[1:]]\n#visualization_model = Model(img_input, successive_outputs)\nvisualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n# Let's prepare a random input image from the training set.\nwrench_img_files = [os.path.join(train_wrench_dir, f) for f in train_wrench_fnames]\nscrewdriver_img_files = [os.path.join(train_wrench_dir, f) for f in train_screwdriver_fnames]\nimg_path = random.choice(wrench_img_files + screwdriver_img_files)\n\nimg = load_img(img_path, target_size=(300,300))  # this is a PIL image\nx = img_to_array(img)  # Numpy array with shape (150, 150, 3)\nx = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)\n\n# Rescale by 1/255\nx /= 255\n\n# Let's run our image through our network, thus obtaining all\n# intermediate representations for this image.\nsuccessive_feature_maps = visualization_model.predict(x)\n\n# These are the names of the layers, so can have them as part of our plot\nlayer_names = [layer.name for layer in model.layers[1:]]\n\n# Now let's display our representations\nfor layer_name, feature_map in zip(layer_names, successive_feature_maps):\n    if len(feature_map.shape) == 4:\n    # Just do this for the conv / maxpool layers, not the fully-connected layers\n        n_features = feature_map.shape[-1]  # number of features in feature map\n    # The feature map has shape (1, size, size, n_features)\n        size = feature_map.shape[1]\n    # We will tile our images in this matrix\n        display_grid = np.zeros((size, size * n_features))\n        for i in range(n_features):\n      # Postprocess the feature to make it visually palatable\n            x = feature_map[0, :, :, i]\n            x -= x.mean()\n            x /= x.std()\n            x *= 64\n            x += 128\n            x = np.clip(x, 0, 255).astype('uint8')\n      # We'll tile each filter into this big horizontal grid\n            display_grid[:, i * size : (i + 1) * size] = x\n    # Display the grid\n        scale = 20. / n_features\n        plt.figure(figsize=(scale * n_features, scale))\n        plt.title(layer_name)\n        plt.grid(False)\n        plt.imshow(display_grid, aspect='auto', cmap='summer')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}