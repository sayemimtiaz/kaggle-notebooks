{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://numer.ai/homepage/img/Numerai-Logo-Side-Black.png)\n\nHow is your Numerai life going?:D\n\nUsing an integration_test-like model is fine, but I believe that it would be more fun to play around a **neural network** (NN). Maybe you can get a better MMC performance because typically predictions from a NN is not well-correlated with ones from a GBDT (Gradient-Boosting Decision Tree) such as XGBoost.\n\nHere I show a plain example of how to fit a **multi-layer perceptron (MLP)** to the Numerai Tournament data. As for the Numerai data, I use the one from [this post](https://forum.numer.ai/t/new-target-nomi-release/959) such that you can choose which target for your MLP to be trained on.\n\nAs a bonus, I also demonstrate how to use the [KerasTuner](https://www.tensorflow.org/tutorials/keras/keras_tuner) to fine-tune the hyperparameters to maximize our validation score.\n\nSo let's get the ball rolling!"},{"metadata":{},"cell_type":"markdown","source":"# Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport sys\nimport math\nimport random\nimport gc\nimport pathlib\nfrom typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score, roc_auc_score, log_loss, mean_squared_error, mean_absolute_error, f1_score\nfrom scipy.stats import spearmanr\n\nimport tensorflow as tf \n# import tensorflow_addons as tfa\n# !pip install -q -U keras-tuner\nimport kerastuner as kt # keras tuner!\n\n# visualize\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\nfrom matplotlib import pyplot\nfrom matplotlib.ticker import ScalarFormatter\nsns.set_context(\"talk\")\nstyle.use('fivethirtyeight')\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Config\nHere you can choose what you do: \n\n- Debug mode (using small proportion of data)?\n- Tuning or not?\n- Which target to predict?\n- Seed number?"},{"metadata":{"trusted":true},"cell_type":"code","source":"class CFG:\n    DEBUG = False # debug option\n    TUNING = True # whether to use the KerasTuner or not\n\n    INPUT_FILE_PATH = '../input/numerai-train-validation-with-kazutsugi-nomi/numerai_training_validation_target_nomi.csv'\n    OUTPUT_DIR = ''\n    TARGET = 'target_nomi' # target_kazutsugi\n    SEED = 2021","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logging is always nice for your experiment:)\ndef init_logger(log_file=CFG.OUTPUT_DIR+'train.log'):\n    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=log_file)\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nlogger = init_logger()\nlogger.info('Start Logging...')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logger.info('DEBUG : {}'.format(CFG.DEBUG))\nlogger.info('TUNING : {}'.format(CFG.TUNING))\nlogger.info('TARGET : {}'.format(CFG.TARGET))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data\nIn this dataset, we have two targets: 'kazutsugi' (old one) and 'nomi' (new one)."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndef get_int(x):\n    try:\n        return int(x[3:])\n    except:\n        return 1000\n\ntrain = pd.read_csv(CFG.INPUT_FILE_PATH)\nvalid = train[train[\"data_type\"] == \"validation\"].reset_index(drop = True)\ntrain = train[train[\"data_type\"] == \"train\"].reset_index(drop = True)\ntrain.drop(columns=['data_type'], inplace=True)\nvalid.drop(columns=['data_type'], inplace=True)\n\n# era int\ntrain[\"era\"] = train[\"era\"].apply(get_int)\nvalid[\"era\"] = valid[\"era\"].apply(get_int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(valid.shape)\nvalid.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# debug \nif CFG.DEBUG:\n    train = train.sample(10000, random_state=SEED)\n    logger.info('reduced train for debug')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Features, Target\nWe use all the features for now. As for a target, you specify which one to use before in the config section."},{"metadata":{"trusted":true},"cell_type":"code","source":"# features\nfeatures = train.columns[train.columns.str.startswith('feature')].values.tolist()\nlogger.info('{:,} features.'.format(len(features)))\nlogger.info(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# target\nsns.jointplot(data=train, x=\"target_kazutsugi\", y=\"target_nomi\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling\nHere we build a simple MLP using tensorflow."},{"metadata":{"trusted":true},"cell_type":"code","source":"# set seed to reproduce the result\ndef seed_everything(seed : int) -> NoReturn :    \n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n\nseed_everything(CFG.SEED) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# my default NN hyperparameters\nparams = {\n    'input_dim': len(features),\n    'input_dropout': 0.0,\n    'hidden_layers': 3,\n    'hidden_units': 256,\n    'hidden_activation': 'relu',\n    'lr': 1e-03,\n    'dropout': 0.2,\n    'batch_size': 128,\n    'epochs': 192\n}\nlogger.info('default NN params:')\nlogger.info(params)\n\ndef create_model(params=params):\n    \"\"\"\n    baseline model\n    \"\"\"\n\n    # NN model architecture\n    n_neuron = params['hidden_units']\n\n    inputs = tf.keras.layers.Input(shape=(params['input_dim'], ))\n    x = tf.keras.layers.BatchNormalization()(inputs)\n    x = tf.keras.layers.Dense(n_neuron, activation=params['hidden_activation'])(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(params['dropout'])(x)\n\n    # stack more layers\n    for i in np.arange(params['hidden_layers'] - 1):\n        x = tf.keras.layers.Dense(n_neuron // (2 * (i+1)), activation=params['hidden_activation'])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(params['dropout'])(x)\n\n    out = tf.keras.layers.Dense(1, activation='linear', name = 'out')(x)\n        \n    # compile\n    model = tf.keras.models.Model(inputs=inputs, outputs=out)\n    loss = tf.keras.losses.MeanSquaredError()\n    opt = tf.keras.optimizers.Adam(lr=params['lr'])\n    model.compile(loss=loss, optimizer=opt, metrics=['mse'])\n    \n    return model\n\ndef tuning_model(hp, params=params):\n    \"\"\"\n    model tuning with KerasTuner\n    \"\"\"\n    \n    inputs = tf.keras.layers.Input(shape=(params['input_dim'], ))\n    x = tf.keras.layers.BatchNormalization()(inputs)\n    x = tf.keras.layers.Dense(hp.Int('num_units_1', 128, 512, step=128), activation=params['hidden_activation'])(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(hp.Float('dropout_1', 0.0, 0.5, step=0.1, default=0.5))(x)\n\n    for i in range(hp.Int('num_layers', 1, 3)):\n        x = tf.keras.layers.Dense(hp.Int(f'num_units_{i+2}', 128, 512, step=128))(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(hp.Float(f'dropout_{i+2}', 0.0, 0.5, step=0.1, default=0.5))(x)\n        \n    # output\n    out = tf.keras.layers.Dense(1, activation='linear', name = 'out')(x)\n    model = tf.keras.models.Model(inputs=inputs, outputs=out)\n   \n    # compile\n    loss = tf.keras.losses.MeanSquaredError()\n    opt = tf.keras.optimizers.Adam(lr=hp.Float('learning_rate', 1e-4, 1e-2, sampling='log'))\n    model.compile(loss=loss, optimizer=opt, metrics=['mse'])\n    \n    return model\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tuning (or not)\nInstantiate the tuner to perform the hypertuning. The Keras Tuner has four tuners available - RandomSearch, Hyperband, BayesianOptimization, and Sklearn. \n\nHere we use the **BayesianOptimization** tuner."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a dataset for NN based on a task\ntrain_set = {'X': train[features].values, 'y': train[CFG.TARGET].values}\nvalid_set = {'X': valid[features].values, 'y': valid[CFG.TARGET].values}  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if CFG.TUNING:\n    # define a custom tuner to tune the batch size\n    class MyTuner(kt.tuners.BayesianOptimization):\n      def run_trial(self, trial, *args, **kwargs):\n        # You can add additional HyperParameters for preprocessing and custom training loops\n        # via overriding `run_trial`\n        kwargs['batch_size'] = trial.hyperparameters.Int('batch_size', 128, 8192, step=128)\n#         kwargs['epochs'] = trial.hyperparameters.Int('epochs', 10, 30)\n        super(MyTuner, self).run_trial(trial, *args, **kwargs)\n\n    # instantiate KerasTuner\n    model_ft = lambda hp: tuning_model(hp, params)\n    tuner = MyTuner(\n        hypermodel=model_ft,\n        objective=kt.Objective('val_loss', direction='min'),\n        num_initial_points=4,\n        max_trials=20,\n        overwrite=True)\n    \n    # perform tuning\n    tuner.search(train_set['X'], train_set['y'], \n                 epochs = 8, validation_data = (valid_set['X'], valid_set['y']))\n\n    # Get the optimal hyperparameters\n    best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n    \n    # Build the model with the optimal hyperparameters and train it on the data\n    model = tuner.hypermodel.build(best_hps)\n    \n    # disp best params\n    logger.info('Best hyperparameters:')\n    logger.info(best_hps.values)\nelse:\n    # baseline (no tuning)\n    model = create_model(params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.utils.plot_model(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fit with the best model\nLet's use the model with the best hyperparameters (if tuned). \n\nHere I use **Early Stopping** such that the model does not overfit. \n\nAs a learning rate scheduler, I use **ReduceLROnPlateau**.\n\nI do not submit the model prediction in this notebook, but to make the submission process a bit easier, I save the entire model!"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# callbacks\nes = tf.keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True, monitor='val_loss')\nlr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=8, verbose=1, mode='min')\n# model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath='mybestweight.hdf5', save_weights_only=True, \n#                                                                verbose=0, monitor='val_loss', save_best_only=True)\nnn_callbacks = [es, lr_scheduler, ]\n\n# fit\nhistory = model.fit(train_set['X'], train_set['y'], callbacks=nn_callbacks, \n                    verbose=2, epochs=params['epochs'], batch_size=best_hps.values['batch_size'], \n                    validation_data=(valid_set['X'], valid_set['y'])) \n\n# you can load the model for inference\n# model = tf.keras.models.load_model(CFG.OUTPUT_DIR + 'saved_model/my_model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the entire model\nmodel.save(CFG.OUTPUT_DIR + 'my_model.h5')\nlogger.info('Entire model saved!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize history for loss\nwith plt.xkcd(): # just for fun\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'valid'], loc='upper left')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validation Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# prediction for valid\npred = model.predict(valid_set['X']).ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(pred, alpha=0.4, label='prediction')\nplt.hist(valid_set['y'], alpha=0.4, label='actual target')\nplt.legend(frameon=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validation Score\nThis is what we care about! Here we compute Numerai-related scores except for MMC (which we cannot compute as we don't have a meta-model prediction).\n\nNote that we split the validation set into the two parts and compute scores on the corresponding eras. This is because the first half validation eras are easy to predict, whereas the last half are hard. It is better to see that our model performs well on the both periods."},{"metadata":{"trusted":true},"cell_type":"code","source":"# naming conventions\nPREDICTION_NAME = 'prediction'\nTARGET_NAME = CFG.TARGET\nEXAMPLE_PRED = 'example_prediction'\n\n# ---------------------------\n# Functions\n# ---------------------------\ndef valid4score(valid : pd.DataFrame, pred : np.ndarray, load_example: bool=True, save : bool=False) -> pd.DataFrame:\n    \"\"\"\n    Generate new valid pandas dataframe for computing scores\n    \n    :INPUT:\n    - valid : pd.DataFrame extracted from tournament data (data_type='validation')\n    \n    \"\"\"\n    valid_df = valid.copy()\n    valid_df['prediction'] = pd.Series(pred).rank(pct=True, method=\"first\")\n    valid_df.rename(columns={TARGET_NAME: 'target'}, inplace=True)\n    \n    if load_example:\n        valid_df[EXAMPLE_PRED] = pd.read_csv(EXP_DIR + 'valid_df.csv')['prediction'].values\n    \n    if save==True:\n        valid_df.to_csv(OUTPUT_DIR + 'valid_df.csv', index=False)\n        logger.info('Validation dataframe saved!')\n    \n    return valid_df\n\ndef compute_corr(valid_df : pd.DataFrame):\n    \"\"\"\n    Compute rank correlation\n    \n    :INPUT:\n    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n    \n    \"\"\"\n    \n    return np.corrcoef(valid_df[\"target\"], valid_df['prediction'])[0, 1]\n\ndef compute_max_drawdown(validation_correlations : pd.Series):\n    \"\"\"\n    Compute max drawdown\n    \n    :INPUT:\n    - validation_correaltions : pd.Series\n    \"\"\"\n    \n    rolling_max = (validation_correlations + 1).cumprod().rolling(window=100, min_periods=1).max()\n    daily_value = (validation_correlations + 1).cumprod()\n    max_drawdown = -(rolling_max - daily_value).max()\n    \n    return max_drawdown\n\ndef compute_val_corr(valid_df : pd.DataFrame):\n    \"\"\"\n    Compute rank correlation for valid periods\n    \n    :INPUT:\n    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n    \"\"\"\n    \n    # all validation\n    correlation = compute_corr(valid_df)\n    logger.info(\"rank corr = {:.4f}\".format(correlation))\n    return correlation\n    \ndef compute_val_sharpe(valid_df : pd.DataFrame):\n    \"\"\"\n    Compute sharpe ratio for valid periods\n    \n    :INPUT:\n    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n    \"\"\"\n    # all validation\n    d = valid_df.groupby('era')[['target', 'prediction']].corr().iloc[0::2,-1].reset_index()\n    me = d['prediction'].mean()\n    sd = d['prediction'].std()\n    max_drawdown = compute_max_drawdown(d['prediction'])\n    logger.info('sharpe ratio = {:.4f}, corr mean = {:.4f}, corr std = {:.4f}, max drawdown = {:.4f}'.format(me / sd, me, sd, max_drawdown))\n    \n    return me / sd, me, sd, max_drawdown\n    \ndef feature_exposures(valid_df : pd.DataFrame):\n    \"\"\"\n    Compute feature exposure\n    \n    :INPUT:\n    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n    \"\"\"\n    feature_names = [f for f in valid_df.columns\n                     if f.startswith(\"feature\")]\n    exposures = []\n    for f in feature_names:\n        fe = spearmanr(valid_df['prediction'], valid_df[f])[0]\n        exposures.append(fe)\n    return np.array(exposures)\n\ndef max_feature_exposure(fe : np.ndarray):\n    return np.max(np.abs(fe))\n\ndef feature_exposure(fe : np.ndarray):\n    return np.sqrt(np.mean(np.square(fe)))\n\ndef compute_val_feature_exposure(valid_df : pd.DataFrame):\n    \"\"\"\n    Compute feature exposure for valid periods\n    \n    :INPUT:\n    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n    \"\"\"\n    # all validation\n    fe = feature_exposures(valid_df)\n    fe1, fe2 = feature_exposure(fe), max_feature_exposure(fe)\n    logger.info('feature exposure = {:.4f}, max feature exposure = {:.4f}'.format(fe1, fe2))\n     \n    return fe1, fe2\n\n# to neutralize a column in a df by many other columns\ndef neutralize(df, columns, by, proportion=1.0):\n    scores = df.loc[:, columns]\n    exposures = df[by].values\n\n    # constant column to make sure the series is completely neutral to exposures\n    exposures = np.hstack(\n        (exposures,\n         np.asarray(np.mean(scores)) * np.ones(len(exposures)).reshape(-1, 1)))\n\n    scores = scores - proportion * exposures.dot(\n        np.linalg.pinv(exposures).dot(scores))\n    return scores / scores.std()\n\n\n# to neutralize any series by any other series\ndef neutralize_series(series, by, proportion=1.0):\n    scores = series.values.reshape(-1, 1)\n    exposures = by.values.reshape(-1, 1)\n\n    # this line makes series neutral to a constant column so that it's centered and for sure gets corr 0 with exposures\n    exposures = np.hstack(\n        (exposures,\n         np.array([np.mean(series)] * len(exposures)).reshape(-1, 1)))\n\n    correction = proportion * (exposures.dot(\n        np.linalg.lstsq(exposures, scores, rcond=None)[0]))\n    corrected_scores = scores - correction\n    neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\n    return neutralized\n\n\ndef unif(df):\n    x = (df.rank(method=\"first\") - 0.5) / len(df)\n    return pd.Series(x, index=df.index)\n\ndef get_feature_neutral_mean(df):\n    feature_cols = [c for c in df.columns if c.startswith(\"feature\")]\n    df.loc[:, \"neutral_sub\"] = neutralize(df, [PREDICTION_NAME],\n                                          feature_cols)[PREDICTION_NAME]\n    scores = df.groupby(\"era\").apply(\n        lambda x: np.corrcoef(x[\"neutral_sub\"].rank(pct=True, method=\"first\"), x[TARGET_NAME])).mean()\n    return np.mean(scores)\n\ndef compute_val_mmc(valid_df : pd.DataFrame):    \n    # MMC over validation\n    mmc_scores = []\n    corr_scores = []\n    for _, x in valid_df.groupby(\"era\"):\n        series = neutralize_series(pd.Series(unif(x[PREDICTION_NAME])),\n                                   pd.Series(unif(x[EXAMPLE_PRED])))\n        mmc_scores.append(np.cov(series, x[TARGET_NAME])[0, 1] / (0.29 ** 2))\n        corr_scores.append(np.corrcoef(unif(x[PREDICTION_NAME]).rank(pct=True, method=\"first\"), x[TARGET_NAME]))\n\n    val_mmc_mean = np.mean(mmc_scores)\n    val_mmc_std = np.std(mmc_scores)\n    val_mmc_sharpe = val_mmc_mean / val_mmc_std\n    corr_plus_mmcs = [c + m for c, m in zip(corr_scores, mmc_scores)]\n    corr_plus_mmc_sharpe = np.mean(corr_plus_mmcs) / np.std(corr_plus_mmcs)\n    corr_plus_mmc_mean = np.mean(corr_plus_mmcs)\n\n    logger.info(\"MMC Mean = {:.6f}, MMC Std = {:.6f}, CORR+MMC Sharpe = {:.4f}\".format(val_mmc_mean, val_mmc_std, corr_plus_mmc_sharpe))\n\n    # Check correlation with example predictions\n    corr_with_example_preds = np.corrcoef(valid_df[EXAMPLE_PRED].rank(pct=True, method=\"first\"),\n                                          valid_df[PREDICTION_NAME].rank(pct=True, method=\"first\"))[0, 1]\n    logger.info(\"Corr with example preds: {:.4f}\".format(corr_with_example_preds))\n    \n    return val_mmc_mean, val_mmc_std, corr_plus_mmc_sharpe, corr_with_example_preds\n    \ndef score_summary(valid_df : pd.DataFrame):\n    score_df = {}\n    \n    try:\n        score_df['correlation'] = compute_val_corr(valid_df)\n    except:\n        print('ERR: computing correlation')\n    try:\n        score_df['corr_sharpe'], score_df['corr_mean'], score_df['corr_std'], score_df['max_drawdown'] = compute_val_sharpe(valid_df)\n    except:\n        print('ERR: computing sharpe')\n    try:\n        score_df['feature_exposure'], score_df['max_feature_exposure'] = compute_val_feature_exposure(valid_df)\n    except:\n        print('ERR: computing feature exposure')\n    try:\n        score_df['mmc_mean'], score_df['mmc_std'], score_df['corr_mmc_sharpe'], score_df['corr_with_example_xgb'] = compute_val_mmc(valid_df)\n    except:\n        print('ERR: computing MMC')\n    \n    return pd.DataFrame.from_dict(score_df, orient='index')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_df = valid4score(valid, pred, load_example=False, save=False)\n\n# scores\nscore_df = pd.DataFrame()\nprint('------------------')\nprint('ALL:')\nprint('------------------')\nall_ = score_summary(valid_df).rename(columns={0: 'all'})\n\nprint('------------------')\nprint('VALID 1:')\nprint('------------------')\nval1_ = score_summary(valid_df.query('era < 150')).rename(columns={0: 'val1'})\n\nprint('------------------')\nprint('VALID 2:')\nprint('------------------')\nval2_ = score_summary(valid_df.query('era > 150')).rename(columns={0: 'val2'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scores\nscore_df = pd.concat([all_, val1_, val2_], axis=1)\nscore_df.style.background_gradient(cmap='viridis', axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nWe can achieve a decent score by using a simple MLP. So why not try it out if you haven't?"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}