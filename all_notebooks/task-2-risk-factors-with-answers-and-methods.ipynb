{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n# **TASK - 2 Answers - NL Questions**\n\n![](https://sportslogohistory.com/wp-content/uploads/2018/09/georgia_tech_yellow_jackets_1991-pres-1.png)\n\n**Executive Summary:** Unsupervised scientific literature understanding system that accepts natural language questions and returns specific answers from the CORD19 scientific paper corpus. The answers are wholly generated by the system from the publicatons cited below the answer.  There is also a link with the question pre-loaded to the CORD19 web-based corpus (QA) search.\n\n**PROBLEM:** When a new virus is discovered and causes a pandemic, it is important for scientists to get information coming from all scientific sources that may help them combat the pandemic.  The challenege, however, is that the number of scientific papers created is large and the papers are published very rapidly, making it nearly impossible for scientists to digest and understand important data in this mass of data.\n\n**SOLUTION:** Unsupervised scientific literature understanding system that accepts natural language quesitons and returns specific answers from the CORD19 scientific paper corpus.\n\n**APPROACH:**\n- meta.csv and full text versions (if available) of the COVID-19 relevant documents were exported to a MySQL database \n- the natural language questions for the task are contained in a list \n- the natural language questions are stemmed and stop words removed for passing to the database query\n- the query is run on the MySQL datatbase using NLP mode in MySQL through a custom API\n- The full text results are parsed into sentences and then scored and ordered for relevance\n- raw_score = total count of the keywords in the sentence\n- final_score = (raw_score/len(sentence))*raw_score - if all terms are in a sentence it recieves +1\n- a csv file is returned to the notebook as a pandas dataframe\n- if the question calls for a specific answer, the quesiton_answer module is sent the first 5 sentences and the original question - shout out to Dave Mezzetti for suggesting this QA impelmentation from huggingface.\n- https://www.kaggle.com/c/tensorflow2-question-answering/discussion/123434\n- if the question calls for a summary, the first 50 sentences are sent to the prepare summary answer.\n- https://pypi.org/project/bert-extractive-summarizer/\n- The question,answers and table of relevant scientific papers are returned in HTML format.\n\n**Pros:** The system provides very responsive and seemingly accurate answers to specific questions. \n\n**Cons:** The system currently indicates study design (right column above author) from a keyword density system that is a work in progress - it also references a list of study design by Dave Mezzetti - it compares these lists and if they match shows one study design number and if not, shows both. It is a work in progress\n\nIn addition, the system is currently being updated to mine the full text for a number followed by the word cases e.g. 2015 cases. If suchs a pattern is found, it shows the \"number of cases\" under the title link. It is a work in progress to find outcomes in patients etc.\n\nStudy Desing Codes\n\n- 1 - Systematic Review\n- 2 - Experimental Study (Randomized)\n- 3 - Experimental Study (Non-Randomized)\n- 4 - Ecological Regression\n- 5 - Prospective Cohort\n- 6 - Time Series Analysis\n- 7 - Retrospective Cohort\n- 8 - Cross Sectional\n- 9 - Case Control\n- 10 - Case Study\n- 11 - Simulation\n- 0 - Unknown Design (Default for no match)"},{"metadata":{"_kg_hide-output":true,"trusted":true,"_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"###################### LOAD PACKAGES ##########################\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk import PorterStemmer\nfrom IPython.core.display import display, HTML\nimport torch\n!pip install -q transformers --upgrade\nfrom transformers import *\n!pip install bert-extractive-summarizer\nfrom summarizer import Summarizer\n#from transformers import pipeline\nimport pandas as pd\n\n#https://colab.research.google.com/drive/1rN0CS0hoxeByoPZu6_zF-AFJijYLsPw3","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def remove_stopwords(query,stopwords):\n    qstr=''\n    qstr=qstr.join(query)\n    #remove punctuaiton\n    qstr = \"\".join(c for c in qstr if c not in ('!','.',',','?','(',')','-'))\n    text_tokens = word_tokenize(qstr)\n    #remove stopwords\n    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n    #stem words\n    #tokens_without_sw=stem_words(tokens_without_sw)\n    str1=''\n    #str1=' '.join(word for word in tokens_without_sw)\n    str1=' '.join(word for word in text_tokens)\n    return str1\n\ndef stem_words(words):\n    stemmer = PorterStemmer()\n    singles=[]\n    for w in words:\n        singles.append(stemmer.stem(w))\n    return singles\n\n# query MySQL database returns most relevant paper sentences in dataframe\ndef get_search_table(query,keyword):\n    query=query.replace(\" \",\"+\")\n    urls=r\"https://edocdiscovery.com/covid_19/covid_19_search_api_v2.php?search_string=\"+query+'&keyword='+keyword\n    table = pd.read_csv(urls,encoding= 'unicode_escape')\n    return table\n\n# BERT pretrained question answering module\ndef answer_question(question,text):\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n    input_text = \"[CLS] \" + question + \" [SEP] \" + text + \" [SEP]\"\n    input_ids = tokenizer.encode(input_text)\n    token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]\n    start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))\n    all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n    #print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\n    answer=(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\n    # show qeustion and text\n    #tokenizer.decode(input_ids)\n    return answer\n\ndef prepare_summary_answer(text):\n    model = Summarizer()\n    #model = pipeline(task=\"summarization\")\n    \n    return model(text)\n\n\n###################### MAIN PROGRAM ###########################\n\n### NL questions\n### focus quesiton with single keyword\nkeyword=['smoking','asthma','copd','comorbidities','co-infections','hypertension','diabetes','pregnant','neonates','socioeconomic','reproduction number','incubation period','serial interval','transmission','age','comorbidities','cancer','mortality rate','comorbidities','death','control']\n#'''\nquestions = [\n'Is a history of smoking a risk factor for more severe cases?',\n'What do we know about patients with asthma and severity?',\n'What do we know about patients with COPD and severity?',\n'What is known about the most common comorbidities?',\n'What do we know about co-infections and risk?',\n'What do we know about risk of severe or fatal disease with hypertension?',\n'What do we know about diabetes and disease severity?',\n'Are pregnant women more susceptible?',\n'What do we know about neonates?',\n'What do we know about socioeconomic impacts?',\n'What was the basic reproduction number?',\n'What is the full incubation period range?',\n'What is the serial interval?',\n'What is known about transmission routes such as person to person, aerosol, droplets, surfaces?',\n'What is known about disease severity by age?',\n'What is known about disease severity based on comorbidities?',\n'Are cancer patients at an increased risk to be infected?',\n'What do we know about the death or mortality rate?',\n'What group has the highest risk for death?',\n'What is the mean age of the death group?',\n'What control measures could be effective to stop spread?'\n]\n#'''\n# use QA or summarize for the realted NL question?\na_type=['sum','qa','sum','sum','sum','sum','sum','qa','sum','sum','qa','qa','qa','sum','sum','sum','qa','sum','sum','qa','qa']\n\n#test one question\n#questions=['Are asthma patients more likely to have a severe case?']\n#keyword=['asthma']\n#test one question type of answer\n#a_type=['qa']\n\nq=0\n\n# loop through the list of questions\nfor question in questions:\n\n    #remove punctuation, stop words and stem words from NL question\n    search_words=remove_stopwords(question,stopwords)\n    \n    #clean up bad stems that do not render search results\n    bad_stems=['phenotyp','deadli','contagi','recoveri','rout','viru', 'surfac','immun','respons','person','protect','includ','smoke','diabet']\n    replace_with=['phenotype','dead','contagious','recovery','route','virus','surface','immune','response','personal','protective','include','smoking','diabetes']\n    r=0\n    for words in bad_stems:\n        search_words=search_words.replace(words,replace_with[r])\n        r=r+1\n    # use to see stemmed query for troubleshooting\n    #print (search_words)\n\n    # get best sentences\n    df_table=get_search_table(search_words,keyword[q])\n    df_answers=df_table\n    \n    # if qa limit dataframe search rows to consider\n    if a_type[q]=='qa':\n        df_answers=df_table.head(5)\n    \n    # if sum expand dataframe search rows to consider\n    if a_type[q]=='sum':\n        df_answers=df_table.head(100)\n    \n    text=''\n    \n    for index, row in df_answers.iterrows():\n        text=text+' '+row['excerpt']\n        \n    display(HTML('<h1>'+question+'</h1>'))\n    \n    #if qa use the question answering function\n    if a_type[q]=='qa':\n        answer=answer_question(question,text)\n        answer=answer.replace(\"#\", \"\")\n        answer=answer.replace(\" . \", \".\")\n        display(HTML('<h4> Answer:</h4> '+ answer))\n         \n    #if sum use the summarizer function\n    if a_type[q]=='sum':\n        summary_answer=prepare_summary_answer(text)\n        #summary_answer=summary_answer[0]['summary_text']\n        display(HTML('<h4> Summarized Answer: </h4><i>'+summary_answer+'</i>'))\n    \n    #print (text)\n    \n    #limit the size of the df for the html table\n    df_table=df_table.head(5)\n    \n    #convert df to html\n    df_table=HTML(df_table.to_html(escape=False,index=False))\n    \n    # show the HTML table with responses\n    display(df_table)\n    \n    # link to web based CORD search preloaded\n    sstr=search_words.replace(\" \",\"+\")\n    cord_link='<a href=\"http://edocdiscovery.com/covid_19/xscript_serp.php?search_string='+sstr+'\">see more web-based results</a>'\n    display(HTML(cord_link))\n    \n    q=q+1\n    \nprint ('done')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}