{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a id=\"top\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#005097; border:0' role=\"tab\" aria-controls=\"home\"><center>Market Response Modeling</center></h1>"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install scikit-learn-extra","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom imblearn.combine import SMOTETomek\nfrom numpy import isnan\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport datetime\nfrom datetime import date\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.linear_model import LogisticRegression\nimport scipy.stats as stats\nfrom sklearn.preprocessing import StandardScaler,RobustScaler\nfrom sklearn_extra.cluster import KMedoids\nfrom imblearn.over_sampling import SMOTE\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nimport scikitplot as skplt\nfrom sklearn.metrics import classification_report,confusion_matrix\nimport optuna\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom functools import partial\nfrom sklearn.feature_selection import VarianceThreshold\nfrom optuna import Trial, visualization\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import FeatureUnion\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest,f_classif\nimport collections\nfrom collections import Counter\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nimport os\n!ls ../input/\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndata_folder = \"/kaggle/input/arketing-campaign/\"\ndataset=pd.read_csv(data_folder+'marketing_campaign.csv',header=0,sep=';') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Table of Contents\n\n* [1. Data Preprocessing](#section_1)\n    * [A. Custom transformers creation](#section_1_1)\n    * [B. Data types and data completeness](#section_1_2)\n    * [C. Class distribution](#section_1_3)\n    * [D. Feature engineering](#section_1_4)\n    * [E. Feature selection](#section_1_5)\n    \n    ___\n* [2. Model Creation](#section_2)\n    * [A. Evaluate algorithm baseline performance with nested cross-validation](#section_2_1)\n    * [B. Hyperparameter tuning with Optuna](#section_2_2)\n    \n    ---\n* [3. Model results](#section_3)\n    * [A. Visual Hyperparameter Optimization Analysis](#section_3_1)\n    * [B. Model Evaluation Metrics and Confusion Matrix](#section_3_2)\n    * [C. Cumulative Gains and Lift charts](#section_3_3)\n    \n    ---"},{"metadata":{},"cell_type":"markdown","source":"# 1. Data Preprocessing <a class=\"anchor\" id=\"section_1\"></a>"},{"metadata":{},"cell_type":"markdown","source":"### A. Custom transformers creation <a class=\"anchor\" id=\"section_1_1\"></a>"},{"metadata":{},"cell_type":"markdown","source":"#### Feature type selection (click to view code)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class feat_sel(BaseEstimator, TransformerMixin):\n    def __init__(self, dtype='numeric'):\n        self.dtype = dtype\n\n    def fit( self, X, y=None ):\n        return self \n\n    def transform(self, X, y=None):\n        if self.dtype == 'numeric':\n            num_cols = X.columns[X.dtypes != object].tolist()\n            return X[num_cols]\n        elif self.dtype == 'category':\n            cat_cols = X.columns[X.dtypes == object].tolist()\n            return X[cat_cols]\n    def get_feature_names(self):\n        if self.dtype == 'numeric':\n            num_cols = X.columns[X.dtypes != object].tolist()\n            return X[num_cols]\n        elif self.dtype == 'category':\n            cat_cols = X.columns[X.dtypes == object].tolist()\n            return X[cat_cols]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" #### Label encoder (click to view code)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class Label_encode(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    def transform(self,X,y=None):\n        #Ordinal encoding by LTV values : Champions have highest LTV followed by Loyal customers etc...\n            #(see previous notebook called \"RFM segmentation and CLV modeling\" if needed)\n        X['Segment']=X['Segment'].replace({'Champions':9,'Loyal customers':8,'Cant loose them':7,'At risk':6,\n                                           'Potential loyalist':5,'Need attention':4,'Promising':3,'New customers':2,\n                                           'About to sleep':1,'Lost':0}, regex=True)\n        #1=In couple, 0=Alone\n        X['Marital_Status']=X['Marital_Status'].replace({'Divorced':0,'Single':0,\n                                                                        'Married':1,'Together':1,                                                  \n                                                         'Absurd':0,'Widow':0,'YOLO':0,'Alone':0}, regex=True)\n        self.columns = X.columns\n        return X\n    def get_feature_names(self):\n        return list(self.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Power transformation (click to view code)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class df_unskewed(TransformerMixin,BaseEstimator):\n    \n    def __init__(self, method='yeo-johnson'):\n        \n        self.method = method\n        self.standardize=False #Standardize = False as we use the custom class \"df_scaler\"\n\n    def fit(self, X, y=None):\n        if self.method == 'yeo-johnson':\n            self.scl = PowerTransformer(method=self.method,standardize=False)\n            self.scl.fit(X)\n            return self\n        elif self.method == 'None':\n            return self\n\n    def transform(self, X):\n        if self.method == 'yeo-johnson':\n            Xscl = self.scl.transform(X)\n            Xscaled = pd.DataFrame(Xscl, index=X.index, columns=X.columns)\n            self.columns = X.columns\n            return Xscaled\n        elif self.method == 'None':\n            Xscl = X\n            Xscaled = pd.DataFrame(Xscl, index=X.index, columns=X.columns)\n            self.columns = X.columns\n            return Xscaled\n    def get_feature_names(self):\n        return list(self.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature scaling (click to view code)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class df_scaler(BaseEstimator, TransformerMixin):\n    def __init__(self, method=StandardScaler()):\n        super().__init__()\n        self.method = method        \n\n    def fit(self, X, y=None):\n        return self.method.fit(X)\n\n    def transform(self, X, y=None):\n        Xscl = self.method.transform(X)\n        Xscaled = pd.DataFrame(Xscl, index=X.index, columns=X.columns)\n        self.columns = X.columns\n        return Xscaled\n    def get_feature_names(self):\n        return list(self.columns)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### FeatureUnion (click to view code)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class FeatureUnion_df(TransformerMixin, BaseEstimator):\n    \n    def __init__(self, transformer_list, n_jobs=None, transformer_weights=None, verbose=False):\n        self.transformer_list = transformer_list\n        self.n_jobs = n_jobs\n        self.transformer_weights = transformer_weights\n        self.verbose = verbose  \n        self.feat_un = FeatureUnion(self.transformer_list, \n                                    self.n_jobs, \n                                    self.transformer_weights, \n                                    self.verbose)\n    def fit(self, X, y=None):\n        self.feat_un.fit(X)\n        return self\n\n    def transform(self, X, y=None):\n        X_tr = self.feat_un.transform(X)\n        columns = []\n        \n        for trsnf in self.transformer_list:\n            cols = trsnf[1].steps[-1][1].get_feature_names()  \n            columns += list(cols)\n\n        X_tr = pd.DataFrame(X_tr, index=X.index, columns=columns)\n        \n        return X_tr\n\n    def get_params(self, deep=True): \n        return self.feat_un.get_params(deep=deep)\n    def get_feature_names(self):\n        return self.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model selection (click to view code)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class Model_selection(BaseEstimator):\n    def __init__(self, estimator = CatBoostClassifier()):\n        self.estimator = estimator\n    def fit(self, X, y=None, **kwargs):\n        self.estimator.fit(X, y)\n        return self\n    def predict(self, X, y=None):\n        return self.estimator.predict(X)\n    def predict_proba(self, X):\n        return self.estimator.predict_proba(X)\n    def score(self, X, y):\n        return self.estimator.score(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### A. Data types and data completeness <a class=\"anchor\" id=\"section_1_1\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dataset.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the __*info function*__, we can pull the following information from our dataset :\n>- We have __3 categorical variables__ and __26 numerical variables__\n>- We have __missing values__ for the __*Income*__ variable\n\nAs a result : __Imputing missing values__ and __encoding catgorical features__ may be necessary if features are kept for prediction"},{"metadata":{},"cell_type":"markdown","source":"### B. Class distribution <a class=\"anchor\" id=\"section_1_2\"></a>"},{"metadata":{},"cell_type":"markdown","source":"On classification problems, analyzing the class distribution is always an important step as highly imbalanced data are common and need special treatment.  \nLet's check if data are imbalanced :"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"target = dataset['Response']\ncounter = Counter(target)\nfor k,v in counter.items():\n    per = v / len(target) * 100\n    print('Class=%s, Count=%d, Percentage=%.2f%%' % (k, v, per))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset is imbalanced. We will have to add a resampling step in our pipeline.  \nWe will use the SMOTETomek technique which combine a synthetic oversampling sequence (SMOTE) followed by an undersampling sequence (TOMEK)  \n**Step 1 :** Oversampling synthetically the minority class  \n**Step 2 :** Undersampling by cleaning the noise generated by the SMOTE technique"},{"metadata":{},"cell_type":"markdown","source":"### C. Feature Engineering <a class=\"anchor\" id=\"section_1_3\"></a>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"dataset.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Having a first look at the row data enables us to start thinking at some useful variables we could create in order to better understand our dataset and reveal precious information.  \n\nWe will create several variables :\n\n>- Variable __*Seniority*__ as the number of months the customer is enrolled with the company\n>- Variable __*Educationnal years*__ as the total number of years of education the individual achieved according to its diploma\n>- Variable __*Accepted offers*__ as the total number of offers accepted during the 5 past marketing campaigns\n>- Variable __*Segment*__ created from the <a href=\"https://www.kaggle.com/raphael2711/rfm-segmentation-and-clv-modeling\">RFM segmentation </a>  \n\nAs seen in the <a href=\"https://www.kaggle.com/raphael2711/data-prep-visual-eda-and-statistical-hypothesis\">previous notebooks</a>, we will remove the outlier in *Income* variable along with removing unused variables"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#features creation\nlast_date = date(2014,10, 4)\ndataset['Seniority']=pd.to_datetime(dataset['Dt_Customer'], dayfirst=True,format = '%Y-%m-%d')\ndataset['Seniority'] = pd.to_numeric(dataset['Seniority'].dt.date.apply(lambda x: (last_date - x)).dt.days, downcast='integer')/30\ndataset['Accepted_offers']=dataset['AcceptedCmp1']+dataset['AcceptedCmp2']+dataset['AcceptedCmp3']+dataset['AcceptedCmp4']+dataset['AcceptedCmp5']\ndataset['Educationnal_years']=dataset['Education'].replace({'Basic':5,'2n Cycle':8,'Graduation':12,'Master':18,'PhD':21})\ndataset = dataset[dataset['NumWebPurchases']+dataset['NumCatalogPurchases']+dataset['NumStorePurchases'] > 0]\ndataset=dataset.drop(columns=['AcceptedCmp1','AcceptedCmp2','AcceptedCmp3','AcceptedCmp4','AcceptedCmp5','Education','Dt_Customer','Z_CostContact','Z_Revenue'])\n#1.Outliers removal\ndataset = dataset.drop(dataset[dataset['Income']> 600000].index).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RFM calculation"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#1.Dataframe creation\nrecency_df = dataset[['ID','Recency']]\nfrequency_df = dataset[['ID','NumWebPurchases','NumCatalogPurchases','NumStorePurchases']]\nfrequency_df['Transactions']=frequency_df['NumWebPurchases']+frequency_df['NumCatalogPurchases']+frequency_df['NumStorePurchases']\nfrequency_df=frequency_df.drop(columns=['NumWebPurchases','NumCatalogPurchases','NumStorePurchases'])\n\ntemp_df = recency_df.merge(frequency_df,on='ID')\nmonetary_df = dataset[['ID','MntWines','MntFruits','MntMeatProducts','MntFishProducts','MntSweetProducts','MntGoldProds']]\nmonetary_df['Spending']=monetary_df['MntWines']+dataset['MntFruits']+dataset['MntMeatProducts']+dataset['MntFishProducts']+dataset['MntSweetProducts']+dataset['MntGoldProds']\nmonetary_df=monetary_df.drop(columns=['MntWines','MntFruits','MntMeatProducts','MntFishProducts','MntSweetProducts','MntGoldProds'])\nRFM=recency_df.merge(frequency_df,on='ID').merge(monetary_df,on='ID')\nRFM.columns = ['ID','Recency','Frequency','Monetary']\n\n#2.Clusters creation\nkmedoids_recency = KMedoids(n_clusters=5, random_state=0, max_iter=1000,init='k-medoids++',metric='euclidean').fit(RFM[['Recency']])\nRFM['RecencyCluster'] = kmedoids_recency.predict(RFM[['Recency']])\nkmedoids_frequency = KMedoids(n_clusters=5, random_state=0, max_iter=1000,init='k-medoids++',metric='euclidean').fit(RFM[['Frequency']])\nRFM['FrequencyCluster'] = kmedoids_frequency.predict(RFM[['Frequency']])\nkmedoids_monetary = KMedoids(n_clusters=5, random_state=0, max_iter=1000,init='k-medoids++',metric='euclidean').fit(RFM[['Monetary']])\nRFM['MonetaryCluster'] = kmedoids_monetary.predict(RFM[['Monetary']])\n\n3.#Clusters ordering\n#function for ordering cluster numbers\ndef order_cluster(cluster_field_name, target_field_name,df,ascending):\n    new_cluster_field_name = 'new_' + cluster_field_name\n    df_new = df.groupby(cluster_field_name)[target_field_name].mean().reset_index()\n    df_new = df_new.sort_values(by=target_field_name,ascending=ascending).reset_index(drop=True)\n    df_new['index'] = df_new.index\n    df_final = pd.merge(df,df_new[[cluster_field_name,'index']], on=cluster_field_name)\n    df_final = df_final.drop([cluster_field_name],axis=1)\n    df_final = df_final.rename(columns={\"index\":cluster_field_name})\n    return df_final\n\nRFM = order_cluster('RecencyCluster', 'Recency',RFM,False)\nRFM = order_cluster('FrequencyCluster', 'Frequency',RFM,True)\nRFM = order_cluster('MonetaryCluster', 'Monetary',RFM,True)\n\n\n#4.Segments creation\nsegt_map = {\n    r'30': 'Promising',\n    r'23': 'Loyal customers',\n    r'24': 'Loyal customers',\n    r'33': 'Loyal customers',\n    r'34': 'Loyal customers',\n    r'43': 'Loyal customers',\n    r'32': 'Potential loyalist',\n    r'31': 'Potential loyalist',\n    r'42': 'Potential loyalist',\n    r'41': 'Potential loyalist',\n    r'21': 'Need attention',\n    r'22': 'Need attention',\n    r'12': 'Need attention',\n    r'11': 'Need attention',\n    r'40': 'New customers',\n    r'20': 'About to sleep',\n    r'14': 'Cant loose them',\n    r'04': 'Cant loose them',\n    r'10': 'Lost',\n    r'00': 'Lost',\n    r'01': 'Lost',\n    r'02': 'At risk',\n    r'03': 'At risk',\n    r'13': 'At risk',\n    r'44': 'Champions',\n}\n#We create the segment and merge with our main dataset\nRFM['Segment'] = RFM['RecencyCluster'].map(str) + RFM['FrequencyCluster'].map(str)\nRFM['Segment'] = RFM['Segment'].replace(segt_map, regex=True)\ndataset=dataset.merge(RFM[['ID','Segment']],on='ID')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# count the number of customers in each segment\nsegments_counts = dataset['Segment'].value_counts().sort_values(ascending=True)\n\nfig, ax = plt.subplots()\n\nbars = ax.barh(range(len(segments_counts)),\n              segments_counts,\n              color='silver')\nax.set_frame_on(False)\nax.tick_params(left=False,\n               bottom=False,\n               labelbottom=False)\nax.set_yticks(range(len(segments_counts)))\nax.set_yticklabels(segments_counts.index)\n\nfor i, bar in enumerate(bars):\n        value = bar.get_width()\n        if segments_counts.index[i] in ['Champions', 'Loyal customers']:\n            bar.set_color('firebrick')\n        ax.text(value,\n                bar.get_y() + bar.get_height()/2,\n                '{:,} ({:}%)'.format(int(value),\n                                   int(value*100/segments_counts.sum())),\n                va='center',\n                ha='left'\n               )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### D. Skew of univariate distribution <a class=\"anchor\" id=\"section_1_4\"></a>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"num_feats=dataset.dtypes[dataset.dtypes!='object'].index\nskew_feats=dataset[num_feats].skew().sort_values(ascending=False)\nskewness=pd.DataFrame({'Skew':skew_feats})\nprint('We have {} skewed numerical features'.format(skewness[abs(skewness) > 0.75].shape[0]))\ndisplay(skewness.style.background_gradient(cmap = 'Reds', axis = 0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will add a power transform step in our pipeline. Depending on the selected model, making data more Gaussian-like could improve the obtained performance"},{"metadata":{},"cell_type":"markdown","source":"### E. Feature Selection <a class=\"anchor\" id=\"section_1_5\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Feature selction enables us to :\n>- Remove redundant data leading to reducing overfitting\n>- Remove irrelevant data leading to improving accuracy\n>- Reduce number of features leading to faster training time\n\nVariance analysis, correlation analysis and multicollinearity analysis will be done.   \nUnivariate feature selection will be done with ANOVA F-value calculation and will be integrated in the gridsearching process"},{"metadata":{},"cell_type":"markdown","source":"### Feature variance analysis"},{"metadata":{},"cell_type":"markdown","source":"Features with low variances are believed to contain very few information as they are approximatively constant and will probably not improve the performance of the model"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Features = []\nval=[]\nvar=[]\nX=dataset.drop(columns=['Segment','Marital_Status','Response'])\n\nfor column in X:\n    most_freq_value = np.round((X[column].value_counts(normalize = True).iloc[0])*100, 2)\n    variance=X[column].var()\n    Features.append(column)\n    val.append(most_freq_value)\n    var.append(variance)\ncount = pd.DataFrame(list(zip(Features, val,var)), columns =['Feature', 'Count%','Variance']).sort_values(ascending=False,by='Count%')\ndisplay(count.style.background_gradient(cmap = 'Reds', axis = 0,subset='Count%'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that \"Complain\" variable has a variance almost equal to 0 and that 99% of the rows have the same value.  \nWe will therefore remove it."},{"metadata":{},"cell_type":"markdown","source":"### Correlation analysis"},{"metadata":{},"cell_type":"markdown","source":"Correlation refers to the relationship between two variables and how they move together.  "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Finding the relations between the variables.\nplt.figure(figsize=(20,10))\nc= dataset.corr(method='spearman')\nsns.heatmap(c,annot=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the variable *Income* has a high correlation with other variables. This can inform us of the presence of multicollinearity."},{"metadata":{},"cell_type":"markdown","source":"### Multicollinearity analysis"},{"metadata":{},"cell_type":"markdown","source":"To detect multicollinearity, we will use the __*Variable Inflation Factors*__ method (VIF). It measures the strength of the correlation between our independant variables.  \nWe set our max threshold at 10"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Filter on non null value and exclude categoricals + target + Complain (because will be removed as seen earlier)\nVIF = dataset[dataset['Income'].notnull()].drop(columns=['Segment','Marital_Status','Response','Complain'])\nvif_data = pd.DataFrame()\nvif_data[\"Feature\"] = VIF.columns\n# calculating VIF for each feature \nvif_data[\"VIF\"] = [variance_inflation_factor(VIF.values, i) for i in range(len(VIF.columns))]\nvif_data=vif_data.sort_values(by='VIF',ascending=False)\nvif_data.style.background_gradient(cmap = 'Reds', axis = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that *NumStorePurchases*, *NumWebVisitsMonth*, *Year_Birth*, *Income* and *Educationnal_years* have high VIF.  \n>- As presumed, the high correlation coefficient of Income result in a high VIF\n>- We assume Year_Birth to be a variable that can be easily predicted by others like Kidhome, Teenhome, Income...  \n\nWe will remove the variable Year_Birth and Income and recheck if the VIF of other variables dropped"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#We remove Year_Birth + Income and recalculate the VIF of other features \nVIF_filter=dataset.drop(columns=['Income','Segment','Marital_Status','Response','Complain','Year_Birth'])\nvif_filtered_data = pd.DataFrame() \nvif_filtered_data[\"Feature\"] = VIF_filter.columns \nvif_filtered_data[\"VIF\"] = [variance_inflation_factor(VIF_filter.values, i) for i in range(len(VIF_filter.columns))]\n#Filter to variables with VIF greater than 5 as the others are ok\nvif_filtered_data=vif_filtered_data[vif_filtered_data[\"VIF\"] > 5]\nvif_filtered_data=vif_filtered_data.sort_values(by='VIF',ascending=False)\nvif_filtered_data.style.background_gradient(cmap = 'Reds', axis = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that VIF of *Educational_years* decreased. Nonetheless, the VIF of *NumWebVisitsMonth* remains above 10.  \nAs this variable is not a direct aggregation of others variables, we decide to remove this single variable and keep the others.\n\n`Nota Bene : From here, we will also remove the variable ID as it's the unique identifier of the customer and it will be useless for the classification model`"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset=dataset.drop(columns=['Income','NumWebVisitsMonth','ID','Complain','Year_Birth'])\n#Final dataset before modeling\ndataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split data into train test"},{"metadata":{},"cell_type":"markdown","source":"We keep 20% of our dataset as unseen data to test the performance of our final model.  \nWe will perform a stratified train-test split to ensure we have the same proportion of examples in each class for both the train and test set"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"X=dataset.drop(columns=['Response'])\nY=dataset['Response']\n#1. Split data into X and Y. We use stratify to keep an equal proportion of examples in each class between train set and test set\nX_train, X_test, y_train, y_test = train_test_split(X,Y ,test_size=0.2, random_state=1,shuffle=True, stratify=Y)\ntarget = y_train\ncounter = Counter(target)\nfor k,v in counter.items():\n    per = v / len(target) * 100\n    print('Class=%s, Count=%d, Percentage=%.2f%%' % (k, v, per))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pipeline creation"},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_pipe = Pipeline([('fs', feat_sel(dtype='numeric')),  # Select only the numeric features\n                         ('unskewed', df_unskewed()), #Unskew data\n                         ('scl', df_scaler()) # Scale data\n                        ])  \ncategorical_pipeline = Pipeline( steps = [( 'fs', feat_sel(dtype='category')), # Select only the categorical features\n                                          ('label_encoder', Label_encode()), #Label encoder\n                                          ])\nprocessing_pipe = FeatureUnion_df(transformer_list=[('cat_pipe', categorical_pipeline),\n                                                    ('num_pipe', numeric_pipe)\n                                                   ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Model creation <a class=\"anchor\" id=\"section_2\"></a>"},{"metadata":{},"cell_type":"markdown","source":"### A. Evaluate algorithm baseline performance with nested cross-validation<a class=\"anchor\" id=\"section_2_1\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Our first step will be to test different models without heavy hyperparameter tuning.  \nWe will evaluate the different models with weighted F1-score.  \nAs we are running model selection and hyperparameter tunning at the same time, we will do a nested cross-validation to get an unbiaised estimate of model performance. **Two cross-validation loops are performed in parallel:** \n- One inner loop by the GridSearchCV estimator for hyperparameter otpimization, preprocessing steps and feature selection \n- One outer loop to measure the prediction performance of the estimator.  \n\nThe resulting scores are unbiased estimates of the prediction score on new data. Nested cross-validation along the use of pipelines enables us to prevent data leakage."},{"metadata":{},"cell_type":"markdown","source":"We use a mix of different models :\n- 2 linear models (Logistic Regression and Linear Discriminant Analysis)\n- 2 nonlinear models (Support Vector Machine and K-Nearest Neighbors)\n- 2 ensemble algorithms (Random Forest and Catboost)\n\nOur Nested cross validation will evaluate a total of 3690 combinations (246x3x5)"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n#Outer loop\ncv_outer = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\nouter_results = list()\n\nfor train_ix, valid_ix in cv_outer.split(X_train,y_train):\n    Xtrain, X_valid = X_train.iloc[train_ix, :], X_train.iloc[valid_ix, :]\n    ytrain, y_valid = y_train.iloc[train_ix], y_train.iloc[valid_ix]\n       \n    #our pipeline    \n    pipe = Pipeline([\n    ('prep',processing_pipe),\n    ('selector', SelectKBest(f_classif, k=10)),\n    ('resample' ,SMOTETomek(sampling_strategy=0.7,random_state=1)),\n    ('est', Model_selection())])\n    \n    #search space containing feature selection, preprocessing steps and model hyperparameters\n    search_space = [\n                {'est__estimator':[SVC()],\n                 'selector__k': [10,14,18],\n                 'est__estimator__C': [0.1,1,10],\n                 'est__estimator__kernel': ['rbf', 'poly'],\n                 'est__estimator__random_state' : [1],\n                 'est__estimator__verbose' : [False],\n                 'prep__num_pipe__unskewed__method': ['yeo-johnson','None'],\n                 'prep__num_pipe__scl__method': [StandardScaler(),RobustScaler()]},\n        \n                {'est__estimator':[LogisticRegression()],\n                'selector__k': [10,14,18],\n                'est__estimator__random_state' : [1],\n                 'est__estimator__verbose' : [False],\n                 'prep__num_pipe__unskewed__method': ['yeo-johnson','None'],\n                 'prep__num_pipe__scl__method': [StandardScaler(),RobustScaler()]},    \n        \n                {'est__estimator':[KNeighborsClassifier()],\n                 'selector__k': [10,14,18],\n                 'est__estimator__n_neighbors': [3,5,11,15],\n                 'est__estimator__metric' : ['euclidean', 'manhattan'],\n                 'prep__num_pipe__unskewed__method': ['yeo-johnson','None'],\n                 'prep__num_pipe__scl__method': [StandardScaler(),RobustScaler()]},\n        \n                {'est__estimator':[LinearDiscriminantAnalysis()],\n                 'selector__k': [10,14,18],\n                 'est__estimator__solver':  ['svd', 'lsqr'],\n                 'prep__num_pipe__unskewed__method': ['yeo-johnson','None'],\n                 'prep__num_pipe__scl__method': [StandardScaler(),RobustScaler()]},\n        \n                {'est__estimator':[RandomForestClassifier()],\n                 'selector__k': [10,14,18],\n                 'est__estimator__random_state' : [1],\n                 'est__estimator__n_estimators': [10, 100, 1000],\n                 'est__estimator__max_features' : ['sqrt', 'log2'],\n                 'prep__num_pipe__scl__method': [StandardScaler(),RobustScaler()]}, \n\n                {'est__estimator': [CatBoostClassifier()],\n                 'selector__k': [10,14,18],\n                 'est__estimator__random_state' : [1],\n                 'est__estimator__silent' : [True],\n                 'est__estimator__early_stopping_rounds':[100],\n                 'est__estimator__loss_function':['CrossEntropy'],\n                 'prep__num_pipe__scl__method': [StandardScaler(),RobustScaler()]},\n                  ]\n    #inner loop\n    cv_inner=StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n    clf = GridSearchCV(pipe, search_space,cv=cv_inner, scoring='f1_weighted',refit=True)\n    clf.fit(Xtrain, ytrain)\n\n    best_model = clf.best_estimator_\n    # evaluate model on the hold out dataset\n    yhat = best_model.predict(X_valid)\n    # F1 score\n    F1 = f1_score(y_valid, yhat,average='weighted')\n    # store the result\n    outer_results.append(F1)\n    # report best model for each fold of the outer loop\n    print('>F1=%.3f, best score=%.3f, model=%s' % (F1, clf.best_score_, clf.best_params_))\n# Get a summarized result\nprint('Weighted F1-score: %.3f (%.3f)' % (mean(outer_results), std(outer_results)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can have a look at the result of the last fold. we can see the top models are indeed SVC, Catboost and Random Forest with 18 features.  \nThese 3 estimators seem to be the most promising and we will retain them for deeper hyperparameter tuning.  \nRegarding our preprocessing steps, we will retain RobustScaler and no power transformation."},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_cv_results(search):\n    df = pd.concat([pd.DataFrame(clf.cv_results_[\"params\"]),pd.DataFrame(clf.cv_results_[\"mean_test_score\"], columns=[\"Score\"])],axis=1)\n    df = df.sort_values(\"Score\", ascending=False)\n    return df.fillna(value=\"\")\ndf_res = format_cv_results(clf)\ndf_res.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### B. Hyperparameter tuning with Optuna <a class=\"anchor\" id=\"section_2_2\"></a>"},{"metadata":{},"cell_type":"markdown","source":"#### Catboost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective_catboost(trial):\n    numeric_pipe = Pipeline([('fs', feat_sel(dtype='numeric')),  # Select only the numeric features\n                             ('scl', df_scaler(method=RobustScaler())) # Scale data\n                             ]) \n                         \n    categorical_pipeline = Pipeline( steps = [( 'fs', feat_sel(dtype='category')), # Select only the categorical features\n                                              ('label_encoder', Label_encode()), #Label encoder\n                                             ])\n    processing_pipe = FeatureUnion_df(transformer_list=[('cat_pipe', categorical_pipeline),\n                                                        ('num_pipe', numeric_pipe)])        \n    \n    cv_outer=StratifiedKFold(n_splits=5, random_state=1,shuffle=True)  \n    param = {        \n        'learning_rate': trial.suggest_float('learning_rate', 0.01,0.1),\n        'n_estimators': trial.suggest_int( \"n_estimators\",  50,300,20),\n        'max_depth': trial.suggest_int( 'max_depth',  1,5,1),\n        'l2_leaf_reg': trial.suggest_int('l2_leaf_reg',1,5,1),\n        'random_state': trial.suggest_categorical('random_state',[1]),\n        'loss_function': trial.suggest_categorical('loss_function',['CrossEntropy']),\n        'eval_metric': trial.suggest_categorical('eval_metric',['F1']),\n        'silent':trial.suggest_categorical('silent',[True]), \n        'early_stopping_rounds':trial.suggest_categorical('early_stopping_rounds',[100])}   \n    pipe = Pipeline([\n        ('prep',processing_pipe),\n        ('resample' ,SMOTETomek(sampling_strategy=0.7,random_state=1)),\n        ('est', CatBoostClassifier(**param))])     \n    return cross_val_score(pipe, X_train, y_train,cv=cv_outer,scoring=\"f1_weighted\").mean()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"%%time\nmodels=[]\nscores=[]\ncatboost_study = optuna.create_study(direction='maximize')\ncatboost_study.optimize(objective_catboost, n_trials=50)\nmodel='Catboost'\nscore=catboost_study.best_trial.value\nmodels.append(model)\nscores.append(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of finished trials:', len(catboost_study.trials))\nprint('Best trial: score {}, params {}'.format(catboost_study.best_trial.value, catboost_study.best_trial.params))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### RandomForest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective_random_forest(trial):\n    numeric_pipe = Pipeline([('fs', feat_sel(dtype='numeric')),  # Select only the numeric features\n                         ('scl', df_scaler(method=RobustScaler())) # Scale data\n                        ])  \n    categorical_pipeline = Pipeline( steps = [( 'fs', feat_sel(dtype='category')), # Select only the categorical features\n                                          ('label_encoder', Label_encode()), #Label encoder\n                                          ])\n    processing_pipe = FeatureUnion_df(transformer_list=[('cat_pipe', categorical_pipeline),\n                                                    ('num_pipe', numeric_pipe)\n                                                   ])\n    \n    cv_outer=StratifiedKFold(n_splits=5, random_state=1,shuffle=True)\n    param = {        \n        'n_estimators': trial.suggest_int( 'n_estimators',  10,1000,200),\n        'max_features': trial.suggest_categorical( 'max_features', ['sqrt', 'log2']),\n        'max_depth': trial.suggest_categorical('max_depth',[10, 20, 30,None]),\n        'bootstrap': trial.suggest_categorical('bootstrap',[True, False]),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf',1,9,2),\n        'random_state': trial.suggest_categorical('random_state',[1]),\n        'criterion':trial.suggest_categorical('criterion',['gini', 'entropy']),\n        'verbose':trial.suggest_categorical('verbose',[0]), \n        }\n        \n    pipe = Pipeline([\n        ('prep',processing_pipe),\n        ('resample' ,SMOTETomek(sampling_strategy=0.7,random_state=1)),\n        ('est', RandomForestClassifier(**param))])     \n    return cross_val_score(pipe, X_train, y_train,  cv=cv_outer,scoring=\"f1_weighted\").mean()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"%%time\nRF_study = optuna.create_study(direction='maximize')\nRF_study.optimize(objective_random_forest, n_trials=50)\nmodel='Random Forest'\nscore=RF_study.best_trial.value\nmodels.append(model)\nscores.append(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of finished trials:', len(RF_study.trials))\nprint('Best trial: score {}, params {}'.format(RF_study.best_trial.value, RF_study.best_trial.params))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SVC Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective_SVC(trial):\n    numeric_pipe = Pipeline([('fs', feat_sel(dtype='numeric')),  # Select only the numeric features\n                         ('scl', df_scaler(method=RobustScaler())) # Scale data\n                        ])  \n    categorical_pipeline = Pipeline( steps = [( 'fs', feat_sel(dtype='category')), # Select only the categorical features\n                                          ('label_encoder', Label_encode()), #Label encoder\n                                          ])\n    processing_pipe = FeatureUnion_df(transformer_list=[('cat_pipe', categorical_pipeline),\n                                                    ('num_pipe', numeric_pipe)\n                                                   ])\n    cv_outer=StratifiedKFold(n_splits=5, random_state=7,shuffle=True)\n    param = {        \n        'C': trial.suggest_loguniform('C', 1e-5, 1e2),\n        'gamma': trial.suggest_loguniform('gamma',1e-4, 1e1), \n        'kernel': trial.suggest_categorical('kernel', ['rbf', 'poly']),\n        'random_state': trial.suggest_categorical('random_state',[1]),\n        'verbose':trial.suggest_categorical('verbose',[0]), \n            }\n        \n    pipe = Pipeline([\n        ('prep',processing_pipe),\n        ('resample' ,SMOTETomek(sampling_strategy=0.7,random_state=1)),\n        ('est', SVC(**param))])     \n    return cross_val_score(pipe, X_train, y_train, cv=cv_outer, scoring=\"f1_weighted\").mean()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"%%time\nSVC_study = optuna.create_study(direction='maximize')\nSVC_study.optimize(objective_SVC, n_trials=50)\nmodel='SVC'\nscore=SVC_study.best_trial.value\nmodels.append(model)\nscores.append(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of finished trials:', len(SVC_study.trials))\nprint('Best trial: score {}, params {}'.format(SVC_study.best_trial.value, SVC_study.best_trial.params))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Best model"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"best_model = pd.DataFrame(list(zip(models, scores)), columns =['Model', 'Score']).sort_values(ascending=False,by='Score')\nbest_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After Hyperparameter tuning, we can see Catboost Classifier get the highest score.\nWe will therefore select Catboost with the parameters presented below :"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"best_params = catboost_study.best_params\nbest_params","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If desired, we can have  look at the trials done by Optuna. We display here the Top 5 trials ranked by weighted F1-score"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"trials_df = catboost_study.trials_dataframe().sort_values(ascending=False,by='value')\ndisplay(trials_df.head(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Model results <a class=\"anchor\" id=\"section_3\"></a>"},{"metadata":{},"cell_type":"markdown","source":"### A. Visual Hyperparameter Optimization Analysis <a class=\"anchor\" id=\"section_3_1\"></a>"},{"metadata":{},"cell_type":"markdown","source":"#### Hyperparameter Importance"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# plot feature importance for algorithm parameters\nvisualization.plot_param_importances(catboost_study)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Weighted F1-score History"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"optuna.visualization.plot_optimization_history(catboost_study)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### B. Model Evaluation Metrics and Confusion Matrix<a class=\"anchor\" id=\"section_3_2\"></a>"},{"metadata":{},"cell_type":"markdown","source":"### Predictions and Classification report"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"numeric_pipe = Pipeline([('fs', feat_sel(dtype='numeric')),  # Select only the numeric features\n                         ('scl', df_scaler(method=StandardScaler())) # Scale data\n                        ])  \ncategorical_pipeline = Pipeline( steps = [( 'fs', feat_sel(dtype='category')), # Select only the categorical features\n                                          ('label_encoder', Label_encode()), #Label encoder\n                                          ])\nprocessing_pipe = FeatureUnion_df(transformer_list=[('cat_pipe', categorical_pipeline),\n                                                    ('num_pipe', numeric_pipe)\n                                                   ])\n\npipe = Pipeline([\n        ('prep',processing_pipe),\n        ('resample' ,SMOTETomek(sampling_strategy=0.7,random_state=1)),\n        ('est', CatBoostClassifier(**best_params))])     \n\npipe.fit(X_train,y_train)\nprediction = pipe.predict(X_test)\n\nprint('Weighted F1_score: %.0f%% ' % (f1_score(y_test, prediction, average='weighted')*100)) \nprint(classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Weighted F1-score and Precision of the Class 1 score on the unseen test dataset are very statisfying for this business case"},{"metadata":{},"cell_type":"markdown","source":"### Confusion matrix"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# confusion marix for the test data\ncm = confusion_matrix(y_test, prediction,  labels=[1,0])\n\nfig, ax= plt.subplots(figsize=(10,10))\nsns.heatmap(cm, annot=True, fmt='g', ax = ax); \n\n# labels, title and ticks\nax.set_xlabel('Predicted labels');\nax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(['Responsive','Not responsive']); \nax.yaxis.set_ticklabels(['Responsive','Not responsive']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### C. Cumulative Gains and Lift charts<a class=\"anchor\" id=\"section_3_3\"></a>"},{"metadata":{},"cell_type":"markdown","source":"In addition to the confusion matrix and F1-score, we can use two other meaningful metrics to measure the efficiency of our model : Cumulative gains and the lift chart.\n- **Cumulative gains :** by ordering customers with the highest probability of accepting the marketing offer, it shows the proportion of customers that would be converted considering a sample size. \n- **Lift chart :** the lift is a measure of the performance of a targeting model at classifying cases as having an enhanced response, measured against a random choice targeting model"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"predicted_probabilities = pipe.predict_proba(X_test)\nskplt.metrics.plot_cumulative_gain(y_test, predicted_probabilities);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we target only the **top 20%** of customers, we would expect to convert almost **80%** of the total customers that would actually accept our marketing campaign offer.  \nThe diagonal line is the \"baseline\" curve, if we select 10% of the customers at random, we would expect to \"gain\" approximately 10% of all of the cases that actually accepted the marketing campaign offer."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"skplt.metrics.plot_lift_curve(y_test, predicted_probabilities);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the lift chart, we can see that by targeting 20% of customers, the model addresses almost **4 times** more targets for this group, compared with addressing without the model, meaning randomly."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}