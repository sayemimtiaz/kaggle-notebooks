{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://www.melhoresdestinos.com.br/wp-content/uploads/2015/11/Dicas-Airbnb.jpg)<br><br>\n\nAirbnb is an **online marketplace** for short-term house and apartment rentals. With it you can, for example, rent out your house for a week while you’re away, or rent out your empty bedroom. Since Airbnb is a market, pricing works just like anything in life: the amount a host usually charges is closely tied to market prices(supply and demand). The search experience on Airbnb looks like this:\n\n<br><br>\n![](https://66.media.tumblr.com/f06784bb36b2d0d2914d1bb8af40f48d/tumblr_nnhegoQEfu1ur02gdo1_1280.png)\n<br><br>\n**Goal of the analysis**\n\nAs seen above, the general Airbnb host is someone who has some spare space and want to get some revenue, without having to worry too much about advertising and insurance. \nAirbnb does it all for the host, but there is one thing it can't do: help price their property. Since the price can vary a lot according to country, currency, neighborhood and the host itself, it is simply too complex to provide a general service on that.\n<br>\nThe goal of the analysis is to visualize and clean overall Airbnb data from **Rio de Janeiro**, and then build a predictive model specific to the city, in order to assist hosts from Rio to price their listings based on the market standard.\n<br>\nSince the model is intended to be used by the general public, possibly some accuracy will be given up in exchange for a higher simplicity.\n<br>\n\n**Deployment**\n\nAfter the model is complete, the goal is to make it available to the general public, first on a web application, and on the future, possibly on an app available to **Android** and **iPhone**. <br>\n\n\n\n**Data**\n\nThe dataset used contains detailed data about every listing on the site for every month(except june 2018, the source did not scrape data for that month), from april 2018, to may 2020. The main characteristics I consider to be relevant for our model are:\n\n`Seasonality`: do prices tend to raise or decrease at specific months?\n`Features of the listing`: How big is the property? how many bedrooms, beds and bathrooms?\n`Behavior of the host`: Do listings with more detailed description tend to be priced higher? Does the host demand clear identification from the renter?\n`Neighborhood`: How much does the geographic position influences the price?\n\n**The currency for every price contained in the dataset is reais(R$).**<br><br>\n\n![](https://super.abril.com.br/wp-content/uploads/2017/12/rio-de-janeiro.png)\n\n\n**Credits**\n\nI want to thank [duygut](https://www.kaggle.com/duygut) for this [great work](https://www.kaggle.com/duygut/airbnb-nyc-price-prediction) on a similar dataset from NYC. Reading it made me learn a lot of statistics and get inspiration for some data visualizations.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First, we import the necessary modules for our analysis:","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np #Deal with numbers and arrays\nimport pandas as pd #Create dataframes and sort/clean data faster\n\nimport matplotlib.pyplot as plt #Visualization module\nimport seaborn as sns #Makes matplotlib prettier\n\n#This blocks imports everything needed from sklearn(models, procession packages, and metrics)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.linear_model import LinearRegression, SGDRegressor, Lasso, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.svm import SVR, LinearSVR\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom xgboost import XGBRegressor # XGB regression model\nimport plotly.express as px #Module for dynamic data visualization","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just for note: this is the code used to generate the dataset used. More info on how it works [here].(https://www.kaggle.com/allanbruno/airbnb-rio-de-janeiro)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"```\n#THIS PART OF THE CODE RAN ONLY ONCE, AND IT'S WHAT MERGED THE DATASETS USED IN THIS ANALYSIS\ndf = pd.DataFrame()\nfilenames = {'agosto2018':8, 'setembro2018':9, 'outubro2018':10, 'novrmbro2018':11, 'dezembro2018':12,'janeiro2019':1, 'fevereiro2019':2, 'maro2019':3, 'abril2019':4, 'maio2019':5, 'junho2019':6, 'julho2019':7, 'agosto2019':8, 'setembro2019':9, 'outubro2019':10, 'novembro2019':11, 'dezembro2019':12, 'janeiro2020':1, 'fevereiro2020':2, 'maro2020':3, 'abril2020':4, 'maio2020':4}\n\nfor month, month_number in filenames.items():\n    df_month = pd.read_csv(f'../input/airbnb-rio-de-janeiro/{month}.csv')\n    df_month['month'] = month_number\n    df = df.append(df_month)\n\ndf.to_csv('total_data.csv')\n```","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Columns of the dataframe:\n\nThe initial goal was to load the entire dataframe and look into every column, but that led to some memory leak. So, in order to make the analysis possible, and to be able to run every single model trial, we'll only import the columns used in the model.\n<br>\nFor more info on every column available, check the [dataset](https://www.kaggle.com/allanbruno/airbnb-rio-de-janeiro).<br><br>\n\nMore about each feature chosen:\n\n- `host_is_superhost`: t for true and f for false. Says if the host is a superhost, which indicates its credibility with the platform<br>\n- `host_listings_count`: amount of listings the host has on the market.<br>\n- `'property_type`: the kind of property(apartment, house).<br>\n- `accommodates`: the maximum amount of people that can be in the listing at a time.<br>\n- `bathrooms`: amount of bathrooms.<br>\n- `bedrooms`: amount of bedrooms.<br>\n- `beds`: amount of beds.<br>\n- `amenities`: refers to whether the listing has TV, wifi, wir conditioning, and much more amenities.<br>\n- `price`: price charged for a day(doesn't include promotional prices to full weeks).<br>\n- `require_guest_profile_picture`: Iif the host requires that the guest has a profile picture(safety measure).<br>\n- `require_guest_phone_verification`: if the host requires that the guest has the phone number verified(safety measure).<br>\n- `month`: month it was scraped from the web(corresponds to 1-12, and doesn't tell the year).<br>\n- `security_deposit`: if the host requires a safety deposit in case anything breaks.<br>\n- `cleaning_fee`: if the host requires a cleaning fee, charged everytime a listing is rented.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Parsing the dataset into a pandas data frame","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/airbnb-rio-de-janeiro/total_data.csv', index_col=False, usecols=['host_is_superhost', 'host_listings_count', 'latitude', 'longitude',\n       'property_type', 'accommodates', 'bathrooms', 'bedrooms', 'beds',\n       'amenities', 'price', 'require_guest_profile_picture',\n       'require_guest_phone_verification', 'month', 'security_deposit','cleaning_fee'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Initial parsing of dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First, let's take a look at the dataset:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Shape of the dataset:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Rows: {df.shape[0]}, Columns: {df.shape[1]}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking for NaN's","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_rows = df.shape[0]\nprint(f'security_deposit missing values are in {round(361064*100/total_rows,1)}% of rows, and cleaning_fee missing values are in {round(269336*100/total_rows,1)}% of rows.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping `security_deposit` and `cleaning_fee` from the dataset seems to be the most optimal decision, since those variables have simply way too much missing values compared to other variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rows_before_drop = df.shape[0]\ndf.drop(['security_deposit','cleaning_fee'], axis=1, inplace=True)\nprint(f\"'security_deposit' and 'cleaning fee' dropped :)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we have a lot of data, we can afford to drop the most obvious NaN's. In the code next ahead, rows with any missing value will be dropped from the dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(inplace=True)\nprint(f'{df.shape} - {rows_before_drop-df.shape[0]} rows dropped')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check if there is any missing value left:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, it is wise to check if any column has any kind of unusual value or data type:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Checking data types","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- `host_listings_count` Should be a `integer` column, not `object`\n\n- `accommodates` should also be an `integer` column, not `object`\n\n- `price should` also be an + `integer` column, not `object`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# CHANGING DATA TYPES\n\n#host_listings_count\ndf['host_listings_count'] = df['host_listings_count'].astype(np.float32, copy=False)\ndf['host_listings_count'] = df['host_listings_count'].astype(np.int16, copy=False)\n#accommodates\ndf['accommodates'] = df['accommodates'].astype(np.int16, copy=False)\n#price\ndf['price'] = df['price'].str.replace('$', '', regex=False)\ndf['price'] = df['price'].str.replace(',', '', regex=False)\ndf['price'] = df['price'].astype(np.float32, copy=False)\ndf['price'] = df['price'].astype(np.int32, copy=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**For programmers**: whenever you have a massive amount of data, it is a good habit to check if you can convert integers and floats from 64 bits to 8-16-32 bits. On a large scale, it represents a significant additional amount of memory available for other uses.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we're good to get into...","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Exploratory data analysis along with data cleaning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The EDA in the notebook will be performed in each variable that contains outliers, in a way that you can visualize the difference how dealing with outliers improves the overall distribution of a dataset.\n<br>\n<br>\n### Dealing with outliers\nTo better deal with outliers than simply remove it by intuition, we'll use the **descriptive statistic** method [**interquartile range**](https://en.wikipedia.org/wiki/Interquartile_range).<br>\nFirst, define a function to get the `max_fence_value`, which is defined as the highest value a variable can have so that it is not considered an outlier. Any value bigger than that will go through some sort of modification.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_max_fence(column):\n    qt = df[column].quantile([0.25,0.75])\n    upper = qt.values[1]\n    iqr = upper-qt.values[0]\n    max_fence = upper + 1.5*(iqr)\n    return max_fence","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, a function to help us better visualize the IQR:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def box_plot(column):\n    fig, (ax1, ax2) = plt.subplots(1,2)\n    fig.set_size_inches(16,6)\n    _ = sns.boxplot(x=df[column], ax = ax1)\n    ax1.set_title(f'{column} boxplot')\n    ax2.set_title(f'Zooming in the {column} boxplot')\n    ax2.set_xlim((-0.1,1.1*get_max_fence(column)))\n    _ = sns.boxplot(x=df[column], ax = ax2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First adjustment: `host_listings_count`\n\nLet's take a look at its distribution:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"column = 'host_listings_count'\nplt.figure(figsize=(16,8))\nax = sns.barplot(x=df[column].value_counts().index, y=df[column].value_counts(), color='g')\nax.set_xticklabels(map(int,df['beds'].index))\nax.set_xlim((-0.5,get_max_fence(column)+2))\nax.set_xlabel('Amount of listings by host')\n_ = ax.set_title('Distribution of host_listings_count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column = 'host_listings_count'\nbox_plot(column)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(get_max_fence('host_listings_count'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to the method chosen, values bigger than 6 are outliers. We have a lot of outlying data. So, what do we do?\n\nSince the goal of the study is to predict how regular people with one or a few vacant properties price them, hosts with more than 6 properties will be deleted from the analysis(this helps removing companies that rent on Airbnb and have way more listings than usual).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rows_before = df.shape[0]\ndf = df[df['host_listings_count'] <= get_max_fence('host_listings_count')]\nprint(f'{rows_before-df.shape[0]} rows were deleted.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This boxplot also shows us that there is some zero values. These values will be changed to 1, since if the hosts didn't have any listing, they wouldn't be on the dataset in the first place.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['host_listings_count'] == 0.0, 'host_listings_count'] = 1.0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we're good","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**PRICE**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nax = sns.distplot(df['price'],norm_hist=True)\n_ = ax.set_title('Price distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"box_plot('price')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_max_fence('price')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This shows us that the daily prices over 1276 reais are outliers of our regression, so let's remove them.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rows_before = df.shape[0]\ndf = df[df['price'] <= get_max_fence('price')]\nprint(f'{rows_before-df.shape[0]} rows were removed')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nax = sns.distplot(df['price'],norm_hist=True)\n_ = ax.set_title('Price distribution - Outliers removed')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The interesting thing that shows us how human the dataset is, is that there are peaks at full values(400,600,1000), which shows a human common behavior: rounding values.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**PROPERTY TYPE**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nax = sns.countplot('property_type', data=df)\nax.tick_params(axis='x', rotation=90)\n_ = ax.set_title('Distribution of property types')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the sake of simplifying categorical variables, we will be appending every property type with less than 1000 occurrences to a single category, called \"Other\".","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"categories_to_append = ('Aparthotel', 'Earth house', 'Chalet', 'Cottage', 'Tiny house',\n                        'Boutique hotel', 'Hotel', 'Casa particular (Cuba)', 'Bungalow',\n                        'Nature lodge', 'Cabin', 'Castle', 'Treehouse', 'Island', 'Boat', 'Tent',\n                        'Resort', 'Hut', 'Campsite', 'Barn', 'Dorm', 'Camper/RV', 'Farm stay', 'Yurt',\n                        'Tipi', 'Pension (South Korea)', 'Dome house', 'Igloo', 'Casa particular',\n                        'Houseboat', 'Lighthouse', 'Plane', 'Train', 'Parking Space')\n\nfor cat in categories_to_append:\n    df.loc[df['property_type'] == cat, 'property_type'] = 'Other'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nax = sns.countplot('property_type', data=df)\nax.tick_params(axis='x', rotation=20)\n_ = ax.set_title('Distribution of property types')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aa = df.groupby(by='property_type').mean().sort_values(by='price',ascending=False).iloc[0:6]\nfig, (ax1,ax2) = plt.subplots(1,2)\nfig.set_size_inches(20,8)\nviolin_data=df.loc[df['property_type'].isin(aa.index)]\n_ =  sns.barplot(x=aa.index, y='price', data=aa,ax=ax1)\n_ = ax1.set_title('Average price of property_type')\n_ = ax2.set_title('Price distribution of property_type')\n_ = sns.violinplot(x = 'property_type', y =  'price',data=violin_data,ax=ax2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**BEDS**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nax = sns.countplot('beds', data=df)\nax.set_xticklabels(map(int,df['beds'].index))\nax.set_xlabel('Amount of beds')\n_ = ax.set_title('Distribution of beds')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"box_plot('beds')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_max_fence('beds')\nrows_before = df.shape[0]\ndf = df[df['beds'] <= get_max_fence('beds')]\nprint(f'{rows_before-df.shape[0]} rows were removed')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nax = sns.countplot('beds', data=df)\nax.set_xticklabels(map(int,df['beds'].index))\n_ = ax.set_title('Distribution of beds')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**AMENITIES**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The analysis can get pretty complicated, if we choose to individually consider each amenity.<br>\n\n**Unique amenities dataset sample:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['amenities'].unique()[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since there is a lot of data for this regression, I will make an approximation:\n\nThe column `amenities` will be dropped, and another column called `n_amenities` will be created, containing the number of amenities each listing has. This way, we won't exactly analyze what the listing has to offer, but we get another parameter to take into consideration: <br>\n### Host behavior\n\nBy taking a look at the data, you'll notice that it is hard to point out what kind of value would amenities bring to the listing. Sometimes, the same thing is written in a different way, and a deeper analysis just makes itself too hard to happen. But, since we're dealing with regular people, my hypothesis is that the effort put by the host to tell which amenities the listing has, indicates that the property is better taken care of. Therefore, it can be an indicator that **listings with more amenities are priced slightly higher**.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['n_amenities'] = df['amenities'].str.split(',').apply(len)+1\ndf['n_amenities'] = df['n_amenities'].astype('int')\ndf.loc[df['amenities'] == '{}', 'n_amenities'] = df['n_amenities'].mode()\ndf = df.drop('amenities', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"box_plot('n_amenities')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing entries with outliers on `n_amenities`:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(get_max_fence('n_amenities'))\nrows_before = df.shape[0]\ndf = df[df['n_amenities'] <= get_max_fence('n_amenities')]\nprint(f'{rows_before-df.shape[0]} rows were removed')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nax = sns.countplot('n_amenities', data=df)\nax.tick_params(axis='x')\nax.set_xticklabels(map(int,df['n_amenities'].index))\n# ax.locator_params(integer=True)\n_ = ax.set_title('Distribution of n_amenities')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation heatmap","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()\ncorr = df.corr()\nplt.figure(figsize=(16,12))\n_ = sns.heatmap(df.corr(), annot=True, cmap='Greens')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`host_listings_count` has a negative correlation with `price`, which can either mean that hosts with fewer listings usually charge more, or that the dataset is not divided equally.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#COORDINATES TO PLOT THE MAP\nbox = (df.longitude.min(), df.longitude.max(), df.latitude.min(), df.latitude.max())\nbox1 = (-43.7370,-43.1041,-23.0729,-22.7497)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax= plt.subplots()\nfig.set_size_inches(16,12)\na = plt.imread('https://i.ibb.co/52dDkxT/map-8.png')\nax.imshow(a,zorder=0, extent=(box[0],-43.1041,-23.082,box[3]), aspect='equal')\nax.set_title('Listings location and info about the city')\nax.set_xlim(box[0],box[1])\nax.set_ylim(box[2],box[3])#-23.0729\nax.annotate('offset values',\n            xy=(-43.18, -22.82),\n            xytext=(-43.2,-22.872), backgroundcolor='yellow',bbox=dict(facecolor='#ffa500', alpha=0.5, edgecolor='red', joinstyle='round'),\n            arrowprops=dict(headwidth=8, width=1, color='#ffa500', connectionstyle=\"arc3, rad=0.3\"),\n            fontsize=12)\nax.annotate('Parque Olímpico',\n            xy=(-43.39, -22.98),\n            xytext=(-43.48,-23.055), backgroundcolor='yellow',bbox=dict(facecolor='#ffa500', alpha=0.5, edgecolor='red', joinstyle='round'),\n            arrowprops=dict(headwidth=8, width=1, color='#ffa500', connectionstyle=\"arc3, rad=-0.2\"),\n            fontsize=12)\nax.annotate('City center',\n            xy=(-43.163, -22.91),\n            xytext=(-43.165,-23.02), backgroundcolor='yellow',bbox=dict(facecolor='#ffa500', alpha=0.5, edgecolor='red', joinstyle='round'),\n            arrowprops=dict(headwidth=8, width=1, color='#ffa500', connectionstyle=\"arc3, rad=0.5\"),\n            fontsize=12)\nax.annotate('Touristic spots',\n            xy=(-43.19, -22.995),\n            xytext=(-43.32,-23.055), backgroundcolor='yellow',bbox=dict(facecolor='#ffa500', alpha=0.5, edgecolor='red', joinstyle='round'),\n            arrowprops=dict(headwidth=8, width=1, color='#ffa500', connectionstyle=\"arc3, rad=0.3\"),\n            fontsize=12)\n_ = sns.scatterplot(x='longitude', y='latitude', data=df, ax=ax, zorder=1,color='black',alpha=0.6, s=0.1,edgecolor=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some datapoints have a weird position(on the top right of the map above). But it seems to be a common offset to every listing for the Island above it(called Ilha do governador). For now, we leave it that way.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_density_mapbox = df.sample(n=15000)\nmap_center = {'lat':df_density_mapbox.latitude.mean(), 'lon':df_density_mapbox.longitude.mean()}\nfig = px.density_mapbox(df_density_mapbox, lat='latitude', lon='longitude',z='price',title=10*'  '+'Daily Price density', radius=2.5,\n                        center=map_center, zoom=10,\n                        mapbox_style='stamen-terrain')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The neighborhoods with the highest prices are between the south zone and city center. It is important to notice that there is a pattern on the map:\n\n1. Zones with the highest prices are usually closer to beaches and to touristic spots.\n2. There is a high price spike close to Parque Olímpico, which is where some international festivals happen, such as Rock in Rio.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Encoding variables","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now that our dataset is clean and organized, it's time to deal with categorical variables, such as: `property_type`, `host_is_superhost`, `require_guest_profile_picture`, and `require_guest_phone_verification`. We'll set 'f' equal to 0  and 't' equal to 1 in every categorial variable but `property_type`, since they are all binary. On property type, we will first experiment with label encoding, and change the method if the result is not satisfying.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_label_encoder = df.copy()\nfor column in ('host_is_superhost', 'require_guest_profile_picture', 'require_guest_phone_verification'):\n    df_label_encoder.loc[df_label_encoder[column] == 'f', column] = 0\n    df_label_encoder.loc[df_label_encoder[column] == 't', column] = 1\n    df_label_encoder[column] = df_label_encoder[column].astype(int)\nencoder = LabelEncoder()\n\n\ndf_label_encoder['property_type'] = encoder.fit_transform(df_label_encoder['property_type']) \n\nprint('Columns encoded')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Defining the method for evaluation\n\nWe'll define a function that will return three evaluation methods:\n\n1. Root mean squared error\n2. Mean absolute error\n3. R² score\n\nBoth evaluation methods above will be compared. Mean absolute error will be used as an 'anchor'. Root mean squared error will be used to evaluate errors abnormal to the error average.\n\n\nR² score will be used as a way to evaluate how well our model can explain variance in the test dataset. That is the best measure we can get to evaluate a model, since people can price their properties in a variety of unpredictable ways.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(model_name, y_test, predictions):\n    RMSE = np.sqrt(mean_squared_error(y_test, predictions))\n    MAE = mean_absolute_error(y_test, predictions)\n    r2 = r2_score(y_test, predictions)\n    return f'model: {model_name}\\nMean Absolute Error: {MAE}\\nRoot Mean Square Error: {RMSE}\\nR² Score: {round(r2*100, 2)}% \\n--------------------------------------------'   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Trying out regression models\n![](https://c.mql5.com/31/423/regression-logo-200x200-4497.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now, let's simply run our dataset and fit it into many models, to see which one performs better.<br>\nThis is a good way of initially selecting the best model to **hypertune**.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_label_encoder['price']\ncolumns_dropped =  ['price']\nmodels = {'Random Forest':RandomForestRegressor(),\n          'Lasso':Lasso(),\n          'ElasticNet': ElasticNet(),\n          'XGBRegressor': XGBRegressor(),\n          'Linear Regression': LinearRegression(),\n          'Linear SVR': LinearSVR(),\n          'sgdregressor' : SGDRegressor(),\n          'decision tree': DecisionTreeRegressor(),\n          'Extra Tree Regressor': ExtraTreesRegressor()\n}\n\nX = df_label_encoder.drop(columns_dropped, axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=1)\nname = 'Extra Trees Regressor'\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    print(evaluate(name, y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. ExtraTreesRegressor was the best model so far. It has achieved a R² score of 97%, which means it dealt really well with data far from the mean.\n2. RandomForest also achived a R² score of 97% but extra tree performed better on other measures.<br>\n3. The MAE is also impressive(14 reais), but what is most surprising is the RMSE, which shows that our model didnt miss by much, on average.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Feature importance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(12,8)\nbp_df = pd.DataFrame({'features': model.feature_importances_}, index=X_test.columns)\nbp_df = bp_df.sort_values(by='features', ascending=False)\nax.tick_params(axis='x', rotation=18)\n_ = ax.set_title('Features importance')\n_ = sns.barplot(x=bp_df.index, y='features', data=bp_df, ax = ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Is there more room for performance improvement by removing columns?\n<br><br>\n\nIn this part, we will try to remove a few columns and see the result to the model's performance.\n\nEverytime we remove a new column, the column that was removed before is replaced, so that we can evaluate how each column influence the model individually.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"So, let's remove these columns individually to see how the model performs:<br>\n- `require_guest_phone_verification`\n- `require_guest_profile_picture`\n- `property_type`\n- `host_listings_count`\n- `host_is_superhost`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_lists = ['require_guest_phone_verification', 'require_guest_profile_picture',\n              'property_type', 'bedrooms',\n              'host_listings_count', 'host_is_superhost']\nfor column in cols_lists: \n    removed = X[column]\n    X.drop(column, axis=1, inplace=True)\n    X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=1)\n    selected_model = ExtraTreesRegressor(random_state=1)\n    selected_model.fit(X_train, y_train)\n    predictions = selected_model.predict(X_test)\n    print(f'col removed: {column}\\n{evaluate(name, y_test, predictions)}')\n    X = pd.concat([X, removed], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model performed best by removing `host_listings_count`, so let's remove it from the dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X.drop('host_listings_count', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's remove each one of the other four variables, to see if their removal sums up to the improvement of the model.<br>\nRemoving `host_is_superhost`:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"removed = X['host_is_superhost']\nX.drop('host_is_superhost', axis=1, inplace=True)\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=1)\nselected_model = ExtraTreesRegressor(random_state=1)\nselected_model.fit(X_train, y_train)\npredictions = selected_model.predict(X_test)\nprint(f'cols removed: host_is_superhost\\n{evaluate(name, y_test, predictions)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing `require_guest_phone_verification`:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"column = 'require_guest_phone_verification'\nremoved = X[column]\nX.drop(column, axis=1, inplace=True)\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=1)\nselected_model = ExtraTreesRegressor(random_state=1)\nselected_model.fit(X_train, y_train)\npredictions = selected_model.predict(X_test)\nprint(f'cols removed: {column}\\n{evaluate(name, y_test, predictions)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing `require_guest_profile_picture`:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"column = 'require_guest_profile_picture'\nremoved = X[column]\nX.drop(column, axis=1, inplace=True)\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=1)\nselected_model = ExtraTreesRegressor(random_state=1)\nselected_model.fit(X_train, y_train)\npredictions = selected_model.predict(X_test)\nprint(f'cols removed: {column}\\n{evaluate(name, y_test, predictions)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although removing `require_guest_profile_picture` made our model slightly worse, I don't intend to put it back, because removing it makes the model faster and simpler, if you consider the intention is to make it **available for the general public**, and such information is not so easy to obtain from unexperienced hosts(a new host not knowing how to check if its profile requires a profile picture from the guest, for example).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Our final features are:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"i=1\nfor feature in X.columns:\n    print(f'Feature number {i}: {feature}')\n    i+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(12,8)\nbp_df = pd.DataFrame({'features': selected_model.feature_importances_}, index=X_test.columns)\nbp_df = bp_df.sort_values(by='features', ascending=False)\nax.tick_params(axis='x', rotation=0)\n_ = ax.set_title('Features importance')\n_ = sns.barplot(x=bp_df.index, y='features', data=bp_df, ax = ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**The location is the most important feature for the model**: no surprises there. But what surprised me the most is: n_amenities is the third most important variable. In my point of view, this probably has two meanings:\n\n1. Listings with more amenities are naturally more expensive(maintaining it costs more).\n2. Hosts that take the time to write every single amenity probably put more work into renting it, and therefore, charge more.\n<br>\n\n`property_type` performed worse than expected. I suspect that it is because it has a high cardinality(too many categories), with little to no difference in meaning. So, probably digging deeper into each category and replacing them with more general categories could improve the performance of our model(maybe that can be included in a next version of the model).<br><br>\nA further analysis will be done soon to know which conditions make the data the most unpredictable.<br><br>\nOverall, I think the model performed great as a tool to give a initial pricing estimative for unexperienced hosts.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}