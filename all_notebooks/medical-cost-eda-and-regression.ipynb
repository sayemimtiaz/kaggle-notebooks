{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 align=\"center\"> Medical Cost: EDA and Regression </h1>\n\n<img src=\"https://qtxasset.com/styles/breakpoint_xl_880px_w/s3/2017-04/healthcare_costs.jpg?kc3g1eGqOgDg6.ABpPzctQ1kmDcH_A0L&itok=EyhX40L5\" width=\"30%\" />\n\nCreated: 2020-09-01\n\nLast updated: 2020-09-01\n\nKaggle Kernel made by ðŸš€ <a href=\"https://www.kaggle.com/rafanthx13\"> Rafael Morais de Assis</a>\n\nIn Progress\n\n**Some References**\n+ https://www.kaggle.com/hely333/eda-regression\n+ https://www.kaggle.com/janiobachmann/patient-charges-clustering-and-regression\n+ https://www.kaggle.com/mariapushkareva/medical-insurance-cost-with-linear-regression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Problem Description\n\n[Kaggle Link DataSet](https://www.kaggle.com/mirichoi0218/insurance)\n\n**Context**\n\nDataSet with the cost of treatment of different patients of US. Of course, there are several factors that influence the price of treatment but in this dataset has: age, bmi, sex, number of children / dependents, region of US, has children or not and finally the cost of treatment.\n\n**File Description**\n\n`insurance.csv`: DataSet with 1,338 rows and 7 columns\n\n## DataSet Description\n\n| Column   | Description                                                                                                                                                                                                                          | Values                                                     |\n|----------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------|\n| age      | age of primary beneficiary                                                                                                                                                                                                           | int :: [18, 64]                                            |\n| sex      | insurance contractor gender, female, male                                                                                                                                                                                            | string :: ['female','male']                                |\n| bmi      | Body mass index, providing an understanding of<br>body, weights that are relatively high or low<br>relative to height, objective index of body <br>weight (kg / m ^ 2) using the ratio of height<br> to weight, ideally 18.5 to 24.9 | number :: [15.960, 53.130]                                 |\n| children | Number of children covered by health insurance.<br>Number of dependents                                                                                                                                                              | number :: [0,5]                                            |\n| smoker   | Smoking or Not                                                                                                                                                                                                                       | string :: ['yes','no']                                     |\n| region   | the beneficiary's residential area in the US                                                                                                                                                                                         | string :: [northeast, southeast, <br>southwest, northwest] |\n| charges  | Individual medical costs billed by health insurance                                                                                                                                                                                  | number :: [1,121.878 , 63,770.428]                         |\n\n\n\n## The Goal\n\nBased on the features predict the cost for a patient\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Table Of Content (TOC) <a id=\"top\"></a>\n\n+ [Import Libs and DataSet](#index01) \n+ [Snippets](#index02)\n+ [EDA](#index03)\n  - [Each feature individually](#index03)\n  - [Each Feauture with 'charges'](#index04)\n  - [Analyze feature crossover](#index05)\n  - [Conclusions of EDA](#index06)\n+ [Pre-Processing](#index07)\n+ [Correlation](#index08)\n+ [Split in Train and Test](#index09)\n+ [Develop Models](#index10)\n  - [Cross Validation](#index11)\n  - [Fit Models](#index12)\n  - [Test Models](#index13)\n  - [Bests Models](#index14)\n+ [Feature Importance](#index15)\n+ [Hyperparameter Tuning Best Model](#index16)\n+ [Evaluate Best Model to Regression](#index20)\n+ [Conclusion](#index25)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Import Libs and DataSet <a id='index01'></a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC</a>","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nimport time\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Configs\npd.options.display.float_format = '{:,.3f}'.format\nsns.set(style=\"whitegrid\")\nplt.style.use('seaborn')\nseed = 42\nnp.random.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_path = '/kaggle/input/insurance/insurance.csv'\ndf = pd.read_csv(file_path)\n\nprint(\"Test DataSet = {} rows and {} columns\\n\".format(\n    df.shape[0], df.shape[1]))\n\nquantitative = [f for f in df.columns if df.dtypes[f] != 'object']\nqualitative  = [f for f in df.columns if df.dtypes[f] == 'object']\n\nprint(\"Qualitative Variables: (Numerics)\", \"\\n=>\", qualitative,\n      \"\\n\\nQuantitative Variable: (Strings)\\n=>\", quantitative)\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Snippets <a id='index02'></a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC</a>","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def eda_categ_feat_desc_plot(series_categorical, title = \"\"):\n    \"\"\"Generate 2 plots: barplot with quantity and pieplot with percentage. \n       @series_categorical: categorical series\n       @title: optional\n    \"\"\"\n    series_name = series_categorical.name\n    val_counts = series_categorical.value_counts()\n    val_counts.name = 'quantity'\n    val_percentage = series_categorical.value_counts(normalize=True)\n    val_percentage.name = \"percentage\"\n    val_concat = pd.concat([val_counts, val_percentage], axis = 1)\n    val_concat.reset_index(level=0, inplace=True)\n    val_concat = val_concat.rename( columns = {'index': series_name} )\n    \n    fig, ax = plt.subplots(figsize = (12,4), ncols=2, nrows=1) # figsize = (width, height)\n    if(title != \"\"):\n        fig.suptitle(title, fontsize=18)\n        fig.subplots_adjust(top=0.8)\n\n    s = sns.barplot(x=series_name, y='quantity', data=val_concat, ax=ax[0])\n    for index, row in val_concat.iterrows():\n        s.text(row.name, row['quantity'], row['quantity'], color='black', ha=\"center\")\n\n    s2 = val_concat.plot.pie(y='percentage', autopct=lambda value: '{:.2f}%'.format(value),\n                             labels=val_concat[series_name].tolist(), legend=None, ax=ax[1],\n                             title=\"Percentage Plot\")\n\n    ax[1].set_ylabel('')\n    ax[0].set_title('Quantity Plot')\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def eda_numerical_feat(series, title=\"\", number_format=\"\", with_label=True):\n    f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15, 4), sharex=False)\n    if(title != \"\"):\n        f.suptitle(title, fontsize=18)\n    sns.distplot(series, ax=ax1, rug=True)\n    sns.boxplot(series, ax=ax2)\n    ax1.set_title(\"distplot\")\n    ax2.set_title(\"boxplot\")\n    if(with_label):\n        describe = series.describe()\n        labels = { 'min': describe.loc['min'], 'max': describe.loc['max'], \n              'Q1': describe.loc['25%'], 'Q2': describe.loc['50%'],\n              'Q3': describe.loc['75%']}\n        if(number_format != \"\"):\n            for k, v in labels.items():\n                ax2.text(v, 0.3, k + \"\\n\" + number_format.format(v), ha='center', va='center', fontweight='bold',\n                         size=10, color='white', bbox=dict(facecolor='#445A64'))\n        else:\n            for k, v in labels.items():\n                ax2.text(v, 0.3, k + \"\\n\" + str(v), ha='center', va='center', fontweight='bold',\n                     size=10, color='white', bbox=dict(facecolor='#445A64'))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def plot_model_score_regression(models_name_list, model_score_list, title=''):\n    fig = plt.figure(figsize=(15, 6))\n    ax = sns.pointplot( x = models_name_list, y = model_score_list, \n        markers=['o'], linestyles=['-'])\n    for i, score in enumerate(model_score_list):\n        ax.text(i, score + 0.002, '{:.4f}'.format(score),\n                horizontalalignment='left', size='large', \n                color='black', weight='semibold')\n    plt.ylabel('Score', size=20, labelpad=12)\n    plt.xlabel('Model', size=20, labelpad=12)\n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n    plt.xticks(rotation=70)\n    plt.title(title, size=20)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing data <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC</a>\n\nHas no missing data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Each feature individually <a id='index03'></a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eda_numerical_feat(df['charges'], \"'charges' Distribution\", \"{:,.0f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eda_numerical_feat(df['bmi'], \"'bmi' Distribution\", \"{:.2f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eda_numerical_feat(df['age'], \"'Age' Distribution\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eda_categ_feat_desc_plot(df['children'], '\"children\" Distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eda_categ_feat_desc_plot(df['sex'], '\"Sex\" Distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eda_categ_feat_desc_plot(df['smoker'], '\"smoker\" Distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eda_categ_feat_desc_plot(df['region'], '\"region\" Distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Each feauture with 'charges' <a id='index04'></a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ((ax1, ax2), (ax3,ax4), (ax5, ax6)) = plt.subplots(figsize = (16,14), ncols=2, nrows=3, sharex=False, sharey=False)\n\n# sex\nsns.boxplot(x=\"sex\", y=\"charges\", data=df, ax=ax1)\nsns.distplot(df[ df['sex'] == 'male']['charges'], ax=ax2, hist=False, label=\"male\")\nsns.distplot(df[ df['sex'] == 'female']['charges'], ax=ax2, hist=False, label=\"female\")\n# region\nsns.boxplot(x=\"region\", y=\"charges\", data=df, ax=ax3)\nsns.kdeplot(df[ df['region'] == 'southwest']['charges'], ax=ax4, label=\"southwest\")\nsns.kdeplot(df[ df['region'] == 'southeast']['charges'], ax=ax4, label=\"southeast\")\nsns.kdeplot(df[ df['region'] == 'northwest']['charges'], ax=ax4, label=\"northwest\")\nsns.kdeplot(df[ df['region'] == 'northeast']['charges'], ax=ax4, label=\"northeast\")\n# children\nsns.boxplot(x=\"children\", y=\"charges\", data=df, ax=ax5)\nsns.distplot(df[ df['children'] == 0]['charges'], ax=ax6, hist=False, label=\"0\")\nsns.distplot(df[ df['children'] == 1]['charges'], ax=ax6, hist=False, label=\"1\")\nsns.distplot(df[ df['children'] == 2]['charges'], ax=ax6, hist=False, label=\"2\")\nsns.distplot(df[ df['children'] == 3]['charges'], ax=ax6, hist=False, label=\"3\")\nsns.distplot(df[ df['children'] == 4]['charges'], ax=ax6, hist=False, label=\"4\")\nsns.distplot(df[ df['children'] == 5]['charges'], ax=ax6, hist=False, label=\"5\")\n\n# Config Titles\nfig.suptitle('Categorical Features with \"charge\"', fontsize=20)\nfont_size = 16\nax1.set_title('charges by sex')\nax2.set_title('charges by sex')\nax3.set_title('charges by children')\nax4.set_title('charges by children')\nax5.set_title('charges by region')\nax6.set_title('charges by region')\n\nplt.legend();\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(figsize = (16,5), ncols=2, sharex=False, sharey=False)\n\nfont_size = 14\nfig.suptitle('charge x smoke', fontsize=18)\n\nsns.boxplot(x=\"smoker\", y=\"charges\", data=df, ax=ax1)\nsns.distplot(df[(df.smoker == 'yes')][\"charges\"],color='c',ax=ax2, label='smoke')\nsns.distplot(df[(df.smoker == 'no')]['charges'],color='b',ax=ax2, label='not smoke')\n\nax1.set_title('charges by smoke or not', fontsize=font_size)\nax2.set_title('Distribution of charges for smokers or  not', fontsize=font_size)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(figsize = (16,5), ncols=2, sharex=False, sharey=False)\n\nfont_size = 14\nfig.suptitle('charge x sex', fontsize=18)\n\nsns.boxplot(x=\"sex\", y=\"charges\", data=df, ax=ax1)\nsns.distplot(df[(df.sex == 'male')][\"charges\"],color='c',ax=ax2, hist=False, label='male')\nsns.distplot(df[(df.sex == 'female')]['charges'],color='b',ax=ax2, hist=False, label='female')\n\nax1.set_title('charges by sex', fontsize=font_size)\nax2.set_title('Distribution of charges for male/female', fontsize=font_size)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(figsize = (16,11), nrows=2)\n\nsns.scatterplot(x=\"age\", y=\"charges\", data=df, ax=ax1)\nsns.boxplot(x=\"age\", y=\"charges\", data=df, palette=\"Set3\", ax=ax2)\n\n# config scatterplot x_axis\nax1.set_xticks(range(18,65)) # show age axis\nax1.set_xlim(17.5,64.5) # remove right/left margin\n\n# Config Titles\nfig.suptitle('charge by age', fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### charges by bmi","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://www.researchgate.net/profile/Selcuk_Nas/publication/320067348/figure/tbl2/AS:614180059090945@1523443345088/Classification-of-body-mass-according-to-body-mass-index-BMI.png\" width=\"40%\"/>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Engineering: Create 'weight_condition' to see better see bmi importance\n\ndf[\"weight_condition\"] = np.nan\nlst = [df]\n\nfor col in lst:\n    col.loc[col[\"bmi\"] < 18.5, \"weight_condition\"] = \"Underweight\"\n    col.loc[(col[\"bmi\"] >= 18.5) & (col[\"bmi\"] < 24.986), \"weight_condition\"] = \"Normal Weight\"\n    col.loc[(col[\"bmi\"] >= 25) & (col[\"bmi\"] < 29.926), \"weight_condition\"] = \"Overweight\"\n    col.loc[col[\"bmi\"] >= 30, \"weight_condition\"] = \"Obese\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.scatterplot(x=\"bmi\", y=\"charges\", hue=\"weight_condition\", data=df)\nax.set_title(\"charges by bmi'\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analyze feature crossover <a id='index05'></a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(figsize = (17,11), ncols=2, nrows=3, sharex=False, sharey=False)\n\nsns.scatterplot(x=\"age\", y=\"charges\", hue=\"smoker\", data=df, ax=ax1)\nsns.scatterplot(x=\"age\", y=\"charges\", hue=\"sex\", data=df, ax=ax2)\nsns.scatterplot(x=\"age\", y=\"charges\", hue=\"weight_condition\", data=df, ax=ax3)\nsns.scatterplot(x=\"age\", y=\"charges\", hue=\"bmi\", data=df, size=\"bmi\", ax=ax4)\n\nsns.scatterplot(x=\"age\", y=\"charges\", hue=\"children\", data=df, ax=ax5)\nsns.scatterplot(x=\"age\", y=\"charges\", hue=\"region\", data=df, ax=ax6)\n\n# Config Titles\nfig.suptitle('charge x age with others features', fontsize=18)\nax1.set_title(\"charges by age and smoke\")\nax2.set_title(\"charges by age and sex\")\nax3.set_title(\"charges by age and weight_condition\")\nax4.set_title(\"charges by age and bmi\")\nax5.set_title(\"charges by age and children\")\nax6.set_title(\"charges by age and region\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Engineering: Create 'weight_condition' to see better 'age' importance\n\ndf['age_cat'] = np.nan\nlst = [df]\n\nfor col in lst:\n    col.loc[(col['age'] >= 18) & (col['age'] <= 30), 'age_cat'] = 'Young Adult'\n    col.loc[(col['age'] >  30) & (col['age'] <= 50), 'age_cat'] = 'Adult'\n    col.loc[(col['age'] >  50) & (col['age'] <= 60), 'age_cat'] = 'Senior'\n    col.loc[ col['age'] >  60, 'age_cat'] = 'Elder'\n    \ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(figsize = (17,11), ncols=2, nrows=3, sharex=False, sharey=False)\n\nsns.scatterplot(x=\"bmi\", y=\"charges\", hue=\"smoker\", data=df, ax=ax1)\nsns.scatterplot(x=\"bmi\", y=\"charges\", hue=\"sex\", data=df, ax=ax2)\n\nsns.scatterplot(x=\"bmi\", y=\"charges\", hue=\"weight_condition\", data=df, ax=ax3)\nsns.scatterplot(x=\"bmi\", y=\"charges\", hue=\"age_cat\", data=df, ax=ax4)\n\nsns.scatterplot(x=\"bmi\", y=\"charges\", hue=\"children\", data=df, ax=ax5)\nsns.scatterplot(x=\"bmi\", y=\"charges\", hue=\"region\", data=df, ax=ax6)\n\n# Config Titles\nfig.suptitle('charge by bmi with others features', fontsize=18)\nax1.set_title(\"charges by bmi and smoke\")\nax2.set_title(\"charges by bmi and sex\")\nax3.set_title(\"charges by bmi and weight_condition\")\nax4.set_title(\"charges by bmi and age_cat\")\nax5.set_title(\"charges by bmi and children\")\nax6.set_title(\"charges by bmi and region\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusions Of EDA <a id='index06'></a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC</a>\n\n==> chart: 'charge' x 'smoke'\n\nAs in 'charge x smoke' the fact that smoking is very important, the distribution of cartoons is clearly different between a smoker and a non-smoker. Most smokers have much higher charges than non-smokers\n\n==> chart: charge by age\n\nThe higher the age the higher the price\n\n==> chart: cartoons by bmi\n\nThe larger the bmi, the greater the tendency to have large values, although only bmi does not explain large costs\n\n==> chart: charge x age with others features\n\nIn 'cartoons by age and smoke' you can clearly perceive 3 classes.\n+ class 1, lower costs, are non-smokers\n+ class 2, average expenses, smokers and non-smokers\n+ class 3, higher expenses, smokers\nAfter looking at 'cartoons by age and weight_condition' we have that in the vast majority this third class is of obese people (high bmi)\n\n==> chart: 'cartoons by bmi with others features'\n\nIn 'cartoons by bmi and age_cat' he shows us the missing piece, together with 'cartoons by bmi and smoke'.\n\nOnly by looking at the data of the mental activity, in which a decision tree is smoke, age and bmi.\n\n1. If you smoke you will have more expenses than non-smokers (a good part of the population), expenses over 15,000\n  - The BMI is analyzed, if not obese, it is in a group between 18,000 and 30,000, if obese over 35,000\n  - For each of these two groups, the older you are, the more expensive it becomes\n\n2. If you don't smoke expenses below 15,000\n  - For non-fulmenates the second criterion would be age, the older the higher the expenditure\n  - In this, bmi does not influence much. Despite this, some with normal weight or above (NormalWeight, Overweight or Obese) can fall in the cost of being as expensive as a smoker, mainly obese\n\n==>  Other features\n\ngender, children and region have very little influence, this will also be seen in the correlations\n\n<!-- \n\nchart: 'charge' x 'smoke'\n\nComo em 'charge x smoke' o fato de fumar Ã© bem importnate, a distribuiÃ§Ã¢o de charges Ã© claramente diferente entre um fumante e um nÃ£o fumante. A maior parte dos fumantes tem encargos muito maiores que os nÃ¢o fumantes\n\nchart: charge by age\n\nQuanto maior a idade maior o preÃ§o\n\nchart: charges by bmi\n\nQuanto maior o bmi maior Ã© a tendencia de se ter grandes valores, apesar disso sÃ³ o bmi nÃ¢o explicar grandes custos\n\nchart: charge x age with others features\n\nEm 'charges by age and smoke' podemo perceber nitidamente 3 classes. \n+ classe 1, menor gastos, sÃ£o os nÃ£o fumantes\n+ classe 2, gastos medianos, fumantes e nÃ£o fumantes\n+ classe 3, maiores gastos, fumantes\nDepois olhando para 'charges by age and weight_condition' temos que em grande maioria essa terceira classe Ã© das pessoas obesas (alto bmi)\n\nchart: 'charges by bmi with others features'\n\nEm 'charges by bmi and age_cat' nos mostra a peÃ§a que falta, junto com 'charges by bmi and smoke'.\n\nSomente olhando os dados dapra fazer mental,emtne uma Ã¡rvore de decisao sÃ£o fumo, idade e bmi.\n\n+ Se fuma tera mais gastos que os nÃ£o fulmantes (boa parte da populaÃ§Ã£o), gastos acima de 15,000\n  - Analisa-se o BMI, se nÃ£o for obseo, fica num grupo entre 18,000 e 30,000, se obeso acima de 35,000\n  - Para cada um desses dois grupos, quanto maior a idade, mais caro fica\n\n+ Se nÃ£o fuma gastos abaixo de 15,000\n  - Para os nao fulmenates o segundo critÃ©rio seria a idade, quanto mais velho maior o gasto\n  - Nisso o bmi nÃ£o influencia muito. Apesar disso alguns com peso normal ou acima (NormalWeight, Overweight or Obese) podem cair no custo de serem tÃ£o caro quanto fumante, principlamente obesos\n\nsexo, children e region influenciam bem pouco, isso tambÃ©m serÃ¡ visto na parte de correlaÃ§Ãµes\n\n\n-->","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Pre-Processing <a id='index07'></a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Before\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\ndf = df.drop(['weight_condition','age_cat'], axis=1)\n\n# sex\nle = LabelEncoder()\nle.fit(df.sex.drop_duplicates()) \ndf.sex = le.transform(df.sex)\n\n# smoker or not\nle.fit(df.smoker.drop_duplicates()) \ndf.smoker = le.transform(df.smoker)\n\n# region\nle.fit(df.region.drop_duplicates()) \ndf.region = le.transform(df.region)\n\ndf.head() # after pre-processing","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation <a id='index08'></a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC</a>\n\n\n<span style='font-size: 15pt'>Numerical correlations with heatmap</span>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = df.corr()\nf, ax1 = plt.subplots(figsize=(18, 6), sharex=False)\n\nax1.set_title('Top Corr to {}'.format('\"charges\"'))\ncols_top = corr_matrix.sort_values(by=\"charges\", ascending=False)['charges'].index\n\ncm = np.corrcoef(df[cols_top].values.T)\nmask = np.zeros_like(cm)\nmask[np.triu_indices_from(mask)] = True\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f',\n                 annot_kws={'size': 14}, yticklabels=cols_top.values,\n                 xticklabels=cols_top.values, mask=mask, ax=ax1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What makes medical costs more expensive are: smoking, age and bmi","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<span style='font-size: 15pt'>ANOVA: to categorical features</span>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/hamelg/python-for-data-26-anova\n\nimport scipy.stats as stats\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\ndef anova_analysis(y_target, x_cat_feats, datf):\n    for x_feat in x_cat_feats:\n        model = ols('{} ~ {}'.format(y_target, x_feat),\n                    data = datf).fit()\n        anova_result = sm.stats.anova_lm(model, typ=2)\n        print(anova_result,'\\n')    \n        \n\n# If PR(>F) is less than 0.05 (alpha = cofiant level) means that the categorical feauture influence 'charges'\nanova_analysis('charges', ['smoker', 'region'], df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"So smoking is really an important factor","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Split in Train and Test <a id='index09'></a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# # Normal Split\n# x = df.drop(['charges'], axis = 1)\n# y = df['charges']\n# x_train,x_test,y_train,y_test = train_test_split(x,y, random_state = 42)\n\n####### OBS: IS better use Polinomal Transform than only split ############\n\n# # Polinomial REgression: Feature Transform : \n#   create x^0, x^1, x^2 .... to linear models to be polinomial\n\nX = df.drop(['charges','region'], axis = 1)\nY = df.charges\n\nquad = PolynomialFeatures(degree = 2)\nx_quad = quad.fit_transform(X)\n\nx_train,x_test,y_train,y_test = train_test_split(x_quad, Y, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Develop Models <a id='index10'></a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC</a>\n\n<span style='font-size: 15pt'>Prepare ML Models and training</span>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, KFold, cross_val_score, train_test_split\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, RobustScaler, scale\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import ElasticNet, LassoCV, BayesianRidge, LassoLarsIC\nfrom sklearn.linear_model import Ridge, RidgeCV, ElasticNet, ElasticNetCV, LinearRegression\nfrom sklearn.kernel_ridge import KernelRidge\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostRegressor, BaggingRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.svm import SVR\n\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setup cross validation folds\n\nkf = KFold(n_splits=4, random_state=42, shuffle=True)\n\n# Define error metrics\ndef rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X=x_train):\n    rmse = np.sqrt(-cross_val_score(model, X, y_train, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create ML Models\n\n# Light Gradient Boosting Regressor\nlightgb_model = LGBMRegressor(objective='regression',  num_leaves=6, learning_rate=0.01,  n_estimators=7000,\n                       max_bin=200,  bagging_fraction=0.8, bagging_freq=4,  bagging_seed=8,\n                       feature_fraction=0.2, feature_fraction_seed=8, min_sum_hessian_in_leaf = 11,\n                       verbose=-1, random_state=42)\n\n# XGBoost Regressor\nxgboost_model = XGBRegressor(learning_rate=0.01, n_estimators=6000, max_depth=4, min_child_weight=0,\n                       gamma=0.6, subsample=0.7, colsample_bytree=0.7, objective='reg:squarederror',\n                       nthread=-1, scale_pos_weight=1, seed=42, reg_alpha=0.00006, random_state=42)\n\n# Linear Regressor\nlinear_model = LinearRegression()\n\n# Ridge Regressor\nridge_alphas = [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, \n                1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\nridge_model = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=kf))\n\n# Lasso Regressor\nlasso_alphas2 = [5e-05, 0.0001, 0.0008, 0.01, 0.1, 1]\nlasso_model = make_pipeline(RobustScaler(),\n                      LassoCV(max_iter=1e7, alphas=lasso_alphas2,\n                              random_state=42, cv=kf))\n\n# Elastic Net Regressor\nelastic_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\nelastic_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\nelasticnet_model = make_pipeline(RobustScaler(),  \n                           ElasticNetCV(max_iter=1e7, alphas=elastic_alphas,\n                                        cv=kf, l1_ratio=elastic_l1ratio))\n\n# Kernel Ridge\nkeridge_model = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n\n# Support Vector Regressor\nsvm_model = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))\n\n# Gradient Boosting Regressor\ngboost_model = GradientBoostingRegressor(n_estimators=6000, learning_rate=0.01, max_depth=4, max_features='sqrt', \n                                min_samples_leaf=15, min_samples_split=10, loss='huber', random_state=42)  \n\n# Random Forest Regressor\nrandomforest_model = RandomForestRegressor(n_estimators=1200, max_depth=15, min_samples_split=5, min_samples_leaf=5,\n                          max_features=None, oob_score=True, random_state=42)\n\n# Neural Net\nneuralnet_model = MLPRegressor()\n\n# Extra Tree Regressor\nextratree_model = ExtraTreesRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SVM and NeuralNet was completally terrible\n\nregressor_models = {\n    'Linear': linear_model,\n    'Ridge': ridge_model,\n    'Lasso': lasso_model,\n    'KernelRidge': keridge_model,\n    'ElasticNet': elasticnet_model,\n#     'SVM': svm_model,\n    'RandomForest': randomforest_model,\n    'ExtraTree': extratree_model,\n#     'NeuralNet': neuralnet_model,\n    'GBoost': gboost_model,\n    'LightGB': lightgb_model,\n    'XGBoost': xgboost_model,\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cross Validation <a id='index11'></a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Cross Validation\n\ncv_scores = {}\nt_start = time.time()\n\nfor model_name, model in regressor_models.items():\n    print('{:17}'.format(model_name), end='')\n    t0 = time.time()\n    score = cv_rmse(model)\n    m, s = score.mean(), score.std()\n    cv_scores[model_name] = [m,s]\n    print('| MSE in CV | mean: {:11,.3f}, | std: {:9,.3f}  | took: {:9,.3f} s |'.format(m,s, time.time() - t0))\n    \nprint('\\nTime total to CrossValidation: took {:9,.3f} s'.format(time.time() - t_start)) # 200s\n\n# Show Sorted DataFrame\ndf_cv = pd.DataFrame(data = cv_scores.values(), columns=['rmse_cv', 'std_cv'], index=cv_scores.keys())\ndf_cv = df_cv.sort_values(by='rmse_cv').reset_index().rename({'index': 'model'}, axis=1)\ndf_cv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fit Models <a id='index12'></a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC</a>\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Def Stack Model: Stack up some the models above, optimized using one ml model\nstack_regressors = (regressor_models['Lasso'],\n                    regressor_models['LightGB'],\n                    regressor_models['Ridge'],\n                    regressor_models['RandomForest'])\n\nstack_model = StackingCVRegressor(regressors = stack_regressors,\n                                meta_regressor = regressor_models['ElasticNet'],\n                                use_features_in_secondary=True)\n\nregressor_models['Stack'] = stack_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_scores = {}\nt_start = time.time()\n\nfor model_name, model in regressor_models.items():\n    print('{:17}'.format(model_name), end='')\n    t0 = time.time()\n    if(model_name == 'Stack'):\n        model  = model.fit(np.array(x_train), np.array(y_train))\n        y_pred = model.predict(np.array(x_train))\n    else:\n        model  = model.fit( x_train, y_train )\n        y_pred = model.predict(x_train)\n    r2, mse = r2_score(y_train, y_pred), mean_squared_error(y_train, y_pred)\n    train_scores[model_name] = [r2, mse, np.sqrt(mse)]\n    text_print = '| Train | r2: {:6,.3f}, | mse: {:15,.3f}  | took: {:9,.3f} s |'\n    print(text_print.format(r2, mse, time.time() - t0))\n    regressor_models[model_name] = model\n    \nprint('\\nTime total to Fit Models: took {:9,.3f} s'.format(time.time() - t_start)) # 200s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor_models.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Blend Model is use a porcentage of some models mixing\nclass BlendModel:\n    \n    @classmethod\n    def predict(self, X):\n        return ((0.10 * regressor_models['Lasso'].predict(X)) + \\\n            (0.10 * regressor_models['GBoost'].predict(X)) + \\\n            (0.15 * regressor_models['XGBoost'].predict(X)) + \\\n            (0.10 * regressor_models['LightGB'].predict(X)) + \\\n            (0.20 * regressor_models['RandomForest'].predict(X)) + \\\n            (0.35 * regressor_models['Stack'].predict(np.array(X))))\n\nregressor_models['BlendModel'] = BlendModel()\ny_pred = BlendModel.predict(x_train)\nr2, mse = r2_score(y_train, y_pred), mean_squared_error(y_train, y_pred)\ntrain_scores['BlendModel'] = [r2, mse, np.sqrt(mse)]\nprint('RMSE score on train data to Blend Model:\\n\\t=>', np.sqrt(mse))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from mlens.ensemble import SuperLearner\n\n# create a list of base-models\ndef get_models_to_super_leaner():\n    models = list()\n    models.append(regressor_models['Linear'])\n    models.append(regressor_models['Ridge'])\n    models.append(regressor_models['Lasso'])\n    models.append(regressor_models['KernelRidge'])\n    models.append(regressor_models['ElasticNet'])\n    models.append(regressor_models['RandomForest'])\n    models.append(regressor_models['GBoost'])\n    return models\n\n# create the super learner\ndef get_super_learner(X):\n    ensemble = SuperLearner(scorer=rmse, folds=5, shuffle=True, sample_size=len(X))\n    # add base models\n    models = get_models_to_super_leaner()\n    ensemble.add(models)\n    # add the meta model\n    ensemble.add_meta(LinearRegression())\n    return ensemble\n\n# key to regressros models\nmodel_name = 'SuperLeaner'\n\n# create the super learner\nensemble = get_super_learner(x_train)\n# fit the super learner\nt0 = time.time()\nensemble.fit(x_train, np.array(y_train)) # took 350s = 6min\n# pred and evaluate in train dataset\ny_pred = ensemble.predict(x_train)\nr2, mse = r2_score(y_train, y_pred), mean_squared_error(y_train, y_pred)\ntrain_scores[model_name] = [r2, mse, np.sqrt(mse)]\n# show results\ntext_print = '| Super Leaner in Train | r2: {:6,.3f}, | mse: {:9,.3f}  | took: {:9,.3f} s |\\n'\nprint(text_print.format(r2, mse, time.time() - t0))\n# set in dict regressors\nregressor_models[model_name] = ensemble\n# summarize base learners\nprint(ensemble.data)\n# evaluate meta model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show train_scores dataframe\ndf_train_scores = pd.DataFrame(data = train_scores.values(),index=train_scores.keys(), columns=['r2_train', 'mse_train', 'rmse_train'])\ndf_train_scores = df_train_scores.sort_values(by='r2_train', ascending=False).reset_index().rename({'index': 'model'}, axis=1)\ndf_train_scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test Models <a id='index13'></a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_scores = {}\n\n# predcit x_test to y_test and compare\nfor model_name, model in regressor_models.items():\n    if(model_name == 'Stack'):\n        y_pred = model.predict(np.array(x_test))\n    else:\n        y_pred = model.predict(x_test)\n    r2, mse = r2_score(y_test, y_pred), mean_squared_error(y_test, y_pred)\n    test_scores[model_name] = [r2, mse, np.sqrt(mse)]\n    \n# Sort DF test scores\ndf_test_scores = pd.DataFrame(data = test_scores.values(), columns=['r2_test', 'mse_test', 'rmse_test'], index=test_scores.keys())\ndf_test_scores = df_test_scores.sort_values(by='r2_test', ascending=False).reset_index().rename({'index': 'model'}, axis=1)\ndf_test_scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Best Models <a id='index14'></a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Include Blend in Train Scores\ndf_train_scores = pd.DataFrame(data = train_scores.values(),index=train_scores.keys(), columns=['r2_train', 'mse_train', 'rmse_train'])\ndf_train_scores = df_train_scores.sort_values(by='r2_train', ascending=False).reset_index().rename({'index': 'model'}, axis=1)\n\n# df_test_scores\ndf_cv2 = df_cv.merge(df_train_scores, how='right'  ,left_on='model', right_on='model')\ndf_final_scores = df_cv2.merge(df_test_scores, how='right' ,left_on='model', right_on='model')\n\nprint(list(df_final_scores.columns))\ndf_final_scores.sort_values(by='mse_test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model_score_regression(list(test_scores.keys()), [r2 for r2, mse, rmse in test_scores.values()], 'Evaluate Models in Test: R2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best model for the test data was the SuperLeaner, after it GBoost. Both had excellent scores on the training data, even though they were not so good on the training data, as they probably did not have overfitting.\n\nUnfortunately, there is no (or do not know how) to do cross validation for SuperLeaner. Since your score on the training and test data is similar to that of GBoost we can assume your score for cross validation, it would be similar to that of GBoost.\n\nIt happened that in the training data some models were overfitted, such as ExtraTree, XGBoost and LightGB. You can see this by observing that they were almost perfect in the training data but failed like any other model in the test data.\n\nEven though some models cannot do corss validation (Blend, Stack, SuperLeaner) when ordering by 'mse_test' we realize that the models that can make cv are in the same position if you compare 'mse_test' with 'rmse_cv'. So Blend and Stack were also good models, too.\n\n<!-- \nO melhor modelo para os dados de teste foi o SuperLeaner, depois dele GBoost. Ambos tiveram Ã³timos scores nos dados de treino, mesmo nÃ¢o sendo tÃ£o boons nos dados de treino, pois provavelmente nÃ£o tiveram overfitting.\n\nInfelismente nÃ£o tem (ou nÃ£o sei fazer) como fazer cross validation para o SuperLeaner. Como a sua pontuaÃ§Ã¢o nos dados de treino e teste sÃ£o parecidas com a do GBoost podemos supor sua pontuaÃ§Ã¢o de cross validation, seria parecida com a do GBoost.\n\nOcorreu que nos dados de treinamento alguns modelo tiveram overfitting, como ExtraTree, XGBoost e LightGB. Ã‰ possÃ­vel notar isso observando que foram quase perfeitos nos dados de treino mas falharam como qualquer outro modelo nos dados de teste.\n\nMesmo que alguns modelos nÃ£o possam fazer corss validation (Blend, Stack, SuperLeaner) ao ordenar por 'mse_test' percebemos que os modelos que podem fazer cv ficam  na mesma colocaÃ§Ã¢o se comparar 'mse_test' com 'rmse_cv'. Dessa forma Blend e Stack tambÃ©m foram bons modelos tambÃ©m.\n-->","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance <a id='index15'></a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# To one of best models: GBoost\n\nplt.figure(figsize = (12,4))\nfeat_importances = pd.Series(regressor_models['GBoost'].feature_importances_)#, index=X.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from yellowbrick.model_selection import FeatureImportances, RFECV\n\n# FeatureImportances and RFECV to a good model, if put GBoost(the best) take a long time\n\nfig, (ax3,ax4) = plt.subplots(figsize = (15,5), ncols=2, sharex=False, sharey=False)\n\nthe_model = 'Linear'\nt_start = time.time()\n\nviz3 = FeatureImportances(regressor_models[the_model], ax=ax3, relative=False)\nviz3.fit(x_train, y_train)\nviz3.finalize()\n\nviz4 = RFECV(regressor_models[the_model], ax=ax4)\nviz4.fit(x_train, y_train)\nviz4.finalize()\n\nprint('Time total to RFECV to {} : took {:9,.3f} s'.format(the_model, time.time() - t_start))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate Best Model to Regression <a id='index20'></a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from yellowbrick.regressor import ResidualsPlot, PredictionError\nfrom yellowbrick.model_selection import FeatureImportances, RFECV\n\n# Can't use 'SuperLeaner' than, use the second place: GBoost\n\nfig, (ax1, ax2) = plt.subplots(figsize = (15,5), ncols=2)\n\nviz1 = ResidualsPlot(regressor_models['GBoost'], ax=ax1)\nviz1.score(x_test, y_test)\nviz1.finalize()\n\nviz2 = PredictionError(regressor_models['GBoost'], ax=ax2)\nviz2.score(x_test, y_test)  \nviz2.finalize()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion <a id='index25'></a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC</a>\n\nThe better model was:\nSuperLeaner with MSE\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_log_error\n\ny_pred = regressor_models['SuperLeaner'].predict(x_test)\nprint('The best Regressor Model to Test DataSet:')\nprint('MAE : {:14,.3f}'.format(mean_absolute_error(y_pred, y_test)))\nprint('MSE : {:14,.3f}'.format(mean_squared_error(y_pred, y_test)))\nprint('RMSE: {:14,.3f}'.format(np.sqrt(mean_squared_error(y_pred, y_test))))\nprint('MSLE: {:14,.3f}'.format(mean_squared_log_error(y_pred, y_test)))\nprint('R2  : {:14,.3f}'.format(r2_score(y_pred, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The better model was SuperLeaner with the results above.\n\n---\n\nThis Kernel is still under development. I would highly appreciate your feedback for improvement and, of course, if you like it, please upvote it!\n\n\nPlease Upvote (It motivates me)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}