{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Injection induced seismic events at The Geysers geothermal field: 2003-2016\n\nThe Geysers is the world's largest geothermal field, containing a complex of 18 geothermal power plants, drawing steam from more than 350 wells, located in the Mayacamas Mountains approximately 72 miles north of San Francisco, California. Geysers produced about 20% of California's renewable energy in 2019.\n\nThe first commercial geothermal power plant in The Geysers in California was put into operation in September 1960  to tap natural steam. But, in the late 1980s, it was found that the flow of steam across the geothermal field had reduced and the reservoir was not recharging quickly enough to meet the required steam supply. As a result, inefficient power plants were shut down.\n\nThe geothermal reservoir is now recharged by injecting recycled wastewater from the city of Santa Rosa and the Lake County sewage treatment plants. 18 million gallons of treated wastewater is supplied each day. \n\nThe injection of cold water into this hot geothermal reservoir induced seismic events. A dense seismic network was installed to monitor the induced seismicity from 2003 to 2016 (data available here: http://ncedc.org/egs/catalog-search.html).\n\nHere, I have collected the injection data from 73 injection wells present in the Northwest of the geothermal field (data available here:https://www.conservation.ca.gov/calgem/Pages/WellFinder.aspx) to try to investigate the relation between induced seismic events and water injection. \n","metadata":{}},{"cell_type":"markdown","source":"It is an ongoing project...","metadata":{}},{"cell_type":"markdown","source":"## ABSTRACT:\n**Objectives:**\n\nPredict the **monthly amount of seismic energy released** and the **monthly b-value**.\n\n**Data:**  \nA seismic catalogue and the injection data for 73 injection wells, that include:\n- monthly amount of injected water,\n- monthly average injection rate,\n- number of day in a month where water ws injected.\n\n**Model evaluation**\n\nSeveral models are used (Lasso, linear, Ridge, Elasticnet, Random forest and xgboost). These model are scored using the explained variance (r2), and the model with the best score is selected for the final prediction.\n\n\n**Results for prediction monthly amount of seismic energy released**\n","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\nprint(\"Model's best score to predict monthly energy released\")\nImage(\"../input/results/Models scores Energy.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image\nprint (\"Original data vs predicted\")\nImage(\"../input/results/Energy -  original vs predicted.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Results for prediction monthly b-value**","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\nprint(\"Model's best score to predict monthly energy released\")\nImage(\"../input/results/Models scores bvalues.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Image(\"../input/results/b values -  original vs predicted.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[1: SEISMIC DATA](#SEISMIC_DATA)\n\n- [1.1: Load the data](#1.1)\n- [1.2: Simple Statistical seismology](#1.2)\n    - [1.2.1: Plot the ECDF of the Earthquake magnitudes](#1.2)\n    - [1.2.2: Computing the b-value](#1.2.2) \n- [1.3: Seicmic activity evolution from 2003 to 2016](#1.3)\n    - [1.3.1: b-value evolution](#1.3)\n    - [1.3.2: monthly seismic energy released](#1.3.2)\n    - [1.3.3: Density maps of induced-earthquakes](#1.3.3)\n    \n[2: INJECTION DATA](#INJECTION_DATA)\n- [2.1: Load and prepare the data](#2.1)\n\n[3: INJECTION DATA VS INDUCED SEISMICITY](#INJ_VS_SEISM)\n- [3.1: injection vs seismic energy](#3.1)\n- [3.2: Injection vs b_value](#3.2)\n- [3.3: lag periods](#3.3)\n\n[4: DATA PREPARATION](#Data_Preparation)\n- [4.1: Dependent variables:](#4.1)\n    - [4.1.1: Check the asymmetry of the probability distribution](#4.1.1)\n    - [4.1.2: Log transform skewed targets:](#4.1.2)\n   \n- [4.2: Independent variables](#4.2) \n    - [4.2.1: define functions used for data preparation](#4.2)\n    - [4.2.2: Features extraction](#4.2.2)\n    - [4.2.3: Injection data: scaling, lag version, and feature reduction](#4.2.3)\n    \n    \n- [4.3: correlations between features and target variables](#4.3)\n    - [4.3.1: function used to plot correlations](#4.3.1)\n    - [4.3.2: prepare dataframe to calculate correlation coefficient](#4.3.2)\n    - [4.3.3:  correlation between seismic energy and:](#4.3.3)\n    - [4.3.4:  correlation between b value and:](#4.3.4)\n    \n - [4.5: Outliers detection:](#4.5)\n    \n[5: MACHINE LEARNING: prediction monthy seismic energy](#ML)\n- [5.1: Features selection and data split for linear models](#5.1)\n- [5.2: functions used for machine learning](#5.2)\n- [5.3: Linear model](#5.3)\n    - [5.3.1: Feature selection with Lasso](#5.3)\n    - [5.3.2: Linear regression](#5.3.2)\n    - [5.3.3: Ridge model](#5.3.3)\n    - [5.3.4: ElasticNet regression](#5.3.4)\n    \n    \n- [5.4: Decision tree methods](#5.4)\n    - [5.4.1: Random Forest Regressor](#5.4.1)\n    - [5.4.2: xgboost regression](#5.4.2)\n    \n- [5.5: SUMMARY PREDICTION ENERGY](#5.5)\n\n[6: MACHINE LEARNING: prediction monthy b-value](#ML2)\n- [6.1: Features selection and data split for linear models](#6.1)\n- [6.2: Linear model](#6.2)\n    - [6.2.1: Feature selection with Lasso](#6.2)\n    - [6.2.2: Linear regression](#6.2.2)\n    - [6.2.3: Ridge model](#6.2.3)\n    - [6.2.4: ElasticNet regression](#6.2.4)\n    \n- [6.3: Decision tree methods](#6.3)\n    - [6.3.1: Random Forest Regressor](#6.3)\n    - [6.3.2: xgboost](#6.3.2)\n    \n- [6.4: Summary -- predition monthly b-value --](#6.4)","metadata":{}},{"cell_type":"markdown","source":"**Import librairies**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# map creation\nimport cartopy.crs as ccrs\nimport cartopy\nimport cartopy.feature as cfeature\nfrom cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n\n# data visualization \nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nimport seaborn as sns\n\n# stat on data\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\n# feature reduction\nfrom sklearn.decomposition import PCA\n\n#---- Machine learning\n# data preparation\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n# model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\n\n# hyperparameter tunnig\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import cross_val_score\n\n# # Import necessary modules for neutral network\n# import keras\n# from keras.layers import Dense, BatchNormalization\n# from keras.models import Sequential\n# from keras.callbacks import EarlyStopping, ModelCheckpoint, History\n\n# Model evaluation\nimport math\nfrom sklearn import metrics\nfrom statsmodels.graphics.api import abline_plot","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Study area location**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(7,7))\n\n# SHOW LOCATION OF THE GEYSER GEOTHERMAL FIELD\nax1 = plt.axes(projection=ccrs.PlateCarree())\nax1.set_extent([-123, -121, 37,39], crs=ccrs.PlateCarree())\n\n# add color\nax1.add_feature(cfeature.OCEAN.with_scale('10m'))\nax1.add_feature(cfeature.LAND)\nax1.add_feature(cfeature.RIVERS)\nax1.coastlines()\n\n# add grid\ngl = ax1.gridlines(crs=ccrs.PlateCarree(), draw_labels=True, linewidth=2, color='gray', alpha=0.5, linestyle='--')\ngl.xlabels_top = False\ngl.ylabels_right = False\ngl.xlocator = mticker.FixedLocator([-122.5, -121.5])\ngl.ylocator = mticker.FixedLocator([38.5, 37.5,37])\ngl.xformatter = LONGITUDE_FORMATTER\ngl.yformatter = LATITUDE_FORMATTER\ngl.xlabel_style = {'size': 13, 'color': 'gray', 'weight': 'bold'}\ngl.ylabel_style = {'size': 13, 'color': 'gray', 'weight': 'bold'}\n\n# San Francisco/Coordinates\nax1.scatter(x =-122.45, y=37.7, s=2000,c='black')\nax1.text(-122.3, 37.6, 'San Francisco', size=16)\n\n# San Francisco/Coordinates\nax1.scatter(x =-122.801046, y=38.821042, s=2000,c='green')\nax1.text(-122.62, 38.8, 'The Geysers', size=16)\nax1.text(-122.68, 38.7, 'geothermal field', size=16)\n\n# set title\nax1.set_title('Induced seismic events location',size=15)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id=\"SEISMIC_DATA\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>1: SEISMIC DATA</center></h3>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1.1\"></a>\n## 1.1: Load the data","metadata":{}},{"cell_type":"code","source":"# step1: Load the seismic catalog \ncatalogue = pd.read_csv(r'../input/water-injection-induced-seismic-events/seismic catalogue 2003 2016.csv')\nprint('They are {} induced-seismic events from 2003 to 2016 in our study area.'.format(catalogue.shape[0]))\ncatalogue.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set date as index\ncatalogue['date'] = pd.to_datetime(catalogue['date'])\ncatalogue = catalogue.set_index('date')\ncatalogue.tail(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"See that the last month has only one day!","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1.2\"></a>\n## 1.2: Simple Statistical seismology\n### 1.2.1: Plot the ECDF of the Earthquake magnitudes","metadata":{}},{"cell_type":"code","source":"def ecdf(data):\n    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n    # Number of data points: n\n    n = len(data)\n    # x-data for the ECDF: x \n    x = np.sort(data)\n    # y-data for the ECDF: y  The y data of the ECDF go from 1/n to 1 in equally spaced increments. \n    y = np.arange(1,n+1) / n\n    \n    return x, y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define figure size\nfig = plt.figure(figsize=(5,5))\n\n# figure title\nfig.suptitle('Empirical Cumulative Distribution Function', fontsize=18)\n\nmags = catalogue['Ml']\nax1 = plt.plot(*ecdf(mags),marker='.',linestyle = 'none')\nax1 = plt.xlabel('magnitude')\nax1 = plt.ylabel('ECDF')\nax1 = plt.text(1.5, 0.2, 'max magnitude {}'.format(mags.max()),fontsize=14)\nax1 = plt.text(1.7, 0.3, 'Nb. events {}'.format(len(mags)),fontsize=14)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.2.2\"></a>\n###  1.2.2: Computing the b-value ","metadata":{}},{"cell_type":"code","source":"# define fonction to compute b-value with confident interval\ndef b_value(mags, mt):\n    \"\"\"Compute the b-value.\"\"\"\n    # Extract magnitudes above completeness threshold: m\n    m = mags[mags >= mt]\n    # Compute b-value: b\n    b = (np.mean(m)-mt)*np.log(10)\n    return b","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Because there are plenty of earthquakes above magnitude 1, \n# we can use mt = 1 as our completeness threshold.\nmt = 1\n\n# Compute b-value and confidence interval\nb = b_value(mags, mt)\n\nprint('The b-value is {0:.2f}'.format(b))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The seismicity at the Geysers follows the Gutenberg-Richter law quite well, with a b-around 1.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1.3\"></a>\n## 1.3: Seicmic activity evolution from 2003 to 2016 \n### 1.3.1: b-value evolution","metadata":{}},{"cell_type":"code","source":"# create new column for month and year\ncatalogue['years_months'] = catalogue.index.to_period('M')\n\n#  calculate b_value each month\nyears_months = catalogue['years_months'].unique()\nlist_b= []\nfor year_month in years_months:\n    df = catalogue[catalogue['years_months']==year_month]\n    mags = df['Ml']\n    b = b_value(mags, mt)\n    list_b.append(b)\n\ndf_b_value = pd.DataFrame({'years_months':years_months,'b_value':list_b})    \ndf_b_value.tail(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop last month (march 2016)\ndf_b_value = df_b_value[:-1]\n# convert period to date\ndf_b_value['years_months'] = df_b_value['years_months'].values.astype('datetime64[M]')\n# set 'years_months' as index\ndf_b_value = df_b_value.set_index('years_months')\ndf_b_value.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define figure size\nfig = plt.figure(figsize=(10,5))\n\n# figure \nfig.suptitle('Evolution b-value', fontsize=18)\nax1 = plt.plot(df_b_value.index,df_b_value['b_value'],marker = 'o',color='red')\nax1 = plt.xlabel('year',size=14)\nax1 = plt.ylabel('b-value',size=14)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.3.2\"></a>\n### 1.3.2: monthly seismic energy released","metadata":{}},{"cell_type":"code","source":"# calculate the amount of energy released  by an earthquake\ncatalogue['Energy'] = 10**(11.8 + 1.5*catalogue['Ml'])\ncatalogue.head(2)\n\n# calculate the monthly amount of seismic energy releases\nmonthly_energy_released = catalogue.groupby(['years_months'])['Energy'].sum()\nmonthly_energy_released = pd.DataFrame(monthly_energy_released)\n\n# convert period to date\nmonthly_energy_released = monthly_energy_released.reset_index()\nmonthly_energy_released['Date'] = monthly_energy_released['years_months'].values.astype('datetime64[M]')\n\n# set date as index\nmonthly_energy_released = monthly_energy_released.set_index('Date')\n\n# drop unwanted column\nmonthly_energy_released = monthly_energy_released.drop(columns=['years_months'])\n\n# drop 3 last month (March 2016 only one days and outliers)\nmonthly_energy_released = monthly_energy_released[:-1]\n\nmonthly_energy_released.tail(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define figure size\nfig = plt.figure(figsize=(10,5))\n\n# figure \nfig.suptitle('Monthly seismic energy released', fontsize=18)\nax1 = plt.plot(monthly_energy_released.index,monthly_energy_released['Energy'])\nax1 = plt.xlabel('year',size=14)\nax1 = plt.ylabel('Total energy per month',size=14)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.3.3\"></a>\n### 1.3.3: Density maps of induced-earthquakes","metadata":{}},{"cell_type":"code","source":"# create nodes' coordinates for interpolation\nx= np.linspace(-122.86, -122.74, 50)\n# select the first two and two number of the list\nx_min = np.mean(x[0:2])\nx_max = np.mean(x[-2:])\n\n# calculate the nodes coordinate between these extremes\ny= np.linspace(38.76, 38.88, 50)\ny_min = np.mean(y[0:2])\ny_max = np.mean(y[-2:])\n\n# calulate the center coordinate of each bin\nx_mean = np.linspace(x_min, x_max, 49)\ny_mean = np.linspace(y_min, y_max, 49)\n# create list with coordinates\nlist_mean_coord = []\nfor longitude in x_mean:\n    for latitude in y_mean:\n        list_mean_coord.append([longitude,latitude])\n        \ndf_mean_coord = pd.DataFrame(list_mean_coord,columns=['mean_long','mean_lat'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.tri as tri\n\ndef plot_snapchot(df,year):\n    # -----------------------\n    # Interpolation on a grid\n    # -----------------------\n    # A contour plot of irregularly spaced data coordinates\n    # via interpolation on a grid.\n    xmin = df['mean_long'].min()\n    xmax = df['mean_long'].max()\n    ymin = df['mean_lat'].min()\n    ymax = df['mean_lat'].max()\n\n    npts = df.shape[0]\n    ngridx = 50\n    ngridy = 50\n\n    # Create grid values first.\n    X_int = np.linspace(xmin, xmax, ngridx)\n    Y_int = np.linspace(ymin, ymax, ngridy)\n\n    # Perform linear interpolation of the data (x,y)\n    # on a grid defined by (xi,yi)\n    triang = tri.Triangulation(df['mean_long'], df['mean_lat'])\n    interpolator = tri.LinearTriInterpolator(triang, df['count'])\n    Xi, Yi = np.meshgrid(X_int, Y_int)\n    Z_int  = interpolator(Xi, Yi)\n\n    # define figure size\n#     fig, ax = plt.subplots(figsize=(6,4))\n    \n    ax = plt.contourf(X_int, Y_int, Z_int, levels=50, cmap=\"RdBu_r\")\n    plt.title('{}'.format(year), fontsize=14)\n    x = [-122.84, -122.8,-122.76]\n    plt.xticks(x)\n    \n    # fig.colorbar()\n    plt.colorbar(ax)\n\n    return ax","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_meq(year):\n    data = catalogue[catalogue['year']==year]\n    # df with only coordinate\n    data = data[['Longitude','Latitude']]\n    data = data.reset_index()\n    data = data.drop('date',axis=1)\n    # bin the data into equally spaced groups\n    x_cut = pd.cut(data.Longitude, np.linspace(-122.86, -122.74, 50), right=False)\n    y_cut = pd.cut(data.Latitude, np.linspace(38.76, 38.88, 50), right=False)\n\n    # group and count\n    result = data.groupby([x_cut,y_cut]).count()\n\n    # rename columns and flatten df\n    result.columns = ['countx','count']\n    result = result.reset_index()\n    # select only count\n    count = result[['count']]\n    # fill NaN value with zero\n    count = count.fillna(0)\n    # append count\n    df_result = pd.concat([df_mean_coord,count],axis=1)\n    return df_result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract year from index\ncatalogue['year'] = pd.DatetimeIndex(catalogue.index).year\n\n# use function count number of seismic events per bins per year\ndf_result_2003 = count_meq(2003)\ndf_result_2004 = count_meq(2004)\ndf_result_2005 = count_meq(2005)\ndf_result_2006 = count_meq(2006)\ndf_result_2007 = count_meq(2007)\ndf_result_2008 = count_meq(2008)\ndf_result_2009 = count_meq(2009)\ndf_result_2010 = count_meq(2010)\ndf_result_2011 = count_meq(2011)\ndf_result_2012 = count_meq(2012)\ndf_result_2013 = count_meq(2013)\ndf_result_2014 = count_meq(2014)\ndf_result_2015 = count_meq(2015)\ndf_result_2016 = count_meq(2016)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(15,15))\nfig.subplots_adjust(hspace=0.4,wspace=0.3)\n\nplt.suptitle('Density maps (number of eartquake per bin) for the year: ',y=0.92,size=16)\n\n# color based on trend\nax1 = fig.add_subplot(5,3,1)\nax1 = plot_snapchot(df_result_2003,'2003')\n\nax2 = fig.add_subplot(5,3,2)\nax2 = plot_snapchot(df_result_2004,'2004')\n\nax1 = fig.add_subplot(5,3,3)\nax1 = plot_snapchot(df_result_2005,'2005')\n\nax1 = fig.add_subplot(5,3,4)\nax1 = plot_snapchot(df_result_2006,'2006')\n\nax1 = fig.add_subplot(5,3,5)\nax1 = plot_snapchot(df_result_2007,'2007')\n\nax2 = fig.add_subplot(5,3,6)\nax2 = plot_snapchot(df_result_2008,'2008')\n\nax1 = fig.add_subplot(5,3,7)\nax1 = plot_snapchot(df_result_2009,'2009')\n\nax1 = fig.add_subplot(5,3,8)\nax1 = plot_snapchot(df_result_2010,'2010')\n\nax1 = fig.add_subplot(5,3,9)\nax1 = plot_snapchot(df_result_2011,'2011')\n\nax1 = fig.add_subplot(5,3,10)\nax1 = plot_snapchot(df_result_2012,'2012')\n\nax1 = fig.add_subplot(5,3,11)\nax1 = plot_snapchot(df_result_2013,'2013')\n\nax1 = fig.add_subplot(5,3,12)\nax1 = plot_snapchot(df_result_2014,'2014')\n\nax1 = fig.add_subplot(5,3,13)\nax1 = plot_snapchot(df_result_2015,'2015')\n\nax1 = fig.add_subplot(5,3,14)\nax1 = plot_snapchot(df_result_2016,'2016')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id=\"INJECTION_DATA\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>2: INJECTION DATA</center></h3>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.1\"></a>\n## 2.1: Load and prepare the data\n\nInitially, I have downloaded 73 files for the 73 injection wells. In each file I have selected four columns:\n- date (YY/MM),\n- \"Gross Injected (1000kg)\", which represent the volume of water injected during the month,\n- \"Water Injection Rate (1000 kg/hr)\", \n- \"Days\", which is the number of days during the month were water was injected.\n\nThen, three files were generated:\n- \"Total Gross Injected Water\", with the \"Gross Injected (1000kg)\" for the 73 wells from 1969-05-01\tto 2021-02-01\n- \"Total Water Injection Rate\", with the \"Water Injection Rate (1000 kg/hr)\" for the 73 wells from 1969-05-01\tto 2021-02-01\n- \"Total days_inj\", with the \"days\" for the 73 wells from 1969-05-01 to 2021-02-01\n\nIn these three files, the columns names are made with the API well numbers. An API is an \"unique, permanent, numeric identifier\" assigned to each well drilled for oil and gas in the United States.\n","metadata":{}},{"cell_type":"code","source":"###-------- LOAD THE INJECTION DATA\n# Load monthly amount of injected water: \"Gross Injected (1000kg)\"\ndf_GIW_per_well = pd.read_csv(r'../input/water-injection-induced-seismic-events/Total Gross Injected Water.csv',sep=';')\n# Load monthly injection rate: \"Water Injection Rate (1000 kg/hr)\"\ndf_RIW_per_well = pd.read_csv(r'../input/water-injection-induced-seismic-events/Total Water Injection Rate.csv',sep=';')\n# Load number of days per month where \ndf_days_per_well = pd.read_csv(r'../input/water-injection-induced-seismic-events/Total days_inj.csv',sep=';')\n\n# set date as index\ndf_GIW_per_well['Date'] = pd.to_datetime(df_GIW_per_well['Date'])\ndf_GIW_per_well = df_GIW_per_well.set_index('Date')\n\ndf_RIW_per_well['Date'] = pd.to_datetime(df_RIW_per_well['Date'])\ndf_RIW_per_well = df_RIW_per_well.set_index('Date')\n\ndf_days_per_well['Date'] = pd.to_datetime(df_days_per_well['Date'])\ndf_days_per_well = df_days_per_well.set_index('Date')\n\n# select the period as during which the induced seismic events are monitored \ndf_GIW_per_well = df_GIW_per_well.loc['2003-05-01':'2016-02-01']\ndf_RIW_per_well = df_RIW_per_well.loc['2003-05-01':'2016-02-01']\ndf_days_per_well = df_days_per_well.loc['2003-05-01':'2016-02-01']\n\ndf_GIW_per_well.tail(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the data\ndf_GIW_per_well.plot(subplots=True, figsize=(15,40))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_RIW_per_well.plot(subplots=True, figsize=(15,40))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_days_per_well.plot(subplots=True, figsize=(15,40))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that many injection wells were not used during this period. they were either not drilled yet or abandonned, so we can remove them.","metadata":{}},{"cell_type":"code","source":"# first we drop all the columns where 'Gross Injected (1000kg)' is 0 everywhere\nprint('Number of columns before dropping well not used for injection during the selected period: {}'.format(df_GIW_per_well.shape[1]))\ndf_GIW_per_well = df_GIW_per_well.loc[:, df_GIW_per_well.any()]\nprint('final number of wells used: {}'.format(df_GIW_per_well.shape[1]))\nprint('')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# then we make sure to keep the same columns in the two other df\ncolumns_to_keep = df_GIW_per_well.columns\ndf_RIW_per_well = df_RIW_per_well.drop(columns=[col for col in df_RIW_per_well if col not in columns_to_keep])\ndf_days_per_well = df_days_per_well.drop(columns=[col for col in df_days_per_well if col not in columns_to_keep])\n\n# add prefix\ndf_GIW_per_well = df_GIW_per_well.add_prefix('GIW_')\ndf_RIW_per_well = df_RIW_per_well.add_prefix('RIW_')\ndf_days_per_well = df_days_per_well.add_prefix('days_')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(15,8),sharex=True)\n# fig.subplots_adjust(hspace=0.4,wspace=0.3)\n\ndf_GIW_per_well.plot(ax=axes[0],legend=None)\naxes[0].set_ylabel('GIW',size=14)\ndf_RIW_per_well.plot(ax=axes[1],legend=None)\naxes[1].set_ylabel('RIW',size=14)\ndf_days_per_well.plot(ax=axes[2],legend=None)\naxes[2].set_ylabel('days',size=14)\naxes[2] = plt.xlabel('year',size=14)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can observe that there is a negative value for the water injection rate (RIW). It is not possible, so we will replace it by '0'. ","metadata":{}},{"cell_type":"code","source":"for col in df_RIW_per_well.columns:\n    df_RIW_per_well[col][df_RIW_per_well[col] < 0] = 0\n    \ndf_RIW_per_well.plot(legend=None)\nplt.ylabel('injection rate (x1000kg/hr)')\nplt.show()\n\nprint('Now, on the initial 73 injection wells only {} were injected during this period'\\\n      .format(df_RIW_per_well.shape[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id=\"INJ_VS_SEISM\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>3: INJECTION DATA VS INDUCED SEISMICITY</center></h3>","metadata":{}},{"cell_type":"code","source":"# create empty df:\ndf_features_vs_sismicity = pd.DataFrame()\n\n# add column with total amount of water injected per month in the area\ntotal_vol_inj = df_GIW_per_well.sum(axis=1)\n# add the monthly seismic energy released in this area\ndf_features_vs_sismicity = pd.concat([monthly_energy_released,df_b_value,total_vol_inj],axis=1)\ndf_features_vs_sismicity.columns = ['Energy','b_value','GIW_sum']\n\ndf_features_vs_sismicity.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.1\"></a>\n### 3.1: injection vs seismic energy","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,5))\n\nplt.title('Seasonal evolutions of the amount of injected water and the seismic activity ',size=18, y=1.06) \n \n\nax.plot(df_features_vs_sismicity.index, df_features_vs_sismicity[\"GIW_sum\"],color='darkblue')\nax.set_xlabel('year',fontsize=15)\nax.set_ylabel('Total injected water (1000kg)', color='darkblue',fontsize=15)\nax.tick_params(axis='both', which='major', labelsize=14)\nax.tick_params('y', colors='darkblue')\n\nax2 = ax.twinx()  #specify that the two lines share the same x-axis\nax2.plot(df_features_vs_sismicity.index,df_features_vs_sismicity['Energy'],color='green')\nax2.set_ylabel('Energy seismic', color='green',fontsize=15)\nax2.tick_params('y', colors='green')\nax2.tick_params(axis='both', which='major', labelsize=14)\n\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.2\"></a>\n### 3.2: Injection vs b_value","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,5))\n\nplt.title('Seasonal evolutions of the amount of injected water and b_value ',size=18, y=1.06) \n \n\nax.plot(df_features_vs_sismicity.index, df_features_vs_sismicity[\"GIW_sum\"],color='darkblue')\nax.set_xlabel('year',fontsize=15)\nax.set_ylabel('Total injected water (1000kg)', color='darkblue',fontsize=15)\nax.tick_params(axis='both', which='major', labelsize=14)\nax.tick_params('y', colors='darkblue')\n\nax2 = ax.twinx()  #specify that the two lines share the same x-axis\nax2.plot(df_features_vs_sismicity.index,df_features_vs_sismicity['b_value'],color='red')\nax2.set_ylabel('b_value', color='red',fontsize=15)\nax2.tick_params('y', colors='red')\nax2.tick_params(axis='both', which='major', labelsize=14)\n\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can clearly observe a saisonanlity in the amount of injected water (more injected water during the winter months and less during the summer months), in the amount of seismic energy released every month, and in the b_value. However, it seems there is a lag period between injection pick, seismicity pick, and b_value pick. ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.3\"></a>\n### 3.3: lag periods","metadata":{}},{"cell_type":"code","source":"# create dictionnary to write shift version on injection data\ndic_GIW_shift = {}\n# create dictionnary to write shorter version on seismic catalogue\ndic_shorter_catalogue={}\ndic_shorter_b_value={}\ndic_GIW_SEISMIC={}\n\nlist_coef_GIW_Energy = []\nlist_coef_GIW_b_value = []\n\nfor i in range (0,6):\n    # Creating a time-shifted dictionnary with the injection data and remove nan values\n    dic_GIW_shift['GIW_shift{}'.format(i)] = df_features_vs_sismicity[\"GIW_sum\"].shift(i).dropna()\n    # create a shorter version of seismic data to fit the lenght of the lagged versions\n    if i == 0:\n        dic_shorter_catalogue['shorter_catalogue{}'.format(i)] = df_features_vs_sismicity['Energy']\n        dic_shorter_b_value['shorter_b_value{}'.format(i)] = df_features_vs_sismicity['b_value']\n    else:\n        dic_shorter_catalogue['shorter_catalogue{}'.format(i)] = df_features_vs_sismicity['Energy'].iloc[:-i]\n        dic_shorter_b_value['shorter_b_value{}'.format(i)] = df_features_vs_sismicity['b_value'].iloc[:-i]\n    # store time-shifted series with the associated seismic moment  \n    dic_GIW_SEISMIC['{}'.format(i)] = pd.concat([dic_GIW_shift['GIW_shift{}'.format(i)],\n                                                 dic_shorter_catalogue['shorter_catalogue{}'.format(i)],\n                                                 dic_shorter_b_value['shorter_b_value{}'.format(i)]],\n                                                 axis=1,join='inner')\n   # calculate the correlation coefficient \n    coef_GIW_Energy = dic_GIW_shift['GIW_shift{}'.format(i)].corr(dic_shorter_catalogue['shorter_catalogue{}'.format(i)] )\n    list_coef_GIW_Energy.append(coef_GIW_Energy)\n    coef_GIW_b_value = dic_GIW_shift['GIW_shift{}'.format(i)].corr(dic_shorter_b_value['shorter_b_value{}'.format(i)] )\n    list_coef_GIW_b_value.append(coef_GIW_b_value)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_month = list(range (0,6))\n\nfig = plt.figure(2,figsize=(16,10))\n\nax1 = fig.add_subplot(2,2,1)\n# plot coef for the lagged version\nax1.plot(list_month,list_coef_GIW_Energy,marker='o',linestyle = '-')\n\n# Label axes and show plot\nax1 = plt.xlabel('lagged version (month)',fontsize=15)\nax1 = plt.xticks(fontsize=14)\nax1 = plt.ylabel('correlation coefficient',fontsize=15)\nax1 = plt.yticks(fontsize=14)\nax1 = plt.title('correlation coefficient between seismic activity and \\nseveral lagged versions of the injection data',fontsize=16,y=1.05)\n\nax2 = fig.add_subplot(2,2,2)\n# plot coef for the lagged version and a linear-regression-line\nax2.scatter(dic_GIW_shift['GIW_shift2'],dic_shorter_catalogue['shorter_catalogue2'],marker='o')\nm, b = np.polyfit(dic_GIW_shift['GIW_shift2'],dic_shorter_catalogue['shorter_catalogue2'], 1)\nax2 = plt.plot(dic_GIW_shift['GIW_shift2'],m*dic_GIW_shift['GIW_shift2']+b,color='k')\n# add coef correlation at location x and y\nax2 = plt.text(2500000, 2e18, 'R=0.34',fontsize=15)\n# # Label axes and show plot\nax2 = plt.xlabel('2 Months lagged version of injection data',fontsize=15)\nax2 = plt.xticks(fontsize=14)\nax2 = plt.ylabel('monthly seismic energy released',fontsize=15)\nax2 = plt.yticks(fontsize=14)\nax2 = plt.title('Injection data (2 months shifted) versus \\n seismic activity',fontsize=16,y=1.05)\n\n\nplt.show()    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_month = list(range (0,6))\n\nfig = plt.figure(2,figsize=(16,10))\n\nax1 = fig.add_subplot(2,2,1)\n# plot coef for the lagged version\nax1.plot(list_month,list_coef_GIW_b_value,marker='o',linestyle = '-', color= 'red')\n\n# Label axes and show plot\nax1 = plt.xlabel('lagged version (month)',fontsize=15)\nax1 = plt.xticks(fontsize=14)\nax1 = plt.ylabel('correlation coefficient',fontsize=15)\nax1 = plt.yticks(fontsize=14)\nax1 = plt.title('correlation coefficient between b_value and \\nseveral lagged versions of the injection data',fontsize=16,y=1.05)\n\nax2 = fig.add_subplot(2,2,2)\n# plot coef for the lagged version and a linear-regression-line\nax2.scatter(dic_GIW_shift['GIW_shift2'],dic_shorter_b_value['shorter_b_value2'],marker='o')\nm, b = np.polyfit(dic_GIW_shift['GIW_shift2'],dic_shorter_b_value['shorter_b_value2'], 1)\nax2 = plt.plot(dic_GIW_shift['GIW_shift2'],m*dic_GIW_shift['GIW_shift2']+b,color='k')\n# add coef correlation at location x and y\n# ax2 = plt.text(2500000, 52, 'R=0.34',fontsize=15)\n# # Label axes and show plot\nax2 = plt.xlabel('2 Months lagged version of injection data',fontsize=15)\nax2 = plt.xticks(fontsize=14)\nax2 = plt.ylabel('monthly b_value',fontsize=15)\nax2 = plt.yticks(fontsize=14)\nax2 = plt.title('Injection data (2 months shifted) versus \\n b_value',fontsize=16,y=1.05)\n\n\nplt.show()    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The highest coefficient correlation was found after 2 months, meaning that seismicity peak occurs 2 months after peak injection.  Same with b-values.","metadata":{}},{"cell_type":"markdown","source":"## <a id=\"Data_Preparation\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>4: DATA PREPARATION</center></h3>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4.1\"></a>\n## 4.1: Dependent variables:\n### 4.1.1: Check the asymmetry of the probability distribution \n#### b-value","metadata":{}},{"cell_type":"code","source":"def plot_distribution(df,variable):\n    ax = sns.displot(data=df, x=variable, kde=True)\n\n    # Get the fitted parameters used by the function\n    (mu, sigma) = norm.fit(df[variable])\n    print( '\\n mu = {:.2E} and sigma = {:.2E}\\n'.format(mu, sigma))\n\n    #Now plot the distribution\n    plt.legend(['Normal dist. ($\\mu=$ {:.2E} and $\\sigma=$ {:.2E} )'.format(mu, sigma)],\n                loc='best')\n    plt.ylabel('Frequency')\n    plt.title('moment distribution')\n\n    #Get also the QQ-plot\n    fig = plt.figure()\n    res = stats.probplot(df[variable], plot=plt)\n    plt.show()\n    \n    #skewness and kurtosis\n    print(\"Skewness: %f\" % df[variable].skew())\n    print(\"Kurtosis: %f\" % df[variable].kurt())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### distribution b_value","metadata":{}},{"cell_type":"code","source":"plot_distribution(df_b_value,'b_value')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Seismic energy released with Mw>4","metadata":{}},{"cell_type":"code","source":"plot_distribution(monthly_energy_released,'Energy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Both target variables have:\n- a distribution close from the normal distribution.\n- a low skewness. (means that the tail on the right side of the distribution is longer or fatter. The mean and median will be greater than the mode.)\n\n- low Kurtosis. (means that the data tends to have heavy tails, or outliers)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4.1.2\"></a>\n### 4.1.2: Log transform skewed targets:\n**Ideally, we want our skewness value to be around 0 and kurtosis less than 3.** ","metadata":{}},{"cell_type":"code","source":"# calculate log b_value\ndf_b_value['log_b_value'] = np.log1p(df_b_value['b_value'])\n# calculate log Energy\nmonthly_energy_released['log_Energy'] = np.log1p(monthly_energy_released['Energy'])\n\n#skewness and kurtosis\nprint(\"Skewness log_b_value: %f\" % df_b_value['log_b_value'].skew())\nprint(\"Kurtosis log_b_value: %f\" % df_b_value['log_b_value'].kurt())\nprint('')\nprint(\"Skewness log_Energy: %f\" % monthly_energy_released['log_Energy'].skew())\nprint(\"Kurtosis log_Energy: %f\" % monthly_energy_released['log_Energy'].kurt())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calcul correlation between the 2 target variables\nmonthly_energy_released['log_Energy'].corr(df_b_value['b_value'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.2\"></a>\n## 4.2: Independent variables  \n### 4.2.1: define functions used for data preparation","metadata":{}},{"cell_type":"markdown","source":"#### Function for features extraction","metadata":{}},{"cell_type":"code","source":"def extract_features(df):\n    df_features = pd.DataFrame()\n    df_features[\"sum\"] = df.sum(axis=1)\n    df_features[\"max\"] = df.max(axis=1)\n    df_features[\"mean\"] = df.mean(axis=1)\n    df_features[\"std\"] = df.std(axis=1)\n    df_features[\"skew\"] = df.skew(axis=1)\n    df_features[\"kurtosis\"] = df.kurtosis(axis=1)\n    df_features[\"5pct\"] = df.quantile(.05,axis=1)\n    df_features[\"10pct\"] = df.quantile(.1,axis=1)\n    df_features[\"15pct\"] = df.quantile(.15,axis=1)\n    df_features[\"20pct\"] = df.quantile(.2,axis=1)\n    df_features[\"25pct\"] = df.quantile(.25,axis=1)\n    df_features[\"30pct\"] = df.quantile(.3,axis=1)\n    df_features[\"35pct\"] = df.quantile(.35,axis=1)\n    df_features[\"40pct\"] = df.quantile(.4,axis=1)\n    df_features[\"45pct\"] = df.quantile(.45,axis=1)\n    df_features[\"50pct\"] = df.quantile(.50,axis=1)\n    df_features[\"55pct\"] = df.quantile(.55,axis=1)\n    df_features[\"60pct\"] = df.quantile(.60,axis=1)\n    df_features[\"65pct\"] = df.quantile(.65,axis=1)\n    df_features[\"70pct\"] = df.quantile(.70,axis=1)\n    df_features[\"75pct\"] = df.quantile(.75,axis=1)\n    df_features[\"80pct\"] = df.quantile(.80,axis=1)\n    df_features[\"85pct\"] = df.quantile(.85,axis=1)\n    df_features[\"90pct\"] = df.quantile(.90,axis=1)\n    df_features[\"95pct\"] = df.quantile(.95,axis=1)\n    return df_features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Function for adding lag versions","metadata":{}},{"cell_type":"code","source":"def create_lag(df):\n    df_shift1 = pd.DataFrame()\n    df_shift1 = df.shift(1)\n    df_shift1 = df.add_prefix('S1_')\n    df_shift2 = pd.DataFrame()\n    df_shift2 = df.shift(2)\n    df_shift2 = df.add_prefix('S2_')\n    df_shift3 = pd.DataFrame()\n    df_shift3 = df.shift(3)\n    df_shift3 = df_shift3.add_prefix('S3_')\n    df_shift4 = pd.DataFrame()\n    df_shift4 = df.shift(4)\n    df_shift4 = df_shift4.add_prefix('S4_')\n\n    df_lags = pd.concat([df,df_shift1,df_shift2,df_shift3, df_shift4], axis=1)\n    df_lags = df_lags.dropna() \n    return df_lags","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.2.2\"></a>\n## 4.2.2: Features extraction","metadata":{}},{"cell_type":"code","source":"# create new dataframe with new features\ndf_fearures_GIW = extract_features(df_GIW_per_well)\ndf_fearures_RIW = extract_features(df_RIW_per_well)\ndf_fearures_days = extract_features(df_days_per_well)\n# add prefix\ndf_fearures_GIW = df_fearures_GIW.add_prefix('GIW_')\ndf_fearures_RIW = df_fearures_RIW.add_prefix('RIW_')\ndf_fearures_days =df_fearures_days.add_prefix('days_')\n# concat df\ndf_features = pd.concat([df_fearures_GIW,df_fearures_RIW,df_fearures_days],axis=1)\ndf_features.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add lag versions :\ndf_features_lag = create_lag(df_features)\ndf_features_lag.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feature reduction\npca = PCA(n_components=0.99)\ndf_features_lag_reduced =  pd.DataFrame(pca.fit_transform(df_features_lag))\ndf_features_lag_reduced = df_features_lag_reduced.add_prefix('PCA_FEATURE_AXE_')\ndf_features_lag_reduced.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.2.3\"></a>\n## 4.2.3: Injection data: scaling, lag version, and feature reduction","metadata":{}},{"cell_type":"code","source":"df_GIW_per_well_lag = create_lag(df_GIW_per_well)\ndf_RIW_per_well_lag = create_lag(df_RIW_per_well)\ndf_days_per_well_lag = create_lag(df_days_per_well)\ndf_GIW_per_well_lag.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scale the 3 dataframes with: GIW, RIW, days \nscaler = StandardScaler()\ndf_GIW_per_well_lag_scaled = pd.DataFrame(scaler.fit_transform(df_GIW_per_well_lag))\ndf_GIW_per_well_lag_scaled = df_GIW_per_well_lag_scaled.add_prefix('GIW_')\ndf_RIW_per_well_lag_scaled = pd.DataFrame(scaler.fit_transform(df_RIW_per_well_lag))\ndf_RIW_per_well_lag_scaled = df_RIW_per_well_lag_scaled.add_prefix('RIW_')\ndf_days_per_well_lag_scaled = pd.DataFrame(scaler.fit_transform(df_days_per_well_lag))\ndf_days_per_well_lag_scaled = df_days_per_well_lag_scaled.add_prefix('days_')\n\n# concat the 3 df\ndf_injection_data_scaled = pd.concat([df_GIW_per_well_lag_scaled,\n                                      df_RIW_per_well_lag_scaled,\n                                      df_days_per_well_lag_scaled],axis=1)\ndf_injection_data_scaled.head(2)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feature reduction\npca = PCA(n_components=0.99)\ndf_injection_data_scaled_reduced = pd.DataFrame(pca.fit_transform(df_injection_data_scaled))\ndf_injection_data_scaled_reduced = df_injection_data_scaled_reduced.add_prefix('PCA_inj_AXE_')\ndf_injection_data_scaled_reduced.shape\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(pca.explained_variance_ratio_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.3\"></a>\n## 4.3: correlations between features and target variables","metadata":{}},{"cell_type":"code","source":"# create list to select features that correlates the best wiht seismic energy\nlist_variable_with_high_corr_Ener=[]\n# create list to select features that correlates the best wiht b-value\nlist_variable_with_high_corr_bval=[]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.3.1\"></a>\n### 4.3.1: function used to plot correlations","metadata":{}},{"cell_type":"code","source":"def plot_correlation(df,var1,var2,color):\n    \"\"\"\n    plot data and a linear regression model fit. \n    Parameters\n    ----------\n    df : dataframe\n    var1: 'column_name' of the variable 1 in df\n    var2: 'column_name' of the variable 2 in df\n    color : color scatter points\n    Returns\n    -------\n    Figure  \n    \"\"\"\n    # transform var1 and var2 into numpy array:\n    xm = np.array(df[var1])\n    ym = np.array(df[var2])\n    # get regression line properties:\n    slope, intercept, r_value, p_value, std_err = stats.linregress(xm, ym)\n    # Plot linear regression with 95% confidence interval and the regression coefficient\n    sns.regplot(x=var1,y=var2,data=df,fit_reg=True,color = color,\n                line_kws={'label':\"R={:.2f}\".format(r_value),\"color\": \"black\"}) \n    # axes and title properties\n    plt.xlabel(var1,fontsize=15)\n    plt.ylabel(var2,fontsize=15)\n    # plot legend\n    plt.legend(prop={'size': 15})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.3.2\"></a>\n### 4.3.2: prepare dataframe to calculate correlation coefficient","metadata":{}},{"cell_type":"code","source":"# resize targets (remove last 4 raws) to match size explanatory variables (4 rows missing due to drop NaN value after shift)\ntarget_Ener = monthly_energy_released['log_Energy'][:-4].reset_index()\ntarget_Ener = target_Ener.drop(columns = ['Date'],axis=1)\n\ntarget_bval = df_b_value['log_b_value'][:-4].reset_index()\ntarget_bval = target_bval.drop(columns = ['years_months'],axis=1)\n\n# injection data vs Energy: \ndf_injection_data_scaled_Ener = pd.concat([df_injection_data_scaled,target_Ener], axis=1)\ndf_pca_injection_data_scaled_Ener = pd.concat([df_injection_data_scaled_reduced,target_Ener], axis=1)\n\n# injection data vs b-value:\ndf_injection_data_scaled_bval = pd.concat([df_injection_data_scaled,target_bval], axis=1)\ndf_pca_injection_data_scaled_bval = pd.concat([df_injection_data_scaled_reduced,target_bval], axis=1)\n\n# features vs Energy:\ndf_features_lag2 = df_features_lag.reset_index().drop(columns = ['Date'],axis=1)\ndf_features_Ener = pd.concat([df_features_lag2,target_Ener],axis=1)\ndf_pca_features_Ener = pd.concat([df_features_lag_reduced,target_Ener], axis=1)\n\n# features vs b-value:\ndf_features_bval = pd.concat([df_features_lag2,target_bval],axis=1)\ndf_pca_features_bval = pd.concat([df_features_lag_reduced,target_bval], axis=1)\n\n# finally create one df with all the data\ndf_all_data = pd.concat([df_injection_data_scaled,df_injection_data_scaled_reduced,\n                         df_features_lag2,df_features_lag_reduced,                         \n                         target_Ener,target_bval], axis=1)\ndf_all_data = df_all_data.set_index(df_features_lag.index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.3.3\"></a>\n### 4.3.3:  correlation between seismic energy and:\n#### Raw injection data :","metadata":{}},{"cell_type":"code","source":"def calcul_sort_save_corr(df,variable,threshold=0.35):\n    # calcul, sort and save in df the correlation coefficient\n    corr = pd.DataFrame(df[df.columns[0:]].corr()[variable][:-1])\n    corr = pd.DataFrame(abs(corr[variable]).sort_values(ascending=False))\n\n    # select coef corr > 0.3 and append corresponding variables to a list\n    highest_corr = corr[corr[variable]>threshold]\n    print(corr.head(10))\n    return highest_corr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_injection_Ener = calcul_sort_save_corr(df_injection_data_scaled_Ener,'log_Energy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define figure size\nfig = plt.figure(figsize=(18,6))\nfig.subplots_adjust(hspace=0.4,wspace=0.3)\n\n# figure title\nfig.suptitle('Highest correlations found between raw injection data and seismic energy', fontsize=18,y=1)\n\n# subplot\nax1 = fig.add_subplot(2,2,1)\nax1 = plot_correlation(df_injection_data_scaled_Ener,'days_205','log_Energy','blue')\nax2 = fig.add_subplot(2,2,2)\nax2 = plot_correlation(df_injection_data_scaled_Ener,'RIW_222','log_Energy','green')\nax3 = fig.add_subplot(2,2,3)\nax3 = plot_correlation(df_injection_data_scaled_Ener,'GIW_58','log_Energy','red')\nax4 = fig.add_subplot(2,2,4)\nax4 = plot_correlation(df_injection_data_scaled_Ener,'GIW_10','log_Energy','black')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_pca_injection_Ener = calcul_sort_save_corr(df_pca_injection_data_scaled_Ener,'log_Energy',threshold=0.26)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define figure size\nfig = plt.figure(figsize=(18,6))\nfig.subplots_adjust(hspace=0.4,wspace=0.3)\n\n# figure title\nfig.suptitle('Highest correlations found between PCA axes from raw injection data and seismic energy', fontsize=18,y=1)\n\n# subplot\nax1 = fig.add_subplot(2,2,1)\nax1 = plot_correlation(df_pca_injection_data_scaled_Ener,'PCA_inj_AXE_4','log_Energy','blue')\nax2 = fig.add_subplot(2,2,2)\nax2 = plot_correlation(df_pca_injection_data_scaled_Ener,'PCA_inj_AXE_1','log_Energy','green')\nax3 = fig.add_subplot(2,2,3)\nax3 = plot_correlation(df_pca_injection_data_scaled_Ener,'PCA_inj_AXE_6','log_Energy','red')\nax4 = fig.add_subplot(2,2,4)\nax4 = plot_correlation(df_pca_injection_data_scaled_Ener,'PCA_inj_AXE_21','log_Energy','black')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### extracted features:","metadata":{}},{"cell_type":"code","source":"corr_features_Ener = calcul_sort_save_corr(df_features_Ener,'log_Energy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define figure size\nfig = plt.figure(figsize=(18,9))\nfig.subplots_adjust(hspace=0.4,wspace=0.3)\n\n# figure title\nfig.suptitle('Highest correlations found between extracted features and seismic energy', fontsize=18,y=0.95)\n\n# subplot\nax1 = fig.add_subplot(3,2,1)\nax1 = plot_correlation(df_features_Ener,'S4_days_90pct','log_Energy','blue')\nax2 = fig.add_subplot(3,2,2)\nax2 = plot_correlation(df_features_Ener,'S4_days_std','log_Energy','green')\nax3 = fig.add_subplot(3,2,3)\nax3 = plot_correlation(df_features_Ener,'S4_days_kurtosis','log_Energy','red')\nax4 = fig.add_subplot(3,2,4)\nax4 = plot_correlation(df_features_Ener,'S4_days_skew','log_Energy','black')\nax5 = fig.add_subplot(3,2,5)\nax5 = plot_correlation(df_features_Ener,'S2_GIW_95pct','log_Energy','gold')\nax6 = fig.add_subplot(3,2,6)\nax6 = plot_correlation(df_features_Ener,'S1_GIW_95pct','log_Energy','darkblue')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### features obtained with PCA","metadata":{}},{"cell_type":"code","source":"corr_pca_features_Ener = calcul_sort_save_corr(df_pca_features_Ener,'log_Energy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define figure size\nfig = plt.figure(figsize=(18,3))\nfig.subplots_adjust(hspace=0.4,wspace=0.3)\n\n# figure title\nfig.suptitle('Highest correlations found between PCA axes from extracted features and seismic energy', fontsize=18,y=1)\n\n# subplot\nax1 = fig.add_subplot(1,2,1)\nax1 = plot_correlation(df_pca_features_Ener,'PCA_FEATURE_AXE_0','log_Energy','blue')\nax2 = fig.add_subplot(1,2,2)\nax2 = plot_correlation(df_pca_features_Ener,'PCA_FEATURE_AXE_2','log_Energy','green')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.3.4\"></a>\n### 4.3.4:  correlation between b value and:\n#### raw injection data","metadata":{}},{"cell_type":"code","source":"corr_injection_bval = calcul_sort_save_corr(df_injection_data_scaled_bval,'log_b_value',threshold = 0.75)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define figure size\nfig = plt.figure(figsize=(18,9))\nfig.subplots_adjust(hspace=0.4,wspace=0.3)\n\n# figure title\nfig.suptitle('Highest correlations found between raw injection data and b-value', fontsize=18,y=1)\n\n# subplot\nax1 = fig.add_subplot(3,2,1)\nax1 = plot_correlation(df_injection_data_scaled_bval,'days_162','log_b_value','blue')\nax2 = fig.add_subplot(3,2,2)\nax2 = plot_correlation(df_injection_data_scaled_bval,'days_210','log_b_value','green')\nax3 = fig.add_subplot(3,2,3)\nax3 = plot_correlation(df_injection_data_scaled_bval,'days_234','log_b_value','red')\nax4 = fig.add_subplot(3,2,4)\nax4 = plot_correlation(df_injection_data_scaled_bval,'days_186','log_b_value','black')\nax5 = fig.add_subplot(3,2,5)\nax5 = plot_correlation(df_injection_data_scaled_bval,'GIW_234','log_b_value','gold')\nax6 = fig.add_subplot(3,2,6)\nax6 = plot_correlation(df_injection_data_scaled_bval,'GIW_186','log_b_value','darkblue')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### PCA axes from injection data :","metadata":{}},{"cell_type":"code","source":"corr_pca_injection_bval = calcul_sort_save_corr(df_pca_injection_data_scaled_bval,'log_b_value',threshold = 0.70)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define figure size\nfig = plt.figure(figsize=(18,6))\nfig.subplots_adjust(hspace=0.4,wspace=0.3)\n\n# figure title\nfig.suptitle('Highest correlations found between PCA axes from raw injection data and b-value', fontsize=18,y=1)\n\n# subplot\nax1 = fig.add_subplot(2,2,1)\nax1 = plot_correlation(df_pca_injection_data_scaled_bval,'PCA_inj_AXE_0','log_b_value','blue')\nax2 = fig.add_subplot(2,2,2)\nax2 = plot_correlation(df_pca_injection_data_scaled_bval,'PCA_inj_AXE_2','log_b_value','green')\nax3 = fig.add_subplot(2,2,3)\nax3 = plot_correlation(df_pca_injection_data_scaled_bval,'PCA_inj_AXE_3','log_b_value','red')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Extracted features","metadata":{}},{"cell_type":"code","source":"corr_features_bval = calcul_sort_save_corr(df_features_bval,'log_b_value',threshold = 0.74)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define figure size\nfig = plt.figure(figsize=(18,9))\nfig.subplots_adjust(hspace=0.4,wspace=0.3)\n\n# figure title\nfig.suptitle('Highest correlations found between extracted features and b-value', fontsize=18,y=.95)\n\n# subplot\nax1 = fig.add_subplot(3,2,1)\nax1 = plot_correlation(df_features_bval,'S4_days_skew','log_b_value','blue')\nax2 = fig.add_subplot(3,2,2)\nax2 = plot_correlation(df_features_bval,'S3_days_skew','log_b_value','green')\nax3 = fig.add_subplot(3,2,3)\nax3 = plot_correlation(df_features_bval,'S4_days_kurtosis','log_b_value','red')\nax4 = fig.add_subplot(3,2,4)\nax4 = plot_correlation(df_features_bval,'S4_days_sum','log_b_value','black')\nax5 = fig.add_subplot(3,2,5)\nax5 = plot_correlation(df_features_bval,'S4_days_mean','log_b_value','gold')\nax6 = fig.add_subplot(3,2,6)\nax6 = plot_correlation(df_features_bval,'S3_days_sum','log_b_value','darkblue')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### features obtained with PCA","metadata":{}},{"cell_type":"code","source":"corr_pca_features_bval = calcul_sort_save_corr(df_pca_features_bval,'log_b_value',threshold = 0.70)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define figure size\nfig = plt.figure(figsize=(18,3))\nfig.subplots_adjust(hspace=0.4,wspace=0.3)\n\n# figure title\nfig.suptitle('Highest correlations found between PCA axes from extracted features and b-value', fontsize=18,y=1)\n\n# subplot\nax1 = fig.add_subplot(1,2,1)\nax1 = plot_correlation(df_pca_features_bval,'PCA_FEATURE_AXE_3','log_b_value','blue')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.5\"></a>\n## 4.5: Outliers detection:","metadata":{}},{"cell_type":"code","source":"print(df_features_Ener['log_Energy'].sort_values(ascending=True).head(3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positon_outliers = [16]\n\n# remove outlier in df with Energy\ndf_injection_data_scaled_Ener = df_injection_data_scaled_Ener.drop(df_injection_data_scaled_Ener.index[positon_outliers])\ndf_pca_injection_data_scaled_Ener = df_pca_injection_data_scaled_Ener.drop(df_pca_injection_data_scaled_Ener.index[positon_outliers])\ndf_features_Ener = df_features_Ener.drop(df_features_Ener.index[positon_outliers])\ndf_pca_features_Ener = df_pca_features_Ener.drop(df_pca_features_Ener.index[positon_outliers])\n\n# remove outlier in df with b_value\ndf_injection_data_scaled_bval = df_injection_data_scaled_bval.drop(df_injection_data_scaled_bval.index[positon_outliers])\ndf_pca_injection_data_scaled_bval = df_pca_injection_data_scaled_bval.drop(df_pca_injection_data_scaled_bval.index[positon_outliers])\ndf_features_bval = df_features_bval.drop(df_features_bval.index[positon_outliers])\ndf_pca_features_bval = df_pca_features_bval.drop(df_pca_features_bval.index[positon_outliers])\n\n# remove outlier in df with all the data\ndf_all_data = df_all_data.drop(df_all_data.index[positon_outliers])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# re-calculate coefficient of correlations:\ncorr_injection_Ener = calcul_sort_save_corr(df_injection_data_scaled_Ener,'log_Energy',threshold=0.30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_pca_injection_Ener = calcul_sort_save_corr(df_pca_injection_data_scaled_Ener,'log_Energy',threshold=0.28)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_features_Ener = calcul_sort_save_corr(df_features_Ener,'log_Energy',threshold=0.30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_pca_features_Ener = calcul_sort_save_corr(df_pca_features_Ener,'log_Energy',threshold=0.26)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_injection_bval = calcul_sort_save_corr(df_injection_data_scaled_bval,'log_b_value',threshold=0.65)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_pca_injection_bval = calcul_sort_save_corr(df_pca_injection_data_scaled_bval,'log_b_value',threshold=0.65)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_features_bval = calcul_sort_save_corr(df_features_bval,'log_b_value',threshold=0.65)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_pca_features_bval = calcul_sort_save_corr(df_pca_features_bval,'log_b_value',threshold=0.65)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concat df with meaningful features\nmeaningful_features_Energy = pd.concat([corr_injection_Ener,corr_pca_injection_Ener,\n                                        corr_features_Ener,corr_pca_features_Ener],axis=0)\nmeaningful_features_Energy = pd.DataFrame(abs(meaningful_features_Energy['log_Energy']).sort_values(ascending=False)).dropna()\nprint(meaningful_features_Energy.shape)\nmeaningful_features_Energy.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concat df with meaningful features\nmeaningful_features_bval = pd.concat([corr_injection_bval,corr_pca_injection_bval,\n                                        corr_features_bval,corr_pca_features_bval],axis=0)\nmeaningful_features_bval = pd.DataFrame(abs(meaningful_features_bval['log_b_value']).sort_values(ascending=False)).dropna()\nprint(meaningful_features_bval.shape)\nmeaningful_features_bval.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id=\"ML\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>5: MACHINE LEARNING: prediction monthy seismic energy</center></h3>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5.1\"></a>\n## 5.1: Features selection and data split for linear models","metadata":{}},{"cell_type":"code","source":"# select only variables highly correlated with targets\nX_Ener = df_all_data[[c for c in df_all_data.columns if c in meaningful_features_Energy.index]]\nX_Ener.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# select only variables highly correlated with targets\nX_bval = df_all_data[[c for c in df_all_data.columns if c in meaningful_features_bval.index]]\nX_bval.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_Ener = df_all_data['log_Energy'].values\ntarget_bval = df_all_data['log_b_value'].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_Ener, X_test_Ener, y_train_Ener, y_test_Ener = train_test_split(X_Ener, target_Ener,test_size = .3, random_state=0)\nX_train_bval, X_test_bval, y_train_bval, y_test_bval = train_test_split(X_bval, target_bval,test_size = .3, random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.2\"></a>\n## 5.2: functions used for machine learning\n### function to evaluate model prediction","metadata":{}},{"cell_type":"code","source":"def get_best_score(grid):\n    best_score = grid.best_score_\n    print(best_score)    \n    print(grid.best_params_)\n    print(grid.best_estimator_)\n    return best_score\n\ndef model_evaluation(y_test,prediction):\n    r2 = round(metrics.r2_score(y_test, prediction), 2)\n    abs_perc_error = np.mean(np.abs((y_test-prediction)/prediction))\n    mean_abs_err = metrics.mean_absolute_error(y_test, prediction)\n    rmse = np.sqrt(metrics.mean_squared_error(y_test, prediction))\n    print(\"R2 (explained variance):\",r2 )\n    print(\"Mean Absolute Perc Error ((|y-pred|/y)/n):\", abs_perc_error)\n    print(\"Mean Absolute Error (|y-pred|/n):\", \"{:,f}\".format(mean_abs_err))\n    print(\"Root Mean Squared Error (sqrt((y-pred)^2/n)):\", \"{:,f}\".format(rmse))\n    ## residuals\n#     prediction = prediction.reshape(len(prediction),1)\n    residuals = y_test - prediction\n    if abs(max(residuals)) > abs(min(residuals)):\n        max_error = max(residuals)  \n    else:\n        max_error = min(residuals) \n    max_idx = list(residuals).index(max(residuals)) if abs(max(residuals)) > abs(min(residuals)) else list(residuals).index(min(residuals))\n    # max_true = y_test[max_idx]\n    max_pred = prediction[max_idx]\n    print(\"Max Error:\", \"{}\".format(max_error))\n    \n    ## Plot predicted vs true\n    fig, ax = plt.subplots(nrows=1, ncols=2,figsize=(10,5))\n    ax[0].scatter(prediction, y_test, color=\"black\")\n    abline_plot(intercept=0, slope=1, color=\"red\", ax=ax[0])\n    # ax[0].vlines(x=max_pred, ymin=max_true, ymax=max_true-max_error, color='red', linestyle='--', alpha=0.7, label=\"max error\")\n    ax[0].grid(True)\n    ax[0].set(xlabel=\"Predicted\", ylabel=\"True\", title=\"Predicted vs True\")\n    ax[0].legend()\n\n    ## Plot predicted vs residuals\n    ax[1].scatter(prediction, residuals, color=\"red\")\n    ax[1].vlines(x=max_pred, ymin=0, ymax=max_error, color='black', linestyle='--', alpha=0.7, label=\"max error\")\n    ax[1].grid(True)\n    ax[1].set(xlabel=\"Predicted\", ylabel=\"Residuals\", title=\"Predicted vs Residuals\")\n    ax[1].hlines(y=0, xmin=np.min(prediction), xmax=np.max(prediction))\n    ax[1].legend()\n    plt.show()\n\n    print('The model explains {}% of the variance of the target variable.'.format(r2*100))\n    print('On average, predictions have an error of {:,.2f}, or theyre wrong by {:,.2f}%.'.format(mean_abs_err,(abs_perc_error)*100)) \n#     print('the average difference between the predicted value and the actual value is {:,.2f}%: '.format(abs_perc_error*100))\n    print('The biggest error on the test set was over {:,.2f}.'.format(max_error))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Functions for plotting features' importance","metadata":{}},{"cell_type":"code","source":"def plot_nb_feature_vs_score(df):\n    # PLOT RESULT:\n    df.plot('number_feat', 'best_score')\n    # Returns index of minimun best_score\n    index = df[['best_score']].idxmax() \n    # get the number of features used to have the best score \n    print(df['number_feat'][index])\n    \ndef plot_feature_importance(importance,n,names,model_type):\n\n    #Create arrays from feature importance and feature names\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n\n    #Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names[:n],'feature_importance':feature_importance[:n]}\n    fi_df = pd.DataFrame(data)\n\n    #Sort the DataFrame in order decreasing feature importance\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n\n    #Define size of bar plot\n    plt.figure(figsize=(15,10))\n    #Plot Searborn bar chart\n    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n    #Add chart labels\n    plt.title(model_type + 'FEATURE IMPORTANCE')\n    plt.xlabel('FEATURE IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Function for features selection","metadata":{}},{"cell_type":"code","source":"def select_model(model_ini,thres, X_train,X_test):   \n    selection = SelectFromModel(model_ini, threshold=thres, prefit=True)\n    n_features = selection.transform(X_train).shape[1]\n    selected_vars = list(X_train.columns[selection.get_support()])\n    X_train_selected = X_train[selected_vars]\n    X_test_selected = X_test[selected_vars]\n    return X_train_selected,X_test_selected","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Function to perform GridsearchCV","metadata":{}},{"cell_type":"code","source":"def perform_grid_search(model,random_grid,X_train, y_train,thres):\n    random = RandomizedSearchCV(estimator = model, param_distributions = random_grid, n_iter = 300, cv = 4, \n                               verbose=2, random_state=12,scoring ='explained_variance')\n    # Fit the random search model\n    random.fit(X_train, y_train)\n    # print output\n    print('-'*60)\n    print('Results from Grid Search with threshold = {}'.format(thres))\n    print(\"Best score:\",random.best_score_)\n    print(\"with the following parameters :\\n\",random.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.3\"></a>\n## 5.3: Linear model\n### 5.3.1: Feature selection with Lasso","metadata":{}},{"cell_type":"code","source":"lasso_score = []\nnumber_feature = []\nfor i in range (1,X_Ener.shape[1]):\n    # select diferent \n    columns = meaningful_features_Energy.index[:i]\n    X = X_Ener[columns].values\n    y = target_Ener\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n    \n    lasso = Lasso(normalize=True)\n    parameters = {'alpha': [1e-8,1e-5,1e-4, 1e-3,1e-2,0.5,1,2],\n              'tol':[1e-6,1e-5,1e-4,1e-3,1e-2]}\n    grid_lasso = GridSearchCV(lasso, parameters, cv=12, verbose=0, scoring = 'explained_variance')\n    grid_lasso.fit(X_train, y_train)\n    \n    sc_lasso = get_best_score(grid_lasso)\n    lasso_score.append(sc_lasso)\n    number_feature.append(i)\n\nresult_lasso =  pd.DataFrame(zip(number_feature,lasso_score),columns = ['number_feat', 'best_score'])","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_nb_feature_vs_score(result_lasso)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Run gridsearch with right number of features","metadata":{}},{"cell_type":"code","source":"# select columns that give best results\ncolumns = meaningful_features_Energy.index[:22]\nX = X_Ener[columns].values\ny = target_Ener\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n\n# Lasso regression model and gridsearch\ngrid_lasso = GridSearchCV(lasso, parameters, cv=12, verbose=1, scoring = 'explained_variance')\ngrid_lasso.fit(X_train, y_train)\n\nsc_lasso = get_best_score(grid_lasso)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Run best fit Lasso","metadata":{}},{"cell_type":"code","source":"## best fit: \nlasso = Lasso(alpha= 0.01, normalize=True,tol = 1e-05)\n## fit the model. \nlasso.fit(X_train, y_train)\n## Predicting the target value based on \"Test_x\"\nprediction = lasso.predict(X_test)\n\n#add score to list\nr2_Lasso = metrics.r2_score(y_test, prediction)\nmse_Lasso = metrics.mean_squared_error(y_test, prediction)\n\n# evaluation model\nmodel_evaluation(y_test,prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.3.2\"></a>\n### 5.3.2: Linear regression\n#### GridSearchCV() with number features = 22","metadata":{}},{"cell_type":"code","source":"# select columns that give best results\ncolumns = meaningful_features_Energy.index[:22]\nX = X_Ener[columns].values\ny = target_Ener\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n\n# linear regression model and gridsearch\nlinreg = LinearRegression()\nparameters = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}\ngrid_linear = GridSearchCV(linreg, parameters, cv=12, verbose=1 , scoring ='explained_variance')\ngrid_linear.fit(X_train, y_train)\n\nsc_linear = get_best_score(grid_linear)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Best result with linear model","metadata":{}},{"cell_type":"code","source":"model_linreg = LinearRegression(copy_X= True, fit_intercept= True, normalize= False)\nmodel_linreg.fit(X_train,y_train)\nprediction = model_linreg.predict(X_test)\n\n#add score to list\nr2_linreg = metrics.r2_score(y_test, prediction)\nmse_linreg = metrics.mean_squared_error(y_test, prediction)\n\n# evaluation model\nmodel_evaluation(y_test,prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.3.3\"></a>\n### 5.3.3: Ridge model\n#### GridSearchCV() with number features = 22","metadata":{}},{"cell_type":"code","source":"# select columns that give best results\ncolumns = meaningful_features_Energy.index[:22]\nX = X_Ener[columns].values\ny = target_Ener\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n\n# linear regression model and gridsearch\nridge = Ridge(normalize=True)\nparameters = {'alpha':[1e-6,1e-5,1e-4,1e-3,1e-2,0.1,0.5,1], \n              'tol':[1e-9,1e-7,1e-6,1e-5,1e-4]}\ngrid_ridge = GridSearchCV(ridge, parameters, cv=12, verbose=1, scoring = 'explained_variance')\ngrid_ridge.fit(X_train, y_train)\n\nsc_ridge = get_best_score(grid_ridge)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Best result with Ridge","metadata":{}},{"cell_type":"code","source":"## best fit: \nridge_bf = Ridge(alpha= 1, normalize=True,tol = 1e-09)\n## fit the model. \nridge_bf.fit(X_train, y_train)\n## Predicting the target value based on \"Test_x\"\nprediction = ridge_bf.predict(X_test)\n\n#add score to list\nr2_ridge = metrics.r2_score(y_test, prediction)\nmse_ridge = metrics.mean_squared_error(y_test, prediction)\n\n# evaluation model\nmodel_evaluation(y_test,prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.3.4\"></a>\n### 5.3.4: ElasticNet regression\n#### GridSearchCV() with number feature = 22","metadata":{}},{"cell_type":"code","source":"# select columns that give best results\ncolumns = meaningful_features_Energy.index[:22]\nX = X_Ener[columns].values\ny = target_Ener\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n\n# Lasso regression model and gridsearch\nelastic_reg = ElasticNet()\nparameters = {\n    'alpha': [1e-6,1e-5, 1e-4, 5e-3,1e-3,5e-2, 1e-2, 5e-1, 1e-1,1.0],\n    'l1_ratio': [1e-6,1e-5, 1e-4,5e-3, 1e-3,5e-2, 1e-2, 5e-1,1e-1,1.0],\n    'tol': [1e-6,1e-5, 1e-4,1e-3]}\ngrid_elas = GridSearchCV(elastic_reg, parameters, cv=12, verbose=1, scoring = 'explained_variance')\ngrid_elas.fit(X_train, y_train)\n\nsc_elas = get_best_score(grid_elas)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Best result with Elastic","metadata":{}},{"cell_type":"code","source":"## best fit: \nelastic = ElasticNet(alpha= 1.0,l1_ratio= 0.005,tol=1e-05)\n## fit the model. \nelastic.fit(X_train, y_train)\n## Predicting the target value based on \"Test_x\"\nprediction = elastic.predict(X_test)\n\n#add score to list\nr2_elas = metrics.r2_score(y_test, prediction)\nmse_elas = metrics.mean_squared_error(y_test, prediction)\n\n# evaluation model\nmodel_evaluation(y_test,prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.4\"></a>\n## 5.4: Decision tree methods ","metadata":{}},{"cell_type":"code","source":"# data selection for decision tree methods\ndf_all_data2 = df_all_data.drop(columns=['log_Energy','log_b_value'], axis =1)\nX_train_Ener, X_test_Ener, y_train_Ener, y_test_Ener = train_test_split(df_all_data2, target_Ener,test_size = .3, random_state=0)\nX_train_bval, X_test_bval, y_train_bval, y_test_bval = train_test_split(df_all_data2, target_Ener,test_size = .3, random_state=0)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.4.1\"></a>\n### 5.4.1: Random Forest Regressor\n- First, we evaulate the importance of features.\n- Then, we create several models trained on different input features and we perform a RandomizedSearchCV on each model. Each model is made of 100 trees to speed the calculation.\n- Finally, we select the number of features that gave the highest score, we initiate a new model and perform a gridsearch \n\n#### feature importance","metadata":{}},{"cell_type":"code","source":"# initial random forest regressor model, fit it \nmodel_rf_ini = RandomForestRegressor(n_estimators= 100,random_state = 0)\nmodel_rf_ini.fit(X_train_Ener, y_train_Ener)\n# extract and plot important features\nplot_feature_importance(model_rf_ini.feature_importances_,60,X_train_Ener.columns,'Random Forest Regressor: ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### feature selection and gridsearch","metadata":{}},{"cell_type":"markdown","source":"Here, we define the grid for random searh that going to be used for each model.","metadata":{}},{"cell_type":"code","source":"# Create the random grid\nrandom_grid_rf = {\n                # measure quality of the split\n                'criterion': ['mse', 'mae'],\n                # Number of features to consider at every split \n               'max_features': ['auto', 'sqrt','log2'],\n                # Maximum number of levels in tree\n               'max_depth': [10,50,100,200,None],\n                # Minimum number of samples required to split a node\n               'min_samples_split': range(2,10,1),\n                # Minimum number of samples required at each leaf node\n               'min_samples_leaf': range(1,10,1),\n                # Method of selecting samples for training each tree\n               'bootstrap': [True, False],\n                'n_estimators' : [100],\n                'random_state': [0]}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # do first random search with threshold = 0.0002\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.0002, X_train_Ener, X_test_Ener)\n# # initiat model\n# model_rf = RandomForestRegressor(random_state = 0)\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_Ener,0.0002)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.0002\n\nBest score: **0.21704545681030946**\n\nwith the following parameters :\n\n{'random_state': 0, 'n_estimators': 100, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 200, 'criterion': 'mse', 'bootstrap': True}","metadata":{}},{"cell_type":"code","source":"# # do first random search with threshold = 0.0004\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.0004, X_train_Ener, X_test_Ener)\n# # initiat model\n# model_rf = RandomForestRegressor(random_state = 0)\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_Ener,0.0004)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.0004\n\nBest score: **0.2181101677673559**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 200, 'criterion': 'mae', 'bootstrap': True}","metadata":{}},{"cell_type":"code","source":"# # do first random search with threshold = 0.0006\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.0006, X_train_Ener, X_test_Ener)\n# # initiat model\n# model_rf = RandomForestRegressor(random_state = 0)\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_Ener,0.0006)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.0006\n\nBest score: **0.2249491184242004**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 100, 'criterion': 'mae', 'bootstrap': True}\n","metadata":{}},{"cell_type":"code","source":"# # do first random search with threshold = 0.0008\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.0008, X_train_Ener, X_test_Ener)\n# # initiat model\n# model_rf = RandomForestRegressor()\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_Ener,0.0008)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.0008\n\nBest score: **0.2327851268229801**\n\nwith the following parameters :\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 9, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'max_depth': None, 'criterion': 'mse', 'bootstrap': True}","metadata":{}},{"cell_type":"code","source":"# # do first random search with threshold = 0.0009\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.0009, X_train_Ener, X_test_Ener)\n# # initiat model\n# model_rf = RandomForestRegressor()\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_Ener,0.0009)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.0009\n\nBest score: **0.23715507366773927**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 100, 'criterion': 'mse', 'bootstrap': True}","metadata":{}},{"cell_type":"code","source":"# # # do first random search with threshold = 0.001\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.001, X_train_Ener, X_test_Ener)\n# # # initiat model\n# model_rf = RandomForestRegressor()\n# # # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_Ener,0.001)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.001\n\nBest score: **0.2386073321504737**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 100, 'criterion': 'mae', 'bootstrap': True}","metadata":{}},{"cell_type":"code","source":"# # do first random search with threshold = 0.0011\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.0011, X_train_Ener, X_test_Ener)\n# # initiat model\n# model_rf = RandomForestRegressor()\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_Ener,0.0011)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.0011\n\nBest score: **0.25370632639103463**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 100, 'criterion': 'mse', 'bootstrap': True}","metadata":{}},{"cell_type":"code","source":"# # do first random search with threshold = 0.0012\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.0012, X_train_Ener, X_test_Ener)\n# # initiat model\n# model_rf = RandomForestRegressor()\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_Ener,0.0012)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.0012\n\nBest score: **0.254835044417125**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 50, 'criterion': 'mae', 'bootstrap': True}","metadata":{}},{"cell_type":"code","source":"# # do first random search with threshold = 0.0013\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.0013, X_train_Ener, X_test_Ener)\n# # initiat model\n# model_rf = RandomForestRegressor()\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_Ener,0.0013)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.0013\n\nBest score: **0.26921830705419325**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 50, 'criterion': 'mae', 'bootstrap': True}","metadata":{}},{"cell_type":"code","source":"# # do first random search with threshold = 0.0014\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.0014, X_train_Ener, X_test_Ener)\n# # initiat model\n# model_rf = RandomForestRegressor()\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_Ener,0.0014)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.0014\n\nBest score: **0.22909710132256744**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 9, 'min_samples_leaf': 8, 'max_features': 'log2', 'max_depth': 10, 'criterion': 'mae', 'bootstrap': False}","metadata":{}},{"cell_type":"code","source":"# # do first random search with threshold = 0.0015\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.0015, X_train_Ener, X_test_Ener)\n# # initiat model\n# model_rf = RandomForestRegressor()\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_Ener,0.0015)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.0015\n\nBest score: **0.25165823131968784**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 4, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'max_depth': 200, 'criterion': 'mae', 'bootstrap': True}","metadata":{}},{"cell_type":"code","source":"# # do first random search with threshold = 0.00175\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.00175, X_train_Ener, X_test_Ener)\n# # initiat model\n# model_rf = RandomForestRegressor()\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_Ener,0.00175)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.00175\n\nBest score: **0.24612827184727618**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 3, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'max_depth': 200, 'criterion': 'mse', 'bootstrap': True}","metadata":{}},{"cell_type":"markdown","source":"#### GridSearchCV with threshold = 0.0013 (best score)","metadata":{}},{"cell_type":"code","source":"# # do first random search with threshold = 0.0013\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.0013, X_train_Ener, X_test_Ener)\n\n# model_rf = RandomForestRegressor()\n\n# param_grid = {\n#     'bootstrap': [True],\n#     'criterion': ['mae'],\n#     'max_depth': range(45,55,1),\n#     'max_features': ['log2'],\n#     'min_samples_leaf': [1,2,3,4,5],\n#     'min_samples_split': [5,6,7,8,9],\n#     'n_estimators': [100],\n#     'random_state': [0]\n# }\n\n# grid_search = GridSearchCV(estimator = model_rf, param_grid = param_grid, cv = 12, verbose = 2,scoring ='explained_variance')\n\n# # Fit the grid search to the data\n# grid_search.fit(X_train_selected, y_train_Ener)\n# grid_search.best_params_","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Result gridsearch:**\n{'bootstrap': True,\n 'criterion': 'mae',\n 'max_depth': 45,\n 'max_features': 'log2',\n 'min_samples_leaf': 2,\n 'min_samples_split': 6,\n 'n_estimators': 100,\n 'random_state': 0}","metadata":{}},{"cell_type":"code","source":"# do first random search with threshold = 0.02\nX_train_selected, X_test_selected =  select_model(model_rf_ini,0.0013, X_train_Ener, X_test_Ener)\n\nmodel_rf = RandomForestRegressor(bootstrap ='True',criterion='mae',max_depth = 45,max_features = 'log2',\n                                min_samples_leaf = 2, min_samples_split = 6, n_estimators = 100,random_state =  0)\nmodel_rf.fit(X_train_selected,y_train_Ener)\nprediction = model_rf.predict(X_test_selected)\n\n#add score to list\nr2_rf = metrics.r2_score(y_test_Ener, prediction)\nmse_rf = metrics.mean_squared_error(y_test_Ener, prediction)\n\n# evaluation model\nmodel_evaluation(y_test_Ener,prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model overfit the data. To reduce over fitting we can try to tune these parameters:\n\n- `n_estimators`: The more trees, the less likely the algorithm is to overfit. \n- `max_features`: This defines how many features each tree is randomly assigned, it could be lowered \n- `max_depth`: lowering this parameter reduce the complexity of the learned models, and so the over fitting risk. \n- `min_samples_leaf`: increase to obtain similar effect as the max_depth parameter.\n","metadata":{}},{"cell_type":"code","source":"# do first random search with threshold = 0.02\nX_train_selected, X_test_selected =  select_model(model_rf_ini,0.0013, X_train_Ener, X_test_Ener)\n\nmodel_rf = RandomForestRegressor(bootstrap ='True',criterion='mae',max_depth = 45,max_features ='log2',\n                                min_samples_leaf = 2, min_samples_split = 6, n_estimators = 3000,random_state =  0)\nmodel_rf.fit(X_train_selected,y_train_Ener)\nprediction = model_rf.predict(X_test_selected)\n\n#add score to list\nr2_rf = metrics.r2_score(y_test_Ener, prediction)\nmse_rf = metrics.mean_squared_error(y_test_Ener, prediction)\n\nprint(r2_rf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.4.2\"></a>\n### 5.4.2: xgboost regression\nAs before we:\n- First, evaulate the importance of features.\n- Then, create several models trained on different input features and we perform a RandomizedSearchCV on each model.\n- Finally, select the number of features that gave the highest score, we initiate a new model and perform a gridsearch \n\n#### feature importance","metadata":{}},{"cell_type":"code","source":"# Create the DMatrix: \ntrain_dmatrix = xgb.DMatrix(data=X_train_Ener, label=y_train_Ener)\n\n# Instantiate the initial regressor model: model_gbm_ini\nmodel_gbm_ini = xgb.XGBRegressor(objective='reg:squarederror',n_estimators = 100,eval_metric='rmse',seed=42)\n\n# Fit andomized_mse to the data\nmodel_gbm_ini.fit(X_train_Ener,y_train_Ener)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_feature_importance(model_gbm_ini.feature_importances_,60,X_train_Ener.columns,'xgboost regression: ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initial the parameters for random search\nrandom_grid_gbm = {\n    'colsample_bytree': [0.6,0.7,0.8,0.9,1],\n    'max_depth': range(1,11,1),\n    'eta' : [0.01,0.05,0.1, 0.15, 0.2],\n    'alpha': [0,0.3,0.6,0.9,1],\n    'min_child_weight': [1],\n    'scale_pos_weight' : [1],\n    'n_estimators' :  [100],\n     'seed' : [42]\n    \n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def perform_grid_search_xgb(model,random_grid,X_train, y_train,thres):\n    # Create the DMatrix: \n    train_dmatrix = xgb.DMatrix(data=X_train, label=y_train)\n    random = RandomizedSearchCV(estimator = model, param_distributions = random_grid, n_iter = 300, cv = 4, \n                               verbose=2, random_state=12,scoring ='explained_variance')\n    # Fit the random search model\n    random.fit(X_train, y_train)\n    # print output\n    print('-'*60)\n    print('Results from Grid Search with threshold = {}'.format(thres))\n    print(\"Best score:\",random.best_score_)\n    print(\"with the following parameters :\\n\",random.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # do first random search with threshold = 0.001\n# X_train_selected, X_test_selected =  select_model(model_gbm_ini,0.001, X_train_Ener, X_test_Ener)\n\n# # Instantiate the regressor: gbm\n# model_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# # perform gridsearchCH\n# perform_grid_search_xgb(model_gbm,random_grid_gbm,X_train_selected, y_train_Ener,0.001)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.001\n\nBest score: **0.23901575756703272**\n\nwith the following parameters :\n {'seed': 42, 'scale_pos_weight': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 2, 'eta': 0.05, 'colsample_bytree': 0.6, 'alpha': 0.6}","metadata":{}},{"cell_type":"code","source":"# # do first random search with threshold = 0.002\n# X_train_selected, X_test_selected =  select_model(model_gbm_ini,0.002, X_train_Ener, X_test_Ener)\n\n# # Instantiate the regressor: gbm\n# model_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# # perform gridsearchCH\n# perform_grid_search_xgb(model_gbm,random_grid_gbm,X_train_selected, y_train_Ener,0.002)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.002\n\nBest score:**0.3211045305680925**\n\nwith the following parameters :\n\n {'seed': 42, 'scale_pos_weight': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 6, 'eta': 0.05, 'colsample_bytree': 0.7, 'alpha': 0.6}","metadata":{}},{"cell_type":"code","source":"# # do first random search with threshold = 0.003\n# X_train_selected, X_test_selected =  select_model(model_gbm_ini,0.003, X_train_Ener, X_test_Ener)\n\n# # Instantiate the regressor: gbm\n# model_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# # perform gridsearchCH\n# perform_grid_search_xgb(model_gbm,random_grid_gbm,X_train_selected, y_train_Ener,0.003)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.003\n\nBest score: **0.33907105956175704**\n\nwith the following parameters :\n\n {'seed': 42, 'scale_pos_weight': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 2, 'eta': 0.15, 'colsample_bytree': 0.8, 'alpha': 0.6}","metadata":{}},{"cell_type":"code","source":"# # do first random search with threshold = 0.004\n# X_train_selected, X_test_selected =  select_model(model_gbm_ini,0.004, X_train_Ener, X_test_Ener)\n\n# # Instantiate the regressor: gbm\n# model_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# # perform gridsearchCH\n# perform_grid_search_xgb(model_gbm,random_grid_gbm,X_train_selected, y_train_Ener,0.004)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.004\n\nBest score: **0.3554615103664501**\n\nwith the following parameters :\n\n {'seed': 42, 'scale_pos_weight': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 8, 'eta': 0.05, 'colsample_bytree': 0.9, 'alpha': 0.3}","metadata":{}},{"cell_type":"code","source":"# # do first random search with threshold = 0.005\n# X_train_selected, X_test_selected =  select_model(model_gbm_ini,0.005, X_train_Ener, X_test_Ener)\n\n# # Instantiate the regressor: gbm\n# model_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# # perform gridsearchCH\n# perform_grid_search_xgb(model_gbm,random_grid_gbm,X_train_selected, y_train_Ener,0.005)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.005\n\nBest score: **0.3678609550293852**\n\nwith the following parameters :\n\n {'seed': 42, 'scale_pos_weight': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 2, 'eta': 0.2, 'colsample_bytree': 0.6, 'alpha': 0.3}","metadata":{}},{"cell_type":"code","source":"# # do first random search with threshold = 0.006\n# X_train_selected, X_test_selected =  select_model(model_gbm_ini,0.006, X_train_Ener, X_test_Ener)\n\n# # Instantiate the regressor: gbm\n# model_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# # perform gridsearchCH\n# perform_grid_search_xgb(model_gbm,random_grid_gbm,X_train_selected, y_train_Ener,0.006)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.006\n\nBest score: **0.3678609550293852**\n\nwith the following parameters :\n\n {'seed': 42, 'scale_pos_weight': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 2, 'eta': 0.2, 'colsample_bytree': 0.6, 'alpha': 0.3}","metadata":{}},{"cell_type":"code","source":"# # do first random search with threshold = 0.007\n# X_train_selected, X_test_selected =  select_model(model_gbm_ini,0.007, X_train_Ener, X_test_Ener)\n\n# # Instantiate the regressor: gbm\n# model_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# # perform gridsearchCH\n# perform_grid_search_xgb(model_gbm,random_grid_gbm,X_train_selected, y_train_Ener,0.007)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.007\n\nBest score: **0.3717616957479021**\n\nwith the following parameters :\n\n {'seed': 42, 'scale_pos_weight': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 2, 'eta': 0.15, 'colsample_bytree': 0.7, 'alpha': 0.6}","metadata":{}},{"cell_type":"code","source":"# # do first random search with threshold = 0.008\n# X_train_selected, X_test_selected =  select_model(model_gbm_ini,0.008, X_train_Ener, X_test_Ener)\n\n# # Instantiate the regressor: gbm\n# model_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# # perform gridsearchCH\n# perform_grid_search_xgb(model_gbm,random_grid_gbm,X_train_selected, y_train_Ener,0.008)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.008\n\nBest score: **0.3577037794408484**\n\nwith the following parameters :\n\n {'seed': 42, 'scale_pos_weight': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 2, 'eta': 0.1, 'colsample_bytree': 0.7, 'alpha': 0}","metadata":{}},{"cell_type":"code","source":"# # do first random search with threshold = 0.009\n# X_train_selected, X_test_selected =  select_model(model_gbm_ini,0.009, X_train_Ener, X_test_Ener)\n\n# # Instantiate the regressor: gbm\n# model_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# # perform gridsearchCH\n# perform_grid_search_xgb(model_gbm,random_grid_gbm,X_train_selected, y_train_Ener,0.009)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.009\n\nBest score: **0.3547285700797372**\n\nwith the following parameters :\n\n {'seed': 42, 'scale_pos_weight': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 2, 'eta': 0.05, 'colsample_bytree': 0.8, 'alpha': 0}","metadata":{}},{"cell_type":"markdown","source":"#### Run gridsearchCV with threshold = 0.007","metadata":{}},{"cell_type":"code","source":"# #### do first random search with threshold = 0.009\n# X_train_selected, X_test_selected =  select_model(model_gbm_ini,0.007, X_train_Ener, X_test_Ener)\n\n# # Instantiate the regressor: gbm\n# model_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# gbm_param_grid = {\n#     'colsample_bytree': [0.65,0.67,0.69, 0.7,0.72,0.74],\n#     'max_depth': range(1,4,1),   \n#     'eta' : [0.13,0.14,0.15, 0.16, 0.17],\n#     'alpha': [0.4,0.5,0.6,0.7,0.8],\n#     'min_child_weight': [1,1.5],\n#     'scale_pos_weight' : [1,1.5],\n#     'n_estimators' :  [100],\n#      'seed' : [42],\n    \n# }\n\n# grid_search = GridSearchCV(estimator = model_gbm, param_grid = gbm_param_grid, cv = 4, verbose = 2,scoring ='explained_variance')\n\n# # Fit the grid search to the data\n# grid_search.fit(X_train_selected, y_train_Ener)\n# grid_search.best_params_\n# grid_search.best_score_","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"best score : **0.40611976681211237**\n\n{'alpha': 0.4,\n 'colsample_bytree': 0.67,\n 'eta': 0.14,\n 'max_depth': 2,\n 'min_child_weight': 1.5,\n 'n_estimators': 100,\n 'scale_pos_weight': 1,\n 'seed': 42}","metadata":{}},{"cell_type":"markdown","source":"### run best gbm","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\n\n# do first random search with threshold = 0.035\nX_train_selected, X_test_selected =  select_model(model_gbm_ini,0.007, X_train_Ener, X_test_Ener)\n\n# Create the DMatrix: \ntrain_dmatrix = xgb.DMatrix(data=X_train_selected, label=y_train_Ener)\n\nxgb  = xgb.XGBRegressor(objective='reg:squarederror',n_estimators = 100,min_child_weight= 1.5,\n                        max_depth = 2, eta = 0.14,colsample_bytree = 0.67,\n                        alpha = 0.4,seed=42,scale_pos_weight=1)\n                        \n# Fit the regressor to the training set\nxgb.fit(X_train_selected,y_train_Ener)\n\n## Predicting the target value\nprediction = xgb.predict(X_test_selected)\n\n# evaluation model\nmodel_evaluation(y_test_Ener,prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model has a better performance on the training set than on the testing set.\nso we will try to lower:\n\n- `colsample_bytree` (the ratio of features used),\n- `subsample` (the ratio of the training instances used),\n- `eta` (the learning rate of our GBM (i.e. how much we update our prediction with each successive tree).\n    \nand to increase:\n\n- `gamma` (the minimum loss reduction required to make a further split),\n- `min_child_weight` (the minimum sum of instance weight needed in a leaf)","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\n# lower 'colsample_bytree': 0.67 to 0.07\nxgb  = xgb.XGBRegressor(objective='reg:squarederror',n_estimators = 100,min_child_weight= 1.5,\n                        max_depth = 2, eta = 0.14,colsample_bytree = 0.07,\n                        alpha = 0.4,seed=42,scale_pos_weight=1)\n                        \n# Fit the regressor to the training set\nxgb.fit(X_train_selected,y_train_Ener)\n\n## Predicting the target value\nprediction = xgb.predict(X_test_selected)\n\n# evaluation model\nr2 = round(metrics.r2_score(y_test_Ener, prediction), 2)\nprint(r2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"R2 evolves from 0.8 to 0.18","metadata":{}},{"cell_type":"code","source":"# decrease subsample from 1 to 0.5\nimport xgboost as xgb\n# lower 'colsample_bytree': 0.67 to 0.07\nxgb  = xgb.XGBRegressor(objective='reg:squarederror',n_estimators = 100,min_child_weight= 1.5,\n                        max_depth = 2, eta = 0.14,colsample_bytree = 0.07,subsample=0.5,\n                        alpha = 0.4,seed=42,scale_pos_weight=1)\n                        \n# Fit the regressor to the training set\nxgb.fit(X_train_selected,y_train_Ener)\n\n## Predicting the target value\nprediction = xgb.predict(X_test_selected)\n\n# evaluation model\nr2 = round(metrics.r2_score(y_test_Ener, prediction), 2)\nprint(r2)\n\n# evaluation model\nmodel_evaluation(y_test_Ener,prediction)\n\n#add score to list\nr2_xgb = metrics.r2_score(y_test, prediction)\nmse_xgb = metrics.mean_squared_error(y_test, prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Other variables didn't help","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5.5\"></a>\n## 5.5: Summary -- predition monthly seismic energy released --","metadata":{}},{"cell_type":"code","source":"# create list with the model score\nlist_score_r2 = [r2_linreg,r2_ridge,r2_Lasso,r2_elas,r2_rf,r2_xgb]\nlist_score_mse = [mse_linreg,mse_ridge,mse_Lasso,mse_elas,mse_rf,mse_xgb]\n# create list with model name\nlist_regressors = ['linear','Ridge','Lasso','ElaNet','RF','xgboost']\n\n# create dictionnary and dataframe\ndic_score = {'model': list_regressors,\n            'score_R2':list_score_r2,\n            'score_mse':list_score_mse,}\n\ndic_score = pd.DataFrame(dic_score)\ndic_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the predictions for each model\nfig, axes = plt.subplots(2,figsize=(15,5))\nax = plt.subplot(1,2,1)\nax = sns.pointplot(x = \"model\", y = \"score_R2\", data = dic_score) \nax.set_ylabel('Score (R2)', size=20, labelpad=12.5)\nax.set_xlabel('Model', size=20, labelpad=12.5)\nax.tick_params(labelsize=14)\n\n# add annotations one by one with a loop\nfor ind in dic_score.index: \n     ax.text(ind,dic_score['score_R2'][ind]+0.002,'{:.5f}'.format(dic_score['score_R2'][ind]),\n             horizontalalignment='left', size='medium', color='black', weight='semibold',fontsize=12)\n        \nax2 = plt.subplot(1,2,2)\nax2 = sns.pointplot(x = \"model\", y = \"score_mse\", data = dic_score) \nax2.set_ylabel('Score (MSE)', size=20, labelpad=12.5)\nax2.set_xlabel('Model', size=20, labelpad=12.5)\nax2.tick_params(labelsize=14)\n\n# add annotations one by one with a loop\nfor ind in dic_score.index: \n     ax2.text(ind,dic_score['score_mse'][ind]+0.002,'{:.5f}'.format(dic_score['score_mse'][ind]),\n             horizontalalignment='left', size='medium', color='black', weight='semibold',fontsize=12)\n        \nplt.title(\"Models' scores\", size=20)\nplt.savefig('Models scores Energy2.png')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### visualize original data vs prediction with RIDGE","metadata":{}},{"cell_type":"code","source":"# select columns that give best results\ncolumns = meaningful_features_Energy.index[:22]\nX = X_Ener[columns].values\ny = target_Ener\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n\n## best fit: \nridge_bf = Ridge(alpha= 1, normalize=True,tol = 1e-09)\n## fit the model. \nridge_bf.fit(X_train, y_train)\n## Predicting the target value based on \"Test_x\"\nlog_prediction = ridge_bf.predict(X_test)\nprediction =  np.exp(log_prediction)\noriginal_data = np.exp(y_test)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the predictions for each model\nfig, axes = plt.subplots(1,figsize=(10,5))\n      \nplt.plot(original_data,'-ob',label='original data')\nplt.plot(prediction,'-dr',label='prediction')\nplt.ylabel('monthly seimic energy',size=16)\nplt.legend()\n\nplt.title(\"Original data vs predicted\", size=20)\n\nplt.savefig('Energy -  original vs predicted.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id=\"ML2\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>6: MACHINE LEARNING: prediction monthy b-value</center></h3>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"6.1\"></a>\n## 6.1: Features selection and data split for linear models","metadata":{}},{"cell_type":"code","source":"# select only variables highly correlated with targets\nX_bval = df_all_data[[c for c in df_all_data.columns if c in meaningful_features_bval.index]]\n# select target: log_b_values\ntarget_bval = df_all_data['log_b_value'].values\n# split the data\nX_train_bval, X_test_bval, y_train_bval, y_test_bval = train_test_split(X_bval, target_bval,test_size = .3, random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.2\"></a>\n## 6.2: Linear model\n### 6.2.1: Feature selection with Lasso","metadata":{}},{"cell_type":"code","source":"lasso_score = []\nnumber_feature = []\nfor i in range (1,X_bval.shape[1]):\n    # select diferent \n    columns = meaningful_features_bval.index[:i]\n    X = X_bval[columns].values\n    y = target_bval\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n    \n    lasso = Lasso(normalize=True)\n    parameters = {'alpha': [1e-8,1e-5,1e-4, 1e-3,1e-2,0.5,1,2],\n              'tol':[1e-6,1e-5,1e-4,1e-3,1e-2]}\n    grid_lasso = GridSearchCV(lasso, parameters, cv=12, verbose=0, scoring = 'explained_variance')\n    grid_lasso.fit(X_train, y_train)\n    \n    sc_lasso = get_best_score(grid_lasso)\n    lasso_score.append(sc_lasso)\n    number_feature.append(i)\n\nresult_lasso =  pd.DataFrame(zip(number_feature,lasso_score),columns = ['number_feat', 'best_score'])","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_nb_feature_vs_score(result_lasso)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Run gridsearch with right number of features","metadata":{}},{"cell_type":"code","source":"# # select columns that give best results\n# columns = meaningful_features_bval.index[:62]\n# X = X_bval[columns].values\n# y = target_bval\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n\n# # Lasso regression model and gridsearch\n# grid_lasso = GridSearchCV(lasso, parameters, cv=12, verbose=1, scoring = 'explained_variance')\n# grid_lasso.fit(X_train, y_train)\n\n# sc_lasso = get_best_score(grid_lasso)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Run best fit Lasso","metadata":{}},{"cell_type":"code","source":"## best fit: \nlasso = Lasso(alpha= 0.0001, normalize=True,tol = 0.01)\n## fit the model. \nlasso.fit(X_train, y_train)\n## Predicting the target value based on \"Test_x\"\nprediction = lasso.predict(X_test)\n\n#add score to list\nr2_Lasso = metrics.r2_score(y_test, prediction)\nmse_Lasso = metrics.mean_squared_error(y_test, prediction)\n\n# evaluation model\nmodel_evaluation(y_test,prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.2.2\"></a>\n### 6.2.2: Linear regression\n#### GridSearchCV() with number features = 22","metadata":{}},{"cell_type":"code","source":"# select columns that give best results\ncolumns = meaningful_features_bval.index[:62]\nX = X_bval[columns].values\ny = target_bval\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n\n# linear regression model and gridsearch\nlinreg = LinearRegression()\nparameters = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}\ngrid_linear = GridSearchCV(linreg, parameters, cv=12, verbose=1 , scoring ='explained_variance')\ngrid_linear.fit(X_train, y_train)\n\nsc_linear = get_best_score(grid_linear)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Best result with linear model","metadata":{}},{"cell_type":"code","source":"model_linreg = LinearRegression(copy_X= True, fit_intercept= True, normalize= False)\nmodel_linreg.fit(X_train,y_train)\nprediction = model_linreg.predict(X_test)\n\n#add score to list\nr2_linreg = metrics.r2_score(y_test, prediction)\nmse_linreg = metrics.mean_squared_error(y_test, prediction)\n\n# evaluation model\nmodel_evaluation(y_test,prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.2.3\"></a>\n### 6.2.3: Ridge model\n#### GridSearchCV() with number features = 62","metadata":{}},{"cell_type":"code","source":"# select columns that give best results\ncolumns = meaningful_features_bval.index[:62]\nX = X_bval[columns].values\ny = target_bval\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n\n# linear regression model and gridsearch\nridge = Ridge(normalize=True)\nparameters = {'alpha':[1e-6,1e-5,1e-4,1e-3,1e-2,0.1,0.5,1], \n              'tol':[1e-9,1e-7,1e-6,1e-5,1e-4]}\ngrid_ridge = GridSearchCV(ridge, parameters, cv=12, verbose=1, scoring = 'explained_variance')\ngrid_ridge.fit(X_train, y_train)\n\nsc_ridge = get_best_score(grid_ridge)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Best result with Ridge","metadata":{}},{"cell_type":"code","source":"## best fit: \nridge_bf = Ridge(alpha= 0.1, normalize=True,tol = 1e-09)\n## fit the model. \nridge_bf.fit(X_train, y_train)\n## Predicting the target value based on \"Test_x\"\nprediction = ridge_bf.predict(X_test)\n\n#add score to list\nr2_ridge = metrics.r2_score(y_test, prediction)\nmse_ridge = metrics.mean_squared_error(y_test, prediction)\n\n# evaluation model\nmodel_evaluation(y_test,prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.2.4\"></a>\n### 6.2.4: ElasticNet regression\n#### GridSearchCV() with number feature = 62","metadata":{}},{"cell_type":"code","source":"# # select columns that give best results\n# columns = meaningful_features_bval.index[:62]\n# X = X_bval[columns].values\n# y = target_bval\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n\n# # Lasso regression model and gridsearch\n# elastic_reg = ElasticNet()\n# parameters = {\n#     'alpha': [1e-6,1e-5, 1e-4, 5e-3,1e-3,5e-2, 1e-2, 5e-1, 1e-1,1.0],\n#     'l1_ratio': [1e-6,1e-5, 1e-4,5e-3, 1e-3,5e-2, 1e-2, 5e-1,1e-1,1.0],\n#     'tol': [1e-6,1e-5, 1e-4,1e-3]}\n# grid_elas = GridSearchCV(elastic_reg, parameters, cv=12, verbose=1, scoring = 'explained_variance')\n# grid_elas.fit(X_train, y_train)\n\n# sc_elas = get_best_score(grid_elas)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Best result with Elastic","metadata":{}},{"cell_type":"code","source":"## best fit: \nelastic = ElasticNet(alpha= 0.05,l1_ratio= 1e-6,tol=1e-06)\n## fit the model. \nelastic.fit(X_train, y_train)\n## Predicting the target value based on \"Test_x\"\nprediction = elastic.predict(X_test)\n\n#add score to list\nr2_elas = metrics.r2_score(y_test, prediction)\nmse_elas = metrics.mean_squared_error(y_test, prediction)\n\n# evaluation model\nmodel_evaluation(y_test,prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.3\"></a>\n## 6.3: Decision tree methods \n### 6.3.1: Random Forest Regressor\n- First, we evaulate the importance of features.\n- Then, we create several models trained on different input features and we perform a RandomizedSearchCV on each model.\n- Finally, we select the number of features that gave the highest score, we initiate a new model and perform a gridsearch \n\n#### feature importance","metadata":{}},{"cell_type":"code","source":"# initial random forest regressor model, fit it \nmodel_rf_ini = RandomForestRegressor(n_estimators= 100,random_state = 0)\nmodel_rf_ini.fit(X_train_bval, y_train_bval)\n# extract and plot important features\nplot_feature_importance(model_rf_ini.feature_importances_,60,X_train_bval.columns,'Random Forest Regressor: ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### feature selection and gridsearch","metadata":{}},{"cell_type":"code","source":"# Create the random grid\nrandom_grid_rf = {\n                # measure quality of the split\n                'criterion': ['mse', 'mae'],\n                # Number of features to consider at every split  \n               'max_features': ['auto', 'sqrt','log2'],\n                # Maximum number of levels in tree\n               'max_depth': [10,50,100,200,None],\n                # Minimum number of samples required to split a node\n               'min_samples_split': range(2,10,1),\n                # Minimum number of samples required at each leaf node\n               'min_samples_leaf': range(1,10,1),\n                # Method of selecting samples for training each tree\n               'bootstrap': [True, False],\n                'n_estimators' : [100],\n                'random_state': [0]}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # do first random search with threshold = 0.002\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.002, X_train_bval, X_test_bval)\n# # initiat model\n# model_rf = RandomForestRegressor(random_state = 0)\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_bval,0.002)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.002\n\nBest score: **0.869887463593535**\n\nwith the following parameters :\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 200, 'criterion': 'mse', 'bootstrap': False}","metadata":{}},{"cell_type":"code","source":"# # do first random search with threshold = 0.004\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.004, X_train_bval, X_test_bval)\n# # initiat model\n# model_rf = RandomForestRegressor(random_state = 0)\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_bval,0.004)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.004\n\nBest score: **0.8717680156094305**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': None, 'criterion': 'mse', 'bootstrap': False}","metadata":{}},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.004\nBest score: **0.8717680156094305**\nwith the following parameters :\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': None, 'criterion': 'mse', 'bootstrap': False","metadata":{}},{"cell_type":"code","source":"# # do first random search with threshold = 0.006\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.006, X_train_bval, X_test_bval)\n# # initiat model\n# model_rf = RandomForestRegressor(random_state = 0)\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_bval,0.006)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.006\n\nBest score: **0.8723766584804427**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 200, 'criterion': 'mse', 'bootstrap': False}","metadata":{}},{"cell_type":"code","source":"# # do first random search with threshold = 0.008\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.008, X_train_bval, X_test_bval)\n# # initiat model\n# model_rf = RandomForestRegressor(random_state = 0)\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_bval,0.008)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.008\n\nBest score: **0.8817541154750542**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 100, 'criterion': 'mse', 'bootstrap': False}","metadata":{}},{"cell_type":"code","source":"# # do first random search with threshold = 0.009\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.009, X_train_bval, X_test_bval)\n# # initiat model\n# model_rf = RandomForestRegressor(random_state = 0)\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_bval,0.009)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.009\n\nBest score: **0.8611314245563841**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 100, 'criterion': 'mse', 'bootstrap': False}","metadata":{}},{"cell_type":"code","source":"# # do first random search with threshold = 0.01\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.01, X_train_bval, X_test_bval)\n# # initiat model\n# model_rf = RandomForestRegressor(random_state = 0)\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_bval,0.01)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.01\n\nBest score: **0.8629705092133909**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 200, 'criterion': 'mse', 'bootstrap': True}","metadata":{}},{"cell_type":"code","source":"# # do first random search with threshold = 0.012\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.012, X_train_bval, X_test_bval)\n# # initiat model\n# model_rf = RandomForestRegressor(random_state = 0)\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_bval,0.012)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.012\n\nBest score: **0.8681300519859287**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 10, 'criterion': 'mse', 'bootstrap': False}","metadata":{}},{"cell_type":"code","source":"# # do first random search with threshold = 0.014\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.014, X_train_bval, X_test_bval)\n# # initiat model\n# model_rf = RandomForestRegressor(random_state = 0)\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_bval,0.014)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.014\n\nBest score: **0.857859455823619**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': None, 'criterion': 'mse', 'bootstrap': False}","metadata":{}},{"cell_type":"code","source":"# # do first random search with threshold = 0.016\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.016, X_train_bval, X_test_bval)\n# # initiat model\n# model_rf = RandomForestRegressor(random_state = 0)\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_bval,0.016)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.016\n\nBest score: **0.8472248203819742**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': None, 'criterion': 'mse', 'bootstrap': False}","metadata":{}},{"cell_type":"code","source":"# # do first random search with threshold = 0.018\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.018, X_train_bval, X_test_bval)\n# # initiat model\n# model_rf = RandomForestRegressor(random_state = 0)\n# # perform gridsearchCH\n# perform_grid_search(model_rf,random_grid_rf,X_train_selected, y_train_bval,0.018)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.018\n\nBest score: **0.8446134637581928**\n\nwith the following parameters :\n\n {'random_state': 0, 'n_estimators': 100, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 100, 'criterion': 'mse', 'bootstrap': True}","metadata":{}},{"cell_type":"markdown","source":"### gridsearchCH with threshold = 0.008 (best score)","metadata":{}},{"cell_type":"code","source":"# measure quality of the split\n                'criterion': ['mse', 'mae'],\n                # Number of features to consider at every split  \n               'max_features': ['auto', 'sqrt','log2'],\n                # Maximum number of levels in tree\n               'max_depth': [10,50,100,200,None],\n                # Minimum number of samples required to split a node\n               'min_samples_split': range(2,10,1),\n                # Minimum number of samples required at each leaf node\n               'min_samples_leaf': range(1,10,1),\n                # Method of selecting samples for training each tree\n               'bootstrap': [True, False],\n                'n_estimators' : [100],\n                'random_state': [0]}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # do first random search with threshold = 0.0013\n# X_train_selected, X_test_selected =  select_model(model_rf_ini,0.0013, X_train_bval, X_test_bval)\n\n# model_rf = RandomForestRegressor()\n\n\n\n# param_grid = {\n#     'bootstrap': [True],\n#     'criterion': ['mse','mae'],\n#     'max_depth': range(75,155,5),\n#     'max_features': ['log2'],\n#     'min_samples_leaf': [1,2,3,4],\n#     'min_samples_split': [2,3,4],\n#     'n_estimators': [100],\n#     'random_state': [0]\n# }\n\n# grid_search = GridSearchCV(estimator = model_rf, param_grid = param_grid, cv = 6\n#                            , verbose = 2,scoring ='explained_variance')\n\n# # Fit the grid search to the data\n# grid_search.fit(X_train_selected, y_train_bval)\n# grid_search.best_params_","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"result gridsearch1:\n`bootstrap`: True,`criterion`: 'mae', `max_depth`: 75, `max_features`: 'log2', `min_samples_leaf`: 1,\n`min_samples_split`: 2,`n_estimators`: 100,`random_state`: 0","metadata":{}},{"cell_type":"code","source":"# do first random search with threshold = 0.08\nX_train_selected, X_test_selected =  select_model(model_rf_ini,0.008, X_train_bval, X_test_bval)\n\nmodel_rf = RandomForestRegressor(bootstrap ='True',criterion='mae',max_depth = 75,max_features = 'log2',\n                                min_samples_leaf = 1, min_samples_split = 2, n_estimators = 100,random_state =  0)\nmodel_rf.fit(X_train_selected,y_train_bval)\nprediction = model_rf.predict(X_test_selected)\n\n#add score to list\nr2_rf = metrics.r2_score(y_test_bval, prediction)\nmse_rf = metrics.mean_squared_error(y_test_bval, prediction)\n\n# evaluation model\nmodel_evaluation(y_test_bval,prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.3.2\"></a>\n### 6.3.2: xgboost\n#### Features importance","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\n\n# Create the DMatrix: \ntrain_dmatrix = xgb.DMatrix(data=X_train_bval, label=y_train_bval)\n\n# Instantiate the initial regressor model: model_gbm_ini\nmodel_gbm_ini = xgb.XGBRegressor(objective='reg:squarederror',n_estimators = 100,eval_metric='rmse',seed=42)\n\n# Fit andomized_mse to the data\nmodel_gbm_ini.fit(X_train_bval,y_train_bval)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_feature_importance(model_gbm_ini.feature_importances_,60,X_train_bval.columns,'xgboost regression: ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### feature selection","metadata":{}},{"cell_type":"code","source":"# # do first random search with threshold = 0.0001\n# X_train_selected, X_test_selected =  select_model(model_gbm_ini,0.0001, X_train_bval, X_test_bval)\n\n# # Instantiate the regressor: gbm\n# model_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# # perform gridsearchCH\n# perform_grid_search_xgb(model_gbm,random_grid_gbm,X_train_selected, y_train_bval,0.0001)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.0001\n\nBest score: **0.8864982237714087**\n\nwith the following parameters :\n {'seed': 42, 'scale_pos_weight': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 2, 'eta': 0.2, 'colsample_bytree': 0.8, 'alpha': 0}","metadata":{}},{"cell_type":"code","source":"# do first random search with threshold = 0.0002\nX_train_selected, X_test_selected =  select_model(model_gbm_ini,0.0002, X_train_bval, X_test_bval)\n\n# Instantiate the regressor: gbm\nmodel_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# perform gridsearchCH\nperform_grid_search_xgb(model_gbm,random_grid_gbm,X_train_selected, y_train_bval,0.0002)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.0002\n\nBest score: **0.8875462686991474**\n\nwith the following parameters :\n {'seed': 42, 'scale_pos_weight': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 2, 'eta': 0.15, 'colsample_bytree': 1, 'alpha': 0}","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# # do first random search with threshold = 0.0003\n# X_train_selected, X_test_selected =  select_model(model_gbm_ini,0.0003, X_train_bval, X_test_bval)\n\n# # Instantiate the regressor: gbm\n# model_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# # perform gridsearchCH\n# perform_grid_search_xgb(model_gbm,random_grid_gbm,X_train_selected, y_train_bval,0.0003)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.0003\n\nBest score: **0.891920367880493**\nResults from Grid Search with threshold = 0.0003\n\nBest score: 0.891920367880493\n\nwith the following parameters : {'seed': 42, 'scale_pos_weight': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 1, 'eta': 0.2, 'colsample_bytree': 0.8, 'alpha': 0}\nwith the following parameters :\n {'seed': 42, 'scale_pos_weight': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 1, 'eta': 0.2, 'colsample_bytree': 0.8, 'alpha': 0}","metadata":{}},{"cell_type":"code","source":"# do first random search with threshold = 0.0004\nX_train_selected, X_test_selected =  select_model(model_gbm_ini,0.0004, X_train_bval, X_test_bval)\n\n# Instantiate the regressor: gbm\nmodel_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# perform gridsearchCH\nperform_grid_search_xgb(model_gbm,random_grid_gbm,X_train_selected, y_train_bval,0.0004)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.0004\n\nBest score: **0.8814383131990927**\n\nwith the following parameters :\n {'seed': 42, 'scale_pos_weight': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 2, 'eta': 0.1, 'colsample_bytree': 0.7, 'alpha': 0}","metadata":{}},{"cell_type":"code","source":"# # do first random search with threshold = 0.0005\n# X_train_selected, X_test_selected =  select_model(model_gbm_ini,0.0005, X_train_bval, X_test_bval)\n\n# # Instantiate the regressor: gbm\n# model_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# # perform gridsearchCH\n# perform_grid_search_xgb(model_gbm,random_grid_gbm,X_train_selected, y_train_bval,0.0005)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results from Grid Search with threshold = 0.0005\n\nBest score: **0.8773872353186494**\n\nwith the following parameters :\n {'seed': 42, 'scale_pos_weight': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 4, 'eta': 0.05, 'colsample_bytree': 0.6, 'alpha': 0}","metadata":{}},{"cell_type":"markdown","source":"#### Run gridsearch with threshold 0.0003 ","metadata":{}},{"cell_type":"code","source":"# #### do first random search with threshold = 0.0003\n# X_train_selected, X_test_selected =  select_model(model_gbm_ini,0.0003, X_train_bval, X_test_bval)\n\n# # Instantiate the regressor: gbm\n# model_gbm = xgb.XGBRegressor(objective='reg:squarederror')\n\n# gbm_param_grid = {\n#     'colsample_bytree': [0.75,0.77,0.79, 0.8,0.82,0.84],\n#     'max_depth': range(1,3,1),   \n#     'eta' : [0.16,0.18,0.2, 0.22, 0.24],\n#     'alpha': [0,0.1,0.2,0.3],\n#     'min_child_weight': [1],\n# #     'scale_pos_weight' : [1],\n#     'n_estimators' :  [100],\n#      'seed' : [42],\n    \n# }\n\n# grid_search = GridSearchCV(estimator = model_gbm, param_grid = gbm_param_grid, cv = 4, verbose = 2,scoring ='explained_variance')\n\n# # Fit the grid search to the data\n# grid_search.fit(X_train_selected, y_train_bval)\n# grid_search.best_params_\n# grid_search.best_score_","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Result gridsearchCV:\n\nBEST SCORE  = **0.891920367880493**, with:\n\n{'alpha': 0,\n 'colsample_bytree': 0.8,\n 'eta': 0.2,\n 'max_depth': 1,\n 'min_child_weight': 1,\n 'n_estimators': 100,\n 'seed': 42}","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\n# lower 'colsample_bytree': 0.67 to 0.07\nxgb  = xgb.XGBRegressor(objective='reg:squarederror',n_estimators = 100,min_child_weight= 1,\n                        max_depth = 1, eta = 0.2,colsample_bytree = 0.8,\n                        alpha = 0.0,seed=42,scale_pos_weight=1)\n                        \n# Fit the regressor to the training set\nxgb.fit(X_train_selected,y_train_bval)\n\n## Predicting the target value\nprediction = xgb.predict(X_test_selected)\n\n# evaluation model\nr2 = round(metrics.r2_score(y_test_bval, prediction), 2)\nprint(r2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model has a better performance on the training set than on the testing set.\nso we will try to lower:\n\n- `colsample_bytree` (the ratio of features used),\n- `subsample` (the ratio of the training instances used),\n- `eta` (the learning rate of our GBM (i.e. how much we update our prediction with each successive tree).\n    \nand to increase:\n\n- `gamma` (the minimum loss reduction required to make a further split),\n- `min_child_weight` (the minimum sum of instance weight needed in a leaf)","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\n# lower 'colsample_bytree': 0.67 to 0.07\nxgb  = xgb.XGBRegressor(objective='reg:squarederror',n_estimators = 100,min_child_weight= 1,\n                        max_depth = 1, eta = 0.1,colsample_bytree = 0.6,\n                        alpha = 0.0,seed=42,scale_pos_weight=1,subsample = 0.8)\n                        \n# Fit the regressor to the training set\nxgb.fit(X_train_selected,y_train_bval)\n\n## Predicting the target value\nprediction = xgb.predict(X_test_selected)\n\n#add score to list\nr2_xgb = metrics.r2_score(y_test_bval, prediction)\nmse_xgb = metrics.mean_squared_error(y_test_bval, prediction)\n\n# evaluation model\nmodel_evaluation(y_test_bval,prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.4\"></a>\n## 6.4: Summary -- predition monthly b-value --","metadata":{}},{"cell_type":"code","source":"# create list with the model score\nlist_score_r2 = [r2_linreg,r2_ridge,r2_Lasso,r2_elas,r2_rf,r2_xgb]\nlist_score_mse = [mse_linreg,mse_ridge,mse_Lasso,mse_elas,mse_rf,mse_xgb]\n# create list with model name\nlist_regressors = ['linear','Ridge','Lasso','ElaNet','RF','xgboost']\n\n# create dictionnary and dataframe\ndic_score = {'model': list_regressors,\n            'score_R2':list_score_r2,\n            'score_mse':list_score_mse,}\n\ndic_score_bval = pd.DataFrame(dic_score)\ndic_score_bval","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the predictions for each model\nfig, axes = plt.subplots(2,figsize=(15,5))\nax = plt.subplot(1,2,1)\nax = sns.pointplot(x = \"model\", y = \"score_R2\", data = dic_score_bval,color='green') \nax.set_ylabel('Score (R2)', size=20, labelpad=12.5)\nax.set_xlabel('Model', size=20, labelpad=12.5)\nax.tick_params(labelsize=14)\n\n# add annotations one by one with a loop\nfor ind in dic_score_bval.index: \n     ax.text(ind,dic_score_bval['score_R2'][ind]+0.002,'{:.5f}'.format(dic_score_bval['score_R2'][ind]),\n             horizontalalignment='left', size='medium', color='black', weight='semibold',fontsize=12)\n        \nax2 = plt.subplot(1,2,2)\nax2 = sns.pointplot(x = \"model\", y = \"score_mse\", data = dic_score_bval,color='green') \nax2.set_ylabel('Score (MSE)', size=20, labelpad=12.5)\nax2.set_xlabel('Model', size=20, labelpad=12.5)\nax2.tick_params(labelsize=14)\n\n# add annotations one by one with a loop\nfor ind in dic_score_bval.index: \n     ax2.text(ind,dic_score_bval['score_mse'][ind]+5e-5,'{:.5f}'.format(dic_score_bval['score_mse'][ind]),\n             horizontalalignment='left', size='medium', color='black', weight='semibold',fontsize=12)\n        \nplt.title(\"Models' scores \", size=20)\nplt.savefig('Models scores bvalues.png')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualize prediction made with Ridge","metadata":{}},{"cell_type":"code","source":"# select columns that give best results\ncolumns = meaningful_features_bval.index[:62]\nX = X_bval[columns].values\ny = target_bval\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n\n## best fit: \nridge_bf = Ridge(alpha= 0.1, normalize=True,tol = 1e-09)\n## fit the model. \nridge_bf.fit(X_train, y_train)\n## Predicting the target value based on \"Test_x\"\nlog_prediction = ridge_bf.predict(X_test)\nprediction =  np.exp(log_prediction)\noriginal_data = np.exp(y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the predictions for each model\nfig, axes = plt.subplots(1,figsize=(10,5))\n      \nplt.plot(original_data,'-ob',label='original data')\nplt.plot(prediction,'-dr',color='green',label='prediction')\nplt.ylabel('monthly b-values',size=16)\nplt.legend()\n\nplt.title(\"Original data vs predicted\", size=20)\n\nplt.savefig('b values -  original vs predicted.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}