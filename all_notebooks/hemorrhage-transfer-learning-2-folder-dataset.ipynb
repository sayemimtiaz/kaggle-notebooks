{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from IPython.display import clear_output\n!pip install imutils\nclear_output()\n\nfrom keras.applications.vgg19 import VGG19,preprocess_input\nfrom keras.applications.xception import Xception,preprocess_input\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.resnet50 import ResNet50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nfrom tqdm import tqdm\nimport cv2\nimport os\nimport shutil\nimport itertools\nimport imutils\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly import tools\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg16 import VGG16, preprocess_input\nfrom keras import layers\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.callbacks import EarlyStopping\n\ninit_notebook_mode(connected=True)\nRANDOM_SEED = 123","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!apt-get install tree\n#clear_output()\n# create new folders\n!mkdir TRAIN TEST VAL TRAIN/YES TRAIN/NO TEST/YES TEST/NO VAL/YES VAL/NO\n!tree -d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMG_PATH = '../input/head-ct-images-for-classification/dataset/head ct/'\n# split the data by train/val/test\nfor CLASS in os.listdir(IMG_PATH):\n    if not CLASS.startswith('.'):\n        IMG_NUM = len(os.listdir(IMG_PATH + CLASS))\n        for (n, FILE_NAME) in enumerate(os.listdir(IMG_PATH + CLASS)):\n            img = IMG_PATH + CLASS + '/' + FILE_NAME\n            if n < 5:\n                shutil.copy(img, 'TEST/' + CLASS.upper() + '/' + FILE_NAME)\n            elif n < 0.8*IMG_NUM:\n                shutil.copy(img, 'TRAIN/'+ CLASS.upper() + '/' + FILE_NAME)\n            else:\n                shutil.copy(img, 'VAL/'+ CLASS.upper() + '/' + FILE_NAME)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data(dir_path, img_size=(100,100)):\n    \"\"\"\n    Load resized images as np.arrays to workspace\n    \"\"\"\n    X = []\n    y = []\n    i = 0\n    labels = dict()\n    for path in tqdm(sorted(os.listdir(dir_path))):\n        if not path.startswith('.'):\n            labels[i] = path\n            for file in os.listdir(dir_path + path):\n                if not file.startswith('.'):\n                    img = cv2.imread(dir_path + path + '/' + file)\n                    X.append(img)\n                    y.append(i)\n            i += 1\n    X = np.array(X)\n    y = np.array(y)\n    print(f'{len(X)} images loaded from {dir_path} directory.')\n    return X, y, labels\n\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.figure(figsize = (6,6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() / 2.\n    cm = np.round(cm,2)\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_DIR = 'TRAIN/'\nTEST_DIR = 'TEST/'\nVAL_DIR = 'VAL/'\nIMG_SIZE = (224,224)\n\n# use predefined function to load the image data into workspace\nX_train, y_train, labels = load_data(TRAIN_DIR, IMG_SIZE)\nX_test, y_test, _ = load_data(TEST_DIR, IMG_SIZE)\nX_val, y_val, _ = load_data(VAL_DIR, IMG_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = dict()\ny[0] = []\ny[1] = []\nfor set_name in (y_train, y_val, y_test):\n    y[0].append(np.sum(set_name == 0))\n    y[1].append(np.sum(set_name == 1))\n\ntrace0 = go.Bar(\n    x=['Train Set', 'Validation Set', 'Test Set'],\n    y=y[0],\n    name='No',\n    marker=dict(color='#33cc33'),\n    opacity=0.7\n)\ntrace1 = go.Bar(\n    x=['Train Set', 'Validation Set', 'Test Set'],\n    y=y[1],\n    name='Yes',\n    marker=dict(color='#ff3300'),\n    opacity=0.7\n)\ndata = [trace0, trace1]\nlayout = go.Layout(\n    title='Count of classes in each set',\n    xaxis={'title': 'Set'},\n    yaxis={'title': 'Count'}\n)\nfig = go.Figure(data, layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_samples(X, y, labels_dict, n=50):\n    \"\"\"\n    Creates a gridplot for desired number of images (n) from the specified set\n    \"\"\"\n    for index in range(len(labels_dict)):\n        imgs = X[np.argwhere(y == index)][:n]\n        j = 10\n        i = int(n/j)\n\n        plt.figure(figsize=(15,6))\n        c = 1\n        for img in imgs:\n            plt.subplot(i,j,c)\n            plt.imshow(img[0])\n\n            plt.xticks([])\n            plt.yticks([])\n            c += 1\n        plt.suptitle('Hemorrhage: {}'.format(labels_dict[index]))\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_samples(X_train, y_train, labels, 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RATIO_LIST = []\nfor set in (X_train, X_test, X_val):\n    for img in set:\n        RATIO_LIST.append(img.shape[1]/img.shape[0])\n        \nplt.hist(RATIO_LIST)\nplt.title('Distribution of Image Ratios')\nplt.xlabel('Ratio Value')\nplt.ylabel('Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def crop_imgs(set_name, add_pixels_value=0):\n    \"\"\"\n    Finds the extreme points on the image and crops the rectangular out of them\n    \"\"\"\n    set_new = []\n    for img in set_name:\n        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        gray = cv2.GaussianBlur(gray, (5, 5), 0)\n\n        # threshold the image, then perform a series of erosions +\n        # dilations to remove any small regions of noise\n        thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n        thresh = cv2.erode(thresh, None, iterations=2)\n        thresh = cv2.dilate(thresh, None, iterations=2)\n\n        # find contours in thresholded image, then grab the largest one\n        cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        cnts = imutils.grab_contours(cnts)\n        c = max(cnts, key=cv2.contourArea)\n\n        # find the extreme points\n        extLeft = tuple(c[c[:, :, 0].argmin()][0])\n        extRight = tuple(c[c[:, :, 0].argmax()][0])\n        extTop = tuple(c[c[:, :, 1].argmin()][0])\n        extBot = tuple(c[c[:, :, 1].argmax()][0])\n\n        ADD_PIXELS = add_pixels_value\n        new_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()\n        set_new.append(new_img)\n\n    return np.array(set_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.imread('../input/head-ct-images-for-classification/dataset/head ct/yes/000.png')\nimg = cv2.resize(\n            img,\n            dsize=(224,224),\n            interpolation=cv2.INTER_CUBIC\n        )\ngray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\ngray = cv2.GaussianBlur(gray, (5, 5), 0)\n\n# threshold the image, then perform a series of erosions +\n# dilations to remove any small regions of noise\nthresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\nthresh = cv2.erode(thresh, None, iterations=2)\nthresh = cv2.dilate(thresh, None, iterations=2)\n\n# find contours in thresholded image, then grab the largest one\ncnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = imutils.grab_contours(cnts)\nc = max(cnts, key=cv2.contourArea)\n\n# find the extreme points\nextLeft = tuple(c[c[:, :, 0].argmin()][0])\nextRight = tuple(c[c[:, :, 0].argmax()][0])\nextTop = tuple(c[c[:, :, 1].argmin()][0])\nextBot = tuple(c[c[:, :, 1].argmax()][0])\n\n# add contour on the image\nimg_cnt = cv2.drawContours(img.copy(), [c], -1, (0, 255, 255), 4)\n\n# add extreme points\nimg_pnt = cv2.circle(img_cnt.copy(), extLeft, 8, (0, 0, 255), -1)\nimg_pnt = cv2.circle(img_pnt, extRight, 8, (0, 255, 0), -1)\nimg_pnt = cv2.circle(img_pnt, extTop, 8, (255, 0, 0), -1)\nimg_pnt = cv2.circle(img_pnt, extBot, 8, (255, 255, 0), -1)\n\n# crop\nADD_PIXELS = 0\nnew_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,6))\nplt.subplot(141)\nplt.imshow(img)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 1. Get the original image')\nplt.subplot(142)\nplt.imshow(img_cnt)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 2. Find the biggest contour')\nplt.subplot(143)\nplt.imshow(img_pnt)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 3. Find the extreme points')\nplt.subplot(144)\nplt.imshow(new_img)\nplt.xticks([])\nplt.yticks([])\nplt.title('Step 4. Crop the image')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply this for each set\nX_train_crop = crop_imgs(set_name=X_train)\nX_val_crop = crop_imgs(set_name=X_val)\nX_test_crop = crop_imgs(set_name=X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_samples(X_train_crop, y_train, labels, 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_new_images(x_set, y_set, folder_name):\n    i = 0\n    for (img, imclass) in zip(x_set, y_set):\n        if imclass == 0:\n            cv2.imwrite(folder_name+'NO/'+str(i)+'.png', img)\n        else:\n            cv2.imwrite(folder_name+'YES/'+str(i)+'.png', img)\n        i += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# saving new images to the folder\n!mkdir TRAIN_CROP TEST_CROP VAL_CROP TRAIN_CROP/YES TRAIN_CROP/NO TEST_CROP/YES TEST_CROP/NO VAL_CROP/YES VAL_CROP/NO\n\nsave_new_images(X_train_crop, y_train, folder_name='TRAIN_CROP/')\nsave_new_images(X_val_crop, y_val, folder_name='VAL_CROP/')\nsave_new_images(X_test_crop, y_test, folder_name='TEST_CROP/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_imgs(set_name, img_size):\n    \"\"\"\n    Resize and apply VGG-15 preprocessing\n    \"\"\"\n    set_new = []\n    for img in set_name:\n        img = cv2.resize(\n            img,\n            dsize=img_size,\n            interpolation=cv2.INTER_CUBIC\n        )\n        set_new.append(preprocess_input(img))\n    return np.array(set_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_prep = preprocess_imgs(set_name=X_train_crop, img_size=IMG_SIZE)\nX_test_prep = preprocess_imgs(set_name=X_test_crop, img_size=IMG_SIZE)\nX_val_prep = preprocess_imgs(set_name=X_val_crop, img_size=IMG_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" plot_samples(X_train_prep, y_train, labels, 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set the paramters we want to change randomly\ndemo_datagen = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.05,\n    height_shift_range=0.05,\n    rescale=1./255,\n    shear_range=0.05,\n    brightness_range=[0.1, 1.5],\n    horizontal_flip=True,\n    vertical_flip=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.mkdir('preview')\nx = X_train_crop[0]  \nx = x.reshape((1,) + x.shape) \n\ni = 0\nfor batch in demo_datagen.flow(x, batch_size=1, save_to_dir='preview', save_prefix='aug_img', save_format='jpg'):\n    i += 1\n    if i > 20:\n        break ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(X_train_crop[0])\nplt.xticks([])\nplt.yticks([])\nplt.title('Original Image')\nplt.show()\n\nplt.figure(figsize=(15,6))\ni = 1\nfor img in os.listdir('preview/'):\n    img = cv2.cv2.imread('preview/' + img)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.subplot(3,7,i)\n    plt.imshow(img)\n    plt.xticks([])\n    plt.yticks([])\n    i += 1\n    if i > 3*7:\n        break\nplt.suptitle('Augemented Images')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -rf preview/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_DIR = 'TRAIN_CROP/'\nVAL_DIR = 'VAL_CROP/'\n\ntrain_datagen = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.1,\n    brightness_range=[0.5, 1.5],\n    horizontal_flip=True,\n    vertical_flip=True,\n    preprocessing_function=preprocess_input\n)\n\ntest_datagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input\n)\n\n\ntrain_generator = train_datagen.flow_from_directory(\n    TRAIN_DIR,\n    color_mode='rgb',\n    target_size=IMG_SIZE,\n    batch_size=32,\n    class_mode='binary',\n    seed=RANDOM_SEED\n)\n\n\nvalidation_generator = test_datagen.flow_from_directory(\n    VAL_DIR,\n    color_mode='rgb',\n    target_size=IMG_SIZE,\n    batch_size=16,\n    class_mode='binary',\n    seed=RANDOM_SEED\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"ResNet50_weight_path = '../input/keras-pretrained-models/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\nresnet50_x = ResNet50(\n    weights=ResNet50_weight_path,\n    include_top=False, \n    input_shape=IMG_SIZE + (3,)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vgg16_weight_path = '../input/keras-pretrained-models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\nvgg = VGG16(\n    weights=vgg16_weight_path,\n    include_top=False, \n    input_shape=IMG_SIZE + (3,)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport math\nimport cv2\nimport matplotlib.pyplot as plt\nimport os\nimport seaborn as sns\nimport umap\nfrom PIL import Image\nfrom scipy import misc\nfrom os import listdir\nfrom os.path import isfile, join\nimport numpy as np\nfrom scipy import misc\nfrom random import shuffle\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras.utils.np_utils import to_categorical\nfrom keras.layers import Input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nimport random\nimport warnings\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom itertools import chain\nfrom skimage.io import imread, imshow, imread_collection, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.morphology import label\n\nfrom keras.models import Model, load_model\nfrom keras.layers import Input\nfrom keras.layers.core import Dropout, Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import backend as K\nimport keras\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.models import Model\nfrom matplotlib import pyplot \nfrom numpy import expand_dims\n\n\nf = plt.figure(figsize=(16,16))\n# load the modelf = plt.figure(figsize=(10,3))\nmodel = VGG16()\n# redefine model to output right after the first hidden layer\nmodel = Model(inputs=model.inputs, outputs=model.layers[1].output)\nmodel.summary()\n# load the image with the required shape\n# convert the image to an array\nimg = img_to_array(X_val_prep[39])\n# expand dimensions so that it represents a single 'sample'\nimg = expand_dims(img, axis=0)\n# prepare the image (e.g. scale pixel values for the vgg)\nimg = preprocess_input(img)\n# get feature map for first hidden layer\nfeature_maps = model.predict(img)\n# plot all 64 maps in an 8x8 squares\nsquare = 8\nix = 1\nfor _ in range(square):\n\tfor _ in range(square):\n\t\t# specify subplot and turn of axis\n\t\tax = pyplot.subplot(square, square, ix)\n\t\tax.set_xticks([])\n\t\tax.set_yticks([])\n\t\t# plot filter channel in grayscale\n\t\tpyplot.imshow(feature_maps[0, :, :, ix-1], cmap='viridis')\n\t\tix += 1\n# show the figure\npyplot.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_CLASSES = 1\n\nvgg16 = Sequential()\nvgg16.add(vgg)\nvgg16.add(layers.Dropout(0.3))\nvgg16.add(layers.Flatten())\nvgg16.add(layers.Dropout(0.5))\nvgg16.add(layers.Dense(NUM_CLASSES, activation='sigmoid'))\n\nvgg16.layers[0].trainable = False\n\nvgg16.compile(\n    loss='binary_crossentropy',\n    optimizer=RMSprop(lr=1e-4),\n    metrics=['accuracy']\n)\nvgg16.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=0.0003, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False), metrics=[\"accuracy\"])\n\nvgg16.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize feature maps output from each block in the vgg model\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.models import Model\nimport matplotlib.pyplot as plt\nfrom numpy import expand_dims\n\n\n\n\n# load the model\nmodel = VGG16()\n# redefine model to output right after the first hidden layer\nixs = [2, 5, 9, 13, 17]\noutputs = [model.layers[i].output for i in ixs]\nmodel = Model(inputs=model.inputs, outputs=outputs)\n# load the image with the required shape\n# convert the image to an array\nimg = img_to_array(X_val_prep[39])\n# expand dimensions so that it represents a single 'sample'\nimg = expand_dims(img, axis=0)\n# prepare the image (e.g. scale pixel values for the vgg)\nimg = preprocess_input(img)\n# get feature map for first hidden layer\nfeature_maps = model.predict(img)\n# plot the output from each block\nsquare = 8\nfor fmap in feature_maps:\n\t# plot all 64 maps in an 8x8 squares\n\tix = 1\n\tfor _ in range(square):\n\t\tplt.figure(figsize=(64,64))\n\t\tfor _ in range(square):\n           \n\n\t\t\t# specify subplot and turn of axis\n\t\t\tax = pyplot.subplot(square, square, ix)\n\t\t\tax.set_xticks([])\n\t\t\tax.set_yticks([])\n\t\t\t\n\t\t\t# plot filter channel in grayscale\n\t\t\tplt.imshow(fmap[0, :, :, ix-1], cmap='viridis')\n\t\t\tix += 1\n\t# show the figure\n\n        \n\tplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\n\nstart = time.time()\n\nvgg16_history = vgg16.fit_generator(\n    train_generator,\n    #steps_per_epoch=18,\n    epochs=120,\n    validation_data=validation_generator,\n    #validation_steps=9,\n)\n\n\nend = time.time()\nprint(end - start)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# validate on val set\npredictions = vgg16.predict(X_test_prep)\npredictions = [1 if x>0.5 else 0 for x in predictions]\n\n_, train_acc = vgg16.evaluate(X_val_prep, y_val, verbose=0)\n_, test_acc = vgg16.evaluate(X_test_prep, y_test, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pyplot.figure(figsize=(12,12))\n# plot loss during training\npyplot.subplot(211)\npyplot.title('Vgg16 Loss')\npyplot.plot(vgg16_history.history['loss'], label='train')\npyplot.plot(vgg16_history.history['val_loss'], label='Validation')\npyplot.legend()\n# plot accuracy during training\npyplot.subplot(212)\npyplot.title('Vgg16 Accuracy')\npyplot.plot(vgg16_history.history['accuracy'], label='train')\npyplot.plot(vgg16_history.history['val_accuracy'], label='Validation')\npyplot.legend()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import make_circles\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n# accuracy: (tp + tn) / (p + n)\naccuracy = accuracy_score(y_test, predictions)\nprint('Accuracy: %f' % accuracy)\n# precision tp / (tp + fp)\nprecision = precision_score(y_test, predictions)\nprint('Precision: %f' % precision)\n# recall: tp / (tp + fn)\nrecall = recall_score(y_test, predictions)\nprint('Recall: %f' % recall)\n# f1: 2 tp / (2 tp + fp + fn)\nf1 = f1_score(y_test, predictions)\nprint('F1 score: %f' % f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kappa = cohen_kappa_score(y_test, predictions)\nprint('Cohens kappa: %f' % kappa)\n# ROC AUC\nauc = roc_auc_score(y_test, predictions)\nprint('ROC AUC: %f' % auc)\n# confusion matrix\nmatrix = confusion_matrix(y_test, predictions)\nprint(matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}