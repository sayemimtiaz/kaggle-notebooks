{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Weather Time Series Project: Estes Park"},{"metadata":{},"cell_type":"markdown","source":"Coded by Luna McBride"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #Plotting\n%matplotlib inline\n\nplt.rcParams['figure.figsize'] = (15,10) #Set the default figure size\nplt.style.use('ggplot') #Set the plotting method\n\nfrom statsmodels.tsa.stattools import grangercausalitytests #Causality Testing\nfrom statsmodels.tsa.seasonal import seasonal_decompose #Seasonal Decomposition\nfrom statsmodels.tsa.stattools import adfuller #Stationary Testing\nfrom statsmodels.graphics.tsaplots import plot_acf #Compute lag for ARIMA\nfrom statsmodels.graphics.tsaplots import plot_pacf #Compute partial lag for ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX #Build the model\n\nfrom sklearn.metrics import mean_absolute_error #Error\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"weather = pd.read_csv(\"../input/weather-analysis/climate_data.csv\") #Read the weather CSV\nweather.head() #Take a peek at the dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Check for Null Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(weather.isnull().any()) #Check for nulls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no null values, but there seems to be some columns that I find unnecessary for what I would like to do. Columns like month does not seem necessary. I also should check if the date fields are the same."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Removing Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(weather.loc[weather[\"Date\"] != weather[\"Date1\"]]) #Check if the date fields are the same","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The date fields are the exact same, so Date1 should be removed.\n\nLooking at the columns, I think I should also remove most of the min and max fields, as I want to look at the average (with exception to temperature, as I am interested in how that one specifically changes). Wind direction also appears to be something that does not change anything, as a gust of wind is a gust of wind from any direction.\n\nI should also not use rainfall as it is here, as it builds until the end of the month/year to get the rain value (as listed on the dataset's kaggle page). I am looking at this in a daily context, so if I want to use rainfall, I should get the difference per day in order to get the rainfall per day. That is not inherently useful either, so I will just drop it."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping the columns I had justified dropping\nweatherClean = weather.drop([\"Average gustspeed (mph)\", \"Average direction (°deg)\", \"Rainfall for month (in)\", \"Rainfall for year (in)\",\n                        \"Maximum rain per minute\", \"Maximum humidity (%)\", \"Minimum humidity (%)\", \"Maximum pressure\", \n                        \"Minimum pressure\", \"Maximum windspeed (mph)\", \"Maximum gust speed (mph)\", \"Maximum heat index (°F)\",\n                        \"Date1\", \"Month\", \"diff_pressure\"], axis = 1)\nweatherClean.head() #Take a peek at the dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Fixing the Labels"},{"metadata":{},"cell_type":"markdown","source":"For this, I will have all the average as just the field name. For example, Average Temperature (°F) would become Temperature. The min and max temp will become minTemperature and maxTemperature respectively"},{"metadata":{"trusted":true},"cell_type":"code","source":"currentLabels = list(weatherClean.columns) #A list of the current columns\n\n#The list of new column names I want\nnewLabels = [\"Temperature\", \"Humidity\", \"Dewpoint\", \"Pressure\", \"Windspeed\", \"MaxTemperature\", \"MinTemperature\"]\nnumLabels = len(newLabels) #Get the length for the for loop\n\n#For loop to change the column names\nfor i in range(0, numLabels):\n    #Set the column names to the new columns, ignoring the Date name (that name is fine)\n    weatherClean = weatherClean.rename(columns = {currentLabels[i + 1] : newLabels[i]})\n    \nweatherClean.head() #Take a peek at the dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(weatherClean.dtypes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Set the Dates to the Index"},{"metadata":{"trusted":true},"cell_type":"code","source":"weatherClean[\"Date\"] = pd.to_datetime(weatherClean[\"Date\"]) #Ensure the date data is in datetime format\nweatherClean.set_index(\"Date\", inplace = True) #Set the date to the index\nweatherClean = weatherClean.asfreq(\"D\") #Set the frequency to daily\n\nweatherClean.head(12) #Take a peek at the data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', 400) #Set the display to allow all the null rows to show\nprint(weatherClean.isnull().any()) #Check for nulls\nprint(weatherClean.loc[weatherClean[\"Temperature\"].isnull()]) #Print all the null rows","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Problem: there are quite a few null values throughout this dataset. This includes the entire month of December, 2009 and large chunks of other months. I looked through the dataset as well as the dataset's author's source. The source was also missing these dates, including December 2009. This a huge note in the process, as I have to either drop the nulls and have Pandas with no frequency or fill these null values and eliminate the context of those months. I am going to continue ahead with this dataset, though, using frontfill to propegate. This will likely cause noticable lines of the same value in the data. Just a warning."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', 15) #Set the max rows back to normal\nweatherClean = weatherClean.fillna(method  = \"ffill\") #Fill the null values\nprint(weatherClean.isnull().any()) #Check for nulls\nprint(weatherClean.index) #Print the index to see the frequency\nprint(weatherClean.head()) #Print the head of the dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weatherGraph = weatherClean[\"Temperature\"].plot(title = \"Average Temperature\") #Plot the temperature data\nweatherGraph.set(ylabel = \"Temperature (F)\") #Put the temperature label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The filled values do not appear to be too noticable on the graph. I do not think it will cause very many issues."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Visualizations"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (10,20) #Set the figure size\nweatherClean.plot(subplots = True, title = \"Variable Visualization\") #Plot all the variables","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (15,10) #Set the figure size for the min-max temperature graph\nminmax = weatherClean[[\"MaxTemperature\", \"MinTemperature\"]].plot(title = \"Min and Max Temperature\") #Plot the min and max temperatures\nminmax.set(ylabel = \"Temperature (F)\") #Put the temperature label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The majority of these features have clear seasonality to them. The general trends still seem to be upheld despite the missing data issues, but the min max graph shows how some larger areas that had gaps in their data,specifically March-April of 2011."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Causality"},{"metadata":{},"cell_type":"markdown","source":"Using this source for causality and cointegration: https://www.kaggle.com/gayatry/daily-climate-forcast"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = weatherClean.columns #Get the columns to use as both rows and columns\nmaxlag = 12 #The largest amount of lag allowable\ntest = \"ssr_chi2test\" #Making sure we are looking at the chi squarred test\n\ncause = pd.DataFrame(np.zeros((len(columns), len(columns))), columns = columns, index = columns) #Build the layout for the causality\n\n#For loop to fill the columns of the causality\nfor column in columns:\n    \n    #For loop to fill the rows of the causality, in conjunction with the column\n    for row in columns:\n        causality = grangercausalitytests(weatherClean[[row, column]], maxlag = maxlag,verbose = False) #Calculate causality values\n        pValues = [round(causality[i+1][0][test][1],5) for i in range(maxlag)] #Collect all the p values in the max lag\n        minValue = np.min(pValues) #Take the smallest P value\n        cause.loc[row, column] = minValue #Put the smallest P value into the causalty\n\ncause.head(7) #Show the causality","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is very little connection between these variables. That means VAR is not useful here. I am also not looking for one variable specifically, so a regular regression does not fit here. Thus, I will use ARIMA."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Seasonal Decomposition"},{"metadata":{"trusted":true},"cell_type":"code","source":"#For each column, decompose and plot the column\nfor column in columns:\n    decomposed = seasonal_decompose(weatherClean[column]) #Seasonally decompose the set column\n    x = decomposed.plot() #Plot the decomposed column","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# ADFuller Stationary Testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"ADFuller Test; Significance: 0.05\") #Print the significance level\n\n#Do the ADFuller test for each column\nfor column in columns:\n    adf = adfuller(weatherClean[column]) #Call adfuller to test\n    print(\"ADF test static for {} is {}\".format(column, adf[1])) #Print the adfuller results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Every column tested below the significance level. The data is stationary."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Training the Model (Dewpoint)"},{"metadata":{},"cell_type":"markdown","source":"## Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"length = int((len(weatherClean)*9)/10) #Get 9/10 of the length of the data\nprint(length) #Print the length to make sure it actually is an int\ntrain = weatherClean[:length] #Split off the training set\ntest = weatherClean[length:] #Split off the testing set\ndew = test[\"Dewpoint\"] #Get the dewpoint data for the testing set\ntest = test.drop(columns = {\"Dewpoint\"}) #Drop the dewpoint so the model will not get mad","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = SARIMAX(train[\"Dewpoint\"], train.drop(columns = {\"Dewpoint\"})) #Build the SARIMAX model\nfitModel = model.fit(disp = 1) #Fit the SARIMAx model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast = fitModel.forecast(len(weatherClean) - length, exog = test.values) #Forecast for validation data\nax = forecast.plot(subplots = True, color = \"red\", linewidth = 2, title = \"Dewpoint Validation\") #Plot the forecasted data\ndew.plot(ax = ax, subplots = True, color = \"royalblue\") #Plot the actual dewpoint\nplt.ylabel(\"Temperature (F)\") #Put the temperature label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The Absolute Error of {} is {}\".format(\"Dewpoint\", mean_absolute_error(dew, forecast))) #Print the error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model is predicting with a small amount of error. This is fine."},{"metadata":{},"cell_type":"markdown","source":"## Forecasting"},{"metadata":{"trusted":true},"cell_type":"code","source":"modelPred = SARIMAX(weatherClean[\"Dewpoint\"], weatherClean.drop(columns = {\"Dewpoint\"})) #Build the prediction model\nfitModelPred = modelPred.fit(disp = 1) #Fit the SARIMAX model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weatherNoDew = weatherClean.drop(columns = {\"Dewpoint\"}) #Get the weather data without the dewpoint\nweatherNoDew = weatherNoDew[len(weatherClean) - 365:] #Limit the size so I can see it better in the graph\ndewShort = dew[len(dew)-365:] #Get a shortened dew dataframe so details can be visible","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecastDew = fitModelPred.forecast(steps = len(weatherNoDew), exog = weatherNoDew.values) #Forecast the next year\nax = forecastDew.plot(subplots = True, color = \"red\", linewidth = 2, title = \"Dewpoint Forecast\") #Plot the dew forecast\ndewShort.plot(ax = ax, subplots = True, color = \"royalblue\") #Plot the shortened dew dataframe\nplt.ylabel(\"Temperature (F)\") #Put the temperature label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.estesparkweather.net/archive_reports.php?date=202011\nprint(\"Actual (Taken from the source): 5.7\") #Print yesterday's the dewpoint from the source\nprint(\"Prediction: {}\".format(forecastDew[\"2020-11-13\"])) #Print the prediction for yesterday","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The prediction for yesterday was off, but previous years had higher dewpoints for this day. It is off in the actual, but it a fair prediction based off other years."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Training the Model (Temperature)"},{"metadata":{},"cell_type":"markdown","source":"## Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainTem = weatherClean[:length] #Get a training split\ntestTem = weatherClean[length:] #Get a testing split\ntemp = testTem[\"Temperature\"] #Get the temperature data for graphing\ntestTem = testTem.drop(columns = {\"Temperature\"}) #Drop the temperature data to test the model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelTemp = SARIMAX(trainTem[\"Temperature\"], trainTem.drop(columns = {\"Temperature\"})) #Build the model\nfitModelTemp = modelTemp.fit(disp = 1) #Fit the SARIMAX model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecastTem = fitModelTemp.forecast(len(weatherClean) - length, exog = testTem.values) #Get the validation forecast\nax = forecastTem.plot(subplots = True, color = \"red\", linewidth = 2, title = \"Temperature Validation\") #Plot the validation data\ntemp.plot(ax = ax, subplots = True, color = \"royalblue\") #Plot the temperature for the test timeframe\nplt.ylab = \"Temperature (F)\" #Put the temperature label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The Absolute Error of {} is {}\".format(\"Temperature\", mean_absolute_error(temp, forecastTem))) #Get the temperature error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelPredTemp = SARIMAX(weatherClean[\"Temperature\"], weatherClean.drop(columns = {\"Temperature\"})) #Build the full temp model\nfitModelPredTemp = modelPredTemp.fit(disp = 1) #Fit the SARIMAX model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weatherNoTemp = weatherClean.drop(columns = {\"Temperature\"}) #Drop the temperature column so the model does not get mad\nweatherNoTemp = weatherNoTemp[len(weatherClean) - 365:] #Split a small section for the model\ntempShort = temp[len(dew)-365:] #Get a shortened section for graphing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecastTemp = fitModelPredTemp.forecast(steps = len(weatherNoTemp), exog = weatherNoTemp.values) #Predict the next year's temps\nax = forecastTemp.plot(subplots = True, color = \"red\", linewidth = 2, title = \"Temperature Forecast\") #Plot the temp forecast\ntempShort.plot(ax = ax, subplots = True, color = \"royalblue\") #Plot the temperature to the forecast\nplt.ylabel(\"Temperature (F)\") #Put the temperature label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.estesparkweather.net/archive_reports.php?date=202011\nprint(\"Actual (Taken from the source): 38.2\") #Print yesterday's actual temperature\nprint(\"Prediction: {}\".format(forecastTemp[\"2020-11-13\"])) #Print yesterday's prediction","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"The predictions seem pretty accurate based on previous years. The dewpoint prediction for yesterday was off, but it was more akin to dewpoints of previous years. Yesterday's average temperature was well predicted.\n\nThe missing data issues did not seem to cause any problems with prediction. The vast volume of data likely negated these sections of filled in data. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}