{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport collections\nimport seaborn as sns\nimport time\nfrom sklearn import metrics\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import (KNeighborsClassifier,\n                               NeighborhoodComponentsAnalysis)\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nimport tensorflow as tf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n\nrandom_state = 0\n\n# df = pd.read_csv('/kaggle/input/indoor-localization-using-ble-and-wifi/BLE_rssi_after_Kalman_Filter_Grid_1.5m_2.5m.csv')\n# df = pd.read_csv('/kaggle/input/indoor-localization-using-ble-and-wifi/BLE_rssi_after_FFT_Filter_Grid_1.5m_2.5m.csv')\ndf = pd.read_csv('/kaggle/input/indoor-localization-using-ble-and-wifi/BLE_rssi_after_FFT_Filter_Grid_1.5m_1.25m.csv')\ndf.sample(frac=1)\n\n\nX = df[['beacon' + str(i) for i in range(1, 6)]]\nY = df['location']\n# print(Y)\n# Split into train/test\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X, Y, test_size=0.3, stratify=Y,\n                     random_state=random_state)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"________________________________________________________________________________________________________________________________\n# Supervised methods with Dimensionality Reduction\n\nNeighborhood Components Analysis (NCA) tries to find a feature space such that a stochastic nearest neighbor algorithm will give the best accuracy. Like LDA, it is a supervised method. One can see that NCA enforces a clustering of the data that is visually meaningful despite the large reduction in dimension.","metadata":{}},{"cell_type":"markdown","source":"# Compare PCA and LDA for all data","metadata":{}},{"cell_type":"code","source":"# n_neighbors = 3\n\n\n# print(X)\ndim = len(X['beacon1'])\nn_classes = len(np.unique(Y))\n\n# Reduce dimension to 2 with PCA\npca = make_pipeline(StandardScaler(),\n                    PCA(n_components=2, random_state=random_state))\n\n# Reduce dimension to 2 with LinearDiscriminantAnalysis\nlda = make_pipeline(StandardScaler(),\n                    LinearDiscriminantAnalysis(n_components=2))\n\n# Reduce dimension to 2 with NeighborhoodComponentAnalysis\nnca = make_pipeline(StandardScaler(),\n                    NeighborhoodComponentsAnalysis(n_components=2,\n                                                   random_state=random_state))\n\npca_accuaracy = []\nlda_accuaracy = []\nnca_accuaracy = []\nk = []\n\n# n_neighbors = 35\nfor n_neighbors in range(1, 100):\n    # Use a nearest neighbor classifier to evaluate the methods\n    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n\n    # Make a list of the methods to be compared\n    # dim_reduction_methods = [('PCA', pca), ('LDA', lda), ('NCA', nca)]\n    dim_reduction_methods = [('PCA', pca), ('LDA', lda)]\n    k.append(n_neighbors)\n    # plt.figure()\n    for i, (name, model) in enumerate(dim_reduction_methods):\n\n        # Fit the method's model\n        model.fit(X_train, y_train)\n\n        # Fit a nearest neighbor classifier on the embedded training set\n        knn.fit(model.transform(X_train), y_train)\n\n        # Compute the nearest neighbor accuracy on the embedded test set\n        acc_knn = knn.score(model.transform(X_test), y_test)\n\n        X_embedded = model.transform(X)\n\n        if i == 0:\n            pca_accuaracy.append(acc_knn)\n        elif i == 1:\n            lda_accuaracy.append(acc_knn)\n\n            \n\n\nplt.plot(k, pca_accuaracy, 'b')\nplt.plot(k, lda_accuaracy, 'g')\n# plt.plot(k, nca_accuaracy, 'r')\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"K (number of neighbors)\")\nplt.legend(['PCA', 'LDA'], loc='lower right')\nplt.title(\"Accuracy with different number of neighbors\")","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compare PCA and LDA for 1100 rows of the dataaet","metadata":{}},{"cell_type":"code","source":"# df = df[:1100]\n# X_prim = df[:1100][['beacon' + str(i) for i in range(1, 6)]]\n# Y_prim = df[:1100]['location']\nX_prim = df[['beacon' + str(i) for i in range(1, 6)]]\nY_prim = df['location']\n# print(Y)\n# Split into train/test\nX_train_prim, X_test_prim, y_train_prim, y_test_prim = \\\n    train_test_split(X_prim, Y_prim, test_size=0.3, stratify=Y_prim,\n                     random_state=random_state)\ndim = len(X['beacon1'])\nn_classes = len(np.unique(Y_prim))\n\n# Reduce dimension to 2 with PCA\npca = make_pipeline(StandardScaler(),\n                    PCA(n_components=2, random_state=random_state))\n\n# Reduce dimension to 2 with LinearDiscriminantAnalysis\nlda = make_pipeline(StandardScaler(),\n                    LinearDiscriminantAnalysis(n_components=2))\n\n# Reduce dimension to 2 with NeighborhoodComponentAnalysis\nnca = make_pipeline(StandardScaler(),\n                    NeighborhoodComponentsAnalysis(n_components=2,\n                                                   random_state=random_state))\n\n\npca_accuaracy = []\nlda_accuaracy = []\nnca_accuaracy = []\nk = []\n\n# n_neighbors = 3\nfor n_neighbors in range(1, 10):\n    # Use a nearest neighbor classifier to evaluate the methods\n    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n\n    # Make a list of the methods to be compared\n    dim_reduction_methods = [('PCA', pca), ('LDA', lda), ('NCA', nca)]\n#     dim_reduction_methods = [('PCA', pca), ('LDA', lda)]\n    k.append(n_neighbors)\n    for i, (name, model) in enumerate(dim_reduction_methods):\n\n        # Fit the method's model\n        model.fit(X_train_prim, y_train_prim)\n\n        # Fit a nearest neighbor classifier on the embedded training set\n        knn.fit(model.transform(X_train_prim), y_train_prim)\n\n        # Compute the nearest neighbor accuracy on the embedded test set\n        acc_knn = knn.score(model.transform(X_test_prim), y_test_prim)\n#         print(y_test)\n        # Embed the data set in 2 dimensions using the fitted model\n        X_embedded = model.transform(X_prim)\n\n        if i == 0:\n            pca_accuaracy.append(acc_knn)\n        elif i == 1:\n            lda_accuaracy.append(acc_knn)\n        elif i == 2:\n            nca_accuaracy.append(acc_knn)\n\n\n\nplt.plot(k, pca_accuaracy, 'b')\nplt.plot(k, lda_accuaracy, 'g')\nplt.plot(k, nca_accuaracy, 'r')\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"K (number of neighbors)\")\nplt.legend(['PCA', 'LDA', 'NCA'], loc='lower right')\nplt.title(\"Accuracy with different number of neighbors\")","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# pca_lda_knn function","metadata":{}},{"cell_type":"code","source":"def pca_lda_knn(X, Y, X_train, X_test, y_train, y_test):\n    dim = len(X['beacon1'])\n    n_classes = len(np.unique(Y))\n    # Reduce dimension to 2 with PCA\n    pca = make_pipeline(StandardScaler(),\n                        PCA(n_components=2, random_state=random_state))\n\n    # Reduce dimension to 2 with LinearDiscriminantAnalysis\n    lda = make_pipeline(StandardScaler(),\n                        LinearDiscriminantAnalysis(n_components=2))\n\n    # Reduce dimension to 2 with NeighborhoodComponentAnalysis\n    nca = make_pipeline(StandardScaler(),\n                        NeighborhoodComponentsAnalysis(n_components=2,\n                                                       random_state=random_state))\n\n    pca_accuaracy = []\n    lda_accuaracy = []\n    nca_accuaracy = []\n    k = []\n    y_pred_pca = []\n    y_pred_lda = []\n    for n_neighbors in range(1, 100):\n        # Use a nearest neighbor classifier to evaluate the methods\n        knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n\n        # Make a list of the methods to be compared\n        # dim_reduction_methods = [('PCA', pca), ('LDA', lda), ('NCA', nca)]\n        dim_reduction_methods = [('PCA', pca), ('LDA', lda)]\n        k.append(n_neighbors)\n        for i, (name, model) in enumerate(dim_reduction_methods):\n\n            # Fit the method's model\n            model.fit(X_train, y_train)\n\n            # Fit a nearest neighbor classifier on the embedded training set\n            knn.fit(model.transform(X_train), y_train)\n\n            # Compute the nearest neighbor accuracy on the embedded test set\n            y_pred_each = model.transform(X_test)\n            acc_knn = knn.score(y_pred_each, y_test)\n            y_pred_each = knn.predict(model.transform(X_test))\n            X_embedded = model.transform(X)\n\n            if i == 0:\n                pca_accuaracy.append(acc_knn)\n                y_pred_pca.append(y_pred_each)\n            elif i == 1:\n                lda_accuaracy.append(acc_knn)\n                y_pred_lda.append(y_pred_each)\n    return y_pred_pca[pca_accuaracy.index(max(pca_accuaracy))], y_pred_lda[lda_accuaracy.index(max(lda_accuaracy))], max(pca_accuaracy), max(lda_accuaracy)\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(pca_lda_knn(X, Y, X_train, X_test, y_train, y_test))","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"_______________________________________________________________________________________________________________________________\n# Decision Tree\n\nDecision tree learning is one of the predictive modelling approaches used in statistics, data mining and machine learning. It uses a decision tree to go from observations about an item to conclusions about the item's target value","metadata":{}},{"cell_type":"code","source":"def Decision_Tree(X, Y, X_train, X_test, y_train, y_test):\n    clf = tree.DecisionTreeClassifier()\n    clf = clf.fit(X, Y)\n    clf.predict(X_test)\n    clf = tree.DecisionTreeClassifier()\n\n    #Train the model using the training sets\n    clf.fit(X_train, y_train)\n\n    #Predict the response for test dataset\n    y_pred = clf.predict(X_test)\n\n    return y_pred ,metrics.accuracy_score(y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(Decision_Tree(X, Y, X_train, X_test, y_train, y_test))","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"________________________________________________________________________________________________________________________________\n# Gradient Boosting\n\nGradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.When a decision tree is the weak learner, the resulting algorithm is called gradient boosted trees, which usually outperforms random forest. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.","metadata":{}},{"cell_type":"code","source":"def Gradian_Boosting(X, Y, X_train, X_test, y_train, y_test):\n    lr_list = [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1]\n    test_accuracy = []\n    best_y_pred = []\n    for learning_rate in lr_list:\n        gb_clf = GradientBoostingClassifier(n_estimators=20, learning_rate=learning_rate, max_features=2, max_depth=2, random_state=0)\n        gb_clf.fit(X_train, y_train)\n        y_pred = gb_clf.predict(X_test)\n        best_y_pred.append(y_pred)\n        test_accuracy.append(gb_clf.score(X_test, y_test))\n\n#     fig = plt.figure()\n#     ax = fig.add_axes([0,0,1,1])\n#     ax.bar(lr_list, test_accuracy)\n#     ax.set_xlabel('Learning Rate')\n#     ax.set_ylabel('Accuracy')\n#     ax.set_title('Accuracy value with different learning rates')\n#     plt.show()\n    return best_y_pred[test_accuracy.index(max(test_accuracy))], max(test_accuracy)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(Gradian_Boosting(X, Y, X_train, X_test, y_train, y_test))","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"________________________________________________________________________________________________________________________________\n# XGBoost Classifier\n\nXGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.","metadata":{}},{"cell_type":"code","source":"def XGBoost(X, Y, X_train, X_test, y_train, y_test):\n    xgb_clf = XGBClassifier()\n    xgb_clf.fit(X_train, y_train)\n    #Predict the response for test dataset\n    y_pred = xgb_clf.predict(X_test)\n    score = xgb_clf.score(X_test, y_test)\n    return y_pred, score","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(XGBoost(X, Y, X_train, X_test, y_train, y_test))","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"________________________________________________________________________________________________________________________________\n# k-Nearest Neighbors\n\nK-Nearest Neighbors algorithm is used for classification and regression. In both cases, the input consists of the k closest training examples in data set.","metadata":{}},{"cell_type":"code","source":"def KNN(X, Y, X_train, X_test, y_train, y_test):\n    dict_classifications = {}\n    for i in range(1, 20):\n        #Create KNN Classifier\n        knn = KNeighborsClassifier(n_neighbors=i)\n\n        #Train the model using the training sets\n        knn.fit(X_train, y_train)\n\n        #Predict the response for test dataset\n        y_pred = knn.predict(X_test)\n        # Model Evaluation for k=5\n\n        # Model Accuracy, how often is the classifier correct?\n        acc = metrics.accuracy_score(y_test, y_pred)\n        if acc not in dict_classifications:\n            dict_classifications[acc] = [y_test, y_pred, i]\n\n    od = collections.OrderedDict(sorted(dict_classifications.items()))\n    \n    return od[list(od.keys())[0]][1], list(od.keys())[0], od[list(od.keys())[0]][2]\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(KNN(X, Y, X_train, X_test, y_train, y_test))","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"________________________________________________________________________________________________________________________________\n# Logistic Regression\n\nLogistic model is used to model the probability of a certain class or event existing such as pass/fail, win/lose, alive/dead or healthy/sick. This can be extended to model several classes of events such as determining whether an image contains a cat, dog, lion, etc","metadata":{}},{"cell_type":"code","source":"def Logistic_Regression(X, Y, X_train, X_test, y_train, y_test):\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n\n    solvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n    scores = []\n    score = 0\n    for i in range(len(solvers)):\n        model = LogisticRegression(solver= solvers[i])\n        model.fit(X_train, y_train)\n        score = model.score(X_test, y_test)\n        if scores == []:\n            scores = [score, solvers[i], model.predict(X_test)]\n        elif score > scores[0]:\n            scores = [score, solvers[i], model.predict(X_test)]\n    return scores[2], scores[0], scores[1]\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(Logistic_Regression(X, Y, X_train, X_test, y_train, y_test))","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"________________________________________________________________________________________________________________________________\n# Support Vector Machine (SVM)\n\nSVM is a supervised machine learning model that uses classification algorithms for two-group classification problems. After giving an SVM model sets of labeled training data for each category, theyâ€™re able to categorize new text","metadata":{}},{"cell_type":"code","source":"def SVM(X, Y, X_train, X_test, y_train, y_test):\n    # kernel_type = ['linear', 'poly', 'rbf', 'sigmoid']\n    kernel_type = ['rbf', 'sigmoid']\n    # gamma_type = ['auto']\n    gamma_type = ['scale', 'auto']\n\n    scores = []\n\n    for i in range(len(kernel_type)):\n        for j in range(len(gamma_type)):\n            for k in range(1, 5):\n                model = svm.SVC(C=k, gamma=gamma_type[j], kernel=kernel_type[i])\n                model.fit(X_train, y_train)\n                score = model.score(X_test, y_test)\n                if scores == []:\n                    scores = [score, k, gamma_type[j], kernel_type[i], model.predict(X_test)]\n                elif score > scores[0]:\n                    scores = [score, k, gamma_type[j], kernel_type[i], model.predict(X_test)]\n\n#     for i in range(5):\n#         print(scores[i])\n    return scores[4], scores[0], scores[0]  \n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def SVM_rbf(X, Y, X_train, X_test, y_train, y_test):\n    clf = svm.SVC(kernel='rbf') # RBF Kernel\n\n    #Train the model using the training sets\n    clf.fit(X_train, y_train)\n\n    #Predict the response for test dataset\n    y_pred = clf.predict(X_test)\n\n    return y_pred, metrics.accuracy_score(y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(SVM_rbf(X, Y, X_train, X_test, y_train, y_test))","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"________________________________________________________________________________________________________________________________\n# Function for loading LSTM NN model","metadata":{}},{"cell_type":"code","source":"def load_LSTM_NN(X, Y, X_train, X_test, y_train, y_test):\n    \n    model = tf.keras.models.load_model('LSTM_saved_model.h5')\n    model.summary()\n    one_hot = pd.get_dummies(df['location'])\n    x = df[['beacon1', 'beacon2', 'beacon3', 'beacon4', 'beacon5']].to_numpy()\n    x = x[:6350].reshape([-1, 5, 5])\n    y = one_hot[:6350].to_numpy()\n    y = y.reshape([-1, 5, 55])\n    y = np.sum(y, 1)\n    for label in y:\n        max_value_index = np.argmax(label)\n        label[max_value_index] = 1\n        label[0:max_value_index] = 0\n        label[max_value_index + 1:] = 0\n    X_train_prim, X_test_prim, y_train_prim, y_test_prim = train_test_split(x, y, test_size=0.3, random_state=42)\n\n    Y_pred = model.predict(X_test_prim)\n    y_pred = np.argmax(Y_pred, axis=1)\n#     print(y_pred)\n    score, accuracy = model.evaluate(X_test_prim, y_test_prim)\n\n    return y_pred, accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(load_LSTM_NN(X, Y, X_train, X_test, y_train, y_test))","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"________________________________________________________________________________________________________________________________\n________________________________________________________________________________________________________________________________\n# Compare all mentioned supervised learning algorithms for different number of rows of the dataset","metadata":{}},{"cell_type":"markdown","source":"# Step1: \nAt first, a list is generated which includes dataframes with different number of rows and labels","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/indoor-localization-using-ble-and-wifi/BLE_rssi_after_FFT_Filter_Grid_1.5m_1.25m.csv')\n\ndf.sample(frac=1)\n\nloc_list = [df['location'][0]]\n\nfor item in df['location']:\n    if item != loc_list[-1]:\n        loc_list.append(item)\nnew_df = df\nnew_loc_list = loc_list\nall_df = []\nglobal_counter = 0\nfor i in range(len(loc_list)):\n    df_prim = []\n    counter = 0\n    flag = 0\n    j = 0\n    while flag == 0 and global_counter < len(df['location']):\n        if new_df['location'][global_counter] != new_loc_list[0]:\n            flag = 1\n        else:\n            global_counter += 1\n        j += 1\n    new_df = new_df.iloc[counter + 1:]\n    new_loc_list = new_loc_list[1:]\n    all_df.append(df.iloc[0:global_counter])\n# print(len(all_df[0]['location']))\n# for i in range(len(all_df)):\n#     print(all_df[i])\n#     print('************************************************')","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step2:\nThis code has been provided in order to illustrate accuracy and elapsed time for each classification method for different number of rows of the dataset ","metadata":{}},{"cell_type":"code","source":"dataframes = []\naccuracy_pca_knn = []\naccuracy_lda_knn = []\naccuracy_Decision_Tree = []\naccuracy_Gradian_Boosting = []\naccuracy_XGBoost = []\naccuracy_KNN = []\naccuracy_Logistic_Regression = []\naccuracy_SVM = []\n# accuracy_LSTM_NN = []\nnum_of_rows = []\n\ntime_pca_lda_knn = []\n\ntime_Decision_Tree = []\ntime_Gradian_Boosting = []\ntime_XGBoost = []\ntime_KNN = []\ntime_Logistic_Regression = []\ntime_SVM = []\n# time_LSTM_NN = []\n\n\nrandom_state = 0\n\n# df = pd.read_csv('/kaggle/input/indoor-localization-using-ble-and-wifi/BLE_rssi_after_FFT_Filter_Grid_1.5m_2.5m.csv')\ndf = pd.read_csv('/kaggle/input/indoor-localization-using-ble-and-wifi/BLE_rssi_after_FFT_Filter_Grid_1.5m_1.25m.csv')\n\ndf.sample(frac=1)\nprint(df.shape)\n\n# accuracy_LSTM_NN = [0] * int(len(all_df) / 4)\naccuracy_LSTM_NN = 0\ntime_LSTM_NN = 0\nX = df[['beacon' + str(i) for i in range(1, 6)]]\nY = df['location']\n# print(Y)\n# Split into train/test\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X, Y, test_size=0.3, stratify=Y,\n                     random_state=random_state)\n\n    \n    \nfor i in range(int(len(all_df)/4)):\n    if i == int(len(all_df)/4) - 1:\n        df = all_df[-1]\n    else:\n        df = all_df[i * 4 + 3]\n    X = df[['beacon' + str(j) for j in range(1, 6)]]\n    Y = df['location']\n    # print(Y)\n    # Split into train/test\n    X_train, X_test, y_train, y_test = \\\n    train_test_split(X, Y, test_size=0.3, stratify=Y,\n                     random_state=random_state)\n    \n    # execute PCA and LDA for KNN classifier\n    time_1 = time.time()\n    pca_y_pred, lda_y_pred, pca_accuracy, lda_accuracy = pca_lda_knn(X, Y, X_train, X_test, y_train, y_test)\n    time_2 = time.time()\n    time_pca_lda_knn.append(time_2 - time_1)\n    accuracy_pca_knn.append(pca_accuracy)\n    accuracy_lda_knn.append(lda_accuracy)\n\n    \n    # execute Decision Tree classifier\n    Decision_Tree_y_pred, Decision_Tree_accuracy = Decision_Tree(X, Y, X_train, X_test, y_train, y_test)\n    time_3 = time.time()\n    time_Decision_Tree.append(time_3 - time_2)\n    accuracy_Decision_Tree.append(Decision_Tree_accuracy)\n\n    # execute Gradian Boosting classifier\n    Gradian_Boosting_y_pred, Gradian_Boosting_accuracy = Gradian_Boosting(X, Y, X_train, X_test, y_train, y_test)\n    time_4 = time.time()\n    time_Gradian_Boosting.append(time_4 - time_3)\n    accuracy_Gradian_Boosting.append(Gradian_Boosting_accuracy)\n    \n    # execute XGBoosting classifier\n    XGBoost_y_pred, XGBoost_accuracy = XGBoost(X, Y, X_train, X_test, y_train, y_test)\n    time_5 = time.time()\n    time_XGBoost.append(time_5 - time_4)\n    accuracy_XGBoost.append(XGBoost_accuracy)\n    \n    # execute KNN classifier\n    KNN_y_pred, KNN_accuracy, num_of_k_neighbors = KNN(X, Y, X_train, X_test, y_train, y_test)\n    time_6 = time.time()\n    time_KNN.append(time_6 - time_5)\n    accuracy_KNN.append(KNN_accuracy)\n    \n    # execute Logistic_Regression classifier\n    Logistic_Regression_y_pred, Logistic_Regression_accuracy, Logistic_Regression_solver = Logistic_Regression(X, Y, X_train, X_test, y_train, y_test)\n    time_7 = time.time()\n    time_Logistic_Regression.append(time_7 - time_6)\n    accuracy_Logistic_Regression.append(Logistic_Regression_accuracy)\n    \n    # execute SVM classifier\n    SVM_y_pred, SVM_accuracy = SVM_rbf(X, Y, X_train, X_test, y_train, y_test)\n    time_8 = time.time()\n    time_SVM.append(time_8 - time_7)\n    accuracy_SVM.append(SVM_accuracy)\n\n    \n    num_of_rows.append(len(all_df[i * 4 + 3]['location']))\n\n#     if i == int(len(all_df)/4) - 1:\n#         time_9 = time.time()\n#         LSTM_NN_y_pred, LSTM_NN_accuracy = load_LSTM_NN(X, Y, X_train, X_test, y_train, y_test)\n#         time_10 = time.time()\n#         time_LSTM_NN = time_10 - time_9\n#         accuracy_LSTM_NN = LSTM_NN_accuracy\n#         num_of_rows.append(len(all_df[-1]['location']))\n#     else:\n#         num_of_rows.append(len(all_df[i * 4 + 3]['location']))\n    \n\n# b g r c m y k w\nplt.plot(num_of_rows, accuracy_pca_knn, 'b')\nplt.plot(num_of_rows, accuracy_lda_knn, 'g')\nplt.plot(num_of_rows, accuracy_Decision_Tree, 'r')\nplt.plot(num_of_rows, accuracy_Gradian_Boosting, 'c')\nplt.plot(num_of_rows, accuracy_XGBoost, 'm')\nplt.plot(num_of_rows, accuracy_KNN, 'k')\nplt.plot(num_of_rows, accuracy_Logistic_Regression, 'y')\nplt.plot(num_of_rows, accuracy_SVM, 'grey')\n# plt.plot(num_of_rows, accuracy_LSTM_NN, 'w')\n# plt.scatter([num_of_rows[-1]], [accuracy_LSTM_NN], color='g')\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Number of rows\")\nplt.legend(['PCA', 'LDA', 'DT', 'GB', 'XGB', 'KNN', 'LR', 'SVM'], loc='upper right', borderaxespad=0., prop={'size': 7}, fontsize=8)\nplt.title(\"Accuracy for different number rows\")\nplt.show()\n\n\n\nplt.plot(num_of_rows, time_pca_lda_knn, 'b')\n# plt.plot(num_of_rows, time_lda_knn, 'g')\nplt.plot(num_of_rows, time_Decision_Tree, 'r')\nplt.plot(num_of_rows, time_Gradian_Boosting, 'c')\nplt.plot(num_of_rows, time_XGBoost, 'm')\nplt.plot(num_of_rows, time_KNN, 'k')\nplt.plot(num_of_rows, time_Logistic_Regression, 'y')\nplt.plot(num_of_rows, time_SVM, 'grey')\n# plt.plot(num_of_rows, accuracy_LSTM_NN, 'w')\n# plt.scatter([num_of_rows[-1]], [time_LSTM_NN], color='g')\nplt.ylabel(\"time(sec)\")\nplt.xlabel(\"Number of rows\")\nplt.legend(['PCA', 'DT', 'GB', 'XGB', 'KNN', 'LR', 'SVM'], loc='upper left', borderaxespad=0.)\nplt.title(\"Elapsed time for different number of rows\")\nplt.show()\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"________________________________________________________________________________________________________________________________\n\n# Compare filtering rssi noise algorithms using KNN and XGBoosting learning mothods","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nrandom_state = 0\n\n\ndf = pd.read_csv('/kaggle/input/indoor-localization-using-ble-and-wifi/BLE_rssi_Before_Filter_Grid_1.5m_1.25m.csv')\ndf.sample(frac=1)\n\ndf_0 = pd.read_csv('/kaggle/input/indoor-localization-using-ble-and-wifi/BLE_rssi_after_Kalman_Filter_Grid_1.5m_1.25m.csv')\ndf_0.sample(frac=1)\n\ndf_1 = pd.read_csv('/kaggle/input/indoor-localization-using-ble-and-wifi/BLE_rssi_after_FFT_Filter_Grid_1.5m_1.25m.csv')\ndf_1.sample(frac=1)\n\ndf_2 = pd.read_csv('/kaggle/input/indoor-localization-using-ble-and-wifi/BLE_rssi_after_Gray_Filter_Grid_1.5m_1.25m.csv')\ndf_2.sample(frac=1)\n\n# df_3 = pd.read_csv('/kaggle/input/indoor-localization-using-ble-and-wifi/BLE_rssi_after_Particle_Filter_Grid_1.5m_1.25m.csv')\n# df_3.sample(frac=1)\n\nX = df[['beacon' + str(i) for i in range(1, 6)]]\nY = df['location']\n\nX_0 = df_0[['beacon' + str(i) for i in range(1, 6)]]\nY_0 = df_0['location']\n\nX_1 = df_1[['beacon' + str(i) for i in range(1, 6)]]\nY_1 = df_1['location']\n\nX_2 = df_2[['beacon' + str(i) for i in range(1, 6)]]\nY_2 = df_2['location']\n\n# X_3 = df_3[['beacon' + str(i) for i in range(1, 6)]]\n# Y_3 = df_3['location']\n\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X, Y, test_size=0.3, stratify=Y,\n                     random_state=random_state)\n\nX_train_0, X_test_0, y_train_0, y_test_0 = \\\ntrain_test_split(X_0, Y_0, test_size=0.3, stratify=Y_0,\n                 random_state=random_state)    \n    \nX_train_1, X_test_1, y_train_1, y_test_1 = \\\n    train_test_split(X_1, Y_1, test_size=0.3, stratify=Y_1,\n                     random_state=random_state)    \n\nX_train_2, X_test_2, y_train_2, y_test_2 = \\\n    train_test_split(X_2, Y_2, test_size=0.3, stratify=Y_2,\n                     random_state=random_state)\n\n# X_train_3, X_test_3, y_train_3, y_test_3 = \\\n#     train_test_split(X_3, Y_3, test_size=0.3, stratify=Y_3,\n#                      random_state=random_state)\n    \n\n# For df :\nXGBoost_y_pred, XGBoost_accuracy = XGBoost(X, Y, X_train, X_test, y_train, y_test)\nKNN_y_pred, KNN_accuracy, num_of_k_neighbors = KNN(X, Y, X_train, X_test, y_train, y_test)\nprint('KNN_accuracy')\nprint(KNN_accuracy)\nprint('******************************************************')\n\n# For df_0 :\nXGBoost_y_pred, XGBoost_accuracy_0 = XGBoost(X_0, Y_0, X_train_0, X_test_0, y_train_0, y_test_0)\nKNN_y_pred, KNN_accuracy_0, num_of_k_neighbors = KNN(X_0, Y_0, X_train_0, X_test_0, y_train_0, y_test_0)\nprint('KNN_accuracy 0 ')\nprint(KNN_accuracy_0)\nprint('******************************************************')\n\n\n# For df_1 :\nXGBoost_y_pred, XGBoost_accuracy_1 = XGBoost(X_1, Y_1, X_train_1, X_test_1, y_train_1, y_test_1)\nKNN_y_pred, KNN_accuracy_1, num_of_k_neighbors = KNN(X_1, Y_1, X_train_1, X_test_1, y_train_1, y_test_1)\nprint('KNN_accuracy 1 ')\nprint(KNN_accuracy_1)\nprint('******************************************************')\n\n\n# For df_2 :\nXGBoost_y_pred, XGBoost_accuracy_2 = XGBoost(X_2, Y_2, X_train_2, X_test_2, y_train_2, y_test_2)\nKNN_y_pred, KNN_accuracy_2, num_of_k_neighbors = KNN(X_2, Y_2, X_train_2, X_test_2, y_train_2, y_test_2)\nprint('KNN_accuracy 2 ')\nprint(KNN_accuracy_2)\nprint('******************************************************')\n\n\n# # For df_3 :\n# XGBoost_y_pred, XGBoost_accuracy_3 = XGBoost(X_3, Y_3, X_train_3, X_test_3, y_train_3, y_test_3)\n# # KNN_y_pred, KNN_accuracy_3, num_of_k_neighbors = KNN(X_3, Y_3, X_train_3, X_test_3, y_train_3, y_test_3)\n\n\n# plot data\nbarWidth = 0.25\nfig = plt.subplots(figsize =(12, 8))\nXGBoost_list = [XGBoost_accuracy,XGBoost_accuracy_0,XGBoost_accuracy_1,XGBoost_accuracy_2]\nKNN_list = [KNN_accuracy,KNN_accuracy_0,KNN_accuracy_1,KNN_accuracy_2]\n# Set position of bar on X axis\nbr1 = np.arange(len(XGBoost_list))\nbr2 = [x + barWidth for x in br1]\nplt.bar(br1, XGBoost_list, color ='r', width = barWidth,\n        edgecolor ='grey', label ='XGBoost')\nplt.bar(br2, KNN_list, color ='g', width = barWidth,\n        edgecolor ='grey', label ='KNN')\nplt.xlabel('Filtering Methods', fontweight ='bold', fontsize = 15)\nplt.ylabel('Accuracy', fontweight ='bold', fontsize = 15)\nplt.xticks([r + barWidth for r in range(len(XGBoost_list))],\n        ['Before Filter', 'Kalman Filter', 'FFT Filter', 'Gray Filter'])\n \nplt.legend()\nplt.title('Compare accuracy of three filtering method using XGBoost and KNN')\nplt.show()\n\nprint(XGBoost_accuracy_1)\nprint(KNN_accuracy_1)\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]}]}