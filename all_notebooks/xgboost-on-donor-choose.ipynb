{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# XGBoost on Donor Choose\n\n![Imgur](https://imgur.com/GZSklhL.png)\n\n# [DonorsChoose](https://en.wikipedia.org/wiki/DonorsChoose)\n\nDonorsChoose is a United States-based nonprofit organization that allows individuals to donate directly to public school classroom projects. The organization has been given Charity Navigator's highest rating every year since 2005.[4] In January 2018, they announced that 1 million projects had been funded. In 80% of public schools in the United States, at least one project has been requested on DonorsChoose. Schools from wealthy areas are more likely to make technology requests, while schools from less affluent areas are more likely to request basic supplies.[6] It has been noted that repeat donors on DonorsChoose typically donate to projects they have no prior relationship with, and most often fund projects serving financially challenged students.\n\n---\n\n# About the DonorsChoose Data Set\n\nThe `train.csv` data set provided by DonorsChoose contains the following features:\n\nFeature | Description \n----------|---------------\n**`project_id`** | A unique identifier for the proposed project. **Example:** `p036502`   \n**`project_title`**    | Title of the project. **Examples:**<br><ul><li><code>Art Will Make You Happy!</code></li><li><code>First Grade Fun</code></li></ul> \n**`project_grade_category`** | Grade level of students for which the project is targeted. One of the following enumerated values: <br/><ul><li><code>Grades PreK-2</code></li><li><code>Grades 3-5</code></li><li><code>Grades 6-8</code></li><li><code>Grades 9-12</code></li></ul>  \n **`project_subject_categories`** | One or more (comma-separated) subject categories for the project from the following enumerated list of values:  <br/><ul><li><code>Applied Learning</code></li><li><code>Care &amp; Hunger</code></li><li><code>Health &amp; Sports</code></li><li><code>History &amp; Civics</code></li><li><code>Literacy &amp; Language</code></li><li><code>Math &amp; Science</code></li><li><code>Music &amp; The Arts</code></li><li><code>Special Needs</code></li><li><code>Warmth</code></li></ul><br/> **Examples:** <br/><ul><li><code>Music &amp; The Arts</code></li><li><code>Literacy &amp; Language, Math &amp; Science</code></li>  \n  **`school_state`** | State where school is located ([Two-letter U.S. postal code](https://en.wikipedia.org/wiki/List_of_U.S._state_abbreviations#Postal_codes)). **Example:** `WY`\n**`project_subject_subcategories`** | One or more (comma-separated) subject subcategories for the project. **Examples:** <br/><ul><li><code>Literacy</code></li><li><code>Literature &amp; Writing, Social Sciences</code></li></ul> \n**`project_resource_summary`** | An explanation of the resources needed for the project. **Example:** <br/><ul><li><code>My students need hands on literacy materials to manage sensory needs!</code</li></ul> \n**`project_essay_1`**    | First application essay<sup>*</sup>  \n**`project_essay_2`**    | Second application essay<sup>*</sup> \n**`project_essay_3`**    | Third application essay<sup>*</sup> \n**`project_essay_4`**    | Fourth application essay<sup>*</sup> \n**`project_submitted_datetime`** | Datetime when project application was submitted. **Example:** `2016-04-28 12:43:56.245`   \n**`teacher_id`** | A unique identifier for the teacher of the proposed project. **Example:** `bdf8baa8fedef6bfeec7ae4ff1c15c56`  \n**`teacher_prefix`** | Teacher's title. One of the following enumerated values: <br/><ul><li><code>nan</code></li><li><code>Dr.</code></li><li><code>Mr.</code></li><li><code>Mrs.</code></li><li><code>Ms.</code></li><li><code>Teacher.</code></li></ul>  \n**`teacher_number_of_previously_posted_projects`** | Number of project applications previously submitted by the same teacher. **Example:** `2` \n\n<sup>*</sup> See the section <b>Notes on the Essay Data</b> for more details about these features.\n\nAdditionally, the `resources.csv` data set provides more data about the resources required for each project. Each line in this file represents a resource required by a project:\n\nFeature | Description \n----------|---------------\n**`id`** | A `project_id` value from the `train.csv` file.  **Example:** `p036502`   \n**`description`** | Desciption of the resource. **Example:** `Tenor Saxophone Reeds, Box of 25`   \n**`quantity`** | Quantity of the resource required. **Example:** `3`   \n**`price`** | Price of the resource required. **Example:** `9.95`   \n\n**Note:** Many projects require multiple resources. The `id` value corresponds to a `project_id` in train.csv, so you use it as a key to retrieve all resources needed for a project:\n\nThe data set contains the following label (the value you will attempt to predict):\n\nLabel | Description\n----------|---------------\n`project_is_approved` | A binary flag indicating whether DonorsChoose approved the project. A value of `0` indicates the project was not approved, and a value of `1` indicates the project was approved.","metadata":{"id":"8wqapAXjsc24"}},{"cell_type":"code","source":"!nvidia-smi\n","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:25:41.822407Z","iopub.execute_input":"2021-07-06T09:25:41.82275Z","iopub.status.idle":"2021-07-06T09:25:42.523638Z","shell.execute_reply.started":"2021-07-06T09:25:41.822718Z","shell.execute_reply":"2021-07-06T09:25:42.522742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('../input/donorschooseorg-application-screening'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfor dirname, _, filenames in os.walk('../input/preprocesseddonorchoose'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n        \nfor dirname, _, filenames in os.walk('../input/glove-vector'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:25:42.525956Z","iopub.execute_input":"2021-07-06T09:25:42.526258Z","iopub.status.idle":"2021-07-06T09:25:42.541662Z","shell.execute_reply.started":"2021-07-06T09:25:42.52623Z","shell.execute_reply":"2021-07-06T09:25:42.540671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## For checking how I am getting the completely pre-processed and cleaned .csv file 'preprocessed_data.csv' you can check my [this notebook](https://www.kaggle.com/paulrohan2020/decision-trees-on-donors-choose) which has the detailed EDA and the pre-processeing part\n\n## 1.1 Loading Data","metadata":{"id":"VWge-b2Zsc3Z"}},{"cell_type":"code","source":"# COLAB WITH GOOGLE DRIVE\n# !pip install wordcloud\n# from google.colab import drive\n# drive.mount('/content/gdrive')\n# root_path = '/content/gdrive/MyDrive/DonorChoose/preprocessed_data.csv'\n# glove_vector_path = '/content/gdrive/MyDrive/DonorChoose/glove_vectors'\n\n#KAGGLE\nroot_path = '../input/preprocesseddonorchoose/preprocessed_data.csv'\nglove_vector_path = '../input/glove-vector/glove_vectors'\n\n# LOCAL MACHINE\n# root_path = '../LARGE_Datasets/preprocessed_data.csv'\n# glove_vector_path = '../LARGE_Datasets/glove_vectors'\n\nimport warnings \nwarnings.filterwarnings(\"ignore\") \n\nimport pandas as pd\nimport numpy as np\n\nfrom tqdm import tqdm\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, Normalizer\nfrom sklearn.model_selection import GridSearchCV\nfrom prettytable import PrettyTable\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom scipy.sparse import hstack\nfrom sklearn.tree import DecisionTreeClassifier\nimport seaborn as sns\nsns.set()\n\nimport pickle\nimport nltk\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import expon\n\nfrom collections import Counter\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\n\n\n# Save a Notebook session: (I could use .pkl as well to dump, but from .pkl could NOT load back a large dump size of 2.4GB)\n# import dill\n# dill.dump_session('notebook_env.db')\n\n# Restore a Notebook session:\n# import dill\n# dill.load_session('notebook_env.db')\n\norg_preprocessed = pd.read_csv(root_path)\norg_preprocessed.head(2)","metadata":{"id":"WDZFu6-usc3d","execution":{"iopub.status.busy":"2021-07-06T09:25:42.54331Z","iopub.execute_input":"2021-07-06T09:25:42.543563Z","iopub.status.idle":"2021-07-06T09:25:44.529418Z","shell.execute_reply.started":"2021-07-06T09:25:42.543538Z","shell.execute_reply":"2021-07-06T09:25:44.528389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sentiment Analysis on 'essay'\nsid    = SentimentIntensityAnalyzer()\n\nnegative_sentiments = []\npositive_sentiments = []\nneutral_sentiments = []\ncompound_sentiments = []\n\nfor i in tqdm(org_preprocessed['essay']):\n  sid_sentiments = sid.polarity_scores(i)\n  negative_sentiments.append(sid_sentiments['neg'])\n  positive_sentiments.append(sid_sentiments['pos'])\n  neutral_sentiments.append(sid_sentiments['neu'])\n  compound_sentiments.append(sid_sentiments['compound'])\n  \n# Now append these sentiments columns/freatures to original preprocessed dataframe\norg_preprocessed['negative_sent'] = negative_sentiments\norg_preprocessed['positive_sent'] = positive_sentiments\norg_preprocessed['neutral_sent'] = neutral_sentiments\norg_preprocessed['compound_sent'] = compound_sentiments\n\norg_preprocessed.head(1)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:25:44.53069Z","iopub.execute_input":"2021-07-06T09:25:44.531024Z","iopub.status.idle":"2021-07-06T09:29:30.171358Z","shell.execute_reply.started":"2021-07-06T09:25:44.530996Z","shell.execute_reply":"2021-07-06T09:29:30.170111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y = org_preprocessed['project_is_approved'].values\nX = org_preprocessed.drop(['project_is_approved'], axis=1)\nprint(Y.shape)\nprint(X.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:29:30.174626Z","iopub.execute_input":"2021-07-06T09:29:30.174938Z","iopub.status.idle":"2021-07-06T09:29:30.192616Z","shell.execute_reply.started":"2021-07-06T09:29:30.174911Z","shell.execute_reply":"2021-07-06T09:29:30.191707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>1.2 Splitting data into Train and cross validation(or test): Stratified Sampling</h2>","metadata":{"id":"6lAMFKFcsc3j"}},{"cell_type":"code","source":"\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)\n\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","metadata":{"id":"ODMijuuNsc3l","execution":{"iopub.status.busy":"2021-07-06T09:29:30.195319Z","iopub.execute_input":"2021-07-06T09:29:30.195648Z","iopub.status.idle":"2021-07-06T09:29:30.228603Z","shell.execute_reply.started":"2021-07-06T09:29:30.195622Z","shell.execute_reply":"2021-07-06T09:29:30.227911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Set 1 - : \n\n### - categorical (instead of one hot encoding, try <a href='https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/handling-categorical-and-numerical-features/'>response coding</a>: use probability values) + \n### - numerical features + \n### - project_title(TFIDF) +  \n### - preprocessed_eassay (TFIDF)+ \n### - sentiment Score of eassay( include all 4 values as 4 features)\n\n# 1.3 Make Data Model Ready: encoding eassay (BoW)\n\n","metadata":{"id":"vsALgl5Asc3u"}},{"cell_type":"code","source":"# As required for Task-1, applying TFIDF on the Essay column\nvectorizer_essay_tfidf = TfidfVectorizer(min_df=10)\n\n# Apply .fit() on this vectorizer on Train data\n# Note .fit() is applied only on the train data, as test and cv should not be fitted\nvectorizer_essay_tfidf.fit(X_train['essay'].values)\n\n# Now use the fitted TfidfVectorizer for converting 'essay' text to Vector form\nX_train_vectorized_tfidf_essay = vectorizer_essay_tfidf.transform(X_train['essay'].values)\nX_test_vectorized_tfidf_essay = vectorizer_essay_tfidf.transform(X_test['essay'].values)\n\nprint('After TFIDF on Essay column checking the shapes ')\nprint(X_train_vectorized_tfidf_essay.shape, y_train.shape)\nprint(X_test_vectorized_tfidf_essay.shape, y_test.shape)","metadata":{"id":"1A_85jbWsc3v","execution":{"iopub.status.busy":"2021-07-06T09:29:30.229793Z","iopub.execute_input":"2021-07-06T09:29:30.23014Z","iopub.status.idle":"2021-07-06T09:29:59.069663Z","shell.execute_reply.started":"2021-07-06T09:29:30.230102Z","shell.execute_reply":"2021-07-06T09:29:59.067973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1.4 Make Data Model Ready: encoding categorical features\n\n## Categorial Featues are => school_state, teacher_prefix, project_grade_category, clean_categories, clean_subcategories, essay\n\n## The function to implement response-encoding by first doint fit and then transform the categorical features on train and test data.","metadata":{"id":"S3Vxxv2Hsc31"}},{"cell_type":"code","source":"\n\"\"\" Function to response-encode categorical features - fit()\n\nBasically generate three Counter dict for total of a feature and \nfeature_when_class_is_0 and feature_when_class_is_1\n\nargs:\n  x_train_feature_total => df of the form (from Training data) => x['feature_name'] \n  x_train_feature_0 => of form (from Training data) => x.loc[x['project_is_approved']== 0]['feature_name']\n  x_train_feature_1 => of form (from Training data) => x.loc[x['project_is_approved']== 1]['feature_name']\n  \n  returns:\n     dictionary (dict) => Of form => Counter({'ca': 53, 'in': 21,... }) for \"school_state\" feature\n\"\"\"\n\n\ndef response_encoding_fit(x_train_feature_total, x_train_feature_0, x_train_feature_1):\n  feature_counter_total = Counter()\n  feature_counter_total.update(i for i in x_train_feature_total)\n  # For 'school_state' feature, above will generate a counter dict like below\n  # Includes both class 0 and 1 in this\n  # ({'ca': 469, 'mi': 80, 'ny': 199, ... })\n  # Noting The counter is a sub-class available inside the dictionary class. Using Counter tool, \n  # we can count the key-value pairs in an object, also called a hash table object.\n  \n  feature_counter_0 = Counter()  # Create a dict variable to act as a Counter\n  feature_counter_0.update(i for i in x_train_feature_0)   # adding values to the Counter from x_train_feature_0\n  # it will be of form => Counter({'fl': 2, 'nv': 1, 'ca': 9,...})\n  \n  # Now update feature_counter_0 with all the non-existing keys\n  # i.e. keys that exist in feature_counter_total but NOT in feature_counter_0\n  for i in feature_counter_total:         # i is each key (e.g. 'ca', 'fl' etc for school_state)\n    if i not in feature_counter_0:        # If a key is not there in feature_counter_0\n      feature_counter_0[i] = 0            # set the value of that key be 0 in dict feature_counter_0\n    \n  # Similary do the same for x_train_feature_1\n  feature_counter_1 = Counter()\n  feature_counter_1.update(i for i in x_train_feature_1)\n    \n  for i in feature_counter_total:\n    if i not in feature_counter_1:\n      feature_counter_1[i] = 0\n      \n  return feature_counter_total, feature_counter_0, feature_counter_1\n\n\"\"\" Now Function to tranform (generate proba array) for response-encoded categorical features\n\nargs:\n  x_feature_train => X_train['feature_name']\n  feature_counter_0 and feature_counter_1 => These are Counter / dict variable returned from response_encoding_fit()  \n  \n  returns:\n     List of Probabilities => Of form => \n     [[0.04761905]\n      [0.16981132]\n      [0.16981132]]\n\"\"\"\n\ndef response_encoding_transform(x_feature_train, feature_counter_total, feature_counter_0, feature_counter_1):\n  feature_proba_arr_0 = []\n  feature_proba_arr_1 = []\n  \n  for i in x_feature_train:\n    # Now loop over each feature-name e.g. 'ca', 'fl' etc for school_state\n    if i in feature_counter_total.keys(): # if the specific unique feature-names exist in train data x_feature_train\n      # .get(i) will give me the value of the key, i.e. the number count for each key (which represents the feature-name)\n      proba_0 = feature_counter_0.get(i)/feature_counter_total.get(i) \n      proba_1 = feature_counter_1.get(i)/feature_counter_total.get(i)      \n      \n      feature_proba_arr_0.append(proba_0)\n      feature_proba_arr_1.append(proba_1)\n    else:\n      feature_proba_arr_0.append(0.5)\n      feature_proba_arr_1.append(0.5)      \n  # Have to convert to array so I can invoke reshape() on these\n  feature_proba_arr_0 = np.array(feature_proba_arr_0)\n  feature_proba_arr_1 = np.array(feature_proba_arr_1)\n    \n  return feature_proba_arr_0.reshape(-1, 1), feature_proba_arr_1.reshape(-1, 1)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:29:59.071073Z","iopub.execute_input":"2021-07-06T09:29:59.071446Z","iopub.status.idle":"2021-07-06T09:29:59.082227Z","shell.execute_reply.started":"2021-07-06T09:29:59.071408Z","shell.execute_reply":"2021-07-06T09:29:59.081367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now make a new dataframe for all the categorical feature from only the train dataset\n# And then I will response-encode these dataset.\n# Categorial Featues are => school_state, teacher_prefix, project_grade_category, clean_categoris, clean_subcategories\ndf_cat_train_before_response_coding = pd.DataFrame(y_train, columns=['project_is_approved'])\ndf_cat_train_before_response_coding['school_state'] = X_train['school_state'].values\ndf_cat_train_before_response_coding['teacher_prefix'] = X_train['teacher_prefix'].values\ndf_cat_train_before_response_coding['project_grade_category'] = X_train['project_grade_category'].values\ndf_cat_train_before_response_coding['clean_categories'] = X_train['clean_categories'].values\ndf_cat_train_before_response_coding['clean_subcategories'] = X_train['clean_subcategories'].values\ndf_cat_train_before_response_coding.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:29:59.08394Z","iopub.execute_input":"2021-07-06T09:29:59.084301Z","iopub.status.idle":"2021-07-06T09:29:59.108059Z","shell.execute_reply.started":"2021-07-06T09:29:59.084264Z","shell.execute_reply":"2021-07-06T09:29:59.107144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Response Encoding school_state","metadata":{}},{"cell_type":"code","source":"x_train_feature_total = df_cat_train_before_response_coding['school_state']\nx_train_feature_0 = df_cat_train_before_response_coding.loc[df_cat_train_before_response_coding['project_is_approved']==0]['school_state']\nx_train_feature_1 = df_cat_train_before_response_coding.loc[df_cat_train_before_response_coding['project_is_approved']==1]['school_state']\n\nschool_state_counter_total, school_state_counter_0, school_state_counter_1 = response_encoding_fit(x_train_feature_total, x_train_feature_0, x_train_feature_1 )\n\nX_train_school_state_response_proba_0, X_train_school_state_response_proba_1 = response_encoding_transform(X_train['school_state'], school_state_counter_total, school_state_counter_0, school_state_counter_1)\n\nX_test_school_state_response_proba_0, X_test_school_state_response_proba_1 = response_encoding_transform(X_test['school_state'], school_state_counter_total, school_state_counter_0, school_state_counter_1)\n\nprint(np.mean(X_train_school_state_response_proba_0, axis=0))\nprint(X_train_school_state_response_proba_0.shape, y_train.shape)\nprint(X_test_school_state_response_proba_0.shape, y_test.shape)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:29:59.109307Z","iopub.execute_input":"2021-07-06T09:29:59.109641Z","iopub.status.idle":"2021-07-06T09:29:59.271848Z","shell.execute_reply.started":"2021-07-06T09:29:59.109607Z","shell.execute_reply":"2021-07-06T09:29:59.27099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Response Encoding teacher_prefix","metadata":{}},{"cell_type":"code","source":"x_train_feature_total = df_cat_train_before_response_coding['teacher_prefix']\nx_train_feature_0 = df_cat_train_before_response_coding.loc[df_cat_train_before_response_coding['project_is_approved']==0]['teacher_prefix']\nx_train_feature_1 = df_cat_train_before_response_coding.loc[df_cat_train_before_response_coding['project_is_approved']==1]['teacher_prefix']\n\nteacher_prefix_counter_total, teacher_prefix_counter_0, teacher_prefix_counter_1 = response_encoding_fit(x_train_feature_total, x_train_feature_0, x_train_feature_1 )\n\nX_train_teacher_prefix_response_proba_0, X_train_teacher_prefix_response_proba_1 = response_encoding_transform(X_train['teacher_prefix'], teacher_prefix_counter_total, teacher_prefix_counter_0, teacher_prefix_counter_1)\n\nX_test_teacher_prefix_response_proba_0, X_test_teacher_prefix_response_proba_1 = response_encoding_transform(X_test['teacher_prefix'], teacher_prefix_counter_total, teacher_prefix_counter_0, teacher_prefix_counter_1)\n\nprint(np.mean(X_train_teacher_prefix_response_proba_0, axis=0))\nprint(X_train_teacher_prefix_response_proba_0.shape, y_train.shape)\nprint(X_test_teacher_prefix_response_proba_0.shape, y_test.shape)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:29:59.273538Z","iopub.execute_input":"2021-07-06T09:29:59.273957Z","iopub.status.idle":"2021-07-06T09:29:59.436206Z","shell.execute_reply.started":"2021-07-06T09:29:59.273922Z","shell.execute_reply":"2021-07-06T09:29:59.435325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Response Encoding project_grade_category","metadata":{}},{"cell_type":"code","source":"x_train_feature_total = df_cat_train_before_response_coding['project_grade_category']\nx_train_feature_0 = df_cat_train_before_response_coding.loc[df_cat_train_before_response_coding['project_is_approved']==0]['project_grade_category']\nx_train_feature_1 = df_cat_train_before_response_coding.loc[df_cat_train_before_response_coding['project_is_approved']==1]['project_grade_category']\n\nproject_grade_category_counter_total, project_grade_category_counter_0, project_grade_category_counter_1 = response_encoding_fit(x_train_feature_total, x_train_feature_0, x_train_feature_1 )\n\nX_train_project_grade_category_response_proba_0, X_train_project_grade_category_response_proba_1 = response_encoding_transform(X_train['project_grade_category'], project_grade_category_counter_total, project_grade_category_counter_0, project_grade_category_counter_1)\n\nX_test_project_grade_category_response_proba_0, X_test_project_grade_category_response_proba_1 = response_encoding_transform(X_test['project_grade_category'], project_grade_category_counter_total, project_grade_category_counter_0, project_grade_category_counter_1)\n\nprint(np.mean(X_train_project_grade_category_response_proba_0, axis=0))\nprint(X_train_project_grade_category_response_proba_0.shape, y_train.shape)\nprint(X_test_project_grade_category_response_proba_0.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:29:59.437862Z","iopub.execute_input":"2021-07-06T09:29:59.43828Z","iopub.status.idle":"2021-07-06T09:29:59.595185Z","shell.execute_reply.started":"2021-07-06T09:29:59.438242Z","shell.execute_reply":"2021-07-06T09:29:59.594347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Response Encoding clean_categories","metadata":{}},{"cell_type":"code","source":"x_train_feature_total = df_cat_train_before_response_coding['clean_categories']\nx_train_feature_0 = df_cat_train_before_response_coding.loc[df_cat_train_before_response_coding['project_is_approved']==0]['clean_categories']\nx_train_feature_1 = df_cat_train_before_response_coding.loc[df_cat_train_before_response_coding['project_is_approved']==1]['clean_categories']\n\nclean_categories_counter_total, clean_categories_counter_0, clean_categories_counter_1 = response_encoding_fit(x_train_feature_total, x_train_feature_0, x_train_feature_1 )\n\nX_train_clean_categories_response_proba_0, X_train_clean_categories_response_proba_1 = response_encoding_transform(X_train['clean_categories'], clean_categories_counter_total, clean_categories_counter_0, clean_categories_counter_1)\n\nX_test_clean_categories_response_proba_0, X_test_clean_categories_response_proba_1 = response_encoding_transform(X_test['clean_categories'], clean_categories_counter_total, clean_categories_counter_0, clean_categories_counter_1)\n\nprint(np.mean(X_train_clean_categories_response_proba_0, axis=0))\nprint(X_train_clean_categories_response_proba_0.shape, y_train.shape)\nprint(X_test_clean_categories_response_proba_0.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:29:59.596528Z","iopub.execute_input":"2021-07-06T09:29:59.596877Z","iopub.status.idle":"2021-07-06T09:29:59.755168Z","shell.execute_reply.started":"2021-07-06T09:29:59.596841Z","shell.execute_reply":"2021-07-06T09:29:59.75434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Response Encoding - clean_subcategories","metadata":{}},{"cell_type":"code","source":"x_train_feature_total = df_cat_train_before_response_coding['clean_subcategories']\nx_train_feature_0 = df_cat_train_before_response_coding.loc[df_cat_train_before_response_coding['project_is_approved']==0]['clean_subcategories']\nx_train_feature_1 = df_cat_train_before_response_coding.loc[df_cat_train_before_response_coding['project_is_approved']==1]['clean_subcategories']\n\nclean_subcategories_counter_total, clean_subcategories_counter_0, clean_subcategories_counter_1 = response_encoding_fit(x_train_feature_total, x_train_feature_0, x_train_feature_1 )\n\nX_train_clean_subcategories_response_proba_0, X_train_clean_subcategories_response_proba_1 = response_encoding_transform(X_train['clean_subcategories'], clean_subcategories_counter_total, clean_subcategories_counter_0, clean_subcategories_counter_1)\n\nX_test_clean_subcategories_response_proba_0, X_test_clean_subcategories_response_proba_1 = response_encoding_transform(X_test['clean_subcategories'], clean_subcategories_counter_total, clean_subcategories_counter_0, clean_subcategories_counter_1)\n\nprint(np.mean(X_train_clean_subcategories_response_proba_0, axis=0))\nprint(X_train_clean_subcategories_response_proba_0.shape, y_train.shape)\nprint(X_test_clean_subcategories_response_proba_0.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:29:59.758128Z","iopub.execute_input":"2021-07-06T09:29:59.758406Z","iopub.status.idle":"2021-07-06T09:29:59.917503Z","shell.execute_reply.started":"2021-07-06T09:29:59.75838Z","shell.execute_reply":"2021-07-06T09:29:59.916623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1.4 Make Data Model Ready: encoding numerical features\n\n## Encoding numerical column - Price","metadata":{}},{"cell_type":"code","source":"normalizer = Normalizer()\n\nnormalizer.fit(X_train['price'].values.reshape(-1, 1))\n\nX_train_normalized_price = normalizer.transform(X_train['price'].values.reshape(-1,1))\nX_test_normalized_price = normalizer.transform(X_test['price'].values.reshape(-1,1))\n\nprint('After Normalizing on price column checking the shapes ')\n# print('X_train_normalized_price ', X_train_normalized_price)\nprint(X_train_normalized_price.shape, y_train.shape)\nprint(X_test_normalized_price.shape, y_test.shape)","metadata":{"id":"RAtihVPqsc33","execution":{"iopub.status.busy":"2021-07-06T09:29:59.91881Z","iopub.execute_input":"2021-07-06T09:29:59.919163Z","iopub.status.idle":"2021-07-06T09:29:59.929809Z","shell.execute_reply.started":"2021-07-06T09:29:59.919128Z","shell.execute_reply":"2021-07-06T09:29:59.92891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encoding numerical column - teacher_number_of_previously_posted_projects","metadata":{}},{"cell_type":"code","source":"normalizer = Normalizer()\n\nnormalizer.fit(X_train['teacher_number_of_previously_posted_projects'].values.reshape(-1, 1))\n\nX_train_normalized_teacher_number_of_previously_posted_projects = normalizer.transform(X_train['teacher_number_of_previously_posted_projects'].values.reshape(-1,1))\nX_test_normalized_teacher_number_of_previously_posted_projects = normalizer.transform(X_test['teacher_number_of_previously_posted_projects'].values.reshape(-1,1))\n\nprint('After Normalizing on teacher_number_of_previously_posted_projects column checking the shapes ')\n# print('X_train_normalized_teacher_number_of_previously_posted_projects ', X_train_normalized_teacher_number_of_previously_posted_projects)\nprint(X_train_normalized_teacher_number_of_previously_posted_projects.shape, y_train.shape)\nprint(X_test_normalized_teacher_number_of_previously_posted_projects.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:29:59.931546Z","iopub.execute_input":"2021-07-06T09:29:59.931949Z","iopub.status.idle":"2021-07-06T09:29:59.942984Z","shell.execute_reply.started":"2021-07-06T09:29:59.931913Z","shell.execute_reply":"2021-07-06T09:29:59.941959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Standardize and then .fit() and .transform() all the Sentiments related Columns","metadata":{}},{"cell_type":"code","source":"sentiments_standardizer = StandardScaler()\n\n# First applying the .fit() on the train data to find Mean and SD\nsentiments_standardizer.fit(X_train['negative_sent'].values.reshape(-1,1))\n\n# Now applying .transform() to train, test and cv data\nX_train_negative_sent_standardized = sentiments_standardizer.transform(X_train['negative_sent'].values.reshape(-1,1))\nX_test_negative_sent_standardized = sentiments_standardizer.transform(X_test['negative_sent'].values.reshape(-1,1))\n\nprint('After Standardizing on negative_sent column checking the shapes ')\nprint(X_train_negative_sent_standardized.shape, y_train.shape)\nprint(X_test_negative_sent_standardized.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:29:59.944872Z","iopub.execute_input":"2021-07-06T09:29:59.945288Z","iopub.status.idle":"2021-07-06T09:29:59.956677Z","shell.execute_reply.started":"2021-07-06T09:29:59.945234Z","shell.execute_reply":"2021-07-06T09:29:59.955353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First applying the .fit() on the train data to find Mean and SD\nsentiments_standardizer.fit(X_train['positive_sent'].values.reshape(-1,1))\n\n# Now applying .transform() to train, test and cv data\nX_train_positive_sent_standardized = sentiments_standardizer.transform(X_train['positive_sent'].values.reshape(-1,1))\nX_test_positive_sent_standardized = sentiments_standardizer.transform(X_test['positive_sent'].values.reshape(-1,1))\n\nprint('After Standardizing on positive_sent column checking the shapes ')\nprint(X_train_positive_sent_standardized.shape, y_train.shape)\nprint(X_test_positive_sent_standardized.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:29:59.958304Z","iopub.execute_input":"2021-07-06T09:29:59.958545Z","iopub.status.idle":"2021-07-06T09:29:59.968767Z","shell.execute_reply.started":"2021-07-06T09:29:59.958523Z","shell.execute_reply":"2021-07-06T09:29:59.967736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First applying the .fit() on the train data to find Mean and SD\nsentiments_standardizer.fit(X_train['neutral_sent'].values.reshape(-1,1))\n\n# Now applying .transform() to train, test and cv data\nX_train_neutral_sent_standardized = sentiments_standardizer.transform(X_train['neutral_sent'].values.reshape(-1,1))\nX_test_neutral_sent_standardized = sentiments_standardizer.transform(X_test['neutral_sent'].values.reshape(-1,1))\n\nprint('After Standardizing on neutral_sent column checking the shapes ')\n# print('X_train_neutral_sent_standardized ', X_train_neutral_sent_standardized)\nprint(X_train_neutral_sent_standardized.shape, y_train.shape)\nprint(X_test_neutral_sent_standardized.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:29:59.970528Z","iopub.execute_input":"2021-07-06T09:29:59.970973Z","iopub.status.idle":"2021-07-06T09:29:59.982625Z","shell.execute_reply.started":"2021-07-06T09:29:59.970939Z","shell.execute_reply":"2021-07-06T09:29:59.981686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First applying the .fit() on the train data to find Mean and SD\nsentiments_standardizer.fit(X_train['compound_sent'].values.reshape(-1,1))\n\n# Now applying .transform() to train, test and cv data\nX_train_compound_sent_standardized = sentiments_standardizer.transform(X_train['compound_sent'].values.reshape(-1,1))\nX_test_compound_sent_standardized = sentiments_standardizer.transform(X_test['compound_sent'].values.reshape(-1,1))\n\nprint('After Standardizing on compound_sent column checking the shapes ')\n# print('X_train_compound_sent_standardized ', X_train_compound_sent_standardized)\nprint(X_train_compound_sent_standardized.shape, y_train.shape)\nprint(X_test_compound_sent_standardized.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:29:59.984164Z","iopub.execute_input":"2021-07-06T09:29:59.984679Z","iopub.status.idle":"2021-07-06T09:29:59.994248Z","shell.execute_reply.started":"2021-07-06T09:29:59.984644Z","shell.execute_reply":"2021-07-06T09:29:59.993127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Set S1 - Merging (with hstack) all the above vectorized features that we created above\n\n`hstack((X1, X2))`\n\n### We need to merge all the numerical vectors i.e catogorical, text, once for Set S1 and then for S2 later","metadata":{}},{"cell_type":"code","source":"X_train_s1_merged = hstack((X_train_vectorized_tfidf_essay, X_train_school_state_response_proba_0, X_train_teacher_prefix_response_proba_0 , X_train_project_grade_category_response_proba_0 , X_train_clean_categories_response_proba_0 , X_train_clean_subcategories_response_proba_0, X_train_normalized_price, X_train_normalized_teacher_number_of_previously_posted_projects, X_train_negative_sent_standardized, X_train_positive_sent_standardized , X_train_neutral_sent_standardized, X_train_compound_sent_standardized )).tocsr()\n\n\n\nX_test_s1_merged = hstack((X_test_vectorized_tfidf_essay, X_test_school_state_response_proba_0, X_test_teacher_prefix_response_proba_0 , X_test_project_grade_category_response_proba_0 , X_test_clean_categories_response_proba_0 , X_test_clean_subcategories_response_proba_0, X_test_normalized_price, X_test_normalized_teacher_number_of_previously_posted_projects, X_test_negative_sent_standardized, X_test_positive_sent_standardized , X_test_neutral_sent_standardized, X_test_compound_sent_standardized )).tocsr()\n\n\nprint(\"X_train_s1_merged.shape & y_train.shape \", X_train_s1_merged.shape, y_train.shape)\nprint(\"X_test_s1_merged.shape & y_train.shape \", X_test_s1_merged.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:29:59.996154Z","iopub.execute_input":"2021-07-06T09:29:59.996534Z","iopub.status.idle":"2021-07-06T09:30:00.808739Z","shell.execute_reply.started":"2021-07-06T09:29:59.996498Z","shell.execute_reply":"2021-07-06T09:30:00.807955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Set S1 - GridSearchCV with XGBClassifier\n","metadata":{}},{"cell_type":"code","source":"# xgb_clf_s1 = XGBClassifier(booster='gblinear', reg_alpha=0, reg_lambda=0, eval_metric='mlogloss')\n# xgb_clf_s1 = XGBClassifier(booster='gblinear', reg_alpha=0, reg_lambda=0, tree_method='gpu_hist', eval_metric='mlogloss')\nxgb_clf_s1 = XGBClassifier(eval_metric='mlogloss')\n\nparams = {\n'eta': [0.0001, 0.001, 0.01, 0.1, 0.2],\n'n_estimators': [30, 40, 50, 60],\n'tree_method':['gpu_hist']\n}\n\n# params = {\n# 'eta': [0.0001, 0.001, 0.01, 0.1, 0.2],\n# 'n_estimators': [30, 40, 50, 60]\n# }\n\ngrid_search_s1 = GridSearchCV(xgb_clf_s1, params, cv=3, scoring='roc_auc', return_train_score=True)\n\ngrid_search_s1.fit(X_train_s1_merged, y_train)\n\nbest_params_gridsearch_xgb_s1 = grid_search_s1.best_params_\n\nprint(\"Best Params from GridSearchCV with XGB for Set s1 \", best_params_gridsearch_xgb_s1)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:30:00.810103Z","iopub.execute_input":"2021-07-06T09:30:00.81048Z","iopub.status.idle":"2021-07-06T09:46:55.976912Z","shell.execute_reply.started":"2021-07-06T09:30:00.810444Z","shell.execute_reply":"2021-07-06T09:46:55.976019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>1.5 Appling Models on different kind of featurization as mentioned in the instructions</h2>\n\n<br>Apply GBDT on different kind of featurization as mentioned in the instructions\n<br> For Every model that you work on make sure you do the step 2 and step 3 of instrucations\n\n## AUC for Set S1","metadata":{"id":"I5wDFj17sc3-"}},{"cell_type":"code","source":"learning_rates = [0.0001, 0.001, 0.01, 0.1, 1]\nn_estimators = [30, 40, 50, 60]\n    \ndef get_auc_matrix(x_train, x_test, y_train, y_test ):    \n\n    train_auc_final_arr, test_auc_final_arr = [], []\n\n    for l_rate in tqdm(learning_rates):\n        train_auc_batch, test_auc_batch = [], []\n        \n        for num in n_estimators:\n            # Below gives large number of warnings \n            # xgb_clf = XGBClassifier(n_estimators=num, eta=l_rate, reg_alpha=0, reg_lambda=0, tree_method='gpu_hist')\n            \n            # below works after including eval_metric='mlogloss'\n            # xgb_clf = XGBClassifier(n_estimators=num, eval_metric='mlogloss', learning_rate=l_rate, reg_alpha=0, reg_lambda=0, tree_method='gpu_hist')\n            \n            # Only changing the name of the parameter learning_rate to eta\n            xgb_clf = XGBClassifier(n_estimators=num, eval_metric='mlogloss', eta=l_rate, reg_alpha=0, reg_lambda=0, tree_method='gpu_hist')\n            \n            xgb_clf.fit(x_train, y_train)\n\n            # I have to predict probabilities (clf.predict_proba) instead of classes for calculating of ROC AUC score:\n            y_train_predicted = xgb_clf.predict_proba(x_train)[:, 1]\n            y_test_predicted = xgb_clf.predict_proba(x_test)[:, 1]\n\n            train_auc = roc_auc_score(y_train, y_train_predicted)\n            test_auc = roc_auc_score(y_test, y_test_predicted)\n\n            train_auc_batch.append(train_auc)\n            test_auc_batch.append(test_auc)\n        \n        train_auc_final_arr.append(train_auc_batch)\n        test_auc_final_arr.append(test_auc_batch)\n\n    return train_auc_final_arr, test_auc_final_arr\n\n\ntrain_auc_final_arr_s1, test_auc_final_arr_s1 = get_auc_matrix(X_train_s1_merged, X_test_s1_merged, y_train, y_test)\n\nprint(train_auc_final_arr_s1)\nprint(test_auc_final_arr_s1)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:46:55.978232Z","iopub.execute_input":"2021-07-06T09:46:55.978731Z","iopub.status.idle":"2021-07-06T09:53:22.792741Z","shell.execute_reply.started":"2021-07-06T09:46:55.978691Z","shell.execute_reply":"2021-07-06T09:53:22.789769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Heatmap for Set S1","metadata":{}},{"cell_type":"code","source":"train_auc_final_df_s1 = pd.DataFrame(train_auc_final_arr_s1, columns=n_estimators, index=learning_rates)\nsns.heatmap(train_auc_final_df_s1, annot=True)\n# train_auc_final_df_s1","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:53:22.794755Z","iopub.execute_input":"2021-07-06T09:53:22.795245Z","iopub.status.idle":"2021-07-06T09:53:23.116141Z","shell.execute_reply.started":"2021-07-06T09:53:22.795207Z","shell.execute_reply":"2021-07-06T09:53:23.115407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_auc_final_df_s1 = pd.DataFrame(test_auc_final_arr_s1, columns=n_estimators, index=learning_rates)\nsns.heatmap(test_auc_final_df_s1, annot=True)\n# test_auc_final_df_s1","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:53:23.119574Z","iopub.execute_input":"2021-07-06T09:53:23.119869Z","iopub.status.idle":"2021-07-06T09:53:23.437093Z","shell.execute_reply.started":"2021-07-06T09:53:23.119839Z","shell.execute_reply":"2021-07-06T09:53:23.436329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ROC Curve for Set S1\n\n#### Note we earlier calculate the Best Params from GridSearchCV with XGB for Set s1 as {'eta': 0.01, 'n_estimators': 30}\n","metadata":{}},{"cell_type":"code","source":"xgb_clf = XGBClassifier(n_estimators=30, learning_rate=0.01, reg_alpha=0, reg_lambda=0, booster='gblinear', tree_method='gpu_hist')\n\nxgb_clf.fit(X_train_s1_merged, y_train)\n\n# I have to predict probabilities (clf.predict_proba) instead of classes for calculating of ROC AUC score:\ny_train_predicted = xgb_clf.predict_proba(X_train_s1_merged)[:, 1]\ny_test_predicted = xgb_clf.predict_proba(X_test_s1_merged)[:, 1]\n\ntrain_fpr_s1, train_tpr_s1, train_thresholds_s1 = roc_curve(y_train, y_train_predicted)\n\ntest_fpr_s1, test_tpr_s1, test_thresholds_s1 = roc_curve(y_test, y_test_predicted)\n\nplt.plot(train_fpr_s1, train_tpr_s1, label=\"Train AUC S1= \"+str(auc(train_fpr_s1, train_tpr_s1)))\nplt.plot(test_fpr_s1, test_tpr_s1, label=\"Test AUC S1 = \"+str(auc(test_fpr_s1, test_tpr_s1)))\nplt.legend()\nplt.xlabel(\"FPR S1\")\nplt.ylabel('TPR S1')\nplt.title('ROC AUC Plot S1')\nplt.grid()\nplt.show()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:53:23.438676Z","iopub.execute_input":"2021-07-06T09:53:23.439047Z","iopub.status.idle":"2021-07-06T09:53:34.91488Z","shell.execute_reply.started":"2021-07-06T09:53:23.439009Z","shell.execute_reply":"2021-07-06T09:53:34.914178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Confusion Matrix for Set S1","metadata":{}},{"cell_type":"code","source":"# First the Function to get the Predicted Y_vector based on a given threshold and fpr & tpr\n# This will be needed for Calculating Confusion Matrix\n\ndef get_predicted_y_vec_from_threshold(proba, threshold, fpr, tpr):\n  \n    # Using argmax to return the position of the largest value.\n    # based on the calculated value of tpr*(1-fpr)\n    # tpr * (1-fpr) i.e. optimal_threshold is maximum when fpr is very low and tpr is very high\n    optimal_threshold = threshold[np.argmax(tpr * (1-fpr))]  \n  \n    predicted_y_vector = []\n    for i in proba:\n        if i >= optimal_threshold:\n            predicted_y_vector.append(1)\n        else:\n            predicted_y_vector.append(0)\n\n    return predicted_y_vector\n\nconfusion_matrix_s1_train = confusion_matrix(y_train, get_predicted_y_vec_from_threshold(y_train_predicted, train_thresholds_s1, train_fpr_s1, train_tpr_s1 ) )\n\nconfusion_matrix_s1_test = confusion_matrix(y_test, get_predicted_y_vec_from_threshold(y_test_predicted, test_thresholds_s1, test_fpr_s1, test_tpr_s1 ) )\n\nprint('confusion_matrix_s1_train ', confusion_matrix_s1_train)\n# Heatmap for Confusion Matrix: Train and SET 1\nheatmap_confusion_matrix_train_s1 = sns.heatmap(confusion_matrix_s1_train, annot=True, fmt='d', cmap=\"YlGnBu\", annot_kws={\"size\": 24})\n\nplt.title('S1 Train Set: Confusion Matrix')\nplt.xlabel('Actual X values')\nplt.ylabel('Predicted Y Values')\nplt.show()\n\nheatmap_confusion_matrix_test_s1 = sns.heatmap(confusion_matrix_s1_test, annot=True, fmt='d', cmap=\"YlGnBu\", annot_kws={\"size\": 24})\n\nplt.title('S1 Test Set: Confusion Matrix')\nplt.xlabel('Actual X values')\nplt.ylabel('Predicted Y Values')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:53:34.916116Z","iopub.execute_input":"2021-07-06T09:53:34.916453Z","iopub.status.idle":"2021-07-06T09:53:35.622284Z","shell.execute_reply.started":"2021-07-06T09:53:34.916424Z","shell.execute_reply":"2021-07-06T09:53:35.621554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Set-2: categorical (response encoded), numerical features +  essay (TFIDF W2V) + Sentiment scores(preprocessed_essay)\n\nThe only change in Set-2 is the \"essay\" column - Now here I have to calculate the TFIDF W2V instead of just TFIDF\n\nThe rest of the Vectorized column will remain same as S1, and hence I just need adjust the merging step with hstack() - by using TFIDF W2V column for 'text' instead of just TFIDF","metadata":{}},{"cell_type":"code","source":"with open(glove_vector_path, 'rb') as f:\n    model = pickle.load(f)\n    glove_words =  set(model.keys())\n    \n# In the TF-IDF Word2Vec vectorization, we have to fit the TfidfVectorizer only on X_train['essay'] and \n# extract 'dictionary' (dictionary with features as the keys and IDF scores as the values) and \n# 'tfidf_words' (a set of all the features extracted from the vectorizer). \n# We have to use the same 'dictionary' and 'tfidf_words' in vectorizing both X_train['essay'] and X_test['essay'].\n\n# Now, at the very top section of this Notebook, we alrady have this code of Vectorizer on X_train data\n# vectorizer_essay_tfidf = TfidfVectorizer(min_df=10)\n# vectorizer_essay_tfidf.fit(X_train['essay'].values)\n\n# Hence we are now converting a dictionary with word as a key, and the idf as a value\ndictionary = dict(zip(vectorizer_essay_tfidf.get_feature_names(), list(vectorizer_essay_tfidf.idf_)))\ntfidf_words = set(vectorizer_essay_tfidf.get_feature_names())     \n\n    \n# Function to generate Word2Vec referencing \"4_Reference_Vectorization.ipynb\" given in the instruction\ndef generate_w2v_from_text(essays_text_arr):\n  # compute average word2vec for each review.\n    tfidf_w2v_vectors = []\n    # the avg-w2v for each sentence/review is stored in this list\n\n    for sentence in tqdm(essays_text_arr):  # for each sentence\n        vector = np.zeros(300)  # as word vectors are of zero length\n        tf_idf_weight = 0\n        # num of words with a valid vector in the sentence\n        for word in sentence.split():  # for each word in a sentence\n            if (word in glove_words) and (word in tfidf_words):\n                vec = model[word]  # getting the vector for each word\n                # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n                tf_idf = dictionary[word] * (\n                    sentence.count(word) / len(sentence.split())\n                )  # getting the tfidf value for each word\n                vector += vec * tf_idf  # calculating tfidf weighted w2v\n                tf_idf_weight += tf_idf\n        if tf_idf_weight != 0:\n            vector /= tf_idf_weight\n        tfidf_w2v_vectors.append(vector)\n    return tfidf_w2v_vectors\n  \nX_train_vectorized_tfidf_w2v_essay = generate_w2v_from_text(X_train['essay'].values)\nX_test_vectorized_tfidf_w2v_essay = generate_w2v_from_text(X_test['essay'].values)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:53:35.623758Z","iopub.execute_input":"2021-07-06T09:53:35.624106Z","iopub.status.idle":"2021-07-06T09:57:40.234316Z","shell.execute_reply.started":"2021-07-06T09:53:35.624071Z","shell.execute_reply":"2021-07-06T09:57:40.23162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from scipy import sparse\n\nX_train_s2_merged = np.hstack(( X_train_vectorized_tfidf_w2v_essay, X_train_school_state_response_proba_0, X_train_teacher_prefix_response_proba_0 , X_train_project_grade_category_response_proba_0 , X_train_clean_categories_response_proba_0 , X_train_clean_subcategories_response_proba_0, X_train_normalized_price, X_train_normalized_teacher_number_of_previously_posted_projects, X_train_negative_sent_standardized, X_train_positive_sent_standardized , X_train_neutral_sent_standardized, X_train_compound_sent_standardized ))\n\n# A = sparse.csr_matrix(X_train_s2_merged)\n# print(A.shape)\n\n\nX_test_s2_merged = np.hstack((X_test_vectorized_tfidf_w2v_essay, X_test_school_state_response_proba_0, X_test_teacher_prefix_response_proba_0 , X_test_project_grade_category_response_proba_0 , X_test_clean_categories_response_proba_0 , X_test_clean_subcategories_response_proba_0, X_test_normalized_price, X_test_normalized_teacher_number_of_previously_posted_projects, X_test_negative_sent_standardized, X_test_positive_sent_standardized , X_test_neutral_sent_standardized, X_test_compound_sent_standardized ))\n\n\nprint(\"X_train_s2_merged.shape & y_train.shape \", X_train_s2_merged.shape, y_train.shape)\nprint(\"X_test_s2_merged.shape & y_train.shape \", X_test_s2_merged.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:57:40.235707Z","iopub.execute_input":"2021-07-06T09:57:40.236033Z","iopub.status.idle":"2021-07-06T09:57:40.480954Z","shell.execute_reply.started":"2021-07-06T09:57:40.235997Z","shell.execute_reply":"2021-07-06T09:57:40.480068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GridSearch on Set S2","metadata":{}},{"cell_type":"code","source":"# xgb_clf_s2 = XGBClassifier(booster='gblinear', reg_alpha=0, reg_lambda=0)\n\nxgb_clf_s2 = XGBClassifier(eval_metric='mlogloss')\n\nparams = {\n'eta': [0.0001, 0.001, 0.01, 0.1, 0.2],\n'n_estimators': [30, 40, 50, 60],\n'tree_method':['gpu_hist']\n}\n\ngrid_search_s2 = GridSearchCV(xgb_clf_s2, params, cv=3, scoring='roc_auc', return_train_score=True)\n\ngrid_search_s2.fit(X_train_s2_merged, y_train)\n\nbest_params_gridsearch_xgb_s2 = grid_search_s2.best_params_\n\nprint(\"Best Params from GridSearchCV with XGB for Set s2 \", best_params_gridsearch_xgb_s2)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:57:40.482597Z","iopub.execute_input":"2021-07-06T09:57:40.482938Z","iopub.status.idle":"2021-07-06T09:59:10.862626Z","shell.execute_reply.started":"2021-07-06T09:57:40.4829Z","shell.execute_reply":"2021-07-06T09:59:10.861586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## AUC for Set S2","metadata":{}},{"cell_type":"code","source":"train_auc_final_arr_s2, test_auc_final_arr_s2 = get_auc_matrix(X_train_s2_merged, X_test_s2_merged, y_train, y_test)\nprint(\"train_auc_final_arr_s2 \", train_auc_final_arr_s2)\nprint('test_auc_final_arr_s2 ', test_auc_final_arr_s2)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:59:10.864147Z","iopub.execute_input":"2021-07-06T09:59:10.864506Z","iopub.status.idle":"2021-07-06T09:59:47.571437Z","shell.execute_reply.started":"2021-07-06T09:59:10.864468Z","shell.execute_reply":"2021-07-06T09:59:47.570573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Heatmap for Set S2","metadata":{}},{"cell_type":"code","source":"train_auc_final_df_s2 = pd.DataFrame(train_auc_final_arr_s2, columns=n_estimators, index=learning_rates)\nsns.heatmap(train_auc_final_df_s2, annot=True)\n# train_auc_final_df_s2","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:59:47.57546Z","iopub.execute_input":"2021-07-06T09:59:47.575711Z","iopub.status.idle":"2021-07-06T09:59:47.864412Z","shell.execute_reply.started":"2021-07-06T09:59:47.575686Z","shell.execute_reply":"2021-07-06T09:59:47.863536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_auc_final_df_s2 = pd.DataFrame(test_auc_final_arr_s2, columns=n_estimators, index=learning_rates)\nsns.heatmap(test_auc_final_df_s2, annot=True)\n# test_auc_final_df_s2","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:59:47.865766Z","iopub.execute_input":"2021-07-06T09:59:47.866111Z","iopub.status.idle":"2021-07-06T09:59:48.158826Z","shell.execute_reply.started":"2021-07-06T09:59:47.866074Z","shell.execute_reply":"2021-07-06T09:59:48.157945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ROC Curve for Set S2\n\n\n### Note we earlier calculate the Best Params from GridSearchCV with XGB for Set S2 as {'eta': 0.2, 'n_estimators': 60}","metadata":{}},{"cell_type":"code","source":"xgb_clf = XGBClassifier(n_estimators=60, learning_rate=0.2, booster='gblinear', tree_method='gpu_hist')\n\nxgb_clf.fit(X_train_s2_merged, y_train)\n\n# I have to predict probabilities (clf.predict_proba) instead of classes for calculating of ROC AUC score:\ny_train_predicted = xgb_clf.predict_proba(X_train_s2_merged)[:, 1]\ny_test_predicted = xgb_clf.predict_proba(X_test_s2_merged)[:, 1]\n\ntrain_fpr_s2, train_tpr_s2, train_thresholds_s2 = roc_curve(y_train, y_train_predicted)\n\ntest_fpr_s2, test_tpr_s2, test_thresholds_s2 = roc_curve(y_test, y_test_predicted)\n\nplt.plot(train_fpr_s2, train_tpr_s2, label=\"Train AUC S2= \"+str(auc(train_fpr_s2, train_tpr_s2)))\nplt.plot(test_fpr_s2, test_tpr_s2, label=\"Test AUC S2= \"+str(auc(test_fpr_s2, test_tpr_s2)))\nplt.legend()\nplt.xlabel(\"FPR S2\")\nplt.ylabel('TPR S2')\nplt.title('ROC AUC Plot S2')\nplt.grid()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:59:48.160045Z","iopub.execute_input":"2021-07-06T09:59:48.160381Z","iopub.status.idle":"2021-07-06T09:59:56.97679Z","shell.execute_reply.started":"2021-07-06T09:59:48.160344Z","shell.execute_reply":"2021-07-06T09:59:56.975924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Confusion Matrix for Set S2","metadata":{}},{"cell_type":"code","source":"# print(y_train)\n# print(get_predicted_y_vec_from_threshold(y_train_predicted, train_thresholds_s2, train_fpr_s2, train_tpr_s2 ))\n\nconfusion_matrix_s2_train = confusion_matrix(y_train, get_predicted_y_vec_from_threshold\n(y_train_predicted, train_thresholds_s2, train_fpr_s2, train_tpr_s2 ) )\n\nconfusion_matrix_s2_test = confusion_matrix(y_test, get_predicted_y_vec_from_threshold(y_test_predicted, test_thresholds_s2, test_fpr_s2, test_tpr_s2 ) )\n\nprint('confusion_matrix_s2_train ', confusion_matrix_s2_train)\n# Heatmap for Confusion Matrix: Train and SET 1\nheatmap_confusion_matrix_train_s2 = sns.heatmap(confusion_matrix_s2_train, annot=True, fmt='d', cmap=\"YlGnBu\", annot_kws={\"size\": 24})\n\nplt.title('s2 Train Set: Confusion Matrix')\nplt.xlabel('Actual X values')\nplt.ylabel('Predicted Y Values')\nplt.show()\n\nheatmap_confusion_matrix_test_s2 = sns.heatmap(confusion_matrix_s2_test, annot=True, fmt='d', cmap=\"YlGnBu\", annot_kws={\"size\": 24})\n\nplt.title('s2 Test Set: Confusion Matrix')\nplt.xlabel('Actual X values')\nplt.ylabel('Predicted Y Values')\nplt.show()","metadata":{"tags":["outputPrepend"],"execution":{"iopub.status.busy":"2021-07-06T09:59:56.978116Z","iopub.execute_input":"2021-07-06T09:59:56.97853Z","iopub.status.idle":"2021-07-06T09:59:57.835386Z","shell.execute_reply.started":"2021-07-06T09:59:56.978499Z","shell.execute_reply":"2021-07-06T09:59:57.834329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>3. Summary</h1>\n\n<br> as mentioned in the step 4 of instructions","metadata":{"id":"YhFN-lDWsc4G"}},{"cell_type":"code","source":"import pandas as pd\n\npd.DataFrame({\n  'Model':['TfIdf',  'Tfidf-W2V'],\n  'Train AUC': [0.87, 0.72],\n  'Test AUC': [0.70, 0.69],\n\n})","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:59:57.839417Z","iopub.execute_input":"2021-07-06T09:59:57.839756Z","iopub.status.idle":"2021-07-06T09:59:57.860406Z","shell.execute_reply.started":"2021-07-06T09:59:57.839721Z","shell.execute_reply":"2021-07-06T09:59:57.859396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# import dill\n# dill.dump_session('notebook_env.pkl')","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:59:57.864671Z","iopub.execute_input":"2021-07-06T09:59:57.865009Z","iopub.status.idle":"2021-07-06T09:59:57.871795Z","shell.execute_reply.started":"2021-07-06T09:59:57.864974Z","shell.execute_reply":"2021-07-06T09:59:57.870819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.close()\ntest_y = [2, 0, 2, 2, 0, 1]\npredict_y = [0, 0, 2, 2, 0, 2]\n\nC = confusion_matrix(test_y, predict_y)\n\nprint(C)\n\nlabels = [1,2,3,4,5,6,7,8,9]\ncmap=sns.light_palette(\"green\")\n# representing A in heatmap format\nprint(\"-\"*50, \"Confusion matrix\", \"-\"*50)\n# plt.figure(figsize=(10,5))\n# sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\nsns.heatmap(C, annot=True, fmt='d', cmap=\"YlGnBu\", annot_kws={\"size\": 24})\nsns.heatmap(C, annot=True)\nplt.xlabel('Predicted Class')\nplt.ylabel('Original Class')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:59:57.873443Z","iopub.execute_input":"2021-07-06T09:59:57.874212Z","iopub.status.idle":"2021-07-06T09:59:58.325087Z","shell.execute_reply.started":"2021-07-06T09:59:57.874156Z","shell.execute_reply":"2021-07-06T09:59:58.324403Z"},"trusted":true},"execution_count":null,"outputs":[]}]}