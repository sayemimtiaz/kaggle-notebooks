{"cells":[{"metadata":{},"cell_type":"markdown","source":"This note book is created to give a basic idea on how to apply different classification algorithm to a dataset\nand evaluate their respective performance.\n\nData Set used :- Breast Cancer Wisconsin (Diagnostic) Data Set\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\n\n# Import the required libraries\n\nimport sys\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nnp.set_printoptions(threshold=sys.maxsize,precision=3)\nfrom IPython.display import display\npd.options.display.max_columns = None\nfrom sklearn.metrics import confusion_matrix as cm\nfrom sklearn.metrics import accuracy_score as accuracy\nimport warnings\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n# Defining a function which will be used later to analyze the accuracy of the model\ndef PrintAccuracy(classifier_name, y_test, y_pred):\n    print(f'Accuracy using {classifier_name} is :- ', (accuracy(y_test,y_pred)*100))\n\n# Ignore python warnings\nwarnings.filterwarnings('ignore')\n\n# Import the data set\ndataset = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')  # This is Dataset Breast Cancer Wisconsin dataset\n\n# Check if there are any missing values\ndataset.isnull().sum()  # Here there are no missing values observed.\n\n# Get the dependent and independent variables\nx = dataset.iloc[:,2:32].values\ny = dataset.iloc[:,1].values.reshape(-1,1)\n\n# Encode diagnosis column\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\ndiagnosis_encoder = LabelEncoder()\ny = diagnosis_encoder.fit_transform(y)\n# diagnosis_encoder = OneHotEncoder()\n# y = diagnosis_encoder.fit_transform(y.reshape(-1,1)).toarray() # TODO :- Check if there is any other better way\n\n# Split the data into training and test set\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x, y, test_size = 0.25)\n\n# Fit the decision tree classifier in the data set\nfrom sklearn.tree import DecisionTreeClassifier\ntree_classifier = DecisionTreeClassifier()\ntree_classifier.fit(x_train,y_train)\n\n# Make predictions\ntree_pred = tree_classifier.predict(x_test)\nPrintAccuracy(classifier_name= 'Decision Tree', y_pred= tree_pred, y_test= y_test)  # 92.3\n\n# Try fitting Logistic Regression to the data\nfrom sklearn.linear_model import LogisticRegression\nlogistic_classifier = LogisticRegression()\nlogistic_classifier.fit(x_train,y_train)\n\n# Make predictions\nlogistic_pred = logistic_classifier.predict(x_test)\nPrintAccuracy(classifier_name= 'Logistic Regression', y_pred= logistic_pred, y_test= y_test) # 95.8\n\n# Fit SVM into the dataset\nfrom sklearn.svm import SVC\nsvc_classifier = SVC(kernel= 'rbf')\nsvc_classifier.fit(x_train, y_train)\n\n# Make predictions\nsvc_pred = svc_classifier.predict(x_test)\nPrintAccuracy(classifier_name= 'Support Vector Machine', y_pred= svc_pred, y_test= y_test) # 62.23\n\n# Fit Naive Bayes to the dataset\nfrom sklearn.naive_bayes import GaussianNB\nnb_classifier = GaussianNB()\nnb_classifier.fit(x_train, y_train)\n\n# Make predictions\nnb_pred = nb_classifier.predict(x_test)\nPrintAccuracy(classifier_name= 'Naive Bayes', y_pred= nb_pred, y_test= y_test) # 94.405\n\n# Fit KNN to the dataset\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_classifier = KNeighborsClassifier(n_neighbors= 5 , metric = 'minkowski', p = 2)\nknn_classifier.fit(x_train, y_train)\n\n# Make predictions\nknn_pred = knn_classifier.predict(x_test)\nPrintAccuracy(classifier_name= 'KNN', y_pred= knn_pred, y_test= y_test) # 92.307\n\n# N- Fold Cross Validations for the different model used\n\n# Utility function\nfrom sklearn.model_selection import cross_val_score\ndef ApplyCrossValidation(regressor, x_train, y_train):\n    accuracies = cross_val_score(estimator = regressor, X = x_train, y = y_train, cv = 10)\n    return accuracies.mean()*100\n\n# 1. Decision Tree Classifier\nprint(f'Average accuracy of Decision Tree Classifier after applying 10- Fold CV is {ApplyCrossValidation(tree_classifier, x_train, y_train)}')\n# 91.07203630175837\n\n# 2. Logistic Regression\nprint(f'Average accuracy of Logistic Regression after applying 10- Fold CV is {ApplyCrossValidation(logistic_classifier, x_train, y_train)}')\n# 95.25808281338628\n\n# 3. SVM\nprint(f'Average accuracy of SVM after applying 10- Fold CV is {ApplyCrossValidation(svc_classifier, x_train, y_train)}')\n# 62.91548496880317\n\n# 4. Naive Bayes\nprint(f'Average accuracy of Naive Bayes after applying 10- Fold CV is {ApplyCrossValidation(nb_classifier, x_train, y_train)}')\n# 93.85138967668748\n\n# 5. KNN\nprint(f'Average accuracy of KNN after applying 10- Fold CV is {ApplyCrossValidation(knn_classifier, x_train, y_train)}')\n# 93.85138967668748","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}