{"cells":[{"metadata":{},"cell_type":"markdown","source":"First, let's load our dataset. The dataset used here is,\nhttps://www.kaggle.com/umar47/usd-try. For preprocessing, again, thanks to the kaggle user https://www.kaggle.com/umar47 and his kernel https://www.kaggle.com/umar47/preprocessing-on-usd-try."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport random\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\n\nimport time\nimport copy\nimport chainer\nimport chainer.functions as F\nimport chainer.links as L\nfrom plotly import tools\nfrom plotly.graph_objs import *\nfrom plotly.offline import init_notebook_mode, iplot, iplot_mpl\nfrom collections import deque\ninit_notebook_mode()\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some flags for the debuging purposes are presented here."},{"metadata":{"trusted":true},"cell_type":"code","source":"# if this flag is set, each step of the environment's state will be printed\nENVIRONMENT_DEBUG = False","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"ds=pd.read_csv('../input/USD_TRY Gemi Verileri.csv')\nds['Tarih']=pd.to_datetime(ds['Tarih'], errors='coerce')\nds['Şimdi']=pd.to_numeric(ds['Şimdi'].str.replace(',', '.'), errors='coerce')\nds['Fark %']=ds['Fark %'].str.replace('%', '')\nds['Fark %']=ds['Fark %'].str.replace(',', '.')\n\nds.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, the reverse version of the data will be used. It is expected to make the learning stronger since from 2002 to this date, usd is increasing with respect to try."},{"metadata":{"trusted":true},"cell_type":"code","source":"X=ds['Şimdi']\nY=ds['Tarih']\nX=np.array(X).reshape((len(X), 1))#i got some issues with shapes but i found this solution on stackoverflow\nY=np.array(Y).reshape((len(Y), 1))\nfig=plt.figure()\nax=fig.add_subplot(111)\nax.plot_date(Y, X, '.')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split train and test data\n\ndate_split = 4000\ntrain = ds[:date_split]\ntest = ds[date_split:]\nlen(train), len(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's define our envireonment."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Environment:\n    \n    def __init__(self, data, tl, history_t=10):\n        self.data = data\n        self.history_t = history_t\n        self.tl_start = tl;\n        self.reset()\n        \n    def reset(self):\n        self.tl = self.tl_start\n        self.usd = 0;\n        self.done = False\n        self.profits = 0\n        self.current_position = \"none\";\n        self.history = [self.data.iloc[x, :]['Fark %'] for x in range(self.history_t)]\n        self.t = self.history_t;\n        self.last_tl = 0;\n        return self.history # obs\n    \n    def step(self, act):\n        #print(\"new step\");\n        reward = 0\n        # act = 0,1,2\n        if act == 0: # hold\n            self.current_position = self.current_position;\n        elif act == 1: # buy\n            if self.current_position == \"none\":\n                # open long\n                self.current_position = \"long\"\n                ## buy usd\n                self.last_tl = self.tl;\n                self.usd = self.tl/(self.data.iloc[self.t, :]['Şimdi'])\n                self.tl = 0;\n                #print(\"bought usd, usd:\", self.usd)\n            else:\n                self.current_position = self.current_position;\n                \n        else: # sell\n            if self.current_position == \"long\":\n                self.current_position = \"none\"\n                ## sell usd\n                self.tl = self.usd*(self.data.iloc[self.t, :]['Şimdi'])\n                self.usd = 0; # dolarları sıfırla\n                \n                self.profits = self.profits + (self.tl - self.last_tl);\n                \n                if (self.tl - self.last_tl) > 0:\n                    reward = 1\n                else:\n                    reward = -1\n                #print(\"sold usd, tl:\", self.tl)\n            else:\n                self.current_position = self.current_position;\n                \n        # set next time\n        #print(\"current position: \",self.current_position)\n        self.t += 1\n        #print(\"history before: \",self.history)\n        self.history.pop(0)\n        self.history.append(self.data.iloc[self.t, :]['Fark %'])\n        #print(\"history after: \",self.history)\n        \n        #print(\"reward: \",reward)\n            \n        if ENVIRONMENT_DEBUG == True:\n            print(\"t: \",(self.t-self.history_t),\" reward: \",reward,\" profits: \",self.profits, \" current position: \",\n              self.current_position,\" done: \",self.done)\n            \n        if self.t == (len(self.data)-1):\n            self.done = True \n            print(\"Total steps: \",(self.t-self.history_t),\" TotalProfit: \",self.profits,\" done: \",self.done)\n        \n        return self.history, reward, self.done, self.profits # obs, reward, done","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The agent class."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Deep Q-learning Agent\nclass DQNAgent:\n    def __init__(self, state_size, action_size, hidden_layer_size):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.hidden_layer_size = hidden_layer_size\n        self.memory = deque(maxlen=2000)\n        self.gamma = 0.95    # discount rate\n        self.epsilon = 1.0  # exploration rate\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.995\n        self.learning_rate = 0.001\n        self.model = self._build_model()\n        \n    def _build_model(self):\n        # Neural Net for Deep-Q learning Model\n        model = Sequential()\n        model.add(Dense(self.hidden_layer_size, input_dim=self.state_size, activation='relu'))\n        model.add(Dense(self.hidden_layer_size, activation='relu'))\n        model.add(Dense(self.action_size, activation='linear'))\n        model.compile(loss='mse',\n                      optimizer=Adam(lr=self.learning_rate))\n        return model\n    \n    def remember(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n        \n    def act(self, state):\n        if np.random.rand() <= self.epsilon:\n            return random.randrange(self.action_size)\n        act_values = self.model.predict(state)\n        return np.argmax(act_values[0])  # returns action\n    \n    def act_greedy(self, state):\n        act_values = self.model.predict(state)\n        return np.argmax(act_values[0])  # returns action\n    \n    def replay(self, batch_size):\n        minibatch = random.sample(self.memory, batch_size)\n        for state, action, reward, next_state, done in minibatch:\n            \n            target = reward\n            if not done:\n              target = reward + self.gamma * \\\n                       np.amax(self.model.predict(next_state)[0])\n            target_f = self.model.predict(state)\n            target_f[0][action] = target\n            self.model.fit(state, target_f, epochs=1, verbose=0)\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Train the DQN"},{"metadata":{"trusted":true},"cell_type":"code","source":"if __name__ == \"__main__\":\n    # macros\n    EPISODES = 2500\n    STATE_SIZE = 90\n    \n    # profits list\n    total_profits = []\n    \n    # initialize environment and the agent\n    env = Environment(train,100,STATE_SIZE) #100tl, 60 history\n    agent = DQNAgent(STATE_SIZE,3,100)\n    \n    # Iterate the game\n    for e in range(EPISODES):\n        #check if the buy and sell actions are taken\n        actions_count = 0\n        \n        # reset state in the beginning of each game\n        state = env.reset()\n        state = np.reshape(state, [1, STATE_SIZE])\n        \n        # time_t represents each frame of the game\n        # the more time_t the more score\n        for time_t in range(5000):\n            \n            # Decide action\n            action = agent.act(state)\n            \n            if (action == 1) or (action == 2):\n                actions_count = actions_count +1\n           \n            # Advance the game to the next frame based on the action.\n            next_state, reward, done, profits = env.step(action)\n            next_state = np.reshape(next_state, [1, STATE_SIZE])\n            \n            # make rewards = profits (EXPERIMENTAL)\n            reward = profits\n            \n            # Remember the previous state, action, reward, and done\n            agent.remember(state, action, reward, next_state, done)\n           \n            # make next_state the new current state for the next frame.\n            state = next_state\n            \n            # done becomes True when the game ends\n            if done:\n                total_profits.append(profits)\n                # print the score and break out of the loop\n                # print(\"number of actions taken other than hold in this iteration is \",actions_count,\"\\n\")\n                # print(\"episode: {}/{}, score: {}\".format(e, EPISODES, time_t))\n                break\n        # train the agent with the experience of the episode\n        agent.replay(32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_profits(total_profits):\n    epoch_count = range(1, len(total_profits) + 1)\n    fig= plt.figure(figsize=(30,10))\n    plt.plot(epoch_count, total_profits,'b-')\n    plt.legend('Total Profits')\n    plt.xlabel('Epoch')\n    plt.ylabel('Total Profits')\n    plt.figure(figsize=(50,10))\n    plt.show();\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_profits(total_profits)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now test the agent with real world data."},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.iloc[::-1]\ntest.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env_test = Environment(test,100,STATE_SIZE) #100tl, 60 history\n\n# Iterate the game\nfor e in range(1):\n    #check if the buy and sell actions are taken\n    actions_count = 0\n        \n    # reset state in the beginning of each game\n    state = env_test.reset()\n    state = np.reshape(state, [1, STATE_SIZE])\n        \n    # time_t represents each frame of the game\n    # the more time_t the more score\n    for time_t in range(5000):\n            \n        # Decide action\n        action = agent.act_greedy(state)\n            \n        if (action == 1) or (action == 2):\n                actions_count = actions_count +1\n           \n        # Advance the game to the next frame based on the action.\n        next_state, reward, done, profits = env_test.step(action)\n        next_state = np.reshape(next_state, [1, STATE_SIZE])\n            \n        # make rewards = profits (EXPERIMENTAL)\n        reward = profits\n            \n        # Remember the previous state, action, reward, and done\n        agent.remember(state, action, reward, next_state, done)\n           \n        # make next_state the new current state for the next frame.\n        state = next_state\n            \n        # done becomes True when the game ends\n        if done:\n            break","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}