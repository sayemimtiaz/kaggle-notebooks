{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Customer segmentation \n\n**A simple and pragmatic approach: V1**   \n*A detailed version will come in succession to this kernel: V2!*  \n*An expansion (segmenting with transactional data) will then follow: V3*   \n\n<img src=\"https://cdn.pixabay.com/photo/2014/04/03/00/41/people-309099_960_720.png\" width=\"250px\" align=\"right\">\n\n\n\nThe purpose with this analysis is to segment the customers in the \"Mall customers\" dataset.  \nThe dataset consist of 200 customers, each identifiable by \"Customer ID\".    \nSegmentation is based on the variables in the dataset:  \n* Gender\t\n* Age  \n* Annual Income (k)*  \n* Spending Score (1-100)*\n\n**Variables used for this approach!*\n\n**Note!** This is a pragmatic approach, focusing on the output!  I am skipping \"details\" here and there, e.g. the K-Means \"bend analysis\" to determine number of clusters, analysis of differences between male/female and age distribution. I am also only focusing on a single method, K-Means clustering.  \n\nIf you want more information on K-Means, [read more on this site.](https://towardsdatascience.com/how-does-k-means-clustering-in-machine-learning-work-fdaaaf5acfa0) \n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#standard data libraries\nimport pandas as pd\nimport numpy as np\n\n#vizualizations\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style('darkgrid')\n\n#machine learning models and related libs\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\nfrom sklearn import metrics\nfrom scipy.spatial.distance import cdist\nfrom sklearn.preprocessing import LabelEncoder\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory data analysis (EDA)  \nIf you only want the segmentation-part of the process, **feel free to skip this step.  **  \n\n**Getting to know the data.**\n\n>We need to know e.g:  \n>>How many rows and columns are in the dataset?  \n>>Are there null-values we need to handle?  \n>>What are the datatype for each variable?  \n>>What are the distributions for each value?\n\n\nI have kept all the information in one \"print\". \n\n*Not userfriendly? I know, just skip it then!*"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df = pd.read_csv('../input/customer-segmentation-tutorial-in-python/Mall_Customers.csv')\nprint('The \"shape\" of the dataset:\\nRows:\\t ', df.shape[0], \n      '\\nColumns: ', df.shape[1], \n      '\\n\\n\\nThe data types: \\n', df.dtypes,\n      '\\n\\n\\nInformation: \\n', df.describe().round(1),\n      '\\n\\n\\nDo we have null values?: \\n', df.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"#I am renaming the columns; just to make the columns names a bit easier to work with.\ndf.rename(\n    columns = {\n        'CustomerID':'id', \n        'Gender':'gender',\n        'Age':'age', \n        'Annual Income (k$)':'income', \n        'Spending Score (1-100)':'score'\n        },\n    inplace = True\n    )\ndf.set_index('id', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visual overview\n**Not necessarily part of the most simpel and pragmatic approach, but I am a visually oriented.**  "},{"metadata":{},"cell_type":"markdown","source":"**Correlation** between the variables can be seen below.  \n\nIn this analysis, I only focus on income and age. We see no signigicant correlation between the variables; that's a green light!  \n\nIf we have a higher number of variables with high correlation, we should consider dimension reduction, e.g. through a Principal Component Analysis (PCA)."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(1, figsize=(4, 4))\nsns.heatmap(abs(df.corr()), annot=df.corr(), cmap='RdYlBu_r', square=True, cbar=False)\nplt.suptitle('Heatmap of correlation between key variables')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A **pairplot** also give a nice overview of the variables.  \n\nThis creates a scatterplot between variables and a histogram/kde of the distribution for each variable. We can e.g. spot outliers and see patterns, such as correlation.  "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(1, figsize=(12, 7))\ng = sns.pairplot(df, hue='gender', palette=['#00616f','#f47920'])\nplt.suptitle('Pairplot overview')\n\nfor i, j in zip(*np.triu_indices_from(g.axes, 1)):\n    g.axes[i, j].set_visible(False)\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we are especially interested in, is the relationship between \"score\" and \"income\", as we use these variables for segmentation.  \n\nLet's have a closer look in the figure below.  \n\nIn this dataset, we can already begin to see patterns. "},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"plt.figure(1, figsize=(6, 6))\nplt.scatter(x=df.income, y=df.score)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # The segments!"},{"metadata":{},"cell_type":"markdown","source":"**Number of clusters**  \n\nThere are a number of different ways to find the \"correct\" number of clusters in a K-Means analysis. The most frequently used is the so-called \"Bend Analysis\".  \n\nI am skipping this step. From a pragmatic approach, I want the segments to a) be large enough to be \"useful\" and b) different enough to be targeted.  \n\n*From a marketers perspective all clusters large enough to constitute as a segment should be deemed valid, as long as segments are \"different enough\" from each other - depending on the purpose of the segmentation, e.g. customer targeting. A general rule of thumb is that a segment should at least 10% of the data set.*"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sc = StandardScaler()\nrs = RobustScaler()\nmm = MinMaxScaler()\n\nSCALER = None\n\ndef clusters(data, no_clusters, scaler):\n    kmeans = KMeans(\n        n_clusters=no_clusters,\n        max_iter=1000000000,\n        n_init=20,\n        init='k-means++',\n        random_state=101,\n        algorithm='full'\n        )\n    if scaler is not None:\n        df = scaler.fit_transform(data)\n    else:\n        df = data\n    kmeans.fit(df)\n    data['cluster'] = kmeans.fit_predict(df)\n    return data\n\ndata = df.drop(['gender','age'], axis=1)\n#data = df.drop(['gender'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's go through different scenarios, between 1 and 9 different clusters, i.e. segments**  \n\nTry going throgh each scenario, from 1 through 9 clusters, to see the clusters evolve... exciting, right!!!"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(1 , figsize=(17 , 17))\nn = 0 \nfor no in range(1,10):\n    n += 1\n    plt.subplot(3 , 3 , n)\n    plt.subplots_adjust(hspace=0.25 , wspace=0.25)\n    segments = clusters(data=data, no_clusters=no, scaler=SCALER)\n    plt.title('No. clusters: {}'.format(no))\n    sns.scatterplot(data=segments, x='income',y='score',hue='cluster', palette='tab10', s=70, legend=False)    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above figure shows the process of dividing data points (customers) into clusters (segments). Does it make sense?"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"def ch_plot_for_k_means(X, k_range, resample):\n    plt.figure(figsize=[12,5])\n    for i in range(3):\n        scores = []\n        for k in k_range:\n            kmeansModel = KMeans(n_clusters = k).fit(X)\n            labels = kmeansModel.labels_\n            scores.append(metrics.calinski_harabasz_score(X, labels)) \n        plt.plot(k_range, scores)\n    plt.xticks(k_range)\n    plt.title(\"Optimal number of clusters Calinski-Harabasz criterion\")\n    plt.xlabel(\"Number of clusters\")\n    plt.ylabel(\"Calinski - Harabasz statistic\")\n    plt.show()\nch_plot_for_k_means(data, range(2,15), 3)\n\n#thanks to: https://www.kaggle.com/gmateusz/customer-segmentation-using-kmeans","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":" def counts(no_clusters, if_print_1):\n    clus = clusters(data, no_clusters, SCALER).reset_index()\n    clus = df.reset_index().merge(clus).set_index('id')\n    clus_count = clus.groupby('cluster')['score'].count()\n    clus_count = clus_count.to_frame()\n    clus_count = clus_count.rename(columns={'score':'no_customers'})\n    clus_count['pct. of total'] = clus_count['no_customers'].apply(lambda x: round(100*(x / len(data)),2))\n    if if_print_1 == 1:\n        mini = str(min(clus_count['pct. of total'])) + '%'\n        print('Smallest cluster count in pct. :', mini, ', with ', no_clusters, ' clusters')\n    clus_count['pct. of total'] = clus_count['pct. of total'].apply(lambda x: str(x) + '%')\n    return clus, clus_count\nclus, clus_count = counts(5, 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's go on using the 5-cluster segmentation.**   \n\nFirst, let's make sure that the segments are large enough to actually make sense. With only 200 customers, I'll use the 10% rule of thumb. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"clus_count","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are above the 10% threshold, great! \n\nBut what about the other cluster solutions? If we e.g. go up to 6 segments, are we still above the threshold?  "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"for num in range(2,10):\n    counts(num, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I've iterated over all scenarios, and we have actually found the segment just within the threshold! With 6 clusters, the smallest segment is still above only 5% of the customers.\n\n\nSecondly, a **pairplot to vizualize** the differences between the clusters, other than the above illustration:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.pairplot(clus, hue='cluster', vars=['income','age','score'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Interpretation"},{"metadata":{},"cell_type":"markdown","source":"**We want data!** \n\nYou got it! The mean value for each variable:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"figdata = clus.pivot_table(\n    index=['cluster'],\n    aggfunc='mean')\n\nfigdata = figdata.reset_index().merge(clus_count.reset_index()[['no_customers','cluster']]).set_index('cluster')\nfigdata = figdata.sort_values('score', ascending=False)\n\nsc = StandardScaler()\nfiginfo = sc.fit_transform(figdata)\nfiginfo = pd.DataFrame(figinfo, columns=figdata.columns)\n\nplt.figure(1 , figsize=(6 ,6))\nsns.heatmap(data=figinfo, annot=figdata, cmap='RdYlBu_r', cbar=False)\nplt.suptitle('Heatmap for interpretation')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## From clusters to segments:\n\n**In order to actually use the segmentation, we need to interpret the output.  \nTo keep it simple, let's just do this without chi2 or other measures, just freestyling.**\n\n**From the top!**\n\n* **Big spenders!**   \n  * High income / high score\n  * Young and with money to spend\n  * Big spenders either due to \n    * \"independence\" (increasing income, still low living costs) \n    * \"nesting\" and/or parenthood (increasing costs)\n\n\n  \n* **Mall-rats!**   \n  * Low income / high score\n  * Young people, maybe hanging out at caf√©s, shopping sneakers\n  * They are likely using a large share-of-wallet at the mall\n  \n  \n\n* **The middleground**\n  * Medium income / medium score\n  * Description\n  * Largest segment\n  \n  \n  \n* **Low demanders ** \n  * Low spenders / low income\n  * Maybe with low disposable income and\n  * not in immidiate target audience\n  \n  \n\n  \n* **Low spenders** \n  * Low spender / high income\n  * Potential!\n  * High income customers, but maybe not in the immidiate target audience  \n  "},{"metadata":{},"cell_type":"markdown","source":"## ... and then?  \nYou have your segments! Go target them, spam them with emails, give them coupons!!!\n"},{"metadata":{},"cell_type":"markdown","source":"## I will make at least one more kernel on customer segmentation. \n***It will include this dataset and analysis in much more detail AND transactional data to make the RFM analysis part of the process. Hang tight!**"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Extra: Single-function solution\n***Just an easy way to test different variations**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clusters(data, no_clusters,  k_range, df):\n    \n    print('Initiating...')\n    \n    def clusters(data, no_clusters):\n        kmeans = KMeans(\n            n_clusters=no_clusters,\n            max_iter=1000000000,\n            n_init=20,\n            init='k-means++',\n            random_state=101,\n            algorithm='full'\n        )\n        kmeans.fit(data)\n        data['cluster'] = kmeans.fit_predict(data)\n        return data\n\n    print('Cluster function created...')\n    \n    plt.figure(1 , figsize=(17 , 17))\n    n = 0 \n    for no in k_range:\n        n += 1\n        plt.subplot(3 , 3 , n)\n        plt.subplots_adjust(hspace=0.25 , wspace=0.25)\n        segments = clusters(data=data, no_clusters=no)\n        plt.title('No. clusters: {}'.format(no))\n        sns.scatterplot(data=segments, x='income',y='score',hue='cluster', palette='tab10', s=70, legend=False)    \n    print('Cluster visuals based only on income and score:')\n    plt.show()\n\n\n    def ch_plot_for_k_means(X, k_range, resample):\n        plt.figure(figsize=[12,5])\n        for i in range(3):\n            scores = []\n            for k in k_range:\n                kmeansModel = KMeans(n_clusters = k).fit(X)\n                labels = kmeansModel.labels_\n                scores.append(metrics.calinski_harabasz_score(X, labels)) \n            plt.plot(k_range, scores)\n        plt.xticks(k_range)\n        plt.title(\"Optimal number of clusters Calinski-Harabasz criterion\")\n        plt.xlabel(\"Number of clusters\")\n        plt.ylabel(\"Calinski - Harabasz statistic\")\n        plt.show()\n    print('Optimal number of clusters, please adjust parameters:')\n    ch_plot_for_k_means(data, range(2,15), 3)\n    \n    \n    def counts(no_clusters, if_print_1):\n        clus = clusters(data, no_clusters).reset_index()\n        clus = df.reset_index().merge(clus).set_index('id')\n        clus_count = clus.groupby('cluster')['score'].count()\n        clus_count = clus_count.to_frame()\n        clus_count = clus_count.rename(columns={'score':'no_customers'})\n        clus_count['pct. of total'] = clus_count['no_customers'].apply(lambda x: round(100*(x / len(data)),2))\n        if if_print_1 == 1:\n            mini = str(min(clus_count['pct. of total'])) + '%'\n            print('Smallest cluster count in pct. :', mini, ', with ', no_clusters, ' clusters')\n        clus_count['pct. of total'] = clus_count['pct. of total'].apply(lambda x: str(x) + '%')\n        return clus, clus_count\n    clus, clus_count = counts(no_clusters, 0)\n    \n    print('')\n    for num in range(2,10):\n        counts(num, 1)\n    print('\\nPlease adjust if needed...')\n    print('\\nCluster difference overview')\n    sns.pairplot(clus, hue='cluster', vars=[col for col in df.columns if df[col].dtype not in ['object','str']])\n    plt.show()\n    print('\\nCluster mean comparison:')\n    figdata = clus.pivot_table(\n    index=['cluster'],\n    aggfunc='mean')\n\n    figdata = figdata.reset_index().merge(clus_count.reset_index()[['no_customers','cluster']]).set_index('cluster')\n    figdata = figdata.sort_values('score', ascending=False)\n\n    sc = StandardScaler()\n    figinfo = sc.fit_transform(figdata)\n    figinfo = pd.DataFrame(figinfo, columns=figdata.columns)\n\n    plt.figure(1 , figsize=(6 ,6))\n    sns.heatmap(data=figinfo, annot=figdata, cmap='RdYlBu_r', cbar=False)\n    plt.suptitle('Heatmap for interpretation')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Including age\n***And 6 clusters***"},{"metadata":{"trusted":true},"cell_type":"code","source":"clusters(data=df[['income','score','age']], no_clusters=6, k_range=range(1,10), df=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Including gender"},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df.copy()\ndf2['Gender'] = df2['gender'].apply(lambda x: 1 if x=='Male' else 0)\n\nclusters(data=df2[['income','score','age','Gender']], no_clusters=6, k_range=range(1,10), df=df.merge(df2))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}