{"cells":[{"metadata":{},"cell_type":"markdown","source":"****DISCLAIMER: This model is not mine.\n\nIt has been found on the following notebooks\nhttps://www.kaggle.com/mobassir/mining-covid-19-scientific-papers\nhttps://www.kaggle.com/theamrzaki/covid-19-bert-researchpapers-semantic-search\n\nand it has been modified according to what we want it in our particular case to do. It here only put as a module from a more general project that i will publish soon. If you want to know more about it, I highly recommend you to check those notebooks as well as this https://towardsdatascience.com/covid-19-bert-literature-search-engine-4d06cdac08bd website where the model is explained. ","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport os\nimport tqdm\nimport textwrap\nimport json\nimport prettytable\nimport logging\nimport pickle\nimport warnings\nwarnings.simplefilter('ignore')\n\nfrom  transformers import *\nimport pandas as pd\nimport scipy\nfrom sentence_transformers import SentenceTransformer\n\nCOVID_BROWSER_ASCII = \"\"\"\n================================================================================\n  _____           _     _      __  ___    ____                                  \n / ____|         (_)   | |    /_ |/ _ \\  |  _ \\                                 \n| |     _____   ___  __| | ___ | | (_) | | |_) |_ __ _____      _____  ___ _ __ \n| |    / _ \\ \\ / / |/ _` ||___|| |\\__, | |  _ <| '__/ _ \\ \\ /\\ / / __|/ _ \\ '__|\n| |___| (_) \\ V /| | (_| |     | |  / /  | |_) | | | (_) \\ V  V /\\__ \\  __/ |   \n \\_____\\___/ \\_/ |_|\\__,_|     |_| /_/   |____/|_|  \\___/ \\_/\\_/ |___/\\___|_|   \n=================================================================================\n\"\"\"\n\nCOVID_BROWSER_INTRO = \"\"\"\nThis demo uses a state-of-the-art language model trained on scientific papers to\nsearch passages matching user-defined queries inside the COVID-19 Open Research\nDataset. Ask something like 'Is smoking a risk factor for Covid-19?' to retrieve\nrelevant abstracts.\\n\n\"\"\"\n\nBIORXIV_PATH = '/kaggle/input/all-cleaned/biroxiv_clean'#because i pre-processed them using the  cleaner for a former notebook\n#the two followings elements are not used so i comment them: \n\n#COMM_USE_PATH = '/kaggle/input/CORD-19-research-challenge/comm_use_subset/comm_use_subset/'\n#NONCOMM_USE_PATH = '/kaggle/input/CORD-19-research-challenge/noncomm_use_subset/noncomm_use_subset/'\n\nMETADATA_PATH = '/kaggle/input/CORD-19-research-challenge/metadata.csv'\n\nDATA_PATH = '/kaggle/input/CORD-19-research-challenge'\nMODELS_PATH = 'models'\nMODEL_NAME = 'scibert-nli'\nCORPUS_PATH = os.path.join(DATA_PATH, 'corpus.pkl')\nMODEL_PATH = os.path.join(MODELS_PATH, MODEL_NAME)\nEMBEDDINGS_PATH = os.path.join(DATA_PATH, f'{MODEL_NAME}-embeddings.pkl')\n\n\ndef load_json_files(dirname):\n    filenames = [file for file in os.listdir(dirname) if file.endswith('.json')]\n    raw_files = []\n\n    for filename in tqdm(filenames):\n        filename = dirname + filename\n        file = json.load(open(filename, 'rb'))\n        raw_files.append(file)\n    print('Loaded', len(raw_files), 'files from', dirname)\n    return raw_files\n\n\ndef create_corpus_from_json(files):\n    corpus = []\n    for file in tqdm(files):\n         for item in file['abstract']:\n            corpus.append(item['text'])\n        for item in file['body_text']:\n            corpus.append(item['text'])\n    print('Corpus size', len(corpus))\n    return corpus\n\n\ndef cache_corpus(mode='CSV'):\n    corpus = []\n    if mode == 'CSV':\n        df = pd.read_csv(METADATA_PATH)\n        corpus = [a for a in df['abstract'] if type(a) == str and a != \"Unknown\"]\n        print('Corpus size', len(corpus))\n    elif mode == 'JSON':\n        biorxiv_files = load_json_files(BIORXIV_PATH)\n        #comm_use_files = load_json_files(COMM_USE_PATH)\n        #noncomm_use_files = load_json_files(NONCOMM_USE_PATH)\n        corpus = create_corpus_from_json(biorxiv_files) #+ comm_use_files + noncomm_use_files\n    else:\n        raise AttributeError('Mode should be either CSV or JSON')\n    '''with open(CORPUS_PATH, 'wb') as file:\n        pickle.dump(corpus, file)'''\n    return corpus\n\n\ndef ask_question(query, model, corpus, corpus_embed, top_k=5):\n    \"\"\"\n    Adapted from https://www.kaggle.com/dattaraj/risks-of-covid-19-ai-driven-q-a\n    \"\"\"\n    queries = [query]\n    query_embeds = model.encode(queries, show_progress_bar=False)\n    for query, query_embed in zip(queries, query_embeds):\n        distances = scipy.spatial.distance.cdist([query_embed], corpus_embed, \"cosine\")[0]\n        else:\n        print(\"Loading the corpus from\", CORPUS_PATH, '...')\n        with open(CORPUS_PATH, 'rb') as corpus_pt:\n            corpus = pickle.load(corpus_pt)\n\n    model =  SentenceTransformer('bert-base-nli-stsb-mean-tokens')\n\n    if not os.path.exists(EMBEDDINGS_PATH):\n        print(\"Computing and caching model embeddings for future use...\")\n        embeddings = model.encode(corpus, show_progress_bar=True)\n        '''with open(EMBEDDINGS_PATH, 'wb') as file:\n            pickle.dump(embeddings, file)'''\n    else:\n        print(\"Loading model embeddings from\", EMBEDDINGS_PATH, '...')\n        with open(EMBEDDINGS_PATH, 'rb') as file:\n            embeddings = pickle.load(file)\n\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here I adapt the queries to the taks of interest to me, which are, the taks around the genetic question. The tasks are: \n1. Real-time tracking of whole genomes\n2. Access to geographic and temporal diverse sample sets to understand geographic distribution and genomic differences\n3. Evidence of whether farmers are infected\n4. Surveillance of mixed wildlife- livestock farms for SARS-CoV-2\n5. Experimental infections to test host range for this pathogen\n6. Animal host(s) and any evidence of continued spill-over to humans\n7. Experimental infections to test host range for this pathogen\n8. Socioeconomic and behavioral risk factors for this spill-over\n9. Sustainable risk reduction strategies\n","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"questions = ['Real-time tracking of whole genomes', ' Access to geographic and temporal diverse sample sets to understand geographic distribution and genomic differences', 'Evidence of whether farmers are infected', 'Surveillance of mixed wildlife- livestock farms for SARS-CoV-2', 'Experimental infections to test host range for this pathogen', 'Animal host(s) and any evidence of continued spill-over to humans', 'Experimental infections to test host range for this pathogen', 'Socioeconomic and behavioral risk factors for this spill-over', 'Sustainable risk reduction strategies']\nfor i in range(len(questions)):\n        query = questions[i]\n        print(f'Query {i+1} : {query}\\n\\n')\n        results = ask_question(query, model, corpus, embeddings)\n        show_answers(results)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}