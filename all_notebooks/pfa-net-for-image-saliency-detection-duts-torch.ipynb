{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\n### In this notebook we use [Pyramid Feature Attention Network](https://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Pyramid_Feature_Attention_Network_for_Saliency_Detection_CVPR_2019_paper.html) for Saliency Detection on [DUTS Image Saliency Dataset](https://www.kaggle.com/balraj98/duts-temp)."},{"metadata":{},"cell_type":"markdown","source":"<h3><center>Saliency Detection ('Ours' refer to PFA-Net results)</center></h3>\n<img src=\"https://www.programmersought.com/images/496/3089fcd6d75f20061bafc1f0e622f710.png\" width=\"1000\" height=\"1000\"/>\n<h4></h4>\n<h4><center><a href=\"https://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Pyramid_Feature_Attention_Network_for_Saliency_Detection_CVPR_2019_paper.html\">Source: Pyramid Feature Attention Network [Ting Zhao et. al.]</a></center></h4>"},{"metadata":{},"cell_type":"markdown","source":"## Acknowledgements\n\n### This work was inspired by and borrows code from [sairajk](https://github.com/sairajk)'s [PyTorch PFA-Net implementation](https://github.com/sairajk/PyTorch-Pyramid-Feature-Attention-Network-for-Saliency-Detection) & [authors' TensorFlow PFA-Net implementation](https://github.com/CaitinZhao/cvpr2019_Pyramid-Feature-Attention-Network-for-Saliency-detection) with minor corrections and visualizations."},{"metadata":{},"cell_type":"markdown","source":"### Libraries ðŸ“šâ¬‡"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport cv2\nimport glob\nimport tqdm\nimport argparse\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Settings âš™ï¸"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of epochs to train the model for\nepochs = 15\n# Batch size\nbs = 6\n# Learning Rate\nlr = 0.0002\n# L2 Weight decay\nwd = 0.0\n# Image size to be used for training\nimg_size = 256\n# Whether to use Image augmentation\naug = True\n# Number of workers to use for loading data\nn_worker = 2\n# Number of epochs after which to test the weights\ntest_interval = 2\n# Number of epochs after which to save the weights. If None, does not save\nsave_interval = 5\n# Whether to save optimizer along with model weights or not\nsave_opt = False\n# Logging interval (in #batches)\nlog_interval = 5    #250\n# Path to the model to resume from\nres_mod = '../input/pyramid-feature-attention-net-pretrained-weights/best-model_epoch-204_mae-0.0505_loss-0.1370.pth'\n# Path to the optimizer to resume from\nres_opt = None\n# Flag to use GPU or not\nuse_gpu = True\n# Base path for the models to be saved\nbase_save_path = './models'\n### Hyper-parameters for Loss\n# weight for saliency loss\nalpha_sal = 0.7\n# w0 for weighted BCE Loss\nwbce_w0 = 1.0\n# w1 for weighted BCE Loss\nwbce_w1 = 1.15","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Attention Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SpatialAttention(nn.Module):\n    def __init__(self, in_channels, kernel_size=9):\n        super(SpatialAttention, self).__init__()\n\n        self.kernel_size = kernel_size\n        self.in_channels = in_channels\n        pad = (self.kernel_size-1)//2  # Padding on one side for stride 1\n\n        self.grp1_conv1k = nn.Conv2d(self.in_channels, self.in_channels//2, (1, self.kernel_size), padding=(0, pad))\n        self.grp1_bn1 = nn.BatchNorm2d(self.in_channels//2)\n        self.grp1_convk1 = nn.Conv2d(self.in_channels//2, 1, (self.kernel_size, 1), padding=(pad, 0))\n        self.grp1_bn2 = nn.BatchNorm2d(1)\n\n        self.grp2_convk1 = nn.Conv2d(self.in_channels, self.in_channels//2, (self.kernel_size, 1), padding=(pad, 0))\n        self.grp2_bn1 = nn.BatchNorm2d(self.in_channels//2)\n        self.grp2_conv1k = nn.Conv2d(self.in_channels//2, 1, (1, self.kernel_size), padding=(0, pad))\n        self.grp2_bn2 = nn.BatchNorm2d(1)\n\n    def forward(self, input_):\n        # Generate Group 1 Features\n        grp1_feats = self.grp1_conv1k(input_)\n        grp1_feats = F.relu(self.grp1_bn1(grp1_feats))\n        grp1_feats = self.grp1_convk1(grp1_feats)\n        grp1_feats = F.relu(self.grp1_bn2(grp1_feats))\n\n        # Generate Group 2 features\n        grp2_feats = self.grp2_convk1(input_)\n        grp2_feats = F.relu(self.grp2_bn1(grp2_feats))\n        grp2_feats = self.grp2_conv1k(grp2_feats)\n        grp2_feats = F.relu(self.grp2_bn2(grp2_feats))\n\n        added_feats = torch.sigmoid(torch.add(grp1_feats, grp2_feats))\n        added_feats = added_feats.expand_as(input_).clone()\n\n        return added_feats\n\n\nclass ChannelwiseAttention(nn.Module):\n    def __init__(self, in_channels):\n        super(ChannelwiseAttention, self).__init__()\n\n        self.in_channels = in_channels\n\n        self.linear_1 = nn.Linear(self.in_channels, self.in_channels//4)\n        self.linear_2 = nn.Linear(self.in_channels//4, self.in_channels)\n\n    def forward(self, input_):\n        n_b, n_c, h, w = input_.size()\n\n        feats = F.adaptive_avg_pool2d(input_, (1, 1)).view((n_b, n_c))\n        feats = F.relu(self.linear_1(feats))\n        feats = torch.sigmoid(self.linear_2(feats))\n        \n        # Activity regularizer\n        ca_act_reg = torch.mean(feats)\n\n        feats = feats.view((n_b, n_c, 1, 1))\n        feats = feats.expand_as(input_).clone()\n\n        return feats, ca_act_reg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Utility Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# helper function for data visualization\ndef visualize(**images):\n    \"\"\"\n    Plot images in one row\n    \"\"\"\n    n_images = len(images)\n    plt.figure(figsize=(20,8))\n    for idx, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n_images, idx + 1)\n        plt.xticks([]); \n        plt.yticks([])\n        # get title from the parameter names\n        plt.title(name.replace('_',' ').title(), fontsize=20)\n        plt.imshow(image)\n    plt.show()\n    \n    \ndef accuracy(y_pred, y_true):\n    return (y_pred.round() == y_true).float().mean()\n\n\ndef precision(y_pred, y_true):\n    return torch.mul(y_pred.round(), y_true).sum() / y_pred.round().sum()\n\n\ndef recall(y_pred, y_true):\n    return torch.mul(y_pred.round(), y_true).sum() / y_true.sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Get Dataloaders"},{"metadata":{"trusted":true},"cell_type":"code","source":"def pad_resize_image(inp_img, out_img=None, target_size=None):\n    \"\"\"\n    Function to pad and resize images to a given size.\n    out_img is None only during inference. During training and testing\n    out_img is NOT None.\n    :param inp_img: A H x W x C input image.\n    :param out_img: A H x W input image of mask.\n    :param target_size: The size of the final images.\n    :return: Re-sized inp_img and out_img\n    \"\"\"\n    h, w, c = inp_img.shape\n    size = max(h, w)\n\n    padding_h = (size - h) // 2\n    padding_w = (size - w) // 2\n\n    if out_img is None:\n        # For inference\n        temp_x = cv2.copyMakeBorder(inp_img, top=padding_h, bottom=padding_h, left=padding_w, right=padding_w,\n                                    borderType=cv2.BORDER_CONSTANT, value=[0, 0, 0])\n        if target_size is not None:\n            temp_x = cv2.resize(temp_x, (target_size, target_size), interpolation=cv2.INTER_AREA)\n        return temp_x\n    else:\n        # For training and testing\n        temp_x = cv2.copyMakeBorder(inp_img, top=padding_h, bottom=padding_h, left=padding_w, right=padding_w,\n                                    borderType=cv2.BORDER_CONSTANT, value=[0, 0, 0])\n        temp_y = cv2.copyMakeBorder(out_img, top=padding_h, bottom=padding_h, left=padding_w, right=padding_w,\n                                    borderType=cv2.BORDER_CONSTANT, value=[0, 0, 0])\n        # print(inp_img.shape, temp_x.shape, out_img.shape, temp_y.shape)\n\n        if target_size is not None:\n            temp_x = cv2.resize(temp_x, (target_size, target_size), interpolation=cv2.INTER_AREA)\n            temp_y = cv2.resize(temp_y, (target_size, target_size), interpolation=cv2.INTER_AREA)\n        return temp_x, temp_y\n\n\ndef random_crop_flip(inp_img, out_img):\n    \"\"\"\n    Function to randomly crop and flip images.\n    :param inp_img: A H x W x C input image.\n    :param out_img: A H x W input image.\n    :return: The randomly cropped and flipped image.\n    \"\"\"\n    h, w = out_img.shape\n\n    rand_h = np.random.randint(h/8)\n    rand_w = np.random.randint(w/8)\n    offset_h = 0 if rand_h == 0 else np.random.randint(rand_h)\n    offset_w = 0 if rand_w == 0 else np.random.randint(rand_w)\n    p0, p1, p2, p3 = offset_h, h+offset_h-rand_h, offset_w, w+offset_w-rand_w\n\n    rand_flip = np.random.randint(10)\n    if rand_flip >= 5:\n        inp_img = inp_img[::, ::-1, ::]\n        out_img = out_img[::, ::-1]\n\n    return inp_img[p0:p1, p2:p3], out_img[p0:p1, p2:p3]\n\n\ndef random_rotate(inp_img, out_img, max_angle=25):\n    \"\"\"\n    Function to randomly rotate images within +max_angle to -max_angle degrees.\n    This algorithm does NOT crops the edges upon rotation.\n    :param inp_img: A H x W x C input image.\n    :param out_img: A H x W input image.\n    :param max_angle: Maximum angle an image can be rotated in either direction.\n    :return: The randomly rotated image.\n    \"\"\"\n    angle = np.random.randint(-max_angle, max_angle)\n    h, w = out_img.shape\n    center = (w / 2, h / 2)\n\n    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n    cos = np.abs(M[0, 0])\n    sin = np.abs(M[0, 1])\n\n    # Compute new dimensions of the image and adjust the rotation matrix\n    new_w = int((h * sin) + (w * cos))\n    new_h = int((h * cos) + (w * sin))\n    M[0, 2] += (new_w / 2) - center[0]\n    M[1, 2] += (new_h / 2) - center[1]\n\n    return cv2.warpAffine(inp_img, M, (new_w, new_h)), cv2.warpAffine(out_img, M, (new_w, new_h))\n\n\ndef random_rotate_lossy(inp_img, out_img, max_angle=25):\n    \"\"\"\n    Function to randomly rotate images within +max_angle to -max_angle degrees.\n    This algorithm crops the edges upon rotation.\n    :param inp_img: A H x W x C input image.\n    :param out_img: A H x W input image.\n    :param max_angle: Maximum angle an image can be rotated in either direction.\n    :return: The randomly rotated image.\n    \"\"\"\n    angle = np.random.randint(-max_angle, max_angle)\n    h, w = out_img.shape\n    center = (w / 2, h / 2)\n    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n    return cv2.warpAffine(inp_img, M, (w, h)), cv2.warpAffine(out_img, M, (w, h))\n\n\ndef random_brightness(inp_img):\n    \"\"\"\n    Function to randomly perturb the brightness of the input images.\n    :param inp_img: A H x W x C input image.\n    :return: The image with randomly perturbed brightness.\n    \"\"\"\n    contrast = np.random.rand(1) + 0.5\n    light = np.random.randint(-20, 20)\n    inp_img = contrast * inp_img + light\n\n    return np.clip(inp_img, 0, 255)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SODLoader(Dataset):\n    \"\"\"\n    DataLoader for DUTS dataset (for training and testing).\n    \"\"\"\n    def __init__(self, mode='train', augment_data=False, target_size=256):\n        if mode == 'train':\n            self.inp_path = '../input/duts-saliency-detection-dataset/DUTS-TR/DUTS-TR-Image'\n            self.out_path = '../input/duts-saliency-detection-dataset/DUTS-TR/DUTS-TR-Mask'\n        elif mode == 'test':\n            self.inp_path = '../input/duts-saliency-detection-dataset/DUTS-TE/DUTS-TE-Image'\n            self.out_path = '../input/duts-saliency-detection-dataset/DUTS-TE/DUTS-TE-Mask'\n        else:\n            print(\"mode should be either 'train' or 'test'.\")\n            sys.exit(0)\n\n        self.augment_data = augment_data\n        self.target_size = target_size\n        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                              std=[0.229, 0.224, 0.225])\n\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            self.normalize,\n        ])  # Not used\n\n        self.inp_files = sorted(glob.glob(self.inp_path + '/*'))\n        self.out_files = sorted(glob.glob(self.out_path + '/*'))\n\n    def __getitem__(self, idx):\n        inp_img = cv2.imread(self.inp_files[idx])\n        inp_img = cv2.cvtColor(inp_img, cv2.COLOR_BGR2RGB)\n        inp_img = inp_img.astype('float32')\n\n        mask_img = cv2.imread(self.out_files[idx], 0)\n        mask_img = mask_img.astype('float32')\n        mask_img /= np.max(mask_img)\n\n        if self.augment_data:\n            inp_img, mask_img = random_crop_flip(inp_img, mask_img)\n            inp_img, mask_img = random_rotate(inp_img, mask_img)\n            inp_img = random_brightness(inp_img)\n\n        # Pad images to target size\n        inp_img, mask_img = pad_resize_image(inp_img, mask_img, self.target_size)\n        inp_img /= 255.0\n        inp_img = np.transpose(inp_img, axes=(2, 0, 1))\n        inp_img = torch.from_numpy(inp_img).float()\n        inp_img = self.normalize(inp_img)\n\n        mask_img = np.expand_dims(mask_img, axis=0)\n\n        return inp_img, torch.from_numpy(mask_img).float()\n\n    def __len__(self):\n        return len(self.inp_files)\n\n\nclass InfDataloader(Dataset):\n    \"\"\"\n    Dataloader for Inference.\n    \"\"\"\n    def __init__(self, img_folder, target_size=256):\n        self.imgs_folder = img_folder\n        self.img_paths = sorted(glob.glob(self.imgs_folder + '/*'))[:200]\n\n        self.target_size = target_size\n        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                              std=[0.229, 0.224, 0.225])\n\n    def __getitem__(self, idx):\n        \"\"\"\n        __getitem__ for inference\n        :param idx: Index of the image\n        :return: img_np is a numpy RGB-image of shape H x W x C with pixel values in range 0-255.\n        And img_tor is a torch tensor, RGB, C x H x W in shape and normalized.\n        \"\"\"\n        img = cv2.imread(self.img_paths[idx])\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        # Pad images to target size\n        img_np = pad_resize_image(img, None, self.target_size)\n        img_tor = img_np.astype(np.float32)\n        img_tor = img_tor / 255.0\n        img_tor = np.transpose(img_tor, axes=(2, 0, 1))\n        img_tor = torch.from_numpy(img_tor).float()\n        img_tor = self.normalize(img_tor)\n\n        return img_np, img_tor\n\n    def __len__(self):\n        return len(self.img_paths)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test Dataloader\nimg_size = 256\nbs = 8\n\ntrain_data = SODLoader(mode='train', augment_data=False, target_size=img_size)\ntest_data = SODLoader(mode='test', augment_data=False, target_size=img_size)\n\ntrain_dataloader = DataLoader(train_data, batch_size=bs, shuffle=True, num_workers=1)\ntest_dataloader = DataLoader(test_data, batch_size=bs, shuffle=False, num_workers=2)\n\nprint(\"Train Dataloader :\")\nfor batch_idx, (inp_imgs, gt_masks) in enumerate(train_dataloader):\n    print('Loop :', batch_idx, inp_imgs.size(), gt_masks.size())\n    if batch_idx == 3:\n        break\n\nprint(\"\\nTest Dataloader :\")\nfor batch_idx, (inp_imgs, gt_masks) in enumerate(test_dataloader):\n    print('Loop :', batch_idx, inp_imgs.size(), gt_masks.size())\n    if batch_idx == 3:\n        break\n\n# Test image augmentation functions\ninp_img = cv2.imread('../input/duts-saliency-detection-dataset/DUTS-TE/DUTS-TE-Image/ILSVRC2012_test_00000003.jpg')\nout_img = cv2.imread('../input/duts-saliency-detection-dataset/DUTS-TE/DUTS-TE-Mask/ILSVRC2012_test_00000003.png', -1)\n# inp_img = inp_img.astype('float32')\nout_img = out_img.astype('float32')\nout_img = out_img / 255.0\n\nprint('\\nImage shapes before processing :', inp_img.shape, out_img.shape)\nx, y = random_crop_flip(inp_img, out_img)\nx, y = random_rotate(x, y)\nx = random_brightness(x)\nx, y = pad_resize_image(x, y, target_size=256)\n# x now contains float values, so either round-off the values or convert the pixel range to 0-1.\nx = x / 255.0\nprint('Image shapes after processing :', x.shape, y.shape)\n\nvisualize(\n    input_image = inp_img[:,:,::-1],\n    gt_saliency_map = out_img,\n    processed_image = x[:,:,::-1],\n    processed_saliency_map = y,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Saliency Loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"class EdgeSaliencyLoss(nn.Module):\n    def __init__(self, device, alpha_sal=0.7):\n        super(EdgeSaliencyLoss, self).__init__()\n\n        self.alpha_sal = alpha_sal\n\n        self.laplacian_kernel = torch.tensor([[-1., -1., -1.], [-1., 8., -1.], [-1., -1., -1.]], dtype=torch.float, requires_grad=False)\n        self.laplacian_kernel = self.laplacian_kernel.view((1, 1, 3, 3))  # Shape format of weight for convolution\n        self.laplacian_kernel = self.laplacian_kernel.to(device)\n\n    @staticmethod\n    def weighted_bce(input_, target, weight_0=1.0, weight_1=1.0, eps=1e-15):\n        wbce_loss = -weight_1 * target * torch.log(input_ + eps) - weight_0 * (1 - target) * torch.log(\n            1 - input_ + eps)\n        return torch.mean(wbce_loss)\n\n    def forward(self, y_pred, y_gt):\n        # Generate edge maps\n        y_gt_edges = F.relu(torch.tanh(F.conv2d(y_gt, self.laplacian_kernel, padding=(1, 1))))\n        y_pred_edges = F.relu(torch.tanh(F.conv2d(y_pred, self.laplacian_kernel, padding=(1, 1))))\n\n        # sal_loss = F.binary_cross_entropy(input=y_pred, target=y_gt)\n        sal_loss = self.weighted_bce(input_=y_pred, target=y_gt, weight_0=1.0, weight_1=1.12)\n        edge_loss = F.binary_cross_entropy(input=y_pred_edges, target=y_gt_edges)\n\n        total_loss = self.alpha_sal * sal_loss + (1 - self.alpha_sal) * edge_loss\n        return total_loss\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><center>Pyramid Feature Attention Network Model Architecture</center></h3>\n<img src=\"https://d3i71xaburhd42.cloudfront.net/34ae14e98dee344289cd7afcd446c2aaf31899cf/3-Figure2-1.png\" width=\"1000\" height=\"750\"/>\n<h4></h4>\n<h4><center><a href=\"https://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Pyramid_Feature_Attention_Network_for_Saliency_Detection_CVPR_2019_paper.html\">Source: Pyramid Feature Attention Network [Ting Zhao et. al.]</a></center></h4>"},{"metadata":{},"cell_type":"markdown","source":"### Model Definition"},{"metadata":{"trusted":true},"cell_type":"code","source":"vgg_conv1_2 = vgg_conv2_2 = vgg_conv3_3 = vgg_conv4_3 = vgg_conv5_3 = None\n\n\ndef conv_1_2_hook(module, input, output):\n    global vgg_conv1_2\n    vgg_conv1_2 = output\n    return None\n\n\ndef conv_2_2_hook(module, input, output):\n    global vgg_conv2_2\n    vgg_conv2_2 = output\n    return None\n\n\ndef conv_3_3_hook(module, input, output):\n    global vgg_conv3_3\n    vgg_conv3_3 = output\n    return None\n\n\ndef conv_4_3_hook(module, input, output):\n    global vgg_conv4_3\n    vgg_conv4_3 = output\n    return None\n\n\ndef conv_5_3_hook(module, input, output):\n    global vgg_conv5_3\n    vgg_conv5_3 = output\n    return None\n\n\nclass CPFE(nn.Module):\n    def __init__(self, feature_layer=None, out_channels=32):\n        super(CPFE, self).__init__()\n\n        self.dil_rates = [3, 5, 7]\n\n        # Determine number of in_channels from VGG-16 feature layer\n        if feature_layer == 'conv5_3':\n            self.in_channels = 512\n        elif feature_layer == 'conv4_3':\n            self.in_channels = 512\n        elif feature_layer == 'conv3_3':\n            self.in_channels = 256\n\n        # Define layers\n        self.conv_1_1 = nn.Conv2d(in_channels=self.in_channels, out_channels=out_channels, kernel_size=1, bias=False)\n        self.conv_dil_3 = nn.Conv2d(in_channels=self.in_channels, out_channels=out_channels, kernel_size=3,\n                                    stride=1, dilation=self.dil_rates[0], padding=self.dil_rates[0], bias=False)\n        self.conv_dil_5 = nn.Conv2d(in_channels=self.in_channels, out_channels=out_channels, kernel_size=3,\n                                    stride=1, dilation=self.dil_rates[1], padding=self.dil_rates[1], bias=False)\n        self.conv_dil_7 = nn.Conv2d(in_channels=self.in_channels, out_channels=out_channels, kernel_size=3,\n                                    stride=1, dilation=self.dil_rates[2], padding=self.dil_rates[2], bias=False)\n\n        self.bn = nn.BatchNorm2d(out_channels*4)\n\n    def forward(self, input_):\n        # Extract features\n        conv_1_1_feats = self.conv_1_1(input_)\n        conv_dil_3_feats = self.conv_dil_3(input_)\n        conv_dil_5_feats = self.conv_dil_5(input_)\n        conv_dil_7_feats = self.conv_dil_7(input_)\n\n        # Aggregate features\n        concat_feats = torch.cat((conv_1_1_feats, conv_dil_3_feats, conv_dil_5_feats, conv_dil_7_feats), dim=1)\n        bn_feats = F.relu(self.bn(concat_feats))\n\n        return bn_feats\n\n\nclass SODModel(nn.Module):\n    def __init__(self):\n        super(SODModel, self).__init__()\n\n        # Load the [partial] VGG-16 model\n        self.vgg16 = models.vgg16(pretrained=True).features\n\n        # Extract and register intermediate features of VGG-16\n        self.vgg16[3].register_forward_hook(conv_1_2_hook)\n        self.vgg16[8].register_forward_hook(conv_2_2_hook)\n        self.vgg16[15].register_forward_hook(conv_3_3_hook)\n        self.vgg16[22].register_forward_hook(conv_4_3_hook)\n        self.vgg16[29].register_forward_hook(conv_5_3_hook)\n\n        # Initialize layers for high level (hl) feature (conv3_3, conv4_3, conv5_3) processing\n        self.cpfe_conv3_3 = CPFE(feature_layer='conv3_3')\n        self.cpfe_conv4_3 = CPFE(feature_layer='conv4_3')\n        self.cpfe_conv5_3 = CPFE(feature_layer='conv5_3')\n\n        self.cha_att = ChannelwiseAttention(in_channels=384)  # in_channels = 3 x (32 x 4)\n\n        self.hl_conv1 = nn.Conv2d(384, 64, (3, 3), padding=1)\n        self.hl_bn1 = nn.BatchNorm2d(64)\n\n        # Initialize layers for low level (ll) feature (conv1_2 and conv2_2) processing\n        self.ll_conv_1 = nn.Conv2d(64, 64, (3, 3), padding=1)\n        self.ll_bn_1 = nn.BatchNorm2d(64)\n        self.ll_conv_2 = nn.Conv2d(128, 64, (3, 3), padding=1)\n        self.ll_bn_2 = nn.BatchNorm2d(64)\n        self.ll_conv_3 = nn.Conv2d(128, 64, (3, 3), padding=1)\n        self.ll_bn_3 = nn.BatchNorm2d(64)\n\n        self.spa_att = SpatialAttention(in_channels=64)\n\n        # Initialize layers for fused features (ff) processing\n        self.ff_conv_1 = nn.Conv2d(128, 1, (3, 3), padding=1)\n\n    def forward(self, input_):\n        global vgg_conv1_2, vgg_conv2_2, vgg_conv3_3, vgg_conv4_3, vgg_conv5_3\n\n        # Pass input_ through vgg16 to generate intermediate features\n        self.vgg16(input_)\n\n        # Process high level features\n        conv3_cpfe_feats = self.cpfe_conv3_3(vgg_conv3_3)\n        conv4_cpfe_feats = self.cpfe_conv4_3(vgg_conv4_3)\n        conv5_cpfe_feats = self.cpfe_conv5_3(vgg_conv5_3)\n\n        conv4_cpfe_feats = F.interpolate(conv4_cpfe_feats, scale_factor=2, mode='bilinear', align_corners=True)\n        conv5_cpfe_feats = F.interpolate(conv5_cpfe_feats, scale_factor=4, mode='bilinear', align_corners=True)\n\n        conv_345_feats = torch.cat((conv3_cpfe_feats, conv4_cpfe_feats, conv5_cpfe_feats), dim=1)\n\n        conv_345_ca, ca_act_reg = self.cha_att(conv_345_feats)\n        conv_345_feats = torch.mul(conv_345_feats, conv_345_ca)\n\n        conv_345_feats = self.hl_conv1(conv_345_feats)\n        conv_345_feats = F.relu(self.hl_bn1(conv_345_feats))\n        conv_345_feats = F.interpolate(conv_345_feats, scale_factor=4, mode='bilinear', align_corners=True)\n\n        # Process low level features\n        conv1_feats = self.ll_conv_1(vgg_conv1_2)\n        conv1_feats = F.relu(self.ll_bn_1(conv1_feats))\n        conv2_feats = self.ll_conv_2(vgg_conv2_2)\n        conv2_feats = F.relu(self.ll_bn_2(conv2_feats))\n\n        conv2_feats = F.interpolate(conv2_feats, scale_factor=2, mode='bilinear', align_corners=True)\n        conv_12_feats = torch.cat((conv1_feats, conv2_feats), dim=1)\n        conv_12_feats = self.ll_conv_3(conv_12_feats)\n        conv_12_feats = F.relu(self.ll_bn_3(conv_12_feats))\n\n        conv_12_sa = self.spa_att(conv_345_feats)\n        conv_12_feats = torch.mul(conv_12_feats, conv_12_sa)\n\n        # Fused features\n        fused_feats = torch.cat((conv_12_feats, conv_345_feats), dim=1)\n        fused_feats = torch.sigmoid(self.ff_conv_1(fused_feats))\n\n        return fused_feats, ca_act_reg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Engine:\n    def __init__(self):\n        self.epochs = epochs\n        self.bs = bs\n        self.lr = lr\n        self.wd = wd\n        self.img_size = img_size\n        self.aug = aug\n        self.n_worker = n_worker\n        self.test_interval = test_interval\n        self.save_interval = save_interval\n        self.save_opt = save_opt\n        self.log_interval = log_interval\n        self.res_mod_path = res_mod\n        self.res_opt_path = res_opt\n        self.use_gpu = use_gpu\n\n        self.alpha_sal = alpha_sal\n        self.wbce_w0 = wbce_w0\n        self.wbce_w1 = wbce_w1\n\n        self.model_path = base_save_path + '/alph-{}_wbce_w0-{}_w1-{}'.format(str(self.alpha_sal), str(self.wbce_w0), str(self.wbce_w1))\n        print('Models would be saved at : {}\\n'.format(self.model_path))\n        if not os.path.exists(os.path.join(self.model_path, 'weights')):\n            os.makedirs(os.path.join(self.model_path, 'weights'))\n        if not os.path.exists(os.path.join(self.model_path, 'optimizers')):\n            os.makedirs(os.path.join(self.model_path, 'optimizers'))\n\n        if torch.cuda.is_available():\n            self.device = torch.device(device='cuda')\n        else:\n            self.device = torch.device(device='cpu')\n\n        self.model = SODModel()\n        self.model.to(self.device)\n        self.criterion = EdgeSaliencyLoss(device=self.device)\n        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.wd)\n\n        # Load model and optimizer if resumed\n        if self.res_mod_path is not None:\n            chkpt = torch.load(self.res_mod_path, map_location=self.device)\n            self.model.load_state_dict(chkpt['model'])\n            print(\"Resuming training with checkpoint : {}\\n\".format(self.res_mod_path))\n        if self.res_opt_path is not None:\n            chkpt = torch.load(self.res_opt_path, map_location=self.device)\n            self.optimizer.load_state_dict(chkpt['optimizer'])\n            print(\"Resuming training with optimizer : {}\\n\".format(self.res_opt_path))\n\n        self.train_data = SODLoader(mode='train', augment_data=self.aug, target_size=self.img_size)\n        self.test_data = SODLoader(mode='test', augment_data=False, target_size=self.img_size)\n        self.train_dataloader = DataLoader(self.train_data, batch_size=self.bs, shuffle=True, num_workers=self.n_worker)\n        self.test_dataloader = DataLoader(self.test_data, batch_size=self.bs, shuffle=False, num_workers=self.n_worker)\n\n    def train(self):\n        best_test_mae = float('inf')\n        for epoch in range(self.epochs):\n            self.model.train()\n            for batch_idx, (inp_imgs, gt_masks) in enumerate(self.train_dataloader):\n                inp_imgs = inp_imgs.to(self.device)\n                gt_masks = gt_masks.to(self.device)\n\n                self.optimizer.zero_grad()\n                pred_masks, ca_act_reg = self.model(inp_imgs)\n                loss = self.criterion(pred_masks, gt_masks) + ca_act_reg  # Activity regularizer from Channel-wise Att.\n\n                loss.backward()\n                self.optimizer.step()\n\n                if batch_idx % self.log_interval == 0:\n                    print('\\rTRAIN :: Epoch : {}\\tBatch : {}/{} ({:.2f}%)\\t\\tTot Loss : {:.4f}\\tReg : {:.4f}'\n                          .format(epoch + 1,\n                                  batch_idx + 1, len(self.train_dataloader),\n                                  (batch_idx + 1) * 100 / len(self.train_dataloader),\n                                  loss.item(),\n                                  ca_act_reg), \n                                  end='', flush=True)\n\n            # Validation\n            if epoch % self.test_interval == 0 or epoch % self.save_interval == 0:\n                te_avg_loss, te_acc, te_pre, te_rec, te_mae = self.test()\n                mod_chkpt = {'epoch': epoch,\n                            'test_mae' : float(te_mae),\n                            'model' : self.model.state_dict(),\n                            'test_loss': float(te_avg_loss),\n                            'test_acc': float(te_acc),\n                            'test_pre': float(te_pre),\n                            'test_rec': float(te_rec)}\n\n                if self.save_opt:\n                    opt_chkpt = {'epoch': epoch,\n                                'test_mae' : float(te_mae),\n                                'optimizer': self.optimizer.state_dict(),\n                                'test_loss': float(te_avg_loss),\n                                'test_acc': float(te_acc),\n                                'test_pre': float(te_pre),\n                                'test_rec': float(te_rec)}\n\n                # Save the best model\n                if te_mae < best_test_mae:\n                    best_test_mae = te_mae\n                    torch.save(mod_chkpt, self.model_path + '/weights/best-model_epoch-{:03}_mae-{:.4f}_loss-{:.4f}.pth'.\n                               format(epoch, best_test_mae, te_avg_loss))\n                    if self.save_opt:\n                        torch.save(opt_chkpt, self.model_path + '/optimizers/best-opt_epoch-{:03}_mae-{:.4f}_loss-{:.4f}.pth'.\n                                   format(epoch, best_test_mae, te_avg_loss))\n                    print('Best Model Saved !!!\\n')\n                    continue\n                \n                # Save model at regular intervals\n                if self.save_interval is not None and epoch % self.save_interval == 0:\n                    torch.save(mod_chkpt, self.model_path + '/weights/model_epoch-{:03}_mae-{:.4f}_loss-{:.4f}.pth'.\n                               format(epoch, te_mae, te_avg_loss))\n                    if self.save_opt:\n                        torch.save(opt_chkpt, self.model_path + '/optimizers/opt_epoch-{:03}_mae-{:.4f}_loss-{:.4f}.pth'.\n                                   format(epoch, best_test_mae, te_avg_loss))\n                    print('Model Saved !!!\\n')\n                    continue\n            print('\\n')\n\n    def test(self):\n        self.model.eval()\n\n        tot_loss = 0\n        tp_fp = 0   # TruePositive + TrueNegative, for accuracy\n        tp = 0      # TruePositive\n        pred_true = 0   # Number of '1' predictions, for precision\n        gt_true = 0     # Number of '1's in gt mask, for recall\n        mae_list = []   # List to save mean absolute error of each image\n\n        with torch.no_grad():\n            for batch_idx, (inp_imgs, gt_masks) in enumerate(self.test_dataloader, start=1):\n                inp_imgs = inp_imgs.to(self.device)\n                gt_masks = gt_masks.to(self.device)\n\n                pred_masks, ca_act_reg = self.model(inp_imgs)\n                loss = self.criterion(pred_masks, gt_masks) + ca_act_reg\n\n                tot_loss += loss.item()\n\n                tp_fp += (pred_masks.round() == gt_masks).float().sum()\n                tp += torch.mul(pred_masks.round(), gt_masks).sum()\n                pred_true += pred_masks.round().sum()\n                gt_true += gt_masks.sum()\n\n                # Record the absolute errors\n                ae = torch.mean(torch.abs(pred_masks - gt_masks), dim=(1, 2, 3)).cpu().numpy()\n                mae_list.extend(ae)\n\n        avg_loss = tot_loss / batch_idx\n        accuracy = tp_fp / (len(self.test_data) * self.img_size * self.img_size)\n        precision = tp / pred_true\n        recall = tp / gt_true\n        mae = np.mean(mae_list)\n\n        print('\\rTEST :: MAE : {:.4f}\\tACC : {:.4f}\\tPRE : {:.4f}\\tREC : {:.4f}\\tAVG-LOSS : {:.4f}\\n'.format(mae,\n                                                                                             accuracy, precision,\n                                                                                             recall, avg_loss), \n                                                                                             end='', flush=True)\n\n        return avg_loss, accuracy, precision, recall, mae","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Driver class\ntrainer = Engine()\ntrainer.train()\ntrainer.test()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Path to folder containing images\nimgs_folder = '../input/duts-saliency-detection-dataset/DUTS-TE/DUTS-TE-Image'\n# Path to model\nmodel_path = sorted(glob.glob('./models/alph-0.7_wbce_w0-1.0_w1-1.15/weights/best-model_epoch-*'), reverse=True)[0]\n# Whether to use GPU or not\nuse_gpu = True\n# Image size to be used\nimg_size = 256\n# Batch Size for testing\nbs = 24\n\nsample_preds_folder = 'sample_predictions/'\nif not os.path.exists(sample_preds_folder):\n    os.makedirs(sample_preds_folder)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_inference():\n    # Determine device\n    if use_gpu and torch.cuda.is_available():\n        device = torch.device(device='cuda')\n    else:\n        device = torch.device(device='cpu')\n\n    # Load model\n    model = SODModel()\n    chkpt = torch.load(model_path, map_location=device)\n    model.load_state_dict(chkpt['model'])\n    model.to(device)\n    model.eval()\n\n    inf_data = InfDataloader(img_folder=imgs_folder, target_size=img_size)\n    # Since the images would be displayed to the user, the batch_size is set to 1\n    # Code at later point is also written assuming batch_size = 1, so do not change\n    inf_dataloader = DataLoader(inf_data, batch_size=1, shuffle=True, num_workers=2)\n\n    with torch.no_grad():\n        for batch_idx, (img_np, img_tor) in enumerate(inf_dataloader, start=1):\n            img_tor = img_tor.to(device)\n            pred_masks, _ = model(img_tor)\n\n            # Assuming batch_size = 1\n            img_np = np.squeeze(img_np.numpy(), axis=0)\n            img_np = img_np.astype(np.uint8)\n            pred_masks_raw = np.squeeze(pred_masks.cpu().numpy(), axis=(0, 1))\n            pred_masks_round = np.squeeze(pred_masks.round().cpu().numpy(), axis=(0, 1))\n        \n            visualize(\n                input_image = img_np,\n                generated_saliency_map = pred_masks_raw,\n                rounded_off_saliency_map = pred_masks_round,\n            )\n            \n\ndef calculate_mae():\n    # Determine device\n    if use_gpu and torch.cuda.is_available():\n        device = torch.device(device='cuda')\n    else:\n        device = torch.device(device='cpu')\n\n    # Load model\n    model = SODModel()\n    chkpt = torch.load(model_path, map_location=device)\n    model.load_state_dict(chkpt['model'])\n    model.to(device)\n    model.eval()\n\n    test_data = SODLoader(mode='test', augment_data=False, target_size=img_size)\n    test_dataloader = DataLoader(test_data, batch_size=bs, shuffle=False, num_workers=2)\n\n    # List to save mean absolute error of each image\n    mae_list = []\n    with torch.no_grad():\n        for batch_idx, (inp_imgs, gt_masks) in enumerate(tqdm.tqdm(test_dataloader), start=1):\n            inp_imgs = inp_imgs.to(device)\n            gt_masks = gt_masks.to(device)\n            pred_masks, _ = model(inp_imgs)\n\n            mae = torch.mean(torch.abs(pred_masks - gt_masks), dim=(1, 2, 3)).cpu().numpy()\n            mae_list.extend(mae)\n\n    print('MAE for the test set is :', np.mean(mae_list))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calculate_mae()\nrun_inference()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}