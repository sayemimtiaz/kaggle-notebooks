{"cells":[{"metadata":{"_uuid":"24005732e1a4a6ab1825ff897578982e9ba4b74b","_cell_guid":"5bf60605-91d5-c530-003e-d2667ff06d69"},"cell_type":"markdown","source":"# A CLUSTER OF COLORS\n#### **PCA APPROACH**"},{"metadata":{"_uuid":"c05564c3c74d1411e3089e3d70097e1cfae1891a","_cell_guid":"45930e60-1dac-0ed6-c761-8e4785305893"},"cell_type":"markdown","source":"# 1. Introduction\n\nThis notebook is not meant to be an exhaustive EDA nor will it attempt to run fancy stuff like XGBoosting or Ensembling methods. The chief focus of this script will simply be to try out sklearn's PCA (Principal Decomposition Analysis) method on a small dataset, hence the choice to only look at mushroom colors. Therefore this notebook is organized as follows:\n\n - Label encoding the categorical values\n - Pearson Correlation to investigate any linear dependence on the color features \n - PCA and KMeans clustering for visualization"},{"metadata":{"_uuid":"a19f35ec5a4ae7648f4c028a31448743bdeec604","_cell_guid":"ddd0414b-a55b-e4e4-25f3-f536e23ead9d"},"cell_type":"markdown","source":"# 2. Extracting only the color features \nThe first step is to extract all the features in the dataset that point to the colors of the mushroom. Inspecting the data, we see that there are 6 columns (features) that allude to colors\n\n 1. cap-color \n 2. gill-color\n 3. stalk-color-above-ring\n 4. stalk-color-below-ring\n 5. veil-color\n 6. spore-print-color"},{"metadata":{"_uuid":"48eeb7df0299ccc2ce4a32c6dd45e6d43588863a","_cell_guid":"69f8898f-f459-084d-cdd9-2ebf5d10865e","trusted":true},"cell_type":"code","source":"# Importing the usual libraries\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns \nimport matplotlib.pyplot as plt\n\n# Read in the mushroom data into a dataframe called \"data\" - what a creative name\ndata = pd.read_csv(\"../input/mushrooms.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6bceaf0d1eb088a0f7913ce88ee29daa2670fe2","_cell_guid":"ec0ef589-b981-543a-c17a-51ccebb8a2d2"},"cell_type":"markdown","source":"Therefore let's extract these 6 color columns into its own dataframe (data_color)"},{"metadata":{"_uuid":"7ad02868a07d5e42da7944c2a5484ace19446130","_cell_guid":"6a85bedc-2c28-a8b8-0287-ae4d55dfa85b","trusted":true},"cell_type":"code","source":"# I use a list \"color_features\" to store the color column names. \n# Not really sure if there is an easier way to do this. Do let me know if there is\ncolor_features = []\nfor i in data.columns:\n    if 'color' in i:\n        color_features.append(i)\n# create our color dataframe and inspect first 5 rows with head()\ndata_color = data[color_features]\ndata_color.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c686f055ce762a4e3be1ea158c93e0a7b4edcf2f","_cell_guid":"9166cf35-d753-1cd5-6824-c779996949b2"},"cell_type":"markdown","source":"### Encoding categorical values\nWe see that the colors are all categorical values. Therefore we need to encode. Since the color's categorical value correspond to one another across columns, I want to ensure that the encoding provides the same output across all columns.  Therefore my idea was to create a dictionary that contains the encoding for the unique values across the dataframe. "},{"metadata":{"_uuid":"5686a8a5280d148be627f6105e6fd63a95388afb","_cell_guid":"a54dde11-10c0-195b-5618-82fdb1a9551c","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n# List to store all unique categories\nListToEncode = pd.Series(data_color.values.ravel()).unique()\n# Use sklearn Labelencoder for transformation\nEncodedList = LabelEncoder().fit_transform(ListToEncode)\n\n# Define a dictionary \"encodedict\" to store our encoding\nencodedict = {}\nfor i in range(0, len(EncodedList)):\n    encodedict.update({ListToEncode[i]:EncodedList[i]})\n\n# Finally use dictionary to generate encoded dataframe\nfor i in range(len(data_color.columns)):\n    for j in range(len(data_color['cap-color'].values)):\n        data_color.values[j][i] =  encodedict[data_color.values[j][i]]\ndata_color.head()       \n  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c972f2a6fd4eafdbb3c5ada46b9762652eb54a4b","_cell_guid":"d1a90423-189e-6bb8-730b-6156dcc2aa5e"},"cell_type":"markdown","source":"# 2. Correlation of color features\nNow let's look at the Pearson correlation of the color features as a sort of first attempt to identify how linearly related they are to one another."},{"metadata":{"_uuid":"eca12f3c9e8070bafe707a7c210608e8c76b0f94","_cell_guid":"2ac28dc4-b165-20bf-a76f-7c3edcb74dee","trusted":false},"cell_type":"code","source":"# correlation matrix using the corr() method\ndata_corr = data_color.astype(float).corr()  # used the astype() or else I get empty results\ndata_corr","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b6c45fd35fdd7ca8f2519809e973488929c5f1e","_cell_guid":"8c6bf94b-9cd0-18ec-2c6c-0fc9339061de"},"cell_type":"markdown","source":"And to visualize this with a more swanky heatmap that everyone is using these days."},{"metadata":{"_uuid":"4392b535ee87bffe8794fbd2c224ec608557893f","_cell_guid":"c098026f-8d31-3808-1cbe-f6ffbad4317e","trusted":false},"cell_type":"code","source":"# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(7, 7))\nplt.title('Pearson Correlation of Mushroom Features')\n# Draw the heatmap using seaborn\nsns.heatmap(data_color.astype(float).corr(),linewidths=0.5,vmax=1.0, square=True, annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f723fe993947002ca9783ab22a0be3494e85ae2","_cell_guid":"d9fc1c33-ba47-176a-0376-b3455700718a"},"cell_type":"markdown","source":"Seems that from this heatmap, we identify about 2 or 3 features that have some weakly to medium positive linear correlation with one another. Therefore as a rough heuristic, let's look at PCA-ing the features into 3 components. "},{"metadata":{"_uuid":"05615b9fcb3ef1039e523fc89aa6cd9e8f42db03","_cell_guid":"d6a8b5cf-bc15-39c0-fbad-64af8b90438d"},"cell_type":"markdown","source":"# 3. Principal Component Analysis with KMeans Clustering"},{"metadata":{"_uuid":"3451de8372430cd5da6f909e44bce62d16dd06bd","_cell_guid":"d0184709-a592-7b0c-fdd3-a3b249a968df"},"cell_type":"markdown","source":"Thankfully, the immense power of the sklearn module can be utilized to implement Principal Component Analysis conveniently. Check out the official sklearn link for a more detailed explanation : http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n\nWe also import the KMeans method so that we can use KMeans clustering to extract our PCA components."},{"metadata":{"_uuid":"9318f715bf205ed70f6de278f0ae7a17800e597f","_cell_guid":"6aecc34a-a6e2-8f90-c06a-27e2f034511c","trusted":true},"cell_type":"code","source":"# import the relevant modules\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba5fa6c38b7f2feff30945ba966f6c23ef49cb27","_cell_guid":"09fac8fe-9642-cd94-7906-8e1aaa2746aa"},"cell_type":"markdown","source":"From the code below, since I am going to look at PCA with 3 components, therefore I assign the PCA parameter \"n_components\" to be equal to 3. The method of \"fit_transform\" fits the model with X ( mushroom color values ) and then reduces the dimensions of X to our stated 3 dimensions."},{"metadata":{"_uuid":"3e9f50ca8c7a8f59bcc04801dff662a33ee36ef5","_cell_guid":"71d1c9ce-f995-a3a2-4265-1054ab9d5fcb","trusted":true},"cell_type":"code","source":"X = data_color.values\n# calling sklearn PCA \npca = PCA(n_components=3)\n# fit X and apply the reduction to X \nx_3d = pca.fit_transform(X)\n\n# Let's see how it looks like in 2D - could do a 3D plot as well\nplt.figure(figsize = (7,7))\nplt.scatter(x_3d[:,0],x_3d[:,1], alpha=0.1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ed664393672fb3de890a47b9a7a566f23548265","_cell_guid":"d0fbe47f-ebc7-0c02-b7f9-1ef1b39c1399"},"cell_type":"markdown","source":"With this 2D plot of the PCA projections, let's try to apply a simple KMeans and see if we can identify any clusters from the projections."},{"metadata":{"_uuid":"2455cf7af61a111d128008bb96a0fe3b8c626009","_cell_guid":"51ad6a87-d142-20a9-6bd0-d9355b328d4c","trusted":true},"cell_type":"code","source":"# Set a 3 KMeans clustering\nkmeans = KMeans(n_clusters=7, random_state=0)\n# Compute cluster centers and predict cluster indices\nX_clustered = kmeans.fit_predict(x_3d)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a9f7ba92c4a0265003e8a6db809aa8631f8cda7","_cell_guid":"10f17228-4bc5-731d-4c57-707f8e6d4e87"},"cell_type":"markdown","source":"Simple visualisation of the 3 clusters with a pre-defined color map"},{"metadata":{"_uuid":"aae8440a63361f2bdbd18d23bccb0d329b2c12be","_cell_guid":"f447135a-7fc8-5767-5ce7-67e7de644ec5","trusted":true},"cell_type":"code","source":"LABEL_COLOR_MAP = {0 : 'limegreen',\n                   1 : 'steelblue',\n                   2 : 'mediumaquamarine', \n                   3 : 'seagreen', \n                   4 : 'slategray', \n                   5 : 'skyblue', \n                   6 : 'yellowgreen'}\n\nlabel_color = [LABEL_COLOR_MAP[l] for l in X_clustered]\nplt.figure(figsize = (7,7))\nplt.scatter(x_3d[:,0],x_3d[:,1], c= label_color, alpha=0.1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13aba6bb988af181592be32a53d50ffecdd3cdba","_cell_guid":"3df9c089-77f4-3c7f-9edd-7b1764b855a0"},"cell_type":"markdown","source":"# CLOSING REMARKS\n\nSince I'm only starting out and am still very green behind the ears around data science, I will stop my notebook at this juncture. However, this PCA decomposition coupled with KMeans clustering (or other clustering methods) can be quite powerful, especially when you imagine that your dataset features contain 100s or 1000s of columns you are able to scale it down by an order of magnitude via this method. To take this further from a qualitative point of view, one would then extract the KMeans clusters and use those as new features in training the Machine Learning model should the effect of this dimensionality reduction + clustering prove helpful. \n\nPlease feel free to leave comments and thoughts on how I could improve this notebook from a data science point of view or plotting point of view or organisational point of view or views from any other point.  :)"},{"metadata":{"_uuid":"3c9a515bd51435d46d4867e64f1af7af3520f57a","_cell_guid":"6d9195a1-4ff0-c419-253f-b1234c79d2da","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"version":"3.5.2","nbconvert_exporter":"python","mimetype":"text/x-python","name":"python","file_extension":".py","pygments_lexer":"ipython3"},"_is_fork":false,"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"_change_revision":0},"nbformat":4,"nbformat_minor":1}