{"cells":[{"metadata":{},"cell_type":"markdown","source":"# üì• Importing needed libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport cv2\nimport time\nfrom timeit import default_timer as timer\nimport matplotlib.pyplot as plt\nimport pickle\n\nfrom keras.models import load_model\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('../input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\nprint(os.listdir('../input'))\n\n# Any results we write to the current directory are saved as output\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# üìÇ Loading *labels*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading csv file with labels' names\n# Loading two columns [0, 1] into Pandas dataFrame\nlabels = pd.read_csv('../input/traffic-signs-preprocessed/label_names.csv')\n\n# Check point\n# Showing first 5 rows from the dataFrame\nprint(labels.head())\nprint()\n\n# To locate by class number use one of the following\n# ***.iloc[0][1] - returns element on the 0 column and 1 row\nprint(labels.iloc[0][1])  # Speed limit (20km/h)\n# ***['SignName'][1] - returns element on the column with name 'SignName' and 1 row\nprint(labels['SignName'][1]) # Speed limit (30km/h)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# üìç Loading trained Keras CNN model for Classification"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Loading trained CNN model to use it later when classifying from 4 groups into one of 43 classes\n#model = load_model('../input/model3x3/model-3x3.h5')\n# Better CNN model (larger resolution of filter)\n#model = load_model('../input/model5x5/model-5x5.h5')\n# Najbetter (uczony na 10k)\nmodel = load_model('../input/modele/model-3x3 (1).h5')\n\n# Loading mean image to use for preprocessing further\n# Opening file for reading in binary mode\nwith open('../input/traffic-signs-preprocessed/mean_image_rgb.pickle', 'rb') as f:\n    mean = pickle.load(f, encoding='latin1')  # dictionary type\n \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Parsing markings dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nimport random\nfrom PIL import Image\n\npath_prefix = \"/root/darknet/rmarkings\"\nvalidate_percentage = 0.2\n\ndef RepresentsInt(s):\n    try:\n        int(s)\n        return True\n    except ValueError:\n        return False\n\ndef parse():\n    train = {}\n    validate = {}\n    cls_labels = {}\n    \n    # Read data\n    with open('dataset_annotations.txt') as file:\n        classes = {}\n        for line in file.readlines():\n            lines = line.strip().split(\",\")\n            file_name, clas_spec = lines[-1], lines[-2]\n            file_name = file_name.replace(\".png\", \".jpg\")\n\n            xs = [int(float(lines[0])), int(float(lines[2])), int(float(lines[4])), int(float(lines[6]))]\n            ys = [int(float(lines[1])), int(float(lines[3])), int(float(lines[5])), int(float(lines[7]))]\n\n            x_min, x_max = min(xs), max(xs)\n            y_min, y_max = min(ys), max(ys)\n\n            width = x_max - x_min\n            height = y_max - y_min\n\n            im = Image.open(file_name)\n            im_width, im_height = im.size\n\n            center_x, center_y = (width / 2) + x_min, (height /2) + y_min\n\n            if classes.get(clas_spec) is None:\n                classes[clas_spec] = []\n            data = {\"name\": file_name, \"x\": center_x / im_width, \"y\": center_y / im_height, \"width\": width / im_width, \"height\": height / im_height}\n            classes[clas_spec].append(data)\n            print(data)\n        \n        it = 0\n        for key, values in classes.items():\n            if len(values) > 20 and not RepresentsInt(key):\n                cls_labels[it] = key\n                random.shuffle(values)\n                test_len = math.floor(len(values) * validate_percentage)\n                train[key] = values[-(len(values) - test_len):]\n                validate[key] = values[:test_len]\n\n\n                print(f\"validate: {key}: {len(validate[key])}\")\n                print(f\"train: {key}: {len(train[key])}\")\n\n                for value in values:\n                    f_name = value[\"name\"].replace(\".jpg\", \".txt\")\n                    with open(f_name, \"w+\") as w_file:\n                        w_file.write(f\"{it} {value['x']} {value['y']} {value['width']} {value['height']}\\n\")\n                it += 1\n\n    with open(\"classes.names\", \"w+\") as cls_file:\n        for cls_name in cls_labels.values():\n            cls_file.write(f\"{cls_name}\\n\")\n\n    with open(\"train.txt\", \"w+\") as data_file:\n        for vls in train.values():\n            for val in vls:\n                data_file.write(f\"{path_prefix}/{val['name']}\\n\")\n\n    with open(\"test.txt\", \"w+\") as data_file:\n        for vls in validate.values():\n            for val in vls:\n                data_file.write(f\"{path_prefix}/{val['name']}\\n\")\n    \n    with open(\"data.data\", \"w+\") as data_file:\n        data_file.write(f\"\"\"classes = {len(cls_labels.keys())}\ntrain = {path_prefix}/train.txt\nvalid = {path_prefix}/test.txt\nnames = {path_prefix}/classes.names\nbackup = backup1\"\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading *trained weights* and *cfg file* into the Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trained weights can be found in the course mentioned above\n# Tutaj po dodaniu do Data zmie≈Ñ drogƒô do pliku kt√≥ry wrzucisz\npath_to_weights = '../input/car-data/znaki_rtx_final.weights'\npath_to_weights_markings = '../input/car-data/poziome_rtx_final.weights'\npath_to_cfg = '../input/traffic-signs-dataset-in-yolo-format/yolov3_ts_test.cfg'\npath_to_cfg_markings = '../input/car-data/markings_test.cfg'\n\n# Loading trained YOLO v3 weights and cfg configuration file by 'dnn' library from OpenCV\nnetwork = cv2.dnn.readNetFromDarknet(path_to_cfg, path_to_weights)\nnetwork_markings = cv2.dnn.readNetFromDarknet(path_to_cfg_markings, path_to_weights_markings)\n\n# To use with GPU\nnetwork.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\nnetwork.setPreferableTarget(cv2.dnn.DNN_TARGET_OPENCL_FP16)\n\nnetwork_markings.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\nnetwork_markings.setPreferableTarget(cv2.dnn.DNN_TARGET_OPENCL_FP16)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Getting *output layers* where detections are made"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting names of all YOLO v3 layers\nlayers_all = network.getLayerNames()\nlayers_names_output = [layers_all[i[0] - 1] for i in network.getUnconnectedOutLayers()]\nprint(layers_names_output)\n\nprint(\"<===========>\")\n\n# Getting names of all YOLO v4 layers\nlayers_all_markings = network_markings.getLayerNames()\nlayers_names_output_markings = [layers_all_markings[i[0] - 1] for i in network_markings.getUnconnectedOutLayers()]\nprint(layers_names_output_markings)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setting *probability*, *threshold* and *colour* for bounding boxes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Minimum probability to eliminate weak detections\nprobability_minimum = 0.1\n\n# Setting threshold to filtering weak bounding boxes by non-maximum suppression\nthreshold = 0.1\n\n# Generating colours for bounding boxes\n# randint(low, high=None, size=None, dtype='l')\ncolours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')\ncolours_markings = np.random.randint(0, 255, size=(1, 3), dtype='uint8')\n\n# Check point\nprint(type(colours))  # <class 'numpy.ndarray'>\nprint(colours.shape)  # (43, 3)\nprint(colours[0])  # [25  65 200]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# üé¨ Reading input video"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading video from a file by VideoCapture object\n#video = cv2.VideoCapture('../input/car-data/70maiMiniDashCam-Dzien.mp4')\n#video = cv2.VideoCapture('../input/car-data/DODRX8W(lusterko)-roadtestwsonecznydzien_podsonce1080p30.mp4')\nvideo = cv2.VideoCapture('../input/car-data/70maiMiniDashCam-Dzien.mp4')\n\n# Writer that will be used to write processed frames\nwriter = None\n\n# Variables for spatial dimensions of the frames\nh, w = None, None\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ‚ûø Processing frames in the loop"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n\n# Setting default size of plots\nplt.rcParams['figure.figsize'] = (3, 3)\n\n# Variable for counting total amount of frames\nf = 0\n\n# Variable for counting total processing time\nt = 0\n\n# Catching frames in the loop\nwhile True:\n    # Capturing frames one-by-one\n    ret, frame = video.read()\n        \n    # If the frame was not retrieved\n    if not ret:\n        break\n    \n    # Getting spatial dimensions of the frame for the first time\n    if w is None or h is None:\n        # Slicing two elements from tuple\n        h, w = frame.shape[:2]\n\n    # Blob from current frame\n    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n\n    # Forward pass with blob through output layers\n    network.setInput(blob)\n    network_markings.setInput(blob)\n    start = time.time()\n    output_from_network = network.forward(layers_names_output)\n    output_from_network_markings = network_markings.forward(layers_names_output_markings)\n    end = time.time()\n\n    # Increasing counters\n    f += 1\n    t += end - start\n\n    # Spent time for current frame\n    print('Frame number {0} took {1:.5f} seconds'.format(f, end - start))\n\n    # Lists for detected bounding boxes, confidences and class's number\n    bounding_boxes = []\n    bounding_boxes_markings = []\n    confidences = []\n    confidences_markings = []\n    class_numbers = []\n    class_numbers_markings = []\n\n    # Going through all output layers after feed forward pass\n    for result in output_from_network:\n        # Going through all detections from current output layer\n        for detected_objects in result:\n            # Getting 80 classes' probabilities for current detected object\n            scores = detected_objects[5:]\n            # Getting index of the class with the maximum value of probability\n            class_current = np.argmax(scores)\n            # Getting value of probability for defined class\n            confidence_current = scores[class_current]\n            # Eliminating weak predictions by minimum probability\n            if confidence_current > probability_minimum:\n                try:\n                    # Scaling bounding box coordinates to the initial frame size\n                    box_current = detected_objects[0:4] * np.array([w, h, w, h])\n\n                    # Getting top left corner coordinates\n                    x_center, y_center, box_width, box_height = box_current\n                    x_min = int(x_center - (box_width / 2))\n                    y_min = int(y_center - (box_height / 2))\n\n                    # Adding results into prepared lists\n                    bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n                    confidences.append(float(confidence_current))\n                    class_numbers.append(class_current)\n                except Exception as e:\n                    print(e)\n                \n\n    # Implementing non-maximum suppression of given bounding boxes\n    results = cv2.dnn.NMSBoxes(bounding_boxes, confidences, probability_minimum, threshold)\n    results_markings = cv2.dnn.NMSBoxes(bounding_boxes_markings, bounding_boxes_markings, probability_minimum, threshold)\n\n    # Checking if there is any detected object been left\n    if len(results) > 0:\n        # Going through indexes of results\n        for i in results.flatten():\n            # Bounding box coordinates, its width and height\n            x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n            box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n            \n            \n            # Cut fragment with Traffic Sign\n            c_ts = frame[y_min:y_min+int(box_height), x_min:x_min+int(box_width), :]\n            \n            if c_ts.shape[:1] == (0,) or c_ts.shape[1:2] == (0,):\n                pass\n            else:\n                # Getting preprocessed blob with Traffic Sign of needed shape\n                blob_ts = cv2.dnn.blobFromImage(c_ts, 1 / 255.0, size=(32, 32), swapRB=True, crop=False)\n                blob_ts[0] = blob_ts[0, :, :, :] - mean['mean_image_rgb']\n                blob_ts = blob_ts.transpose(0, 2, 3, 1)\n\n                # Feeding to the Keras CNN model to get predicted label among 43 classes\n                scores = model.predict(blob_ts)\n\n                # Scores is given for image with 43 numbers of predictions for each class\n                # Getting only one class with maximum value\n                prediction = np.argmax(scores)\n\n\n                # Colour for current bounding box\n                colour_box_current = colours[class_numbers[i]].tolist()\n\n                # Drawing bounding box on the original current frame\n                cv2.rectangle(frame, (x_min, y_min),\n                              (x_min + box_width, y_min + box_height),\n                              colour_box_current, 2)\n\n                # Preparing text with label and confidence for current bounding box\n                text_box_current = '{}: {:.4f}'.format(labels['SignName'][prediction],\n                                                       confidences[i])\n\n                # Putting text with label and confidence on the original image\n                cv2.putText(frame, text_box_current, (x_min, y_min - 5),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, colour_box_current, 2)\n                \n                alpha = 0.05\n                \n                if prediction == 11:\n                    intersection = cv2.imread('../input/intersections/Rysunek2-Model-1.png')\n                    intersection = cv2.resize(intersection, (100, 100), interpolation = cv2.INTER_AREA)\n                    half_frame_width = int(frame.shape[1]/ 2)\n                    \n                    changed_region = cv2.addWeighted(frame[25:125, half_frame_width:half_frame_width+100, :], alpha, intersection[0:100, 0:100, :], 1 - alpha, 0)\n                    frame[25:125, half_frame_width:half_frame_width+100] = changed_region\n                    \n                elif prediction == 12:\n                    intersection = cv2.imread('../input/intersections/Rysunek2-Model-1.png')\n                    intersection = cv2.resize(intersection, (100, 100), interpolation = cv2.INTER_AREA)\n                    half_frame_width = int(frame.shape[1]/ 2)\n                    \n                    changed_region = cv2.addWeighted(frame[25:125, half_frame_width:half_frame_width+100, :], alpha, intersection[0:100, 0:100, :], 1 - alpha, 0)\n                    frame[25:125, half_frame_width:half_frame_width+100] = changed_region\n                \n                elif prediction == 13:\n                    intersection = cv2.imread('../input/intersections/Rysunek3-Model-1.png')\n                    intersection = cv2.resize(intersection, (100, 100), interpolation = cv2.INTER_AREA)\n                    intersection = cv2.rotate(intersection, cv2.cv2.ROTATE_90_CLOCKWISE) \n                    half_frame_width = int(frame.shape[1]/ 2)\n                    \n                    changed_region = cv2.addWeighted(frame[25:125, half_frame_width:half_frame_width+100, :], alpha, intersection[0:100, 0:100, :], 1 - alpha, 0)\n                    frame[25:125, half_frame_width:half_frame_width+100] = changed_region\n                    \n                elif prediction == 40:\n                    intersection = cv2.imread('../input/intersections/Rysunek6-Ukad2-1.png')\n                    intersection = cv2.resize(intersection, (100, 100), interpolation = cv2.INTER_AREA)\n                    half_frame_width = int(frame.shape[1]/ 2)\n                    \n                    changed_region = cv2.addWeighted(frame[25:125, half_frame_width:half_frame_width+100, :], alpha, intersection[0:100, 0:100, :], 1 - alpha, 0)\n                    frame[25:125, half_frame_width:half_frame_width+100] = changed_region\n\n    # For markings\n    for result in output_from_network_markings:\n        for detected_objects in result:\n            scores = detected_objects[5:]\n            class_current = np.argmax(scores)\n            confidence_current = scores[class_current]\n            if confidence_current > probability_minimum:\n                try:\n                    box_current = detected_objects[0:4] * np.array([w, h, w, h])\n\n                    x_center, y_center, box_width, box_height = box_current\n                    x_min = int(x_center - (box_width / 2))\n                    y_min = int(y_center - (box_height / 2))\n\n                    bounding_boxes_markings.append([x_min, y_min, int(box_width), int(box_height)])\n                    confidences_markings.append(float(confidence_current))\n                    class_numbers_markings.append(class_current)\n                except Exception as e:\n                    print(e)\n\n    if len(results_markings) > 0:\n        for i in results_markings.flatten():\n            x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n            box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n\n            cv2.rectangle(frame, (x_min, y_min),\n                          (x_min + box_width, y_min + box_height),\n                            colours[0].toList(), 2)\n\n\n    # Initializing writer only once\n    if writer is None:\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n\n        # Writing current processed frame into the video file\n        writer = cv2.VideoWriter('result.mp4', fourcc, 25,\n                                 (frame.shape[1], frame.shape[0]), True)\n\n    # Write processed current frame to the file\n    writer.write(frame)\n\n\n# Releasing video reader and writer\nvideo.release()\nwriter.release()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## üèÅ FPS results"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total number of frames', f)\nprint('Total amount of time {:.5f} seconds'.format(t))\nprint('FPS:', round((f / t), 1))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Saving locally without committing\nfrom IPython.display import FileLink\nimport os\n\n#os.chdir(r'kaggle/working')\nFileLink('result.mp4')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# üîé Example of the result"},{"metadata":{},"cell_type":"markdown","source":"![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3400968%2Fa57f58b38e3caab6fbf72169895f5074%2Fresult.gif?generation=1585955236302060&alt=media)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}