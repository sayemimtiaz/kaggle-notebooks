{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://miro.medium.com/max/2972/1*7DkqpU3E-E9yknyw9c7vCQ.png)"},{"metadata":{},"cell_type":"markdown","source":"Named Entity means anything that is a real-world object such as a person, a place, any organisation, any product which has a name. \n\nIn Machine Learning Named Entity Recognition (NER) is a task of Natural Language Processing to identify the named entities in a certain piece of text.\n\nGrammarly identifies all the incorrect spellings and punctuations in the text and corrects it. But it does not do anything with the named entities, as it is also using the same technique."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndata = pd.read_csv('../input/named-entity-recognition-ner/ner_dataset.csv', encoding= 'unicode_escape')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import chain\ndef get_dict_map(data, token_or_tag):\n    tok2idx = {}\n    idx2tok = {}\n    \n    if token_or_tag == 'token':\n        vocab = list(set(data['Word'].to_list()))\n    else:\n        vocab = list(set(data['Tag'].to_list()))\n    \n    idx2tok = {idx:tok for  idx, tok in enumerate(vocab)}\n    tok2idx = {tok:idx for  idx, tok in enumerate(vocab)}\n    return tok2idx, idx2tok\ntoken2idx, idx2token = get_dict_map(data, 'token')\ntag2idx, idx2tag = get_dict_map(data, 'tag')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Word_idx'] = data['Word'].map(token2idx)\ndata['Tag_idx'] = data['Tag'].map(tag2idx)\ndata_fillna = data.fillna(method='ffill', axis=0)\n# Groupby and collect columns\ndata_group = data_fillna.groupby(\n['Sentence #'],as_index=False\n)['Word', 'POS', 'Tag', 'Word_idx', 'Tag_idx'].agg(lambda x: list(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\n\ndef get_pad_train_test_val(data_group, data):\n\n    #get max token and tag length\n    n_token = len(list(set(data['Word'].to_list())))\n    n_tag = len(list(set(data['Tag'].to_list())))\n\n    #Pad tokens (X var)    \n    tokens = data_group['Word_idx'].tolist()\n    maxlen = max([len(s) for s in tokens])\n    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value= n_token - 1)\n\n    #Pad Tags (y var) and convert it into one hot encoding\n    tags = data_group['Tag_idx'].tolist()\n    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int32', padding='post', value= tag2idx[\"O\"])\n    n_tags = len(tag2idx)\n    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]\n    \n    #Split train, test and validation set\n    tokens_, test_tokens, tags_, test_tags = train_test_split(pad_tokens, pad_tags, train_size=0.8, random_state=42)\n    train_tokens, val_tokens, train_tags, val_tags = train_test_split(tokens_,tags_,train_size =0.8, random_state=42)\n\n    print(\n        'train_tokens length:'  , len(train_tokens),\n        '\\ntrain_tags:         ', len(train_tags),\n        '\\ntest_tokens length: ', len(test_tokens),\n        '\\ntest_tags:          ', len(test_tags),\n        '\\nval_tokens:         ', len(val_tokens),\n        '\\nval_tags:           ', len(val_tags),\n    )\n    \n    return train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags\n\ntrain_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags = get_pad_train_test_val(data_group, data)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport tensorflow\nfrom tensorflow.keras import Sequential, Model, Input\nfrom tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\nfrom tensorflow.keras.utils import plot_model\nfrom numpy.random import seed\nseed(1)\ntensorflow.random.set_seed(13)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_dim = len(list(set(data['Word'].to_list())))+1\noutput_dim = 64\ninput_length = max([len(s) for s in data_group['Word_idx'].tolist()])\nn_tags = len(tag2idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_bilstm_lstm_model():\n    model = Sequential()\n\n    # Add Embedding layer\n    model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n\n    # Add bidirectional LSTM\n    model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'))\n\n    # Add LSTM\n    model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\n\n    # Add timeDistributed Layer\n    model.add(TimeDistributed(Dense(n_tags, activation=\"relu\")))\n\n    #Optimiser \n    # adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n\n    # Compile model\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    model.summary()\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(X, y, model):\n    loss = list()\n    for i in range(50):\n        # fit model for one epoch on this sequence\n        hist = model.fit(X, y, batch_size=1024, verbose=1, epochs=1, validation_split=0.2)\n        loss.append(hist.history['loss'][0])\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = pd.DataFrame()\nmodel_bilstm_lstm = get_bilstm_lstm_model()\nplot_model(model_bilstm_lstm)\nresults['with_add_lstm'] = train_model(train_tokens, np.array(train_tags), model_bilstm_lstm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nfrom spacy import displacy\nnlp = spacy.load('en_core_web_sm')\ntext = nlp(\"\"\"AI-based machine learning techniques are going beyond the cloud-based data center, as processing of vital IoT sensor data moves much closer to where the data first resides.\n\nThe move will be enabled by new artificial intelligence (AI)-equipped chips. These include embedded microcontrollers with narrower memory and power consumption requirements than GPUs (graphical processing units), FPGAs (field-programmable gate arrays) and other specialized IC types first used to answer data scientists’ questions in the cloud data centers of Amazon Web Services, Microsoft and Google.\n\nIt was in these clouds that machine learning and related neural network use exploded. But the rise of IoT created a data onslaught that required edge-based machine learning as well.\n\nNow, cloud providers, Internet of Things (IoT) platform makers, and others see benefit in processing data at the edge before turning it over to the cloud for analytics.\n\nMaking AI decisions at the edge reduces latency and makes real-time response to sensor data more practical and possible. Still, what people call “edge AI” takes many forms. And how to power it with next-gen IoT presents challenges in terms of presenting good-quality actionable data.\n\nEdge Computing Workloads Grow\n\nEdge-based machine learning could drive significant growth of AI in the IoT market, which Mordor Intelligence estimates will grow at a 27.3% CAGR through to 2026.\n\nThat is buttressed by Eclipse Foundation IoT Group research in 2020, which pegged AI at 30% as the most commonly cited edge computing workload among IoT developers.\n\nFor many applications, replicating the endless racks of servers that enabled parallel machine learning on the cloud is not an option. IoT edge cases that benefit from local processing are many, and highlighted by varied cases of operations monitoring. The processors, for example, watch events triggered by pressure gauge changes on an oil rig, detection of an anomaly on a distant power line, or captured video surveillance of an issue at a factory.\n\nThe last case is one of those most widely pursued. Application of AI that parses image data at the edge has proved a fertile area. But there are many complex processing needs for event processing using IoT device-gathered data.\n\nThe Value of Edge Compute\n\nStill, cloud-based IoT analytics will endure, said Steve Conway, senior adviser, Hyperion Research. But the distance data must travel brings processing latency. Moving data to and from a cloud naturally creates lag; the round trip takes time.\n\n“There is something called the speed of light,” Conway quips. “And you cannot exceed it.” As result, a hierarchy of processing is developing on the edge.\n\nOther than devices and board-level implementations, this hierarchy includes IoT gateways and data centers in manufacturing that expand architectural options available for next-generation IoT system development.\n\nIn the long view, edge AI architecture is yet another generational shift in data processing  focus  – but a key one, according to Saurabh Mishra, senior manager for product marketing at SAS’s IoT and Edge division.\n\n“There is a progression here,” he said. “At one time, the idea was centralizing your data. You can do that for certain industries and certain use cases – ones where data was already created in a context, such as in a data center,” he said.\n\nIt’s not really possible to efficiently – and economically – move that to the cloud for analysis,” Mishra said, who noted that SAS has created validated edge IoT reference architectures on top of which customers can build AI and analytical applications. Striking a balance between cloud and edge AI will be a fundamental requirement, he said.\n\nFinding balance begins with consideration of the amount of data needed to run machine learning models, according to Frédéric Desbiens, program manager, IoT and Edge Computing at the Eclipse Foundation. That is where the new intelligent processors come in.\n\n“AI accelerators at the edge can do local processing before sending the data somewhere else. But, this requires you to think about the functional requirements, including the software stack and storage needed,” Desbiens said.\n\nAI Edge Chip Abundance\n\nThe rise of cloud-based machine learning was influenced by the rise of the high-memory bandwidth GPU, often in the form of a NVIDIA semiconductor. That success drew the attention of other chip makers.\n\nIn-house AI-specific processors followed from hyperscale cloud-players Google, AWS and Microsoft.\n\nThat AI chip battle has been joined by leading lights such as AMD, Intel, Qualcomm, and ARM Technology (which, for its part, last year was acquired by NVIDIA).\n\nIn turn, embedded microprocessor and systems-on-a-chip mainstays like Maxim Integrated, NXP Semiconductors, Silicon Labs, STM Microelectronics and others began to focus on adding AI abilities to the edge.\n\nToday,  IoT and edge processing needs have attracted AI chip start-ups that include EdgeQ,  Graphcore, Hailo, Mythic and others. Processing on the edge is constrained. Barriers include memory available, energy consumed and cost, emphasizes Hyperion’s Steve Conway.\n\n“The embedded processors are very important, as energy use is very important,” Conway said. “The GPUs and CPUs are not tiny dies, and GPUs, particularly, use a ton of energy,” he said, referring to the relatively large silicon form factors GPUs and CPUs can take on.\n\nMaking Neurals Fit the Part\n\nData movement is a factor in energy consumption on the edge, advises Kris Ardis, executive director of Maxim Integrated’s microcontroller and software algorithm businesses. Recently, the company released the MAX78000, which pairs a low-power controller with a neural net processor that can run on battery-powered IoT devices.\n\n“If you can do a computation at the very edge, you save bandwidth, and communications power. The challenge is taking the neural net and making it fit in the part,” Ardis said.\n\nIndividual IoT devices based on the chip can feed IoT gateways, which also have a useful part to play, combining rollups of data from devices, and further filtering data that may go to the cloud in order to analyze overall operations, he indicated.\n\nOther semiconductor device makers also are adjusting to a trend that sees compute moving nearer to where data is. They are part of the effort to broaden the capabilities of developers, even as their hardware choices grow.\n\nBill Pearson, vice president of Intel’s IoT group admits there was a time when “the CPU was the answer to all problems.” Trends like edge AI belie that now.\n\nHe uses the term “XPU” to represent a variety of chip types that support different uses. But, he adds, the variety should be supported by a single software application programming interface (API).\n\nTo aid software developers, Intel recently released Version 2021.2 of the OpenVINO toolkit for inference on edge systems. It provides a, common environment for development among Intel components including CPUs, GPUs, and Movidius Visual Processing Units. As well, Intel offers DevCloud for the Edge software to forecast performance of neural network inference on different Intel hardware, according to Pearson.\n\nThe drive to simplify is marked at GPU powerhouse NVIDIA too.\n\n“The industry has to make it easier for people that aren’t AI specialists,” said Justin Boitano, vice president and general manager for Enterprise and Edge Computing, NVIDIA.\n\nThat may take the form of NVIDIA Jetson, which includes a low-power ARM processor. Named with a nod to the ‘60s science-fiction cartoon series, Jetson is intended to provide GPU-accelerated parallel processing in mobile embedded systems.\n\nRecently, to ease vision system development, NVIDIA rolled out Jetson JetPack 4.5, which includes the first production version of its Vision Programming Interface (VPI).\n\nWith time, edge AI development chores will be handled more by IT shops, and less by AI researchers with deep knowledge of machine learning, Boitano said.\n\nThe Tiny ML That Roared\n\nThe skills needed to migrate machine learning methods from the vast cloud to the constrained edge device are not easily gained. But new software techniques are being applied to enable compact edge AI, while easing the task of the developer.\n\nIn fact, industry has experienced the rise of “Tiny ML” approaches. These make do with less power and use limited memory, while achieving capable inference-operations-per-second ratings.\n\nVarious machine learning tooling to reduce edge processing requirements have emerged, including Apache MXNet,  Edge Impulse’s EON, Facebook’s Glow, Foghorn Lightning Edge ML, Google TensorFlow Lite, Microsoft ELL, OctoML’s Octomizer and others.\n\nDown-sizing neural net processing is a main target here, and the techniques are several. Among these are quantization, binarization and pruning, according to Sastry Malladi, who is CTO at Foghorn, a maker of a software platform that supports a variety of edge and on-premises implementations.\n\nQuantization of neural net processing focuses on use of low bit-width math. Binarization, in turn, is used to reduce the complexity of computations. And, pruning is used to reduce the number of neural nodes that must be processed.\n\nMalladi admits that is a daunting gamut for most developers to traverse – especially across a range of hardware. The efforts behind Foghorn’s Lightning platform, he said, are intended to abstract the complexity in machine learning on the edge.\n\nThe goal is to allow line operators and reliability engineers, for example, to work with drag-and-drop interfaces, rather than application programming interfaces and software development kits, which are less intuitive and require more coding knowledge.\n\nSoftware that simplifies development and runs across multiple types of edge AI hardware is also a focus for Edge Impulse, makers of a development platform for embedded machine learning.\n\nUltimately, machine learning maturation means some model miniaturization, according to Zach Shelby, CEO, Edge Impulse.\n\n“Once, the direction of the research was toward bigger and bigger models of more and more complexity,” Shelby said. “But, as machine learning hit prime time, people started to care about efficiency again.” That led to Tiny ML.\n\nSoftware that can work on existing IoT infrastructure is necessary, while supporting a path to new varieties of hardware, he said. Edge Impulse tools allow cloud-based modeling of algorithms and events on available hardware, Shelby continued, so that users can try different options before they make selections.\n\nKeep Your Eyes on Vision\n\nOn the edge, computer vision has become a prominent use case for AI, especially in the form of deep learning, which employs multiple layers of neural networks and unsupervised techniques to achieve results in image pattern recognition.\n\nVision system architecture is undergoing shifts today, as cameras on the very edge add processing capabilities via embedded hardware for deep learning, according to Forrester Research’s Kjell Carlsson, principal analyst. But finding the best application targets can be a challenge.\n\n“The issue with AI on the edge is that you more frequently end up looking at use cases that are ‘net new,’” he said.\n\nDeveloping these greenfield solutions has inherent risk, Carlsson said, so a helpful tactic is to focus on use cases that offer a high benefit to cost ratio, even if the pattern recognition accuracy might trail that of full-fledged existing systems.\n\nOverall, Carlsson said edge AI could help fulfill IoT’s original promise, which has lagged at times as implementers sorted through myriad potential use cases.\n\n“IoT on its own had some limitations. Now, with AI, machine learning and deep learning that makes IoT more applicable – as well as valuable,” he said.\"\"\")\ndisplacy.render(text, style = 'ent', jupyter=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}