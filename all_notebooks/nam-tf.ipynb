{"cells":[{"metadata":{},"cell_type":"markdown","source":"# HyperParameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"global batch_size, training_epochs, learning_rate, output_regularization, l2_regularization\ntraining_epochs = 150\nlearning_rate = 1e-3\noutput_regularization = 0\nl2_regularization = 0\nbatch_size = 1024\nlogdir = '/kaggle/working/logs'\ndataset_name = 'CMS_Muon_Momentum'\ndecay_rate = 0.94\ndropout = 0.1\ndata_split = 1\ntf_seed = 242\nfeature_dropout = 0.0\nnum_basis_functions = 512\nunits_multiplier = 6\ncross_val = False\nmax_checkpoints_to_keep = 1\nsave_checkpoint_every_n_epochs = 1\nn_models = 1\nnum_splits = 2\nfold_num = 1\nactivation = 'exu'\nregression = True\ndebug = False\nshallow = True\nuse_dnn = False\nearly_stopping_epochs = 25\n_N_FOLDS = 5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip uninstall tensorflow -y\n!pip -qq install tensorflow==1.15","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from typing import Union, List, Tuple, Iterator, Dict\nimport functools\nfrom typing import Union, List, Optional, Tuple, Callable, Dict\nfrom sklearn import metrics as sk_metrics\nimport tensorflow.compat.v1 as tf\nimport operator\nimport os\nimport gzip\nimport os.path as osp\nimport tarfile\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.preprocessing import OneHotEncoder\ngfile = tf.io.gfile\n\ntf.enable_eager_execution()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model.py"},{"metadata":{"trusted":true},"cell_type":"code","source":"TfInput = Union[np.ndarray, tf.Tensor]\n\n\ndef exu(x, weight, bias):\n  \"\"\"ExU hidden unit modification.\"\"\"\n  return tf.exp(weight) * (x - bias)\n\n\n# Activation Functions\ndef relu(x, weight, bias):\n  \"\"\"ReLU activation.\"\"\"\n  return tf.nn.relu(weight * (x - bias))\n\n\ndef relu_n(x, n = 1):\n  \"\"\"ReLU activation clipped at n.\"\"\"\n  return tf.clip_by_value(x, 0, n)\n#   return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ActivationLayer(tf.keras.layers.Layer):\n  \"\"\"Custom activation Layer to support ExU hidden units.\"\"\"\n\n  def __init__(self,\n               num_units,\n               name = None,\n               activation = 'exu',\n               trainable = True):\n    \"\"\"Initializes ActivationLayer hyperparameters.\n    Args:\n      num_units: Number of hidden units in the layer.\n      name: The name of the layer.\n      activation: Activation to use. The default value of `None` corresponds to\n        using the ReLU-1 activation with ExU units while `relu` would use\n        standard hidden units with ReLU activation.\n      trainable: Whether the layer parameters are trainable or not.\n    \"\"\"\n    super(ActivationLayer, self).__init__(trainable=trainable, name=name)\n    self.num_units = num_units\n    self._trainable = trainable\n    if activation == 'relu':\n      self._activation = relu\n      self._beta_initializer = 'glorot_uniform'\n    elif activation == 'exu':\n      self._activation = lambda x, weight, bias: relu_n(exu(x, weight, bias))\n      self._beta_initializer = tf.initializers.truncated_normal(\n          mean=4.0, stddev=0.5)\n    else:\n      raise ValueError('{} is not a valid activation'.format(activation))\n\n  def build(self, input_shape):\n    \"\"\"Builds the layer weight and bias parameters.\"\"\"\n    self._beta = self.add_weight(\n        name='beta',\n        shape=[input_shape[-1], self.num_units],\n        initializer=self._beta_initializer,\n        trainable=self._trainable)\n    self._c = self.add_weight(\n        name='c',\n        shape=[1, self.num_units],\n        initializer=tf.initializers.truncated_normal(stddev=0.5),\n        trainable=self._trainable)\n    super(ActivationLayer, self).build(input_shape)\n\n  @tf.function\n  def call(self, x):\n    \"\"\"Computes the output activations.\"\"\"\n    center = tf.tile(self._c, [tf.shape(x)[0], 1])\n    out = self._activation(x, self._beta, center)\n    return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FeatureNN(tf.keras.layers.Layer):\n  \"\"\"Neural Network model for each individual feature.\n  Attributes:\n    hidden_layers: A list containing hidden layers. The first layer is an\n      `ActivationLayer` containing `num_units` neurons with specified\n      `activation`. If `shallow` is False, then it additionally contains 2\n      tf.keras.layers.Dense ReLU layers with 64, 32 hidden units respectively.\n    linear: Fully connected layer.\n  \"\"\"\n\n  def __init__(self,\n               num_units,\n               dropout = 0.5,\n               trainable = True,\n               shallow = True,\n               feature_num = 0,\n               name_scope = 'model',\n               activation = 'exu'):\n    \"\"\"Initializes FeatureNN hyperparameters.\n    Args:\n      num_units: Number of hidden units in first hidden layer.\n      dropout: Coefficient for dropout regularization.\n      trainable: Whether the FeatureNN parameters are trainable or not.\n      shallow: If True, then a shallow network with a single hidden layer is\n        created, otherwise, a network with 3 hidden layers is created.\n      feature_num: Feature Index used for naming the hidden layers.\n      name_scope: TF name scope str for the model.\n      activation: Activation and type of hidden unit(ExUs/Standard) used in the\n        first hidden layer.\n    \"\"\"\n    super(FeatureNN, self).__init__()\n    self._num_units = num_units\n    self._dropout = dropout\n    self._trainable = trainable\n    self._tf_name_scope = name_scope\n    self._feature_num = feature_num\n    self._shallow = shallow\n    self._activation = activation\n\n  def build(self, input_shape):\n    \"\"\"Builds the feature net layers.\"\"\"\n    self.hidden_layers = [\n        ActivationLayer(\n            self._num_units,\n            trainable=self._trainable,\n            activation=self._activation,\n            name='activation_layer_{}'.format(self._feature_num))\n    ]\n    if not self._shallow:\n      self._h1 = tf.keras.layers.Dense(\n          64,\n          activation='relu',\n          use_bias=True,\n          trainable=self._trainable,\n          name='h1_{}'.format(self._feature_num),\n          kernel_initializer='glorot_uniform')\n      self._h2 = tf.keras.layers.Dense(\n          32,\n          activation='relu',\n          use_bias=True,\n          trainable=self._trainable,\n          name='h2_{}'.format(self._feature_num),\n          kernel_initializer='glorot_uniform')\n      self.hidden_layers += [self._h1, self._h2]\n    self.linear = tf.keras.layers.Dense(\n        1,\n        use_bias=False,\n        trainable=self._trainable,\n        name='dense_{}'.format(self._feature_num),\n        kernel_initializer='glorot_uniform')\n    super(FeatureNN, self).build(input_shape)\n\n  @tf.function\n  def call(self, x, training):\n    \"\"\"Computes FeatureNN output with either evaluation or training mode.\"\"\"\n    with tf.name_scope(self._tf_name_scope):\n      for l in self.hidden_layers:\n        x = tf.nn.dropout(\n            l(x), rate=tf.cond(training, lambda: self._dropout, lambda: 0.0))\n      x = tf.squeeze(self.linear(x), axis=1)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NAM(tf.keras.Model):\n  \"\"\"Neural additive model.\n  Attributes:\n    feature_nns: List of FeatureNN, one per input feature.\n  \"\"\"\n\n  def __init__(self,\n               num_inputs,\n               num_units,\n               trainable = True,\n               shallow = True,\n               feature_dropout = 0.0,\n               dropout = 0.0,\n               **kwargs):\n    \"\"\"Initializes NAM hyperparameters.\n    Args:\n      num_inputs: Number of feature inputs in input data.\n      num_units: Number of hidden units in first layer of each feature net.\n      trainable: Whether the NAM parameters are trainable or not.\n      shallow: If True, then shallow feature nets with a single hidden layer are\n        created, otherwise, feature nets with 3 hidden layers are created.\n      feature_dropout: Coefficient for dropping out entire Feature NNs.\n      dropout: Coefficient for dropout within each Feature NNs.\n      **kwargs: Arbitrary keyword arguments. Used for passing the `activation`\n        function as well as the `name_scope`.\n    \"\"\"\n    super(NAM, self).__init__()\n    self._num_inputs = num_inputs\n    if isinstance(num_units, list):\n      assert len(num_units) == num_inputs\n      self._num_units = num_units\n    elif isinstance(num_units, int):\n      self._num_units = [num_units for _ in range(self._num_inputs)]\n    self._trainable = trainable\n    self._shallow = shallow\n    self._feature_dropout = feature_dropout\n    self._dropout = dropout\n    self._kwargs = kwargs\n\n  def build(self, input_shape):\n    \"\"\"Builds the FeatureNNs on the first call.\"\"\"\n    self.feature_nns = [None] * self._num_inputs\n    for i in range(self._num_inputs):\n      self.feature_nns[i] = FeatureNN(\n          num_units=self._num_units[i],\n          dropout=self._dropout,\n          trainable=self._trainable,\n          shallow=self._shallow,\n          feature_num=i,\n          **self._kwargs)\n    self._bias = self.add_weight(\n        name='bias',\n        initializer=tf.keras.initializers.Zeros(),\n        shape=(1,),\n        trainable=self._trainable)\n    self._true = tf.constant(True, dtype=tf.bool)\n    self._false = tf.constant(False, dtype=tf.bool)\n#     self.lin0 = tf.keras.layers.Dense(\n#                                   16,\n#                                   activation='relu',\n#                                   use_bias=True,\n#                                   kernel_initializer='glorot_uniform')\n#     self.lin1 = tf.keras.layers.Dense(\n#                                   16,\n#                                   activation='relu',\n#                                   use_bias=True,\n#                                   kernel_initializer='glorot_uniform')\n#     self.lin2 = tf.keras.layers.Dense(\n#                                   1,\n#                                   activation='relu',\n#                                   use_bias=True,\n#                                   kernel_initializer='glorot_uniform')\n\n  def call(self, x, training = True):\n    \"\"\"Computes NAM output by adding the outputs of individual feature nets.\"\"\"\n    individual_outputs = self.calc_outputs(x, training=training)\n    stacked_out = tf.stack(individual_outputs, axis=-1)\n    training = self._true if training else self._false\n    dropout_out = tf.nn.dropout(\n        stacked_out,\n        rate=tf.cond(training, lambda: self._feature_dropout, lambda: 0.0))\n#     dropout_out0 = self.lin0(dropout_out)\n#     dropout_out0 = self.lin1(dropout_out0)\n# #     dropout_out1 = tf.nn.dropout(\n# #         stacked_out,\n# #         rate=tf.cond(training, lambda: self._feature_dropout, lambda: 0.0))\n#     dropout_out2 = self.lin2(dropout_out0)\n#     return dropout_out2\n    \n    out = tf.reduce_sum(dropout_out, axis=-1)\n    return tf.sigmoid(out + self._bias)\n\n  def _name_scope(self):\n    \"\"\"Overrides the default function to fix name_scope for bias.\"\"\"\n    tf_name_scope = self._kwargs.get('name_scope', None)\n    name_scope = super(NAM, self)._name_scope()\n    if tf_name_scope:\n      return tf_name_scope + '/' + name_scope\n    else:\n      return name_scope\n\n  def calc_outputs(self, x, training = True):\n    \"\"\"Returns the output computed by each feature net.\"\"\"\n    training = self._true if training else self._false\n    list_x = tf.split(x, self._num_inputs, axis=-1)\n    return [\n        self.feature_nns[i](x_i, training=training)\n        for i, x_i in enumerate(list_x)\n    ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Graph_builder.py"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.warnings.filterwarnings('ignore')\nLossFunction = Callable[[tf.keras.Model, TfInput, TfInput], tf.Tensor]\nGraphOpsAndTensors = Dict[str, Union[tf.Tensor, tf.Operation, tf.keras.Model]]\nEvaluationMetric = Callable[[tf.Session], float]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def penalized_loss(loss_func,\n                   model,\n                   inputs,\n                   targets,\n                   output_regularization,\n                   l2_regularization = 0.0,\n                   use_dnn = False):\n  \"\"\"Computes penalized loss with L2 regularization and output penalty.\n  Args:\n    loss_func: Loss function.\n    model: Neural network model.\n    inputs: Input values to be fed into the model for computing predictions.\n    targets: Target values containing either real values or binary labels.\n    output_regularization: Coefficient for feature output penalty.\n    l2_regularization: Coefficient for L2 regularization.\n    use_dnn: Whether using DNN or not when computing L2 regularization.\n  Returns:\n    The penalized loss.\n  \"\"\"\n  loss = loss_func(model, inputs, targets)\n  reg_loss = 0.0\n  if output_regularization > 0:\n    reg_loss += output_regularization * feature_output_regularization(\n        model, inputs)\n  if l2_regularization > 0:\n    num_networks = 1 if use_dnn else len(model.feature_nns)\n    reg_loss += l2_regularization * weight_decay(\n        model, num_networks=num_networks)\n  return loss + reg_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def penalized_mse_loss(model,\n                       inputs,\n                       targets,\n                       output_regularization,\n                       l2_regularization = 0.0,\n                       use_dnn = False):\n  \"\"\"Mean Squared Error with L2 regularization and output penalty.\"\"\"\n  return penalized_loss(mse_loss, model, inputs, targets, output_regularization,\n                        l2_regularization, use_dnn)\n\ndef feature_output_regularization(model,\n                                  inputs):\n  \"\"\"Penalizes the L2 norm of the prediction of each feature net.\"\"\"\n  per_feature_outputs = model.calc_outputs(inputs, training=False)\n  per_feature_norm = [  # L2 Regularization\n      tf.reduce_mean(tf.square(outputs)) for outputs in per_feature_outputs\n  ]\n  return tf.add_n(per_feature_norm) / len(per_feature_norm)\n\n\ndef weight_decay(model, num_networks = 1):\n  \"\"\"Penalizes the L2 norm of weights in each feature net.\"\"\"\n  l2_losses = [tf.nn.l2_loss(x) for x in model.trainable_variables]\n  return tf.add_n(l2_losses) / num_networks\n\n\ndef mse_loss(model, inputs,\n             targets):\n  \"\"\"Mean squared error loss for regression.\"\"\"\n  predicted = model(inputs, training=True)\n  predicted = tf.squeeze(predicted)\n  targets = tf.squeeze(targets)\n  return tf.losses.mean_squared_error(predicted, targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_predictions(pred_tensor, dataset_init_op,\n                         sess):\n  \"\"\"Iterates over the `pred_tensor` to compute predictions.\n  Args:\n    pred_tensor: Nested structure representing the next prediction element\n      obtained from the `get_next` call on a `tf.compat.v1.data.Iterator`.\n    dataset_init_op: Dataset iterator initializer for `pred_tensor`.\n    sess: Tensorflow session.\n  Returns:\n    Predictions obtained over the dataset iterated using `pred_tensor`.\n  \"\"\"\n  sess.run(dataset_init_op)\n  y_pred = []\n  while True:\n    try:\n      y_pred.extend(sess.run(pred_tensor))\n    except tf.errors.OutOfRangeError:\n      break\n  return y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmse_loss(sess, y_true, pred_tensor,\n              dataset_init_op):\n  \"\"\"Calculates the RMSE error.\"\"\"\n  y_pred = generate_predictions(pred_tensor, dataset_init_op, sess)\n  return rmse(y_true, y_pred)\n\ndef rmse_loss_2(sess, y_true, pred_tensor,\n              dataset_init_op):\n  \"\"\"Calculates the RMSE error.\"\"\"\n  y_pred = generate_predictions(pred_tensor, dataset_init_op, sess)\n  try:\n    df = pd.read_csv('valid.csv')\n    col_name = 'pred'+str(sorted([int(i.split('pred')[-1]) for i in df.columns if 'actual' not in i])[-1]+1)\n    df[col_name] = np.array(y_pred).reshape((-1))\n    df.to_csv('valid.csv', index=False)\n  except:\n    df = pd.DataFrame()\n    df['actual'] = np.array(y_true).reshape((-1))\n    df['pred0'] = np.array(y_pred).reshape((-1))\n    df.to_csv('valid.csv', index=False)\n  return rmse(y_true, y_pred)\n\n\ndef rmse(y_true, y_pred):\n  \"\"\"Root mean squared error between true and predicted values.\"\"\"\n  return float(np.sqrt(sk_metrics.mean_squared_error(y_true, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def grad(\n    model,\n    inputs,\n    targets,\n    loss_fn = rmse_loss,\n    train_vars = None\n):\n  \"\"\"Calculates gradient w.r.t. `train_vars` of the `loss_fn` for `model`.\"\"\"\n  loss_value = loss_fn(model, inputs, targets)\n  if train_vars is None:\n    train_vars = model.trainable_variables\n  return loss_value, tf.gradients(loss_value, train_vars)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_iterators(\n    datasets,\n    batch_size):\n  \"\"\"Create tf.Dataset iterators from a tuple of one or more numpy arrays.\n  Args:\n    datasets: Single or pair of input numpy arrays containing  features.\n    batch_size: Batch size for iterating over the datasets.\n  Returns:\n    Sampling tensor and Initializable iterator(s) for the input datasets.\n  \"\"\"\n  tf_datasets = [\n      tf.data.Dataset.from_tensor_slices(data).batch(batch_size)\n      for data in datasets\n  ]\n  input_iterator = tf.data.Iterator.from_structure(tf_datasets[0].output_types,\n                                                   tf_datasets[0].output_shapes)\n  init_ops = [input_iterator.make_initializer(data) for data in tf_datasets]\n  x_batch = input_iterator.get_next()\n  return x_batch, init_ops","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_nam_model(x_train,\n                     dropout,\n                     feature_dropout = 0.0,\n                     num_basis_functions = 1000,\n                     units_multiplier = 2,\n                     activation = 'exu',\n                     name_scope = 'model',\n                     shallow = True,\n                     trainable = True):\n  \"\"\"Create the NAM model.\"\"\"\n  global num_unique_vals, num_units, num_inputs\n  num_unique_vals = [\n      len(np.unique(x_train[:, i])) for i in range(x_train.shape[1])\n  ]\n  num_units = [\n      min(num_basis_functions, i * units_multiplier) for i in num_unique_vals\n  ]\n  num_inputs = x_train.shape[-1]\n  nn_model = NAM(\n      num_inputs=num_inputs,\n      num_units=num_units,\n      dropout=np.float32(dropout),\n      feature_dropout=np.float32(feature_dropout),\n      activation=activation,\n      shallow=shallow,\n      trainable=trainable,\n      name_scope=name_scope)\n  return nn_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_graph(\n    x_train,\n    y_train,\n    x_test,\n    y_test,\n    learning_rate,\n    batch_size,\n    output_regularization,\n    dropout,\n    decay_rate,\n    shallow,\n    l2_regularization = 0.0,\n    feature_dropout = 0.0,\n    num_basis_functions = 1000,\n    units_multiplier = 2,\n    activation = 'exu',\n    name_scope = 'model',\n    regression = False,\n    use_dnn = False,\n    trainable = True\n):\n  \"\"\"Constructs the computation graph with specified hyperparameters.\"\"\"\n  if regression:\n    ds_tensors = tf.data.Dataset.from_tensor_slices((x_train, y_train)).apply(\n        tf.data.experimental.shuffle_and_repeat(buffer_size=len(x_train[0])))\n    ds_tensors = ds_tensors.batch(batch_size)\n  else:\n    # Create a balanced dataset to handle class imbalance\n    ds_tensors = create_balanced_dataset(x_train, y_train, batch_size)\n  x_batch, (train_init_op, test_init_op) = create_iterators((x_train, x_test),\n                                                            batch_size)\n\n  if use_dnn:\n    nn_model = DNN(dropout=dropout, trainable=trainable)\n  else:\n    nn_model = create_nam_model(\n        x_train=x_train,\n        dropout=dropout,\n        feature_dropout=feature_dropout,\n        activation=activation,\n        num_basis_functions=num_basis_functions,\n        shallow=shallow,\n        units_multiplier=units_multiplier,\n        trainable=trainable,\n        name_scope=name_scope)\n\n  global_step = tf.train.get_or_create_global_step()\n  learning_rate = tf.Variable(learning_rate, trainable=False)\n  lr_decay_op = learning_rate.assign(decay_rate * learning_rate)\n  optimizer = tf.train.AdamOptimizer(learning_rate)\n\n  predictions = nn_model(x_batch, training=False)\n#   test_prediction = nn_model(test_data_gen, training=False)\n  tf.logging.info(nn_model.summary())\n  train_vars = nn_model.trainable_variables\n  if regression:\n    loss_fn, y_pred = penalized_mse_loss, predictions\n  else:\n    # Apply sigmoid transformation for binary classification\n    loss_fn, y_pred = penalized_cross_entropy_loss, tf.nn.sigmoid(predictions)\n  loss_fn = functools.partial(\n      loss_fn,\n      output_regularization=output_regularization,\n      l2_regularization=l2_regularization,\n      use_dnn=use_dnn)\n\n  iterator = ds_tensors.make_initializable_iterator()\n  x1, y1 = iterator.get_next()\n  loss_tensor, grads = grad(nn_model, x1, y1, loss_fn, train_vars)\n  update_step = optimizer.apply_gradients(\n      zip(grads, train_vars), global_step=global_step)\n  avg_loss, avg_loss_update_op = tf.metrics.mean(\n      loss_tensor, name='avg_train_loss')\n  tf.summary.scalar('avg_train_loss', avg_loss)\n\n  running_mean_vars = tf.get_collection(\n      tf.GraphKeys.LOCAL_VARIABLES, scope='avg_train_loss')\n  running_vars_initializer = tf.variables_initializer(\n      var_list=running_mean_vars)\n\n  # Use RMSE for regression and ROC AUC for classification.\n  evaluation_metric = rmse_loss if regression else roc_auc_score\n  train_metric = functools.partial(\n      evaluation_metric,\n      y_true=y_train,\n      pred_tensor=y_pred,\n      dataset_init_op=train_init_op)\n  test_metric = functools.partial(\n      rmse_loss_2,\n      y_true=y_test,\n      pred_tensor=y_pred,\n      dataset_init_op=test_init_op)\n\n  summary_op = tf.summary.merge_all()\n\n  graph_tensors = {\n      'train_op': [update_step, avg_loss_update_op],\n      'lr_decay_op': lr_decay_op,\n      'summary_op': summary_op,\n      'iterator_initializer': iterator.initializer,\n      'running_vars_initializer': running_vars_initializer,\n      'nn_model': nn_model,\n      'global_step': global_step,\n  }\n  eval_metric_scores = {'test': test_metric, 'train': train_metric}\n  return graph_tensors, eval_metric_scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Nam_train.py"},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_split_with_cross_validation():\n  return (data_split == 1) or (not cross_val)\n\ndef _get_train_and_lr_decay_ops(\n    graph_tensors_and_ops,\n    early_stopping):\n  \"\"\"Returns training and learning rate decay ops.\"\"\"\n  train_ops = [\n      g['train_op']\n      for n, g in enumerate(graph_tensors_and_ops)\n      if not early_stopping[n]\n  ]\n  lr_decay_ops = [\n      g['lr_decay_op']\n      for n, g in enumerate(graph_tensors_and_ops)\n      if not early_stopping[n]\n  ]\n  return train_ops, lr_decay_ops\n\n\ndef _update_latest_checkpoint(checkpoint_dir,\n                              best_checkpoint_dir):\n  \"\"\"Updates the latest checkpoint in `best_checkpoint_dir` from `checkpoint_dir`.\"\"\"\n  for filename in tf.io.gfile.glob(os.path.join(best_checkpoint_dir, 'model.*')):\n    gfile.remove(filename)\n  for name in tf.io.gfile.glob(os.path.join(checkpoint_dir, 'model.*')):\n    gfile.copy(\n        name,\n        os.path.join(best_checkpoint_dir, os.path.basename(name)),\n        overwrite=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _create_computation_graph(\n    x_train, y_train, x_validation,\n    y_validation, batch_size\n):\n  \"\"\"Build the computation graph.\"\"\"\n  graph_tensors_and_ops = []\n  metric_scores = []\n  for n in range(n_models):\n    graph_tensors_and_ops_n, metric_scores_n = build_graph(\n        x_train=x_train,\n        y_train=y_train,\n        x_test=x_validation,\n        y_test=y_validation,\n        activation=activation,\n        learning_rate=learning_rate,\n        batch_size=batch_size,\n        shallow=shallow,\n        output_regularization=output_regularization,\n        l2_regularization=l2_regularization,\n        dropout=dropout,\n        num_basis_functions=num_basis_functions,\n        units_multiplier=units_multiplier,\n        decay_rate=decay_rate,\n        feature_dropout=feature_dropout,\n        regression=regression,\n        use_dnn=use_dnn,\n        trainable=True,\n        name_scope=f'model_{n}')\n    graph_tensors_and_ops.append(graph_tensors_and_ops_n)\n    metric_scores.append(metric_scores_n)\n  return graph_tensors_and_ops, metric_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _create_graph_saver(graph_tensors_and_ops,\n                        logdir, num_steps_per_epoch):\n  \"\"\"Create saving hook(s) as well as model and checkpoint directories.\"\"\"\n  saver_hooks, model_dirs, best_checkpoint_dirs = [], [], []\n  save_steps = num_steps_per_epoch * save_checkpoint_every_n_epochs\n  # The MonitoredTraining Session counter increments by `n_models`\n  save_steps = save_steps * n_models\n  for n in range(n_models):\n    saver=tf.train.Saver(\n            var_list=graph_tensors_and_ops[n]['nn_model'].trainable_variables,\n            save_relative_paths=True,\n            max_to_keep=max_checkpoints_to_keep) \n    scaffold = tf.train.Scaffold(saver=saver)\n    model_dirs.append(os.path.join(logdir, 'model_{}').format(n))\n    best_checkpoint_dirs.append(os.path.join(model_dirs[-1], 'best_checkpoint'))\n    tf.io.gfile.makedirs(best_checkpoint_dirs[-1])\n    saver_hook = tf.train.CheckpointSaverHook(\n        checkpoint_dir=model_dirs[-1], save_steps=save_steps, scaffold=scaffold)\n    saver_hooks.append(saver_hook)\n  return saver_hooks, model_dirs, best_checkpoint_dirs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _update_metrics_and_checkpoints(sess,\n                                    epoch,\n                                    metric_scores,\n                                    curr_best_epoch,\n                                    best_validation_metric,\n                                    best_train_metric,\n                                    model_dir,\n                                    best_checkpoint_dir,\n                                    graph,\n                                    metric_name = 'RMSE'\n                                    ):\n  \"\"\"Update metric scores and latest checkpoint.\"\"\"\n  # Minimize RMSE and maximize AUROC\n  compare_metric = operator.lt if regression else operator.gt\n  # Calculate the AUROC/RMSE on the validation split\n  validation_metric = metric_scores['test'](sess)\n  tf.logging.info('Epoch %d %s Val %.4f', epoch, metric_name,\n                validation_metric)\n  print('Epoch ', epoch, metric_name,' Val ', validation_metric)\n  if compare_metric(validation_metric, best_validation_metric):\n    curr_best_epoch = epoch\n    best_validation_metric = validation_metric\n    best_train_metric = metric_scores['train'](sess)\n    # copy the checkpoints files *.meta *.index, *.data* each time\n    # there is a better result\n    _update_latest_checkpoint(model_dir, best_checkpoint_dir)\n#     save_test_predictions(sess, )\n  return curr_best_epoch, best_validation_metric, best_train_metric","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def training(x_train, y_train, x_validation,\n             y_validation,\n             logdir):\n  \"\"\"Trains the Neural Additive Model (NAM).\n  Args:\n    x_train: Training inputs.\n    y_train: Training labels.\n    x_validation: Validation inputs.\n    y_validation: Validation labels.\n    logdir: dir to save the checkpoints.\n  Returns:\n    Best train and validation evaluation metric obtained during NAM training.\n  \"\"\"\n  tf.logging.info('Started training with logdir %s', logdir)\n  print('Started training with logdir ', logdir)\n  global batch_size\n  batch_size = min(batch_size, x_train.shape[0])\n  num_steps_per_epoch = x_train.shape[0] // batch_size\n  # Keep track of the best validation RMSE/AUROC and train AUROC score which\n  # corresponds to the best validation metric score.\n  if regression:\n    best_train_metric = np.inf * np.ones(n_models)\n    best_validation_metric = np.inf * np.ones(n_models)\n  else:\n    best_train_metric = np.zeros(n_models)\n    best_validation_metric = np.zeros(n_models)\n  # Set to a large value to avoid early stopping initially during training\n  curr_best_epoch = np.full(n_models, np.inf)\n  # Boolean variables to indicate whether the training of a specific model has\n  # been early stopped.\n  early_stopping = [False] * n_models\n  # Classification: AUROC, Regression : RMSE Score\n  metric_name = 'RMSE' if regression else 'AUROC'\n  tf.reset_default_graph()\n  with tf.Graph().as_default():\n    tf.set_random_seed(tf_seed)\n    # Setup your training.\n    graph_tensors_and_ops, metric_scores = _create_computation_graph(\n        x_train, y_train, x_validation, y_validation, batch_size)\n\n    train_ops, lr_decay_ops = _get_train_and_lr_decay_ops(\n        graph_tensors_and_ops, early_stopping)\n    global_step = tf.train.get_or_create_global_step()\n    increment_global_step = tf.assign(global_step, global_step + 1)\n    saver_hooks, model_dirs, best_checkpoint_dirs = _create_graph_saver(\n        graph_tensors_and_ops, logdir, num_steps_per_epoch)\n    if debug:\n      summary_writer = tf.summary.FileWriter(os.path.join(logdir, 'tb_log'))\n\n    with tf.train.MonitoredSession(hooks=saver_hooks) as sess:\n      for n in range(n_models):\n        sess.run([\n            graph_tensors_and_ops[n]['iterator_initializer'],\n            graph_tensors_and_ops[n]['running_vars_initializer']\n        ])\n      for epoch in range(1, training_epochs + 1):\n        if not all(early_stopping):\n          for _ in range(num_steps_per_epoch):\n            sess.run(train_ops)  # Train the network\n          # Decay the learning rate by a fixed ratio every epoch\n          sess.run(lr_decay_ops)\n        else:\n          tf.logging.info('All models early stopped at epoch %d', epoch)\n          print('All models early stopped at epoch ', epoch)\n          break\n#         print(model.predict(test_data_gen[0]))\n\n        for n in range(n_models):\n          if early_stopping[n]:\n            sess.run(increment_global_step)\n            continue\n          # Log summaries\n          if debug:\n            global_summary, global_step = sess.run([\n                graph_tensors_and_ops[n]['summary_op'],\n                graph_tensors_and_ops[n]['global_step']\n            ])\n            summary_writer.add_summary(global_summary, global_step)\n\n          if epoch % save_checkpoint_every_n_epochs == 0:\n            (curr_best_epoch[n], best_validation_metric[n],\n             best_train_metric[n]) = _update_metrics_and_checkpoints(\n                 sess, epoch, metric_scores[n], curr_best_epoch[n],\n                 best_validation_metric[n], best_train_metric[n], model_dirs[n],\n                 best_checkpoint_dirs[n], graph_tensors_and_ops[n], metric_name)\n            if curr_best_epoch[n] + early_stopping_epochs < epoch:\n              tf.logging.info('Early stopping at epoch {}'.format(epoch))\n              print('Early stopping at epoch {}'.format(epoch))\n              early_stopping[n] = True  # Set early stopping for model `n`.\n              train_ops, lr_decay_ops = _get_train_and_lr_decay_ops(\n                  graph_tensors_and_ops, early_stopping)\n          # Reset running variable counters\n#           graph_tensors_and_ops[n]['nn_model'].save_weights('epoch{}.h5'.format(epoch))\n          sess.run(graph_tensors_and_ops[n]['running_vars_initializer'])\n\n  tf.logging.info('Finished training.')\n  print('Finished training.')\n  for n in range(n_models):\n    print(\n        'Model ', n,': Best Epoch ', curr_best_epoch[n],', Individual ',metric_name,': Train ',best_train_metric[n],', Validation %.4f',\n        best_validation_metric[n])\n    tf.logging.info(\n        'Model %d: Best Epoch %d, Individual %s: Train %.4f, Validation %.4f',\n        n, curr_best_epoch[n], metric_name, best_train_metric[n],\n        best_validation_metric[n])\n\n  return np.mean(best_train_metric), np.mean(best_validation_metric)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_test_train_fold(\n    fold_num\n):\n  \"\"\"Splits the dataset into training and held-out test set.\"\"\"\n  data_x, data_y, _ = load_dataset(dataset_name)\n  print('Dataset: ', dataset_name, ', Size: ', data_x.shape[0])\n  print('Cross-val fold: ', fold_num, _N_FOLDS)\n  tf.logging.info('Dataset: %s, Size: %d', dataset_name, data_x.shape[0])\n  tf.logging.info('Cross-val fold: %d/%d', fold_num, _N_FOLDS)\n  # Get the training and test set based on the StratifiedKFold split\n  (x_train_all, y_train_all), test_dataset = get_train_test_fold(\n      data_x,\n      data_y,\n      fold_num=fold_num,\n      num_folds=_N_FOLDS,\n      stratified=not regression)\n  data_gen = split_training_dataset(\n      x_train_all,\n      y_train_all,\n      num_splits,\n      stratified=not regression)\n  return data_gen, test_dataset\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def single_split_training(data_gen,\n                          logdir):\n  \"\"\"Uses a specific (training, validation) split for NAM training.\"\"\"\n  for _ in range(data_split):\n    (x_train, y_train), (x_validation, y_validation) = next(data_gen)\n  curr_logdir = os.path.join(logdir, 'fold_{}',\n                             'split_{}').format(fold_num,\n                                                data_split)\n  training(x_train, y_train, x_validation, y_validation, curr_logdir)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data_utils.py"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_cms_muon_momentum(\n):\n  df = pd.read_csv('/kaggle/input/cmsnewsamples/new-smaples.csv').drop(columns = 'Unnamed: 0')\n  df = df.drop(columns = [i for i in df.columns if '_1' in i])\n  df['non_hits'] = df[[i for i in df.columns if 'mask' in i]].sum(axis=1)\n  df = df[df['non_hits']==0].reset_index(drop=True)\n  df['1/pT'] = df['q/pt'].abs()\n  features = ['emtf_phi_'+str(i) for i in [0,2,3,4]] + ['emtf_theta_'+str(i) for i in [0,2,3,4]] + ['old_emtf_phi_'+str(i) for i in [0,2,3,4]]\n\n  new_features = []\n  for i in range(len(features)-1):\n    for j in range(i+1, (i//4+1)*4):\n        new_features.append('delta_'+'_'.join(features[i].split('_')[:-1])+'_'+str((j)%4)+'_'+str(i%4))\n        df[new_features[-1]]=df[features[j]]-df[features[i]]\n\n  features += new_features[:]\n#   scaler_1 = MinMaxScaler()\n  scaler_1 = StandardScaler()\n  df[features] = scaler_1.fit_transform(df[features])\n\n#   for i in range(int(len(features)//3)):\n#         features.append('phi_theta_'+str(i))\n#         features.append('phi_phi_'+str(i))\n#         df[features[-2]]=df[features[i]]-df[features[int(len(features)//3)+1]]\n#         df[features[-1]]=df[features[i]]-df[features[2*int(len(features)//3)+1]]\n  \n  return {\n      'problem': 'regression',\n      'X': pd.DataFrame(df[features].to_numpy()),\n      'y': df[['1/pT']].to_numpy(),\n  }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_dataset(dataset_name):\n  \"\"\"Loads the dataset according to the `dataset_name` passed.\n  Args:\n    dataset_name: Name of the dataset to be loaded.\n  Returns:\n    data_x: np.ndarray of size (n_examples, n_features) containining the\n      features per input data point where n_examples is the number of examples\n      and n_features is the number of features.\n    data_y: np.ndarray of size (n_examples, ) containing the label/target\n      for each example where n_examples is the number of examples.\n    column_names: A list containing the feature names.\n  \"\"\"\n  if dataset_name == 'CMS_Muon_Momentum':\n    dataset = load_cms_muon_momentum()\n  else:\n    raise ValueError('{} not found!'.format(dataset_name))\n\n  data_x, data_y = dataset['X'].copy(), dataset['y'].copy()\n  problem_type = dataset['problem']\n  data_x, column_names = transform_data(data_x)\n  data_x = data_x.astype('float32')\n  if (problem_type == 'classification') and \\\n      (not isinstance(data_y, np.ndarray)):\n    data_y = pd.get_dummies(data_y).values\n    data_y = np.argmax(data_y, axis=-1)\n  data_y = data_y.astype('float32')\n  return data_x, data_y, column_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_train_test_fold(\n    data_x,\n    data_y,\n    fold_num,\n    num_folds,\n    stratified = True,\n    random_state = 42):\n  \"\"\"Returns a specific fold split for K-Fold cross validation.\n  Randomly split dataset into `num_folds` consecutive folds and returns the fold\n  with index `fold_index` for testing while the `num_folds` - 1 remaining folds\n  form the training set.\n  Args:\n    data_x: Training data, with shape (n_samples, n_features), where n_samples\n      is the number of samples and n_features is the number of features.\n    data_y: The target variable, with shape (n_samples), for supervised learning\n      problems.  Stratification is done based on the y labels.\n    fold_num: Index of fold used for testing.\n    num_folds: Number of folds.\n    stratified: Whether to preserve the percentage of samples for each class in\n      the different folds (only applicable for classification).\n    random_state: Seed used by the random number generator.\n  Returns:\n    (x_train, y_train): Training folds containing 1 - (1/`num_folds`) fraction\n      of entire data.\n    (x_test, y_test): Test fold containing 1/`num_folds` fraction of data.\n  \"\"\"\n  if stratified:\n    stratified_k_fold = StratifiedKFold(\n        n_splits=num_folds, shuffle=True, random_state=random_state)\n  else:\n    stratified_k_fold = KFold(\n        n_splits=num_folds, shuffle=True, random_state=random_state)\n  assert fold_num <= num_folds and fold_num > 0, 'Pass a valid fold number.'\n  for train_index, test_index in stratified_k_fold.split(data_x, data_y):\n    if fold_num == 1:\n      x_train, x_test = data_x[train_index], data_x[test_index]\n      y_train, y_test = data_y[train_index], data_y[test_index]\n      return (x_train, y_train), (x_test, y_test)\n    else:\n      fold_num -= 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_training_dataset(\n    data_x,\n    data_y,\n    n_splits,\n    stratified = True,\n    test_size = 0.2,\n    random_state = 1337):\n  \"\"\"Yields a generator that randomly splits data into (train, validation) set.\n  The train set is used for fitting the DNNs/NAMs while the validation set is\n  used for early stopping.\n  Args:\n    data_x: Training data, with shape (n_samples, n_features), where n_samples\n      is the number of samples and n_features is the number of features.\n    data_y: The target variable, with shape (n_samples), for supervised learning\n      problems.  Stratification is done based on the y labels.\n    n_splits: Number of re-shuffling & splitting iterations.\n    stratified: Whether to preserve the percentage of samples for each class in\n      the (train, validation) splits. (only applicable for classification).\n    test_size: The proportion of the dataset to include in the validation split.\n    random_state: Seed used by the random number generator.\n  Yields:\n    (x_train, y_train): The training data split.\n    (x_validation, y_validation): The validation data split.\n  \"\"\"\n  if stratified:\n    stratified_shuffle_split = StratifiedShuffleSplit(\n        n_splits=n_splits, test_size=test_size, random_state=random_state)\n  else:\n    stratified_shuffle_split = ShuffleSplit(\n        n_splits=n_splits, test_size=test_size, random_state=random_state)\n  split_gen = stratified_shuffle_split.split(data_x, data_y)\n\n  for train_index, validation_index in split_gen:\n    x_train, x_validation = data_x[train_index], data_x[validation_index]\n    y_train, y_validation = data_y[train_index], data_y[validation_index]\n    assert x_train.shape[0] == y_train.shape[0]\n    yield (x_train, y_train), (x_validation, y_validation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def transform_data(df):\n  \"\"\"Apply a fixed set of transformations to the pd.Dataframe `df`.\n  Args:\n    df: Input dataframe containing features.\n  Returns:\n    Transformed dataframe and corresponding column names. The transformations\n    include (1) encoding categorical features as a one-hot numeric array, (2)\n    identity `FunctionTransformer` for numerical variables. This is followed by\n    scaling all features to the range (-1, 1) using min-max scaling.\n  \"\"\"\n  column_names = df.columns\n  new_column_names = []\n  is_categorical = np.array([dt.kind == 'O' for dt in df.dtypes])\n  categorical_cols = df.columns.values[is_categorical]\n  numerical_cols = df.columns.values[~is_categorical]\n  for index, is_cat in enumerate(is_categorical):\n    col_name = column_names[index]\n    if is_cat:\n      new_column_names += [\n          '{}: {}'.format(col_name, val) for val in set(df[col_name])\n      ]\n    else:\n      new_column_names.append(col_name)\n  cat_ohe_step = ('ohe', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n\n  cat_pipe = Pipeline([cat_ohe_step])\n  num_pipe = Pipeline([('identity', FunctionTransformer(validate=True))])\n  transformers = [('cat', cat_pipe, categorical_cols),\n                  ('num', num_pipe, numerical_cols)]\n  column_transform = ColumnTransformer(transformers=transformers)\n\n  pipe = CustomPipeline([('column_transform', column_transform),\n                         ('min_max', MinMaxScaler((-1, 1))), ('dummy', None)])\n  df = pipe.apply_transformation(df)\n  return df, new_column_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomPipeline(Pipeline):\n  \"\"\"Custom sklearn Pipeline to transform data.\"\"\"\n\n  def apply_transformation(self, x):\n    \"\"\"Applies all transforms to the data, without applying last estimator.\n    Args:\n      x: Iterable data to predict on. Must fulfill input requirements of first\n        step of the pipeline.\n    Returns:\n      xt: Transformed data.\n    \"\"\"\n    xt = x\n    for _, transform in self.steps[:-1]:\n      xt = transform.fit_transform(xt)\n    return xt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Main"},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.logging.set_verbosity(tf.logging.WARN)\ndata_gen, test_data_gen = create_test_train_fold(fold_num)\nsingle_split_training(data_gen, logdir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tf.reset_default_graph()\n# model = NAM(\n#       num_inputs=num_inputs,\n#       num_units=num_units,\n#       dropout=np.float32(dropout),\n#       feature_dropout=np.float32(feature_dropout),\n#       activation=activation,\n#       shallow=shallow,\n#       trainable=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.read_csv('valid.csv').head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}