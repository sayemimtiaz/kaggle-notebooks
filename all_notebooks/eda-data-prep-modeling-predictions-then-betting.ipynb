{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Libraries loading\nimport pandas as pd\nimport os\nfrom datetime import datetime,timedelta\nimport warnings\nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt  \nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nfrom scipy import stats\nfrom scipy.stats import norm, skew #statistics for normality and skewness\nimport numpy as np\nip = get_ipython()\nibe = ip.configurables[-1]\nibe.figure_formats = { 'pdf', 'png'}\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_DIR='/kaggle/input/atp-and-wta-tennis-data'\ndf_atp = pd.read_csv(os.path.join(DATA_DIR,\"df_atp.csv\"),index_col=0)\ndf_atp[\"Date\"] =df_atp.Date.apply(lambda x:datetime.strptime(x, '%Y-%m-%d'))\ndf_atp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total number of matches : \"+str(len(df_atp)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(list(df_atp.columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We drop the ATP column (Tournament number (men)) because it does seem intuitively unimportant  specially for the prediction phase and it might be even contagious to our model if we forget it in the training phase (Example : it might add some leakage into our model)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verifying the shape of the dataset before droping the 'ATP' column.\nprint(\"Shape of the Dataset before droping the 'ATP' Column : {} \".format(df_atp.shape))\n\n#Saving the column (maybe for later use ?)\ndf_atp_ID = df_atp['ATP']\n\n#Droping the column \ndf_atp.drop(\"ATP\", axis = 1, inplace = True)\n\n#verifying the shape of the dataset after droping the 'ATP' column\nprint(\"\\nShape of the Dataset after droping the 'ATP' Column : {} \".format(df_atp.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Quick glumpse at the data and answering some questions"},{"metadata":{},"cell_type":"markdown","source":"** 1.\tWho are the three ATP players with the most wins ? **"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_atp['Winner'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_atp['Winner'].value_counts()[0:3]\n#Return a Series containing counts of unique values in descending order so that the first element is the most frequently-occurring element.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_atp['Loser'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_atp['Loser'].value_counts()[0:3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that 'Federer R.' is by far the player with most victories in tournaments being ahead of the second most winner in tournament by 230 matches. While 'Lopez F.' being the player with most losses in Tournaments is only ahead the second most loser one after him with only 14 matches."},{"metadata":{},"cell_type":"markdown","source":"Let's see how these 3  player we were talking about have done in both victories and losses."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"'Federer R.'  have won \" + str(len(df_atp[df_atp['Winner']=='Federer R.']) )+\" and lost \" +str(len(df_atp[df_atp['Loser']=='Federer R.' ])))\nprint(\"'Nadal R.'    have won \" + str(len(df_atp[df_atp['Winner']=='Nadal R.'])) +\" and lost \" +str(len(df_atp[df_atp['Loser']=='Nadal R.' ])))\nprint(\"'Djokovic N.' have won \" + str(len(df_atp[df_atp['Winner']=='Djokovic N.'])) +\" and lost \" +str(len(df_atp[df_atp['Loser']=='Djokovic N.' ])))\nprint(\"'Lopez F.'    have won \" + str(len(df_atp[df_atp['Winner']=='Lopez F.'])) +\" and lost \" +str(len(df_atp[df_atp['Loser']=='Lopez F.' ])))\nprint(\"'Youzhny M.'  have won \" + str(len(df_atp[df_atp['Winner']=='Youzhny M.'])) +\" and lost \" +str(len(df_atp[df_atp['Loser']=='Youzhny M.' ])))\nprint(\"'Verdasco F.' have won \" + str(len(df_atp[df_atp['Winner']=='Verdasco F.'])) +\" and lost \" +str(len(df_atp[df_atp['Loser']=='Verdasco F.' ])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the top 3 performers: 'Federer R.', 'Nadal R.' and 'Djokovic N.' have won many matches but lost at most less than the quarter of that number in matches. While the top player who lost the most matches : 'Lopez F.', 'Youzhny M.' and 'Verdasco F.'  have only won about the same number of matches."},{"metadata":{},"cell_type":"markdown","source":"**2.\tHow many sets did the player “Federer R.” win in total ?** "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_atp['Lsets']= pd.to_numeric(df_atp['Lsets'], errors='coerce')#tranforming str to numeric values and replcing with nan when we can't\nN_sets = df_atp['Wsets'][df_atp['Winner']=='Federer R.'].sum() + df_atp['Lsets'][df_atp['Loser']=='Federer R.'].sum()\n\nprint('\\nPlayer “Federer R.” won a total of : ' + str(N_sets) + ' sets.\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** 3.\tHow many sets did the player “Federer R.” win during the years 2016 and 2017 ?**"},{"metadata":{},"cell_type":"markdown","source":"Number of sets the player 'Federer R.' won in 2016 alone:"},{"metadata":{"trusted":true},"cell_type":"code","source":"beg = datetime(2016,1,1)\nend = datetime(2017,1,1)\ndf_atp_2016 = df_atp[(df_atp['Date']>=beg)&(df_atp['Date']<end)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_atp_2016['Wsets'][df_atp_2016['Winner']=='Federer R.'].sum() + df_atp_2016['Wsets'][df_atp_2016['Loser']=='Federer R.'].sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Number of sets the player 'Federer R.' won in 2017 alone:"},{"metadata":{"trusted":true},"cell_type":"code","source":"beg = datetime(2017,1,1)\nend = datetime(2018,1,1)\ndf_atp_2017 = df_atp[(df_atp['Date']>=beg)&(df_atp['Date']<end)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_atp_2017['Wsets'][df_atp_2017['Winner']=='Federer R.'].sum() + df_atp_2017['Wsets'][df_atp_2017['Loser']=='Federer R.'].sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Number of sets the player 'Federer R.' won during 2016 and 2017: 68+131 = 199 or : "},{"metadata":{"trusted":true},"cell_type":"code","source":"beg = datetime(2016,1,1)\nend = datetime(2018,1,1)\ndf_atp_2017 = df_atp[(df_atp['Date']>=beg)&(df_atp['Date']<=end)]\ndf_atp_2017['Wsets'][df_atp_2017['Winner']=='Federer R.'].sum() + df_atp_2017['Wsets'][df_atp_2017['Loser']=='Federer R.'].sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**4.\tFor each match, what is the percentage of victories of the winner in the past ?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_player_index_and_score = {}\n#Dictionary containing the player name as a key and the tuple (player_unique_index,x,y)\n#x : number_of_matches_won\n#y : number_of_matches played\n# x and y are intiated 0 in the bigining but as we go through the data set we increment x and y by 1 if the player wins a match\n# or we increment only y with 1 if the player loses a matches\ni=0\nfor player in df_atp['Winner'].unique():\n    if player not in unique_player_index_and_score.keys():\n        unique_player_index_and_score[player] = (i,0,0)\n        i+=1\nfor player in df_atp['Loser'].unique():\n    if player not in unique_player_index_and_score.keys():\n        unique_player_index_and_score[player] = (i,0,0)\n        i+=1\n        \nprint('Number of unqiue player names : ',i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"winner_loser_score_tracking_vector = np.zeros((len(df_atp),2)) \n# two columns one to track the winner percetage and the other for the loser percentage ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Sorting dataset by date so we can perform our calculation of the player prior win poucentage coorectly by looping one time trough the dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_atp=df_atp.sort_values(by='Date')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for c,row in enumerate(df_atp[['Winner','Loser']].values):\n    score_winner = unique_player_index_and_score[row[0]]#Winner up-to date score tracking from the dictionary \n    score_loser = unique_player_index_and_score[row[1]]#Loser up-to date score tracking from the dictionary\n    #we consider new player that haven't yet played 5 matches as the have 20% of winning in the past \n    #(kind of a fair approach as they worked hard to get to play in the tournement:))\n    if score_winner[2]<5:\n        winner_loser_score_tracking_vector[c,0]=0.2\n    else:\n        winner_loser_score_tracking_vector[c,0] =score_winner[1]/score_winner[2]\n    if score_loser[2]<5:\n        winner_loser_score_tracking_vector[c,1]=0.2\n    else:\n        winner_loser_score_tracking_vector[c,1] = score_loser[1]/score_loser[2]\n    #updating the dictionary based on the new outcome of the current match\n    unique_player_index_and_score[row[0]] = (score_winner[0],score_winner[1]+1,score_winner[2]+1)#Winner\n    unique_player_index_and_score[row[1]] = (score_loser[0],score_loser[1],score_loser[2]+1)#loser\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_atp['Winner_percentage'] = winner_loser_score_tracking_vector[:,0]\ndf_atp['Loser_percentage'] = winner_loser_score_tracking_vector[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_atp['Winner_percentage'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_atp['Loser_percentage'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df_atp['Winner_percentage'], label=\"Winners\")\nsns.distplot(df_atp['Loser_percentage'], label=\"Losers\")\nplt.ylabel('Frequency')\nplt.xlabel('Winners and Losers prior win probabilityt')\nplt.title('Winners and Losers prior win probability Distributionn')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we say earlier, the are some winner (the top 3 or 4 performers maybe) have a prior win probability that is very high in the range of [0.75,0.82]. We can see also that Winners tend to win and loser tend to lose from the intersection of the two histograms."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df_atp['Winner_percentage'] , fit=norm);\n\n#Récupèrer les paramètres ajustés utilisés par la fonction\n(mu, sigma) = norm.fit(df_atp['Winner_percentage'])\n#Tracer la ditribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('Winner percentage Distribution')\nfig = plt.figure()\nres = stats.probplot(df_atp['Winner_percentage'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df_atp['Loser_percentage'] , fit=norm);\n\n#Récupèrer les paramètres ajustés utilisés par la fonction\n(mu, sigma) = norm.fit(df_atp['Loser_percentage'])\n#Tracer la ditribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('Loser pourcentage Distribution')\nfig = plt.figure()\nres = stats.probplot(df_atp['Loser_percentage'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the distribution of prior win probability for winner haveextremes values near 0.8  and the ditribution for losers have many values in 0.2 (these are for the playes that have not yet played 5 matches in in tournement in their lifetime). Beside that, both distributions follow an almost normal ditribution Loking at their histogram plot and probability plot expet in the range of "},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data analisys and Data processing:"},{"metadata":{},"cell_type":"markdown","source":"We'll start by the amount of missing data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_na = (df_atp.isnull().sum() / len(df_atp)) * 100\ntrain_na = train_na.drop(train_na[train_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Pourcentage of missing values' :train_na})\nmissing_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#With a visiualisation:\nf, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=train_na.index, y=train_na)\nplt.xlabel('Columns', fontsize=15)\nplt.ylabel('Pourcentage of missing values', fontsize=15)\nplt.title('Pourcentage of missing values by variables', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some explications about this missing values:\n- Most of the columns with big missing values are from the odds of betting sites: we shall remove all these columns and keep the columns of the three betting sites :  Bet365, EX and PS. First we are going to train a model with data containing these values and also another one with data not containing these values to see its effects on our score. We will be using thoses to also calculate how much we'll win or lose in we used a very simple betting stategy based on the prediction of our model :)\n\n- The columns L5,L4,W4,W5 (Number of games won in 5th\\4th set by match winner\\loser respectivly) have missing values because some matches have only 'best out of 3sets' rule while some have 'best out of 5sets' rules.\n- We can think intuitively that we can't use either the columns L1 trough L5 or W5 trought W1 in our predictive modeling phase training data as those variable are set after the matches finishes and we'll be using a discriminative approach in our modeling. So we'll remove those columns too. We can maybe use them if we want to calculate a modified wining set prior probability of each player (in a more advanced modelisation we can think of predicting those values).\n\nThe following columns:\n\n- MaxW= Maximum odds of match winner (as shown by Oddsportal.com)\n- MaxL= Maximum odds of match loser (as shown by Oddsportal.com)\n- AvgW= Average odds of match winner (as shown by Oddsportal.com)\n- AvgL= Average odds of match loser (as shown by Oddsportal.com)\n\nmaybe having missing as there are mssing values in some matches from all the betting site ? We shall remove them as more than 60 of these column vlues are missing.\n\n- The columns Lstes and Wsets wich mean Number of sets won by match loser/winner can't be used in as entry data to our model as they are too know after the moatches have finished. But, we we'll keep them to make another variable witch will be the prior probability of winning a sets in the past. We will replace the nan values by mean or median after we explore these columns.\n\n- As for Lrank or WRank columns, I can't imagine another senario as the one of a new player that just got for the first time in the tournements. we will replace those too by mean or median after we explore those columns too.\n\nThe same applies to the following columns :\n\n- WPts = ATP Entry points of the match winner as of the start of the tournament\n- LPts = ATP Entry points of the match loser as of the start of the tournament\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop the columns with missing values and that we won't be using:\nfor column in train_na.index[:26]:\n    df_atp.drop(column, axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#With a visiualisation:\ntrain_na = (df_atp.isnull().sum() / len(df_atp)) * 100\ntrain_na = train_na.drop(train_na[train_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Pourcentage of missing values' :train_na})\nf, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=train_na.index, y=train_na)\nplt.xlabel('Columns', fontsize=15)\nplt.ylabel('Pourcentage of missing values', fontsize=15)\nplt.title('Pourcentage of missing values by variables', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We also drop the columns W1,L1,W2,L2 as they are intuitivly useless as explained earlier.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_atp.drop('W1', axis = 1, inplace = True)\ndf_atp.drop('L1', axis = 1, inplace = True)\ndf_atp.drop('W2', axis = 1, inplace = True)\ndf_atp.drop('L2', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let\"s explore the remaining columns to see what the best possible values to replace the nan values."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df_atp['WPts'].dropna(), label=\"Winners\")\nsns.distplot(df_atp['LPts'].dropna(), label=\"Losers\")\nplt.ylabel('Frequency')\nplt.xlabel('Pourcentage of victory in the past')\nplt.title('Winners and Losers pourcentage Distribution')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"seems legit to replace it by mode"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df_atp['PSW'].dropna(), label=\"Winners\")\nsns.distplot(df_atp['PSL'].dropna(), label=\"Losers\")\nplt.ylabel('Frequency')\nplt.xlabel('Pourcentage of victory in the past')\nplt.title('Winners and Losers pourcentage Distribution')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"also mode"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_atp['EXW']= pd.to_numeric(df_atp['EXW'], errors='coerce')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_atp['EXW']= pd.to_numeric(df_atp['EXW'], errors='coerce')\nsns.distplot(df_atp['EXW'].dropna(), label=\"Winners\")\nsns.distplot(df_atp['EXL'].dropna(), label=\"Losers\")\nplt.ylabel('Frequency')\nplt.xlabel('Pourcentage of victory in the past')\nplt.title('Winners and Losers pourcentage Distribution')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_atp['B365W']= pd.to_numeric(df_atp['B365W'], errors='coerce')\nsns.distplot(df_atp['B365W'].dropna(), label=\"Winners\")\nsns.distplot(df_atp['B365L'].dropna(), label=\"Losers\")\nplt.ylabel('Frequency')\nplt.xlabel('Pourcentage of victory in the past')\nplt.title('Winners and Losers pourcentage Distribution')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"also mode"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_atp['Wsets']=pd.to_numeric(df_atp['Wsets'],errors='coerce' )\n#df_atp['Lsets']=pd.to_numeric(df_atp['Lsets'],errors='coerce')\n#df_atp['Wsets'].replace('scott', np.nan, inplace=True)\nsns.distplot(df_atp['Wsets'].dropna(), label=\"Winners\",kde=False)\nsns.distplot(df_atp['Lsets'].dropna(), label=\"Losers\")\nplt.ylabel('Frequency')\nplt.xlabel('Pourcentage of victory in the past')\nplt.title('Winners and Losers pourcentage Distribution')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_atp['LRank']=pd.to_numeric(df_atp['LRank'],errors='coerce' )\ndf_atp['WRank']=pd.to_numeric(df_atp['WRank'],errors='coerce' )\nsns.distplot(df_atp['LRank'].dropna(), label=\"Winners\")\nsns.distplot(df_atp['WRank'].dropna(), label=\"Losers\")\nplt.ylabel('Frequency')\nplt.xlabel('Pourcentage of victory in the past')\nplt.title('Winners and Losers pourcentage Distribution')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"replacring the value by the mode or mean then plotting poucentage of missing again."},{"metadata":{"trusted":true},"cell_type":"code","source":"columns=['WPts','LPts','PSW','PSL','EXW','EXL','B365W','B365L','Lsets','Wsets','LRank','WRank']\nfor column in columns:\n    df_atp[column]=df_atp[column].fillna(float(df_atp[column].mode()[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_na = (df_atp.isnull().sum() / len(df_atp)) * 100\ntrain_na = train_na.drop(train_na[train_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Pourcentage of missing values' :train_na})\nmissing_data\n#No more missing values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## More data processing for our datasets so we can use it for prediction and EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_atp.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Description of the rest of the Data:\n- 'B365L', 'B365W','EXL', 'EXW','PSL', 'PSW' : Just betting odds for wiiner and loser repsctivly.\n- 'Bestof' : Maximum number of sets playable in match.\n- 'Comment' :Comment on the match (Completed, won through retirement of loser, or via Walkover)\n- 'Court' :  Type of court (outdoors or indoors)\n- 'Date' : date of the match obviously.\n- 'WPts','LPts' : ATP Entry points of the match winner/loser as of the start of the tournament repectivly.\n- 'Location' : Venue of tournament.\n- 'Winner','Loser' :  Name of winner/loser respectively.\n- 'Wsets','Lsets' : Number of sets won by match winner/loser respectively\n- 'Round' : Round of match\n- 'Series' : Name of ATP tennis series (Grand Slam, Masters, International or International Gold)\n- 'Surface' : Type of surface (clay, hard, carpet or grass)\n- 'Tournament' : me of tounament (including sponsor if relevant)\n- 'WRank','LRank' : ATP Entry ranking of the match winner/loser respectively as of the start of the tournament\n- 'Winner_percentage','Loser_percentage': ..."},{"metadata":{},"cell_type":"markdown","source":"As we said earlier, the column 'B365L', 'B365W','EXL', 'EXW','PSL', 'PSW' will be used in a our modelisationat first and we develop also the same models without those columns. We can think of these columns as some already calculated features given to us by the betting company.\n\n- The 'comment' column is an event that happend after the matches is finisehd so we'll not be using it too.\n\n- 'Wsets' and 'Lsets' columns are that reprensents events that happens after the matche is finnished so we'll not be usig it in our model but we'll make of it a new column that reprensets the winner/loser percentage of winning a sets in the past. But we'll drop them after we do some feature engineering with that won't include them having doing some leakage of the target variable; we'll use the calclculate the probability of a player to win a set in the past.\n\n- All the rest of the columns will be used except maybe fot the tournement name and Location that we'll need to explore them first to see."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_atp.drop(\"Comment\", axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_atp.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_atp.Tournament.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_atp.Location.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df_atp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both columns have only 214,115 unique in a 52298 Data set. thinking about also removing the 2018 and 2017 matches (the later for the tests) and using a cross validation function. I'm not sure if they have any predcitve power seing the few samples for each class. but we'll be keeping them as predictive variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_atp.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Calculating the player prior win set probability : "},{"metadata":{"trusted":true},"cell_type":"code","source":"#winner prior sets winns pourcentage column\nunique_player_index_and_score = {}\n#Dictionary containing the player name as a key and the tuple (player_unique_index,x,y)\n#x : number_of_set_won\n#y : number_of_sets_played\n# x and y are intiated 0 in the bigining but as we go through the data set we increment x Wsets(or Lsets) witch are the number of\n# won by matches winner(orloser) and we increment y by Wsets+Lsets wich is the number of stes played in that match\ni=0\nfor player in df_atp['Winner'].unique():\n    if player not in unique_player_index_and_score.keys():\n        unique_player_index_and_score[player] = (i,0,0)\n        i+=1\nfor player in df_atp['Loser'].unique():\n    if player not in unique_player_index_and_score.keys():\n        unique_player_index_and_score[player] = (i,0,0)\n        i+=1\n        \nprint('Number of unqiue player names : ',i)\nwinner_loser_score_tracking_vector = np.zeros((len(df_atp),2)) \n# two columns one to track the winner percetage and the other for the loser percentage \ndf_atp=df_atp.sort_values(by='Date')\nfor i in range(len(df_atp)):\n    row=[df_atp.Winner[i],df_atp.Loser[i]]\n    score_winner = unique_player_index_and_score[row[0]]#Winner up-to date set win score tracking from the dictionary \n    score_loser = unique_player_index_and_score[row[1]]#Loser up-to date  set win score tracking from the dictionary\n    #we consider new player that haven't yet had 15 sets yet as they had a 20% of winning in the past \n    #(kind of a fair optimist approach as the worked hard to get to play in the tournement:))\n    if int(score_winner[2])<15:\n        winner_loser_score_tracking_vector[i,0]=0.2\n    else:\n        winner_loser_score_tracking_vector[i,0] =score_winner[1]/score_winner[2]\n    if score_loser[2]<15:\n        winner_loser_score_tracking_vector[i,1]=0.2\n    else:\n        winner_loser_score_tracking_vector[i,1] = score_loser[1]/score_loser[2]\n    #updating the dictionary based on the new outcome of the current match\n    unique_player_index_and_score[row[0]] = (score_winner[0],score_winner[1]+float(df_atp.Wsets[i]),score_winner[2]+float(df_atp.Wsets[i]+df_atp.Lsets[i]))#Winner\n    unique_player_index_and_score[row[1]] = (score_loser[0],score_loser[1]+float(df_atp.Lsets[i]),score_loser[2]+float(df_atp.Wsets[i]+df_atp.Lsets[i]))#loser\n    \ndf_atp['Winner_set_percentage'] = winner_loser_score_tracking_vector[:,0]\ndf_atp['Loser_set_percentage'] = winner_loser_score_tracking_vector[:,1]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_atp['Winner_set_percentage'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df_atp['Winner_set_percentage'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_atp['Loser_set_percentage'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df_atp['Loser_set_percentage'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again we can remark that winners tend to win and loser tend to lose."},{"metadata":{},"cell_type":"markdown","source":"###### Calculating the Elo ranking features : "},{"metadata":{},"cell_type":"markdown","source":"The Elo rating system is a method for calculating the relative skill levels of players in zero-sum games such as chess. It is named after its creator Arpad Elo, a Hungarian-American physics professor.\n\nThe difference in the ratings between two players serves as a predictor of the outcome of a match. Two players with equal ratings who play against each other are expected to score an equal number of wins. A player whose rating is 100 points greater than their opponent's is expected to score 64%; if the difference is 200 points, then the expected score for the stronger player is 76%."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Not mine, I took from the internet But i got a full understanding of it :)\ndef compute_elo_rankings(data):\n    \"\"\"\n    Given the list on matches in chronological order, for each match, computes \n    the elo ranking of the 2 players at the beginning of the match\n    \n    \"\"\"\n    print(\"Elo rankings computing...\")\n    players=list(pd.Series(list(data.Winner)+list(data.Loser)).value_counts().index)\n    elo=pd.Series(np.ones(len(players))*1500,index=players)\n    ranking_elo=[(1500,1500)]\n    for i in range(1,len(data)):\n        w=data.iloc[i-1,:].Winner\n        l=data.iloc[i-1,:].Loser\n        elow=elo[w]\n        elol=elo[l]\n        pwin=1 / (1 + 10 ** ((elol - elow) / 400))    \n        K_win=32\n        K_los=32\n        new_elow=elow+K_win*(1-pwin)\n        new_elol=elol-K_los*(1-pwin)\n        elo[w]=new_elow\n        elo[l]=new_elol\n        ranking_elo.append((elo[data.iloc[i,:].Winner],elo[data.iloc[i,:].Loser])) \n        if i%5000==0:\n            print(str(i)+\" matches computed...\")\n    ranking_elo=pd.DataFrame(ranking_elo,columns=[\"elo_winner\",\"elo_loser\"])    \n    ranking_elo[\"proba_elo\"]=1 / (1 + 10 ** ((ranking_elo[\"elo_loser\"] - ranking_elo[\"elo_winner\"]) / 400))   \n    return ranking_elo","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Elo =  compute_elo_rankings(df_atp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_atp[\"Elo_Winner\"] = Elo[\"elo_winner\"]\ndf_atp[\"Elo_Loser\"] = Elo[\"elo_loser\"]\ndf_atp[\"Proba_Elo\"]= Elo[\"proba_elo\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df_atp[\"Elo_Winner\"], label=\"Winners\")\nsns.distplot(df_atp[\"Elo_Loser\"], label=\"Losers\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"again winners tend to win biger elo rating and loser tend to have a smaller one."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df_atp[\"Proba_Elo\"],fit=norm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our distribution of probability is little skewwed trough the right."},{"metadata":{},"cell_type":"markdown","source":"* Let's now drop those 'Wsets' and 'Lsets' columns as they reprensents events that happens after the matche  is finished.\n "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_atp.drop(['Wsets','Lsets'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One last transformation of our data to augmented it and in the mean time to make it ready for modelisation and to explore it more too.\n\n-For each row describing a match we'll be having two resulting row. One with target variable 1 and we keep the columns as they are and one with the target variable 0 we put the invert the column of the wiinner with that of the loser and the column of the loser with that the wiinner and we keep the rest of the columsn as they are. That will double the amount of our traning data and will also transform our problem to a binary classification problem."},{"metadata":{"trusted":true},"cell_type":"code","source":"target_1 = np.ones(len(df_atp))\ntarget_2 = np.zeros(len(df_atp))\ntarget_1 = pd.DataFrame(target_1,columns=['label'])\ntarget_2 = pd.DataFrame(target_2,columns=['label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\nimport category_encoders as ce\nprint(df_atp.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_categorical = df_atp[[\"Series\",\"Court\",\"Surface\",\"Round\",\"Best of\",\"Tournament\"]].copy()\nfeatures_onehot = pd.get_dummies(features_categorical)\n#tournaments_encoded = features_tournaments_encoding(df_atp)\n#features_binary = pd.concat([features_categorical_encoded,tournaments_encoded],1)\n\n## For the moment we have one row per match. \n## We \"duplicate\" each row to have one row for each outcome of each match. \n## Of course it isn't a simple duplication of  each row, we need to \"invert\" some features\n\n# Elo data\nelo_rankings = df_atp[[\"Elo_Winner\",\"Elo_Loser\",\"Proba_Elo\"]]\nelo_1 = elo_rankings\nelo_2 = elo_1[[\"Elo_Loser\",\"Elo_Winner\",\"Proba_Elo\"]]\nelo_2.columns = [\"Elo_Winner\",\"Elo_Loser\",\"Proba_Elo\"]\nelo_2.Proba_Elo = 1-elo_2.Proba_Elo\n# Player prior win probability\nwin_pourcentage = df_atp[['Winner_percentage', 'Loser_percentage']]\nwin_1 = win_pourcentage\nwin_2 = win_1[['Loser_percentage','Winner_percentage']]\nwin_2.columns = ['Winner_percentage', 'Loser_percentage']\n# Player prior win set probability\nset_win_pourcentage = df_atp[['Winner_set_percentage','Loser_set_percentage']]\nset_1 = set_win_pourcentage\nset_2 = set_1[['Loser_set_percentage','Winner_set_percentage']]\nset_2.columns = ['Winner_set_percentage','Loser_set_percentage']\n# Player entry points\nPts = df_atp[['WPts','LPts']]\nPts_1 = Pts\nPts_2 = Pts_1[['LPts','WPts']]\nPts_2.columns = ['WPts','LPts']\n# Player Entry Ranking\nRank = df_atp[['WRank','LRank']]\nRank_1 = Rank\nRank_2 = Rank_1[['LRank','WRank']]\nRank_2.columns = ['LRank','WRank']\n#Player Odds for winning\nOdds = df_atp[['EXW','EXL','PSW','PSL','B365W','B365L']]\nOdds_1 = Odds\nOdds_2 = Odds_1[['EXL','EXW','PSL','PSW','B365L','B365W']]\nOdds_2.columns = ['EXW','EXL','PSW','PSL','B365W','B365L']\n#Date \nDate_1 = df_atp.Date\nDate_2 = df_atp.Date\nelo_1.index = range(0,2*len(elo_1),2)\nelo_2.index = range(1,2*len(elo_1),2)\nwin_1.index = range(0,2*len(win_1),2)\nwin_2.index = range(1,2*len(win_1),2)\nset_1.index = range(0,2*len(set_1),2)\nset_2.index = range(1,2*len(set_1),2)\nPts_1.index = range(0,2*len(Pts_1),2)\nPts_2.index = range(1,2*len(Pts_1),2)\nRank_1.index = range(0,2*len(Rank_1),2)\nRank_2.index = range(1,2*len(Rank_1),2)\nOdds_1.index = range(0,2*len(Odds_1),2)\nOdds_2.index = range(1,2*len(Odds_1),2)\nDate_1.index = range(0,2*len(Date_1),2)\nDate_2.index = range(1,2*len(Date_1),2)\ntarget_1.index = range(0,2*len(target_1),2)\ntarget_2.index = range(1,2*len(target_1),2)\nfeatures_elo_ranking = pd.concat([elo_1,elo_2]).sort_index(kind='merge')\nfeatures_win_pourcentage = pd.concat([win_1,win_2]).sort_index(kind='merge')\nfeatures_set_pourcentage = pd.concat([set_1,set_2]).sort_index(kind='merge')\nfeatures_Pts = pd.concat([Pts_1,Pts_2]).sort_index(kind='merge')\nfeatures_Rank =  pd.concat([Rank_1,Rank_2]).sort_index(kind='merge')\nfeatures_Odds = pd.concat([Odds_1,Odds_2]).sort_index(kind='merge')\ntarget = pd.concat([target_1,target_2]).sort_index(kind='merge')\nDate = pd.concat([Date_1,Date_2]).sort_index(kind='merge').to_frame()\n'''\nfeatures_Odds.reset_index(drop=True, inplace=True)\nfeatures_elo_ranking.reset_index(drop=True, inplace=True)\n#features_onehot.reset_index(drop=True, inplace=True)\nfeatures_win_pourcentage.reset_index(drop=True, inplace=True)\nfeatures_set_pourcentage.reset_index(drop=True, inplace=True)\nfeatures_set_pourcentage.reset_index(drop=True, inplace=True)\nfeatures_Pts.reset_index(drop=True, inplace=True)\nfeatures_Rank.reset_index(drop=True, inplace=True)\nfeatures_Odds.reset_index(drop=True, inplace=True)\ntarget.reset_index(drop=True, inplace=True)\n'''\nfeatures_onehot = pd.DataFrame(np.repeat(features_onehot.values,2, axis=0),columns=features_onehot.columns)\nfeatures_onehot.set_index(pd.Series(range(0,2*len(df_atp))), inplace=True)\nfeatures_Odds.set_index(pd.Series(range(0,2*len(df_atp))), inplace=True)\nfeatures_elo_ranking.set_index(pd.Series(range(0,2*len(df_atp))), inplace=True)\nfeatures_win_pourcentage.set_index(pd.Series(range(0,2*len(df_atp))), inplace=True)\nfeatures_set_pourcentage.set_index(pd.Series(range(0,2*len(df_atp))), inplace=True)\nfeatures_Pts.set_index(pd.Series(range(0,2*len(df_atp))), inplace=True)\nfeatures_Rank.set_index(pd.Series(range(0,2*len(df_atp))), inplace=True)\nfeatures_Odds.set_index(pd.Series(range(0,2*len(df_atp))), inplace=True)\ntarget.set_index(pd.Series(range(0,2*len(df_atp))), inplace=True)\nDate.set_index(pd.Series(range(0,2*len(df_atp))),inplace=True)\n### Building of the pre final dataset \n# We can remove some features to see the effect on our model\nfeatures = pd.concat([features_win_pourcentage,\n                  features_set_pourcentage,\n                  features_elo_ranking,\n                  features_Pts,\n                  features_Rank,\n                  features_Odds,\n                  features_onehot,\n                  Date,\n                  target],1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Setting the 2019 matches as the test dataset.\n#beg = datetime(2016,1,1)\nend_train = datetime(2019,1,1)\nbeg_test = datetime(2019,1,1)\nend_test = datetime(2020,1,1)\ntrain = features[features['Date']<end_train]\ntest = features[(features['Date']>=beg_test)&(features['Date']<end_test)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#For saving the features\n#features.to_csv(\"df_atp_features.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#loading after saveing\n#features = pd.read_csv('df_atp_features.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train))\nprint(len(test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{},"cell_type":"markdown","source":"Let's see what might be our most important Features at first try. ."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\ndf = features.drop(columns=['Date','label'])\nfeat_forest = RandomForestClassifier(n_jobs=-1)\nfeat_forest.fit(X=df, y=features['label'])\n\nplt.figure(figsize=(10, 10))\nfeat_imp = feat_forest.feature_importances_\n\nfeat_imp, cols = zip(*sorted(zip(feat_imp, df.columns)))\nfeat_imp = np.array(feat_imp)[-30:]\ncols = np.array(cols)[-30:]\nd = {'feat_name': cols\n    ,'feat_imp': feat_imp }\nimportance =  pd.DataFrame(data=d)\nsns.barplot( x=  importance['feat_imp'],y = importance['feat_name']\n           );\nplt.yticks(range(len(cols[-30:])), cols[-30:])\nplt.title(\"Features Relevance for Classification\")\nplt.xlabel(\"Relevance Percentage\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will try to build models that are not relying on betting odds,because I've tested models relying on betting odds and the performance were great almost 98% for the best model. Let's remove those columns from the train and test set and see what we'll get; the concerned columns that needs to be removed are : 'EXW','EXL','PSW','PSL','B365W' and 'B365L'"},{"metadata":{},"cell_type":"markdown","source":"## Modeling and performance evaluation:"},{"metadata":{},"cell_type":"markdown","source":"We will use a StratifiedKFold as our cross validation strategy(with k=10) to evaluate our models performance on the training phase. After that, we assess our models on the testing data as well and see if those score are signitifictly closer to each other or that we're over-fiting."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split,StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n# We will be using the accuracy, precision,recall and the f1  as scores to asses our model performence\n#Importing most important alogorithms \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC#we will not be using SVM due tot he huge training time required on our dataset.\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis,LinearDiscriminantAnalysis\n\nfrom sklearn import model_selection #Cross-validation multiple scoring function\n\n#features.drop(Odds_1.columns,axis=1,inplace=True)\nX = train.drop(columns=['Date','label','EXW', 'EXL', 'PSW', 'PSL', 'B365W', 'B365L'])\nY = train['label']\n# prepare configuration for cross validation test harness\nseed = 42\n# prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('QDA',QuadraticDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier(5, n_jobs=-1)))\nmodels.append(('CART', DecisionTreeClassifier(max_depth=10)))\nmodels.append(('NB', GaussianNB()))\n#models.append(('SVM_linear', SVC(kernel=\"linear\", C=0.025)))\n#models.append(('SVM_',SVC(gamma=2, C=1)))\nmodels.append(('RandomForest',RandomForestClassifier( n_estimators=100, n_jobs=-1)))\nmodels.append(('MLP',MLPClassifier(alpha=0.0001)))\nmodels.append(('ADABoost',AdaBoostClassifier()))\n\n# evaluate each model in turn\n\nresults = []\nscoring = {'accuracy': make_scorer(accuracy_score),\n          'precision_score': make_scorer(precision_score),\n          'recall_score' : make_scorer(recall_score),\n          'f1_score' : make_scorer(f1_score)}\nnames = []\nfor name, model in models:\n    stratifiedKFold = model_selection.StratifiedKFold(n_splits=10, random_state=seed)\n    cv_results = model_selection.cross_validate(model, X, Y, cv=stratifiedKFold, scoring=scoring) \n    results.append(cv_results)\n    names.append(name)\n    msg ='-------------------------------------------------------------------------------------------------------------\\n'\n    msg = \"Model : %s \\n\" % (name)\n    msg = msg +'\\n'\n    msg =  msg + \"Accuracy :  %f (%f)\\n\" % (cv_results['test_accuracy'].mean(),cv_results['test_accuracy'].std())\n    msg =  msg + \"Precision score :  %f (%f)\\n\" % (cv_results['test_precision_score'].mean(),cv_results['test_precision_score'].std())\n    msg =  msg + \"Recall score :  %f (%f)\\n\" % (cv_results['test_recall_score'].mean(),cv_results['test_recall_score'].std())\n    msg =  msg + \"F1 score :  %f (%f)\\n\" % (cv_results['test_f1_score'].mean(),cv_results['test_f1_score'].std())\n    msg = msg + '------------------------------------------------------------------------------------------------------------\\n'\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Accuracy = []\nPrecision = []\nRecall = []\nF1 = []\nfor idx,scores in enumerate(results):\n    Accuracy.append(scores['test_accuracy'])\n    Precision.append(scores['test_precision_score'])\n    Recall.append(scores['test_recall_score'])\n    F1.append(scores['test_f1_score'])\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(14,12))\nfig.suptitle('Algorithms Comparison')\nax = fig.add_subplot(221)\nplt.boxplot(Accuracy)\nplt.title('Accuracy score')\nax.set_xticklabels(names)\nax = fig.add_subplot(222)\nplt.boxplot(Precision)\nplt.title('Precision Score')\nax.set_xticklabels(names)\nax = fig.add_subplot(223)\nplt.boxplot(Recall)\nax.set_xticklabels(names)\nplt.title('Recall score')\nax = fig.add_subplot(224)\nplt.title('F1 score')\nplt.boxplot(F1)\nax.set_xticklabels(names)\n\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see clearly that almoust all our models have accuracy, precision, recaal, f1 score that are somewhat descent for a first try. But the neural network model with 100 hidden layers is the most well performing by far, followed by the CART model and followed suprisingly by K nearst neighbour classifiers. Let's see if this same performance is reflected on our test set too."},{"metadata":{"trusted":true},"cell_type":"code","source":"#now to test\nfrom time import time\n\nX_test = test.drop(columns=['Date','label','EXW', 'EXL', 'PSW', 'PSL', 'B365W', 'B365L'])\nY_test = test['label']\n\ny_pred = []\ntrain_time = []\n\nfor name, model in models:\n    tic = time()\n    model.fit(X, Y)\n    toc = time()\n    \n    y_pred.append(model.predict(X_test))\n    train_time.append(toc - tic)\n    \n    print(\"Classifier : {} ===> Training duration : {} sec\".format(name, train_time[-1]))\n    \n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reports = []\nmetrics = [\"Classifier\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\",'Training Duration (seconds)']\nfor idx, y_clf in enumerate(y_pred):\n    acc = accuracy_score(Y_test, y_clf)\n    pre = precision_score(Y_test, y_clf)\n    rec = recall_score(Y_test, y_clf)\n    f1s = f1_score(Y_test, y_clf)\n    report = (models[idx][0], acc, pre, rec, f1s,train_time[idx])\n    reports.append(report)       \ndisplay(pd.DataFrame.from_records(reports, columns=metrics))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reports = pd.DataFrame.from_records(reports, columns=metrics)\nplt.figure(figsize=(10,10))\nplt.plot(reports['Classifier'].values, reports['Accuracy'].values,\n             label='Accuracy' )\nplt.plot(reports['Classifier'], reports['Precision'], lw=1, alpha=0.6,\n             label='Precision' )\nplt.plot(reports['Classifier'], reports['Recall'], lw=1, alpha=0.6,\n             label='Recall' )\nplt.plot(reports['Classifier'], reports['F1-Score'], lw=1, alpha=0.6,\n             label='F1-Score' )\n\n\nplt.xlabel('Algorithm')\nplt.ylabel('score')\nplt.title('Algorithms comparison on test set')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The scores from the cross-validation strategy are almost close to the ones found on the test set. and as we saw in the cross valisation phase We can see clearly that almoust all our models have accuracy, precision, recaal, f1 score that are somewhat descent for a first try. But the neural network model with 100 hidden layers is the most well performing by far, followed by the Random forest and CART models wich are followed by K nearst neighbour classifiers. Let's see if this same performance is reflected on our test set too."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfrom scipy import interp\n\ny_prob = []\n\nfor name, model in models:\n    y_prob.append(model.predict_proba(X_test)[:,1])\n    \ntprs = []\naucs = []\nmean_fpr = np.linspace(0, 1, 100)\n\ni = 0\nplt.figure(figsize=(10,10))\nfor idx, y_clf in enumerate(y_prob):\n    # Compute ROC curve and area the curve\n    fpr, tpr, thresholds = roc_curve(Y_test, y_clf)\n    tprs.append(interp(mean_fpr, fpr, tpr))\n    tprs[-1][0] = 0.0\n    roc_auc = auc(fpr, tpr)\n    aucs.append(roc_auc)\n    plt.plot(fpr, tpr, lw=1, alpha=0.6,\n             label='ROC  Model %s (AUC = %0.2f)' % (models[idx][0], roc_auc))\n\n    i += 1\nplt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n         label='Chance', alpha=.7)\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the neural entwork model with 100 hiddens units has the best trade-off between sensitivity (true positive rate) and specificity (1 – false positive rate ) with Area Under ROC Curve close to 1."},{"metadata":{},"cell_type":"markdown","source":"# Betting\n## Using our model for betting"},{"metadata":{},"cell_type":"markdown","source":"We will try to assess the return on our investement if you relied on those models for our betting decisions."},{"metadata":{"trusted":true},"cell_type":"code","source":"betting_columns = ['EXL','EXW','PSL','PSW','B365L','B365W']\n#Columns containg the Odds\nBetting_Odds =  test[betting_columns]\n\n#Our Capital will be 1500Euros for each strategy and for each betting site for a single model. \nbudget_1 = 1500","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport random\n\n\ndef rollDice():\n    roll = random.randint(1,100)\n\n    if roll == 100:\n        return False\n    elif roll <= 50:\n        return False\n    elif 100 > roll >= 50:\n        return True\n\n\n'''\nSimple bettor, betting the same amount each time. This will be our baselane.\n'''\ndef simple_bettor(data,y_true,budget):\n    #return on investement for each betting site\n    ROI_1 = budget\n    ROI_2 = budget\n    ROI_3 = budget\n    wager = 10\n\n    currentWager = 0\n\n    for i in range(len(data)):\n        if rollDice() and y_true.values[i]==1:\n            ROI_1 += wager*(data['EXW'].values[i]-1)\n            ROI_2 += wager*(data['PSW'].values[i]-1)\n            ROI_3 += wager*(data['B365W'].values[i]-1)\n            #checking if we are already broke\n            if ROI_1<=0:\n                ROI_1 = -100000000000000000000\n            if ROI_2<=0:\n                ROI_2 = -100000000000000000000\n            if ROI_3<=0:\n                ROI_3 = -100000000000000000000                \n\n        elif rollDice() and y_true.values[i]==0:\n            ROI_1 -= wager\n            ROI_2 -= wager\n            ROI_3 -= wager\n            #checking if we are already broke\n            if ROI_1<=0:\n                ROI_1 = -100000000000000000000\n            if ROI_2<=0:\n                ROI_2 = -100000000000000000000\n            if ROI_3<=0:\n                ROI_3 = -100000000000000000000                \n\n        elif not rollDice() and y_true.values[i]==0:\n            ROI_1 += wager*(data['EXL'].values[i]-1)\n            ROI_2 += wager*(data['PSL'].values[i]-1)\n            ROI_3 += wager*(data['B365L'].values[i]-1)\n            #checking if we are already broke\n            if ROI_1<=0:\n                ROI_1 = -100000000000000000000\n            if ROI_2<=0:\n                ROI_2 = -100000000000000000000\n            if ROI_3<=0:\n                ROI_3 = -100000000000000000000                \n\n        else :\n            ROI_1 -= wager\n            ROI_2 -= wager\n            ROI_3 -= wager\n            #checking if we are already broke\n            if ROI_1<=0:\n                ROI_1 = -100000000000000000000\n            if ROI_2<=0:\n                ROI_2 = -100000000000000000000\n            if ROI_3<=0:\n                ROI_3 = -100000000000000000000                \n\n    if ROI_1<0:\n        ROI_1 = 0\n    if ROI_2<0:\n        ROI_2 = 0\n    if ROI_3<0:\n        ROI_3 = 0\n    return [(ROI_1-budget)/budget,(ROI_2-budget)/budget,(ROI_3-budget)/budget]\n\n\n\n#If our model predict that a player is going to win, we'll invest 10Euros on that match for that player winning \n# and compare it with the real value to see if we won or lost\n\ndef strategy_1(data,y_pred,y_true,budget):\n    '''\n    \n       If our model predict that a player is going to win, we'll invest 10Euros on that match for that player winning \n       and compare it with the real value to see if we won or lost\n       \n    '''\n    #Retrun on investement for each betting site\n    ROI_1 = budget\n    ROI_2 = budget\n    ROI_3 = budget\n    for i in range(0,len(test)):\n        if y_pred[i]==1 and y_true.values[i]==1.0:\n            ROI_1 += 10*(data['EXW'].values[i]-1)\n            ROI_2 += 10*(data['PSW'].values[i]-1)\n            ROI_3 += 10*(data['B365W'].values[i]-1)\n            #checking if we are already broke\n            if ROI_1<=0:\n                ROI_1 = -100000000000000000000\n            if ROI_2<=0:\n                ROI_2 = -100000000000000000000\n            if ROI_3<=0:\n                ROI_3 = -100000000000000000000                \n\n        elif y_pred[i]==1 and y_true.values[i]==0.0:\n            ROI_1 += -10\n            ROI_2 += -10\n            ROI_3 += -10\n            #checking if we are already broke\n            if ROI_1<=0:\n                ROI_1 = -100000000000000000000\n            if ROI_2<=0:\n                ROI_2 = -100000000000000000000\n            if ROI_3<=0:\n                ROI_3 = -100000000000000000000                \n\n        elif y_pred[i]==0 and y_true.values[i] == 1.0:\n            #checking if we are already broke\n            ROI_1 += -10\n            ROI_2 += -10\n            ROI_3 += -10\n            if ROI_1<=0:\n                ROI_1 = -100000000000000000000\n            if ROI_2<=0:\n                ROI_2 = -100000000000000000000\n            if ROI_3<=0:\n                ROI_3 = -100000000000000000000                \n\n        else :\n            ROI_1 += 10*(data['EXL'].values[i]-1)\n            ROI_2 += 10*(data['PSL'].values[i]-1)\n            ROI_3 += 10*(data['B365L'].values[i]-1)\n            #checking if we are already broke\n            if ROI_1<=0:\n                ROI_1 = -100000000000000000000\n            if ROI_2<=0:\n                ROI_2 = -100000000000000000000\n            if ROI_3<=0:\n                ROI_3 = -100000000000000000000                \n            \n    if ROI_1<0:\n        ROI_1 = 0\n    if ROI_2<0:\n        ROI_2 = 0\n    if ROI_3<0:\n        ROI_3 = 0\n    return [(ROI_1-budget)/budget,(ROI_2-budget)/budget,(ROI_3-budget)/budget]\n\ndef strategy_2(data,y_proba,y_true,budget):\n    '''\n    \n      In each match we'll invest 10(probability_player_win)Euros for the player winning, and 10(probability_player_lose)Euros\n      for the player losing\n\n    \n    '''\n    ROI_1 = budget\n    ROI_2 = budget\n    ROI_3 = budget\n    for i in range(0,len(test)):\n        if y_true.values[i]==1.0:\n            ROI_1 += y_proba[i]*10*(data['EXW'].values[i]-1) -(1- y_proba[i])*10\n            ROI_2 += y_proba[i]*10*(data['PSW'].values[i]-1) - (1-y_proba[i])*10\n            ROI_3 += y_proba[i]*10*(data['B365W'].values[i]-1) - (1-y_proba[i])*10\n            #checking if we are already broke\n            if ROI_1<=0:\n                ROI_1 = -100000000000000000000\n            if ROI_2<=0:\n                ROI_2 = -100000000000000000000\n            if ROI_3<=0:\n                ROI_3 = -100000000000000000000                \n        else :\n            ROI_1 += (1-y_proba[i])*10*(data['EXL'].values[i]-1) - y_proba[i]*10\n            ROI_2 += (1-y_proba[i])*10*(data['PSL'].values[i]-1) - y_proba[i]*10\n            ROI_3 += (1-y_proba[i])*10*(data['B365L'].values[i]-1) - y_proba[i]*10\n            #checking if we are already broke\n            if ROI_1<=0:\n                ROI_1 = -100000000000000000000\n            if ROI_2<=0:\n                ROI_2 = -100000000000000000000\n            if ROI_3<=0:\n                ROI_3 = -100000000000000000000                \n\n    if ROI_1<0:\n        ROI_1 = 0\n    if ROI_2<0:\n        ROI_2 = 0\n    if ROI_3<0:\n        ROI_3 = 0\n    return [(ROI_1-budget)/budget,(ROI_2-budget)/budget,(ROI_3-budget)/budget]\n\n\n#P.S: Seing how we contructed the dataset (Each row repeated one time). We'll be actualy investing 20Euros of our capital in each match instead of 10\n\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Our Capital will be 1500Euros for each strategy and for each betting site for a single model. \nreports = []\nmetrics = [\"Classifier\",  \"Strat 1 EX\", \"Strat 2 EX\", \"Strat 1 PS\", \"Strat 2 PS\", \"Strat 1 B365\", \"Strat 2 B365\" ,'Random EX', 'Random PS','Random B365']\nfor idx, y_clf in enumerate(y_pred):\n    Random = simple_bettor(Betting_Odds ,Y_test,budget_1)\n    strat_1 = strategy_1(Betting_Odds,y_clf,Y_test,budget_1)\n    strat_2 = strategy_2(Betting_Odds,y_prob[idx],Y_test,budget_1)\n    report = (models[idx][0],strat_1[1],strat_2[1],strat_1[1],strat_2[1],strat_1[2],strat_2[2],Random[0],Random[1],Random[2])\n    reports.append(report)       \ndisplay(pd.DataFrame.from_records(reports, columns=metrics))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reports = pd.DataFrame.from_records(reports, columns=metrics)\nplt.figure(figsize=(10,10))\nplt.plot(reports['Classifier'].values, reports['Strat 1 EX'].values,\n             label='EX : Strategy 1' )\nplt.plot(reports['Classifier'], reports['Strat 2 EX'], lw=1, alpha=0.6,\n             label='EX : Strategy 2' )\nplt.plot(reports['Classifier'], reports['Strat 1 PS'], lw=1, alpha=0.6,\n             label='PS : Strategy 1' )\nplt.plot(reports['Classifier'], reports['Strat 2 PS'], lw=1, alpha=0.6,\n             label='PS : Strategy 2' )\nplt.plot(reports['Classifier'], reports['Strat 1 B365'], lw=1, alpha=0.6,\n             label='B365 : Strategy 1' )\nplt.plot(reports['Classifier'], reports['Strat 2 B365'], lw=1, alpha=0.6,\n             label='B365 : Strategy 2' )\n\n\nplt.xlabel('Algorithm')\nplt.ylabel('Return on investement')\nplt.title('Algorithms ROI on Test set')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The strategy number one on the EX betting site is always the most profitable for all our model. But, Only with the neural network model and the Decision tree and Random forest models we can achieve a return on investment of 30% and 25% repectivly with a Capital of 1500Euros and a 20Euros investement per match."},{"metadata":{},"cell_type":"markdown","source":"### Further Improvements"},{"metadata":{},"cell_type":"markdown","source":"- Hyper-parameters tuning for these models.\n- Using a generative Modeling approach (Markov chain).\n\n- Trying more discriminative models : SVM, XGBOOST.\n\n- Modeling Fatigue of players.\n- Using external Data about injuries.\n\n- More features engineering (hand-crafted features).\n- Doing more extensive EDA\n\n- Changing the Encodings of our categorical variables.\n\n\n\nI did not train an SVM model on this dataset as it takes too much times to train taking into account the number of columns in the training data and the considerable number of rows."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}