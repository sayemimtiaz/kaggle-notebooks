{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"Created by: Sangwook Cheon\n\nDate: Dec 24, 2018\n\nThis is step-by-step guide to Clustering using scikit-learn, which I created for reference. I added some useful notes along the way to clarify things. \nThis notebook's content is from A-Z Datascience course, and I hope this will be useful to those who want to review materials covered, or anyone who wants to learn the basics of clustering.\n\n# Content:\n### 1. K-means Clustering \n### 2. Hierarchical Clustering\n\n________\n_________\n\n# K-means Clustering\n![i1](https://i.imgur.com/p3GHQXL.png)\n\n### Random Initialization Trap\nIf the initial centroids are placed at random, they can potentially determine the way clusters form, which is not idea as there exists optimal initial centroids that need to be placed so that the algorithm works properly. In order to achieve, this K-means ++ algorithm is used. This happens in the background of libraries, so no need to do this manually.\n\n### Choosing the right number of clusters (initial clusters)\nWithin Cluster Sum of Squares (WSCC) algorithm\n![i13](https://i.imgur.com/NZ2jRt3.png)\n\nElbow method:\n![i14](https://i.imgur.com/0k6ALfB.png)\n\nChoose the point where there is a significant drop from behind, and low drop after that point.\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndataset = pd.read_csv('../input/Mall_Customers.csv')\nx = dataset.iloc[:, [3, 4]].values\n\n#Using elbow method to find the optimal number of clusters\nfrom sklearn.cluster import KMeans\nwcss = []\n\n#Iterating over 1, 2, 3, ---- 10 clusters\nfor i in range(1, 11): \n    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n    kmeans.fit(x)\n    wcss.append(kmeans.inertia_) # intertia_ is an attribute that has the wcss number\nplt.plot(range(1,11), wcss)\nplt.title(\"Elbow method applied to Mall_customers dataset\")\nplt.xlabel(\"number of clusters\")\nplt.ylabel(\"wcss\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cff05cef3fe708d2e9f21ce4961380040830858b"},"cell_type":"markdown","source":"As can be seen here, the optimal value is 5 clusters. "},{"metadata":{"trusted":true,"_uuid":"9d9acf56dd34addca92251a63e6efe4e7904dea2"},"cell_type":"code","source":"# Apply kmeans to the dataset\nkmeans = KMeans(n_clusters = 5, max_iter = 300, init = 'k-means++', n_init = 10, random_state = 0)\ny_kmeans = kmeans.fit_predict(x) #predict which cluster each point belongs to\ny_kmeans\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a04e737ad2ad6b59dae6901538ccf34ace394e2d"},"cell_type":"code","source":"#visualizing the clusters\n\nplt.figure(2)\nplt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], s = 50, c = 'red', label = 'Customer 1')\nplt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], s = 50, c = 'blue', label = 'Customer 2')\nplt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1], s = 50, c = 'green', label = 'Customer 3')\nplt.scatter(x[y_kmeans == 3, 0], x[y_kmeans == 3, 1], s = 50, c = 'cyan', label = 'Customer 4')\nplt.scatter(x[y_kmeans == 4, 0], x[y_kmeans == 4, 1], s = 50, c = 'purple', label = 'Customer 5')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'centroids' )\n# _centers_ --> also an attribute that can be accessed.\n# s --> size of each point\nplt.title(\"K-means clustering applied to Mall_Customers\")\nplt.xlabel('Annual Income ($)')\nplt.ylabel('Spending score (1-100)')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a81ebeb80c987a387d20a53119272c6de187a61"},"cell_type":"code","source":"#Now that each group is identified, we can name each category.\n\nplt.figure(3)\nplt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], s = 50, c = 'red', label = 'careful')\nplt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], s = 50, c = 'blue', label = 'standard')\nplt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1], s = 50, c = 'green', label = 'target')\nplt.scatter(x[y_kmeans == 3, 0], x[y_kmeans == 3, 1], s = 50, c = 'cyan', label = 'reckless')\nplt.scatter(x[y_kmeans == 4, 0], x[y_kmeans == 4, 1], s = 50, c = 'purple', label = 'sensible')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'centroids' )\n# _centers_ --> also an attribute that can be accessed.\n# s --> size of each point\nplt.title(\"K-means clustering applied to Mall_Customers\")\nplt.xlabel('Annual Income ($)')\nplt.ylabel('Spending score (1-100)')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a0f4e7f9e95e507f657d49bbffbb34320225230"},"cell_type":"markdown","source":"# Hierarchical Clustering\n## Type 1: Agglomerative\n![i154](https://i.imgur.com/aoMmHRy.png)\nOptions to measure distance between clusters:\n![i345](https://i.imgur.com/i3G9Lnt.png)\nHow dendrograms work (which are the memory of HC model):\n![i10923](https://i.imgur.com/6Fy4Cxv.png)\nVertical height of the column represents the distance, and the horizontal lines signifies that the two clusters have been combined.\n\nHow to use the dendrogram: You can set the threshold value for dissimilarity (distance between the clusters), so that when combining two leads to a higher value, the algorithm would prevent merge from happening. This threshold can be different by context, but an optimal threshold can be roughly found.\n\n--> Set the threshold that it crosses the largest distance of the vertical line there is on the dendrogram. The vertical line with the largest distance should not cross any extended horizontal lines of columns. Example:\n![i83](https://i.imgur.com/7geN4Wi.png)\n\n## Type 2: Divisive"},{"metadata":{"trusted":true,"_uuid":"54ddc173c97aaf8cc49a9a885bc7763ca62dec40"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndataset = pd.read_csv('../input/Mall_Customers.csv')\nx = dataset.iloc[:, [3, 4]].values\n# y is not available, as we do not know the patterns we expect. It's our job to find patterns from x.\n\n#Use dendrogram to find the optimal number of clusters\nimport scipy.cluster.hierarchy as sch\ndendrogram_1 = sch.dendrogram(sch.linkage(x, method = 'ward'))\nplt.title('Dendograms')\nplt.xlabel('Customers')\nplt.ylabel('Euclidean distances')\nplt.show()\n# ward method tries to minimize variance between clusters","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb7e9e7050fc5da0ca4c24461e2faa72a5af1953"},"cell_type":"markdown","source":"Here the longest distance would be:\n![i238047](https://i.imgur.com/efEqQ5F.png)\nAs no extended horizontal lines cross this line. Then, count the number of vertical lines that are included in the section (between two red horizontal lines). As shown on the image, there are 5 vertical lines, showing that 5 clusters are the optimal number. This result has already been shown by K-means clustering"},{"metadata":{"trusted":true,"_uuid":"7eafab406d70240142751a9040a8d9b89a790ba5"},"cell_type":"code","source":"# Fitting HC to the mall dataset\n\nfrom sklearn.cluster import AgglomerativeClustering \nhc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward')\ny_hc = hc.fit_predict(x)\nplt.scatter(x[y_hc == 0, 0], x[y_hc == 0, 1], s = 50, c = 'red', label = 'Customer 1')\nplt.scatter(x[y_hc == 1, 0], x[y_hc == 1, 1], s = 50, c = 'green', label = 'Customer 1')\nplt.scatter(x[y_hc == 2, 0], x[y_hc == 2, 1], s = 50, c = 'blue', label = 'Customer 1')\nplt.scatter(x[y_hc == 3, 0], x[y_hc == 3, 1], s = 50, c = 'purple', label = 'Customer 1')\nplt.scatter(x[y_hc == 4, 0], x[y_hc == 4, 1], s = 50, c = 'orange', label = 'Customer 1')\nplt.title('Applying Hierarchical Clustering to Mall_customer')\nplt.xlabel('Annual income ($)')\nplt.ylabel('Spending Score (1-100)')\nplt.legend()\nplt.show()\n\n#The above segment of code is only for visualizing 2D data, not higher-dimension","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}