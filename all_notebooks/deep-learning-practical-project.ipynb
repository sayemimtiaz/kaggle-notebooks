{"cells":[{"metadata":{"trusted":true,"_uuid":"14262f53c5d8c42f05bca45f7f929ebc4bbda216"},"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nfrom sklearn.utils import shuffle","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc569956eb5355cc1e4284b578ae2e554ce5debe"},"cell_type":"markdown","source":"### Este Notebook Ã© resultado de meus estudos do curso de Redes Neurais Convolucionais do Lazy Programmer. Segue: https://lazyprogrammer.me/deep-learning-courses/."},{"metadata":{"_uuid":"5f13964efaac378bf668c73a8ee5062315b390db"},"cell_type":"markdown","source":"# Prepare the Data"},{"metadata":{"trusted":true,"_uuid":"723d95b864c9022b52d1f99c73a7c837b222ad96"},"cell_type":"code","source":"def getData(balance_ones=True):\n    Y = []\n    X = []\n    first = True\n    for line in open('../input/fer20131.csv'):\n        if first:\n            first = False\n        else:\n            row = line.split(',')\n            Y.append(int(row[0]))\n            X.append([int(p) for p in row[1].split()])\n\n    X, Y = np.array(X) / 255.0, np.array(Y)\n    \n    if balance_ones:\n        X0, Y0 = X[Y!=1, :], Y[Y!=1]\n        X1 = X[Y==1, :]\n        X1 = np.repeat(X1, 9, axis=0)\n        X = np.vstack([X0, X1])\n        Y = np.concatenate((Y0, [1]*len(X1)))\n\n    return X, Y\n\ndef getImageData():\n    X, Y = getData()\n    N, D = X.shape\n    d = int(np.sqrt(D))\n    X = X.reshape(N, d, d, 1)\n    return X, Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d960dc8885e452ebb6e3a4518cee4f1860ea28ac"},"cell_type":"code","source":"X, Y = getImageData()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d4d546f96ce41b63fec9d4aafd9e1d030b26a01"},"cell_type":"code","source":"labels = list(set(Y))\n\nfig = plt.figure(figsize=(15, 20))\ncolumns = 5\nrows = 5\n\nfor i in labels:\n    imagens = X[Y==i]\n    qtd_imagens = len(imagens)\n    imagem = imagens[random.randint(0, qtd_imagens), :].reshape((48, 48))\n    ax = fig.add_subplot(rows, columns, i + 1)\n    ax.set_title('%d' % (i))\n    plt.imshow(imagem, cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# Creating Neural Network using Tensorflow"},{"metadata":{"trusted":true,"_uuid":"f40a0819bdbb5d596255630927a7140f4a630229"},"cell_type":"code","source":"def init_weight_and_bias(M1, M2):\n    W = np.random.randn(M1, M2) / np.sqrt(M1)\n    b = np.zeros(M2)\n    return W, b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37a8e038c4065d1b1f4172104b808052c1becec9"},"cell_type":"code","source":"class HiddenLayer(object):\n    def __init__(self, M1, M2, activation=None):\n        self.M1 = M1\n        self.M2 = M2\n        self.activation = activation\n        W, b = init_weight_and_bias(M1, M2)\n        self.W = tf.Variable(W.astype(np.float32))\n        self.b = tf.Variable(b.astype(np.float32))\n        self.params = [self.W, self.b]\n    \n    def forward(self, X, is_training):\n        act_value = tf.matmul(X,self.W) + self.b\n        if self.activation is not None:\n            act_value = self.activation(act_value)\n        return act_value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39b0fc501b8c55d96a72da15e0d4b059a1fb4e99"},"cell_type":"code","source":"class ANN(object):\n    def __init__(self, hidden_layer_sizes, layer_class=HiddenLayer):\n        self.hidden_layer_sizes = hidden_layer_sizes\n        self.layer_class = layer_class\n    \n    def build_Layers(self, X, Y, activation):\n        _, D = X.shape\n        K = len(set(Y))\n        \n        self.layers = []\n        M1 = D\n        for M2 in self.hidden_layer_sizes:\n            h = self.layer_class(M1, M2, activation)\n            self.layers.append(h)\n            M1 = M2\n        \n        h = HiddenLayer(M1, K)\n        self.layers.append(h)\n        \n        self.params = []\n        for h in self.layers:\n            self.params += h.params\n        \n        return (None, D)\n    \n    def train(self, inputs, labels, Xtrain, Ytrain, Xvalid, Yvalid, epochs, n_batches, batch_sz, print_period):\n        costs = []\n        init = tf.global_variables_initializer()\n        with tf.Session() as session:\n            session.run(init)\n            for i in range(epochs):\n                Xtrain, Ytrain = shuffle(Xtrain, Ytrain)\n                for j in range(n_batches):\n                    Xbatch = Xtrain[j*batch_sz:(j*batch_sz + batch_sz)]\n                    Ybatch = Ytrain[j*batch_sz:(j*batch_sz + batch_sz)]\n                    session.run(self.train_op, feed_dict={inputs: Xbatch, labels: Ybatch})\n                    \n                    if ((j + 1) % print_period == 0):\n                        c = session.run(self.cost_op, feed_dict={inputs: Xvalid, labels: Yvalid})\n                        p = session.run(self.predict_op, feed_dict={inputs: Xvalid})\n                        costs.append(c)\n                        acc = np.mean(p != Yvalid)\n                        print(\"i:\", i, \"j:\", j, \"nb:\", n_batches, \"cost:\", c, \"error rate:\", acc)\n        return costs\n    \n    def fit(self, X, Y, activation=tf.nn.relu, learning_rate=1e-3, reg=1e-3,mu=0.99, decay=0.99999, print_period=20, epochs=20, batch_sz=100, show_fig=False):\n        learning_rate = np.float32(learning_rate)\n        mu = np.float32(mu)\n        reg = np.float32(reg)\n        decay = np.float32(decay)\n        \n        X = X.astype(np.float32)\n        Y = Y.astype(np.int32)\n        X, Y = shuffle(X, Y)\n        Xvalid  = X[-1000:]\n        Yvalid  = Y[-1000:]\n        Xtrain = X[:-1000]\n        Ytrain = Y[:-1000]\n        \n        N = Xtrain.shape[0]\n        \n        input_shape = self.build_Layers(X, Y, activation)\n        \n        if batch_sz is None:\n            batch_sz = N\n            \n        inputs = tf.placeholder(tf.float32, shape=input_shape, name='inputs')\n        labels = tf.placeholder(tf.int32, shape=(None,), name='labels')\n        logits = self.forward(inputs, is_training=True)\n        \n        self.cost_op = tf.reduce_mean(\n            tf.nn.sparse_softmax_cross_entropy_with_logits(\n                logits=logits,\n                labels=labels\n            )\n        )\n        \n        if(reg is not None):\n            rcost = reg*sum([tf.nn.l2_loss(p) for p in self.params])\n            self.cost_op += rcost\n        \n        self.train_op = tf.train.RMSPropOptimizer(learning_rate, decay=decay, momentum=mu).minimize(self.cost_op)\n        #self.train_op = tf.train.MomentumOptimizer(learning_rate, momentum=0.9, use_nesterov=True).minimize(self.cost_op)\n\n        self.predict_op = self.predict(inputs)\n        \n        n_batches = N // batch_sz\n        \n        costs = self.train(inputs, labels, Xtrain, Ytrain, Xvalid, Yvalid, \n                           epochs, n_batches, batch_sz, print_period)\n       \n        if show_fig:\n            plt.plot(costs)\n            plt.show()\n            \n    def forward(self, X, is_training):\n        out = X\n        for h in self.layers:\n            out = h.forward(out, is_training)\n        return out\n    \n    def predict(self, X):\n        pY = self.forward(X, is_training=False)\n        return tf.argmax(pY, 1)\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2158907360779e8681bde2f125e53255c8dbc53c"},"cell_type":"markdown","source":"## CNN"},{"metadata":{"trusted":true,"_uuid":"bc0cd2fc20d1125f17104faa4d9a3f03ac6d609a","scrolled":true},"cell_type":"code","source":"class ConvPoolLayer(object):\n    def __init__(self, filter_width, filter_height, feature_in, feature_out, pool_sz=(2,2)):\n        self.pool_sz = pool_sz\n        self.shape = (filter_width, filter_height, feature_in, feature_out)\n        self.init_filter()\n        \n    def init_filter(self):\n        W_init = np.random.randn(*self.shape) * np.sqrt(2) / np.sqrt(np.prod(self.shape[:-1]) + self.shape[-1]*np.prod(self.shape[:-2] / np.prod(self.pool_sz)))\n        b_init = np.zeros(self.shape[-1], dtype=np.float32)\n        self.W = tf.Variable(W_init.astype(np.float32))\n        self.b = tf.Variable(b_init)\n        self.params = [self.W, self.b]\n        \n    def convpool(self, X):\n        conv_out = tf.nn.conv2d(X, self.W, strides=[1, 1, 1, 1], padding='SAME')\n        conv_out = tf.nn.bias_add(conv_out, self.b)\n        ksize = [1, self.pool_sz[0], self.pool_sz[1], 1]\n        pool_out = tf.nn.max_pool(conv_out, ksize=ksize, strides=ksize, padding='SAME')\n        return pool_out\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb9504859fb9e4e060b0fe625437f57f7d1e7361"},"cell_type":"code","source":"class CNN(ANN):\n    def __init__(self, hidden_layer_sizes, convpool_layer_sizes):\n        ANN.__init__(self, hidden_layer_sizes)\n        self.convpool_layer_sizes = convpool_layer_sizes\n        \n\n    def build_Layers(self, X, Y, activation):\n        _, H, W, C = X.shape\n        pool_sz = (2, 2)\n        K = len(set(Y))\n        \n        self.convpool_layers = []\n        self.params = []\n        self.layers = []\n        \n        feature_in = C\n        for feature_out, filter_w, filter_h in self.convpool_layer_sizes:\n            layer = ConvPoolLayer(filter_w, filter_h, feature_in, feature_out, pool_sz)\n            self.params += layer.params\n            self.convpool_layers.append(layer)\n            feature_in = feature_out\n            \n        M1 = feature_in * ((H // (len(self.convpool_layers) * pool_sz[0])) * \n                           (W // (len(self.convpool_layers) * pool_sz[1])))\n       \n        for M2 in self.hidden_layer_sizes:\n            layer = self.layer_class(M1, M2, activation)\n            self.params += layer.params\n            self.layers.append(layer)\n            M1 = M2\n        \n        h = HiddenLayer(M1, K)\n        self.params += h.params\n        self.layers.append(h)\n        \n        \n        return (None, H, W, C)\n    \n    def forward(self, X, is_training):\n        out = X\n        for layer in self.convpool_layers:\n            out = layer.convpool(out)\n        \n        out_shape = out.get_shape().as_list()\n        out = tf.reshape(tensor=out, shape=[-1, np.prod(out_shape[1:])])\n        \n        for layer in self.layers:\n            out = layer.forward(out, is_training)\n            \n        return out","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ffc99ca88feee6c3fe8ded8a0ff67ea8b93e2175"},"cell_type":"markdown","source":"# Experiments"},{"metadata":{"trusted":true,"_uuid":"f0c9cb9e804c6137a9a259f5543cd1db04ec083d"},"cell_type":"code","source":"## Experiment 1\n\nX, Y = getData()\n\nmodel = ANN([2000, 1000, 500])\nmodel.fit(X, Y, show_fig=True, batch_sz=100, epochs=10, learning_rate=1e-2, decay=0.999)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d72a70bd27f46271f9dc0ddc28976d9832b1791"},"cell_type":"code","source":"## Experiment 2\n\nX, Y = getImageData()\n\nmodel = CNN(convpool_layer_sizes=[(20, 5, 5), (20, 5, 5)],\n           hidden_layer_sizes=[500, 300])\nmodel.fit(X, Y, show_fig=True, batch_sz=30, epochs=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4728ae2fad7f56c0cddbc4e2c0df7cc8af90613"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}