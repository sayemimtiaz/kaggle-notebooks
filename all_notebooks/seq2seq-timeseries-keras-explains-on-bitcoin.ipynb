{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Seq2Seq timeseries bidirectionnal GRU/LSTM Keras explains on Bitcoin datasets"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Table of Contents\n\n1. [Context](#context)  \n2. [Importations](#importations)  \n3. [Informations](#informations)\n4. [Set parameters](#set_parameters)\n5. [Data exploration](#data_exploration)  \n    5.1 [Import](#import)  \n    5.2 [General analysis](#general_analysis)  \n    5.3 [Visualization](#visualisation)  \n6. [Feature engineering](#feature_engineering)\n7. [Data preparation](#data_preparation)\n8. [Modelisation](#modelisation)  \n    8.1 [Learning](#learning)    \n    8.2 [Learning curves](#learning_curves)  \n    8.3 [Learning rate](#learning_rate)  \n9. [Results](#results)  \n10. [Conclusion](#conclusion)\n11. [References](#references)  "},{"metadata":{},"cell_type":"markdown","source":"# 1. Context <a id=\"context\"></a>"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align:center;\">\n    <img src=\"https://unsplash.com/photos/iGYiBhdNTpE/download?force=true\" style=\"height:100%; width:100%\"/>\n</p>\n<p style=\"text-align:right;\">Credits : Photo by André François McKenzie on Unsplash</p>"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"The goal of this notebook is to show and explain how to implement a Seq2Seq model with GRU/LSTM cells (bidirectionnal or not). In this examples we will used the Bitcoin dataset and try to predict the future value a Bitcoin thanks to the past.\n\nThis notebook will not focus on data preprocessing, bitcoin data are only use as data example."},{"metadata":{},"cell_type":"markdown","source":"# 2. Importations <a id=\"importations\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Get version python/keras/tensorflow/sklearn\nfrom platform import python_version\nimport sklearn\nimport keras\nimport tensorflow as tf\n\n# Folder manipulation\nimport os\n\n# Garbage collector\nimport gc\n\n# Linear algebra and data processing\nimport numpy as np\nimport pandas as pd\nfrom pandas import datetime\n\n# Visualisation of picture and graph\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Keras importation\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom keras.layers import Input, Dense, RNN, Bidirectional, concatenate, GRUCell, LSTMCell\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import Adam\n\n# Others\nfrom tqdm import tqdm, tqdm_notebook","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Informations <a id=\"informations\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input\"))\nprint(\"Keras version : \" + keras.__version__)\nprint(\"Tensorflow version : \" + tf.__version__)\nprint(\"Python version : \" + python_version())\nprint(\"Sklearn version : \" + sklearn.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Set parameters <a id=\"set_parameters\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"MAIN_DIR = \"../input/bitcoin-historical-data/\"\nDATA = \"bitstampUSD_1-min_data_2012-01-01_to_2019-08-12.csv\"\n\n# Size of the dataset in percentage\nTEST_SIZE = 5\nVAL_SIZE = 5\n\nOUTPUT_SIZE = 30\nINPUT_SIZE = 30\n\n# Set graph font size\nsns.set(font_scale=1.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Data exploration <a id=\"data_exploration\"></a>"},{"metadata":{},"cell_type":"markdown","source":"## 5.1 Import <a id=\"import\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data():\n    df = pd.read_csv(MAIN_DIR+DATA)\n    # We don't take all the data in order to reduce training time\n    df = df[-25000:].reset_index(drop=True)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_raw = load_data()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.2 General analysis <a id=\"general_analysis\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Shape of dataset : {data_raw.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_raw.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_raw.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_raw = data_raw.dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For simplicity I drop the **Nan** values. I recommend this post for further information [[1]](https://machinelearningmastery.com/handle-missing-data-python/)."},{"metadata":{},"cell_type":"markdown","source":"## 5.3 Visualization <a id=\"visualization\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_curves(df, var_y, var_x='MJD'):\n    fig, ax = plt.subplots(figsize=(16,5))\n    sns.lineplot(x=var_x,y=var_y, data=df, ax=ax)\n\n    ax.set_title(f\"'{var_y}' value evolution in function of time\")\n    ax.set_xlabel(f'{var_x}')\n    ax.set_ylabel(f\"'{var_y}' value\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_curves(data_raw, 'Weighted_Price', 'Timestamp')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will not continue the analysis because the goal of this notebook is not to try to predict Bitcoin evolution but to show an implementation of Seq2Seq GRU/LSTM on timeseries."},{"metadata":{},"cell_type":"markdown","source":"# 6. Feature engineering <a id=\"feature_engineering\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_engineering(data):\n    drop_feat = ['Timestamp', 'Open', 'High', 'Low', 'Close', 'Volume_(BTC)',\n       'Volume_(Currency)']\n    \n    df = data.copy()\n    \n    # Drop useless feature\n    df = df.drop(drop_feat, axis=1)\n    df.columns = ['target']\n    \n    # Drop Nan for simplicity\n    df = df.dropna()\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_raw = load_data()\ndata = feature_engineering(data_raw)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"New dataset shape : {data.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Data preparation <a id=\"data_preparation\"></a>"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Suppose you have a timeseries like this :\n```python\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n```\n\nYou want to predict 5 days with the last two days. So, you will get a training dataset in this shape :\n```python\nX = [[0, 1]\n     [1, 2]\n     [2, 3]\n     [3, 4]\n     [4, 5]]\ny = [[2, 3, 4, 5, 6]\n     [3, 4, 5, 6, 7]\n     [4, 5, 6, 7, 8]\n     [5, 6, 7, 8, 9]\n     [6, 7, 8, 9, 10]]\n```\n\nIf you decide to split your data in training and validation set you will get (for example) :\n```python\nX_train = [[0, 1]\n           [1, 2]\n           [2, 3]]\ny_train = [[2, 3, 4, 5, 6]\n           [3, 4, 5, 6, 7]\n           [4, 5, 6, 7, 8]]\n\nX_val = [[3, 4]\n         [4, 5]]\ny_val = [[5, 6, 7, 8, 9]\n         [6, 7, 8, 9, 10]]\n```\n\nIn pratice you need to add a dimension at the the end for feature. So the input shape will be **(number of batch, timestep input, number of features).** Note this is not the only way to split your dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get index limit of train/val/test index in the dataset\ndef get_limit_split(data, val_size, test_size, output_size):\n    # Convert percentage into value\n    val_size = int((val_size*0.01)*data.shape[0])\n    test_size = int((test_size*0.01)*data.shape[0])\n\n    limit_train = data.shape[0] - val_size - test_size - output_size + 1\n    limit_val = limit_train + val_size\n    limit_test = limit_val + test_size\n    \n    return limit_train, limit_val, limit_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It would be better to use Keras or Sklearn normalize implementation...\ndef normalize(data, val_size, test_size, output_size):\n    def apply(X, mean, std):\n        X = (X - mean) / std\n        return X\n    \n    df = data.copy()\n    \n    val_size = int((val_size*0.01)*df.shape[0])\n    test_size = int((test_size*0.01)*df.shape[0])\n    \n    limit_train = df.shape[0] - val_size - test_size - output_size + 1\n    limit_val = limit_train + val_size\n    limit_test = limit_val + test_size\n    \n    mean = df.iloc[0:limit_train]['target'].mean()\n    std = df.iloc[0:limit_train]['target'].std()\n    \n    df.iloc[0:limit_train]['target'] = apply(df.iloc[0:limit_train]['target'].values, mean, std)\n    df.iloc[limit_train:limit_val]['target'] = apply(df.iloc[limit_train:limit_val]['target'].values, mean, std)\n    df.iloc[limit_val:limit_test]['target'] = apply(df.iloc[limit_val:limit_test]['target'].values, mean, std)\n    \n    return df, mean, std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It would be better to use Keras or Sklearn normalize implementation...\ndef denormalize(X, mean, std):\n    X = (X * std) + mean\n    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_data_split(data, val_size, test_size, output_size):\n    df = data.copy()\n    \n    limit_train, limit_val, limit_test = get_limit_split(df, val_size, test_size, output_size)\n    \n    df.at[0:limit_train, 'dataset'] = 'train'\n    df.at[limit_train:limit_val, 'dataset'] = 'val'\n    df.at[limit_val:limit_test, 'dataset'] = 'test'\n    \n    fig, ax = plt.subplots(figsize=(16,5))\n    sns.lineplot(x=df.index,y='target', data=df, ax=ax, hue='dataset')\n\n    ax.set_title(f\"Bitcoin value evolution in function of time\")\n    ax.set_xlabel(f'Index in dataset')\n    ax.set_ylabel(f\"Bicoin value\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_val_test_split(X, y, val_size=10, test_size=10, input_size=1, output_size=0):\n    # Convert percentage into value\n    val_size = int((val_size*0.01)*X.shape[0])\n    test_size = int((test_size*0.01)*X.shape[0])\n    \n    limit_train = X.shape[0] - val_size - test_size - output_size + 1\n    limit_val = limit_train + val_size\n    limit_test = limit_val + test_size\n    \n    # TRAINING SET\n    X_train = []\n    y_train = []\n    for i in range(input_size,limit_train):\n        X_train.append(X[i-input_size:i,:])\n        y_train.append(y[i:i+output_size,:])\n    X_train, y_train = np.array(X_train), np.array(y_train)\n    \n    # VALIDATION SET\n    X_val = []\n    y_val = []\n    for i in range(limit_train,limit_val):\n        X_val.append(X[i-input_size:i,:])\n        y_val.append(y[i:i+output_size,:])\n    X_val, y_val = np.array(X_val), np.array(y_val)\n    \n    # TEST SET\n    X_test = []\n    y_test = []\n    for i in range(limit_val,limit_test):\n        X_test.append(X[i-input_size:i,:])\n        y_test.append(y[i:i+output_size,:])\n    X_test, y_test = np.array(X_test), np.array(y_test)\n    \n    return X_train, y_train, X_val, y_val, X_test, y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize data\ndata_norm, mean, std = normalize(data, val_size=VAL_SIZE, test_size=TEST_SIZE, output_size=OUTPUT_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reshape data and get different set (train, validation and test set)\nX_norm = data_norm.values\nX_train, y_train, X_val, y_val, _, _= train_val_test_split(X_norm, X_norm, \n                                                           val_size=VAL_SIZE, \n                                                           test_size=TEST_SIZE, \n                                                           input_size=INPUT_SIZE, \n                                                           output_size=OUTPUT_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"X_train shape : {X_train.shape}\")\nprint(f\"y_train shape : {y_train.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this case we will use the last the time t-60 at t to predict the time t+1 at t+30. In other words, we will use the last 60 days to predict the next 30 days."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data_split(data, val_size=VAL_SIZE, test_size=TEST_SIZE, output_size=OUTPUT_SIZE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8. Modelisation <a id=\"modelisation\"></a>"},{"metadata":{},"cell_type":"markdown","source":"For more information about the architecture of a \"BiRNN\" or \"Bidirectionnal RNN\" (stacked or not), see more on [[2]](https://towardsdatascience.com/time-series-forecasting-with-deep-stacked-unidirectional-and-bidirectional-lstms-de7c099bd918)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(layers, n_in_features=1, n_out_features=1, gru=False, bidirectional=False):\n    \n    keras.backend.clear_session()\n    \n    n_layers = len(layers)\n    \n    ######################\n    # MODEL\n    ######################\n    \n    ## Encoder\n    encoder_inputs = Input(shape=(None, n_in_features))\n    \n    if(gru):\n        rnn_cells = [GRUCell(hidden_dim) for hidden_dim in layers]\n    else:\n        rnn_cells = [LSTMCell(hidden_dim) for hidden_dim in layers]\n        \n    if bidirectional:\n        encoder = Bidirectional(RNN(rnn_cells, return_state=True), merge_mode=None)\n        \n        encoder_outputs_and_states = encoder(encoder_inputs)\n        encoder_states = []\n        \n        if(gru):\n            bi_encoder_states = encoder_outputs_and_states[2:]\n            sep_states = int(len(bi_encoder_states)/2)\n        \n            for i in range(0, sep_states):\n                temp = concatenate([bi_encoder_states[i],bi_encoder_states[sep_states + i]], axis=-1)\n                encoder_states.append(temp)\n        else:\n            bi_encoder_states = encoder_outputs_and_states[2:]\n            sep_states = int(len(bi_encoder_states)/2)\n            \n            for i in range(sep_states):\n                temp = concatenate([bi_encoder_states[i],bi_encoder_states[2*n_layers + i]], axis=-1)\n                encoder_states.append(temp)\n        \n    else:  \n        encoder = RNN(rnn_cells, return_state=True)\n        encoder_outputs_and_states = encoder(encoder_inputs)\n        encoder_states = encoder_outputs_and_states[1:]\n    \n    ## Decoder\n    decoder_inputs = Input(shape=(None, n_out_features))\n    \n    if(gru):\n        if bidirectional:\n            decoder_cells = [GRUCell(hidden_dim*2) for hidden_dim in layers]\n        else:\n            decoder_cells = [GRUCell(hidden_dim) for hidden_dim in layers]\n    else:\n        if bidirectional:\n            decoder_cells = [LSTMCell(hidden_dim*2) for hidden_dim in layers]\n        else:\n            decoder_cells = [LSTMCell(hidden_dim) for hidden_dim in layers]\n        \n    decoder = RNN(decoder_cells, return_sequences=True, return_state=True)\n\n    decoder_outputs_and_states = decoder(decoder_inputs,\n                                         initial_state=encoder_states)\n    decoder_outputs = decoder_outputs_and_states[0]\n\n    decoder_dense = Dense(n_out_features, activation='linear') \n    decoder_outputs = decoder_dense(decoder_outputs)\n    \n    model = Model([encoder_inputs,decoder_inputs], decoder_outputs)\n    \n    ######################\n    # INFERENCE ENCODER\n    ######################\n    \n    encoder_model = Model(encoder_inputs, encoder_states)\n    \n    ######################\n    # INFERENCE DECODER\n    ######################\n    \n    if(gru):\n        if bidirectional:\n            decoder_states_inputs = [Input(shape=(None, hidden_dim*2)) for hidden_dim in layers]\n            decoder_outputs_and_states = decoder(decoder_inputs, initial_state=decoder_states_inputs)\n        else:\n            decoder_states_inputs = [Input(shape=(None, hidden_dim)) for hidden_dim in layers]\n            decoder_outputs_and_states = decoder(decoder_inputs, initial_state=decoder_states_inputs)\n    else:\n        layers_repeat = np.repeat(np.array(layers), 2)\n        if bidirectional:\n            decoder_states_inputs = [Input(shape=(None, hidden_dim*2)) for hidden_dim in layers_repeat]\n            decoder_outputs_and_states = decoder(decoder_inputs, initial_state=decoder_states_inputs)\n        else:\n            decoder_states_inputs = [Input(shape=(None, hidden_dim)) for hidden_dim in layers_repeat]\n            decoder_outputs_and_states = decoder(decoder_inputs, initial_state=decoder_states_inputs)\n        \n    decoder_states = decoder_outputs_and_states[1:]\n    decoder_outputs = decoder_outputs_and_states[0]\n    \n    decoder_outputs = decoder_dense(decoder_outputs)\n    \n    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n    \n    model.summary()\n    \n    return model, encoder_model, decoder_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8.1 Learning <a id=\"learning\"></a>"},{"metadata":{},"cell_type":"markdown","source":"A popular common way to train a Seq2Seq network is to use teacher forcing. This is the method we will use.\n\n<p style=\"text-align:center;\">\n    <img src=\"https://drive.google.com/uc?id=1aYV3IzNjOhDbhVRNDn71zRVonpkaOPZb\" style=\"height:75%; width:75%\"/>\n</p>\n<p style=\"text-align:center;font-style:italic\">Figure 1 : Learning with \"teacher forcing\" on RNN Seq2Seq with one layer</p>\n\nSo in order to reuse the last example your training set will looks like :\n\n```python\nX_train = [[0, 1]\n           [1, 2]\n           [2, 3]]\ny_train = [[2, 3, 4, 5, 6]\n           [3, 4, 5, 6, 7]\n           [4, 5, 6, 7, 8]]\nX_train_bis = [[_, 1]\n               [_, 2]\n               [_, 3]]\n```\n\nGenerally we replace the _ with a 0 or 1 for training. In this case we will use a zero."},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(infenc, infdec, source, n_steps):\n    output = np.empty((0, n_steps, 1), np.float64)\n    \n    for row in tqdm(range(source.shape[0])):\n        \n        states = infenc.predict(source[row:row+1])\n        states = [np.reshape(state, (1, 1, state.shape[-1])) for state in states]\n        \n        output_row = np.empty((1, 0, 1), np.float64)\n        target_seq = np.zeros((1, 1, 1))\n        input_states = [target_seq] + states\n    \n        for t in range(n_steps):\n            output_states = infdec.predict(input_states)\n            output_row = np.concatenate((output_row, output_states[0]), axis=1)\n        \n            # update state\n            states = output_states[1:]\n            \n            # update target sequence\n            target_seq = output_states[0]\n            input_states = output_states\n        \n        output = np.concatenate((output, output_row), axis=0)\n            \n    return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(X_train, y_train, X_val, y_val):\n    bidirectional = True\n    layers = [512, 512, 512]\n    epochs = 20\n\n    cbs = [ReduceLROnPlateau(monitor='loss', factor=0.5, patience=1, min_lr=1e-6, verbose=0),\n           EarlyStopping(monitor='val_loss', min_delta=1e-7, patience=10, verbose=1, mode='min', restore_best_weights=True)]#,\n    \n    model, encoder, decoder = build_model(layers, X_train.shape[2], y_train.shape[2], gru=True, bidirectional=True)\n    model.compile(optimizer=keras.optimizers.Adam(1e-3), loss='mean_absolute_error', metrics=['mae'])\n    \n    X_train_bis = np.pad(y_train, ((0, 0), (1, 0), (0, 0)),\n                         mode='constant')[:, :-1]\n    X_val_bis = np.pad(y_val, ((0, 0), (1, 0), (0, 0)),\n                       mode='constant')[:, :-1]\n    \n    history = model.fit([X_train, X_train_bis], y_train,\n                        validation_data=([X_val, X_val_bis],y_val),\n                        epochs=epochs,\n                        batch_size=32,\n                        shuffle=True,\n                        callbacks=cbs)\n    \n    return history, model, encoder, decoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history, model, encoder, decoder = train(X_train, y_train, X_val, y_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <span style=\"color:red\">Warning !</span>"},{"metadata":{},"cell_type":"markdown","source":"In Keras it's tricky to have different input/ouput dimension. A way to circumvent the problem is to create this type of architecture (like above) and do training like this :"},{"metadata":{},"cell_type":"markdown","source":"```python\n# ...some code\nmodel, _, _ = model.fit([X_train, np.zeros((y_train.shape[0], y_train.shape[1], y_train.shape[2]))])\n# ...some code\n```"},{"metadata":{},"cell_type":"markdown","source":"After training you can do prediction like this :"},{"metadata":{},"cell_type":"markdown","source":"```python\n# ...some code\ny_pred_train = model.predict([X_train, np.zeros((y_train.shape[0], y_train.shape[1], y_train.shape[2]))])\ny_pred_val = model.predict([X_val, np.zeros((y_val.shape[0], y_val.shape[1], y_val.shape[2]))])\ny_pred_test = model.predict([X_test, np.zeros((y_test.shape[0], y_test.shape[1], y_test.shape[2]))])\n# ...some code\n```"},{"metadata":{},"cell_type":"markdown","source":"Do training and prediction like this will give you a **RNN \"Many to Many\"** not a **RNN Seq2Seq** because each output is not reinserted into the decoder. You will get the following network (Figure 2) :"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<p style=\"text-align:center;\">\n    <img src=\"https://drive.google.com/uc?id=1rzyINUsUSGCxB4lGf1VN6UxANYC1D-W6\" style=\"height:75%; width:75%\"/>\n</p>\n<p style=\"text-align:center;font-style:italic\">Figure 2 : RNN \"Many to Many\" with on layer in Keras</p>"},{"metadata":{},"cell_type":"markdown","source":"As you can see we have no reinsertion in the decoder part, it's a **RNN Many to Many** with different input/output."},{"metadata":{},"cell_type":"markdown","source":"## 8.2 Learning curves <a id=\"learning_curves\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting learning curve\ndef plot_loss(history):\n    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Plot train/val MAE\n    ax[0].plot(history.history['mean_absolute_error'])\n    ax[0].plot(history.history['val_mean_absolute_error'])\n    ax[0].set_title('Model accuracy')\n    ax[0].set_ylabel('MSE')\n    ax[0].set_xlabel('Epochs')\n    ax[0].legend(['Train', 'Test'], loc='upper left')\n    \n    # Plot train/val loss\n    ax[1].plot(history.history['loss'])\n    ax[1].plot(history.history['val_loss'])\n    ax[1].set_title('Model Loss')\n    ax[1].set_ylabel('Loss')\n    ax[1].set_xlabel('Epochs')\n    ax[1].legend(['Train', 'Test'], loc='upper left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_loss(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8.3 Learning rate <a id=\"learning_rate\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_lr(history, info):\n    fig, ax = plt.subplots(figsize=(7, 5))\n    \n    # Plot learning rate\n    ax.plot(history.history['lr'])\n    ax.set_title(f\"{info} learning rate evolution in function of epoch\")\n    ax.set_ylabel('Learning rate value')\n    ax.set_xlabel('Epochs')\n    ax.legend(['Train'], loc='upper right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_lr(history, info=\"Model\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 9. Results <a id=\"results\"></a>"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align:center;\">\n    <img src=\"https://drive.google.com/uc?id=1UV9gReji3YcZN_rumYwCi2qnvcxPJzVM\" style=\"height:75%; width:75%\"/>\n</p>\n<p style=\"text-align:center;font-style:italic\">Figure 3 : RNN Seq2Seq with one layer used for prediction</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some functions to help out with\ndef plot_predictions(y_true, y_pred, title, inter_start, inter_end):\n    \n    if(inter_start and inter_end):\n        y_true = y_true.ravel()[inter_start:inter_end]\n        y_pred = y_pred.ravel()[inter_start:inter_end]\n    \n    fig, ax = plt.subplots(1, 1, figsize=(15, 5))\n    \n    y_true = y_true.ravel()\n    y_pred = y_pred.ravel()\n    \n    err = np.mean(np.abs(y_true - y_pred))\n    \n    ax.plot(y_true)\n    ax.plot(y_pred)\n    ax.set_title(f\"{title} : {err} (MAE)\")\n    ax.set_ylabel('Value')\n    ax.set_xlabel('Index')\n    ax.legend(['Real', 'Predict'], loc='upper left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reshape(X):\n    return np.reshape(X,(1, X.shape[0], 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"limit_train, limit_val, limit_test = get_limit_split(data, \n                                                     val_size=VAL_SIZE, \n                                                     test_size=TEST_SIZE, \n                                                     output_size=OUTPUT_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_val_norm = reshape(X_norm[limit_train-OUTPUT_SIZE:limit_train])\ninput_test_norm = reshape(X_norm[limit_val-OUTPUT_SIZE:limit_val])\n\n# Make prediction\ny_pred_val_normalize = predict(infenc=encoder, \n                               infdec=decoder, \n                               source=input_val_norm, \n                               n_steps=limit_val-limit_train)\ny_pred_test_normalize = predict(infenc=encoder, \n                                infdec=decoder, \n                                source=input_test_norm, \n                                n_steps=limit_test-limit_val)\n\n# Denormalize data\ny_pred_val = denormalize(y_pred_val_normalize, mean, std)\ny_pred_test = denormalize(y_pred_test_normalize, mean, std)\nX = denormalize(X_norm, mean, std)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We perform prediction on all the validation set and compare on all the validation set\nplot_predictions(X[limit_train:limit_val], y_pred_val[:,:,0], 'Model', None, None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We perform prediction on all the validation set and compare only on the 30 first examples\nplot_predictions(X[limit_train:limit_train+30], y_pred_val[:,0:30,0], 'Model', None, None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We perform prediction on all the test set and compare on all the test set\nplot_predictions(X[limit_val:limit_test], y_pred_test[:,:,0], 'Model', None, None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We perform prediction on all the test set and compare only on the 30 first examples\nplot_predictions(X[limit_val:limit_val+30], y_pred_test[:,0:30,0], 'Model', None, None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see the model performs **poorly**. You can improve the performance by adding more feature, perform some feature engineering, ...etc this is not the goal of this notebook here."},{"metadata":{},"cell_type":"markdown","source":"# 10. Conclusion <a id=\"conclusion\"></a>"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Now you know how to implement a complete Seq2Seq bidirectionnal model with GRU/LSTM cells and use \"Teacher Forcing\" methods for learning in Keras. For more informations see the reference parts."},{"metadata":{},"cell_type":"markdown","source":"# 11. References <a id=\"references\"></a>"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"[[1]](https://machinelearningmastery.com/handle-missing-data-python/) MachineLearningMastery blog post on how to handle missing data  \n[[2]](https://towardsdatascience.com/time-series-forecasting-with-deep-stacked-unidirectional-and-bidirectional-lstms-de7c099bd918) MachineLearningMastery blog post on Seq2Seq for timeseries  \n[[3]](https://towardsdatascience.com/time-series-forecasting-with-deep-stacked-unidirectional-and-bidirectional-lstms-de7c099bd918) Towards Data science post on Deep stacked unidirectional and bidirectional lstms"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}