{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#First we are importing the libraries we are going smeelingly use. Later on, we will import other libraries if they are necessary\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os\nimport glob\nimport json\nimport re\nimport string\nimport networkx as nx\nimport gc\n\nfrom IPython.utils import io\n#with io.capture_output() as captured:\n\n # !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz\n\nfrom pprint import pprint\nfrom copy import deepcopy\n\nfrom collections import  Counter\nfrom tqdm.notebook import tqdm\n#LANGUAGE DETECTION\n!pip install langdetect\nimport langdetect\nfrom langdetect import detect\nfrom langdetect import DetectorFactory\n#NATURAL LANGUAGE PROCESSING\n\n#STOPWORDS IN ENGLISH SPACY\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\n#TOKENIZER\nfrom spacy.tokens import Span\n\n#GENSIM\n\nimport gensim\n# SPACY MODEL FOR LDA visualization\n\nimport pyLDAvis\nimport pyLDAvis.gensim\n\n#TENSOR FLOWS\nimport tensorflow_hub as hub\nimport tensorflow as tf\n\n#MATCHER\nfrom spacy.matcher import Matcher \nSKLEARN\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n#from sklearn.cluster import DBSCAN\n#os.listdir('../input/CORD-19-research-challenge/')\n#with open('../input/CORD-19-research-challenge/metadata.readme', 'r') as f:\n#   data = f.read()\n#    print(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir( '/kaggle/input/all-cleaned')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"biorxiv='/kaggle/input/all-cleaned/biroxiv_clean''","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Filter papers containing all words in list\ndef filter_papers_word_list(word_list):\n    papers_id_list = []\n    for idx, paper in biorxiv.iterrows():\n        if all(x in paper.text for x in word_list):\n            papers_id_list.append(paper.paper_id)\n\n    return papers_id_list","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"pd.set_option(\"display.max_colwidth\", 100000) # Extend the display width to prevent split functions to not cover full text\n\nbiorxiv_environment = filter_papers_word_list([\"coronavirus\"])\nprint(\"Papers containing coronavirus: \", len(biorxiv_environment))\n\nbiorxiv_environment = filter_papers_word_list([\"environment\"])\nprint(\"Papers containing environment: \", len(biorxiv_environment))\n\nbiorxiv_environmental = filter_papers_word_list([\"environmental\"])\nprint(\"Papers containing environmental: \", len(biorxiv_environmental))\n\nprint(\"Intersection of environment and environmental: \", len(set(biorxiv_environment)-(set(biorxiv_environment)-set(biorxiv_environmental))))\n\nbiorxiv_environment_transmission = filter_papers_word_list([\"coronavirus\", \"environment\", \"transmission\"])\nprint(\"Number of papers containing coronavirus, environment and transmission: \", len(biorxiv_environment_transmission))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alternative filter: word lemmatization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"    tokens = [token.lemma_ for token in text]\n    return ' '.join([token for token in tokens])\n\n# Filter papers containing all words in list\ndef filter_papers_word_list_lemma(word_list):\n    papers_id_list = []\n    word_list_lemma = lemmatizer(nlp(str(' '.join([token for token in word_list]))))\n    for idx, paper in biorxiv.iterrows():\n        if all(w in lemmatizer(nlp(paper.text)) for w in word_list_lemma):\n            papers_id_list.append(paper.paper_id)\n\n    return papers_id_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extract the abstract section\n\nMost scientific papers contain a Conclusion ABSTRACT, which consists on a summary of the main observations and results from the study. In order to reduce the amount of data to analyze, it may prove useful to focus on the ABSTRACT instead of performing a full search in the paper.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_abstract(df, papers_id_list):\n    data = df.loc[df['paper_id'].isin(papers_id_list)]\n    conclusion = []\n    for idx, paper in data.iterrows():\n        paper_text = paper.text\n        if \"\\nAbstract\\n\" in paper.text:\n            conclusion.append(paper_text.split('\\nAbstract\\n')[1])\n        else:\n            conclusion.append(\"No Abstract section\")\n    data['abstract'] = abstract\n        \n    return data\n\npd.reset_option('^display.', silent=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now extract the Conclusion section from all papers containing the words \"coronavirus\", \"environment\" and \"transmission\":","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"environ_trans_conclusion = extract_conclusion(biorxiv, biorxiv_environment_transmission)\nenviron_trans_conclusion.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"study most common words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.width', 100000)\n\ndef word_bar_graph_function(df,column,title):\n\n# adapted from https://www.kaggle.com/benhamner/most-common-forum-topic-words\ntopic_words = [ z.lower() for y in\n                   [ x.split() for x in df[column] if isinstance(x, str)]\n                   for z in y]\nword_count_dict = dict(Counter(topic_words))\npopular_words = sorted(word_count_dict, key = word_count_dict.get, reverse = True)\npopular_words_nonstop = [w for w in popular_words if w not in stopwords.words(\"english\")]\nplt.barh(range(50), [word_count_dict[w] for w in reversed(popular_words_nonstop[0:50])])\n  plt.yticks([x + 0.5 for x in range(50)], reversed(popular_words_nonstop[0:50]))\nplt.title(title)\nplt.show()\nplt.figure(figsize=(10,10)) word_bar_graph_function(environ_trans_conclusion, \"conclusion\", \"Most common words in papers with coronavirus, environment & transmission\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that some of these words are merely situational (i.e. conclusion, section, medrxiv), but others may be particularly common in the subset of papers we have filtered. For example, words like \"epicenter\", \"humidity\", \"city\", \"distance\" and \"lockdown\" seem particularly related to the transmission of the virus and the environmental effects, and they probably won't be that frequent in other articles.\n\nLet's compare this case with papers containing the word \"susceptibility\":","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"biorxiv_susceptibility = filter_papers_word_list([\"coronavirus\", \"susceptibility\"])\nsusceptibility_conclusion = extract_conclusion(biorxiv, biorxiv_susceptibility)\nplt.figure(figsize=(10,10))\nword_bar_graph_function(susceptibility_conclusion, \"conclusion\", \"Most common words in papers with coronavirus and susceptibility\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, in this case there is no trace of the words \"epicenter\", \"humidity\", \"city\", \"distance\" or \"lockdown\". Instead, we now see large frequencies for words like \"reporting\", \"rate\", \"scenarios\", \"diagnostic\" and \"protocol\", which are more related to the contagion suscpetibility of the population.\n\nwhat commes now?\n\nCompare lists of words. Provide a technique to verify if papers containing two different sets of lists are redundant Most common words in conclusion. Except for stopwords, extract the most common words in the Conclusion section for all papers related to coronavirus Clean common words in conclusion by topic. Compute the most common words by topic, but this time without irrelevant information","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def compare_papers_by_word(list1, list2):\n    biorxiv_1 = filter_papers_word_list(list1)\n    biorxiv_2 = filter_papers_word_list(list2)\n    if len(biorxiv_2) == len(set(biorxiv_1)-(set(biorxiv_1)-set(biorxiv_2))):\n        answer = \"List 1 contains List2\"\n    else:\n        answer = \"Lists are different\"\n    return answer\n\nlist1 = [\"environment\"]\nlist2 = [\"environmental\"]\ncompare_papers_by_word(list1, list2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most common words in conclusion Now we need to know which are the most common words from all papers related to coronavirus, so we can substract them when plotting the most common words from papers of a given topic:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"biorxiv_conclusion = filter_papers_word_list([\"coronavirus\"])\nbiorxiv_coronavirus_conclusion = extract_conclusion(biorxiv, biorxiv_conclusion)\n\n# Filter out papers without Conclusions section\nbiorxiv_conclusion_informed = biorxiv_coronavirus_conclusion[biorxiv_coronavirus_conclusion[\"conclusion\"] != \"No Conclusion section\"]\n# Split in words\nlist_of_words = \"\".join([c for c in biorxiv_conclusion_informed.conclusion]).split()\n# Remove stopwords\nall_text_conclusion_nonstop = [w.lower() for w in list_of_words if w not in stopwords.words(\"english\")]\n# List of 100 most common words\nmost_common_words_conclusion = Counter(all_text_conclusion_nonstop).most_common(100)\nmost_common_words_conclusion","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"manual_filter = [('preprint', 204),\n('.', 137),\n ('license', 115),\n ('medrxiv', 108),\n ('the', 107),\n ('copyright', 78),\n ('holder', 78),\n ('peer-reviewed)', 78),\n ('available', 73),\n ('(which', 72),\n ('doi:', 64),\n ('granted', 63),\n ('display', 62),\n ('it', 62),\n ('author/funder,', 59),\n ('made', 53),\n ('4.0', 52),\n ('international', 52),\n ('data', 51),\n ('perpetuity.is', 35),\n ('cc-by-nc-nd', 28),\n ('study', 26),\n ('in', 21),\n ('we', 20),\n ('figure', 19),\n ('using', 18),\n ('author/funder.', 18),\n ('biorxiv', 18),\n ('number', 17),\n ('used', 17),\n ('sars-cov-2', 17),\n ('results', 16),\n ('covid-19', 16),\n ('perpetuity.the', 15),\n ('all', 15),\n ('1', 14),\n ('without', 13),\n ('show', 13),\n ('contributed', 13),\n ('our', 12),\n ('different', 12),\n ('2019-ncov', 12),\n ('also', 11),\n ('table', 11),\n ('coronavirus', 11),\n ('information', 11),\n ('cc-by-nc', 11),\n ('cc-by', 11),\n ('rights', 10),\n ('reserved.', 10),\n ('no', 10),\n ('reuse', 10),\n ('allowed', 10),\n ('based', 10),\n ('may', 10),\n ('consent', 10),\n ('authors', 10),\n ('https://doi.org/10.1101/2020.02.07.20021139', 10),\n ('2', 9),\n ('for', 9),\n ('however,', 9),\n ('this', 9),\n ('virus', 9),\n ('might', 8),\n ('manuscript', 8),\n ('https://doi.org/10.1101/2020.02.', 8),\n ('manuscript.', 8),\n ('reading', 8)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clean common words in conclusion by topic Now the easy part, use the functions from previous sections to provide alist of common words in the Conclusion section of a given subtopic:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Modify slightly Paul's function to prevent irrelevant words to appear\ndef word_bar_graph_function_mod(df,column,title,exception):\n    # adapted from https://www.kaggle.com/benhamner/most-common-forum-topic-words\n    topic_words = [ z.lower() for y in\n                       [ x.split() for x in df[column] if isinstance(x, str)]\n                       for z in y]\n    word_count_dict = dict(Counter(topic_words))\n    popular_words = sorted(word_count_dict, key = word_count_dict.get, reverse = True)\n    popular_words_nonstop = [w.lower() for w in popular_words if w not in stopwords.words(\"english\")]\n    popular_words_noexcep = [w.lower() for w in popular_words_nonstop if w not in exception]\n    plt.barh(range(50), [word_count_dict[w] for w in reversed(popular_words_noexcep[0:50])])\n    plt.yticks([x + 0.5 for x in range(50)], reversed(popular_words_noexcep[0:50]))\n    plt.title(title)\n    plt.show()\n    \n    \n# Create a general function for cleaner\ndef common_words_conclusion_clean(word_list, exception_list):\n    \n    \"\"\"\n    word_list = list of words that should contain the paper\n    exception_list = list of most common words in desired papers, in format [(word1, counter1), (word2, counter2),...]\n    \"\"\"\n\n    # Papers with the word coronavirus (I've verified that it contains all papers with COVID-19 and other key words)\n    biorxiv_coronavirus = filter_papers_word_list([\"coronavirus\"])\n    print(\"Papers containing coronavirus: \", len(biorxiv_coronavirus))\n\n    # Extract Conclusion from these papers, filter out papers without it\n    biorxiv_coronavirus_conclusion = extract_conclusion(biorxiv, biorxiv_coronavirus)\n    biorxiv_coronavirus_conclusion = biorxiv_coronavirus_conclusion[biorxiv_coronavirus_conclusion[\"conclusion\"] != \"No Conclusion section\"]\n\n    exception_words = [x[0] for x in exception_list]\n\n    plt.figure(figsize=(10,10))\n    title = \"Most common words in Conclusions on papers with \", word_list\n    word_bar_graph_function_mod(biorxiv_coronavirus_conclusion, \"conclusion\", title, exception_words)\n    \n    \ncommon_words_conclusion_clean([\"coronavirus\", \"environment\", \"transmission\"], manual_filter)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"his is a cleaner list of common words for COVID-19 papers related to environment and transmission. This could help people filter the papers they are interested in, so that there's no need to read the whole literature.\n\nNOW COMES THE TEXT SUMMARIZATION\n\nWord scoring. Compute the term frequency - inverse frequency document (TFIDF) score for each word in the document Split. Split the paper's text into sentences, using dot and line break as delimiters Sentence scoring. Use text rank, with the sentence score based on word scores. Filter out sentences with >50 words Summary. The summary consists on the top 10 sentences with higher score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def tfidf(data):\n    tfidf_vectorizer = TfidfVectorizer(analyzer='word', stop_words = 'english', sublinear_tf=True)\n    matrix = tfidf_vectorizer.fit_transform(data)\n    return matrix, tfidf_vectorizer\n\nlist_corpus = list(biorxiv[biorxiv.paper_id == '6d0127f985edbe088bc279865bef25a31f54a066'].text)\n\ntfidf_matrix, tfidf_vectorizer = tfidf(list_corpus)\nword_scores_df = pd.DataFrame(tfidf_matrix.toarray(), columns = tfidf_vectorizer.get_feature_names())   # extract a df with the words' scores\nword_scores = dict(zip(list(word_scores_df.columns), list(word_scores_df.iloc[0])))  # convert to dict\n\nprint(\"Arbitrary slice of 10 words from the dictionary\")\ndict(list(word_scores.items())[100:110])  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"complete dictionary for all words in the paper, with their respective TFIDF scores. To split the paper into sentences, we replace all line breaks (\\n, \\r) by dots (.), and then split the document using the dot as a delimiter. Each sentence's score is the sum of all its word's scores, effectively penalizing short sentences (which I consider a desired criteria).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split into sentences\nsentences_list = [nlp(s) for s in list(biorxiv[biorxiv.paper_id == '6d0127f985edbe088bc279865bef25a31f54a066'].text.str.replace('\\n', '.').replace('\\r', '.'))]\nsentences_list = str(sentences_list[0]).split('.')   # Split sentences by .\nsentences_scores = {}\n\n# Define the sentence scoring function\ndef get_sentence_score(sentence: str, word_scores: dict):\n    words = sentence.split()\n    if len(words) < 50:\n        score = sum([word_scores.get(w.lower(),0) for w in words])\n    else:\n        score=0\n    return score\n\n# Assign scores and join the top10 sentences into the final summary\nfor s in sentences_list:\n    sentences_scores[s] = get_sentence_score(s, word_scores)\n    \ntop10_sentences = nlargest(10, sentences_scores, key=sentences_scores.get)\ntop10_sentences = [s for s in top10_sentences ]\nsummary = ' '.join(top10_sentences)\n\nprint(\"Original paper size: \", len(list_corpus[0].split()))\nprint(\"Summary size: \", len(summary.split()))\nprint(\"Reduction percentage: \", 100-round(len(summary.split())/len(list_corpus[0].split())*100, 2), \"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This method has reduced the original document to a 18.86% of the original, which considering the increasingly large number of papers in the literature, would save many time to anyone interested into reading them partially. The obtained summary is the following:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Term frequency - inverse document frequency function\ndef tfidf(data):\n    tfidf_vectorizer = TfidfVectorizer(analyzer='word', stop_words = 'english', sublinear_tf=True)\n    matrix = tfidf_vectorizer.fit_transform(data)\n    return matrix, tfidf_vectorizer\n\n\n# Define the sentence scoring function\ndef get_sentence_score(sentence: str, word_scores: dict):\n    words = sentence.split()\n    if len(words) < 50:\n        score = sum([word_scores.get(w.lower(),0) for w in words])\n    else:\n        score=0\n    return score\n\n\n# Summary extraction function\ndef extract_summary(df, paper_id):\n\n    list_corpus = list(biorxiv[biorxiv.paper_id == paper_id].text)\n    tfidf_matrix, tfidf_vectorizer = tfidf(list_corpus)\n    word_scores_df = pd.DataFrame(tfidf_matrix.toarray(), columns = tfidf_vectorizer.get_feature_names())   # extract a df with the words' scores\n    word_scores = dict(zip(list(word_scores_df.columns), list(word_scores_df.iloc[0])))  # convert to dict\n\n    # Split into sentences\n  \n    sentences_list = [nlp(s) for s in list(biorxiv[biorxiv.paper_id == paper_id].text.str.replace('\\n', '.').replace('\\r', '.'))]\n    sentences_list = str(sentences_list[0]).split('.')   # Split sentences by .\n    sentences_scores = {}\n\n    # Assign scores and join the top10 sentences into the final summary\n    for s in sentences_list:\n        sentences_scores[s] = get_sentence_score(s, word_scores)\n\n    top10_sentences = nlargest(10, sentences_scores, key=sentences_scores.get)\n    top10_sentences = [s for s in top10_sentences ]\n    summary = ' '.join(top10_sentences)\n        \n    return summary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice that an input DataFrame is required, in order to generate the new summary column. In case you are interested into summarizing only some papers, you can first filter these papers from the dataframe. Results of the the previous case for reports with the words \"coronavirus\", \"environment\" and \"transmision\" are the following:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"environ_trans_summary = environ_trans_conclusion.copy()\nenviron_trans_summary['summary'] = environ_trans_summary['paper_id'].apply(lambda x: extract_summary(environ_trans_summary, x))\nenviron_trans_summary.head(5)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}