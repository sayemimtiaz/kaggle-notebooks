{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Install dependencies"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/timm-pytorch-image-models/pytorch-image-models-master/","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport random\nimport time\nimport warnings\n\nimport albumentations as A\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport timm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as torchdata\n\nfrom pathlib import Path\nfrom typing import List\n\nfrom albumentations.pytorch import ToTensorV2\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Utilities"},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_seed(seed=42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef get_device() -> torch.device:\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndef prepare_model_for_inference(model, path: Path):\n    if not torch.cuda.is_available():\n        ckpt = torch.load(path, map_location=\"cpu\")\n    else:\n        ckpt = torch.load(path)\n    model.load_state_dict(ckpt[\"model_state_dict\"])\n    model.eval()\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data "},{"metadata":{"trusted":true},"cell_type":"code","source":"# For on-site competition, we need to change this part!!!\nDATADIR = Path(\"../input/riadd-public-test/Evaluation_Set\")\nMODELDIR = Path(\"../input/riadd-trained-weights/\")\nall_pngs = list(DATADIR.glob(\"*.png\"))\nall_pngs.sort(key=lambda x: int(x.name.split(\".\")[0]))\nall_png_names = [path.name for path in all_pngs]\nprint(all_png_names[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.DataFrame({\n    \"ID\": [int(n.split(\".\")[0]) for n in all_png_names]\n})\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"def crop_image_from_gray(image: np.ndarray, threshold: int = 7):\n    if image.ndim == 2:\n        mask = image > threshold\n        return image[np.ix_(mask.any(1), mask.any(0))]\n    elif image.ndim == 3:\n        gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    mask = gray_image > threshold\n\n    check_shape = image[:, :, 0][np.ix_(mask.any(1), mask.any(0))].shape[0]\n    if (check_shape == 0):\n        return image\n    else:\n        image1 = image[:, :, 0][np.ix_(mask.any(1), mask.any(0))]\n        image2 = image[:, :, 1][np.ix_(mask.any(1), mask.any(0))]\n        image3 = image[:, :, 2][np.ix_(mask.any(1), mask.any(0))]\n\n        image = np.stack([image1, image2, image3], axis=-1)\n        return image\n\n\ndef center_crop(image: np.ndarray, ar: float):\n    h, w, _ = image.shape\n    new_h = int(ar * w)\n    start = (h - new_h) // 2\n    return image[start:start + new_h, :, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TestDatasetV1(torchdata.Dataset):\n    def __init__(self, df: pd.DataFrame, datadir: Path, transform=None, center_crop=False):\n        self.df = df\n        self.filenames = df[\"ID\"].values\n        self.datadir = datadir\n        self.transform = transform\n        self.center_crop = center_crop\n\n    def __len__(self):\n        return len(self.filenames)\n\n    def __getitem__(self, index: int):\n        filename = self.filenames[index]\n        path = self.datadir / f\"{filename}.png\"\n        image = cv2.cvtColor(cv2.imread(str(path)), cv2.COLOR_BGR2RGB)\n        h, w, _ = image.shape\n        if h == 1424 and w == 2144:\n            camera = \"C1\"\n        elif h == 2848 and w == 4288:\n            camera = \"C2\"\n        else:\n            camera = \"C3\"\n\n        if self.center_crop:\n            image = crop_image_from_gray(image)\n            if camera != \"C2\":\n                image = center_crop(image, ar=0.834)\n\n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented[\"image\"]\n        return {\n            \"ID\": filename,\n            \"image\": image\n        }\n\n\nclass TestDataset(torchdata.Dataset):\n    def __init__(self, df: pd.DataFrame, datadir: Path, transform=None, center_crop=True):\n        self.df = df\n        self.filenames = df[\"ID\"].values\n        self.datadir = datadir\n        self.transform = transform\n        self.center_crop = center_crop\n\n    def __len__(self):\n        return len(self.filenames)\n\n    def __getitem__(self, index: int):\n        filename = self.filenames[index]\n        path = self.datadir / f\"{filename}.png\"\n        image = cv2.cvtColor(cv2.imread(str(path)), cv2.COLOR_BGR2RGB)\n        h, w, _ = image.shape\n        if h == 1424 and w == 2144:\n            camera = \"C1\"\n        elif h == 2848 and w == 4288:\n            camera = \"C2\"\n        else:\n            camera = \"C3\"\n\n        if self.center_crop:\n            image = crop_image_from_gray(image)\n            if camera != \"C2\":\n                image = center_crop(image, ar=0.834)\n\n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented[\"image\"]\n        return {\n            \"ID\": filename,\n            \"image\": image\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_transforms(img_size: int, mode=\"train\"):\n    if mode == \"train\":\n        return A.Compose([\n            A.RandomResizedCrop(\n                height=img_size,\n                width=img_size,\n                scale=(0.9, 1.1),\n                ratio=(0.9, 1.1),\n                p=0.5),\n            A.ShiftScaleRotate(\n                shift_limit=0.1,\n                scale_limit=0.1,\n                rotate_limit=180,\n                border_mode=cv2.BORDER_CONSTANT,\n                value=0,\n                mask_value=0,\n                p=0.5),\n            A.RandomBrightnessContrast(\n                brightness_limit=0.1, contrast_limit=0.1, p=0.5),\n            A.HueSaturationValue(\n                hue_shift_limit=5,\n                sat_shift_limit=5,\n                val_shift_limit=5,\n                p=0.5),\n            A.Resize(img_size, img_size),\n            A.Normalize(\n                mean=[0.485, 0.456, 0.4406],\n                std=[0.229, 0.224, 0.225],\n                always_apply=True),\n            ToTensorV2()\n        ])\n    elif mode == \"valid\":\n        return A.Compose([\n            A.Resize(img_size, img_size),\n            A.Normalize(\n                mean=[0.485, 0.456, 0.4406],\n                std=[0.229, 0.224, 0.225],\n                always_apply=True),\n            ToTensorV2()\n        ])\n    else:\n        return A.Compose([\n            A.Resize(img_size, img_size),\n            A.Normalize(\n                mean=[0.485, 0.456, 0.4406],\n                std=[0.229, 0.224, 0.225],\n                always_apply=True),\n            ToTensorV2()\n        ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def init_layer(layer):\n    nn.init.xavier_uniform_(layer.weight)\n\n    if hasattr(layer, \"bias\"):\n        if layer.bias is not None:\n            layer.bias.data.fill_(0.)\n\n\ndef gem(x: torch.Tensor, p=3, eps=1e-6):\n    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1. / p)\n\n\nclass GeM(nn.Module):\n    def __init__(self, p=3, eps=1e-6):\n        super().__init__()\n        self.p = nn.Parameter(torch.ones(1) * p)\n        self.eps = eps\n\n    def forward(self, x):\n        return gem(x, p=self.p, eps=self.eps)\n\n    def __repr__(self):\n        return self.__class__.__name__ + f\"(p={self.p.data.tolist()[0]:.4f}, eps={self.eps})\"\n\n\nclass TimmModel(nn.Module):\n    def __init__(self, base_model_name=\"tf_efficientnet_b0_ns\", pooling=\"GeM\", pretrained=True, num_classes=24):\n        super().__init__()\n        self.base_model = timm.create_model(base_model_name, pretrained=pretrained)\n        if hasattr(self.base_model, \"fc\"):\n            in_features = self.base_model.fc.in_features\n            self.base_model.fc = nn.Linear(in_features, num_classes)\n        elif hasattr(self.base_model, \"classifier\"):\n            in_features = self.base_model.classifier.in_features\n            self.base_model.classifier = nn.Linear(in_features, num_classes)\n        elif hasattr(self.base_model, \"head\"):\n            in_features = self.base_model.head.fc.in_features\n            self.base_model.head.fc = nn.Linear(in_features, num_classes)\n        else:\n            raise NotImplementedError\n\n        if pooling == \"GeM\":\n            if hasattr(self.base_model, \"head\"):\n                self.base_model.head.global_pool = GeM()\n            else:\n                self.base_model.avg_pool = GeM()\n\n        self.init_layer()\n\n    def init_layer(self):\n        if hasattr(self.base_model, \"fc\"):\n            init_layer(self.base_model.fc)\n        elif hasattr(self.base_model, \"classifier\"):\n            init_layer(self.base_model.classifier)\n        elif hasattr(self.base_model, \"head\"):\n            init_layer(self.base_model.head.fc)\n        else:\n            raise NotImplementedError\n\n    def forward(self, x):\n        return self.base_model(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Configuration"},{"metadata":{"trusted":true},"cell_type":"code","source":"DATASET_TYPE = {\n    \"012\": TestDatasetV1, \"014\": TestDatasetV1, \"015\": TestDatasetV1,\n    \"022\": TestDataset, \"023\": TestDataset, \"026\": TestDataset,\n    \"027\": TestDataset, \"030\": TestDataset, \"038\": TestDataset,\n    \"039\": TestDataset, \"040\": TestDataset\n}\n\n\nIMAGE_SIZE = {\n    \"012\": 320, \"014\": 320, \"015\": 480, \"022\": 480, \"023\": 320,\n    \"026\": 640, \"027\": 320, \"030\": 320, \"038\": 384, \"039\": 456,\n    \"040\": 384\n}\n\n\nMODELS = {\n    \"012\": {\n        \"base_model_name\": \"tf_efficientnet_b0_ns\",\n        \"pooling\": \"GeM\",\n        \"pretrained\": False,\n        \"num_classes\": 29\n    },\n    \"014\": {\n        \"base_model_name\": \"tf_efficientnet_b1_ns\",\n        \"pooling\": \"GeM\",\n        \"pretrained\": False,\n        \"num_classes\": 29\n    },\n    \"015\": {\n        \"base_model_name\": \"tf_efficientnet_b0_ns\",\n        \"pooling\": \"GeM\",\n        \"pretrained\": False,\n        \"num_classes\": 29\n    },\n    \"022\": {\n        \"base_model_name\": \"tf_efficientnet_b0_ns\",\n        \"pooling\": \"GeM\",\n        \"pretrained\": False,\n        \"num_classes\": 29\n    },\n    \"023\": {\n        \"base_model_name\": \"tf_efficientnet_b0_ns\",\n        \"pooling\": \"GeM\",\n        \"pretrained\": False,\n        \"num_classes\": 29\n    },\n    \"026\": {\n        \"base_model_name\": \"tf_efficientnet_b0_ns\",\n        \"pooling\": \"GeM\",\n        \"pretrained\": False,\n        \"num_classes\": 29\n    },\n    \"027\": {\n        \"base_model_name\": \"dm_nfnet_f0\",\n        \"pooling\": \"\",\n        \"pretrained\": False,\n        \"num_classes\": 29\n    },\n    \"030\": {\n        \"base_model_name\": \"tf_efficientnet_b3_ns\",\n        \"pooling\": \"GeM\",\n        \"pretrained\": False,\n        \"num_classes\": 29\n    },\n    \"038\": {\n        \"base_model_name\": \"tf_efficientnet_b3_ns\",\n        \"pooling\": \"GeM\",\n        \"pretrained\": False,\n        \"num_classes\": 29\n    },\n    \"039\": {\n        \"base_model_name\": \"tf_efficientnet_b5_ns\",\n        \"pooling\": \"GeM\",\n        \"pretrained\": False,\n        \"num_classes\": 29\n    },\n    \"040\": {\n        \"base_model_name\": \"tf_efficientnet_b4_ns\",\n        \"pooling\": \"GeM\",\n        \"pretrained\": False,\n        \"num_classes\": 29\n    }\n}\n\n\nloader_params = {\n    \"batch_size\": 32,\n    \"num_workers\": 4,\n    \"shuffle\": False\n}\n\n\ntarget_columns = [\n    \"Disease_Risk\", \"DR\", \"ARMD\", \"MH\", \"DN\",\n    \"MYA\", \"BRVO\", \"TSLN\", \"ERM\", \"LS\", \"MS\",\n    \"CSR\", \"ODC\", \"CRVO\", \"TV\", \"AH\", \"ODP\",\n    \"ODE\", \"ST\", \"AION\", \"PT\", \"RT\", \"RS\", \"CRS\",\n    \"EDN\", \"RPEC\", \"MHL\", \"RP\", \"OTHER\"\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_models = [\n    \"012\", \"014\", \"015\", \"022\", \"023\",\n    \"026\", \"027\", \"030\", \"038\", \"039\", \"040\"\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inference loop"},{"metadata":{"trusted":true},"cell_type":"code","source":"t0 = time.time()\n\nset_seed(1213)\ndevice = get_device()\n\nmodel_predictions = {}\nfor model_name in all_models:\n    img_size = IMAGE_SIZE[model_name]\n    dataset_cls = DATASET_TYPE[model_name]\n    test_dataset = dataset_cls(test_df, datadir=DATADIR, transform=get_transforms(img_size, mode=\"test\"))\n    test_loader = torchdata.DataLoader(test_dataset, **loader_params)\n    \n    model0 = TimmModel(**MODELS[model_name])\n    model1 = TimmModel(**MODELS[model_name])\n    model2 = TimmModel(**MODELS[model_name])\n    model3 = TimmModel(**MODELS[model_name])\n    model4 = TimmModel(**MODELS[model_name])\n    \n    model0 = prepare_model_for_inference(model0, MODELDIR / f\"best0_{model_name}.pth\").to(device)\n    model1 = prepare_model_for_inference(model1, MODELDIR / f\"best1_{model_name}.pth\").to(device)\n    model2 = prepare_model_for_inference(model2, MODELDIR / f\"best2_{model_name}.pth\").to(device)\n    model3 = prepare_model_for_inference(model3, MODELDIR / f\"best3_{model_name}.pth\").to(device)\n    model4 = prepare_model_for_inference(model4, MODELDIR / f\"best4_{model_name}.pth\").to(device)\n    \n    predictions = []\n    ids = []\n    for batch in tqdm(test_loader, desc=f\"Model: {model_name}\"):\n        input_ = batch[\"image\"].to(device)\n        id_ = batch[\"ID\"]\n        ids.extend(id_.cpu().numpy().tolist())\n        with torch.no_grad():\n            out0 = torch.sigmoid(model0(input_).detach())\n            out1 = torch.sigmoid(model1(input_).detach())\n            out2 = torch.sigmoid(model2(input_).detach())\n            out3 = torch.sigmoid(model3(input_).detach())\n            out4 = torch.sigmoid(model4(input_).detach())\n            out = (out0 / 5 + out1 / 5 + out2 / 5 + out3 / 5 + out4 / 5).cpu().numpy()\n            predictions.append(out)\n            \n        pred_array = np.concatenate(predictions, axis=0)\n        pred_df = pd.DataFrame({\n            \"ID\": ids\n        })\n        pred_df = pd.concat([pred_df, pd.DataFrame(pred_array, columns=target_columns)], axis=1)\n\n    model_predictions[model_name] = pred_df\n    \nelapsed = time.time() - t0\nelapsed_min = int(elapsed // 60)\nelapsed_sec = elapsed % 60\nprint(f\"Elapsed time: {elapsed_min}min {elapsed_sec:.4f}seconds.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_predictions[\"012\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Blending"},{"metadata":{"trusted":true},"cell_type":"code","source":"weights = {\n    \"Disease_Risk\": [\n        0.056106002705367315,\n        0.0705532053116401,\n        0.04997655821435919,\n        0.18143715276113276,\n        0.03404688654322489,\n        0.1525338006486202,\n        0.10291028003467365,\n        0.03781232312031158,\n        0.047585711586795906,\n        0.19312290400942153,\n        0.0739151750644528\n    ],\n    \"DR\": [\n        0.07198363177835458,\n        0.00296027390362025,\n        0.012653951328298996,\n        0.032170520284754256,\n        0.04279789377360028,\n        0.09263422763553994,\n        0.16614894387991772,\n        0.18296522800764742,\n        0.03542622767823671,\n        0.2598184636697298,\n        0.10044063806030003\n    ],\n    \"ARMD\": [\n        0.057843489172587775,\n        0.20248546755195435,\n        0.02373066185550471,\n        0.03295197047227677,\n        0.03320676673880146,\n        0.0401967049626236,\n        0.003720803304882151,\n        0.19942890821432857,\n        0.2417245089032871,\n        0.14174976312725734,\n        0.022960955696496036\n    ],\n    \"MH\": [\n        0.18063768877183053,\n        0.12812994889046606,\n        0.025071869951291095,\n        0.005656853197003577,\n        0.03304591463137401,\n        0.025032441154225468,\n        0.11804684372971683,\n        0.1443760855567267,\n        0.040293048980509026,\n        0.15860267553985744,\n        0.1411066295969991\n    ],\n    \"DN\": [\n        0.005555578435664791,\n        0.009461843717928782,\n        0.02619002967485514,\n        0.24672396271576713,\n        0.09831273817841059,\n        0.19991777433766447,\n        0.12245972479660357,\n        0.06283758943073323,\n        0.03495926801965508,\n        0.061713026745616115,\n        0.131868463947101\n    ],\n    \"MYA\": [\n        0.012658375475527782,\n        0.14876486352517537,\n        0.03530771434213804,\n        0.19634499647164505,\n        0.05017875645252962,\n        0.016782261343092974,\n        0.22460975976495337,\n        0.011413311089070968,\n        0.18560344913667234,\n        0.02972406682598728,\n        0.08861244557320717\n    ],\n    \"BRVO\": [\n        0.23370240056638322,\n        0.006032935764162051,\n        0.003794645292139292,\n        0.022909738502784147,\n        0.055597755279284206,\n        0.2053972692562277,\n        0.013285495756493383,\n        0.07288308882556965,\n        0.031124035526781182,\n        0.1846973554073865,\n        0.17057527982278875\n    ],\n    \"TSLN\": [\n        0.037977535980047494,\n        0.08343754676179876,\n        0.019029574855066095,\n        0.08669341941694406,\n        0.15803134801908564,\n        0.05401161985098037,\n        0.22472671926196883,\n        0.022968059342951406,\n        0.21156717029289818,\n        0.02826659865645776,\n        0.07329040756180134\n    ],\n    \"ERM\": [\n        0.02987969639610825,\n        0.11601818286443207,\n        0.004368242077352041,\n        0.010978346487480818,\n        0.15686054755221301,\n        0.18812373995743054,\n        0.04906336998625799,\n        0.20077657513546693,\n        0.042369353245629224,\n        0.04271018680379536,\n        0.15885175949383373\n    ],\n    \"LS\": [\n        0.013475172072800584,\n        0.06761712055928822,\n        0.005202017030359687,\n        0.013836877201252509,\n        0.015488355609524464,\n        0.15936495891531166,\n        0.133979239273297,\n        0.02743590060208177,\n        0.21341277637556347,\n        0.18079331478114724,\n        0.16939426757937331\n    ],\n    \"MS\": [\n        0.18603178688696712,\n        0.19682821622652283,\n        0.10488846154101043,\n        0.1416172053648892,\n        0.06736749790196353,\n        0.01684103130532239,\n        0.010080807749732506,\n        0.032777056198835856,\n        0.03797495350611392,\n        0.04308032678273061,\n        0.1625126565359116\n    ],\n    \"CSR\": [\n        0.057350920206716206,\n        0.061296793225116074,\n        0.12545705584565167,\n        0.1372387652767952,\n        0.06068788930201581,\n        0.005560240141343137,\n        0.005004647684992925,\n        0.16087871903243436,\n        0.10312285229510063,\n        0.10777370944001727,\n        0.17562840754981668\n    ],\n    \"ODC\": [\n        0.01195084027988455,\n        0.07675929050051418,\n        0.0030758554601049816,\n        0.0033816356769156663,\n        0.06464306918256114,\n        0.16868674206947862,\n        0.11464176461383777,\n        0.08480850394403597,\n        0.04859804254378671,\n        0.23296727277207402,\n        0.19048698295680636\n    ],\n    \"CRVO\": [\n        0.12102894112999622,\n        0.0031509059245401704,\n        0.13679362364474282,\n        0.09899274595582706,\n        0.009164664100228311,\n        0.022047566613933846,\n        0.15906731196113055,\n        0.1269106862968374,\n        0.12420508313400691,\n        0.031122954325171627,\n        0.16751551691358507\n    ],\n    \"TV\": [\n        0.17062068225925456,\n        0.0692155548789209,\n        0.04804573175048877,\n        0.019901312958279273,\n        0.038990165222716956,\n        0.18476438490637934,\n        0.03408905613670613,\n        0.17766772858592783,\n        0.07476804442359734,\n        0.027603256063401724,\n        0.15433408281432714\n    ],\n    \"AH\": [\n        0.0033388256708909688,\n        0.0541692934828976,\n        0.08612813146312515,\n        0.03136878799982958,\n        0.16733652087812734,\n        0.028659361987300472,\n        0.18154730215053777,\n        0.06875072775404474,\n        0.17131128227419462,\n        0.1857738141109412,\n        0.021615952228110465\n    ],\n    \"ODP\": [\n        0.05305579799351704,\n        0.10626602615312912,\n        0.07622993045776295,\n        0.029767343899165695,\n        0.021463328952529205,\n        0.00708842717739818,\n        0.08526899815623941,\n        0.04301537901825068,\n        0.1945457784972824,\n        0.23710910477716382,\n        0.14618988491756144\n    ],\n    \"ODE\": [\n        0.006645959887180597,\n        0.22230155846173286,\n        0.020712902820439652,\n        0.12460869515530355,\n        0.11614078629843315,\n        0.036919185881771406,\n        0.061225201067100714,\n        0.08466864420934254,\n        0.17641524324191088,\n        0.04509678392179723,\n        0.10526503905498734\n    ],\n    \"ST\": [\n        0.05579449519377708,\n        0.14607751358489285,\n        0.03571624663130894,\n        0.09789365139466331,\n        0.002355030098990704,\n        0.029921377114556013,\n        0.07055169114960552,\n        0.15532684929337387,\n        0.16655048630466712,\n        0.17945188215469346,\n        0.06036077707947124\n    ],\n    \"AION\": [\n        0.1253097445158149,\n        0.010609858617663016,\n        0.02414591511047431,\n        0.02267531836833228,\n        0.18376319058087542,\n        0.1879722453959952,\n        0.030761469108309188,\n        0.170634687147818,\n        0.005135651616286887,\n        0.15506477455432696,\n        0.08392714498410385\n    ],\n    \"PT\": [\n        0.04257503481566441,\n        0.22939887369083786,\n        0.0311583003444886,\n        0.039664902221607026,\n        0.04199424916904942,\n        0.05360881339210934,\n        0.013725477194634123,\n        0.008448911423130704,\n        0.23817423893661696,\n        0.12443357823568726,\n        0.17681762057617426\n    ],\n    \"RT\": [\n        0.0634731983460384,\n        0.16038012253823364,\n        0.008558991261568201,\n        0.021056623924998723,\n        0.13048489441904895,\n        0.09263821493358795,\n        0.006869220305358243,\n        0.06525941509926471,\n        0.21613863283704512,\n        0.10461268665414064,\n        0.13052799968071538\n    ],\n    \"RS\": [\n        0.08804964471352277,\n        0.03859347885588542,\n        0.05109940389344141,\n        0.16556706594047788,\n        0.021829330715815054,\n        0.14642460936672982,\n        0.007882200489736717,\n        0.19488223732391807,\n        0.08861198275562883,\n        0.0963850475655661,\n        0.10067499837927785\n    ],\n    \"CRS\": [\n        0.07926642442851686,\n        0.05880524826639643,\n        0.1297631329688441,\n        0.17953766475290905,\n        0.05394588896044773,\n        0.1486489446572299,\n        0.0028443503450392505,\n        0.17732452101836324,\n        0.13901813095338925,\n        0.018398667696572262,\n        0.012447025952291944\n    ],\n    \"EDN\": [\n        0.11035786480778485,\n        0.013689350044015053,\n        0.19969147311488267,\n        0.16144229238216626,\n        0.0015713391526851511,\n        0.06222407980433003,\n        0.14387780194835653,\n        0.06996229454262844,\n        0.19933498437765143,\n        0.0031547262880191667,\n        0.034693793537480505\n    ],\n    \"RPEC\": [\n        0.10560357351808497,\n        0.010968325783864087,\n        0.07659658526818408,\n        0.15456612319864108,\n        0.05085413397046142,\n        0.0027468535400073774,\n        0.1097888922714797,\n        0.04402210839258307,\n        0.17352724754791207,\n        0.09133712572970676,\n        0.1799890307790754\n    ],\n    \"MHL\": [\n        0.005278870991800131,\n        0.008465487750850442,\n        0.04883094612939742,\n        0.1714560178166613,\n        0.016440023199584004,\n        0.007782228844155979,\n        0.18643351113327544,\n        0.10275259136556744,\n        0.07631094005311971,\n        0.20161979244347814,\n        0.17462959027211003\n    ],\n    \"RP\": [\n        0.046659215896313236,\n        0.13550470865806924,\n        0.13729709238051147,\n        0.12828666773359546,\n        0.11580858932263724,\n        0.055451832302981646,\n        0.046575480858785254,\n        0.11132009991747341,\n        0.03460867738356404,\n        0.09163839330279117,\n        0.09684924224327789\n    ],\n    \"OTHER\": [\n        0.11277343565542515,\n        0.22937878979814655,\n        0.0037428550518870275,\n        0.01902220378335148,\n        0.01716102632074993,\n        0.047214018465549436,\n        0.3387451735681028,\n        0.08186764813762831,\n        0.0967134016431368,\n        0.0024065151482787436,\n        0.05097493242774383\n    ]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = model_predictions[\"012\"].copy()\nfor column in target_columns:\n    submission_df[column] = 0.0\n    for i, model_name in enumerate(all_models):\n        submission_df[column] += weights[column][i] * model_predictions[model_name][column]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.to_csv(\"chizu & arai & okada_results.csv\", index=False)\npd.read_csv(\"chizu & arai & okada_results.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}