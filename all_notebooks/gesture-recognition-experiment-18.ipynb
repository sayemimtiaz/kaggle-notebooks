{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Deep Learning Course Project - Gesture Recognition\n\n### Problem Statement\nImagine you are working as a data scientist at a home electronics company which manufactures state of the art smart televisions. You want to develop a cool feature in the smart-TV that can recognise five different gestures performed by the user which will help users control the TV without using a remote.\n\nThe gestures are continuously monitored by the webcam mounted on the TV. Each gesture corresponds to a specific command:\n \n| Gesture | Corresponding Action |\n| --- | --- | \n| Thumbs Up | Increase the volume. |\n| Thumbs Down | Decrease the volume. |\n| Left Swipe | 'Jump' backwards 10 seconds. |\n| Right Swipe | 'Jump' forward 10 seconds. |\n| Stop | Pause the movie. |\n\nEach video is a sequence of 30 frames (or images).\n\n### Objectives:\n1. **Generator**:  The generator should be able to take a batch of videos as input without any error. Steps like cropping, resizing and normalization should be performed successfully.\n\n2. **Model**: Develop a model that is able to train without any errors which will be judged on the total number of parameters (as the inference(prediction) time should be less) and the accuracy achieved. As suggested by Snehansu, start training on a small amount of data and then proceed further.\n\n3. **Write up**: This should contain the detailed procedure followed in choosing the final model. The write up should start with the reason for choosing the base model, then highlight the reasons and metrics taken into consideration to modify and experiment to arrive at the final model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"## Checking the GPU configuration\n\n!nvidia-smi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from skimage.transform import resize\nfrom imageio import imread","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the necessary libraries\n\nimport numpy as np\nimport os\n\nimport datetime\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport abc\nfrom sys import getsizeof","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We set the random seed so that the results don't vary drastically."},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(30)\nimport random as rn\nrn.seed(30)\nfrom keras import backend as K\nimport tensorflow as tf\n\ntf.random.set_seed(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing some other libraries which will be needed for model building.\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\nfrom keras.layers.convolutional import Conv3D, MaxPooling3D, Conv2D, MaxPooling2D\nfrom keras.layers.recurrent import LSTM\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom keras import optimizers\nfrom keras.layers import Dropout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"project_folder='../input/gesture-recognition/Project_data'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Plotting"},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to plot the training/validation accuracies/losses.\n\ndef plot(history):\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\n    axes[0].plot(history.history['loss'])   \n    axes[0].plot(history.history['val_loss'])\n    axes[0].grid()\n    axes[0].legend(['loss','val_loss'])\n    axes[1].plot(history.history['categorical_accuracy'])   \n    axes[1].plot(history.history['val_categorical_accuracy'])\n    axes[1].grid()\n    axes[1].legend(['categorical_accuracy','val_categorical_accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generator\nThis is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"class ModelBuilder(metaclass= abc.ABCMeta):\n    # initialisng the path where project data resides\n    def initialize_path(self,project_folder):\n        self.train_doc = np.random.permutation(open(project_folder + '/' + 'train.csv').readlines())\n        self.val_doc = np.random.permutation(open(project_folder + '/' + 'val.csv').readlines())\n        self.train_path = project_folder + '/' + 'train'\n        self.val_path =  project_folder + '/' + 'val'\n        self.num_train_sequences = len(self.train_doc)\n        self.num_val_sequences = len(self.val_doc)\n    # initialising the image properties    \n    def initialize_image_properties(self,image_height=100,image_width=100):\n        self.image_height=image_height\n        self.image_width=image_width\n        self.channels=3\n        self.num_classes=5\n        self.total_frames=30\n    # initialising the batch size, frames to sample and the no. of epochs\n    def initialize_hyperparams(self,frames_to_sample=16,batch_size=8,num_epochs=20):\n        self.frames_to_sample=frames_to_sample\n        self.batch_size=batch_size\n        self.num_epochs=num_epochs\n        \n    # MOST IMPORTANT PART HERE - The generator function        \n    def generator(self,source_path, folder_list, augment=False):\n        img_idx = np.round(np.linspace(0,self.total_frames-1,self.frames_to_sample)).astype(int)\n        batch_size=self.batch_size\n        while True:\n            t = np.random.permutation(folder_list)\n            num_batches = len(t)//batch_size\n        \n            for batch in range(num_batches): \n                batch_data, batch_labels= self.one_batch_data(source_path,t,batch,batch_size,img_idx,augment)\n                yield batch_data, batch_labels \n\n            remaining_seq=len(t)%batch_size\n        \n            if (remaining_seq != 0):\n                batch_data, batch_labels= self.one_batch_data(source_path,t,num_batches,batch_size,img_idx,augment,remaining_seq)\n                yield batch_data, batch_labels \n    \n    \n    def one_batch_data(self,source_path,t,batch,batch_size,img_idx,augment,remaining_seq=0):\n    \n        seq_len = remaining_seq if remaining_seq else batch_size\n    \n        batch_data = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels)) \n        batch_labels = np.zeros((seq_len,self.num_classes)) \n    \n        if (augment): batch_data_aug = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels))\n\n        \n        for folder in range(seq_len): \n            imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) \n            for idx,item in enumerate(img_idx):\n                #performing image reading and resizing\n                image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n                image_resized=resize(image,(self.image_height,self.image_width,3))\n            \n                #normalizing the images\n                batch_data[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n                batch_data[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n                batch_data[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n            \n                if (augment):\n                    shifted = cv2.warpAffine(image, \n                                             np.float32([[1, 0, np.random.randint(-30,30)],[0, 1, np.random.randint(-30,30)]]), \n                                            (image.shape[1], image.shape[0]))\n                    \n                    gray = cv2.cvtColor(shifted,cv2.COLOR_BGR2GRAY)\n\n                    x0, y0 = np.argwhere(gray > 0).min(axis=0)\n                    x1, y1 = np.argwhere(gray > 0).max(axis=0) \n                    # cropping the images to have the targeted gestures and remove the noise from the images.\n                    cropped=shifted[x0:x1,y0:y1,:]\n                    \n                    image_resized=resize(cropped,(self.image_height,self.image_width,3))\n                    \n                    #shifted = cv2.warpAffine(image_resized, \n                    #                        np.float32([[1, 0, np.random.randint(-3,3)],[0, 1, np.random.randint(-3,3)]]), \n                    #                        (image_resized.shape[1], image_resized.shape[0]))\n            \n                    batch_data_aug[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n                    batch_data_aug[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n                    batch_data_aug[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n                \n            \n            batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n            \n    \n        if (augment):\n            batch_data=np.concatenate([batch_data,batch_data_aug])\n            batch_labels=np.concatenate([batch_labels,batch_labels])\n\n        \n        return(batch_data,batch_labels)\n    \n    \n    def train_model(self, model, augment_data=False):\n        train_generator = self.generator(self.train_path, self.train_doc,augment=augment_data)\n        val_generator = self.generator(self.val_path, self.val_doc)\n\n        model_name = 'model_init' + '_' + str(datetime.datetime.now()).replace(' ','').replace(':','_') + '/'\n    \n        if not os.path.exists(model_name):\n            os.mkdir(model_name)\n        \n        filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n\n        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n        \n        earlystop = EarlyStopping( monitor=\"val_loss\", min_delta=0,patience=10,verbose=1)\n        callbacks_list = [checkpoint, LR, earlystop]\n\n        if (self.num_train_sequences%self.batch_size) == 0:\n            steps_per_epoch = int(self.num_train_sequences/self.batch_size)\n        else:\n            steps_per_epoch = (self.num_train_sequences//self.batch_size) + 1\n\n        if (self.num_val_sequences%self.batch_size) == 0:\n            validation_steps = int(self.num_val_sequences/self.batch_size)\n        else:\n            validation_steps = (self.num_val_sequences//self.batch_size) + 1\n    \n        history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1, \n                            callbacks=callbacks_list, validation_data=val_generator, \n                            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n        return history\n\n        \n    @abc.abstractmethod\n    def define_model(self):\n        pass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model\nHere you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."},{"metadata":{"trusted":true},"cell_type":"code","source":"class ModelConv3D1(ModelBuilder):\n    \n    def define_model(self):\n\n        model = Sequential()\n        model.add(Conv3D(16, (3, 3, 3), padding='same',\n                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n        model.add(Activation('relu'))\n        model.add(BatchNormalization())\n        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\n        model.add(Conv3D(32, (2, 2, 2), padding='same'))\n        model.add(Activation('relu'))\n        model.add(BatchNormalization())\n        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\n        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n        model.add(Activation('relu'))\n        model.add(BatchNormalization())\n        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\n        model.add(Conv3D(128, (2, 2, 2), padding='same'))\n        model.add(Activation('relu'))\n        model.add(BatchNormalization())\n        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\n        model.add(Flatten())\n        model.add(Dense(128,activation='relu'))\n        model.add(BatchNormalization())\n        model.add(Dropout(0.5))\n\n        model.add(Dense(64,activation='relu'))\n        model.add(BatchNormalization())\n        model.add(Dropout(0.25))\n\n\n        model.add(Dense(self.num_classes,activation='softmax'))\n\n        optimiser = optimizers.Adam()\n        #optimiser = 'sgd'\n        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n        return model","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"conv_3d1=ModelConv3D1()\nconv_3d1.initialize_path(project_folder)\nconv_3d1.initialize_image_properties(image_height=160,image_width=160)\nconv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=8,num_epochs=1)\nconv_3d1_model=conv_3d1.define_model()\nconv_3d1_model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Sample Cropping"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_generator=ModelConv3D1()\ntest_generator.initialize_path(project_folder)\ntest_generator.initialize_image_properties(image_height=160,image_width=160)\ntest_generator.initialize_hyperparams(frames_to_sample=16,batch_size=3,num_epochs=1)\n\ng=test_generator.generator(test_generator.val_path,test_generator.val_doc,augment=True)\nbatch_data, batch_labels=next(g)\nfig, axes = plt.subplots(nrows=1, ncols=2)\naxes[0].imshow(batch_data[0,15,:,:,:])   \naxes[1].imshow(batch_data[3,15,:,:,:])   \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets augment the data with ***slight rotation*** as well and run the same set of models again"},{"metadata":{},"cell_type":"markdown","source":"## Let's apply some data augmentation & check the model performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"class ModelBuilderMoreAugmentation(metaclass= abc.ABCMeta):\n    \n    def initialize_path(self,project_folder):\n        self.train_doc = np.random.permutation(open(project_folder + '/' + 'train.csv').readlines())\n        self.val_doc = np.random.permutation(open(project_folder + '/' + 'val.csv').readlines())\n        self.train_path = project_folder + '/' + 'train'\n        self.val_path =  project_folder + '/' + 'val'\n        self.num_train_sequences = len(self.train_doc)\n        self.num_val_sequences = len(self.val_doc)\n        \n    def initialize_image_properties(self,image_height=100,image_width=100):\n        self.image_height=image_height\n        self.image_width=image_width\n        self.channels=3\n        self.num_classes=5\n        self.total_frames=30\n          \n    def initialize_hyperparams(self,frames_to_sample=16,batch_size=8,num_epochs=20):\n        self.frames_to_sample=frames_to_sample\n        self.batch_size=batch_size\n        self.num_epochs=num_epochs\n        \n        \n    def generator(self,source_path, folder_list, augment=False):\n        img_idx = np.round(np.linspace(0,self.total_frames-1,self.frames_to_sample)).astype(int)\n        batch_size=self.batch_size\n        while True:\n            t = np.random.permutation(folder_list)\n            num_batches = len(t)//batch_size\n        \n            for batch in range(num_batches): \n                batch_data, batch_labels= self.one_batch_data(source_path,t,batch,batch_size,img_idx,augment)\n                yield batch_data, batch_labels \n\n            remaining_seq=len(t)%batch_size\n        \n            if (remaining_seq != 0):\n                batch_data, batch_labels= self.one_batch_data(source_path,t,num_batches,batch_size,img_idx,augment,remaining_seq)\n                yield batch_data, batch_labels \n    \n    \n    def one_batch_data(self,source_path,t,batch,batch_size,img_idx,augment,remaining_seq=0):\n    \n        seq_len = remaining_seq if remaining_seq else batch_size\n    \n        batch_data = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels)) \n        batch_labels = np.zeros((seq_len,self.num_classes)) \n    \n        if (augment): batch_data_aug = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels))\n\n        \n        for folder in range(seq_len): \n            imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) \n            for idx,item in enumerate(img_idx): \n                image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n                image_resized=resize(image,(self.image_height,self.image_width,3))\n            \n\n                batch_data[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n                batch_data[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n                batch_data[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n            \n                if (augment):\n                    shifted = cv2.warpAffine(image, \n                                             np.float32([[1, 0, np.random.randint(-30,30)],[0, 1, np.random.randint(-30,30)]]), \n                                            (image.shape[1], image.shape[0]))\n                    \n                    gray = cv2.cvtColor(shifted,cv2.COLOR_BGR2GRAY)\n\n                    x0, y0 = np.argwhere(gray > 0).min(axis=0)\n                    x1, y1 = np.argwhere(gray > 0).max(axis=0) \n                    \n                    cropped=shifted[x0:x1,y0:y1,:]\n                    \n                    image_resized=resize(cropped,(self.image_height,self.image_width,3))\n                    \n                    M = cv2.getRotationMatrix2D((self.image_width//2,self.image_height//2),\n                                                np.random.randint(-10,10), 1.0)\n                    rotated = cv2.warpAffine(image_resized, M, (self.image_width, self.image_height))\n                    \n                    #shifted = cv2.warpAffine(image_resized, \n                    #                        np.float32([[1, 0, np.random.randint(-3,3)],[0, 1, np.random.randint(-3,3)]]), \n                    #                        (image_resized.shape[1], image_resized.shape[0]))\n            \n                    batch_data_aug[folder,idx,:,:,0] = (rotated[:,:,0])/255\n                    batch_data_aug[folder,idx,:,:,1] = (rotated[:,:,1])/255\n                    batch_data_aug[folder,idx,:,:,2] = (rotated[:,:,2])/255\n                \n            \n            batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n            \n    \n        if (augment):\n            batch_data=np.concatenate([batch_data,batch_data_aug])\n            batch_labels=np.concatenate([batch_labels,batch_labels])\n\n        \n        return(batch_data,batch_labels)\n    \n    \n    def train_model(self, model, augment_data=False):\n        train_generator = self.generator(self.train_path, self.train_doc,augment=augment_data)\n        val_generator = self.generator(self.val_path, self.val_doc)\n\n        model_name = 'model_init' + '_' + str(datetime.datetime.now()).replace(' ','').replace(':','_') + '/'\n    \n        if not os.path.exists(model_name):\n            os.mkdir(model_name)\n        \n        filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n\n        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n        callbacks_list = [checkpoint, LR]\n\n        if (self.num_train_sequences%self.batch_size) == 0:\n            steps_per_epoch = int(self.num_train_sequences/self.batch_size)\n        else:\n            steps_per_epoch = (self.num_train_sequences//self.batch_size) + 1\n\n        if (self.num_val_sequences%self.batch_size) == 0:\n            validation_steps = int(self.num_val_sequences/self.batch_size)\n        else:\n            validation_steps = (self.num_val_sequences//self.batch_size) + 1\n    \n        history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1, \n                            callbacks=callbacks_list, validation_data=val_generator, \n                            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n        return history\n\n        \n    @abc.abstractmethod\n    def define_model(self):\n        pass\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Test(ModelBuilderMoreAugmentation):\n    def define_model(self):\n        pass\n\ntest_generator=Test()\ntest_generator.initialize_path(project_folder)\ntest_generator.initialize_image_properties(image_height=160,image_width=160)\ntest_generator.initialize_hyperparams(frames_to_sample=16,batch_size=3,num_epochs=1)\n\ng=test_generator.generator(test_generator.val_path,test_generator.val_doc,augment=True)\nbatch_data, batch_labels=next(g)\nfig, axes = plt.subplots(nrows=1, ncols=2)\naxes[0].imshow(batch_data[0,15,:,:,:])   \naxes[1].imshow(batch_data[3,15,:,:,:]) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model 17 - Transfer Learning with GRU and training all weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications import mobilenet\n\nmobilenet_transfer = mobilenet.MobileNet(weights='imagenet', include_top=False)\n\nclass RNNCNN_TL2(ModelBuilderMoreAugmentation):\n    \n    def define_model(self,gru_cells=64,dense_neurons=64,dropout=0.25):\n        \n        model = Sequential()\n        model.add(TimeDistributed(mobilenet_transfer,input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n \n        \n        model.add(TimeDistributed(BatchNormalization()))\n        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n        model.add(TimeDistributed(Flatten()))\n\n        model.add(GRU(gru_cells))\n        model.add(Dropout(dropout))\n        \n        model.add(Dense(dense_neurons,activation='relu'))\n        model.add(Dropout(dropout))\n        \n        model.add(Dense(self.num_classes, activation='softmax'))\n        \n        \n        optimiser = optimizers.Adam()\n        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n        return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rnn_cnn_tl2=RNNCNN_TL2()\nrnn_cnn_tl2.initialize_path(project_folder)\nrnn_cnn_tl2.initialize_image_properties(image_height=120,image_width=120)\nrnn_cnn_tl2.initialize_hyperparams(frames_to_sample=16,batch_size=8,num_epochs=30)\nrnn_cnn_tl2_model=rnn_cnn_tl2.define_model(gru_cells=128,dense_neurons=128,dropout=0.25)\nrnn_cnn_tl2_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"print(\"Total Params:\", rnn_cnn_tl2_model.count_params())\nhistory_model17=rnn_cnn_tl2.train_model(rnn_cnn_tl2_model,augment_data=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(history_model17)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  99+% Training accuracy and 94% validation accuracy , seems overfitting"},{"metadata":{},"cell_type":"markdown","source":"# Loading model and Testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nfrom keras.models import load_model\nmodel = load_model('model_init_2021-02-0814_26_33.405710/model-00029-0.01751-0.99548-0.12903-0.96000.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_generator=RNNCNN_TL2()\ntest_generator.initialize_path(project_folder)\ntest_generator.initialize_image_properties(image_height=120,image_width=120)\ntest_generator.initialize_hyperparams(frames_to_sample=16,batch_size=8,num_epochs=20)\n\ng=test_generator.generator(test_generator.val_path,test_generator.val_doc,augment=False)\nbatch_data, batch_labels=next(g)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.argmax(model.predict(batch_data[:,:,:,:,:]),axis=1))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}