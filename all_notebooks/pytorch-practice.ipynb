{"cells":[{"metadata":{},"cell_type":"markdown","source":"Reading classics [Deep Learning Models](https://github.com/rasbt/deeplearning-models)\n## Basic Examples","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np,pandas as pd,pylab as pl\nimport h5py,torch\nfrom torchvision.datasets import MNIST as tmnist\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader as tdl\nfrom torch.utils.data import Dataset as tds\nimport torch.nn.functional as tnnf\nfrom sklearn.datasets import make_classification\ndev=torch.device(\"cuda:0\" if torch.cuda.is_available() \n                 else \"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# artificial data\nN=500; n=int(.2*N)\nX,y=make_classification(n_samples=N,n_features=2,\n                        n_redundant=0,n_informative=2)\nmu,std=np.mean(X,axis=0),np.std(X,axis=0)\nX=(X-mu)/std\nX,y=X.astype('float32'),y.astype('int32')\npl.figure(figsize=(11,3)); pl.grid()\npl.scatter(X[:,0],X[:,1],marker='o',\n           s=10,c=y,cmap='cool');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shuffling & splitting\nshuffle_ids=np.arange(N)\nnp.random.RandomState(23).shuffle(shuffle_ids)\nX,y=X[shuffle_ids],y[shuffle_ids]\nX_test,X_train=X[:n],X[n:]\ny_test,y_train=y[:n],y[n:]\npl.figure(figsize=(11,3)); pl.grid()\npl.scatter(X_test[:,0],X_test[:,1],marker='o',\n           s=10,c=y_test,cmap='cool');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Perceptron():\n    def __init__(self,num_features):\n        self.num_features=num_features\n        self.weights=torch.zeros(num_features,1, \n                                 dtype=torch.float32,device=dev)\n        self.bias=torch.zeros(1,dtype=torch.float32,device=dev)\n    def forward(self,x):\n        values=torch.add(torch.mm(x,self.weights),self.bias)\n        a,b=torch.ones(values.size()[0],1),torch.zeros(values.size()[0],1)\n        predictions=torch.where(values>0.,a,b).float()\n        return predictions        \n    def backward(self,x,y):  \n        predictions=self.forward(x)\n        errors=y-predictions\n        return errors        \n    def train(self,x,y,epochs):\n        for e in range(epochs):            \n            for i in range(y.size()[0]):\n                errors=self.backward(x[i].view(1,self.num_features),\n                                     y[i]).view(-1)\n                self.weights+=(errors*x[i]).view(self.num_features,1)\n                self.bias+=errors                \n    def acc(self,x,y):\n        predictions=self.forward(x).view(-1)\n        accuracy=torch.sum(predictions==y).float()/y.size()[0]\n        return accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Perceptron(num_features=2)\ntX_train=torch.tensor(X_train,dtype=torch.float32,\n                      device=dev)\nty_train=torch.tensor(y_train,dtype=torch.float32,\n                      device=dev)\nmodel.train(tX_train,ty_train,epochs=5)\nprint('Weights: %s'%model.weights)\nprint('Bias: %s'%model.bias)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluating\ntX_test=torch.tensor(X_test,dtype=torch.float32,\n                     device=dev)\nty_test=torch.tensor(y_test,dtype=torch.float32,\n                     device=dev)\nacc_test=model.acc(tX_test,ty_test)\nprint('Test accuracy: %.2f%%'%(acc_test*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"W,b=model.weights,model.bias\nx_min=-2; x_max=2\ny_min=((-(W[0]*x_min)-b[0])/W[1])\ny_max=((-(W[0]*x_max)-b[0])/W[1])\nfig,ax=pl.subplots(1,2,sharex=True,figsize=(11,3))\nax[0].plot([x_min,x_max],[y_min,y_max],c='red')\nax[1].plot([x_min,x_max],[y_min,y_max],c='red')\nax[0].scatter(X_train[:,0],X_train[:,1],\n              c=y_train,s=10,cmap=pl.cm.cool)\nax[1].scatter(X_test[:,0], X_test[:,1],\n              c=y_test,s=10,cmap=pl.cm.cool)\nax[0].grid(); ax[1].grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LogisticRegression():\n    def __init__(self,num_features):\n        self.num_features=num_features\n        self.weights=torch.zeros(num_features,1, \n                                dtype=torch.float32,device=dev)\n        self.bias=torch.zeros(1,dtype=torch.float32,device=dev)\n    def forward(self,x):\n        values=torch.add(torch.mm(x,self.weights),self.bias)\n        probs=self._sigmoid(values)\n        return probs       \n    def backward(self,probs,y):  \n        errors=y-probs.view(-1)\n        return errors            \n    def predict_labels(self,x):\n        probs=self.forward(x)\n        a=torch.ones(probs.size()[0],1)\n        b=torch.zeros(probs.size()[0],1)\n        labels=torch.where(probs>=.5,a,b)\n        return labels                \n    def acc(self,x,y):\n        labels=self.predict_labels(x).float()\n        accuracy=torch.sum(labels.view(-1)==y).float()/y.size()[0]\n        return accuracy    \n    def _sigmoid(self,z):\n        return 1./(1.+torch.exp(-z))    \n    def _logit_cost(self,y,prob):\n        tmp1=torch.mm(-y.view(1,-1),torch.log(prob))\n        tmp2=torch.mm((1-y).view(1,-1),torch.log(1-prob))\n        return tmp1-tmp2\n    def train(self,x,y,epochs,learning_rate=.01):\n        for e in range(epochs):\n            probs=self.forward(x)\n            errors=self.backward(probs,y)\n            neg_grad=torch.mm(x.transpose(0,1),errors.view(-1,1))\n            self.weights+=learning_rate*neg_grad\n            self.bias+=learning_rate*torch.sum(errors)\n            print('Epoch: %03d'%(e+1),end=\"\")\n            print(' | Train accuracy: %.3f'%self.acc(x,y),end=\"\")\n            print(' | Cost: %.3f'%self._logit_cost(y,self.forward(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"model=LogisticRegression(num_features=2)\nmodel.train(tX_train,ty_train,epochs=10,learning_rate=.02)\nprint('Weights: %s'%model.weights)\nprint('Bias: %s'%model.bias)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluating\nacc_test=model.acc(tX_test,ty_test)\nprint('Test accuracy: %.2f%%'%(acc_test*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"W,b=model.weights,model.bias\nx_min=-2; x_max=2\ny_min=((-(W[0]*x_min)-b[0])/W[1])\ny_max=((-(W[0]*x_max)-b[0])/W[1])\nfig,ax=pl.subplots(1,2,sharex=True,figsize=(11,3))\nax[0].plot([x_min,x_max],[y_min,y_max],c='red')\nax[1].plot([x_min,x_max],[y_min,y_max],c='red')\nax[0].scatter(X_train[:,0],X_train[:,1],\n              c=y_train,s=10,cmap=pl.cm.cool)\nax[1].scatter(X_test[:,0], X_test[:,1],\n              c=y_test,s=10,cmap=pl.cm.cool)\nax[0].grid(); ax[1].grid()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Softmax Regression","execution_count":null},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"random_seed=23; batch_size=128\ntrain=tmnist(root='data',train=True,download=True,\n            transform=transforms.ToTensor())\ntest=tmnist(root='data',train=False, \n            transform=transforms.ToTensor())\ntrain_loader=tdl(dataset=train,shuffle=True, \n                 batch_size=batch_size)\ntest_loader=tdl(dataset=test,shuffle=False, \n                batch_size=batch_size)\nfor images,labels in train_loader:  \n    print('Image dimensions: %s'%str(images.shape))\n    print('Label dimensions: %s'%str(labels.shape))\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate=.1; epochs=15\nnum_features=784; num_classes=10\nclass SoftmaxRegression(torch.nn.Module):\n    def __init__(self,num_features,num_classes):\n        super(SoftmaxRegression,self).__init__()\n        self.linear=torch.nn.Linear(num_features,num_classes)        \n        self.linear.weight.detach().zero_()\n        self.linear.bias.detach().zero_()     \n    def forward(self,x):\n        logits=self.linear(x)\n        probs=tnnf.softmax(logits,dim=1)\n        return logits,probs\nmodel=SoftmaxRegression(num_features=num_features,\n                        num_classes=num_classes)\nmodel.to(dev)\noptimizer=torch.optim.SGD(model.parameters(),\n                          lr=learning_rate) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_acc(model,data_loader,num_features):\n    correct_preds,num_examples=0,0    \n    for features,targets in data_loader:\n        features=features.view(-1,num_features).to(dev)\n        targets=targets.to(dev)\n        logits,probs=model(features)\n        _,pred_labels=torch.max(probs,1)\n        num_examples+=targets.size(0)\n        correct_preds+=(pred_labels==targets).sum()        \n    return correct_preds.float()/num_examples*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"for epoch in range(epochs):\n    for batch_ids,(features,targets) in enumerate(train_loader):        \n        features=features.view(-1,num_features).to(dev)\n        targets=targets.to(dev)\n        logits,probs=model(features)\n        cost=tnnf.cross_entropy(logits,targets)\n        optimizer.zero_grad(); cost.backward()\n        optimizer.step()\n        if not batch_ids%200:\n            print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' \n                   %(epoch+1,epochs,batch_ids, \n                     len(train)//batch_size,cost))           \n    with torch.set_grad_enabled(False):\n        print('Epoch: %03d/%03d train accuracy: %.2f%%'%\\\n              (epoch+1,epochs,model_acc(model,train_loader,num_features)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Test accuracy: %.2f%%'%(model_acc(model,test_loader,num_features)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying to Color Images","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fpath='../input/flower-color-images/'\nf=h5py.File(fpath+'FlowerColorImages.h5','r')\nkeys=list(f.keys()); print(keys)\nX=np.array(f[keys[0]],dtype='float32')/255\ny=np.array(f[keys[1]],dtype='int32')\nN=len(y); n=int(.2*N); batch_size=16\nshuffle_ids=np.arange(N)\nnp.random.RandomState(23).shuffle(shuffle_ids)\nX,y=X[shuffle_ids],y[shuffle_ids]\nX_test,X_train=X[:n],X[n:]\ny_test,y_train=y[:n],y[n:]\nX_train.shape,y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TData(tds):\n    def __init__(self,X,y):   \n        self.X=torch.tensor(X,dtype=torch.float32)\n        self.y=torch.tensor(y,dtype=torch.int32)\n    def __getitem__(self,index):\n        train_img,train_lbl=self.X[index],self.y[index]\n        return train_img,train_lbl\n    def __len__(self):\n        return self.y.shape[0]\ntrain=TData(X_train,y_train)\ntest=TData(X_test,y_test)\ntrain_loader=tdl(dataset=train,batch_size=batch_size,shuffle=True)\ntest_loader=tdl(dataset=test,batch_size=batch_size,shuffle=False)\nfor images,labels in train_loader:  \n    print('Image dimensions: %s'%str(images.shape))\n    print('Label dimensions: %s'%str(labels.shape))\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate=.01; epochs=25\nnum_features=49152; num_classes=10\ntorch.manual_seed(random_seed)\nmodel=SoftmaxRegression(num_features=num_features,\n                         num_classes=num_classes)\n\nmodel.to(dev)\noptimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"for epoch in range(epochs):\n    for batch_ids,(features,targets) in enumerate(train_loader):        \n        features=features.view(-1,num_features).to(dev)\n        targets=targets.to(dev)\n        logits,probs=model(features)\n        cost=tnnf.cross_entropy(logits,targets.long())\n        optimizer.zero_grad(); cost.backward()\n        optimizer.step()\n        if not batch_ids%10:\n            print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' \n                   %(epoch+1,epochs,batch_ids, \n                     len(train)//batch_size,cost))           \n    with torch.set_grad_enabled(False):\n        print('Epoch: %03d/%03d train accuracy: %.2f%%'%\\\n              (epoch+1,epochs,model_acc(model,train_loader,num_features)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Test accuracy: %.2f%%'%(model_acc(model,test_loader,num_features)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}