{"cells":[{"metadata":{},"cell_type":"markdown","source":"#WHO and UNICEF warn of a decline in vaccinations during COVID-19\n\nGENEVA/NEW YORK, 15 July 2020 – The World Health Organization and UNICEF warned today of an alarming decline in the number of children receiving life-saving vaccines around the world. This is due to disruptions in the delivery and uptake of immunization services caused by the COVID-19 pandemic. According to new data by WHO and UNICEF, these disruptions threaten to reverse hard-won progress to reach more children and adolescents with a wider range of vaccines, which has already been hampered by a decade of stalling coverage.\n\n“Vaccines are one of the most powerful tools in the history of public health, and more children are now being immunized than ever before,” said Dr Tedros Adhanom Ghebreyesus, WHO Director-General. “But the pandemic has put those gains at risk. The avoidable suffering and death caused by children missing out on routine immunizations could be far greater than COVID-19 itself. But it doesn’t have to be that way. Vaccines can be delivered safely even during the pandemic, and we are calling on countries to ensure these essential life-saving programmes continue.” https://www.who.int/news-room/detail/15-07-2020-who-and-unicef-warn-of-a-decline-in-vaccinations-during-covid-19","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQaRDzuCOBIi9GzsD5Io6sedXCfmJ49VO_8GA&usqp=CAU)the-scientist.com","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Matplotlib and seaborn for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Scipy for statistics\nfrom scipy import stats\n\n# os to manipulate files\nimport os\n\nfrom sklearn.metrics import mean_absolute_error,r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom sklearn.preprocessing import PolynomialFeatures\n\ncolors = [ \"#3498db\", \"#e74c3c\", \"#2ecc71\",\"#9b59b6\", \"#34495e\", \"#95a5a6\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df = pd.read_csv('../input/hackathon/task_2-owid_covid_data-21_June_2020.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Lets first handle numerical features with nan value\nnumerical_nan = [feature for feature in df.columns if df[feature].isna().sum()>1 and df[feature].dtypes!='O']\nnumerical_nan","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"## Replacing the numerical Missing Values\n\nfor feature in numerical_nan:\n    ## We will replace by using median since there are outliers\n    median_value=df[feature].median()\n    \n    df[feature].fillna(median_value,inplace=True)\n    \ndf[numerical_nan].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# categorical features with missing values\ncategorical_nan = [feature for feature in df.columns if df[feature].isna().sum()>1 and df[feature].dtypes=='O']\nprint(categorical_nan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# replacing missing values in categorical features\nfor feature in categorical_nan:\n    df[feature] = df[feature].fillna('None')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, StandardScaler\n#fill in mean for floats\nfor c in df.columns:\n    if df[c].dtype=='float16' or  df[c].dtype=='float32' or  df[c].dtype=='float64':\n        df[c].fillna(df[c].mean())\n\n#fill in -999 for categoricals\ndf = df.fillna(-999)\n# Label Encoding\nfor f in df.columns:\n    if df[f].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(df[f].values))\n        df[f] = lbl.transform(list(df[f].values))\n        \nprint('Labelling done.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Codes from Eduardo M de Morais  https://www.kaggle.com/emdemor/prediction-of-total-price/notebook","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#Defining Functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_to_number(col,convert_type=int,changes = ['-']):\n    \n    # string will be considered as object\n    if col.dtype.name == 'object':\n        col_temp = col.copy()\n        \n        # Change any occurence in changes to ''\n        for change in changes:\n                col_temp = col_temp.str.replace(change,'')\n                \n        # Changes empty string elements for NaN\n        col_temp.loc[(col_temp == '')] = np.nan\n        \n        # Convert to number the not nan elements\n        col_temp[col_temp.notna()] = col_temp[col_temp.notna()].astype(convert_type)\n        \n        # Fill nan elements with the mean\n        col_temp = col_temp.fillna(int(col_temp.mean()))\n        \n        return col_temp\n    else:\n        return col","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_predictions(X_new,y_new,descr = ''):\n    y_col = 'total_cases'\n\n    #cols = ['area', 'hoa','rent amount','property tax','fire insurance']\n    cols = ['total_deaths', 'new_cases','total_cases_per_million','population', 'total_tests', 'total_deaths_per_million', 'aged_65_older']\n    k = 0\n    for x_col in cols:\n        plt.close()\n        plt.figure(figsize=(8, 5))\n        plt.scatter(X_trn[x_col],y_trn,c='lightgray',label = 'Training Dataset',marker='o',zorder=1)\n        plt.scatter(X_new[x_col],y_new, label = 'Predictions on Test Dataset',marker='.', c=colors[k], lw = 0.5,zorder=2,alpha = 0.8)\n        #plt.scatter(X_tst[x_col],y_pr_tst, label = 'Predictions',marker='.', c='tab:blue', lw = 0.5,zorder=2)\n\n\n        plt.xlabel(x_col, size = 18)\n        plt.ylabel(y_col, size = 18); \n        plt.legend(prop={'size': 12});\n        plt.title(descr+y_col+' vs '+x_col, size = 20);\n        plt.show()\n        k += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Pre-Processing","execution_count":null},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Import Dataset\ndf1 = pd.read_csv('../input/hackathon/task_2-owid_covid_data-21_June_2020.csv').drop('new_tests_smoothed',axis=1)\n#df2 = pd.read_csv(os.path.join(dirname,'houses_to_rent_v2.csv'))\n#df2.columns = df1.columns\n\n# elements to remove from the dataset\nremove = ['R','$',',','-','Sem info','Incluso']\n\n# columns of numerical data\ncols = ['total_cases', 'total_deaths', 'new_cases','total_cases_per_million','population', 'total_tests', 'total_deaths_per_million', 'aged_65_older']\n\n# Making the substitutions\nfor col in cols:\n    df1[col]  = convert_to_number(df1[col],changes=remove)\n    \n# converting floor to int \n#df1['total_cases'] = df1['total_cases'].astype('int')\n\n# Getting dummies\n#df1[['continent','date', 'iso_code', 'location', 'tests_units']] = pd.get_dummies(df1[['continent','date', 'iso_code', 'location', 'tests_units']], prefix_sep='_', drop_first=True)\n\n# dealing with outliers\ncols = ['total_cases', 'total_deaths', 'new_cases','total_cases_per_million','population', 'total_tests', 'total_deaths_per_million', 'aged_65_older']\nfor col in cols:\n    df1 = df1[np.abs(stats.zscore(df1[col])) < 6]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Correlation Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"correlations = df.corr()['total_tests'].abs().sort_values(ascending=False).drop('total_tests',axis=0).to_frame()\ncorrelations.plot(kind='bar');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#totalprice correlation matrix\nk = 10 #number of variables for heatmap\nplt.figure(figsize=(16,8))\ncorr = df.corr()\n\nhm = sns.heatmap(corr, \n                 cbar=True, \n                 annot=True, \n                 square=True, fmt='.2f', \n                 annot_kws={'size': 10}, \n                 yticklabels=corr.columns.values,\n                 xticklabels=corr.columns.values,\n                 cmap=\"YlGnBu\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#One Feature Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting features and target\nx_col = 'total_tests'\ny_col = 'total_cases'\n\nX = df[[x_col]]\ny = df[y_col]\n\n# splitting\nX_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=0.333)\n\n# Create regression object\nMLR = linear_model.LinearRegression()\n\npoly = PolynomialFeatures(degree=1)\nX_trn_pl = poly.fit_transform(X_trn)\nX_tst_pl = poly.fit_transform(X_tst)\n\n\n# Train the model using the training sets\nMLR.fit(X_trn_pl,y_trn)\n\ny_pr_tst = MLR.predict(X_tst_pl)\ny_pr_trn = MLR.predict(X_trn_pl)\n\nmae = mean_absolute_error(y_tst,y_pr_tst)\nr2 = r2_score(y_tst,y_pr_tst)\n\nprint('MAE:{:7.2f},{:7.2f}% of mean'.format(mae,100*mae/y_pr_tst.mean()))\nprint('R2:{:6.3f}'.format(r2))\n\n# Plotting\nplt.figure(figsize=(8, 5))\nplt.scatter(X_tst,y_tst,c='lightgray',label = 'observations',alpha = 0.6,marker='.',zorder=1)\nplt.plot(X_tst,y_pr_tst, label = 'Predictions', c='tab:blue', lw = 3,zorder=2)\nplt.xlabel(x_col, size = 18)\nplt.ylabel(y_col, size = 18); \nplt.legend(prop={'size': 16});\nplt.title(y_col+' vs '+x_col, size = 20);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Double Feature Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting features and target\nX = df[['total_tests','population']]\ny = df['total_cases']\n\n# splitting\nX_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=0.333)\n\n# Create regression object\nMLR = linear_model.LinearRegression()\n\npoly = PolynomialFeatures(degree=1)\nX_trn_pl = poly.fit_transform(X_trn)\nX_tst_pl = poly.fit_transform(X_tst)\n\n# Train the model using the training sets\nMLR.fit(X_trn_pl,y_trn)\n\ny_pr_tst = MLR.predict(X_tst_pl)\ny_pr_trn = MLR.predict(X_trn_pl)\n\nmae = mean_absolute_error(y_tst,y_pr_tst)\nr2 = r2_score(y_tst,y_pr_tst)\n\n\nprint('MAE:{:7.2f},{:7.2f}% of mean'.format(mae,100*mae/y_pr_tst.mean()))\nprint('R2:{:6.3f}'.format(r2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting features and target\nX = df[['total_tests','total_cases_per_million']]\ny = df['total_cases']\n\n# splitting\nX_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=0.333)\n\n# Create regression object\nMLR = linear_model.LinearRegression()\n\npoly = PolynomialFeatures(degree=1)\nX_trn_pl = poly.fit_transform(X_trn)\nX_tst_pl = poly.fit_transform(X_tst)\n\n# Train the model using the training sets\nMLR.fit(X_trn_pl,y_trn)\n\ny_pr_tst = MLR.predict(X_tst_pl)\ny_pr_trn = MLR.predict(X_trn_pl)\n\nmae = mean_absolute_error(y_tst,y_pr_tst)\nr2 = r2_score(y_tst,y_pr_tst)\n\n\nprint('MAE:{:7.2f},{:7.2f}% of mean'.format(mae,100*mae/y_pr_tst.mean()))\nprint('R2:{:6.3f}'.format(r2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Selecting features and target\nX = df[['aged_65_older','stringency_index']]\ny = df['total_cases']\n\n# splitting\nX_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=0.333)\n\n# Create regression object\nMLR = linear_model.LinearRegression()\n\npoly = PolynomialFeatures(degree=1)\nX_trn_pl = poly.fit_transform(X_trn)\nX_tst_pl = poly.fit_transform(X_tst)\n\n# Train the model using the training sets\nMLR.fit(X_trn_pl,y_trn)\n\ny_pr_tst = MLR.predict(X_tst_pl)\ny_pr_trn = MLR.predict(X_trn_pl)\n\nmae = mean_absolute_error(y_tst,y_pr_tst)\nr2 = r2_score(y_tst,y_pr_tst)\n\n\nprint('MAE:{:7.2f},{:7.2f}% of mean'.format(mae,100*mae/y_pr_tst.mean()))\nprint('R2:{:6.3f}'.format(r2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting features and target\nX = df[['diabetes_prevalence','new_cases']]\ny = df['total_cases']\n\n# splitting\nX_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=0.333)\n\n# Create regression object\nMLR = linear_model.LinearRegression()\n\npoly = PolynomialFeatures(degree=1)\nX_trn_pl = poly.fit_transform(X_trn)\nX_tst_pl = poly.fit_transform(X_tst)\n\n# Train the model using the training sets\nMLR.fit(X_trn_pl,y_trn)\n\ny_pr_tst = MLR.predict(X_tst_pl)\ny_pr_trn = MLR.predict(X_trn_pl)\n\nmae = mean_absolute_error(y_tst,y_pr_tst)\nr2 = r2_score(y_tst,y_pr_tst)\n\n\nprint('MAE:{:7.2f},{:7.2f}% of mean'.format(mae,100*mae/y_pr_tst.mean()))\nprint('R2:{:6.3f}'.format(r2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Regression with all features ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting features and target\nX = df.drop(['total_cases','new_tests_smoothed_per_thousand'],axis=1).copy()\ny = df['total_cases'].copy()\n\n# splitting\nX_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=0.333)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"MLR = linear_model.LinearRegression()\n\npoly = PolynomialFeatures(degree=2)\nX_trn_pl = poly.fit_transform(X_trn)\nX_tst_pl = poly.fit_transform(X_tst)\nMLR.fit(X_trn_pl,y_trn)\n\ny_pr_tst = MLR.predict(X_tst_pl)\nmae = mean_absolute_error(y_tst,y_pr_tst)\nr2 = r2_score(y_tst,y_pr_tst)\n\nprint('MAE:{:7.2f},{:7.2f}% of mean'.format(mae,100*mae/y_pr_tst.mean()))\nprint('R2:{:6.3f}'.format(r2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting a reduced sample to test\nsize = len(X_tst)\nindexes = np.random.choice(len(X_tst), size, replace=False)\nX_new = X_tst.iloc[indexes]\ny_new = MLR.predict(poly.fit_transform(X_new))\n\nplot_predictions(X_new,y_new,descr = 'Linear Regression: ')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Decision Tree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\nd_tree = DecisionTreeRegressor()\nd_tree.fit(X_trn,y_trn)\n\ny_pr_tst = d_tree.predict(X_tst)\n\nmae = mean_absolute_error(y_tst,y_pr_tst)\nr2 = r2_score(y_tst,y_pr_tst)\n\nprint('MAE:{:7.2f},{:7.2f}% of mean'.format(mae,100*mae/y_pr_tst.mean()))\nprint('R2:{:6.3f}'.format(r2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting a reduced sample to test\nsize = len(X_tst)\nindexes = np.random.choice(len(X_tst), size, replace=False)\nX_new = X_tst.iloc[indexes]\ny_new = d_tree.predict(X_new)\n\nplot_predictions(X_new,y_new,descr = 'Decision Tree: ')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrnd_frst = RandomForestRegressor()\nrnd_frst.fit(X_trn,y_trn)\n\ny_pr_tst = rnd_frst.predict(X_tst)\n\nmae = mean_absolute_error(y_tst,y_pr_tst)\nr2 = r2_score(y_tst,y_pr_tst)\n\nprint('MAE:{:7.2f},{:7.2f}% of mean'.format(mae,100*mae/y_pr_tst.mean()))\nprint('R2:{:6.3f}'.format(r2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting a reduced sample to test\nsize = len(X_tst)\nindexes = np.random.choice(len(X_tst), size, replace=False)\nX_new = X_tst.iloc[indexes]\ny_new = rnd_frst.predict(X_new)\n\nplot_predictions(X_new,y_new,descr = 'Random Forest: ')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Das War's Kaggle Notebook Runner: Marília Prata  @mpwolke","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}