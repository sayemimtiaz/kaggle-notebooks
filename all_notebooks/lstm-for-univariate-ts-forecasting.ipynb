{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hi guys, here we'll use a simple LSTM network and solve an univariate forecasting problems. We will use 1-step ahead forecasting approach. You can checkout my previous notebook, if you want to learn all the different type of sequence models and required required LSTM formations.\n\nhttps://www.kaggle.com/amar09/paradigms-of-various-sequence-models-lstm"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# load and plot dataset\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom matplotlib import pyplot\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom math import sqrt\n\nshampoo_df = pd.read_csv('../input/sales-of-shampoo-over-a-three-year-period/sales-of-shampoo-over-a-three-ye.csv', header=0)\nshampoo_df.dropna(inplace=True) # Droping the last row which contains NaN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Month needs to be reformatted\nshampoo_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_data(df):\n    processed_df = df.rename(columns={\n        'Sales of shampoo over a three year period': 'shampoo_sales'\n    }).copy()\n\n    processed_df['Month'] = pd.to_datetime(processed_df.Month.apply(lambda val: '190'+val))\n    processed_df = processed_df.set_index('Month')\n\n    return processed_df\n\nprocessed_df = preprocess_data(shampoo_df)\n# line plot\nprocessed_df.plot(figsize=(16, 7));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- 1901 had no trend, there was only one spike at the end of the year. But later, from year 1902 we do have a trend.\n\n- We see a significant spike at the end of each years. "},{"metadata":{"trusted":true},"cell_type":"code","source":"result = seasonal_decompose(processed_df.shampoo_sales, model='additive')\nresult.plot();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"If you see Seasonal plot, there is a YoY shift in pattern."},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation for LSTM:\nNote that we need to prepare our input dat[](http://)a and expected output data and shape in a form of timesteps and features, as expected by LSTM model."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 1. Transform the time series into a supervised learning problem\n\nSo unlike Classical Timeseries Statistical methods i.e. ARIMA, VAR etc, in LSTM, we are expected to train model in supervised fashon. So basically, we need to have a shifted series, working as an independent variables. We'll understand more as we move forward."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_supervised_independent_var(series):\n    return series.shift(1).fillna(0).values\n\nprocessed_df['X_shifted_sales'] = get_supervised_independent_var(processed_df.shampoo_sales)\nprocessed_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 2. Transform the time series data so that it is stationary.\n\nStationary data is easier to model and will very likely result in more skillful forecasts. Our sales data is clearly non-stationary, mainly because of increasing trend and sudden spikes and dips. So it will be better if we make our series stationary and later, after modelling, reverse transform to obtain the original series.\n\nTo remove trend, we can simply use differencing. that means:\n> T(n) = T(n) - T(n-1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def perform_diff(series):\n    return pd.Series(series.diff().values)\n\nprocessed_df['diff_shampoo_sales'] = perform_diff(processed_df['shampoo_sales']).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_df['diff_shampoo_sales'].plot(figsize=(16,7));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# invert differenced value\ndef invert_diff_transform(initial_actual_item, diff_series):\n    return np.r_[initial_actual_item, diff_series].cumsum()\n\nprocessed_df['invert_diff_sales'] = invert_diff_transform(processed_df['shampoo_sales'].iloc[0], processed_df['diff_shampoo_sales'].iloc[1:])\nprocessed_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 3. Transform the observations to have a specific scale.\n\nScaling is required to get reliable results in LSTM. The reason is our activation will return values in a range of 0-1 or -1 to 1, based on the choice of activation. For model, it's easier to learn and produce results if input is in a similar range. So here we are going to scale values between -1 and 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform scale\ndef scale_transform(series):\n    series = series.reshape(len(series), 1)\n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    scaler = scaler.fit(series)\n    return  scaler, scaler.transform(series)\n\nscaler, processed_df['scaled_shampoo_sales'] = scale_transform(processed_df['shampoo_sales'].values)\nprocessed_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that, when we are preparing train and test set, we will have to use the train scaler on test set as well, so let's modify our function accoringly"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Simply use \nprocessed_df['inv_transformed_shampoo_sales'] = scaler.inverse_transform(processed_df['scaled_shampoo_sales'].values.reshape(-1, 1))\nprocessed_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Modelling function for same\n\n# scale train and test data to [-1, 1]\ndef scale_transform(train, test):\n    # fit scaler\n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    scaler = scaler.fit(train)\n    # transform train\n    train = train.reshape(train.shape[0], train.shape[1])\n    train_scaled = scaler.transform(train)\n    # transform test\n    test = test.reshape(test.shape[0], test.shape[1])\n    test_scaled = scaler.transform(test)\n\n    return scaler, train_scaled, test_scaled\n\n\n# invert transform\ndef invert_scale(scaler, X, value):\n    new_row = [x for x in X] + [value]\n    array = np.array(new_row)\n    array = array.reshape(1, len(array))\n    inverted = scaler.inverse_transform(array)\n    return inverted[0, -1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LSTM Model Development\n\nIf you have seen my previous Notebook, I've explained about various sequence models i.e. one-to-one, many-to-one, one-to-many, and many-to-many. Here, we will go with a one-to-one model with a single feature/timestep, and a single timestep/sample.\n\nWe are going to use stacked LSTM to get a good performance. As we learnt earlier, in this case, first layer needs to have input_shape and it should return an output sequence going into the next LSTM layer.\n\n"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"def fit_lstm(train, batch_size, nb_epoch, neurons_l1):\n    X, y = train[:, 0:-1], train[:, -1]\n    X = X.reshape(X.shape[0], 1, X.shape[1])\n    model = Sequential()\n    model.add(LSTM(neurons_l1, \n                   batch_input_shape=(batch_size, X.shape[1], X.shape[2]), \n                   stateful=True))\n    model.add(Dense(1))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n\n    for i in range(nb_epoch):\n        model.fit(X, y, epochs=1, batch_size=batch_size, verbose=1, shuffle=False)\n        model.reset_states()\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"def forecast(model, batch_size, row):\n    x = row.reshape(1, 1, len(row))\n    yhat = model.predict(x, batch_size=batch_size)\n    return yhat[0,0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Putting it all together\nLets use all of these function in a structured and sequential way to prepare data, train LSTM, and do 1-step ahead forecasting."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# preprocess dataset.\nmodel_df = preprocess_data(shampoo_df)\n\n# Transforming the data to a supervised learning problem.\ndiff_series = perform_diff(model_df.shampoo_sales).dropna()\n\n# Transforming the data to be stationary.\nX_series = get_supervised_independent_var(diff_series)\nsupervised_values = pd.DataFrame([X_series, diff_series.values]).T.values\n\ntrain, test = supervised_values[0:-12], supervised_values[-12:]\n\n# transform the scale of the data\nscaler, train_scaled, test_scaled = scale_transform(train, test)\n\n# Fitting a stateful LSTM network model to the training data.\nlstm_model = fit_lstm(train=train_scaled, batch_size=1, nb_epoch=3000, neurons_l1=4)\n# forecast the entire training dataset to build up state for forecasting\ntrain_reshaped = train_scaled[:, 0].reshape(len(train_scaled), 1, 1)\n# Evaluating the static LSTM model on the test data.\nlstm_model.predict(train_reshaped, batch_size=1)\n# # Report the performance of the forecasts.\n\n# walk-forward validation on the test data\npredictions = list()\nfor i in range(len(test_scaled)):\n    # make one-step forecast: so that statefulness can help LSTM model to learn from previous forecast as well \n    X_row, y_row = test_scaled[i, 0:-1], test_scaled[i, -1]\n    yhat = forecast(lstm_model, 1, X_row)\n    # invert scaling\n    \n    yhat = invert_scale(scaler, X_row, yhat)\n\n    # invert differencing\n    yhat = yhat + model_df.shampoo_sales.values[-1]\n    print(yhat)\n    # store forecast\n    predictions.append(yhat)\n    expected = model_df.shampoo_sales[len(train) + i + 1]\n    print('Month=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expected))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# report performance\nrmse = sqrt(mean_squared_error(model_df.shampoo_sales.values[-12:], predictions))\nprint('Test RMSE: %.3f' % rmse)\n# line plot of observed vs predicted\npyplot.plot(model_df.shampoo_sales.values[-12:])\npyplot.plot(predictions)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"It's not much good, but considering it as a basic model, it's a good starting point. There are many improvement areas in this method and modle building approach. Go ahead.. experiment and share your thoughts in comments.\n\n**Reference:** <br>\n    An amazing book by: Jason Brownlee\n    [Deep Learning for Timeseries Forecasting](https://github.com/amdp-chauhan/datascience-ebooks-and-papers/blob/main/Deep%20Learning/deep-learning-for-time-series-forecasting-predict-the-future-with-mlps-cnns-and-lstms-in-python%20(Jason%20Brownlee).pdf)\n    \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}