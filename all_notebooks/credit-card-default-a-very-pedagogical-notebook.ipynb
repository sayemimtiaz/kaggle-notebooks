{"metadata":{"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","nbconvert_exporter":"python","mimetype":"text/x-python","version":"3.6.3","file_extension":".py","name":"python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat_minor":1,"nbformat":4,"cells":[{"metadata":{"_uuid":"a74f371efbfb4ad4087242386c893e69c72d720d","_cell_guid":"41ecf8cf-6358-4bfd-88b7-a721976fccfc"},"cell_type":"markdown","source":"# Introduction\nThis notebook was created to learn basic techniques of data manipulation and machine learning. The idea is to use the dataset UCI_Credit_Card to improve basic skills of data cleaning, data analysis, data visualization and machine learning. It is primarily intended to help myself understanding what to do and how. Any feedback is welcome.\n\n\n## Variables\nThere are 25 variables:\n\n* ID: ID of each client\n* LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit\n* SEX: Gender (1=male, 2=female)\n* EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n* MARRIAGE: Marital status (1=married, 2=single, 3=others)\n* AGE: Age in years\n* PAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, ... 8=payment delay for eight months, 9=payment delay for nine months and above)\n* PAY_2: Repayment status in August, 2005 (scale same as above)\n* PAY_3: Repayment status in July, 2005 (scale same as above)\n* PAY_4: Repayment status in June, 2005 (scale same as above)\n* PAY_5: Repayment status in May, 2005 (scale same as above)\n* PAY_6: Repayment status in April, 2005 (scale same as above)\n* BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\n* BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n* BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n* BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n* BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n* BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n* PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n* PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n* PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n* PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n* PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n* PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\n* default.payment.next.month: Default payment (1=yes, 0=no)\n\n## Goal and plan\n\nLooking at the problem, I see a potential use of this kind of data: how well can we predict, month by month, the default of our clients? In other words, how well our model performs if we just use the data of the first 2 months with respect to when we use 6 months of payment history? \n\nHowever, since I have a lot of ground to cover, I will focus on a simpler problem: can I predict the default with a month of advance?\n\nThe notebook is structured as follows:\n\n* First exploration: just to see what we have.\n* Cleaning: time to make choices about undocumented labels\n* Feature engineering: time to be creative\n* Final result and lessons learned\n\nThere is an extra step: blind machine learning, because with my short experience I have already found a few pitfalls in my process and I want to share them."},{"metadata":{"_uuid":"5ba24adc75c0d29f38ff44a0718bb2d8034ba584","_cell_guid":"2bbbd9be-d5d6-4607-9786-072d67b87303"},"execution_count":null,"cell_type":"code","outputs":[],"source":"# Import basic libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# import visualization libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom ggplot import *\n%matplotlib inline"},{"metadata":{"_uuid":"866eacd59f720b77731a2e041471b5999ac4906c","_cell_guid":"373de127-2f57-439e-b3fb-febc86cf821c"},"execution_count":null,"cell_type":"code","outputs":[],"source":"# Load the data\n\ndf = pd.read_csv('../input/UCI_Credit_Card.csv')\ndf.sample(5)"},{"metadata":{"_uuid":"391f4b0edb2c804475fd6e35cd738025095f00b5","_cell_guid":"c855e309-85e8-426f-ab80-050b16db868c"},"cell_type":"markdown","source":"As a first step, let's have a look if there are missing or anomalous data"},{"metadata":{"_uuid":"bf10536e7bcca34164565abf554bffe558ed0d5f","_cell_guid":"f6bc9ca2-b40e-4346-838a-922e110d5f20"},"execution_count":null,"cell_type":"code","outputs":[],"source":"df.info()"},{"metadata":{"_uuid":"7ff6b6f89760fafc327b3852e03333fb547ff784","_cell_guid":"f1796586-38ba-4b68-990e-33323cbc2856"},"execution_count":null,"cell_type":"code","outputs":[],"source":"# Categorical variables description\ndf[['SEX', 'EDUCATION', 'MARRIAGE']].describe()"},{"metadata":{"_uuid":"53976f540202b3d7d62074e0fb9b5e3803664a55","_cell_guid":"0f1e309a-146d-4784-86cd-75bfe13e60e9"},"cell_type":"markdown","source":"No missing data, but a few anomalous things:\n* EDUCATION has category 5 and 6 that are unlabelled, moreover the category 0 is undocumented.\n* MARRIAGE has a label 0 that is undocumented"},{"metadata":{"_uuid":"1a40fe4404d10cc9f0073c2fb2b87f4b9f40d4ac","_cell_guid":"39110051-7da4-4e26-900e-2eea1e8efd28"},"execution_count":null,"cell_type":"code","outputs":[],"source":"# Payment delay description\ndf[['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']].describe()"},{"metadata":{"_uuid":"e612682a3c7ca90d276a518d009b39b3d75fb5da","_cell_guid":"b77758bb-773d-4908-9dc0-421d35127259"},"cell_type":"markdown","source":"They all present an undocumented label -2. If 1,2,3, etc are the months of delay, 0 should be labeled 'pay duly' and every negative value should be seen as a 0. But we will get to that later"},{"metadata":{"_uuid":"8960233ca3309cd4ff06610b8f8132a7e10d5abb","_cell_guid":"bcdaba60-c776-491b-92e5-80d5fdca8b30"},"execution_count":null,"cell_type":"code","outputs":[],"source":"# Bill Statement description\ndf[['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']].describe()"},{"metadata":{"_uuid":"ac8957e78333c9288d64c97cbff72ad08ab77f31","_cell_guid":"465b66a4-0da7-49a9-939a-67f3e7dc37c8"},"cell_type":"markdown","source":"Negative values can be interpreted as credit? Has to be investigated"},{"metadata":{"_uuid":"110119ecd8d7d36b1d835a2e821a91dd7fd9527c","_cell_guid":"8d96485a-4215-47bf-9d0f-247aebddf408"},"execution_count":null,"cell_type":"code","outputs":[],"source":"#Previous Payment Description\ndf[['PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']].describe()"},{"metadata":{"_uuid":"0fb66c7c87b1b63dd7018ab642f7faef11ef70ed","_cell_guid":"56705789-b5a9-4a63-8ef1-26687270609e"},"execution_count":null,"cell_type":"code","outputs":[],"source":"df.LIMIT_BAL.describe()"},{"metadata":{"_uuid":"8695b9a4741e2d35bf59f238154a072e6f585192","_cell_guid":"745c76aa-1d96-4588-84b3-4718168d54c0"},"cell_type":"markdown","source":"The range is very broad, Investigation required.\n\nTwo columns bother me because are poorly labeled."},{"metadata":{"_uuid":"7a0dfd1323e523a69799ec877fc1107fb38ce019","_cell_guid":"5e7e7299-3a20-4266-a2df-036123ed86e4"},"execution_count":null,"cell_type":"code","outputs":[],"source":"df = df.rename(columns={'default.payment.next.month': 'def_pay', \n                        'PAY_0': 'PAY_1'})\ndf.head()"},{"metadata":{"_uuid":"8df4001b8ccd4dfd2d4213db898e7d07070c0ce7","_cell_guid":"0cb4683d-aba0-44a9-8a46-db37c5b42c88"},"execution_count":null,"cell_type":"code","outputs":[],"source":"# I am interested in having a general idea of the default probability\ndf.def_pay.sum() / len(df.def_pay)"},{"metadata":{"_uuid":"a4d87d8e1a1deb2400eac839db29f6b1bbcc5cda","_cell_guid":"1a08707b-0b25-4946-9e56-fa842722137f"},"execution_count":null,"cell_type":"code","outputs":[],"source":"# Other ways of getting this kind of numbers (as a reference for newbies like myself)\nprint(df.shape)\nprint(df.shape[0])\nprint(df.def_pay.count())\nprint(len(df.axes[1]))"},{"metadata":{"_uuid":"7ab7b3cf1dcdd693a67f619b2d804498145d0359","_cell_guid":"2143ca59-f241-4bfb-9f4a-fef7947ac54f"},"cell_type":"markdown","source":"# Blind machine learning\n\nI define it blind because I will just throw everything I have in them and nothing more than that. My hope is to see significant improvements once that I will engineer some features.\n\nThis is a classification problem and this means that we can use the following algorithms:\n* Logistic regression\n* Decision tree\n* Random forest\n* Support Vector Classifications\n* Stocastic Gradient Descend\n* Nearest Neighbours Classifiers  \n* Gaussian Process Classification\n* Other ensemble\n* Neural Network Models\n* XGBoost \n\nSince the plan is mostly to use a try and error approach to see how things goes, I will first start on those that I am more familiar with: Decision tree. \n\n## Decision tree\n\nThe Tree is very easy to use and imagine, it can handle both numerical and categorical variables, but it is easy to fall into the trap of overfitting (i.e. creating a very complex model that perfectly describes your training set but fails in predicting).\nIt is also said to be senstive to small changes in the data, leading to different results. For this reason, it is common to use ensembles such as Random Forest to avoid this risk.\n\nThere are a few parameters to be settle for the classifier, such as the function to determine the quality of a split, or how deep the tree has to be. We will first test it with a random choice of those parameters and then see a better process."},{"metadata":{"_uuid":"a5a945b48780176ff95737e1d08a8d7803cb31af","collapsed":true,"_cell_guid":"439157f2-50fe-46f3-a0fb-d7f2216158a6"},"execution_count":null,"cell_type":"code","outputs":[],"source":"#importing libraries\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, make_scorer\nfrom sklearn.model_selection import train_test_split"},{"metadata":{"_uuid":"14d5a0436312b5c695a8bc5342f8774e1ebe61cc","_cell_guid":"37bc114f-4697-4ed0-9db0-823276ae0e5c"},"execution_count":null,"cell_type":"code","outputs":[],"source":"# create the target variable\ny = df['def_pay'].copy()\ny.sample(5)"},{"metadata":{"_uuid":"7de1c539331a0ad78c84bce2512fc62c91cddfeb","_cell_guid":"1f8c6629-3430-4a20-a862-53f72f97ea4d"},"execution_count":null,"cell_type":"code","outputs":[],"source":"# create the features, which now will be everything in the original df\nfeatures = ['LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_1', 'PAY_2',\n       'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2',\n       'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\nX = df[features].copy()\nX.columns"},{"metadata":{"_uuid":"df9288dcdf4f0eb7912acfc2190e3d9ec37f877a","collapsed":true,"_cell_guid":"a36de282-d4ad-40ae-a20b-af8c731ab329"},"execution_count":null,"cell_type":"code","outputs":[],"source":"# split the df into train and test, it is important these two do not communicate during the training\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n# this means we will train on 80% of the data and test on the remaining 20%."},{"metadata":{"_uuid":"a7f6a692e78376c21527fc328bcbf0879a998fb6","_cell_guid":"fdfc6907-fde9-483a-952c-73b48966f6db"},"execution_count":null,"cell_type":"code","outputs":[],"source":"#check that the target is not far off\nprint(df.def_pay.describe())\nprint(\"---------------------------\")\nprint(y_train.describe())\nprint(\"---------------------------\")\nprint(y_test.describe())"},{"metadata":{"_uuid":"2a550b5331ff2bd9f6f671f89cc2a9ce7c562b7b","_cell_guid":"a18eb4d9-da5a-45f5-a76c-67cc31ae9077"},"execution_count":null,"cell_type":"code","outputs":[],"source":"#create the classifier\nclassifier = DecisionTreeClassifier(max_depth=10, random_state=14) \n# training the classifier\nclassifier.fit(X_train, y_train)\n# do our predictions on the test\npredictions = classifier.predict(X_test)\n# see how good we did on the test\naccuracy_score(y_true = y_test, y_pred = predictions)"},{"metadata":{"_uuid":"95af8e1cdbae3f6cfe55a6c0c0b8d32e8a9d8966","_cell_guid":"6e1684d1-95f2-4661-b95f-0f437582ba2e"},"cell_type":"markdown","source":"Not bad for a beginner, 81% is a result you just get with a random choices on the parameters. On this matter, let's see an overfitting example"},{"metadata":{"_uuid":"c212bed812f783b499a8fd483d525b6401f0b439","_cell_guid":"f52ca39a-8acc-475b-8dba-806a599cb01a"},"execution_count":null,"cell_type":"code","outputs":[],"source":"classifier = DecisionTreeClassifier(max_depth=100, random_state=14) \nclassifier.fit(X_train, y_train)\npredictions = classifier.predict(X_test)\naccuracy_score(y_true = y_test, y_pred = predictions)"},{"metadata":{"_uuid":"a08a114449ece556137ee4c51df1b474a90daf97","_cell_guid":"742983c7-95ed-41f0-8f41-6df560818ffd"},"cell_type":"markdown","source":"So it is easy to mess up your model, apparently. There is a better way that I know to get your parameters right, which is to use **GridSearchCV**. Essentially we give a parameters space and test our model on every point of this space, returning the ideal combination of parameters. \n\nBe aware that the more you complicate the grid, the longer it takes, because it is running it every time with a different combination of parameters. \n\nIt is a good chance to discuss the three parameters that I understand\n* **criterion ** can be gini (measures how often a randomly chosen element would be incorrectly identified) or entropy (measures the information contained by each attribute, thus we estimate the reduction in entropy due to sorting on the attribute)\n* **max depth** is the maximum amount of steps the tree should do\n*  **max leaf nodes** maximum number of nodes\n* **min sample split** a limit to stop further splitting, useful to avoid overfitting"},{"metadata":{"_uuid":"6bdbfeb0185516543d756e6c0df04793c7d619c3","collapsed":true,"_cell_guid":"ab0d57fc-eda8-418f-bd13-5ec763d58922"},"execution_count":null,"cell_type":"code","outputs":[],"source":"from sklearn.model_selection import GridSearchCV"},{"metadata":{"_uuid":"0ddf1487ebfc5e62fdb05d47911aa8993ed83079","_cell_guid":"afd3de4f-754c-4f7a-a993-aa9b7c2cdefa"},"execution_count":null,"cell_type":"code","outputs":[],"source":"# define the parameters grid\nparam_grid = {'max_depth': np.arange(3, 10),\n             'criterion' : ['gini','entropy'],\n             'max_leaf_nodes': [5,10,20,100],\n             'min_samples_split': [2, 5, 10, 20]}\n\n# create the grid\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), param_grid, cv = 5, scoring= 'accuracy')\n# the cv option will be clear in a few cells\n\n#training\ngrid_tree.fit(X_train, y_train)\n#let's see the best estimator\nprint(grid_tree.best_estimator_)\n#with its score\nprint(np.abs(grid_tree.best_score_))"},{"metadata":{"_uuid":"a95ca1e52a6062a0096a2e23054aa6cf85c3cf79","_cell_guid":"b49d8320-1f14-4b26-9923-c94242e4ee27"},"execution_count":null,"cell_type":"code","outputs":[],"source":"classifier = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n            max_features=None, max_leaf_nodes=20,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=20,\n            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n            splitter='best')\nclassifier.fit(X_train, y_train)\npredictions = classifier.predict(X_test)\naccuracy_score(y_true = y_test, y_pred = predictions)"},{"metadata":{"_uuid":"3671988c0613f2e4880b233aaa0948fb706e2718","_cell_guid":"4ef63f0d-7592-45ef-886e-f0305c1528a5"},"cell_type":"markdown","source":"We got a +0.07% in accuracy just by choosing the right parameters..\n\nAnother thing we should do is to cross validate. Actually, it is kind of included in the gridsearch but it is still unclear to me and we get a chance of exploring another tool of the sklearn library: **Kfold**"},{"metadata":{"_uuid":"c36c3e7ae2598c16acbe1d4e0efe51145f199438","collapsed":true,"_cell_guid":"07d5128a-3e46-4508-a08e-4718f69e3c8f"},"execution_count":null,"cell_type":"code","outputs":[],"source":"from sklearn.model_selection import KFold"},{"metadata":{"_uuid":"bd3b309e6993c66de9f033c7794a52d2ff363a1f","_cell_guid":"5642816c-e615-4b24-8255-c879f2aef992"},"execution_count":null,"cell_type":"code","outputs":[],"source":"kf = KFold(n_splits=5,random_state=42,shuffle=True)\n\nfold = []\nscr = []\n\nfor i,(train_index, test_index) in enumerate(kf.split(df)):\n    training = df.iloc[train_index,:]\n    valid = df.iloc[test_index,:]\n    feats = training[features] #defined above\n    label = training['def_pay']\n    valid_feats = valid[features]\n    valid_label = valid['def_pay']\n    classifier.fit(feats,label) #it is the last one we run, the best one\n    pred = classifier.predict(valid_feats)\n    score = accuracy_score(y_true = valid_label, y_pred = pred)\n    fold.append(i+1)\n    scr.append(score)\n    \n#create a small df with the scores\nperformance = pd.DataFrame({'Score':scr,'Fold':fold})\n# let's see what we have with ggplot\ng = ggplot(performance,aes(x='Fold',y='Score')) + geom_point() + geom_line()\nprint(g)"},{"metadata":{"_uuid":"0a4a0a6969d0ed2339c8b1b72fd165501cf4747a","_cell_guid":"11873f36-0748-4c0c-97d7-0a77ee3a9bf8"},"cell_type":"markdown","source":"Now, this does not mean much now, since it is only one model and, depending on how we split the df into train and test, we get  a precision between 0.816 and 0.8275, which may or may not be relevant depending on the purpose of the model. However, in the future we will compare different models in order to find the one more stable and this small experiment will be useful (I hope, I really don't know much). \n\nNext, we can see how important are our features for this model."},{"metadata":{"_uuid":"e36b1e450f0a2e03beaaddfe77c21d562acfc9ff","_cell_guid":"60316c6d-bf93-44a6-b889-2187b0b442ad"},"execution_count":null,"cell_type":"code","outputs":[],"source":"def get_feature_importance(clsf, ftrs):\n    imp = clsf.feature_importances_.tolist()\n    feat = ftrs\n    result = pd.DataFrame({'feat':feat,'score':imp})\n    result = result.sort_values(by=['score'],ascending=False)\n    return result\n\nget_feature_importance(classifier, features)"},{"metadata":{"_uuid":"c0034b553c5b5f58e985dab83153bace0d79c1f9","_cell_guid":"42013291-45de-4d02-98ba-314ab04c702b"},"cell_type":"markdown","source":"The variable PAY_1 are assigned to a specific client. It is so overwhelmingly important that I am tempted to see what happens if I just train on that one. I have no other place to be today, so here we go"},{"metadata":{"_uuid":"5aa8ffd37a110a272f84ff21f3032488dd687b24","_cell_guid":"64353fa0-13ad-4d86-8a0b-3d21fd73155a"},"execution_count":null,"cell_type":"code","outputs":[],"source":"X = df[['PAY_1']].copy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\nclassifier.fit(X_train, y_train) #same classifier as before\npredictions = classifier.predict(X_test)\naccuracy_score(y_true = y_test, y_pred = predictions)"},{"metadata":{"_uuid":"2f3bcf7e13835b443f0bc22eb1a9efd8abda1341","_cell_guid":"611ab8f4-2522-4a3f-baaa-bda266b39d7c"},"cell_type":"markdown","source":"This is just to humble us in front of the machine supremacy of using a very sofisticated if statement.\n\nThe good thing about trees is that it is easy to visualize "},{"metadata":{"_uuid":"f06e1fa75fef17421c44fad4e8eede15f81dc98c","_cell_guid":"5c7d767b-69cc-49f6-9480-1432299ed1e0"},"execution_count":null,"cell_type":"code","outputs":[],"source":"from sklearn import tree\nimport graphviz\ndot_data = tree.export_graphviz(classifier, out_file=None)  \ngraph = graphviz.Source(dot_data)  \ngraph"},{"metadata":{"_uuid":"0cd65266a69be7d5f9b74c4d0b8e1c4636852842","_cell_guid":"a5b3ae3a-791f-4ee8-b969-123e15a2271d"},"cell_type":"markdown","source":"However, I have a doubt: in our dataset 20% of clients default, so if I make a model that just assignes 0 to def_pay I actually get an 80% of accuracy. In this sense, accuracy can be a misleading metric of the quality of our model. \n\nA better metric is the **f1-score**, which takes into account the false positives, the false negatives etc.\n\nSo we define the precision as TP/(TP+FP) and recall as TP/(TP+FN) and we have F1 = 2 * (Prec * Rec) / (Prec + Rec)"},{"metadata":{"_uuid":"9c917c1f95c272eb2ef24ce752087be5bb0c7894","_cell_guid":"1d6a8ee9-3c2d-4e32-8030-c576311e6dcb"},"execution_count":null,"cell_type":"code","outputs":[],"source":"# import the tool\nfrom sklearn.metrics import f1_score\n#recreate the model and evaluate it\nX = df[features].copy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\nclassifier.fit(X_train, y_train) #same classifier as before\npredictions = classifier.predict(X_test)\nf1_score(y_true = y_test, y_pred = predictions)"},{"metadata":{"_uuid":"012775bea680696f5583522cb721b3f880042fe4","_cell_guid":"be66e917-eb99-487f-aea4-c8f2f17a2049"},"execution_count":null,"cell_type":"code","outputs":[],"source":"# True Positive (TP): we predict a label of 1 (positive), and the true label is 1.\nTP = np.sum(np.logical_and(predictions == 1, y_test == 1))\n \n# True Negative (TN): we predict a label of 0 (negative), and the true label is 0.\nTN = np.sum(np.logical_and(predictions == 0, y_test == 0))\n \n# False Positive (FP): we predict a label of 1 (positive), but the true label is 0.\nFP = np.sum(np.logical_and(predictions == 1, y_test == 0))\n \n# False Negative (FN): we predict a label of 0 (negative), but the true label is 1.\nFN = np.sum(np.logical_and(predictions == 0, y_test == 1))\n \nprint('TP: {}, FP: {}, TN: {}, FN: {}'.format(TP,FP,TN,FN))"},{"metadata":{"_uuid":"7f572faeeda1b3685786af08d34607f5ccccc68d","_cell_guid":"cecd9c52-8bdb-486e-8ebd-f49d5491c652"},"cell_type":"markdown","source":"Which is a bad performance, especially if we consider how much can cost a false negative or a false positive. \n\nTo be fair, we have chosen the parameters by picking the best in accuracy, let's try with this new metric"},{"metadata":{"_uuid":"0fd368fa5022a6993e3a44dc6d2fdfdc21229305","_cell_guid":"c9c24bf6-0581-4a19-b5a4-2cea08b75e32"},"execution_count":null,"cell_type":"code","outputs":[],"source":"param_grid = {'max_depth': np.arange(3, 10),\n             'criterion' : ['gini','entropy'],\n             'max_leaf_nodes': [5,10,20,100],\n             'min_samples_split': [2, 5, 10, 20]}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), param_grid, cv = 5, scoring= 'f1')\ngrid_tree.fit(X_train, y_train)\nbest = grid_tree.best_estimator_\nprint(grid_tree.best_estimator_)\nprint(np.abs(grid_tree.best_score_))\nclassifier = best\nclassifier.fit(X_train, y_train)\npredictions = classifier.predict(X_test)\nprint(\"-------------\")\nprint(f1_score(y_true = y_test, y_pred = predictions))\nprint(get_feature_importance(classifier, features))\nprint(\"-------------\")\nTP = np.sum(np.logical_and(predictions == 1, y_test == 1))\nTN = np.sum(np.logical_and(predictions == 0, y_test == 0))\nFP = np.sum(np.logical_and(predictions == 1, y_test == 0))\nFN = np.sum(np.logical_and(predictions == 0, y_test == 1))\nprint('TP: {}, FP: {}, TN: {}, FN: {}'.format(TP,FP,TN,FN))"},{"metadata":{"_uuid":"9deb4150dcc35a605d3fe177fef8473e832a91d4","_cell_guid":"c39eb066-3361-4e90-ad79-653c879c81da"},"cell_type":"markdown","source":"Which is better, but still pretty bad for any practical purpose.\n\nI have learned a lot by this mistake, so I leave it for people like me and apologize to those that already knew it.\n\nActually, there is an option for this classifier that should help, let's try it"},{"metadata":{"_uuid":"ba7e8cc36cfa512d29f5ebd1841d6f6cfd12d554","_cell_guid":"c496d3e0-eb44-4ab1-91cc-09510636749f"},"execution_count":null,"cell_type":"code","outputs":[],"source":"param_grid = {'max_depth': np.arange(3, 10),\n             'criterion' : ['gini','entropy'],\n             'max_leaf_nodes': [5,10,20,100],\n             'min_samples_split': [2, 5, 10, 20],\n             'class_weight' : ['balanced']}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), param_grid, cv = 5, scoring= 'f1')\ngrid_tree.fit(X_train, y_train)\nbest = grid_tree.best_estimator_\nprint(grid_tree.best_estimator_)\nprint(np.abs(grid_tree.best_score_))\nclassifier = best\nclassifier.fit(X_train, y_train)\npredictions = classifier.predict(X_test)\nprint(\"-------------\")\nprint(f1_score(y_true = y_test, y_pred = predictions))\nprint(get_feature_importance(classifier, features))\nprint(\"-------------\")\nTP = np.sum(np.logical_and(predictions == 1, y_test == 1))\nTN = np.sum(np.logical_and(predictions == 0, y_test == 0))\nFP = np.sum(np.logical_and(predictions == 1, y_test == 0))\nFN = np.sum(np.logical_and(predictions == 0, y_test == 1))\nprint('TP: {}, FP: {}, TN: {}, FN: {}'.format(TP,FP,TN,FN))"},{"metadata":{"_uuid":"64c07e515cd0372fc1d85cdcf43edc843074eb00","_cell_guid":"5c261688-c5fd-4f6a-8ee9-b2f57bada11c"},"cell_type":"markdown","source":"It is significantly better, but still pretty bad. Considering that is a result you reach by doing almost nothing, I find it very nice because it settle a lower bound in what we can achieve.\n\nLet's continue with the standard procedure and come back to exploring the data, cleaning them and create some new feature\n\n\n# Data exploration\n\nWe have already had a feel of the dataset in the first few cells, let's have a better one while also exploring some very basic techniques.\n\n## Categorical variables\n\nThat are SEX, MARRIAGE, EDUCATION, I want to see both how our dataset is divided and if there are sparse classes which can cause overfit of our models."},{"metadata":{"_uuid":"6f5929982863b1ba9b725a75a1e78ac11733da9d","_cell_guid":"d6b324e3-5a99-4aa5-a244-ffd4d70e9775"},"execution_count":null,"cell_type":"code","outputs":[],"source":"df.SEX.value_counts() #this is fine, more women than men"},{"metadata":{"_uuid":"d0a0f450ea65aa187ee74116fc0a264d19d56055","_cell_guid":"f958a5dc-2b8d-44ad-b82a-4c18213bb0f8"},"execution_count":null,"cell_type":"code","outputs":[],"source":"df['MARRIAGE'].value_counts()"},{"metadata":{"_uuid":"1ad9cd7c226e37185beea1a226fdd85fc5ea2f91","_cell_guid":"c321f70c-4f94-464b-82c3-f11f1df654ef"},"execution_count":null,"cell_type":"code","outputs":[],"source":"df.EDUCATION.value_counts() # yes, I am using different ways of calling a column"},{"metadata":{"_uuid":"0bf2d391267553c41dd58a366f50aa8aa045d97e","_cell_guid":"fe229871-4c6f-4533-93de-9b13de6d2f9d"},"cell_type":"markdown","source":"If you are fancy people (and, let's face it, you are) you might speed up the exploration by just looking at a bar plot"},{"metadata":{"_uuid":"24e9de5c82048081b900cb4dbfd1c55f9d66963f","_cell_guid":"98f0c984-e90f-499c-9d0e-2fb29268035c"},"execution_count":null,"cell_type":"code","outputs":[],"source":"df.MARRIAGE.value_counts().plot(kind = 'bar')"},{"metadata":{"_uuid":"f7f5f43fc118686e242bb338572731429e965b07","_cell_guid":"2813b5dd-881e-43f0-b7bf-f0abca32b65f"},"cell_type":"markdown","source":"Or a better one even"},{"metadata":{"_uuid":"1ac329a976489b364580eb884bc3ee9d5d99b9d0","_cell_guid":"9dc8c0a1-f245-4127-8e3f-3d5cda26be50"},"execution_count":null,"cell_type":"code","outputs":[],"source":"df.EDUCATION.value_counts().plot(kind = \"barh\")"},{"metadata":{"_uuid":"0b61fc31fbede21a792c992c9df0dbed0bfac809","_cell_guid":"b79c65de-ad8d-4bf2-aaae-ba301ca6ea3f"},"cell_type":"markdown","source":"Which shows immediately that some classes will need some cleaning.\n\nLet's move on with what was supposed to be the first exploration\n\n## Numerical variables\n\nI want to use some histograms to have a feel of the distribution, checking if they make sense, the presence of outliers etc"},{"metadata":{"_uuid":"f20c8ab4ce6b0d2410e3096e038dc57de3df358a","collapsed":true,"_cell_guid":"ce01718e-010c-46b5-a5e1-edefab94c31d"},"execution_count":null,"cell_type":"code","outputs":[],"source":"def draw_histograms(df, variables, n_rows, n_cols, n_bins):\n    fig=plt.figure()\n    for i, var_name in enumerate(variables):\n        ax=fig.add_subplot(n_rows,n_cols,i+1)\n        df[var_name].hist(bins=n_bins,ax=ax)\n        ax.set_title(var_name)\n    fig.tight_layout()  # Improves appearance a bit.\n    plt.show()"},{"metadata":{"_uuid":"5fb8a92dbad6e461d892652861fb1046d6d639eb","_cell_guid":"3f7fb4aa-5857-40b9-90dc-3166f1253894"},"execution_count":null,"cell_type":"code","outputs":[],"source":"bills = df[['BILL_AMT1','BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']]\ndraw_histograms(bills, bills.columns, 2, 3, 20)"},{"metadata":{"_uuid":"bf8ac7474045da42464c1f3595d8c5f4a7762781","_cell_guid":"29022099-4561-439e-bc79-2fca5b022d5e"},"execution_count":null,"cell_type":"code","outputs":[],"source":"pay = df[['PAY_AMT1','PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']]\ndraw_histograms(pay, pay.columns, 2, 3, 20)"},{"metadata":{"_uuid":"13cd421330b659ca6a2b90ecacd434c7dafbf06c","_cell_guid":"7ecfafe6-4a83-4744-858c-08a1813b559c"},"execution_count":null,"cell_type":"code","outputs":[],"source":"late = df[['PAY_1','PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']]\ndraw_histograms(late, late.columns, 2, 3, 10)\n\n#this is probably more of a category"},{"metadata":{"_uuid":"719e4e12f92d3cc6ca98c014f8ce74714a78a91e","_cell_guid":"9415d2f2-43b8-434e-9d0c-78f49c967ebb"},"execution_count":null,"cell_type":"code","outputs":[],"source":"df.AGE.hist()"},{"metadata":{"_uuid":"3f36e8b4d416dec5919c0215aefa3ccc17782e5f","_cell_guid":"600d2e2b-c651-4b64-91a2-de7eb8a4e8d7"},"execution_count":null,"cell_type":"code","outputs":[],"source":"df.LIMIT_BAL.hist(bins = 20)"},{"metadata":{"_uuid":"68597a81fd2ed643dc5f63d5eaac5b436f7a342a","_cell_guid":"fa644428-c814-4eed-a868-e1ebaab34335"},"cell_type":"markdown","source":"What I observe is that the quantities referred to some amount of money have a very large range, which can cause problems to some some models (for example in linear regression models)\n\n# Data Cleaning\n\nAs seen previously, some categories are mislabeled or undocumented. Before proceeding, it is time to fix it.\n\nThe 0 in MARRIAGE can be safely categorized as 'Other' (thus 3). \n\nThe 0 (undocumented), 5 and 6 (label unknown) in EDUCATION can also be put in a 'Other' cathegory (thus 4)\n\nThus is a good occasion to learn how to use the .loc function"},{"metadata":{"_uuid":"404163c74b1cc932b2c505e94d3199afe19ccf30","_cell_guid":"47964752-cfba-4e75-9deb-756b6a6c84e5"},"execution_count":null,"cell_type":"code","outputs":[],"source":"fil = (df.EDUCATION == 5) | (df.EDUCATION == 6) | (df.EDUCATION == 0)\ndf.loc[fil, 'EDUCATION'] = 4\ndf.EDUCATION.value_counts()"},{"metadata":{"_uuid":"836947201e30f36acc53aa66a38e60a1faef0160","_cell_guid":"ed120564-0428-46d4-830a-a3f6e09502dc"},"execution_count":null,"cell_type":"code","outputs":[],"source":"df.loc[df.MARRIAGE == 0, 'MARRIAGE'] = 3\ndf.MARRIAGE.value_counts()"},{"metadata":{"_uuid":"bcfa6ae2a86746fea3568ed66db8d2b2f4bc6ac0","_cell_guid":"55ab286e-661f-4a52-9974-6ac5816eb0ce"},"cell_type":"markdown","source":"One might wonder what these labels might mean.\n\n* \"Other\" in education can be an education lower than the high school level.\n* \"Other\" in marriage could be, for example, \"divorced\". \n\nAccording to our documentation, the PAY_n variables indicate the number of months of delay and indicates \"pay duly\"with -1. Then what is -2? And what is 0? It seems to me the label has to be adjusted to 0 for pay duly."},{"metadata":{"_uuid":"990178659c1fd30a834150fa5e3102b49e5c0c8e","_cell_guid":"05d1e306-91b8-4716-aead-e671fd716d17"},"execution_count":null,"cell_type":"code","outputs":[],"source":"fil = (df.PAY_1 == -2) | (df.PAY_1 == -1) | (df.PAY_1 == 0)\ndf.loc[fil, 'PAY_1'] = 0\nfil = (df.PAY_2 == -2) | (df.PAY_2 == -1) | (df.PAY_2 == 0)\ndf.loc[fil, 'PAY_2'] = 0\nfil = (df.PAY_3 == -2) | (df.PAY_3 == -1) | (df.PAY_3 == 0)\ndf.loc[fil, 'PAY_3'] = 0\nfil = (df.PAY_4 == -2) | (df.PAY_4 == -1) | (df.PAY_4 == 0)\ndf.loc[fil, 'PAY_4'] = 0\nfil = (df.PAY_5 == -2) | (df.PAY_5 == -1) | (df.PAY_5 == 0)\ndf.loc[fil, 'PAY_5'] = 0\nfil = (df.PAY_6 == -2) | (df.PAY_6 == -1) | (df.PAY_6 == 0)\ndf.loc[fil, 'PAY_6'] = 0\nlate = df[['PAY_1','PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']]\ndraw_histograms(late, late.columns, 2, 3, 10)"},{"metadata":{"_uuid":"2158891d661ec3f6c8871ea7f096ea6d3f6d5158","_cell_guid":"1ac1786d-52e9-4eb0-97cb-4bab9fa44f3a"},"cell_type":"markdown","source":"Next, I am puzzled by how wide is the range of possible values in the features regarding money, so let's see, for example the PAY_AMT1 that are anomalous."},{"metadata":{"_uuid":"249a28c4b32788318fa54df7a31a65873447e95e","_cell_guid":"b07d67b3-62ae-4487-95e0-4ca888cda457"},"execution_count":null,"cell_type":"code","outputs":[],"source":"df[df.PAY_AMT1 > 300000][['LIMIT_BAL', 'PAY_1', 'PAY_2', 'BILL_AMT2', \n                          'PAY_AMT1', 'BILL_AMT1', 'def_pay']]\n# doesn't look weird after all"},{"metadata":{"_uuid":"fb6071224f71b697d2d21727ec73c1fea5e9ae15","_cell_guid":"4c5825e1-8dd7-4f28-beb1-ff6530e95fbf"},"execution_count":null,"cell_type":"code","outputs":[],"source":"df[df.PAY_AMT2 > 300000][['LIMIT_BAL', 'PAY_2', 'PAY_3', 'BILL_AMT3', \n                          'PAY_AMT2', 'BILL_AMT2', 'def_pay']]\n# doesn't look weird after all"},{"metadata":{"_uuid":"ca7cd53510c683b1fcf81b3144092cd124248112","_cell_guid":"44bc8ed7-1c42-4801-a3d5-36144c47791c"},"cell_type":"markdown","source":"A quick check on all the others reveal that they are not outliers but simply clients with a lot of money.\n\nHowever, I will keep this in mind in case of algorithms sensible to scales.\n\n## Further analysis\n\nNow that I have fixed what looked strange, I can look at the correlations with the target variable.\n\nThe goal is to see how relevant each feature is, what is its meaning, if it can be used to create new features, and, as usual, play a bit with other basic techniques."},{"metadata":{"_uuid":"db3501e4684cbdd0bc28a8f85780571e75fcbe03","_cell_guid":"7e6a15ce-156c-4efc-8a91-8bbfdc54e678"},"execution_count":null,"cell_type":"code","outputs":[],"source":"df.groupby(['SEX', 'def_pay']).size()"},{"metadata":{"_uuid":"98fa430e3b5d00f81fac9e7ccf8d6d2217608d74","_cell_guid":"ba1a01cd-3b7d-46f8-a995-338196afbadf"},"cell_type":"markdown","source":"Well, this doesn't look very good, why don't we create a dataframe out of it?"},{"metadata":{"_uuid":"afd1f86f7e1f77d07a1e03fbfffcf24da8d16dc7","_cell_guid":"f099360c-daf0-4342-8eef-cb8ec283c188"},"execution_count":null,"cell_type":"code","outputs":[],"source":"gender = df.groupby(['SEX', 'def_pay']).size().unstack(1)\n# 1 is the default for unstack, but I put it to show explicitly what we are unstacking\ngender"},{"metadata":{"_uuid":"bcaaa8a130af18ec53326d4fd7666ee207588b1e","_cell_guid":"90059780-96e5-4181-a4ff-6a29ca4c4fe8"},"execution_count":null,"cell_type":"code","outputs":[],"source":"# Another, easier, way is to just use crosstab\npd.crosstab(df.SEX, df.def_pay)"},{"metadata":{"_uuid":"26c57601d799a5ae2bf1c713a7dc7b4e1bcec9ff","_cell_guid":"0fcd4103-b848-40e3-ba9e-e96f48558022"},"cell_type":"markdown","source":"We can do two things: plot directly or compute the probability for each gender to default according to our dataset"},{"metadata":{"_uuid":"8a5a1f095b245a19bc58dcd8727e259d48dc44d7","_cell_guid":"b0115fd2-33b9-4c33-882f-f630f0f103ba"},"execution_count":null,"cell_type":"code","outputs":[],"source":"gender.plot(kind='bar', stacked = True)"},{"metadata":{"_uuid":"3725a852bff9e3416e09e660af89a8d2378b9ad4","_cell_guid":"d8185ff7-15f3-4137-8a1d-7a6230dd2e4d"},"execution_count":null,"cell_type":"code","outputs":[],"source":"gender['perc'] = (gender[1]/(gender[0] + gender[1])) \n#this creates a new column in our dataset\ngender"},{"metadata":{"_uuid":"ad44eb0df0edb8a9619e119ca028195201b09c31","_cell_guid":"4511a7d8-7ee1-4d1c-a85b-e45deaf37827"},"cell_type":"markdown","source":" Let's see a slightly different way of obtaining the same percentages."},{"metadata":{"_uuid":"03af4e86dabd537905acbed843432f3649ee35b2","_cell_guid":"87cdcb73-4b77-45eb-9944-8ba665d8ebc2"},"execution_count":null,"cell_type":"code","outputs":[],"source":"df[[\"SEX\", \"def_pay\"]].groupby(['SEX'], \n                                        as_index=False).mean().sort_values(by='def_pay', \n                                                                           ascending=False)"},{"metadata":{"_uuid":"fa57ab90df1d760e23b9f6898ab39fa06f2ddbc6","_cell_guid":"d6597270-f7b6-42ca-a86c-6533c8a58e99"},"execution_count":null,"cell_type":"code","outputs":[],"source":"# I like playing with options, so here we go\ndf[[\"SEX\", \"def_pay\"]].groupby(['SEX']).mean().sort_values(by='def_pay')"},{"metadata":{"_uuid":"1a6f9c0d81ed79372286c2760db2a87e2fbf175b","_cell_guid":"fc420212-8a05-4553-9ec3-9603947dff48"},"cell_type":"markdown","source":"Considering that about 22% of the customers will default, we see a couple of things:\n* there are significantly more women than men\n* men are most likely going to default the next month\n\nHowever, we don't have to jump to any conclusion just yet since there might be some lurking variable that justifies the data better (and, being SEX the first variable we look at, it is most likely the case). However, nice result and move on.\n\nActually no, before looking at EDUCATION I want to speed up the process and exercise on my poor python skills, let's create a function.\n"},{"metadata":{"_uuid":"e8e3e1aac026f9862e1466ce6d25bc37d5360b49","_cell_guid":"aaaf7ef7-abd7-4c69-9a1b-0d81102ee922"},"execution_count":null,"cell_type":"code","outputs":[],"source":"def corr_2_cols(Col1, Col2):\n    res = df.groupby([Col1, Col2]).size().unstack()\n    res['perc'] = (res[res.columns[1]]/(res[res.columns[0]] + res[res.columns[1]]))\n    return res\n\n\"\"\"\nSide note, you could use res[1] and res[0] and still have a function that \ndoes what we did before. However, that would mean that you are reffering to the column \nlabeled 0 and 1, not the position of it. Thus the function will not work if the unstacked \nvariable has different values. \n\nMoreover, a good exercise is to generalize the function so that the unstacked variable can\nhave more than 2 values\n\"\"\""},{"metadata":{"_uuid":"12157e0a7f5de9fe86eb79dedfffad7a70038825","_cell_guid":"2836fe5b-d19b-4c09-8248-2c408cf84959"},"execution_count":null,"cell_type":"code","outputs":[],"source":"corr_2_cols('EDUCATION', 'def_pay')"},{"metadata":{"_uuid":"b77bbaee1cc777cc11acfe38a9bc71421901d440","_cell_guid":"9f0b127c-6f83-40ae-a2c4-3a55df870a46"},"cell_type":"markdown","source":"It seems that the higher is the education, the lower is the probability of defaulting the next month. Only exception is for the category labeled \"Other\" that, if we stick to the documentation, would be lower than high school. However, numerically they will not have much weight in the final result.\n\nLet's see with MARRIAGE."},{"metadata":{"_uuid":"8aa34a0bbfa1eef8d6b4a122928e3d4b31fe8790","_cell_guid":"8b1c2e37-4210-4bd1-aef4-d65cd520c1dc"},"execution_count":null,"cell_type":"code","outputs":[],"source":"corr_2_cols('MARRIAGE', 'def_pay')"},{"metadata":{"_uuid":"46642124b23f6aa18a37071b84f68b6e960556ac","_cell_guid":"8757e94a-005a-47f4-b727-044288e6dcaa"},"cell_type":"markdown","source":"Here it seems that married people are most likely to default as well as the misterious category \"Other\" ( which is again numerically less relevant than the others)\n\nAll considered, these three categories seem to affect the result we want to predict. Thus we keep them in mind for later. \n\nI try to explain these first results and, while I can imagine how marital status or education can determine the balance of your credit card, I can't find a way of explaining why the type of genitals can do that as well. This particular result could probably get more meaning when put in the context of the society this people belong to.\n\nRevealing gender inequalities in not our priority (at least not on a beginner notebook on Kaggle), so we move on."},{"metadata":{"_uuid":"16a66c9632ed18b68b0b49ef6c980164af8e3e6f","_cell_guid":"8890a591-19d5-4f6e-a732-8779ef6400fb"},"execution_count":null,"cell_type":"code","outputs":[],"source":"corr_2_cols('MARRIAGE', 'SEX')"},{"metadata":{"_uuid":"60c8970abe75ec1e73a80a45637cf28025f48454","_cell_guid":"3a23e607-f57e-4216-bed7-c099981c7c5c"},"execution_count":null,"cell_type":"code","outputs":[],"source":"corr_2_cols('EDUCATION', 'SEX')"},{"metadata":{"_uuid":"cbf041798ab56d1de6ce79cadf4704f11b5cfc08","_cell_guid":"a611301c-c85d-4ef2-824e-24d6c5171d35"},"cell_type":"markdown","source":"Now I want to see if my suspects on the payments and the bills have some foundation"},{"metadata":{"_uuid":"6569de99757bc4d48b8b444d8cc4b581c5421abc","_cell_guid":"205d5c79-1046-483e-8b38-fb8497698636"},"execution_count":null,"cell_type":"code","outputs":[],"source":"df[['PAY_AMT6', 'BILL_AMT6', 'PAY_AMT5', \n     'BILL_AMT5', 'PAY_AMT4', 'BILL_AMT4', 'PAY_AMT3', 'BILL_AMT3', \n     'PAY_AMT2', 'BILL_AMT2',\n     'PAY_AMT1', 'BILL_AMT1',\n     'LIMIT_BAL', 'def_pay']].sample(30)"},{"metadata":{"_uuid":"8b27b604597d9b11083134a16ad80555e0182d1a","_cell_guid":"aa6f7e7c-6947-4e4e-b285-21a4ed528358"},"execution_count":null,"cell_type":"code","outputs":[],"source":"df[df.def_pay == 1][['BILL_AMT2',\n     'PAY_AMT1', 'BILL_AMT1', 'PAY_1',\n     'LIMIT_BAL']].sample(30)"},{"metadata":{"_uuid":"1f856681971dd7cdbc3859d392b917cca473959c","_cell_guid":"559da3a9-eb66-470f-ba67-0db9caf40367"},"cell_type":"markdown","source":"To me it seems that it goes like that:\n* I have a BILL of X, I pay Y\n* The month after I have to pay X-Y + X', being X' my new expenses, I pay Y'\n* The month after I have to pay X+X' - Y - Y' + X'' , I pay Y''\n* So on so forth\n\nOn top of that I may or may not have months of delay.\n\nIt seems that if by september I have a bill too close to my limit, I generally fail. However, I can already see some dramatic exceptions.\n\nMoreover, I can spot some clients that joined our dataset at a later month: they have 0 in BILL and PAY AMT for a while and then they start. I have to keep that in mind as well.\n\nNow I want to see how the month of delay gets assigned. To this end, I will consider only people with no delays 6 months ago and see how their payments go."},{"metadata":{"_uuid":"b24a5811d1d5027c8e6534f6ace094282341e184","_cell_guid":"ce491284-22d5-4bfe-b50a-edffd6187455"},"execution_count":null,"cell_type":"code","outputs":[],"source":"fil = ((df.PAY_6 == 0) & (df.BILL_AMT6 > 0) & (df.PAY_5 > 0))\ndf[fil][['BILL_AMT6', 'PAY_AMT5', 'BILL_AMT5', 'PAY_5']].sample(20)"},{"metadata":{"_uuid":"e58acb5c4f070e8ddd3bf405c745fef0012ae5f2","_cell_guid":"8f1b569c-dca4-41b9-85d8-0ee82218b5fa"},"execution_count":null,"cell_type":"code","outputs":[],"source":"fil = ((df.PAY_6 == 0) & (df.BILL_AMT6 > 0) & (df.PAY_5 > 0) & (df.PAY_AMT5 == 0))\ndf[fil][['BILL_AMT6', 'PAY_AMT5', 'BILL_AMT5', 'PAY_5']]"},{"metadata":{"_uuid":"0bcc0e901b977eb7e82ca5359d279e52100a930e","_cell_guid":"d54aaacd-17f5-4473-a3cd-ac70d9af2b50"},"cell_type":"markdown","source":"I am puzzled by a few things:\n* why there is no PAY_5 at 1 but only at 2? This is the first month the client doesn't pay (or doesn't pay enough)\n* Sometimes the client did pay but still got a delay value bigger than 0"},{"metadata":{"_uuid":"d090523f6568c04f41509627920de73f9c03a6c9","_cell_guid":"f0f28c8f-cc33-41b0-9576-92100627fff3"},"execution_count":null,"cell_type":"code","outputs":[],"source":"fil = ((df.PAY_AMT1 > df.BILL_AMT2) & (df.PAY_1 > 0) & (df.PAY_2 == 0))\ndf[fil][['BILL_AMT2', 'PAY_2', 'PAY_AMT2', 'BILL_AMT1', 'PAY_1', 'LIMIT_BAL', 'def_pay']].head(15)"},{"metadata":{"_uuid":"ba5f874881ade373b749516b1057058ca40f9bf0","_cell_guid":"f79056cd-b0ce-43cf-be3e-14781cc66cd5"},"execution_count":null,"cell_type":"code","outputs":[],"source":"g = sns.FacetGrid(df, col = 'def_pay')\ng.map(plt.hist, 'AGE')"},{"metadata":{"_uuid":"770ceec7c834f5617ea9e0bcaea48f6516171257","_cell_guid":"3834276a-b98c-4c1e-8b9d-8db0daa1bece"},"cell_type":"markdown","source":"This throws me off. There are clients that paid more there were asked to, had even a negative bill in Sept., and still have a month of delay, and even defaulted the next month. I am incline of not considering the variables PAY_n for my models because I can't give sense to them (even though it seems they can play a big role). I can transform them into a binary variable (late/notlate) because that is something I can understand.\n\n**Do not run the next cell if you are not comfortable with my ignorance.**"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"df.loc[df.PAY_1 > 0, 'PAY_1'] = 1\ndf.loc[df.PAY_2 > 0, 'PAY_2'] = 1\ndf.loc[df.PAY_3 > 0, 'PAY_3'] = 1\ndf.loc[df.PAY_4 > 0, 'PAY_4'] = 1\ndf.loc[df.PAY_5 > 0, 'PAY_5'] = 1\ndf.loc[df.PAY_6 > 0, 'PAY_6'] = 1\nlate = df[['PAY_1','PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']]\ndraw_histograms(late, late.columns, 2, 3, 10)"},{"metadata":{},"cell_type":"markdown","source":"Before going to a much funnier part of the notebook, I want to use some seaborn I have seen in other notebook. The variable that gives me a reason to do so is AGE"},{"metadata":{"_uuid":"5661fa4fb62eb6a67a6675980812e2210e4e8edf","_cell_guid":"334ab9eb-3e53-4f40-b480-8be605436d16"},"execution_count":null,"cell_type":"code","outputs":[],"source":"g = sns.FacetGrid(df, col = 'def_pay', row = 'SEX')\ng.map(plt.hist, 'AGE')"},{"metadata":{"_uuid":"5af2c26dd45b50a8b915f86a327d294d3d070000","_cell_guid":"306707a6-a0a6-4e94-acad-eb1342f03016"},"execution_count":null,"cell_type":"code","outputs":[],"source":"g = sns.FacetGrid(df, col='SEX', hue='def_pay')\ng.map(plt.hist, 'AGE', alpha=0.6, bins=25) #alpha is for opacity\ng.add_legend()"},{"metadata":{"_uuid":"7ab0f42549a62893d4698210d445ad8fe62141d7","_cell_guid":"2a2af28a-3cda-499d-870f-8a5736b7a2ee"},"execution_count":null,"cell_type":"code","outputs":[],"source":"g = sns.FacetGrid(df, col='def_pay', row= \"MARRIAGE\", hue='SEX')\ng.map(plt.hist, 'AGE', alpha=0.3, bins=25) \ng.add_legend()"},{"metadata":{"_uuid":"1d0706cc0446eb357066d310e142a550bf3197c7","_cell_guid":"9472ee05-4b8b-4176-b321-30c87b7a2a03"},"cell_type":"markdown","source":"This tells me that I can probably combine age with other categories to get a more descriptive feature\n\n# Feature Engineering\n\nWhen I talked with more experienced Kagglers or, generically, other Data Scientists about feature engineering I often got the answer \"it is more an art than a science\". I am always suspicious towards such claims, often used to hide the inability of explaining what leads to some choices. Therefore I will try to be pedantic once more and explain why I do what I do. After that we can see how the previous process changes and it leads to better results or not.\n\nI performed an analysis on the categorical variables before and I got some insight out of it, can I design variables to help my models?\n\nI saw that men are most likely to default and also that married people are most likely to default. Thus why not combine them in a single variable given by the product of the two? I chose these two first because they are both labeled 1,2 and they both decrease the probability of defaulting if their value increase. According to our models, SEX and MARRIAGE are the least important variables, so it can be a good exercise to see if combining them is a waste of time.\n\nThis operation will create a category for married man, which I expect to be the one with high probability of defaulting, a combination of married women and single man, a category (expected to be numerically less populated) of \"divorced\" men (in other words, the \"other\" in marriage times the \"male\" in sex), one for single women and one for \"divorced\" women."},{"metadata":{"_uuid":"55bc1d4196d2339feab962af782b7e0d6f58a931","_cell_guid":"79e3c840-5a97-4c47-8639-73263dda2d0e"},"execution_count":null,"cell_type":"code","outputs":[],"source":"df['SE_MA'] = df.SEX * df.MARRIAGE\ncorr_2_cols('SE_MA', 'def_pay')"},{"metadata":{"_uuid":"93c5e3c484b67f0486439469d0a35b76f84a260d","_cell_guid":"1413d9b4-5464-4e34-9765-32b203e4ae5d"},"cell_type":"markdown","source":"In general it can be good enough, but there can be a better way of doing it."},{"metadata":{"_uuid":"806974e0aaaaabf759e32097bde4a0732abb1d18","_cell_guid":"0c32f531-bbec-4d11-8895-5c8b59437263"},"execution_count":null,"cell_type":"code","outputs":[],"source":"df['SE_MA_2'] = 0\ndf.loc[((df.SEX == 1) & (df.MARRIAGE == 1)) , 'SE_MA_2'] = 1 #married man\ndf.loc[((df.SEX == 1) & (df.MARRIAGE == 2)) , 'SE_MA_2'] = 2 #single man\ndf.loc[((df.SEX == 1) & (df.MARRIAGE == 3)) , 'SE_MA_2'] = 3 #divorced man\ndf.loc[((df.SEX == 2) & (df.MARRIAGE == 1)) , 'SE_MA_2'] = 4 #married woman\ndf.loc[((df.SEX == 2) & (df.MARRIAGE == 2)) , 'SE_MA_2'] = 5 #single woman\ndf.loc[((df.SEX == 2) & (df.MARRIAGE == 3)) , 'SE_MA_2'] = 6 #divorced woman\ncorr_2_cols('SE_MA_2', 'def_pay')"},{"metadata":{"_uuid":"90e45a6a2212743d484dd0e3539bc98e50dabb5b","_cell_guid":"cb94446e-0fea-48c3-941d-25cbbfcefe85"},"cell_type":"markdown","source":"I like this better because I can see clearly that married men have a higher probability of defaulting, single men have nothing special with respect to the all population and single women have a lowe probability of defaulting. Among the divorced, men have more troubles (but these categories are not very much populated).\n\nIt can be useful to create age categories.  We can do it in three ways (that I know of).\n\nFirst, we could simply create a column and put a bunch of filters to fill it with the help of loc."},{"metadata":{"_uuid":"d5e28e54440bac7ff38f18c92fa46f8e437c6046","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[],"source":"del df['SE_MA']\ndf = df.rename(columns={'SE_MA_2': 'SE_MA'})"},{"metadata":{"_uuid":"60d65e4b016a8a2a4707a695eb31aa92bb1ca710","_cell_guid":"ee2915e1-3cb1-4d81-9b47-0edc0e2b1b84"},"execution_count":null,"cell_type":"code","outputs":[],"source":"df['AgeBin'] = 0 #creates a column of 0\ndf.loc[((df['AGE'] > 20) & (df['AGE'] < 30)) , 'AgeBin'] = 1\ndf.loc[((df['AGE'] >= 30) & (df['AGE'] < 40)) , 'AgeBin'] = 2\ndf.loc[((df['AGE'] >= 40) & (df['AGE'] < 50)) , 'AgeBin'] = 3\ndf.loc[((df['AGE'] >= 50) & (df['AGE'] < 60)) , 'AgeBin'] = 4\ndf.loc[((df['AGE'] >= 60) & (df['AGE'] < 70)) , 'AgeBin'] = 5\ndf.loc[((df['AGE'] >= 70) & (df['AGE'] < 81)) , 'AgeBin'] = 6\ndf.AgeBin.hist()"},{"metadata":{"_uuid":"515d86abe0771bcb4772c40008bc2ad423e78aef","_cell_guid":"2af7d4a4-4413-40bb-a986-9c3a8d05a0af"},"cell_type":"markdown","source":"This works gives you control of how big the bins are BUT, let's face it, now that we know how loc works (sort of) it is not practical. We can use the second method that I know, which is to cut"},{"metadata":{"_uuid":"98fcbc5af275ee736ea4667e32e9306c4409daf6","_cell_guid":"c1295780-e244-493d-b417-964c904cc608"},"execution_count":null,"cell_type":"code","outputs":[],"source":"bins = [20, 29, 39, 49, 59, 69, 81]\nbins_names = [1, 2, 3, 4, 5, 6]\ndf['AgeBin2'] = pd.cut(df['AGE'], bins, labels=bins_names)\ndf.AgeBin2.hist()"},{"metadata":{"_uuid":"efab1af4db647b178ba155bab73ec65a45d0f3a8","_cell_guid":"529cdbf7-77cc-48ac-ac04-8dc1949ece07"},"cell_type":"markdown","source":"We notice 2 things:\n* the bins have to be defined in a slightly counter intuitive way (at first) due to the fact that it includes the upper limit (as you can check by just changing the bins). You can play with the option \"right\" that is True by default\n* the bins names have to be less numerous than the bins, i.e. with one bin you do bins = [20,81] and bins_names = [ 1 ] \n\nThere is actually a faster way of doing 6 bins with cut, at the price of losing control on how big these bins are"},{"metadata":{"_uuid":"d22bec661afbb115d6c44d56e320e36a82047097","_cell_guid":"d1a1d61a-984d-472f-bcc1-8999907e03e4"},"execution_count":null,"cell_type":"code","outputs":[],"source":"df['AgeBin3'] = pd.cut(df['AGE'], 6)\ndf.AgeBin3.value_counts()"},{"metadata":{"_uuid":"1b5237b4b3eb1e2b899d13fbfce10cf381c69a81","_cell_guid":"426e3ffd-88a0-4f27-92a1-4a14e25ec1de"},"execution_count":null,"cell_type":"code","outputs":[],"source":"df['AgeBin3'] = pd.cut(df['AGE'], 6, labels=bins_names) #just added one option\ndf.AgeBin3.hist()"},{"metadata":{"_uuid":"052489aaeab4c1f070552bdad6a89cb41e3c43ff","_cell_guid":"84574153-db25-44b1-89c4-076364944fc7"},"cell_type":"markdown","source":"Another way of cutting a countinuos variable can be with a quantile-based discretization. This is done by the function qcut\n\nThis can be useful if, for example, you have outliers (like in the balance variable it is possible there will be some) because those outliers would just fall into the extremal categories."},{"metadata":{"_uuid":"3ff93275e171f00349fb49f935fb22f8e9cdbb2d","_cell_guid":"0f5fe1ad-6a9b-47ed-8d9b-9a58f3e1ec47"},"execution_count":null,"cell_type":"code","outputs":[],"source":"df['AgeBin4'] = pd.qcut(df['AGE'], 6)\ndf.AgeBin4.value_counts()"},{"metadata":{"_uuid":"5ce7a422a62bdcc63805acd2920cd8e851e23e22","_cell_guid":"6e6c88fc-03db-4d22-84ce-b80976a61b5d"},"execution_count":null,"cell_type":"code","outputs":[],"source":"df['AgeBin4'] = pd.qcut(df['AGE'], 6, labels=bins_names)\ndf.AgeBin4.hist()"},{"metadata":{"_uuid":"5a9242a1de733644dc008b1d4824c7f00f3cc0d8","_cell_guid":"d8bec6c6-19f4-464e-84f1-30f268e508e3"},"cell_type":"markdown","source":"We don't need all of them. The normal cut is easier to explain, but I need to get rid to the category 6 since it is not populated enough and can ruin the model. I will group it with the 5, effectively treating every client over 60 in the same way.\n\nI will get rid of all the others."},{"metadata":{"_uuid":"085aefaacc00dd8b1a8a3d5503fa35a8baec3f15","_cell_guid":"96fad707-8f34-4444-b223-80fe1375ba14"},"execution_count":null,"cell_type":"code","outputs":[],"source":"del df['AgeBin2']\ndel df['AgeBin3']\ndel df['AgeBin4'] # we don't need these any more\ndf['AgeBin'] = pd.cut(df['AGE'], 6, labels = [1,2,3,4,5,6])\n#because 1 2 3 ecc are \"categories\" so far and we need numbers\ndf['AgeBin'] = pd.to_numeric(df['AgeBin'])\ndf.loc[(df['AgeBin'] == 6) , 'AgeBin'] = 5\ndf.AgeBin.hist()"},{"metadata":{"_uuid":"a427f8c78379b0f9304648f898e074fd3f84e20f","_cell_guid":"428340e1-48ad-4e62-8cba-dd6b21a8df6b"},"execution_count":null,"cell_type":"code","outputs":[],"source":"corr_2_cols('AgeBin', 'def_pay')"},{"metadata":{"_uuid":"7a73e14ef15a4a6c3cd19c7f41cb08713751867b","_cell_guid":"d2f7bf76-882c-4b35-b49b-0b604620b299"},"execution_count":null,"cell_type":"code","outputs":[],"source":"corr_2_cols('AgeBin', 'SEX')"},{"metadata":{"_uuid":"34b1f08774e5baf2aef702e2fcaf24f88fd0a0c7","_cell_guid":"6400069e-1b80-4fdc-9012-8b1a53abc333"},"cell_type":"markdown","source":"I see that default probability goes down in your 30's and then goes higher and higher. At the same time, the percentage of men is growing with the age category. Thus I want to combine the two as before and create a combination of the two."},{"metadata":{"_uuid":"02019d1e796f048847902f6aa20dd70a4c22c7b1","_cell_guid":"814feb21-df0a-4976-9a21-d6a86bcb02e5"},"execution_count":null,"cell_type":"code","outputs":[],"source":"df['SE_AG'] = 0\ndf.loc[((df.SEX == 1) & (df.AgeBin == 1)) , 'SE_AG'] = 1 #man in 20's\ndf.loc[((df.SEX == 1) & (df.AgeBin == 2)) , 'SE_AG'] = 2 #man in 30's\ndf.loc[((df.SEX == 1) & (df.AgeBin == 3)) , 'SE_AG'] = 3 #man in 40's\ndf.loc[((df.SEX == 1) & (df.AgeBin == 4)) , 'SE_AG'] = 4 #man in 50's\ndf.loc[((df.SEX == 1) & (df.AgeBin == 5)) , 'SE_AG'] = 5 #man in 60's and above\ndf.loc[((df.SEX == 2) & (df.AgeBin == 1)) , 'SE_AG'] = 6 #woman in 20's\ndf.loc[((df.SEX == 2) & (df.AgeBin == 2)) , 'SE_AG'] = 7 #woman in 30's\ndf.loc[((df.SEX == 2) & (df.AgeBin == 3)) , 'SE_AG'] = 8 #woman in 40's\ndf.loc[((df.SEX == 2) & (df.AgeBin == 4)) , 'SE_AG'] = 9 #woman in 50's\ndf.loc[((df.SEX == 2) & (df.AgeBin == 5)) , 'SE_AG'] = 10 #woman in 60's and above\ncorr_2_cols('SE_AG', 'def_pay')"},{"metadata":{"_uuid":"85eb10987dd1f5711dae1b82f2438b9250a22b24","_cell_guid":"802b89e2-2859-477e-9aaa-a26f6c5027b3"},"cell_type":"markdown","source":"Now it is time to do something that indicates being a client at a given month or not. I would say that if PAY, BILL_AMT and PAY_AMT are 0, then we are not talking about a client"},{"metadata":{"_uuid":"82c67f3a9d0a9b52526268fe8f6ad9416ad997da","_cell_guid":"8b9e0f9c-b011-41b9-bb77-bc16493fd393"},"execution_count":null,"cell_type":"code","outputs":[],"source":"df['Client_6'] = 1\ndf['Client_5'] = 1\ndf['Client_4'] = 1\ndf['Client_3'] = 1\ndf['Client_2'] = 1\ndf['Client_1'] = 1\ndf.loc[((df.PAY_6 == 0) & (df.BILL_AMT6 == 0) & (df.PAY_AMT6 == 0)) , 'Client_6'] = 0\ndf.loc[((df.PAY_5 == 0) & (df.BILL_AMT5 == 0) & (df.PAY_AMT5 == 0)) , 'Client_5'] = 0\ndf.loc[((df.PAY_4 == 0) & (df.BILL_AMT4 == 0) & (df.PAY_AMT4 == 0)) , 'Client_4'] = 0\ndf.loc[((df.PAY_3 == 0) & (df.BILL_AMT3 == 0) & (df.PAY_AMT3 == 0)) , 'Client_3'] = 0\ndf.loc[((df.PAY_2 == 0) & (df.BILL_AMT2 == 0) & (df.PAY_AMT2 == 0)) , 'Client_2'] = 0\ndf.loc[((df.PAY_1 == 0) & (df.BILL_AMT1 == 0) & (df.PAY_AMT1 == 0)) , 'Client_1'] = 0\npd.Series([df[df.Client_6 == 1].def_pay.count(),\n          df[df.Client_5 == 1].def_pay.count(),\n          df[df.Client_4 == 1].def_pay.count(),\n          df[df.Client_3 == 1].def_pay.count(),\n          df[df.Client_2 == 1].def_pay.count(),\n          df[df.Client_1 == 1].def_pay.count()], [6,5,4,3,2,1])"},{"metadata":{"_uuid":"324705b02a9860475d88e7e4ef6e9ea295d53096","_cell_guid":"d6070017-a6e8-4ab3-a271-9ba07003703f"},"cell_type":"markdown","source":"I will need these variables if I want to see how my predictions will change if I anticipate them (i. e. if I use data up to june instead of up to september)\n\nNext, I want something to describe the expenses of a client, following the idea I have expressed before when I looked at the BILL AMT and the PAY AMT. I am thinking of something like average expenses at any given month (again, for this first prediction I will just care about the last month but I might need the others later). Moreover, I want to give some context to it, so I will divide that number by LIMIT_BAL, so that we have a sort of extimation of how \"careless\" a client is."},{"metadata":{"_uuid":"e861b0a7e9cca51f9e387b2ec5a53a36d9c816b4","_cell_guid":"6733a56c-59e2-4335-ae18-367a38c9f9e5"},"execution_count":null,"cell_type":"code","outputs":[],"source":"df['Avg_exp_5'] = ((df['BILL_AMT5'] - (df['BILL_AMT6'] - df['PAY_AMT5']))) / df['LIMIT_BAL']\ndf['Avg_exp_4'] = (((df['BILL_AMT5'] - (df['BILL_AMT6'] - df['PAY_AMT5'])) +\n                 (df['BILL_AMT4'] - (df['BILL_AMT5'] - df['PAY_AMT4']))) / 2) / df['LIMIT_BAL']\ndf['Avg_exp_3'] = (((df['BILL_AMT5'] - (df['BILL_AMT6'] - df['PAY_AMT5'])) +\n                 (df['BILL_AMT4'] - (df['BILL_AMT5'] - df['PAY_AMT4'])) +\n                 (df['BILL_AMT3'] - (df['BILL_AMT4'] - df['PAY_AMT3']))) / 3) / df['LIMIT_BAL']\ndf['Avg_exp_2'] = (((df['BILL_AMT5'] - (df['BILL_AMT6'] - df['PAY_AMT5'])) +\n                 (df['BILL_AMT4'] - (df['BILL_AMT5'] - df['PAY_AMT4'])) +\n                 (df['BILL_AMT3'] - (df['BILL_AMT4'] - df['PAY_AMT3'])) +\n                 (df['BILL_AMT2'] - (df['BILL_AMT3'] - df['PAY_AMT2']))) / 4) / df['LIMIT_BAL']\ndf['Avg_exp_1'] = (((df['BILL_AMT5'] - (df['BILL_AMT6'] - df['PAY_AMT5'])) +\n                 (df['BILL_AMT4'] - (df['BILL_AMT5'] - df['PAY_AMT4'])) +\n                 (df['BILL_AMT3'] - (df['BILL_AMT4'] - df['PAY_AMT3'])) +\n                 (df['BILL_AMT2'] - (df['BILL_AMT3'] - df['PAY_AMT2'])) +\n                 (df['BILL_AMT1'] - (df['BILL_AMT2'] - df['PAY_AMT1']))) / 5) / df['LIMIT_BAL']\ndf[['LIMIT_BAL', 'Avg_exp_5', 'BILL_AMT5', 'Avg_exp_4', 'BILL_AMT4','Avg_exp_3', 'BILL_AMT3',\n    'Avg_exp_2', 'BILL_AMT2', 'Avg_exp_1', 'BILL_AMT1', 'def_pay']].sample(20)"},{"metadata":{"_uuid":"31b63f0a26ae95b2015aea7b13c6d55dd30493d0","_cell_guid":"78c0f8b5-662e-4030-b47e-6c951b218f22"},"cell_type":"markdown","source":"One last thing: how far the bill is from the limit should matter, thus I will create that variable. Since the result can vary a lot from one client to the other (the LIMIT_BAL variable has a very wide range), I will again weight this difference on the LIMIT_BAL feature (in my mind it has more meaning, maybe I am wrong). Again, I want to keep the historical trend of the variable"},{"metadata":{"_uuid":"e5e1602c0b25604274995cf43136d938fd83b4dd","_cell_guid":"84d49d81-eac2-4394-bda5-8f043dfd8bd5"},"execution_count":null,"cell_type":"code","outputs":[],"source":"df['Closeness_6'] = (df.LIMIT_BAL - df.BILL_AMT6) / df.LIMIT_BAL\ndf['Closeness_5'] = (df.LIMIT_BAL - df.BILL_AMT5) / df.LIMIT_BAL\ndf['Closeness_4'] = (df.LIMIT_BAL - df.BILL_AMT4) / df.LIMIT_BAL\ndf['Closeness_3'] = (df.LIMIT_BAL - df.BILL_AMT3) / df.LIMIT_BAL\ndf['Closeness_2'] = (df.LIMIT_BAL - df.BILL_AMT2) / df.LIMIT_BAL\ndf['Closeness_1'] = (df.LIMIT_BAL - df.BILL_AMT1) / df.LIMIT_BAL\ndf[['Closeness_6', 'Closeness_5', 'Closeness_4', 'Closeness_3', 'Closeness_2',\n   'Closeness_1', 'def_pay']].sample(20)"},{"metadata":{"_uuid":"5c6a836b922580793ea8a97701cd6a1d2ead139b","collapsed":true,"_cell_guid":"164fe740-d61f-44d4-836f-54bf6a5368f6"},"cell_type":"markdown","source":"It is now time to move on to the (hopefully less blind) machine learning part of this notebook.\n\n# Find my balance and learn my machine\n\nWe have seen in the blind machine learning section of this notebook how poorly the algorithm performs if we have **imbalanced classes**. As far as I know, there are 2 \"methods\" of doing so: by appropriately sampling my training data and by using the right algorithm. It is something very new for me, so I will try to do both, step by step.\n\nFirst, I decide which features I want to include in my models and split the data into training and testing set. It is very important to never let the learning algorithm \"see\" the testing set or you will always be considered a cheater.\n\nI will not include the PAY_n features because I don't know how they appear, I will use AgeBin rather than AGE, I will not include SEX because the small correlation do not make sense to me and I have created two categories that include SEX."},{"metadata":{"_uuid":"dc8b7978ab76c02d6b2d233497182ec75fed0085","collapsed":true,"_cell_guid":"6df6e5f0-153a-4f90-84ac-2b21a7fd7cf1"},"execution_count":null,"cell_type":"code","outputs":[],"source":"features = ['LIMIT_BAL', 'EDUCATION', 'MARRIAGE', 'PAY_1','PAY_2', 'PAY_3', \n            'PAY_4', 'PAY_5', 'PAY_6','BILL_AMT1', 'BILL_AMT2',\n            'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n            'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6', \n            'SE_MA', 'AgeBin', 'SE_AG', 'Avg_exp_5', 'Avg_exp_4',\n            'Avg_exp_3', 'Avg_exp_2', 'Avg_exp_1', 'Closeness_5',\n            'Closeness_4', 'Closeness_3', 'Closeness_2','Closeness_1']\ny = df['def_pay'].copy() # target\nX = df[features].copy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"},{"metadata":{"_uuid":"050d6524239d67e450f5cbcc0e8dbceccfa0f10b","collapsed":true,"_cell_guid":"9d0bacdc-d66a-4cee-977e-ace5839c8dab"},"cell_type":"markdown","source":"## Samples, samples, samples\n\nWithin my train dataset I still have a certain unbalance in the target class, thus I can do the following:\n\n* I don't care, I will just pick the best model\n* I upsample the minority class\n* I downsample the majority class\n* I create a syntetic sample\n* I will just do all of it and see how it goes.\n\nIMPORTANT: everything will happen on the train dataset"},{"metadata":{"_uuid":"037cf6047cc840d54b113f509d2726617a4584d1","_cell_guid":"750603d9-6056-4c03-9c10-b7e63cff40d5"},"execution_count":null,"cell_type":"code","outputs":[],"source":"# create the training df by remerging X_train and y_train\ndf_train = X_train.join(y_train)\ndf_train.sample(10)"},{"metadata":{"_uuid":"e12508f9e3dc6d545c85701ace0090a3ca7c726f","collapsed":true,"_cell_guid":"fa7b6241-ec35-4bd9-a6c8-6796cab5bd66"},"execution_count":null,"cell_type":"code","outputs":[],"source":"from sklearn.utils import resample"},{"metadata":{"_uuid":"4e9b499fc885ae9cd60a952bc7fee9e66e82a453","_cell_guid":"a0f384f7-0e82-47e8-a1cc-b45501bd74cd"},"execution_count":null,"cell_type":"code","outputs":[],"source":"# Separate majority and minority classes\ndf_majority = df_train[df_train.def_pay==0]\ndf_minority = df_train[df_train.def_pay==1]\n\nprint(df_majority.def_pay.count())\nprint(\"-----------\")\nprint(df_minority.def_pay.count())\nprint(\"-----------\")\nprint(df_train.def_pay.value_counts())"},{"metadata":{"_uuid":"1cd3116618a42760cc4436cb36f4d3701e359d72","_cell_guid":"4ada9173-97ec-4f78-8fbb-6929e9ed50bb"},"execution_count":null,"cell_type":"code","outputs":[],"source":"# Upsample minority class\ndf_minority_upsampled = resample(df_minority, \n                                 replace=True,     # sample with replacement\n                                 n_samples=18677,    # to match majority class\n                                 random_state=587) # reproducible results\n# Combine majority class with upsampled minority class\ndf_upsampled = pd.concat([df_majority, df_minority_upsampled])\n# Display new class counts\ndf_upsampled.def_pay.value_counts()"},{"metadata":{"_uuid":"85d677968a1dfba6642ef2090addc38ebb53c063","_cell_guid":"054f2635-e062-45c6-9072-ec21c26bd861"},"execution_count":null,"cell_type":"code","outputs":[],"source":"# Downsample majority class\ndf_majority_downsampled = resample(df_majority, \n                                 replace=False,    # sample without replacement\n                                 n_samples=5323,     # to match minority class\n                                 random_state=587) # reproducible results\n# Combine minority class with downsampled majority class\ndf_downsampled = pd.concat([df_majority_downsampled, df_minority])\n# Display new class counts\ndf_downsampled.def_pay.value_counts()"},{"metadata":{},"cell_type":"markdown","source":"The upsample has the disadvantage of increasing the likelihood of overfitting since it replicates the minority class event. It usually outperform the downsampling.\n\nThe downsample can discard potentially useful information and the sample can be biased, but it helps improving the run time\n\nTo create a syntetic sample I want to use the SMOTE algorithm, which is an oversampling method which creates syntetic samples from the minority class instead of creating copies. It selects 2 or more similar instances and perturb them one at a time by random amount. This techniques should avoid overfitting problems but it risks adding noise to the model"},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","outputs":[],"source":"from imblearn.over_sampling import SMOTE"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"sm = SMOTE(random_state=589, ratio = 1.0)\nX_SMOTE, y_SMOTE = sm.fit_sample(X_train, y_train)\nprint(len(y_SMOTE))\nprint(y_SMOTE.sum())"},{"metadata":{},"cell_type":"markdown","source":"So now we have 4 training sets:\n\n* X_train, y_train, with their unbalance and their authenticity\n* df_upsample, which is balanced but at overfitting risk\n* df_downsample, which will be fast but also potentially useless in terms of predictability (the test set is even bigger than it)\n* X_SMOTE, y_SMOTE, which is syntetic and I don't really know it but should avoid overfitting\n\nI will do everything 4 times so that I can see the results. Now, in my mind the process goes like that:\n\n1. K-fold evaluation, run on different models should give us an idea on which one is more stable towards splitting of train and test\n2. Hyperparameter selection, so that we can have the best version of every algorithm\n3. Training\n4. Testing\n\nIn these steps one pitfall we encountered already is how to evaluate. We have already seen that using accuracy is not very convenient, while using (for example) the f1 metric is.\n\nAnother step would be probability calibration, which is a topic I am not particulartly familiar with (ok, *another* topic I am not particularly familiar with, smarty pants) and I will ignore for now but I will maybe elaborate in a future version of this notebook.\n\n# Algorithm Selection\n\nA very common (and profitable) technique is to use ensambles: combinations of different classifiers that create a more robust model. The basic predictor will be again a Decision tree and I will test a few different ensambles\n\nThe main \"classes\" are:\n\n* baggin ensambles: which reduce the chances of overfitting by running a series of uncostrained learners in parallel and combine them\n* boosting ensambles: which improve the the flexibility of simple models by running a series of constrained learners in sequence. Each one of them will learn from the mistakes of the previous one. At the end it combines them in one unconstrained learner\n\nAs a bagging ensambles I know **Random Forest**. It trains on a large number of trees and combine them. The randomness comes from the fact that each tree is allowed to choose from a random subset of features to split on and each tree is trained on a random subset of observations.\n\nThe first boosting ensamble I know is the **Gradient Tree Boosting**. It tries to minimize a loss function (difference between the real value and output of the learner) using the gradient descent method. Then there is **Ada-Boost** that trains the learners to make strong prediction by focusing on examples harder and harder to classify. This is done by assigning weights to the instances and increasing the one of missclassified one.\n\nThere are others, in Kaggle is very popular XG Boost, but I don't know how they work so I will ignore them even though they generally give better results."},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","outputs":[],"source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier, AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import f1_score"},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","outputs":[],"source":"# classifier list for the normal training set\nclf_list = [DecisionTreeClassifier(max_depth = 3, class_weight = \"balanced\"), \n            RandomForestClassifier(n_estimators = 100, class_weight = \"balanced\"), \n            AdaBoostClassifier(DecisionTreeClassifier(max_depth = 3, class_weight = \"balanced\"),\n                               n_estimators = 100), \n            GradientBoostingClassifier(), \n            XGBClassifier()\n           ]\n# the parameters are set in order to have the same kind of tree every time"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"# use Kfold to evaluate the normal training set\nkf = KFold(n_splits=5,random_state=42,shuffle=True)\n\nmdl = []\nfold = []\nscr = []\n\nfor i,(train_index, test_index) in enumerate(kf.split(df_train)):\n    training = df.iloc[train_index,:]\n    valid = df.iloc[test_index,:]\n    print(i)\n    for clf in clf_list:\n        model = clf.__class__.__name__\n        feats = training[features] #defined above\n        label = training['def_pay']\n        valid_feats = valid[features]\n        valid_label = valid['def_pay']\n        clf.fit(feats,label) \n        pred = clf.predict(valid_feats)\n        score = f1_score(y_true = valid_label, y_pred = pred)\n        fold.append(i+1)\n        scr.append(score)\n        mdl.append(model)\n        print(model)\n    \n#create a small df with the scores\nperformance = pd.DataFrame({'Model': mdl, 'Score':scr,'Fold':fold})\ng_normal = ggplot(performance,aes(x='Fold',y='Score',group = 'Model',color = 'Model')) + geom_point() + geom_line()\nprint(g_normal)"},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","outputs":[],"source":"# classifier list for the downsampled training set\nclf_list = [DecisionTreeClassifier(max_depth = 3), \n            RandomForestClassifier(n_estimators = 100), \n            AdaBoostClassifier(DecisionTreeClassifier(max_depth = 3), n_estimators = 100), \n            GradientBoostingClassifier(), \n            XGBClassifier()\n           ]\n# the parameters are set in order to have the same kind of tree every time\n"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"# use Kfold to evaluate the upsampled training set\nkf = KFold(n_splits=5,random_state=42,shuffle=True)\n\nmdl = []\nfold = []\nscr = []\n\nfor i,(train_index, test_index) in enumerate(kf.split(df_downsampled)):\n    training = df.iloc[train_index,:]\n    valid = df.iloc[test_index,:]\n    print(i)\n    for clf in clf_list:\n        model = clf.__class__.__name__\n        feats = training[features] #defined above\n        label = training['def_pay']\n        valid_feats = valid[features]\n        valid_label = valid['def_pay']\n        clf.fit(feats,label) \n        pred = clf.predict(valid_feats)\n        score = f1_score(y_true = valid_label, y_pred = pred)\n        fold.append(i+1)\n        scr.append(score)\n        mdl.append(model)\n        print(model)\n    \n#create a small df with the scores\nperformance = pd.DataFrame({'Model': mdl, 'Score':scr,'Fold':fold})\ng_downsampled = ggplot(performance,aes(x='Fold',y='Score',group = 'Model',color = 'Model')) + geom_point() + geom_line()\nprint(g_downsampled)"},{"metadata":{},"cell_type":"markdown","source":"I need some help to do it for the SMOTE and the upsampled df's because the indexing is different. Also, these results are confusing for now.\n\nI will focus on the three ensambles I know and tune them"},{"metadata":{},"cell_type":"markdown","source":"### Random Forest with different samples"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"# normal training set\nparam_grid = {'n_estimators': [200, 400, 600, 1000], # It is going to be a long search\n              'criterion': ['entropy', 'gini'],\n              'class_weight' : ['balanced'], 'n_jobs' : [-1]} #use all the computational power you have\nacc_scorer = make_scorer(f1_score)\ngrid_forest = GridSearchCV(RandomForestClassifier(), param_grid, scoring = acc_scorer, cv=5)\n%time grid_forest = grid_forest.fit(X_train, y_train)\nprint(grid_forest.best_estimator_)\nprint(grid_forest.best_score_)\nforest_normal = grid_forest.best_estimator_"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"#cell added because on Kaggle it takes too much time to run\nforest_normal = RandomForestClassifier(bootstrap=True, class_weight='balanced',\n            criterion='entropy', max_depth=None, max_features='auto',\n            max_leaf_nodes=None, min_impurity_decrease=0.0,\n            min_impurity_split=None, min_samples_leaf=1,\n            min_samples_split=2, min_weight_fraction_leaf=0.0,\n            n_estimators=400, n_jobs=-1, oob_score=False,\n            random_state=None, verbose=0, warm_start=False)\nprint(0.449643478486)"},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","outputs":[],"source":"y_upsampled = df_upsampled.def_pay\nX_upsampled = df_upsampled.drop(['def_pay'], axis= 1)"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"# upsampled training set\nparam_grid = {'n_estimators': [200, 400, 600, 1000],\n              'criterion': ['entropy', 'gini'], 'n_jobs' : [-1]}\nacc_scorer = make_scorer(f1_score)\ngrid_forest = GridSearchCV(RandomForestClassifier(), param_grid, scoring = acc_scorer, cv=5)\n%time grid_forest = grid_forest.fit(X_upsampled, y_upsampled)\nprint(grid_forest.best_estimator_)\nprint(grid_forest.best_score_)\nforest_upsampled = grid_forest.best_estimator_"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"#cell added because on Kaggle it takes too much time to run\nforest_upsampled = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=600, n_jobs=-1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\nprint(0.93295704261)"},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","outputs":[],"source":"y_downsampled = df_downsampled.def_pay\nX_downsampled = df_downsampled.drop(['def_pay'], axis = 1)"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"# downsampled training set\nparam_grid = {'n_estimators': [200, 400, 600, 1000],\n              'criterion': ['entropy', 'gini'], 'n_jobs' : [-1]}\nacc_scorer = make_scorer(f1_score)\ngrid_forest = GridSearchCV(RandomForestClassifier(), param_grid, scoring = acc_scorer, cv=5)\n%time grid_forest = grid_forest.fit(X_downsampled, y_downsampled)\nprint(grid_forest.best_estimator_)\nprint(grid_forest.best_score_)\nforest_downsampled = grid_forest.best_estimator_"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"forest_downsampled = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\nprint(0.686692645307)"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"# SMOTE training set\nparam_grid = {'n_estimators': [200, 400, 600, 1000],\n              'criterion': ['entropy', 'gini'], 'n_jobs' : [-1]}\nacc_scorer = make_scorer(f1_score)\ngrid_forest = GridSearchCV(RandomForestClassifier(), param_grid, scoring = acc_scorer, cv=5)\n%time grid_forest = grid_forest.fit(X_SMOTE, y_SMOTE)\nprint(grid_forest.best_estimator_)\nprint(grid_forest.best_score_)\nforest_SMOTE = grid_forest.best_estimator_"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"forest_SMOTE = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=600, n_jobs=-1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\nprint(0.831423121548)"},{"metadata":{},"cell_type":"markdown","source":"The number of estimators is not worth to be searched in this way, I will do some tests later.\n\nI can see how upsampling is helping already, we will see the results on the test later. I can also see how the models less prone to overfitting can use more estimators to get a better result.\n\n### Ada Boost Classifier with different samples\n"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"# normal training set\nparam_grid = {'n_estimators': [200,300],\n              'algorithm': ['SAMME', 'SAMME.R'],\n              'learning_rate' : [0.5, 0.75, 1.0]}\nacc_scorer = make_scorer(f1_score)\ngrid_ada = GridSearchCV(AdaBoostClassifier(DecisionTreeClassifier(class_weight = \"balanced\")), \n                        param_grid, scoring = acc_scorer, cv=5)\n%time grid_ada = grid_ada.fit(X_train, y_train)\nprint(grid_ada.best_estimator_)\nprint(grid_ada.best_score_)\nada_normal = grid_ada.best_estimator_"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"ada_normal = AdaBoostClassifier(algorithm='SAMME',\n          base_estimator=DecisionTreeClassifier(class_weight='balanced', criterion='gini',\n            max_depth=None, max_features=None, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n            splitter='best'),\n          learning_rate=0.75, n_estimators=300, random_state=None)\nprint(0.408435692436)"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"# upsampled training set\nparam_grid = {'n_estimators': [200,300],\n              'algorithm': ['SAMME', 'SAMME.R'],\n              'learning_rate' : [0.5, 0.75, 1.0]}\nacc_scorer = make_scorer(f1_score)\ngrid_ada = GridSearchCV(AdaBoostClassifier(), param_grid, scoring = acc_scorer, cv=5)\n%time grid_ada = grid_ada.fit(X_upsampled, y_upsampled)\nprint(grid_ada.best_estimator_)\nprint(grid_ada.best_score_)\nada_upsampled = grid_ada.best_estimator_"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"ada_upsampled = AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n          learning_rate=1.0, n_estimators=300, random_state=None)\nprint(0.689430792925)"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"# downsampled training set\nparam_grid = {'n_estimators': [200,300],\n              'algorithm': ['SAMME', 'SAMME.R'],\n              'learning_rate' : [0.5, 0.75, 1.0]}\nacc_scorer = make_scorer(f1_score)\ngrid_ada = GridSearchCV(AdaBoostClassifier(), param_grid, scoring = acc_scorer, cv=5)\n%time grid_ada = grid_ada.fit(X_downsampled, y_downsampled)\nprint(grid_ada.best_estimator_)\nprint(grid_ada.best_score_)\nada_downsampled = grid_ada.best_estimator_"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"ada_downsampled = AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n          learning_rate=1.0, n_estimators=200, random_state=None)\nprint(0.673783146613)"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"# SMOTE training set\nparam_grid = {'n_estimators': [200,300],\n              'algorithm': ['SAMME', 'SAMME.R'],\n              'learning_rate' : [0.5, 0.75, 1.0]}\nacc_scorer = make_scorer(f1_score)\ngrid_ada = GridSearchCV(AdaBoostClassifier(), param_grid, scoring = acc_scorer, cv=5)\n%time grid_ada = grid_ada.fit(X_SMOTE, y_SMOTE)\nprint(grid_ada.best_estimator_)\nprint(grid_ada.best_score_)\nada_SMOTE = grid_ada.best_estimator_"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"ada_SMOTE = AdaBoostClassifier(algorithm='SAMME', base_estimator=None, learning_rate=0.5,\n          n_estimators=200, random_state=None)\nprint(0.797835003041)"},{"metadata":{},"cell_type":"markdown","source":"### Gradient Boost with different samples"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"# normal training set\nparam_grid = {'n_estimators': [200,300],\n              'learning_rate' : [0.5, 0.75, 1.0]}\nacc_scorer = make_scorer(f1_score)\ngrid_gbc = GridSearchCV(GradientBoostingClassifier(), param_grid, scoring = acc_scorer, cv=5)\n%time grid_gbc = grid_gbc.fit(X_train, y_train)\nprint(grid_gbc.best_estimator_)\nprint(grid_gbc.best_score_)\ngbc_normal = grid_gbc.best_estimator_"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"gbc_normal = GradientBoostingClassifier(criterion='friedman_mse', init=None,\n              learning_rate=0.5, loss='deviance', max_depth=3,\n              max_features=None, max_leaf_nodes=None,\n              min_impurity_decrease=0.0, min_impurity_split=None,\n              min_samples_leaf=1, min_samples_split=2,\n              min_weight_fraction_leaf=0.0, n_estimators=200,\n              presort='auto', random_state=None, subsample=1.0, verbose=0,\n              warm_start=False)\nprint(0.443888650557)"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"#upsampled training set\nparam_grid = {'n_estimators': [200,300],\n              'learning_rate' : [0.5, 0.75, 1.0]}\nacc_scorer = make_scorer(f1_score)\ngrid_gbc = GridSearchCV(GradientBoostingClassifier(), param_grid, scoring = acc_scorer, cv=5)\n%time grid_gbc = grid_gbc.fit(X_upsampled, y_upsampled)\nprint(grid_gbc.best_estimator_)\nprint(grid_gbc.best_score_)\ngbc_upsampled = grid_gbc.best_estimator_"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"gbc_upsampled = GradientBoostingClassifier(criterion='friedman_mse', init=None,\n              learning_rate=1.0, loss='deviance', max_depth=3,\n              max_features=None, max_leaf_nodes=None,\n              min_impurity_decrease=0.0, min_impurity_split=None,\n              min_samples_leaf=1, min_samples_split=2,\n              min_weight_fraction_leaf=0.0, n_estimators=300,\n              presort='auto', random_state=None, subsample=1.0, verbose=0,\n              warm_start=False)\nprint(0.831025217754)"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"#downsampled training set\nparam_grid = {'n_estimators': [200,300],\n              'learning_rate' : [0.5, 0.75, 1.0]}\nacc_scorer = make_scorer(f1_score)\ngrid_gbc = GridSearchCV(GradientBoostingClassifier(), param_grid, scoring = acc_scorer, cv=5)\n%time grid_gbc = grid_gbc.fit(X_downsampled, y_downsampled)\nprint(grid_gbc.best_estimator_)\nprint(grid_gbc.best_score_)\ngbc_downsampled = grid_gbc.best_estimator_"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"gbc_downsampled = GradientBoostingClassifier(criterion='friedman_mse', init=None,\n              learning_rate=0.5, loss='deviance', max_depth=3,\n              max_features=None, max_leaf_nodes=None,\n              min_impurity_decrease=0.0, min_impurity_split=None,\n              min_samples_leaf=1, min_samples_split=2,\n              min_weight_fraction_leaf=0.0, n_estimators=300,\n              presort='auto', random_state=None, subsample=1.0, verbose=0,\n              warm_start=False)\nprint(0.67183972731)"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"#SMOTE training set\nparam_grid = {'n_estimators': [200,300],\n              'learning_rate' : [0.5, 0.75, 1.0]}\nacc_scorer = make_scorer(f1_score)\ngrid_gbc = GridSearchCV(GradientBoostingClassifier(), param_grid, scoring = acc_scorer, cv=5)\n%time grid_gbc = grid_gbc.fit(X_SMOTE, y_SMOTE)\nprint(grid_gbc.best_estimator_)\nprint(grid_gbc.best_score_)\ngbc_SMOTE = grid_gbc.best_estimator_"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"gbc_SMOTE = GradientBoostingClassifier(criterion='friedman_mse', init=None,\n              learning_rate=0.5, loss='deviance', max_depth=3,\n              max_features=None, max_leaf_nodes=None,\n              min_impurity_decrease=0.0, min_impurity_split=None,\n              min_samples_leaf=1, min_samples_split=2,\n              min_weight_fraction_leaf=0.0, n_estimators=200,\n              presort='auto', random_state=None, subsample=1.0, verbose=0,\n              warm_start=False)\nprint(0.78976849349)"},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"The common result is that the model trained on the normal dataset, which is unbalanced, give a lower f1 score. This also tells me that the a penalized learner (the one with \"balanced\" options) do not perform as well as one that learned on a balanced sample. The upsampled learners seem to be more promising but, as mentioned, they fall into the overfitting trap more often, so we will see how they perform on the test data.\n\n\n\n\n# Training and testing\n\nIt is time to see how well our models perform for real. I will run them all and hopefully see significant differences in how they reach a result.\n\n## Random Forests"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"# Normal sample training\n%time forest_normal.fit(X_train, y_train)\npredictions = forest_normal.predict(X_test)\nprint(\"-------------\")\nprint(\"f1 score: {}\".format(round(f1_score(y_true = y_test, y_pred = predictions),3)))\nprint(\"Accuracy: {}\".format(round(accuracy_score(y_true = y_test, y_pred = predictions),3)))\nprint(\"-------------\")\nprint(get_feature_importance(forest_normal, features))\nprint(\"-------------\")\nTP = np.sum(np.logical_and(predictions == 1, y_test == 1))\nTN = np.sum(np.logical_and(predictions == 0, y_test == 0))\nFP = np.sum(np.logical_and(predictions == 1, y_test == 0))\nFN = np.sum(np.logical_and(predictions == 0, y_test == 1))\npred = len(predictions)\n\nprint('True Positives: {}'.format(TP))\nprint('False Positive: {}'.format(FP))\nprint('True Negative: {}'.format(TN))\nprint('False Negative: {}'.format(FN))\nprint('Precision: {}'.format(round(TP/(TP+FP),2)))\nprint('Recall: {}'.format(round(TP/(TP+FN),2)))\nprint('Problematic ratio: {}'.format(round(FN/(FN+TP),2)))"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"# Upsample training\n%time forest_upsampled.fit(X_upsampled, y_upsampled)\npredictions = forest_upsampled.predict(X_test)\nprint(\"-------------\")\nprint(\"f1 score: {}\".format(round(f1_score(y_true = y_test, y_pred = predictions),3)))\nprint(\"Accuracy: {}\".format(round(accuracy_score(y_true = y_test, y_pred = predictions),3)))\nprint(\"-------------\")\nprint(get_feature_importance(forest_upsampled, features))\nprint(\"-------------\")\nTP = np.sum(np.logical_and(predictions == 1, y_test == 1))\nTN = np.sum(np.logical_and(predictions == 0, y_test == 0))\nFP = np.sum(np.logical_and(predictions == 1, y_test == 0))\nFN = np.sum(np.logical_and(predictions == 0, y_test == 1))\npred = len(predictions)\n\nprint('True Positives: {}'.format(TP))\nprint('False Positive: {}'.format(FP))\nprint('True Negative: {}'.format(TN))\nprint('False Negative: {}'.format(FN))\nprint('Precision: {}'.format(round(TP/(TP+FP),2)))\nprint('Recall: {}'.format(round(TP/(TP+FN),2)))\nprint('Problematic ratio: {}'.format(round(FN/(FN+TP),2)))"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"# Downsample training\n%time forest_downsampled.fit(X_downsampled, y_downsampled)\npredictions = forest_downsampled.predict(X_test)\nprint(\"-------------\")\nprint(\"f1 score: {}\".format(round(f1_score(y_true = y_test, y_pred = predictions),3)))\nprint(\"Accuracy: {}\".format(round(accuracy_score(y_true = y_test, y_pred = predictions),3)))\nprint(\"-------------\")\nprint(get_feature_importance(forest_downsampled, features))\nprint(\"-------------\")\nTP = np.sum(np.logical_and(predictions == 1, y_test == 1))\nTN = np.sum(np.logical_and(predictions == 0, y_test == 0))\nFP = np.sum(np.logical_and(predictions == 1, y_test == 0))\nFN = np.sum(np.logical_and(predictions == 0, y_test == 1))\npred = len(predictions)\n\nprint('True Positives: {}'.format(TP))\nprint('False Positive: {}'.format(FP))\nprint('True Negative: {}'.format(TN))\nprint('False Negative: {}'.format(FN))\nprint('Precision: {}'.format(round(TP/(TP+FP),2)))\nprint('Recall: {}'.format(round(TP/(TP+FN),2)))\nprint('Problematic ratio: {}'.format(round(FN/(FN+TP),2)))"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"# SMOTE training\n%time forest_SMOTE.fit(X_SMOTE, y_SMOTE)\npredictions = forest_SMOTE.predict(X_test)\nprint(\"-------------\")\nprint(\"f1 score: {}\".format(round(f1_score(y_true = y_test, y_pred = predictions),3)))\nprint(\"Accuracy: {}\".format(round(accuracy_score(y_true = y_test, y_pred = predictions),3)))\nprint(\"-------------\")\nprint(get_feature_importance(forest_SMOTE, features))\nprint(\"-------------\")\nTP = np.sum(np.logical_and(predictions == 1, y_test == 1))\nTN = np.sum(np.logical_and(predictions == 0, y_test == 0))\nFP = np.sum(np.logical_and(predictions == 1, y_test == 0))\nFN = np.sum(np.logical_and(predictions == 0, y_test == 1))\npred = len(predictions)\n\nprint('True Positives: {}'.format(TP))\nprint('False Positive: {}'.format(FP))\nprint('True Negative: {}'.format(TN))\nprint('False Negative: {}'.format(FN))\nprint('Precision: {}'.format(round(TP/(TP+FP),2)))\nprint('Recall: {}'.format(round(TP/(TP+FN),2)))\nprint('Problematic ratio: {}'.format(round(FN/(FN+TP),2)))"},{"metadata":{},"cell_type":"markdown","source":"## Ada Boost"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"# Normal sample training\n%time ada_normal.fit(X_train, y_train)\npredictions = ada_normal.predict(X_test)\nprint(\"-------------\")\nprint(\"f1 score: {}\".format(round(f1_score(y_true = y_test, y_pred = predictions),3)))\nprint(\"Accuracy: {}\".format(round(accuracy_score(y_true = y_test, y_pred = predictions),3)))\nprint(\"-------------\")\nprint(get_feature_importance(ada_normal, features))\nprint(\"-------------\")\nTP = np.sum(np.logical_and(predictions == 1, y_test == 1))\nTN = np.sum(np.logical_and(predictions == 0, y_test == 0))\nFP = np.sum(np.logical_and(predictions == 1, y_test == 0))\nFN = np.sum(np.logical_and(predictions == 0, y_test == 1))\npred = len(predictions)\n\nprint('True Positives: {}'.format(TP))\nprint('False Positive: {}'.format(FP))\nprint('True Negative: {}'.format(TN))\nprint('False Negative: {}'.format(FN))\nprint('Precision: {}'.format(round(TP/(TP+FP),2)))\nprint('Recall: {}'.format(round(TP/(TP+FN),2)))\nprint('Problematic ratio: {}'.format(round(FN/(FN+TP),2)))"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"# Upsample training\n%time ada_upsampled.fit(X_upsampled, y_upsampled)\npredictions = ada_upsampled.predict(X_test)\nprint(\"-------------\")\nprint(\"f1 score: {}\".format(round(f1_score(y_true = y_test, y_pred = predictions),3)))\nprint(\"Accuracy: {}\".format(round(accuracy_score(y_true = y_test, y_pred = predictions),3)))\nprint(\"-------------\")\nprint(get_feature_importance(ada_upsampled, features))\nprint(\"-------------\")\nTP = np.sum(np.logical_and(predictions == 1, y_test == 1))\nTN = np.sum(np.logical_and(predictions == 0, y_test == 0))\nFP = np.sum(np.logical_and(predictions == 1, y_test == 0))\nFN = np.sum(np.logical_and(predictions == 0, y_test == 1))\npred = len(predictions)\n\nprint('True Positives: {}'.format(TP))\nprint('False Positive: {}'.format(FP))\nprint('True Negative: {}'.format(TN))\nprint('False Negative: {}'.format(FN))\nprint('Precision: {}'.format(round(TP/(TP+FP),2)))\nprint('Recall: {}'.format(round(TP/(TP+FN),2)))\nprint('Problematic ratio: {}'.format(round(FN/(FN+TP),2)))"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"# Downsample training\n%time ada_downsampled.fit(X_downsampled, y_downsampled)\npredictions = ada_downsampled.predict(X_test)\nprint(\"-------------\")\nprint(\"f1 score: {}\".format(round(f1_score(y_true = y_test, y_pred = predictions),3)))\nprint(\"Accuracy: {}\".format(round(accuracy_score(y_true = y_test, y_pred = predictions),3)))\nprint(\"-------------\")\nprint(get_feature_importance(ada_downsampled, features))\nprint(\"-------------\")\nTP = np.sum(np.logical_and(predictions == 1, y_test == 1))\nTN = np.sum(np.logical_and(predictions == 0, y_test == 0))\nFP = np.sum(np.logical_and(predictions == 1, y_test == 0))\nFN = np.sum(np.logical_and(predictions == 0, y_test == 1))\npred = len(predictions)\n\nprint('True Positives: {}'.format(TP))\nprint('False Positive: {}'.format(FP))\nprint('True Negative: {}'.format(TN))\nprint('False Negative: {}'.format(FN))\nprint('Precision: {}'.format(round(TP/(TP+FP),2)))\nprint('Recall: {}'.format(round(TP/(TP+FN),2)))\nprint('Problematic ratio: {}'.format(round(FN/(FN+TP),2)))"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"# SMOTE training\n%time ada_SMOTE.fit(X_SMOTE, y_SMOTE)\npredictions = ada_SMOTE.predict(X_test)\nprint(\"-------------\")\nprint(\"f1 score: {}\".format(round(f1_score(y_true = y_test, y_pred = predictions),3)))\nprint(\"Accuracy: {}\".format(round(accuracy_score(y_true = y_test, y_pred = predictions),3)))\nprint(\"-------------\")\nprint(get_feature_importance(ada_SMOTE, features))\nprint(\"-------------\")\nTP = np.sum(np.logical_and(predictions == 1, y_test == 1))\nTN = np.sum(np.logical_and(predictions == 0, y_test == 0))\nFP = np.sum(np.logical_and(predictions == 1, y_test == 0))\nFN = np.sum(np.logical_and(predictions == 0, y_test == 1))\npred = len(predictions)\n\nprint('True Positives: {}'.format(TP))\nprint('False Positive: {}'.format(FP))\nprint('True Negative: {}'.format(TN))\nprint('False Negative: {}'.format(FN))\nprint('Precision: {}'.format(round(TP/(TP+FP),2)))\nprint('Recall: {}'.format(round(TP/(TP+FN),2)))\nprint('Problematic ratio: {}'.format(round(FN/(FN+TP),2)))"},{"metadata":{},"cell_type":"markdown","source":"## Gradient Boosting"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"# Normal sample training\n%time gbc_normal.fit(X_train, y_train)\npredictions = gbc_normal.predict(X_test)\nprint(\"-------------\")\nprint(\"f1 score: {}\".format(round(f1_score(y_true = y_test, y_pred = predictions),3)))\nprint(\"Accuracy: {}\".format(round(accuracy_score(y_true = y_test, y_pred = predictions),3)))\nprint(\"-------------\")\nprint(get_feature_importance(gbc_normal, features))\nprint(\"-------------\")\nTP = np.sum(np.logical_and(predictions == 1, y_test == 1))\nTN = np.sum(np.logical_and(predictions == 0, y_test == 0))\nFP = np.sum(np.logical_and(predictions == 1, y_test == 0))\nFN = np.sum(np.logical_and(predictions == 0, y_test == 1))\npred = len(predictions)\n\nprint('True Positives: {}'.format(TP))\nprint('False Positive: {}'.format(FP))\nprint('True Negative: {}'.format(TN))\nprint('False Negative: {}'.format(FN))\nprint('Precision: {}'.format(round(TP/(TP+FP),2)))\nprint('Recall: {}'.format(round(TP/(TP+FN),2)))\nprint('Problematic ratio: {}'.format(round(FN/(FN+TP),2)))"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"# Upsample training\n%time gbc_upsampled.fit(X_upsampled, y_upsampled)\npredictions = gbc_upsampled.predict(X_test)\nprint(\"-------------\")\nprint(\"f1 score: {}\".format(round(f1_score(y_true = y_test, y_pred = predictions),3)))\nprint(\"Accuracy: {}\".format(round(accuracy_score(y_true = y_test, y_pred = predictions),3)))\nprint(\"-------------\")\nprint(get_feature_importance(gbc_upsampled, features))\nprint(\"-------------\")\nTP = np.sum(np.logical_and(predictions == 1, y_test == 1))\nTN = np.sum(np.logical_and(predictions == 0, y_test == 0))\nFP = np.sum(np.logical_and(predictions == 1, y_test == 0))\nFN = np.sum(np.logical_and(predictions == 0, y_test == 1))\npred = len(predictions)\n\nprint('True Positives: {}'.format(TP))\nprint('False Positive: {}'.format(FP))\nprint('True Negative: {}'.format(TN))\nprint('False Negative: {}'.format(FN))\nprint('Precision: {}'.format(round(TP/(TP+FP),2)))\nprint('Recall: {}'.format(round(TP/(TP+FN),2)))\nprint('Problematic ratio: {}'.format(round(FN/(FN+TP),2)))"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"# Downsample training\n%time gbc_downsampled.fit(X_downsampled, y_downsampled)\npredictions = gbc_downsampled.predict(X_test)\nprint(\"-------------\")\nprint(\"f1 score: {}\".format(round(f1_score(y_true = y_test, y_pred = predictions),3)))\nprint(\"Accuracy: {}\".format(round(accuracy_score(y_true = y_test, y_pred = predictions),3)))\nprint(\"-------------\")\nprint(get_feature_importance(gbc_downsampled, features))\nprint(\"-------------\")\nTP = np.sum(np.logical_and(predictions == 1, y_test == 1))\nTN = np.sum(np.logical_and(predictions == 0, y_test == 0))\nFP = np.sum(np.logical_and(predictions == 1, y_test == 0))\nFN = np.sum(np.logical_and(predictions == 0, y_test == 1))\npred = len(predictions)\n\nprint('True Positives: {}'.format(TP))\nprint('False Positive: {}'.format(FP))\nprint('True Negative: {}'.format(TN))\nprint('False Negative: {}'.format(FN))\nprint('Precision: {}'.format(round(TP/(TP+FP),2)))\nprint('Recall: {}'.format(round(TP/(TP+FN),2)))\nprint('Problematic ratio: {}'.format(round(FN/(FN+TP),2)))"},{"metadata":{},"execution_count":null,"cell_type":"code","outputs":[],"source":"# SMOTE training\n%time gbc_SMOTE.fit(X_SMOTE, y_SMOTE)\npredictions = gbc_SMOTE.predict(X_test)\nprint(\"-------------\")\nprint(\"f1 score: {}\".format(round(f1_score(y_true = y_test, y_pred = predictions),3)))\nprint(\"Accuracy: {}\".format(round(accuracy_score(y_true = y_test, y_pred = predictions),3)))\nprint(\"-------------\")\nprint(get_feature_importance(gbc_SMOTE, features))\nprint(\"-------------\")\nTP = np.sum(np.logical_and(predictions == 1, y_test == 1))\nTN = np.sum(np.logical_and(predictions == 0, y_test == 0))\nFP = np.sum(np.logical_and(predictions == 1, y_test == 0))\nFN = np.sum(np.logical_and(predictions == 0, y_test == 1))\npred = len(predictions)\n\nprint('True Positives: {}'.format(TP))\nprint('False Positive: {}'.format(FP))\nprint('True Negative: {}'.format(TN))\nprint('False Negative: {}'.format(FN))\nprint('Precision: {}'.format(round(TP/(TP+FP),2)))\nprint('Recall: {}'.format(round(TP/(TP+FN),2)))\nprint('Problematic ratio: {}'.format(round(FN/(FN+TP),2)))"},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"These results are certainly not good. I can see how some aspects of the predictions were improved by a more scientific way of training the models, but I can't see anyone using them for any practical purpose.\n\nIn spite of using very basic models and not going too deep into how to improve them, I can see that setting a specific goal would have directed this notebook in a more specific direction. For example, if the interest of a bank is to predict the default of its clients, it might be better to use a model less incline to return false negatives. However, if the bank (and it is most likely the case) will then invest resources to prevent the default of its clients, the role of false positives will become more and more relevant.\n\nIn other words, here I have evaluated everything with a generally good indicator (the f1-score) but my goal will very rarely be to get the highest result. Thus it can be a nice thing to define a cost function myself, which would take into account the cost of TP, FP, TN, and FN, and train my models in order to minimize it.\n\nMore importantly, I analyzed and created features without using any domanin knowledge, but rather just what I can define as common sense. This is not the right way of proceeding because it can push me in the wrong direction and then get me very poor results. Just saying.\n\nOn the technical point of view, I hope this is going to be a nice starting point, I know more things than when I started and thus I don't feel it was a complete waste of time. Speaking of which, the grid search was, I feel, a complete waste of time. Now we know.\n\nIf you have any recommendations on how to improve my code or, even better, you have noticed that I have completely overlooked an important aspect of the problem, please let me know."},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","outputs":[],"source":""}]}