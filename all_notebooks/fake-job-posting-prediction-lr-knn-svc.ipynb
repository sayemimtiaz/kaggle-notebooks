{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/real-or-fake-fake-jobposting-prediction/fake_job_postings.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Preprocessing and Cleaning\n\n## 1.1. Missing Values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.isna().sum()*100/df.shape[0])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_drop = ['job_id','title','department','salary_range','company_profile','requirements','benefits']\n\ndf = df.drop(to_drop, axis = 1).sort_index() #drop columns that aren't needed\n\ndf = df.dropna(subset = ['description', 'location'])\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.2. Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn import model_selection, naive_bayes\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stemmer = PorterStemmer() \nstop_words = stopwords.words('english')\n\ndef preprocess(text):\n    text = re.sub('[^a-zA-Z\\s]', '', text) #tokenizatoin\n    text = text.lower() #to lower case\n    split = text.split() #getting rid of stop words and Porter2 stemming\n    for word in split :\n      if word in stop_words :\n        word = ''\n      else :\n        stemmer.stem(word)\n    return ' '.join([word for word in split])\n\ndf['description'] = df['description'].apply(preprocess)\n\ndf['description'].head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.3. Preparation of Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#define the variables\nx = df['description']\ny = df['fraudulent']\n\n#split it into training and test sets\nx_train, x_test, y_train, y_test = model_selection.train_test_split(x, y, test_size=0.2, random_state=42)\n\n#encoding\nencoder = LabelEncoder()\ny_train = encoder.fit_transform(y_train)\ny_test = encoder.fit_transform(y_test)\n\n#vectorization\nMAX = 2000\nvectorizer = TfidfVectorizer(max_features = MAX)\nvectorizer.fit(x_train)\n\nx_trainvec = vectorizer.transform(x_train)\nx_testvec = vectorizer.transform(x_test)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Model\n\n## 2.1. Base Models\n\n### 2.1.1. Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# fit \nlogreg = LogisticRegression()\nlogreg.fit(x_trainvec, y_train)\n\n# predict\ny_pred_lr = logreg.predict(x_testvec)\n\n# accuracy\nprint(\"Accuracy Score of LogReg :\", accuracy_score(y_pred_lr, y_test), \"\\n\") #96.3%\n\n# confusion matrix \nprint(\"Confusion Matrix of LogReg:\\n\", confusion_matrix(y_test, y_pred_lr), \"\\n\") # [[3324, 0], [128, 55]]\n\n#classifcation report\nprint(\"Classification Report of LogReg:\\n\", classification_report(y_test, y_pred_lr), \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.1.2. KNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\"\"\" I COMMENTED THIS OUT SINCE IT TAKES FOREVER, BUT YOU CAN TRY IT ON YOUR OWN IF YOU WANT. K=1 gave me the best score\n\n\nknn = KNeighborsClassifier()\n\ngrid = GridSearchCV(knn, param_grid={'n_neighbors':range(1,40)}, scoring='accuracy')\ngrid.fit(x_trainvec, y_train)\n\ngrid.best_params_\n\nfor i in range(0, len(grid.cv_results_['mean_test_score'])):\n    print('N_Neighbors {}: {} '.format(i+1, grid.cv_results_['mean_test_score'][i]*100))\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn1 = knn = KNeighborsClassifier(n_neighbors = 1)\nknn1.fit(x_trainvec, y_train)\n\ny_pred_knn = knn1.predict(x_testvec)\n\n# accuracy\nprint(\"Accuracy Score of KNN :\", accuracy_score(y_pred_knn, y_test), \"\\n\") #97.9%\n\n# confusion matrix \nprint(\"Confusion Matrix of KNN:\\n\", confusion_matrix(y_test, y_pred_knn), \"\\n\") # [[3314, 10], [61, 122]]\n\n# classifcation report\nprint(\"Classification Report of KNN:\\n\", classification_report(y_test, y_pred_knn), \"\\n\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.1.3. SVC","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvc = SVC(kernel = 'rbf')\nsvc.fit(x_trainvec, y_train)\n\ny_pred_svc = svc.predict(x_testvec)\n\n# accuracy\nprint(\"Accuracy Score of SVC :\", accuracy_score(y_pred_svc, y_test), \"\\n\") #97.3%\n\n# confusion matrix \nprint(\"Confusion Matrix of SVC:\\n\", confusion_matrix(y_test, y_pred_svc), \"\\n\") # [[3324, 0], [96, 87]]\n\n# classifcation report\nprint(\"Classification Report of SVC:\\n\", classification_report(y_test, y_pred_svc), \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2. Comparision of Models and Discussion\n\n### 2.2.1. Sensitivity, Specificity and Accuracy\n\nIf we look at the three models, all of them give us quite high of an accuracy (LR: 96.5%, KNN: 97.6%, SVC: 97.3%). \n\nHowever, this doesn't tell us much. The 'null accuracy' itself is 94.78%. This means that if we simply classify all job ads as real, we will still have a ~95% accuracy. Hence, accuracy cannot be a good way to asssess our models in this case.\n\nLet's look at other metrics.\n\n**Recall/TPF/Sensitivity (False Negative Rate):**\n\nLR: 0.30 (0.70) , KNN: 0.67 (0.33) , SVC: 0.48 (0.52)\n\n**Specificity/TNR (False Positive Rate):**\n\nLR: 1 (0) , KNN: 0.997 (0.003) , SVC: 1 (0)\n\n\nThis is very interesting. Looking at the specificity, we can see that all models perform very well. Logistic Regression and SVC have a perfect TNR which means they haven't incorrectly classified any real job as fake. However, KNN is just a tad bit worse. It has classified approximately 10 real job ads as fake.\n\nOn the other hand, things look a little less rosy when we inspect Sensitivity. KNN outperforms both LR and SVC here with a Sensitivity of 0.67. This means that it accurately classified 67% of all fake job ads as fake (the rest, ostensibly, as real job ads). Hence, all our models give a lot of false negatives. \n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(1 - y_test.mean()) #null accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2.2. ROC Curves and AUC\n\nIf we look at the ROC curves, we can see that KNN clearly outperforms LR and SVC. Hence, if we have to pick one model, we should pick KNN. But it is necessary to remember that KNN's specificity is not perfect. \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score\n\nresults_table = pd.DataFrame(columns = ['models', 'fpr','tpr','auc'])\n\npredictions = {'LR': y_pred_lr, 'KNN': y_pred_knn, 'SVC': y_pred_svc}\n\nfor key in predictions:\n    fpr, tpr, _ = roc_curve(y_test, predictions[key])\n    auc = roc_auc_score(y_test, predictions[key])\n    \n    results_table = results_table.append({'models': key,\n                                         'fpr' : fpr,\n                                         'tpr' : tpr,\n                                         'auc' : auc}, ignore_index=True)\n    \nresults_table.set_index('models', inplace=True)\n\nprint(results_table)\n\nfig = plt.figure(figsize = (8,6))\n\nfor i in results_table.index:\n    plt.plot(results_table.loc[i]['fpr'], \n             results_table.loc[i]['tpr'], \n             label = \"{}, AUC={:.3f}\".format(i, results_table.loc[i]['auc']))\n    \nplt.plot([0,1], [0,1], color = 'black', linestyle = '--')\n\nplt.xticks(np.arange(0.0, 1.1, step=0.1))\nplt.xlabel(\"False Positive Rate\", fontsize=15)\n\nplt.yticks(np.arange(0.0, 1.1, step=0.1))\nplt.ylabel(\"True Positive Rate\", fontsize=15)\n\nplt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\nplt.legend(prop = {'size':13}, loc = 'lower right')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2.3. Implications\n\nThe good thing about all our models is that they all have high Specificity. There are hardly any real jobs being classified as fake (only 10 in KNN's case, otherwise none). This is very important in the practical sense as any job portal wouldn't want to classify a real job posting as a fake one, since it can be damaging for the company. \n\nOn the other hand, the Sensitivity tells us that there is a sizable chunk of fake jobs that are being classified as real ones. This is not so troublesome for the companies but is a nuisance for applicants. A lot of them might unknowingly apply to fake jobs if the job portal wasn't able to correctly classify a posting as fake.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### NOTE\n\nI will extend this notebook by testing a stacked model using the three base models that I've trained and see if it leads to an improvement.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}