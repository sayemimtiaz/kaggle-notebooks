{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1><center>Decision Trees for Binary Classification on Breast Cancer Wisconsin (Diagnostic) Data Set</center></h1>"},{"metadata":{},"cell_type":"markdown","source":"## DataSet \nBreast Cancer Wisconsin (Diagnostic) Data Set (https://www.kaggle.com/uciml/breast-cancer-wisconsin-data)\n\n## Overview\nFeatures are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. n the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n\nAttribute Information:\n\n1) ID number 2) Diagnosis (M = malignant, B = benign) 3-32)\n\nTen real-valued features are computed for each cell nucleus:\n\na) radius (mean of distances from center to points on the perimeter) b) texture (standard deviation of gray-scale values) c) perimeter d) area e) smoothness (local variation in radius lengths) f) compactness (perimeter^2 / area - 1.0) g) concavity (severity of concave portions of the contour) h) concave points (number of concave portions of the contour) i) symmetry j) fractal dimension (\"coastline approximation\" - 1)\n\nThe mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n\nAll feature values are recoded with four significant digits.\n\n## Reference\n1. https://www.kaggle.com/uciml/breast-cancer-wisconsin-data\n1. https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\n1. https://www.coursera.org/learn/machine-learning-with-python/home/welcome\n1. https://scikit-learn.org/stable/modules/tree.html\n1. https://statinfer.com/204-3-10-pruning-a-decision-tree-in-python/\n1. https://www.datacamp.com/community/tutorials/decision-tree-classification-python"},{"metadata":{},"cell_type":"markdown","source":"## Task\n* Apply Decision Tree on the selected dataset\n* Apply two different heuristics for split (Entropy, Gini)\n* Apply pruning as well"},{"metadata":{},"cell_type":"markdown","source":"## Import usefull libraries\n\n* numpy (as np)\n* pandas\n* DecisionTreeClassifier from sklearn.tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics # for score / accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#display input file path\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load dataset\ndataset = pd.read_csv(\"/kaggle/input/breast-cancer-wisconsin-data/data.csv\", delimiter=\",\")\n\n# print dataset\ndataset[:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Select Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"#print all columns (features and lable)\ndataset.columns\n# len(dataset.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Select Features \nX = dataset[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst','perimeter_worst', 'area_worst', 'smoothness_worst','compactness_worst','concavity_worst', 'concave points_worst','symmetry_worst', 'fractal_dimension_worst']].values\nX [0:2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lables\n\n1. **M** = malignant\n1. **B** = benign"},{"metadata":{"trusted":true},"cell_type":"code","source":"#display Unique Lable along with its count\ndataset['diagnosis'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = dataset['diagnosis'] \ny","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split dataset into two part train and test dataset 70% training and 30% testset (Random)\nfrom sklearn.model_selection import train_test_split\nX_trainset, X_testset, y_trainset, y_testset = train_test_split(X, y, test_size=0.3, random_state=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Apply First Heuristic (Entropy)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def MyDecisionTreeClassifier(heuristic, tree_depth = None):\n    decision_tree_clfr = DecisionTreeClassifier(criterion = heuristic, max_depth = tree_depth)\n    \n    #Apply classifier on training dataset\n    decision_tree_clfr.fit(X_trainset, y_trainset)\n    \n    return decision_tree_clfr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heuristic = \"entropy\"\ndecision_tree = MyDecisionTreeClassifier(heuristic)\n\n# predict lables using remaining testset\npredTree = decision_tree.predict(X_testset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Decision Trees's Accuracy using entropy: \", metrics.accuracy_score(y_testset, predTree))\nprint(\"Depth of Decision Tree: \", decision_tree.tree_.max_depth)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualization\nInstall below package for visualization\n1. Install pydotplus \n1. Install python-graphviz"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!conda install -c conda-forge pydotplus -y\n!conda install -c conda-forge python-graphviz -y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.externals.six import StringIO\nimport pydotplus\nimport matplotlib.image as mpimg\nfrom sklearn import tree\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fileName_entropy = \"decision-tree-entropy.png\"\n\ndot_data = StringIO()\nfeatureNames = dataset.columns[2:32]\nlabedNames = dataset[\"diagnosis\"].unique().tolist()\n    \n# export_graphviz will convert decision tree classifier into dot file\ntree.export_graphviz(decision_tree,feature_names = featureNames, out_file = dot_data,\n                         class_names = np.unique(y_trainset), filled = True,  special_characters = True,rotate = False) \n    \n# Convert dot file int pgn using pydotplus\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n    \n#write pgn into file\ngraph.write_png(fileName_entropy)\n\n#display tree image\nimg_entropy = mpimg.imread(fileName_entropy)\nplt.figure(figsize=(100, 200))\nplt.imshow(img_entropy, interpolation='nearest')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Apply Second Heuristic (Gini)"},{"metadata":{"trusted":true},"cell_type":"code","source":"heuristic_g = \"gini\"\ndecision_tree_g = DecisionTreeClassifier(criterion = heuristic_g)\n\n#Apply classifier on train dataset\ndecision_tree_g.fit(X_trainset,y_trainset)\n\n# predict lables using remaining testset\npredTree_g = decision_tree_g.predict(X_testset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Decision Trees's Accuracy using Gini: \", metrics.accuracy_score(y_testset, predTree_g))\nprint(\"Depth of Decision Tree: \", decision_tree_g.tree_.max_depth)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"fileName_g = \"decision-tree-gini.png\"\n\n\ndot_data = StringIO()\nfeatureNames = dataset.columns[2:32]\nlabedNames = dataset[\"diagnosis\"].unique().tolist()\n    \n# export_graphviz will convert decision tree classifier into dot file\ntree.export_graphviz(decision_tree_g,feature_names = featureNames, out_file = dot_data,\n                         class_names = np.unique(y_trainset), filled = True,  special_characters = True,rotate = False) \n    \n# Convert dot file int pgn using pydotplus\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n    \n#write pgn into file\ngraph.write_png(fileName_g)\n\n#display tree image\nimg_g = mpimg.imread(fileName_g)\nplt.figure(figsize=(100, 200))\nplt.imshow(img_g, interpolation='nearest')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pruning to Avoid Overfitting\n\nHow we can avoid Overfitting ?\nOverfitting can be avoided by using these parameters \n* max_leaf_nodes\n* min_samples_leaf\n* max_depth\n\nDescription\n1. max_leaf_nodes: This parameter can be used to define the max number of leaf nodes\n1. min_samples_leaf: This parameter can be userd to restrict the size of sample leaf\n1. max_depth: It can be used to reduce the depth of the tree to build a generalized tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"startingPoint = 2\naccuracy_1 = np.zeros((decision_tree.tree_.max_depth - 1))\n\nfor x in range(startingPoint, decision_tree.tree_.max_depth + 1):\n    heuristic = \"entropy\"\n    decision_tree = MyDecisionTreeClassifier(heuristic)\n\n    # predict lables using remaining testset\n    predTree = decision_tree.predict(X_testset)\n    \n    accuracy_1 [x-startingPoint] = metrics.accuracy_score(y_testset, predTree)\n    \n    print(\"Decision Trees's Accuracy (entropy) with depth:\", x , \" is \", accuracy_1 [x-startingPoint],\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ndef ShowAccuracy(_range, data):\n    plt.plot(range(2,_range+1),data,'g')\n    plt.legend(('Accuracy'))\n    plt.ylabel('Accuracy ')\n    plt.xlabel('Depth')\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ShowAccuracy(decision_tree.tree_.max_depth, accuracy_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"startingPoint = 2\ndepth = np.zeros((decision_tree_g.tree_.max_depth - 1))\naccuracy_2 = np.zeros((decision_tree_g.tree_.max_depth - 1))\n\nfor x in range(startingPoint, decision_tree_g.tree_.max_depth + 1):\n    heuristic = \"gini\"\n    decision_tree = MyDecisionTreeClassifier(heuristic)\n\n    # predict lables using remaining testset\n    predTree = decision_tree.predict(X_testset)\n    \n    depth [x-startingPoint] = x;\n    accuracy_2 [x-startingPoint] = metrics.accuracy_score(y_testset, predTree)\n    \n    print(\"Decision Trees's Accuracy (Gini) with depth:\",x, \" is \", accuracy_2 [x-startingPoint],\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ShowAccuracy(decision_tree_g.tree_.max_depth, accuracy_2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pruning Results\n\n* in case of first heuristic the best accuracy was 0.96 when tree depth was 4\n* In case of second heuristic the best accuracy was 0.95 when tre depth was 7"},{"metadata":{},"cell_type":"markdown","source":"## Conclusion"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}