{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"os.chdir(\"../input\")\nos.listdir()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Read"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../input/fetalhr/CTG.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from warnings import filterwarnings\nfilterwarnings('ignore')\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop unnecessaries\ndf=df.drop([\"FileName\",\"Date\",\"SegFile\",\"b\",\"e\",\"A\", \"B\",\"C\", \"D\" ,\"E\", \"AD\", \"DE\" ,\"LD\", \"FS\", \"SUSP\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Coloumns names\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# process to delete all nan data\ndf = df.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This dataset can be used for both 10-class classifications and 3-class classifications.\n# Selecting the data to be used for the 3-class model\nX=df.drop([\"NSP\",\"CLASS\"],axis=1)\n\ny=df[\"NSP\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nsp_classes = y.unique()\nnsp_classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import utils as np_utils\nfrom sklearn.preprocessing import LabelEncoder\n# Encode class values as integers and perform one-hot-encoding\nencoder = LabelEncoder()\nencoder.fit(y)\ny = encoder.transform(y)\ny = np_utils.to_categorical(y)\nprint(y)\n\ny.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Standardization with Standard Scaler\nfrom sklearn.preprocessing import StandardScaler\nScaler=StandardScaler()\nX=Scaler.fit_transform(X)\n\nX[0:3]\n\nX.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# shuffle and split training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n                                                    random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Multi Layer Perceptron Artificial Neural Network\nfrom sklearn.neural_network import MLPClassifier \n\n# Setting up a primitive (non-validated) model\nmlpc = MLPClassifier(random_state = 0)# ANN model object created\n\nmlpc.fit(X_train, y_train) # ANN model object fit\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Forecasting on the Unvalidated Model\ny_pred = mlpc.predict(X_test) # model prediction process over test set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn.metrics as metrics\n\n# Accuracy\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test,y_pred))\n\n# f1 score\n\nprint(\"f1_weighted:\",metrics.f1_score(y_test, y_pred,average='weighted'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Grid Search Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross Validation Process\n# Parameters for CV created in dictionary structure\n# INFORMATION ABOUT THE INPUTED PARAMETERS\n# alpha: float, default = 0.0001 L2 penalty (regularization term) parameter. (penalty parameter)\n   \nmlpc_params = {\"alpha\": [0.1, 0.01, 0.0001],\n              \"hidden_layer_sizes\": [(10,10,10),\n                                     (100,100,100),\n                                     (100,100)],\n              \"solver\" : [\"lbfgs\",\"adam\",\"sgd\"],\n              \"activation\": [\"relu\",\"logistic\"]}\n\nfrom sklearn.model_selection import GridSearchCV\n\n\n\n\nmlpc = MLPClassifier(random_state = 0) # ANN model object created\n\n# Model CV process \nmlpc_cv_model = GridSearchCV(mlpc, mlpc_params, \n                         cv = 5, # To make a 5-fold CV\n                         n_jobs = -1, # Number of jobs to be run in parallel (-1: means to use all processors)\n                         verbose = 2) # Controls the level of detail: higher means more messages gets value as integer.\n\nmlpc_cv_model.fit(X_train, y_train) \n\n\n# The best parameter obtained as a result of CV process\n\nprint(\"The best parameters: \" + str(mlpc_cv_model.best_params_))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Tuning\n# Setting the Final Model with the best parameter\n\nmlpc_tuned = mlpc_cv_model.best_estimator_\n\n# Fitting Final Model\nmlpc_tuned.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# K-fold f1_weighted\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n# K fold\nkf = KFold(shuffle=True, n_splits=5) # To make a 5-fold CV\n\ncv_results_kfold = cross_val_score(mlpc_tuned, X_test, np.argmax(y_test, axis=1), cv=kf, scoring= 'f1_weighted')\n\nprint(\"K-fold Cross Validation f1_weigted Results: \",cv_results_kfold)\nprint(\"K-fold Cross Validation f1_weigted Results Mean: \",cv_results_kfold.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# K-fold accuracy\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n# K fold\nkf = KFold(shuffle=True, n_splits=5) # To make a 5-fold CV\n\ncv_results_kfold = cross_val_score(mlpc_tuned, X_test, np.argmax(y_test, axis=1), cv=kf, scoring= 'accuracy')\n\nprint(\"K-fold Cross Validation accuracy Results: \",cv_results_kfold)\nprint(\"K-fold Cross Validation accuracy Results Mean: \",cv_results_kfold.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tune Model Prediction\n# Prediction process of Final Model over test set\ny_pred = mlpc_tuned.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy and f1_weighted value of Final Model\n\n# %% f1 score\nimport sklearn.metrics as metrics\nprint(\"f1_weighted:\",metrics.f1_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1),average='weighted'))\n\n# %% Accuracy\n\nprint(\"accuracy:\",metrics.accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% Confusion Matrix and Classification Report\nfrom sklearn.metrics import confusion_matrix, classification_report \n\n# Classification Report\nmodel_report = classification_report(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\nprint(model_report)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix\n# multilabel-indicator is not supported so np.argmax should be used!\nmodel_conf = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\nprint(model_conf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% ROC-AUC Curve\n\ny_score = mlpc_tuned.predict_proba(X_test)\n\nfrom scipy import interp\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\n# Learn to predict each class against the other\n\n\nn_classes = 3 # number of class\n\n\n\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The process of drawing a roc-auc curve belonging to a specific class\n\nplt.figure()\nlw = 2 # line_width\nplt.plot(fpr[2], tpr[2], color='darkorange',\n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2]) # Drawing Curve according to 2. class \nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC EÄŸrisi')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Process of plotting roc-auc curve belonging to all classes.\n\nfrom itertools import cycle\n\n# First aggregate all false positive rates\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n# Then interpolate all ROC curves at this points\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(n_classes):\n    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\n# Finally average it and compute AUC\nmean_tpr /= n_classes\n\nfpr[\"macro\"] = all_fpr\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n# Plot all ROC curves\nplt.figure()\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='micro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"micro\"]),\n         color='deeppink', linestyle=':', linewidth=4)\n\nplt.plot(fpr[\"macro\"], tpr[\"macro\"],\n         label='macro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"macro\"]),\n         color='navy', linestyle=':', linewidth=4)\n\ncolors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\nfor i, color in zip(range(n_classes), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n             label='ROC curve of class {0} (area = {1:0.2f})'\n             ''.format(i, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--', lw=lw)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Extending the ROC Curve to Multi-Class')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}