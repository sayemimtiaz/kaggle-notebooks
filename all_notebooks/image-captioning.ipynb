{"cells":[{"metadata":{},"cell_type":"markdown","source":"## data preperation"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\ndata_paths=[]\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        data_paths.append(os.path.join(dirname, filename))\nprint(data_paths[0],data_paths[1])\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom os import listdir\nfrom pickle import dump\nfrom keras.applications.vgg16 import VGG16\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.models import Model\nimport string\ndef extract_features(directory):\n\t# load the model\n\tmodel = VGG16()\n\t# re-structure the model\n\tmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n\t# summarize\n\tprint(model.summary())\n\t# extract features from each photo\n\tfeatures = dict()\n\tfor name in listdir(directory):\n\t\t# load an image from file\n\t\tfilename = directory + '/' + name\n\t\timage = load_img(filename, target_size=(224, 224))\n\t\t# convert the image pixels to a numpy array\n\t\timage = img_to_array(image)\n\t\t# reshape data for the model\n\t\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n\t\t# prepare the image for the VGG model\n\t\timage = preprocess_input(image)\n\t\t# get features\n\t\tfeature = model.predict(image, verbose=0)\n\t\t# get image id\n\t\timage_id = name.split('.')[0]\n\t\t# store feature\n\t\tfeatures[image_id] = feature\n\t\t#print('>%s' % name)\n\treturn features\n\n\n\n \ndef embedds(wordtoidx,vocab_size):\n        embeddings_index = {} \n        f = open('../input/glove6b300dtxt/glove.6B.300d.txt', encoding=\"utf-8\")\n        for line in f:\n            values = line.split()\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype='float32')\n            embeddings_index[word] = coefs\n        embedding_dim = 300\n        embedding_matrix = np.zeros((vocab_size, embedding_dim))\n        for word, i in wordtoidx.items():\n                embedding_vector = embeddings_index.get(word)\n                if embedding_vector is not None:\n                    embedding_matrix[i] = embedding_vector\n        return  embedding_matrix  \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature extraction  &  Defining a model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract descriptions for images\ndef load_descriptions(doc):\n\tmapping = dict()\n\t# process lines\n\tfor line in doc.split('\\n'):\n\t\t# split line by white space\n\t\ttokens = line.split()\n\t\tif len(line) < 2:\n\t\t\tcontinue\n\t\t# take the first token as the image id, the rest as the description\n\t\timage_id, image_desc = tokens[0], tokens[1:]\n\t\t# remove filename from image id\n\t\timage_id = image_id.split('.')[0]\n\t\t# convert description tokens back to string\n\t\timage_desc = ' '.join(image_desc)\n\t\t# create the list if needed\n\t\tif image_id not in mapping:\n\t\t\tmapping[image_id] = list()\n\t\t# store description\n\t\tmapping[image_id].append(image_desc)\n\treturn mapping\n \n# load doc into memory\ndef load_doc(filename):\n\t# open the file as read only\n\tfile = open(filename, 'r')\n\t# read all text\n\ttext = file.read()\n\t# close the file\n\tfile.close()\n\treturn text\ndef load_set(filename):\n\tdoc = load_doc(filename)\n\tdataset = list()\n\t# process line by line\n\tfor line in doc.split('\\n')[1:]:\n\t\t# skip empty lines\n\t\tif len(line) < 1:\n\t\t\tcontinue\n\t\tif line=='image,caption':\n\t\t\tcontinue\n\t\t# get the image identifier\n\t\tidentifier = line.split('.')[0]\n\t\tdataset.append(identifier)\n\treturn set(dataset)\n\n# load clean descriptions into memory\ndef load_clean_descriptions(filename, dataset):\n\t# load document\n\tdoc = load_doc(filename)\n\tdescriptions = dict()\n\tfor line in doc.split('\\n'):\n\t\t# split line by white space\n\t\ttokens = line.split()\n\t\t# split id from description\n\t\timage_id, image_desc = tokens[0], tokens[1:]\n\t\t# skip images not in the set\n\t\tif image_id in dataset:\n\t\t\t# create list\n\t\t\tif image_id not in descriptions:\n\t\t\t\tdescriptions[image_id] = list()\n\t\t\t# wrap description in tokens\n\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n\t\t\t# store\n\t\t\tdescriptions[image_id].append(desc)\n\treturn descriptions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef clean_descriptions(descriptions):\n\t# prepare translation table for removing punctuation\n\ttable = str.maketrans('', '', string.punctuation)\n\tfor key, desc_list in descriptions.items():\n\t\tfor i in range(len(desc_list)):\n\t\t\tdesc = desc_list[i]\n\t\t\t# tokenize\n\t\t\tdesc = desc.split()\n\t\t\t# convert to lower case\n\t\t\tdesc = [word.lower() for word in desc]\n\t\t\t# remove punctuation from each token\n\t\t\tdesc = [w.translate(table) for w in desc]\n\t\t\t# remove hanging 's' and 'a'\n\t\t\tdesc = [word for word in desc if len(word)>1]\n\t\t\t# remove tokens with numbers in them\n\t\t\tdesc = [word for word in desc if word.isalpha()]\n\t\t\t# store as string\n\t\t\tdesc_list[i] =  ' '.join(desc)\n\n# convert the loaded descriptions into a vocabulary of words\ndef to_vocabulary(descriptions):\n\t# build a list of all description strings\n\tall_desc = set()\n\tfor key in descriptions.keys():\n\t\t[all_desc.update(d.split()) for d in descriptions[key]]\n\treturn all_desc\n\n# save descriptions to file, one per line\ndef save_descriptions(descriptions, filename):\n\tlines = list()\n\tfor key, desc_list in descriptions.items():\n\t\tfor desc in desc_list:\n\t\t\tlines.append(key + ' ' + desc)\n\tdata = '\\n'.join(lines)\n\tfile = open(filename, 'w')\n\tfile.write(data)\n\tfile.close()\n    # load photo features\ndef load_photo_features(filename, dataset):\n\t# load all features\n\tall_features = load(open(filename, 'rb'))\n\t# filter features\n\tfeatures = {k: all_features[k] for k in dataset}\n\treturn features\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract features from all images\ndirectory = '../input/flickr8kimagescaptions/flickr8k/images'\nfeatures = extract_features(directory)\nprint('Extracted Features: %d' % len(features))\n# save to file\ndump(features, open('features.pkl', 'wb'))\nfilename = '../input/flickr8kimagescaptions/flickr8k/captions.txt'\n# load descriptions\ndoc = load_doc(filename)\n# parse descriptions\ndescriptions = load_descriptions(doc)\nprint('Loaded: %d ' % len(descriptions))\n# clean descriptions\nclean_descriptions(descriptions)\n# summarize vocabulary\nvocabulary = to_vocabulary(descriptions)\nprint('Vocabulary Size: %d' % len(vocabulary))\n# save to file\nsave_descriptions(descriptions, 'descriptions.txt')    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom pickle import load\n# load training dataset (6K)\nfilename = '../input/flickr8kimagescaptions/flickr8k/captions.txt'\ntrain = load_set(filename)\nprint('Dataset: %d' % len(train))\ntest=set(list(train)[7091:])\ntrain=set(list(train)[:7091])\n# descriptions\ntest_descriptions = load_clean_descriptions('descriptions.txt', test)\nprint('Descriptions: test=%d' % len(test_descriptions))\n# photo features\ntest_features = load_photo_features('features.pkl', test)\n#print('Photos: test=%d' % len(test_features))\n\n# descriptions\ntrain_descriptions = load_clean_descriptions('descriptions.txt', train)\nprint('Descriptions: train=%d' % len(train_descriptions))\n# photo features\ntrain_features = load_photo_features('features.pkl', train)\n#print('Photos: train=%d' % len(train_features))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert a dictionary of clean descriptions to a list of descriptions\ndef to_lines(descriptions):\n\tall_desc = list()\n\tfor key in descriptions.keys():\n\t\t[all_desc.append(d) for d in descriptions[key]]\n\treturn all_desc\n \n# fit a tokenizer given caption descriptions\ndef create_tokenizer(descriptions):\n\tlines = to_lines(descriptions)\n\ttokenizer = Tokenizer()\n\ttokenizer.fit_on_texts(lines)\n\treturn tokenizer\n \n# prepare tokenizer\ntokenizer = create_tokenizer(train_descriptions)\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary Size: %d' % vocab_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dump(tokenizer, open('tokenizer.pkl', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import to_categorical\n\n# calculate the length of the description with the most words\ndef max_length(descriptions):\n\tlines = to_lines(descriptions)\n\treturn max(len(d.split()) for d in lines)\n# create sequences of images, input sequences and output words for an image\ndef create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n\tX1, X2, y = list(), list(), list()\n\t# walk through each description for the image\n\tfor desc in desc_list:\n\t\t# encode the sequence\n\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n\t\t# split one sequence into multiple X,y pairs\n\t\tfor i in range(1, len(seq)):\n\t\t\t# split into input and output pair\n\t\t\tin_seq, out_seq = seq[:i], seq[i]\n\t\t\t# pad input sequence\n\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n\t\t\t# encode output sequence\n\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n\t\t\t# store\n\t\t\tX1.append(photo)\n\t\t\tX2.append(in_seq)\n\t\t\ty.append(out_seq)\n\treturn np.array(X1), np.array(X2), np.array(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_length=max_length(descriptions)\nwordtoidx=tokenizer.word_index\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(max_length)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix=embedds(wordtoidx ,vocab_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.layers  import Input,Dense,Flatten,LSTM, GRU, Bidirectional, Embedding,add,Dropout,Conv2D\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\ndef caption_model(vocab_size,max_length):\n            embds=Embedding(vocab_size,300,mask_zero=True,weights=[embedding_matrix])\n            image_inputs=Input(shape=(4096,))\n            reg0=Dropout(0.5)(image_inputs)\n            x=Dense(300,'relu')(reg0)\n            text_input=Input(shape=(max_length,))\n            emb=embds(text_input)\n            reg1=Dropout(0.5)(emb)\n            seq0=GRU(300)(reg1,x)\n            seq1=add([x,seq0])\n            decoder0=Dense(300,'relu')(seq1)\n            decoder1=Dense(300,'relu')(decoder0)\n            decoder2=Dense(300,'relu')(decoder1)\n            decoder3=Dense(vocab_size,'softmax')(decoder2)\n            model=Model(inputs=[image_inputs,text_input],outputs=decoder3)\n            print(model.summary())\n            model.compile( optimizer='adam', loss='categorical_crossentropy',metrics='acc')\n            return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=caption_model(vocab_size,max_length)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data generator, intended to be used in a call to model.fit_generator()\ndef data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n\t# loop for ever over images\n\twhile 1:\n\t\tfor key, desc_list in descriptions.items():\n\t\t\t# retrieve the photo feature\n\t\t\tphoto = photos[key][0]\n\t\t\tin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n\t\t\tyield [in_img, in_seq], out_word","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# test the data generator\ngenerator = data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size)\ninputs, outputs = next(generator)\nprint(inputs[0].shape)\nprint(inputs[1].shape)\nprint(outputs.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train the model, run epochs manually and save after each epoch\nepochs = 20\nsteps = len(train_descriptions)\nfor i in range(epochs):\n\t# create the data generator\n\tgenerator = data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size)\n\t# fit for one epoch\n\tmodel.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=2)\n\t# save model\n\tmodel.save('model_' + str(i) + '.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# map an integer to a word\ndef word_for_id(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n             if index == integer:\n                     return word\n    return None\n \n# generate a description for an image\ndef generate_desc(model, tokenizer, photo, max_length):\n    # seed the generation process\n    in_text = 'startseq'\n     # iterate over the whole length of the sequence\n    for i in range(max_length):\n        # integer encode input sequence\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        # pad input\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        # predict next word\n        yhat = model.predict([photo,sequence], verbose=1)\n        # convert probability to integer\n        yhat = np.argmax(yhat)\n        # map integer to word\n        word = word_for_id(yhat, tokenizer)\n        # stop if we cannot map the word\n        if word is None:\n            break\n             # append as input for generating the next word\n        in_text += ' ' + word\n        # stop if we predict the end of the sequence\n        if word == 'endseq':\n            break\n    return in_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\n# evaluate the skill of the model\ndef evaluate_model(model, descriptions, photos, tokenizer, max_length):\n    actual, predicted = list(), list()\n    # step over the whole set\n    for key, desc_list in descriptions.items():\n        # generate description\n        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n        # store actual and predicted\n        references = [d.split() for d in desc_list]\n        actual.append(references)\n        predicted.append(yhat.split())\n    # calculate BLEU score\n    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_model(model, test_descriptions, test_features, tokenizer, 31)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generate_desc(model, tokenizer, pic, 31)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}