{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div>\n    <img src='https://storage.googleapis.com/kaggle-datasets-images/568973/1032238/7ff23ec0b526773506bd5964d4f100d1/dataset-cover.jpg' />\n</div>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\n\nimport torch\nfrom torch import optim\nfrom torch import nn\n\nimport torchaudio\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\nfrom sklearn.utils import shuffle\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id=\"dataset\" style=\"color:#c7ced6; background:#dc9231; border:0.5px dotted;\"> \n    <center>Dataset\n        <a class=\"anchor-link\" href=\"#dataset\" target=\"_self\">¶</a>\n    </center>\n</h1>","metadata":{}},{"cell_type":"code","source":"class MusicDS(Dataset):\n    def __init__(self, path):\n        labels = os.listdir(path)\n        self.idx_to_labels = {k:v for k,v in enumerate(labels)}\n        self.labels_to_idx = {v:k for k,v in enumerate(labels)}\n        \n        songs_lists = [os.listdir(path + l) for l in labels]\n        songs_lists = [list(map(list, zip([path + labels[i] + '/' for a in range(len(sl))],sl))) for i,sl in enumerate(songs_lists)]\n        labels = np.array([[s[0].split('/')[-2] for s in l] for l in songs_lists])\n        labels = labels.reshape(labels.shape[0] * labels.shape[1])\n        \n        songs_lists = np.array([[s[0] + s[1] for s in l] for l in songs_lists])\n        songs_lists = songs_lists.reshape(songs_lists.shape[0] * songs_lists.shape[1])\n        self.labels, self.songs_lists = shuffle(labels, songs_lists)\n        \n    def plot_specgram(self, waveform, sample_rate, title=\"Spectrogram\", xlim=None):\n        waveform = waveform.numpy()\n\n        num_channels, num_frames = waveform.shape\n        time_axis = torch.arange(0, num_frames) / sample_rate\n\n        figure, axes = plt.subplots(num_channels, 1)\n        if num_channels == 1:\n            axes = [axes]\n        for c in range(num_channels):\n            axes[c].specgram(waveform[c], Fs=sample_rate)\n            if num_channels > 1:\n                axes[c].set_ylabel(f'Channel {c+1}')\n            if xlim:\n                axes[c].set_xlim(xlim)\n        figure.suptitle(title)\n        plt.show(block=False)\n        \n    def print_stats(self, waveform, sample_rate=None, src=None):\n        if src:\n            print(\"-\" * 10)\n            print(\"Source:\", src)\n            print(\"-\" * 10)\n        if sample_rate:\n            print(\"Sample Rate:\", sample_rate)\n        print(\"Shape:\", tuple(waveform.shape))\n        print(\"Dtype:\", waveform.dtype)\n        print(f\" - Max:     {waveform.max().item():6.3f}\")\n        print(f\" - Min:     {waveform.min().item():6.3f}\")\n        print(f\" - Mean:    {waveform.mean().item():6.3f}\")\n        print(f\" - Std Dev: {waveform.std().item():6.3f}\")\n        print()\n        print(waveform)\n        print()\n        \n    def get_sample(self, path, sample_rate=4000):\n        effects = [\n          [\"lowpass\", \"-1\", \"150\"], # apply single-pole lowpass filter\n          [\"speed\", \"0.9\"],  # reduce the speed\n                             # This only changes sample rate, so it is necessary to\n                             # add `rate` effect with original sample rate after this.\n          [\"rate\", f\"{sample_rate}\"],\n          [\"reverb\", \"-w\"],  # Reverbration gives some dramatic feeling\n        ]\n        return torchaudio.sox_effects.apply_effects_file(path, effects=effects)\n    \n    def __len__(self):\n        return len(self.songs_lists)\n    \n    def __getitem__(self, idx):\n        song_path = self.songs_lists[idx]\n        try:\n            waveform, frame_num = self.get_sample(song_path)\n        except:\n            idx += 1\n            song_path = self.songs_lists[idx]\n            waveform, frame_num = self.get_sample(song_path)\n        waveform = torch.unsqueeze(waveform, 0)\n        waveform = F.interpolate(waveform, size=(300134))\n        waveform = torch.squeeze(waveform, 0)\n        return waveform, frame_num, self.labels_to_idx[self.labels[idx]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '../input/gtzan-dataset-music-genre-classification/Data/genres_original/'\n\nmusic_ds = MusicDS(path)\nwaveform, frame_num, label = music_ds[100]\nmusic_ds.plot_specgram(waveform, frame_num)\nmusic_ds.print_stats(waveform, frame_num)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_size = int(len(music_ds) * 0.2)\ntrain_size = int(len(music_ds) - test_size)\n\ntrain_dataset, test_dataset = torch.utils.data.random_split(music_ds, [train_size, test_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=4, num_workers=0, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=1, num_workers=0, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id=\"network\" style=\"color:#c7ced6; background:#dc9231; border:0.5px dotted;\"> \n    <center>Network\n        <a class=\"anchor-link\" href=\"#network\" target=\"_self\">¶</a>\n    </center>\n</h1>","metadata":{}},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self, n_input=2, n_output=10, stride=16, n_channel=32):\n        super().__init__()\n        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n        self.bn1 = nn.BatchNorm1d(n_channel)\n        self.pool1 = nn.MaxPool1d(4)\n        self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n        self.bn2 = nn.BatchNorm1d(n_channel)\n        self.pool2 = nn.MaxPool1d(4)\n        self.conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n        self.bn3 = nn.BatchNorm1d(2 * n_channel)\n        self.pool3 = nn.MaxPool1d(4)\n        self.conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n        self.bn4 = nn.BatchNorm1d(2 * n_channel)\n        self.pool4 = nn.MaxPool1d(4)\n        self.fc1 = nn.Linear(2 * n_channel, n_output)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(self.bn1(x))\n        x = self.pool1(x)\n        x = self.conv2(x)\n        x = F.relu(self.bn2(x))\n        x = self.pool2(x)\n        x = self.conv3(x)\n        x = F.relu(self.bn3(x))\n        x = self.pool3(x)\n        x = self.conv4(x)\n        x = F.relu(self.bn4(x))\n        x = self.pool4(x)\n        x = F.avg_pool1d(x, x.shape[-1])\n        x = x.permute(0, 2, 1)\n        x = self.fc1(x)\n        return F.log_softmax(x, dim=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nnet = Net().to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id=\"training\" style=\"color:#c7ced6; background:#dc9231; border:0.5px dotted;\"> \n    <center>Training\n        <a class=\"anchor-link\" href=\"#training\" target=\"_self\">¶</a>\n    </center>\n</h1>","metadata":{}},{"cell_type":"code","source":"optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=0.0001)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, epoch):\n\n    losses = []\n    for batch_idx, (data, num, target) in enumerate(train_loader):\n\n        data = data.to(device)\n        target = target.to(device)\n\n        output = model(data).to(device)\n\n        loss = F.nll_loss(output.squeeze(), target)\n        losses.append(loss.item())\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n    return losses","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def number_of_correct(pred, target):\n    return pred.squeeze().eq(target).sum().item()\n\n\ndef get_likely_index(tensor):\n    return tensor.argmax(dim=-1)\n\n\ndef test(model, epoch):\n    model.eval()\n    correct = 0\n\n    for data, num, target in test_loader:\n\n        data = data.to(device)\n        target = target.to(device)\n\n        output = model(data).to(device)\n\n        pred = get_likely_index(output)\n        correct += number_of_correct(pred, target)\n\n    accuracy = 100. * correct / len(test_loader.dataset)\n    return accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_epoch = 80\n\nlosses = []\naccuracies = []\n\nfor epoch in range(1, n_epoch + 1):\n    loss = train(net, epoch)\n    losses.append(sum(loss) / len(loss))\n\n    accuracy = test(net, epoch)\n    accuracies.append(accuracy)\n    scheduler.step()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id=\"analysis\" style=\"color:#c7ced6; background:#dc9231; border:0.5px dotted;\"> \n    <center>Analysis\n        <a class=\"anchor-link\" href=\"#analysis\" target=\"_self\">¶</a>\n    </center>\n</h1>","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(14,8))\nplt.title('Accuracies')\nplt.plot(accuracies)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14,8))\nplt.title('Losses')\nplt.plot(losses)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]}]}