{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Semantic Question Answering System\n\n---\n\n> What is this? In this notebook, I attempt to build/put-together a question answering system using the latest state-of-the-art libraries and models available today.\n\n# Objective\n\n- `Problem`  - **Return the most needed content (as opposed to all content).**\n- `Solution` - *Build a system that allows a user to ask questions on the literature and receive the most needed content (e.g., the content/answer cannot be the size of the entire article!).*\n\n\nTo add to the problem of information overload, we cannot pass a full article to the question answering model as this will be painfully slow. The code below is an attempt to solve this and make a query to output fast for humans.\n\n> Overview/Summary of the QA System\n\n<img src=\"https://github.com/amoux/corona/blob/master/src/img/SemanticQuestionAnsweringSystem.png?raw=true\"/>\n\n- Steps:\n\n    - build the cord-19 dataset:\n        - apply pre-processing and normalization to raw-texts.\n        - transform (tokenize) raw-texts from n articles to sentences.\n \n    - build the embedding store (similar to a DB but optimized for similarity search):\n        - encode sentences to embeddings.\n        \n    - other:\n        - extract questions from the dataset (papers).\n        - apply the same steps used for the sentences to the questions.\n        - build a terminology graph of the questions extracted.\n        \n    - final:\n        - build the question answering engine.\n        - query the questions extracted from the dataset.\n\n---\n\n## Is too much information a problem?\n\n> The following summary can help us understand the question.\n\n- The output comes from the exact model applied in this notebook. Also, note that it is indeed related to the problem stated previously.\n    - Question extracted from article: [Evaluation and mechanism for outcomes exploration of providing public health care in contract service in Rural China: a multiple-case study with complex adaptive systems design](https://bmcpublichealth.biomedcentral.com/articles/10.1186/s12889-015-1540-9)\n\n        - **question**: *What would be the value to the user of federating search results from many discovery tools?*\n\n        - **answer**  : `satisfaction and engagement`\n\n        - **context** : *`To what degree would an optimal searching environment enhance the satisfaction and engagement of existing users? How can we better understand how our discovery tools are being used and assess whether we are returning the most needed content (as opposed to all content)? Likewise, participation in knowledge-generating cases, whether direct or vicarious, seems integral to learning or appreciating the nature of scientific research. The central coordination of this global DoD surveillance system afforded multiple opportunities for enhanced utilization of partner capabilities, as well as concise information sharing with other DoD organizations and external agencies (Table 2). e. g., sharing and promoting one's work, perpetuation of bias by discovery systems) They permit structured searches and comparison of data in different clearinghouses and give the user adequate information to find data and use it in an appropriate context [107].`*\n\n# About\n\n- **Live Application**:\n    - I additionally put together a simple web-app that shows how the source-code and models implemented in this notebook can be used in an application setting.\n        - application data based on:\n            * dataset : `2020-04-24`\n            * subsets : `comm_use_subset, noncomm_use_subset, biorxiv_medrxiv`\n            * papers : `14,565`\n            * text-source : `body_text`\n            * embeddings/sentences: `2,569,779`\n        \n    - WebApp URL : [COVID-19 Semantic Question Answering System](http://corona-nlp.ngrok.io.ngrok.io/?fbclid=IwAR2h4wYcxXN00dEO-wZlisQzQO-MInla8Po98ZhyZBuPDBTdlout4_sQ9aE)\n    \n\n- **Transformer Models**:\n    - base model for both models below : `allenai/scibert_scivocab_uncased`\n\n        - `scibert_nli` (uncased) : First fine-tuned on the `AllNLI dataset`, then on train set of `STS benchmark` for sentence embeddings.\n            - NOTE: This model is currently available only from my Google Drive and can only be used with the `sentence_transformers` library.\n   \n        - `scibert_nli_squad` (uncased) : Previously finetuned on `AllNLI dataset` then, on the `SQUAD 2.0 dataset` for question answering task.\n            - Available for download on Huggingface's website. : [Model: amoux/scibert_nli_squad](https://huggingface.co/amoux/scibert_nli_squad)\n            \n            \n > Enough of talking, lets build this thing!\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!\"/opt/conda/bin/python3.7\" -m pip install --upgrade pip\n!pip install googledrivedownloader\n!pip install -U transformers\n!pip install -U --no-deps sentence_transformers\n!pip install bert-extractive-summarizer\n!pip install scikit_learn\n\n# IT REALLY SUCKS THIS MODEL COULD NOT BE INSTALLED! :(\n# !pip install -U scispacy\n# !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_sm-0.2.4.tar.gz\n\n!pip install -U spacy thinc\n!python -m spacy download en_core_web_sm\n!conda install faiss-cpu -c pytorch --yes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import concurrent.futures\nfrom multiprocessing import cpu_count\nimport functools\nimport json\nimport pickle\nimport random\nimport re\nfrom collections import Counter\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom string import punctuation\nfrom typing import (IO, Any, Dict, Callable, Iterator,\n                    List, Sequence, Tuple, Union)\n\nimport faiss\nimport numpy as np\nimport spacy\nimport torch\nfrom google_drive_downloader import GoogleDriveDownloader\nfrom nltk.tokenize import word_tokenize\nfrom sentence_transformers import SentenceTransformer\nfrom spacy.lang.en import English\nfrom spacy.tokens.span import Span\nfrom summarizer import Summarizer\nfrom tqdm.auto import tqdm\nfrom transformers import (BertConfig, BertForQuestionAnswering, BertModel,\n                          BertTokenizer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Download the Sentence Encoder Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"file_id = \"1Qm7EL7eOsSgB66v5Zn_n-nAhR2OXo0UW\"\nfilepath = \"models/scibert-nli.zip\"\ngdrive = GoogleDriveDownloader()\ngdrive.download_file_from_google_drive(file_id,\n                                       dest_path=filepath, unzip=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Source Code\n\n> The following source code was implemented uniquely for the CORD-19 Dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize_whitespace(string: str) -> str:\n    \"\"\"Normalize excessive whitespace.\"\"\"\n    linebreak = re.compile(r\"(\\r\\n|[\\n\\v])+\")\n    nonebreaking_space = re.compile(r\"[^\\S\\n\\v]+\", flags=re.UNICODE)\n    return nonebreaking_space.sub(\" \", linebreak.sub(r\"\\n\", string)).strip()\n\n\ndef clean_punctuation(text: str) -> str:\n    punct = re.compile(\"[{}]\".format(re.escape(punctuation)))\n    tokens = word_tokenize(text)\n    text = \" \".join(filter(lambda t: punct.sub(\"\", t), tokens))\n    return normalize_whitespace(text)\n\n\ndef clean_tokenization(sequence: str) -> str:\n    \"\"\"Clean up spaces before punctuations and abbreviated forms.\"\"\"\n    return (\n        sequence.replace(\" .\", \".\")\n        .replace(\" ?\", \"?\")\n        .replace(\" !\", \"!\")\n        .replace(\" ,\", \",\")\n        .replace(\" ' \", \"'\")\n        .replace(\" n't\", \"n't\")\n        .replace(\" 'm\", \"'m\")\n        .replace(\" do not\", \" don't\")\n        .replace(\" 's\", \"'s\")\n        .replace(\" 've\", \"'ve\")\n        .replace(\" 're\", \"'re\")\n        .replace(\" / \", \"/\")\n        .replace(\" )\", \")\")\n        .replace(\"( \", \"(\")\n        .replace(\"[ \", \"[\")\n        .replace(\" ]\", \"]\")\n        .replace(\" ;\", \";\")\n        .replace(\" - \", \"-\")\n    )\n\n\nclass DataIO:\n    @staticmethod\n    def save_data(file_path: str, data_obj: Any) -> IO:\n        file_path = Path(file_path)\n        if file_path.is_dir():\n            if not file_path.exists():\n                file_path.mkdir(parents=True)\n        with file_path.open(\"wb\") as pkl:\n            pickle.dump(data_obj, pkl, pickle.HIGHEST_PROTOCOL)\n\n    @staticmethod\n    def load_data(file_path: str) -> Any:\n        file_path = Path(file_path)\n        with file_path.open(\"rb\") as pkl:\n            return pickle.load(pkl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class PaperIndexer:\n    def __init__(self, source: Union[str, List[str]],\n                 index_start=1, sort_first=False, extension=\".json\"):\n        self.index_start = index_start\n        self.extension = extension\n        self.is_files_sorted = sort_first\n        self._bins: List[int] = []\n        self.paths: List[Path] = []\n        self.paper_index: Dict[str, int] = {}\n        self.index_paper: Dict[int, str] = {}\n        if not isinstance(source, list):\n            source = [source]\n        file_paths = []\n        for path in source:\n            path = Path(path)\n            if path.is_dir():\n                files = [file for file in path.glob(f\"*{extension}\")]\n                if sort_first:\n                    files.sort()\n                file_paths.extend(files)\n                self.paths.append(path)\n                self._bins.append(len(files))\n            else:\n                raise ValueError(f\"Path, {path} directory not found.\")\n        self._map_files_to_ids(file_paths)\n\n    @property\n    def num_papers(self):\n        return len(self.index_paper)\n\n    @property\n    def source_name(self):\n        if len(self.paths) == 1:\n            return self.paths[0].name\n        return [p.name for p in self.paths]\n\n    def _map_files_to_ids(self, json_files: List[str]) -> None:\n        for index, file in enumerate(json_files, self.index_start):\n            paper_id = file.name.replace(self.extension, \"\")\n            if paper_id not in self.paper_index:\n                self.paper_index[paper_id] = index\n                self.index_paper[index] = paper_id\n\n    def _index_dirpath(self, index: int) -> Path:\n        if index <= self._bins[0]:\n            return self.paths[0]\n        else:\n            size = 0\n            for i in range(len(self._bins)):\n                size += self._bins[i]\n                if index <= size:\n                    return self.paths[i]\n\n    def _load_data(self, paper_id: str):\n        path = self._index_dirpath(self.paper_index[paper_id])\n        file_path = path.joinpath(f\"{paper_id}{self.extension}\")\n        with file_path.open(\"rb\") as file:\n            return json.load(file)\n\n    def _encode(self, paper_ids: List[str]) -> List[int]:\n        pid2idx = self.paper_index\n        return [pid2idx[pid] for pid in paper_ids if pid in pid2idx]\n\n    def _decode(self, indices: List[int]) -> List[str]:\n        idx2pid = self.index_paper\n        return [idx2pid[idx] for idx in indices if idx in idx2pid]\n\n    def load_paper(self, index: int = None, paper_id: str = None):\n        \"\"\"Load a single paper and data by either index or paper ID.\"\"\"\n        if index is not None:\n            paper = self.load_papers([index], None)\n        elif paper_id is not None:\n            paper = self.load_papers(None, [paper_id])\n        return paper[0]\n\n    def load_papers(self, indices: List[int] = None, paper_ids: List[str] = None):\n        \"\"\"Load many papers and data by either indices or paper ID's.\"\"\"\n        if indices is not None:\n            if isinstance(indices, list) and isinstance(indices[0], int):\n                paper_ids = self._decode(indices)\n                return [self._load_data(pid) for pid in paper_ids]\n            else:\n                raise ValueError(\"Indices not of type List[int].\")\n\n        elif paper_ids is not None:\n            if isinstance(paper_ids, list) and isinstance(paper_ids[0], str):\n                return [self._load_data(pid) for pid in paper_ids]\n            else:\n                raise ValueError(\"Paper ID's not of type List[str].\")\n\n    def __getitem__(self, item):\n        if isinstance(item, int):\n            return self.index_paper[item]\n        elif isinstance(item, str):\n            return self.paper_index[item]\n\n    def __len__(self):\n        return self.num_papers\n\n    def __repr__(self):\n        return \"PaperIndexer(papers={}, files_sorted={}, source={})\".format(\n            self.num_papers, self.is_files_sorted, self.source_name)\n\n\n@dataclass\nclass Sentences:\n    indices: List[int] = field(default_factory=list, repr=False)\n    counts: int = 0\n    maxlen: int = 0\n    strlen: int = 0\n\n    def init_cluster(self) -> Dict[int, List[str]]:\n        return dict([(index, []) for index in self.indices])\n\n    def __len__(self):\n        return self.counts\n\n\n@dataclass\nclass Papers:\n    sentences: Sentences = field(repr=False)\n    cluster: Dict[int, List[str]] = field(repr=False)\n    avg_strlen: float = field(init=False)\n    num_papers: int = field(init=False)\n    num_sents: int = field(init=False)\n    _meta: List[Tuple[int, int]] = field(init=False, repr=False)\n\n    def __post_init__(self):\n        if isinstance(self.sentences, Sentences):\n            for key, val in self.sentences.__dict__.items():\n                setattr(self, key, val)\n        self.avg_strlen = round(self.strlen / self.counts, 2)\n        self.num_papers = len(self.indices)\n        self.num_sents = self.counts\n        self._meta = list(self._edges())\n\n    def _edges(self):\n        for i in self.indices:\n            for j in range(0, len(self.cluster[i])):\n                yield (i, j)\n\n    def string(self, sent_id: int) -> str:\n        \"\"\"Retrive a single string from a sentence ID.\n\n        * Same as `self[sent_id]`\n        \"\"\"\n        return self[sent_id]\n\n    def lookup(self, sent_ids: List[int]) -> List[Dict[str, int]]:\n        locs = []\n        for i in sent_ids:\n            node, item = self._meta[i]\n            locs.append({\"sent_id\": i, \"paper_id\": node,\n                         \"loc\": (node, item)})\n        return locs\n\n    def sents(self, paper_id: int) -> List[str]:\n        \"\"\"Retrive all sentences belonging to the given paper ID.\"\"\"\n        return self.cluster[paper_id]\n\n    def to_disk(self, path: str):\n        \"\"\"Save the current state to a directory.\"\"\"\n        DataIO.save_data(path, self)\n\n    @staticmethod\n    def from_disk(path: str):\n        \"\"\"Load the state from a directory.\"\"\"\n        return DataIO.load_data(path)\n\n    def __len__(self):\n        return self.num_sents\n\n    def __getitem__(self, item):\n        node, item = self._meta[item]\n        return self.cluster[node][item]\n\n    def __iter__(self):\n        for index in self.cluster:\n            for sentence in self.cluster[index]:\n                yield sentence\n\n\ndef merge_papers(papers: List[Papers]) -> Papers:\n    \"\"\"Merge a list of instances of Papers into one.\"\"\"\n    if isinstance(papers, list):\n        if not isinstance(papers[0], Papers):\n            raise TypeError(\"Expected a List[Papers], but found \"\n                            f\"a List[{type(papers[0])}] instead.\")\n    i = Sentences()\n    c = i.init_cluster()\n    for p in papers:\n        i.strlen += p.strlen\n        i.counts += p.counts\n        i.maxlen = max(i.maxlen, p.maxlen)\n        i.indices.extend(p.indices)\n        c.update(p.cluster)\n    return Papers(i, c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def frequency_summarizer(text: Union[str, List[str]],\n                         topk=7, min_tokens=30, nlp=None) -> str:\n    \"\"\"Frequency Based Summarization.\n\n    :param text: sequences of strings or an iterable of string sequences.\n    :param topk: number of topmost leading scored sentences.\n    :param min_tokens: minimum number of tokens to consider in a sentence.\n    \"\"\"\n    if nlp is None:\n        nlp = spacy.load(\"en_core_web_sm\")\n\n    doc = nlp(\" \".join(text) if isinstance(text, list) else text)\n\n    vocab = {}\n    for token in doc:\n        if not token.is_stop and not token.is_punct:\n            if token.text not in vocab:\n                vocab[token.text] = 1\n            else:\n                vocab[token.text] += 1\n\n    for word in vocab:\n        vocab[word] = vocab[word] / max(vocab.values())\n\n    score = {}\n    for sent in doc.sents:\n        for token in sent:\n            if len(sent) > min_tokens:\n                continue\n            if token.text in vocab:\n                if sent not in score:\n                    score[sent] = vocab[token.text]\n                else:\n                    score[sent] += vocab[token.text]\n\n    nlargest = sorted(score, key=score.get, reverse=True)[:topk]\n    summary = \" \".join([sent.text for sent in nlargest])\n    return summary\n\n\ndef common_tokens(texts: List[str], minlen=3, nlp=None,\n                  pos_tags=(\"NOUN\", \"ADJ\", \"VERB\", \"ADV\",)):\n    \"\"\"Top Common Tokens (removes stopwords and punctuation).\n\n    :param texts: iterable of string sequences.\n    :param minlen: dismiss tokens with a minimum length.\n    :param nlp: use an existing spacy language instance.\n    :param pos_tags: lemmatize tokens based on part-of-speech tags.\n    \"\"\"\n    common = {}\n    if nlp is None:\n        nlp = spacy.load(\"en_core_web_sm\")\n\n    for doc in nlp.pipe(texts):\n        tokens = []\n        for token in doc:\n            if token.is_stop:\n                continue\n            if token.pos_ in pos_tags:\n                tokens.append(token.lemma_)\n            else:\n                tokens.append(token.text)\n\n        text = \" \".join(tokens)\n        text = clean_punctuation(text)\n        for token in word_tokenize(text):\n            if len(token) < minlen:\n                continue\n            if token not in common:\n                common[token] = 1\n            else:\n                common[token] += 1\n\n    common = sorted(common.items(),\n                    key=lambda k: k[1], reverse=True)\n    return common\n\n\ndef extract_questions(papers: Papers, min_length=30, sentence_ids=False):\n    \"\"\"Extract questions from an instance of papers.\n\n    :param min_length: minimum length of a question to consider.\n    :param sentence_ids: whether to return the decoded ids `paper[index]`.\n    \"\"\"\n    interrogative = ['how', 'why', 'when',\n                     'where', 'what', 'whom', 'whose']\n    sents = []\n    ids = []\n    for index in tqdm(range(len(papers)), desc='sentences'):\n        string = papers[index]\n        if len(string) < min_length:\n            continue\n        toks = string.lower().split()\n        if toks[0] in interrogative and toks[-1].endswith(\"?\"):\n            sents.append(string)\n            ids.append(index)\n\n    questions = list(set(sents))\n    print(f'found {len(questions)} interrogative questions.')\n\n    if not sentence_ids:\n        return questions\n    return questions, ids\n\n\nclass SpacySentenceTokenizer:\n    def __init__(\n        self,\n        nlp_model=\"en_core_web_sm\",\n        disable=[\"ner\", \"tagger\"],\n        max_length=2_000_000,\n    ):\n        \"\"\"Spacy Sentence Tokenizer.\n\n        :params nlp_model: spaCy model to use for the tokenizer.\n        :params disable: name of spaCy's pipeline components to disable.\n        \"\"\"\n        self.nlp_model = nlp_model\n        self.disable = disable\n        self.max_length = max_length\n\n    @property\n    def cache(self):\n        info = self.nlp.cache_info()\n        if info.hits:\n            return info.hits\n\n    @functools.lru_cache()\n    def nlp(self) -> List[English]:\n        nlp_ = spacy.load(self.nlp_model, disable=self.disable)\n        nlp_.max_length = self.max_length\n        return nlp_\n\n    def tokenize(self, doc: str) -> List[Span]:\n        \"\"\"Tokenize to sentences from a string of sequences to sentences.\"\"\"\n        doc = self.nlp()(doc)\n        return list(doc.sents)\n\n    def __repr__(self):\n        model, pipe = self.nlp_model, self.disable\n        return f\"<SpacySentenceTokenizer({model}, disable={pipe})>\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CORD19Dataset(PaperIndexer):\n    def __init__(\n            self,\n            source: Union[str, List[str]],\n            text_keys: Tuple[str] = (\"abstract\", \"body_text\",),\n            index_start: int = 1,\n            sort_first: bool = False,\n            nlp_model: str = \"en_core_web_sm\",\n            sentence_tokenizer: Callable = None,\n    ):\n        super(CORD19Dataset, self).__init__(source, index_start, sort_first)\n        self.text_keys = text_keys\n        self.sentence_tokenizer = sentence_tokenizer\n        if sentence_tokenizer is not None:\n            if not hasattr(sentence_tokenizer, 'tokenize'):\n                raise AttributeError(f'Callable[{sentence_tokenizer.__name__}]'\n                                     ' missing ``self.tokenize()`` attribute.')\n        else:\n            self.sentence_tokenizer = SpacySentenceTokenizer(nlp_model)\n\n    def sample(self, k: int = None, seed: int = None) -> List[int]:\n        \"\"\"Return all or k iterable of paper-id to index mappings.\n        `k`: A sample from all available papers use `k=-1`. Otherwise, pass\n            `k=n` number of indices to load from the available dataset files.\n        \"\"\"\n        random.seed(seed)\n        indices = list(self.index_paper.keys())\n        if k == -1:\n            return indices\n        assert k <= self.num_papers\n        return random.sample(indices, k=k)\n\n    def title(self, index: int = None, paper_id: str = None) -> str:\n        return self.load_paper(index, paper_id)[\"metadata\"][\"title\"]\n\n    def titles(self, indices: List[int] = None,\n               paper_ids: List[str] = None) -> Iterator:\n        for paper in self.load_papers(indices, paper_ids):\n            yield paper[\"metadata\"][\"title\"]\n\n    def docs(self, indices: List[int] = None,\n             paper_ids: List[str] = None, suffix=\"\\n\") -> Iterator:\n        for paper in self.load_papers(indices, paper_ids):\n            doc = []\n            for key in self.text_keys:\n                for line in paper[key]:\n                    doc.append(line[\"text\"])\n            yield suffix.join(doc)\n\n    def lines(self, indices: List[int] = None,\n              paper_ids: List[str] = None) -> Iterator:\n        for paper in self.load_papers(indices, paper_ids):\n            for key in self.text_keys:\n                for line in paper[key]:\n                    yield line[\"text\"]\n\n    def build(self, indices: List[int], minlen: int = 20) -> Papers:\n        \"\"\"Return an instance of papers with texts transformed to sentences.\"\"\"\n        index = Sentences(indices)\n        cluster = index.init_cluster()\n        docs = self.docs(indices)\n\n        for paper in cluster:\n            for line in self.sentence_tokenizer.tokenize(next(docs)):\n                string = normalize_whitespace(line.text)\n                string = clean_tokenization(string)\n                length = len(string)\n                if length <= minlen:\n                    continue\n                if string not in cluster[paper]:\n                    index.strlen += length\n                    index.counts += 1\n                    index.maxlen = max(index.maxlen, length)\n                    cluster[paper].append(string)\n\n        return Papers(index, cluster=cluster)\n\n    def batch(self, indices: List[int], minlen=20, workers=None) -> Papers:\n        maxsize = len(indices)\n        workers = cpu_count() if workers is None else workers\n\n        jobs = []\n        for i in range(0, maxsize, workers):\n            tasks = indices[i: min(i + workers, maxsize)]\n            jobs.append(tasks)\n\n        with tqdm(total=maxsize, desc=\"papers\") as pbar:\n            batch_: List[Papers] = []\n            with concurrent.futures.ThreadPoolExecutor(workers) as pool:\n                future_to_ids = {\n                    pool.submit(self.build, job, minlen): job for job in jobs\n                }\n                for future in concurrent.futures.as_completed(future_to_ids):\n                    ids = future_to_ids[future]\n                    try:\n                        papers = future.result()\n                    except Exception as e:\n                        print(f\"{ids} generated an exception: {e}\")\n                        raise\n                    else:\n                        batch_.append(papers)\n                        pbar.update(len(ids))\n\n        return merge_papers(batch_)\n\n    def __repr__(self):\n        return \"CORD19Dataset(papers={}, files_sorted={}, source={})\".format(\n            self.num_papers, self.is_files_sorted, self.source_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CORD19Dataset\n\n> The `CORD19Dataset` class handles loading the content of single or many papers by `int: index` or `str:paper_id.` This input style makes it easier and eliminates the need to load articles via paths explicitly.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"source = \"/kaggle/input/CORD-19-research-challenge/document_parses/pdf_json/\"\ncord19 = CORD19Dataset(source=source,\n                       text_keys=(\"body_text\",),\n                       sort_first=True,\n                       nlp_model=\"en_core_web_sm\")\nprint(cord19)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# general view of how to access items via index/paper_id\n\npaper_id = cord19[100]\ntitle = cord19.title(None, paper_id)\nlines = cord19.lines(None, [paper_id])\n\nprint(f'paper       : index=100 <-> id={paper_id}.json')\nprint(f'paper-title : {title}')\nprint(f'paper-lines : {len(list(lines))}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Papers to Sentences\n\n> The following method `cord19.batch()` employs the subsequent pre-processing steps: *skips duplicates normalizes syntax and tokenizes texts to sentences* for `4,500 papers` and yields about `~970,000` sentences. Additionally, the method utilizes multithreading for faster batching. Though, since we do need a GPU environment (for encoding sentences to embeddings), despite multithreading, batch speeds are slow with limited resources. On the other hand, if you have a machine with `8-cores` and `SSD` - the equivalent number of samples (4500 papers) takes around `14 minutes`.\n\n\n## Error when running batch()\n\n- **If you got an error when running the cell below**:\n    - Unfortunately this is a known bug related to multithreading in spaCy's ml library `thinc`. If you see this error please click on `Cancel Run` in the toolbar above - make sure its not running and re-run the cell with `SHIFT + ENTER` it should work after,\n    \n[Issue on Github](https://github.com/explosion/spaCy/issues/4349)\n\n- Error log:\n\n```bash\nUndefined operator: >>\n    Called by (<thinc.neural._classes.function_layer.FunctionLayer object at 0x7fee2769ccd0>,\n    <thinc.neural._classes.feed_forward.FeedForward object at 0x7fee300f7d10>)\n  Available:\n  [1;38;5;4mTraceback:[0m\n  â”œâ”€ [1mfrom_disk[0m in /opt/conda/lib/python3.7/site-packages/spacy/util.py:654\n  â”œâ”€â”€â”€ [1m<lambda>[0m in /opt/conda/lib/python3.7/site-packages/spacy/language.py:936\n  â””â”€â”€â”€â”€â”€ [1mTok2Vec[0m in /opt/conda/lib/python3.7/site-packages/spacy/_ml.py:323\n    [38;5;1m     >>>[0m return _legacy_tok2vec.Tok2Vec(width, embed_size, **kwargs)\n```","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Tasks\n\n> First lets get all papers matching the tasks","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the sentence encoder\nencoder = SentenceTransformer('models/scibert-nli')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TASKS = [\n    'Effectiveness of drugs being developed and tried to treat COVID-19 patients.',\n    ('Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such '\n     'as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication.'),\n    'Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.',\n    'Exploration of use of best animal models and their predictive value for a human vaccine.',\n    ('Capabilities to discover a therapeutic (not vaccine) for the disease, and '\n     'clinical effectiveness studies to discover therapeutics, to include antiviral agents.'),\n    ('Alternative models to aid decision makers in determining how to prioritize and distribute scarce, '\n     'newly proven therapeutics as production ramps up. This could include identifying approaches '\n     'for expanding production capacity to ensure equitable and timely distribution to populations in need.'),\n    'Efforts targeted at a universal coronavirus vaccine.',\n    'Efforts to develop animal models and standardize challenge studies.',\n    'Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers.',\n    'Approaches to evaluate risk for enhanced disease after vaccination.',\n    ('Assays to evaluate vaccine immune response and process development for vaccines, '\n     'alongside suitable animal models in conjunction with therapeutics.')\n]\n\n# (-1) loads all indexed paper ids\nsample = cord19.sample(-1)\npaper_id_to_title = {}\nfor paper_id in tqdm(sample, desc='titles'):\n    title = cord19.title(paper_id)\n    title = normalize_whitespace(title)\n    if len(title) <= 10:\n        continue\n    if paper_id not in paper_id_to_title:\n        paper_id_to_title[paper_id] = title\n\npaper_id_to_index = dict(enumerate(paper_id_to_title.keys()))\ntitles = list(paper_id_to_title.values())\n\n# encode the titles (db) and tasks (queries)\ntitles_embed = np.array(\n    encoder.encode(titles, show_progress_bar=False))\ntasks_embed = np.array(\n    encoder.encode(TASKS, show_progress_bar=False))\n\ntopk = 410  # we want 410 neighbors for each centroid\nndim = titles_embed.shape[1]\nindex = faiss.IndexFlat(ndim)\nindex.add(titles_embed)\n\n# query the topmost similar neighbors to the queries\nD, I = index.search(tasks_embed, topk)\nI","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"goal_size = 4500  # we want 4,500 papers for the dataset\ngold_ids = []\nfor i in I.flatten().tolist():\n    paper_id = paper_id_to_index[i]\n    gold_ids.append(paper_id)\n\ngold_ids = sorted(set(gold_ids))\nntotal = len(gold_ids)\nprint('number of uniques :', ntotal)\n\nextra_ids = []\nif ntotal < goal_size:\n    needs = goal_size - ntotal\n    count = 0\n    for need_id in sample:\n        if need_id in gold_ids:\n            continue\n        if count < needs:\n            extra_ids.append(need_id)\n            count += 1\n            \n    assert len(extra_ids)+len(gold_ids) == goal_size\n    print(f'goal needed {len(extra_ids)} extra number of ids!')\n\n# small test case (make sure the ids are in sync)\ngold_id = gold_ids[10]\ngold_title = cord19.title(gold_id)\nprint(f'id    : {gold_id}')\nprint(f'title : {gold_title}')\nprint(f'match : {paper_id_to_title[gold_id]}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = []\nsample.extend(gold_ids)\nsample.extend(extra_ids)\nsample.sort()\n\npapers = cord19.batch(sample, minlen=25)\nprint(papers)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **Warning** - Since we only needed the following objects to build the sample of papers ids - we can now delete them to free-up RAM.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"del paper_id_to_title, paper_id_to_index, titles, titles_embed, tasks_embed, index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Structure Overview","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# access sentences via index (like a list)\n\nsent_ids = []\nfor i in range(5):\n    x = random.randint(i, papers.num_sents)\n    sentence = papers[x]\n    sent_ids.append(x)\n    print(f\"{x}:\\t{sentence[:90]}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# it is also possible to retrive titles by sentence ids\n\nfor x in papers.lookup(sent_ids):\n    title = cord19.title(x['paper_id'])\n    print(f'{x[\"sent_id\"]}:\\t{title[:70]}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# keep in mind only sentences are kept in memory, e.g.,\n# titles are loaded from disk (since it's a less common action)\n\nmaxids = 10\nfor index in papers.indices[:maxids]:  # iterate over the indexed papers ids\n    sents = len(papers.sents(index))   # retrive all sentences for a single paper/article\n    title = cord19.title(index)        # retrive the title for the paper/article\n    paper_id = cord19[index]           # decode the index id back to a string (paper/article file-id)\n    \n    print(f'paper_id: {paper_id}, num_sents: {sents}\\n* {title[:90]}\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sentences to Embeddings","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# encode the sentences to embeddings:\nembedding = np.asarray(\n    encoder.encode(papers, show_progress_bar=True)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert embedding.shape[0] == len(papers)\nprint('shape :', embedding.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Faiss\n\n> Faiss is a library for fast and efficient search and clustering of embeddings\n\nPlease refer to the project for more information. [faiss github repo](https://github.com/facebookresearch/faiss)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nlist = 10 # centroids\nnbyte = 32\nn_dim = embedding.shape[1]\nquantizer = faiss.IndexHNSWFlat(n_dim, nbyte)\nindex_ivf = faiss.IndexIVFFlat(quantizer, n_dim, nlist, faiss.METRIC_L2)\nindex_ivf.verbose = True\nif not index_ivf.is_trained:\n    index_ivf.train(embedding)\nif index_ivf.ntotal == 0:\n    index_ivf.add(embedding)\nassert index_ivf.ntotal == embedding.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Saving","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = Path('data')\nif not data_dir.exists(): data_dir.mkdir()\nsents_file = data_dir.joinpath(f'sents_{papers.num_papers}.pkl')\n# embed_file = data_dir.joinpath(f'embed_{papers.num_papers}.npy')\nindex_file = data_dir.joinpath(f'index_{papers.num_papers}.index')\n\n# save the db\npapers.to_disk(sents_file)\n# np.save(embed_file, embedding)  # saving the embedding requires +HDD\nfaiss.write_index(index_ivf, index_file.as_posix())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **Warning** - We have everything we need and saved to disk - delete these objects to free-up RAM.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"del embedding, index_ivf, cord19","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset Questions\n\n> In the following code we'll extract questions from the literature, sort them based on similarity and grouped them via clustering (KNN). We'll use `faiss.Kmeans` indexer for this task.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# get all the questions from the instance of papers:\nquestions = extract_questions(papers, min_length=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encode the questions to embeddings\nembedding = np.asarray(\n    encoder.encode(questions, show_progress_bar=True)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nless = 1  # remove additional questions (optional)\nnlist = 10  # n centroids fluctuates on random sampling partitions (papers)\nniter = 20\nitems = len(questions)\n\n# topk : top neighbors per centroid\ntopk = (items // nlist) - (nlist * nless)\nndim = embedding.shape[1]\n\n# build kmeans\nkmeans = faiss.Kmeans(ndim, nlist, niter=niter, verbose=True)\nkmeans.train(embedding)\n\n# finally, build the indexer\nindex = faiss.IndexFlat(ndim)\nindex.add(embedding)\nD, I = index.search(kmeans.centroids, topk)\n\n# \"sorting\" the questions in relation to k-nn scores\ncluster = [[] for _ in range(I.shape[0])]\nfor k in range(I.shape[0]):\n    for nn in I[k]:\n        cluster[k].append(questions[nn])\n\nprint(f'(centroids, neighbors) : {I.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nn = I.shape[1]\ncats = {}\nfor k in range(I.shape[0]):\n    toks = common_tokens(cluster[k])\n    ents = toks[:nn -1 if nn % 2 else nn]\n    if k not in cats:\n        cats[k] = ents\n\n# preview the results\nfor k in cats:\n    category = cats[k][0]\n    entities = cats[k][1:6]\n    print(f\"{category}\\t-> {entities}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Questions Terminology Graph","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import graphviz as graphviz\n\npairs = 2\nedges = []\nfor cat in cats:\n    common = cats[cat]\n    maxlen = len(common)\n    for i in range(0, maxlen, pairs):\n        x = common[i: min(i + pairs, maxlen)]\n        nodes, k = zip(*x)\n        edges.append(nodes)\n\n# build the questions graph\ngraph = graphviz.Digraph()\nfor tail, head in edges:\n    graph.edge(tail, head)\n    \ngraph  # visualize how the questions relate in terms of entitites","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"graph.render('/kaggle/working/questions-graph-table.gv', view=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"questions = {}\nfor k in cats:\n    # we'll use the topmost (1st) token for \n    # each cluster as the \"master\" entity\n    label = cats[k][0][0]\n    if label not in questions:\n        questions[label] = cluster[k]\n    else:  # join groups with same label (if any)\n        questions[label].extend(cluster[k])\n\n# finally save the questions to use below!\nDataIO.save_data(data_dir.joinpath('k-questions.plk'),\n                 questions)\n\n# we now have a good collection of\n# questions we can use with the QA model!\nprint('topics :', questions.keys())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **Warning** - We have everything we need and saved to disk - delete these objects to free-up RAM.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"del index, quantizer, kmeans, papers, embedding, questions, cluster","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Question Answering Engine","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertSummarizer:\n    @staticmethod\n    def load(model: str, tokenizer: BertTokenizer, device=None) -> Summarizer:\n        config = BertConfig.from_pretrained(model)\n        config.output_hidden_states = True\n        bert_model = BertModel.from_pretrained(model, config=config)\n        if device is not None:\n            bert_model = bert_model.to(device)\n        return Summarizer(custom_model=bert_model, custom_tokenizer=tokenizer)\n\n\nclass QuestionAnsweringEngine(CORD19Dataset):\n    def __init__(self, source: Union[str, List[str]], papers: str,\n                 index: str, encoder: str, model: str, **kwargs):\n        \"\"\"CORD-19 Dataset Question Answering Engine.\n\n        :**kwargs: `sort_first:bool`, `nlp_model:str`\n        \"\"\"\n        super(QuestionAnsweringEngine, self).__init__(source, **kwargs)\n        self.device = torch.device(\n            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        )\n        self.papers = Papers.from_disk(papers)\n        self.index = faiss.read_index(index)\n        self.encoder = SentenceTransformer(encoder, device=self.device)\n        self.tokenizer = BertTokenizer.from_pretrained(model,\n                                                       do_lower_case=False)\n        self.model = BertForQuestionAnswering.from_pretrained(model)\n        self.model.to(self.device)\n        self.nlp = self.sentence_tokenizer.nlp()\n        self._freq_summarizer = frequency_summarizer\n        self._bert_summarizer = BertSummarizer.load(model, device=self.device,\n                                                    tokenizer=self.tokenizer)\n\n    def compress(self, sentences: Union[str, List[str]], mode=\"freq\") -> str:\n        if mode == \"freq\":\n            return self._freq_summarizer(sentences, nlp=self.nlp)\n        elif mode == \"bert\":\n            if isinstance(sentences, list):\n                sentences = \" \".join(sentences)\n            return self._bert_summarizer(sentences)\n\n    def encode(self, sentences: List[str]) -> np.array:\n        embedding = self.encoder.encode(sentences, show_progress_bar=False)\n        return np.array(embedding)\n\n    def similar(self, string: str, k=5) -> Tuple[np.array, np.array]:\n        string = normalize_whitespace(string.replace(\"?\", \" \"))\n        embedd = self.encode([string])\n        return self.index.search(embedd, k)\n\n    def decode(self, question: str, context: str) -> Tuple[str, str]:\n        inputs = self.tokenizer.encode_plus(question.strip(),\n                                            text_pair=context,\n                                            max_length=510,\n                                            add_special_tokens=True,\n                                            return_tensors='pt').to(self.device)\n        top_k = self.model(**inputs)\n        start, end = (torch.argmax(top_k[0]),\n                      torch.argmax(top_k[1]) + 1)\n        input_ids = inputs[\"input_ids\"].tolist()\n        answer = self.tokenizer.decode(input_ids[0][start:end],\n                                       skip_special_tokens=True)\n        if len(answer.strip()) > 0:  # did the model answer the question?\n            context = self.tokenizer.decode(input_ids[0],\n                                            skip_special_tokens=True)\n        return answer, context\n\n    def answer(self, question: str, k=15, mode: str = None) -> Dict[str, Any]:\n        question = question.strip()\n        dists, indices = self.similar(question, k=k)\n\n        sentences = []\n        for index in indices.flatten():\n            string = self.papers[index]\n            if string == question:\n                string = self.papers[index + 1]\n\n            doc = self.nlp(string)\n            for sent in doc.sents:\n                string = clean_tokenization(sent.text)\n                if len(sent) > 1 and sent[0].is_title:\n                    if (not sent[-1].like_num\n                        and not sent[-1].is_bracket\n                        and not sent[-1].is_quote\n                        and not sent[-1].is_stop\n                            and not sent[-1].is_punct):\n                        string = f\"{string}.\"\n                if string in sentences:\n                    continue\n                sentences.append(string)\n\n        context = \" \".join(sentences)\n        if mode is not None and mode in (\"freq\", \"bert\",):\n            context = self.compress(context, mode=mode)\n        context = normalize_whitespace(context)\n\n        answer, context = self.decode(question, context)\n        context = clean_tokenization(context)\n        dists, indices = dists.tolist()[0], indices.tolist()[0]\n\n        return {\"answer\": answer,\n                \"context\": context, \"dist\": dists, \"ids\": indices}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# after building the db (index/papers) we can now load everything we\n# need from a single configuration. note that if you set sort_first=True\n# - it also needs to be set here.\n\nengine_config = {\n    'source': (\n        '../input/CORD-19-research-challenge/document_parses/pdf_json'\n    ),\n    'papers': 'data/sents_4500.pkl',\n    'index': 'data/index_4500.index',\n    'encoder': 'models/scibert-nli',\n    'model': 'amoux/scibert_nli_squad',\n    'sort_first': True,\n    'nlp_model': 'en_core_web_sm'\n}\n\n# load the clustered questions\nKNNQ = DataIO.load_data('data/k-questions.plk')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# start the QA engine!\nqa = QuestionAnsweringEngine(**engine_config)\nprint(qa)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def answer_questions_randomly(n: int, k=15, mode=None, context=False):\n    random.seed(n + k)\n    misses = 0\n    for cat in KNNQ:\n        questions = KNNQ[cat][:n]\n        random.shuffle(questions)\n        for question in questions:\n            output = qa.answer(question, k=k, mode=mode)\n            if len(output['answer']) == 0:\n                misses += 1\n                continue\n            print(f'\\n====== {cat.title()} ======\\n')\n            print(f\"Q : {question}\")\n            print(f\"A : {output['answer']}\\n\")\n            if context:\n                print(f\"C : {output['context']}\\n\")\n\n    total = len(KNNQ)*n\n    score = round(((total - misses)/total)*100, 2)\n    print(f'-------- score : {score}% --------n')\n\n\ndef print_output(output, query: str = None, title_width=60):\n    answer = output['answer']\n    if query is not None:\n        print(f\"\\nQ : {query}\\n\")\n    print(f\"Answer  : {answer[:1].upper() + answer[1:]}\\n\")\n    print(f\"Context : {output['context']}\\n\")\n    print(\"\\t================= TITLES ðŸ¤— =================\\n\")\n\n    paper_ids = []\n    for lookup in qa.papers.lookup(output['ids']):\n        paper_ids.append(lookup['paper_id'])\n    paper_freq = Counter(paper_ids)\n    sums = sum(paper_freq.values())\n\n    minlen = 0\n    for i, (pid, freq) in enumerate(paper_freq.items()):\n        title = qa.title(pid).strip()\n        if len(title) == 0:\n            title = '< missing-title >'\n        weight = round((freq/sums) * 100, 2)\n        k_dist = round(output['dist'][i], 2)\n        print(f'D: {k_dist}\\tW: {weight}% \\t {title[:title_width]}')\n    print('\\t______________________________________________\\n')\n\n\ndef contradiction(premise, category) -> None:\n    # run sequences through the model pre-trained on MNLI\n    hypothesis = f'This text is about {category}'\n    input_ids = qa.tokenizer.encode(text=premise, text_pair=hypothesis,\n                                    return_tensors='pt').to(qa.device)\n    # entail contradiction logits\n    logits = qa.model(input_ids)[0]\n    true_prob = logits[..., [0, 2]].softmax(dim=1)[..., 1].item()\n    return round(true_prob*100, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"categories = list(KNNQ.keys())\n\none_cat = categories[0]\nquestion = random.choice(KNNQ[one_cat])\ntrue_prob = contradiction(question, category=one_cat)\n\nprint(f\"question: {question}\\n\")\nprint(f\"probability question's category: {cat} is true: {true_prob}%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# output without compressing the context before the model\n\noutput = qa.answer(question, k=5, mode=None)\nprint_output(output, title_width=80)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here the context is compressed via basic frequency\n# metrics before passing it as input to the QA model\n\noutput = qa.answer(question, k=10, mode=\"freq\")\nprint_output(output, title_width=80)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here we use a transformer based summarization to compress the context\n\noutput = qa.answer(question, k=15, mode=\"bert\")\nprint_output(output, title_width=80)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Let's now choose `n` randomly selected questions per *question-category* to all three modes `None, 'freq', 'bert` and see which mode performs better. Note choosing `n=4` + `k=15` in all modes will set the same random seed.","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"answer_questions_randomly(n=3, k=15, mode=None, context=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"answer_questions_randomly(n=3, k=25, mode=\"freq\", context=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"answer_questions_randomly(n=3, k=25, mode=\"bert\", context=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Task Results\n\n> Here we simply pass the tasks as queries to the question answering model. We will test the model with only `freq` and `bert` modes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def answer_tasks(k=15, mode=None, context=False):\n    misses = 0\n    for task in TASKS:\n        output = qa.answer(task, k=k, mode=mode)\n        if len(output['answer']) == 0:\n            misses += 1\n            continue\n        print(f'\\n==== << TASK >> ====\\n')\n        print(f\"T : {task}\\n\")\n        print(f\"A : {output['answer']}\\n\")\n        if context:\n            print(f\"C : {output['context']}\\n\")\n\n    total = len(TASKS)\n    score = round(((total - misses)/total)*100, 2)\n    print(f'======== score >> {score}% ========\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"answer_tasks(k=25, mode=\"freq\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"answer_tasks(k=45, mode=\"bert\", context=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`D` : Distance, lower scores -> more similar\n`W` : Weighted number of sentences in relation to all articles used for the context.\n\n> As we can see, a single query to topmost similar sentences automatically yields related articles to the query. And perhaps more accurate than a direct question to most similar titles.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for task in TASKS:\n    output = qa.answer(task, k=45, mode='bert')\n    if len(output['answer']) == 0:\n        continue\n    print_output(output, query=task, title_width=70)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final\n\n> If you have any questions about the idea, models, or code please ask!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}