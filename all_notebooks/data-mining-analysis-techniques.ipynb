{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### Author: **Ãlvaro M.D.**\n#### Twitter: **@alvarojonsson**\n#### Master's Degree in Data Science - University of Alicante","metadata":{"_cell_guid":"bc030392-f929-4638-8486-534e32101e0b","_uuid":"54ceeabb-d45a-4aa5-8746-0095556170b5"}},{"cell_type":"markdown","source":"# 1. Dataset\n\nThe selected data was published by the United Nations Statistics Division. This dataset covers import and export volumes for 5.000 commodities across most countries on Earth over the last 30 years.\n\nPersonally, I find commodities quite interesting because not only represents one of the biggest chunks of income of a country but also shows the international behaviours and relations that some countries have.\n\nMoreover, this dataset is already quite clean, hence we don't have to repeat all the process from the previous task but focus on the main goals.\n\nThe original dataset can be found here: http://data.un.org/Explorer.aspx","metadata":{"_cell_guid":"def232ae-2327-448f-98da-78237614e010","_uuid":"b0eb748b-14b2-4b3a-9b70-c8bd20730801"}},{"cell_type":"code","source":"# first, we will import all the necessary libraries\n\n# utils\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nfrom sklearn.decomposition import PCA\n\n# models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.cluster import MiniBatchKMeans\n\n# pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\n\n# model evaluation\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import plot_confusion_matrix\n\n# general configuration\nimport warnings\nwarnings.filterwarnings('ignore')\npd.options.display.float_format = '{:.2f}'.format","metadata":{"_cell_guid":"1dce7442-3fbb-43d0-95d4-517cf5b2946c","_uuid":"6baa6a8a-fd24-4661-a5a1-023caad8ccab","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataset from kaggle, makes easier the import of the file since it's >1Gb of data\ndf_raw = pd.read_csv('../input/global-commodity-trade-statistics/commodity_trade_statistics_data.csv', sep=',', header=0)\ndf_raw = df_raw.sample(frac = 0.05) # from 8.2 mill rows, we'll sample 400k to reduce computing time","metadata":{"_cell_guid":"f5c956d2-860a-4310-98c5-4e889e6dc970","_uuid":"2f197899-a954-4776-82d5-cd8d72ed9dc7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Exploring and preprocessing","metadata":{"_cell_guid":"6a0d9e9a-f2de-4bbc-90cc-831c3784fc05","_uuid":"4047a85a-fb10-48c0-a702-78a73751805a"}},{"cell_type":"code","source":"# total number of rows and columns in the dataset\ndf_raw.shape","metadata":{"_cell_guid":"356a1909-2af9-4ce0-9e65-13753ee1b947","_uuid":"544a409a-1470-40f7-9c3a-d203870bad30","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The original dataset contains 10 columns and 8.225.871 rows, but we have sampled a fraction (5%) and got 411294 rows.","metadata":{"_cell_guid":"7da118d3-82f6-4769-bb97-26c77cb2eb4e","_uuid":"e1ec5150-63e0-4db3-b7bc-3234236ffa70"}},{"cell_type":"markdown","source":"Description of the columns:\n- **country_or_area**: country name of record\n- **year**: year in which the trade has taken place\n- **comm_code**: the harmonized and coding system generally referred\n- **commodity**: description of a particular commodity code\n- **flow**: flow of trade i.e. export, import, others\n- **trade_usd**: value of the trade in usd\n- **weight_kg**: weight of the commodity in kilograms\n- **quantity_name**: description of the quantity measurement type given the type of item (i.e. number of items, weight in, etc)\n- **quantity**: count of the quantity of a given item based of the quantity name\n- **category**: category to identify commodity","metadata":{"_cell_guid":"108c19f7-048e-475e-9acc-a2655b8a3dbb","_uuid":"ee719c89-a7f7-4d7a-85dc-7fbd0fef5fa0"}},{"cell_type":"code","source":"df_raw.head()","metadata":{"_cell_guid":"804d5793-9662-4e42-ae4c-82314e6f8d1d","_uuid":"e3ab41e0-0b2b-4e2c-8ab3-597c009fc6f4","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pandas info() allow os to get column names, type and count number of nulls all in just one command\ndf_raw.info()","metadata":{"_cell_guid":"e70cc355-b905-475a-935d-2ca069a35931","_uuid":"9906aa76-e200-4019-94eb-cc675b70737a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We find some int and floats, and some other objects, let's explore first. We can also see most of the objects are plain strings such as the country name or the commodity.","metadata":{"_cell_guid":"61b4d196-b542-421f-a56c-4687a31150a4","_uuid":"bd516775-fa55-44c5-828e-fcb44f3dc8fb"}},{"cell_type":"code","source":"# check how many null values we got\ndf_raw.isnull().sum()","metadata":{"_cell_guid":"fe4f0642-085b-489a-9c75-62f32f89d441","_uuid":"6d8df5fc-2f32-490c-87d5-07a10f7499ee","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Only 'weight_kg' and 'quantity' have null values. We'll get percentages of missing values to decide if we can just remove this data, but before we'll explore a bit to get a better understanding of the data set.","metadata":{"_cell_guid":"cc9be7cd-029e-4042-8db6-cb44ba3db683","_uuid":"462b3df4-a430-4edf-9139-2299d56db8c2"}},{"cell_type":"markdown","source":"Now we will check how balanced the classes are.","metadata":{}},{"cell_type":"code","source":"df_raw['country_or_area'].value_counts().plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_raw['category'].value_counts().plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_raw['flow'].value_counts().plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since 'flow' is one of the problems we have chosen to tackle, we're going to include an under-sampling step in the pipeline to balance the classes. For so, we will use the library imblearn.","metadata":{}},{"cell_type":"code","source":"# groups null values by country, gets size of each group, sort in descending order, get first 10 rows\ntotal_nulls_country = df_raw[df_raw['weight_kg'].isnull()].groupby('country_or_area').size().sort_values(ascending=False)\ntotal_nulls_country.head(10)","metadata":{"_cell_guid":"91b7f16b-560c-4983-b39c-761890bc724a","_uuid":"6b44c242-6025-4543-8f70-8b1137e2fc57","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# repeat but grouping categories\ntotal_nulls_category = df_raw[df_raw['weight_kg'].isnull()].groupby('category').size().sort_values(ascending=False)\ntotal_nulls_category.head(10)","metadata":{"_cell_guid":"6a2ce75a-99ff-48f3-87f9-dd00683bb6f4","_uuid":"1103ac98-b237-46b3-b27a-0f29a33cdf17","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# % of missing rows for each column\nmissing_percentage = df_raw.isnull().sum() * 100 / len(df_raw)\nmissing_percentage","metadata":{"_cell_guid":"a29d242f-487c-4b6d-9ff3-0b27608269f4","_uuid":"c16177a7-0985-49e5-9169-3002614c3af2","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of unique values for each variable\ndf_raw.nunique(axis=0)","metadata":{"_cell_guid":"fa7ab3fa-35aa-489c-85fb-f66661fbdf97","_uuid":"01623247-40b5-42c3-a559-1332cd2bda1b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After some exploration, we checked that missing values only correspond to a certain types of categories where we assume data is more difficult to obtain, retain or update.\n\nMoreover, since the total percentage of missing values of both 'weight_kg' and 'quantity' is lower than 4% and taking into account we cannot discover or assume the data (i.e. we cannot randomly write the amount of kilogram that was sent 20 years ago), we will just remove this rows.","metadata":{"_cell_guid":"aaef838f-7681-47d1-a3f0-2d344050d9a0","_uuid":"6fc6b683-03a8-457e-b4e7-5a870dfa7819"}},{"cell_type":"code","source":"# drop rows with at least 1 NA value, get new shape\ndf = df_raw.dropna()","metadata":{"_cell_guid":"5accd748-33f9-4948-8229-3179e7ddb407","_uuid":"5adddbdc-484c-4c6d-8677-2a982f72ac5e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cleaning results\nprint(f'Rows before cleaning: {df_raw.shape[0]}')\nprint(f'Rows after cleaning: {df.shape[0]}')\nprint(f'Rows deleted: {df_raw.shape[0] - df.shape[0]}.')\nprint(f'Percentage of rows deleted: {((df_raw.shape[0] - df.shape[0]) * 100) / df_raw.shape[0]:.2f}%')","metadata":{"_cell_guid":"a13f6343-3fc4-4d4e-81b0-1e6d0c9ec9ab","_uuid":"77784680-6941-4c21-88c5-efec8928341b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# statistical summary of numeric variables\ndf.describe()","metadata":{"_cell_guid":"55ba5fc3-26a2-4ce4-ace6-36c8ac5224cc","_uuid":"c19ed29d-5560-4181-a757-497b01ca4d05","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On a first sight, values look quite normal with the exception of the maximum values found. Since we're not experts on international commodity trading, we can use plots to discover how far are the outliers from the other values.","metadata":{"_cell_guid":"b32be542-03a4-4baa-9dfb-6899df838de4","_uuid":"c055b22a-48d6-4145-bb33-9b4b5b974339"}},{"cell_type":"code","source":"sns.boxplot(x=df['trade_usd'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x=df['quantity'])","metadata":{"_cell_guid":"4a5e8b37-c98d-4f26-91aa-efaaf0485996","_uuid":"dd7679e9-a1a8-464a-81f1-97e69a4ef7ad","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x=df['weight_kg'])","metadata":{"_cell_guid":"67ed7029-409c-4444-9f9d-0edc85e168a9","_uuid":"7fb10572-6a91-40c7-9eba-f69edc1cedf2","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now is confirmed there is at least one outlier in the data. However, apparently only one value is quite larger than rest. What if we plot the same but removing only the maximum value found?","metadata":{"_cell_guid":"664e1718-3ff5-4044-b41e-62e9fe3b7ac3","_uuid":"1cc3c6a9-10c5-4501-8561-75330a50587c"}},{"cell_type":"code","source":"aux = df[~(df['quantity']>=df['quantity'].max())]\nsns.boxplot(x=aux['quantity'])","metadata":{"_cell_guid":"b2e47702-b8bc-466c-9ea0-99d5c85d2109","_uuid":"12025fb4-75c6-4e37-9016-aeb0e84963fe","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aux = df[~(df['weight_kg']>=df['weight_kg'].max())]\nsns.boxplot(x=aux['weight_kg'])","metadata":{"_cell_guid":"f64ac4fe-6e55-4bf9-a6c3-25460fc62d41","_uuid":"86479d64-92d8-41e4-aa99-4f38daf1f88f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove the max outlier\ndf_clean = df[~(df['quantity']>=df['quantity'].max())]","metadata":{"_cell_guid":"6dff3ce4-1240-4244-a49c-e0d1a2c3eecb","_uuid":"33dac815-c599-46ff-a2a4-11d2b129e56b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Another way of removing outliers is to use the IQR score. We first will calculate IQR and then remove every row that goes below or above with a threshold of 1.5 times the IQR.","metadata":{"_cell_guid":"48c3a08f-afcf-4ab7-9c6f-8cd4a3f4bb0b","_uuid":"3545235c-bcd9-44cb-81a7-3b7e7d9e624a"}},{"cell_type":"code","source":"Q1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","metadata":{"_cell_guid":"e866b4ed-bff4-43b7-aee9-3f26947c8fc4","_uuid":"228bc7be-7d5d-4d5f-add4-48752be4f2b9","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get cleaned dataframe without outliers based on IQR\ndf_iqr = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]","metadata":{"_cell_guid":"73fffeb9-4b7f-4c61-a3fe-6008c27a0573","_uuid":"31d1c46a-c205-4240-9dec-1021923598b4","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x=df_iqr['quantity'])","metadata":{"_cell_guid":"926f9586-9aa9-4285-ab2c-60de0a824be6","_uuid":"6b0d0ccc-0db7-4c76-b1c6-c1db7852ed61","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x=df_clean['trade_usd'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Outliers have been successfully removed. Let's compare the shape of our current dataset versus the previous one.","metadata":{"_cell_guid":"b10253ff-d6e7-4da6-b728-4d6008197e8a","_uuid":"da25fca8-0f25-429f-9d4d-61cf939046ba"}},{"cell_type":"code","source":"# cleaning results\nprint(f'Rows before cleaning: {df.shape[0]}')\nprint(f'Rows after cleaning: {df_iqr.shape[0]}')\nprint(f'Rows deleted: {df.shape[0] - df_iqr.shape[0]}.')\nprint(f'Percentage of rows deleted: {((df.shape[0] - df_iqr.shape[0]) * 100) / df.shape[0]:.2f}%')","metadata":{"_cell_guid":"374f855a-ab63-4790-8e27-f39625fdf434","_uuid":"8d2216b8-16ec-412a-b376-36869ea2c20a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Oops! We have just removed ~= 20% of the rows. How come is this possible? They were supposed to be outliers.\n\nIf we go back to the exploration data, we can see there are several commodities in different categories. Each category has it's own range of weight and quantity, which means we cannot generalize the IQR score to the whole dataset. This can be seen if we plot the quantity of rows per category.","metadata":{"_cell_guid":"66f42e3b-9766-457c-83bd-06ed8912354f","_uuid":"2e161baf-56c9-42b7-89a7-e02636eb3d3e"}},{"cell_type":"code","source":"df.groupby('category').size().sort_values(ascending=False).plot.density();","metadata":{"_cell_guid":"05ff208f-ffd6-47f7-8e5e-0bf9cd934095","_uuid":"0aed221b-aef0-4056-a2a0-dd4119b6f769","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plot shows different categories having different densities of rows in the dataset. Hence, we will discard the previous IQR filter and keep working with the dataset after max value was removed.\n\nAfter that, we can scale the data to avoid the noise and extreme values from different categories and ranges.","metadata":{"_cell_guid":"2feeb3e7-1fa7-4582-a141-12e4538787f2","_uuid":"674cb648-0b06-4cda-b5e8-577f44b564db"}},{"cell_type":"markdown","source":"# 3. Definition of the problem\n\nNow that the data was processed and cleaned, we can define the problem or questions that we can answer with the dataset.\n\nThe requirements for the job are:\n\n**Part 1**\n- perform a regression problem\n- perform a classification problem\n\n**Part 2**\n- perform an ensemble problem\n- perform a clustering problem\n\n**General tasks**\n- try a variety of algorithms and compare results\n- interpreting the results with error values, metrics, confusion matrix and ROC curve (ROC matrix and curve only for classification)\n\nNow that requirements are clear, let's define our goals for each of the algorithms to implement.\n\n- **Regression**: we will focus on the value of the \"trade_usd\" to check how much a product or category will increase its import/export for the next years.\n- **Classification**: sometimes a row will have missing data about what was the flow of this trade. We will do a multilabel classification to predict which class of trade was done, such as \"Import\", \"Export\", \"Re-Import\" or \"Re-Export\".\n- **Ensemble**: we will repeat the classification problem but using an ensemble algorithm that improves previous results.\n- **Clustering**: the \"commodity\" column has 5031 different unique values that correspond to the specific description of each category. We will cluster look-alike commodities by their category description in order to have more specific information of the import/export trades each country is operating.","metadata":{"_cell_guid":"e942bc45-902a-433d-8842-4097cd9e3452","_uuid":"017b8bed-acc5-49f4-9000-b8f65ca18f2a"}},{"cell_type":"markdown","source":"# 4. Preparation of the Data for Machine Learning Algorithms\n\nBefore apply any ML algorithm we have to prepare the data, in our case, by:\n\n- handling text and categorical attributes\n- performing feature scaling\n\nThis will allow us to create a transformation pipeline that will execute the needed steps in the right order every time.","metadata":{"_cell_guid":"b5951187-e638-48b0-9101-ac020c0e18b1","_uuid":"34fa9d18-e1dc-4b76-977a-d5be5d764472"}},{"cell_type":"code","source":"# let's check the data again\ndf = df_clean.copy()\ndf.head()","metadata":{"_cell_guid":"30c9ac19-7108-4729-94d6-dd8e10ba34e3","_uuid":"65b471be-1766-47f0-b4bb-6f783354a279","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Handling text and categorical attributes\n\nWe have several attributes to handle: \"country_or_area\", \"flow\", \"category\". There's no need to handle \"commodity\" since we also have \"comm_code\".","metadata":{"_cell_guid":"16af813f-e564-48b9-be58-1eb0c6790f57","_uuid":"41b0952e-3f02-4660-ab63-c59bfd4d7a19"}},{"cell_type":"code","source":"cat_encoder = OneHotEncoder()\n\n# one-hot encode text/categorical attributes\ncountry_cat_1hot = cat_encoder.fit_transform(df[['country_or_area']])\nflow_cat_1hot = cat_encoder.fit_transform(df[['flow']])\ncategory_cat_1hot = cat_encoder.fit_transform(df[['category']])\ncategory_cat_1hot","metadata":{"_cell_guid":"2d5fbecf-0296-4c21-a3b0-79b7942ce62e","_uuid":"9a4af812-bb0a-4592-b2ad-5687de392db4","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature scaling\nSince Machine Learning algorithms don't perform well when the input numerical attributes have very different scales and we have already explored the data to confirm this is our case, we will have to perform feature scaling. For this problem we will use a normalization technique called min-max scaling.","metadata":{"_cell_guid":"0c7b1f0e-385e-4f56-9f2d-c34183d75748","_uuid":"c2fb2ae4-28dc-4e0a-93f6-5f1a36d9206d"}},{"cell_type":"code","source":"scaler = MinMaxScaler()\ndata = df[['trade_usd', 'weight_kg', 'quantity']]\nscaled = scaler.fit_transform(data)\nprint(scaled)","metadata":{"_cell_guid":"eae61e2a-cfec-42eb-8151-c96dba805ba8","_uuid":"bf835427-b78b-426c-8983-990444c6e4d5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transformation Pipeline\n\nSo far, we have handled the categorical columns and the numerical columns separately. It would be more convenient to have a pipeline to handle all the transformations. We will use Scikit-Learn ColumnTransformer for this purpose.","metadata":{"_cell_guid":"2921f277-96ec-47f8-995b-c41ebe30844d","_uuid":"4fca2e0f-8e70-46dd-80b4-8302aa1cc8fb"}},{"cell_type":"code","source":"def transform_data(num_at, cat_at, dataframe):\n    \"\"\"Passes the input df through the \"\"\"\n    pipeline = ColumnTransformer([\n        ('num', MinMaxScaler(), num_at),\n        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_at), #ignore errors because dataset is huge and might encounter new categories\n    ])\n    return pipeline.fit_transform(dataframe), pipeline","metadata":{"_cell_guid":"837d9d86-bfcb-41ff-8a79-75635cefa224","_uuid":"c1d93e14-92f8-42dd-b484-481e2ac08d3a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# utility functions to improve prints\ndef display_scores(scores):\n    print(f\"Scores: {np.round(scores/1000000, decimals=2)}\")\n    print(f\"RMSE: {to_millions(scores.mean()):.2f}\")\n    print(f\"Standard deviation: {scores.std()/1000000:.2f}\")\n\ndef to_millions(usd):\n    return round(usd/1000000, 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train and Test set\n\nWe will use native Scikit-Learn function \"train_test_split\" to split the dataset into multiple subsets.","metadata":{"_cell_guid":"8135fb6f-a510-42a7-8622-1e7568747be7","_uuid":"0e12d80c-3a95-49e0-9b2e-250e6a537046"}},{"cell_type":"code","source":"train_set, test_set = train_test_split(df, test_size=0.3, random_state=42)","metadata":{"_cell_guid":"6cb36879-8397-4a72-b27e-c44366acedf1","_uuid":"09eb52a1-9e6a-4329-a638-42c0ceab601a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the next function, we will choose which columns we want as inputs and outputs for each model and prepare the data for the algorithm fitting and evaluation steps.","metadata":{}},{"cell_type":"code","source":"def prepare_data(dataset, chosen_column, df_num_attribs, df_cat_attribs, test=False):\n    \n    df_input = dataset.drop(chosen_column, axis=1)\n    df_output = dataset[chosen_column].copy()\n\n    \n    if not test:\n        df_prepared, pipeline = transform_data(df_num_attribs, df_cat_attribs, df_input)\n        return df_prepared, df_output, pipeline\n    else:\n        return df_input, df_output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5 . Regression","metadata":{"_cell_guid":"3f94dfad-eda3-44fb-8f0a-e466563c43b8","_uuid":"e80c813f-a102-4a35-be83-41102a71bf9d"}},{"cell_type":"markdown","source":"We will prepare the data for the regression problem and use the previous designed pipeline to perform scaling and one-hot encoding. As we defined in point 2, we will perform linear regression over the 'trade_usd' column. ","metadata":{}},{"cell_type":"code","source":"df_prepared, df_output, pipeline = prepare_data(train_set,\n                          'trade_usd',\n                          ['weight_kg', 'quantity'],\n                          ['country_or_area', 'flow', 'category'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before fitting any model, let's check what's the mean value of the column we want to predict. This will help us with the interpretation of the evaluation results.","metadata":{}},{"cell_type":"code","source":"# Mean of 'trade_usd' in millions\nprint(f\"Mean of 'trade_usd': {round(df['trade_usd'].mean()/1000000, 2)} millions\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Linear Regression","metadata":{}},{"cell_type":"code","source":"lin_reg = LinearRegression()\nlin_reg.fit(df_prepared, df_output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the model fitted, let's evaluate:","metadata":{}},{"cell_type":"code","source":"# evaluation of the model\ndf_predictions = lin_reg.predict(df_prepared)\nlin_mse = mean_squared_error(df_output, df_predictions)\nlin_rmse = np.sqrt(lin_mse)\nprint(f'R2: {r2_score(df_output, df_predictions):.3f}')\nprint(f'RMSE: {to_millions(lin_rmse)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the R2 we can say the model is not capturing all the variation of the data. Moreover, the RMSE is quite high. However, we'll also perform cross validation to get the average mean of all the data splits and also the standard deviation to get a better understanding of the model results.","metadata":{}},{"cell_type":"code","source":"scores = cross_val_score(lin_reg, df_prepared, df_output, scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-scores)\ndisplay_scores(lin_rmse_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With a mean of 21 million, having RMSE=503.85 is really bad.","metadata":{}},{"cell_type":"markdown","source":"We can already forsee results will be bad, but anyway let's evaluate our model on the test set.","metadata":{}},{"cell_type":"code","source":"X_test, y_test = prepare_data(test_set,\n                          'trade_usd',\n                          ['weight_kg', 'quantity'],\n                          ['country_or_area', 'flow', 'category'], True)\nX_test_prepared = pipeline.transform(X_test)\nfinal_predictions = lin_reg.predict(X_test_prepared)\nlin_mse = mean_squared_error(y_test, final_predictions)\nlin_rmse = np.sqrt(lin_mse)\nprint(f'R2: {r2_score(y_test, final_predictions):.3f}')\nprint(f'RMSE: {to_millions(lin_rmse)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confidence = 0.95\nsquared_errors = (final_predictions - y_test) ** 2\ninterval = np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n                        loc=squared_errors.mean(),\n                        scale=stats.sem(squared_errors)))\n\nprint(f'Confidence interval: {np.round(interval/1000000, decimals=2)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the given R2, the model only explains ~10% of the variance. Taking into consideration the confidence interval, a RMSE=481 is too large.\n\nTherefore, we can conclude the model is not generalizing well and therefore Linear Regression is not a valid model for this dataset.","metadata":{}},{"cell_type":"markdown","source":"### Support Vector Regression","metadata":{}},{"cell_type":"code","source":"svr_reg = SVR(max_iter=1000, coef0=2, C=50, kernel=\"poly\")\nsvr_reg.fit(df_prepared, df_output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluation of the model\ndf_predictions = svr_reg.predict(df_prepared)\nsvr_mse = mean_squared_error(df_output, df_predictions)\nsvr_rmse = np.sqrt(svr_mse)\nprint(f'R2: {r2_score(df_output, df_predictions):.3f}')\nprint(f'RMSE: {to_millions(svr_rmse)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the SVR model, R2 tell us the model can only explain a limited amount of the data. RMSE is too high. Let's repeat cross validation for a more exhaustive exploration to be sure this result is accurate.","metadata":{}},{"cell_type":"code","source":"scores = cross_val_score(svr_reg, df_prepared, df_output, scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-scores)\ndisplay_scores(lin_rmse_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test, y_test = prepare_data(test_set,\n                          'trade_usd',\n                          ['weight_kg', 'quantity'],\n                          ['country_or_area', 'flow', 'category'], True)\nX_test_prepared = pipeline.transform(X_test)\nfinal_predictions = svr_reg.predict(X_test_prepared)\nsvr_mse = mean_squared_error(y_test, final_predictions)\nsvr_rmse = np.sqrt(svr_mse)\nprint(f'R2: {r2_score(y_test, final_predictions):.3f}')\nprint(f'RMSE: {to_millions(svr_rmse)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confidence = 0.95\nsquared_errors = (final_predictions - y_test) ** 2\ninterval = np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n                        loc=squared_errors.mean(),\n                        scale=stats.sem(squared_errors)))\n\nprint(f'Confidence interval: {np.round(interval/1000000, decimals=2)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, the model cannot fit the data properly and is showing a low R2 both in train and test dataset and a huge RMSE that makes the model useless so far.\n\nAlthough the results are not good, we can draw some conclusions from this regression problem:\n- Data is not showing linear dependencies\n- There must be other techniques we could apply to improve the results, such as bucketing ranges of 'trade_usd' or use a deep learning model like LSTM\n- Huge datasets make the job even harder and are more difficult to assess","metadata":{}},{"cell_type":"markdown","source":"# 6. Classification","metadata":{}},{"cell_type":"markdown","source":"As we stated in point 2, sometimes a row will have missing data about what was the flow of this trade. We will do a multilabel classification to predict which class of trade was done, such as \"Import\", ","metadata":{}},{"cell_type":"code","source":"# choosing attributes for classification\ndf_prepared, df_output, pipeline = prepare_data(train_set,\n                          'flow',\n                          ['weight_kg', 'trade_usd', 'quantity'],\n                          ['country_or_area', 'category'])\n\nX_test, y_test = prepare_data(test_set,\n                          'flow',\n                          ['weight_kg', 'trade_usd', 'quantity'],\n                          ['country_or_area', 'category'], True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression","metadata":{}},{"cell_type":"code","source":"lr_clf = LogisticRegression(C=100, class_weight='balanced')\nlr_clf.fit(df_prepared, df_output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict train and test\nX_test_prepared = pipeline.transform(X_test)\ntest_predicted = lr_clf.predict(X_test_prepared)\n\n# evaluate with confusion matrix\nplot_confusion_matrix(lr_clf, X_test_prepared, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precision = precision_score(y_test, test_predicted, average='weighted')\naccuracy = lr_clf.score(X_test_prepared, y_test)\nf1_score_ = f1_score(y_test, test_predicted, average='weighted')\n\nprint(f'Accuracy: {round(accuracy, 2)}')\nprint(f'Precision: {round(precision, 2)}')\nprint(f'F1: {round(f1_score_, 2)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the confusion matrix, precision and accuracy, we can say the model is not good at all. Most of the classes are missclassified. This is due to an imbalanced dataset and, eventhough we are using the class_weight hyperparameter as 'balanced' to automatically adjust weights, model is generalizing to both of the most frequent labels: Export and Import.","metadata":{}},{"cell_type":"markdown","source":"To tackle the imbalanced dataset, we're going to perform over-sampling because if we do under-sampling we'll have less features than the original dataset and we won't be able to train the model. \n\nFirst, let's check how imbalanced the classes are.","metadata":{}},{"cell_type":"code","source":"imp, exp, rexp, reimp = df['flow'].value_counts()\ndf['flow'].value_counts().plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Second, we will divide the dataset into classes. and we will sample ","metadata":{}},{"cell_type":"code","source":"df_import = df[df['flow'] == 'Import']\ndf_export = df[df['flow'] == 'Export']\ndf_re_import = df[df['flow'] == 'Re-Import']\ndf_re_export = df[df['flow'] == 'Re-Export']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_re_export_under = df_re_export.sample(reimp, replace=True)\ndf_import_under = df_import.sample(reimp, replace=True)\ndf_export_under = df_export.sample(reimp, replace=True)\ndf_under = pd.concat([df_import_under, df_export_under, df_re_export_under, df_re_import], axis=0)\ndf_under['flow'].value_counts().plot(kind='bar');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that classes were over-sampled we can train and evaluate the model again.","metadata":{}},{"cell_type":"code","source":"cls_train_set, cls_test_set = train_test_split(df_under, random_state=42)\n\ncls_df_prepared, cls_df_output, cls_pipeline = prepare_data(df,\n                          'flow',\n                          ['weight_kg', 'trade_usd', 'quantity'],\n                          ['country_or_area', 'category'])\n\ncls_X_test, cls_y_test = prepare_data(cls_test_set,\n                          'flow',\n                          ['weight_kg', 'trade_usd', 'quantity'],\n                          ['country_or_area', 'category'], True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_clf = LogisticRegression(C=100, class_weight='balanced')\nlr_clf.fit(cls_df_prepared, cls_df_output)\n\n# predict train and test\nX_test_prepared = pipeline.transform(cls_X_test)\ntest_predicted = lr_clf.predict(X_test_prepared)\n\n# evaluate with confusion matrix\nplot_confusion_matrix(lr_clf, X_test_prepared, cls_y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precision = precision_score(cls_y_test, test_predicted, average='weighted')\naccuracy = lr_clf.score(X_test_prepared, cls_y_test)\nf1_score_ = f1_score(cls_y_test, test_predicted, average='weighted')\n\nprint(f'Precision: {round(precision, 2)}')\nprint(f'Accuracy: {round(accuracy, 2)}')\nprint(f'F1: {round(f1_score_, 2)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After over-sampling the original dataset, we find out that precision is even lower than before. The main reason for this is that classes were so imbalanced that now the most over-sampled class ('Re-Import') is causing the model to overfit, but the under-sampling has improved both accuracy and F1.","metadata":{}},{"cell_type":"markdown","source":"### Gaussian Naive Bayes","metadata":{}},{"cell_type":"code","source":"gn_clf = GaussianNB()\ngn_clf.fit(df_prepared.todense(), df_output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict train and test\ntrain_predicted = gn_clf.predict(df_prepared.todense())\nX_test_prepared = pipeline.transform(X_test)\ntest_predicted = gn_clf.predict(X_test_prepared.todense())\n\nplot_confusion_matrix(gn_clf, X_test_prepared.todense(), y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precision = precision_score(y_test, test_predicted, average='weighted')\naccuracy = gn_clf.score(X_test_prepared.todense(), y_test)\nf1_score_ = f1_score(y_test, test_predicted, average='weighted')\n\nprint(f'Precision: {round(precision, 2)}')\nprint(f'Accuracy: {round(accuracy, 2)}')\nprint(f'F1: {round(f1_score_, 2)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When we use a probabilistic approach, having imbalanced classes make the model to overfit on most frequent. Both accuracy and F1 are really low and therefore the model is not valid for our purpose.","metadata":{}},{"cell_type":"markdown","source":"### Stochastic Gradient Classifier","metadata":{}},{"cell_type":"markdown","source":"We're going to use the SGD instead ov SVC because the fit time of SVC scales at least quadratically with the number of samples and may be impractical with our dataset that contains more than 400k of samples.","metadata":{}},{"cell_type":"code","source":"sgd_clf = SGDClassifier(max_iter=1000, tol=1e-3)\nsgd_clf.fit(df_prepared, df_output)\n\n# predict train and test\nX_test_prepared = pipeline.transform(X_test)\ntest_predicted = lr_clf.predict(X_test_prepared)\n\n# evaluate with confusion matrix\nplot_confusion_matrix(sgd_clf, X_test_prepared, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precision = precision_score(y_test, test_predicted, average='weighted')\naccuracy = sgd_clf.score(X_test_prepared, y_test)\nf1_score_ = f1_score(y_test, test_predicted, average='weighted')\n\nprint(f'Precision: {round(precision, 2)}')\nprint(f'Accuracy: {round(accuracy, 2)}')\nprint(f'F1: {round(f1_score_, 2)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try it again with the balanced dataset:","metadata":{}},{"cell_type":"code","source":"sgd_clf = SGDClassifier(max_iter=1000, tol=1e-3)\nsgd_clf.fit(cls_df_prepared, cls_df_output)\n\n# predict train and test\nX_test_prepared = pipeline.transform(cls_X_test)\ntest_predicted = sgd_clf.predict(X_test_prepared)\n\n# evaluate with confusion matrix\nplot_confusion_matrix(sgd_clf, X_test_prepared, cls_y_test);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precision = precision_score(cls_y_test, test_predicted, average='weighted')\naccuracy = sgd_clf.score(X_test_prepared, cls_y_test)\nf1_score_ = f1_score(cls_y_test, test_predicted, average='weighted')\n\nprint(f'Precision: {round(precision, 2)}')\nprint(f'Accuracy: {round(accuracy, 2)}')\nprint(f'F1: {round(f1_score_, 2)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to the metrics, with proper balancing of the data and tuning a SGD classifier we could further improve results.\n\nHowever, as with regression, we can confirm this dataset is difficult to work with and overall results are not good.","metadata":{}},{"cell_type":"markdown","source":"# 7. Ensemble","metadata":{}},{"cell_type":"markdown","source":"For the ensemble problem, we would try to apply an averaging method (Bagging) that build several estimators and average their predictions. This would theoretically perform better than any of the single base estimator because its variance is reduced.","metadata":{}},{"cell_type":"code","source":"# choosing attributes for classification\ndf_prepared, df_output, pipeline = prepare_data(train_set,\n                          'flow',\n                          ['weight_kg', 'trade_usd', 'quantity'],\n                          ['country_or_area', 'category'])\n\nX_test, y_test = prepare_data(test_set,\n                          'flow',\n                          ['weight_kg', 'trade_usd', 'quantity'],\n                          ['country_or_area', 'category'], True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_models=10\nbagging = BaggingClassifier(DecisionTreeClassifier(max_depth=1, max_leaf_nodes=2),\n                            n_estimators=num_models)\nens_clf = bagging.fit(df_prepared, df_output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict train and test\nX_test_prepared = pipeline.transform(X_test)\ntest_predicted = ens_clf.predict(X_test_prepared)\n\n# evaluate with confusion matrix\nplot_confusion_matrix(ens_clf, X_test_prepared, y_test);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precision = precision_score(y_test, test_predicted, average='weighted')\naccuracy = ens_clf.score(X_test_prepared, y_test)\nf1_score_ = f1_score(y_test, test_predicted, average='weighted')\n\nprint(f'Precision: {round(precision, 2)}')\nprint(f'Accuracy: {round(accuracy, 2)}')\nprint(f'F1: {round(f1_score_, 2)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_err = (ens_clf.predict(df_prepared) != df_output).mean()\nprint(f'Train error: {train_err:.1%}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Having a this train error and overall results, we can try to apply under-sample this time to see if we can improve them. For so, we have to balance the data to the less frequent class, 'Re-Import'.","metadata":{}},{"cell_type":"code","source":"df['flow'].value_counts().plot(kind='bar');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_import = df[df['flow'] == 'Import']\ndf_export = df[df['flow'] == 'Export']\ndf_re_import = df[df['flow'] == 'Re-Import']\ndf_re_export = df[df['flow'] == 'Re-Export']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_re_export_under = df_re_export.sample(reimp, replace=True)\ndf_import_under = df_import.sample(reimp, replace=True)\ndf_export_under = df_export.sample(reimp, replace=True)\ndf_under = pd.concat([df_import_under, df_export_under, df_re_export_under, df_re_import], axis=0)\ndf_under['flow'].value_counts().plot(kind='bar');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that the dataset was under-sampled we can try to fit the model again and compare results. But we have to prepare the data first.","metadata":{}},{"cell_type":"code","source":"cls_train_set, cls_test_set = train_test_split(df_under, random_state=42)\n\ndf_prepared, df_output, pipeline = prepare_data(cls_train_set,\n                          'flow',\n                          ['weight_kg', 'trade_usd', 'quantity'],\n                          ['country_or_area', 'category'])\n\nX_test, y_test = prepare_data(cls_test_set,\n                          'flow',\n                          ['weight_kg', 'trade_usd', 'quantity'],\n                          ['country_or_area', 'category'], True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_models=10\nbagging = BaggingClassifier(DecisionTreeClassifier(),\n                            n_estimators=num_models)\nens_clf = bagging.fit(df_prepared, df_output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict train and test\nX_test_prepared = pipeline.transform(X_test)\ntest_predicted = ens_clf.predict(X_test_prepared)\n\n# evaluate with confusion matrix\nplot_confusion_matrix(ens_clf, X_test_prepared, y_test);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precision = precision_score(y_test, test_predicted, average='weighted')\naccuracy = ens_clf.score(X_test_prepared, y_test)\nf1_score_ = f1_score(y_test, test_predicted, average='weighted')\n\nprint(f'Precision: {round(precision, 2)}')\nprint(f'Accuracy: {round(accuracy, 2)}')\nprint(f'F1: {round(f1_score_, 2)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that dataset is balanced we have achieved a better result not only in accuracy but also for all the classes since F1 is higher tan before.","metadata":{"_cell_guid":"7c5dc4ce-decc-4db3-93aa-752eee552eaa","_uuid":"4f62d931-0715-46ce-b976-5820792c13af"}},{"cell_type":"markdown","source":"# 8. Clustering","metadata":{}},{"cell_type":"markdown","source":"We will cluster look-alike commodities by their category description in order to have more specific information of the import/export trades each country is operating.","metadata":{}},{"cell_type":"markdown","source":"Since our dataset is huge, we will reduce computation times by applying the Mini Batch version of KMeans.","metadata":{}},{"cell_type":"code","source":"# choosing attributes for classification\ndf_prepared, df_output, pipeline = prepare_data(train_set,\n                          'category',\n                          ['weight_kg', 'trade_usd', 'quantity'],\n                          ['country_or_area', 'flow'])\n\nX_test, y_test = prepare_data(test_set,\n                          'category',\n                          ['weight_kg', 'trade_usd', 'quantity'],\n                          ['country_or_area', 'flow'], True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### K-Means","metadata":{}},{"cell_type":"markdown","source":"KMeans requires to know how many clusters we want to group. To determine the best K by comparing the results for different k values, we can use the Elbow method, which uses the sum of squared distance between each point and the centroid in a cluster.","metadata":{}},{"cell_type":"code","source":"from scipy.spatial.distance import cdist\n\nsse = {}\nfor k in range(1, 10):\n    kmeans = MiniBatchKMeans(n_clusters=k, max_iter=1000).fit(df_prepared)\n    df_output = kmeans.labels_\n    sse[k] = kmeans.inertia_ # sum of distances of samples to their closest centroid\n\nplt.figure()\nplt.plot(list(sse.keys()), list(sse.values()))\nplt.xlabel(\"Number of cluster\")\nplt.ylabel(\"SSE\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the elbow method we can determine that the best K is 3 or 5. Since this is a learning exercise, we would just use 3 for saving computing time.","metadata":{}},{"cell_type":"markdown","source":"Since we're not which K number use between 3 and 6, a more precise approach is to use the silhouette score, which is the mean silhouette coefficient over all the instances.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import silhouette_score\n\nsl = {}\nfor k in range(3, 6):\n    kmeans = MiniBatchKMeans(n_clusters=k, max_iter=1000).fit(df_prepared)\n    sl[k] = silhouette_score(df_prepared, kmeans.labels_)\n\nplt.figure()\nplt.plot(list(sl.keys()), list(sl.values()))\nplt.xlabel(\"Number of cluster\")\nplt.ylabel(\"SSE\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans = MiniBatchKMeans(n_clusters = 3, max_iter=1000).fit(df_prepared)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_prepared = pipeline.transform(X_test)\ncluster_predictions = kmeans.predict(X_test_prepared)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we checked everything is working, we can perform a PCA to reduce the dimensional space into a 2-dimensional space in order to plot the data and the clusters. This is done also because otherwise we won't be able to make an interpretation of the cluster results.","metadata":{}},{"cell_type":"code","source":"reduced_data = PCA(n_components=2).fit_transform(df_prepared.todense())\nkmeans = MiniBatchKMeans(n_clusters=3, max_iter=1000).fit(reduced_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step size of the mesh. Decrease to increase the quality of the VQ.\nh = .02     # point in the mesh [x_min, x_max]x[y_min, y_max].\n\n# Plot the decision boundary. For that, we will assign a color to each\nx_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\ny_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n# Obtain labels for each point in mesh. Use last trained model.\nZ = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure(1)\nplt.clf()\nplt.imshow(Z, interpolation=\"nearest\",\n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap=plt.cm.Paired, aspect=\"auto\", origin=\"lower\")\n\nplt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n# Plot the centroids as a white X\ncentroids = kmeans.cluster_centers_\nplt.scatter(centroids[:, 0], centroids[:, 1], marker=\"x\", s=169, linewidths=3,\n            color=\"w\", zorder=10)\nplt.title(\"K-means clustering on the digits dataset (PCA-reduced data)\\n\"\n          \"Centroids are marked with white cross\")\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.xticks(())\nplt.yticks(())\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will now try K=5 in order to deal with the lowest SSE possible but still following the Elbow Method.","metadata":{}},{"cell_type":"code","source":"kmeans = MiniBatchKMeans(n_clusters=5, max_iter=1000).fit(reduced_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step size of the mesh. Decrease to increase the quality of the VQ.\nh = .02     # point in the mesh [x_min, x_max]x[y_min, y_max].\n\n# Plot the decision boundary. For that, we will assign a color to each\nx_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\ny_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n# Obtain labels for each point in mesh. Use last trained model.\nZ = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure(1)\nplt.clf()\nplt.imshow(Z, interpolation=\"nearest\",\n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap=plt.cm.Paired, aspect=\"auto\", origin=\"lower\")\n\nplt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n# Plot the centroids as a white X\ncentroids = kmeans.cluster_centers_\nplt.scatter(centroids[:, 0], centroids[:, 1], marker=\"x\", s=169, linewidths=3,\n            color=\"w\", zorder=10)\nplt.title(\"K-means clustering on the digits dataset (PCA-reduced data)\\n\"\n          \"Centroids are marked with white cross\")\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.xticks(())\nplt.yticks(())\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results show that if K=5 we have two centroids that are pretty much together. Therefore, it is worth it to give it a try with K=4 this time and check if both centroids mix into one.","metadata":{}},{"cell_type":"code","source":"kmeans = MiniBatchKMeans(n_clusters=4, max_iter=1000).fit(reduced_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step size of the mesh. Decrease to increase the quality of the VQ.\nh = .02     # point in the mesh [x_min, x_max]x[y_min, y_max].\n\n# Plot the decision boundary. For that, we will assign a color to each\nx_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\ny_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n# Obtain labels for each point in mesh. Use last trained model.\nZ = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure(1)\nplt.clf()\nplt.imshow(Z, interpolation=\"nearest\",\n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap=plt.cm.Paired, aspect=\"auto\", origin=\"lower\")\n\nplt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n# Plot the centroids as a white X\ncentroids = kmeans.cluster_centers_\nplt.scatter(centroids[:, 0], centroids[:, 1], marker=\"x\", s=169, linewidths=3,\n            color=\"w\", zorder=10)\nplt.title(\"K-means clustering on the digits dataset (PCA-reduced data)\\n\"\n          \"Centroids are marked with white cross\")\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.xticks(())\nplt.yticks(())\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics.pairwise import euclidean_distances\n\ndists = euclidean_distances(kmeans.cluster_centers_)\n\n# euclidean distance between points of clusters 3 and 1\ntri_dists = dists[np.triu_indices(3, 1)]\nmax_dist, avg_dist, min_dist = tri_dists.max(), tri_dists.mean(), tri_dists.min()\nprint(max_dist)\nprint(avg_dist)\nprint(min_dist)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we expected, now all the data is properly clustered and the euclidean distance. Let's train it again with K=4 but without reduced data and predict with test data to see if the algorithm is clustering.","metadata":{}},{"cell_type":"code","source":"kmeans = MiniBatchKMeans(n_clusters=4, max_iter=1000).fit(df_prepared)\nX_test_prepared = pipeline.transform(X_test)\nkmeans_predicted = kmeans.predict(X_test_prepared)\nprint(kmeans_predicted)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DBSCAN","metadata":{}},{"cell_type":"markdown","source":"DBSCAN defines clusters as continuous regions of high density. We can use it to find clusters of arbitrary shape, modelled as dense regions in the data space, separated by sparse regions.","metadata":{}},{"cell_type":"markdown","source":"Let's train our model selecting 0.3 for eps and setting min_samples to 5.","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import DBSCAN\n\ndbscan = DBSCAN(eps=0.3, min_samples=5)\ndbscan.fit(df_prepared)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The labels_ property contains the list of clusters and their respective points.","metadata":{}},{"cell_type":"code","source":"dbscan.labels_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we will map every individual cluster to a color and plot the results. \n\nAs we can see on the chart below, all the dark blue points are considered noise.","metadata":{}},{"cell_type":"markdown","source":"Since DBSCAN does not have a predict method, we can train a KNeighborsClassifier to predict which cluster a new instance belongs to.","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=50)\nknn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can predict which cluster they most likely belong to and even estimate a probability for each cluster.","metadata":{}},{"cell_type":"code","source":"X_test_prepared = pipeline.transform(X_test)\npredict = knn.predict(X_test_prepared)\nprint(predict)\nprint(knn.predict_proba(X_test_prepared))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\n\nlabels = dbscan.labels_\ncore_samples_mask = np.zeros_like(labels, dtype=bool)\ncore_samples_mask[dbscan.core_sample_indices_] = True\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\n\nprint('Estimated number of clusters: %d' % n_clusters_)\nprint('Estimated number of noise points: %d' % n_noise_)\nprint(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(df_output, labels))\nprint(\"Completeness: %0.3f\" % metrics.completeness_score(df_output, labels))\nprint(\"V-measure: %0.3f\" % metrics.v_measure_score(df_output, labels))\nprint(\"Adjusted Rand Index: %0.3f\"\n      % metrics.adjusted_rand_score(df_output, labels))\nprint(\"Adjusted Mutual Information: %0.3f\"\n      % metrics.adjusted_mutual_info_score(df_output, labels))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_labels = set(labels)\ncolors = [plt.cm.Spectral(each)\n          for each in np.linspace(0, 1, len(unique_labels))]\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n\n    class_member_mask = (labels == k)\n\n    xy = df_prepared.todense()[class_member_mask & core_samples_mask]\n    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n             markeredgecolor='k', markersize=14)\n\n    xy = df_prepared.todense()[class_member_mask & ~core_samples_mask]\n    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n             markeredgecolor='k', markersize=6)\n\nplt.title('Estimated number of clusters: %d' % n_clusters_)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the above graph plots, we're obtaining a huge amount of clusters that does not correspond within KMeans results. Therefore, hyperparameter tuning should be applied to improve this model, in case we need to use it. However, since this notebook is just for learning, there's no need to further explore the improvement possibilities.","metadata":{}},{"cell_type":"markdown","source":"We have proven that commodities can be clustered with K=4 with KMeans clustering and by density with DBSCAN, which determines the best number of clusters by itself.\n\nThe chosen dataset is quite complex for a person without specific knowledge of international trading. Therefore, we will skip the interpretation phase that normally follows a clustering problem and conclude the notebook with an advice: huge datasets are not meant to be used for learning purposes since there are many things a beginner could miss within all the different techniques and models we have used for the aforementioned notebook.","metadata":{}}]}