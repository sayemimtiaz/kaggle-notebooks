{"cells":[{"metadata":{},"cell_type":"markdown","source":"<font size=\"+4\" color=teal><u><center>Analysis of \"The Social Dilemma\" tweets </center></u></font>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"top\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Table of content</h3>\n\n* [Introduction](#intro)\n* [Data cleaning and Feature extraction](#data)\n* [1. Source of Tweets ](#1)\n* [2. Day of tweets ](#2)\n* [3. Month Day of tweets](#3)\n* [4. User Followers ](#4)\n* [5. Top 30 Influencers](#5)\n* [6. Countries](#6)\n* [7. Verified Users](#7)\n* [8. Hashtags](#8)\n* [9. Mentions](#9)\n* [10. Links](#10)\n* [11. Common Words](#11)\n* [12. Unigrams, Bigrams and Trigrams](#12)\n* [13. Tweet Length](#13)\n* [14. Positive Tweet](#14)\n* [15. Negative Tweet](#15)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"intro\"></a>\n<font size=\"+2\" color=\"blue\"><b>Introduction and Imports</b></font><br>\n\n<font size=\"+1\" color=\"magenta\">\n**Summary: Tech experts sound the alarm on the dangerous human impact of social networking.**\n**Released on : January 2020**\n    From the creators of Chasing Ice and Chasing Coral, The Social Dilemma blends documentary investigation and narrative drama to disrupt the disrupters, unveiling the hidden machinations behind everyone’s favorite social media and search platforms.\n</font>\n</br>\n</br>\n"},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"teal\">\nPlan is to install the below \n1. pycountry - to convert country name to country code\n2. Folium - for drawing country markers\n </font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pycountry-convert","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# installed for maps\n!pip install folium","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport re\nimport nltk\n\nimport plotly.figure_factory as ff\nfrom plotly.colors import n_colors\nfrom plotly.subplots import make_subplots\n\nfrom wordcloud import WordCloud, STOPWORDS\nimport pycountry\nimport folium\n\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nimport re\nfrom collections import Counter\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"data\"></a>\n<font size=\"+2\" color=\"blue\"><b>Data cleaning and Feature extraction </b></font><br>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dil = pd.read_csv(\"/kaggle/input/the-social-dilemma-tweets/TheSocialDilemma.csv\", parse_dates=['date','user_created'])\nprint(\"Shape of df: \",dil.shape)\nprint(\"Info of df: \",dil.info())\nprint(\"Describe df: \",dil.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Renamed the column names\ndil.rename(columns={'user_friends':'friends',\"user_followers\":\"followers\",\"user_location\":\"location\",\"user_verified\":\"verified\",\"user_description\":\"description\",\"user_favourites\":\"favourites\"}, inplace=True)\n\n# Cleaning location\ndil['location'] = dil['location'].astype('str').str.split(\".\").str[0]\ndil['location'] = dil['location'].str.replace(r'[^a-zA-Z,]', \" \").str.strip()\ndil['location'] = dil['location'].fillna('nan')\n\n# Extracting day, hour, weekday\ndil['month_day'] = dil.date.dt.day\ndil['hour'] = dil.date.dt.hour\ndil['week_day'] = dil.date.dt.weekday\ndil['week_day'] = dil['week_day'].map({0:'Monday', 1:'Tuesday', 2:'Wednesday', 3:'Thursday', 4:'Friday', 5:'Saturday',6:'Sunday'})\n\n# calculating the age of user as on tweet day\ndil['account_age'] = (dil['date'] - dil['user_created']).astype('str')\n\nf = lambda x: x.split(\" \")[0]\ndil[\"account_age\"] = dil[\"account_age\"].apply(f)\ndil[\"account_age\"] = dil[\"account_age\"].astype('int')\n\n# calculating the age of user from today to the tweet day\ndil['days_passed'] = (pd.datetime.now() - dil['date']).astype('str')\n\nf = lambda x: x.split(\" \")[0]\ndil[\"days_passed\"] = dil[\"days_passed\"].apply(f)\ndil[\"days_passed\"] = dil[\"days_passed\"].astype('int')\n\n# Give nan hashtags 'no_tag' values\ndil['hashtags'] = dil['hashtags'].fillna(\"no_tag\")\n\n# Checking columns with nulls\nnull_check = dil.isnull().sum() \nprint(null_check[null_check > 0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract city and country for location\ndef separateCountry(loc):\n    t=[]          \n    if loc != 'nan':\n        tokens = loc.split(\",\")\n        if len(tokens) == 0:           \n            return \"no_country\"\n        elif len(tokens) == 2:           \n            return tokens[1].strip()\n        elif len(tokens) > 2:\n            #print(\">2 country.. \",tokens)            \n            return tokens[-1].strip()\n        else:           \n            t = tokens\n            tokens = ['no_city', t[0].strip()]            \n            return tokens[1].strip()       \n    else:\n        tokens = ['no_city','no_country']       \n        return tokens[1]  \n\ndef separateCity(loc):\n    t=[]  \n    if loc != 'nan':\n        tokens = loc.split(\",\")\n        if len(tokens) == 2:            \n            return tokens[0].strip()\n        elif len(tokens) > 2:\n            t = tokens\n            #print(\">2 city.. \",tokens)\n            tokens = [' '.join(t[0:-1]), t[-1].strip()]            \n            return tokens[0].strip()\n        else:            \n            t = tokens\n            tokens = ['no_city', t[0].strip()]            \n            return tokens[0].strip()       \n    else:\n        tokens = ['no_city','no_country']        \n        return tokens[0].strip()  \n\nprint(\"User Location: \",dil['location'].nunique(), dil['location'].unique())\n\n\ndil['country'] = dil['location'].apply(separateCountry)\ndil['city'] = dil['location'].apply(separateCity)\ndil.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Very noisy location data..hence manually imputing\n# Correcting Atul Khatri's city, country\nidx1 = dil[dil['user_name'] == 'Atul Khatri'].index\ndil.loc[idx1,'city']='Mumbai'\ndil.loc[idx1,'country']='India'\n\n# Correcting Sreedhar Pillai's city, country\nidx2 = dil[dil['user_name'] == 'Sreedhar Pillai'].index\ndil.loc[idx2,'city']='Chennai'\ndil.loc[idx2,'country']='India'\n\n# Correcting Shiv Aroor's city, country\nidx3 = dil[dil['user_name'] == 'Shiv Aroor'].index\ndil.loc[idx3,'city']='Delhi'\ndil.loc[idx3,'country']='India'\n\n# Correcting Rahul Bose's city, country\nidx3 = dil[dil['user_name'] == 'Rahul Bose'].index\ndil.loc[idx3,'city']='Mumbai'\ndil.loc[idx3,'country']='India'\n\n# Correcting Rotten Tomatoes's city, country\nidx4 = dil[dil['user_name'] == 'Rotten Tomatoes'].index\ndil.loc[idx4,'city']='Los Angeles'\ndil.loc[idx4,'country']='USA'\n\n# Correcting diddy's city, country\nidx5 = dil[dil['user_name'] == 'Diddy'].index\ndil.loc[idx5,'city']='CA'\ndil.loc[idx5,'country']='USA'\n\n# Correcting E_L_James's city, country\nidx6 = dil[dil['user_name'] == 'E_L_James'].index\ndil.loc[idx6,'city']='West London'\ndil.loc[idx6,'country']='England'\n\n# Correcting tyler oakley's city, country\nidx7 = dil[dil['user_name'] == 'tyler oakley'].index\ndil.loc[idx7,'city']='NYC'\ndil.loc[idx7,'country']='USA'\n\n# Correcting Arianna Huffington's city, country\nidx1 = dil[dil['user_name'] == 'Arianna Huffington'].index\ndil.loc[idx1,'city']='AZ'\ndil.loc[idx1,'country']='USA'\n\n# Correcting BobSaget's city, country\nidx7 = dil[dil['user_name'] == 'bob saget'].index\ndil.loc[idx7,'city']='NJ'\ndil.loc[idx7,'country']='USA'\n\n# Correcting Sophie C's city, country\nidx7 = dil[dil['user_name'] == 'Sophie C'].index\ndil.loc[idx7,'city'] = 'Hyderabad'\ndil.loc[idx7,'country']='India'\n\n# Correcting DuckDuckGo's city, country\nidx7 = dil[dil['user_name'] == 'DuckDuckGo'].index\ndil.loc[idx7,'city'] = 'Pennsylvania'\ndil.loc[idx7,'country']='USA'\n\n# Correcting DuckDuckGo's city, country\nidx7 = dil[dil['user_name'] == 'VOGUE India'].index\ndil.loc[idx7,'city'] = 'Bengaluru'\n\n# Correcting DNA's city, country\nidx7 = dil[dil['user_name'] == 'DNA'].index\ndil.loc[idx7,'city'] = 'Pune'\n\n\ndef correct_city_country(df):    \n    if df.lower() == 'california' or df.lower() == 'usa' or df.lower() == 'new york' or df.lower() == 'los angeles' or df.lower() == 'texas' or df.lower() == 'mi' or df.lower() == 'oh' or df.lower() == 'va' or df.lower() == 'pa' or df.lower() == 'az' or df.lower() == 'or' or df.lower() == 'co' or df.lower() == 'fl' or df.lower() == 'ma'or df.lower() == 'dc' or df.lower() == 'nc' or df.lower() == 'il'  or df.lower() == 'united states' or df.lower() == 'tn' or df.lower() == 'brooklyn' or df.lower() == 'pittsburgh' or df.lower() == 'in' or df.lower() == 'wa' or df.lower() == 'oklahoma city' or df.lower() == 'ny' or df.lower() == 'tx' or df.lower() == 'ca' or df.lower() == 'ga':        \n        df = 'United States'\n    elif df.lower() == 'new south wales' or df.lower() == 'victoria':        \n        df = 'Australia'\n    elif df.lower() == 'british columbia' or df.lower() == 'ontario':        \n        df = 'Canada'\n    elif df.lower() == 'barcelona'  or df.lower() == 'comunidad de madrid':\n        df = 'Spain'\n    elif df.lower() == 'hamirpur' or df.lower() == 'delhi' or df.lower() == 'bengaluru' or df.lower() == 'new delhi' or df.lower() == 'mumbai'or df.lower() == 'rajasthan':\n        df = 'India'\n    elif df.lower() == 'london' or df.lower() == 'kent'  or df.lower() == 'united kingdom':\n        df = 'UK'\n    elif df.lower() == '':\n        df = 'no_country'\n    return df\n    \ndil['country'] = dil['country'].apply(lambda x : correct_city_country(x))\n\ndef get_country_code(co):\n    mapping = {country.name: country.alpha_2 for country in pycountry.countries}    \n    return mapping.get(co)\n    \ndil['code'] = dil['country'].map(get_country_code)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n<font size=\"+2\" color=\"blue\"><b>Source of tweets - Device/App </b></font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"source_df = dil.groupby(['source']).agg('count').reset_index().rename(columns={'user_name':'count'})\nsource_df = source_df.sort_values(by=['count'],ascending=0)\nsource_df = source_df.drop(['month_day','favourites','location','user_created','description','followers','friends','verified','days_passed','hour','date','hashtags','text','is_retweet','week_day','account_age','city','country'],axis=1)\n\ntop_source_df = source_df[:6]\nfig = px.bar(top_source_df, x='source', y='count', hover_data=['source', 'count'], height=400)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"magenta\"><b>Inference - Source of tweets is from phones and from web </b></font>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a>\n<font size=\"+2\" color=\"blue\"><b>Day of tweets </b></font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"week_day_df = dil.groupby(['week_day']).agg('count').reset_index().rename(columns={'week_day':\"week_day\",'user_name':'count'})\nweek_day_df = week_day_df.sort_values(by=['count'],ascending=0)\nweek_day_df = week_day_df.drop(['code','Sentiment','month_day','favourites','location','user_created','description','followers','friends','verified','days_passed','hour','date','hashtags','text','is_retweet','source','account_age','city','country'],axis=1)\nweek_day_df.style.background_gradient(cmap='jet_r', subset=pd.IndexSlice[:, ['count']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"magenta\"><b>Inference - The tweets are from 08-Sep (falls on saturday) and slowing day by day the viewing of the film increased and the tweets also increased by the week with highest on sunday and the next day.  </b></font>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a>\n<font size=\"+2\" color=\"blue\"><b>Month day of tweets </b></font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"month_df = dil.groupby(['month_day']).agg('count').reset_index().rename(columns={\"user_name\":\"count\"})\nmonth_df = month_df.drop(['code','Sentiment','favourites','location','user_created','description','followers','friends','verified','week_day','days_passed','hour','date','hashtags','text','is_retweet','source','account_age','city','country'],axis=1)\n\nmonth_df.style.background_gradient(cmap='rainbow_r', subset=pd.IndexSlice[:, ['count']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"magenta\"><b>Inference - The tweets are from 08-Sep and are picking up with each day with highest on weekends and the effect on following monday. </b></font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig=go.Figure(go.Scatter(x=month_df['month_day'],\n                                y=month_df['count'],\n                               mode='markers+lines',\n                               name=\"Submissions\",\n                               marker_color='dodgerblue'))\n\nfig.update_layout(title_text='Tweets per Day',template=\"plotly_dark\",title_x=0.5)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a>\n<font size=\"+2\" color=\"blue\"><b>User Followers </b></font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"followers_df = dil.groupby(['followers','friends','favourites','city','country','account_age','Sentiment'])['user_name'].agg(sum).reset_index()\nfollowers_df = followers_df.sort_values(by=\"followers\",ascending=False)\nfollowers_df[:30]\n\ntop_followers_df = followers_df[:20]\ntop_followers_df.style.background_gradient(cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"magenta\"><b>Inference - This table shows us influencers and in some cases they have tweeted more than once and the sentiment is not uniform across tweets. </b></font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"top30_followers_df = followers_df[:30]\nfig = px.bar(top30_followers_df, x=\"user_name\", y=\"account_age\", color=\"Sentiment\", title=\"Top 30 Influencers and their sentiments\", \n             labels={\"user_name\": \"User Name\", \"account_age\": \"account_age\"},)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"magenta\"><b>Inference - Influencers like Derek o Brien have tweeted more than once and each time the sentiment is different.  Same is the case with Tyler Oakley or Miss Malini. Also notice all top influencers are ahving more or less similar account age. (They more or less registered with twitter around same time.) </b></font>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\"></a>\n<font size=\"+2\" color=\"blue\"><b>Top Countries with most tweets</b></font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"country_df = dil['country'].value_counts().to_frame().reset_index().rename(columns={'index':'country','country':'count'})\n\nfig = go.Figure(go.Bar(\n    x=country_df['country'][:10],y=country_df['count'][:10],\n    marker={'color': country_df['count'][:10], \n    'colorscale': 'greens'},  \n    text=country_df['count'][:10],\n    textposition = \"outside\",\n))\nfig.update_layout(title_text='Top Countries with most tweets',xaxis_title=\"Countries\",\n                  yaxis_title=\"Number of Tweets\",template=\"plotly_dark\",height=700,title_x=0.5)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"magenta\"><b>Inference - Notice that because of no data avalible, around 4027 entries are 'no_country'. United States is next top country followed by India. This also shows that Netflix is watched a lot by US population. While 4th highest country England is only 1/4 of tweets from from India. India and Philipines need a special mention because they are not primarily English speaking country.</b></font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using Folium Maps\n\ntop_followers_df['Lat'] = [36.8, 45.7, 42.7,42.7,19.1 , 34.1, 34.5,19.1,19.1,19.1, 17.3,40.1,34.2 , 18.5, 18.1,19.1,17.1 , 41,12.9 , 22.6]\ntop_followers_df['Long'] = [-110.4, -84,-70,-64,72.8 , -111.2,-111.1 , 72.8,72.8,72.8, 78.4, -74.4,-118.2, 77.6, 73.8 ,71.8,70.8, -77, 77.5, 88.4]\n\n\nworld_map = folium.Map(location=[10,0], tiles=\"cartodbpositron\", zoom_start=2,max_zoom=6,min_zoom=2)\nfor i in range(0,len(top_followers_df)):\n    \n    folium.Circle(\n        location=[top_followers_df.iloc[i]['Lat'], top_followers_df.iloc[i]['Long']],\n        tooltip = \"<h5 style='text-align:center;font-weight: bold'>\"+top_followers_df.iloc[i]['country']+\"</h5>\"+                    \n                    \"<div style='text-align:center;'>\"+str((top_followers_df.iloc[i]['city']))+\"</div>\"+\n                    \"<hr style='margin:10px;'>\"+\n                    \"<ul style='color: #444;list-style-type:circle;align-item:left;padding-left:20px;padding-right:20px'>\"+ \"</ul>\"\n        , radius=(int((np.log(200+1.00001)))+0.2)*50000,\n        color='#ff6600', fill_color='#ff8533', fill=True).add_to(world_map)\n\nworld_map","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"magenta\"><b>Inference - Visual representation of countries of top 10 influencers</b></font>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7\"></a>\n<font size=\"+2\" color=\"blue\"><b>Are accounts user verified ? </b></font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (10, 5))\n\nuser_verified_df = dil['verified'].value_counts().to_frame().reset_index()\nuser_verified_df.columns = ['verified','counts']\nuser_verified_df['verified'] = user_verified_df['verified'].map({False:0,True:1})\n\nfig = px.bar(user_verified_df, x='verified', y='counts',width=600, height=400)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"magenta\"><b>Inference - Most of the accounts are unverified.</b></font>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"8\"></a>\n<font size=\"+2\" color=\"blue\"><b>Analyzing HashTags</b></font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"ht = dict()\ndef create_hashtag_dict(hashtags):\n    \n    hashtags = hashtags.replace('[', \"\").strip()\n    hashtags = hashtags.replace(']', \"\").strip()\n    hashtags = hashtags.replace(\"'\", \"\").strip()\n    tags_list = hashtags.split(\",\")\n    \n    length = int(len(tags_list))\n    \n    for l in range(length):\n        key = tags_list[l].strip()\n        \n        if key in ht.keys(): \n            ht[key] += 1\n        else:\n            ht[key] = 1  \n    return ht  \n               \nhash_dict = dil['hashtags'].map(create_hashtag_dict)\n\nsorted_hash_dict = {k: v for k, v in sorted(hash_dict[0].items(), key=lambda item: item[1], reverse=True)}\ndel sorted_hash_dict['no_tag'] \n\n\ntags_df = pd.DataFrame.from_dict(sorted_hash_dict, orient='index' ).reset_index().rename(columns={'index':\"hashtags\",0:\"count\"})\n\ntop_tags_df = tags_df[:20]\nfig = px.bar(top_tags_df, x='hashtags', y='count',  hover_data=['hashtags', 'count'], color='count',height=400, title=\"Hash Tags Trending\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"magenta\"><b>Inference - '#TheSocialDilemma' is the right hashtag trending very high (13k). There are couple of wrongly capitalized hashtags too. Next hashtag trending is #Netflix is only in hundreds.</b></font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"dil['text'] = dil['text'].apply(lambda x: x.lower())\n\nstopword = nltk.corpus.stopwords.words('english')\nstopword.extend(['thank','damn','always','might','well','smfh','li','yall','u','r','nt','ok','i', 'must','please','knew','go','brb','m', 'even','much','yes','hi','wow','the', 'frm','ah','us','of', 'on','also','us','okey','one', 'you', 'me', 'my', 'haa', 'erm','hey','okay', 'in', 'with', 'and', 'we', 'don','day', 'amp','re'])\n\ntags_array = []\nhashs_array = []\nurls_array = []\n\ndef separate_url_tag(txt):    \n    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', txt)\n    txt = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '',txt)\n    txt = re.sub(r'\\bamp\\b|\\bthi\\b|\\bha\\b',' ',txt)\n    if urls :\n        urls_array.append(\" \".join(urls))\n    \n    tags = re.findall(r\"@(\\w+)\", txt)\n    txt = re.sub(r\"@(\\w+)\", '',txt)\n    if tags :\n        tags_array.append(\" \".join(tags))\n    \n    hashs = re.findall(r\"#(\\w+)\", txt)\n    txt = re.sub(r\"#(\\w+)\", '',txt)\n    hashs_array.append(\" \".join(hashs))\n    \n    txt = re.sub('\\d+', '',txt)\n    txt = re.findall('\\w+', txt)\n    \n    txt = [word for word in txt if word not in stopword]\n    txt = \" \".join(txt)\n    return txt\n\ndil['cleaned_text'] = dil['text'].map(separate_url_tag)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"9\"></a>\n<font size=\"+2\" color=\"blue\"><b>Analyzing Mentions</b></font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"t_dict = {}\ndef create_tags_dict(tags_list):    \n    length = int(len(tags_list))    \n    for l in range(length):\n        key = tags_list[l]\n        \n        if key in t_dict.keys(): \n            t_dict[key] += 1\n        else:\n            t_dict[key] = 1  \n    return t_dict  \n\ntags_dict = create_tags_dict(tags_array)\ntags_dict\n\nsorted_tags_dict = {k: v for k, v in sorted(tags_dict.items(), key=lambda item: item[1], reverse=True)}\nsorted_tags_dict\n\ntag_df = pd.DataFrame.from_dict(sorted_tags_dict, orient='index' ).reset_index().rename(columns={'index':\"tags\",0:\"count\"})#,columns=['tags', 'count'])\n\ntop_tag_df = tag_df[:20]\nfig = px.bar(top_tag_df, x='tags', y='count',  hover_data=['tags', 'count'], color='count',height=400, title=\"Mentions used\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"magenta\"><b>Inference - '@netflix' is in mentions. The next mention is tristanharris. Tristan Harris is an American ethicist, computer scientist, and businessperson. He is the president and a co-founder of the Center for Humane Technology. Earlier, he worked as a design ethicist at Google. Other countries netflix are also mentioned like - netflixindia, netflixuk, netflixsa. There is also mention of rosenstein - Rosenstein is the founder of a nonprofit organization called One Project- using technology for social good as part of “one human project for global thriving”</b></font>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"10\"></a>\n<font size=\"+2\" color=\"blue\"><b>Analyzing Links</b></font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"l_dict = {}\ndef create_links_dict(tags_list):    \n    length = int(len(tags_list))    \n    for l in range(length):\n        key = tags_list[l]\n        \n        if key in l_dict.keys(): \n            l_dict[key] += 1\n        else:\n            l_dict[key] = 1  \n    return l_dict  \n\nlinks_dict = create_links_dict(urls_array)\nlinks_dict\n\nsorted_links_dict = {k: v for k, v in sorted(links_dict.items(), key=lambda item: item[1], reverse=True)}\nsorted_links_dict\n\nlinks_df = pd.DataFrame.from_dict(sorted_links_dict, orient='index' ).reset_index().rename(columns={'index':\"links\",0:\"count\"})#,columns=['tags', 'count'])\n\ntop_links_df = links_df[:20]\nfig = px.bar(top_links_df, x='links', y='count',  hover_data=['links', 'count'], color='count',height=400, title=\"Links used\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"magenta\"><b>Inference - There are bunch of links that are used twice.</b></font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"ps = nltk.PorterStemmer()\nwn = nltk.WordNetLemmatizer()\n\ndef stemming_lemmatizing(text):\n    text = [ps.stem(word) for word in text]\n    text = [wn.lemmatize(word) for word in text]\n    return text\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"11\"></a>\n<font size=\"+2\" color=\"blue\"><b>Most used words</b></font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating Word Clouds\nstopwords = set(STOPWORDS)\nstopwords.update([\"tweet\", \"please\"])\nwc = WordCloud(width=1400, height=800, min_word_length=4, stopwords= stopwords, max_words=200).generate(\"\".join(dil['cleaned_text']) )\nplt.figure(figsize=(12,10))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title('Most Used long Words in tweets',fontsize=35)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"12\"></a>\n<font size=\"+2\" color=\"blue\"><b>Unigrams,Bigrams and Trigrams</b></font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def ngram_df(corpus,nrange,n=None):\n    vec = CountVectorizer(stop_words = 'english',ngram_range=nrange).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    total_list=words_freq[:n]\n    df=pd.DataFrame(total_list,columns=['text','count'])\n    return df\nunigram_df=ngram_df(dil['cleaned_text'],(1,1),20)\nbigram_df=ngram_df(dil['cleaned_text'],(2,2),20)\ntrigram_df=ngram_df(dil['cleaned_text'],(3,3),20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(\n    rows=3, cols=1,subplot_titles=(\"Unigram\",\"Bigram\",'Trigram'),\n    specs=[[{\"type\": \"scatter\"}],\n           [{\"type\": \"scatter\"}],\n           [{\"type\": \"scatter\"}]\n          ])\n\nfig.add_trace(go.Bar(\n    y=unigram_df['text'][::-1],\n    x=unigram_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=unigram_df['count'],\n    textposition = \"outside\",\n    orientation=\"h\",\n    name=\"Months\",\n),row=1,col=1)\n\nfig.add_trace(go.Bar(\n    y=bigram_df['text'][::-1],\n    x=bigram_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=bigram_df['count'],\n     name=\"Days\",\n    textposition = \"outside\",\n    orientation=\"h\",\n),row=2,col=1)\n\nfig.add_trace(go.Bar(\n    y=trigram_df['text'][::-1],\n    x=trigram_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=trigram_df['count'],\n     name=\"Days\",\n    orientation=\"h\",\n    textposition = \"outside\",\n),row=3,col=1)\n\nfig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_layout(title_text='Top N Grams',xaxis_title=\" \",yaxis_title=\" \",\n                  showlegend=False,title_x=0.5,height=1200, template='plotly_dark')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"magenta\"><b>Inference - Tweets have been around encouraging people to watch, at the same time keepng kids safe onliine.</b></font>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"13\"></a>\n<font size=\"+2\" color=\"blue\"><b>Tweet length</b></font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"text_df = dil.copy()\ntext_df['text_length'] = text_df['cleaned_text'].map(lambda x : len(x))\n\nfig = go.Figure(data=go.Violin(y=text_df['text_length'], box_visible=True, line_color='black',\n                               meanline_visible=True, fillcolor='teal', opacity=0.7, x0='Tweet Text Length'))\n\nfig.update_layout(yaxis_zeroline=False,title=\"Distribution of Text length\",template='ggplot2')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"magenta\"><b>Inference - Mean and median tweet length is around 46 and 49 characters. Min and Max tweeth lengths are 0/105 characters</b></font>"},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"print(\"Average length of Positive Sentiment tweets : {}\".format(round(text_df[text_df['Sentiment']== 'Positive']['text_length'].mean(),2)))\nprint(\"Average length of Neutral Sentiment tweets : {}\".format(round(text_df[text_df['Sentiment']== 'Neutral']['text_length'].mean(),2)))\nprint(\"Average length of Negative Sentiment tweets : {}\".format(round(text_df[text_df['Sentiment']=='Negative']['text_length'].mean(),2)))\nfig = go.Figure()\n\nfig.add_trace(go.Violin(y=text_df[text_df['Sentiment']== 'Positive']['text_length'], box_visible=False, line_color='black',\n                               meanline_visible=True, fillcolor='limegreen', opacity=0.6,name=\"Positive\", x0='Positive')\n             )\n\nfig.add_trace(go.Violin(y=text_df[text_df['Sentiment']== 'Neutral']['text_length'], box_visible=False, line_color='black',\n                               meanline_visible=True, fillcolor='skyblue', opacity=0.6,name=\"Neutral\", x0='Neutral')\n             )\n\nfig.add_trace(go.Violin(y=text_df[text_df['Sentiment']== 'Negative']['text_length'], box_visible=False, line_color='black',\n                               meanline_visible=True, fillcolor='red', opacity=0.6,name=\"Negative\", x0='Negative')\n             )\n\nfig.update_traces(box_visible=False, meanline_visible=True)\nfig.update_layout(title_text=\"Violin - Tweet Length\",title_x=0.5)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"magenta\"><b>Inference - Mean/Median/Q1/Q3 tweet length for both positive and negative tweets is around the same. For neutral tweets, mean/median/Q1/Q3 are all lower than positive or negative tweets.</b></font>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"14\"></a>\n<font size=\"+2\" color=\"blue\"><b>Positive Tweets</b></font><br>"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"sentiment_country_pos_df=text_df[text_df['Sentiment']=='Positive']['country'].value_counts().reset_index().rename(columns={'index':'country','country':'count'})\ntop15_pos_sentiment = sentiment_country_pos_df[:15]\n\n# data is very noisy, so imputed some values\ntop15_pos_sentiment.insert(loc=2,column= \"code\", value=['USA','USA','IND','GBR','GBR','CAN','AUS','ZAF','IRL','PHL','GBR','KEN','PAK','DEU','IND']) \nfig = go.Figure(data=go.Choropleth(\n    locations = top15_pos_sentiment['code'],\n    z = top15_pos_sentiment['count'],   text = top15_pos_sentiment['country'],\n    colorscale = 'reds', autocolorscale=False,  reversescale=False,\n    marker_line_color='darkgray',\n    marker_line_width=0.8,     colorbar_title = '# of Tweets', ))\n\nfig.update_layout(\n    title_text='Positive Tweets over the world',title_x=0.5,\n    geo=dict(showframe=True, showcoastlines=False, projection_type='equirectangular',    ) )\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_country_pos_df = text_df[text_df['Sentiment']=='Positive']['cleaned_text'].reset_index()#.rename(columns={'country':'count'})\nsentiment_country_pos_df\n\n# Generating Word Clouds\nstopwords = set(STOPWORDS)\nstopwords.update([\"tweet\", \"please\"])\nwc = WordCloud(width=1600, height=800, min_word_length=4, stopwords= stopwords, max_words=200).generate(\"\".join(sentiment_country_pos_df['cleaned_text']) )\nplt.figure(figsize=(12,10))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title('Common Words in Positive tweets',fontsize=35)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"15\"></a>\n<font size=\"+2\" color=\"blue\"><b>Negative Tweets</b></font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_country_neg_df=text_df[text_df['Sentiment']=='Negative']['country'].value_counts().reset_index().rename(columns={'index':'country','country':'count'})\ntop15_neg_sentiment = sentiment_country_neg_df[:15]\ntop15_neg_sentiment\n\n# data is very noisy, so imputed some values\ntop15_neg_sentiment.insert(loc=2,column= \"code\", value=['USA','USA','IND','GBR','GBR','CAN','ZAF','AUS','IRL','GBR','PHL','IND','IDN','ESP','USA']) \ntop15_neg_sentiment\nfig = go.Figure(data=go.Choropleth(\n    locations = top15_neg_sentiment['code'],\n    z = top15_neg_sentiment['count'],  text = top15_neg_sentiment['country'],\n    colorscale = 'viridis',   autocolorscale=False,     reversescale=False,    marker_line_color='darkgray',\n    marker_line_width=0.8,    colorbar_title = '# of Tweets',\n))\n\nfig.update_layout(\n    title_text='Negative Tweets over the world',title_x=0.5,\n    geo=dict(       showframe=True,         showcoastlines=False,        projection_type='equirectangular',\n    ) )\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_country_neg_df = text_df[text_df['Sentiment']=='Negative']['cleaned_text'].reset_index()\nsentiment_country_neg_df\n\n# Generating Word Clouds\nstopwords = set(STOPWORDS)\nstopwords.update([\"tweet\", \"please\"])\nwc = WordCloud(width=1600, height=800, min_word_length=4, stopwords= stopwords, max_words=200).generate(\"\".join(sentiment_country_pos_df['cleaned_text']) )\nplt.figure(figsize=(12,10))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title('Common Words in Negative tweets',fontsize=35)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Friends - Please upvote if you like."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}