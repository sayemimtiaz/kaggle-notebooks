{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n- In this handson you will be building an deep neural network network for multiclass classification using softmax regression.\n- You will also be implementing minibatch gradient descent to train you network\n- Follow the instruction provided for cell to write the code in each cell."},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"- The data is provided as file named 'data.csv'.\n- The data has two features feature1 and feature2 and one targer variable which is a binary value\n- Using pandas read the csv file and assign the resulting dataframe to variable 'data'   \n- for example if file name is 'xyz.csv' read file as **pd.read_csv('xyz.csv')**\n- Packages to import: **pandas** (to read csv file)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndata = pd.read_csv('../input/data.csv')\nprint(data.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Extract feature1 and feature2 values from dataframe 'df' and assign it to variable 'X'\n- Extract target variable 'traget' and assign it to variable 'y'.  \nHint:\n - Use .values to exract values from dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX =  data[['feature1', 'feature2']].values                                        #Extract feature1 and feature2 values\nY =  data['class'].values                                        #Extract target values\n\nprint(\"target labels: \", list(set(Y)))\n\nassert X.shape == (2000, 2)\nassert Y.shape == (2000, )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- There are four target lables mapped as 0,1,2 and 3\n- Run the below cell to visualize the data in x-y plane. (visualization code has been written for you)\n- The blue spots corresponds to target value 0, green corresponds to 1, red corresponds to 2 and yellow to 4."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.colors\n%matplotlib inline\ncolors=['blue','green','red', 'yellow']\ncmap = matplotlib.colors.ListedColormap(colors)\n#Plot the figure\nplt.figure()\nplt.title('Non-linearly separable classes')\nplt.scatter(X[:,0], X[:,1], c=Y,\n           marker= 'o', s=50,cmap=cmap,alpha = 0.5 )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"import tensorflow package as tf"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\n#import tensorflow","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- perform one hot encoding on target Y. Use tensorflow's one_hot() function, make sure mapping is done column-wise"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = list(set(Y))\ndepth =  len(np.unique(Y))                  #number of unique labels\n\nprint(\"labels: \", labels)\nwith tf.Session() as sess:\n    YtrainOneHot = tf.one_hot(Y, depth, axis = 0) # columnwise: axis = 0, axis=-1 row-wise\n    Y_onehot = sess.run(YtrainOneHot)\nassert Y_onehot.shape == (4, 2000)\n\nprint(\"\\nfirst five rows of target Y:\\n\", Y[:5])\nprint(\"\\nfirst five rows of target Y_onehot:\\n\", Y_onehot[:,:5])\nprint(\"X dimension:{} ,Y_onehot dimension:{}\".format(X.shape, Y_onehot.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- In order to feed the network the input has to be of **shape (number of features, number of samples)** and target should be of shape **(1, number of samples)**\n- Transpose X and assign it to variable 'X_data'\n- target Y_onehot is already in shape, so just assign Y to variable Y_data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_data = X.T\nY_data = Y_onehot\nprint(Y_data.shape, Y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define the network dimension to have two input features, two hidden layers with 25 nodes each, 4 output nodes at final layer. "},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlayer_dims = [2, 25, 25, 4]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define a function named placeholders to return two placeholders one for input data as A_0 and one for output data as Y.\n- Set the datatype of placeholders as float64\n- parameters - num_features\n- Returns - A_0 with shape (num_feature, None) and Y with shape(1,None)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def placeholders(num_features, num_classes):\n    A_0 = tf.placeholder(dtype = tf.float64, shape = ([num_features,None]))\n    Y = tf.placeholder(dtype = tf.float64, shape = ([num_classes,None]))\n    return A_0,Y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define function named initialize_parameters_deep() to initialize weights and bias for each layer\n- Use tf.get_variable to initialise weights and bias, set datatype as float64\n- Make sure you are using xavier initialization for weigths and initialize bias to zeros\n- Parameters - layer_dims\n- Returns - dictionary of weights and bias"},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_parameters_deep(layer_dims):\n    tf.set_random_seed(1)\n    L = len(layer_dims)\n    parameters = {}\n    for l in range(1,L):\n        parameters['W' + str(l)] = tf.get_variable(\"W\" + str(l), shape=[layer_dims[l], layer_dims[l-1]], dtype = tf.float64,\n                                   initializer=tf.contrib.layers.xavier_initializer())\n        parameters['b' + str(l)] = tf.get_variable(\"b\"+ str(l), shape = [layer_dims[l], 1], dtype= tf.float64, initializer= tf.zeros_initializer() )\n    return parameters ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define functon named linear_forward_prop() to define forward propagation for a given layer.\n- parameters: A_prev(output from previous layer), W(weigth matrix of current layer), b(bias vector for current layer),activation(type of activation to be used for out of current layer)  \n- returns: A(output from the current layer)\n- Use relu activation for hidden layers and for final output layer return the output unactivated i.e if activation is 'softmax'"},{"metadata":{"trusted":true},"cell_type":"code","source":"def linear_forward_prop(A_prev,W,b, activation):\n    Z =   tf.add(tf.matmul(W, A_prev), b)    \n    if activation == \"softmax\":\n        A = Z\n    elif activation == \"relu\":\n        A =  tf.nn.relu(Z) \n    return A","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"forward propagation for entire network is defined for you as l_layer_forward(). We are not using any regularization technique here."},{"metadata":{"trusted":true},"cell_type":"code","source":"def l_layer_forwardProp(A_0, parameters):\n    A = A_0\n    L = len(parameters)//2\n    for l in range(1,L):\n        A_prev = A\n        A = linear_forward_prop(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n    A = linear_forward_prop(A, parameters['W' + str(L)], parameters['b' + str(L)], \"softmax\" )\n    return A","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Define cost function as final_cost()\n- The logits and target value has to be in shape (number of samples, number of class) for      \nsoftmax_cross_entropy_with_logits() function to calculate cost\n- since Z_final and Y are tensoflow object we are using tf.transpose() before feeding  softmax_cross_entropy_with_logits()"},{"metadata":{"trusted":true},"cell_type":"code","source":"def final_cost(Z_final, Y ):\n    logits = tf.transpose(Z_final)\n    labels = tf.transpose(Y)\n    ###Start code\n    cost = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels)                  #use tensorflow's softmax_cross_entropy to calculate cost\n    ###End code\n    return tf.reduce_mean(cost)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define the function to generate mini-batches."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\ndef random_samples_minibatch(X, Y, batch_size, seed = 1):\n    np.random.seed(seed)\n    m = X.shape[1]                                 #number of samples\n    num_batches = int(m/batch_size)                #number of batches derived from batch_size\n    indices = np.random.permutation(m)             # generate ramdom indicies\n    shuffle_X = X[:,indices]\n    shuffle_Y = Y[:,indices]\n    mini_batches = []\n    \n    #generate minibatch\n    for i in range(num_batches):\n        ##Start code here\n        X_batch = shuffle_X[:,i * batch_size:(i+1) * batch_size]\n        Y_batch = shuffle_Y[:,i * batch_size:(i+1) * batch_size]\n        ##End code\n        \n        assert X_batch.shape == (X.shape[0], batch_size)\n        assert Y_batch.shape == (Y.shape[0], batch_size)\n        \n        mini_batches.append((X_batch, Y_batch))\n    \n    #generate batch with remaining number of samples\n    if m % batch_size != 0:\n        ##Srart code here\n        X_batch = shuffle_X[:, (num_batches * batch_size):]\n        Y_batch = shuffle_Y[:, (num_batches * batch_size):]\n        ##Srart code here\n        mini_batches.append((X_batch, Y_batch))\n    return mini_batches","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define the model to train the network using minibatch\n- parameters:\n  - X_train, Y_train: input and target data\n  - layer_dims: network configuration\n  - learning_rate\n  - num_iter: number of epoches\n  - mini_batch_size: number of samples to be considered in each minibatch\n- return: dictionary of trained parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_with_minibatch(X_train,Y_train, layer_dims, learning_rate, num_iter, mini_batch_size):\n    \n    tf.reset_default_graph()\n    num_features, num_samples = X_train.shape\n    num_classes = Y_train.shape[0]\n    A_0, Y = placeholders(num_features, num_classes)\n    parameters = initialize_parameters_deep(layer_dims)\n    Z_final = l_layer_forwardProp(A_0, parameters)\n    cost = final_cost(Z_final, Y)\n    train_net = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n    seed = 1\n    num_minibatches = int(num_samples / mini_batch_size)\n    init = tf.global_variables_initializer()\n    costs = []\n    \n    with tf.Session() as sess:\n        sess.run(init)\n        for epoch in range(num_iter):\n            epoch_cost = 0\n            mini_batches = random_samples_minibatch(X_train, Y_train, mini_batch_size, seed) #generate array of minibatch using random_samples_minibatch()\n            seed = seed + 1\n            \n            #perform gradient descent for each mini-batch\n            for mini_batch in mini_batches: \n                ##Start code\n                X_batch, Y_batch = mini_batch\n                _,mini_batch_cost = sess.run([train_net, cost], feed_dict={A_0: X_batch, Y: Y_batch})\n                ##End code\n                \n                epoch_cost += mini_batch_cost/num_minibatches\n            if epoch % 1 == 0:\n                costs.append(epoch_cost)\n            if epoch % 10 == 0:\n                print(epoch_cost)\n        with open('output.txt', 'w') as file:\n            file.write(\"cost = %f\" % costs[-1])\n        plt.ylim(0, max(costs), 0.0001)\n        plt.xlabel(\"epoches per 100\")\n        plt.ylabel(\"cost\")\n        plt.plot(costs)\n        plt.show()\n        params = sess.run(parameters)\n    return params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = model_with_minibatch(X.T, Y_onehot, layer_dims, 0.01, 100, 32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The function to predict the ouput has been defined for you.  \n- The function takes input and trained parameters and predict the output using forward propagation  \n- the arg_max function returns the index of largest probability value in out y of each sample  "},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(X_train, params):\n    with tf.Session() as sess:\n        Y = tf.arg_max(l_layer_forwardProp(X_train,params), dimension=0)\n        return sess.run(Y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run the next two cell to visualize the boundary predicted by the model (This takes slightly longer time for the plot to appear)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_decision_boundary1( X, y, model):\n    plt.clf()\n    # Set min and max values and give it some padding\n    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1   \n    colors=['blue','green','red', 'yellow']\n    cmap = matplotlib.colors.ListedColormap(colors)   \n    h = 0.01\n    # Generate a grid of points with distance h between them\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    # Predict the function value for the whole grid\n    A = model(np.c_[xx.ravel(), yy.ravel()])\n    A = A.reshape(xx.shape)\n    # Plot the contour and training examples\n    plt.contourf(xx, yy, A, cmap=\"spring\")\n    plt.ylabel('x2')\n    plt.xlabel('x1')\n    plt.scatter(X[0, :], X[1, :], c=y, s=8,cmap=cmap)\n    plt.title(\"Decision Boundary\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_decision_boundary1(X_data,Y,lambda x: predict(x.T,params))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}