{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Dataset Information\nThis dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Content\nThere are 25 variables:\n* ID: ID of each client\n* LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit\n* SEX: Gender (1=male, 2=female)\n* EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n* MARRIAGE: Marital status (1=married, 2=single, 3=others)\n* AGE: Age in years\n* PAY_0: Repayment status in September 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, … 8=payment delay for eight months, 9=payment delay for nine months and above)\n* PAY_2: Repayment status in August 2005 (scale same as above)\n* PAY_3: Repayment status in July 2005 (scale same as above)\n* PAY_4: Repayment status in June 2005 (scale same as above)\n* PAY_5: Repayment status in May 2005 (scale same as above)\n* PAY_6: Repayment status in April 2005 (scale same as above)\n* BILL_AMT1: Amount of bill statement in September 2005 (NT dollar)\n* BILL_AMT2: Amount of bill statement in August 2005 (NT dollar)\n* BILL_AMT3: Amount of bill statement in July 2005 (NT dollar)\n* BILL_AMT4: Amount of bill statement in June 2005 (NT dollar)\n* BILL_AMT5: Amount of bill statement in May 2005 (NT dollar)\n* BILL_AMT6: Amount of bill statement in April 2005 (NT dollar)\n* PAY_AMT1: Amount of previous payment in September 2005 (NT dollar)\n* PAY_AMT2: Amount of previous payment in August 2005 (NT dollar)\n* PAY_AMT3: Amount of previous payment in July 2005 (NT dollar)\n* PAY_AMT4: Amount of previous payment in June 2005 (NT dollar)\n* PAY_AMT5: Amount of previous payment in May 2005 (NT dollar)\n* PAY_AMT6: Amount of previous payment in April 2005 (NT dollar)\n* default.payment.next.month: Default payment (1=yes, 0=no)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"You have noticed that we have that 25 data features to deal with and we will mainly use the machine learning models:\n* **Logistic Regression**\n* **Random Forest Classifier**\n* **XGBoost Classifier**\n\nI hope that you have the data handy and let's dive into the coding part,","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **1. Import Necessary Packages**\nAt first, import the necessary packages that you think will help us for predicting results.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport sys\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Load Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/default-of-credit-card-clients-dataset/UCI_Credit_Card.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After loading the dataset, just have a look at the first five rows of the dataset. Now we try to execute some basic computations to understand the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Look at the number and name of columns in dataset.\nprint(df.columns)\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check the info() of the dataset whether all the columns in dataset have the same datatype or not.\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's pretty great, we have columns of datatype int64 and float64 only. There is no object type data feature. Now let's check whether our dataset has missing values or not.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking for missing values\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok! that's even great we don't have any missing values to handle. \n# 3. Data Analysis\nLet's have a look at the target variable \"default.payment.next.month\" and distribution of that feature data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check the distribution of data\ndf['default.payment.next.month'].value_counts().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above result, you can conclude that many of the clients are not interested in a payment next month.\nNow let's go through some quick data analysis and look at the distribution of data of the other data features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['SEX'].value_counts().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It finds that The number of Male credit holder is less than Female.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['AGE'],kde=True,bins=30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a large number of clients whose age is between 25 to 40.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['EDUCATION'].value_counts().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like most of the client's education level belongs to category 2,1 and 3.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['MARRIAGE'].value_counts().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We noticed that there is very less number of values for category 3 and 0.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='SEX', data=df,hue=\"default.payment.next.month\", palette=\"muted\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For females, the count of default.payment.next.month = 0 is highter than males.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='EDUCATION',data=df,hue=\"default.payment.next.month\",palette=\"muted\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='MARRIAGE',data=df,hue=\"default.payment.next.month\", palette=\"muted\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Almost there is an equal number of clients who default the payment next month for both the Married and Single category.\n\nI would appreciate it if you go much deeper into the univariate and bivariate analysis. As this dataset is for pratice purpose then learn by doing more investigations at personnel level.\n\nNow let's do some data pre-processing steps and find some interesting patterns in the dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 4. Data Processing\nLet's extract some insights for some data features if they want to tell us something. We will find it out.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['PAY_0'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is some double-digit count of values of the PAY_0 data feature and for some data features also. So we will create one single category of all low categories having less count.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fill = (df.PAY_0 == 4) | (df.PAY_0==5) | (df.PAY_0==6) | (df.PAY_0==7) | (df.PAY_0==8)\ndf.loc[fill,'PAY_0']=4\ndf.PAY_0.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's do it for the rest of the data features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fill = (df.PAY_2 == 4) | (df.PAY_2 == 1) | (df.PAY_2 == 5) | (df.PAY_2 == 7) | (df.PAY_2 == 6) | (df.PAY_2 == 8)\ndf.loc[fill,'PAY_2']=4\n#df.PAY_2.value_counts()\nfill = (df.PAY_3 == 4) | (df.PAY_3 == 1) | (df.PAY_3 == 5) | (df.PAY_3 == 7) | (df.PAY_3 == 6) | (df.PAY_3 == 8)\ndf.loc[fill,'PAY_3']=4\n#df.PAY_3.value_counts()\nfill = (df.PAY_4 == 4) | (df.PAY_4 == 1) | (df.PAY_4 == 5) | (df.PAY_4 == 7) | (df.PAY_4 == 6) | (df.PAY_4 == 8)\ndf.loc[fill,'PAY_4']=4\n#df.PAY_4.value_counts()\nfill = (df.PAY_5 == 4) | (df.PAY_5 == 7) | (df.PAY_5 == 5) | (df.PAY_5 == 6) | (df.PAY_5 == 8)\ndf.loc[fill,'PAY_5']=4\n#df.PAY_5.value_counts()\nfill = (df.PAY_6 == 4) | (df.PAY_6 == 7) | (df.PAY_6 == 5) | (df.PAY_6 == 6) | (df.PAY_6 == 8)\ndf.loc[fill,'PAY_6']=4\n#df.PAY_6.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now when you look at data there are some data features that have values on a large scale like bill_amt variables and many more. So we need to scale that variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns = df.columns.map(str.lower)\ncol_to_norm = ['limit_bal', 'age', 'bill_amt1', 'bill_amt2', 'bill_amt3', 'bill_amt4', 'bill_amt5', 'bill_amt6', 'pay_amt1', 'pay_amt2', 'pay_amt3', 'pay_amt4', 'pay_amt5', 'pay_amt6']\n#you can inbuilt StandardScalar() or MinMaxScalar() also\ndf[col_to_norm] = df[col_to_norm].apply(lambda x :( x-np.mean(x))/np.std(x))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great! Feature scaling is done.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 5. Correlation\nNow we check the correlation of the independent variables with our target(dependent) variable.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation = df.corr()\nplt.subplots(figsize=(30,10))\nsns.heatmap(correlation, square=True, annot=True, fmt=\".1f\" )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the heatmap, you figured out that target variable d**efault.payment.next.month** depends on **pay variables** more. But I don't suggest you drop the other features because it will be the loss of information. You can have a try of training the model with the most dependent features and evaluate the model also.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 6. Predictive Modeling\nOkk! now move towards predictive modeling. First, we split the training data into train and test using train_test_split().","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop([\"id\"],1)\nX = df.iloc[:,:-1].values\ny = df.iloc[:,-1].values\n#We split the data into train(0.75) and test(0.25) size.\n \nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.25,random_state = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's apply different machine learning models and evaluate the accuracy of the model.\n\n* **Logistic Regression Model**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Start with logistic regression model\nfrom sklearn.linear_model import LogisticRegression\nlogmodel = LogisticRegression(random_state=1)\nlogmodel.fit(X_train,y_train)\ny_pred = logmodel.predict(X_test)\nfrom sklearn.metrics import  accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\nroc=roc_auc_score(y_test, y_pred)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nresults = pd.DataFrame([['Logistic Regression', acc,prec,rec, f1,roc]],\ncolumns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting the confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's a great attempt, we got the accuracy of 0.8088. Let's apply some different models also,\n\n* **Random Forest Classifier Model**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Apply Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators = 100,criterion = 'entropy',random_state = 0)\nrfc.fit(X_train,y_train)\ny_pred = rfc.predict(X_test)\nroc=roc_auc_score(y_test, y_pred)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nresults = pd.DataFrame([['Random tree Classifier', acc,prec,rec, f1,roc]],\ncolumns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting the confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's a good improvement, let's try with another model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Apply XGBoost classifier model\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier()\nxgb.fit(X_train, y_train)\ny_pred =xgb.predict(X_test)\nroc=roc_auc_score(y_test, y_pred)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nresults = pd.DataFrame([['XGBOOST Classifier', acc,prec,rec, f1,roc]],\ncolumns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting the confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now you can evaluate that our XGBoost classifier model got a higher accuracy of 0.814933.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Observations\nUsing the three classifier models, the accuracy we obtained is as follows:\n* Logistic Regression: 0.8088\n* Random Forest Classifier: 0.8144\n* XGBoost Classifier: 0.8149","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Key Takeaways\n* Apply more different classifier models and evaluate them.\n* Perform feature engineering and train the model with more relevant features.\n* Apply hyperparameter tuning, get the best parameters, and obtain greater accuracy.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}