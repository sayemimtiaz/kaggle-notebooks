{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The Convolutional Classifier"},{"metadata":{},"cell_type":"markdown","source":"* Use modern `deep-learning` networks to build an `image classifier` with Keras\n* Design your own `custom convnet` with reusable blocks\n* Learn the fundamental ideas behind visual `feature extraction`\n* Master the art of `transfer learning` to boost your models\n* Utilize `data augmentation` to extend your dataset\n\nGoal is to learn how a neural network can understand a natural image well-enough to solve the same kinds of problems the human visual system can solve. \n\n`Convolutional neural networks, convnet, CNN` is the best network for this task. Convolution is the mathematical operation that gives the layers of a convnet their unique structure. This `CNN` will be applied to `image classification`. At the end, should learn dvanced applications like `generative adversarial networks` and `image segmentation`."},{"metadata":{},"cell_type":"markdown","source":"### The Convolutional Classifier\nA convnet used for image classification consists of two parts: a `convolutional base` and a `dense head`.\n![](https://i.imgur.com/U0n5xjU.png)\n- The base is used to `extract the features` from an image.\n- The head is used to `determine the class` of the image. \n\nVisual feature could be a line, a color, a texture, a shape, a pattern -- or some complicated combination.\n![](https://i.imgur.com/UUAafkn.png)"},{"metadata":{},"cell_type":"markdown","source":"### Training the Classifier\nThe goal of the network during training is to learn two things:\n1. which features to extract from an image (base),\n2. which class goes with what features (head).\n\nConvnets are rarely trained form scratch. Mostly `reuse the base of a pretrained model`. To the pretrained base can attach an `untrained head`. \n![](https://imgur.com/E49fsmV.png)\nThe head usually consists of only a few dense layers, very accurate classifiers can be created from relatively little data. Reusing a pretrained model is a technique known as `transfer learning`. "},{"metadata":{},"cell_type":"markdown","source":"### Example - Train a Convnet Classifier\nWill create classifiers to solve: `is this a picture of a Car or of a Truck?`. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step1: load the data: training split `ds_train` and validation split `ds_valid`\nimport os, warnings\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\n\n# Reproducability\ndef set_seed(seed=31415):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nset_seed(31415)\n\n# Set Matplotlib defaults\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\nplt.rc('image', cmap='magma')\nwarnings.filterwarnings(\"ignore\") # to clean up output cells\n\n\n# Load training and validation sets\nds_train_ = image_dataset_from_directory(\n    '../input/car-or-truck/train',\n    labels='inferred',\n    label_mode='binary',\n    image_size=[128, 128],\n    interpolation='nearest',\n    batch_size=64,\n    shuffle=True,\n)\nds_valid_ = image_dataset_from_directory(\n    '../input/car-or-truck/valid',\n    labels='inferred',\n    label_mode='binary',\n    image_size=[128, 128],\n    interpolation='nearest',\n    batch_size=64,\n    shuffle=False,\n)\n\n# Data Pipeline\ndef convert_to_float(image, label):\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image, label\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nds_train = (\n    ds_train_\n    .map(convert_to_float)\n    .cache()\n    .prefetch(buffer_size=AUTOTUNE)\n)\nds_valid = (\n    ds_valid_\n    .map(convert_to_float)\n    .cache()\n    .prefetch(buffer_size=AUTOTUNE)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The most commonly used dataset for pretraining is [ImageNet](http://image-net.org/about-overview), a large dataset of many kind of natural images. Keras includes a variety models pretrained on ImageNet in its `applications` [module](https://www.tensorflow.org/api_docs/python/tf/keras/applications). The pretrained model will use is called `VGG16` which can import from [CV_Course_Models](https://www.kaggle.com/ryanholbrook/cv-course-models)"},{"metadata":{},"cell_type":"markdown","source":"`pretrained_base.trainable = False` since in transfer learning, the loaded `pretrained_base` is already trained and no need to train again."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step2: Define Pretrained Base\npretrained_base = tf.keras.models.load_model(\n    '../input/cv-course-models/cv-course-models/vgg16-pretrained-base',\n)\npretrained_base.trainable = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step3: Attach Classifier Head\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# `Flatten` layer transforms the two dimensional outputs of the base \n# into the one dimensional inputs needed by the head\nmodel = keras.Sequential([\n    pretrained_base,\n    layers.Flatten(),\n    layers.Dense(6, activation='relu'),\n    layers.Dense(1, activation='sigmoid'),\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step4: Train the model\n# use the binary versions of crossentropy and accuracy\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy'],\n)\n\nhistory = model.fit(\n    ds_train,\n    validation_data=ds_valid,\n    epochs=30,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When training a neural network, it's always a good idea to examine the `loss` and `metric` plots. The `history` object contains this information in a dictionary `history.history`."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\nhistory_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss', 'val_loss']].plot()\nhistory_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Learned about the structure of a convnet classifier: a `head` to act as a classifier atop of a `base` which performs the feature extraction. The head is an ordinary classifier. It uses those features extracted by the base. This is the basic idea behind convolutional classifiers: that we can attach a unit that performs feature engineering to the classifier itself.\n\n"},{"metadata":{},"cell_type":"markdown","source":"### Exercise: The Convolutional Classifier\nUsed [car-or-truck](https://www.kaggle.com/ryanholbrook/car-or-truck), [computer-vision-resources](https://www.kaggle.com/ryanholbrook/computer-vision-resources) and [cv-courses-models](https://www.kaggle.com/ryanholbrook/cv-course-models) datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"from learntools.core import binder\nbinder.bind(globals())\nfrom learntools.computer_vision.ex1 import *\n\n# Imports\nimport os, warnings\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\n\n# Reproducability\ndef set_seed(seed=31415):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nset_seed()\n\n# Set Matplotlib defaults\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\nplt.rc('image', cmap='magma')\nwarnings.filterwarnings(\"ignore\") # to clean up output cells\n\n\n# Load training and validation sets\nds_train_ = image_dataset_from_directory(\n    '../input/car-or-truck/train',\n    labels='inferred',\n    label_mode='binary',\n    image_size=[128, 128],\n    interpolation='nearest',\n    batch_size=64,\n    shuffle=True,\n)\nds_valid_ = image_dataset_from_directory(\n    '../input/car-or-truck/valid',\n    labels='inferred',\n    label_mode='binary',\n    image_size=[128, 128],\n    interpolation='nearest',\n    batch_size=64,\n    shuffle=False,\n)\n\n# Data Pipeline\ndef convert_to_float(image, label):\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image, label\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nds_train = (\n    ds_train_\n    .map(convert_to_float)\n    .cache()\n    .prefetch(buffer_size=AUTOTUNE)\n)\nds_valid = (\n    ds_valid_\n    .map(convert_to_float)\n    .cache()\n    .prefetch(buffer_size=AUTOTUNE)\n)\n\n# Load InceptionV1 model pretrained on ImageNet\nimport tensorflow_hub as hub\n\npretrained_base = tf.keras.models.load_model(\n    '../input/cv-course-models/cv-course-models/inceptionv1'\n)\npretrained_base.trainable = False\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    pretrained_base,\n    layers.Flatten(),\n    layers.Dense(6, activation='relu'),\n    layers.Dense(1, activation='sigmoid')\n])\noptimizer = tf.keras.optimizers.Adam(epsilon=0.01)\nmodel.compile(\n    optimizer=optimizer,\n    loss = 'binary_crossentropy',\n    metrics=['binary_accuracy'],\n)\nhistory = model.fit(\n    ds_train,\n    validation_data=ds_valid,\n    epochs=30,\n)\n\nimport pandas as pd\nhistory_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss', 'val_loss']].plot()\nhistory_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Convolution and ReLU\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom itertools import product\n\ndef show_kernel(kernel, label=True, digits=None, text_size=28):\n    # Format kernel\n    kernel = np.array(kernel)\n    if digits is not None:\n        kernel = kernel.round(digits)\n\n    # Plot kernel\n    cmap = plt.get_cmap('Blues_r')\n    plt.imshow(kernel, cmap=cmap)\n    rows, cols = kernel.shape\n    thresh = (kernel.max()+kernel.min())/2\n    # Optionally, add value labels\n    if label:\n        for i, j in product(range(rows), range(cols)):\n            val = kernel[i, j]\n            color = cmap(0) if val > thresh else cmap(255)\n            plt.text(j, i, val, \n                     color=color, size=text_size,\n                     horizontalalignment='center', verticalalignment='center')\n    plt.xticks([])\n    plt.yticks([])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convolutional classifier has two parts: a convolutional `base` and a `head` of dense layers. The job of the base is to extract visual features from an image, which the head would then use to classify the image.\n.\nWill learn about the two most important types of layers that found in the base of  convolutional image classifier: `convolutional layer` with `ReLU activation` and `maximum pooling layer`"},{"metadata":{},"cell_type":"markdown","source":"### Feature Extraction\nThe `feature extraction` performed by the base consists of three basic operations:\n- `Filter` an image for a particular feature (convolution)\n- `Detect` that feature within the filtered image (ReLU)\n- `Condense` the image to enhance the features (maximum pooling)\n\n![](https://i.imgur.com/IYO9lqp.png)\nThe figure shows how these three operations are able to isolate some particular characteristic of the original image.\n- Network will perform several extractions in parallel on a single image.\n- Not uncommon for the final layer in the base to be producing over 1000 unique visual features."},{"metadata":{},"cell_type":"markdown","source":"### Filter with Convolution\nA convolutional layer carries out the `filtering` step. Convolutional layer in Keras can be define as:\n```\nlayers.Conv2D(filters=64, kernel_size=3)\n```\n"},{"metadata":{},"cell_type":"markdown","source":"#### Weights\nThe `weights`, a convnet learns during training are primarily contained in its convolutional layers. These weights are called `kernels`. ![](https://i.imgur.com/uJfD9r9.png)\n\nA kernel operates by scanning over an image and producing a weighted sum of pixel values. Kernel will act sort of like a polarized lens, emphasizing or deemphasizing certain patterns of information.\n<figure>\n<img src=\"https://i.imgur.com/j3lk26U.png\" width=\"300\">\n</figure>\n\n- Kernels define how a convolutional layer is connected to the layer. In above figure:\n    - kernal will connect each neuron in the output to nine neurons in the input.\n    - By setting the dimensions of the kernels with `kernel_size`, telling the convnet how to form these connections.\n    - Most often, a kernel will have odd-numbered dimensions --> `kernel_size=(3, 3)` or `(5, 5)`\n\nThe kernels in a convolutional layer determine what kinds of features it creates. During training, a convnet tries to learn what features it needs to solve the classification problem. "},{"metadata":{},"cell_type":"markdown","source":"#### Activations\nThe `activations` in the network are called `feature maps`. They are what result when apply a filter to an image; contain the visual features the kernel extracts.\n<figure>\n<img src=\"https://i.imgur.com/JxBwchH.png\" width=\"650\">\n</figure>\n\nWhat a convolution accentuates in its inputs will match the shape of the positive numbers in the kernel.\n- left and middle kernels will filter for horizontal shapes.\n- With the `filters` parameter, can tell the convolutional layer how many feature maps want it to create as output."},{"metadata":{},"cell_type":"markdown","source":"### Detect with ReLU\nAfter filtering, the feature maps pass through the activation function. The `rectifier function` has a graph like this:\n\n<figure>\n    <img src=\"https://i.imgur.com/DxGJuTH.png\", width=450>\n</figure>\n\nA neuron with a rectifier attached is called a `rectified linear unit`. The `ReLU activation` can be defined in its own `Activation layer`, but most often can include the activation function in `Conv2D`.\n```\nlayers.Conv2D(filters=64, kernel_size=3, activation='relu')\n```\n\n`Activation function` as scoring pixel values according to some measure of importance. The `ReLU activation` maps negative values to `0`.\n<figure>\n    <img src=\"https://i.imgur.com/dKtwzPY.png\", width=650>\n</figure>\n\nLike other activation functions, `ReLU` function is nonlinear. Total effect of all the layers in the network becomes different than by just adding effects together. The nonlinearity ensures features will combine in interesting ways as they move deeper into the network."},{"metadata":{},"cell_type":"markdown","source":"### Example - Apply Convolution and ReLU\nWill do extraction to understand better what convolutional networks are `behind the scenes`. This image will be used."},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\nplt.rc('image', cmap='magma')\n\nimage_path = '../input/computer-vision-resources/car_feature.jpg'\nimage = tf.io.read_file(image_path)\nimage = tf.io.decode_jpeg(image)\n\nplt.figure(figsize=(6, 6))\nplt.imshow(tf.squeeze(image), cmap='gray')\nplt.axis('off')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nFor the filtering step, define a kernel and apply convolution.\n- kernel is for `edge detection` and define with `tf.constant`\n- try to keep the sum of the numbers between 0 and 1\n'''\n\nimport tensorflow as tf\nimport learntools.computer_vision.visiontools as visiontools\n\nkernel = tf.constant([\n    [-1, -1, -1],\n    [-1, 8, -1],\n    [-1, -1, -1],\n])\n\n# plt.figure(figsize=(3, 3))\nvisiontools.show_kernel(kernel)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"TensorFlow includes many common operations performed by neural networks in its [tf.nn module](https://www.tensorflow.org/api_docs/python/tf/nn). `conv2d` and `relu` will be used. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reformat for batch compatibility.\nimage = tf.image.convert_image_dtype(image, dtype=tf.float32)\nimage = tf.expand_dims(image, axis=0)\nkernel = tf.reshape(kernel, [*kernel.shape, 1, 1])\nkernel = tf.cast(kernel, dtype=tf.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# conv2d\nimage_filter = tf.nn.conv2d(\n    input=image,\n    filters=kernel,\n\n    strides=1,\n    padding='SAME',\n)\n\nplt.figure(figsize=(6, 6))\nplt.imshow(tf.squeeze(image_filter))\nplt.axis('off')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ReLU after conc2d\nimage_detect = tf.nn.relu(image_filter)\nf1 = plt.figure(1)\nplt.imshow(tf.squeeze(image_detect))\nplt.axis('off')\nplt.title(\"Figure for ReLU after conv2d\")\n\n# ----------------extra task--------------- \n# ReLU to original photo\nimage_detect = tf.nn.relu(image)\nf2= plt.figure(2)\nplt.imshow(tf.squeeze(image_detect))\nplt.axis('off')\nplt.title(\"Figure for ReLU without conv2d\")\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Images like these are what the head uses to solve its classification problem. \n\nFeature extraction: filter with `Conv2D` layers and detect with `relu` activation."},{"metadata":{},"cell_type":"markdown","source":"### Exercise: Convolution and ReLU\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"image_path = '../input/computer-vision-resources/car_illus.jpg'\nimage = tf.io.read_file(image_path)\nimage = tf.io.decode_jpeg(image, channels=1)\nimage = tf.image.resize(image, size=[400, 400])\n\nimg = tf.squeeze(image).numpy()\nplt.figure(figsize=(6, 6))\nplt.imshow(img, cmap='gray')\nplt.axis('off')\nplt.show();\n\n\nimport learntools.computer_vision.visiontools as visiontools\nfrom learntools.computer_vision.visiontools import edge, bottom_sobel, emboss, sharpen\n\nkernels = [edge, bottom_sobel, emboss, sharpen]\nnames = [\"Edge Detect\", \"Bottom Sobel\", \"Emboss\", \"Sharpen\"]\n\nplt.figure(figsize=(12, 12))\nfor i, (kernel, name) in enumerate(zip(kernels, names)):\n    plt.subplot(1, 4, i+1)\n    visiontools.show_kernel(kernel)\n    plt.title(name)\nplt.tight_layout()\n\n\nkernel = tf.constant([\n    [-0.5, -1.5, -0.5],\n    [1.5, 2, 1.5],\n    [-0.5, -1.5, -0.5]\n])\nvisiontools.show_kernel(kernel)\n\n\nimage = tf.image.convert_image_dtype(image, dtype=tf.float32)\nimage = tf.expand_dims(image, axis=0)\nkernel = tf.reshape(kernel, [*kernel.shape, 1, 1])\nkernel = tf.cast(kernel, dtype=tf.float32)\n\n# -------------------------------------\nimage_filter = conv_fn(\n    input=image,\n    filters=kernel,\n    strides=1, # or (1, 1)\n    padding='SAME',\n)\nplt.imshow(\n    # Reformat for plotting\n    tf.squeeze(image_filter)\n)\nplt.axis('off')\nplt.show();\n\n# -------------------------------------\nimage_detect = relu_fn(image_filter)\nplt.imshow(\n    # Reformat for plotting\n    tf.squeeze(image_detect)\n)\nplt.axis('off')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observe Convolution on a Numerical Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sympy is a python library for symbolic mathematics. It has a nice\n# pretty printer for matrices, which is all we'll use it for.\nimport sympy\nsympy.init_printing()\nfrom IPython.display import display\n\nimage = np.array([\n    [0, 1, 0, 0, 0, 0],\n    [0, 1, 0, 0, 0, 0],\n    [0, 1, 0, 0, 0, 0],\n    [0, 1, 0, 0, 0, 0],\n    [0, 1, 0, 1, 1, 1],\n    [0, 1, 0, 0, 0, 0],\n])\n\nkernel = np.array([\n    [1, -1],\n    [1, -1],\n])\n\ndisplay(sympy.Matrix(image))\ndisplay(sympy.Matrix(kernel))\n# Reformat for Tensorflow\nimage = tf.cast(image, dtype=tf.float32)\nimage = tf.reshape(image, [1, *image.shape, 1])\nkernel = tf.reshape(kernel, [*kernel.shape, 1, 1])\nkernel = tf.cast(kernel, dtype=tf.float32)\n\nimage_filter = tf.nn.conv2d(\n    input=image,\n    filters=kernel,\n    strides=1,\n    padding='VALID',\n)\nimage_detect = tf.nn.relu(image_filter)\n\n# The first matrix is the image after convolution, and the second is\n# the image after ReLU.\ndisplay(sympy.Matrix(tf.squeeze(image_filter).numpy()))\ndisplay(sympy.Matrix(tf.squeeze(image_detect).numpy()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Maxing Pooling"},{"metadata":{},"cell_type":"markdown","source":"- How `base` in convnet performs feature extraction, \n- How `conv2d` layer and `relu` activation layer process,\n- How `MaxPool2D` layer `condense` with `maximum pooling`."},{"metadata":{},"cell_type":"markdown","source":"### Condense with Maximum Pooling\n```\nlayers.Conv2D(filters=64, kernel_size=3), # activation is None\nlayers.MaxPool2D(pool_size=2)\n```\nA `MaxPool2D` layer is much like a `Conv2D` layer, except that it uses a simple maximum function instead of a kernel, with the `pool_size` parameter analogous to kernel_size. A `MaxPool2D` layer doesn't have any trainable weights like a `convolutional` layer. \n\nMaxPool2D is the `Condense` step.\n<figure>\n    <img src='https://i.imgur.com/IYO9lqp.png' width=400>\n</figure>\n\nAfter applying the `ReLU` function (*Detect*) the feature map ends up with a lot of `dead space`, large areas containing only 0's (the black areas in the image). \n- carrying `0` activations through the entire network would increase the size of the model without adding much useful information.\n- `condense` the feature map to retain only the most useful part \n\n`Maximum pooling` takes a patch of activations in the original feature map and replaces them with the maximum activation in that patch.\n<figure>\n    <img src='https://imgur.com/hK5U2cd.png' width=500>\n</figure>\n\n- When applied after `ReLU` activation, it has the effect of `intensifying` features. \n- The pooling step increases the proportion of `active pixels` to `zero pixels`."},{"metadata":{},"cell_type":"markdown","source":"### Example - Apply Maximum Pooling\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport warnings\n\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\nplt.rc('image', cmap='magma')\nwarnings.filterwarnings(\"ignore\") # to clean up output cells\n\n# Read image\nimage_path = '../input/computer-vision-resources/car_feature.jpg'\nimage = tf.io.read_file(image_path)\nimage = tf.io.decode_jpeg(image)\n\n# Define kernel\nkernel = tf.constant([\n    [-1, -1, -1],\n    [-1,  8, -1],\n    [-1, -1, -1],\n], dtype=tf.float32)\n\n# Reformat for batch compatibility.\nimage = tf.image.convert_image_dtype(image, dtype=tf.float32)\nimage = tf.expand_dims(image, axis=0)\nkernel = tf.reshape(kernel, [*kernel.shape, 1, 1])\n\n# Filter step\nimage_filter = tf.nn.conv2d(\n    input=image,\n    filters=kernel,\n    # we'll talk about these two in the next lesson!\n    strides=1,\n    padding='SAME'\n)\n\n# Detect step\nimage_detect = tf.nn.relu(image_filter)\n\n# Show what we have so far\nplt.figure(figsize=(12, 6))\nplt.subplot(131)\nplt.imshow(tf.squeeze(image), cmap='gray')\nplt.axis('off')\nplt.title('Input')\nplt.subplot(132)\nplt.imshow(tf.squeeze(image_filter))\nplt.axis('off')\nplt.title('Filter')\nplt.subplot(133)\nplt.imshow(tf.squeeze(image_detect))\nplt.axis('off')\nplt.title('Detect')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For pooling step, use `tf.nn.pool` function."},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\nimage_condense = tf.nn.pool(\n    input=image_detect, # image in the Detect step above\n    window_shape=(2, 2),\n    pooling_type='MAX',\n    strides=(2, 2),\n    padding='SAME',\n)\n\nplt.figure(figsize=(6, 6))\nplt.imshow(tf.squeeze(image_condense))\nplt.axis('off')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Translation Invariance\nZero-pixels is called `unimportant`. Zero-pixels carry informational, `positional information`. The blank space still positions the feature within the image. When `MaxPool2D` removes some of these pixels, it removes some of the positional information in the feature map giving the `translational invariance` property in convnet. \n- convnet with maximum pooling will tend not to distinguish features by their location in the image\n    - `Translation` is for changing the position of something without rotating or changing its shape or size\n![](https://i.imgur.com/97j8WA1.png)\n- The two dots in the original image became indistinguishable after repeated pooling.\n- Pooling destroyed some of their positional information.\n- Network can no longer distinguish between them in the feature maps, it can't distinguish them in the original image either: it has become invariant to that difference in position.\n- Pooling only creates translation invariance in a network over small distances, as with the two dots in the image.\n- Features that begin far apart will remain distinct after pooling; only some of the positional information was lost, but not all of it.\n![](https://i.imgur.com/kUMWdcP.png)\n\n"},{"metadata":{},"cell_type":"markdown","source":"### Exercise: Maximum Pooling\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"image_path = '../input/computer-vision-resources/car_illus.jpg'\nimage = tf.io.read_file(image_path)\nimage = tf.io.decode_jpeg(image, channels=1)\nimage = tf.image.resize(image, size=[400, 400])\n\n# Embossing kernel\nkernel = tf.constant([\n    [-2, -1, 0],\n    [-1, 1, 1],\n    [0, 1, 2],\n])\n\n# Reformat for batch compatibility.\nimage = tf.image.convert_image_dtype(image, dtype=tf.float32)\nimage = tf.expand_dims(image, axis=0)\nkernel = tf.reshape(kernel, [*kernel.shape, 1, 1])\nkernel = tf.cast(kernel, dtype=tf.float32)\n\nimage_filter = tf.nn.conv2d(\n    input=image,\n    filters=kernel,\n    strides=1,\n    padding='VALID',\n)\n\nimage_detect = tf.nn.relu(image_filter)\n\n# Show what we have so far\nplt.figure(figsize=(12, 6))\nplt.subplot(131)\nplt.imshow(tf.squeeze(image), cmap='gray')\nplt.axis('off')\nplt.title('Input')\nplt.subplot(132)\nplt.imshow(tf.squeeze(image_filter))\nplt.axis('off')\nplt.title('Filter')\nplt.subplot(133)\nplt.imshow(tf.squeeze(image_detect))\nplt.axis('off')\nplt.title('Detect')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_condense = tf.nn.pool(\n    input=image_detect,\n    window_shape=(2,2),\n    pooling_type='MAX',\n    strides=(2,2),\n    padding='SAME'\n)\n\nplt.figure(figsize=(8, 6))\nplt.subplot(121)\nplt.imshow(tf.squeeze(image_detect))\nplt.axis('off')\nplt.title(\"Detect (ReLU)\")\nplt.subplot(122)\nplt.imshow(tf.squeeze(image_condense))\nplt.axis('off')\nplt.title(\"Condense (MaxPool)\")\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nrandomly apply a small shift to a circle and then condense the image several times with maximum pooling\n'''\n\n\nREPEATS = 4\nSIZE = [64, 64]\n\n# Create a randomly shifted circle\nimage = visiontools.circle(SIZE, r_shrink=4, val=1)\nimage = tf.expand_dims(image, axis=-1)\nimage = visiontools.random_transform(image, jitter=3, fill_method='replicate')\nimage = tf.squeeze(image)\n\nplt.figure(figsize=(16, 4))\nplt.subplot(1, REPEATS+1, 1)\nplt.imshow(image, vmin=0, vmax=1)\nplt.title(\"Original\\nShape: {}x{}\".format(image.shape[0], image.shape[1]))\nplt.axis('off')\n\n# Now condense with maximum pooling several times\nfor i in range(REPEATS):\n    ax = plt.subplot(1, REPEATS+1, i+2)\n    image = tf.reshape(image, [1, *image.shape, 1])\n    image = tf.nn.pool(image, window_shape=(2,2), strides=(2, 2), padding='SAME', pooling_type='MAX')\n    image = tf.squeeze(image)\n    plt.imshow(image, vmin=0, vmax=1)\n    plt.title(\"MaxPool {}\\nShape: {}x{}\".format(i+1, image.shape[0], image.shape[1]))\n    plt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Global Average Pooling\nAverage pooling has largely been superceeded by maximum pooling within the convolutional base. *Global average pooling*, `GlobalAvgPool2D` layer is used as an alternative to some or all of the hidden `Dense` layers in the head of the network.\n```\npretrained_base,\nlayers.GlobalAvgPool2D(),\nlayers.Dense(1, activation='sigmoid'),\n```\n- Not having the `Flatten` layer that usually comes after the base to transform the 2D feature data to 1D data needed by the classifier. \n- `GlobalAvgPool2D` layer is serving like this function. But, instead of *unstacking* the feature, `Flatten`, it replaces the entire feature map with its average value."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_maps = [visiontools.random_map([5, 5], scale=0.1, decay_power=4) for _ in range(8)]\n\ngs = gridspec.GridSpec(1, 8, wspace=0.01, hspace=0.01)\nplt.figure(figsize=(18, 2))\nfor i, feature_map in enumerate(feature_maps):\n    plt.subplot(gs[i])\n    plt.imshow(feature_map, vmin=0, vmax=1)\n    plt.axis('off')\nplt.suptitle('Feature Maps', size=18, weight='bold', y=1.1)\nplt.show()\n\n# reformat for TensorFlow\nfeature_maps_tf = [tf.reshape(feature_map, [1, *feature_map.shape, 1])\n                   for feature_map in feature_maps]\n\nglobal_avg_pool = tf.keras.layers.GlobalAvgPool2D()\npooled_maps = [global_avg_pool(feature_map) for feature_map in feature_maps_tf]\nimg = np.array(pooled_maps)[:,:,0].T\n\nplt.imshow(img, vmin=0, vmax=1)\nplt.axis('off')\nplt.title('Pooled Feature Maps')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\n\n# Load VGG16\npretrained_base = tf.keras.models.load_model(\n    '../input/cv-course-models/cv-course-models/vgg16-pretrained-base',\n)\n\nmodel = keras.Sequential([\n    pretrained_base,\n    # Attach a global average pooling layer after the base\n    layers.GlobalAvgPool2D(),\n])\n\n# Load dataset\nds = image_dataset_from_directory(\n    '../input/car-or-truck/train',\n    labels='inferred',\n    label_mode='binary',\n    image_size=[128, 128],\n    interpolation='nearest',\n    batch_size=1,\n    shuffle=True,\n)\nds_iter = iter(ds)\n\n\ncar = next(ds_iter)\ncar_tf = tf.image.resize(car[0], size=[128, 128])\ncar_features = model(car_tf)\ncar_features = tf.reshape(car_features, shape=(16, 32))\nlabel = int(tf.squeeze(car[1]).numpy())\n\nplt.figure(figsize=(8, 4))\nplt.subplot(121)\nplt.imshow(tf.squeeze(car[0]))\nplt.axis('off')\nplt.title([\"Car\", \"Truck\"][label])\nplt.subplot(122)\nplt.imshow(car_features)\nplt.title('Pooled Feature Maps')\nplt.axis('off')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# The Sliding Window"},{"metadata":{},"cell_type":"markdown","source":"Learned three operations that carry out feature extraction from an image:\n1. filter with a convolution layer\n2. detect with ReLU activation\n3. condense with a maximum pooling layer\n\nConvolution and pooling operations share a common feature: both performed over a `sliding window`. With convolution, `window` is given by dimension of kernel, `kernel_size`. Pooling window is given by `pool_size`.\n<figure>\n    <img src='https://i.imgur.com/LueNK6b.gif', width=350>\n</figure>\n\nThere are two additional parameters affecting both convolution and pooling layers: \n- `strides` of the window: how far the window should move at each step,\n- `padding` to use at image edges:  describes how to handle the pixels at the edges of the input.\n```\nlayers.Conv2D(filters=64,\n              kernel_size=3,\n              strides=1,\n              padding='same',\n              activation='relu'),\nlayers.MaxPool2D(pool_size=2,\n                 strides=1,\n                 padding='same')\n```"},{"metadata":{},"cell_type":"markdown","source":"### Stride\nThe distance the window moves at each step is called `stride`. Need to specify stride in `both dimensions` of the image: one for moving `left to right` and one for moving `top to bottom`. Below animation is for `strides=(2,2)`.\n<figure>\n    <img src='https://i.imgur.com/Tlptsvt.gif', width=350>\n</figure>\nWhenever the stride in either direction is greater than `1`, the sliding window will skip over some of the pixels in the input at each step. \n- Since high-quality features will be needed for `classification`, convolutional layers will most often have `strides=(1, 1)`. \n    - Increasing `strides` can miss out . \n- Maximum pooling layers will have `stride` value greater than `1` [`(2,2)` or `(3,3)`], but not larger than window itself.\n\nWhen the value of `strides` is the same number in both directions, can set with an interger: `strides=(2, 2)` == `strides=2`."},{"metadata":{},"cell_type":"markdown","source":"### Padding\nWhat the convolution does with these boundary values is determined by its `padding` parameter. In Tensorflow, parameter can be `padding='same'` or `padding='valid'`. \n- `valid`: the convolution window will stay entirely inside the input. \n    - output shrinks (loses pixels)\n    - shrinks more for larger kernels\n    - will limit the number of layers the network can contain, especially when inputs are small in size.\n- `same`: to pad the input with `0's` around its borders,\n    - to make the size of the output the same as the size of the input\n    - effect of diluting the influence of pixels at the borders.\n    \nThe following animation is for `same`.\n<figure>\n    <img src='https://i.imgur.com/RvGM2xb.gif', width=350>\n</figure>\nMost modern convnets will use some combination of the two."},{"metadata":{},"cell_type":"markdown","source":"### Example - Exploring Sliding Windows\nTo better understand the effect of the sliding window parameters, a feature extraction on a low-resolution image will perform."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom itertools import product\nfrom skimage import draw, transform\n\ndef circle(size, val=None, r_shrink=0):\n    circle = np.zeros([size[0]+1, size[1]+1])\n    rr, cc = draw.circle_perimeter(\n        size[0]//2, size[1]//2,\n        radius=size[0]//2 - r_shrink,\n        shape=[size[0]+1, size[1]+1],\n    )\n    if val is None:\n        circle[rr, cc] = np.random.uniform(size=circle.shape)[rr, cc]\n    else:\n        circle[rr, cc] = val\n    circle = transform.resize(circle, size, order=0)\n    return circle\n\ndef show_kernel(kernel, label=True, digits=None, text_size=28):\n    # Format kernel\n    kernel = np.array(kernel)\n    if digits is not None:\n        kernel = kernel.round(digits)\n\n    # Plot kernel\n    cmap = plt.get_cmap('Blues_r')\n    plt.imshow(kernel, cmap=cmap)\n    rows, cols = kernel.shape\n    thresh = (kernel.max()+kernel.min())/2\n    # Optionally, add value labels\n    if label:\n        for i, j in product(range(rows), range(cols)):\n            val = kernel[i, j]\n            color = cmap(0) if val > thresh else cmap(255)\n            plt.text(j, i, val, \n                     color=color, size=text_size,\n                     horizontalalignment='center', verticalalignment='center')\n    plt.xticks([])\n    plt.yticks([])\n\ndef show_extraction(image,\n                    kernel,\n                    conv_stride=1,\n                    conv_padding='valid',\n                    activation='relu',\n                    pool_size=2,\n                    pool_stride=2,\n                    pool_padding='same',\n                    figsize=(10, 10),\n                    subplot_shape=(2, 2),\n                    ops=['Input', 'Filter', 'Detect', 'Condense'],\n                    gamma=1.0):\n    # Create Layers\n    model = tf.keras.Sequential([\n                    tf.keras.layers.Conv2D(\n                        filters=1,\n                        kernel_size=kernel.shape,\n                        strides=conv_stride,\n                        padding=conv_padding,\n                        use_bias=False,\n                        input_shape=image.shape,\n                    ),\n                    tf.keras.layers.Activation(activation),\n                    tf.keras.layers.MaxPool2D(\n                        pool_size=pool_size,\n                        strides=pool_stride,\n                        padding=pool_padding,\n                    ),\n                   ])\n\n    layer_filter, layer_detect, layer_condense = model.layers\n    kernel = tf.reshape(kernel, [*kernel.shape, 1, 1])\n    layer_filter.set_weights([kernel])\n\n    # Format for TF\n    image = tf.expand_dims(image, axis=0)\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32) \n    \n    # Extract Feature\n    image_filter = layer_filter(image)\n    image_detect = layer_detect(image_filter)\n    image_condense = layer_condense(image_detect)\n    \n    images = {}\n    if 'Input' in ops:\n        images.update({'Input': (image, 1.0)})\n    if 'Filter' in ops:\n        images.update({'Filter': (image_filter, 1.0)})\n    if 'Detect' in ops:\n        images.update({'Detect': (image_detect, gamma)})\n    if 'Condense' in ops:\n        images.update({'Condense': (image_condense, gamma)})\n    \n    # Plot\n    plt.figure(figsize=figsize)\n    for i, title in enumerate(ops):\n        image, gamma = images[title]\n        plt.subplot(*subplot_shape, i+1)\n        plt.imshow(tf.image.adjust_gamma(tf.squeeze(image), gamma))\n        plt.axis('off')\n        plt.title(title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport learntools.computer_vision.visiontools as visiontools\n\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\nplt.rc('image', cmap='magma')\n\nimage = circle([64, 64], val=1.0, r_shrink=3)\nimage = tf.reshape(image, [*image.shape, 1])\n# Bottom sobel\nkernel = tf.constant(\n    [[-1, -2, -1],\n     [0, 0, 0],\n     [1, 2, 1]],\n)\n\nvisiontools.show_kernel(kernel)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_extraction(\n    image, kernel,\n\n    # Window parameters\n    conv_stride=1,\n    pool_size=2,\n    pool_stride=2,\n\n    subplot_shape=(1, 4),\n    figsize=(14, 6),\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# changed the strides of the convolution to 3\nshow_extraction(\n    image, kernel,\n\n    # Window parameters\n    conv_stride=3,\n    pool_size=2,\n    pool_stride=2,\n\n    subplot_shape=(1, 4),\n    figsize=(14, 6),    \n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To reduce the quality of the feature extracted. A convolution with strides of 3 is too coarse to produce a good feature map from it. \n\nSometimes, a model will use a convolution with a larger stride in it's initial layer. \n- `ResNet50` model uses `7 x 7` kernels with strides of `2` in its first layer. \n- to accelerate production of large-scale features without the sacrifice of too much information from the input."},{"metadata":{},"cell_type":"markdown","source":"### Exercise: The Sliding Window\n- explore the operations a couple of popular convnet architectures use for feature extraction, \n- learn about how convnets can capture large-scale visual features through stacking layers, and \n- see how convolution can be used on one-dimensional data, in `time series`."},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport learntools.computer_vision.visiontools as visiontools\nfrom learntools.computer_vision.visiontools import edge, blur, bottom_sobel, emboss, sharpen, circle\nimport matplotlib.pyplot as plt\n\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\nplt.rc('image', cmap='magma')\n\nimage_dir = '../input/computer-vision-resources/'\ncircle_64 = tf.expand_dims(circle([64, 64], val=1.0, r_shrink=4), axis=-1)\nkaggle_k = visiontools.read_image(image_dir + str('k.jpg'), channels=1)\ncar = visiontools.read_image(image_dir + str('car_illus.jpg'), channels=1)\ncar = tf.image.resize(car, size=[200, 200])\nimages = [(circle_64, \"circle_64\"), (kaggle_k, \"kaggle_k\"), (car, \"car\")]\n\nplt.figure(figsize=(14, 4))\nfor i, (img, title) in enumerate(images):\n    plt.subplot(1, len(images), i+1)\n    plt.imshow(tf.squeeze(img))\n    plt.axis('off')\n    plt.title(title)\nplt.show();\n\nkernels = [(edge, \"edge\"), (blur, \"blur\"), (bottom_sobel, \"bottom_sobel\"),\n           (emboss, \"emboss\"), (sharpen, \"sharpen\")]\nplt.figure(figsize=(14, 4))\nfor i, (krn, title) in enumerate(kernels):\n    plt.subplot(1, len(kernels), i+1)\n    visiontools.show_kernel(krn, digits=2, text_size=20)\n    plt.title(title)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kernel = [edge, blur, bottom_sobel, emboss, sharpen]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image = circle_64\n\nfor i in range(len(kernel)):\n    \n    visiontools.show_extraction(\n        image, kernel[i],\n\n        # YOUR CODE HERE: set parameters\n        conv_stride=1,\n        conv_padding='valid',\n        pool_size=2,\n        pool_stride=2,\n        pool_padding='same',\n\n        subplot_shape=(1, 4),\n        figsize=(14, 6),\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image = kaggle_k\n\nfor i in range(len(kernel)):\n    \n    visiontools.show_extraction(\n        image, kernel[i],\n\n        # YOUR CODE HERE: set parameters\n        conv_stride=1,\n        conv_padding='valid',\n        pool_size=2,\n        pool_stride=2,\n        pool_padding='same',\n\n        subplot_shape=(1, 4),\n        figsize=(14, 6),\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image = car\n\nfor i in range(len(kernel)):\n    \n    visiontools.show_extraction(\n        image, kernel[i],\n\n        # YOUR CODE HERE: set parameters\n        conv_stride=1,\n        conv_padding='valid',\n        pool_size=2,\n        pool_stride=2,\n        pool_padding='same',\n\n        subplot_shape=(1, 4),\n        figsize=(14, 6),\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Receptive Field\nAll of the input pixels a neuron is connected to is that neuron's receptive field. The receptive field just tells which parts of the input image a neuron receives information from.\n\nIf first layer is convolution with `3x3` kernels, each neuron in that layer gets input from a  3×3  patch of pixels. \n<figure>\n<img src=\"https://i.imgur.com/HmwQm2S.png\" width=250>\n</figure>\n\nEach neuron in the `3×3` patch in the middle layer is connected to a `3×3` input patch, but they overlap in a `5×5` patch. So that neuron at top has a `5×5` receptive field."},{"metadata":{},"cell_type":"markdown","source":"#### One-Dimensional Convolution\nConvolutional networks turn out to be useful not only (two-dimensional) images, but also on things like time-series (one-dimensional) and video (three-dimensional).\n\nTime series from [Google Trends](https://trends.google.com/trends/) will use. It measures the popularity of the search term `machine learning` for weeks from January 25, 2015 to January 15, 2020.\n- Images are two-dimensional and so kernels were 2D arrays.\n- A time-series is one-dimensional, so kernels were 1D arrays.\n```\ndetrend = tf.constant([-1, 1], dtype=tf.float32)\naverage = tf.constant([0.2, 0.2, 0.2, 0.2, 0.2], dtype=tf.float32)\nspencer = tf.constant([-3, -6, -5, 3, 21, 46, 67, 74, 67, 46, 32, 3, -5, -6, -3], dtype=tf.float32) / 320\n```\n\nConvolution on a sequence(`Time-series`) works just like convolution on an image. The difference is just that a sliding window on a sequence only has one direction, `left to righ`. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\n# Load the time series as a Pandas dataframe\nmachinelearning = pd.read_csv(\n    '../input/computer-vision-resources/machinelearning.csv',\n    parse_dates=['Week'],\n    index_col='Week',\n)\n\nmachinelearning.plot();\n\n# some 1D kernels sometimes used on time-series data\ndetrend = tf.constant([-1, 1], dtype=tf.float32)\naverage = tf.constant([0.2, 0.2, 0.2, 0.2, 0.2], dtype=tf.float32)\nspencer = tf.constant([-3, -6, -5, 3, 21, 46, 67, 74, 67, 46, 32, 3, -5, -6, -3], dtype=tf.float32) / 320\n\n# -------- choose kernel type -----------\n# kernel = detrend\n# kernel = average\nkernel = spencer\n\n# Reformat for TensorFlow\nts_data = machinelearning.to_numpy()\nts_data = tf.expand_dims(ts_data, axis=0)\nts_data = tf.cast(ts_data, dtype=tf.float32)\nkern = tf.reshape(kernel, shape=(*kernel.shape, 1, 1))\n\nts_filter = tf.nn.conv1d(\n    input=ts_data,\n    filters=kern,\n    stride=1,\n    padding='VALID',\n)\n\n# Format as Pandas Series\nmachinelearning_filtered = pd.Series(tf.squeeze(ts_filter).numpy())\n\nmachinelearning_filtered.plot();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`detrend` kernel filters for *changes* in the series, while `average` and `spencer` are both *smoothers* that filter for low-frequency components in the series."},{"metadata":{},"cell_type":"markdown","source":"----"},{"metadata":{},"cell_type":"markdown","source":"# Custom Convnets"},{"metadata":{},"cell_type":"markdown","source":"### Simple to Refined\nLearned how convolutional networks perform `feature extraction` through three operations: `filter, detect, and condense`. A single round of feature extraction can only extract relatively simple features from an image. If convnets repeat this extraction over and over again, features more complex and refined as deeper as into the network.\n![](https://i.imgur.com/VqmC1rm.png)"},{"metadata":{},"cell_type":"markdown","source":"### Convolutional Blocks\nDone by passing a long chains of `convolutional blocks` which perform extraction.\n<figure>\n    <img src=\"https://i.imgur.com/pr8VwCZ.png\" width=700>\n</figure>\nThese convolutional blocks are stacks of `Conv2D` and `MaxPool2D` layers, whose role in feature extraction. \n<figure>\n    <img src=\"https://i.imgur.com/8D6IhEw.png\" width=600>\n</figure>\nEach block represents a round of extraction, and by composing these blocks the convnet can combine and recombine the features produced, growing them and shaping them to better fit the problem at hand. The deep structure of modern convnets is what allows this sophisticated feature engineering and has been largely responsible for their superior performance."},{"metadata":{},"cell_type":"markdown","source":"### Example - Design a Convnet\nWill create a Keras `Sequence` model and then train for Cars dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n-----------------------------------------------------------\n---------------------Step1 - Load Data---------------------\n-----------------------------------------------------------\n'''\nimport os, warnings\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\n\n# Reproducability\ndef set_seed(seed=31415):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nset_seed()\n\n# Set Matplotlib defaults\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\nplt.rc('image', cmap='magma')\nwarnings.filterwarnings(\"ignore\") # to clean up output cells\n\n\n# Load training and validation sets\nds_train_ = image_dataset_from_directory(\n    '../input/car-or-truck/train',\n    labels='inferred',\n    label_mode='binary',\n    image_size=[128, 128],\n    interpolation='nearest',\n    batch_size=64,\n    shuffle=True,\n)\nds_valid_ = image_dataset_from_directory(\n    '../input/car-or-truck/valid',\n    labels='inferred',\n    label_mode='binary',\n    image_size=[128, 128],\n    interpolation='nearest',\n    batch_size=64,\n    shuffle=False,\n)\n\n# Data Pipeline\ndef convert_to_float(image, label):\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image, label\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nds_train = (\n    ds_train_\n    .map(convert_to_float)\n    .cache()\n    .prefetch(buffer_size=AUTOTUNE)\n)\nds_valid = (\n    ds_valid_\n    .map(convert_to_float)\n    .cache()\n    .prefetch(buffer_size=AUTOTUNE)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](https://i.imgur.com/U1VdoDJ.png)\nDefine the model with three blocks of `Conv2D` and `MaxPool2D`(base) followed by head of `Dense` layers. Number of filters doubled block-by-block: `64, 128, 256`, common pattern. Since `MaxPool2D` layer is reducing the size of the feature maps, should afford to increase the quantity to create."},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n-----------------------------------------------------------\n------------------Step2 - Define Model---------------------\n-----------------------------------------------------------\n'''\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n\n    # First Convolutional Block\n    layers.Conv2D(filters=32, kernel_size=5, activation=\"relu\", padding='same', \n                  input_shape=[128, 128, 3]), # [height, width, color channels(RGB)]\n    layers.MaxPool2D(),\n\n    # Second Convolutional Block\n    layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\", padding='same'),\n    layers.MaxPool2D(),\n\n    # Third Convolutional Block\n    layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\", padding='same'),\n    layers.MaxPool2D(),\n\n    # Classifier Head\n    layers.Flatten(),\n    layers.Dense(units=6, activation=\"relu\"),\n    layers.Dense(units=1, activation=\"sigmoid\"),\n])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n-----------------------------------------------------------\n----------------------Step3 - Train------------------------\n-----------------------------------------------------------\n'''\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(epsilon=0.01),\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy']\n)\n\nhistory = model.fit(\n    ds_train,\n    validation_data=ds_valid,\n    epochs=40,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\nhistory_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss', 'val_loss']].plot()\nhistory_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exercise: Custom Convnets\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from learntools.core import binder\nbinder.bind(globals())\nfrom learntools.computer_vision.ex5 import *\n\n# Imports\nimport os, warnings\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\n\n# Reproducability\ndef set_seed(seed=31415):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nset_seed()\n\n# Set Matplotlib defaults\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\nplt.rc('image', cmap='magma')\nwarnings.filterwarnings(\"ignore\") # to clean up output cells\n\n\n# Load training and validation sets\nds_train_ = image_dataset_from_directory(\n    '../input/car-or-truck/train',\n    labels='inferred',\n    label_mode='binary',\n    image_size=[128, 128],\n    interpolation='nearest',\n    batch_size=64,\n    shuffle=True,\n)\nds_valid_ = image_dataset_from_directory(\n    '../input/car-or-truck/valid',\n    labels='inferred',\n    label_mode='binary',\n    image_size=[128, 128],\n    interpolation='nearest',\n    batch_size=64,\n    shuffle=False,\n)\n\n# Data Pipeline\ndef convert_to_float(image, label):\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image, label\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nds_train = (\n    ds_train_\n    .map(convert_to_float)\n    .cache()\n    .prefetch(buffer_size=AUTOTUNE)\n)\nds_valid = (\n    ds_valid_\n    .map(convert_to_float)\n    .cache()\n    .prefetch(buffer_size=AUTOTUNE)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Design a Convnet\n![](https://i.imgur.com/Vko6nCK.png)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    # Block One\n    layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same',\n                  input_shape=[128, 128, 3]),\n    layers.MaxPool2D(),\n\n    # Block Two\n    layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'),\n    layers.MaxPool2D(),\n\n    # Block Three\n    # YOUR CODE HERE\n    layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'),\n    layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'),\n    layers.MaxPool2D(),\n\n    # Head\n    layers.Flatten(),\n    layers.Dense(6, activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(1, activation='sigmoid'),\n])\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(epsilon=0.01),\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy'],\n)\n\nhistory = model.fit(\n    ds_train,\n    validation_data=ds_valid,\n    epochs=50,\n)\n\nimport pandas as pd\nhistory_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss', 'val_loss']].plot()\nhistory_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Data Augmentation"},{"metadata":{},"cell_type":"markdown","source":"Will learn a trick to boost image classifirs called `data augmentation`."},{"metadata":{},"cell_type":"markdown","source":"### The Usefulness of Fake Data\nBest way to improve ML performance is to train it on more data. The more examples the model has to learn from, the better it will be able to recognize which differences in images matter and which do not. More data helps the model to generalize better.\n\nOne easy way of getting more data is to use the data that already have. If the images in our dataset can be transformed  in that preserve the class, the classifier can be taught to ignore those kinds of transformations. \n- whether a car is facing left or right in a photo doesn't change the fact that it is a Car and not a Truck.\n- if we `augment` our training data with flipped images, our classifier will learn that `left or right` is a difference it should ignore.\n\nThe idea behind `data augmentation`: add in some extra fake data that looks reasonably like the real data and classifier will improve."},{"metadata":{},"cell_type":"markdown","source":"### Using Data Augmentation\nMany kinds of transformation are used when augmenting a dataset. These include `rotating images`, `adjusting color or contrast`, `warping images`, or many other things, applied in combination. The following picture shows the different ways of a image that can be transformed.\n<figure>\n    <img src='https://i.imgur.com/UaOm0ms.png' width=450>\n</figure>\n\nData augmentation is usually done `online`: as the images are being fed into the network for training. Training is usually done on mini-batches of data. This is what a `batch of 16 images` might look like when data augmentation is used.\n<figure>\n    <img src='https://i.imgur.com/MFviYoE.png' width=450>\n</figure>\n\nEach time an image is used during training, a new random transformation is applied. Model is always seeing something a little different than what it's seen before. This extra variance in the training data is what helps the model on new data.\n\n- Not every transformation will be useful on a given problem.\n- Whatever transformations used should not mix up the classes.\n    - if training [digit recognizer](https://www.kaggle.com/c/digit-recognizer), rotating images would mix up '9's and '6's.\n- best approach for finding good augmentations is the same as with most ML problems"},{"metadata":{},"cell_type":"markdown","source":"### Example - Training with Data Augmentation\nKeras lets augment the data in two ways: to include it in the data pipeline with a function like `ImageDataGenerator`, to include it in the model definition by using Keras's `preprocessing layers`. \n\nThe primary advantage is that the imge transormations will be computed on the `GPU` instead of the `CPU`: speeding up training."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imports\nimport os, warnings\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\n\n# Reproducability\ndef set_seed(seed=31415):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nset_seed()\n\n# Set Matplotlib defaults\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\nplt.rc('image', cmap='magma')\nwarnings.filterwarnings(\"ignore\") # to clean up output cells\n\n\n# Load training and validation sets\nds_train_ = image_dataset_from_directory(\n    '../input/car-or-truck/train',\n    labels='inferred',\n    label_mode='binary',\n    image_size=[128, 128],\n    interpolation='nearest',\n    batch_size=64,\n    shuffle=True,\n)\nds_valid_ = image_dataset_from_directory(\n    '../input/car-or-truck/valid',\n    labels='inferred',\n    label_mode='binary',\n    image_size=[128, 128],\n    interpolation='nearest',\n    batch_size=64,\n    shuffle=False,\n)\n\n# Data Pipeline\ndef convert_to_float(image, label):\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image, label\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nds_train = (\n    ds_train_\n    .map(convert_to_float)\n    .cache()\n    .prefetch(buffer_size=AUTOTUNE)\n)\nds_valid = (\n    ds_valid_\n    .map(convert_to_float)\n    .cache()\n    .prefetch(buffer_size=AUTOTUNE)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define Model\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n# these are a new feature in TF 2.2\nfrom tensorflow.keras.layers.experimental import preprocessing\n\n\npretrained_base = tf.keras.models.load_model(\n    '../input/cv-course-models/cv-course-models/vgg16-pretrained-base',\n)\npretrained_base.trainable = False\n\n# add a couple of simple transformations in the mdoel\nmodel = keras.Sequential([\n    # Preprocessing\n    preprocessing.RandomFlip('horizontal'), # flip left-to-right\n    preprocessing.RandomContrast(0.5), # contrast change by up to 50%\n    # Base\n    pretrained_base,\n    # Head\n    layers.Flatten(),\n    layers.Dense(6, activation='relu'),\n    layers.Dense(1, activation='sigmoid'),\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train and Evaluate\n\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy'],\n)\n\nhistory = model.fit(\n    ds_train,\n    validation_data=ds_valid,\n    epochs=15,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\nhistory_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss', 'val_loss']].plot()\nhistory_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exercise: Data Augmentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setup feedback system\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.computer_vision.ex6 import *\n\nimport tensorflow.keras as keras\nimport tensorflow.keras.layers as layers\nimport tensorflow.keras.layers.experimental.preprocessing as preprocessing\n\n# Imports\nimport os, warnings\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\n\n# Reproducability\ndef set_seed(seed=31415):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nset_seed()\n\n# Set Matplotlib defaults\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\nplt.rc('image', cmap='magma')\nwarnings.filterwarnings(\"ignore\") # to clean up output cells\n\n\n# Load training and validation sets\nds_train_ = image_dataset_from_directory(\n    '../input/car-or-truck/train',\n    labels='inferred',\n    label_mode='binary',\n    image_size=[128, 128],\n    interpolation='nearest',\n    batch_size=64,\n    shuffle=True,\n)\nds_valid_ = image_dataset_from_directory(\n    '../input/car-or-truck/valid',\n    labels='inferred',\n    label_mode='binary',\n    image_size=[128, 128],\n    interpolation='nearest',\n    batch_size=64,\n    shuffle=False,\n)\n\n# Data Pipeline\ndef convert_to_float(image, label):\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image, label\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nds_train = (\n    ds_train_\n    .map(convert_to_float)\n    .cache()\n    .prefetch(buffer_size=AUTOTUNE)\n)\nds_valid = (\n    ds_valid_\n    .map(convert_to_float)\n    .cache()\n    .prefetch(buffer_size=AUTOTUNE)\n\n\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# uncomment to see different kind of Preprocessing\n\n# all of the \"factor\" parameters indicate a percent-change\naugment = keras.Sequential([\n    # preprocessing.RandomContrast(factor=0.5),\n    preprocessing.RandomFlip(mode='horizontal'), # meaning, left-to-right\n    # preprocessing.RandomFlip(mode='vertical'), # meaning, top-to-bottom\n    # preprocessing.RandomWidth(factor=0.15), # horizontal stretch\n    # preprocessing.RandomRotation(factor=0.20),\n    # preprocessing.RandomTranslation(height_factor=0.1, width_factor=0.1),\n])\n\n\nex = next(iter(ds_train.unbatch().map(lambda x, y: x).batch(1)))\n\nplt.figure(figsize=(10,10))\nfor i in range(16):\n    image = augment(ex, training=True)\n    plt.subplot(4, 4, i+1)\n    plt.imshow(tf.squeeze(image))\n    plt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[EuroSAT](https://www.kaggle.com/ryanholbrook/eurosat) dataset consists of satellite images of the Earth classified by geographic feature.\n![](https://i.imgur.com/LxARYZe.png)\n\nIt seems to this author that flips and rotations would be worth trying first since there's no concept of orientation for pictures taken straight overhead. None of the transformations seem likely to confuse classes, however."},{"metadata":{},"cell_type":"markdown","source":"The [TensorFlow Flowers](https://www.kaggle.com/ryanholbrook/tensorflow-flowers) dataset consists of photographs of flowers of several species.\n![](https://i.imgur.com/Mt7PR2x.png)\n\nIt seems to this author that horizontal flips and moderate rotations would be worth trying first. Some augmentation libraries include transformations of hue (like red to blue). Since the color of a flower seems distinctive of its class, a change of hue might be less successful. On the other hand, there is suprising variety in cultivated flowers like roses, so, depending on the dataset, this might be an improvement after all!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    layers.InputLayer(input_shape=[128, 128, 3]),\n    \n    # Data Augmentation\n    preprocessing.RandomContrast(factor=0.10),\n    preprocessing.RandomFlip(mode='horizontal'),\n    preprocessing.RandomRotation(factor=0.10),\n\n    # Block One\n    layers.BatchNormalization(renorm=True),\n    layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'),\n    layers.MaxPool2D(),\n\n    # Block Two\n    layers.BatchNormalization(renorm=True),\n    layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'),\n    layers.MaxPool2D(),\n\n    # Block Three\n    layers.BatchNormalization(renorm=True),\n    layers.Conv2D(filters=256, kernel_size=3, activation='relu', padding='same'),\n    layers.Conv2D(filters=256, kernel_size=3, activation='relu', padding='same'),\n    layers.MaxPool2D(),\n\n    # Head\n    layers.BatchNormalization(renorm=True),\n    layers.Flatten(),\n    layers.Dense(8, activation='relu'),\n    layers.Dense(1, activation='sigmoid'),\n])\n\noptimizer = tf.keras.optimizers.Adam(epsilon=0.01)\nmodel.compile(\n    optimizer=optimizer,\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy'],\n)\n\nhistory = model.fit(\n    ds_train,\n    validation_data=ds_valid,\n    epochs=50,\n)\n\n# Plot learning curves\nimport pandas as pd\nhistory_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss', 'val_loss']].plot()\nhistory_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The learning curves in this model stayed close together for much longer than in previous models. This suggests that the augmentation helped prevent overfitting, allowing the model to continue improving.\n\nAnd notice that this model achieved the highest accuracy of all the models in the course! This won't always be the case, but it shows that a well-designed custom convnet can sometimes perform as well or better than a much larger pretrained model. Depending on your application, having a smaller model (which requires fewer resources) could be a big advantage."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"[Petals to the Metal](https://www.kaggle.com/c/tpu-getting-started) competition [submission](https://www.kaggle.com/minyannaing/create-your-first-submission/edit) tutorial.\n\n[Getting Started: TPUs + Cassava Leaf Disease](https://www.kaggle.com/minyannaing/getting-started-tpus-cassava-leaf-disease/edit)."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}