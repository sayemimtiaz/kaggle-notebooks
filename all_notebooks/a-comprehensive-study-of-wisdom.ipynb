{"cells":[{"metadata":{},"cell_type":"markdown","source":"Twitter is one of the biggest social media platforms of our time. In twitter, one can find from the thoughts of their favorite celebrities to the policies of their government. In short, twitter is one of the major ways to connect with people of every field. There are many twitter accounts which tweet many wise things reelated to self improvement there. In these kernels, I am going to analyse more than 30K+ tweets from 40 different such acccounts. Let's move forward and see what we can find. \n\nI have scrapped tweets of mainly these twitter handles:\n\n@naval, @Via_Benjamin, @RortyWitt, @EdLatimore, @StoopToRise, @AymPlanet, @Wealth_Theory, @alanhliang, @TradingNirvana, @modestproposal1, CrazyPolymath, @SentientBonobo, @webdevMason, @stoic_dilettant, @AJA_Cortes, @martyrmade, @PresentWitness_, @millstoic, @z3nblack, @TheChuChu_, @orangebook_, @yawyr_vk, @Noahpinion, @ThomasSowell, @LifeMathMoney, @DejaRu22, @lawsofaurelius, @48_quotes, @shl, @cryptoseneca, @paulg, @TheCreativeFury, @Kpaxs, @TheAncientSage, @DeeperThrill, @mmay3r, @DrRalphNap, @TheStoicEmperor, @uncannyinsights\n\nThere are other authors also in the dataset because one of the above mentioned account retweeted their tweets. \n\nYou can find the script I wrote for scrapping all these tweets [here](https://github.com/Hsankesara/Tweets-Scrapper). You can also find the same dataset at Github. [Click here](https://github.com/Hsankesara/The-Tweets-of-Wisdom) to go there. \nLet's get started...."},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n* Describe the data\n* Understanding each column\n* Removing NAN"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/the-tweets-of-wisdom/tweets.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df.tweet_content.isna()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df.author_name.isna()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** These tweets are NaN because the author has posted only an image. I'll remove them as they has no significance at all in my analysis. **\n\n\n** Only one tweet has an author with NaN. Gonna drop it also **"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Understanding the Author, Likes and Retweets\n* How much likes and retweets are related. \n* Value count of tweets of each author. \n* Do more tweets increase the mean likes an author got?\n* Variation of likes for each author.\n* Definition of a \"viral\" tweet. "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(df.author_name.unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.handle.value_counts().head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's take top 10 authors\ntop_ten_authors = df.handle.value_counts().head(10).index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_ten_authors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.distplot(df.author_name.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.distplot(df.likes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nprint(df[df['author_name'] == 'Thomas Sowell'].likes.mean(), df[df['author_name'] == 'Thomas Sowell'].likes.std())\nsns.distplot(df[df['author_name'] == 'Thomas Sowell'].likes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nprint(df[df['author_name'] == 'Thomas Sowell'].retweets.mean(), df[df['author_name'] == 'Thomas Sowell'].retweets.std())\nsns.distplot(df[df['author_name'] == 'Thomas Sowell'].retweets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.heatmap(df[['likes', 'retweets']].corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** High Correlation between number of Retweets and likes which is expected **"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\np  = pd.concat([df.groupby('handle').likes.mean(), df.groupby('handle').retweets.mean(), df.groupby('handle').likes.count()], axis=1 )\np.columns = ['Mean Likes per Author', 'Mean Retweets per Author' ,'Number of tweets per authors']\nsns.heatmap(p.corr() ,annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Negligible Correlation between likes and number of tweets which came as a surprise. It means your content matter more than the number of tweets. Quality over Quantity**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\n# Sort the dataframe by target\ntarget_0 = df[df['handle'] == 'ThomasSowell']\ntarget_1 = df[df['handle'] == 'TheAncientSage']\ntarget_2 = df[df['handle'] == 'orangebook_']\n\nsns.distplot(target_0[['likes']], hist=False)\nsns.distplot(target_1[['likes']], hist=False)\nsns.distplot(target_2[['likes']], hist=False)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\n# Sort the dataframe by target\ntarget_0 = df[df['handle'] == 'ThomasSowell ']\ntarget_1 = df[df['handle'] == 'TheAncientSage']\ntarget_2 = df[df['handle'] == 'orangebook_']\n\nsns.distplot(target_0[['retweets']], hist=False)\nsns.distplot(target_1[['retweets']], hist=False)\nsns.distplot(target_2[['retweets']], hist=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,10))\nsns.boxplot(x=\"handle\", y=\"likes\", data=df[df['handle'].isin(top_ten_authors)], ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,10))\nsns.violinplot(x=\"handle\", y=\"likes\", data=df[df['handle'].isin(top_ten_authors)], ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** ThomasSowell is leading by a large margin.**\n\n** It's amazing how ThomasSowell has such a flatter distribution.**\n\n** Let's throw ThomasSowell out of the game.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"l  = top_ten_authors.to_list()\nl.remove('ThomasSowell')\nfig, ax = plt.subplots(figsize=(15,10))\nsns.boxplot(x=\"handle\", y=\"retweets\", data=df[df['handle'].isin(l)], ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l  = top_ten_authors.to_list()\nl.remove('ThomasSowell')\nfig, ax = plt.subplots(figsize=(15,10))\nsns.violinplot(x=\"handle\", y=\"likes\", data=df[df['handle'].isin(l)], ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Too much outliers. That is strange. Might be the \"viral\" factor of the internet**\n\n** The distribution looks like Poisson distribution for each author. **\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ratio of Likes and Retweets\nfig, ax = plt.subplots(figsize=(15,10))\nsns.distplot(df.likes / (1 + df.retweets))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean of the ratio distribution', (df.likes / (1 + df.retweets)).mean(), '\\nStandard Deviation of the ratio distribution',(df.likes / (1 + df.retweets)).std())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ThomasSowell"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df[df.handle=='ThomasSowell'].likes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean of the likes in ThomasSowell tweets', df[df.handle=='ThomasSowell'].likes.mean(), '\\nStandard Deviation of the likes in ThomasSowell tweets', df[df.handle=='ThomasSowell'].likes.std())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Distribution of top 2 to 10 authors"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(3, 3, figsize=(15,15))\nfor i in range(3):\n    for j in range(3):\n        sns.distplot(df[df.handle==top_ten_authors[i*3 + j  + 1]].likes, ax=ax[i][j])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(3):\n    for j in range(3):\n        print('Mean of the likes in', top_ten_authors[i*3 + j  + 1], 'tweets', df[df.handle==top_ten_authors[i*3 + j  + 1]].likes.mean(), '\\nStandard Deviation of the likes in', top_ten_authors[i*3 + j  + 1], 'tweets', df[df.handle==top_ten_authors[i*3 + j + 1]].likes.std(), '\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Aren't they look like poisson distribution with mean and std as lambda ** \n\n** Let's plot distribution as the difference between mean and standard deviation **"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\ndis = df.groupby('handle').filter(lambda group: group.size > 10)\nsns.distplot(dis.groupby('handle').likes.mean() - dis.groupby('handle').likes.std())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Oh! Mean revolves around zero with small deviation. We can estimate the curve as a poisson distribution. Let's see if we can use it in the future. **"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Time to focus on time\n* Relation between time and likes.\n* Likes distribution wrt timestamp for each author\n* Does time of tweet has any relation with likes?\n* Does author's presence on twitter effects the likes author would get?\n* How mean likes change over the time. \n* How mean tweet count change over the time. \n* Graph between the duration between two tweets (everyday presence) and it's relation with the likes. \n* Does viral tweets matter?"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.dates as mdates\nfrom matplotlib.dates import DateFormatter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.created_at.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.created_at = pd.to_datetime(df.created_at, format='%Y-%m-%d %H:%M:%S')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the plot space upon which to plot the data\nfig, ax = plt.subplots(figsize=(30, 15))\n\n# Add the x-axis and the y-axis to the plot\nax.plot(df.created_at,\n        df.likes, '-o',\n        color='purple')\n\n# Clean up the x axis dates (reviewed in lesson 4)\nax.xaxis.set_major_locator(mdates.WeekdayLocator(interval=1))\nax.xaxis.set_major_formatter(DateFormatter(\"%m\"))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Older tweets has less likes than newer ones. One reason might be the growing rate of Twitter which means more users which means more likes **"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['creation_month'] = df.created_at.dt.month\ndf['creation_day'] = df.created_at.dt.day\ndf['creation_year'] = df.created_at.dt.year\ndf['creation_hour'] = df.created_at.dt.hour","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the plot space upon which to plot the data\nfig, ax = plt.subplots(figsize=(15, 10))\n\n\n# Add the x-axis and the y-axis to the plot\nax.plot(np.sort(df.creation_month.unique()),\n        df.groupby('creation_month').likes.sum(), '-o',\n        color='purple')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(15, 10))\nsns.countplot(df.creation_month)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** A great rise in the number of tweets as well as sum of likes in the month of Aug and Sept **"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the plot space upon which to plot the data\nfig, ax = plt.subplots(figsize=(15, 10))\n\n\n# Add the x-axis and the y-axis to the plot\nax.plot(np.sort(df.creation_month.unique()),\n        df.groupby('creation_month').likes.mean(), '-o',\n        color='purple')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** The likes per tweet is highest for the tweets that are created in january. As we observed previously, tweets has very low rate of getting time as it gets older. I might be reaching here but since most of these tweets are motivational and january is the time for New year Resolution, there might be chances that people follow motivational tweets in January more. As there new year reolution, there engagement with the tweet also declines in Feb and March. **"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the plot space upon which to plot the data\nfig, ax = plt.subplots(figsize=(15, 10))\n\n\n# Add the x-axis and the y-axis to the plot\nax.plot(np.sort(df.creation_year.unique()),\n        df.groupby('creation_year').likes.sum(), '-o',\n        color='purple')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(15, 10))\nsns.countplot(df.creation_year)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** This is the living proof of exponential growth of Twitter after 2014 **\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the plot space upon which to plot the data\nfig, ax = plt.subplots(figsize=(15, 10))\n\n\n# Add the x-axis and the y-axis to the plot\nax.plot(np.sort(df.creation_year.unique()),\n        df.groupby('creation_year').likes.mean(), '-o',\n        color='purple')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Here is something interesting, Likes per tweet is highest in 2014 and then decreases. It might be because number of users has increased drastically. **"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the plot space upon which to plot the data\nfig, ax = plt.subplots(figsize=(15, 10))\n\n\n# Add the x-axis and the y-axis to the plot\nax.plot(np.sort(df.creation_hour.unique()),\n        df.groupby('creation_hour').likes.sum(), '-o',\n        color='purple')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(15, 10))\nsns.countplot(df.creation_hour)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Twittter acocunts are from different countries. Some of them are from USA, some are from India and the rest are from the Europe and Africa**\n\n** But the count of tweets decrease and increase from the 5th hour to 13th hour (8 hours). Now the way it is decreasing it looks like it is sleeping time for the majority of authors. **\n\n** Even in the total likes graph, the graph is lowest between (5-10). It implies even the followers are sleeping at the time of tweet** \n\n** Also at the 16-18 hours, both the tweets count as well as aggregated hit peak. If we assume 5-13 is the sleeping time, we can say that 16-18 is noon time which means the lunch time.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the plot space upon which to plot the data\nfig, ax = plt.subplots(figsize=(15, 10))\n\n\n# Add the x-axis and the y-axis to the plot\nax.plot(np.sort(df.creation_hour.unique()),\n        df.groupby('creation_hour').likes.mean(), '-o',\n        color='purple')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(25, 15))\nsns.countplot(df.creation_month, hue=df.creation_year)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(25, 15))\nsns.countplot(df.creation_day, hue=df.creation_year)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(25, 15))\nsns.countplot(df.creation_hour, hue=df.creation_year)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(25, 15))\nsns.countplot(df.creation_day, hue=df.creation_month)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Author Wise Tweets-time Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df[['likes', 'creation_year', 'creation_month', 'creation_hour']].corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(3, 3, figsize=(15,15))\nfor i in range(3):\n    for j in range(3):\n        ax[i][j].plot(np.sort(df[df['handle'] == top_ten_authors[3 * i + j]].creation_year.unique()), df[df['handle'] == top_ten_authors[3 * i + j]].groupby('creation_year').likes.mean(), '-o', color='purple')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Even many of the top authors have started a less than a year ago. I wonder if your duration on twitter can affect your likes. Let's find out**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(datetime.today() - df.groupby('handle').created_at.min()).dt.days","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(pd.concat([df.groupby('handle').likes.mean(), (datetime.today() - df.groupby('handle').created_at.min()).dt.days], axis=1).corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(pd.concat([df.groupby('handle').likes.sum(), (datetime.today() - df.groupby('handle').created_at.min()).dt.days], axis=1).corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Well there is no significant correlation between the profile duration on twitter and mean tweet. Though there is a small positive correlation in total tweet vs duration. But it is expected. My question was does author's presence has any significance i.e does it matter that author created account in 2008 and the one is 2019. As we have saw before that older tweets has lesser likes and hence older tweets has less impact on users. Also, there is almost no correlation between average likes and age of author on twitter. Both of these evidence strongly suggests that it doesn't matter what the author age on twitter.**"},{"metadata":{},"cell_type":"markdown","source":"** Does \"viral\" tweets impact average likes? Logically, viral tweets(Tweets whose number of likes are greater than interquartile range) invite more user. But does it statistically matter? **"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nfrom scipy.ndimage.interpolation import shift\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Outliers(\"Viral tweets\") are in yellow and else are in blue\nfig, ax = plt.subplots(3, 3, figsize=(30,15))\nfor i in range(3):\n    for j in range(3):\n        ax[i][j].scatter(df[df['handle'] == top_ten_authors[3 * i + j]].created_at, df[df['handle'] == top_ten_authors[3 * i + j]].likes, facecolors='blue',alpha=.85, s=30)\n        ax[i][j].scatter(df[df['handle'] == top_ten_authors[3 * i + j]][(np.abs(stats.zscore(df[df['handle'] == top_ten_authors[3 * i + j]].likes)) > 3)].created_at, df[df['handle'] == top_ten_authors[3 * i + j]][(np.abs(stats.zscore(df[df['handle'] == top_ten_authors[3 * i + j]].likes)) > 3)].likes, color=\"yellow\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_avg_likes_between_viral_twts(author, ax):\n    t = df[df['handle'] == author].sort_values(by = 'created_at').reset_index()\n    idx = t[(np.abs(stats.zscore(t.likes)) > 3)].index\n    idx_created = t[(np.abs(stats.zscore(t.likes)) > 3)].created_at\n    x = t.created_at\n    y = [0 for i in range(t.shape[0])]\n    y2 = []\n    prev = 0\n    for i in idx:\n        m = t[prev:i].likes.mean()\n        for j in range(prev, i):\n            y[j] = m\n        y[i] = None\n        y2.append(t.iloc[i].likes)\n        prev = i + 1\n    ax.plot(x, y, linewidth=2)\n    ax.scatter(idx_created, y2, color=\"red\")\n    ax.set_title(author)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## The red ones are the viral tweets and the blue one is average of likes between two consecutive viral tweets\nfig, ax = plt.subplots(3, 3, figsize=(30,20))\nfor i in range(3):\n    for j in range(3):\n        plot_avg_likes_between_viral_twts(top_ten_authors[3 * i + j], ax[i][j])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** It can be observed that a \"viral\" tweet increases the likes by a very little. But bunch of viral tweets in close time do increase likes on future tweets. This is expected behaviour but I was expecting the affect to be more. The affect is short term as well as  little as compare to the viralness of the tweet **"},{"metadata":{"trusted":true},"cell_type":"code","source":"def change_in_likes_after_viral_twts(author, ax):\n    t = df[df['handle'] == author].sort_values(by = 'created_at').reset_index()\n    idx = t[(np.abs(stats.zscore(t.likes)) > 3)].index\n    delta_change = []\n    prev = 0\n    prev_del = 0\n    for i in idx:\n        m = t[prev:i].likes.mean() - prev_del\n        if(np.isnan(m) == False):\n            delta_change.append(m)\n            prev_del =  t[prev:i].likes.mean()\n        prev = i + 1\n    sns.distplot(np.array(delta_change) - shift(delta_change, 1, cval=0), ax=ax).set_title(author)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## The red ones are the viral tweets and the blue one is average of likes between two consecutive viral tweets\nfig, ax = plt.subplots(3, 3, figsize=(30,20))\nfor i in range(3):\n    for j in range(3):\n        change_in_likes_after_viral_twts(top_ten_authors[3 * i + j], ax[i][j])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** In few of the cases, viral tweets increases the avg amount of likes one gets. But sometimes, it might decrease them which came as a surprise to me. Moreover in most of the cases, the average of the distribution tends to zero. I am start having doubts on viral tweets. I thought they are always good but statistically they have a negligible relevance over a long term. We've seen previously that author's tweeter age also does not matter neither the author's number of tweet. Does that mean, only your content determines the likes you got? I don't know but will investigate it soon. **"},{"metadata":{},"cell_type":"markdown","source":"** Now let's talk about author's tweeter presence which I will estimate using duration between two tweets **"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Duration between two consecutive tweets distribution\nfig, ax = plt.subplots(3, 3, figsize=(30,20))\nfor i in range(3):\n    for j in range(3):\n        sns.distplot((df[df['handle'] == top_ten_authors[3 * i + j]].sort_values(by = 'created_at').created_at.diff() / np.timedelta64(1, 'h')).dropna(), ax=ax[i][j])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Correlation between duration between two tweets and difference of likes both of them have. \nfig, ax = plt.subplots(3, 3, figsize=(30,20))\nfor i in range(3):\n    for j in range(3):\n        temp = df[df['handle'] == top_ten_authors[3 * i + j]].sort_values(by = 'created_at')[['created_at', 'likes']].diff().dropna()\n        sns.heatmap(pd.concat([temp.created_at.dt.seconds, temp.likes], axis=1).corr(), ax=ax[i][j], annot=True).set_title(top_ten_authors[3 * i + j])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Technically, a negative correlation means smaller the duration, greater the difference between likes. Vice versa for the positive correlation. But most of the correlation is too close to zero. It implies that duration between two tweets cannot influence the likes of the latter one. **"},{"metadata":{},"cell_type":"markdown","source":"# Analysing the content inside the tweet\n\n* wordcount distribution and correlation with the likes.\n* Wordcloud for each author. \n* Understanding what makes some tweets viral and some not so much. \n* Bigrams and trigram analysis of viral tweets. \n* What are the sentiments of tweets. What kind  of sentiments make a tweet viral. \n* variation of author's word count and word cloud with respect to time. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df['word_count'] = df.tweet_content.str.len()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Twitter has 280 word limit on a tweet. On 7 November 2017, twitter increased it's character limits from 124-280. It will be interesting to see that change in the dataset ** "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15, 10))\nsns.distplot(df.word_count, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the plot space upon which to plot the data\nfig, ax = plt.subplots(figsize=(15, 10))\n\n\n# Add the x-axis and the y-axis to the plot\nax.plot(np.sort(df.creation_year.unique()),\n        df.groupby('creation_year').word_count.mean(), '-o',\n        color='purple')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## The red ones are the viral tweets and the blue one is average of likes between two consecutive viral tweets\nfig, ax = plt.subplots(3, 3, figsize=(30,20))\nfor i in range(3):\n    for j in range(3):\n        ax[i][j].plot(np.sort(df[df.handle == top_ten_authors[3*i + j]].creation_year.unique()), df[df.handle == top_ten_authors[3*i + j]].groupby('creation_year').word_count.mean(), '-o', color='purple')\n        ax[i][j].set_title(top_ten_authors[3*i + j])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** It can be easily seen that after 2017, the average tweet length jump drastically **\n\n** Most of the authors has a very little change in word count. Some even has decrease the average word count with respect to time. ** "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15, 10))\nsns.heatmap(df[['word_count', 'likes']].corr(), ax=ax, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Negligible correlation between word count and number of likes. Might be because 280 words are not too much and hence most of the people can read it easily. **"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_wordcloud(text, ax, mask=None, max_words=400, max_font_size=120, figure_size=(24.0,16.0), title = None, title_size=40, image_color=False):\n    \"\"\"\n    Function Credit: https://www.kaggle.com/aashita/word-clouds-of-various-shapes\n    \"\"\"\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='white',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    mask = mask)\n    wordcloud.generate(text)\n    \n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        ax.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        ax.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        ax.imshow(wordcloud);\n        ax.set_title(title, fontdict={'size': title_size, 'color': 'green', \n                                  'verticalalignment': 'bottom'})\n    ax.axis('off');\n        \ndef plot_the_author(name, ax=plt):\n    author_tweets = df[df.handle == name].tweet_content\n    plot_wordcloud('\\n'.join(author_tweets), max_words=600, max_font_size=120,  title = name + ' tweets', title_size=20, figure_size=(10,12), ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15, 10))\nplot_wordcloud('\\n'.join(df.tweet_content), ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15, 10))\nviral_tweets_all = df[(np.abs(stats.zscore(df.likes)) > 3)].tweet_content\nplot_wordcloud('\\n'.join(viral_tweets_all), ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** You might be expecting this because I am. Words like \"will\", \"time\", \"life\", \"people\", \"mind\" and many more words which focuses more on individual responsibility are in abundance. **"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Correlation between duration between two tweets and difference of likes both of them have. \nfig, ax = plt.subplots(3, 3, figsize=(25,20))\nfor i in range(3):\n    for j in range(3):\n        plot_the_author(top_ten_authors[3 * i + j], ax=ax[i][j])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Aren't all the word clouds seem alike. Most frequent words of all the top 10 authors are almost same. They talk about the same content using different language. This also tells us motivational people (who focuses on individual responsibility) are more of less uses the same formulla to preach. **\n\n** Let's focus on viral tweet's wordcloud now. We are plotting wordcloud of all the viral tweets as well as word cloud of the least liked tweets of the author ** "},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_viral_twts_cloud(name, ax):\n    author_tweets = df[df.handle == name].tweet_content\n    plot_wordcloud('\\n'.join(author_tweets), max_words=600, max_font_size=120,  title = name + ' tweets', title_size=20, figure_size=(10,12), ax=ax[0])\n    t = df[df['handle'] == name]\n    viral_tweets_content = t[(np.abs(stats.zscore(t.likes)) > 3)].tweet_content\n    t = df[(df['handle'] == name) & ~((df.creation_year ==2019) & (df.creation_month==9))] # Trying to remove recent tweets\n    least_liked_tweets = t.sort_values(by = 'likes').head(10).tweet_content\n    plot_wordcloud('\\n'.join(viral_tweets_content), max_words=600, max_font_size=120,  title = name + ' viral tweets', title_size=20, figure_size=(10,12), ax=ax[1])\n    plot_wordcloud('\\n'.join(least_liked_tweets), max_words=600, max_font_size=120,  title = name + ' least liked tweets', title_size=20, figure_size=(10,12), ax=ax[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(5, 3, figsize=(30,20))\nfor i in range(5):\n    plot_viral_twts_cloud(top_ten_authors[i], ax[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** So many things to unfold. But you can see that words like \"political\", \"unions\", \"argument\" etc political terms helped ThomasSowell getting viral tweets **\n\n** The difference is apparent in a few cases and obsure in some. It is better if you observe them by yourself. Also let me know if you found something interesting about the plot in comments. **\n\n** I'm targeting bigrams and trigrams for analysis the tweets. **\n\nInspiraton: [Data cleaning, Data Processing & Data Analysis](https://www.kaggle.com/hsankesara/data-cleaning-data-processing-data-analysis)"},{"metadata":{},"cell_type":"markdown","source":"** Removing Stop words **"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom tqdm import tqdm\ntqdm.pandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['tweet_tokens'] = df['tweet_content'].progress_apply(nltk.word_tokenize)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"en_stopwords = set(nltk.corpus.stopwords.words('english'))\ndf['tweet_tokens'] = df['tweet_tokens'].progress_apply(lambda x: [item for item in x if item not in en_stopwords])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#function to filter for ADJ/NN bigrams\ndef rightTypes(ngram):\n    if '-pron-' in ngram or 't' in ngram:\n        return False\n    for word in ngram:\n        if word in en_stopwords or word.isspace():\n            return False\n    acceptable_types = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n    second_type = ('NN', 'NNS', 'NNP', 'NNPS')\n    tags = nltk.pos_tag(ngram)\n    if tags[0][1] in acceptable_types and tags[1][1] in second_type:\n        return True\n    else:\n        return False\n#filter bigrams\n#filtered_bi = bigramFreqTable[bigramFreqTable.bigram.map(lambda x: rightTypes(x))]\n#function to filter for trigrams\ndef rightTypesTri(ngram):\n    if '-pron-' in ngram or 't' in ngram:\n        return False\n    for word in ngram:\n        if word in en_stopwords or word.isspace():\n            return False\n    first_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n    third_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n    tags = nltk.pos_tag(ngram)\n    if tags[0][1] in first_type and tags[2][1] in third_type:\n        return True\n    else:\n        return False\n#filter trigrams\n#filtered_tri = trigramFreqTable[trigramFreqTable.trigram.map(lambda x: rightTypesTri(x))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_bigram_trigrams(tokens, title):\n    bigrams = nltk.collocations.BigramAssocMeasures()\n    trigrams = nltk.collocations.TrigramAssocMeasures()\n    bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(tokens)\n    trigramFinder = nltk.collocations.TrigramCollocationFinder.from_words(tokens)\n    #bigrams\n    bigram_freq = bigramFinder.ngram_fd.items()\n    bigramFreqTable = pd.DataFrame(list(bigram_freq), columns=['bigram','freq']).sort_values(by='freq', ascending=False)\n    #trigrams\n    trigram_freq = trigramFinder.ngram_fd.items()\n    trigramFreqTable = pd.DataFrame(list(trigram_freq), columns=['trigram','freq']).sort_values(by='freq', ascending=False)\n    filtered_bi = bigramFreqTable[bigramFreqTable.bigram.map(lambda x: rightTypes(x))]\n    filtered_tri = trigramFreqTable[trigramFreqTable.trigram.map(lambda x: rightTypesTri(x))]\n    print(title)\n    print(filtered_bi.head(20))\n    print(filtered_tri.head(20))\n    return bigramFinder, trigramFinder, bigrams, trigrams","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time generic_bigrams_finders, generic_trigrams_finders, generic_bigrams, generic_trigrams = get_bigram_trigrams(np.concatenate(df.tweet_tokens.to_list()), title=\"Extracting generic Bi/Tri-grams\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Let me clear some things here. &lt;Q> and &lt;/Q> are the tags I have used when a author retweets a tweet with a comment. The author's comment is followed by &lt;Q> tag and then the content of the retweet comes which is followed by &lt;/Q>. **\n\n** Most frequent bigrams are just @ <author's name>. That is a let down ** \n\n** Also trigrams has most of the trigrams regarding workout which might have come from a same twitter accout. That is why individual analysis is necessary. \n\n** Let's analysis bi/tri grams of top 5 authors **"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_five_author_bigram_finder = [None for i in range(5)]\ntop_five_author_trigram_finder = [None for i in range(5)]\ntop_five_author_bigrams = [None for i in range(5)]\ntop_five_author_trigrams = [None for i in range(5)]\nfor i in range(5):\n     top_five_author_bigram_finder[i], top_five_author_trigram_finder[i], top_five_author_bigrams[i], top_five_author_trigrams[i] = get_bigram_trigrams(np.concatenate(df[df.handle == top_ten_authors[i]].tweet_tokens.to_list()), title=top_ten_authors[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Let's checkout bi/tri-grams of all the viral tweets ** "},{"metadata":{"trusted":true},"cell_type":"code","source":"top_five_author_bigram_finder_v = [None for i in range(5)]\ntop_five_author_trigram_finder_v = [None for i in range(5)]\ntop_five_author_bigrams_v = [None for i in range(5)]\ntop_five_author_trigrams_v = [None for i in range(5)]\nfor i in range(5):\n    t = df[df['handle'] == top_ten_authors[i]]\n    top_five_author_bigram_finder_v[i], top_five_author_trigram_finder_v[i], top_five_author_bigrams_v[i], top_five_author_trigrams_v[i] = get_bigram_trigrams(np.concatenate(t[(np.abs(stats.zscore(t.likes)) > 3)].tweet_tokens.to_list()), title=top_ten_authors[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Viral tweets usually do not have same bigrams/trigrams which makes sense as they are far lesser in number that chance of such collision reduces drastically. Plus the viral tweet is something which appeals to not only your niche group of followers but also a general crowd. **\n\n** Let's Analyse the bigrams and trigrams now. Let's now find the bigrams and trigrams with highest pointwise mututal information. The main intuition is that it measures how much more likely the words co-occur than if they were independent. However, it is very sensitive to rare combination of words.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_pointwise_mi_scores(bigramFinder, trigramFinder, bigrams, trigrams, title):\n    #filter for only those with more than 20 occurences\n    bigramFinder.apply_freq_filter(20)\n    trigramFinder.apply_freq_filter(20)\n    bigramPMITable = pd.DataFrame(list(bigramFinder.score_ngrams(bigrams.pmi)), columns=['bigram','PMI']).sort_values(by='PMI', ascending=False)\n    trigramPMITable = pd.DataFrame(list(trigramFinder.score_ngrams(trigrams.pmi)), columns=['trigram','PMI']).sort_values(by='PMI', ascending=False)\n    print('Exploring Point wise Mututal Information in bigrams and trigrams of ' + title)\n    print('-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n    print('-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n    print(bigramPMITable.head(10))\n    print('-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n    print(trigramPMITable.head(10))\n    print('-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n    return bigramPMITable, trigramPMITable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_, __ = get_pointwise_mi_scores(generic_bigrams_finders, generic_trigrams_finders, generic_bigrams, generic_trigrams, \"all the tweets\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(5):\n    _, __ = get_pointwise_mi_scores(top_five_author_bigram_finder[i], top_five_author_trigram_finder[i], top_five_author_bigrams[i], top_five_author_trigrams[i], top_ten_authors[i] + \"\\'s tweets\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** You may have seen few tuples which qualitaatively convey no information but have highest correlation/mutual information. To find out significance of each tuple, we'll use t score and score the bi and tri grams **"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_t_scores( bigramFinder, trigramFinder, bigrams, trigrams, title):\n    bigramTtable = pd.DataFrame(list(bigramFinder.score_ngrams(bigrams.student_t)), columns=['bigram','t']).sort_values(by='t', ascending=False)\n    trigramTtable = pd.DataFrame(list(trigramFinder.score_ngrams(trigrams.student_t)), columns=['trigram','t']).sort_values(by='t', ascending=False)\n    #filters\n    filteredT_bi = bigramTtable[bigramTtable.bigram.map(lambda x: rightTypes(x))]\n    filteredT_tri = trigramTtable[trigramTtable.trigram.map(lambda x: rightTypesTri(x))]\n    print('Exploring t scores between the words in bigrams and trigrams of ' + title)\n    print('-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n    print('-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n    print(filteredT_bi.head(10))\n    print('-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n    print(filteredT_tri.head(10))\n    print('-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n    return filteredT_bi, filteredT_tri","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_, __ = get_t_scores(generic_bigrams_finders, generic_trigrams_finders, generic_bigrams, generic_trigrams, \"all the tweets\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(5):\n    _, __ = get_t_scores(top_five_author_bigram_finder[i], top_five_author_trigram_finder[i], top_five_author_bigrams[i], top_five_author_trigrams[i], top_ten_authors[i] + \"\\'s tweets\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** I believe bigrams and the trigrams are a great window to get gist of an author's work. If you want to know more about more authors, just run your copy  and check them out. ** \n\n** Let's move forward to sentiment analysis. The last but not the least fascinating topic of the notebook.  I'm going to use [TextBlob](https://github.com/sloria/TextBlob) library which contains state-of-the NLP methods and is very easy to use.**\n\n* Distribution of subjectivity and polarity. \n* The correlation between subjectivity, polarity and likes. \n* Author wise subjectivity and polarity analysis. \n* Variation of polarity and subjectivity over the time. Both holistic as well as individual analysis. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from textblob import TextBlob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def  get_sentiments(text):\n    s = TextBlob(text).sentiment\n    return  s.polarity ,s.subjectivity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['polarity'], df['subjectivity'] = zip(*df['tweet_content'].progress_apply(get_sentiments))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Here polarity is the sentiment of a tweet. It is float which lies between [-1,1] where -1 is for completely negative tweet and 1 for totally positive tweet. Subjective sentences generally refer to personal opinion, emotion or judgment whereas objective refers to factual information. Subjectivity is also a float which lies in the range of [0,1]. **"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15, 10))\nsns.distplot(df.polarity, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15, 10))\nsns.distplot(df.subjectivity, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df[['subjectivity', 'polarity', 'likes']].corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Likes has no correlation with the polarity/subjectivity and likes. Polarity and subjectivity has some positive correlation. **"},{"metadata":{"trusted":true},"cell_type":"code","source":"for author in top_ten_authors:\n    print(\"Mean Polarity of \"+ author + \" is \" + str(df[df['handle'] == author].polarity.mean()) + ' and standard deviation is ' + str(df[df['handle'] == author].polarity.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for author in top_ten_authors:\n    print(\"Mean subjectivity of \"+ author +  \" is \" + str(df[df['handle'] == author].subjectivity.mean())  + ' and standard deviation is ' + str(df[df['handle'] == author].subjectivity.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** The standard deviation of polarity and subjectivity of each author from top 10 authors are close.**"},{"metadata":{},"cell_type":"markdown","source":"# The End\n\nI tried to understand every aspect of the tweets and along the way discovered many surprising results. I hope that you like this notebook. Feel free to copy and edit it for your own exploration.  \n\nThanks for reading this kernel. If you have any question, advice or doubt, please write it in the comments. If you like the kernel, Please leave an upvote. "},{"metadata":{},"cell_type":"markdown","source":"## Contact Me\n\n[<img src=\"http://i.imgur.com/0o48UoR.png\" width=\"35\">](https://github.com/Hsankesara/)    [<img src=\"https://i.imgur.com/0IdggSZ.png\" width=\"35\">](https://www.linkedin.com/in/heet-sankesara-72383a152/)     [<img src=\"http://i.imgur.com/tXSoThF.png\" width=\"35\">](https://twitter.com/TheSankesara)   [<img src=\"https://loading.io/s/icon/vzeour.svg\" width=\"35\">](https://www.kaggle.com/hsankesara) [<img src=\"https://image.flaticon.com/icons/svg/2111/2111505.svg\" width=\"35\">](https://medium.com/@heetsankesara3)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}