{"cells":[{"metadata":{},"cell_type":"markdown","source":"![Logistic Regression](https://i.ibb.co/Ptg3Czv/kaggle-ml-part1.png)\n\n- **ML Part 1 - Logistic Regression**\n- **ML Part 2** - K-Nearest Neighbors (KNN)\n- **ML Part 3** - Support Vector Machine (SVM)\n- **ML Part 4** - Artificial Neural Network (NN) \n- **ML Part 5** - Classification and Regression Tree (CART)\n- **ML Part 6** - Random Forests\n- **ML Part 7** - Gradient Boosting Machines (GBM)\n- **ML Part 8** - XGBoost\n- **ML Part 9** - LightGBM\n- **ML Part 10** - CatBoost\n\n\nLike linear regression, Logistic regression is the right algorithm to start with classification algorithms. Although it has the name 'regression', this is a classification model, not a regression model. It uses a logistical function to frame the binary output model. The output of the logistic regression will be a probability (0≤x≤1) and can be used to predict binary 0 or 1 as output (x <0.5, output = 0, otherwise output = 1).\n\n## Basic Theory\nLogistic Regression behaves quite similar to linear regression. It also calculates the linear output, then follows a storage function through the regression output. The sigmoid function is the logistic function that is used frequently. Below you can clearly see that the z value is the same as the linear regression output in Equation (1).\n\n![](https://i.ibb.co/X7NGG5W/Ek-A-klama-2020-08-27-113356.jpg)\n\nThe value of h (θ) here corresponds to P (y = 1 | x), that is, the probability that the output is binary 1 when input x is given. P (y = 0 | x) will be equal to 1-h ().\n\nWhen the value of z is 0, g (z) will be 0.5. When Z is positive, h () will be greater than 0.5 and the output will be binary 1. Similarly, when z is negative, the value of y will be 0. When we use a linear equation to find the classifier, the output model will also be a linear dimension, i.e. divide the input size into two spaces so that all points in a field correspond to the same label.\n\nThe figure below shows the distribution of a sigmoid function.\n\n![](https://i.ibb.co/mtbyBzZ/Ek-A-klama-2020-08-27-113557.jpg)\n\n## Loss Function\n\nWe cannot use Mean Squared Error as a loss function (like linear regression) because we are using a nonlinear sigmoid function at the end. The MSE function can fetch local minimums and will affect the Gradient Descent algorithm.\n\nSo here we are using Cross Entropy as the missing function. y = 1 and two equations corresponding to y = 0 will be used. The basic logic here is that when my guess is too wrong (for example: y '= 1 & y = 0), the cost will be -log (0), which is infinite.\n\n![](https://i.ibb.co/MDRJZKw/Ek-A-klama-2020-08-27-113827.jpg)\n\nIn the given equation, m represents the training data size, y 'represents the estimated output and y represents the actual output.\n\n\n## Advantages\n- Easy, fast and simple method of classification.\n- θ parameters describe the direction and density of the importance of independent variables on the dependent variable.\n- It can also be used for multi-class classifications.\n- Its lost function is always convex.\n\n\n## Disadvantages\n- It cannot be applied to nonlinear classification problems.\n- Appropriate feature selection is required.\n- Good signal-to-noise ratio is expected.\n- Collinearity and outliers deteriorate the accuracy of the LR model.\n\n\n## Hyperparameters\nLogistic regression hyperparameters are similar to those of linear regression. The learning speed (α) and the smoothing parameter (λ) must be set correctly to achieve high accuracy.\n\n\n## Comparison with Other Models\n\n\n\n![Logistic Regression vs SVM](https://i.ibb.co/MDTY8GG/lg-vs-svm.png)\n- While SVM can handle nonlinear solutions, logistic regression can only process linear solutions.\n- Linear SVM manages outliers better as it achieves maximum margin solution.\n- The hinge loss in SVM outperforms its daily loss in LR.\n\n![Logistic Regression vs Decision Trees](https://i.ibb.co/3czHSLR/lg-vs-Decision-Trees.png)\n- Decision tree handles collinearity better than LR.\n- Decision trees cannot deduce the importance of features, but LR can.\n- Decision trees are better for categorical values than LR.\n\n![Logistic Regression vs Neural Network(NN)](https://i.ibb.co/PTCJCx8/lr-vs-Neural-Network.png)\n- NN can support nonlinear solutions that LR cannot.\n- LR has a convex loss function so it won't hang at a local minimum whereas NN can hang.\n- While LR performs better than NN when training data is less and features are large, NN needs large training data.\n\n![Logistic Regression vs Decision Trees](https://i.ibb.co/Z8cgqwd/lg-vs-naive-bayes.png)\n- Naive Bayes is a productive model, while LR is a distinctive model.\n- Naive Bayes works well with small data sets, whereas LR regulation can provide similar performance.\n- Since Naive Bayes expects all features to be independent, LR outperforms Naive Bayes on linearity.\n\n![Logistic Regression vs KNN](https://i.ibb.co/tXjJMmM/lr-vs-knn.png)\n- KNN is a nonparametric model in which LR is a parametric model.\n- KNN is relatively slower than Logistic Regression.\n- KNN supports nonlinear solutions where LR only supports linear solutions.\n- LR can derive (about the estimate) confidence level while KNN can only omit tags.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Coding Time\n![](https://ac-cdn.azureedge.net/infusionnewssiteimages/agingcare/21e637ea-aa74-4ae2-b278-181d2cded7a3.jpg)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Import the necessary packages","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nimport sklearn\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix\nfrom sklearn.metrics import r2_score, roc_auc_score, roc_curve, classification_report","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Import and read dataset","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"input_ = \"../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv\"\ndata = pd.read_csv(input_)\ndf = data.copy()\n\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(data, \"age\", title=\"Age Distribution\", width=750)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(data, \"time\", title=\"Time Distribution\", width=750)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(data, \"creatinine_phosphokinase\", title=\"Creatinine Phosphokinase Distribution\", width=750)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(data, \"ejection_fraction\", title=\"Ejection Fraction Distribution\", width=750)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(data, \"platelets\", title=\"Platelets Distribution\", width=750)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(data, \"serum_creatinine\", title=\"Serum Creatinine Distribution\", width=750)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(data, \"serum_sodium\", title=\"Serum Sodium Distribution\", width=750)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"anaemia_dis = data[\"anaemia\"].value_counts().reset_index()\nfig = px.bar(anaemia_dis, x=\"index\", y=\"anaemia\", title=\"Anaemia Distribution\",\n             width=750, labels={\"index\": \"Anaemia\", \"anaemia\": \"Count\"})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes_dis = data[\"diabetes\"].value_counts().reset_index()\nfig = px.bar(diabetes_dis, x=\"index\", y=\"diabetes\", title=\"Diabetes Distribution\", \n             width=750, labels={\"index\": \"Diabetes\", \"diabetes\": \"Count\"})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hbp_dis = data[\"high_blood_pressure\"].value_counts().reset_index()\nfig = px.bar(hbp_dis, x=\"index\", y=\"high_blood_pressure\", title=\"High Blood Pressure Distribution\",\n             width=750, labels={\"index\": \"High Blood Pressure\", \"high_blood_pressure\": \"Count\"})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sex_dis = data[\"sex\"].value_counts().reset_index()\nfig = px.bar(sex_dis, x=\"index\", y=\"sex\", title=\"Sex Distribution\",\n             width=750, labels={\"index\": \"Sec\", \"sex\": \"Count\"})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smooking_dis = data[\"smoking\"].value_counts().reset_index()\nfig = px.bar(smooking_dis, x=\"index\", y=\"smoking\", title=\"Sex Distribution\",\n             width=750, labels={\"index\": \"Smooking\", \"smoking\": \"Count\"})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"death_dis = data[\"DEATH_EVENT\"].value_counts().reset_index()\nfig = px.bar(death_dis, x=\"index\", y=\"DEATH_EVENT\", title=\"DEATH EVENT Distribution\",\n             width=750, labels={\"index\": \"DEATH_EVENT\", \"DEATH_EVENT\": \"Count\"})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.pie(data, values='DEATH_EVENT',names='sex', title='GENDER',\n      width=680, height=480)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(14,14))\nsns.heatmap(data.corr(), annot=True, linewidths=.5, fmt=\".1f\", ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data[['age', 'creatinine_phosphokinase',\n       'ejection_fraction', 'platelets',\n       'serum_creatinine', 'serum_sodium','time',\n       'DEATH_EVENT']], hue=\"DEATH_EVENT\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Construction (with our own talent)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"inp_data = data.drop(data[['DEATH_EVENT']], axis=1)\nout_data = data[['DEATH_EVENT']]\n\nscaler = StandardScaler()\ninp_data = scaler.fit_transform(inp_data)\n\nX_train, X_test, y_train, y_test = train_test_split(inp_data, out_data, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"X_train Shape : \", X_train.shape)\nprint(\"X_test Shape  : \", X_test.shape)\nprint(\"y_train Shape : \", y_train.shape)\nprint(\"y_test Shape  : \", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def weightInitialization(n_features):\n    w = np.zeros((1, n_features))\n    b = 0\n    return w,b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid_activation(result):\n    final_result = 1/(1 + np.exp(-result))\n    return final_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cost Formula\n![Cost Formula](https://miro.medium.com/max/2908/1*dEZxrHeNGlhfNt-JyRLpig.png)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_optimize(w, b, X, Y):\n    m = X.shape[0]\n    \n    # Prediction\n    final_result = sigmoid_activation(np.dot(w,X.T) + b)\n    cost = (-1/m)*(np.sum(Y.T * np.log(final_result)) + ((1-Y.T) * (np.log(1-final_result))))\n    \n    # Gradient Calculation\n    dw = (1/m)*(np.dot(X.T, (final_result-Y.T).T)) # look down (photo)\n    db = (1/m)*(np.sum(final_result-Y.T))\n    \n    grads = {\n        \"dw\": dw,\n        \"db\": db\n    }\n    \n    return grads, cost","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](https://i.ibb.co/ZV334Mn/20.png)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_predict(w, b, X, Y, learning_rate, no_iterations):\n    costs = []\n    for i in range(no_iterations):\n        grads, cost = model_optimize(w, b, X, Y)\n        dw = grads['dw']\n        db = grads['db']\n        \n        w = w - (learning_rate * dw.T) # look up (photo)\n        b = b - (learning_rate * db)\n        \n        if (i % 100 == 0):\n            costs.append(cost)\n            \n    # final parameters\n    coeff = {\"w\":w, \"b\":b}\n    gradient = {\"dw\":dw, \"db\":db}\n    \n    return coeff, gradient, costs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(final_pred, m):\n    y_pred = np.zeros((1,m))\n    for i in range(final_pred.shape[1]):\n        if final_pred[0][i] > 0.5:\n            y_pred[0][i] = 1\n    return y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get number of features\nn_features = X_train.shape[1]\nprint('Number of Features: {}'.format(n_features))\n\nw, b = weightInitialization(n_features)\n# Gradient Descent\ncoeff, gradient, costs = model_predict(w, b, X_train, y_train.values.reshape(-1,1), learning_rate=0.0001,no_iterations=4500)\n# Final Prediction\nw = coeff['w']\nb = coeff['b']\nprint('Optimized weights: {}'.format(w))\nprint('Optimized intercept: {}'.format(b))\n\nfinal_train_pred = sigmoid_activation(np.dot(w,X_train.T)+b)\nfinal_test_pred = sigmoid_activation(np.dot(w,X_test.T)+b)\n\nprint(\"=\"*60)\n\ny_train_pred = predict(final_train_pred, X_train.shape[0])\nprint('Training Accuracy             : {:.4f}'.format(accuracy_score(y_train_pred.T, y_train)))\n\ny_test_pred = predict(final_test_pred, X_test.shape[0])\nprint('Test Accuracy                 : {:.4f}'.format(accuracy_score(y_test_pred.T, y_test)))\n\nprint('Logistic Regression f1-score  : {:.4f}'.format(f1_score(y_test_pred.T, y_test)))\nprint('Logistic Regression precision : {:.4f}'.format(precision_score(y_test_pred.T, y_test)))\nprint('Logistic Regression recall    : {:.4f}'.format(recall_score(y_test_pred.T, y_test)))\nprint(\"\\n\",classification_report(y_test_pred.T, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cf_matrix = confusion_matrix(y_test_pred.T, y_test)\nsns.heatmap((cf_matrix / np.sum(cf_matrix)*100), annot = True, fmt=\".2f\", cmap=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reporting\n\nI evaluated the results I found with Confusion Matrix, the results are as follows:\n\n**Correctly predicted -> %81.67 (244 of 299 predict are correct)**\n- True Negative -> %56.67 -> Those who were predicted not to die and who did not die\n- True Positive -> %25.00 -> Those who were predicted to die and who did die\n\n**Wrong predicted-> %18.33 (50 of 299 predict are wrong)**\n- False Positive -> %16.67 -> Those who were predicted to die but who did not die\n- False Negative -> %01.67 -> Those who were predicted to not die but who did die\n\n**Not dead**\n- 203 -> Those who haven't died in the real data set\n- 219 -> Predicted for test data set\n\n**The dead**\n- 96 -> Those who have died in the real data set\n- 80 -> Predicted for test data set","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## After the SMOTE process (Shortcut for logistic regression)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression\n\nsms = SMOTE(random_state=12345)\nX_res, y_res = sms.fit_sample(inp_data, out_data)\n\nprint(\"X_train Shape : \", X_train.shape)\nprint(\"X_test Shape  : \", X_test.shape)\nprint(\"y_train Shape : \", y_train.shape)\nprint(\"y_test Shape  : \", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\ny_pred = logreg.predict(X_test)\n\nprint('Accuracy of logistic regression classifier on test set: {}'.format(logreg.score(X_test, y_test)))\nprint('Logistic Regression f1-score  : {:.4f}'.format(f1_score(y_pred, y_test)))\nprint('Logistic Regression precision : {:.4f}'.format(precision_score(y_pred, y_test)))\nprint('Logistic Regression recall    : {:.4f}'.format(recall_score(y_pred, y_test)))\nprint(\"\\n\",classification_report(y_pred, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cf_matrix = confusion_matrix(y_pred, y_test)\nsns.heatmap((cf_matrix / np.sum(cf_matrix)*100), annot = True, fmt=\".2f\", cmap=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\nplt.figure(figsize=(10,6))\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Reporting')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}