{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pandasql import *\nimport requests\nimport numpy as np\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\nfrom sklearn.tree import plot_tree\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import plot_confusion_matrix\nfrom collections import Counter \nfrom imblearn.under_sampling import RandomUnderSampler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings            \nwarnings.filterwarnings(\"ignore\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hos_in_pt_dis_df = pd.read_csv('../input/2015-deidentified-ny-inpatient-discharge-sparcs/Hospital_Inpatient_Discharges__SPARCS_De-Identified___2015.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hos_in_pt_dis_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(hos_in_pt_dis_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hos_in_pt_dis_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hos_in_pt_dis_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data Wrangling**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Data Wrangling\nNext, let's check for the presence of null values in our table and deal with them appropriately. Here, I find that some columns have a few null values, which can be dropped easily, while other columns have large number of null values. For these columns, I simply drop them as they do not provide much information.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hos_in_pt_dis_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets drop Other Provider License Number since over a almost 2/3 of these rows have NaN values. Lets also drop the other columns with a significant amount on NaNs since it will be difficult to extract value from them","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets drop Other Provider License Number since over a almost 2/3 of these rows have NaN values\n#Lets also drop the other columns with a significant amount on NaNs since it will be difficult to extract value from them\nhos_df = hos_in_pt_dis_df.drop(['Other Provider License Number','Payment Typology 2','Payment Typology 3','Operating Provider License Number'], axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hos_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#next, lets drop rows with NaNs for columns such as the APR Severity of Illness Description and APR Risk of Mortality since these have a \n#relatively small number of NaN values and thus will be insignificant to drop.\nhos_df = hos_df.dropna(subset=['APR Severity of Illness Description', 'APR Risk of Mortality','Zip Code - 3 digits','Facility Id','Attending Provider License Number'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hos_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, I would like to convert some of these object columns to other data types where applicable. First I will look at the length of stay column, which is currently an object. Although I would like to convert it to int to allow for the possibility of regression analysis, some entries are written as '120 +', which creates problems for this conversion. Thus, I will treat all entires '120 +' as 120. I do similar processes for the other numeric columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here, the apply operation is used as an alternative to an if statement for highest computational efficiency\n#Convert number objects\nhos_df['Length of Stay'] = hos_df['Length of Stay'].apply(lambda x: str(x).split(' ')[0])\nhos_df['Length of Stay'] = pd.to_numeric(hos_df['Length of Stay'])\nhos_df['Total Costs'] = hos_df['Total Costs'].apply(lambda x: str(x).replace('$',''))\nhos_df['Total Costs'] = pd.to_numeric(hos_df['Total Costs'])\nhos_df['Total Charges'] = hos_df['Total Charges'].apply(lambda x: str(x).replace('$',''))\nhos_df['Total Charges'] = pd.to_numeric(hos_df['Total Charges'])\n#upon inspection, I also found that some entries in the zip code column had the string OOS instead of a number. 67,000 rows had this\n#which seems to large to simply drop these rows. Looking into the information about the dataset, these zipcodes refer to out of state. \n#This could be useful because these people might be rich so there might be differences in length of stay\n#Thus, I will keep these rows and signify them with a 999, which now indicates out of state\nhos_df['Zip Code - 3 digits'] = hos_df['Zip Code - 3 digits'].apply(lambda x: str(x).replace('OOS','999'))  \nhos_df['Zip Code - 3 digits'] = pd.to_numeric(hos_df['Zip Code - 3 digits'])\ndisplay(hos_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Now lets visualize some initial stats on the results of the data cleaning above\nhos_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Make a heatmap\nhos_df.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(11, 9))\ncorr = hos_df.corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this initial look at a correlation matrix of our data, we can see that a couple of features have relatively strong positive correlation with each other, and more importantly, length of stay. Although the correlation matrix does not make sense to look at for all of our columns, we can see from it that APR Severity of Illness Code has a strong positive correlation with length of stay, along with total charges and total costs. CCS Diagnosis code also seems to have a slight positive correlation with length of stay. We can see other positive correlations in the dataset between features such as CCS Diagnosis codes and APR DRG codes.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Exploratory Data Analysis & Data Visualization**\nIn this section, I will explore and visualize underlying relationships in the data. For the following analysis, I keep length of stay as the primary varaiable along the y-axis since it is the predictor variable for this project. Thus, I am most interested at looking how features in this dataset affect length of stay. Some of the features found to have the most variance in length of stay are listed below:\n\nPayment Typology\nSeverity of Illness\nAge Group\nType of Admission\nFirst, let's look at the univariate distribution of length of stay values in the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(font_scale=1.2)\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(11, 9))\nsns.distplot(hos_df['Length of Stay'], norm_hist=False);\nplt.ylabel('Probability Density')\nplt.title('Univariate Distribution Plot of Length of Stay')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see this distribution is very skewed. Let's take a look at birthweight next as I noticed that the range of the value seems odd. From the df.describe() output, we can see that the 25%, 50% and 75% quartile of values are all 0. The histogram below confirms that most of the birth weights are 0. This makes sense when we consider that this column is probably only relevant for new born babies, and is entered as 0 for all other patients. Let's confirm this with some further analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets see if we can confirm that there is data only input for the youngest age group\n\nbirth_weight_df = hos_df[['Type of Admission', 'Birth Weight']].groupby('Type of Admission').mean()\ndisplay(birth_weight_df)\nweight_age_df = hos_df[['Age Group', 'Birth Weight']].groupby('Age Group').mean()\ndisplay(weight_age_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets look at the relationship between birth weight and length of stay only for the newborns as this feature makes the most sense\n#for this group\n\nbirth_youngest_stay = hos_df[hos_df['Type of Admission'].str.contains('Newborn')]\nbirth_youngest_stay['Birth Weight'] = birth_youngest_stay['Birth Weight'].apply(lambda x: float(x/454)) #convert from grams to pounds\nf, ax = plt.subplots(figsize=(11, 9))\nsns.scatterplot(x=\"Birth Weight\", y=\"Length of Stay\",\n                data=birth_youngest_stay)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When looking at a scatter plot of birth weight and length of stay, we can see two distinct clusters - one at the lower left hand corner and one at the upper right. It is interesting to see some newborns with a birth weight of approximately 20 pounds (newborns must be a loose term here and it is most likely referring to children under a certain age). These babies seem to have particularly long lengths of stay as they are clustered at the top right hand corner of the graph. This indicates to me that birth weight might be an important feature for predicting length of stay for newborns. However, for the rest of patients, the data does not contain much value as over 75% of the values are 0. This column will be dropped for the bulk of the analysis, with the option of being added back in to create a model separately for new born babies.\n\nNext, I am curious what the babies who have the longest lengths of stay are diagnosed with.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"longest_newborn_df = birth_youngest_stay[birth_youngest_stay['Length of Stay']==120]\ndisplay(longest_newborn_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am interested in seeing what procedures the babies who stayed in the hospital for 120 days or longer had to recieve. A frequency plot for each procedure description in this group of newborns is shown below","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(25, 15))\nsns.countplot(x='CCS Procedure Description', data = longest_newborn_df)\nplt.xticks(rotation=90)\nplt.title('Procedure Descriptions for Newborns Who Stay 120 Days or Longer')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the babies in this group required a ventilator. This may imply that if a baby requires to be put on a ventilator and is having respiratory issues, there is a high probability that they will have a long stay at the hospital (120+ days)\n\nBelow, I begin the bulk of the exploratory data visualization for this project as I analyze the relationships bewteen various features and the predictor feaature for this project: length of stay. Major takeaways and interpretations are described below each figure. However, many of the graphs are pretty intuitive and simply show an interesting underlying relationship between features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(11, 9))\nsns.boxplot(x=\"Gender\", y=\"Length of Stay\",\n            hue=\"Race\",\n            data=hos_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the length of stay data is very skewed across all genders and races, which makes it difficult to see relationships. I will limit the range of the y axis from now on in order to more clearly see all underlying distributions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nf, ax = plt.subplots(figsize=(11, 9))\nsns.boxplot(x=\"Gender\", y=\"Length of Stay\",\n            hue=\"Race\",\n            data=hos_df)\nax.set(ylim=(0, 30))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(11, 9))\nsns.boxplot(x=\"Age Group\", y=\"Length of Stay\",\n            data=hos_df)\nax.set(ylim=(0, 30))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(11, 9))\nsns.boxplot(x=\"Age Group\", y=\"Length of Stay\", order=['0 to 17','18 to 29','30 to 49','50 to 69','70 or Older'],\n            palette=\"Set1\", data=hos_df)\nax.set(ylim=(0, 30))\nplt.title('Length of Stay vs. Age Group')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we can see some significant variance in length of stay across different age groups. It is evident that age groups 50-69 and 70 or older tend to have the longest stay distributions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nf, ax = plt.subplots(figsize=(11, 9))\nsns.boxplot(x=\"Race\", y=\"Length of Stay\", data=hos_df, palette='Set1')\nplt.title('Length of Stay vs. Patient Race')\nax.set(ylim=(0, 30))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What is the average length of stay for each diagnosis description in the dataset?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(35, 20))\nsns.barplot(x=\"CCS Diagnosis Description\", y=\"Length of Stay\", data=hos_df[['CCS Diagnosis Description','Length of Stay']].groupby('CCS Diagnosis Description', as_index=False).mean())\nax.set(ylim=(0, 20))\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This plot makes it nice to visualize the spread of length of stay values for different diagnoses, but there are a lot of categories which makes it difficult to read. Let's create a dataframe and list diagnoses in descending order of average length of stay to see the diagnosis descriptions that have the longest length of stay.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"diag_stay_df = hos_df[['CCS Diagnosis Description','Length of Stay']].groupby('CCS Diagnosis Description', as_index=False).mean()\ndiag_stay_df = diag_stay_df.sort_values(by='Length of Stay', ascending=False, ignore_index=True)\ndisplay(diag_stay_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(font_scale=1.2)\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 9))\nsns.barplot(y=\"CCS Diagnosis Description\", x=\"Length of Stay\", data=diag_stay_df[0:5], palette='Set1')\n#plt.xticks(rotation=45)\nplt.title('Top 5 Diagnoses with Longest Average Length of Stay')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see which diagnosis descriptions cost the hospital the most money","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(30, 15))\nsns.barplot(x=\"CCS Diagnosis Description\", y=\"Total Costs\", data=hos_df[['CCS Diagnosis Description','Total Costs']].groupby('CCS Diagnosis Description', as_index=False).mean())\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, this plot is hard to read so lets print out a dataframe with the rows listed in descending order of cost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"diag_costs_df = hos_df[['CCS Diagnosis Description','Total Costs']].groupby('CCS Diagnosis Description', as_index=False).mean()\ndiag_costs_df = diag_costs_df.sort_values(by='Total Costs', ascending=False, ignore_index=True)\ndisplay(diag_costs_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, through the use of an inner join, let's see how the diagnosis descriptions that cost the most compare with their length of stay","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Rank the total costs and then do an inner join\ndiag_costs_df = diag_costs_df.reset_index()\njoined_df = diag_stay_df.merge(right=diag_costs_df, how='inner', on='CCS Diagnosis Description')\ndisplay(joined_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there is definitely a strong relationship between the most expensive diagnoses and a longer inpatient length of stay. However, for example, the diagnosis description with the second longest average length of stay is only the fifth most expensive to the hospital","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(font_scale=1.25)\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(15, 9))\nsns.boxplot(x=\"Payment Typology 1\", y=\"Length of Stay\", data=hos_df, palette='Set1')\n#plt.title('Type of Patient Payments vs. Length of Stay')\nax.set(ylim=(0, 20))\nplt.xticks(rotation=80)\nplt.title('Length of Stay vs. Primary Payment Typology')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we see another feature that has a significant amount of variance in length of stay across its categories. Medicare patients tend to have one of the longest length of stays. This may be because most medicare patients are in an older age bracket. Let's see if we can visually support this hypothesis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(11, 9))\nsns.countplot(x='Age Group', data = hos_df[hos_df['Payment Typology 1']=='Medicare'], order=['0 to 17','18 to 29','30 to 49','50 to 69','70 or Older'], palette='Set1')\nplt.title('Number of Medicare Patients in Each Age Group')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This hypothesis is supported by the graph above. Age/Payment typology seem to be correlated in some ways, and these seem like they have an effect on the length of stay of the patient\n\nNext, let's take a look at how length of stay varies across different patient zip code areas","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(25, 9))\nsns.boxplot(x=\"Zip Code - 3 digits\", y=\"Length of Stay\", data=hos_df)\nax.set(ylim=(0, 20))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plots above urge me to wonder how much income has an affect on length of stay since we can see differences across zipcodes and health insurance programs, which are highly correlated with income differences. Let's see if we can webscrape the income for each 3-digit zipcode provided and see if a relationship between these variables truly does exist.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Webscraping**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This section primarily serves to extract a more useful feature than raw zipcode for the models I will train: webscraped average income data for each zipcode. Following this section, I will be able to make use of this feature instead of having to encode the zipcode column, which could have the result of drastically increasing the dimensionality of my dataset if the column is one hot encoded. Furthermore, patterns in average income may be more informative for any given model than a raw zipcode","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#We can scrap the data from the web, but I have downloaded the file and uploaded the same.\n\n#Scraped income data by zipcode from web (data from 2006-2010)\n\n#dls = \"https://www.psc.isr.umich.edu/dis/census/Features/tract2zip/MeanZIP-3.xlsx\"\n#resp = requests.get(dls)\n\n#output = open('zip_incomes.xlsx', 'wb')\n#output.write(resp.content)\n#output.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"zip_income_df = pd.read_excel('../input/mean-zip/MeanZIP-3.xlsx')\ndisplay(zip_income_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we only want 5 digit zipcodes at first (since some zipcodes in the table have 4 digits. These zipcodes are assumed to have a 0 at the beginning, which would place them in areas such as Massachusetts). Next, I will truncate the zipcodes to only contain the first three digits. This is because HIPAA regulations result in only the 3-digit zipcode data being available. Therefore, the zipcodes in the dataset only have 3 digits. As such, I will do an aggregate mean on the zipcodes with the same first three digits to get an average income for all zipcodes which share the same first three digits.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"zip_income_df['Zip'] = zip_income_df['Zip'].apply(lambda x: int(x))\nincome_df = zip_income_df[zip_income_df['Zip'] > 9999]\nincome_df['Zip'] = income_df['Zip'].apply(lambda x: math.floor(x/100)) #cut down zip code to just first three digits\ndisplay(income_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"income_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, lets group by and join using SQL commands to do this most efficiently. Not only is this more intuitive to write, but we can make sure that we are utilizing any behind the scenes query optimization built into the SQL language. Median income is used since income distributions are known to be skewed, and median provides a better measure of central tendency for these distributions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"query = '''SELECT ZIP, AVG(MEDIAN) as median FROM income_df GROUP BY ZIP'''\navg_income = sqldf(query, locals())\ndisplay(avg_income)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now, we can inner join this onto our dataframe to get the average income for each patient zipcode\n\navg_income['Zip'] = avg_income['Zip'].astype('object')\n\nquery = '''SELECT d.*, a.median as AvgIncome FROM hos_df d inner join avg_income a on d.'Zip Code - 3 digits' = a.Zip'''\nhos_sql_df = sqldf(query, locals())\ndisplay(hos_sql_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nHowever, there is still one more issue we need to take care of. If you remember from above, I changed out of state zipcodes (encoded as OOS) to the number 999. Now, these zipcodes got joined with the average income for zipcodes associated with 999, even though they are not actually associated with this location. In order to deal with this, I will first find the most common health insurance program(s) for out of state patients (zipcode 999). Then, I will find the average income for this health insurance program(s) and use it for patients with zipcode 999 (out of state patients).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(11, 9))\nsns.countplot(x=\"Payment Typology 1\",\n            data=hos_sql_df[hos_sql_df['Zip Code - 3 digits']==999])\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As illustrated by the plot above, medicare and private health insurance are the two most common health insurance programs. I will find the average income for these two programs and use that number as the income for all patients with zipcode 999.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#for this calculation, I will exlclude the data with zipcode equal to 999\ndf_no999 = hos_sql_df[hos_sql_df['Zip Code - 3 digits'] != 999]\ninsurance_df = df_no999[df_no999['Payment Typology 1'].isin(['Medicare','Private Health Insurance'])]\nmean_zip999 = insurance_df['AvgIncome'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#round zipcodes to make replace easier\nhos_sql_df['AvgIncome'] = hos_sql_df['AvgIncome'].round(2)\nhos_sql_df[hos_sql_df['Zip Code - 3 digits']==999]['AvgIncome']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hos_sql_df = hos_sql_df.replace(47010.32, round(mean_zip999,2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below, I plot the zip code against average income to visualize the distribution of the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(11, 9))\nsns.barplot(x=\"Zip Code - 3 digits\", y=\"AvgIncome\",\n            data=hos_sql_df, palette='Set1')\nplt.ylabel('Median Income')\nplt.title('Income Distribution Across 3 Digit Patient Zipcodes')\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#From left to right we have increasing income, organized this way since the x-axis labels are hard to read\nf, ax = plt.subplots(figsize=(25, 9))\nsns.boxplot(x=\"AvgIncome\", y=\"Length of Stay\",\n            data=hos_sql_df.sort_values(by='AvgIncome', ascending=True))\nplt.xticks(rotation=90)\nax.set(ylim=(0, 20))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there is not a huge amount of variation in length of stay across the zipcodes. However, they will still be more useful than the raw zipcode column, so I use average income as a feature instead in the modeling section of this project","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **More Data Visualization**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now let's use this new webscraped feature to visuaize its correlation with other features in our dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(font_scale=1.25)\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(15, 9))\nsns.barplot(x=\"Payment Typology 1\", y=\"AvgIncome\",\n            data=hos_sql_df, palette='Set1')\nplt.xticks(rotation=80)\nplt.title('Median Income vs. Primary Payment Typology')\nplt.ylabel('Median Income')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see some variance in average income across this feature, which makes sense as different insurance programs are more common for people from different economic backgrounds. This gives me confidence that my webscrape was at least somewhat useful, supporting the notion of proceeding with it as a feature","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(11, 9))\nsns.boxplot(x=\"Health Service Area\", y=\"Length of Stay\",\n            data=hos_sql_df)\nplt.xticks(rotation=90)\nax.set(ylim=(0, 20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(11, 9))\nsns.boxplot(x=\"APR Severity of Illness Description\", y=\"Length of Stay\",\n            data=hos_sql_df, palette='Reds')\nax.set(ylim=(0, 40))\nplt.title('Length of Stay vs. Severity of Illness')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is high amounts of variance in length of stay against the severity of illness feature. This will be an important feature to include in the model and will improve its predictive ability","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(11, 9))\nsns.boxplot(x=\"Ethnicity\", y=\"Length of Stay\",\n            data=hos_sql_df)\nax.set(ylim=(0, 20))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see the ethnicity column encodes less information then the Race column and does not have much variance across different ethnicities, so we will drop it later","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(11, 9))\nsns.boxplot(x=\"Type of Admission\", y=\"Length of Stay\",\n            data=hos_sql_df)\nax.set(ylim=(0, 20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(11, 9))\nsns.boxplot(x=\"Type of Admission\", y=\"Length of Stay\",\n            data=hos_sql_df, palette='Set1')\nplt.title('Length of Stay vs. Type of Admission')\nax.set(ylim=(0, 20))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's visualize the positive linear relationship between length of stay and total costs, just out of curiousity for the slope of this correlation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(11, 9))\nsns.regplot(x=\"Length of Stay\", y=\"Total Costs\",\n            data=hos_sql_df[0:200000])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's also plot a frequency plot of severity of illness for all patients who stay for 120 days or longer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10, 8))\nsns.countplot(x='APR Severity of Illness Description', data = hos_sql_df[hos_sql_df['Length of Stay']==120])\nplt.title('Severity of Illness vs. Length of Stay for Patients With 120+ Lengths of Stay')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that of the approximately 1900 patients who have stays of 120 days or longer, most of them were extreme illnesses, which makes sense. Now, lets visualize the univariate distributions of each of the variables through a pairplot, which provides a quick way of seeing this for all our numeric variables","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Wordclouds**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this section, I perform text parsing and analysis on diagnosis descriptions, followed by the creation of wordclouds to visualize these results. From this analysis, we can see stark differences in the diagnosis descriptions for each illness severity type. These differences are interesting to compare, and some of the results are summarized below:\n\nMinor Illness: Pregnancy, disease, liveborn, complications, osteoarthritis\nModerate Illness: Schizophrenia, disease, complications, psychotic\nMajor Illness: Failure, disorders, acute\nExtreme Illness: Septicemia, cerebrovascular, tuberculosis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"diagnosis_desc_df = hos_sql_df[['CCS Diagnosis Description']][0:10000]\ndiagnosis_desc_df['CCS Diagnosis Description'] = diagnosis_desc_df['CCS Diagnosis Description'].astype('str', errors = 'ignore')\ndiagnosis_desc_df['CCS Diagnosis Description'] = diagnosis_desc_df['CCS Diagnosis Description'].apply(lambda x: x.lower())\n\nlist_of_titles = []\nfrom nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer(r'\\w+')\ndiagnosis_desc_df['Tokens'] = diagnosis_desc_df['CCS Diagnosis Description'].apply(lambda x: tokenizer.tokenize(x))\nlist_of_tokens = diagnosis_desc_df['Tokens'].tolist()\ndiagnosis_words = []\nfor sublist in list_of_tokens:\n    for item in sublist:\n        diagnosis_words.append(item)\n\nfrom wordcloud import WordCloud\nimport nltk\nfrom nltk.corpus import stopwords\n\n#Let's remove stop words as well, such as \"a\", \"and\", and \"the\"\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english')) \nfor word in list(diagnosis_words):\n    if word in stop_words:\n        diagnosis_words.remove(word)\n\nfrom collections import Counter\nCounter1 = Counter(diagnosis_words)\nmost_occur = Counter1.most_common(30) \ndiagnosis_counter = []\nfor item in most_occur:\n  diagnosis_counter.append(item[0])\n\n#Create word cloud plot\ncloud_words = ' '\nfor words in diagnosis_counter: \n    cloud_words = cloud_words + words + ' '\n\ndiagnosis_word_plot = WordCloud(width = 800, height = 800).generate(cloud_words)\nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(diagnosis_word_plot)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's make one of these for each severity type to see if we can notice any differences.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Minor\nhos_sql_df['CCS Diagnosis Description'] = hos_sql_df['CCS Diagnosis Description'].astype('str', errors = 'ignore')\nhos_sql_df['CCS Diagnosis Description'] = hos_sql_df['CCS Diagnosis Description'].apply(lambda x: x.lower())\nminor_df = hos_sql_df[['CCS Diagnosis Description']][hos_sql_df['APR Severity of Illness Description']=='Minor']\nminor_df = minor_df[['CCS Diagnosis Description']][0:40000]\n\n#lets create a function for this\ndef generate_cloud(type_df):\n  list_of_titles = []\n  from nltk.tokenize import RegexpTokenizer\n  tokenizer = RegexpTokenizer(r'\\w+')\n  type_df['Tokens'] = type_df['CCS Diagnosis Description'].apply(lambda x: tokenizer.tokenize(x))\n  list_of_tokens = type_df['Tokens'].tolist()\n  type_words = []\n  for sublist in list_of_tokens:\n      for item in sublist:\n          type_words.append(item)\n\n  for word in list(type_words):\n    if word in stop_words:\n        type_words.remove(word)\n\n  Counter1 = Counter(type_words)\n  most_occur = Counter1.most_common(30) \n  diagnosis_counter = []\n  for item in most_occur:\n    diagnosis_counter.append(item[0])\n  \n  from wordcloud import WordCloud\n  #Create word cloud plot\n  cloud_words = ' '\n  for words in diagnosis_counter: \n      cloud_words = cloud_words + words + ' '\n\n  type_word_plot = WordCloud(width = 800, height = 800).generate(cloud_words)\n  plt.figure(figsize = (8, 8), facecolor = None) \n  plt.imshow(type_word_plot)\n  plt.show()\n\ngenerate_cloud(minor_df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"moderate_df = hos_sql_df[['CCS Diagnosis Description']][hos_sql_df['APR Severity of Illness Description']=='Moderate']\nmoderate_df = moderate_df[['CCS Diagnosis Description']][0:40000]\n\ngenerate_cloud(moderate_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"major_df = hos_sql_df[['CCS Diagnosis Description']][hos_sql_df['APR Severity of Illness Description']=='Major']\nmajor_df = major_df[['CCS Diagnosis Description']][0:40000]\n\ngenerate_cloud(major_df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extreme_df = hos_sql_df[['CCS Diagnosis Description']][hos_sql_df['APR Severity of Illness Description']=='Extreme']\nextreme_df = extreme_df[['CCS Diagnosis Description']][0:40000]\n\ngenerate_cloud(extreme_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop Columns\nNow, in preparation for the modeling component of this project and following the data visualization section where I used some of these columns, I will now drop all the columns that won't help our model predict length of stay. In this section, I end up dropping the following columns:\n\nZip Code\nFacility Name\nCCS Diagnosis Description\nCCS Procedure Description\nAPR DRG Description\nAPR MDC Description\nDischarge Year\nOperating Certificate Number\nEthnicity\nHospital County\nBirth Weight\nAPR Severity of Illness Description\nAttending Provider License Number\nPatient Disposition\nMost of these columns have corresponding columns with numeric values which I use instead in order to prevent the need for one-hot encoding columns with string values, which would dramatically increase the dimensionality of the dataset. I drop these types of columns after comparing them with the length of their corresponding \"code\" columns to ensure they have the same number of distinct columns and thus contain the same information.\n\nAlthough there are a few other columns that might not be strong predictors, such as attending provider license number, I decide to keep them after noticing slight decreases in model accuracy when the columns are dropped","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finally, lets drop the columns the rest of the columns we won't need for the modeling portion\n\n\nfig, ax =plt.subplots(1,2, figsize=(14,5))\nsns.countplot(hos_sql_df['APR Severity of Illness Description'], ax=ax[0])\nsns.countplot(hos_sql_df['APR Severity of Illness Code'], ax=ax[1])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hos_sql_df_1 = hos_sql_df.drop(['APR Severity of Illness Description'], axis=1)  #after confirming the illness code column encodes the same information\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_county = hos_sql_df_1['Hospital County'].unique().tolist()\nnum_zip = hos_sql_df_1['Zip Code - 3 digits'].unique().tolist()\nprint(\"Number of Hospital County's:\",len(num_county))\nprint(\"Number of Zipcodes:\",len(num_zip))\n#Below, we can see hospital county and zipcode do not encode same info.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_facilities = hos_sql_df_1['Facility Id'].unique().tolist()\nnum_facname = hos_sql_df_1['Facility Name'].unique().tolist()\nprint(\"Number of Facility Ids:\",len(num_facilities))\nprint(\"Number of Facility Names:\",len(num_facname))\n#We can see that these most likely encode the same info even though they are on off, so I will drop the names column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_diag_code = hos_sql_df_1['CCS Diagnosis Code'].unique().tolist()\nnum_diag_desc = hos_sql_df_1['CCS Diagnosis Description'].unique().tolist()\nprint(\"Number of Diagnosis Codes:\",len(num_diag_code))\nprint(\"Number of Diagnosis Descriptions:\",len(num_diag_desc))\n#Diagnosis Codes and Descriptions encode the same info so we will drop the descriptions.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Types of Procedure Descriptions:\",hos_sql_df_1['CCS Procedure Description'].unique().tolist())\n#I want to see a list of the possible descriptions since many of them say NO PROC. Below is the output\n#Lets verify the procedure code encodes the same info and then drop this column\nnum_proc_code = hos_sql_df_1['CCS Procedure Code'].unique().tolist()\nnum_proc_desc = hos_sql_df_1['CCS Procedure Description'].unique().tolist()\nprint(\"Number of Procedure Codes:\",len(num_proc_code))\nprint(\"Number of Procedure Descriptions:\",len(num_proc_desc))\n#They do contain the same information, so lets drop the descriptions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_drg_code = hos_sql_df_1['APR DRG Code'].unique().tolist()\nnum_drg_desc = hos_sql_df_1['APR DRG Description'].unique().tolist()\nprint(\"Number of DRG Codes:\",len(num_drg_code))\nprint(\"Number of DRG Descriptions:\",len(num_drg_desc))\n#Same number of unique values, so drop descriptions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_mdc_code = hos_sql_df_1['APR MDC Code'].unique().tolist()\nnum_mdc_desc = hos_sql_df_1['APR MDC Description'].unique().tolist()\nprint(\"Number of MDC Codes:\",len(num_mdc_code))\nprint(\"Number of MDC Descriptions:\",len(num_mdc_desc))\n#Same number of unique values, so drop descriptions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of Attending Provider License Numbers:\",len(hos_sql_df_1['Attending Provider License Number'].unique().tolist()))\n#This person is responsible for the overall care of the inpatient. Thus, they might play a large role in how long that person stays\n#so we will keep this column since there are 27,085 different attending providers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Run these once you have compiled all of them!\nhos_sql_df_1 = hos_sql_df_1.drop(['Zip Code - 3 digits'], axis=1) #use average income as a feature instead\nhos_sql_df_1 = hos_sql_df_1.drop(['Facility Name'], axis=1)\nhos_sql_df_1 = hos_sql_df_1.drop(['CCS Diagnosis Description'], axis=1)\nhos_sql_df_1 = hos_sql_df_1.drop(['CCS Procedure Description'], axis=1)\nhos_sql_df_1 = hos_sql_df_1.drop(['APR DRG Description'], axis=1)\nhos_sql_df_1 = hos_sql_df_1.drop(['APR MDC Description'], axis=1)\nhos_sql_df_1 = hos_sql_df_1.drop(['Discharge Year'], axis=1)    #since these are all 2015 since the dataset is from 2015 inpatient records\nhos_sql_df_1 = hos_sql_df_1.drop(['Operating Certificate Number'], axis=1)  #drop this column since it should not be a predictor for inpatient length of stay\nhos_sql_df_1 = hos_sql_df_1.drop(['Ethnicity'], axis=1) #contains less information than and is contained within the Race column, so let's drop\nhos_sql_df_1 = hos_sql_df_1.drop(['Hospital County'], axis=1) #lets drop hospital county column for computational efficiency\nhos_sql_df_1 = hos_sql_df_1.drop(['Birth Weight'], axis=1) #Now we will drop birth weight, which we looked at up above\nhos_sql_df_1 = hos_sql_df_1.drop(['Attending Provider License Number'], axis=1) #to enable generalization of model to any attending providers\nhos_sql_df_1 = hos_sql_df_1.drop(['Patient Disposition'], axis=1) #data leakage feature","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Feature Encoding**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we are ready to format our dataframe for modeling. However, for the data cleaning process, I converted a few columns that represent categorical features into numerical values to facilitate some processes. Now, these must be converted back to categorical columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hos_sql_df_1.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mort_string_index = {'Minor': 1, 'Moderate': 2, 'Major': 3, 'Extreme': 4}\nage_string_index = {'0 to 17': 1, '18 to 29': 2, '30 to 49': 3, '50 to 69': 4, '70 or Older': 5}\n\nhos_sql_df_1['Age Group'] = hos_sql_df_1['Age Group'].apply(lambda x: age_string_index[x])\nhos_sql_df_1['APR Risk of Mortality'] = hos_sql_df_1['APR Risk of Mortality'].apply(lambda x: mort_string_index[x])\ndisplay(hos_sql_df_1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One-hot encoding of all applicable columns is performed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_df = pd.get_dummies(hos_sql_df_1)\ndisplay(encoded_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_df['Facility Id'] = encoded_df['Facility Id'].astype('category')\nencoded_df['CCS Diagnosis Code'] = encoded_df['CCS Diagnosis Code'].astype('category')\nencoded_df['CCS Procedure Code'] = encoded_df['CCS Procedure Code'].astype('category')\nencoded_df['APR DRG Code'] = encoded_df['APR DRG Code'].astype('category')\nencoded_df['APR MDC Code'] = encoded_df['APR MDC Code'].astype('category')\nencoded_df['APR Severity of Illness Code'] = encoded_df['APR Severity of Illness Code'].astype('category')\n#encoded_df['Attending Provider License Number'] = encoded_df['Attending Provider License Number'].astype('category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's make the correlation matrix one more time to assess for correlation between our categorical variables. Here we can see some patches of strong positive and negative correlations in our data. This motivates the use of dimensionality reduction of correlated variables later on","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(11, 9))\ncorr = encoded_df.corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Preliminary Modeling**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here, I prepare my data for predictive modeling using machine learning methods. This preliminary process conists of two main steps:\n\nFirst, the the training data is separated from the labels by dropping the label column and dropping columns that would not be present at the time of length of stay prediction in a real-world scenario (such as total charges and total costs). This effectively eliminates any data leakage from occuring\nThen, the data is split into train and test sets for use in the remainder of the project","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = encoded_df.drop(['Length of Stay','Total Charges','Total Costs'], axis=1) #remove data leakage features\ny = encoded_df[['Length of Stay']]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For our first model, let's try a simple linear regression model to predict length of stay. This means treating length of stay as a somewhat continous variable ranging from interger values from 1 to 120","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Linear Regression**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Below, I initialize, train, and test a linear regression model on the data. I output model mean square error, in addition to an accuracy score by rounding the results of the prediction ouputs. This allows for easier comparison with classification models tested later on.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = LinearRegression().fit(X_train, y_train)\ny_train_pred = reg.predict(X_train)\ny_pred = reg.predict(X_test)\n\nmse_test = mean_squared_error(y_test, y_pred)\nmse_train = mean_squared_error(y_train, y_train_pred)\ny_round_pred = np.ndarray.round(y_pred)\ny_round_train_pred = np.ndarray.round(y_train_pred)\ntest_acc = accuracy_score(y_test, y_round_pred)\ntrain_acc = accuracy_score(y_train, y_round_train_pred)\nprint(mse_test)\nprint(mse_train)\nprint('Test accuracy:', test_acc)\nprint('Train accuracy:', train_acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the linear regression model does not perform particularly well. These results motivate me to move forward with machine learning multi-class classification models as opposed to regression models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **PCA**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Next, I perform principal component analysis (PCA) to see if this can improve future model results and efficiency by scaling and reducing the dimensionality of the data. As is shown below, I am able to eliminate ~20 columns from the dataframe while retaining 95% explained variance in the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = StandardScaler().fit_transform(X_train)\nx_test = StandardScaler().fit_transform(X_test)\n\npca1 = PCA()\npca1.fit(x_train)\nexplained_variance_ratio = pca1.explained_variance_ratio_\npc_vs_variance = np.cumsum(pca1.explained_variance_ratio_)\nplt.plot(pc_vs_variance)\nplt.xlabel('Number of Components')\nplt.ylabel('% Explained Variance')\nplt.title('PCA Explained Variance vs. Number of Components')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"variance_95 = list(filter(lambda i: i > 0.95, pc_vs_variance))[0]\ncomponent_95 = pc_vs_variance.tolist().index(variance_95)\nprint(component_95)\n\n#perform the dimensionality reduction\npca2 = PCA(n_components=component_95)\nx_train = pca2.fit_transform(x_train)\nx_test = pca2.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Which are the most important features?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Below, I briefly explore the relative feature importance in the dataset using the pca results performed above. This is done for as it will help answer questions such as: which factors actually influence how long patients stay in a hospital? This information could prove useful for hospitals as it can highlight important factors to pay attention to\n\nI calculate the features of greatest importance based off their maximum contribution along the top two principal components of the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pc_df = pd.DataFrame(pca2.components_,columns=X_train.columns)\ntop_pc = pc_df[0:1]\ndisplay(top_pc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_top_pc = top_pc.abs()\nfeatures = []\nfor i in range(0,10):\n  features.append(new_top_pc.idxmax(axis=1).tolist())\n  new_top_pc = new_top_pc.drop(columns=features[i])\nprint(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_pc = top_pc.abs()\nlist_magnitude = top_pc.loc[0, :].values.tolist()\nlabels = top_pc.columns.tolist()\nfeature_importance_df = pd.DataFrame({'Feature': labels, 'Relative Importance': list_magnitude})\n#feature_importance_df = feature_importance_df.nlargest(10, 'Relative Importance')\n\nfig, ax =plt.subplots(figsize=(18,15))\nsns.barplot(x='Feature', y='Relative Importance', data=feature_importance_df.reset_index())\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this figure, we can see that features such as the illness code, age group, type of admission, risk of mortality, payment typology and emergency department indicator play a large role in explaining the variance in the dataset. This supports the findings suggested by the figures in the data visualization section.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Decision Tree**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For our next model, let's train a decision tree classifier. Decision trees are one of the most common models for classification problems, so let's see how one performs on this data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Decision Tree - on non-pca data\n#from sklearn.tree import DecisionTreeClassifier\ndtree=DecisionTreeClassifier()\ndtree.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn import metrics\ntrain_predictions = dtree.predict(X_train)\ntest_predictions = dtree.predict(X_test)\nprint(\"Train Accuracy:\",metrics.accuracy_score(y_train, train_predictions))\nprint(\"Test Accuracy:\",metrics.accuracy_score(y_test, test_predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Without parameter optimization, we can see that we have definitely overfit. This is evident in the extremely high training accuracy, but with a much lower test accuracy. However, we are performing better than the linear regression model. Let's see if we can do better by setting some model hyperparameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.tree import DecisionTreeClassifier\ndtree=DecisionTreeClassifier(max_depth= 10, max_leaf_nodes=150)\ndtree.fit(x_train,y_train)\n\n#from sklearn import metrics\ntrain_predictions = dtree.predict(x_train)\ntest_predictions = dtree.predict(x_test)\nprint(\"Train Accuracy:\",metrics.accuracy_score(y_train, train_predictions))\nprint(\"Test Accuracy:\",metrics.accuracy_score(y_test, test_predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By setting the max depth and max leaf nodes parameter, I have successfully prevented overfitting of the decision tree. Below is a visualization of the tree, which allows us to asses model complexity visually by looking at the depth of the tree and the number of leaf nodes present","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.tree import plot_tree\nf, ax = plt.subplots(figsize=(50, 30))\nplot_tree(dtree)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To confirm the use of classification for the remainder of the modeling, I test out a decision tree regressor. We can see that it performs worse than its classification counterpart","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.tree import DecisionTreeRegressor\n#from sklearn.metrics import mean_squared_error\n#from sklearn.metrics import accuracy_score\n\nclf=DecisionTreeRegressor(max_depth=10, max_leaf_nodes=150)\nclf.fit(x_train,y_train)\ntrain_pred = clf.predict(x_train)\ntest_pred = clf.predict(x_test)\n\nmse_test = mean_squared_error(y_test, test_pred)\nmse_train = mean_squared_error(y_train, train_pred)\ny_round_pred = np.ndarray.round(test_pred)\ny_round_train_pred = np.ndarray.round(train_pred)\ntest_acc = accuracy_score(y_test, y_round_pred)\ntrain_acc = accuracy_score(y_train, y_round_train_pred)\nprint(mse_test)\nprint(mse_train)\nprint('Test accuracy:', test_acc)\nprint('Train accuracy:', train_acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We could keep tuning parameters with the decision tree classifier to get better results, but we will get better performance using a random forest - an ensemble of decision trees. However, before attempting to implement another model, I decide to look at another way of increasing model performance.\n\nAfter observing that decision tree classification seems to perform better than the regression, I am inclined to continue with classification. Along these lines, I will create bins of length of stay to construct a new categorical predictor column. Not only do these bins make sense given the fact that there is not a large difference between staying at a hospital for 57 days vs. 58 days, for example, but this will also increase the prediction accuracy of the model I build. This is evident in that the model can now predict a given patient will stay bewteen a given range of days, instead of having to predict an exact number of days. Below, I will perform this construction of new categories","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Label/Predictor Construction**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Summary of findings from this section:\n\nFrom the exploratory data analysis section of this project, I discovered the huge class imbalance present in the dataset given that most of the length of stays are clustered in the 1-5 day range\nClass imbalance can have significant negative effects of model performance. For example, model accuracy can be misleadinly high if the model simply only predicts the classes that appear alot, in the process disregarding features that may point to that data row belonging to another, less common class. I find that this is the case with my models.\nIn this section, I attempt to deal with this class imbalance in a few different ways after observing the affects of this class imbalance on model performance. These effects are dicussed in this section.\nAfter trying numerous different labeling schemes that involves length of stay bins of different sizes, and different numbers of length of stay bins that allowed for more/less specificty in predictions, I came to an optimal binning format that had a high accuracy score for most models trained on it, while not losing large amounts of specifity in predictions\nThe bins I decided to use for my final models are listed here: [0-3, 3-6, 6-9, 9-13, 13-20, 20-50, 50-120+] where all units are in days\nThis bin format results in a baseline accuracy of 1/7, or ~14.3% accuracy if the model were to guess randomly\nBelow shows some snippets of the iterative process I went through in order to determine this optimal bin format\n\nAs I create these categories, I will keep in mind that I want to create a somewhat even distribution of the number of data rows within each bin. The below label construction was made with the fact that small differences in length of stay matter less for patients who have long lengths of stay than for patients who are only staying 1 or 2 days. In other words, the difference between staying at a hospital for 1 day versus 2 days is more significant than staying 119 days vs. 120 days","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = [0,1,2,3,4,5,6,7,8,9,10,15,20,25,30,35,40,45,50,60,70,80,90,100,120]\nencoded_df['stay_bin']=pd.cut(x = encoded_df['Length of Stay'],\n                        bins = bins)\nencoded_df['stay_label']=pd.cut(x = encoded_df['Length of Stay'],\n                        bins = bins,\n                        labels = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23])\nencoded_df['stay_bin'] = encoded_df['stay_bin'].apply(lambda x: str(x).replace(',',' -'))\nencoded_df['stay_bin'] = encoded_df['stay_bin'].apply(lambda x: str(x).replace('120','120+')) #make this bin more descriptive\ndisplay(encoded_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I visualize my new class distribution below","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(20, 15))\nsns.countplot(x='stay_bin', data = encoded_df)\nplt.xticks(rotation=90)\nplt.title('Class Distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's run another decision tree classifier and see how much our classification improves now with these bins","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.model_selection import train_test_split\n\n#create train and test sets\nnew_X = encoded_df.drop(['Length of Stay','Total Charges','Total Costs','stay_bin','stay_label'], axis=1)\nnew_y = encoded_df[['stay_label']]\nX_train, X_test, y_train, y_test = train_test_split(new_X, new_y, test_size=0.3)\n\n#perform pca\n#from sklearn.preprocessing import StandardScaler\n#from sklearn.decomposition import PCA\nx_train = StandardScaler().fit_transform(X_train)\nx_test = StandardScaler().fit_transform(X_test)\n\npca = PCA(n_components=29) #50 components, as found above\nx_train = pca.fit_transform(x_train)\nx_test = pca.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are getting a similar accuracy as with our decision tree above. This caused me to raise an eyebrow and hypothesize that that there is an issue with class imbalancing, as the model may just be predicting shorter lengths of stay in both models since there are the most instances of these classes. Let's plot a confusion matrix to see if we can confirm this","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create confusion matrix\n#from sklearn.metrics import plot_confusion_matrix\nf, ax = plt.subplots(figsize=(20, 20))\nplot_confusion_matrix(dtree, x_test, y_test, cmap=plt.cm.Blues, normalize='true', ax=ax)\nplt.title('Confusion Matrix with Normalization')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the dramatic effects of the class imbalance on this model from this confusion matrix. As we can see, the model is mostly predicting labels 0-4. Then, it almost never predicts any values between 5 and 9 or 11 and 23. Instead, it classifies the remainder of the predictions in the 10 label. An idea for the reason behind this is that there is a spike in class count in this bin as seen from the class distribution plot above. Thus, the model predicts rows with features that correspond to longer lenghts of stay entirely in this bin as the most of these instances correspond to this label. This result illustrates the danger of class imbalance in a modeling problem\n\nNext, I explore two ways of dealing with class imbalance: undersampling and assinging class weights that penalize overpredicting the most common label","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Undersampling Techniques Following Label Construction**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#We can see that we have major class imbalance issues. I deal with that here\n#from collections import Counter \ncounts = y_train['stay_label'].value_counts().tolist()\nprint(counts)\n#df_class_0_under = df_class_0.sample(count_class_1)\n#df_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from imblearn.under_sampling import RandomUnderSampler\n\nresample_dict = {0:112307, 1:112307, 2:112307, 3:112307} #resample the first four classes to have the same number of instances as the (10-15] bucket\nrus = RandomUnderSampler(random_state=0, sampling_strategy=resample_dict)\nx_resampled, y_resampled = rus.fit_resample(x_train, y_train)\nunique_elements, counts_elements = np.unique(y_resampled, return_counts=True)\nprint(np.asarray((unique_elements, counts_elements)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot the new class distribution in the train set\n\n#y_resampled_df = pd.DataFrame(data=y_resampled, columns=['label'])\ny_resampled_df = pd.DataFrame(data=y_resampled, columns=['stay_label'])\n\nf, ax = plt.subplots(figsize=(20, 15))\nsns.countplot(x='stay_label', data = y_resampled_df)\nplt.title('Class Distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now lets run the decision tree and confusion matrix again\n#from sklearn.tree import DecisionTreeClassifier\ndtree=DecisionTreeClassifier(max_depth= 10, max_leaf_nodes=300)\ndtree.fit(x_resampled,y_resampled)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn import metrics\ntrain_predictions = dtree.predict(x_train)\ntest_predictions = dtree.predict(x_test)\nprint(\"Train Accuracy:\",metrics.accuracy_score(y_train, train_predictions))\nprint(\"Test Accuracy:\",metrics.accuracy_score(y_test, test_predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create confusion matrix\n#from sklearn.metrics import plot_confusion_matrix\nf, ax = plt.subplots(figsize=(20, 20))\nplot_confusion_matrix(dtree, x_test, y_test, cmap=plt.cm.Blues, normalize='true', ax=ax)\nplt.title('Confusion Matrix with Normalization')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are still over-predicting in certain bins. I undersample the originial training data even further until I reach a distribution that causes the trend visualized in the confusion matrix above to disappear.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets downsample all the way to one of our smallest bins (the last bin that contains lenght of stays 100-120+)\n#from collections import Counter \ncounts = y_train['stay_label'].value_counts().tolist()\nprint(counts)\n\n#from imblearn.under_sampling import RandomUnderSampler\n\nresample_dict = {0:1000, 1:1000, 2:1000, 3:1000, 4:1000, 5:1000, 6:1000, 7:1000, 8:1000, 9:1000, 10:1000, 11:1000, 12:1000, 13:1000,\n                 14:1000, 15:1000, 16:1000, 17:1000, 18:1000, 19:1000, 20:1000, 23:1000 } #lets take everything down to the 14th bucket size = 6785\nrus = RandomUnderSampler(random_state=0, sampling_strategy=resample_dict)\nx_resampled, y_resampled = rus.fit_resample(x_train, y_train)\nunique_elements, counts_elements = np.unique(y_resampled, return_counts=True)\nprint(np.asarray((unique_elements, counts_elements)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now lets run the decision tree and confusion matrix again\n#from sklearn.tree import DecisionTreeClassifier\ndtree=DecisionTreeClassifier(max_depth= 30, max_leaf_nodes=500)\ndtree.fit(x_resampled,y_resampled)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn import metrics\ntrain_predictions = dtree.predict(x_train)\ntest_predictions = dtree.predict(x_test)\nprint(\"Train Accuracy:\",metrics.accuracy_score(y_train, train_predictions))\nprint(\"Test Accuracy:\",metrics.accuracy_score(y_test, test_predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.metrics import plot_confusion_matrix\nf, ax = plt.subplots(figsize=(20, 20))\nplot_confusion_matrix(dtree, x_test, y_test, cmap=plt.cm.Blues, normalize='true', ax=ax)\nplt.title('Confusion Matrix with Normalization')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we can see that our training accuracy has gone down, but we no longer see over-prediction of any particular category bins. Thus, we have successfuly dealt with the negative effect of class imbalance on model performance. Next, I will explore the second method of dealing with the class imbalance and comapre the results between the two in order to proceed with the optimal method. The below results are on an iteration of the dataset with fewer labels than used above. While the thought process behind this is explained later, the main result of dealing with class balance is emphasized here.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#perform decision tree classification no undersampling with non-pca data\n#from sklearn.tree import DecisionTreeClassifier\ndtree=DecisionTreeClassifier(max_depth= 10, max_leaf_nodes=300)\ndtree.fit(X_train,y_train)\n\n#from sklearn import metrics\ntrain_predictions = dtree.predict(X_train)\ntest_predictions = dtree.predict(X_test)\nprint(\"Train Accuracy:\",metrics.accuracy_score(y_train, train_predictions))\nprint(\"Test Accuracy:\",metrics.accuracy_score(y_test, test_predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is where I implement the weights method of dealing with clas imbalance. The balanced mode for the class weight parameter of the decision tree uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#perform decision tree classification with balanced class weight parameter\n#from sklearn.tree import DecisionTreeClassifier\ndtree=DecisionTreeClassifier(max_depth= 15, max_leaf_nodes=300, class_weight='balanced')\ndtree.fit(x_train,y_train)\n\n#from sklearn import metrics\ntrain_predictions = dtree.predict(x_train)\ntest_predictions = dtree.predict(x_test)\nprint(\"Train Accuracy:\",metrics.accuracy_score(y_train, train_predictions))\nprint(\"Test Accuracy:\",metrics.accuracy_score(y_test, test_predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#confusion matrix for unbalanced classes with class_weights balanced\n#from sklearn.metrics import plot_confusion_matrix\nf, ax = plt.subplots(figsize=(15, 15))\nplot_confusion_matrix(dtree, x_test, y_test, cmap=plt.cm.Blues, normalize='true', ax=ax)\nplt.title('Confusion Matrix for Balanced Class Weights no Undersampling with Normalization')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that we get similar results with class weights as with when we perform class balancing - interesting and useful to know","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Class Balancing Pt. 2**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#bins = [0,3,6,9,13,20,50,120]\n#labels = [3,6,9,13,20,50,120]\n\nbins = [0,5,10,20,30,50,120]\nlabels = [5,10,20,30,50,120]\nencoded_df['stay_bin']=pd.cut(x = encoded_df['Length of Stay'], #encoded df is the raw dataframe following one-hot encoding\n                        bins = bins)\nencoded_df['stay_label']=pd.cut(x = encoded_df['Length of Stay'],\n                        bins = bins,\n                        labels = labels) #lets also rename our bins to be more descriptive since now they are much larger\nencoded_df['stay_bin'] = encoded_df['stay_bin'].apply(lambda x: str(x).replace(',',' -'))\nencoded_df['stay_bin'] = encoded_df['stay_bin'].apply(lambda x: str(x).replace('120','120+')) #make this bin more descriptive\ndisplay(encoded_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(15, 11))\nsns.countplot(x='stay_bin', data = encoded_df, palette='Reds')\nplt.xticks(rotation=90)\nplt.title('Class Distribution')\nplt.xlabel('Length of Stay Bins')\nplt.ylabel('Patient Count (millions)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n#create train and test sets\nnew_X = encoded_df.drop(['Length of Stay','Total Charges','Total Costs','stay_bin','stay_label'], axis=1)\nnew_y = encoded_df[['stay_label']]\nX_train, X_test, y_train, y_test = train_test_split(new_X, new_y, test_size=0.3)\n\n#perform pca\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nx_train = StandardScaler().fit_transform(X_train)\nx_test = StandardScaler().fit_transform(X_test)\n\npca = PCA(n_components=29) #29 components, as found above\nx_train = pca.fit_transform(x_train)\nx_test = pca.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#logisitic regression without class balance\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nlog_reg = LogisticRegression(multi_class='ovr').fit(x_train, y_train)\ny_train_pred = log_reg.predict(x_train)\ny_pred = log_reg.predict(x_test)\n\ntest_acc = accuracy_score(y_test, y_pred)\ntrain_acc = accuracy_score(y_train, y_train_pred)\n\nprint('Test accuracy:', test_acc)\nprint('Train accuracy:', train_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create confusion matrix for no class balancing\nfrom sklearn.metrics import plot_confusion_matrix\nf, ax = plt.subplots(figsize=(11, 9))\nplot_confusion_matrix(log_reg, x_test, y_test, cmap=plt.cm.Blues, normalize='true', ax=ax)\nplt.title('Confusion Matrix Without Class Balancing')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#logistic regression - works better with standardized but no PCA performed on data\n#from sklearn.linear_model import LogisticRegression\n#from sklearn.metrics import accuracy_score\n#import numpy as np\nlog_reg = LogisticRegression(class_weight='balanced', multi_class='ovr').fit(x_train, y_train)\ny_train_pred = log_reg.predict(x_train)\ny_pred = log_reg.predict(x_test)\n\ntest_acc = accuracy_score(y_test, y_pred)\ntrain_acc = accuracy_score(y_train, y_train_pred)\n\nprint('Test accuracy:', test_acc)\nprint('Train accuracy:', train_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Another example of importance of class balancing\n#Create confusion matrix\nfrom sklearn.metrics import plot_confusion_matrix\nf, ax = plt.subplots(figsize=(11, 9))\nplot_confusion_matrix(log_reg, x_test, y_test, cmap=plt.cm.Blues, normalize='true', ax=ax)\nplt.title('Confusion Matrix With Class Balancing')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the balanced class weight parameter has again prevented over-prediction in specific label values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#overfit tree\nfrom sklearn.tree import DecisionTreeClassifier\ndtree=DecisionTreeClassifier(class_weight='balanced')\ndtree.fit(X_train,y_train)\n\nfrom sklearn import metrics\ntrain_predictions = dtree.predict(X_train)\ntest_predictions = dtree.predict(X_test)\nprint(\"Train Accuracy:\",metrics.accuracy_score(y_train, train_predictions))\nprint(\"Test Accuracy:\",metrics.accuracy_score(y_test, test_predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's optimize parameters to prevent overfitting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import validation_curve\nparam_range = [15,20,30,35,100]\ntrain_scores, test_scores = validation_curve(dtree, x_train, y_train, param_name='max_depth', \n                                             param_range=param_range, cv=3, scoring=\"accuracy\")\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\n\nplt.subplots(figsize=(11,9))\nplt.title(\"Validation Curve with Decision Tree Classification\")\nplt.xlabel('Max Depth')\nplt.ylabel(\"Score\")\nplt.ylim(0.0, 1.1)\nlw = 2\nplt.plot(param_range, train_scores_mean, label=\"Training score\",\n             color=\"darkorange\", lw=lw)\nplt.fill_between(param_range, train_scores_mean - train_scores_std,\n                 train_scores_mean + train_scores_std, alpha=0.2,\n                 color=\"darkorange\", lw=lw)\nplt.plot(param_range, test_scores_mean, label=\"Cross-validation score\",\n             color=\"navy\", lw=lw)\nplt.fill_between(param_range, test_scores_mean - test_scores_std,\n                 test_scores_mean + test_scores_std, alpha=0.2,\n                 color=\"navy\", lw=lw)\nplt.legend(loc=\"best\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import validation_curve\ndtree=DecisionTreeClassifier(class_weight='balanced')\ndtree.fit(X_train,y_train)\nparam_range = [15,20,30,35,100]\ntrain_scores, test_scores = validation_curve(dtree, X_train, y_train, param_name='max_depth', \n                                             param_range=param_range, cv=3, scoring=\"accuracy\")\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\n\nplt.subplots(figsize=(11,9))\nplt.title(\"Validation Curve with Decision Tree Classification\")\nplt.xlabel('Max Depth')\nplt.ylabel(\"Score\")\nplt.ylim(0.0, 1.1)\nlw = 2\nplt.plot(param_range, train_scores_mean, label=\"Training score\",\n             color=\"darkorange\", lw=lw)\nplt.fill_between(param_range, train_scores_mean - train_scores_std,\n                 train_scores_mean + train_scores_std, alpha=0.2,\n                 color=\"darkorange\", lw=lw)\nplt.plot(param_range, test_scores_mean, label=\"Cross-validation score\",\n             color=\"navy\", lw=lw)\nplt.fill_between(param_range, test_scores_mean - test_scores_std,\n                 test_scores_mean + test_scores_std, alpha=0.2,\n                 color=\"navy\", lw=lw)\nplt.legend(loc=\"best\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we can see the effect of the max depth parameter on decision tree performance. As max depth is increased, the decision tree becomes more prone to overfitting, resulting in a somewhat linear increase in training accuracy, but a constant cross-validation score throughout. Next, let's perform a grid search on this max depth parameter along with max leaf nodes in order to find the optimal combination of parameters for the model\n\nNow I will test a random forest model on the data. With this large of a dataset, the random forest classifier is slow to run, and thus a hyperparameter optimization search is not the most feasible. Instead, let's try a randomized grid search on the decision tree classifier following the random forest model test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#try a randomized search on decision tree with 3-fold cross validation\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\ndtree = DecisionTreeClassifier(class_weight='balanced')\nsearch_vals = dict(max_depth=[35,50,75,100], max_leaf_nodes=[800,1000,1500,2000])\ndtree_search = RandomizedSearchCV(dtree, search_vals, cv=3)\nsearch = dtree_search.fit(X_train,y_train)\nsearch.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's discover the improvement in model accuracy we can achieve when implementing these optimal parameters. The optimal parameters found were all on the largest end of the search parameter distributions, which suggests that larger values for max depth and max leaf nodes are most beneificial. For further improvement, a randomized grid search could be conducted over a wider distribution of parameter values. However, due to the large size of this dataset, computational efficiency is a limiting factor.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#decision tree optimal parameters\nfrom sklearn.tree import DecisionTreeClassifier\ndtree=DecisionTreeClassifier(max_depth= 50, max_leaf_nodes=1000, class_weight='balanced')\ndtree.fit(X_train,y_train)\n\nfrom sklearn import metrics\ntrain_predictions = dtree.predict(X_train)\ntest_predictions = dtree.predict(X_test)\nprint(\"Train Accuracy:\",metrics.accuracy_score(y_train, train_predictions))\nprint(\"Test Accuracy:\",metrics.accuracy_score(y_test, test_predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, test_predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#randomized search on random forest with 3-fold CV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\nrf = RandomForestClassifier(class_weight='balanced')\nsearch_vals = dict(max_depth=[15,25,50], max_leaf_nodes=[600,800,1400], n_estimators=[100,300,500])\ndtree_search = RandomizedSearchCV(rf, search_vals, cv=3)\nsearch = dtree_search.fit(X_train,y_train)\nsearch.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=150, max_depth=15, class_weight='balanced')\nrf.fit(X_train,y_train)\n\ntrain_predictions = rf.predict(X_train)\ntest_predictions = rf.predict(X_test)\nprint(\"Train Accuracy:\",metrics.accuracy_score(y_train, train_predictions))\nprint(\"Test Accuracy:\",metrics.accuracy_score(y_test, test_predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, test_predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nModel accuracy ultimately did not increase signficantly with the hyperparameter search. Now let's look at the importance of the features in this dataset. These are calculated through the use of a metric called mean decrease impurity. This metric is defined as the total decrease in node impurity (weighted by the probability of reaching that node (which is approximated by the proportion of samples reaching that node)) averaged over all trees of the ensemble.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_importances = rf.feature_importances_\nfeat_names = X_train.columns.tolist()\n\nplt.subplots(figsize=(18,11))\nplt.xticks(rotation=90)\nplt.bar(x=feat_names, height=feat_importances)\nplt.title('Importance of Input Features on Length of Stay Predictor in Random Forest Model')\nplt.ylabel('Feature Importance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This provides yet another metric for feature importance, which allows for comparison bewteen the results found in the PCA section.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Adaboost classifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\ndtree = DecisionTreeClassifier(random_state = 1, class_weight = \"balanced\", max_depth = 15)\nboost = AdaBoostClassifier(dtree, n_estimators=75, random_state=0)\nboost.fit(X_train, y_train)\n\ntrain_predictions = boost.predict(X_train)\ntest_predictions = boost.predict(X_test)\nprint(\"Train Accuracy:\", accuracy_score(y_train, train_predictions))\nprint(\"Test Accuracy:\", accuracy_score(y_test, test_predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, test_predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Additional Data Insights**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hos_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we found out how many Type of Admission\nprint(\"Type of Admission in Dataset:\\n\")\nprint(hos_df['Type of Admission'].unique())\n#we found out how many Age group\nprint(\"\\n\\nAge Group in Dataset:\\n\")\nprint(hos_df['Age Group'].unique())\n#we found out how many ARP Risk of Mortality\nprint(\"\\n\\nARP Risk of Mortality:\\n\")\nprint(hos_df['APR Risk of Mortality'].unique())\n#we found out how many hospital country in our data\nprint(\"\\n\\nHospital Country in Dataset:\\n\")\nprint(\"There are {} different values\\n\".format(len(hos_df['Hospital County'].unique())))\nprint(hos_df['Hospital County'].unique())\n#we found out how many ARP MDC Description\nprint(\"\\n\\nARP MDC Description(disease diagnosis) in Dataset:\\n\")\nprint(\"There are {} different values\\n\".format(len(hos_df['APR MDC Description'].unique())))\nprint(hos_df['APR MDC Description'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We group features by data numbers\n#show it if missing value(dropna=False)\nhos_df['Type of Admission'].value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#number of patients by age groups\n#show it if missing value(dropna=False)\nhos_df['Age Group'].value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#show it if missing value(dropna=False)\nprint(\"Patients with or without abortion:\\n\")\nprint(hos_df['Abortion Edit Indicator'].value_counts(dropna=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#filtering\nhos_df_newborn=hos_df['Type of Admission']=='Newborn'\nprint(\"Total Newborns:\",hos_df_newborn.count())\nhos_df[hos_df_newborn].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#grouping of mortality risk values\n#show it if missing value(dropna=False)\nhos_df['APR Severity of Illness Description'].value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Tidy Data(Melting)**\n\n    We have transformed into a different structure with the melt () method to find out the features of the first five elements in our dataset ['Age_Group', 'Length_of_Stay', 'Type_of_Admission'].\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hos_df_new = hos_df.head()\nhos_df_melted = pd.melt(frame = hos_df_new, id_vars = 'APR MDC Description', value_vars = ['Age Group','Type of Admission'])\nhos_df_melted","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Concatenating Data\n\n    age group of the diagnosis and the patient\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#firstly lets create 2 data frame\nhos_df_data1=hos_df['APR MDC Description'].tail()\nhos_df_data2=hos_df['Age Group'].tail()\n\nconc_hos_df_col=pd.concat([hos_df_data1,hos_df_data2],axis=1)\nconc_hos_df_col","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Building Data Frames From Scratch","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#data frames from dictionary\nHospital=list(hos_df['Hospital County'].head())\nFacility=list(hos_df['Facility Name'].head())\nYear=list(hos_df['Discharge Year'].head())\nCosts=list(hos_df['Total Costs'].head())\n\nlist_label=[\"hospital_country\",\"facility_name\",\"discharge_year\",\"total_costs\"]\nlist_col=[Hospital,Facility,Year,Costs]\nzipped=list(zip(list_label,list_col))\nhos_df_dict=dict(zipped)\n\nhos_df_diff=pd.DataFrame(hos_df_dict)\nhos_df_diff\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Visual Exploratory Data Analysis**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nhos_df_data1=hos_df.loc[:,[\"Total Costs\",\"Total Charges\",\"Birth Weight\",\"Length of Stay\"]]\nhos_df_data1.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hos_df_data1.plot(subplots=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hos_df_data1.plot(kind=\"hist\",y=\"Total Costs\",bins=50,range=(0,250))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#with non cumulative an cumulative\nfig,axes=plt.subplots(nrows=2,ncols=1)\n\nhos_df_data1.plot(kind=\"hist\",y=\"Total Costs\",bins=50,range=(0,250),ax=axes[0])\nhos_df_data1.plot(kind=\"hist\",y=\"Total Costs\",bins=50,range=(0,250),ax=axes[1],cumulative=True)\n\nplt.savefig(\"Graph.png\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(hos_df['Discharge Year'])\nhos_df['Discharge Year'] =pd.to_datetime(hos_df['Discharge Year'])\n#lets make discharge_year as index\nhos_df_dis=hos_df.set_index(\"Discharge Year\")\nhos_df_dis","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(hos_df.loc[85,['APR DRG Description']])\n#selecting only some columns\nhos_df[[\"APR DRG Description\",\"Age Group\",\"Length of Stay\"]].head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(hos_df.loc[1:10,\"Race\":\"Length of Stay\"])\n\nhos_df.loc[1:10,\"Gender\":]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total hospitalization times for patients admitted to the hospital as Urgent:\",\n      hos_df['Length of Stay'][hos_df['Type of Admission']=='Urgent'].sum())\n\n#The first value of unique races of patients coming to the hospital\nhos_df.groupby(\"Race\").first()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total hospitalization times for patients admitted to the hospital as Emergency:\",\n      hos_df['Length of Stay'][hos_df['Type of Admission']=='Emergency'].sum())\n\n#The first value of unique races of patients coming to the hospital\nhos_df.groupby(\"Race\").first()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}