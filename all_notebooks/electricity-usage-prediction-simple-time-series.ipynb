{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Electricity Usage Prediction Using Time Series \n<font size=\"3\">@Cicily Wu</font>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://assets.greentechmedia.com/assets/content/cache/made/assets/content/cache/remote/https_assets.greentechmedia.com/content/images/articles/Urban_Electric_Grid_721_420_80_s_c1.jpg)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1. Project Statement","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Time series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series forecasting is the use of a model to predict future values based on previously observed values.\n\nThe demand for electricity has been continuously increasing over the years. To understand the future consumption, a good predictive method is entailed, which is time series analysis.\n\nIn this project, I will build two different models to predict the electricity usage:\n1. ARIMA model: ARIMA, short for 'Auto Regressive Integrated Moving Average' is actually a class of models that 'explains' a given time series based on its own past values, that is, its own lags and the lagged forecast errors, so that equation can be used to forecast future values.\n2. LSTM Neural Network model: Long short-term memory is an artificial recurrent neural network architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can not only process single data points, but also entire sequences of data. \n\nThe dataset includes the monthly electricity usage data from 1985-01-01 to 2018-01-01. ","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf \nfrom statsmodels.tsa.seasonal import seasonal_decompose \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/time-series-datasets/Electric_Production.csv',parse_dates=[0])\ndf=df.rename(columns={'IPG2211A2N':'usage','DATE':'date'})\ndf = df.set_index('date')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\ndf.plot(figsize=(8,4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seasonal = seasonal_decompose(df.usage,model='add')\nfig = plt.figure()  \nfig = seasonal.plot()  \nfig.set_size_inches(10, 8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color='purple'>From above plots we can see, the usage data appeares a strong seasonal trend. Therefore, instead of ARIMA, we choose to use SRIMA.</font>\n\nSeasonal Autoregressive Integrated Moving Average, SARIMA or Seasonal ARIMA, is an extension of ARIMA that explicitly supports univariate time series data with a seasonal component.\nConfiguring a SARIMA requires selecting hyperparameters for both the trend and seasonal elements of the series.\n\n**Trend Elements:**\nThere are three trend elements that require configuration.\n\nThey are the same as the ARIMA model; specifically:\n\np: Trend autoregression order.\nd: Trend difference order.\nq: Trend moving average order.\n\n**Seasonal Elements:**\nThere are four seasonal elements that are not part of ARIMA that must be configured; they are:\n\nP: Seasonal autoregressive order.\nD: Seasonal difference order.\nQ: Seasonal moving average order.\nm: The number of time steps for a single seasonal period.\n\nTogether, the notation for an SARIMA model is specified as:\n\n**SARIMA(p,d,q)(P,D,Q)m**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2. Seasonal ARIMA Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font color='purple'>I will use Dickey-Fuller test to see the stationarity of these data.</font>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller   #Dickey-Fuller test\ndef test_stationarity(timeseries):\n\n    #Determing rolling statistics\n    rolmean = timeseries.rolling(window=20).mean()\n    rolstd = timeseries.rolling(window=20).std()\n\n    #Plot rolling statistics:\n    fig = plt.figure(figsize=(12, 6))\n    orig = plt.plot(timeseries, color='blue',label='Original')\n    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show()\n\n    #Perform Dickey-Fuller test:\n    print('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')  #autolag : {‘AIC’, ‘BIC’, ‘t-stat’, None}\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print(dfoutput)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_stationarity(df.usage)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](http://)<font color='purple'> We can see that the data are not stationary. Thus, I will build a “First Order Difference\" column to stabilize the standard deviation.</font>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['first_difference'] = df.usage - df.usage.shift(1)   \ntest_stationarity(df.first_difference.dropna(inplace=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color='purple'> After pocessing, the data are stationary for now. Let's build the model. \"Pmdarima\" is a good package to help us find the best parameters for SARIMA model. This helper function makes whole process easiear since I do not have to tune parameters manually.</font>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pmdarima","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pmdarima as pm\nfrom pmdarima.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=df.drop(columns='first_difference')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = train_test_split(df1, train_size=320)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = train_test_split(df1, train_size=320)\n\n# Fit your model\nmodel = pm.auto_arima(train, seasonal=True, m=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.statespace.sarimax import SARIMAX\npred_model = SARIMAX(train.usage, order=(1,0,2), seasonal_order=(0,1,1,12))\nresults = pred_model.fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred=test.copy()\ntest_pred = results.predict(start = len(train), end = len(df)-1, typ=\"levels\")  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['usage'].plot(figsize = (12,5), label='real usage')\ntest_pred.plot(label = 'predicted usage')\nplt.legend(loc='upper right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tools.eval_measures import rmse\narima_rmse_error = rmse(test['usage'], test_pred)\narima_mse_error = arima_rmse_error**2\nprint(f'MSE Error: {arima_mse_error}\\nRMSE Error: {arima_rmse_error}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](http://)<font color='purple'> From above plot and the statistics we can see, the model reached MSE as 22.843, which is pretty good.</font>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 3. LSTM Neural Network Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](http://)<font color='purple'> First, I want to process the data. A common operation on time-series data is to shift or \"lag\" the values back and forward in time, such as to calculate percentage change from sample to sample. The pandas method for this is .shift(), which will shift the values in the index by a specified number of units of the index's period.</font>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train1=pd.concat([train, train.shift(-1), train.shift(-2),train.shift(-3),train.shift(-4),train.shift(-5),\n                 train.shift(-6),train.shift(-7),train.shift(-8),train.shift(-9),train.shift(-10),train.shift(-11),train.shift(-12)\n                 ], axis=1).dropna()\ntrain1.columns = ['usage', 'usage1', 'usage2','usage3','usage4', 'usage5','usage6'\n                 ,'usage7', 'usage8','usage9','usage10', 'usage11', 'usage12']\ntrain1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test1=pd.concat([test, test.shift(-1), test.shift(-2),test.shift(-3),test.shift(-4),test.shift(-5),\n                 test.shift(-6),test.shift(-7),test.shift(-8),test.shift(-9),test.shift(-10),test.shift(-11),test.shift(-12)\n                 ], axis=1).dropna()\ntest1.columns = ['usage', 'usage1', 'usage2','usage3','usage4', 'usage5','usage6'\n                 ,'usage7', 'usage8','usage9','usage10', 'usage11','usage12']\ntest1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1_y=train1.loc[:, train1.columns == 'usage']\ntrain1_x=train1.loc[:, train1.columns != 'usage']\n\ntest1_y=test1.loc[:, test1.columns == 'usage']\ntest1_x=test1.loc[:, test1.columns != 'usage']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\n\nmodel = Sequential()\n\nmodel.add(LSTM(20, activation='relu',input_shape=(12, 1)))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse',metrics=['mean_squared_error'])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](http://)<font color='purple'>The LSTM model's required input data shape is 3-dimensions. Since our data is 2-dimensions data, we need to modify it.</font>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train1_x = np.expand_dims(train1_x, 2)\ntest1_x = np.expand_dims(test1_x, 2)\nprint(\"New train data shape:\")\nprint(train1_x.shape)\nprint(\"New test data shape:\")\nprint(test1_x.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run=model.fit(train1_x,train1_y,epochs=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(run.epoch,run.history.get('loss'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(test1_x,test1_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](http://)<font color='purple'> After around 6-10 epochs, the model reached a 10.87 MSE while the same number on test dataset is 17.224. We can see the model had a good performance.</font>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test1_pred=model.predict(test1_x)\ntest_pred=pd.DataFrame(test1_pred, columns=['test_pred']) \n#test_true=pd.DataFrame(test1_y, columns=['test_true']) \ntest_pred.index=test1_y.index\ntest_pred=test_pred.merge(test1_y,left_index=True, right_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nplt.plot( test_pred.index, 'usage', data=test_pred, markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2,label='reality')\nplt.plot( test_pred.index, 'test_pred', data=test_pred, color='orange', linewidth=2,label='prediction')\nplt.legend(loc='upper right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](http://)<font color='purple'> The comparison of real usage and predicted usage on testdataset is shown in the above plot.</font>","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}