{"cells":[{"metadata":{"_uuid":"2cfb27e74699e539b237df2f4018986223ed9259"},"cell_type":"markdown","source":"## Import Libs"},{"metadata":{"ExecuteTime":{"end_time":"2018-04-04T06:02:48.295105Z","start_time":"2018-04-04T06:02:47.939168Z"},"trusted":true,"_uuid":"168f7a50c59881d213fc8944aba4daf5a80e5709"},"cell_type":"code","source":"import numpy as np\nimport os, sys, json, scipy, shutil\nimport random, datetime, time\nfrom PIL import Image, ImageDraw\nfrom glob import glob\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Import tensorflow and keras\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import Input\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras.layers import (Dense, Reshape, LeakyReLU, Conv2D, Conv2DTranspose,\n                                     Flatten, Dropout, Concatenate, BatchNormalization,\n                                    UpSampling2D)\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import RMSprop, Adam\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n\n# Define root path\ncwd = os.getcwd()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(tf.__version__)\n# print(keras.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Set paths"},{"metadata":{"trusted":true},"cell_type":"code","source":"IMAGE_PATH = '/kaggle/input/celebaalligned/celeb_id_aligned'\nOUT_DIR = '/kaggle/working'\nCHECKPOINT_DIR = os.path.join(OUT_DIR, 'model_checkpoints')\nSAMPLE_DIR = os.path.join(OUT_DIR, 'sample')\nCROPPED_IMG_PATH = os.path.join(OUT_DIR,'cropped')\n\nif not os.path.exists(CHECKPOINT_DIR):\n    os.makedirs(CHECKPOINT_DIR)\n    \nif not os.path.exists(SAMPLE_DIR):\n    os.makedirs(SAMPLE_DIR)\n\nif not os.path.exists(CROPPED_IMG_PATH):\n    os.makedirs(CROPPED_IMG_PATH)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training params"},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_EPOCHS = 20\nBATCH_SIZE = 1\nMAX_IMGS_TO_TRAIN = 2000","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Utils functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_images_on_window(path, img_filenames):\n    \n    plt.figure(figsize=(12,12))\n    for idx in range(9):\n        img = Image.open(os.path.join(path, img_filenames[idx]))\n        img_array = np.array(img)\n        plt.subplot(3,3,(idx+1))\n        plt.grid(False)\n        plt.axis('off')\n        plt.imshow(img_array)\n        \ndef retrieve_eyes_coords(img_name, data):\n    filename = img_name.rsplit('.')[0]\n    person = filename.rsplit('-',1)[0]\n    \n    try:\n        person_data_list = data[person]\n    except:\n        return None,None,None,None\n\n    person_imgs = []\n    for person_dict in person_data_list:\n        person_imgs.append(person_dict['filename']) \n\n    person_imgs = sorted(person_imgs)\n    \n    try:\n        img_dict = [img_dict for img_dict in person_data_list if img_dict['filename'] == img_name]\n        img_dict = img_dict[0]\n        eye_left, box_left, eye_right, box_right = img_dict['eye_left'], img_dict['box_left'], img_dict['eye_right'], img_dict['box_right']\n        return eye_left,box_left,eye_right,box_right\n    \n    except:\n        return None,None,None,None\n\ndef draw_eye_on_image(img, eye_coords, clr):\n    x0,y0,x1,y1 = eye_coords\n    \n    img = img.convert('RGBA')\n    overlay = Image.new('RGBA', img.size)\n    draw = ImageDraw.Draw(overlay)\n    draw.rectangle(((x0,y0),(x1,y1)), fill=clr)\n    \n    img = Image.alpha_composite(img, overlay)\n    img = img.convert(\"RGB\")\n    \n    return img\n\ndef paint_eyes_on_image(IMAGE_PATH, imgs_list, data, eye_left_clr, eye_right_clr, number_of_images=9):\n\n    random.shuffle(imgs_list)\n\n    for idx in range(number_of_images):\n        img_path = os.path.join(IMAGE_PATH, imgs_list[idx])\n        try:\n            eye_l,box_l,eye_r,box_r = retrieve_eyes_coords(imgs_list[idx], data)\n\n            x0 = eye_l['x'] - box_l['w'] // 2\n            y0 = eye_l['y'] - box_l['w'] // 2\n            x1 = x0 + box_l['w']\n            y1 = y0 + box_l['h']\n            eye_l_coords = (x0,y0,x1,y1)\n\n            img = Image.open(img_path)\n            img = draw_eye_on_image(img, eye_l_coords, eye_left_clr)\n            x0 = eye_r['x'] - box_r['w'] // 2\n            y0 = eye_r['y'] - box_r['w'] // 2\n            x1 = x0 + box_r['w']\n            y1 = y0 + box_r['h']\n            eye_r_coords = (x0,y0,x1,y1)\n\n            img = draw_eye_on_image(img, eye_r_coords, eye_right_clr)\n\n            img_array = np.array(img)\n            plt.subplot(3,3,(idx+1))\n            plt.axis('off')\n            plt.grid(False)\n            plt.imshow(img_array)\n\n        except:\n            print('It was impossible to read the data for person: ', person)\n            \ndef save_painted_eye_images(img_folder, imgs_list, data, dest_folder, eye_left_clr, eye_right_clr):\n    \n    input_imgs = []\n    painted_imgs = []\n    \n    # If dest_folder exists, delete it first\n    if os.path.exists(dest_folder) and os.path.isdir(dest_folder):\n        shutil.rmtree(dest_folder)\n    \n    # After delete it, create folder again\n    os.makedirs(dest_folder)\n        \n    # Shuffle imgs list\n    random.shuffle(imgs_list)\n\n    for filename in tqdm(imgs_list):\n        img_path = os.path.join(img_folder, filename)\n\n        try:\n            img = Image.open(img_path)\n            eye_l,box_l,eye_r,box_r = retrieve_eyes_coords(filename, data)\n            \n            if eye_l == None:\n                continue\n\n            x0 = eye_l['x'] - box_l['w'] // 2\n            y0 = eye_l['y'] - box_l['w'] // 2\n            x1 = x0 + box_l['w']\n            y1 = y0 + box_l['h']\n            \n            if x0 == x1 or y0 == y1:\n                continue\n            else:\n                eye_l_coords = (x0,y0,x1,y1)\n\n            img = draw_eye_on_image(img, eye_l_coords, eye_left_clr)\n            \n            x0 = eye_r['x'] - box_r['w'] // 2\n            y0 = eye_r['y'] - box_r['w'] // 2\n            x1 = x0 + box_r['w']\n            y1 = y0 + box_r['h']\n            \n            if x0 == x1 or y0 == y1:\n                continue\n            else:\n                eye_r_coords = (x0,y0,x1,y1)\n\n            img = draw_eye_on_image(img, eye_r_coords, eye_right_clr)\n            \n            dest_name = filename.rsplit('.')[0] + '_painted.' + filename.rsplit('.')[-1]\n            dest_path = os.path.join(dest_folder, dest_name)\n            img.save(dest_path)\n            \n            input_imgs.append(filename)\n            painted_imgs.append(dest_name)\n            \n        except:\n            print('It was impossible to read the data for person: ', person)\n            \n    return input_imgs, painted_imgs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Examine dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Alternative num_imgs = len(os.listdir(IMAGE_PATH))\nos.chdir(IMAGE_PATH)\nimgs_list = glob('*.jpg')\n\n# Remove duplicates\nimgs_list = list(dict.fromkeys(imgs_list))\nsorted(imgs_list)\n\nos.chdir(cwd)\nprint(len(imgs_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show_images_on_window(IMAGE_PATH, imgs_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read Json "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check json is available\njson_cat = IMAGE_PATH + \"/data.json\"\nwith open(json_cat, 'r') as f:\n    data = json.load(f)\n    \n# # Explore data dict\n# print(data)\n# print(data.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# filename = imgs_list[0].rsplit('.')[0]\n# person = filename.rsplit('-',1)[0]\n# person_data_list = data[person]\n\n# person_imgs = []\n# for person_dict in person_data_list:\n#     person_imgs.append(person_dict['filename']) \n\n# person_imgs = sorted(person_imgs)\n\n# img_dict = [img_dict for img_dict in person_data_list if img_dict['filename'] == imgs_list[0]]\n# img_dict = img_dict[0]\n# # print(type(img_dict))\n# eye_left, box_left, eye_right, box_right = img_dict['eye_left'], img_dict['box_left'], img_dict['eye_right'], img_dict['box_right']\n# print(eye_left)\n# print(box_left)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Show eyes boxes on images"},{"metadata":{"trusted":true},"cell_type":"code","source":"# transparency = 0.0  # Degree of transparency, 0-100%\n# opacity = int(255 * (1.0-transparency))\n# eye_left_clr = (255,255,255,opacity)\n# eye_right_clr = (0,255,0,opacity)\n\n# plt.figure(figsize=(12,12))\n\n# paint_eyes_on_image(IMAGE_PATH, imgs_list, data, eye_left_clr, eye_right_clr)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Obtain painted eyes images"},{"metadata":{"trusted":true},"cell_type":"code","source":"transparency = 0.0  # Degree of transparency, 0-100%\nopacity = int(255 * (1.0-transparency))\neye_left_clr = (255,255,255,opacity)\neye_right_clr = eye_left_clr\n\nrandom.shuffle(imgs_list)\n\n# Only crop MAX_IMGS_TO_TRAIN from the orig dataset\ninput_imgs, painted_imgs = save_painted_eye_images(IMAGE_PATH, imgs_list[:MAX_IMGS_TO_TRAIN], data, CROPPED_IMG_PATH, eye_left_clr, eye_right_clr)\nprint(len(input_imgs), len(painted_imgs))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Examine saved images"},{"metadata":{"trusted":true},"cell_type":"code","source":"# show_images_on_window(CROPPED_IMG_PATH, painted_imgs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# pairs = list(zip(input_imgs, painted_imgs))  # make pairs out of the two lists\n# pairs = random.sample(pairs, 3)  # pick 3 random pairs\n# batch_images_A, batch_images_B = zip(*pairs)  # separate the pairs\n\n# print(batch_images_A)\n# print(batch_images_B)\n\n# for img_A, img_B in pairs[:2]:\n#     print(img_A, ' and ', img_B)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Eyes_dataset():\n    def __init__(self, files_A, files_B, img_res=(128, 128)):\n        self.files_A = sorted(files_A)\n        self.files_B = sorted(files_B)\n        self.img_res = img_res\n\n    def load_data(self, batch_size=1, is_testing=False):\n        data_type = \"train\" if not is_testing else \"test\"\n        \n#         files_A = sorted(glob(os.path.join(self.path_A,'*')))\n#         files_B = sorted(glob(os.path.join(self.path_B,'*')))\n               \n        pairs = list(zip(self.files_A, self.files_B))  # make pairs out of the two lists\n        batch_images = random.sample(pairs, batch_size)  # pick random pairs f size=batch_size\n#         batch_images_A, batch_images_B = zip(*pairs)  # separate the pairs\n\n        imgs_A = []\n        imgs_B = []\n        \n        for img_path_A, img_path_B in batch_images:\n            img_A = self.imread(img_path_A)\n            img_B = self.imread(img_path_B)\n            \n#             h, w, _ = img_A.shape\n#             _w = int(w/2)\n            \n#             img_A, img_B = img_A[:, :_w, :], img_B[:, _w:, :]\n\n            img_A = scipy.misc.imresize(img_A, self.img_res)\n            img_B = scipy.misc.imresize(img_B, self.img_res)\n\n#             # If training => do random flip\n#             if not is_testing and np.random.random() < 0.5:\n#                 img_A = np.fliplr(img_A)\n#                 img_B = np.fliplr(img_B)\n\n            imgs_A.append(img_A)\n            imgs_B.append(img_B)\n\n        imgs_A = np.array(imgs_A)/127.5 - 1.\n        imgs_B = np.array(imgs_B)/127.5 - 1.\n\n        return imgs_A, imgs_B\n\n    def load_batch(self, batch_size=1, is_testing=False):\n        data_type = \"train\" if not is_testing else \"val\"\n        #         path = glob('./datasets/%s/%s/*' % (self.dataset_name, data_type))\n        \n#         files_A = sorted(glob(os.path.join(self.path_A,'*')))\n#         files_B = sorted(glob(os.path.join(self.path_B,'*')))\n        \n        pairs = list(zip(self.files_A, self.files_B))  # make pairs out of the two lists\n        self.n_batches = int(len(pairs) / batch_size)\n\n        # for i in range(self.n_batches-1):\n        for i in range(self.n_batches):\n            batch = pairs[i*batch_size:(i+1)*batch_size]\n            imgs_A, imgs_B = [], []\n            for img_A_path, img_B_path in batch:\n                img_A = self.imread(img_A_path)\n                img_B = self.imread(img_B_path)\n                \n#                 h, w, _ = img_A.shape\n#                 half_w = int(w/2)\n                \n#                 img_A = img_A[:, :half_w, :]\n#                 img_B = img_B[:, half_w:, :]\n\n                img_A = scipy.misc.imresize(img_A, self.img_res)\n                img_B = scipy.misc.imresize(img_B, self.img_res)\n\n#                 if not is_testing and np.random.random() > 0.5:\n#                         img_A = np.fliplr(img_A)\n#                         img_B = np.fliplr(img_B)\n\n                imgs_A.append(img_A)\n                imgs_B.append(img_B)\n\n            imgs_A = np.array(imgs_A)/127.5 - 1.\n            imgs_B = np.array(imgs_B)/127.5 - 1.\n\n            yield imgs_A, imgs_B\n\n    def imread(self, path):\n        return scipy.misc.imread(path, mode='RGB').astype(np.float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"paired = list(zip(input_imgs, painted_imgs))\nrandom.shuffle(paired)\n# paired = paired[:MAX_IMGS_TO_TRAIN]\ninput_imgs = [val[0] for val in paired]\npainted_imgs = [val[1] for val in paired]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_imgs_path = sorted([os.path.join(IMAGE_PATH, img) for img in input_imgs])\npainted_imgs_path = sorted([os.path.join(CROPPED_IMG_PATH, img) for img in painted_imgs])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data_loader = Eyes_dataset(input_imgs_path, painted_imgs_path, img_res=(256, 256))\n# imgs_A, imgs_B = data_loader.load_data(batch_size=3)\n\n# fig = plt.figure()\n# for idx, pair in enumerate(zip(imgs_A, imgs_B)):\n#     img_A, img_B = pair\n#     paired_img = np.concatenate([img_A, img_B])\n\n#     # Rescale images 0 - 1\n#     paired_img = 0.5 * paired_img + 0.5\n\n#     plt.imshow(paired_img)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Pix2Pix():\n    def __init__(self, files_A, files_B):\n        \n        # Input shape\n        self.img_rows = 256\n        self.img_cols = 256\n        self.channels = 3\n        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n\n        # Configure data loader\n        self.data_loader = Eyes_dataset(files_A, files_B,\n                                      img_res=(self.img_rows, self.img_cols))\n\n        # Calculate output shape of D (PatchGAN)\n        patch = int(self.img_rows / 2**4)\n        self.disc_patch = (patch, patch, 1)\n\n        # Number of filters in the first layer of G and D\n        self.gf = 64\n        self.df = 64\n        \n        # Define optimizer\n        optimizer = Adam(0.0002, 0.5)\n\n        # Build and compile the discriminator\n        self.discriminator = self.build_discriminator()\n        self.discriminator.compile(loss='mse',\n            optimizer=optimizer,\n            metrics=['accuracy'])\n\n        #-------------------------\n        # Construct Computational\n        #   Graph of Generator\n        #-------------------------\n\n        # Build the generator\n        self.generator = self.build_generator()\n\n        # Input images and their conditioning images\n        img_A = Input(shape=self.img_shape) # Real\n        img_B = Input(shape=self.img_shape) # Conditioning image to generator\n\n        # By conditioning on B generate a fake version of A\n        fake_A = self.generator(img_B)\n\n        # For the combined model we will only train the generator\n        self.discriminator.trainable = False\n\n        # Discriminators determines validity of translated images / condition pairs\n        valid = self.discriminator([fake_A, img_B])\n\n        self.combined = Model(inputs=[img_A, img_B], outputs=[valid, fake_A])\n        self.combined.compile(loss=['mse', 'mae'],\n                              loss_weights=[1, 100],\n                              optimizer=optimizer)\n\n    def build_generator(self):\n        \"\"\"U-Net Generator\"\"\"\n\n        def conv2d(layer_input, filters, f_size=4, bn=True):\n            \"\"\"Layers used during downsampling\"\"\"\n            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n            d = LeakyReLU(alpha=0.2)(d)\n            if bn:\n                d = BatchNormalization(momentum=0.8)(d)\n            return d\n\n        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n            \"\"\"Layers used during upsampling\"\"\"\n            u = UpSampling2D(size=2)(layer_input)\n            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n            if dropout_rate:\n                u = Dropout(dropout_rate)(u)\n            u = BatchNormalization(momentum=0.8)(u)\n            u = Concatenate()([u, skip_input])\n            return u\n\n        # Image input\n        d0 = Input(shape=self.img_shape)\n\n        # Downsampling\n        d1 = conv2d(d0, self.gf, bn=False)\n        d2 = conv2d(d1, self.gf*2)\n        d3 = conv2d(d2, self.gf*4)\n        d4 = conv2d(d3, self.gf*8)\n        d5 = conv2d(d4, self.gf*8)\n        d6 = conv2d(d5, self.gf*8)\n        d7 = conv2d(d6, self.gf*8)\n\n        # Upsampling\n        u1 = deconv2d(d7, d6, self.gf*8)\n        u2 = deconv2d(u1, d5, self.gf*8)\n        u3 = deconv2d(u2, d4, self.gf*8)\n        u4 = deconv2d(u3, d3, self.gf*4)\n        u5 = deconv2d(u4, d2, self.gf*2)\n        u6 = deconv2d(u5, d1, self.gf)\n\n        u7 = UpSampling2D(size=2)(u6)\n        output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u7)\n\n        return Model(d0, output_img)\n\n    def build_discriminator(self):\n\n        def d_layer(layer_input, filters, f_size=4, bn=True):\n            \"\"\"Discriminator layer\"\"\"\n            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n            d = LeakyReLU(alpha=0.2)(d)\n            if bn:\n                d = BatchNormalization(momentum=0.8)(d)\n            return d\n\n        img_A = Input(shape=self.img_shape)\n        img_B = Input(shape=self.img_shape)\n\n        # Concatenate image and conditioning image by channels to produce input\n        combined_imgs = Concatenate(axis=-1)([img_A, img_B])\n\n        d1 = d_layer(combined_imgs, self.df, bn=False)\n        d2 = d_layer(d1, self.df*2)\n        d3 = d_layer(d2, self.df*4)\n        d4 = d_layer(d3, self.df*8)\n\n        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n\n        return Model([img_A, img_B], validity)\n\n    def train(self, epochs, batch_size=1, sample_interval=50):\n\n        start_time = datetime.datetime.now()\n\n        # Adversarial loss ground truths\n        valid = np.ones((batch_size,) + self.disc_patch)\n        fake = np.zeros((batch_size,) + self.disc_patch)\n        \n        self.d_losses = []\n        self.g_losses = []\n\n        for epoch in range(1,epochs+1):\n            epoch_d_losses = []\n            epoch_g_losses = []\n            \n            for batch_i, (imgs_A, imgs_B) in enumerate(self.data_loader.load_batch(batch_size), start=1):\n#                 print('Batch number: ',batch_i)\n\n                # ---------------------\n                #  Train Discriminator\n                # ---------------------\n\n                # Condition on B and generate a translated version\n                fake_A = self.generator.predict(imgs_B)\n\n                # Train the discriminators (original images = real / generated = Fake)\n                d_loss_real = self.discriminator.train_on_batch([imgs_A, imgs_B], valid)\n                d_loss_fake = self.discriminator.train_on_batch([fake_A, imgs_B], fake)\n                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n                # -----------------\n                #  Train Generator\n                # -----------------\n\n                # Train the generators\n                g_loss = self.combined.train_on_batch([imgs_A, imgs_B], [valid, imgs_A])\n                \n#                 print('Loss per batch')\n#                 print(d_loss[0])\n#                 print(g_loss[0])\n#                 epoch_d_losses.append(d_loss[0])\n#                 epoch_g_losses.append(g_loss[0])\n                \n#                 elapsed_time = datetime.datetime.now() - start_time\n                \n#                 # Plot the progress\n#                 print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %f] time: %s\" % (epoch, epochs,\n#                                                                         batch_i, self.data_loader.n_batches,\n#                                                                         d_loss[0], 100*d_loss[1],\n#                                                                         g_loss[0],\n#                                                                         elapsed_time))\n\n                # If at save interval => save generated image samples\n                if batch_i % sample_interval == 0:\n                    elapsed_time = datetime.datetime.now() - start_time\n                    print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %f] time: %s\" % \n                           (epoch, epochs,batch_i, self.data_loader.n_batches, \n                            d_loss[0], 100*d_loss[1], g_loss[0], elapsed_time))\n                    self.sample_images(epoch, batch_i, OUT_DIR)\n            \n#             # Plot the progress\n#             elapsed_time = datetime.datetime.now() - start_time\n#             print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %f] time: %s\" % \n#                    (epoch, epochs,batch_i, self.data_loader.n_batches, \n#                     d_loss[0], 100*d_loss[1], g_loss[0], elapsed_time))\n\n#             self.sample_images(epoch, batch_i, OUT_DIR)\n            \n#             # Store losses during at the end of the epoch\n#             print('Loss per epoch')\n#             print(epoch_d_losses)\n#             print(epoch_g_losses)\n#             self.d_losses.append(np.average(epoch_d_losses, axis=0))\n#             self.g_losses.append(np.average(epoch_g_losses, axis=0))\n            self.d_losses.append(epoch_d_losses)\n            self.g_losses.append(epoch_g_losses)\n        \n        # ---------------------\n        #  End Training\n        # ---------------------\n        \n        # save generator and discriminator models\n        gen_filename = 'gen_model_' + str(epoch) + '.h5' \n        self.save_model(self.generator, gen_filename, OUT_DIR)\n\n        combined_filename = 'comb_model_' + str(epoch) + '.h5'  \n        self.save_model(self.combined, combined_filename, OUT_DIR)\n        \n        # Plot losses\n        self.plot_losses(self.g_losses, self.d_losses, OUT_DIR)\n        \n#     def test(self):\n#         image_paths = glob(self.data_loader.testing_raw_path+\"*\")\n#         for image_path in image_paths:\n#             image = np.array(imageio.imread(image_path))\n#             image_normalized = Helpers.normalize(image)\n#             generated_batch = self.generator.predict(np.array([image_normalized]))\n#             concat = Helpers.unnormalize(np.concatenate([image_normalized, generated_batch[0]], axis=1))\n#             cv2.imwrite(BASE_OUTPUT_PATH+os.path.basename(image_path), cv2.cvtColor(np.float32(concat), cv2.COLOR_RGB2BGR))\n        \n    def save_model(self, model, filename, folder):\n        weights_filename = filename.rsplit('.')[0] + '_weights.h5'\n        \n        filepath = os.path.join(folder, filename)\n        weights_path = os.path.join(folder, weights_filename)\n        try:\n            # If file exists, delete it first\n            if os.path.exists(filepath):\n                os.remove(filepath)\n            if os.path.exists(weights_path):\n                os.remove(weights_path)\n\n            model.save(filepath)\n            model.save_weights(weights_path)\n\n        except:\n            print('Model ',filename,' could not be saved')\n            \n    def plot_losses(self, gen_loss, disc_loss, folder):\n        \n        plt.figure(figsize=(10, 10))\n        plt.plot(disc_loss, label='Discriminitive loss')\n        plt.plot(gen_loss, label='Generative loss')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.legend()\n        \n        timestamp = time.strftime(\"%d_%m_%y-%H_%M\", time.localtime())\n        filename = 'pix2pix_loss_' + timestamp + '.png'\n        plt.show()\n        plt.savefig(os.path.join(folder, filename))\n    \n    def sample_images(self, epoch, batch_i, out_dir):\n        r, c = 3, 3\n\n        imgs_A, imgs_B = self.data_loader.load_data(batch_size=3, is_testing=True)\n        fake_A = self.generator.predict(imgs_B)\n\n        gen_imgs = np.concatenate([imgs_B, fake_A, imgs_A])\n\n        # Rescale images 0 - 1\n        gen_imgs = 0.5 * gen_imgs + 0.5\n\n        titles = ['Condition', 'Generated', 'Original']\n        fig, axs = plt.subplots(r, c)\n        cnt = 0\n        for i in range(r):\n            for j in range(c):\n                axs[i,j].imshow(gen_imgs[cnt])\n                axs[i, j].set_title(titles[i])\n                axs[i,j].axis('off')\n                cnt += 1\n        \n        filename = 'epoch_' + str(epoch) + '_batch_' + str(batch_i) + '.png'\n        fig.savefig(os.path.join(out_dir, filename))\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"gan = Pix2Pix(input_imgs_path, painted_imgs_path)\ngan.train(epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, sample_interval=1)\n\n# Save outputs as a zip\n# shutil.make_archive(\"output\", \"zip\", \"./output\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}