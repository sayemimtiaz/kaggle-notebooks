{"cells":[{"metadata":{},"cell_type":"markdown","source":"# What Is a Perceptron?\n\nA perceptron is a simple binary classification algorithm, proposed by Cornell scientist [Frank Rosenblatt](https://en.wikipedia.org/wiki/Frank_Rosenblatt). It helps to divide a set of input signals into two parts—“yes” and “no”. But unlike many other classification algorithms, the perceptron was modeled after the essential unit of the human brain—the neuron and has an uncanny ability to learn and solve complex problems.\n\n\n![https://missinglink.ai/wp-content/uploads/2018/11/Frame-3.png](https://missinglink.ai/wp-content/uploads/2018/11/Frame-3.png)\n\n\nA perceptron is a very simple learning machine. It can take in a few inputs, each of which has a weight to signify how important it is, and generate an output decision of “0” or “1”. However, when combined with many other perceptrons, it forms an artificial neural network. A neural network can, theoretically, answer any question, given enough training data and computing power."},{"metadata":{},"cell_type":"markdown","source":"# What Is a Multilayer Perceptron?\n\nA multilayer perceptron (MLP) is a perceptron that teams up with additional perceptrons, stacked in several layers, to solve complex problems. The diagram below shows an MLP with three layers. Each perceptron in the first layer on the left (the input layer), sends outputs to all the perceptrons in the second layer (the hidden layer), and all perceptrons in the second layer send outputs to the final layer on the right (the output layer).\n\n\n![https://missinglink.ai/wp-content/uploads/2018/11/multilayer-perceptron.png](https://missinglink.ai/wp-content/uploads/2018/11/multilayer-perceptron.png)\n\n\nsends multiple signals, one signal going to each perceptron in the next layer. For each signal, the perceptron uses different weights. In the diagram above, every line going from a perceptron in one layer to the next layer represents a different output. Each layer can have a large number of perceptrons, and there can be multiple layers, so the multilayer perceptron can quickly become a very complex system. The multilayer perceptron has another, more common name—a neural network. A three-layer MLP, like the diagram above, is called a Non-Deep or Shallow Neural Network. An MLP with four or more layers is called a Deep Neural Network. One difference between an MLP and a neural network is that in the classic perceptron, the decision function is a step function and the output is binary. In neural networks that evolved from MLPs, other activation functions can be used which result in outputs of real values, usually between 0 and 1 or between -1 and 1. This allows for probability-based predictions or classification of items into multiple labels."},{"metadata":{},"cell_type":"markdown","source":"# Structure of a Perceptron\n\nThe perceptron, or neuron in a neural network, has a simple but ingenious structure. It consists of four parts, illustrated below.\n\n![https://missinglink.ai/wp-content/uploads/2018/11/Frame-5.png](https://missinglink.ai/wp-content/uploads/2018/11/Frame-5.png)\n"},{"metadata":{},"cell_type":"markdown","source":"# The Perceptron Learning Process\n\nA perceptron follows these steps:\n\n1. Takes the inputs, multiplies them by their weights, and computes their sum Why It’s Important The weights allow the perceptron to evaluate the relative importance of each of the outputs. Neural network algorithms learn by discovering better and better weights that result in a more accurate prediction. There are several algorithms used to fine tune the weights, the most common is called backpropagation.\n\n2. Adds a bias factor, the number 1 multiplied by a weight Why It’s Important This is a technical step that makes it possible to move the activation function curve up and down, or left and right on the number graph. It makes it possible to fine-tune the numeric output of the perceptron. For more details see our guide on neural network bias.\n\n3. Feeds the sum through the activation function Why It’s Important The activation function maps the input values to the required output values. For example, input values could be between 1 and 100, and outputs can be 0 or 1. The activation function also helps the perceptron to learn, when it is part of a multilayer perceptron (MLP). Certain properties of the activation function, especially its non-linear nature, make it possible to train complex neural networks. For more details see our guide on activation functions.\n\n4. The result is the perceptron output The perceptron output is a classification decision. In a multilayer perceptron, the output of one layer’s perceptrons is the input of the next layer. The output of the final perceptrons, in the “output layer”, is the final prediction of the perceptron learning model."},{"metadata":{},"cell_type":"markdown","source":"#  A practical example of MLP"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pylab as pl\nimport pandas as pd\nimport matplotlib.pyplot as plt \n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.utils import shuffle\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/malware-analysis-datasets-api-call-sequences/dynamic_api_call_sequence_per_malware_100_0_306.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Eliminate irrelevant variables in analysis such as hash"},{"metadata":{"trusted":true},"cell_type":"code","source":"data1 = data.drop(columns=['hash'],\n\n                 axis=1)\ndata1 = data1.dropna(how='any')\nprint(data1.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"VISUALIZING THE DATA"},{"metadata":{"trusted":true},"cell_type":"code","source":"cnt_pro = data1['malware'].value_counts()\nplt.figure(figsize=(12,4))\nsns.barplot(cnt_pro.index, cnt_pro.values, alpha=0.8)\nplt.ylabel('Number of data', fontsize=12)\nplt.xlabel('malware', fontsize=12)\nplt.xticks(rotation=90)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SPLITING DATA\nData for training and testing To select a set of training data that will be input in the Machine Learning algorithm, to ensure that the classification algorithm training can be generalized well to new data. For this study using a sample size of 20%."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nY = data1['malware']\nX = data1.drop(columns=['malware'])\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('X train shape: ', X_train.shape)\nprint('Y train shape: ', Y_train.shape)\nprint('X test shape: ', X_test.shape)\nprint('Y test shape: ', Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Build Model use MLP\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\n\n# We define the model\nmlp = MLPClassifier(hidden_layer_sizes=(100,100,100),max_iter=1000, random_state=42)\n\n# We train model\nmlp.fit(X_train, Y_train)\n\n\n# We predict target values\nprediction = mlp.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_acc_mlp = round(mlp.fit(X_train,Y_train).score(X_test, Y_test)* 100, 2)\ntrain_acc_mlp = round(mlp.fit(X_train, Y_train).score(X_train, Y_train)* 100, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Measurement\nWhen the classification process was already done. This work evaluated the results using the Confusion Matrix."},{"metadata":{},"cell_type":"markdown","source":"# Confusion matrix\nConfusion Matrix is commonly used for a summarization of prediction results on a classification problem.The number of correct and incorrect predictions is summarized with counting values and each value broken down for each class. Each of them is the key to the confusion matrix. It shows the classification model is confused when it makes predictions, at this point in here it gives us insight not only into the errors being made by a classifier but also show the types of errors that are being made [[2](https://www.geeksforgeeks.org/confusion-matrix-machine-learning/)]."},{"metadata":{"trusted":true},"cell_type":"code","source":"# The confusion matrix\nmlp_cm = confusion_matrix(Y_test, prediction)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(mlp_cm, annot=True, linewidth=0.7, linecolor='black', fmt='g', ax=ax, cmap=\"BuPu\")\nplt.title('MLP Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Result\n\n# Accuracy\nis closeness of the measurements to a specific value. Accuracy has two definitions:\n\n* More commonly, it is a description of systematic errors, a measure of statistical bias; low accuracy causes a difference between a result and a \"true\" value. ISO calls this trueness.\n* Alternatively, ISO defines[[3](https://en.wikipedia.org/wiki/Accuracy_and_precision)] accuracy as describing a combination of both types of observational error above (random and systematic), so high accuracy requires both high precision and high trueness."},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = pd.DataFrame({\n    'Model': ['MLP'],\n    'Train Score': [train_acc_mlp],\n    'Test Score': [test_acc_mlp]\n})\nmodel1.sort_values(by='Test Score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Precision and Recall\nPrecision is a description of random errors, a measure of statistical variability. In simpler terms, given a set of data points from repeated measurements of the same quantity, the set can be said to be accurate if their average is close to the true value of the quantity being measured, while the set can be said to be precise if the values are close to each other. While Recall is defined as the fraction of relevant documents retrieved compared to the total number of relevant documents (true positives divided by true positives+false negatives)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(Y_test, prediction)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import plot_precision_recall_curve\nimport matplotlib.pyplot as plt\n\ndisp = plot_precision_recall_curve(mlp,X_train, Y_train)\ndisp.ax_.set_title('2-class Precision-Recall curve: '\n                   'AP={0:0.2f}'.format(average_precision))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reference\n[1] Detail Information about Perceptron and MLP :\n\nhttps://missinglink.ai/guides/neural-network-concepts/perceptrons-and-multi-layer-perceptrons-the-artificial-neuron-at-the-core-of-deep-learning/\n\n[2] Confusion matrix:\n\nhttps://www.geeksforgeeks.org/confusion-matrix-machine-learning/\n\n[3] Accuracy, precision and recall:\n\nhttps://en.wikipedia.org/wiki/Accuracy_and_precision"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}