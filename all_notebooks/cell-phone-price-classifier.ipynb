{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Mobile Phone Price Classification\nby : Hesham Asem\n\n\nhere we have two datasets , the training which contain 2000 sample of cell phone features & the price range between 0 , 1 , 2 , 3\n\nthen a test dataset which contain 1000 sample size with the same features \n\nhttps://www.kaggle.com/iabhishekofficial/mobile-price-classification\n\nour task is to classifiy the test dataset , to know the price range for each one of the 1000 test dataset\n\n____\n\nlet's start with importing the libraries\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import SGDClassifier ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"then let's read the datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/train.csv')\ntest = pd.read_csv('/kaggle/input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"how about training dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"& here its shape"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"also here the test dataset ( which we'll not need it right now )"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"also its shape"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"____\n\n# Data Processing\n\nwe need to have a quick look to the features in the training dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can notice there are some features which are binary ( either 0 or 1 ) like : blue , dual_sim , fc , four_g  and so \n\nalso there are no categorical values , so we'll not be in need for aking dummies\n\nhow about the usual problem the Nulls . "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"great , clean data with no nulls nor missing data . let's move on . \n\n____\n\n# Features Effect\n\nwe need to measure the effect of some features , to know weather we'll keep them ot drop them if they are useless\n\nlet's first have a look to distribution of price range among all data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"price_range\", data=train,facecolor=(0, 0, 0, 0),linewidth=5,edgecolor=sns.color_palette(\"dark\", 3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ok , it's equally distributed , 500 sample size for each class . \n\nnow let's have a list of all features"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ok , lets have a look to the battery_power feature , to know unique values for it\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('number of unique values for attery power is : {}'.format(len(train.battery_power.unique())))\ntrain.battery_power.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"more than 1000 unique values will make it not easy to show its statistics , ok let's make a temporary new feature , which will be rounded values for dividing it by 100 , so we can reduce number of unizue values , to make it easy"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['battery code'] = round(train['battery_power']/100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now how about unique values for the new feature "},{"metadata":{"trusted":true},"cell_type":"code","source":"print('number of unique values for attery power is : {}'.format(len(train['battery code'].unique())))\ntrain['battery code'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"great  , only 16 values make it easier now to start using seaborn on it , to find correlation between it & price range"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(\"battery code\", \"price_range\", train, kind='kde')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"looks that majority of cell phones with low battery power , concentrated in the area of less price range , & vice versa\n\nhow about the amount of phones with specific battery power versus price range , lets use barplot"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x=\"battery code\", y=\"price_range\", data=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"it show kinda same result , which means that battery power is an important feature or training cause it affects the price \n\n\nok , we don't need that new feature any more , let's drop it & we'll use the original battery_power in the training later"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['battery code'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"how about the relationship between dual sim & price range  ? "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x=\"dual_sim\", y=\"price_range\", data=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"oh , it looks that existing or vanishing the dual sim in the phone , will not affect so much in the price range , & that mean that this feature is kinda useless\n\nso it's better to drop it , to avoid any misleading in the training\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['dual_sim'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"____\n\nnow let's check the bluetooth & its affect on price"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x=\"blue\", y=\"price_range\", data=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"again , it doesn't matter weather the phone got bluetooth or not , so dropping it will be a good idea to avoid any misleading"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['blue'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's check the clock speed"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(\"clock_speed\", \"price_range\", train, kind='kde')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"yea it's affect is not so much , but some how it will be useful , so let's keep it \n\n____________\n\n# Feature Correlation\n\nalso we need to have a look to the correlation some features & the output (Price_range)\n\nsince we have several features , so making a one confusion matrix will not be a good idea , cause it will show nothing \n\nso we'll make a temporary sliced dataframe , which will contain some feature each time , added to the price range"},{"metadata":{"trusted":true},"cell_type":"code","source":"sliced_train = train.loc[:,['price_range','battery_power','clock_speed'] ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's have a look to it "},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sliced_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ok , let's use heatmap from seaborn to see its correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(sliced_train.corr(), annot=True, linewidths=.5, fmt= '.1f')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"clock speed have no strong correlation , but let's keep it\n\nnow we'll repeat the same step with other features"},{"metadata":{"trusted":true},"cell_type":"code","source":"sliced_train = train.loc[:,['price_range','fc', 'four_g', 'int_memory', 'm_dep', 'mobile_wt', 'n_cores']]   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sliced_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(sliced_train.corr(), annot=True, linewidths=.5, fmt= '.1f')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"again with other features"},{"metadata":{"trusted":true},"cell_type":"code","source":"sliced_train = train.loc[:,['price_range', 'pc', 'px_height', 'px_width', 'ram', 'sc_h', 'sc_w']]   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sliced_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(sliced_train.corr(), annot=True, linewidths=.5, fmt= '.1f')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"and the last part"},{"metadata":{"trusted":true},"cell_type":"code","source":"sliced_train = train.loc[:,['price_range', 'talk_time', 'three_g', 'touch_screen', 'wifi']]   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sliced_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(sliced_train.corr(), annot=True, linewidths=.5, fmt= '.1f')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"___\n\n# Splitting the Data\n\nok , since we are ready now , we'll need first to divide the training data into training & test datasets , to be able to check the model accuracy \n\nlet's first specify features & output "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_data = train.drop(['price_range'], axis=1, inplace=False)\ny_data = train['price_range']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now how X looks ? "},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"X_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now let's use sklearn to divide it intro training & testing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.33, random_state=44, shuffle =True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"and have a look to their dimensions"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('X_train shape is ' , X_train.shape)\nprint('X_test shape is ' , X_test.shape)\nprint('y_train shape is ' , y_train.shape)\nprint('y_test shape is ' , y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"___\n\n# Building the Model\n\n\nhow about using SVC ? he is a good classifier\n\nso let's use girdsearch tool , to pick the best parameters for it , specially th kernel type & the value of C\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"SelectedModel = SVC(gamma='auto_deprecated')\nSelectedParameters = {'kernel':('linear', 'rbf'), 'C':[1,2,3,4,5]}\n\n\nGridSearchModel = GridSearchCV(SelectedModel,SelectedParameters, cv = 2,return_train_score=True)\nGridSearchModel.fit(X_train, y_train)\nsorted(GridSearchModel.cv_results_.keys())\nGridSearchResults = pd.DataFrame(GridSearchModel.cv_results_)[['mean_test_score', 'std_test_score', 'params' , 'rank_test_score' , 'mean_fit_time']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"____\n\nok , what are the best values for it  ? "},{"metadata":{"trusted":true},"cell_type":"code","source":"print('All Results are :\\n', GridSearchResults )\nprint('===========================================')\nprint('Best Score is :', GridSearchModel.best_score_)\nprint('===========================================')\nprint('Best Parameters are :', GridSearchModel.best_params_)\nprint('===========================================')\nprint('Best Estimator is :', GridSearchModel.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"a 97% accuracy looks great , now let's use the best estimator ( with linear kernel & C = 1) to fit our data"},{"metadata":{"trusted":true},"cell_type":"code","source":"SVCModel =  GridSearchModel.best_estimator_\nSVCModel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now how about its score in training & testing data ? "},{"metadata":{"trusted":true},"cell_type":"code","source":"print('SVCModel Train Score is : ' , SVCModel.score(X_train, y_train))\nprint('SVCModel Test Score is : ' , SVCModel.score(X_test, y_test))\nprint('----------------------------------------------------')\n\n\ny_pred = SVCModel.predict(X_test)\nprint('Predicted Value for SVCModel is : ' , y_pred[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"a very good accuracy , & even we avoided OF , since test accuracy is 96%\n\n____\n\nok let's check if Logistic Regression might help us , also using GirdSearch tool "},{"metadata":{"trusted":true},"cell_type":"code","source":"SelectedModel = LogisticRegression(penalty='l2' , solver='sag',random_state=33)\nSelectedParameters = {'C':[1,2,3,4,5]}\n\n\nGridSearchModel = GridSearchCV(SelectedModel,SelectedParameters, cv = 4,return_train_score=True)\nGridSearchModel.fit(X_train, y_train)\nsorted(GridSearchModel.cv_results_.keys())\nGridSearchResults = pd.DataFrame(GridSearchModel.cv_results_)[['mean_test_score', 'std_test_score', 'params' , 'rank_test_score' , 'mean_fit_time']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"how about its result ? "},{"metadata":{"trusted":true},"cell_type":"code","source":"print('All Results are :\\n', GridSearchResults )\nprint('===========================================')\nprint('Best Score is :', GridSearchModel.best_score_)\nprint('===========================================')\nprint('Best Parameters are :', GridSearchModel.best_params_)\nprint('===========================================')\nprint('Best Estimator is :', GridSearchModel.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ohhh , only 62 % , which will not be suitable at all . . \n\n____\n\nok how about using Voting Classifier , which will use (Decision Tree, Gaussian NB , Bernoulli NB , Multinomial NB , and SGD Classifier ) ? \n\nlet's use it "},{"metadata":{"trusted":true},"cell_type":"code","source":"DTModel_ = DecisionTreeClassifier(criterion = 'entropy',max_depth=3,random_state = 33)\nGaussianNBModel_ = GaussianNB()\nBernoulliNBModel_ = BernoulliNB(alpha = 0.1)\nMultinomialNBModel_= MultinomialNB(alpha = 0.1)\nSGDModel_ = SGDClassifier(loss='log', penalty='l2', max_iter=10000, tol=1e-5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#loading Voting Classifier\nVotingClassifierModel = VotingClassifier(estimators=[('DTModel',DTModel_),('GaussianNBModel',GaussianNBModel_),\n                                                     ('BernoulliNBModel',BernoulliNBModel_),\n                                                     ('MultinomialNBModel',MultinomialNBModel_),\n                                                     ('SGDModel',SGDModel_)], voting='hard')\nVotingClassifierModel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"how about the score ? "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculating Details\nprint('VotingClassifierModel Train Score is : ' , VotingClassifierModel.score(X_train, y_train))\nprint('VotingClassifierModel Test Score is : ' , VotingClassifierModel.score(X_test, y_test))\nprint('----------------------------------------------------')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"not very good , only SVC which show best accuracy \n\n____\n\nso let's use it now to apply for the read test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"SVCModel =  SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,decision_function_shape='ovr', degree=3,\n                gamma='auto_deprecated',kernel='linear', max_iter=-1, probability=False, random_state=None,\n                shrinking=True, tol=0.001, verbose=False)\nSVCModel.fit(X_train, y_train)\n\nprint('SVCModel Train Score is : ' , SVCModel.score(X_train, y_train))\nprint('SVCModel Test Score is : ' , SVCModel.score(X_test, y_test))\nprint('----------------------------------------------------')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"___\n\n# Predict Test Data\n\nok , let's now have look to the real test data (which is different from test data used in the model) "},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we need to drop 'id' feature , plus the two unused feature from training data (blue & dual_sim)"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.drop(['id','blue','dual_sim'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"also we need to be sure that there are only 18 features"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Test Dimension is {}'.format(test.shape))\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"which will have to be exactly like the 18 features in training dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('X_train Dimension is {}'.format(X_train.shape))\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"perfect , now lets predict it"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_result = SVCModel.predict(test)\nfinal_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"and insert it as a new feature in the test dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.insert(18,'Expected Price',final_result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"here we go , here is the final result"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"test.head(30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"___\n\n# Finally\n\nhope you enjoyed it & found this kernel useful \n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}