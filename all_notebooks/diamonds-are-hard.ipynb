{"nbformat":4,"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","version":"3.6.3","file_extension":".py","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":1,"cells":[{"cell_type":"markdown","metadata":{},"source":"### I'm fairly new to data science and would appreciate any feedback or any comments in general. Thank you!"},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\n\nfrom sklearn.linear_model import Ridge, LinearRegression, Lasso\nimport sklearn.ensemble as skens\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split, KFold\nimport sklearn.metrics as metrics\n\n%matplotlib inline"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"data = pd.read_csv('../input/diamonds.csv',index_col=0)\ndata.head()"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"data.describe()"},{"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"86c117c5-97fa-4c2c-86e7-3e30e3e79967","collapsed":true,"_uuid":"f85f1571c1b2e90b2496b823a995c0a347c62f71"},"outputs":[],"source":"# Pasted from Content Page:\n# price price in US dollars ($326--$18,823)\n# carat weight of the diamond (0.2--5.01)\n# cut quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n# color diamond colour, from J (worst) to D (best)\n# clarity a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n# x length in mm (0--10.74)\n# y width in mm (0--58.9)\n# z depth in mm (0--31.8)\n# depth total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43--79)\n# table width of top of diamond relative to widest point (43--95)"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"# x, y, and z should not have zeroes. \n# e.g. --> if you go to Jared's and pick out a diamond for your fiance that's 0 mm wide,\n# you're braver than me.\n\nd = data.shape[0]\nprint('# Rows in the data before: {}'.format(d))\ndata = data[(data.x > 0) & ((data.y > 0) & (data.z > 0))]\n\nprint('# Rows after: {}'.format(data.shape[0]))\nprint('--------\\nDifference of: {}'.format(d - data.shape[0]))\n\n# Looks like a 20 instances of a \"0\" in one of x,y,z were removed\n# only 20 out of ~54k, so I'm not gonna worry about possibly correcting/imputing\n# these although that is a possibility"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"data['log_price'] = np.log(data.price)\n\nplt.figure(figsize=(10,5))\nplt.subplot(1,2,1);\nsns.distplot(data.price);\nplt.subplot(1,2,2);\nsns.distplot(data.log_price);\n\ndata.drop('price', axis=1, inplace=True)\n\n# 'price' is skewed. Makes sense. Pricier / quality diamonds are rarer for a reason.\n# Perform a log transformation on the skewed 'price' data. Results: not normal,\n# but bimodal and a big improvement.\n# Information on log transforming technique: \n# https://stats.stackexchange.com/questions/107610/what-is-the-reason-the-log-transformation-is-used-with-right-skewed-distribution"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"### DATA VIZ ###\n\n# plot the c's\n\nc = ['cut','clarity','color']\n\nplt.figure(figsize=(17,5))\nfor i in range(len(c)):\n    plt.subplot(1,3,i+1)\n    sns.countplot(data[c[i]], palette='Set2');\n    plt.title('Value Counts of {}'.format(c[i]))"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"# good predictors?\n\np = ['carat','table','depth']\nsns.pairplot(x_vars=p, y_vars=['log_price'], data=data, size=4.0);\n\n# hmm in general heavy diamonds (high carat) are pricier!\n\n# The carat is a unit of mass equal to 200 mg and is \n# used for measuring gemstones and pearls. - Wikipedia"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"# 'carat' seems to be a really good predicator of price\n# Let's see what else we can found out about diamonds ...\n\nsns.pairplot(x_vars=['carat','x','depth'], y_vars=['log_price'], data=data, hue='color', size=4.5);"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"sns.pairplot(x_vars=['carat','x','depth'], y_vars=['log_price'], data=data, hue='cut', size=4.5);"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"sns.pairplot(x_vars=['carat','x','depth'], y_vars=['log_price'], data=data, hue='clarity', size=4.5);\n\n# Kind of hard to tell, but for the majority of diamonds, it looks like bigger (larger carat and x)\n# do not mean necessarily mean the diamond is of the a better class of 'clarity','color', or 'cut'"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"# What about these guys?\n\nxyz = ['x','y','z']\nsns.pairplot(x_vars=xyz, y_vars=['log_price'], data=data, size=4.0);\n\n# x and log_price seem to have a positive relationship\n# Definitely a few outliers messing up our view in y and z"},{"execution_count":null,"cell_type":"code","metadata":{"scrolled":false},"outputs":[],"source":"# lists from before are concatenated\nnumerics = xyz + p\n\n# Normalize and plot\nplt.figure(figsize=(18,14))\nfor col in numerics:\n    mean = np.mean(data[col])\n    std = np.std(data[col])\n    data[col] = (data[col] - mean) / std\n\nplt.subplot(2,1,1);\nax = sns.violinplot(data=data[numerics]);\nax.set_title('Violinplots of diamond data');\n\nplt.subplot(2,1,2);\nax = sns.boxplot(data=data[numerics]);\nax.set_title('Boxplots of diamond data');\n\n# Outliers strike again!"},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true},"outputs":[],"source":"# map the c's ...\n# Clarity:  (worst)I1, SI2, SI1, VS2, VS1, VVS2, VVS1, IF(best)\n# Color: D (best) <---> J (worst)\n# Cut: Fair (worst) - Good - Very Good - Premium - Ideal (best)\n\ndata.color = data.color.map({'J':1,'I':2,'H':3,'G':4,'F':5,'E':6,'D':7})\ndata.clarity = data.clarity.map({'I1':1, 'SI2':2, 'SI1':3, 'VS2':4, 'VS1':5, 'VVS2':6, 'VVS1':7, 'IF':8})\ndata.cut = data.cut.map({'Fair':1,'Good':2,'Very Good':3,'Premium':4,'Ideal':5})\n\n# *** Good to note if you're new to data science: ***\n# An order relationship must exist to encode this way.\n# -- For example within the 'cut' attribute a value of 'Fair' is less (not equal to / not equally as valuable as) than\n# a value of 'Ideal', as opposed to an attribute like 'Male or Female', which would be OneHotEncoded \n# (into two new binary columns) because both are equally as meaningful (Male is not greater than Female, & vice versa)\n# Thus, an order relationship exists, and so 'Fair' --> 1 is less than 'Ideal' --> 5 \n# Our regression model can pick up on this numerical relationship."},{"execution_count":null,"cell_type":"code","metadata":{"scrolled":true},"outputs":[],"source":"data.head()"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"plt.figure(figsize=(10,8))\nsns.heatmap(data.corr(),annot=True,linewidths=0.5);"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"sns.pairplot(data);"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"# From our heatmap and pairplots: carat and x,y,z are highly correlated, but\n# should we address what looks like outliers in y and z?\n\nsns.pairplot(x_vars=xyz, y_vars='carat', data=data, size=4.0);"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"plt.figure(figsize=(18,5))\n\n# outlier discussion:\n# Are outliers EVIL? \n# Should you remove them??\n# Let's look at 'y' and 'z', which seem to have a few questionably large points.\n\nplt.subplot(1,4,1);\n# y and z should/could look like x\nax = sns.regplot(x='x', y='carat', data=data, ci=0);\nax.set_title('Carat and X');\n\n# what does y look like? Is the outlier affecting the regression line?\n# Let's plot it again.\nplt.subplot(1,4,2);\nax = sns.regplot(x='y', y='carat', data=data, ci=0);\nax.set_title('Carat and Y WITH outliers');\n# Hmm maybe? It's possible those two points way down on the x axis are dragging the line down.\n\n# Let's remove outliers and see how it looks:\ndata['y_test'] = data[data.y < 20].y # removing the two points with large values\nplt.subplot(1,4,3);\nax = sns.regplot(x='y_test', y='carat', data=data, ci=0);\nax.set_title('Carat and Y WITHOUT outliers');\ndata.drop('y_test',axis=1,inplace=True)\n# Looks pretty much like x. Nice!\n\n\n# BUT let's go back and zoom in on the Carat and Y w/ outliers plot:\nplt.subplot(1,4,4);\nax = sns.regplot(x='y', y='carat', data=data, ci=0);\nax.set_title('Carat and Y WITH outliers ZOOMED IN');\nax.set_ylim([-2,10]); # control the zoom of the plot.\nax.set_xlim([-2,5]);\n# Hmm looks like x \n\n# Are these points influential?\n# No, the third and fourth plots look identical and \n# the regression line still pretty much crosses through point (4,4) in both.\n# Conclusion: outliers are real and not necessarily your enemy. \n# Huge (outlier-ish) diamonds like this actually exist and\n# actually aren't the most important factor in determining price\n# Source: http://www.jewelrywise.com/engagement-wedding/article/does-the-size-of-the-diamond-matter"},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true},"outputs":[],"source":"# Even though it was fun to play around with them, we're going to drop 'x','y','z'\n# They're all heavily correlated with 'carat' and will negatively affect a linear model.\n\ndata.drop(xyz, axis=1, inplace=True)"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"# Final look at our data\ndata.head()"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"# missing values?\nmissing = pd.DataFrame(data.isnull().sum(), columns=['total'])\nmissing\n\n# nope."},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"### Model Building ###\n\ndef inv_log(preds):\n    # apply inverse log function\n    transformed_preds = []\n    for val in preds:\n        transformed_preds.append(np.round(math.exp(val), 2))\n    return np.array(transformed_preds)\n    \n\nX = data.iloc[:,:-1]\ny = data.log_price\n\nX_train, X_50, y_train, y_50 = train_test_split(X,y, test_size=0.5, random_state=2)\n\nprint('Data split 50/50...\\nShape of training: {}\\nShape of Other: {}\\n---'.format(X_train.shape, X_50.shape))\n\nX_valid, X_test, y_valid, y_test = train_test_split(X_50, y_50, test_size=0.5, random_state=2)\n\nprint(\"Further split 'Other' into 50/50 validation and test sets...\\nShape of Validation Set: {}\\nShape of Test Set: {}\".format(X_valid.shape, X_test.shape))\n\n# Now we have a 50/25/25 split on our data"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"# Try some models\n\nlr = LinearRegression()\nrid = Ridge()\nrf = skens.RandomForestRegressor()\ngb = skens.GradientBoostingRegressor()\n\nclassifiers = [lr, rid, rf, gb]\n\nkf = KFold(n_splits=5, shuffle=True, random_state=11)\n\nresults = []\nnames = []\nfor clf in classifiers:\n    scores = cross_val_score(clf, X_train, y_train, scoring='r2', cv=kf, n_jobs=5)\n    results.append(scores)\n    name = str(clf.__class__).strip(\"'>\").split('.')[-1]\n    names.append(name)\n    print(name + ':', scores)\n    print('Average R-Squared Score:', np.mean(scores),'\\n----')\n\n# ensembles yield higher scores, but take a little longer to execute."},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"plt.figure(figsize=(13,6))\nsns.boxplot(x=results, y=names);"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"rid = Ridge().fit(X_train, y_train)\ny_pred_train = rid.predict(X_train)\ny_pred = rid.predict(X_valid)\nprint('R Squared:\\ntraining -- {}\\nvalidation -- {}'.format(metrics.r2_score(y_train, y_pred_train), metrics.r2_score(y_valid, y_pred)))\nerror = inv_log(y_valid) - inv_log(y_pred)\nprint('Average Price Error: {}'.format(error.mean()))"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"# visualized\nsns.regplot(x=y_pred, y=y_valid, marker='x',line_kws={'color':'red'});"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"# RF\n\nrf = skens.RandomForestRegressor(n_estimators=15).fit(X_train, y_train)\ny_pred_train = rf.predict(X_train)\ny_pred = rf.predict(X_valid)\nprint('R Squared:\\ntraining -- {}\\nvalidation -- {}'.format(metrics.r2_score(y_train, y_pred_train), metrics.r2_score(y_valid, y_pred)))\nerror = inv_log(y_valid) - inv_log(y_pred)\nprint('Average Price Error: {}'.format(error.mean()))"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"# As expected: tighter than the linear regression model\nsns.regplot(x=y_pred, y=y_valid, marker='x',line_kws={'color':'red'});"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"feats = pd.DataFrame(rf.feature_importances_, columns=['Importance'],\n             index=X_train.columns).sort_values('Importance', ascending=False)\n# feats.plot(kind='barh')\nfeats\n# Carats are key!"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"# GB\n\ngb = skens.GradientBoostingRegressor().fit(X_train, y_train)\ny_pred_train = gb.predict(X_train)\ny_pred = gb.predict(X_valid)\nprint('R Squared:\\ntraining -- {}\\nvalidation -- {}'.format(metrics.r2_score(y_train, y_pred_train), metrics.r2_score(y_valid, y_pred)))\nerror = inv_log(y_valid) - inv_log(y_pred)\nprint('Average Price Error: {}'.format(error.mean()))"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"sns.regplot(x=y_pred, y=y_valid, marker='x',line_kws={'color':'red'});"},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true},"outputs":[],"source":"# more to come (maybe) with GridSearchCV and parameters"}]}