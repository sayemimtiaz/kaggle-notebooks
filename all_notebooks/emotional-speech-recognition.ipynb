{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <center>Emotional Speech Recognition"},{"metadata":{"_uuid":"a399e368-7306-4d35-992c-77760316a39e","_cell_guid":"7e18703a-b1a4-43af-9d09-13261b337ca3","trusted":true},"cell_type":"code","source":"import os\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# librosa is a Python library for analyzing audio and music.\n# It can be used to extract the data from the audio files we will see it later\nimport librosa \nimport librosa.display\n\n# to play the audio files\nfrom IPython.display import Audio\nplt.style.use('seaborn-white')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Since the feature extraction require a huge amount of time, before we start, in the codelines below, specify if there are already dataframes available and if so the path of those ones.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_FRAMES = True\nfem_path = '../input/features/Female_features.csv'\nmal_path = '../input/features/Male_features.csv'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c48bfdd2-3a51-4d8c-91b4-b776abb04c16","_cell_guid":"a981411f-b899-458e-8733-14a260731e4a","trusted":true},"cell_type":"markdown","source":"# <center>Emotions Speech datasets<center>  \n\n**Content**\nData set contains files from RAVDESS speechs, CREMA-D, SAVEE, TESS.\n   \nOut of all files data sets make up:\n* CREMA-D - 7,442 \n* TESS - 2,800 \n* RAVDESS - 2,076 \n* SAVEE - 480"},{"metadata":{"_uuid":"142fee18-07b4-49df-a886-4ee6a3c2e3d7","_cell_guid":"7072a6fc-9273-4fb3-89c6-7e1e7246a484","trusted":true},"cell_type":"code","source":"TESS = \"../input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"\nRAV = \"../input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\nSAVEE = \"../input/surrey-audiovisual-expressed-emotion-savee/ALL/\"\nCREMA = \"../input/cremad/AudioWAV/\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d64dee5-8878-49e3-b6b8-13fa0f528588","_cell_guid":"a78f27ff-88d3-4220-bef3-ce16e2472153","trusted":true},"cell_type":"code","source":"# Get the data location for SAVEE\ndir_list = os.listdir(SAVEE)\n\n# parse the filename to get the emotions\nemotion=[]\npath = []\nfor i in dir_list:\n    if i[-8:-6]=='_a':\n        emotion.append('angry')\n    elif i[-8:-6]=='_d':\n        emotion.append('disgust')\n    elif i[-8:-6]=='_f':\n        emotion.append('fear')\n    elif i[-8:-6]=='_h':\n        emotion.append('happy')\n    elif i[-8:-6]=='_n':\n        emotion.append('neutral')\n    elif i[-8:-6]=='sa':\n        emotion.append('sad')\n    elif i[-8:-6]=='su':\n        emotion.append('surprise')\n    else:\n        emotion.append('unknown') \n    path.append(SAVEE + i)\n\n# Now check out the label count distribution \nSAVEE_df = pd.DataFrame(emotion, columns = ['labels'])\nSAVEE_df = pd.concat([SAVEE_df, pd.DataFrame(path, columns = ['path'])], axis = 1)\nprint('SAVEE dataset')\nSAVEE_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e76fe25-cb0e-4816-b5b0-331fdce27070","_cell_guid":"7f8faee9-ebb5-4147-a013-39eb98c725c4","trusted":true},"cell_type":"code","source":"# Get the data location for TESS\npath = []\nemotion = []\ndir_list = os.listdir(TESS)\n\nfor i in dir_list:\n    fname = os.listdir(TESS + i)   \n    for f in fname:\n        if i == 'OAF_angry' or i == 'YAF_angry':\n            emotion.append('angry')\n        elif i == 'OAF_disgust' or i == 'YAF_disgust':\n            emotion.append('disgust')\n        elif i == 'OAF_Fear' or i == 'YAF_fear':\n            emotion.append('fear')\n        elif i == 'OAF_happy' or i == 'YAF_happy':\n            emotion.append('happy')\n        elif i == 'OAF_neutral' or i == 'YAF_neutral':\n            emotion.append('neutral')                                \n        elif i == 'OAF_Pleasant_surprise' or i == 'YAF_pleasant_surprised':\n            emotion.append('surprise')               \n        elif i == 'OAF_Sad' or i == 'YAF_sad':\n            emotion.append('sad')\n        else:\n            emotion.append('Unknown')\n        path.append(TESS + i + \"/\" + f)\n\nTESS_df = pd.DataFrame(emotion, columns = ['labels'])\n#TESS_df['source'] = 'TESS'\nTESS_df = pd.concat([TESS_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nprint('TESS dataset')\nTESS_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d34c2cbb-a405-4f5f-b408-7036c94c5358","_cell_guid":"221fd993-f8d0-4099-a9db-e82873b95a76","trusted":true},"cell_type":"code","source":"# Importing datas from RAVDESS\ndir = os.listdir(RAV)\n\nmales = []\nfemales = [] \n        \nfor actor in dir:\n       \n    files = os.listdir(RAV + actor)\n        \n    for file in files: \n        part = file.split('.')[0]\n        part = part.split(\"-\")           \n            \n        temp = int(part[6])        \n                \n        if part[2] == '01':\n            emotion = 'neutral'\n        elif part[2] == '02':\n            emotion = 'calm'\n        elif part[2] == '03':\n            emotion = 'happy'\n        elif part[2] == '04':\n            emotion = 'sad'\n        elif part[2] == '05':\n            emotion = 'angry'\n        elif part[2] == '06':\n            emotion = 'fear'\n        elif part[2] == '07':\n            emotion = 'disgust'\n        elif part[2] == '08':\n            emotion = 'surprise'\n        else:\n            emotion = 'unknown'\n            \n        if temp%2 == 0:\n            path = (RAV + actor + '/' + file)\n            #emotion = 'female_'+emotion\n            females.append([emotion, path]) \n        else:\n            path = (RAV + actor + '/' + file)\n             #emotion = 'male_'+emotion\n            males.append([emotion, path])   \n    \n   \nRavFemales_df = pd.DataFrame(females)\nRavFemales_df.columns = ['labels', 'path']\n\nRavMales_df = pd.DataFrame(males)\nRavMales_df.columns = ['labels', 'path']\n\nprint('RAVDESS datasets')\nRavFemales_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"933a3449-7f09-40cd-b984-df2198f4c291","_cell_guid":"031a3f2e-8331-47db-9853-fd6c8e7c3598","trusted":true},"cell_type":"code","source":"RavMales_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e4028e1-2f7c-4afc-8e9a-b817e2b0e0fc","_cell_guid":"e8294225-2e2b-4377-92df-b06c13413a07","trusted":true},"cell_type":"code","source":"files = os.listdir(CREMA)\n\nfemale = [1002,1003,1004,1006,1007,1008,1009,1010,1012,1013,1018,1020,1021,1024,1025,1028,1029,1030,1037,1043,1046,1047,1049,\n          1052,1053,1054,1055,1056,1058,1060,1061,1063,1072,1073,1074,1075,1076,1078,1079,1082,1084,1089,1091]\nmales = []\nfemales = []\n\nfor file in files: \n    part = file.split('_')   \n    \n    if part[2] == 'SAD':\n        emotion = 'sad'\n    elif part[2] == 'ANG':\n        emotion = 'angry'\n    elif part[2] == 'DIS':\n        emotion = 'disgust'\n    elif part[2] == 'FEA':\n        emotion = 'fear'\n    elif part[2] == 'HAP':\n        emotion = 'happy'\n    elif part[2] == 'NEU':\n        emotion = 'neutral'  \n    else:\n        emotion = 'unknown'\n        \n    if int(part[0]) in female:\n        path = (CREMA + '/' + file)\n        #emotion = 'female_'+emotion\n        females.append([emotion, path]) \n    else:\n        path = (CREMA + '/' + file)\n        #emotion = 'male_'+emotion\n        males.append([emotion, path])   \n    \nCremaFemales_df = pd.DataFrame(females)\nCremaFemales_df.columns = ['labels', 'path']\n\nCremaMales_df = pd.DataFrame(males)\nCremaMales_df.columns = ['labels', 'path']\n    \nprint('CREMA datasets')\nCremaFemales_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c4832b0-5fc3-4ea5-aac7-bae8daacd67d","_cell_guid":"b196cb88-d2a2-40a5-8b8e-842803891f2b","trusted":true},"cell_type":"code","source":"CremaMales_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3772939b-4f84-4c8f-94fa-dbf9b133fe71","_cell_guid":"92189d48-d2d0-4b9f-8457-9da943152128","trusted":true},"cell_type":"code","source":"# Now lets merge all the dataframe\nMales = pd.concat([SAVEE_df, RavMales_df, CremaMales_df], axis = 0)\nMales.to_csv(\"males_emotions_df.csv\", index = False)\n\nFemales = pd.concat([TESS_df, RavFemales_df, CremaFemales_df], axis = 0)\nFemales.to_csv(\"females_emotions_df.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7421547a-5a2f-4222-bf01-2a07cf1de863","_cell_guid":"7c939896-00d4-4e4b-a0b3-6d6c1cf539d9","trusted":true},"cell_type":"markdown","source":"# <center> Data Visualization\n\nFirst, we will plot the number of emotions (of wich above there are the proportions).\nThen using Librosa there will be some waveplots related to each emotion"},{"metadata":{"_uuid":"1371d315-b165-4330-b4c9-3d2729a3b982","_cell_guid":"2e0a4556-54dc-4380-9dd9-e26f0ae827bb","trusted":true},"cell_type":"code","source":"order = ['angry','calm','disgust','fear','happy','neutral','sad','surprise']\n\nfig = plt.figure(figsize=(17, 5))\n\nfig.add_subplot(121)\nplt.title('Count of Females Emotions', size=16)\nsns.countplot(Females.labels, order = order)\nplt.ylabel('Count', size=12)\nplt.xlabel('Emotions', size=12)\nsns.despine(top=True, right=True, left=False, bottom=False)\n\nfig.add_subplot(122)\nplt.title('Count of Males Emotions', size=16)\nsns.countplot(Males.labels, order = order)\nplt.ylabel('Count', size=12)\nplt.xlabel('Emotions', size=12)\nsns.despine(top=True, right=True, left=False, bottom=False)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ec88691-2fb9-4c91-8b9a-11756aefe497","_cell_guid":"5f48d589-0add-4bb7-a8fb-f9f4a12e3c52","trusted":true},"cell_type":"code","source":"def create_waveplot(data, sr, e):\n    plt.figure(figsize=(10, 3))\n    plt.title(f'Waveplot for audio with {e} emotion', size=15)\n    librosa.display.waveplot(data, sr=sr)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b6d7b8c-74c0-4853-b8d4-a6e7dce7576c","_cell_guid":"a4a58f6d-3303-4793-b896-81fcd808596f","trusted":true},"cell_type":"code","source":"emotion='Angry'\npath = '../input/ravdess-emotional-speech-audio/Actor_01/03-01-05-01-01-01-01.wav'\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\nAudio(path)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34b0c3f7-6787-44c5-80bd-4c207ca8b536","_cell_guid":"f97d6a01-87fa-49d7-be49-c74640b7bc2b","trusted":true},"cell_type":"code","source":"emotion='Very Angry' \npath = '../input/ravdess-emotional-speech-audio/Actor_01/03-01-05-02-01-01-01.wav'\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\nAudio(path)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"139edccd-bd19-4ed8-9c61-157f411e5381","_cell_guid":"d6860263-c1df-4fd2-b0fc-77e385798849","trusted":true},"cell_type":"code","source":"emotion='Sing Angry'\npath = '../input/ravdess-emotional-song-audio/Actor_01/03-02-05-01-01-01-01.wav'\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\nAudio(path)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aaa11732-ab21-471e-98ea-86e993fe9aa1","_cell_guid":"f55752af-56ed-489c-82ad-6bd3aa4b3c6b","trusted":true},"cell_type":"code","source":"emotion='Sing Very Angry' \npath = '../input/ravdess-emotional-song-audio/Actor_01/03-02-05-02-01-01-01.wav'\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\nAudio(path)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5d08055-1801-45f3-9a1f-a625543bb3a8","_cell_guid":"bd1806f1-d8a1-4e41-a0e2-17a6c2ee41dc","trusted":true},"cell_type":"markdown","source":"## Adding augmentation"},{"metadata":{},"cell_type":"markdown","source":"#### Definition:\n* Data augmentation is the process by which we create new synthetic training samples by adding small perturbations on our initial training set.\n* The objective is to make our model invariant to those perturbations and enhace its ability to generalize.\n* In order to this to work adding the perturbations must conserve the same label as the original training sample.\n* In images data augmention can be performed by shifting the image, zooming, rotating ...\n* In our case we will add noise, stretch and roll, pitch shift ..."},{"metadata":{"_uuid":"7742f44f-b268-4b59-8185-36e7313d994a","_cell_guid":"b249fc24-d1f1-437d-80e6-174e8453da8f","trusted":true},"cell_type":"code","source":"def noise(data):\n    noise_amp = 0.04*np.random.uniform()*np.amax(data)\n    data = data + noise_amp*np.random.normal(size=data.shape[0])\n    return data\n\ndef stretch(data, rate=0.70):\n    return librosa.effects.time_stretch(data, rate)\n\ndef shift(data):\n    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n    return np.roll(data, shift_range)\n\ndef pitch(data, sampling_rate, pitch_factor=0.8):\n    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n\ndef higher_speed(data, speed_factor = 1.25):\n    return librosa.effects.time_stretch(data, speed_factor)\n\ndef lower_speed(data, speed_factor = 0.75):\n    return librosa.effects.time_stretch(data, speed_factor)\n\n# taking any example and checking for techniques.\npath = path = '../input/ravdess-emotional-speech-audio/Actor_01/03-01-05-01-01-01-01.wav'\ndata, sample_rate = librosa.load(path)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0af9e19e-92bc-4262-8ca7-20307e328acd","_cell_guid":"47835287-183e-4152-902e-cb6bb390b36c","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,3))\nx = noise(data)\nlibrosa.display.waveplot(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9d85f0d-2d2c-4be2-9577-b597e20f4a20","_cell_guid":"867418b6-84c8-4afe-9253-912cfe11b20f","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,3))\nx = stretch(data)\nlibrosa.display.waveplot(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8bb38c4d-24bd-443f-8c96-6fcccfb8f781","_cell_guid":"ecf5b48f-15a6-451a-b2b9-bacba7173fd8","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,3))\nx = shift(data)\nlibrosa.display.waveplot(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d69f44d9-2f5e-41ea-a924-f56164eb7ab0","_cell_guid":"69c72492-3a27-49ce-b0e9-59a760c079a9","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,3))\nx = pitch(data, sample_rate)\nlibrosa.display.waveplot(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,3))\nx = higher_speed(data)\nlibrosa.display.waveplot(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,3))\nx = lower_speed(data)\nlibrosa.display.waveplot(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f180b40-2a67-4f2e-bad4-70489a4e7270","_cell_guid":"b0f0f1bc-26cb-4e46-afe4-6a87f340d216","trusted":true},"cell_type":"markdown","source":"# <center> Feature Extraction</center>"},{"metadata":{"_uuid":"933364f6-33c2-4836-b004-5d8e273aa95c","_cell_guid":"34f5928d-28ba-44eb-a208-747dff0496fa","trusted":true},"cell_type":"markdown","source":"As we understand, the data provided from audio cannot be understood by the models directly, so we need to convert them into an understandable format for which feature extraction is used.\nThe audio signal is a three-dimensional signal in which three axes represent time, amplitude and frequency.\n\n\nLooking at the waveplots above seems clear (from an eye test) that the waveform itself may not necessarily yield clear class identifying information. Infact they look quite similar.<br/>  \nIt turns out one of the best tool to feature extract from audio waveforms ( and digital signal in general) is   **Mel Frequency Cepstral Coefficents (MFCCs)**.  Below we will go through a brief technical discussion, just to see how MFCCs works\n\n## add to references \n* All the infos about feature extraction and audio processing were taken from https://medium.com/comet-ml/applyingmachinelearningtoaudioanalysis-utm-source-kdnuggets11-19-e160b069e88\n* Mel Frequency Cepstral Coefficients (MFCCs), introduced by Davis and Mermelstein in 1980."},{"metadata":{"_uuid":"2bba8bf9-d3d6-49c7-b3c6-8ca6fd55426a","_cell_guid":"c409646e-553a-432b-b79f-2d4339243c62","trusted":true},"cell_type":"markdown","source":"## Mel-Frequency Cepstral Coefficients (MFCCs)\nThis feature is one of the most important method to extract a feature of an audio signal and is used majorly whenever working on audio signals. The mel frequency cepstral coefficients (MFCCs) of a signal are a small set of features (usually about 10â€“20) which concisely describe the overall shape of a spectral envelope."},{"metadata":{"_uuid":"588ccbb7-3456-49da-9c75-5fcf5f21cb0e","_cell_guid":"495d6361-676f-4ef5-9b74-2f16e7157e61","trusted":true},"cell_type":"code","source":"#sample_rate = 22050\n\ndef extract_features(data):\n    \n    result = np.array([])\n    \n    #mfccs = librosa.feature.mfcc(y=data, sr=22050, n_mfcc=42) #42 mfcc so we get frames of ~60 ms\n    mfccs = librosa.feature.mfcc(y=data, sr=22050, n_mfcc=58)\n    mfccs_processed = np.mean(mfccs.T,axis=0)\n    result = np.array(mfccs_processed)\n     \n    return result\n\ndef get_features(path):\n    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n    data, sample_rate = librosa.load(path, duration=3, offset=0.5, res_type='kaiser_fast') \n    \n    #without augmentation\n    res1 = extract_features(data)\n    result = np.array(res1)\n    \n    #noised\n    noise_data = noise(data)\n    res2 = extract_features(noise_data)\n    result = np.vstack((result, res2)) # stacking vertically\n    \n    #stretched\n    stretch_data = stretch(data)\n    res3 = extract_features(stretch_data)\n    result = np.vstack((result, res3))\n    \n    #shifted\n    shift_data = shift(data)\n    res4 = extract_features(shift_data)\n    result = np.vstack((result, res4))\n    \n    #pitched\n    pitch_data = pitch(data, sample_rate)\n    res5 = extract_features(pitch_data)\n    result = np.vstack((result, res5)) \n    \n    #speed up\n    higher_speed_data = higher_speed(data)\n    res6 = extract_features(higher_speed_data)\n    result = np.vstack((result, res6))\n    \n    #speed down\n    lower_speed_data = higher_speed(data)\n    res7 = extract_features(lower_speed_data)\n    result = np.vstack((result, res7))\n    \n    return result","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c82f628-b3c0-4638-ae9a-a0a5fccb7639","_cell_guid":"078e399f-f2c0-4d25-b650-11d7d127bcad","trusted":true},"cell_type":"code","source":"if not DATA_FRAMES:\n    \n    female_X, female_Y = [], []\n    for path, emotion in zip(Females.path, Females.labels):\n        features = get_features(path)\n        #adding augmentation, get_features return a multi dimensional array (for each augmentation), so we have to use a loop to fill the df\n        for elem in features: \n            female_X.append(elem)        \n            female_Y.append(emotion)\n    \n\n    male_X, male_Y = [], []\n    for path, emotion in zip(Males.path, Males.labels):\n        features = get_features(path)\n        for elem in features:\n            male_X.append(elem)\n            male_Y.append(emotion)\n            \n    print(f'Check shapes:\\nFemale features: {len(female_X)}, labels: {len(female_Y)}\\nMale features:   {len(male_X)}, labels: {len(male_Y)}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"463eb973-74c3-4861-8828-4ba04028613f","_cell_guid":"997a1461-28e9-431b-b90d-cf332e2f9ef3","trusted":true},"cell_type":"code","source":"def setup_dataframe(gender, features, labels):\n    df = pd.DataFrame(features)\n    df['labels'] = labels\n    df.to_csv(f'{gender}_features.csv', index=False)\n    \n    print(f'{gender} dataframe')\n    df.sample(frac=1).head()\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2feb21f-71da-4d97-90dc-c3410cd71f5e","_cell_guid":"b18b718f-2a23-48a3-8212-aead72c1fec7","trusted":true},"cell_type":"code","source":"if not DATA_FRAMES:\n    Females_Features = setup_dataframe('Female', female_X, female_Y)\nelse:\n    Females_Features = pd.read_csv(fem_path)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed61be7b-9002-481d-bcc4-2f6e6fe07baf","_cell_guid":"4784e5ad-2918-41b3-99fd-0904301a468f","trusted":true},"cell_type":"code","source":"if not DATA_FRAMES:\n    Males_Features = setup_dataframe('Male', male_X, male_Y)\nelse:\n    Males_Features = pd.read_csv(mal_path)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eeedb444-b208-4303-a886-6f4ac9bcd4b9","_cell_guid":"a7defac7-9b53-44e4-9dcb-62efc952a60f","trusted":true},"cell_type":"markdown","source":"# <center>Data Preparation\nAs of now we have extracted the data, now we need to normalize and split our data for training and testing."},{"metadata":{"_uuid":"ac744cf0-b81c-4d73-b7d4-f46aad3d8dc4","_cell_guid":"8b957998-6082-49eb-817a-a515aede8ca0","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58bb4e1a-0eeb-40b8-ae0c-f17fdfd2387e","_cell_guid":"74df1a59-4147-48e3-a126-9864ea08ce89","trusted":true},"cell_type":"code","source":"female_X = Females_Features.iloc[: ,:-1].values\nfemale_Y = Females_Features['labels'].values\n\nmale_X = Males_Features.iloc[: ,:-1].values\nmale_Y = Males_Features['labels'].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b99d9b98-3984-43b1-ad27-3058d6f4b5db","_cell_guid":"c81ac313-e56c-4c47-8de9-47c9ce51c9ba","trusted":true},"cell_type":"code","source":"# As this is a multiclass classification problem onehotencoding our Y.\nencoder = OneHotEncoder()\n\nfemale_Y = encoder.fit_transform(np.array(female_Y).reshape(-1,1)).toarray()\nmale_Y = encoder.fit_transform(np.array(male_Y).reshape(-1,1)).toarray()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"267c61f3-0658-46e9-bcde-889a8dff29e1","_cell_guid":"a7492ee6-3b5b-4a4d-ac73-facf7e5d3d35","trusted":true},"cell_type":"markdown","source":"## Splitting data"},{"metadata":{},"cell_type":"markdown","source":"Just for adding more proves that gender separation have sense"},{"metadata":{"trusted":true},"cell_type":"code","source":"nogender_X = np.concatenate((female_X, male_X))\nnogender_Y = np.concatenate((female_Y, male_Y))\n\nx_train, x_test, y_train, y_test = train_test_split(nogender_X, nogender_Y, random_state=0, test_size=0.20, shuffle=True)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca782f6d-060b-42b4-ab37-0ace11841509","_cell_guid":"7faf1771-a5e8-4e39-aff7-9177bfdb7eab","trusted":true},"cell_type":"code","source":"x_trainF, x_testF, y_trainF, y_testF = train_test_split(female_X, female_Y, random_state=0, test_size=0.20, shuffle=True)\nx_trainF.shape, y_trainF.shape, x_testF.shape, y_testF.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"094de711-d89e-49ab-8afa-57bb9a2856ab","_cell_guid":"989d4298-07cf-4875-81c8-f64950eb598a","trusted":true},"cell_type":"code","source":"x_trainM, x_testM, y_trainM, y_testM = train_test_split(male_X, male_Y, random_state=0, test_size=0.20, shuffle=True)\nx_trainM.shape, y_trainM.shape, x_testM.shape, y_testM.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87cbc221-1eed-48b5-8113-e6764762485e","_cell_guid":"cd518403-6a48-4430-b95c-7597b8708d8e","trusted":true},"cell_type":"markdown","source":"* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html  \nWe are going to scale our features throught the StandarScaler module, it standardize the features in a **Normal curve**, i.e.:<br><br>\n  <center> $Z = (X -{\\mu})/{\\sigma}$. </center><br>\n*Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance).*"},{"metadata":{"_uuid":"0ee2bea2-7ee3-43ce-ae2a-cd4a28f1c41f","_cell_guid":"f4640e2b-eaed-41d7-81b4-ba65bb8e0486","trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\n\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\n\nx_trainF = scaler.fit_transform(x_trainF)\nx_testF = scaler.transform(x_testF)\n\nx_trainM = scaler.fit_transform(x_trainM)\nx_testM = scaler.transform(x_testM)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"254dc31b-fb2b-4af2-b9e7-e3c0e9c0153e","_cell_guid":"365d8795-2307-48cf-b01c-a4028253126b","trusted":true},"cell_type":"markdown","source":"## Making our data compatible to model"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = np.expand_dims(x_train, axis=2)\nx_test = np.expand_dims(x_test, axis=2)\nx_train.shape, y_train.shape , x_test.shape , y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b6187b7-751b-4f3f-91ed-c114778cfc44","_cell_guid":"c22879c0-9f9f-4895-8ec2-2b5a5041842a","trusted":true},"cell_type":"code","source":"x_trainF = np.expand_dims(x_trainF, axis=2)\nx_testF = np.expand_dims(x_testF, axis=2)\nx_trainF.shape, y_trainF.shape, x_testF.shape, y_testF.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5fc3272f-e82f-4645-83bd-917876decca5","_cell_guid":"786a150e-181d-45d3-972d-1c688adcdb6d","trusted":true},"cell_type":"code","source":"x_trainM = np.expand_dims(x_trainM, axis=2)\nx_testM = np.expand_dims(x_testM, axis=2)\nx_trainM.shape, y_trainM.shape, x_testM.shape, y_testM.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a24a0e5-62b8-47a8-8623-fdb122fbb967","_cell_guid":"c9d8ce9e-3075-4a91-9fa2-bb6f468b3ab9","trusted":true},"cell_type":"markdown","source":"# <center>Modeling<center>"},{"metadata":{"_uuid":"276d227e-ccc8-47a8-b7f0-e1f9af5b01f8","_cell_guid":"3784657e-a214-4643-9b37-c1cdc0d779b5","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization, AveragePooling1D\nfrom keras.utils import np_utils, to_categorical\nfrom keras.callbacks import ModelCheckpoint","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02667e51-976b-45be-9424-c6031256dc5b","_cell_guid":"617f366b-db5a-4a5f-b2ec-c989d92220c7","trusted":true},"cell_type":"code","source":"print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f302020-e0cc-4583-8396-b65e53f9bdbc","_cell_guid":"03735553-216e-4a5d-932d-a7084ec7c3d3","trusted":true},"cell_type":"code","source":"# Create a MirroredStrategy.\nstrategy = tf.distribute.MirroredStrategy()\nprint('Number of devices: {}'.format(strategy.num_replicas_in_sync))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ed66eef-3c8d-40f6-bf6c-2dae3f074d7e","_cell_guid":"025457f6-da9c-49c5-820c-0258547185cb","trusted":true},"cell_type":"code","source":"with strategy.scope():\n    \n    def build_model(in_shape):\n        \n        model=Sequential()\n        model.add(Conv1D(256, kernel_size=6, strides=1, padding='same', activation='relu', input_shape=(in_shape, 1)))\n        model.add(AveragePooling1D(pool_size=4, strides = 2, padding = 'same'))\n\n        model.add(Conv1D(128, kernel_size=6, strides=1, padding='same', activation='relu'))\n        model.add(AveragePooling1D(pool_size=4, strides = 2, padding = 'same'))\n\n        model.add(Conv1D(128, kernel_size=6, strides=1, padding='same', activation='relu'))\n        model.add(AveragePooling1D(pool_size=4, strides = 2, padding = 'same'))\n        model.add(Dropout(0.2))\n\n        model.add(Conv1D(64, kernel_size=6, strides=1, padding='same', activation='relu'))\n        model.add(MaxPooling1D(pool_size=4, strides = 2, padding = 'same'))\n        \n        model.add(Flatten())\n        model.add(Dense(units=32, activation='relu'))\n        model.add(Dropout(0.3))\n\n        model.add(Dense(units=8, activation='softmax'))\n        model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n          \n        \n        return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca817c54-848e-4bc3-8182-48f792a3a616","_cell_guid":"0669ff45-5978-45bb-aa26-2e8f419f98e7","trusted":true},"cell_type":"code","source":"def model_build_summary(mod_dim, tr_features, val_features, val_labels):\n    model = build_model(mod_dim)\n    model.summary()\n    \n    score = model.evaluate(val_features, val_labels, verbose = 1)\n    accuracy = 100*score[1]\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4fcfecf-a921-45ee-a37a-ac8674d790e4","_cell_guid":"8df4fe0c-88ee-4170-9835-188a15b06574","trusted":true},"cell_type":"markdown","source":"*ReduceLROnPlateau* reduce learning rate when a metric has stopped improving.<br>\n\nModels often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This callback monitors a quantity and if no improvement is seen for a 'patience' number of epochs, the learning rate is reduced."},{"metadata":{"_uuid":"6f9617eb-6955-4264-86ff-16de5fa2b4ff","_cell_guid":"6f0e6211-dcfe-4ebc-abe8-57ad3d31c0fb","trusted":true},"cell_type":"code","source":"rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=4, min_lr=0.000001)\n\nbatch_size = 32\nn_epochs = 75","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"657294c0-b559-421f-9f50-3ea13b1ed691","_cell_guid":"b9dbc777-4949-4982-8534-48dc46cc6dcd","trusted":true},"cell_type":"code","source":"def show_graphs(history):\n    epochs = [i for i in range(n_epochs)]\n    fig , ax = plt.subplots(1,2)\n    train_acc = history.history['accuracy']\n    train_loss = history.history['loss']\n    test_acc = history.history['val_accuracy']\n    test_loss = history.history['val_loss']\n\n    fig.set_size_inches(30,12)\n    ax[0].plot(epochs , train_loss , label = 'Training Loss')\n    ax[0].plot(epochs , test_loss , label = 'Testing Loss')\n    ax[0].set_title('Training & Testing Loss')\n    ax[0].legend()\n    ax[0].set_xlabel(\"Epochs\")\n\n    ax[1].plot(epochs , train_acc , label = 'Training Accuracy')\n    ax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\n    ax[1].set_title('Training & Testing Accuracy')\n    ax[1].legend()\n    ax[1].set_xlabel(\"Epochs\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8db12a0-b5f5-4831-ae7c-ce3b80b64f30","_cell_guid":"d0162cd2-734a-45cf-bd6f-1d33037feb36","trusted":true},"cell_type":"markdown","source":"## Model Summary and Pre-training Accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"total_model = model_build_summary(x_train.shape[1], x_train, x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbc003e4-b7f4-4fd6-9540-67adf0a11d6a","_cell_guid":"853d7c4b-3942-4e2e-92c8-6f1416d07bfd","trusted":true},"cell_type":"code","source":"female_model = model_build_summary(x_trainF.shape[1], x_trainF, x_testF, y_testF)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4236892-65fe-43c6-a430-66fd9ce42fd3","_cell_guid":"dc2e6c56-87b3-4830-aa1d-2bb0ff9b4e9d","trusted":true},"cell_type":"code","source":"male_model = model_build_summary(x_trainM.shape[1], x_trainM, x_testM, y_testM)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1d4a537-a729-47ea-b935-4fe454580ecf","_cell_guid":"e2b79fb2-daf8-4a17-a012-a37263f9a8d0","trusted":true},"cell_type":"markdown","source":"## Model training"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = total_model.fit(x_train, y_train, batch_size=batch_size, epochs=n_epochs, validation_data=(x_test, y_test), callbacks=[rlrp])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dff9b1bf-1325-4ba2-936a-78cca34c36fb","_cell_guid":"683e39d7-fb4e-4c17-9e95-02de1ca51cd5","trusted":true},"cell_type":"code","source":"female_history = female_model.fit(x_trainF, y_trainF, batch_size=batch_size, epochs=n_epochs, validation_data=(x_testF, y_testF), callbacks=[rlrp])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf5377cb-11fe-458a-888e-dc9bdbcd2784","_cell_guid":"f731dbe4-7c4e-4dce-9d53-1acfdaa2b024","trusted":true},"cell_type":"code","source":"male_history = male_model.fit(x_trainM, y_trainM, batch_size=batch_size, epochs=n_epochs, validation_data=(x_testM, y_testM), callbacks=[rlrp])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Uncomment the code below to see the output of a specific layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nfrom keras import backend as K\n\nlayer_name = 'conv1d_11'\nintermediate_layer_model = keras.Model(inputs=female_model.input,\n                                       outputs=female_model.get_layer(layer_name).output)\nintermediate_output = intermediate_layer_model(x_testF)\nprint(intermediate_output[1,0])\n'''","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b58f999-e771-492f-b28a-ca7d010f73ab","_cell_guid":"dfe9a30b-84ad-4628-a18b-c3db6af78163","trusted":true},"cell_type":"markdown","source":"## Performance Evaluations"},{"metadata":{"trusted":true},"cell_type":"code","source":"# genderless\nscore = total_model.evaluate(x_train,y_train, verbose = 0)\nprint(\"Mixed-gender emotions training Accuracy: {0:.2%}\".format(score[1]))\n\nscore = total_model.evaluate(x_test, y_test, verbose=0)\nprint(\"Mixed-gender emotions testing Accuracy: {0:.2%}\".format(score[1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ed7d9cb-4602-4f7a-80d2-05cab4d6abb4","_cell_guid":"b3be869d-46c0-4d0b-b093-6ada4af95417","trusted":true},"cell_type":"code","source":"score = female_model.evaluate(x_trainF,y_trainF, verbose = 0)\nprint(\"Female emotions training Accuracy: {0:.2%}\".format(score[1]))\n\nscore = female_model.evaluate(x_testF, y_testF, verbose=0)\nprint(\"Female emotions testing Accuracy: {0:.2%}\".format(score[1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b0462f9-1f5b-4908-a3f0-315477dd0176","_cell_guid":"2efe6f55-a21c-4ae1-8c2f-eda2c9564469","trusted":true},"cell_type":"code","source":"score = male_model.evaluate(x_trainM,y_trainM, verbose = 0)\nprint(\"Male emotions training Accuracy: {0:.2%}\".format(score[1]))\n\nscore = male_model.evaluate(x_testM, y_testM, verbose=0)\nprint(\"Male emotions testing Accuracy: {0:.2%}\".format(score[1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"221e07a6-f3cb-41fb-a2c4-52536a15b8c7","_cell_guid":"2ae8241f-ac42-4312-ad18-2643b39f2ae8","trusted":true},"cell_type":"markdown","source":"## Training and Validation trends"},{"metadata":{"trusted":true},"cell_type":"code","source":"show_graphs(history)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be327c88-e087-4eb8-a928-a29c60f8633d","_cell_guid":"16efab44-61a0-4393-87e6-e8d379f853d7","trusted":true},"cell_type":"code","source":"show_graphs(female_history)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72f72742-ccc6-480a-88f0-97f8477639fb","_cell_guid":"26b8f452-bce7-422f-a988-16de7685da76","trusted":true},"cell_type":"code","source":"show_graphs(male_history)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aaa74df4-a2dd-460a-85d1-93dede24d13c","_cell_guid":"ce5b7034-fbdc-4ba5-be4f-ca91023e53bf","trusted":true},"cell_type":"markdown","source":"## Confusion matrix"},{"metadata":{"_uuid":"26fea659-f7ad-47eb-8e04-119ff1169329","_cell_guid":"4984e605-6e49-460c-a9e1-4042abc00f78","trusted":true},"cell_type":"code","source":"# predicting on test data.\npred_test = female_model.predict(x_testF)\ny_pred = encoder.inverse_transform(pred_test)\ny_test_ = encoder.inverse_transform(y_testF)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f083eaea-21b0-49a3-bea9-e693e7a77453","_cell_guid":"08b69691-4a7e-4c71-961b-ab16c020eb79","trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test_, y_pred)\nplt.figure(figsize = (12, 10))\ncm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\nsns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\nplt.title('Confusion Matrix for Female Emotions', size=20)\nplt.xlabel('Predicted Labels', size=14)\nplt.ylabel('Actual Labels', size=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9afc111-20f1-4a9b-b302-f3fdd0d1e1a7","_cell_guid":"27a1522a-d7f3-40f3-a621-e6aadb093de7","trusted":true},"cell_type":"code","source":"# predicting on test data.\npred_test = male_model.predict(x_testM)\ny_pred = encoder.inverse_transform(pred_test)\ny_test_ = encoder.inverse_transform(y_testM)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e45908f7-d67f-4442-92cd-11d50f224aa6","_cell_guid":"678e9d91-862a-4159-9877-caa275302c8e","trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test_, y_pred)\nplt.figure(figsize = (12, 10))\ncm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\nsns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\nplt.title('Confusion Matrix for Male Emotions', size=20)\nplt.xlabel('Predicted Labels', size=14)\nplt.ylabel('Actual Labels', size=14)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}