{"cells":[{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install spacy-langdetect\n!pip install language-detector\n!pip install symspellpy\n!pip install sentence-transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#covid-papers-browser/models/scibert-nli/","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Topic Modeling with BERT, LDA, and Clustering\n#### Latent Dirichlet Allocation (LDA) probabilistic topic assignment and pre-trained sentence embeddings from BERT/RoBERTa"},{"metadata":{},"cell_type":"markdown","source":"**Insired by:** \n\nShoa, S. (2020). **Contextual Topic Identification: Identifying meaningful topics for sparse Steam reviews**. Medium. Available at: https://blog.insightdatascience.com/contextual-topic-identification-4291d256a032 [Accessed 25 Mar. 2020]."},{"metadata":{},"cell_type":"markdown","source":"## Description\n\nIn response to the COVID-19 pandemic, the White House and a coalition of leading research groups have prepared the COVID-19 Open Research Dataset (CORD-19). CORD-19 is a resource of over 44,000 scholarly articles, including over 29,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses."},{"metadata":{},"cell_type":"markdown","source":"![](https://static.seattletimes.com/wp-content/uploads/2020/03/coronavirus-mask-illo-color-768x557.jpg)"},{"metadata":{},"cell_type":"markdown","source":"## Meet our team\n\n* [William Green](https://www.kaggle.com/dskswu)\n* [Frank Mitchell](https://www.kaggle.com/fmitchell259)\n* [Salmon Chen](https://www.kaggle.com/tianyeeee)\n* [Aarti](https://www.kaggle.com/aarti9)\n"},{"metadata":{},"cell_type":"markdown","source":"![](http://)"},{"metadata":{},"cell_type":"markdown","source":"## Model Deep Dive\n\nThe author used: \n\n* LDA for probabilistic topic assignment vector.\n* Bert for sentence embedding vector.\n\n1. Concatenated both LDA and Bert vectors with a weight hyperparameter to balance the relative importance of information from each source.\n2. Used autoencoder to learn a lower dimensional latent space representation of the concatenated vector.\n\n* The assumption is that the concatendate vector shoul have a manifold shaep in the high dimensional space. \n* USed clustering on the latent space representations to get topics. \n\n"},{"metadata":{},"cell_type":"markdown","source":"![Contextual Topic Identification model design](https://miro.medium.com/max/1410/1*OKCYnB-JbGq1NDwNSKd5Zw.png)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/mallet'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing all dependencies.\nimport os\nimport json\nimport pandas as pd\nfrom tqdm import tqdm\nimport numpy as np\nfrom nltk.corpus import wordnet\nimport re\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords \n#from geotext import GeoText\n#/kaggle/input","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Upload Data"},{"metadata":{},"cell_type":"markdown","source":"**Data pipeline (from development to deployment**)\n\n![Data pipeline (from development to deployment)](https://miro.medium.com/max/1348/1*Cdp4y1tfMxqoj96o6lUdFg.png)\n\n\nSource:Shoa "},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/jieyang0311/covid-19-topic-modeling-lda\n#load library\nimport os\nimport pandas as pd\nimport numpy as np\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim import corpora, models\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\n\nimport datetime\nimport time\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nimport nltk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta = pd.read_csv(\"/kaggle/input/CORD-19-research-challenge/metadata.csv\")\nprint(meta.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### first filter by meta file. select only papers after 2020\nmeta[\"publish_time\"] = pd.to_datetime(meta[\"publish_time\"])\nmeta[\"publish_year\"] = (pd.DatetimeIndex(meta['publish_time']).year)\nmeta[\"publish_month\"] = (pd.DatetimeIndex(meta['publish_time']).month)\nmeta = meta[meta[\"publish_year\"] == 2020]\nprint(meta.shape[0], \" papers are available after 2020 Jan 1.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#count how many has abstract\ncount = 0\nindex = []\nfor i in range(len(meta)):\n    #print(i)\n    if type(meta.iloc[i, 8])== float:\n        count += 1\n    else:\n        index.append(i)\n\nprint(len(index), \" papers have abstract available.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##extract the abstract to pandas \ndocuments = meta.iloc[index, 8]\ndocuments=documents.reset_index()\ndocuments.drop(\"index\", inplace = True, axis = 1)\n\n##create pandas data frame with all abstracts, use as input corpus\ndocuments[\"index\"] = documents.index.values\ndocuments.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"documents.head(15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Utils"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nfrom sklearn.metrics import silhouette_score\nimport umap\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom gensim.models.coherencemodel import CoherenceModel\nimport numpy as np\nimport os\n\n\ndef get_topic_words(token_lists, labels, k=None):\n    \"\"\"\n    get top words within each topic from clustering results\n    \"\"\"\n    if k is None:\n        k = len(np.unique(labels))\n    topics = ['' for _ in range(k)]\n    for i, c in enumerate(token_lists):\n        topics[labels[i]] += (' ' + ' '.join(c))\n    word_counts = list(map(lambda x: Counter(x.split()).items(), topics))\n    # get sorted word counts\n    word_counts = list(map(lambda x: sorted(x, key=lambda x: x[1], reverse=True), word_counts))\n    # get topics\n    topics = list(map(lambda x: list(map(lambda x: x[0], x[:10])), word_counts))\n\n    return topics\n\ndef get_coherence(model, token_lists, measure='c_v'):\n    \"\"\"\n    Get model coherence from gensim.models.coherencemodel\n    :param model: Topic_Model object\n    :param token_lists: token lists of docs\n    :param topics: topics as top words\n    :param measure: coherence metrics\n    :return: coherence score\n    \"\"\"\n    if model.method == 'LDA':\n        cm = CoherenceModel(model=model.ldamodel, texts=token_lists, corpus=model.corpus, dictionary=model.dictionary,\n                            coherence=measure)\n    else:\n        topics = get_topic_words(token_lists, model.cluster_model.labels_)\n        cm = CoherenceModel(topics=topics, texts=token_lists, corpus=model.corpus, dictionary=model.dictionary,\n                            coherence=measure)\n    return cm.get_coherence()\n\ndef get_silhouette(model):\n    \"\"\"\n    Get silhouette score from model\n    :param model: Topic_Model object\n    :return: silhouette score\n    \"\"\"\n    if model.method == 'LDA':\n        return\n    lbs = model.cluster_model.labels_\n    vec = model.vec[model.method]\n    return silhouette_score(vec, lbs)\n\ndef plot_proj(embedding, lbs):\n    \"\"\"\n    Plot UMAP embeddings\n    :param embedding: UMAP (or other) embeddings\n    :param lbs: labels\n    \"\"\"\n    n = len(embedding)\n    counter = Counter(lbs)\n    for i in range(len(np.unique(lbs))):\n        plt.plot(embedding[:, 0][lbs == i], embedding[:, 1][lbs == i], '.', alpha=0.5,\n                 label='cluster {}: {:.2f}%'.format(i, counter[i] / n * 100))\n    plt.legend(loc = 'best')\n    plt.grid(color ='grey', linestyle='-',linewidth = 0.25)\n\n\ndef visualize(model):\n    \"\"\"\n    Visualize the result for the topic model by 2D embedding (UMAP)\n    :param model: Topic_Model object\n    \"\"\"\n    if model.method == 'LDA':\n        return\n    reducer = umap.UMAP()\n    print('Calculating UMAP projection ...')\n    vec_umap = reducer.fit_transform(model.vec[model.method])\n    print('Calculating UMAP projection. Done!')\n    plot_proj(vec_umap, model.cluster_model.labels_)\n    dr = '/kaggle/working/contextual_topic_identification/docs/images/{}/{}'.format(model.method, model.id)\n    if not os.path.exists(dr):\n        os.makedirs(dr)\n    plt.savefig('/kaggle/working/2D_vis')\n\ndef get_wordcloud(model, token_lists, topic):\n    \"\"\"\n    Get word cloud of each topic from fitted model\n    :param model: Topic_Model object\n    :param sentences: preprocessed sentences from docs\n    \"\"\"\n    if model.method == 'LDA':\n        return\n    print('Getting wordcloud for topic {} ...'.format(topic))\n    lbs = model.cluster_model.labels_\n    tokens = ' '.join([' '.join(_) for _ in np.array(token_lists)[lbs == topic]])\n\n    wordcloud = WordCloud(width=800, height=560,\n                          background_color='white', collocations=False,\n                          min_font_size=10).generate(tokens)\n\n    # plot the WordCloud image\n    plt.figure(figsize=(8, 5.6), facecolor=None)\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.tight_layout(pad=0)\n    dr = '/kaggle/working/{}/{}'.format(model.method, model.id)\n    if not os.path.exists(dr):\n        os.makedirs(dr)\n    plt.savefig('/kaggle/working' + '/Topic' + str(topic) + '_wordcloud')\n    print('Getting wordcloud for topic {}. Done!'.format(topic))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing "},{"metadata":{"trusted":true},"cell_type":"code","source":"from stop_words import get_stop_words\nfrom nltk.stem.porter import PorterStemmer\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom language_detector import detect_language\n\nimport pkg_resources\nfrom symspellpy import SymSpell, Verbosity\n\nsym_spell = SymSpell(max_dictionary_edit_distance=3, prefix_length=7)\ndictionary_path = pkg_resources.resource_filename(\n    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\nif sym_spell.word_count:\n    pass\nelse:\n    sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n\n\n###################################\n#### sentence level preprocess ####\n###################################\n\n# lowercase + base filter\n# some basic normalization\ndef f_base(s):\n    \"\"\"\n    :param s: string to be processed\n    :return: processed string: see comments in the source code for more info\n    \"\"\"\n    # normalization 1: xxxThis is a --> xxx. This is a (missing delimiter)\n    s = re.sub(r'([a-z])([A-Z])', r'\\1\\. \\2', s)  # before lower case\n    # normalization 2: lower case\n    s = s.lower()\n    # normalization 3: \"&gt\", \"&lt\"\n    s = re.sub(r'&gt|&lt', ' ', s)\n    # normalization 4: letter repetition (if more than 2)\n    s = re.sub(r'([a-z])\\1{2,}', r'\\1', s)\n    # normalization 5: non-word repetition (if more than 1)\n    s = re.sub(r'([\\W+])\\1{1,}', r'\\1', s)\n    # normalization 6: string * as delimiter\n    s = re.sub(r'\\*|\\W\\*|\\*\\W', '. ', s)\n    # normalization 7: stuff in parenthesis, assumed to be less informal\n    s = re.sub(r'\\(.*?\\)', '. ', s)\n    # normalization 8: xxx[?!]. -- > xxx.\n    s = re.sub(r'\\W+?\\.', '.', s)\n    # normalization 9: [.?!] --> [.?!] xxx\n    s = re.sub(r'(\\.|\\?|!)(\\w)', r'\\1 \\2', s)\n    # normalization 10: ' ing ', noise text\n    s = re.sub(r' ing ', ' ', s)\n    # normalization 11: noise text\n    s = re.sub(r'product received for free[.| ]', ' ', s)\n    # normalization 12: phrase repetition\n    s = re.sub(r'(.{2,}?)\\1{1,}', r'\\1', s)\n\n    return s.strip()\n\n\n# language detection\ndef f_lan(s):\n    \"\"\"\n    :param s: string to be processed\n    :return: boolean (s is English)\n    \"\"\"\n\n    # some reviews are actually english but biased toward french\n    return detect_language(s) in {'English', 'French','Spanish','Chinese'}\n\n\n###############################\n#### word level preprocess ####\n###############################\n\n# filtering out punctuations and numbers\ndef f_punct(w_list):\n    \"\"\"\n    :param w_list: word list to be processed\n    :return: w_list with punct and number filter out\n    \"\"\"\n    return [word for word in w_list if word.isalpha()]\n\n\n# selecting nouns\ndef f_noun(w_list):\n    \"\"\"\n    :param w_list: word list to be processed\n    :return: w_list with only nouns selected\n    \"\"\"\n    return [word for (word, pos) in nltk.pos_tag(w_list) if pos[:2] == 'NN']\n\n\n# typo correction\ndef f_typo(w_list):\n    \"\"\"\n    :param w_list: word list to be processed\n    :return: w_list with typo fixed by symspell. words with no match up will be dropped\n    \"\"\"\n    w_list_fixed = []\n    for word in w_list:\n        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=3)\n        if suggestions:\n            w_list_fixed.append(suggestions[0].term)\n        else:\n            pass\n            # do word segmentation, deprecated for inefficiency\n            # w_seg = sym_spell.word_segmentation(phrase=word)\n            # w_list_fixed.extend(w_seg.corrected_string.split())\n    return w_list_fixed\n\n\n# stemming if doing word-wise\np_stemmer = PorterStemmer()\n\n\ndef f_stem(w_list):\n    \"\"\"\n    :param w_list: word list to be processed\n    :return: w_list with stemming\n    \"\"\"\n    return [p_stemmer.stem(word) for word in w_list]\n\n\n# filtering out stop words\n# create English stop words list\n\nstop_words = (list(\n    set(get_stop_words('en'))\n    |set(get_stop_words('es'))\n    |set(get_stop_words('de'))\n    |set(get_stop_words('it'))\n    |set(get_stop_words('ca'))\n    #|set(get_stop_words('cy'))\n    |set(get_stop_words('pt'))\n    #|set(get_stop_words('tl'))\n    |set(get_stop_words('pl'))\n    #|set(get_stop_words('et'))\n    |set(get_stop_words('da'))\n    |set(get_stop_words('ru'))\n    #|set(get_stop_words('so'))\n    |set(get_stop_words('sv'))\n    |set(get_stop_words('sk'))\n    #|set(get_stop_words('cs'))\n    |set(get_stop_words('nl'))\n    #|set(get_stop_words('sl'))\n    #|set(get_stop_words('no'))\n    #|set(get_stop_words('zh-cn'))\n))\n\n\n\n\n\ndef f_stopw(w_list):\n    \"\"\"\n    filtering out stop words\n    \"\"\"\n    return [word for word in w_list if word not in stop_words]\n\n\ndef preprocess_sent(rw):\n    \"\"\"\n    Get sentence level preprocessed data from raw review texts\n    :param rw: review to be processed\n    :return: sentence level pre-processed review\n    \"\"\"\n    s = f_base(rw)\n    if not f_lan(s):\n        return None\n    return s\n\n\ndef preprocess_word(s):\n    \"\"\"\n    Get word level preprocessed data from preprocessed sentences\n    including: remove punctuation, select noun, fix typo, stem, stop_words\n    :param s: sentence to be processed\n    :return: word level pre-processed review\n    \"\"\"\n    if not s:\n        return None\n    w_list = word_tokenize(s)\n    w_list = f_punct(w_list)\n    w_list = f_noun(w_list)\n    w_list = f_typo(w_list)\n    w_list = f_stem(w_list)\n    w_list = f_stopw(w_list)\n\n    return w_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Autoencoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nclass Autoencoder:\n    \"\"\"\n    Autoencoder for learning latent space representation\n    architecture simplified for only one hidden layer\n    \"\"\"\n\n    def __init__(self, latent_dim=32, activation='relu', epochs=200, batch_size=128):\n        self.latent_dim = latent_dim\n        self.activation = activation\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.autoencoder = None\n        self.encoder = None\n        self.decoder = None\n        self.his = None\n\n    def _compile(self, input_dim):\n        \"\"\"\n        compile the computational graph\n        \"\"\"\n        input_vec = Input(shape=(input_dim,))\n        encoded = Dense(self.latent_dim, activation=self.activation)(input_vec)\n        decoded = Dense(input_dim, activation=self.activation)(encoded)\n        self.autoencoder = Model(input_vec, decoded)\n        self.encoder = Model(input_vec, encoded)\n        encoded_input = Input(shape=(self.latent_dim,))\n        decoder_layer = self.autoencoder.layers[-1]\n        self.decoder = Model(encoded_input, self.autoencoder.layers[-1](encoded_input))\n        self.autoencoder.compile(optimizer='adam', loss=keras.losses.mean_squared_error)\n\n    def fit(self, X):\n        if not self.autoencoder:\n            self._compile(X.shape[1])\n        X_train, X_test = train_test_split(X)\n        self.his = self.autoencoder.fit(X_train, X_train,\n                                        epochs=200,\n                                        batch_size=128,\n                                        shuffle=True,\n                                        validation_data=(X_test, X_test), verbose=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom gensim import corpora\nimport gensim\nimport numpy as np\n#from Autoencoder import *\n#from preprocess import *\nfrom datetime import datetime\n\n\ndef preprocess(docs, samp_size=None):\n    \"\"\"\n    Preprocess the data\n    \"\"\"\n    if not samp_size:\n        samp_size = 100\n\n    print('Preprocessing raw texts ...')\n    n_docs = len(docs)\n    sentences = []  # sentence level preprocessed\n    token_lists = []  # word level preprocessed\n    idx_in = []  # index of sample selected\n    #     samp = list(range(100))\n    samp = np.random.choice(n_docs, samp_size)\n    for i, idx in enumerate(samp):\n        sentence = preprocess_sent(docs[idx])\n        token_list = preprocess_word(sentence)\n        if token_list:\n            idx_in.append(idx)\n            sentences.append(sentence)\n            token_lists.append(token_list)\n        print('{} %'.format(str(np.round((i + 1) / len(samp) * 100, 2))), end='\\r')\n    print('Preprocessing raw texts. Done!')\n    return sentences, token_lists, idx_in\n\n\n# define model object\nclass Topic_Model:\n    def __init__(self, k=10, method='TFIDF'):\n        \"\"\"\n        :param k: number of topics\n        :param method: method chosen for the topic model\n        \"\"\"\n        if method not in {'TFIDF', 'LDA', 'BERT', 'LDA_BERT'}:\n            raise Exception('Invalid method!')\n        self.k = k\n        self.dictionary = None\n        self.corpus = None\n        #         self.stopwords = None\n        self.cluster_model = None\n        self.ldamodel = None\n        self.vec = {}\n        self.gamma = 15  # parameter for reletive importance of lda\n        self.method = method\n        self.AE = None\n        self.id = method + '_' + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n\n    def vectorize(self, sentences, token_lists, method=None):\n        \"\"\"\n        Get vecotr representations from selected methods\n        \"\"\"\n        # Default method\n        if method is None:\n            method = self.method\n\n        # turn tokenized documents into a id <-> term dictionary\n        self.dictionary = corpora.Dictionary(token_lists)\n        # convert tokenized documents into a document-term matrix\n        self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n\n        if method == 'TFIDF':\n            print('Getting vector representations for TF-IDF ...')\n            tfidf = TfidfVectorizer()\n            vec = tfidf.fit_transform(sentences)\n            print('Getting vector representations for TF-IDF. Done!')\n            return vec\n\n        elif method == 'LDA':\n            print('Getting vector representations for LDA ...')\n            if not self.ldamodel:\n                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary,\n                                                                passes=20)\n\n            def get_vec_lda(model, corpus, k):\n                \"\"\"\n                Get the LDA vector representation (probabilistic topic assignments for all documents)\n                :return: vec_lda with dimension: (n_doc * n_topic)\n                \"\"\"\n                n_doc = len(corpus)\n                vec_lda = np.zeros((n_doc, k))\n                for i in range(n_doc):\n                    # get the distribution for the i-th document in corpus\n                    for topic, prob in model.get_document_topics(corpus[i]):\n                        vec_lda[i, topic] = prob\n\n                return vec_lda\n\n            vec = get_vec_lda(self.ldamodel, self.corpus, self.k)\n            print('Getting vector representations for LDA. Done!')\n            return vec\n\n        elif method == 'BERT':\n\n            print('Getting vector representations for BERT ...')\n            from sentence_transformers import SentenceTransformer\n            model = SentenceTransformer('bert-base-nli-max-tokens')\n            vec = np.array(model.encode(sentences, show_progress_bar=True))\n            print('Getting vector representations for BERT. Done!')\n            return vec\n\n             \n        elif method == 'LDA_BERT':\n        #else:\n            vec_lda = self.vectorize(sentences, token_lists, method='LDA')\n            vec_bert = self.vectorize(sentences, token_lists, method='BERT')\n            vec_ldabert = np.c_[vec_lda * self.gamma, vec_bert]\n            self.vec['LDA_BERT_FULL'] = vec_ldabert\n            if not self.AE:\n                self.AE = Autoencoder()\n                print('Fitting Autoencoder ...')\n                self.AE.fit(vec_ldabert)\n                print('Fitting Autoencoder Done!')\n            vec = self.AE.encoder.predict(vec_ldabert)\n            return vec\n\n    def fit(self, sentences, token_lists, method=None, m_clustering=None):\n        \"\"\"\n        Fit the topic model for selected method given the preprocessed data\n        :docs: list of documents, each doc is preprocessed as tokens\n        :return:\n        \"\"\"\n        # Default method\n        if method is None:\n            method = self.method\n        # Default clustering method\n        if m_clustering is None:\n            m_clustering = KMeans\n\n        # turn tokenized documents into a id <-> term dictionary\n        if not self.dictionary:\n            self.dictionary = corpora.Dictionary(token_lists)\n            # convert tokenized documents into a document-term matrix\n            self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n\n        ####################################################\n        #### Getting ldamodel or vector representations ####\n        ####################################################\n\n        if method == 'LDA':\n            if not self.ldamodel:\n                print('Fitting LDA ...')\n                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary,\n                                                                passes=20)\n                print('Fitting LDA Done!')\n        else:\n            print('Clustering embeddings ...')\n            self.cluster_model = m_clustering(self.k)\n            self.vec[method] = self.vectorize(sentences, token_lists, method)\n            self.cluster_model.fit(self.vec[method])\n            print('Clustering embeddings. Done!')\n\n    def predict(self, sentences, token_lists, out_of_sample=None):\n        \"\"\"\n        Predict topics for new_documents\n        \"\"\"\n        # Default as False\n        out_of_sample = out_of_sample is not None\n\n        if out_of_sample:\n            corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n            if self.method != 'LDA':\n                vec = self.vectorize(sentences, token_lists)\n                print(vec)\n        else:\n            corpus = self.corpus\n            vec = self.vec.get(self.method, None)\n\n        if self.method == \"LDA\":\n            lbs = np.array(list(map(lambda x: sorted(self.ldamodel.get_document_topics(x),\n                                                     key=lambda x: x[1], reverse=True)[0][0],\n                                    corpus)))\n        else:\n            lbs = self.cluster_model.predict(vec)\n        return lbs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"#from model import *\n#from utils import *\n\nimport pandas as pd\nimport pickle\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore', category=Warning)\n\nimport argparse\n\n#def model(): #:if __name__ == '__main__':\n\ndef main():\n    \n    \n    method = \"LDA_BERT\"\n    samp_size = 51000\n    ntopic = 10\n    \n    #parser = argparse.ArgumentParser(description='contextual_topic_identification tm_test:1.0')\n\n    #parser.add_argument('--fpath', default='/kaggle/working/train.csv')\n    #parser.add_argument('--ntopic', default=10,)\n    #parser.add_argument('--method', default='TFIDF')\n    #parser.add_argument('--samp_size', default=20500)\n    \n    #args = parser.parse_args()\n\n    data = documents #pd.read_csv('/kaggle/working/train.csv')\n    data = data.fillna('')  # only the comments has NaN's\n    rws = data.abstract\n    sentences, token_lists, idx_in = preprocess(rws, samp_size=samp_size)\n    # Define the topic model object\n    #tm = Topic_Model(k = 10), method = TFIDF)\n    tm = Topic_Model(k = ntopic, method = method)\n    # Fit the topic model by chosen method\n    tm.fit(sentences, token_lists)\n    # Evaluate using metrics\n    with open(\"/kaggle/working/{}.file\".format(tm.id), \"wb\") as f:\n        pickle.dump(tm, f, pickle.HIGHEST_PROTOCOL)\n\n    print('Coherence:', get_coherence(tm, token_lists, 'c_v'))\n    print('Silhouette Score:', get_silhouette(tm))\n    # visualize and save img\n    visualize(tm)\n    for i in range(tm.k):\n        get_wordcloud(tm, token_lists, i)\n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TODO:\n\n* Implement models.ldamulticore â€“ parallelized Latent Dirichlet Allocation using all CPU cores to parallelize and speed up model training.\n* Switch from BERT/RoBERTa to SciBERT, BART, and or other models. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}