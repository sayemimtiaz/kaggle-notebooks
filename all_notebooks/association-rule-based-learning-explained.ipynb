{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Association Rule Mining - Market Basket Analysis with Mlxtend","metadata":{}},{"cell_type":"markdown","source":"# 1. Intro","metadata":{}},{"cell_type":"markdown","source":"**Have you ever considered the techniques behind the online recommendation systems or product placement strategies ?** <br><br>\n**If your answer is 'Yes', you are definetely at right place !**","metadata":{}},{"cell_type":"markdown","source":"<img width=\"1000\" height=\"400\" alt=\"netflix\" align=\"left\" src=\"https://user-images.githubusercontent.com/36535914/78720348-369df700-792e-11ea-9950-8f5731409171.png\">\n***","metadata":{}},{"cell_type":"markdown","source":"Association rule learning is a rule-based machine learning method for discovering relations between variables in large databases. Goal is to identify strong relations discovered in datasets using some measures such as confidence or lift.\n\nAn association rule is an implication expression of the form X→Y, where X and Y are seperate itemsets. A more concrete example based on consumer behaviour would be {Diapers}→{Beer} suggesting that people who buy diapers are also likely to buy beer. To evaluate the \"interest\" of such an association rule, different metrics have been developed. The current implementation make use of the confidence and lift metrics as which we just mentioned above","metadata":{}},{"cell_type":"markdown","source":"- ***If a customer buys bread, he’s 70% likely of buying milk***","metadata":{}},{"cell_type":"markdown","source":"In the above association rule, bread is the antecedent and milk is the consequent. Simply put, it can be understood as a retail store’s association rule to target their customers better. If the above rule is a result of a thorough analysis of some data sets, it can be used to not only improve customer service but also improve the company’s revenue.\n\nAdditional to above the association rule based learning techniques are also used in many applications such as recommendation systems, medical diagnosis, protein sequence, census data or even crime prevention, [click here for details ](https://www.upgrad.com/blog/most-common-examples-of-data-mining/)\n\nThe Apriori algorithm is one of the main technique of association rule mining, which can be basically descibed as finding the most frequent itemsets in a dataset. We will be using that algorithm as well in our tutorial.\n\nSo, keep reading for details ! ","metadata":{}},{"cell_type":"markdown","source":"# 2. Data Load","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport networkx as nx\nfrom mlxtend.preprocessing import TransactionEncoder\nfrom mlxtend.frequent_patterns import apriori,association_rules\nimport matplotlib.pyplot as plt\nplt.style.use('default')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-22T15:58:47.470338Z","iopub.execute_input":"2021-07-22T15:58:47.470981Z","iopub.status.idle":"2021-07-22T15:58:47.47848Z","shell.execute_reply.started":"2021-07-22T15:58:47.47094Z","shell.execute_reply":"2021-07-22T15:58:47.477758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"../input/d/hemanthkumar05/market-basket-optimization/Market_Basket_Optimisation.csv\", header=None)\ndata.shape","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-07-22T15:58:47.480304Z","iopub.execute_input":"2021-07-22T15:58:47.480687Z","iopub.status.idle":"2021-07-22T15:58:47.522011Z","shell.execute_reply.started":"2021-07-22T15:58:47.480656Z","shell.execute_reply":"2021-07-22T15:58:47.521296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-22T15:58:47.523143Z","iopub.execute_input":"2021-07-22T15:58:47.523578Z","iopub.status.idle":"2021-07-22T15:58:47.548432Z","shell.execute_reply.started":"2021-07-22T15:58:47.523546Z","shell.execute_reply":"2021-07-22T15:58:47.547414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-22T15:58:47.551351Z","iopub.execute_input":"2021-07-22T15:58:47.551759Z","iopub.status.idle":"2021-07-22T15:58:47.673723Z","shell.execute_reply.started":"2021-07-22T15:58:47.551702Z","shell.execute_reply":"2021-07-22T15:58:47.672704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[1]","metadata":{"execution":{"iopub.status.busy":"2021-07-22T15:58:47.676492Z","iopub.execute_input":"2021-07-22T15:58:47.676855Z","iopub.status.idle":"2021-07-22T15:58:47.686128Z","shell.execute_reply.started":"2021-07-22T15:58:47.676806Z","shell.execute_reply":"2021-07-22T15:58:47.685127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Data Visualizations","metadata":{}},{"cell_type":"markdown","source":"***- The most demanded items in dataset / Top10***","metadata":{}},{"cell_type":"code","source":"# 1. Gather All Items of Each Transactions into Numpy Array\ntransaction = []\nfor i in range(0, data.shape[0]):\n    for j in range(0, data.shape[1]):\n        transaction.append(data.values[i,j])\n\ntransaction = np.array(transaction)\n\n# 2. Transform Them a Pandas DataFrame\ndf = pd.DataFrame(transaction, columns=[\"items\"]) \ndf[\"incident_count\"] = 1 # Put 1 to Each Item For Making Countable Table, to be able to perform Group By\n\n# 3. Delete NaN Items from Dataset\nindexNames = df[df['items'] == \"nan\" ].index\ndf.drop(indexNames , inplace=True)\n\n# 4. Final Step: Make a New Appropriate Pandas DataFrame for Visualizations  \ndf_table = df.groupby(\"items\").sum().sort_values(\"incident_count\", ascending=False).reset_index()\n\n# 5. Initial Visualizations\ndf_table.head(10).style.background_gradient(cmap='Blues')","metadata":{"execution":{"iopub.status.busy":"2021-07-22T15:58:47.68802Z","iopub.execute_input":"2021-07-22T15:58:47.68853Z","iopub.status.idle":"2021-07-22T15:58:49.956649Z","shell.execute_reply.started":"2021-07-22T15:58:47.688482Z","shell.execute_reply":"2021-07-22T15:58:49.955347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***- The most demanded items in dataset / Top30***","metadata":{}},{"cell_type":"code","source":"df_table[\"all\"] = \"all\" # to have a same origin\n\nfig = px.treemap(df_table.head(30), path=['all', \"items\"], values='incident_count',\n                  color=df_table[\"incident_count\"].head(30), hover_data=['items'],\n                  color_continuous_scale='Blues',\n                  )\nfig.show()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-07-22T15:58:49.958376Z","iopub.execute_input":"2021-07-22T15:58:49.958748Z","iopub.status.idle":"2021-07-22T15:58:50.411847Z","shell.execute_reply.started":"2021-07-22T15:58:49.958692Z","shell.execute_reply":"2021-07-22T15:58:50.410872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***- Lets check whether the items have multiple records in a transaction or not***<br>\n***- If the answer is \"Yes\", we need to handle them since they might mislead the apriori algorithm in further steps***","metadata":{}},{"cell_type":"code","source":"# Transform Every Transaction to Seperate List & Gather Them into Numpy Array\n# By Doing So, We Will Be Able To Iterate Through Array of Transactions\n\ntransaction = []\nfor i in range(data.shape[0]):\n    transaction.append([str(data.values[i,j]) for j in range(data.shape[1])])\n    \ntransaction = np.array(transaction)\n\n# Create a DataFrame In Order To Check Status of Top20 Items\n\ntop20 = df_table[\"items\"].head(20).values\narray = []\ndf_top20_multiple_record_check = pd.DataFrame(columns=top20)\n\nfor i in range(0, len(top20)):\n    array = []\n    for j in range(0,transaction.shape[0]):\n        array.append(np.count_nonzero(transaction[j]==top20[i]))\n        if len(array) == len(data):\n            df_top20_multiple_record_check[top20[i]] = array\n        else:\n            continue\n            \n\ndf_top20_multiple_record_check.head(10)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-22T15:58:50.413326Z","iopub.execute_input":"2021-07-22T15:58:50.413569Z","iopub.status.idle":"2021-07-22T15:58:53.788289Z","shell.execute_reply.started":"2021-07-22T15:58:50.413535Z","shell.execute_reply":"2021-07-22T15:58:53.787354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_top20_multiple_record_check.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-22T15:58:53.789669Z","iopub.execute_input":"2021-07-22T15:58:53.789916Z","iopub.status.idle":"2021-07-22T15:58:53.876175Z","shell.execute_reply.started":"2021-07-22T15:58:53.789888Z","shell.execute_reply":"2021-07-22T15:58:53.875331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- ***As you see above, only Chocolate has a max value of 2. The others have max value of 1. From that reason we can say that the data is homogenous, we can proceed without any inference.***","metadata":{}},{"cell_type":"markdown","source":"***- Choice Analysis / Customers' First Choices***","metadata":{}},{"cell_type":"code","source":"# 1. Gather Only First Choice of Each Transactions into Numpy Array\n# Similar Pattern to Above, Only Change is the Column Number \"0\" in Append Function\ntransaction = []\nfor i in range(0, data.shape[0]):\n    transaction.append(data.values[i,0])\n\ntransaction = np.array(transaction)\n\n# 2. Transform Them a Pandas DataFrame\ndf_first = pd.DataFrame(transaction, columns=[\"items\"])\ndf_first[\"incident_count\"] = 1\n\n# 3. Delete NaN Items from Dataset\nindexNames = df_first[df_first['items'] == \"nan\" ].index\ndf_first.drop(indexNames , inplace=True)\n\n# 4. Final Step: Make a New Appropriate Pandas DataFrame for Visualizations  \ndf_table_first = df_first.groupby(\"items\").sum().sort_values(\"incident_count\", ascending=False).reset_index()\ndf_table_first[\"food\"] = \"food\"\ndf_table_first = df_table_first.truncate(before=-1, after=15) # Fist 15 Choice\n","metadata":{"execution":{"iopub.status.busy":"2021-07-22T15:58:53.877578Z","iopub.execute_input":"2021-07-22T15:58:53.877819Z","iopub.status.idle":"2021-07-22T15:58:54.010438Z","shell.execute_reply.started":"2021-07-22T15:58:53.877792Z","shell.execute_reply":"2021-07-22T15:58:54.009504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams['figure.figsize'] = (20, 20)\nfirst_choice = nx.from_pandas_edgelist(df_table_first, source = 'food', target = \"items\", edge_attr = True)\npos = nx.spring_layout(first_choice)\nnx.draw_networkx_nodes(first_choice, pos, node_size = 12500, node_color = \"lavender\")\nnx.draw_networkx_edges(first_choice, pos, width = 3, alpha = 0.6, edge_color = 'black')\nnx.draw_networkx_labels(first_choice, pos, font_size = 18, font_family = 'sans-serif')\nplt.axis('off')\nplt.grid()\nplt.title('Top First Choices', fontsize = 25)\nplt.show()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-07-22T15:58:54.011685Z","iopub.execute_input":"2021-07-22T15:58:54.011905Z","iopub.status.idle":"2021-07-22T15:58:54.45148Z","shell.execute_reply.started":"2021-07-22T15:58:54.011879Z","shell.execute_reply":"2021-07-22T15:58:54.450626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***- Choice Analysis / Customers' Second Choices***","metadata":{}},{"cell_type":"code","source":"# 1. Gather Only Second Choice of Each Transaction into Numpy Array\n\ntransaction = []\nfor i in range(0, data.shape[0]):\n    transaction.append(data.values[i,1])\n\ntransaction = np.array(transaction)\n\n# 2. Transform Them a Pandas DataFrame\ndf_second = pd.DataFrame(transaction, columns=[\"items\"]) \ndf_second[\"incident_count\"] = 1\n\n# 3. Delete NaN Items from Dataset\nindexNames = df_second[df_second['items'] == \"nan\" ].index\ndf_second.drop(indexNames , inplace=True)\n\n# 4. Final Step: Make a New Appropriate Pandas DataFrame for Visualizations  \ndf_table_second = df_second.groupby(\"items\").sum().sort_values(\"incident_count\", ascending=False).reset_index()\ndf_table_second[\"food\"] = \"food\"\ndf_table_second = df_table_second.truncate(before=-1, after=15) # Fist 15 Choice\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-22T15:58:54.454625Z","iopub.execute_input":"2021-07-22T15:58:54.455088Z","iopub.status.idle":"2021-07-22T15:58:54.584695Z","shell.execute_reply.started":"2021-07-22T15:58:54.455033Z","shell.execute_reply":"2021-07-22T15:58:54.583813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nsecond_choice = nx.from_pandas_edgelist(df_table_second, source = 'food', target = \"items\", edge_attr = True)\npos = nx.spring_layout(second_choice)\nnx.draw_networkx_nodes(second_choice, pos, node_size = 12500, node_color = \"honeydew\")\nnx.draw_networkx_edges(second_choice, pos, width = 3, alpha = 0.6, edge_color = 'black')\nnx.draw_networkx_labels(second_choice, pos, font_size = 18, font_family = 'sans-serif')\nplt.rcParams['figure.figsize'] = (20, 20)\nplt.axis('off')\nplt.grid()\nplt.title('Top Second Choices', fontsize = 25)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-22T15:58:54.585974Z","iopub.execute_input":"2021-07-22T15:58:54.586396Z","iopub.status.idle":"2021-07-22T15:58:55.024997Z","shell.execute_reply.started":"2021-07-22T15:58:54.586344Z","shell.execute_reply":"2021-07-22T15:58:55.02377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***- Choice Analysis / Customers' Third Choices***","metadata":{}},{"cell_type":"code","source":"# 1. Gather Only Third Choice of Each Transaction into Numpy Array\n## For Column \"2\"\ntransaction = []\nfor i in range(0, data.shape[0]):\n    transaction.append(data.values[i,2])\n\ntransaction = np.array(transaction)\n\n# 2. Transform Them a Pandas DataFrame\ndf_third = pd.DataFrame(transaction, columns=[\"items\"]) # Transaction Item Name\ndf_third[\"incident_count\"] = 1 # Put 1 to Each Item For Making Countable Table, Group By Will Be Done Later On\n\n# 3. Delete NaN Items from Dataset\nindexNames = df_third[df_third['items'] == \"nan\" ].index\ndf_third.drop(indexNames , inplace=True)\n\n# 4. Final Step: Make a New Appropriate Pandas DataFrame for Visualizations  \ndf_table_third = df_third.groupby(\"items\").sum().sort_values(\"incident_count\", ascending=False).reset_index()\ndf_table_third[\"food\"] = \"food\"\ndf_table_third = df_table_third.truncate(before=-1, after=15) # Fist 15 Choice\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-22T15:58:55.026444Z","iopub.execute_input":"2021-07-22T15:58:55.026708Z","iopub.status.idle":"2021-07-22T15:58:55.156545Z","shell.execute_reply.started":"2021-07-22T15:58:55.026676Z","shell.execute_reply":"2021-07-22T15:58:55.155858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure(data=[go.Bar(x=df_table_third[\"items\"], y=df_table_third[\"incident_count\"],\n            hovertext=df_table_third[\"items\"], text=df_table_third[\"incident_count\"], textposition=\"outside\")])\n\nfig.update_traces(marker_color='rgb(158,202,225)', marker_line_color='rgb(8,48,107)',\n                  marker_line_width=1.5, opacity=0.65)\nfig.update_layout(title_text=\"Customers' Third Choices\", template=\"plotly_dark\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-22T15:58:55.157537Z","iopub.execute_input":"2021-07-22T15:58:55.157905Z","iopub.status.idle":"2021-07-22T15:58:55.461697Z","shell.execute_reply.started":"2021-07-22T15:58:55.157876Z","shell.execute_reply":"2021-07-22T15:58:55.461029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Data Pre-Processing","metadata":{}},{"cell_type":"markdown","source":"***In order to be able to use apriori algorithm and get most frequent itemsets, we have to transform our dataset into a 1 – 0 matrix where rows are transactions and columns are products. In that matrix, “1” should be encoded if the product has been bought on that transaction and “0” should be encoded if the product has not been bought on that transaction. This preprocessing is required for use of the algorithm.***\n\n\nhttp://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/ <br>\nhttp://rasbt.github.io/mlxtend/user_guide/preprocessing/TransactionEncoder/\n","metadata":{}},{"cell_type":"code","source":"# Transform Every Transaction to Seperate List & Gather Them into Numpy Array\n\ntransaction = []\nfor i in range(data.shape[0]):\n    transaction.append([str(data.values[i,j]) for j in range(data.shape[1])])\n    \ntransaction = np.array(transaction)\ntransaction","metadata":{"execution":{"iopub.status.busy":"2021-07-22T15:58:55.462694Z","iopub.execute_input":"2021-07-22T15:58:55.463033Z","iopub.status.idle":"2021-07-22T15:58:57.647649Z","shell.execute_reply.started":"2021-07-22T15:58:55.463004Z","shell.execute_reply":"2021-07-22T15:58:57.646944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"te = TransactionEncoder()\nte_ary = te.fit(transaction).transform(transaction)\ndataset = pd.DataFrame(te_ary, columns=te.columns_)\ndataset","metadata":{"execution":{"iopub.status.busy":"2021-07-22T15:58:57.648629Z","iopub.execute_input":"2021-07-22T15:58:57.648969Z","iopub.status.idle":"2021-07-22T15:58:57.923794Z","shell.execute_reply.started":"2021-07-22T15:58:57.648941Z","shell.execute_reply":"2021-07-22T15:58:57.923119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-22T15:58:57.924784Z","iopub.execute_input":"2021-07-22T15:58:57.925138Z","iopub.status.idle":"2021-07-22T15:58:57.93065Z","shell.execute_reply.started":"2021-07-22T15:58:57.925109Z","shell.execute_reply":"2021-07-22T15:58:57.929609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***We have 121 columns/features at the moment. Extracting the most frequent itemsets from 121 feature would be compelling for a start*** <br>\n***From that reason, we will start with Top 50 items which are already illustrated in Section-3***\n\n","metadata":{}},{"cell_type":"code","source":"first50 = df_table[\"items\"].head(50).values # Select Top50\ndataset = dataset.loc[:,first50] # Extract Top50\ndataset","metadata":{"execution":{"iopub.status.busy":"2021-07-22T15:58:57.931954Z","iopub.execute_input":"2021-07-22T15:58:57.932191Z","iopub.status.idle":"2021-07-22T15:58:57.976526Z","shell.execute_reply.started":"2021-07-22T15:58:57.932164Z","shell.execute_reply":"2021-07-22T15:58:57.975519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.columns\n# We extracted first 50 column successfully.","metadata":{"execution":{"iopub.status.busy":"2021-07-22T15:58:57.977766Z","iopub.execute_input":"2021-07-22T15:58:57.978004Z","iopub.status.idle":"2021-07-22T15:58:57.985643Z","shell.execute_reply.started":"2021-07-22T15:58:57.977977Z","shell.execute_reply":"2021-07-22T15:58:57.98468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert dataset into 1-0 encoding\n\ndef encode_units(x):\n    if x == False:\n        return 0 \n    if x == True:\n        return 1\n    \ndataset = dataset.applymap(encode_units)\ndataset.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T15:58:57.987068Z","iopub.execute_input":"2021-07-22T15:58:57.987407Z","iopub.status.idle":"2021-07-22T15:58:58.278704Z","shell.execute_reply.started":"2021-07-22T15:58:57.987367Z","shell.execute_reply":"2021-07-22T15:58:58.27796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Data preprocessign is completed.***\n","metadata":{}},{"cell_type":"markdown","source":"# 5. Algorithm Implementation","metadata":{}},{"cell_type":"markdown","source":"## 5.1. Main Concepts of Association Rules / Apriori Algorithm","metadata":{}},{"cell_type":"markdown","source":"### 5.1.2. Support","metadata":{}},{"cell_type":"markdown","source":"*Support is an indication of how frequently the itemset appears in the dataset.*<br>\n*In other words, this is an indication of how popular an itemset is in a dataset*\n   \n![Support](https://user-images.githubusercontent.com/36535914/81746687-98163000-94af-11ea-834a-8d9577f28930.png)","metadata":{}},{"cell_type":"markdown","source":"### 5.1.2. Confidence","metadata":{}},{"cell_type":"markdown","source":"*Confidence is an indication of how often the rule has been found to be true* <br>\n*In other words, confidence says how likely item Y is purchased when item X is purchased*\n\n![Confidence](https://user-images.githubusercontent.com/36535914/82349639-094f6900-9a03-11ea-8163-8f4d9de06e14.png)","metadata":{}},{"cell_type":"markdown","source":"### 5.1.3. Lift","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"*Lift is a metric to measure the ratio of X and Y occur together to X and Y occurrence if they were statistically independent. In other words, lift illustrates how likely item Y is purchased when item X is purchased, while controlling for how popular item Y is.*\n\n* A Lift score that is close to 1 indicates that the antecedent and the consequent are independent and occurrence of antecedent has no impact on occurrence of consequent.\n\n* A Lift score that is bigger than 1 indicates that the antecedent and consequent are dependent to each other, and the occurrence of antecedent has a positive impact on occurrence of consequent.\n\n* A Lift score that is smaller than 1 indicates that the antecedent and the consequent are substitute each other that means the existence of antecedent has a negative impact to consequent or visa versa.\n\n***Therefore having lift bigger than 1 is critial for proving associations***\n\n![Lift](https://user-images.githubusercontent.com/36535914/85957011-5de6ec00-b992-11ea-8c5e-243f449d0241.png)\n","metadata":{}},{"cell_type":"markdown","source":"### 5.1.4. Conviction","metadata":{}},{"cell_type":"markdown","source":"*Conviction measures the implication strength of the rule from statistical independence\nConviction score is a ratio between the probability that X occurs without Y while they were dependent and the actual probability of X existence without Y. For instance; if (French fries) --> (beer) association has a conviction score of 1.8; the rule would be incorrect 1.8 times as often (80% more often) if the association between totally independent.*\n\n","metadata":{}},{"cell_type":"markdown","source":"![Conviction](https://user-images.githubusercontent.com/36535914/82484880-42f7a100-9ae3-11ea-8219-cd27a9d96d6f.png)","metadata":{}},{"cell_type":"markdown","source":"### 5.1.5. Consequents & Antecedents","metadata":{}},{"cell_type":"markdown","source":"*The IF component of an association rule is known as the antecedent. The THEN component is known as the consequent. The antecedent and the consequent are disjoint; they have no items in common.*","metadata":{}},{"cell_type":"markdown","source":"![Cons_Ant](https://user-images.githubusercontent.com/36535914/82491863-efd71b80-9aed-11ea-80d5-2ffebd16251d.png)","metadata":{}},{"cell_type":"markdown","source":"## 5.2. Implementation","metadata":{}},{"cell_type":"markdown","source":"***The most widely used library for Association Rules Learning implementations is 'Mlxtend'. We will be using that library as well***","metadata":{}},{"cell_type":"code","source":"# Extracting the most frequest itemsets via Mlxtend.\n# The length column has been added to increase ease of filtering.\n\nfrequent_itemsets = apriori(dataset, min_support=0.01, use_colnames=True)\nfrequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\nfrequent_itemsets","metadata":{"execution":{"iopub.status.busy":"2021-07-22T15:58:58.279842Z","iopub.execute_input":"2021-07-22T15:58:58.280288Z","iopub.status.idle":"2021-07-22T15:58:58.632847Z","shell.execute_reply.started":"2021-07-22T15:58:58.280242Z","shell.execute_reply":"2021-07-22T15:58:58.632138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***We can easily explore the itemsets via below snippes***","metadata":{}},{"cell_type":"code","source":"frequent_itemsets[ (frequent_itemsets['length'] == 2) &\n                   (frequent_itemsets['support'] >= 0.05) ]","metadata":{"execution":{"iopub.status.busy":"2021-07-22T15:58:58.633984Z","iopub.execute_input":"2021-07-22T15:58:58.634407Z","iopub.status.idle":"2021-07-22T15:58:58.648225Z","shell.execute_reply.started":"2021-07-22T15:58:58.634368Z","shell.execute_reply":"2021-07-22T15:58:58.647501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frequent_itemsets[ (frequent_itemsets['length'] == 3) ].head()","metadata":{"execution":{"iopub.status.busy":"2021-07-22T15:58:58.649991Z","iopub.execute_input":"2021-07-22T15:58:58.650578Z","iopub.status.idle":"2021-07-22T15:58:58.673154Z","shell.execute_reply.started":"2021-07-22T15:58:58.650537Z","shell.execute_reply":"2021-07-22T15:58:58.672341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Now, we will use the extracted frequent itemsets in rule creation***","metadata":{}},{"cell_type":"code","source":"# We can create our rules by defining metric and its threshold.\n\n# For a start, \n#      We set our metric as \"Lift\" to define whether antecedents & consequents are dependent our not.\n#      Treshold is selected as \"1.2\" since it is required to have lift scores above than 1 if there is dependency.\n\nrules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.2)\nrules[\"antecedents_length\"] = rules[\"antecedents\"].apply(lambda x: len(x))\nrules[\"consequents_length\"] = rules[\"consequents\"].apply(lambda x: len(x))\nrules.sort_values(\"lift\",ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T15:58:58.674675Z","iopub.execute_input":"2021-07-22T15:58:58.675174Z","iopub.status.idle":"2021-07-22T15:58:58.722344Z","shell.execute_reply.started":"2021-07-22T15:58:58.675127Z","shell.execute_reply":"2021-07-22T15:58:58.721514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***According to above table, we can easily say that the dependency between (herb & pepper) and (ground beef) is high since lift score is approximately 2.5x of threshold and the confidence score is promising (32%)***","metadata":{}},{"cell_type":"markdown","source":"***In order to get more insights from the data, let’s look into confidence!***","metadata":{}},{"cell_type":"code","source":"# Sort values based on confidence\n\nrules.sort_values(\"confidence\",ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T15:58:58.723485Z","iopub.execute_input":"2021-07-22T15:58:58.723853Z","iopub.status.idle":"2021-07-22T15:58:58.757248Z","shell.execute_reply.started":"2021-07-22T15:58:58.723823Z","shell.execute_reply":"2021-07-22T15:58:58.756257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- ***According to above table, the customers who bought (eggs, ground beef) is expected to buy (mineral water) with a likelihood of 50% (confidence). Lift & conviction scores support that hypothesis too*** \n- ***It would be better to keep them close to increase sales !***","metadata":{}},{"cell_type":"markdown","source":"***Since the most demanded product is mineral water in the dataset, the association results are mainly dominated by it. From that reason, to get more insights, it’s better to create a confidence table excluding the mineral water***","metadata":{}},{"cell_type":"code","source":"rules[~rules[\"consequents\"].str.contains(\"mineral water\", regex=False) & \n      ~rules[\"antecedents\"].str.contains(\"mineral water\", regex=False)].sort_values(\"confidence\", ascending=False).head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T15:58:58.758533Z","iopub.execute_input":"2021-07-22T15:58:58.758795Z","iopub.status.idle":"2021-07-22T15:58:58.795464Z","shell.execute_reply.started":"2021-07-22T15:58:58.758757Z","shell.execute_reply":"2021-07-22T15:58:58.794612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***According to mineral water excluded table above, we can say that there is a significant relationship between ground beef and spaghetti, red wine and spaghetti. Lift and conviction scores supports that too***","metadata":{}},{"cell_type":"markdown","source":"***As you might have noticed, ground beef is on the top in both mineral water included and excluded table. From that reason, in order to catch new associations related to ground beef and boost the sales, let’s look into associations where ground beef is antecedent.***","metadata":{}},{"cell_type":"code","source":"rules[rules[\"antecedents\"].str.contains(\"ground beef\", regex=False) & rules[\"antecedents_length\"] == 1].sort_values(\"confidence\", ascending=False).head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T15:58:58.796728Z","iopub.execute_input":"2021-07-22T15:58:58.796952Z","iopub.status.idle":"2021-07-22T15:58:58.827495Z","shell.execute_reply.started":"2021-07-22T15:58:58.796926Z","shell.execute_reply":"2021-07-22T15:58:58.826299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- ***There are many associations with high confidence and lift score. We are on the right way!***","metadata":{}},{"cell_type":"markdown","source":"# 6. Results","metadata":{}},{"cell_type":"markdown","source":"***As you seen on above investigations, the flexibility of the algorithm and the mlxtend library is high therefore we can easily investigate different aspects and get new associations from the data. From that reason, the investigations could be further detailed by taking other products (rest of the Top50) into calculation or changing the criteria threshold. Nevertheless, since the association rule learning has an iterative schema, data understanding and interpretation skills and activities are really important. In that case, we should give enough importance to data visualization and/or data cleansing (if required) steps to be sure we are on the right way.***","metadata":{}},{"cell_type":"markdown","source":"\n# 7. Useful Links\n\n- https://www.kdnuggets.com/2016/04/association-rules-apriori-algorithm-tutorial.html\n- http://www.borgelt.net/doc/apriori/apriori.html#cvct\n- http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/association_rules/\n- https://stackoverflow.com/questions/11350770/select-by-partial-string-from-a-pandas-dataframe\n- https://en.wikipedia.org/wiki/Association_rule_learning\n- https://www.upgrad.com/blog/association-rule-mining-an-overview-and-its-applications/\n- https://www.scss.tcd.ie/Khurshid.Ahmad/Teaching/Lectures_on_Financial_Informatics/Zookeeper_Forward_Chains.pdf\n- https://docs.oracle.com/cd/B28359_01/datamine.111/b28129/algo_apriori.htm#BGBCDHEB","metadata":{}}]}