{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To remove depricated warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Loading train and test files:\ntrain_path=\"/kaggle/input/human-activity-recognition-with-smartphones/train.csv\"\ndf_train=pd.read_csv(train_path)\ntest_path=\"/kaggle/input/human-activity-recognition-with-smartphones/test.csv\"\ndf_test=pd.read_csv(test_path)\ndf = pd.concat([df_train,df_test])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"                                                        About Project dataset\nHuman activity Recognition using smartphones Data set The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data.\n\nThe sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain.\n\nAttribute Information:\n\nFor each record in the dataset it is provided:\n\nTriaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration.\nTriaxial Angular velocity from the gyroscope.\nA 561-feature vector with time and frequency domain variables.\nIts activity label.\nAn identifier of the subject who carried out the experiment.\nData Analysis:\n\n1. From the give signals of x,y,z axis of accelrometer and gyroscope, we aim to provide the classification to following groups.\n\nLaying\nStanding\nSitting\nWalking\nWalking_Upstairs\nWalking_Downstairs"},{"metadata":{},"cell_type":"markdown","source":"                                                      What to expect from this Notebook?\n1. PreProcessing steps\n    * Reading and understanding the data with basic panda library\n    * Encoding categorical values\n    * Finding outliners using pyod library\n    * Feature selection using PCA\n2. Model selection and comaprision\n    * Pipeline to create Classifier models\n    * Hyper tuning the models\n    * Display of classification results and comparision of models.\n    "},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns\n# Program to remove all whitespaces\nimport re\n# matches all whitespace characters\npattern = r'[()-.,]+'\n\ntempcol = []\ncol_new = []\nfor col in df_train.columns:\n  new_string = re.sub(pattern, '_', col) \n  col_new.append(new_string)\n  tempcol.append(new_string.split('_')[0])\n  \ndf.columns = col_new\n\n#The main features are:\nprint('The main columns are:')\nfor temp in list(set(tempcol)):\n    print(temp)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The total 563 columns basically consist of above parameter divided on x, y and z axis."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Returns a Summary dataframe for numeric columns only \n# output will be same as host_df.describe()\ndf.describe(exclude='O')\n# Returns a Summary dataframe \n#  for object type (or categorical) columns only \ndf.describe(include='O')\n#Cheking data disribtion\n#1. Value greater then -1 and 1 , data is higly skewed\n#2. Values between -1 to -0.5 and 0.5 to 1 is less skewed\n#3. Values between -0.5 to 0.5 is almost symmetrically distributed\ndf.skew()\n\n# Create correlation matrix\ncorr_matrix = df.corr().abs()\nprint(corr_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Since its a classification problem, its important to know if data is balanced or not?\nprint(df.Activity.value_counts())\ndf.Activity.value_counts().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encoding catageorical values"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n#le.fit([\"WALKING\", \"LAYING\", \"STANDING\", \"SITTING\",\"WALKING_UPSTAIRS\",\"WALKING_DOWNSTAIRS\"])\ndf['Activity'] = le.fit_transform(df['Activity'])\ndf_en = df.drop(columns=['subject']) #dropping unwaned columns\ndf_en_data = df_en.drop(columns = ['Activity'])\ndf_en_target = df_en['Activity']\ndf_en_target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyod","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# To detect outliners using pyod lib"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom pyod.models.abod import ABOD\nfrom pyod.models.cblof import CBLOF\n#from pyod.models.feature_bagging import FeatureBagging\nfrom pyod.models.iforest import IForest\nfrom pyod.models.knn import KNN\nfrom pyod.models.lof import LOF\n\nrandom_state = np.random.RandomState(42)\n#Removing 5% outliners\noutliers_fraction = 0.05\n# Define outlier detection tools to be compared\nclassifiers = {\n        'Angle-based Outlier Detector (ABOD)': ABOD(contamination=outliers_fraction),\n        'Cluster-based Local Outlier Factor (CBLOF)':CBLOF(contamination=outliers_fraction,check_estimator=False, random_state=random_state),\n        #'Feature Bagging':FeatureBagging(LOF(n_neighbors=35),contamination=outliers_fraction,check_estimator=False,random_state=random_state),\n        'Isolation Forest': IForest(contamination=outliers_fraction,random_state=random_state),\n        'K Nearest Neighbors (KNN)': KNN(contamination=outliers_fraction),\n        'Average KNN': KNN(method='mean',contamination=outliers_fraction)\n}\n\nfor i, (clf_name, clf) in enumerate(classifiers.items()):\n    clf.fit(df_en)\n    # predict raw anomaly score\n    scores_pred = clf.decision_function(df_en) * -1\n        \n    # prediction of a datapoint category outlier or inlier\n    y_pred = clf.predict(df_en)\n    n_inliers = len(y_pred) - np.count_nonzero(y_pred)\n    n_outliers = np.count_nonzero(y_pred == 1)\n    \n    print('OUTLIERS : ',n_outliers,'INLIERS : ',n_inliers, clf_name)\n    print('-'*50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scaling the dataset , because many outliners are detected"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nUsing Robust based technique to scale data as it has outliners\n'''\nfrom sklearn.preprocessing import RobustScaler\nrobustscaler = RobustScaler()\ndf_robust = robustscaler.fit_transform(df_en_data)\ndf_robust","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Applying PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.preprocessing import StandardScaler\n#df_en_std = StandardScaler().fit_transform(df_en)\nprint('Covariance matrix \\n')\ndf_en_cov_mat= np.cov(df_robust, rowvar=False)\ndf_en_cov_mat\ndf_en_cov_mat = np.cov(df_robust.T)\neig_vals, eig_vecs = np.linalg.eig(df_en_cov_mat)\nprint('Eigenvectors \\n%s' %eig_vecs)\nprint('\\nEigenvalues \\n%s' %eig_vals)\ntot = sum(eig_vals)\nprint(\"\\n\",tot)\nvar_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\nprint(\"\\n\\n1. Variance Explained\\n\",var_exp)\ncum_var_exp = np.cumsum(var_exp)\nprint(\"\\n\\n2. Cumulative Variance Explained\\n\",cum_var_exp)\nprint(\"\\n\\n3. Percentage of variance the first 200 principal components each contain\\n \",var_exp[0:200])\nprint(\"\\n\\n4. Percentage of variance the first 200 principal components together contain\\n\",sum(var_exp[0:200]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the training and test data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=200)\nprincipalComponents = pca.fit_transform(df_robust)\ndf_pca = pd.DataFrame(data = principalComponents)\n\nfrom sklearn.model_selection import train_test_split\nxtrain,xtest,ytrain,ytest = train_test_split(df_pca,df_en_target,test_size=0.4, random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Apply model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"Classifiers=[LogisticRegression(max_iter=1000, C=0.1,solver= 'newton-cg'),\n             DecisionTreeClassifier(class_weight =  'balanced', criterion = 'entropy'),\n             RandomForestClassifier(class_weight =  'balanced', criterion = 'entropy'),\n             GradientBoostingClassifier(),\n             AdaBoostClassifier(),\n             ExtraTreesClassifier(),\n             KNeighborsClassifier(),\n             SVC(kernel=\"linear\",degree=3,C=10,gamma=0.001),\n             GaussianNB()]\npipelines = []\nfor model in Classifiers:\n    pipeline = make_pipeline(\n              model)\n    pipelines.append(pipeline)\nfor pipeline in pipelines:\n    pipeline.fit(xtrain, ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Accuracy_mean = []\nAccuracy_std = []\nmodel_names = ['LR','DTC','RFC','GBC','AB','ET','KNN','SVC','GNB']\noutcome = []\n#models scores\nfor pipeline in pipelines:\n    print(pipeline)\n    print('Train Score: ',pipeline.score(xtrain, ytrain))\n    print('Test Score: ',pipeline.score(xtest, ytest))\n    pred = pipeline.predict(xtest)\n    precision_score_temp = precision_score(ytest, pred, average='micro')\n    recall_score_temp = recall_score(ytest, pred, average='micro')\n    f1_score_temp = f1_score(ytest, pred, average='micro')\n    all_accuracies = cross_val_score(estimator=pipeline, X=xtrain, y=ytrain, cv=5)\n    print(f'All Accuracies: {all_accuracies}')\n    print(f'Mean Accuracies: {all_accuracies.mean()}')\n    print(f'Std of Accuracies: {all_accuracies.std()}')\n    print(f'Accuracy: {accuracy_score(ytest, pred)}')\n    #print(f'Precision: {precision_score_temp}')\n    #print(f'Recall: {recall_score_temp}')\n    #print(f'f1: {f1_score_temp}')\n    print(classification_report(ytest, pred))\n    print('Confusion_matrix:')\n    print(f'{confusion_matrix(ytest, pred ,labels=[0,1,2,3,4,5])}')\n    Accuracy_mean.append(all_accuracies.mean())\n    Accuracy_std.append(all_accuracies.std())\n    outcome.append(all_accuracies)\n    print('*'*50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig = plt.figure()\nfig.suptitle('Machine Learning Model Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(outcome)\nax.set_xticklabels(model_names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig = plt.figure()\nfig.suptitle('Machine Learning Model Comparison')\nax = fig.add_subplot(111)\nplt.bar(model_names,Accuracy_mean)\nax.set_xticklabels(model_names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"                            Code For Hyper-tuning the model using grid search and finding the best parameter"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making pipeline for Logestic regression:\n#from sklearn.linear_model import LogisticRegression\n#steps = [('LR', LogisticRegression())]\n#make_pipeline = Pipeline(steps) # define the pipeline object.    \n#parameteres = {'LR__max_iter':[1000, 5000],'LR__C':[0.1,10,100,10e5], 'LR__fit_intercept':[True, False], 'LR__class_weight':[None , 'balanced'], 'LR__solver' : ['newton-cg', 'lbfgs', 'liblinear']}\n#grid_LR = GridSearchCV(make_pipeline, param_grid=parameteres)\n#grid_LR.fit(xtrain, ytrain)\n\n#grid_preds = grid_LR.predict(xtest)\n#accuracy = accuracy_score(ytest, grid_preds)\n#precision = precision_score(ytest, grid_preds, average='micro')\n#recall = recall_score(ytest, grid_preds, average='micro')\n#f1 = f1_score(ytest, grid_preds, average='micro')\n#['LAYING', 'SITTING', 'STANDING', 'WALKING', 'WALKING_DOWNSTAIRS','WALKING_UPSTAIRS']\n#confusion_matrix(ytest, grid_preds ,labels=[0,1,2,3,4,5])\n#print('Best params: ', grid_LR.best_params_)\n#print('Best score: ', grid_LR.best_score_)\n#print('LogisticRegression accuracy: ', accuracy)\n#print('LogisticRegression precision: ', precision)\n#print('LogisticRegression recall: ', recall)\n#print('LogisticRegression f1: ', f1)\n#print(\"score = %3.2f\" %(grid_LR.score(xtest,ytest)))\n#print(f'Accuracy LogisticRegression classifier on training set {grid_LR.score(xtrain, ytrain)}')\n#print(f'Accuracy LogisticRegression classifier on testing set {grid_LR.score(xtest, ytest)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making pipeline for SVC:\n#from sklearn.svm import SVC\n#steps = [('SVC', SVC())]\n#make_pipeline = Pipeline(steps) # define the pipeline object.    \n#parameteres = {'SVC__C':[0.001,0.1,10,100,10e5], 'SVC__kernel':['linear', 'poly', 'rbf', 'sigmoid'], 'SVC__degree':[3,4,5], 'SVC__class_weight' : [None,'balanced']}\n#grid_SVC = GridSearchCV(make_pipeline, param_grid=parameteres)\n#grid_SVC.fit(xtrain, ytrain)\n\n#grid_preds = grid_SVC.predict(xtest)\n#accuracy = accuracy_score(ytest, grid_preds)\n#precision = precision_score(ytest, grid_preds, average='micro')\n#recall = recall_score(ytest, grid_preds, average='micro')\n#f1 = f1_score(ytest, grid_preds, average='micro')\n#['LAYING', 'SITTING', 'STANDING', 'WALKING', 'WALKING_DOWNSTAIRS','WALKING_UPSTAIRS']\n#confusion_matrix(ytest, grid_preds ,labels=[0,1,2,3,4,5])\n#print('Best params: ', grid_SVC.best_params_)\n#print('Best score: ', grid_SVC.best_score_)\n#print('SVC accuracy: ', accuracy)\n#print('SVC precision: ', precision)\n#print('SVC recall: ', recall)\n#print('SVC f1: ', f1)\n#print(\"score = %3.2f\" %(grid_SVC.score(xtest,ytest)))\n#print(f'Accuracy SVC classifier on training set {grid_SVC.score(xtrain, ytrain)}')\n#print(f'Accuracy SVC classifier on testing set {grid_SVC.score(xtest, ytest)}')\n#print(classification_report(ytest, grid_preds))\n#print(f'{confusion_matrix(ytest, grid_preds ,labels=[0,1,2,3,4,5])}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusion: Logestic regression and SVC gave 97% accuracy with hypertuning of models and using PCA"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}