{"cells":[{"metadata":{},"cell_type":"markdown","source":"**This notebook is an exercise in the [Intro to Deep Learning](https://www.kaggle.com/learn/intro-to-deep-learning) course.  You can reference the tutorial at [this link](https://www.kaggle.com/ryanholbrook/stochastic-gradient-descent).**\n\n---\n"},{"metadata":{},"cell_type":"markdown","source":"# Introduction #\n\nIn this exercise you'll train a neural network on the *Fuel Economy* dataset and then explore the effect of the learning rate and batch size on SGD.\n\nWhen you're ready, run this next cell to set everything up!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setup plotting\nimport matplotlib.pyplot as plt\nfrom learntools.deep_learning_intro.dltools import animate_sgd #Cool visualization\n\nplt.style.use('seaborn-whitegrid')\n# Set Matplotlib defaults\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\nplt.rc('animation', html='html5')\n\n# Setup feedback system\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.deep_learning_intro.ex3 import *","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the *Fuel Economy* dataset your task is to predict the fuel economy of an automobile given features like its type of engine or the year it was made. \n\nFirst load the dataset by running the cell below."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import make_column_transformer, make_column_selector\nfrom sklearn.model_selection import train_test_split\n\nfuel = pd.read_csv('../input/dl-course-data/fuel.csv')\nprint(fuel.shape)\n\nX = fuel.copy()\nprint('Before preprocessor X.head():\\n', X.head(3))\n# Remove target\ny = X.pop('FE')\nprint('Before, y values:\\n', pd.Series(y).value_counts(bins=3).sort_index())\n\npreprocessor = make_column_transformer(\n    (StandardScaler(),\n     make_column_selector(dtype_include=np.number)),\n    (OneHotEncoder(sparse=False),\n     make_column_selector(dtype_include=object)),\n)\n\nX = preprocessor.fit_transform(X)\nprint('After preprocessor X.head():\\n', pd.DataFrame(X).head(3))\ny = np.log(y) # log transform target instead of standardizing\nprint('After, y values:\\n', pd.Series(y).value_counts(bins=3).sort_index())\n\ninput_shape = [X.shape[1]]\nprint(\"Input shape: {}\".format(input_shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Take a look at the data if you like. Our target in this case is the `'FE'` column and the remaining columns are the features."},{"metadata":{"trusted":true},"cell_type":"code","source":"#original data\nfuel.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#processed features\npd.DataFrame(X[:10,:]).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run the next cell to define the network we'll use for this task."},{"metadata":{"trusted":true},"cell_type":"code","source":"input_shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    layers.Dense(128, activation='relu', input_shape=input_shape),\n    layers.Dense(128, activation='relu'),    \n    layers.Dense(64, activation='relu'),\n    layers.Dense(1),\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1) Add Loss and Optimizer\n\nBefore training the network we need to define the loss and optimizer we'll use. Using the model's `compile` method, add the Adam optimizer and MAE loss."},{"metadata":{"lines_to_next_cell":0,"trusted":true},"cell_type":"code","source":"# YOUR CODE HERE\nmodel.compile(optimizer='adam',\n              loss='mae'\n)\n\n# Check your answer\nq_1.check()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lines below will give you a hint or solution code\n#q_1.hint()\n#q_1.solution()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2) Train Model\n\nOnce you've defined the model and compiled it with a loss and optimizer you're ready for training. Train the network for 200 epochs with a batch size of 128. The input data is `X` with target `y`."},{"metadata":{"lines_to_next_cell":0,"trusted":true},"cell_type":"code","source":"# YOUR CODE HERE\nhistory = model.fit(X, y,\n                    #validation_data = (X_valid, y_valid),\n                    batch_size=128,\n                    epochs=200\n)\n\n# Check your answer\nq_2.check()","execution_count":null,"outputs":[]},{"metadata":{"lines_to_next_cell":0,"trusted":true},"cell_type":"code","source":"# Lines below will give you a hint or solution code\n#q_2.hint()\n#q_2.solution()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The last step is to look at the loss curves and evaluate the training. Run the cell below to get a plot of the training loss."},{"metadata":{"lines_to_next_cell":0,"trusted":true},"cell_type":"code","source":"import pandas as pd\n\nhistory_df = pd.DataFrame(history.history)\n# Start the plot at epoch 5. You can change this to get a different view.\nhistory_df.loc[5:, ['loss']].plot();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3) Evaluate Training\n\nIf you trained the model longer, would you expect the loss to decrease further?"},{"metadata":{"lines_to_next_cell":0,"trusted":true},"cell_type":"code","source":"# View the solution (Run this cell to receive credit!)\nq_3.check()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the learning rate and the batch size, you have some control over:\n- How long it takes to train a model\n- How noisy the learning curves are\n- How small the loss becomes\n\nTo get a better understanding of these two parameters, we'll look at the linear model, our simplest neural network. Having only a single weight and a bias, it's easier to see what effect a change of parameter has.\n\nThe next cell will generate an animation like the one in the tutorial. Change the values for `learning_rate`, `batch_size`, and `num_examples` (how many data points) and then run the cell. (It may take a moment or two.) Try the following combinations, or try some of your own:\n\n| `learning_rate` | `batch_size` | `num_examples` |\n|-----------------|--------------|----------------|\n| 0.05            | 32           | 256            |\n| 0.05            | 2            | 256            |\n| 0.05            | 128          | 256            |\n| 0.02            | 32           | 256            |\n| 0.2             | 32           | 256            |\n| 1.0             | 32           | 256            |\n| 0.9             | 4096         | 8192           |\n| 0.99            | 4096         | 8192           |"},{"metadata":{"trusted":true},"cell_type":"code","source":"# YOUR CODE HERE: Experiment with different values for the learning rate, batch size, and number of examples\nlearning_rate = 0.05\nbatch_size = 32\nnum_examples = 8192\n\nanimate_sgd(\n    learning_rate=learning_rate,\n    batch_size=batch_size,\n    num_examples=num_examples,\n    # You can also change these, if you like\n    steps=50, # total training steps (batches seen)\n    true_w=3.0, # the slope of the data\n    true_b=2.0, # the bias of the data\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate = 0.2\nbatch_size = 32\nnum_examples = 8192\n\nanimate_sgd(\n    learning_rate=learning_rate,\n    batch_size=batch_size,\n    num_examples=num_examples,\n    # You can also change these, if you like\n    steps=50, # total training steps (batches seen)\n    true_w=3.0, # the slope of the data\n    true_b=2.0, # the bias of the data\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate = 0.5\nbatch_size = 128\nnum_examples = 8192\n\n\nanimate_sgd(\n    learning_rate=learning_rate,\n    batch_size=batch_size,\n    num_examples=num_examples,\n    # You can also change these, if you like\n    steps=50, # total training steps (batches seen)\n    true_w=3.0, # the slope of the data\n    true_b=2.0, # the bias of the data\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate = 0.2\nbatch_size = 1024\nnum_examples = 8192\n\n\nanimate_sgd(\n    learning_rate=learning_rate,\n    batch_size=batch_size,\n    num_examples=num_examples,\n    # You can also change these, if you like\n    steps=50, # total training steps (batches seen)\n    true_w=3.0, # the slope of the data\n    true_b=2.0, # the bias of the data\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate = 0.2\nbatch_size = 4096\nnum_examples = 8192\n\nanimate_sgd(\n    learning_rate=learning_rate,\n    batch_size=batch_size,\n    num_examples=num_examples,\n    # You can also change these, if you like\n    steps=50, # total training steps (batches seen)\n    true_w=3.0, # the slope of the data\n    true_b=2.0, # the bias of the data\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### learntools/learntools/deep_learning_intro/dltools.py\n### https://github.com/Kaggle/learntools/blob/c1db0c47b6c67bbbddbd2ef9775f63a136736626/learntools/deep_learning_intro/dltools.py\n\n```\ndef animate_sgd(num_examples, batch_size, steps, learning_rate,\n                true_w=3.0, true_b=2.0, seed=0):\n    # Define model\n    class Model(object):\n        def __init__(self, w_init=-1.0, b_init=-1.0):\n            self.W = tf.Variable(w_init)\n            self.b = tf.Variable(b_init)\n\n        def __call__(self, x):\n            return self.W * x + self.b\n            \n    def loss(target_y, predicted_y):\n        return tf.reduce_mean(tf.square(target_y - predicted_y))\n\n    def train(model, inputs, outputs, learning_rate):\n        with tf.GradientTape() as t:\n            current_loss = loss(outputs, model(inputs))\n            dW, db = t.gradient(current_loss, [model.W, model.b])\n            model.W.assign_sub(learning_rate * dW)\n            model.b.assign_sub(learning_rate * db)\n\n    # Data\n    inputs  = tf.random.normal(shape=[num_examples], seed=seed)\n    noise   = tf.random.normal(shape=[num_examples], seed=seed+1)\n    outputs = inputs * true_w + true_b + noise\n    ds = (tf.data.Dataset\n          .from_tensor_slices((inputs, outputs))\n          .shuffle(1000, seed=seed)\n          .batch(batch_size)\n          .repeat())\n    ds = iter(ds)\n    model = Model()        \n...\n```"},{"metadata":{},"cell_type":"markdown","source":"# 4) Learning Rate and Batch Size\n\nWhat effect did changing these parameters have? After you've thought about it, run the cell below for some discussion."},{"metadata":{"lines_to_next_cell":0,"trusted":true},"cell_type":"code","source":"# View the solution (Run this cell to receive credit!)\nq_4.check()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Keep Going #\n\nLearn how to [**improve your model's performance**](https://www.kaggle.com/ryanholbrook/overfitting-and-underfitting) by tuning capacity or adding an early stopping callback."},{"metadata":{},"cell_type":"markdown","source":"---\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum/191966) to chat with other Learners.*"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}