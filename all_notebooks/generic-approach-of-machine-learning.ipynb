{"cells":[{"metadata":{},"cell_type":"markdown","source":"As a beginner, I made this notebook to present a generic approach to \"play\" with the concepts of machine learning and neural network. I have also tried to provide some clean Python code. To sum up, it is a synthesis of my current knowledge (and sorry for my English).\n\nThis is a first version which will be improved compete after compete.\n\nThe basic steps to define a 'Generic approach of Machine Learning' are:\n1. Define the problem, I mean understand the data you got and define what are the inputs (attributes) and what is the output (target) of your Machine Learning\n2. Summarize the dataset content in a statistical form\n3. Prepare the dataset for your Machine Learning processing\n4. Evaluate a set of algorithms based on you understanding of the data\n5. Improve the results of your Machine Learning by refining the algorithms\n6. Present the results of your Machine Learning\n7. Deploy or save your Machine Learning\n\n**NOTE: Please, feel free to correct and enhance this notebook ;)**"},{"metadata":{},"cell_type":"markdown","source":"To define the problem, we have first to choose the subject we will work on. The point 1.b provides different datasets we can use to play. For each dataset, a comment describes the problem to address. \nWe will consider two different problems:\n1. One about classification (the basic one is the Iris classification)\n2. One about regression (Melbourne housing prices)\n\nThis is the part that cannot be generic. The generic behavior proposed here is parameterized by the set of parameters defined in point b.1.\n\nNote: In point b.1, to swith to another problem, just comment the current one and uncomment the problem to play with\n\nSwitching to Python code, the first step is to load all the required libraries (1.a) and to choose the problem to solve, let's say Iris classification."},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import division # Import floating-point division (1/4=0.25) instead of Euclidian division (1/4=0)\n\n# 1. Prepare Problem\n\n# a) Load libraries\nimport os, warnings, argparse, io, operator, requests\n\nimport numpy as np # Linear algebra\nimport matplotlib.pyplot as plt # Data visualization\nimport pandas as pd # Data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom pandas_profiling import ProfileReport\n\nimport sklearn\nfrom sklearn import model_selection\nfrom sklearn import linear_model # Regression\nfrom sklearn import discriminant_analysis\nfrom sklearn import neighbors # Clustering\nfrom sklearn import naive_bayes\nfrom sklearn import tree # Decisional tree learning\nfrom sklearn import svm # Support Vector Machines\nfrom sklearn import ensemble # Support RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor\n\nimport xgboost as xgb # Gradient Boosted Decision Trees algorithm\n\nfrom sklearn.base import is_classifier, is_regressor # To check if the model is for regression or classification (do not work for Keras)\n\nfrom sklearn.impute import SimpleImputer \n\nfrom sklearn.preprocessing import LabelEncoder # Labelling categorical feature from 0 to N_class - 1('object' type)\nfrom sklearn.preprocessing import LabelBinarizer # Binary labelling of categorical feature\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.preprocessing import StandardScaler # Data normalization\nfrom sklearn.preprocessing import MinMaxScaler # Data normalization\nfrom sklearn.preprocessing import MaxAbsScaler # Data normalization\n\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.inspection import permutation_importance\n\nimport pickle # Use to save and load models\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\n# Neural Network\nimport tensorflow as tf\nimport keras\nfrom keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First of all, we have to define the problem:\n1. Understand the data, see point b.1) below\n2. Prepare the basics of your code such as loading the libraries and your data, see points a) and b) below\n\nIn point b.1, we have a set of parameters strongly linked to the problem to solve. These parameters are used to configure the execution of 'Generic approach of Machine Learning':\n- DATABASE_NAME: The URI of the dataset\n- COLUMNS_LABEL: Columns label of the dataset. Default: None, means that labels are already present in the loaded dataset- COLUMNS_TO_DROP: The useless columns to drop after loading the dataset\n- POST_LOAD_PROCESSING: a lamdba function to apply to the whole loaded dataset\n- FEATURES_SELECTION: The list of the features for the ML inputs. Default: None, means - all columns (excepted the output columns) are concidered as features\n- TARGET_COLUMN: The output column\n- OUTPUT_IS_REGRESSION: Indicates if the ML is about either regression (True) or classification (False)\n- DATE_TIME_COLUMNS: The list of the date/time column in customized format such as string format\n- FEATURES_PRE_PROCESSING: This dictionary attaches a lamdba function to apply to a column. The Lambda function is a processing to apply to the column just after loading the dataset (point 1.c).\n- NON_TRANSFORMABLE_COLUMNS: Indicates a list of columns which shall not be included in the transformation process (point 3.b)\n\nTODO: Add  features creation lamba code\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# b) Helpers\n\n# b.1) Define global parameters\n# Regression\n\n# To predict house price using the famous Melbourne housing dataset\nDATABASE_NAME = 'https://raw.githubusercontent.com/nagoya-foundation/r-functions-performance/master/data/Melbourne_housing_FULL.csv'\nPOST_LOAD_PROCESSING = None\nCOLUMNS_LABEL = None\nCOLUMNS_TO_DROP = ['Address', 'Method', 'Postcode', 'CouncilArea', 'Propertycount', 'Regionname', 'SellerG']\nFEATURES_SELECTION = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude']\nTARGET_COLUMN = 'Price'\nOUTPUT_IS_REGRESSION=True\nDATE_TIME_COLUMNS = ['Date']\nFEATURES_PRE_PROCESSING = None\nNON_TRANSFORMABLE_COLUMNS = None\n# Suburb\n# Address\n# Rooms\n# Type\n# Price\n# Method\n# SellerG\n# Date\n# Distance\n# Postcode\n# Bedroom2\n# Bathroom\n# Car\n# Landsize\n# BuildingArea\n# YearBuilt\n# CouncilArea\n# Lattitude\n# Longtitude\n# Regionname\n# Propertycount\n\n# Classification\n# To categorize an iris flower according to the dimensions of its sepals & petals \n# Famous database; from Fisher, 1936\n#DATABASE_NAME = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n#COLUMNS_TO_DROP = None\n#FEATURES_SELECTION = None\n#TARGET_COLUMN = 'class'\n#OUTPUT_IS_REGRESSION=False\n#DATE_TIME_COLUMNS = None\n#COLUMNS_LABEL = ['sepal length in cm', 'sepal width in cm', 'petal length in cm', 'petal width in cm', 'class']\n\n# To predict survival on the Titanic\n# This is the list of the passengers\n#DATABASE_NAME = 'https://raw.githubusercontent.com/alexisperrier/packt-aml/master/ch4/titanic.csv'\n#POST_LOAD_PROCESSING = lambda x: pd.concat([pd.DataFrame({'passengerid': np.arange(x.shape[0])}), x], axis = 1)\n#COLUMNS_LABEL = None\n#COLUMNS_TO_DROP = ['name', 'ticket', 'embarked', 'boat', 'body', 'home.dest']\n    # We assume that Name,Ticket and are not relevant information\n    # This can be confirm by the correlation matrix\n#FEATURES_SELECTION = None\n#TARGET_COLUMN = 'survived'\n#OUTPUT_IS_REGRESSION=False\n#DATE_TIME_COLUMNS = None\n#FEATURES_PRE_PROCESSING = { 'Cabin': lambda p_value : p_value[0:1] if not p_value is np.NaN else np.NaN }\n#NON_TRANSFORMABLE_COLUMNS = ['passengerid']\n#  PassengerId: Unique passenger id\n#  Survived: Survival status ('Yes' or 'No')\n#  Pclass: The class the passeger belong (1st, 2nd or 3rd class)\n#  Name: Name of the passenger\n#  Sex: The sex of the passenger ('male' of 'female')\n#  Age: The age of the passenger (in years)\n#  SibSp: # of siblings / spouses aboard the Titanic\n#  Parch: # of parents / children aboard the Titanic\n#  Ticket: No description available for this field, perhaps the travel company identifier\n#  Fare: Ticket price\n#  Cabin: Identifier of the cabin. The first character identifies the deck.\n#         This could be interesting fo the ML, creating a new feature Deck\n#  Embarked: Port of Embarkation\n\n# This dataset describes the medical records for Pima Indians and whether or not each patient will have an onset of diabetes within \fve years.\n# NOTE: Disable flag DATA_CLEANING_FLAG, this dataset is already ready to be used by ML \n#DATABASE_NAME = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv'\n#COLUMNS_TO_DROP = None\n#FEATURES_SELECTION = None\n#TARGET_COLUMN = 'class'\n#OUTPUT_IS_REGRESSION=False\n#DATE_TIME_COLUMNS = None\n#FEATURES_PRE_PROCESSING = None\n#COLUMNS_LABEL = ['preg', 'plas', 'pres (mm Hg)', 'skin (mm)', 'test (mu U/ml)', 'mass', 'pedi', 'age (years)', 'class']\n#  preg = Number of times pregnant\n#  plas = Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n#  pres = Diastolic blood pressure\n#  skin = Triceps skin fold thickness (mm)\n#  test = 2-Hour serum insulin (mu U/ml)\n#  mass = Body mass index (weight in kg/(height in m)^2)\n#  pedi = Diabetes pedigree function\n#  age = Age (years)\n#  class = Class variable (1:tested positive for diabetes, 0: tested negative for diabetes)\n\n# This dataset collects information from 100k medical appointments in Brazil and is focused on the question of whether or not patients show up for their appointment. A number of characteristics about the patient are included in each row\n#DATABASE_NAME = 'https://github.com/jbrownlee/Datasets/blob/master/pima-indians-diabetes.data.csv'\n#COLUMNS_TO_DROP = None\n#FEATURES_SELECTION = None\n#TARGET_COLUMN = 'No-show'\n#OUTPUT_IS_REGRESSION=False\n#DATE_TIME_COLUMNS = None\n#FEATURES_PRE_PROCESSING = None\n#COLUMNS_LABEL = None\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before to load and to examine our dataset, we are just going to set a number of defaults such as the settings for the plotting operation, Deep Learning parameters... (point b.2)\n\nNote: Point b.3 shall be used if you want to 'reassemble the notebook code and create a standalone Python scrypt"},{"metadata":{"trusted":true},"cell_type":"code","source":"# b.2) Set some defaults\ndef set_mp_default() -> None:\n    \"\"\"\n    Some default setting for Matplotlib plots\n    \"\"\"\n    plt.rc('figure', autolayout=True)\n    plt.rc('axes', labelweight='bold', labelsize='large', titleweight='bold', titlesize=18, titlepad=10)\n    plt.rc('image', cmap='magma')\n    warnings.filterwarnings(\"ignore\") # to clean up output cells\n    pd.set_option('precision', 3)\n    # End of function set_mp_default\n\n# Basic Deep Learning parameters\nDL_BATCH_SIZE = 32\nDL_EPOCH_NUM = 128\nDL_DROP_RATE = 0.3\n\n# Fix random values for reproductibility\nSEED_HARCODED_VALUE = 666\n\ndef set_seed(p_seed: int = SEED_HARCODED_VALUE) -> None:\n    \"\"\"\n    Random reproducability\n    :parameter p_seed: Set the seed value for random functions\n    \"\"\"\n    np.random.seed(p_seed)\n    sklearn.utils.check_random_state(p_seed)\n    #tf.set_seed(p_seed)\n    os.environ['PYTHONHASHSEED'] = str(p_seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    # End of function set_seed\n\ndef modules_versions() -> None:\n    \"\"\"\n    Print the different modules version\n    \"\"\"\n    print('----------------------------- modules_versions -----------------------------')\n    print(\"Numpy version: \" + np.__version__)\n    print(\"Pandas version: \" + pd.__version__)\n    print(\"sklearn version: \" + sklearn.__version__)\n    print(\"Tensorflow version: \" + tf.__version__)\n    print('modules_versions: Done')\n    # End of function modules_versions\n    \ndef kaggle_tpu_detection():\n    \"\"\"\n    TPU detection\n    :return: The appropriate distribution strategy\n    \"\"\"\n    print('----------------------------- kaggle_tpu_detection -----------------------------')\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n        print('kaggle_tpu_detection: Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    else:\n        strategy = tf.distribute.get_strategy() \n    print('kaggle_tpu_detection: %s' % str(strategy.num_replicas_in_sync))\n    print('kaggle_tpu_detection: %s' % str(type(strategy)))\n    print('kaggle_tpu_detection Done')\n    return strategy\n    # End of function kaggle_tpu_detection\n\n# b.3) Set execution control flags\nfrom enum import IntFlag\n\nclass ExecutionFlags(IntFlag):\n    \"\"\"\n    This class provides some execution control flags to enable/disable some part of the whole script execution\n    \"\"\"\n    NONE                     = 0b00000000 # All flags disabled\n    ALL                      = 0b11111111 # All flags enabled\n    DATA_STAT_SUMMURIZE_FLAG = 0b00000001 # Enable statitistical analyzis\n    DATA_VISUALIZATION_FLAG  = 0b00000010 # Enable data visualization\n    DATA_CLEANING_FLAG       = 0b00000100 # Enable data cleaning (feature engineering, data transform)\n    USE_NEURAL_NETWORK_FLAG  = 0b00001000 # Enable data visualization\n    # End of class ExecutionFlags","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we are ready to load our dataset and examine it to understand the data it contains. This function accept any URI (e.g. file:///... or http://... or https://...).\nLoading the dataset, you can specify or overwrite columns labels. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# c) Load dataset\ndef kaggle_load_dataset(p_url: str, p_labels: list = None) -> pd.core.frame.DataFrame:\n    \"\"\"\n    This function load the dataset specified by p_url and add the labels if required\n    :parameters p_url: The URI of the dataset (http:// or file://)\n    :parameters p_labels: The label of the columns to be used. Default: None\n    :return: The dataset handle\n    :exception: Raised if specified link is not correct\n    \"\"\"\n    print('----------------------------- kaggle_load_dataset -----------------------------')\n\n    df = None\n    if p_url.startswith('file://'):\n        df = pd.read_csv(p_url[7:])\n    elif p_url.startswith('http'):\n        ds = requests.get(p_url).content\n        df = pd.read_csv(io.StringIO(ds.decode('utf-8')))\n    if df is None:\n        raise Exception('kaggle_load_dataset: Failed to load data frame', 'url=%s' % (url))\n    if not p_labels is None:\n        df.columns = p_labels\n    # Apply post processing after loading dataset\n    if not POST_LOAD_PROCESSING is None:\n        df = POST_LOAD_PROCESSING(df)\n    # Drop columns if any\n    if not COLUMNS_TO_DROP is None:\n        df.drop(COLUMNS_TO_DROP, inplace = True, axis = 1)\n    if not FEATURES_PRE_PROCESSING is None and isinstance(FEATURES_PRE_PROCESSING, dict):\n        for key in FEATURES_PRE_PROCESSING.keys():\n            df[key] = df[key].apply(FEATURES_PRE_PROCESSING[key])\n    print('kaggle_load_dataset: Done: %s - %s' % (p_url, df.shape))\n    return df\n    # End of function kaggle_load_dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Examining the dataset means get a global overview of its data from statistical point of view, using:\n1. Some basics statistical tools such as means, stds, quartiles and correlation (2.a)\n2. Some visualization tools such as histograms, density plots (2.b)\n\nUnderstanding the data is the most important step. The kaggle_summurize_data() function provide you a lot of information to help you in this task:\n- Dataset info: It provides information about the structure of the data:\n1) The number of features (or attributes or columns), and the name (or label) of each. Here, it is important to understand what each feature means, what can be the values for this feature, take care of the units... A lot of research work to understand our problem,\n2) The types of each feature. 'object' type indicates categorical features, it means we should have to do some imputations,\n3) One or several of these feature will be our ML output and some of them could be removed later because of poor interest to solve our problem (e.g. features with huge correlation, feature reduction using ACP...),\n3) The number of observations (or samples) in the dataset. This will be useful to split our datatset into training, validation and test dataset.\n- Dataset columns labels: It indicates the name (or label) of each attributes\n- Means: It provides you the mean value for each features (also provided by statistical abstract, see below)\n- Dataset statistical abstract: It provides, for each feature, basic statistical metrics such as means, stds, quartiles...\n- Dataset Head: It displays the fisrt samples of the dataset. It provides you some indication of the value of each observation. Note that it is not suffisient to detect specific values such as NULL or NaN values, zeros, string values, categorical values... \n- Unique possible columns: It provides, for each feature, the list of the unique values. This will help you during the data transformation to rescale and center the feature values (see point 3.c). Very often, a feature with few unique values (e.g. 2 or 3) indicates also a categorical fetaure,\n- Correlation table: It provides the correlation between all couple of features and the list of the correlation values in the range > 0.7 and < -0.7. The will be used to reduce the number of features due to strong link between some features (see p_correlation_threshold parameter)\n\nNote: Here we use pandas_profiling to generate an analyze report in HTML format. This report is higly valuable because of the information it provides for each columns:\n1. Specific value indicators such as zeros, NaN...\n2. Distincts values\n3. Statistical values such as mean, min/max..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2. Summarize the dataset content in a statistical form\n# a) Descriptive statistics\ndef kaggle_summurize_data(p_df: pd.core.frame.DataFrame, p_correlation_threshold: float = 0.7) -> None:\n    \"\"\"\n    This function provides a statistical view of the current dataset\n    :parameters p_df: The dataset handle\n    \"\"\"\n    print('----------------------------- kaggle_summurize_data -----------------------------')\n    # General information\n    print('Dataset info:')\n    print(p_df.info())\n    print('----------------------------- kaggle_summurize_data: Dataset columns labels:')\n    print(p_df.columns)\n    print('----------------------------- kaggle_summurize_data: Means:')\n    print(p_df.mean())\n    print('----------------------------- kaggle_summurize_data: Dataset statistical abstract:')\n    print(p_df.describe().T)\n    print('----------------------------- kaggle_summurize_data: Dataset Head:')\n    print(p_df.head(20))\n    # NaN values\n    print('----------------------------- kaggle_summurize_data: NaN values: %d - %f' % (p_df.isnull().sum().sum(), 100 * p_df.isnull().sum().sum() / np.product(p_df.shape)))\n    print('----------------------------- kaggle_summurize_data: NaN values distribution:')\n    print(p_df.isnull().sum().sort_values(ascending = False))\n    # Zeros per columns\n    print('----------------------------- kaggle_summurize_data: Zeros per columns:')\n    for column in p_df.columns:\n        if p_df[column].dtype == 'int64' or p_df[column].dtype == 'float64':\n            zeros = p_df[column].isin([0]).sum()\n            s = p_df[column].sum()\n            print('{}: {}'.format(column, zeros, 100 * zeros / s))\n        else:\n            print('%s: Not numerical column' % column)\n    #  Unique possible columns\n    print('----------------------------- kaggle_summurize_data: Unique possible columns:')\n    for column in p_df.columns:\n        print('{}: {}'.format(column, p_df[column].unique()))\n    # Build Correlation matrix\n    print('----------------------------- kaggle_summurize_data: Correlation table:')\n    print(p_df.corr(method = 'pearson'))\n    # Extract correlation > 0.7 and < -0.7\n    print('----------------------------- kaggle_summurize_data: Correlations in range > %f and < -%f:' % (p_correlation_threshold, p_correlation_threshold))\n    corr = p_df.corr().unstack().reset_index() # Group together pairwise\n    corr.columns = ['var1', 'var2', 'corr'] # Rename columns to something readable\n    print(corr[ (corr['corr'].abs() > p_correlation_threshold) & (corr['var1'] != corr['var2']) ] )\n    # Finally, create Pandas Profiling\n    print('----------------------------- kaggle_summurize_data: Pandas Profiling:')\n    file = ProfileReport(p_df)\n    #file.to_file('./eda.html')\n    file.to_notebook_iframe()\n    print('kaggle_summurize_data: Done')\n    # End of function kaggle_summurize_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The kaggle_visualization() function provides different plot to explore the data distrubution (gaussian, exponecial...) and to detect outlier values. It will help 1) during the data cleaning and 2) later, to choose the ML algorithms (e.g. Outliers do not affect a tree-based algorithm).\nThere are two kind of data visualition:\n- The Univariate Plots which are related to each features, and\n- The Multivariate Plots which are related to interaction between features\n\nThe Univariate Plots:\n- Histograms: It provides a graphical representation of the distribution of a dataset. For a continuous numerical, it show the underlying frequency distribution or the probability  distribution of signal (see https://towardsdatascience.com/histograms-why-how-431a5cfbfcd5)\n- Density: It is the continuous form of the histogram (see above) and it shows an estimate of the continuous distribution of a feature (Gaussian distribution, exponential distribution...)\n\nThe Multivariate Plots\n- Correlationan: It provides indications about the changes between two features\n- scatter_matrix: It shows how much one feature is affected by another or the relationship between them"},{"metadata":{"trusted":true},"cell_type":"code","source":"# b) Data visualizations\ndef kaggle_visualization(p_df: pd.core.frame.DataFrame) -> None:\n    \"\"\"\n    This method provides different views of the dataset (plot)\n    :parameters p_df: The dataset handle\n    \"\"\"\n    print('----------------------------- kaggle_visualization_data -----------------------------')\n    # Histogram plot\n    print('kaggle_summurize_data: Histograms')\n    p_df.hist()\n    #plt.savefig('kaggle_histo.png')\n    plt.show()\n    # Density plot\n    print('kaggle_summurize_data: Density')\n    p_df.plot(kind='density', subplots = True, layout = (4, 4))\n    #plt.savefig('density.png')\n    plt.show()\n    # Correlation plots\n    print('kaggle_summurize_data: Correlation')\n    correlations = p_df.corr()\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(correlations, vmin = -1, vmax = 1, interpolation = 'none')\n    fig.colorbar(cax)\n    ticks = np.arange(0, len(p_df.columns), 1)\n    ax.set_xticks(ticks)\n    ax.set_yticks(ticks)\n    ax.set_xticklabels(p_df.columns)\n    ax.set_yticklabels(p_df.columns)\n    #plt.savefig('correlations_matrix.png')\n    plt.show()\n    # Pandas scatter_matrix plot\n    print('kaggle_summurize_data: scatter_matrix')\n    from pandas.plotting import scatter_matrix\n    scatter_matrix(p_df)\n    #plt.savefig('scatter_matrix.png')\n    plt.show()\n    print('kaggle_summurize_data: Done')\n    # End of function kaggle_visualization","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The function kaggle_ml_quick_and_dirty() provides a 'quick and dirty' evaluation of a ML based on RandomForestClassifier algorithm with estimators parameter set to 300\nFIXME: To be refined, does not work as expected :("},{"metadata":{"trusted":true},"cell_type":"code","source":"# c) Basic ML for a quick & dirty evaluation\ndef kaggle_ml_quick_and_dirty(p_df: pd.core.frame.DataFrame, p_validation_size: float = 0.20, p_seed:int = SEED_HARCODED_VALUE) -> np.ndarray:\n    \"\"\"\n    This method provides a first ML evalulation based on RandomForest algorithm\n    :parameters p_df: The dataset handle\n    :parameter p_validation_size: The amount of data fir training and validation. Default: 20% of the dataset will be used for validation, 80% of the dataset will be used for training\n    :return: The predictions based on the  \n    \"\"\"\n    print('----------------------------- kaggle_ml_quick_and_dirty -----------------------------')\n    p = p_df.copy()\n    # Remove NaN values\n    p.dropna(axis = 0, inplace = True)\n    # Ignore categorical values\n    p = p.select_dtypes(exclude=['object'])\n    # Build training & validation datasets\n    Y = p[TARGET_COLUMN]\n    if FEATURES_SELECTION is None:\n        X = p.drop([TARGET_COLUMN], axis = 1)\n    else:\n        X = p[FEATURES_SELECTION]\n    print('----------------------------- kaggle_ml_quick_and_dirty: X')\n    print(X.head())\n    X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size = p_validation_size, random_state = p_seed)\n    model = None\n    # Use classical model\n    if OUTPUT_IS_REGRESSION:\n        model = ensemble.RandomForestRegressor(random_state = p_seed)\n    else:\n        model = ensemble.RandomForestClassifier(random_state = p_seed)\n    # Train the model\n    model.fit(X_train, Y_train)\n    # Do predictions\n    y_predictions = model.predict(X_validation)\n    # Get scoring\n    if OUTPUT_IS_REGRESSION:\n        print('kaggle_ml_quick_and_dirty: Model R2 score: %0.4f' % r2_score(Y_validation, y_predictions))\n        print('kaggle_ml_quick_and_dirty: Model Mean absolute error regression loss: %0.4f' % mean_absolute_error(Y_validation, y_predictions))\n        print('kaggle_ml_quick_and_dirty: Mean squared error regression loss: %0.4f' % mean_squared_error(Y_validation, y_predictions))\n    else:\n        print('kaggle_ml_quick_and_dirty: Model accuracy score: %0.4f' % accuracy_score(Y_validation, y_predictions))\n        print('kaggle_ml_quick_and_dirty: Confusion matrix: %s' % str(confusion_matrix(Y_validation, y_predictions)))\n        print('kaggle_ml_quick_and_dirty: Classification report:\\n%s' % str(classification_report(Y_validation, y_predictions)))\n    print('kaggle_ml_quick_and_dirty: Done')\n    return y_predictions\n    # End of function kaggle_ml_quick_and_dirty","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we need to do a break and we need:\n1. to understand exactly what each column is?\n2. to learn from the results we got\nFor instance:\n1. Let's take a look to the number of '0' value in each columns and tryu to understand what does it mean? In PIMA diabetes dataset, what does it mean a Glucose or a blood pressure value of 0? It's not possible!!! In this case, the solution is to replace '0' bay a NaN value and the Impute process will do the job ;) Idem with maximal value per column\n\nSo, the next step is to prepare the data for ML. Usually, you have better result when all the features (features and outputs) are in numerical format (int or float).\n\n1. Feature engineering. It eliminates NULL or NaN values, duplicate values, and it transforms date/time column, categorical columns into numerical fetures. It identifies & handles outliers... (3.a). Categorical columns are usually of type object and the objective here is to transform these categorical columns into numerical columns. Date/time columns can be either object (e.g. date/time in string format) of type datetime64[ns]. For sepcific features such as 'Age', it is possible to create new feature grouping the Age values per range, between from the lower Age value to the upper Age value\n2. Data transformation. It applies some numerical transformation such as standardization of features... (3.b)\n3. Features selection. It selects and prepares the dataset for the training and the validation (3.c)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3. Prepare the dataset for your Machine Learning processing\n# a) Data Cleaning\nEncoders = dict()\nEncoder_Instance = LabelEncoder() # Use global variable for future reverse features engineering\nImputer_Instance = None\ndef kaggle_features_engineering(p_df: pd.core.frame.DataFrame, p_missing_value_method: str = 'drop_columns', p_duplicated_value_method: str = 'drop_columns', p_categorical_onehot_threshold: int = 10, p_date_time_columns: list = None, p_date_time_engineering: str = 'python_time') -> pd.core.frame.DataFrame:\n    \"\"\"\n    This function performs a cleaning of the dataset to remove null values, duplicate values, based on the specified method\n    :parameters p_df: The dataset handle\n    :parameters p_missing_value_method: The method to cleanup NaN values in the dataset ('drop_columns', 'drop_lines', 'mean', 'median'). Default: 'drop_columns'\n    :parameters p_duplicated_value_method: The method to cleanup duplicated in the dataset ('drop_columns', 'drop_lines', 'mean', 'median'). Default: 'drop_columns'\n    :parameters p_categorical_onehot_threshold: The maximum cardinality to apply OneHotEncoder to a categorical variable. Defaut: 5\n    :parameters p_date_time_engineering: The method to convert Date/Time. Default: 'python_time'\n    :return: The dataset after the cleanup process\n    \"\"\"\n    global Encoders, Encoder_Instance, Imputer_Instance\n    \n    print('----------------------------- kaggle_features_engineering -----------------------------')\n    # Cleanup dataset\n    old_shape = p_df.shape\n    p = p_df.copy() # The final dataset\n\n    # Convert Date/time columns\n    # dtype = 'datetime64[ns]'\n    print('----------------------------- kaggle_features_engineering: Processing Date/Time columns')\n    if p_date_time_columns is not None: # Process specified columns\n        # Check date/time formats\n        for column in p_date_time_columns: # TODO Check if all DateTime values have the same format, i.e. same length\n            date_lengths = p[column].str.len().value_counts()\n            print('kaggle_features_engineering: %s lengths:' % column)\n            print('%s - %d' % (str(date_lengths), len(date_lengths)))\n            # End of 'for' statement\n        p[p_date_time_columns] = p[p_date_time_columns].astype('datetime64[ns]')\n        p[p_date_time_columns] = p[p_date_time_columns].astype('int64')\n        print('kaggle_features_engineering: Date/time columns processed')\n    else:\n        print('kaggle_features_engineering: No date/time values')\n    # Be sure there is no more 'datetime64[ns]' types in the dataset\n    datetime_columns = [col for col in p.columns if p[col].dtype == 'datetime64[ns]']\n    if len(datetime_columns) != 0:\n        raise Exception('kaggle_features_engineering: There still has datetime64[ns] type in dataset', 'method=%s' % str(p.info()))\n\n    # Find N/A values for categorical columns and replace them by the value with the higher frequency\n    print('----------------------------- kaggle_features_engineering: Processing NaN values')\n    categorical_columns_with_nan = [col for col in p.columns if p[col].dtype == 'object' and p[col].isna().sum() != 0]\n    if len(categorical_columns_with_nan) != 0:\n        print('----------------------------- kaggle_features_engineering: Impute NaN values for categorical columns with MAX value')\n        for col in categorical_columns_with_nan:\n            p[col].fillna(p[col].value_counts().idxmax(), inplace = True)\n            # End of 'for'statement\n        # Check that there are no more categorical columns with NaN\n        categorical_columns_with_nan = [col for col in p.columns if p[col].dtype == 'object' and p[col].isna().sum() != 0]\n        if len(categorical_columns_with_nan) != 0:\n            raise Exception('kaggle_features_engineering: There still has categorical columns with NaN value in dataset', 'method=%s' % str(categorical_columns_with_nan))\n    else:\n        print('----------------------------- kaggle_features_engineering: No NaN value in categorical columns')\n    # Use Imputation to replace NaN in numerical columns\n    print('----------------------------- kaggle_features_engineering: Impute NaN values for numerical columns with %s method' % p_missing_value_method)\n    numerical_columns_with_nan = [col for col in p.columns if (p[col].dtype == 'int64' or p[col].dtype == 'float64') and p[col].isna().sum() != 0]\n    if len(numerical_columns_with_nan) != 0:\n        print('kaggle_features_engineering: cols_with_missing: %s' % (str(numerical_columns_with_nan)))\n        # Find rows with missing values\n        rows_with_null = p[numerical_columns_with_nan].isnull().any(axis=1)\n        rows_with_missing = p[rows_with_null]\n        print('kaggle_features_engineering: rows_with_missing: %s/%s' % (rows_with_missing.shape[0], p.shape[0]))\n        # Impute missimg values\n        if p_missing_value_method == 'drop_columns' and len(numerical_columns_with_nan) != 0: # Impute removing columns\n            p = p.drop(numerical_columns_with_nan, axis = 1)\n        elif p_missing_value_method == 'drop_lines' and len(rows_with_null) != 0: # Impute removing rows\n            p = p.dropna()\n        else: # Imputate using SimpleImputer\n            labels = p.columns # Save labels\n            if p_missing_value_method == 'mean':\n                Imputer_Instance = SimpleImputer(strategy='mean')\n            elif p_missing_value_method == 'median':\n                Imputer_Instance = SimpleImputer(strategy='median')\n            else:\n                raise Exception('kaggle_features_engineering: Invalid method', 'method=%s' % (p_missing_value_method))\n            p[numerical_columns_with_nan] = pd.DataFrame(Imputer_Instance.fit_transform(p[numerical_columns_with_nan]))\n            # Restore column names\n            p.columns = labels\n            print('kaggle_features_engineering: Cleaning NaN values: old_shape: %s / new shape: %s' % (str(old_shape), str(p.shape)))\n    else:\n        print('kaggle_features_engineering: No missing values in numerical columns')\n    print('----------------------------- kaggle_features_engineering: After First round:')\n    print(p.head())\n    print(p.describe().T)\n\n    # Search for categorical variables\n    print('----------------------------- kaggle_features_engineering: Encoding categorical columns:')\n    categorical_columns = [col for col in p.columns if p[col].dtype == 'object']\n    new_categorical_columns = None\n    if len(categorical_columns) != 0:\n        print('kaggle_features_engineering: categorical_columns: ' + str(categorical_columns))\n        # Compute cardinalities of the categorical vairiables\n        categorical_columns_cardinalities = list(map(lambda col: p[col].nunique(), categorical_columns))\n        print('kaggle_features_engineering: categorical_columns_cardinalities: ')\n        print(categorical_columns_cardinalities)\n        print('kaggle_features_engineering: OneHotEncoder thresholds: %d' % p_categorical_onehot_threshold)\n        # Apply OneHot encoding to categorical value with very low cardinality\n        cols_processed = []\n        new_categorical_columns = categorical_columns.copy()\n        for i in range(0, len(categorical_columns)):\n            if categorical_columns_cardinalities[i] == 2: # Use BinaryEncoder\n                print('kaggle_features_engineering: LabelBinarizer: %s' % categorical_columns[i])\n                Encoders[categorical_columns[i]] = LabelBinarizer()\n                p[categorical_columns[i]] = Encoders[categorical_columns[i]].fit_transform(p[categorical_columns[i]])\n                new_categorical_columns.remove(categorical_columns[i])\n            elif categorical_columns_cardinalities[i] <= p_categorical_onehot_threshold:\n                print('kaggle_features_engineering: OneHotEncoder: %s' % categorical_columns[i])\n                Encoders[categorical_columns[i]] = OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n                new_col = Encoders[categorical_columns[i]].fit_transform(pd.DataFrame(p[categorical_columns[i]]))\n                new_col = pd.DataFrame(new_col, columns = [(categorical_columns[i] + \"_\" + str(j)) for j in range(new_col.shape[1])])\n                new_col.index = p[categorical_columns[i]].index\n                p = p.drop(categorical_columns[i], axis = 1).join(new_col)\n                cols_processed.append(categorical_columns[i])\n                # Update the list of the categorical columns\n                new_categorical_columns.remove(categorical_columns[i])\n                new_categorical_columns.extend(new_col.columns.tolist())\n            else:\n                # Just drop them for the time being\n                # FIXME To be refined using TargetEncoder\n                p.drop(categorical_columns[i], axis = 1, inplace = True)\n                # Update the list of the categorical columns\n                new_categorical_columns.remove(categorical_columns[i])\n            # End of 'for' statement\n        if len(cols_processed) != 0:\n            print('kaggle_features_engineering: Encoders applied on %s' % str(cols_processed))\n            print('kaggle_features_engineering: New datase structure:')\n            print(p.describe().T)\n            print(p.head())\n            categorical_columns = [col for col in p.columns if p[col].dtype == 'object']\n            print('kaggle_features_engineering: Cleaning categorical values: old_shape: %s / new shape: %s' % (str(old_shape), str(p.shape)))\n            print('kaggle_features_engineering: new Categorical columns:')\n            print(categorical_columns)\n            # Compute new cardinalities of the categorical vairiables\n            categorical_columns_cardinalities = list(map(lambda col: p[col].nunique(), categorical_columns))\n            print('kaggle_features_engineering: New categorical_columns_cardinalities: ')\n            print(categorical_columns_cardinalities)\n        # TODO: Drop categorical variables with extrem cardinalities\n        # Encode categorical variables using numerical mapping\n        for col in categorical_columns:\n            p[col] = Encoder_Instance.fit_transform(p[col].astype(str))\n            # End of 'for' statement\n            print('kaggle_features_engineering: Labelling:')\n            print('kaggle_features_engineering: after second round:')\n            print(p.head())\n            # End of 'for' statement\n    else:\n        print('kaggle_features_engineering: No categorical values')\n    # Be sure there is no more 'object' types in the dataset\n    categorical_columns = [col for col in p.columns if p[col].dtype == 'object']\n    if len(categorical_columns) != 0:\n        raise Exception('kaggle_features_engineering: There still has object type in dataset', 'method=%s' % str(categorical_columns))\n    print('----------------------------- kaggle_features_engineering: After Second round:')\n    print(p.head())\n    print(p.describe().T)\n\n    # TODO: Removing duplicated records\n    # Build Correlation matrix\n    print('----------------------------- kaggle_features_engineering: Correlation table:')\n    # Extract correlation > 0.7 and < -0.7\n    cor_matrix = p_df.corr(method = 'pearson')\n    print('----------------------------- kaggle_features_engineering: cor_matrix:')\n    print(cor_matrix)\n    upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k = 1).astype(np.bool))\n    print('----------------------------- kaggle_features_engineering: upper_tri:')\n    print(upper_tri)\n    p_correlation_threshold = 0.7\n    print('----------------------------- kaggle_features_engineering: Correlations in range > %f and < -%f:' % (p_correlation_threshold, p_correlation_threshold))\n    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > p_correlation_threshold)]\n    print('kaggle_features_engineering: Drop ', to_drop)\n    # Drop correlated columns\n    p.drop(to_drop, axis = 1, inplace = True)\n    print('----------------------------- kaggle_features_engineering: After Third round:')\n    print(p.head())\n    print(p.describe().T)\n    \n    # Identifying & handling outliers\n    # Covered by feature transformation: Scaling will remove the outliers \n\n #   raise Exception('Stop', 'Stop')\n\n    print('kaggle_features_engineering: ', list(new_categorical_columns)) \n    print('kaggle_features_engineering: Done') \n    return p, new_categorical_columns\n    # End of function kaggle_features_engineering","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are different kinds of data transformation:\n- Standardization: It removes the mean and scaling to unit variance of the feature (see point 2.a)\n- Scaling: It rescales the feature values in a range of 0 and 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"# b) Data Transforms\ndef kaggle_data_transform(p_df: pd.core.frame.DataFrame, p_columns:list = None, p_transform: str = 'standard') -> pd.core.frame.DataFrame:\n    \"\"\"\n    Apply data transformation to the provided dataset\n    :parameters p_df: The dataset handle\n    :parameters p_columns: The columns to apply transformation (e.g. we don't apply transformation on categorical column)\n    :parameter p_transform: The type of transormation. Default: 'standard'\n                            'standard': Remove the mean and scaling to unit variance\n                            'scale': Scale feature to a Min/max range\n                            'abs_scale': Scale feature to a range [-1, 1]\n    :return: The dataset after features selection\n    \"\"\"\n    print('----------------------------- kaggle_data_transform -----------------------------')\n    transform = None\n    if p_transform == 'standard':\n        # Standardization, or mean removal and variance scaling\n        transform = StandardScaler()\n    elif p_transform == 'scale':\n        # Scaling features to a range\n        transform = MinMaxScaler()\n    elif p_transform == 'abs_scale':\n        # Scaling features to a range\n        transform = MaxAbsScaler()\n    else:\n        raise Exception('kaggle_data_transform: Wrong parameters', 'p_transform=%s' % p_transform)\n    p = None\n    if p_columns is None: # Apply transformamtion to the whole dataset\n        p = transform.fit_transform(p_df)\n        p = pd.DataFrame(data = p, columns = p_df.columns)\n    else:\n        p = p_df.copy()\n        for column in p_columns:\n            p[column] = pd.DataFrame(transform.fit_transform(pd.DataFrame(p[column])), columns = [column])\n    print('kaggle_data_transform: Dataset Head:')\n    print(p.head())\n    \n    print('kaggle_data_transform: Done')\n    return p\n    # End of function kaggle_data_transform","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The kaggle_feature_selection() function reorganize the whole dataset to put the fetaures first and the outputs after. \nIf the parameter p_attributes is None, all the features excepted the outputs are concidered as inputs of the ML."},{"metadata":{"trusted":true},"cell_type":"code","source":"# b) Feature Selection\ndef kaggle_feature_selection(p_df: pd.core.frame.DataFrame, p_target: str, p_attributes: list = None) -> pd.core.frame.DataFrame:\n    \"\"\"\n    Reorganize the dataset to keep only provided attributes, the target column is the last column of the new dataset\n    :parameters p_df: The dataset handle\n    :parameter p_target The outputs of the Machine Learning\n    :parameter p_attributes The inputs of the Machine Learning. Default: None, means that all the features are used to train and validate the model\n    :return: The dataset after features selection\n    \"\"\"\n    print('----------------------------- kaggle_feature_selection -----------------------------')\n    p = p_df.copy()\n    y_values = p[[p_target]]\n    print('----------------------------- kaggle_feature_selection: y_values:')\n    print(y_values.head())\n    p.drop(p_target, inplace = True, axis = 1)\n    x_values = None\n    print('kaggle_feature_selection: p_attributes: ', p_attributes)\n    if p_attributes is None:\n        x_values = p\n    else:\n        x_values = p[p_attributes]\n    print('----------------------------- kaggle_feature_selection: x_values:')\n    print(x_values.head())\n    p = pd.concat([x_values, y_values], axis=1)\n    print('----------------------------- kaggle_feature_selection: new dataset:')\n    print(p.head())\n\n    print('kaggle_feature_selection: Done')\n    return p\n    # End of function kaggle_feature_selection","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After cleaning and transforming the initial dataset, we can use it to train and validate our ML. So, The next step is to shuffle our dataset in three different 'sub-datasets' (point 4.a):\n1. The training dataset, used to evaluate the ML models\n2. The validation dataset, used to validate the selected model\n3. The test dataset, use to test the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4. Evaluate Algorithms\n# a) Split-out validation dataset\ndef kaggle_split_dataset(p_df: pd.core.frame.DataFrame, p_validation_size: int = 0.20, p_random_state: int = SEED_HARCODED_VALUE) -> list:\n    \"\"\"\n    The dataset is shuffled in three different 'sub' datasets (4.a): 1) the training dataset, 2) the validation dataset, 3) the test dataset\n    10% of the dataset is used for testing the model\n    :parameter p_df: The raw dataset\n    :parameter p_validation_size: The amount of data for training and validation. Default: 0.2 (i.e. 20%) of the 90 % of the dataset will be used for validation, 80% of the 90% of dataset will be used for training.    :return: The list of the three datasets (x_training, x_validation, y_training, y_validation, x_test, y_test)\n    \"\"\"\n    print('----------------------------- kaggle_split_dataset -----------------------------')\n    # First, extract test samples (10% of the full dataset)\n    p = p_df.copy()\n    df_values_columns = p.columns\n    s = p.sample(frac = 0.1, random_state = p_random_state)\n    df_values = s.values\n    test_inputs = df_values[:,0:len(p.columns) - 1] # X contains the first columns, this is the inputs of the ML\n    test_inputs_columns = df_values_columns[:len(p.columns) - 1]\n    test_outputs = df_values[:,len(p.columns) - 1]  # Y contains the Target column, this is the output, this is the output of the ML\n    test_outputs_columns = [df_values_columns[-1]]\n    p = p[~p.isin(s)].dropna() # Remove samples\n    print('----------------------------- kaggle_split_dataset: test_inputs/test_outputs')\n    print(test_inputs_columns)\n    print(test_outputs_columns)\n    print(test_inputs[:5])\n    print(test_outputs[:5])\n    # Extract training and validation dataset\n    df_values = p.values\n    ml_inputs = df_values[:,0:len(p.columns) - 1] # X contains the first columns, this is the inputs of the ML\n    ml_outputs = df_values[:,len(p.columns) - 1]  # Y contains the Target column, this is the output, this is the output of the ML\n    print('----------------------------- kaggle_split_dataset: ml_inputs/ml_outputs')\n    print(ml_inputs[:5])\n    print(ml_outputs[:5])\n    result = model_selection.train_test_split(ml_inputs, ml_outputs, train_size = 1 - p_validation_size, random_state = p_random_state)\n    result.append(test_inputs)\n    result.append(test_outputs)\n    result.append([test_inputs_columns, test_outputs_columns])\n\n    print('kaggle_split_dataset: Done')\n    return result\n    # End of function kaggle_split_dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can apply different models (linear, non-linear, ensemble...) to build our ML and evaluate their efficiency (4.b)\n\nTODO Enhance model scoring method for both Regressor & Classifier\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# b) Check models\ndef kaggle_check_models(p_models: list, p_inputs_training_df: pd.core.frame.DataFrame, p_outputs_training_df: pd.core.frame.DataFrame, p_kparts: int = 10, p_random_state: int = SEED_HARCODED_VALUE, p_cross_validation: str = 'k-fold', p_scoring: str = 'accuracy') -> list:\n    \"\"\"\n    Apply different models to train our Machine Learning and evaluate their efficiency\n    :parameter p_models: A list of models to use for to train the Machine Learning\n    :parameter p_inputs_training_df: The training inputs dataset (training attributes)\n    :parameter p_outputs_training_df: The training output dataset (training target)\n    :parameter p_inputs_valid_df: The validation inputs dataset (validation attributes)\n    :parameter p_outputs_valid_df: The validation output dataset (validation target)\n    :parameter p_kparts: \n    :parameter p_random_state: \n    :parameter p_cross_validation: \n    :parameter p_scoring: \n    :return: The list of couple (result, model name)\n    \"\"\"\n    print('----------------------------- kaggle_check_models -----------------------------')\n    results = []\n    names = []\n    for name, model in p_models:\n        print('kaggle_check_models: Processing %s with type %s' % (name, type(model)))\n        # Create train/test indices to split data in train/test sets\n        if p_cross_validation == 'k-fold':\n            kfold = model_selection.KFold(n_splits = p_kparts, random_state = p_random_state, shuffle = True) # K-fold Cross Validation\n        elif p_cross_validation == 'k-fold':\n            kfold = model_selection.StratifiedKFold(n_splits = p_kparts, random_state = p_random_state, shuffle = True) # K-fold Cross Validation\n        else:\n            raise Exception('kaggle_check_models: Wrong parameters', 'p_cross_validation:%s' % p_cross_validation)\n        cv_results = None\n        # Evaluate model performance\n        if p_cross_validation == 'k-fold' or p_cross_validation == 'k-stratifie-kfold':\n            cv_results = model_selection.cross_val_score(model, p_inputs_training_df, p_outputs_training_df, cv = kfold, scoring = p_scoring)\n        else:\n            cv_results = model_selection.cross_val_score(model, p_inputs_training_df, p_outputs_training_df, cv = LeaveOneOut(), scoring = p_scoring)\n        print('kaggle_check_models: cv_result=%s' % str(cv_results))\n        results.append(cv_results)\n        names.append(name)\n        msg = 'kaggle_check_models: %s metric: %s: %f (%f)' % (p_scoring, name, cv_results.mean(), cv_results.std())\n        print(msg)\n        print()\n        # End of 'for' loop\n\n    print('kaggle_check_models: Done')\n    return results, names\n    # End of function kaggle_check_models","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then, we select the best model based on the scoring (4.c)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def kaggle_compare_algorithms_perf(p_names: list, p_metrics: list, p_title: str, p_x_label: str, p_y_label:str) -> int:\n    print('----------------------------- kaggle_compare_algorithms_perf -----------------------------')\n    # Extract means & std\n    means = []\n    stds = []\n    for i in range (len(p_names)):\n        cv_results = p_metrics[i]\n        means.append(cv_results.mean())\n        stds.append(cv_results.std())\n        # End of 'for' statement\n    # Display means/standard deviation\n    plt.title(p_title)\n    plt.xlabel(p_x_label)\n    plt.ylabel(p_y_label)\n    plt.errorbar(p_names, means, stds, linestyle='None', marker='^')\n    #plt.savefig('kaggle_algorithms_comparison.png')\n    plt.show()\n    # Select the best algorithm\n    m = np.array(means)\n    maxv = np.amax(m)\n    idx = np.where(m == maxv)[0][0]\n    print('kaggle_compare_algorithms_perf: Max value: %d:%f +/- %f ==> %s' % (idx, maxv, 2 * stds[idx], p_names[idx]))\n\n    print('kaggle_compare_algorithms_perf: Done')\n    return idx\n    # End of function kaggle_compare_algorithms_perf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sorry, work still in progress"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 5. Improve Accuracy\n# a) Algorithm Tuning\ndef kaggle_algorithm_tuning(p_algorithm: list, p_inputs_training_df: pd.core.frame.DataFrame, p_outputs_training_df: pd.core.frame.DataFrame, p_validation_data: list = None):\n    print('----------------------------- kaggle_algorithms_tuning -----------------------------')\n    print('----------------------------- kaggle_algorithms_tuning: %s' % p_algorithm.__class__.__name__)\n    model = p_algorithm[1]\n    if not p_algorithm.__class__.__name__.startswith('Keras'):\n        model.fit(p_inputs_training_df, p_outputs_training_df)\n    else:\n        early_stopping = keras.callbacks.EarlyStopping(patience = 5, min_delta = 0.001, restore_best_weights = True)\n        history = model.fit(p_inputs_training_df, p_outputs_training_df, validation_data = p_validation_data, epochs = DL_EPOCH_NUM, batch_size = DL_BATCH_SIZE * strategy.num_replicas_in_sync, callbacks = [early_stopping])\n        print('----------------------------- kaggle_algorithms_tuning: loss/val_loss plot')\n        history = pd.DataFrame(history.history)\n        history.loc[:, ['loss', 'val_loss']].plot(title=\"loss/val_loss\")\n        print('kaggle_compare_algorithms_perf: Minimum Validation Loss: {:0.4f}' & history_df['val_loss'].min())\n\n    print('----------------------------- kaggle_algorithms_tuning: model summary:')\n    print(model)\n    print('kaggle_compare_algorithms_perf: Done')\n    return model\n    # End of function kaggle_algorithm_tuning\n# b) Ensembles\n# 6. Finalize Model\n# a) Predictions on validation dataset\ndef kaggle_validation_prediction(p_model, p_inputs, p_expected_outputs) -> np.ndarray:\n    \"\"\"\n    Executes prediction (p_inputs) and compares outputs against expected outputs (Validation) using the specified ML model\n    :parameter p_model: \n    :parameter p_inputs: \n    :parameter p_expected_outputs: \n    \"\"\"\n    print('----------------------------- kaggle_validation_prediction -----------------------------')\n    print('kaggle_validation_prediction: model=%s - is_class:%s - is_regr:%s' % (p_model.__class__.__name__, str(is_classifier(p_model)), str(is_regressor(p_model))))\n    y_predictions = p_model.predict(p_inputs)\n    if is_regressor(p_model) or p_model.__class__.__name__ == 'KerasRegressor': # Regression metrics (continuous target values)\n        print('kaggle_validation_prediction: Model R2 score=%f' % (r2_score(p_expected_outputs, y_predictions)))\n        print('kaggle_validation_prediction: : Model Mean absolute error regression loss (MAE): %0.4f' % mean_absolute_error(p_expected_outputs, y_predictions))\n        print('kaggle_validation_prediction: : Model Mean squared error regression loss (MSE): %0.4f' % mean_squared_error(p_expected_outputs, y_predictions))\n        print('kaggle_validation_prediction: : Mean squared error regression loss (RMSE): %0.4f' % np.sqrt(mean_squared_error(p_expected_outputs, y_predictions)))\n        # Analyze residual errors\n        plt.scatter(p_expected_outputs, y_predictions)\n        plt.show()\n        # TODO Interpreting the Cofficients if possible\n    elif is_classifier(p_model) or p_model.__class__.__name__ == 'KerasClassifier': # Classification metrics (class target values)\n        print('kaggle_validation_prediction: accuracy=%s' %(accuracy_score(p_expected_outputs, y_predictions)))\n        print('kaggle_validation_prediction: ROC=%s' %(roc_auc_score(p_expected_outputs, y_predictions)))\n        print('kaggle_validation_prediction: Confusion_matrix:%s' % str(confusion_matrix(p_expected_outputs, y_predictions)))\n        print('kaggle_validation_prediction: Classification report:\\n%s' % str(classification_report(p_expected_outputs, y_predictions)))\n    else:\n        raise Exception('kaggle_validation_prediction: Invalid model')\n    print('kaggle_validation_prediction: prediction is %s' % (str(y_predictions)))\n\n    print('kaggle_validation_prediction: Done')\n    return y_predictions\n    # End of function kaggle_validation_prediction\n\ndef kaggle_prediction(p_model, p_inputs) -> np.ndarray:\n    \"\"\"\n    Executes prediction (p_inputs) using the specified ML model\n    :parameter p_model: \n    :parameter p_inputs:  \n    \"\"\"\n    print('----------------------------- kaggle_prediction -----------------------------')\n    inputs = []\n    inputs.append(p_inputs)\n    y_prediction = p_model.predict(inputs)\n    print('kaggle_prediction: prediction is %s' %(str(y_prediction)))\n    print('kaggle_prediction: Done')\n    return y_prediction\n# b) Create standalone model on entire training dataset\n# TODO","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The functions below are some helper to save the model and to save our Machine Learning outcomes in Kaggle compete format."},{"metadata":{"trusted":true},"cell_type":"code","source":"# c) Save model for later use\ndef kaggle_save_model(p_model, p_file_name:str) -> None:\n    \"\"\"\n    Save the provided model in JSON format and the weights on HD5 format\n    :parameter p_model: The ML model to save\n    :parameter p_file_name: The file name woithout extension file (e.g. './MyModel')\n    \"\"\"\n    print('----------------------------- kaggle_save_model -----------------------------')\n    # Serialize the model\n    pickle.dump(p_model, open(p_file_name + '.pkl', 'wb'))\n    print('kaggle_save_model: Done: %s' % (p_file_name + '.pkl'))\n    # End of function kaggle_save_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The function kaggle_explore_ml() provides some insights from our ML."},{"metadata":{"trusted":true},"cell_type":"code","source":"def kaggle_explore_ml(p_model, p_x_validation: pd.core.frame.DataFrame, p_y_validation: pd.core.frame.DataFrame, p_random_state:int = SEED_HARCODED_VALUE) -> None:\n    \"\"\"\n    Apply feature importance concept to our ML \n    :parameter p_model: The predictions to save\n    \"\"\"\n    print('----------------------------- kaggle_explore_ml -----------------------------')\n    result = permutation_importance(p_model, p_x_validation, p_y_validation, n_repeats = 32, random_state = p_random_state)\n    sorted_idx = result.importances_mean.argsort()\n    print('----------------------------- kaggle_explore_ml: result:')\n    print(sorted_idx)\n\n    fig, ax = plt.subplots()\n    ax.boxplot(result.importances[sorted_idx].T, vert = False, labels = p_x_validation.columns[sorted_idx])\n    ax.set_title(\"Permutation Importances (Validation set)\")\n    fig.tight_layout()\n    plt.show()\n    print('kaggle_explore_ml: Done')\n    # End of function kaggle_explore_ml","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The code below is specific to machine learning. It provides callbacks to create DL models."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Start of main application\nDL_INPUT_SHAPE = None\n\ndef kaggle_create_sequential_classifier_model(p_optimizer:str = 'adam', p_loss:str = 'binary_crossentropy', p_metrics:list = ['accuracy']) -> tf.keras.Sequential:\n    \"\"\"\n    Build a Neural network model for classification\n    \"\"\"\n    print('----------------------------- kaggle_create_sequential_classifier_model -----------------------------')\n    model = tf.keras.Sequential([\n            tf.keras.layers.BatchNormalization(input_shape = DL_INPUT_SHAPE),\n            tf.keras.layers.Dense(64, activation='relu'),\n            tf.keras.layers.Dropout(rate = DL_DROP_RATE),\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.Dense(64, activation='relu'),\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.Dropout(rate = DL_DROP_RATE),\n            tf.keras.layers.Dense(64, activation='relu'),\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.Dropout(rate = DL_DROP_RATE),\n            tf.keras.layers.Dense(1, activation='sigmoid'),\n    ])\n    model.compile(optimizer=p_optimizer, loss = p_loss, metrics = p_metrics)\n    return model\n    # End of function kaggle_create_sequential_classifier_model\n\ndef kaggle_create_sequential_regressor_model(p_optimizer:str = 'adam', p_loss:str = 'mae', p_metrics:list = ['mae']) -> tf.keras.Sequential:\n    \"\"\"\n    Build a Neural network model for regression\n    \"\"\"\n    print('----------------------------- kaggle_create_sequential_regressor_model -----------------------------')\n    model = tf.keras.Sequential([\n            tf.keras.layers.BatchNormalization(input_shape = DL_INPUT_SHAPE),\n            tf.keras.layers.Dense(64, activation='relu'),\n            tf.keras.layers.Dropout(rate = DL_DROP_RATE),\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.Dense(64, activation='relu'),\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.Dropout(rate = DL_DROP_RATE),\n            tf.keras.layers.Dense(64, activation='relu'),\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.Dropout(rate = DL_DROP_RATE),\n            tf.keras.layers.Dense(1, activation='relu'),\n    ])\n    model.compile(optimizer=p_optimizer, loss = p_loss, metrics = p_metrics)\n    return model\n    # End of function kaggle_create_sequential_regressor_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finaly, here is the entry point function and the main call:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def kaggle_main() -> None:\n    global DL_INPUT_SHAPE\n    \n    # Set defaults\n    set_seed()\n    set_mp_default()\n    \n    # Current path\n    print(os.path.abspath(os.getcwd()))\n    # Kaggle current path and files\n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n\n    # Modules version\n    modules_versions()\n\n    # Parse arguments. Used only if this notebook code is used as a standalone Python script\n    #flags = ExecutionFlags.NONE\n    flags = ExecutionFlags.ALL & ~ExecutionFlags.USE_NEURAL_NETWORK_FLAG & ~ExecutionFlags.DATA_STAT_SUMMURIZE_FLAG & ~ExecutionFlags.DATA_VISUALIZATION_FLAG\n    #parser = argparse.ArgumentParser()\n    #parser.add_argument('--summarize', help = 'Process statistical analyze', action='store_true')\n    #parser.add_argument('--summarize-only', help = 'Process only statistical analyze', action='store_true')\n    #parser.add_argument('--visualize', help = 'Generate different plots based on statistical analyze', action='store_true')\n    #parser.add_argument('--no-data-cleaning', help = 'Do not apply Data Cleaning', action='store_true')\n    #parser.add_argument('--neural-network', help = 'Use neural network as ML', action='store_true')\n    #args = parser.parse_args()\n    #if args.summarize or args.summarize_only:\n    #    flags |= ExecutionFlags.DATA_STAT_SUMMURIZE_FLAG\n    #if args.visualize:\n    #    flags |= ExecutionFlags.DATA_VISUALIZATION_FLAG\n    #if args.no_data_cleaning:\n    #    flags |= ~ExecutionFlags.DATA_CLEANING_FLAG\n    #if args.neural_network:\n    #    flags |= ExecutionFlags.USE_NEURAL_NETWORK_FLAG\n    \n    # TODO Uncomment if using Pima Indians iabetes dataset\n    #flags &= ~ExecutionFlags.DATA_CLEANING_FLAG\n    print('generic template approach to ''play'' with the Machine Learning concepts: flags=%s' % str(flags))\n\n    strategy = None\n    if flags & ExecutionFlags.USE_NEURAL_NETWORK_FLAG == ExecutionFlags.USE_NEURAL_NETWORK_FLAG:\n        strategy = kaggle_tpu_detection()\n\n    df = kaggle_load_dataset(p_url = DATABASE_NAME, p_labels = COLUMNS_LABEL)\n\n    # Do a basic ML evaluation as reference for the end\n    y_basic_predictions = kaggle_ml_quick_and_dirty(df)\n\n    numerical_columns = None\n    categorical_columns = None\n    if flags & ExecutionFlags.DATA_STAT_SUMMURIZE_FLAG == ExecutionFlags.DATA_STAT_SUMMURIZE_FLAG:\n        kaggle_summurize_data(df)\n    #    if args.summarize_only:\n    #        return\n\n    if flags & ExecutionFlags.DATA_VISUALIZATION_FLAG == ExecutionFlags.DATA_VISUALIZATION_FLAG:\n        kaggle_visualization(df)\n        \n    if flags & ExecutionFlags.DATA_CLEANING_FLAG == ExecutionFlags.DATA_CLEANING_FLAG:\n        df, categorical_column = kaggle_features_engineering(df, p_date_time_columns = DATE_TIME_COLUMNS, p_missing_value_method = 'mean')\n        # Extract non  categorical columns based on categorical_column list\n        columns_to_transform = list(set(df.columns) - set(categorical_column))\n        if not NON_TRANSFORMABLE_COLUMNS is None:\n            columns_to_transform = list(set(columns_to_transform) - set(NON_TRANSFORMABLE_COLUMNS))\n        print('Main: columns_to_transform = ', columns_to_transform)\n        df = kaggle_data_transform(df, columns_to_transform, p_transform = 'scale')\n\n    df = kaggle_feature_selection(df, p_target = TARGET_COLUMN, p_attributes = FEATURES_SELECTION)\n\n    ml_inputs_training_df, ml_inputs_validation_df, ml_outputs_training_df, ml_outputs_validation_df, ml_inputs_test_df, ml_outputs_test_df, columns_list = kaggle_split_dataset(df)\n    print('Main: training dataset shape: ', ml_inputs_training_df.shape)\n    print('Main: validation dataset shape: ', ml_inputs_validation_df.shape)\n    print('Main: test dataset shape: ', ml_inputs_test_df.shape)\n    print('Main: columns: ', columns_list)\n    \n    # Stacking models\n    models = []\n    DL_INPUT_SHAPE = [ml_inputs_training_df.shape[1]]\n    scoring = None\n    if OUTPUT_IS_REGRESSION: # Use regression algorithms\n        scoring = 'r2' # 'r2' or 'neg_mean_absolute_error'\n        models.append(('LR', linear_model.LinearRegression()))\n        models.append(('LASSO', linear_model.Lasso()))\n        models.append(('EN', linear_model.ElasticNet()))\n        models.append(('KNN', neighbors.KNeighborsRegressor()))\n        models.append(('CART', tree.DecisionTreeRegressor(max_leaf_nodes = 256, random_state = SEED_HARCODED_VALUE)))\n        models.append(('XGB', xgb.XGBRegressor(random_state = SEED_HARCODED_VALUE)))\n        models.append(('RF', ensemble.RandomForestRegressor(n_estimators = 128, random_state = SEED_HARCODED_VALUE)))\n        #models.append(('SVR', svm.SVR()))\n        if flags & ExecutionFlags.USE_NEURAL_NETWORK_FLAG == ExecutionFlags.USE_NEURAL_NETWORK_FLAG: # DL model\n            model = KerasRegressor(build_fn = kaggle_create_sequential_regressor_model, epochs = DL_EPOCH_NUM, batch_size = DL_BATCH_SIZE * strategy.num_replicas_in_sync)\n            models.append(('NRR', model))\n    else: # Use classifier algorithms\n        scoring = 'accuracy'\n        models.append(('LR', linear_model.LogisticRegression()))\n        models.append(('LDA', discriminant_analysis.LinearDiscriminantAnalysis()))\n        models.append(('KNN', neighbors.KNeighborsClassifier()))\n        models.append(('CART', tree.DecisionTreeClassifier(max_leaf_nodes = 256, random_state = SEED_HARCODED_VALUE)))\n        models.append(('XGB', xgb.XGBClassifier(random_state = SEED_HARCODED_VALUE)))\n        models.append(('RF', ensemble.RandomForestClassifier(n_estimators = 128, random_state = SEED_HARCODED_VALUE)))\n        models.append(('NB', naive_bayes.GaussianNB(random_state = SEED_HARCODED_VALUE)))\n        models.append(('SVM', svm.SVC(random_state = SEED_HARCODED_VALUE)))\n        if flags & ExecutionFlags.USE_NEURAL_NETWORK_FLAG == ExecutionFlags.USE_NEURAL_NETWORK_FLAG: # DL model\n            model = KerasClassifier(build_fn = kaggle_create_sequential_classifier_model, epochs = DL_EPOCH_NUM, batch_size = DL_BATCH_SIZE * strategy.num_replicas_in_sync)\n            models.append(('NNC', model))\n\n    results, names = kaggle_check_models(models, ml_inputs_training_df, ml_outputs_training_df, p_scoring = scoring)\n    best_alg = kaggle_compare_algorithms_perf(names, results, 'Algorithms Comparison', 'Algorithms', 'Accuracy')\n    ml = kaggle_algorithm_tuning(models[best_alg], ml_inputs_training_df, ml_outputs_training_df, (ml_inputs_validation_df, ml_outputs_validation_df))\n    y_predictions = kaggle_validation_prediction(ml, ml_inputs_validation_df, ml_outputs_validation_df)\n    y_predictions = kaggle_validation_prediction(ml, ml_inputs_test_df, ml_outputs_test_df)\n    \n    kaggle_save_model(ml, '/kaggle/working/' + ml.__class__.__name__)\n    \n    # Exploring the ML\n    kaggle_explore_ml(ml, pd.DataFrame(ml_inputs_validation_df, columns = columns_list[0]), pd.DataFrame(ml_outputs_validation_df, columns = columns_list[1]))\n    \n    # End of function kaggle_main","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ouf, now, we can execute all the sequences described above and get some result:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Entry point\nkaggle_main()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you liked this Notebook, please upvote.\nGives Motivation to make new Notebooks :)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}