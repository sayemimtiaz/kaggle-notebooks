{"cells":[{"metadata":{"_uuid":"b66656cb4b2286d748e068a616bd89b5064d104e"},"cell_type":"markdown","source":"Welcome to my Data Science Analysis using IPL Data. As I have a passion for Cricket, this sets of analysis will look at how I take a public dataset on Kaggle and attempt to answer some interesting questions."},{"metadata":{"_uuid":"6078b2229b52d7287696cfdb10f946c4593b8ef1"},"cell_type":"markdown","source":"Background and Introduction\n\nSince its introduction in 2003, T20 has become the star attraction within cricket for its high voltage action and exciting phases of play. As part of its popularity, the IPL (Indian Premier League), formed in 2008, began and kicked off one of the largest annual sports events in the world alongside the Football Premier League and NBA (National Basketball Association) until today. As a result of its increasing demand, teams are spending money towards a variety of avenues to gain a distinct competitive edge. One of these avenues is within data science to analyse player performance and opposition performance. As a result, the analysis of a league like this IPL is of growing importance. The data found from Kaggle. It uses two sets of data based from 2008 -2017; match-by-match and ball by ball statistics. The datasets are suitable to extract some key data, provide some statistical descriptive and visualizations and apply some machine learning techniques using Python. In this report, we aim to answer 2 key questions:\n\nWhat is the best classification algorithm to predict the results of two teams accurately, demonstrating the most important features within it?\nWhat is the best regression algorithm to predict a bowlerâ€™s runs conceded?"},{"metadata":{"trusted":true,"_uuid":"eb0b9e0fcfecd8bd2e0b2323d34bed825f54f01a"},"cell_type":"code","source":"# Import all packages required for this project. \nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom scipy.stats import norm\nimport sys\nimport pandas\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom pandas import DataFrame\nfrom sklearn.model_selection import train_test_split\nfrom pandas.plotting import scatter_matrix\nimport matplotlib.pyplot as plt\nfrom sklearn import model_selection\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4db67b9650ad6a5d01bd7dccb11d922b891b1b9b"},"cell_type":"code","source":"# Read the files and name them accordingly\niplmatches = pd.read_csv('../input/matches.csv')\nipldelivery = pd.read_csv('../input/deliveries.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a985168ca61b451fc95192010f5cad113f25d0ed"},"cell_type":"code","source":"#Ensure that the file is being read correctly\nipldelivery.head(5)\niplmatches.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b14bc55d1fac5fb18a2f693478af513a16e73064"},"cell_type":"code","source":"## BATSMEN DATA GROUPED BY MATCH\n# Here the data is grouped to provide deeper depth of statistics and later for the team classificaiton\n\nbatsman_grp = ipldelivery.groupby([\"match_id\", \"inning\", \"batting_team\", \"batsman\"])\nbatsmen = batsman_grp[\"batsman_runs\"].sum().reset_index()\n\n# Ignore the wide balls.\nballs_faced = ipldelivery[ipldelivery[\"wide_runs\"] == 0]\nballs_faced = balls_faced.groupby([\"match_id\", \"inning\", \"batsman\"])[\"batsman_runs\"].count().reset_index()\nballs_faced.columns = [\"match_id\", \"inning\", \"batsman\", \"balls_faced\"]\nbatsmen = batsmen.merge(balls_faced, left_on=[\"match_id\", \"inning\", \"batsman\"], \n                        right_on=[\"match_id\", \"inning\", \"batsman\"], how=\"left\")\n\nfours = ipldelivery[ ipldelivery[\"batsman_runs\"] == 4]\nsixes = ipldelivery[ ipldelivery[\"batsman_runs\"] == 6]\n\nfours_per_batsman = fours.groupby([\"match_id\", \"inning\", \"batsman\"])[\"batsman_runs\"].count().reset_index()\nsixes_per_batsman = sixes.groupby([\"match_id\", \"inning\", \"batsman\"])[\"batsman_runs\"].count().reset_index()\n\nfours_per_batsman.columns = [\"match_id\", \"inning\", \"batsman\", \"4s\"]\nsixes_per_batsman.columns = [\"match_id\", \"inning\", \"batsman\", \"6s\"]\n\nbatsmen = batsmen.merge(fours_per_batsman, left_on=[\"match_id\", \"inning\", \"batsman\"], \n                        right_on=[\"match_id\", \"inning\", \"batsman\"], how=\"left\")\nbatsmen = batsmen.merge(sixes_per_batsman, left_on=[\"match_id\", \"inning\", \"batsman\"], \n                        right_on=[\"match_id\", \"inning\", \"batsman\"], how=\"left\")\nbatsmen['SR'] = np.round(batsmen['batsman_runs'] / batsmen['balls_faced'] * 100, 2)\n\nfor col in [\"batsman_runs\", \"4s\", \"6s\", \"balls_faced\", \"SR\"]:\n    batsmen[col] = batsmen[col].fillna(0)\n\ndismissals = ipldelivery[ pd.notnull(ipldelivery[\"player_dismissed\"])]\ndismissals = dismissals[[\"match_id\", \"inning\", \"player_dismissed\", \"dismissal_kind\", \"fielder\"]]\ndismissals.rename(columns={\"player_dismissed\": \"batsman\"}, inplace=True)\nbatsmen = batsmen.merge(dismissals, left_on=[\"match_id\", \"inning\", \"batsman\"], \n                        right_on=[\"match_id\", \"inning\", \"batsman\"], how=\"left\")\n\nbatsmen = iplmatches[['id','season']].merge(batsmen, left_on = 'id', right_on = 'match_id', how = 'left').drop('id', axis = 1)\nbatsmen.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2238b67daf20e8ae453503568105f19785d002fe"},"cell_type":"code","source":"## Bowlers grouped by sets of data\n# Data is grouped for bowlers to provide greater depth of information. Very important for the regression analysis.\n\nbowler_grp = ipldelivery.groupby([\"match_id\", \"inning\", \"bowling_team\", \"bowler\", \"over\"])\nbowlers = bowler_grp[\"total_runs\", \"wide_runs\", \"bye_runs\", \"legbye_runs\", \"noball_runs\"].sum().reset_index()\n\nbowlers[\"runs\"] = bowlers[\"total_runs\"] - (bowlers[\"bye_runs\"] + bowlers[\"legbye_runs\"])\nbowlers[\"extras\"] = bowlers[\"wide_runs\"] + bowlers[\"noball_runs\"]\n\ndel( bowlers[\"bye_runs\"])\ndel( bowlers[\"legbye_runs\"])\ndel( bowlers[\"total_runs\"])\n\ndismissal_kinds_for_bowler = [\"bowled\", \"caught\", \"lbw\", \"stumped\", \"caught and bowled\", \"hit wicket\"]\ndismissals = ipldelivery[ipldelivery[\"dismissal_kind\"].isin(dismissal_kinds_for_bowler)]\ndismissals = dismissals.groupby([\"match_id\", \"inning\", \"bowling_team\", \"bowler\", \"over\"])[\"dismissal_kind\"].count().reset_index()\ndismissals.rename(columns={\"dismissal_kind\": \"wickets\"}, inplace=True)\n\nbowlers = bowlers.merge(dismissals, left_on=[\"match_id\", \"inning\", \"bowling_team\", \"bowler\", \"over\"], \n                        right_on=[\"match_id\", \"inning\", \"bowling_team\", \"bowler\", \"over\"], how=\"left\")\nbowlers[\"wickets\"] = bowlers[\"wickets\"].fillna(0)\n\nbowlers_over = bowlers.groupby(['match_id', 'inning', 'bowling_team', 'bowler'])['over'].count().reset_index()\nbowlers = bowlers.groupby(['match_id', 'inning', 'bowling_team', 'bowler']).sum().reset_index().drop('over', 1)\nbowlers = bowlers_over.merge(bowlers, on=[\"match_id\", \"inning\", \"bowling_team\", \"bowler\"], how = 'left')\nbowlers['Econ'] = np.round(bowlers['runs'] / bowlers['over'] , 2)\nbowlers = iplmatches[['id','season']].merge(bowlers, left_on = 'id', right_on = 'match_id', how = 'left').drop('id', axis = 1)\n\nbowlers.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fb1030282681c758cac5dea7ebd4a57b80d3d95"},"cell_type":"code","source":"# Ensure the data is grouped carefully. Name them accordingly as above. \niplmatches.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7e62dd713e24efab7056617654896350be4f1c6"},"cell_type":"markdown","source":"Below is some Data Visualization techniques and basic Statistical Techniques"},{"metadata":{"trusted":true,"_uuid":"f1140668c7f508f700605e7ae2c1db0e9a5f4399"},"cell_type":"code","source":"sns.countplot(x = 'season', data = iplmatches)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1760b008017e0eaee00cb0e2911a11b0c6e6cc77"},"cell_type":"code","source":"sns.countplot( x = 'toss_winner', data = iplmatches)\nplt.xticks(rotation='vertical')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b31eec9ca59b872875b49e5272ae2d5de81dfbf9"},"cell_type":"code","source":"winneroft = iplmatches['toss_winner'] == iplmatches['winner']\nwinneroft.groupby(winneroft).size()\nsns.countplot(winneroft)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ebee6f2438a7a26ed40b1f3b0d522984d7e8676"},"cell_type":"code","source":"winneroftoss = iplmatches[(iplmatches['toss_winner']) == (iplmatches['winner'])]\n\nwot = sns.countplot( x = 'winner', hue='season', data=winneroftoss)\nsns.set(rc={'figure.figsize':(8,6)})\nplt.xticks(rotation = 'vertical')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.xlabel(\"Teams\")\nplt.ylabel(\"Number of Wins\")\nplt.title(\"Number of Teams who won, given they win the toss, every season\")\nplt.show(wot)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3527f526850b7336548c6ac84cdd18dea441c7b7"},"cell_type":"code","source":"top_players = iplmatches.player_of_match.value_counts()[:10]\n#sns.barplot(x=\"day\", y=\"total_bill\", data=tips)\nfig, ax = plt.subplots()\nax.set_ylim([0,20])\nax.set_ylabel(\"Number of Awards\")\nax.set_xlabel(\"Name of Players\")\nax.set_title(\"Top player of the match Winners\")\n#top_players.plot.bar()\nsns.barplot(x = top_players.index, y = top_players, orient='v', palette=\"RdBu\");\nplt.xticks(rotation = 'vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ba135588365f94249adbf045dac497194e38399"},"cell_type":"code","source":"## Question regarding top bastsmen and top bowlers in history of IPL.\n\nbatsman_runsperseason = batsmen.groupby(['season', 'batting_team', 'batsman'])['batsman_runs'].sum().reset_index()\nbatsman_runsperseason = batsman_runsperseason.groupby(['season', 'batsman'])['batsman_runs'].sum().unstack().T\nbatsman_runsperseason['Total'] = batsman_runsperseason.sum(axis=1) #add total column to find batsman with the highest runs\nbatsman_runsperseason = batsman_runsperseason.sort_values(by = 'Total', ascending = False).drop('Total', 1)\nax.set_ylabel('Number of Runs')\nax = batsman_runsperseason[:8].T.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47e5b249775ec9bddc22fc9d2e3fff8fe54520d1"},"cell_type":"code","source":"bowler_wicketsperseason = bowlers.groupby(['season', 'bowling_team', 'bowler'])['wickets'].sum().reset_index()\nbowler_wicketsperseason = bowler_wicketsperseason.groupby(['season', 'bowler'])['wickets'].sum().unstack().T\nbowler_wicketsperseason ['Total'] = bowler_wicketsperseason .sum(axis=1) #add total column to find bowler with the highest number of wickets\nbowler_wicketsperseason  = bowler_wicketsperseason .sort_values(by = 'Total', ascending = False).drop('Total', 1)\nplt.ylabel('Number of Wickets')\nax = bowler_wicketsperseason [:8].T.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d440067211b542ae0eeccb252ad70fe5e252c7e0"},"cell_type":"code","source":"runs_scored=batsmen.groupby(['batsman'])['batsman_runs'].sum()\nruns_scored=runs_scored.sort_values(ascending=False)\ntop10runs = runs_scored.head(8)\ntop10runs.plot('barh')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c696bad0dd04bb15d822832a4c35d1543abc2cbc"},"cell_type":"code","source":"## Barplot of Runs\n\n#sns.barplot(x=\"day\", y=\"total_bill\", data=tips)\nfig, ax = plt.subplots()\n#fig.figsize = [16,10]\n#ax.set_ylim([0,20])\nax.set_xlabel(\"Runs\")\nax.set_title(\"Winning by Runs - Team Performance\")\n#top_players.plot.bar()\nsns.boxplot(y = 'winner', x = 'win_by_runs', data=iplmatches[iplmatches['win_by_runs']>0], orient = 'h'); #palette=\"Blues\");\nplt.show()\n\n## Barplot of Wickets Win\n\n#sns.barplot(x=\"day\", y=\"total_bill\", data=tips)\nfig, ax = plt.subplots()\n#fig.figsize = [16,10]\n#ax.set_ylim([0,20])\nax.set_title(\"Winning by Wickets - Team Performance\")\n#top_players.plot.bar()\nsns.boxplot(y = 'winner', x = 'win_by_wickets', data=iplmatches[iplmatches['win_by_wickets']>0], orient = 'h'); #palette=\"Blues\");\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd0cf81ce781a8d9ab43c999b2fdd00be8c60649"},"cell_type":"markdown","source":"====================================================================================================="},{"metadata":{"_uuid":"5b2bdbcbe5777de74e6806b5de40cdd93f0b5bb8"},"cell_type":"markdown","source":"Now we are going to do some Classification.\nPlease note that I will do very little explaining and more so for the user to explore and figure out the statistical methods used under it. \n"},{"metadata":{"trusted":true,"_uuid":"31c47bbd9a8282d6e56982906291fa026883e1ff"},"cell_type":"code","source":"# Import the new Dataset.\n# Now for this dataset, I removed some features that I felt were unnecessary from the original IPL\n# Dataset. However, you can use that or use the one below with a screenshot of the headings.\nmatches = pandas.read_csv('../input/matches1234.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae075836201b80e65b7e9b8e74ec695c8415311d"},"cell_type":"code","source":"matches.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"852b66e8b336cfcd4cc34751d259f92157c90a45"},"cell_type":"code","source":"# Make a copy of the dataset that you imported or used before\ncopy_data = matches.copy()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80857b88f5494babbc89f6780367d4e2399f3c0f"},"cell_type":"code","source":"# As to ensure data integrity, make sure there are no missing values. In some cases you can take the mean to ensure fairness. In this case\n# the values missing are types and so I filled them with these names. You could do otherwise if you wish\ncopy_data['city'].fillna('Dubai',inplace=True)\ncopy_data['umpire1'].fillna('Aleem Dar',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9c441fb868968dec8f24f2bb50557c892f3b5d6"},"cell_type":"code","source":"# Firstly, we should have a look whether the data is completed or not.\n# Because the missing value will have an adverse impact on the building of regression model.\n\nnull_values_col = copy_data.isnull().sum()\nnull_values_col = null_values_col[null_values_col != 0].sort_values(ascending = False).reset_index()\nnull_values_col.columns = [\"variable\", \"number of missing\"]\nnull_values_col.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b76267cfd83be308627ad55997dc54f37f235d02"},"cell_type":"code","source":"print(copy_data.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d98b5ad2b7689ed62be4431cf93054f5f59fc16"},"cell_type":"code","source":"#Create now a dataframe copy of the data and all its rows and named columns.\ndf = DataFrame(copy_data,columns=['team1', 'team2', 'toss_decision','toss_winner','city', 'venue', 'season', 'win_by_runs', 'win_by_wickets', 'umpire1', 'winner'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3cfa28147347e8ee2c8c12eb3983955765ab050"},"cell_type":"code","source":"# Now what we have done is replace the name values with numbers. Regression can only be run with \n# numbers and not anything else. \ndf['winner'].fillna('Draw', inplace=True)\ndf.replace(['Mumbai Indians','Kolkata Knight Riders','Royal Challengers Bangalore','Deccan Chargers','Chennai Super Kings',\n                 'Rajasthan Royals','Delhi Daredevils','Gujarat Lions','Kings XI Punjab',\n                 'Sunrisers Hyderabad','Rising Pune Supergiants','Kochi Tuskers Kerala','Pune Warriors']\n                ,['MI','KKR','RCB','DC','CSK','RR','DD','GL','KXIP','SRH','RPS','KTK','PW'],inplace=True)\n\nencode = {'team1': {'MI':1,'KKR':2,'RCB':3,'DC':4,'CSK':5,'RR':6,'DD':7,'GL':8,'KXIP':9,'SRH':10,'RPS':11,'KTK':12,'PW':13},\n          'team2': {'MI':1,'KKR':2,'RCB':3,'DC':4,'CSK':5,'RR':6,'DD':7,'GL':8,'KXIP':9,'SRH':10,'RPS':11,'KTK':12,'PW':13},\n          'toss_winner': {'MI':1,'KKR':2,'RCB':3,'DC':4,'CSK':5,'RR':6,'DD':7,'GL':8,'KXIP':9,'SRH':10,'RPS':11,'KTK':12,'PW':13},\n          'winner': {'MI':1,'KKR':2,'RCB':3,'DC':4,'CSK':5,'RR':6,'DD':7,'GL':8,'KXIP':9,'SRH':10,'RPS':11,'KTK':12,'PW':13,'Draw':14}}\ndf.replace(encode, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"742f5d348933b28c17540959aa3fe9248b113664"},"cell_type":"code","source":"dicVal = encode['winner']\nprint(dicVal['MI']) #key value\nprint(list(dicVal.keys())[list(dicVal.values()).index(1)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94dded967bdc07048b5eb56b0862314a02d0691d"},"cell_type":"code","source":"# If any of the types are objects, then this needs to be changed to integers\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c78a5c603c8e1c01c201e537e1f12de4efe25cc"},"cell_type":"code","source":"# This allows any columns to be changed with the corresponding values.\nfrom sklearn.preprocessing import LabelEncoder\nvar_mod = ['toss_decision', 'city', 'venue', 'umpire1']\nle = LabelEncoder()\nfor i in var_mod:\n    df[i] = le.fit_transform(df[i])\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4df20e32740b430c4a3cb3c2fcc6285c73222bc0"},"cell_type":"code","source":"#Compare the data at the beginning to now, ensuring no string value remains.\ndf.head(150)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"264cbfe4fc8f47d9a3a6b2eee00ee0cbe0f604dd"},"cell_type":"code","source":"# Now we are going to split the training and test models in a typical 60:20:20 set.\nx = df[['team1', 'team2', 'toss_decision','toss_winner','city', 'venue', 'season', 'win_by_runs', 'win_by_wickets', 'umpire1']]\ny = df[['winner']]\n\nx_model, x_test, y_model, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\nx_train, x_val, y_train, y_val = train_test_split(x_model, y_model, test_size=0.2, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffb9b50172124c2b35f2161d5fcbf9983639ed26"},"cell_type":"code","source":"# Model Tuning\n\n# 5-fold cross validation\n\nfrom sklearn.model_selection import KFold, cross_val_score\n\ndef rmse_cv(model):\n    kf = KFold(5, shuffle=True, random_state= 42).get_n_splits(x_model.values)\n    predictions = model.predict(x_test)\n    rmse= np.sqrt(-cross_val_score(model, x_model.values, y_model, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\nx_model, x_test, y_model, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\nx_train, x_val, y_train, y_val = train_test_split(x_model, y_model, test_size=0.2, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73145a0a14011e6f3514241974c2f4ceaaf565a2"},"cell_type":"code","source":"# How to find K?\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import KFold\n\ntrain_scores = []\nvalidation_scores = []\n\nx_model_values = x_model.values\ny_model_values = y_model.values\n\n# 5-fold cross validation\n\nkfold = KFold(5, shuffle=True, random_state=42)\n\nfor i in range(1,20):\n    knn = KNeighborsClassifier(i)\n    \n    tr_scores = []\n    va_scores = []\n    \n    for a, b in kfold.split(x_model_values):\n\n        x_train_fold, y_train_fold = x_model_values[a], y_model_values[a]\n        x_val_fold, y_val_fold = x_model_values[b], y_model_values[b]\n        \n        knn.fit(x_train_fold, y_train_fold.ravel())\n        \n        va_scores.append(knn.score(x_val_fold, y_val_fold))\n        tr_scores.append(knn.score(x_train_fold, y_train_fold))\n        \n    validation_scores.append(np.mean(va_scores))\n    train_scores.append(np.mean(tr_scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96b54c12b55c34d9cdc7010c36afd500aad21e42"},"cell_type":"code","source":"plt.title('k-NN Varying number of neighbours')\nplt.plot(range(1,20),validation_scores,label=\"Validation\")\nplt.plot(range(1,20),train_scores,label=\"Train\")\nplt.legend()\nplt.xticks(range(1,20))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9de6a734d639dffb1f12bb713f7e7456ff947e0b"},"cell_type":"code","source":"# Learning Curve\n\n# How KNN algorithm performs in both small-size data and big-size data \n\n# choose an acceptable color\n# https://www.spycolor.com/ff8040\n\nfrom sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, val_scores = learning_curve(KNeighborsClassifier(5), \n        x_model, \n        y_model,\n        # Number of folds in cross-validation\n        cv=5,\n        # Evaluation metric\n        scoring='accuracy',\n        # Use all computer cores\n        n_jobs=-1, \n        # 50 different sizes of the training set\n        train_sizes=np.linspace(0.1, 1.0, 5))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of validation set scores\nval_mean = np.mean(val_scores, axis=1)\nval_std = np.std(val_scores, axis=1)\n\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#ff8040\",  label=\"Training score\")\nplt.plot(train_sizes, val_mean, color=\"#40bfff\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve \\n k-fold=5, number of neighbours=5\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd35c04e48adc4be7689317ad20f1d782211dc1f"},"cell_type":"code","source":"# curse of dimensionality\n\n# one or two features are simple, but it cannot recognize and divide our categories. more features means\n# more evidence in different dimensions, but it could cause overfitting.\n\nx = df[['team1', 'team2', 'toss_decision','toss_winner','city', 'venue', 'season', 'win_by_runs', 'win_by_wickets', 'umpire1']]\ny = df[['winner']]\n\nx_model, x_test, y_model, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\nx_train, x_val, y_train, y_val = train_test_split(x_model, y_model, test_size=0.2, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"103ae3cb5db210be290dcc86caf3e53d00055f19"},"cell_type":"code","source":"# [:, :2]extract columns\n\n# convert[[1],[2],[3],...] to [1,2,3,4,0]\n# x_train_values_list = np.array(x_train_values).tolist() \n\n'''\ny_train_value = [j for i in y_train_values for j in i] - delete sublists to just one list\n\ndimensionality = []\nfor i in range(10):\n\na = [item[:, :2] for item in list(x_train_values)]\nprint(a)\n'''\n\nd_train = []\nd_val = []\n\nx_train_values = x_train.values\ny_train_values = y_train.values\nx_val_values = x_val.values\ny_val_values = y_val.values\n\nfor i in range(1,11):\n    \n    x_train_value = x_train_values[:,:i].tolist() #convert dataframe\n    x_val_value = x_val_values[:,:i].tolist()\n    \n    knn = KNeighborsClassifier(5)\n    Knn = knn.fit(x_train_value, y_train_values.ravel())\n\n    d_train.append(Knn.score(x_train_value, y_train_values))\n    d_val.append(Knn.score(x_val_value, y_val_values))\n\nplt.title('K-NN Curse of Dimensionality')\nplt.plot(range(1,11),d_val,label=\"Validation\")\nplt.plot(range(1,11),d_train,label=\"Train\")\nplt.xlabel('Number of Features')\nplt.ylabel('Score (Accuracy)')\nplt.legend()\nplt.xticks(range(1,11))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"014f71777327fdc64f28617ff2f45fa4702eb45a"},"cell_type":"code","source":"# The best result is captured at k = 5 hence it is used for the final model.\n\n#Setup a knn classifier with k neighbors\n\nkfold = KFold(5, shuffle=True, random_state=42)\nknn = KNeighborsClassifier(5)\n\nfor m,n in kfold.split(x_model_values):\n        \n        x_train_fold, y_train_fold = x_model_values[m], y_model_values[m]\n        \n        Knn = knn.fit(x_train_fold, y_train_fold.ravel())\n\nprint('When k=5, the testing score(accuracy) is: ')\nprint(Knn.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1e3f9da08264dbfb3db9015a42df918a63ff649"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n\ny_predict_knn = knn.predict(x_test)\ncm = confusion_matrix(y_test, y_predict_knn) \n\n# Transform to df for easier plotting\ncm_df = pd.DataFrame(cm,\n                     index = ['MI','KKR','RCB','DC','CSK','RR','DD','GL','KXIP','SRH','RPS','KTK','PW'], \n                     columns = ['MI','KKR','RCB','DC','CSK','RR','DD','GL','KXIP','SRH','RPS','KTK','PW' ])\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_df, annot=True)\nplt.title('KNN \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, y_predict_knn)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44901a7f90151cda3ce67e47e9a3cb997b9b7406"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39a297bd676b1164b11c2623d2139bfbb93f20ff"},"cell_type":"code","source":"##Using SVM - Note that I have not fine tuned this method. This is just for practice purposes and ensuring I can correctly do the data science behind it.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55c299577dbeeab1269f8c7af7c046358608bb39"},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.multiclass import OneVsRestClassifier\n\nclassifier = SVC(gamma = 'auto')\nsvm_model = OneVsRestClassifier(classifier, n_jobs=1).fit(x_train, y_train)\n\nprint(svm_model.score(x_train,y_train))\nprint(svm_model.score(x_val,y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"076b26cb0b69af625166011db01c313145cdbd5a"},"cell_type":"code","source":"\naccuracy=[]\ngamma=[0.0001, 0.001, 0.005, 0.01, 0.1, 0.2, 0.3, 0.5, 0.1]\n\nfor a in gamma:\n    classifier = SVC(C=1, \n        kernel='rbf', \n        degree=2, \n        gamma=a, \n        coef0=1,\n        shrinking=True, \n        tol=0.5,\n        probability=False, \n        cache_size=200, \n        class_weight=None,\n        verbose=False, \n        max_iter=-1, \n        decision_function_shape=None, \n        random_state=None)\n    svm_model = OneVsRestClassifier(classifier, n_jobs=1)\n    svm_model.fit(x_train, y_train)\n    predict=svm_model.predict(x_val)\n    accuracy.append(svm_model.score(x_val,y_val))\nprint(accuracy)\nplt.scatter(gamma, accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dce92000bff489221bcac40b89fb807a0ff098f9"},"cell_type":"code","source":"gamma=np.arange(0.0001, 0.005, 0.0003) \naccuracy=[]\n\nfor a in gamma:\n    classifier = SVC(C=1, \n        kernel='rbf', \n        degree=2, \n        gamma=a, \n        coef0=1,\n        shrinking=True, \n        tol=0.5,\n        probability=False, \n        cache_size=200, \n        class_weight=None,\n        verbose=False, \n        max_iter=-1, \n        decision_function_shape=None, \n        random_state=None)\n    svm_model = OneVsRestClassifier(classifier, n_jobs=1)\n    svm_model.fit(x_train, y_train)\n    predict=svm_model.predict(x_val)\n    accuracy.append(svm_model.score(x_val,y_val))\nprint(accuracy)\nplt.scatter(gamma, accuracy)\nplt.scatter(gamma, accuracy)\nplt.title(\"Finding Gamma\")\nplt.xlabel(\"Gamma\")\nplt.ylabel(\"Accuracy Score\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36d538597fdfbcfafee099a33ed85fe2dfe7b3dc"},"cell_type":"code","source":"accuracy=[]\nC=np.arange(1,10,1) \n\nfor a in C:\n    classifier = SVC(C=a, \n        kernel='rbf', \n        degree=2, \n        gamma=0.0013, \n        coef0=1,\n        shrinking=True, \n        tol=0.5,\n        probability=False, \n        cache_size=200, \n        class_weight=None,\n        verbose=False, \n        max_iter=-1, \n        decision_function_shape=None, \n        random_state=None)\n    svm_model = OneVsRestClassifier(classifier, n_jobs=1)\n    svm_model.fit(x_train, y_train)\n    predict=svm_model.predict(x_val)\n    accuracy.append(svm_model.score(x_val,y_val))\nprint(accuracy)\nplt.scatter(C, accuracy)\nplt.title(\"Finding C\")\nplt.xlabel(\"C\")\nplt.ylabel(\"Accuracy Score\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a8583a8f7e8baa87390d94211c1ad3188075659"},"cell_type":"code","source":"classifier = SVC(C=9, # Regularization parameter\n        kernel='rbf', # kernel type, rbf working fine here\n        degree=2, # default value\n        gamma=0.005, # kernel coefficient\n        coef0=1, # change to 1 from default value of 0.0\n        shrinking=True, # using shrinking heuristics\n        tol=0.5, # stopping criterion tolerance \n        probability=False, # no need to enable probability estimates\n        cache_size=200, # 200 MB cache size\n        class_weight=None, # all classes are treated equally \n        verbose=False, # print the logs \n        max_iter=-1, # no limit, let it run\n        decision_function_shape=None, # will use one vs rest explicitly \n        random_state=None)\nsvm_model = OneVsRestClassifier(classifier, n_jobs=1).fit(x_train, y_train)\n\nprint(svm_model.score(x_train,y_train))\nprint(svm_model.score(x_val,y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30a8293fdc9809ade8ee7b2348dcc0a039203cb9"},"cell_type":"code","source":"# Learning Curve\n\ntrain_sizes, train_scores, val_scores = learning_curve(OneVsRestClassifier(classifier, n_jobs=1), \n        x_model, \n        y_model,\n        # Number of folds in cross-validation\n        cv=5,\n        # Evaluation metric\n        scoring='accuracy',\n        # Use all computer cores\n        # 50 different sizes of the training set\n        train_sizes=np.linspace(0.1, 1.0, 5))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of validation set scores\nval_mean = np.mean(val_scores, axis=1)\nval_std = np.std(val_scores, axis=1)\n\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#ff8040\",  label=\"Training score\")\nplt.plot(train_sizes, val_mean, color=\"#40bfff\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve \\n C=1, gamma=0.0013\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5969910eb83dfb729483d3a473b6a20345b745c"},"cell_type":"code","source":"# curse of dimensionality\n\n# one or two features are simple, but it cannot recognize and divide our categories. more features means\n# more evidence in different dimensions, but it could cause overfitting.\n\n# https://thispointer.com/select-rows-columns-by-name-or-index-in-dataframe-using-loc-iloc-python-pandas/\n\nd_train = []\nd_val = []\n\nfor i in range(1,11):\n    \n    x_train_index = x_train.iloc[: , 0:i]\n    x_val_index = x_val.iloc[: , 0:i]\n    \n    classifier = SVC(C=9, # Regularization parameter\n                    kernel='rbf', # kernel type, rbf working fine here\n                    degree=2, # default value\n                    gamma=0.005, # kernel coefficient\n                    coef0=1, # change to 1 from default value of 0.0\n                    shrinking=True, # using shrinking heuristics\n                    tol=0.5, # stopping criterion tolerance \n                    probability=False, # no need to enable probability estimates\n                    cache_size=200, # 200 MB cache size\n                    class_weight=None, # all classes are treated equally \n                    verbose=False, # print the logs \n                    max_iter=-1, # no limit, let it run\n                    decision_function_shape=None, # will use one vs rest explicitly \n                    random_state=None)\n    svm_model = OneVsRestClassifier(classifier, n_jobs=1).fit(x_train_index, y_train)\n\n    d_train.append(svm_model.score(x_train_index, y_train))\n    d_val.append(svm_model.score(x_val_index, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08e2165387bb4c5ae826f1bc1ea33acc3a901889"},"cell_type":"code","source":"plt.title('SVM Curse of Dimensionality')\nplt.plot(range(1,11),d_val,label=\"Validation\")\nplt.plot(range(1,11),d_train,label=\"Train\")\nplt.xlabel('Number of Features')\nplt.ylabel('Score (Accuracy)')\nplt.legend()\nplt.xticks(range(1,11))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75cbcdd031c33d4cd6c11b49eaabc639506abc42"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"664e97f28af0e01b724e6cc7df95e3c650bdfbed"},"cell_type":"code","source":"## Using Naive Bayes Theorem","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2552642f3964a647d9a8087678b294b58e207305"},"cell_type":"code","source":"# NB assumes that the features themselves are not correlated to each other. Therefore, if the collinearity of our features are low, the model will perform better.\n\nx = df[['team1', 'team2', 'toss_decision','toss_winner','city', 'venue', 'season', 'win_by_runs', 'win_by_wickets', 'umpire1']]\ny = df[['winner']]\n\nx_model, x_test, y_model, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\nx_train, x_val, y_train, y_val = train_test_split(x_model, y_model, test_size=0.2, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb2632f2a2395841ac3a576f02e0e1bfc9de32a8"},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\ngaussian = GaussianNB()\nnb_model = gaussian.fit(x_train, y_train.values.ravel())\n\nprint(nb_model.score(x_train,y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a3e24efe4db6a14ae685782b769dfcb6b071b3e"},"cell_type":"code","source":"train_score = []\nval_score = []\na = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1, 1]\n\n#for i in np.arange(1,20):\nfor i in a:\n    gaussian = GaussianNB(priors=None, var_smoothing=i)\n    nb_model = gaussian.fit(x_train, y_train.values.ravel())\n    train_score.append(nb_model.score(x_train, y_train))\n    val_score.append(nb_model.score(x_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a702b2db9e61934f5637252000f74221ec69598"},"cell_type":"code","source":"plt.plot(a,train_score)\nplt.plot(a,val_score)\nplt.legend(['Training Accuracy','Validation Accuracy'])\nplt.title('Naive Bayes Tuning')\nplt.xlabel('Variance Smoothing')\nplt.ylabel('Accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52c9156182df3f64b7462370ea1d546030eefa17"},"cell_type":"code","source":"gaussian = GaussianNB(priors=None, var_smoothing=0.03)\nnb_model = gaussian.fit(x_train, y_train.values.ravel())\n\nprint(nb_model.score(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4885c3ef1a372ae68eb4dc9caa80c65fcea57e4a"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n\ny_predict_nb = nb_model.predict(x_test)\ncm = confusion_matrix(y_test, y_predict_nb) \n\n# Transform to df for easier plotting\ncm_df = pd.DataFrame(cm,\n                     index = ['MI','KKR','RCB','DC','CSK','RR','DD','GL','KXIP','SRH','RPS','KTK','PW'], \n                     columns = ['MI','KKR','RCB','DC','CSK','RR','DD','GL','KXIP','SRH','RPS','KTK','PW' ])\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_df, annot=True)\nplt.title('Naive Bayes \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, y_predict_nb)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11ae47cccca19ba89fc2d05ef72e8a33773b4686"},"cell_type":"code","source":"# Learning Curve\n\ntrain_sizes, train_scores, val_scores = learning_curve(OneVsRestClassifier(GaussianNB(priors=None, var_smoothing=0.1)), \n        x_model, \n        y_model,\n        # Number of folds in cross-validation\n        cv=5,\n        # Evaluation metric\n        scoring='accuracy',\n        # Use all computer cores\n        # 50 different sizes of the training set\n        train_sizes=np.linspace(0.1, 1.0, 5))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of validation set scores\nval_mean = np.mean(val_scores, axis=1)\nval_std = np.std(val_scores, axis=1)\n\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#ff8040\",  label=\"Training score\")\nplt.plot(train_sizes, val_mean, color=\"#40bfff\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"NB Learning Curve \\n \")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c23a10c80ab0bb11808b9ff0d74ba52a497fae5e"},"cell_type":"code","source":"d_train = []\nd_val = []\n\nfor i in range(1,11):\n    \n    x_train_index = x_train.iloc[: , 0:i]\n    x_val_index = x_val.iloc[: , 0:i]\n    \n    classifier = GaussianNB(priors=None, var_smoothing=0.1)\n    nb_model = gaussian.fit(x_train_index, y_train.values.ravel())\n\n    d_train.append(nb_model.score(x_train_index, y_train))\n    d_val.append(nb_model.score(x_val_index, y_val))\nplt.title('Naive Bayes Curse of Dimensionality')\nplt.plot(range(1,11),d_val,label=\"Validation\")\nplt.plot(range(1,11),d_train,label=\"Train\")\nplt.xlabel('Number of Features')\nplt.ylabel('Score (Accuracy)')\nplt.legend()\nplt.xticks(range(1,11))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0fad1502a88db6963d11218ac1e317029ca23272"},"cell_type":"code","source":"## Decision Tree Classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca52d2d0e1ffb08ffb7b47fbf231d7890173154f"},"cell_type":"code","source":"\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecision_tree_model = DecisionTreeClassifier()\ndecision_tree_model.fit(x_train, y_train)\nprint(decision_tree_model.score(x_train,y_train))\nprint(decision_tree_model.score(x_val,y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e34b6a79e4b5886eff18ac705bd4cef22527b33"},"cell_type":"code","source":"plt.bar(range(len(x_train.columns.values)), decision_tree_model.feature_importances_)\nplt.xticks(range(len(x_train.columns.values)),x_train.columns.values, rotation= 45)\nplt.title('Figure 1.7 Importance of each Feature')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82ac5d310034da26b0189b41ee55389339350492"},"cell_type":"code","source":"train_score = []\nval_score = []\nfor depth in np.arange(1,11):\n    decision_tree = tree.DecisionTreeClassifier(max_depth = depth,min_samples_leaf = 5)\n    decision_tree.fit(x_train, y_train)\n    train_score.append(decision_tree.score(x_train, y_train))\n    val_score.append(decision_tree.score(x_val, y_val))\n\nplt.plot(np.arange(1,11),train_score)\nplt.plot(np.arange(1,11),val_score)\nplt.legend(['Training Accuracy','Validation Accuracy'])\nplt.title('Decision Tree Tuning')\nplt.xlabel('Depth')\nplt.ylabel('Accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e8a8fbd85e2a5c43d4910584447e131d40e33f7"},"cell_type":"code","source":"train_score = []\nval_score = []\nfor depth in np.arange(1,15):\n    decision_tree = tree.DecisionTreeClassifier(max_depth = depth,min_samples_leaf = 5)\n    decision_tree.fit(x_train, y_train)\n    train_score.append(decision_tree.score(x_train, y_train))\n    val_score.append(decision_tree.score(x_val, y_val))\n\nplt.plot(np.arange(1,15),train_score)\nplt.plot(np.arange(1,15),val_score)\nplt.legend(['Training Accuracy','Validation Accuracy'])\nplt.title('Decision Tree Tuning')\nplt.xlabel('Depth')\nplt.ylabel('Accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a64fae5b509e964579e9b188bc9f7d4d0371d25"},"cell_type":"code","source":"train_score = []\nval_score = []\nfor leaf in np.arange(1,20):\n    decision_tree = tree.DecisionTreeClassifier(max_depth = 9, min_samples_leaf = leaf)\n    decision_tree.fit(x_train, y_train)\n    train_score.append(decision_tree.score(x_train, y_train))\n    val_score.append(decision_tree.score(x_val, y_val))\n\nplt.plot(np.arange(1,20),train_score)\nplt.plot(np.arange(1,20),val_score)\nplt.legend(['Training Accuracy','Validation Accuracy'])\nplt.title('Decision Tree Tuning')\nplt.xlabel('Minimum Samples Leaf')\nplt.ylabel('Accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1914b0b4b418c495cde4d1111ea4e47c4cbb238c"},"cell_type":"code","source":"my_decision_tree_model = DecisionTreeClassifier(max_depth = 9, min_samples_leaf = 3)\nmy_decision_tree_model.fit(x_train, y_train)\nprint(my_decision_tree_model.score(x_train,y_train))\nprint(my_decision_tree_model.score(x_val,y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04f89146d46fe33da31e33c6ae2d21c4009313b4"},"cell_type":"code","source":"print(my_decision_tree_model.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c73ab47a04f7963c63aa76ee736dfb3c6fcd8166"},"cell_type":"code","source":"y_predict_decision = my_decision_tree_model.predict(x_test)\ncm = confusion_matrix(y_test, y_predict_decision) \n\n# Transform to df for easier plotting\ncm_df = pd.DataFrame(cm,\n                     index = ['MI','KKR','RCB','DC','CSK','RR','DD','GL','KXIP','SRH','RPS','KTK','PW'], \n                     columns = ['MI','KKR','RCB','DC','CSK','RR','DD','GL','KXIP','SRH','RPS','KTK','PW' ])\n\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_df, annot=True)\nplt.title('Decision Tree \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, y_predict_decision)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b86df516cd7f1a055a9b80c92c738ec6eea3fd0"},"cell_type":"code","source":"# Learning Curve\ntrain_sizes, train_scores, val_scores = learning_curve(OneVsRestClassifier(DecisionTreeClassifier(max_depth = 5, min_samples_leaf = 6)), \n        x_model, \n        y_model,\n        # Number of folds in cross-validation\n        cv=5,\n        # Evaluation metric\n        scoring='accuracy',\n        # Use all computer cores\n        # 50 different sizes of the training set\n        train_sizes=np.linspace(0.1, 1.0, 5))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of validation set scores\nval_mean = np.mean(val_scores, axis=1)\nval_std = np.std(val_scores, axis=1)\n\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#ff8040\",  label=\"Training score\")\nplt.plot(train_sizes, val_mean, color=\"#40bfff\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Decision Tree Learning Curve \\n \")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63e72b0335384c3ce3012aaf1feb6ea63867c6a7"},"cell_type":"code","source":"# Curse of Dimensionality\n\nd_train = []\nd_val = []\n\nfor i in range(1,11):\n    \n    x_train_index = x_train.iloc[: , 0:i]\n    x_val_index = x_val.iloc[: , 0:i]\n    \n    classifier = DecisionTreeClassifier(max_depth = 5, min_samples_leaf = 6)\n    dt_model = classifier.fit(x_train_index, y_train.values.ravel())\n\n    d_train.append(dt_model.score(x_train_index, y_train))\n    d_val.append(dt_model.score(x_val_index, y_val))\nplt.title('Decision Tree Curse of Dimensionality')\nplt.plot(range(1,11),d_val,label=\"Validation\")\nplt.plot(range(1,11),d_train,label=\"Train\")\nplt.xlabel('Number of Features')\nplt.ylabel('Score (Accuracy)')\nplt.legend()\nplt.xticks(range(1,11))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f066cc0611d0af975365105f629fb0d2831e2aa"},"cell_type":"code","source":"## Using Logisitic Regression\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ffdc990a377a55b8e6989f425c25f0670f17207"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e0dc9333fb4b2bf58a08d8684db49832b382e37d"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nlogistic_model = LogisticRegression()\nlogistic_model.fit(x_train, y_train.values.ravel())\n\nprint(logistic_model.score(x_train,y_train))\nprint(logistic_model.score(x_val,y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"017755aaeed27c13ad85887e26e7689af39a1f63"},"cell_type":"code","source":"train_score = []\nval_score=[]\n\nfor i in np.arange(1,80):\n    \n    logistic_model = LogisticRegression(penalty = 'l2', C = i,random_state = 0)\n    \n    logistic_model.fit(x_train,y_train.values.ravel()) \n    \n    train_score.append(logistic_model.score(x_train, y_train))\n    val_score.append(logistic_model.score(x_val,y_val))\n\n    \nplt.plot(np.arange(1,80),train_score)\nplt.plot(np.arange(1,80),val_score)\nplt.legend(['Training Accuracy','Validation Accuracy'])\nplt.title('Logistic Regression Tuning')\nplt.xlabel('C')\nplt.ylabel('Accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90960ed8a2386eeb426939d30b12e5ab0507f7a0"},"cell_type":"code","source":"my_logistic_regression_model = LogisticRegression(penalty = 'l2', C = 48, random_state = 0)\nmy_logistic_regression_model.fit(x_train, y_train)\nprint(my_logistic_regression_model.score(x_train,y_train))\nprint(my_logistic_regression_model.score(x_val,y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ddffbbf1b25a8bc71b4cd0528a65decdbb6de0e1"},"cell_type":"code","source":"print(my_logistic_regression_model.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fcff2b15e16ab0c9d58e10371e92fb57bcd44e01"},"cell_type":"code","source":"y_predict_logit = my_logistic_regression_model.predict(x_test)\ncm = confusion_matrix(y_test, y_predict_logit) \n\n# Transform to df for easier plotting\ncm_df = pd.DataFrame(cm,\n                     index = ['MI','KKR','RCB','DC','CSK','RR','DD','GL','KXIP','SRH','RPS','KTK','PW'], \n                     columns = ['MI','KKR','RCB','DC','CSK','RR','DD','GL','KXIP','SRH','RPS','KTK','PW' ])\n\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_df, annot=True)\nplt.title('Logistic Regression \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, y_predict_logit)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98b6edfd7d91cd3bc7cf56882f2ba3f15bb88322"},"cell_type":"code","source":"# Learning Curve\n\ntrain_sizes, train_scores, val_scores = learning_curve(OneVsRestClassifier(LogisticRegression(penalty = 'l2', C = 48, random_state = 0)), \n        x_model, \n        y_model,\n        # Number of folds in cross-validation\n        cv=5,\n        # Evaluation metric\n        scoring='accuracy',\n        # Use all computer cores\n        # 50 different sizes of the training set\n        train_sizes=np.linspace(0.1, 1.0, 5))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of validation set scores\nval_mean = np.mean(val_scores, axis=1)\nval_std = np.std(val_scores, axis=1)\n\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#ff8040\",  label=\"Training score\")\nplt.plot(train_sizes, val_mean, color=\"#40bfff\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Logistic Regression Learning Curve \\n \")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f68327bd96b81f694bbd00071e5eac6392ba00a4"},"cell_type":"code","source":"# Curse of Dimensionality\n\nd_train = []\nd_val = []\n\nfor i in range(1,11):\n    \n    x_train_index = x_train.iloc[: , 0:i]\n    x_val_index = x_val.iloc[: , 0:i]\n    \n    classifier = LogisticRegression(penalty = 'l2', C = 48, random_state = 0)\n    lr_model = classifier.fit(x_train_index, y_train.values.ravel())\n\n    d_train.append(lr_model.score(x_train_index, y_train))\n    d_val.append(lr_model.score(x_val_index, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"697e3661e368d56202b744b202fed5dab03205c8"},"cell_type":"code","source":"plt.title('Logistic Regression Curse of Dimensionality')\nplt.plot(range(1,11),d_val,label=\"Validation\")\nplt.plot(range(1,11),d_train,label=\"Train\")\nplt.xlabel('Number of Features')\nplt.ylabel('Score (Accuracy)')\nplt.legend()\nplt.xticks(range(1,11))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32877e336c081ea7337dfaa1dc20cdf4d51cecf4"},"cell_type":"code","source":"print(\"Logistic Regression \\nAccuracy:{0:.4f}\".format(accuracy_score(y_test, y_predict_logit)))\nprint(\"Decision Tree \\nAccuracy:{0:.4f}\".format(accuracy_score(y_test, y_predict_decision)))\nprint(\"Naive Bayes \\nAccuracy:{0:.4f}\".format(accuracy_score(y_test, y_predict_nb)))\nprint(\"KNN Accuracy \\nAccuracy:{0:.4f}\".format(accuracy_score(y_test, y_predict_knn)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d87a8d68f66f5341c7065fa3b2544947a41733b"},"cell_type":"markdown","source":"=============================================================================================="},{"metadata":{"_uuid":"c25d874617abde65671b68c66a8928fc0dad3717"},"cell_type":"markdown","source":"Now we will be running the bowler's Regression to estimate the total number of runs conceded when they bowl in the IPL "},{"metadata":{"_uuid":"424fac6afdae6e2972426bfd3831bf8c29e19697"},"cell_type":"markdown","source":"At this point, it is important to remember that we earlier grouped the deliveries data set that is viable for each and every bowler. Having named this as a data strucure, we save this file as a csv and now we call upon it seperately. As with the classification, I have omitted certain columns. You can do so otherwise or keep them at your discretion."},{"metadata":{"trusted":true,"_uuid":"4506ec9e6867d020541b6c00ebf77ce3d72758b5"},"cell_type":"code","source":"# Import the dataset\nBowlers = pandas.read_csv('../input/Bowlers.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa52eeb49a0aeab494f3dad31ca8a082152fde51"},"cell_type":"code","source":"# Make a copy\ncopy_data = Bowlers.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44ca4c7e6906140e28292060a5d74179ef55d879"},"cell_type":"code","source":"#Ensure there are no missing values\nnull_values_col = copy_data.isnull().sum()\nnull_values_col = null_values_col[null_values_col != 0].sort_values(ascending = False).reset_index()\nnull_values_col.columns = [\"variable\", \"number of missing\"]\nnull_values_col.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02f936d53c8b47712c1b970a40cb49c3a3b2cca8"},"cell_type":"code","source":"copy_data.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d04b2c7c8cad108973eddb0162c305566425d4dd"},"cell_type":"code","source":"#This shows us the top correlated variables with respect to one another\ndf = DataFrame(copy_data,columns=['over', 'wide_runs', 'noball_runs', 'runs', 'extras', 'wickets', 'Econ'])\n\n'''\npandas.DataFrame.corr\nmethod : {â€˜pearsonâ€™, â€˜kendallâ€™, â€˜spearmanâ€™}\npearson : standard correlation coefficient\nkendall : Kendall Tau correlation coefficient\nspearman : Spearman rank correlation\n\nmin_periods : int, optional\nMinimum number of observations required per pair of columns to have a valid result. Currently only available for pearson and spearman correlation\n'''\n\ncorrmat = df.corr(method='pearson', min_periods=1)\nr_square = corrmat ** 2\n\n## Top 8 correlated variables\nk = 9 #number of variables for heatmap\ncols = r_square.nlargest(k, 'runs')['runs'].index\ncm = df[cols].corr()\ncm_square = cm ** 2\nf, ax = plt.subplots(figsize=(10, 10))\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm_square, cbar=False, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"acc16118cd564980cacf599098860c3b54d188c7"},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\n# RMSE for testing data\n\ndef rmse_model(model, x_test, y_test):\n    predictions = model.predict(x_test)\n    rmse = np.sqrt(mean_squared_error(predictions, y_test))\n    return(rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2bfac8b4cb6b103d81cbfe015a8f48ae17a3365b"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx = df[['over', 'wide_runs', 'noball_runs','extras', 'wickets', 'Econ']]\ny = df[['runs']]\n\nx_model, x_test, y_model, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\nx_train, x_val, y_train, y_val = train_test_split(x_model, y_model, test_size=0.2, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7c17799246ef0500548be69f3cf0085231697fc"},"cell_type":"code","source":"print(\"the number of data for training:\")\nprint(y_train.count())\nprint(\"the number of data for validation:\")\nprint(y_val.count())\nprint(\"the number of data for testing:\")\nprint(y_test.count())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eeb33c9ad00facc5f60ab22f68cf4c9d5f20c448"},"cell_type":"code","source":"#Basic Linear Regression\nfrom sklearn.linear_model import LinearRegression\n\nlinear_regression = LinearRegression()\nlinear_regression.fit(x_train, y_train)\n\nprint(rmse_model(linear_regression, x_test, y_test))\nprint(linear_regression.coef_)\nprint(linear_regression.intercept_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9ea9a20091fff5ea72ebdbec492ed13a7ca66d4"},"cell_type":"code","source":"# Bias-Variance Trade-off\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\ntrain_rmses = []\nval_rmses = []\ndegrees = range(1,9)\n\nfor i in degrees:\n    \n    poly = PolynomialFeatures(degree=i, include_bias=False)\n    x_train_poly = poly.fit_transform(x_train)\n\n    poly_reg = LinearRegression()\n    poly_reg.fit(x_train_poly, y_train)\n    \n    # training RMSE\n    y_train_pred = poly_reg.predict(x_train_poly)\n    train_poly_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n    \n    train_rmses.append(train_poly_rmse)\n    \n    # validation RMSE\n    x_val_poly = poly.fit_transform(x_val)\n    y_val_pred = poly_reg.predict(x_val_poly)\n    \n    val_poly_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n    val_rmses.append(val_poly_rmse)\n\n    print('degree = %s, training RMSE = %.5f, validation RMSE = %.5f' % (i, train_poly_rmse, val_poly_rmse))\n        \nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(degrees, train_rmses,label= 'training set')\nax.plot(degrees, val_rmses,label= 'validation set')\nax.set_yscale('log')\nax.set_xlabel('Degree')\nax.set_ylabel('RMSE')\nax.set_title('Bias/Variance Trade-off')  \nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90e69cb4957f1df2361292b8a0ffb801f4511fa2"},"cell_type":"code","source":"# RMSE for testing data\n\nsecond_poly = PolynomialFeatures(degree=2, include_bias=False)\nx_train_poly = second_poly.fit_transform(x_train)\n\nsecond_reg = LinearRegression()\nsecond_reg.fit(x_train_poly, y_train)\n\nx_test_second_poly = second_poly.fit_transform(x_test)\ny_test_pred = second_reg.predict(x_test_second_poly)\n\nprint(rmse_model(second_reg, x_test_second_poly, y_test))\nprint(second_reg.coef_)\nprint(second_reg.intercept_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a81d66c42cee3f26e6e05dcd2df395efd753555"},"cell_type":"code","source":"# RMSE for testing data\n\nsecond_poly = PolynomialFeatures(degree=1, include_bias=False)\nx_train_poly = second_poly.fit_transform(x_train)\n\nsecond_reg = LinearRegression()\nsecond_reg.fit(x_train_poly, y_train)\n\nx_test_second_poly = second_poly.fit_transform(x_test)\ny_test_pred = second_reg.predict(x_test_second_poly)\n\nprint(rmse_model(second_reg, x_test_second_poly, y_test))\nprint(second_reg.coef_)\nprint(second_reg.intercept_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37bf3506c75b28b2a7984b6286e7bca77cb138c2"},"cell_type":"code","source":"#We now use Regularization to test co-efficent effectiveness","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f4a215f0ac3d866e78b9a4043c1f909e42e578f"},"cell_type":"code","source":"# At first, we calculate the RMSE before regularization.\n\npoly = PolynomialFeatures(degree=6, include_bias=False)\nx_train_poly = poly.fit_transform(x_train)\n\npoly_reg = LinearRegression()\npoly_reg.fit(x_train_poly, y_train)\n\nx_test_poly = poly.fit_transform(x_test)\ny_test_pred = poly_reg.predict(x_test_poly)\n\nprint(rmse_model(poly_reg, x_test_poly, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6f3858feeb5ca1adf1f7a63c222de13ad6f533a"},"cell_type":"code","source":"# RMSE for testing data\n\nsecond_poly = PolynomialFeatures(degree=6, include_bias=False)\nx_train_poly = second_poly.fit_transform(x_train)\n\nsecond_reg = LinearRegression()\nsecond_reg.fit(x_train_poly, y_train)\n\nx_test_second_poly = second_poly.fit_transform(x_test)\ny_test_pred = second_reg.predict(x_test_second_poly)\n\nprint(rmse_model(second_reg, x_test_second_poly, y_test))\nprint(second_reg.coef_)\nprint(second_reg.intercept_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"09cec520dafc8741ba7b901c990af3f47321dcfc"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"628052c91f804c1b4412e6129851a4ae7a75381e"},"cell_type":"code","source":"#Ridge Regularization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78d9a08f8057a8158304a7e91a9403b0d8f40c3c"},"cell_type":"code","source":"# Ridge\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\n\nrmse=[]\nalpha=[1, 2, 5, 10, 20, 30, 40, 50, 75, 100]\n\nfor a in alpha:\n    ridge = make_pipeline(PolynomialFeatures(6), Ridge(alpha=a))\n    ridge.fit(x_train, y_train)\n    predict=ridge.predict(x_val)\n    rmse.append(np.sqrt(mean_squared_error(predict, y_val)))\nprint(rmse)\nplt.ylabel('RMSE')\nplt.xlabel('Hyperparameter: alpha')\nplt.scatter(alpha, rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f5b775aff8b2183e05c74a2987cfce999e14433"},"cell_type":"code","source":"# Adjust alpha based on previous result\n\nalpha=np.arange(1, 20, 2)\nrmse=[]\n\nfor a in alpha:\n    #ridge=Ridge(alpha=a, copy_X=True, fit_intercept=True)\n    #ridge.fit(x_train, y_train)\n    ridge = make_pipeline(PolynomialFeatures(4), Ridge(alpha=a))\n    ridge.fit(x_train, y_train)\n    predict=ridge.predict(x_val)\n    rmse.append(np.sqrt(mean_squared_error(predict, y_val)))\nprint(rmse)\nplt.ylabel('RMSE')\nplt.xlabel('Hyperparameter: alpha')\nplt.scatter(alpha, rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7d557b834e468cd49863986b01f9d5e5cb35a8a"},"cell_type":"code","source":"# Adjust alpha based on previous result\n\nalpha=np.arange(1, 5, 0.2)\nrmse=[]\n\nfor a in alpha:\n    #ridge=Ridge(alpha=a, copy_X=True, fit_intercept=True)\n    #ridge.fit(x_train, y_train)\n    ridge = make_pipeline(PolynomialFeatures(6), Ridge(alpha=a))\n    ridge.fit(x_train, y_train)\n    predict=ridge.predict(x_val)\n    rmse.append(np.sqrt(mean_squared_error(predict, y_val)))\nprint(rmse)\nplt.ylabel('RMSE')\nplt.xlabel('Hyperparameter: alpha')\nplt.scatter(alpha, rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bebab36467d69cbf510fb6d4c6a046f64126d8fa"},"cell_type":"code","source":"# Use alpha=4.2 to predict the test data\n\nridge = make_pipeline(PolynomialFeatures(6), Ridge(alpha=4.2))\nridge_model = ridge.fit(x_train, y_train)\n\npredictions = ridge_model.predict(x_test)\nprint(\"Ridge RMSE is: \" + str(rmse_model(ridge_model, x_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d53a910af3c44270b82c94da440ce73e750ac1d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2dad264c8915e683926f37b93d007bf257b2eee0"},"cell_type":"code","source":"# Lasso\n\nfrom sklearn.linear_model import Lasso\n\nrmse=[]\nalpha=[0.0001, 0.001, 0.01, 0.1, 1]\n\nfor a in alpha:\n    lasso=make_pipeline(PolynomialFeatures(6), Lasso(alpha=a))\n    lasso.fit(x_train, y_train)\n    predict=lasso.predict(x_val)\n    rmse.append(np.sqrt(mean_squared_error(predict, y_val)))\nprint(rmse)\nplt.ylabel('RMSE')\nplt.xlabel('Hyperparameter: alpha')\nplt.scatter(alpha, rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf38a803c194927d8460eefc948a4a0716ee3ff5"},"cell_type":"code","source":"lasso = make_pipeline(PolynomialFeatures(6), Lasso(alpha=0.001))\nlasso_model = lasso.fit(x_train, y_train)\npredictions = lasso_model.predict(x_test)\nprint(\"RMSE in Testing : \" + str(rmse_model(lasso_model, x_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3061406d7cc07f89b0509985ecb38d2be8a81eb"},"cell_type":"code","source":"# Comparison\n\nprint(\"For testing dataset\\n\")\n\nprint(\"Linear RMSE is: \" + str(rmse_model(linear_regression, x_test, y_test)))\nprint(\"2nd Polynomial RMSE is: \" + str(rmse_model(second_reg, x_test_second_poly, y_test)))\n\nprint(\"\\nFor 6th order polynomial (RMSE = 5.027401601327215 before regualarization)\")\nprint(\"Ridge RMSE is: \" + str(rmse_model(ridge_model, x_test, y_test)))\nprint(\"Lasso RMSE is: \" + str(rmse_model(lasso_model, x_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1df4ed2457664807c7cbdad5172f07ef4d2ba2a9"},"cell_type":"code","source":"data = np.array([['','Parameter','RMSE'],\n                ['1st-order Poly',1,3.6925289],\n                ['2nd-order Poly',2,0.0035514],\n                ['6th-order Poly',4,5.02740160],\n                ['6th-order Lasso','<0.001',0.0455626],\n                ['6th-order Ridge',4.2,0.1035378]])\n                \nregression_comparison = pd.DataFrame(data=data[1:,1:],\n                                      index=data[1:,0],\n                                    columns=data[0,1:])\nregression_comparison","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"379fdeb086cdd095f8d6651495dfb4f0b6653e34"},"cell_type":"code","source":"my_ridge = Ridge(alpha = 4.2, normalize = True)\nmy_ridge.fit(x_train, y_train) \n#pd.Series(my_ridge.coef_,index = ['NBA_DraftNumber', 'Age', 'WS', 'BPM'])\nmy_ridge.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61e2a154e8394e22a6511185b37fb83b8d5146d4"},"cell_type":"code","source":"my_lasso = Lasso(alpha = 0.001, normalize = True)\nmy_lasso.fit(x_train, y_train) \nmy_lasso.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffef02436575e97379df15ff086174f6096dc92c"},"cell_type":"code","source":"data = np.array([['','over', 'wide_runs', 'noball_runs','extras', 'wickets', 'Econ'],\n                ['Ridge',1.19244951, 0.2525031 , 0.79495296, 0.285872  , 0.0395364 , 0.33603287],\n                ['Lasso',8.52176098,  0,  0.53551824,  0.22068013, -0.59004948,  2.48183302]])\n                \nregularization_comparison = pd.DataFrame(data=data[1:,1:],\n                                      index=data[1:,0],\n                                    columns=data[0,1:])\nregularization_comparison","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43267d272cb571a65f3c07b25eccca755aa45a69"},"cell_type":"markdown","source":"This is the end of the Project. Please message for any reccomendations or thoughts. I aim to update this within a year using new techniques as I learn. \n=================================================="},{"metadata":{"trusted":true,"_uuid":"21551e49137467c1831f8d5881a6acf62ac556d1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}