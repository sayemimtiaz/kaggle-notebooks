{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"## install dependencies\n# !pip install datasets==1.1.2 pytorch_lightning==1.0.3 wandb==0.10.8 transformers==3.4.0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 0. Dependencies"},{"metadata":{"trusted":true},"cell_type":"code","source":"# utils \nimport os\nimport gc\nimport tqdm\nimport torch\nimport pandas as pd\n\n# data\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\nfrom torch.utils.data import random_split, Dataset, DataLoader\n\n\n\n# model\nfrom transformers import AutoModel\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\n\n# training and evaluation \nimport wandb\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import EarlyStopping, ProgressBar, ModelCheckpoint\nfrom pytorch_lightning.loggers import WandbLogger\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score, roc_curve\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# device  \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# seed\ntorch.manual_seed(42)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Custom Dataset "},{"metadata":{"trusted":true},"cell_type":"code","source":"class NewsDataset(Dataset):\n    \"Custom Dataset class to create the torch dataset\"\n    \n    def __init__(self, root_dir, tokenizer,  max_len=128):\n        \"\"\"\n            root_dir: path where data is residing\n            tokenizer: tokenizer will be used to tokenize the text\n            max_len: max_len for text, padding/trimming will be applied to follow this rule\n        \"\"\"\n        \n        self.tokenizer = tokenizer\n        \n        self.data = load_dataset(\"csv\", data_files=[os.path.join(root_dir, \"Fake.csv\"), os.path.join(root_dir, \"True.csv\")])['train']\n        \n        \n        self.text = self.data['title']\n        self.label = self.data['label']\n                        \n        self.max_len = max_len\n                                        \n        \n        \n    def __len__(self):\n        \"__len__ function returns the size of the data =\"\n        return len(self.text)\n    \n    def __getitem__(self, idx):\n        \"\"\"\n            idx: index of the data to retrieve\n\n            returns: A dictionary containing input ids based on tokenizer's vocabulary, attention mask and label tensors \n        \"\"\"\n        \n        text = self.text[idx]\n        label = self.label[idx]\n        \n        input_encoding = self.tokenizer.encode_plus(\n            text=text,\n            truncation=True,\n            max_length=self.max_len,\n            return_tensors=\"pt\",\n            return_attention_mask=True,\n            padding=\"max_length\",\n        )\n        \n        return {\n            \"input_ids\":input_encoding['input_ids'].squeeze(),\n            \"attention_mask\":input_encoding['attention_mask'].squeeze(),\n            \"label\":torch.tensor([label], dtype=torch.float)\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n\n    \"\"\" \n        Fake News Classifier Model\n        A pretrained model is used as for contextualized embedding and a classifier on top of that. \n    \n    \"\"\"\n\n    def __init__(self, model_name, num_classes=2):\n        \"\"\"\n            model_name:  What base model to use from hugginface transformers\n            num_classes: Number of classes to classify. This is simple binary classification hence 2 classes\n        \"\"\"\n        super().__init__()\n        \n\n        # pretrained transformer model as base\n        self.base = AutoModel.from_pretrained(pretrained_model_name_or_path=model_name)\n\n\n        # nn classifier on top of base model\n        self.classfier = nn.Sequential(*[\n            nn.Linear(in_features=768, out_features=256),\n            nn.LeakyReLU(),\n            nn.Linear(in_features=256, out_features=num_classes),\n            nn.Sigmoid()\n        ])\n\n\n    def forward(self, input_ids, attention_mask=None):\n        \"\"\"\n            input_ids: input ids tensors for tokens  shape = [batch_size, max_len]\n            attention_mask: attention for input ids, 0 for pad tokens and 1 for non-pad tokens [batch_size, max_len]\n\n            returns: logits tensors as output, shape = [batch, num_classes]\n\n        \"\"\"\n\n\n        outputs = self.base(input_ids=input_ids, attention_mask=attention_mask)\n        \n        pooler = outputs[1]\n        # pooler.shape = [batch_size, hidden_size]\n\n        logits = self.classfier(pooler)\n\n\n        return logits\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. PyTorchLightning Data and Trainer Module "},{"metadata":{},"cell_type":"markdown","source":"#### Data Module"},{"metadata":{"trusted":true},"cell_type":"code","source":"class FakeNewsDataModule(pl.LightningDataModule):\n\n    \"\"\"Lightning Data Module to detach data from model\"\"\"\n\n    def __init__(self, config):\n\n        \"\"\"\n            config: a dicitonary containing data configuration such as batch size, split_size etc\n        \"\"\"\n        super().__init__()\n\n        self.config = config\n\n\n        # prepare and setup the dataset\n        self.prepare_data()\n        self.setup()\n\n    def prepare_data(self):\n        \"\"\"prepare datset\"\"\"\n\n        tokenizer = AutoTokenizer.from_pretrained(self.config['model_name'])\n\n        self.dataset = NewsDataset(root_dir=self.config['root_dir'], tokenizer=tokenizer, max_len=self.config['max_len'])\n\n\n\n    def setup(self):\n        \"\"\"make assignments here (val/train/test split)\"\"\"\n        \n        train_size = self.config['train_size']\n\n        lengths = [int(len(self.dataset)*train_size), len(self.dataset)-int(len(self.dataset)*train_size)]\n\n        self.train_datset, self.test_dataset =  random_split(dataset=self.dataset, lengths=lengths)\n\n\n    def train_dataloader(self):\n\n        return DataLoader(dataset=self.train_datset, batch_size=self.config['batch_size'], shuffle=True, num_workers=self.config['num_workers'])\n\n    def val_dataloader(self):\n        return DataLoader(dataset=self.test_dataset, batch_size=self.config['batch_size'], shuffle=False, num_workers=self.config['num_workers'])\n\n    def test_dataloader(self):\n        # same as validation data\n        return DataLoader(dataset=self.test_dataset, batch_size=self.config['batch_size'], shuffle=False, num_workers=self.config['num_workers'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Trainer Module"},{"metadata":{"trusted":true},"cell_type":"code","source":"class LightningModel(pl.LightningModule):\n\n    \"\"\"\n        LightningModel as trainer model\n    \"\"\"\n    \n    def __init__(self, config):\n        \"\"\"\n            config: training and other conifguration\n        \"\"\"\n\n        super(LightningModel, self).__init__()\n        \n        self.config = config\n        \n        self.model = Model(model_name=self.config['model_name'], num_classes=self.config['num_classes'])\n\n        \n    def forward(self, input_ids, attention_mask=None):\n        logits  = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        return logits.squeeze()\n    \n    def configure_optimizers(self):\n        return optim.AdamW(params=self.parameters(), lr=self.config['lr'])\n  \n    \n    def training_step(self, batch, batch_idx):\n        \n        input_ids, attention_mask, targets = batch['input_ids'], batch['attention_mask'], batch['label'].squeeze()\n        logits = self(input_ids=input_ids, attention_mask=attention_mask)\n        loss = F.mse_loss(logits, targets)\n        \n        pred_labels = logits.cpu() > 0.5 # logits.argmax(dim=1).cpu() for non-sigmoid\n        acc = accuracy_score(targets.cpu(), pred_labels)\n        f1 = f1_score(targets.cpu(), pred_labels, average=self.config['average'])\n        wandb.log({\"loss\":loss, \"accuracy\":acc, \"f1_score\":f1})\n        return {\"loss\":loss, \"accuracy\":acc, \"f1_score\":f1}\n\n    \n    def validation_step(self, batch, batch_idx):\n        input_ids, attention_mask, targets = batch['input_ids'], batch['attention_mask'], batch['label'].squeeze()\n        logits = self(input_ids=input_ids, attention_mask=attention_mask)\n        loss = F.mse_loss(logits, targets)\n        pred_labels = logits.cpu() > 0.5 # logits.argmax(dim=1).cpu() for non-sigmoid\n        acc = accuracy_score(targets.cpu(), pred_labels)\n        f1 = f1_score(targets.cpu(), pred_labels, average=self.config['average'])\n        precision = precision_score(targets.cpu(), pred_labels, average=self.config['average'])\n        recall = recall_score(targets.cpu(), pred_labels, average=self.config['average'])\n        return {\"val_loss\":loss, \"val_accuracy\":torch.tensor([acc]), \"val_f1\":torch.tensor([f1]), \"val_precision\":torch.tensor([precision]), \"val_recall\":torch.tensor([recall])}\n    \n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        avg_acc = torch.stack([x['val_accuracy'] for x in outputs]).mean()\n        avg_f1 = torch.stack([x['val_f1'] for x in outputs]).mean()\n        avg_precision = torch.stack([x['val_precision'] for x in outputs]).mean()\n        avg_recall = torch.stack([x['val_recall'] for x in outputs]).mean()\n        wandb.log({\"val_loss\":avg_loss, \"val_accuracy\":avg_acc, \"val_f1\":avg_f1, \"val_precision\":avg_precision, \"val_recall\":avg_recall})\n        return {\"val_loss\":avg_loss, \"val_accuracy\":avg_acc, \"val_f1\":avg_f1, \"val_precision\":avg_precision, \"val_recall\":avg_recall}\n    \n    \n    def test_step(self, batch, batch_idx):\n        input_ids, attention_mask, targets = batch['input_ids'], batch['attention_mask'], batch['label'].squeeze()\n        logits = self(input_ids=input_ids, attention_mask=attention_mask)\n        loss = F.mse_loss(logits, targets)\n        pred_labels = logits.cpu() > 0.5 # logits.argmax(dim=1).cpu() for non-sigmoid\n        acc = accuracy_score(targets.cpu(), pred_labels)\n        f1 = f1_score(targets.cpu(), pred_labels, average=self.config['average'])\n        precision = precision_score(targets.cpu(), pred_labels, average=self.config['average'])\n        recall = recall_score(targets.cpu(), pred_labels, average=self.config['average'])\n        return {\"test_loss\":loss, \"test_precision\":torch.tensor([precision]), \"test_recall\":torch.tensor([recall]), \"test_accuracy\":torch.tensor([acc]), \"test_f1\":torch.tensor([f1])}\n    \n    def test_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n        avg_acc = torch.stack([x['test_accuracy'] for x in outputs]).mean()\n        avg_f1 = torch.stack([x['test_f1'] for x in outputs]).mean()\n        avg_precision = torch.stack([x['test_precision'] for x in outputs]).mean()\n        avg_recall = torch.stack([x['test_recall'] for x in outputs]).mean()\n        return {\"test_loss\":avg_loss, \"test_precision\":avg_precision, \"test_recall\":avg_recall, \"test_acc\":avg_acc, \"test_f1\":avg_f1}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Training and Evaluation"},{"metadata":{},"cell_type":"markdown","source":"##### Preprocessing\n- csv file does not have label coloum, adding it to both csv files and save write access wokring/ directory, will read the data from here now "},{"metadata":{"trusted":true},"cell_type":"code","source":"root_dir = \"../working/Fake-News/\"\nos.makedirs(root_dir, exist_ok=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fake = pd.read_csv(os.path.join('../input/fake-and-real-news-dataset/', \"Fake.csv\"))\nreal = pd.read_csv(os.path.join('../input/fake-and-real-news-dataset/', \"True.csv\"))\n\nfake['label'] = [1]*fake.shape[0]\nreal['label'] = [0]*real.shape[0]\n\nfake.to_csv(os.path.join(root_dir, \"Fake.csv\"))\nreal.to_csv(os.path.join(root_dir, \"True.csv\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"config = {\n\n    # data \n    \"root_dir\":root_dir,\n    \"model_name\":\"roberta-base\",\n    \"num_classes\":1,\n    \"max_len\":128,\n    \"train_size\":0.85,\n    \"batch_size\":32,\n    \"num_workers\":4,\n\n\n\n    # training\n    \"average\":\"macro\",\n    \"save_dir\":\"./\",\n    \"project\":\"fake-news-classification\",\n    \"lr\":2e-5,\n    \"monitor\":\"val_accuracy\",\n    \"min_delta\":0.005,\n    \"patience\":2,\n    \"filepath\":\"./checkpoints/{epoch}-{val_f1:4f}\",\n    \"precision\":32,\n    \"epochs\":5,\n    \n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Logger, EarlyStopping and Callbacks\nlogger = WandbLogger(\n    name=config['model_name'],\n    save_dir=config[\"save_dir\"],\n    project=config[\"project\"],\n    log_model=True,\n)\nearly_stopping = EarlyStopping(\n    monitor=config[\"monitor\"],\n    min_delta=config[\"min_delta\"],\n    patience=config[\"patience\"],\n)\ncheckpoints = ModelCheckpoint(\n    filepath=config[\"filepath\"],\n    monitor=config[\"monitor\"],\n    save_top_k=1\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer = pl.Trainer(\n    logger=logger,\n    gpus=[0],\n    checkpoint_callback=checkpoints,\n    callbacks=[early_stopping],\n    default_root_dir=\"./models/\",\n    max_epochs=config[\"epochs\"],\n    precision=config[\"precision\"],\n    automatic_optimization=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dm = FakeNewsDataModule(config=config)\nlm = LightningModel(config=config)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer.fit(\n    model=lm,\n    datamodule=dm\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer.test(\n    model=lm,\n    datamodule=dm\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Test from Checkpoint"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loader = dm.test_dataloader()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../working/checkpoints/\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l  = torch.load(f=\"./checkpoints/epoch=2-val_f1=0.999528.ckpt\")\nlm.load_state_dict(l['state_dict'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer.test(\n    model=lm,\n    datamodule=dm\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Get the pred probs and predicted labels for test set to compute other metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"actual_label = []\npred_label = []\npred_probs = []\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# [1, 2, 3] > 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for batch in tqdm.tqdm(test_loader):\n    \n    y_ = lm(input_ids=batch['input_ids'].to(device), attention_mask=batch['attention_mask'].to(device)).detach().cpu()\n    \n    \n    \n    actual_label += batch['label'].squeeze().tolist()\n    \n    pred_probs += y_.tolist()\n    \n    pred_label += y_ > 0.5\n    \n\n    \n    del batch\n    gc.collect()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(classification_report(y_true=actual_label, y_pred=pred_label, digits=5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"ROBERTa with MSE\")\nprint(classification_report(y_true=actual_label, y_pred=pred_label, digits=5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## AUC ROC Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"ns_probs = [0 for _ in range(len(actual_label))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc = roc_auc_score(y_true=actual_label, y_score=pred_probs)\nns_auc = roc_auc_score(y_true=actual_label, y_score=ns_probs)\nprint(f'ROC_AUC_Score = {roc_auc:.4f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_fpr, lr_tpr, _ = roc_curve(y_true=actual_label, y_score=pred_probs)\nns_fpr, ns_tpr, _ = roc_curve(y_true=actual_label, y_score=ns_probs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the roc curve for the model\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\nplt.plot(lr_fpr, lr_tpr, marker='.', label='RoBERTa')\n# axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}