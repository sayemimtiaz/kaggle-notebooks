{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Quantum Convolutional Networks\n- [x] **torch dataset**\n- [x] process bangla numbers data \n- [x] **quanvolution** circuit testing\n- [x] create a hybrid model: quantum-classical\n- [ ] test results","metadata":{}},{"cell_type":"markdown","source":"# Prepare the dataset to work with torch and qiskit","metadata":{}},{"cell_type":"code","source":"#------------------------------\n# imports\n#------------------------------\nimport os\nimport pandas as pd\nimport torch\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom torch.utils.data import Dataset,DataLoader\nfrom torchvision.transforms import ToTensor\n#------------------------------\n# setting up seed and checking GPU\n#------------------------------\n'''\n    as we will be using a random_circuit this seed is needed\n'''\nseed = 47\nnp.random.seed(seed)        \ntorch.manual_seed(seed)     \n\nif torch.cuda.is_available():\n    DEVICE = torch.device('cuda')\n    print('Using PyTorch version:', torch.__version__, ' Device:', DEVICE)\n    print('cuda index:', torch.cuda.current_device())\n    print('GPU:', torch.cuda.get_device_name())\nelse:\n    DEVICE = torch.device('cpu')\n    print('Using PyTorch version:', torch.__version__, ' Device:', DEVICE)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#--------------------------\n# parameters\n#--------------------------\nBATCH_SIZE    = 10\nEPOCHS        = 10     \nPREPROCESS    = True           # If False, skip quantum processing\n\nn_train       = 50     # Size of the train dataset\nn_test        = 30     # Size of the test dataset\nn_class       = 10     ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#------------------------------\n# dataset class for torch\n#------------------------------\nclass BanglaNumbersDataset(Dataset):\n    def __init__(self, \n                 img_dir,\n                 df,\n                 data_dim=28,\n                 transform=None):\n        # for labels and path\n        self.df        = df\n        # directory\n        self.img_dir   = img_dir\n        # transformations\n        self.transform = transform\n        # dimension\n        self.data_dim  = data_dim\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        # define path\n        img_path = os.path.join(self.img_dir, f\"{self.df.iloc[idx, 0]}.bmp\")\n        # read\n        image = cv2.imread(img_path,0)\n        image = cv2.resize(image,(self.data_dim,self.data_dim))\n        label = self.df.iloc[idx, 1]\n        \n        if self.transform:\n            image = self.transform(image)\n        return image,label\n\n#--------------------------\n# test train balanced split\n#--------------------------\n\n# read\ndf=pd.read_csv(\"/kaggle/input/banglaq/numbers.csv\")\nbangla_labels=sorted(list(df.label.unique()))\n# normalize labels\ndf.label=df.label.apply(lambda x: bangla_labels.index(x))\n\ntrain_dfs=[]\ntest_dfs =[]\n\n'''\n    Note:since there are thousands of data we wont bother with df empty checks or head tail duplications\n'''\n# collect class-wise balanced data\nfor label in df.label.unique():\n    _df=df.loc[df.label==label]\n    train_dfs.append(_df.head(int(n_train/n_class)))\n    test_dfs.append(_df.tail(int(n_test/n_class)))\n# concat and shuffle\ntrain_df=pd.concat(train_dfs,ignore_index=True)\ntrain_df=train_df.sample(frac=1)\n\ntest_df =pd.concat(test_dfs,ignore_index=True)\ntest_df=test_df.sample(frac=1)\n\nprint(f\" Total Data:{len(df)} \\n Train Data:{len(train_df)} \\n Test Data:{len(test_df)}\")\n#----------------------------\n# datasets and dataloaders\n#------------------------------\ntrain_ds        =      BanglaNumbersDataset(img_dir=\"/kaggle/input/banglaq/numbers/\",\n                                            df=train_df,\n                                            transform=ToTensor())\ntest_ds         =      BanglaNumbersDataset(img_dir=\"/kaggle/input/banglaq/numbers/\",\n                                            df=test_df,\n                                            transform=ToTensor())\n\n\ntrain_dataloader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\ntest_dataloader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True)\n#----------------------------\n# display dataset\n#------------------------------                                            \n# Display image and label.\ntrain_images,train_labels= next(iter(train_dataloader))\n# train_images=np.array(batch[\"image\"]) \n# train_labels=np.array(batch[\"label\"]) \nprint(f\"Images batch shape: {train_images.shape}\")\nprint(f\"Labels batch shape: {train_labels.shape}\")\nimg   = train_images[0].squeeze()\nlabel = train_labels[0]\nplt.imshow(img, cmap=\"gray\")\nplt.show()\nprint(f\"Label: {label}\")\nprint(\"unique values:\",np.unique(img))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Libraries and setup","metadata":{}},{"cell_type":"code","source":"from IPython.display import clear_output\n!pip install qiskit==0.24.0\n!pip install qiskit-aer-gpu\n!pip install -U numpy\n!pip install pylatexenc\nclear_output()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Quanvolutional Networks\n* **Step-1**:Take a small region of the input image and embed in into a quantum circuit \n```python\nThis can be done with parameterized rotations applied to the qubits initialized in the ground state. i.e-\ntheta           = list of circuit parameter theta for all qubits \nnumber of qubits= kernel_size square \nfor each qubit: \n    apply Single-qubit rotation about the X axis.\n```\n* **Reminder** :\n\n\\begin{align}\\begin{aligned}\\newcommand{\\th}{\\frac{\\theta}{2}}\\\\\\begin{split}RX(\\theta) = exp(-i \\th X) =\n    \\begin{pmatrix}\n        \\cos{\\th}   & -i\\sin{\\th} \\\\\n        -i\\sin{\\th} & \\cos{\\th}\n    \\end{pmatrix}\\end{split}\\end{aligned}\\end{align}\n  \n\n* **Step-2**: Perform a quantum computaion associated with a unitary. \n> Simply using VQC (variational quantum circuit) or a Random-circuit is enough\n\n* **Step-3**:Measure the system to get classical expectation values. \n","metadata":{}},{"cell_type":"code","source":"import qiskit\nfrom qiskit import IBMQ\nfrom qiskit.visualization import *\nfrom qiskit.circuit.random import random_circuit","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# formulate and check the circuit\n* https://qiskit.org/documentation/stubs/qiskit.circuit.QuantumCircuit.html\n* https://qiskit.org/documentation/stubs/qiskit.circuit.random.random_circuit.html","metadata":{}},{"cell_type":"code","source":"from qiskit.circuit import Parameter\nfrom qiskit.compiler import transpile\n#----------------------\n# circuit definitions\n#----------------------\nkernel_size=2\n# needed number of qbits\nn_qubits = kernel_size** 2\n# initialize circuit\n_circuit = qiskit.QuantumCircuit(n_qubits)\n# get rotation theta at ground state\nparams = [Parameter('θ_{}'.format(i+1)) for i in range(n_qubits)]\n# apply rotation\nfor i in range(n_qubits):\n    # theta and qubit\n    _circuit.rx(params[i],i)\n# add barrier\n_circuit.barrier()\n# random circuit\n_circuit += random_circuit(n_qubits, 2)\n# add measurements\n_circuit.measure_all()\nprint(_circuit.parameters)\n_circuit.draw(output='mpl')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#-------------------------\n# running the circuit\n#------------------------\nshots      = 100\nthreshold  = 127\n# create backend\n#backend = qiskit.Aer.get_backend('qasm_simulator')\nbackend = qiskit.providers.aer.QasmSimulator(method = \"statevector_gpu\")\n# create random data 2x2 data\ndata = torch.tensor([[0, 200], [100, 255]])\n# reshape\ndata = torch.reshape(data, (1,n_qubits))\n\n# encoding data to parameters\n## calculate theta\n'''\n    val > self.threshold  : |1> - rx(pi)\n    val <= self.threshold : |0> - rx(0)\n'''\n                \nthetas = []\nfor dat in data:\n    theta = []\n    for val in dat:\n        if val > threshold:\n            theta.append(np.pi)\n        else:\n            theta.append(0)\n    thetas.append(theta)\n# bind the parameters\nparam_dict = dict()\nfor theta in thetas:\n    for i in range(n_qubits):\n        param_dict[params[i]] = theta[i]\nparam_binds = [param_dict]\nprint(_circuit.parameters)\nprint(param_binds)\n# # transpile the circuit\n_circuit=transpile(_circuit)\n# execute quantum circuit\n'''\n    since we are using simulator we dont have to keep track of the job\n'''\nresult = qiskit.execute(_circuit, \n                         backend, \n                         shots = shots , \n                         parameter_binds = param_binds).result().get_counts()\n# decoding the result\ncounts = 0\nfor key, val in result.items():\n    cnt = sum([int(char) for char in key])\n    counts += cnt * val\n\n# Compute probabilities for each state\nprobabilities = counts / (shots  * n_qubits)\nprint(\"Convolution result:\",probabilities)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create A wrapper class for torch to use","metadata":{}},{"cell_type":"code","source":"class QCircuit:\n    def __init__(self, \n                 kernel_size, \n                 backend, \n                 shots, \n                 threshold):\n        '''\n            the quantum circuit class to execute the convolution operation\n            args:\n                kernel_size   =   size of (same as classical conv) kernel/ the image dimension patch\n                backend       =   the quantum computer hardware to use (only simulator is used here)\n                shots         =   how many times to run the circuit to get a probability distribution\n                threshold     =   the threshold value (0-255) to assign theta\n        '''\n        # the number of qubits needed\n        self.n_qubits = kernel_size ** 2\n        # initiate the circuit\n        self._circuit = qiskit.QuantumCircuit(self.n_qubits)\n        # parameters\n        self._params = [Parameter('θ_{}'.format(i)) for i in range(self.n_qubits)]\n\n        for i in range(self.n_qubits):\n            self._circuit.rx(self._params[i], i)\n        \n        self._circuit.barrier()\n        # add unitary random circuit\n        self._circuit += random_circuit(self.n_qubits, 2)\n        self._circuit.measure_all()\n        # initialize\n        self.backend   = backend\n        self.shots     = shots\n        self.threshold = threshold\n\n    def run(self, data):\n        # reshape input data-> [1, kernel_size, kernel_size] -> [1, self.n_qubits]\n        data = torch.reshape(data, (1, self.n_qubits))\n        # encoding data to parameters\n        thetas = []\n        for dat in data:\n            theta = []\n            for val in dat:\n                if val > self.threshold:\n                    theta.append(np.pi)\n                else:\n                    theta.append(0)\n            thetas.append(theta)\n        # for binding parameters\n        param_dict = dict()\n        for theta in thetas:\n            for i in range(self.n_qubits):\n                param_dict[self._params[i]] = theta[i]\n        param_binds = [param_dict]\n\n        # execute random quantum circuit\n        result = qiskit.execute(self._circuit, \n                                self.backend, \n                                shots = self.shots, \n                                parameter_binds = param_binds).result().get_counts()\n            \n        # decoding the result\n        counts = 0\n        for key, val in result.items():\n            cnt = sum([int(char) for char in key])\n            counts += cnt * val\n\n        # Compute probabilities for each state\n        probabilities = counts / (self.shots * self.n_qubits)\n        \n        return probabilities","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Torch forward and backward","metadata":{}},{"cell_type":"code","source":"#----------------------------\n# torch imports\n#----------------------------\n\nfrom torch.autograd import Function\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm.auto import tqdm\n#----------------------------\n# layer adoptation\n#----------------------------\n'''\n    see the textbook reference under materials\n    the backward is used as it is- no change needed\n'''\nclass QuanvFunction(Function):\n    \"\"\" Quanv function definition \"\"\"\n    \n    @staticmethod\n    def forward(ctx, \n                inputs, \n                in_channels, \n                out_channels, \n                kernel_size, \n                quantum_circuits, \n                shift,\n                verbose=True):\n        \"\"\" Forward pass computation \"\"\"\n        # input  shape : (-1, 1, 28, 28)\n        # otuput shape : (-1, 6, 24, 24)\n        ctx.in_channels      = in_channels\n        ctx.out_channels     = out_channels\n        ctx.kernel_size      = kernel_size\n        ctx.quantum_circuits = quantum_circuits\n        ctx.shift            = shift\n\n        # adjust length \n        _, _, len_x, len_y = inputs.size()\n        len_x = len_x - kernel_size + 1\n        len_y = len_y - kernel_size + 1\n        \n        # this calculates the conv results for nxn patches\n        features = []\n        ## loop over the images\n        for input in inputs:\n            feature = []\n            ## loop over the circuits\n            for circuit in quantum_circuits:\n                # save the results\n                xys = []\n                for x in range(len_x):\n                    ys = []\n                    for y in range(len_y):\n                        # get the patches\n                        data = input[0, x:x+kernel_size, y:y+kernel_size]\n                        # store the results\n                        res=circuit.run(data)\n                        ys.append(res)\n                    xys.append(ys)\n                feature.append(xys)\n            features.append(feature)\n        # construct the tensor\n        result = torch.tensor(features)\n\n        ctx.save_for_backward(inputs, result)\n        return result\n        \n    @staticmethod\n    def backward(ctx, grad_output): \n        \"\"\" Backward pass computation \"\"\"\n        input, expectation_z = ctx.saved_tensors\n        input_list = np.array(input.tolist())\n        \n        shift_right = input_list + np.ones(input_list.shape) * ctx.shift\n        shift_left = input_list - np.ones(input_list.shape) * ctx.shift\n        \n        gradients = []\n        for i in range(len(input_list)):\n            expectation_right = ctx.quantum_circuit.run(shift_right[i])\n            expectation_left  = ctx.quantum_circuit.run(shift_left[i])\n            \n            gradient = torch.tensor([expectation_right]) - torch.tensor([expectation_left])\n            gradients.append(gradient)\n        gradients = np.array([gradients]).T\n        return torch.tensor([gradients]).float() * grad_output.float(), None, None\n\n#----------------------------\n# module\n#----------------------------\nclass Quanv(nn.Module):\n    \"\"\" Quanvolution(Quantum convolution) layer definition \"\"\"\n    \n    def __init__(self, \n                 in_channels, \n                 out_channels, \n                 kernel_size, \n                 backend=qiskit.providers.aer.QasmSimulator(method = \"statevector_gpu\"), \n                 shots=100, \n                 shift=np.pi/2):\n        \n        super(Quanv, self).__init__()\n        \n        self.quantum_circuits = [QCircuit(kernel_size=kernel_size, \n                                          backend=backend, \n                                          shots=shots, \n                                          threshold=0.5) for i in range(out_channels)]\n        self.in_channels  = in_channels\n        self.out_channels = out_channels\n        self.kernel_size  = kernel_size\n        self.shift        = shift\n        \n    def forward(self, inputs):\n        return QuanvFunction.apply(inputs, \n                                   self.in_channels, \n                                   self.out_channels, \n                                   self.kernel_size,\n                                   self.quantum_circuits, \n                                   self.shift)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"!pip install torchsummary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchsummary import summary\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.quanv = Quanv(1, 6, kernel_size=5)\n        self.conv = nn.Conv2d(6, 16, kernel_size=5)\n        self.dropout = nn.Dropout2d()\n        self.fc1 = nn.Linear(256, 64)\n        self.fc2 = nn.Linear(64, 10)\n\n    def forward(self, x):\n        x = F.relu(self.quanv(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv(x))\n        x = F.max_pool2d(x, 2)\n        x = self.dropout(x)\n        x = torch.flatten(x, start_dim=1)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n\nmodel = Net()\n#summary(model,(1,28,28))\nprint(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"\n# params\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nloss_func = nn.CrossEntropyLoss()\nepochs = 20\nloss_list = []\n\n# train the model\nmodel.train()\n\nfor epoch in range(epochs):\n    total_loss = []\n    for batch_idx, (data, target) in tqdm(enumerate(train_dataloader)):\n        optimizer.zero_grad()\n        # Forward pass\n        output = model(data)\n        # Calculating loss\n        loss = loss_func(output, target)\n        # Backward pass\n        loss.backward()\n        # Optimize the weights\n        optimizer.step()\n        total_loss.append(loss.item())\n    loss_list.append(sum(total_loss)/len(total_loss))\n    print('Training [{:.0f}%]\\tLoss: {:.4f}'.format(100. * (epoch + 1) / epochs, loss_list[-1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation Code ","metadata":{}},{"cell_type":"code","source":"\nmodel.eval()\nwith torch.no_grad():\n    correct = 0\n    for batch_idx, (data, target) in enumerate(test_dataloader):\n        data = data.cuda()\n        target = target.cuda()\n        output = model(data).cuda()\n        pred = output.argmax(dim=1, keepdim=True) \n        correct += pred.eq(target.view_as(pred)).sum().item()\n        loss = loss_func(output, target)\n        total_loss.append(loss.item())\n    print('Performance on test data:\\n\\tLoss: {:.4f}\\n\\tAccuracy: {:.1f}%'.format(\n        sum(total_loss) / len(total_loss),\n        correct / len(test_loader) * 100 / batch_size)\n        )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References and Materials\n* [Previous Work(Basics to get things started)](https://github.com/mnansary/pyQIBM)\n* [Paper: Quantum convolutional neural networks](https://www.nature.com/articles/s41567-019-0648-8)\n* [Video Explaination of the paper](https://www.youtube.com/watch?v=C7kK5m7d10Q)\n* [Tensorflow-quantum-github](https://github.com/tensorflow/quantum/tree/v0.4.0)\n* [CNN with Tensorflow-quantum](https://www.tensorflow.org/quantum/tutorials/qcnn#1_build_a_qcnn)\n* [Qiskit-pytorch hybrid basic (From Textbook)](https://qiskit.org/textbook/ch-machine-learning/machine-learning-qiskit-pytorch.html)\n* [Paper:Quanvolutional Neural Networks: Powering Image Recognition with Quantum Circuits](https://arxiv.org/pdf/1904.04767.pdf)\n\n\nThanks to-\n* [Dohun Kim](https://github.com/yh08037/)\n* [pennylane.ai](pennylane.ai)\n\n\n# How Does Tensoflow Work? (OpKernel,OpContext): Basic Reminder C++\n* Defines a graph\n* Call DirectSession::Run() Internally:\n* The graph gets processed (split/optimized/etc...)\n* Executors get created for each subgraph\n* A RunState gets created\n* Inputs get sent (/stored) in the Rendevouz\n* Executor::RunAsync() for all executors\n* FillContextMap assigns contexts to all nodes\n* The Process(TaggedNode,..) function gets scheduled to the ThreadPool for all root nodes\nThere:\n* The input tensors and parameters get prepared (and an OpKernel and OpContext get created)\n* Device::Compute(kernel,context,..) gets called (sync or async)\n* Device::Compute, when the device is a GPU device, has a different implementation: it checks if the inputs are in a context different from the on for this node (and wait for their completion via an event hook if they're not)\n* Then the current stream gets changed (global for CUDA I think) \n* The OpKernel::Compute method gets called where if there's any kernel to be executed it is practically queued on the stream (which is the equivalent of a command queue in OpenCL).","metadata":{}}]}