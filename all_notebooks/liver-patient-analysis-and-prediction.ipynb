{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Importing the Relevant Library"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/indian-liver-patient-records/indian_liver_patient.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataset Column<br>\n* 1 - Patient with liver disease\n* 2 - Patient with no disease"},{"metadata":{},"cell_type":"markdown","source":"#### Checking for Null Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks for column containing the Null Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data['Albumin_and_Globulin_Ratio'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks for Correct metrics to replace the Null value"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Albumin_and_Globulin_Ratio'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Albumin_and_Globulin_Ratio'].plot.hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here data are normally distrubuted but sightly right skewed.So, we can can go with Mean or Mediam. Here i am going with median."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Albumin_and_Globulin_Ratio'].median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating checkpoint\ndf = data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Albumin_and_Globulin_Ratio'].fillna(data['Albumin_and_Globulin_Ratio'].median(), inplace=True)\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Looking for data description"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe(percentiles=[0.3,.5,.8]).round(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Total number of Rows {df.shape[0]}\\nTotal number of Columns {df.shape[1]}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df, hue='Dataset', palette='viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Looking into univairate features"},{"metadata":{},"cell_type":"markdown","source":"Lets look for Total Bilirubin and Direct Bilirubin\n* Normal results for a total bilirubin test are 1.2 milligrams per deciliter (mg/dL) for adults and usually 1 mg/dL for those under 18.\n* Normal results for direct bilirubin are generally 0.3 mg/dL.\n\n*https://www.mayoclinic.org/tests-procedures/bilirubin/about/pac-20393041#:~:text=Normal%20results%20for%20a%20total,are%20generally%200.3%20mg%2FdL.*\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(5,7))\nax = sns.boxplot(data = df['Total_Bilirubin'],orient='v')\nax.set_ylabel(\"Total Bilirubin\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we could see that the datasets contains huge outliers as Normal Total Bilirubin must contains data around 1.2 but here data shows max upto 75, which are false or huge outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(5,7))\nax = sns.boxplot(data = df['Direct_Bilirubin'],color='Red',orient='v')\nax.set_ylabel(\"Direct Bilirubin\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here also we could see that the datasets contains huge outliers as Normal Direct Bilirubin must contains data around 0.3 but here data shows max upto 20, which are False or huge outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Alkaline_Phosphotase'].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Aspartate_Aminotransferase'].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Total_Protiens'].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Albumin'].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Albumin_and_Globulin_Ratio'].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,6))\nax = sns.countplot(x = df['Dataset'].apply(lambda x:'Liver Disease' if x == 1 else 'Non-Liver Disease'), hue=df['Gender'])\nax.set_xlabel('Patient Condition')\nfor p in ax.patches:\n  ax.annotate(f'{p.get_height()}',(p.get_x()+0.15, p.get_height()+3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets see by Age Category in datset"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(map(lambda x: 'Old_Age' if x>=90 else 'Adult_Age' if x > 21 else \"Young_Age\",df['Age'])).value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 91.5% of data are from 21-89 Age.\n* 8% of data are from below 22 Age.\n* 0.1% of data are from above 89 Age"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('Gender').sum()['Total_Protiens'].plot.bar(color='#253660').set_ylabel('Total_Proteins')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Male consuming more protients then Female. And also there low number of Female in dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating Checkpoint\ndf2 = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Lets Encode the nominal features"},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = pd.get_dummies(data=df2,columns=['Gender','Dataset'], drop_first=True)\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.rename(columns={'Dataset_2':'Have_Disease'},inplace=True)\ndf2 = df2[['Gender_Male', 'Age', 'Total_Bilirubin', 'Direct_Bilirubin', 'Alkaline_Phosphotase',\n       'Alamine_Aminotransferase', 'Aspartate_Aminotransferase',\n       'Total_Protiens', 'Albumin', 'Albumin_and_Globulin_Ratio',\n       'Have_Disease']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Looking for Correlation between features"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,6))\nsns.heatmap(df2.corr(),cmap='GnBu',annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can conclude that,\n* Total_Bilirubin and Direct_Bilirubin are highly colorelated also, Alamine_Aminotransferase and Aspartate_Aminotransferase and Total_Protiens and Albumin.\n* We can delete one features to increase the model training speed and accuracy\n*But this not always true.For reference look into this\nhttps://datascience.stackexchange.com/questions/24452/in-supervised-learning-why-is-it-bad-to-have-correlated-features"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating the check point after removing the correlated data\ndf3 = df2.drop(['Direct_Bilirubin','Aspartate_Aminotransferase','Albumin'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3['Have_Disease'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Here datasets conatins unbalanced Classes so we will try to resample our data to reduce the incorrectness."},{"metadata":{"trusted":true},"cell_type":"code","source":"df3.describe(percentiles=[0.30,0.60,0.90])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Scaling the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n#checkpoint\ndf4 = pd.concat([df3['Gender_Male'],pd.DataFrame(sc.fit_transform(df3.iloc[:,1:7])),df3['Have_Disease']], axis=1)\ndf4.columns = df3.columns\ndf4.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Buliding our models"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will look for metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, precision_score, accuracy_score, recall_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://static.packt-cdn.com/products/9781838555078/graphics/C13314_06_05.jpg\"/>&nbsp;&nbsp;&nbsp;&nbsp;\n\n\n<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2020/04/Equation_Accuracy.png\"/>&nbsp;&nbsp;&nbsp;&nbsp;\n* Accuracy is defined as the ratio of correctly predicted examples by the total examples.\n* Remember, accuracy is a very useful metric when all the classes are equally important. But this might not be the case if we are predicting if a patient has Liver Cancer. In this example, we can probably tolerate FPs but not FNs.\n\n<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2020/04/Confusion-matrix_Precision.png\"/>&nbsp;&nbsp;&nbsp;&nbsp;\n* Precision tells us how many of the correctly predicted cases actually turned out to be positive.\n* Precision is a useful metric in cases where False Positive is a higher concern than False Negatives.\n* Precision is important in music or video recommendation systems, e-commerce websites, etc. Wrong results could lead to customer churn and be harmful to the business.\n\n<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2020/04/Confusion-matrix_Recall.png\"/>&nbsp;&nbsp;&nbsp;&nbsp;\n* Recall tells us how many of the actual positive cases we were able to predict correctly with our model.\n* Recall is a useful metric in cases where False Negative trumps False Positive.\n* Recall is important in medical cases where it doesnâ€™t matter whether we raise a false alarm but the actual positive cases should not go undetected!"},{"metadata":{},"cell_type":"markdown","source":"So,here we will try to focus on recall score value."},{"metadata":{"trusted":true},"cell_type":"code","source":"def ml_algorthims(data):\n    fig, axes = plt.subplots(3,2, figsize=(10,10))\n    print(\"::::::::::::::::::: Splitting the dataset into train and test ::::::::::\")\n    x = data.drop('Have_Disease',axis=1).values\n    y = data.iloc[:,-1].values\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n    print(f\"X_Train : {x_train.shape}\\t X_Test : {x_test.shape}\\nY_Train : {y_train.shape}\\t Y_Test : {y_test.shape}\\n\")\n    print()\n    print(\" :::::::::::::::Logistic Regression::::::::::::: \")\n    lg = LogisticRegression().fit(x_train,y_train)\n    lg_pred = lg.predict(x_test)\n    ax = sns.heatmap(confusion_matrix(y_test,lg_pred),annot=True, ax=axes[0,0])\n    ax.set_title(\"Logistic Confusion Matrix\")\n    # print(classification_report(lg_pred,y_test))\n    print(f'Accuracy : {accuracy_score(y_test,lg_pred):0.2f}')\n    print(f'Precision : {precision_score(y_test,lg_pred):0.2f}')\n    print(f'Recall : {recall_score(y_test,lg_pred):0.2f}')\n    print()\n    print(\" :::::::::::::::Decision Tree Classifier::::::::::::: \")\n    dtree = DecisionTreeClassifier().fit(x_train,y_train)\n    dtree_pred = dtree.predict(x_test)\n    ax = sns.heatmap(confusion_matrix(y_test,dtree_pred),annot=True, ax=axes[0,1])\n    ax.set_title(\"Decision Tree Confusion Matrix\")\n    # print(classification_report(dtree_pred,y_test))\n    print(f'Accuracy : {accuracy_score(y_test,dtree_pred):0.2f}')\n    print(f'Precision : {precision_score(y_test,dtree_pred):0.2f}')\n    print(f'Recall : {recall_score(y_test,dtree_pred):0.2f}')\n    print()\n    print(\" :::::::::::::::Random Forest Classifier::::::::::::: \")\n    rftree = RandomForestClassifier().fit(x_train,y_train)\n    rftree_pred = rftree.predict(x_test)\n    ax = sns.heatmap(confusion_matrix(y_test,rftree_pred),annot=True, ax=axes[1,0])\n    ax.set_title(\"Random Forest Confusion Matrix\")\n    # print(classification_report(rftree_pred,y_test))\n    print(f'Accuracy : {accuracy_score(y_test,rftree_pred):0.2f}')\n    print(f'Precision : {precision_score(y_test,rftree_pred):0.2f}')\n    print(f'Recall : {recall_score(y_test,rftree_pred):0.2f}')\n    print()\n    print(\" :::::::::::::::Xgboost::::::::::::: \")\n    xgb_model = XGBClassifier().fit(x_train,y_train)\n    xgb_model_pred = xgb_model.predict(x_test)\n    ax = sns.heatmap(confusion_matrix(y_test,xgb_model_pred),annot=True, ax=axes[1,1])\n    ax.set_title(\"Xgb Confusion Matrix\")\n    # print(classification_report(xgb_model_pred,y_test))\n    print(f'Accuracy : {accuracy_score(y_test,xgb_model_pred):0.2f}')\n    print(f'Precision : {precision_score(y_test,xgb_model_pred):0.2f}')\n    print(f'Recall : {recall_score(y_test,xgb_model_pred):0.2f}')\n    print()\n    print(\" :::::::::::::::K Nearest Neighbour::::::::::::: \")\n    knn = KNeighborsClassifier().fit(x_train,y_train)\n    knn_pred = knn.predict(x_test)\n    ax = sns.heatmap(confusion_matrix(y_test,knn_pred),annot=True, ax=axes[2,0])\n    ax.set_title(\"KNN Confusion Matrix\")\n    # print(classification_report(knn_pred,y_test))\n    print(f'Accuracy : {accuracy_score(y_test,knn_pred):0.2f}')\n    print(f'Precision : {precision_score(y_test,knn_pred):0.2f}')\n    print(f'Recall : {recall_score(y_test,knn_pred):0.2f}')\n    print()\n    print(\" :::::::::::::::Support Vector Machine (SVM)::::::::::::: \")\n    svm = SVC().fit(x_train,y_train)\n    svm_pred = svm.predict(x_test)\n    ax = sns.heatmap(confusion_matrix(y_test,svm_pred),annot=True, ax=axes[2,1])\n    ax.set_title(\"SVM Confusion Matrix\")\n    # print(classification_report(svm_pred,y_test))\n    print(f'Accuracy : {accuracy_score(y_test,svm_pred):0.2f}')\n    print(f'Precision : {precision_score(y_test,svm_pred):0.2f}')\n    print(f'Recall : {recall_score(y_test,svm_pred):0.2f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings  \nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Unbalanced(Original) Dataset"},{"metadata":{},"cell_type":"markdown","source":"Looking for model accuracy when model is treated with unbalanced dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"ml_algorthims(df4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Balancing the imbalanced Dataset"},{"metadata":{},"cell_type":"markdown","source":"#### 1. Undersampling the majority class"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_class_0 = df4[df4['Have_Disease'] == 0].copy()\ndf_class_1 = df4[df4['Have_Disease'] == 1].copy()\nundersample = df_class_0.sample(df_class_1.shape[0]).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ml_algorthims(pd.concat([undersample,df_class_1],axis=0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2. Oversampling minority class"},{"metadata":{"trusted":true},"cell_type":"code","source":"oversample = df_class_1.sample(df_class_0.shape[0], replace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ml_algorthims(pd.concat([oversample,df_class_0],axis=0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.SMOTE(Synthetic Minority Oversampling Techinque)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsmote = SMOTE(sampling_strategy = 'minority')\nx_sm, y_sm = smote.fit_sample(df4.drop('Have_Disease',axis=1),df4['Have_Disease'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nd = pd.concat([pd.DataFrame(x_sm),pd.DataFrame(y_sm)],axis=1)\nnd.columns = df4.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ml_algorthims(nd)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Upvote if you like, Feedback and suggestions are always welcomeðŸ˜Š"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}