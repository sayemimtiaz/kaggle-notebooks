{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Research Goal\n\nMain goal of this research is to analyze the data and find **What is known about transmission, incubation, and environmental stability? What do we know about natural history, transmission, and diagnostics for the virus? What have we learned about infection prevention and control?**"},{"metadata":{},"cell_type":"markdown","source":"**Reference Kernal:**\n1. https://www.kaggle.com/mobassir/mining-covid-19-scientific-papers/\n2. https://www.kaggle.com/danielwolffram/topic-modeling-finding-related-articles/"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport spacy\nimport re\nimport numpy as np\nimport pandas as pd\nfrom pprint import pprint\n\n# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\n\nfrom nltk.stem import WordNetLemmatizer\nimport spacy\n\n# Plotting tools\nimport pyLDAvis\nimport pyLDAvis.gensim  \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n# Enable logging for gensim - optional\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nn = 1000\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"!pip install  tensorflow-gpu==2.1.0","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"!pip install bert-tensorflow","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"!wget https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip    ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"!unzip cased_L-12_H-768_A-12.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\ntf.version","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import bert\nfrom bert import run_classifier\nfrom bert import optimization\nfrom bert import tokenization\nfrom bert import modeling\nimport numpy as np\nimport itertools","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Reading The updated clean CSV files from this kernel CORD-19: EDA, parse JSON and generate clean CSV**"},{"metadata":{"trusted":true},"cell_type":"code","source":"biorxiv_clean = pd.read_csv('/kaggle/input/cord-19-eda-parse-json-and-generate-clean-csv/biorxiv_clean.csv')\nclean_comm_use = pd.read_csv('/kaggle/input/cord-19-eda-parse-json-and-generate-clean-csv/clean_comm_use.csv')\nclean_noncomm_use =  pd.read_csv('/kaggle/input/cord-19-eda-parse-json-and-generate-clean-csv/clean_noncomm_use.csv')\nclean_pmc =  pd.read_csv('/kaggle/input/cord-19-eda-parse-json-and-generate-clean-csv/clean_pmc.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> # Part 1 :  Finding similarity research papers on biorxiv"},{"metadata":{},"cell_type":"markdown","source":"# biorxiv_clean papers Abstract - frequent words (400 sample)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\n\nstopwords = set(STOPWORDS)\n#https://www.kaggle.com/gpreda/cord-19-solution-toolbox\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=200,\n        max_font_size=30, \n        scale=5,\n        random_state=1\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(10,10))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=14)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_wordcloud(biorxiv_clean['abstract'], title = 'biorxiv_clean - papers Abstract - frequent words (400 sample)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Find similar research papers using universalsentenceencoderlarge4"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df = biorxiv_clean\ndf = df.abstract.dropna()\ndata = df.values.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"%%time\n#ref : https://gist.github.com/gaurav5430/8d7810495ec3f914ffb151458f352c60\n\n'''import tensorflow_hub as hub\nfrom sklearn.metrics.pairwise import cosine_similarity\ndef prepare_similarity(vectors):\n    similarity=cosine_similarity(vectors)\n    return similarity\n\ndef get_top_similar(sentence, sentence_list, similarity_matrix, topN):\n    # find the index of sentence in list\n    index = sentence_list.index(sentence)\n    # get the corresponding row in similarity matrix\n    similarity_row = np.array(similarity_matrix[index, :])\n    # get the indices of top similar\n    indices = similarity_row.argsort()[-topN:][::-1]\n    return [sentence_list[i] for i in indices]\n\n\nmeta=pd.read_csv(\"../input/CORD-19-research-challenge/metadata.csv\")\nmodule_url = \"../input/universalsentenceencoderlarge4\" \nembed = hub.load(module_url)\n\n\n# Creating an empty Dataframe with column names only\nsimsentence = pd.DataFrame()\n\ntitles=meta['title'].fillna(\"Unknown\")\nembed_vectors=embed(titles[:5000].values)['outputs'].numpy()\nsentence_list=titles.values.tolist()\nfor i in range(5):\n\n    sentences=titles.iloc[i]\n    #print(\">>>>>>>>>>>>Using title Find similar research papers for :\",sentences, \"<<<<<<<<<<<<\")\n\n    similarity_matrix=prepare_similarity(embed_vectors)\n    similar=get_top_similar(sentences,sentence_list,similarity_matrix,6)\n    for sentence in similar:\n        #print(sentence)\n        simsentence = simsentence.append({'sentence': sentences, 'similar': sentence}, ignore_index=True)\n        #print(\"\\n\") '''\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The results of similar sentences are pre-loaded as simsentence.csv **"},{"metadata":{"trusted":true},"cell_type":"code","source":"simsentence = pd.read_csv('../input/simsentence/simsentence.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build LDA model"},{"metadata":{},"cell_type":"markdown","source":"**The function below converts sentences to words using gensim**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  \n\ndata_words = list(sent_to_words(data))\n\nprint(data_words[:1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build the bigram and trigram models using gensim"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build the bigram and trigram models\nbigram = gensim.models.Phrases(data_words, min_count=5, threshold=20) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[data_words], threshold=20)  \n\n# Faster way to get a sentence clubbed as a trigram/bigram\nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)\n\n# See trigram example\nprint(trigram_mod[bigram_mod[data_words[1]]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nstop_words.extend(['background', 'methods', 'introduction', 'conclusions', 'results', \n                   'purpose', 'materials', 'discussions','methodology','result analysis'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#https://github.com/cjriggio/classifying_medical_innovation\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\n\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https://spacy.io/api/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Remove Stop Words\ndata_words_nostops = remove_stopwords(data_words)\n\n# Form Bigrams\ndata_words_bigrams = make_bigrams(data_words_nostops)\n\n# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n# python3 -m spacy download en\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\n# Do lemmatization keeping only noun, adj, vb, adv\ndata_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n\nprint(data_lemmatized[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data_lemmatized[:1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Dictionary,Corpus and Document Frequency"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Dictionary\nid2word = corpora.Dictionary(data_lemmatized)\n\n# Create Corpus\ntexts = data_lemmatized\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\n\n# View\nprint(corpus[:1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Human readable format of corpus (term-frequency)"},{"metadata":{"trusted":true},"cell_type":"code","source":"[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=8, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True\n                                        )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Print the Keyword in the n topics"},{"metadata":{"trusted":true},"cell_type":"code","source":"pprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compute Perplexity and Coherence Score"},{"metadata":{},"cell_type":"markdown","source":"*perplexity is a measurement of how well a probability distribution or probability model predicts a sample where The coherence score is for assessing the quality of the learned topics. *\n\n\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.#\n\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Hyperparameter tuning\ndef compute_coherence_values(limit, start=2, step=3):\n    \"\"\"\n    Compute c_v coherence for various number of topics\n\n    Parameters:\n    ----------\n    dictionary : Gensim dictionary\n    corpus : Gensim corpus\n    texts : List of input texts\n    limit : Max num of topics\n\n    Returns:\n    -------\n    model_list : List of LDA topic models\n    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n    \"\"\"\n    coherence_values = []\n    model_list = []\n    for num_topics in range(start, limit, step):\n        model=gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=num_topics,random_state=100,update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)\n        model_list.append(model)\n        coherencemodel = CoherenceModel(model=model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n        coherence_values.append(coherencemodel.get_coherence())\n    return model_list, coherence_values\n\nmodel_list, coherence_values = compute_coherence_values(start=2, limit=40, step=6)\n# Show graph\nimport matplotlib.pyplot as plt\nlimit=40; start=2; step=6;\nx = range(start, limit, step)\nplt.plot(x, coherence_values)\nplt.xlabel(\"Num Topics\")\nplt.ylabel(\"Coherence score\")\nplt.legend((\"coherence_values\"), loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualize the topics"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"%%time\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\npyLDAvis.save_html(vis, './lda4topics_v2.html')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vis","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Open Research Tasks: **\nTopic 2 & 4: transmission, Topic 7:physical science of the coronavirus, Topic 6: Infection and Incutation\n* Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery.\n* Prevalence of asymptomatic shedding and transmission .\n* Seasonality of transmission. \n\n* Physical science of the coronavirus (e.g., charge distribution, adhesion to hydrophilic/phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding).\n* Persistence and stability on a multitude of substrates and sources (e.g., nasal discharge, sputum, urine, fecal matter, blood).\n* Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic).\n* Natural history of the virus and shedding of it from an infected person\n* Implementation of diagnostics and products to improve clinical processes\n* Disease models, including animal models for infection, disease and transmission\n* Tools and studies to monitor phenotypic change and potential adaptation of the virus\n* Immune response and immunity\n* Effectiveness of movement control strategies to prevent secondary transmission in health care and community settings\n* Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings\n* Role of the environment in transmission"},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install scispacy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!conda install -y -c conda-forge spacy-model-en_core_web_lg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Wordcloud of Top N words in each topic"},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.colors as mcolors\n\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  \n\ncloud = WordCloud(stopwords=stop_words,\n                  background_color='white',\n                  width=2500,\n                  height=1800,\n                  max_words=100,\n                  colormap='tab10',\n                  color_func=lambda *args, **kwargs: cols[i],\n                  prefer_horizontal=1.0)\n\ntopics = lda_model.show_topics(formatted=False, \n                               num_words=30)\n\nfig, axes = plt.subplots(5, 1, figsize=(10,20), sharex=True, sharey=True)\n\nfor i, ax in enumerate(axes.flatten()):\n    fig.add_subplot(ax)\n    topic_words = dict(topics[i][1])\n    cloud.generate_from_frequencies(topic_words, max_font_size=500)\n    plt.gca().imshow(cloud)\n    plt.gca().set_title('Topic ' + str(i + 1), fontdict=dict(size=16))\n    plt.gca().axis('off')\n\n\nplt.subplots_adjust(wspace=0, hspace=0)\nplt.axis('off')\nplt.margins(x=0, y=0)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get topic weights and dominant topics"},{"metadata":{"trusted":true},"cell_type":"code","source":"from bokeh.models import HoverTool\nfrom sklearn.manifold import TSNE\nfrom bokeh.plotting import figure, output_file, show\nfrom bokeh.models import Label\nfrom bokeh.io import output_notebook\n\n# Get topic weights\ntopic_weights = []\nfor i, row_list in enumerate(lda_model[corpus]):\n    topic_weights.append([w for i, w in row_list[0]])\n\n# Array of topic weights    \narr = pd.DataFrame(topic_weights).fillna(0).values\n\n# Keep the well separated points (optional)\narr = arr[np.amax(arr, axis=1) > 0.35]\n\n# Dominant topic number in each doc\ntopic_num = np.argmax(arr, axis=1)\n\n# tSNE Dimension Reduction\ntsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\ntsne_lda = tsne_model.fit_transform(arr)\n\n# Plot the Topic Clusters using Bokeh\noutput_notebook()\nn_topics = 5\nmycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n\n\nplot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n              plot_width=800, plot_height=600)\nplot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\nshow(plot)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom pprint import pprint","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https://spacy.io/api/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n    return texts_out\n\n    # # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n    # # Run in terminal: python3 -m spacy download en\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\n    # # Do lemmatization keeping only Noun, Adj, Verb, Adverb\ndata_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n\nprint(data_lemmatized[:2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer(analyzer='word', min_df=10,                        # minimum reqd occurences of a word \n                              stop_words='english',             # remove stop words\n                              lowercase=True,                   # convert all words to lowercase\n                              token_pattern='[a-zA-Z0-9]{3,}'  # num chars > 3\n                              # max_features=50000,             # max number of uniq words\n                             )\ndata_vectorized = vectorizer.fit_transform(data_lemmatized)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scispacy\nimport spacy\n\n\nfrom scipy.spatial.distance import jensenshannon\n\nimport joblib\n\nfrom IPython.display import HTML, display\n\nfrom ipywidgets import interact, Layout, HBox, VBox, Box\nimport ipywidgets as widgets\nfrom IPython.display import clear_output\n\nfrom tqdm import tqdm\nfrom os.path import isfile\n\nimport seaborn as sb\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"biorxiv_clean.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc_topic_dist = pd.DataFrame(lda.transform(data_vectorized))\ndoc_topic_dist.to_csv('doc_topic_dist.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc_topic_dist.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# medium model\nnlp = spacy.load('en_core_web_lg', disable=[\"tagger\", \"parser\", \"ner\"])\nnlp.max_length = 2000000\ndef spacy_tokenizer(sentence):\n    return [word.lemma_ for word in nlp(sentence) if not (word.like_num or word.is_stop or word.is_punct or word.is_space or len(word)==1)]\nlda = LatentDirichletAllocation(n_components=50, random_state=0)\nlda.fit(data_vectorized)\njoblib.dump(lda, 'lda.csv')\nis_covid19_article = biorxiv_clean.text.str.contains('COVID-19|SARS-CoV-2|2019-nCov|SARS Coronavirus 2|2019 Novel Coronavirus')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Find similar papers related by topics**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_k_nearest_docs(doc_dist, k=5, lower=1950, upper=2020, only_covid19=False, get_dist=False):\n    '''\n    doc_dist: topic distribution (sums to 1) of one article\n    \n    Returns the index of the k nearest articles (as by Jensenâ€“Shannon divergence in topic space). \n    '''\n    \n\n    temp = doc_topic_dist[is_covid19_article]\n   \n    distances = temp.apply(lambda x: jensenshannon(x, doc_dist), axis=1)\n    k_nearest = distances[distances != 0].nsmallest(n=k).index\n    \n    if get_dist:\n        k_distances = distances[distances != 0].nsmallest(n=k)\n        return k_nearest, k_distances\n    else:\n        return k_nearest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_article_dna(paper_id, width=20):\n    t = df[df.paper_id == paper_id].title.values[0]\n    doc_topic_dist[df.paper_id == paper_id].T.plot(kind='bar', legend=None, title=t, figsize=(width, 4))\n    plt.xlabel('Topic')\n\ndef compare_dnas(paper_id, recommendation_id, width=20):\n    t = df[df.paper_id == recommendation_id].title.values[0]\n    temp = doc_topic_dist[df.paper_id == paper_id]\n    ymax = temp.max(axis=1).values[0]*1.25\n    temp = pd.concat([temp, doc_topic_dist[df.paper_id == recommendation_id]])\n    temp.T.plot(kind='bar', title=t, figsize=(width, 4), ylim= [0, ymax])\n    plt.xlabel('Topic')\n    plt.legend(['Selection', 'Recommendation'])\n\n\n\ndef dna_tabs(paper_ids):\n    k = len(paper_ids)\n    outs = [widgets.Output() for i in range(k)]\n\n    tab = widgets.Tab(children = outs)\n    tab_titles = ['Paper ' + str(i+1) for i in range(k)]\n    for i, t in enumerate(tab_titles):\n        tab.set_title(i, t)\n    display(tab)\n\n    for i, t in enumerate(tab_titles):\n        with outs[i]:\n            ax = plot_article_dna(paper_ids[i])\n            plt.show(ax)\n\ndef compare_tabs(paper_id, recommendation_ids):\n    k = len(recommendation_ids)\n    outs = [widgets.Output() for i in range(k)]\n\n    tab = widgets.Tab(children = outs)\n    tab_titles = ['Paper ' + str(i+1) for i in range(k)]\n    for i, t in enumerate(tab_titles):\n        tab.set_title(i, t)\n    display(tab)\n\n    for i, t in enumerate(tab_titles):\n        with outs[i]:\n            ax = compare_dnas(paper_id, recommendation_ids[i])\n            plt.show(ax)\n            \n# Search related papers to a chosen one            \ndef recommendation(paper_id, k=5, lower=1950, upper=2020, only_covid19=False, plot_dna=False):\n    '''\n    Returns the title of the k papers that are closest (topic-wise) to the paper given by paper_id.\n    '''\n    \n    print(df.title[df.paper_id == paper_id].values[0])\n\n    recommended, dist = get_k_nearest_docs(doc_topic_dist[df.paper_id == paper_id].iloc[0], k, lower, upper, only_covid19, get_dist=True)\n    recommended = df.iloc[recommended].copy()\n    recommended['similarity'] = 1 - dist \n    \n    h = '<br/>'.join(['<a href=\"' + l + '\" target=\"_blank\">'+ n + '</a>' +' (Similarity: ' + \"{:.2f}\".format(s) + ')' for n, s in recommended[['title', 'similarity']].values])\n    display(HTML(h))\n    \n    if plot_dna:\n        compare_tabs(paper_id, recommended.paper_id.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def related_papers(df):\n    '''\n    Creates a widget where you can select one of many papers about covid-19 and then displays related articles from the whole dataset.\n    '''\n    covid_papers = df[df.text.str.contains('COVID-19|SARS-CoV-2|2019-nCov|SARS Coronavirus 2|2019 Novel Coronavirus')][['paper_id', 'title']] # are there more names?\n    title_to_id = covid_papers.set_index('title')['paper_id'].to_dict()\n    \n    def main_function(bullet, k=5, year_range=[1950, 2020], only_covid19=False):\n        recommendation(title_to_id[bullet], k, lower=year_range[0], upper=year_range[1], only_covid19=only_covid19)\n    \n    yearW = widgets.IntRangeSlider(min=1950, max=2020, value=[2010, 2020], description='Year Range', \n                                   continuous_update=False, layout=Layout(width='40%'))\n    covidW = widgets.Checkbox(value=False,description='Only COVID-19-Papers',disabled=False, indent=False, layout=Layout(width='20%'))\n    kWidget = widgets.IntSlider(value=10, description='k', max=50, min=1, layout=Layout(width='20%'))\n\n    bulletW = widgets.Select(options=title_to_id.keys(), layout=Layout(width='90%', height='200px'), description='Title:')\n\n    widget = widgets.interactive(main_function, bullet=bulletW, k=kWidget, year_range=yearW, only_covid19=covidW)\n\n    controls = VBox([Box(children=[widget.children[:-1][1], widget.children[:-1][2], widget.children[:-1][3]], \n                         layout=Layout(justify_content='space-around')), widget.children[:-1][0]])\n    output = widget.children[-1]\n    display(VBox([controls, output]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"related_papers(biorxiv_clean)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* # Part 3 : Semantic Search using Sentence Transformer on clean_pmc.csv file"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install sentence-transformers\n\"\"\"\nThis is a simple application for sentence embeddings: semantic search\nWe have a corpus with various sentences. Then, for a given query sentence,\nwe want to find the most similar sentence in this corpus.\nThis script outputs for various queries the top 5 most similar sentences in the corpus.\n\"\"\"\n# taken from : https://github.com/UKPLab/sentence-transformers/blob/master/examples/application_semantic_search.py\nfrom sentence_transformers import SentenceTransformer\nimport scipy.spatial\n\nembedder = SentenceTransformer('bert-base-nli-mean-tokens')\n\n# Corpus with example sentences\ncorpus = simsentence.similar.tolist()\ncorpus_embeddings = embedder.encode(corpus)\n\n# Query sentences:\nqueries = ['Range of incubation periods for the disease in humans', 'Persistence of virus on surfaces of different materials','social distancing','personal protective equipment reduce risk of transmission', 'Implementation of diagnostics and products to improve clinical processes']\nquery_embeddings = embedder.encode(queries)\n\n# Find the closest  sentences of the corpus for each query sentence based on cosine similarity\nclosest_n = 5\nfor query, query_embedding in zip(queries, query_embeddings):\n    distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings, \"cosine\")[0]\n\n    results = zip(range(len(distances)), distances)\n    results = sorted(results, key=lambda x: x[1])\n\n    print(\"\\n\\n======================\\n\\n\")\n    print(\"Query:\", query)\n    print(\"\\nTop 5 most similar sentences from similar\")\n\n    for idx, distance in results[0:closest_n]:\n        print(corpus[idx].strip(), \"(Score: %.4f)\" % (1-distance))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}