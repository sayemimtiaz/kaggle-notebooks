{"cells":[{"metadata":{},"cell_type":"markdown","source":"#  COVID-19 Risk Factor Analysis - Task 2\n \n## Introduction:\n  \nThe notebook is for task 2: risk factor analysis for covid-19. The main purpose is to generate most relavanet research papers based on given questions which are related to risk factors for COVID-19. The risk factors for COVID-19 contains the following aspects:\n\n- Data on potential risks factors\n- Smoking, pre-existing pulmonary disease\n- Co-infections (determine whether co-existing respiratory/viral infections make the virus more transmissible or virulent) and other co-morbidities\n- Neonates and pregnant women\n- Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.\n- Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors\n- Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups\n- Susceptibility of populations\n- Public health mitigation measures that could be effective for control\n\nWe build multiple solutions to answer these questions. The model in this notebook contains:\n\n- Top-N recommended papers based on Word2Vec key words similarity: we trained multiple word2vec models that are used to   find the most similar word (tokens) for given question and match the most relevant content in data base (papers)\n   \n\n- Topic modeling using LDA: use LDA topic modeling, we generate the relevant scores for each risk factors given papers\n  ONE STEP TODO: generate most relevant articles\n\n- Classification based supervised RNN: we also leverage w2v models we have trained to label (assign) each risk factors on given paper. We then train a RNN (recurrent neural network) to learn the classes in supervised learning. The output will be the most revevant papers with details including paper id, title, abstract and its short summary. \n\n** The notebook is dependent on package(sumy) which will be installed using pip install. So please turn on the internet options from the kernel settings before running the notebook. **\n"},{"metadata":{},"cell_type":"markdown","source":" # 0. Install some libs\n "},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install sumy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Load libs\n\n- load all neccesary libs used in the analysis\n- Select Risk Factor if you want to search for specific risk factor question listed in this section"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gensim\nimport re\nimport os\nimport json\nimport os\nimport glob\nimport gc\nimport pickle\nimport sys\nimport tensorflow as tf\nimport nltk\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport time\nimport spacy\n\nimport pyLDAvis\nimport pyLDAvis.gensim \nimport logging\nimport seaborn as sns\nimport wordcloud\n\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n\n\nfrom tqdm import tqdm\nfrom copy import deepcopy\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom collections import OrderedDict\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict\nfrom collections import Counter\n\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.models import FastText\nfrom gensim.corpora import Dictionary\nfrom gensim.models import LdaModel, LdaMulticore,CoherenceModel\nfrom gensim.models.fasttext import FastText\n\n\nfrom sklearn.manifold import TSNE\nfrom nltk.tokenize import RegexpTokenizer,word_tokenize,sent_tokenize\nfrom nltk.corpus import stopwords,wordnet\nfrom nltk.stem import WordNetLemmatizer,SnowballStemmer\nfrom nltk.stem.porter import *\nfrom nltk import ngrams\nfrom nltk import pos_tag\n\nfrom nltk import WordPunctTokenizer\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('stopwords')\nfrom pprint import pprint\nfrom plotly.offline import plot\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.decomposition import PCA\nfrom scipy.spatial.distance import cosine\nfrom nltk.stem import PorterStemmer\nfrom scipy.spatial import distance\nfrom matplotlib import pyplot as plt\nfrom wordcloud import WordCloud\n\nfrom string import punctuation\nfrom sumy.parsers.plaintext import PlaintextParser\nfrom sumy.nlp.tokenizers import Tokenizer\nfrom sumy.summarizers.lsa import LsaSummarizer\nfrom sumy.nlp.stemmers import Stemmer\nfrom sumy.utils import get_stop_words\n \nfrom sumy.summarizers.luhn import LuhnSummarizer\nfrom sumy.summarizers.edmundson import EdmundsonSummarizer\n\nfrom gensim.summarization.summarizer import summarize\nfrom sumy.summarizers.lex_rank import LexRankSummarizer \n\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation,GRU, Conv1D,GlobalAveragePooling2D,SpatialDropout1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D,concatenate,Concatenate,GlobalAvgPool1D,BatchNormalization,Flatten\nfrom keras.layers import GlobalAveragePooling1D,GlobalMaxPooling1D,PReLU,Reshape,Conv2D,MaxPool2D,add,SpatialDropout1D\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras.callbacks import Callback\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint\n\nfrom IPython.display import display, Markdown, HTML, Javascript\nimport ipywidgets as widgets\n\nnp.random.seed(2018)\n\n%matplotlib inline\nprint(\"Python %s\" % sys.version)\nprint(\"NumPy %s\" % np.__version__)\nprint(\"matplotlib %s\" % matplotlib.__version__)\nprint(\"TensorFlow %s\" % tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"questions_to_ask = ['Corona virus for smoking, pre-existing pulmonary or lung disease', \n                    'Co-infections or co-existing respiratory and viral infections make the virus more transmissible or virulent and other co-morbidities', \n                    'Neonates and pregnant women in corona virus',\n                    'Socio-economic and behavioral factors on corona virus',\n                    'Transmission dynamics, reproductive number, incubation period, serial interval, modes of transmission and environmental factors on corona virus',\n                    'Severity, risk of fatality among symptomatic hospitalized patients on corona virus',\n                    'Susceptibility of populations on corona virus',\n                    'Public health mitigation measures on corona virus']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initiate the dropdown box\ndropdown_question = widgets.Dropdown(options = questions_to_ask, description='Risk factor:')\n\n# Display the dropdown boxes\ndisplay(dropdown_question)\n\ndef execute_analysis(ev):\n    display(Javascript('IPython.notebook.execute_cell_range(IPython.notebook.get_selected_index()+1, IPython.notebook.ncells())'))\n\nbutton = widgets.Button(description=\"Execute analysis\")\nbutton.on_click(execute_analysis)\ndisplay(button)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"asked_question = dropdown_question.value\nasked_question","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CL_INPUT_DIRECTORY = '/kaggle/input/'\nCL_WORKING_DIRECTORY = '/kaggle/classification/'\nW2V_WORKING_DIRECTORY = '/kaggle/w2v/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = RegexpTokenizer(r'\\w+')\nstopwords_list = stopwords.words('english')\nwordnet_lemmatizer = WordNetLemmatizer()\nen_stop = set(nltk.corpus.stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summarizer = LexRankSummarizer(Stemmer('eng'))\nsummarizer.stop_words = get_stop_words('eng')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Helper functions\n\n- some helper functions for analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# some functions used in the work\ndef format_name(author):\n    try:\n        middle_name = \" \".join(author['middle'])\n\n        if author['middle']:\n            return \" \".join([author['first'], middle_name, author['last']])\n        else:\n            return \" \".join([author['first'], author['last']])\n    except:\n        return \"\"\n\n\ndef format_affiliation(affiliation):\n    try:\n        text = []\n        location = affiliation.get('location')\n        if location:\n            text.extend(list(affiliation['location'].values()))\n\n        institution = affiliation.get('institution')\n        if institution:\n            text = [institution] + text\n        return \", \".join(text)\n    except:\n        return \"\"\n\ndef format_authors(authors, with_affiliation=False):\n    try:\n        name_ls = []\n\n        for author in authors:\n            name = format_name(author)\n            if with_affiliation:\n                affiliation = format_affiliation(author['affiliation'])\n                if affiliation:\n                    name_ls.append(f\"{name} ({affiliation})\")\n                else:\n                    name_ls.append(name)\n            else:\n                name_ls.append(name)\n\n        return \", \".join(name_ls)\n    except:\n        return \"\"\n\ndef format_body(body_text):\n    try:\n        texts = [(di['section'], di['text']) for di in body_text]\n        texts_di = {di['section']: \"\" for di in body_text}\n\n        for section, text in texts:\n            texts_di[section] += text\n\n        body = \"\"\n\n        for section, text in texts_di.items():\n            body += section\n            body += \"\\n\\n\"\n            body += text\n            body += \"\\n\\n\"\n\n        return body\n    except:\n        return \"\"\n\ndef format_bib(bibs):\n    try:\n        if type(bibs) == dict:\n            bibs = list(bibs.values())\n        bibs = deepcopy(bibs)\n        formatted = []\n\n        for bib in bibs:\n            bib['authors'] = format_authors(\n                bib['authors'], \n                with_affiliation=False\n            )\n            formatted_ls = [str(bib[k]) for k in ['title', 'authors', 'venue', 'year']]\n            formatted.append(\", \".join(formatted_ls))\n\n\n        return \"; \".join(formatted)\n    except:\n        return \"\"\n\n\ndef load_files(dirname):\n    filenames = os.listdir(dirname)\n    raw_files = []\n\n    for filename in tqdm(filenames):\n        filename = dirname + filename\n        file = json.load(open(filename, 'rb'))\n        raw_files.append(file)\n    \n    return raw_files\n\ndef generate_clean_df(all_files,save_ram=True):\n    \"\"\"\n    the main function to parse all json files\n    \"\"\"\n    cleaned_files = []\n    paper_dict = dict()\n\n    for f in tqdm(all_files):\n        file = json.load(open(f,'rb'))\n        try:\n            if save_ram:\n                features = [\n                    file['paper_id'],\n                    file['metadata']['title'],\n                    format_body(file['abstract']),\n#                     format_body(file['body_text']),\n                ]\n            else:\n                features = [\n                    file['paper_id'],\n                    file['metadata']['title'],\n                    format_authors(file['metadata']['authors']),\n                    format_authors(file['metadata']['authors'], with_affiliation=True),\n                    format_body(file['abstract']),\n                    format_body(file['body_text']),\n                    format_bib(file['bib_entries']),\n                    file['metadata']['authors'],\n                    file['bib_entries']\n                ]\n            paper_dict[file['paper_id']] = f\n            \n        except:\n            pass\n\n        cleaned_files.append(features)\n    if save_ram:\n        col_names = ['paper_id', 'title', 'abstract']\n    else:\n        col_names = ['paper_id', 'title', 'abstract',\n                     'affiliations', 'authors', 'text', \n                     'bibliography','raw_authors','raw_bibliography'\n                    ]\n\n    clean_df = pd.DataFrame(cleaned_files, columns=col_names)\n#     clean_df = clean_df.drop(['raw_authors', 'raw_bibliography'], axis=1)    \n    return clean_df,paper_dict\n\n\ndef clean_text(x):\n    x = x.lower()\n    regex = re.compile('([^\\s\\w]|_)+')\n    sentence = regex.sub('', x).lower()\n    sentence = sentence.split(\" \")\n    \n    for word in list(sentence):\n        if word in stopwords_list:\n            sentence.remove(word)\n    tokens = [wordnet_lemmatizer.lemmatize(w.strip()) for w in sentence if w not in stopwords_list and len(w)>=2]\n    return tokens\n\n\ndef collect_simi_tokens(q_tokens,topn=20):\n    simi_words = []\n    simi_words += q_tokens\n    for i,w in enumerate(q_tokens):\n        if w in list(covid19_w2v_300.wv.vocab.keys()): \n            msword_1 = covid19_w2v_300.wv.most_similar(w,topn=topn)\n            simi_words += list(set([v[0] for v in msword_1]))\n            \n        if w in list(covid19_w2v_30.wv.vocab.keys()):\n            msword_2 = covid19_w2v_30.wv.most_similar(w,topn=topn)\n            simi_words += list(set([v[0] for v in msword_2]))\n\n        if w in list(covid19_w2v_50.wv.vocab.keys()):\n            msword_3 = covid19_w2v_50.wv.most_similar(w,topn=topn)\n            simi_words += list(set([v[0] for v in msword_3]))\n            \n            \n    simi_words = list(set([w.lower() for w in simi_words]))\n    simi_words_lem = [wordnet_lemmatizer.lemmatize(w) for w in simi_words]\n    corona_words = ['corona','covid','coronavirus','2019ncov']\n    return set(simi_words+simi_words_lem+corona_words)\n    \n    \ndef get_answer(question,features_list,topn,showabs = False,print_info=False):\n    q_tokens = clean_text(question)\n    q_tokens = set(list(collect_simi_tokens(q_tokens,10)))\n    clean_df['title_abs_score'] = clean_df.apply(lambda row:scoring(q_tokens,row,features_list),axis=1)\n\n    top_ind = clean_df.sort_values('title_abs_score',ascending=False)[:topn].index\n    ids = []\n    for ind in list(top_ind):\n        article = clean_df.iloc[ind]\n        pid = article['paper_id']\n        \n        if pid in paper_dict.keys():\n            path = paper_dict[pid]\n            f = json.load(open(path,'rb'))\n            title = f['metadata']['title']\n#             print('kkk',title)\n            if print_info:\n                print('paper_idï¼š{} \\n'.format(pid))\n                print('Title: {}\\n'.format(title))\n                if showabs:\n                    print('Abstract: {}\\n'.format(format_body(f['abstract'])))\n                print('='*100)\n            ids.append(pid)\n        else:\n            continue\n    return ids\n    \ndef get_summerization(paper_id,sentence_count=20):\n    try:\n        f = paper_dict[paper_id]\n        file = json.load(open(f,'rb'))\n        txt = format_body(file['body_text'])\n        language = \"english\"\n        sentence_count = sentence_count\n        parser = PlaintextParser(txt, Tokenizer(language))\n        summary = summarizer(parser.document, sentence_count)\n        sentences = []\n        for sentence in summary:\n            print(sentence)\n        return txt\n    \n    except:\n        return 'No text for this paper'\n         \n    \ndef scoring(q_tokens,row,features_list):\n    t_tokens = []\n    for f in features_list:\n        t_tokens+=row[f]\n    if len(t_tokens)>10: # only think articles with more tokens\n        t_tokens = [w.lower() for w in t_tokens if w not in stopwords_list and len(w)>2] \n        t_tonens_lem = [wordnet_lemmatizer.lemmatize(w) for w in t_tokens if w not in stopwords_list and len(w)>2]\n        t_tokens = t_tokens + t_tonens_lem\n        common_tokens = q_tokens.intersection(set(t_tokens))\n        return len(q_tokens.intersection(set(t_tokens))) #+N\n    else:\n        return 0\n    \nclass Lemma_Nword_phases():\n\n    def __init__(self,\n                 if_single_lemma,\n                 sentence_token\n                 ):\n        '''\n        Input for this class should be tokenized, cleaned sentence\n\n        '''\n        #Taken from Su Nam Kim Paper...\n        grammar = r\"\"\"\n                        NBAR:\n                            {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n\n                        NP:\n                            {<NBAR>}\n                            {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n                    \"\"\"\n\n        self.wnl=WordNetLemmatizer()\n        self.chunker=nltk.RegexpParser(grammar)\n        self._grammar=grammar\n                \n        self.result_ = None\n        self.tokens_= None\n        self.tagged_sent_= None\n        \n        self._if_single_lemma = if_single_lemma\n        self._sentence_token=sentence_token\n    \n    def sentence_token_fit(self):\n        self.tagged_sent_=  pos_tag(self._sentence_token)\n        if self._if_single_lemma==False:\n            self.chunker = nltk.RegexpParser(self._grammar)\n\n    \n    def singleW_lemma(self):\n            \n        def _get_wordnet_pos(tag):\n            if tag.startswith('J'):\n                return wordnet.ADJ\n            elif tag.startswith('V'):\n                return wordnet.VERB\n            elif tag.startswith('N'):\n                return wordnet.NOUN\n            elif tag.startswith('R'):\n                return wordnet.ADV\n            else:\n                return None\n        ## parse according to pos better->good\n        lemmas_sent = []\n        for tag in self.tagged_sent_:\n            wordnet_pos = _get_wordnet_pos(tag[1]) or wordnet.NOUN\n            lemmas_sent.append(self.wnl.lemmatize(tag[0], pos=wordnet_pos).lower()) \n\n        lemmas_sent = [w for w in lemmas_sent]\n        try:\n            lemmas_sent[0].remove('``')\n        except:\n            pass\n        self.result_= lemmas_sent\n        \ndef lemma_main(if_single_lemma,sentence_token):\n    Lemma=Lemma_Nword_phases(if_single_lemma,sentence_token)\n    Lemma.sentence_token_fit()\n    Lemma.singleW_lemma()\n\n    return ' '.join(Lemma.result_)\n\nclass Cleaning:\n    def __init__(self,\n                stopwords_list,\n                medical_stopwords,\n                sentence_token):\n        \n        self.sentence = sentence_token\n        self.stopwords = stopwords_list\n        self.medical_stopwords = medical_stopwords\n        \n    def _remove_nonalphaW(self):\n        self.sentence = self.sentence.lower()\n        \n        try:\n            self.sentence = re.sub('[^A-Za-z\\s]+','',str(self.sentence))\n        except:\n            self.sentence = re.sub('^A-Za-z\\s+','',str(self.sentence))\n            \n    def _remove_stopW(self):\n        self.sentence = [x.strip() for x in self.sentence.split() if (x not in self.stopwords + self.medical_stopwords)]\n        \ndef cleaning_call(sentence):\n    clean = Cleaning(stopwords_list,['title','introduction','abstract'],sentence)\n    clean._remove_nonalphaW()\n    clean._remove_stopW()\n    return(clean.sentence)\n\n#Helper for getting synonyms for aparticular word using wordnet\n\nfrom nltk.corpus import wordnet as wn\n\n#Get all synonym sets\ndef get_all_synsets(word, pos=None):\n    for ss in wn.synsets(word):\n        for lemma in ss.lemma_names():\n            yield (lemma, ss.name())\n\n#Get all hyponyms\ndef get_all_hyponyms(word, pos=None):\n    for ss in wn.synsets(word, pos=pos):\n            for hyp in ss.hyponyms():\n                for lemma in hyp.lemma_names():\n                    yield (lemma, hyp.name())\n\n#get all similar words\ndef get_all_similar_tos(word, pos=None):\n    for ss in wn.synsets(word):\n            for sim in ss.similar_tos():\n                for lemma in sim.lemma_names():\n                    yield (lemma, sim.name())\n#Get all related \ndef get_all_also_sees(word, pos=None):\n        for ss in wn.synsets(word):\n            for also in ss.also_sees():\n                for lemma in also.lemma_names():\n                    yield (lemma, also.name())\n\n#Get all synonyms - calls above functions\ndef get_all_synonyms(word, pos=None):\n    for x in get_all_synsets(word, pos):\n        yield (x[0])\n    for x in get_all_hyponyms(word, pos):\n        yield (x[0])\n    for x in get_all_similar_tos(word, pos):\n        yield (x[0])\n    for x in get_all_also_sees(word, pos):\n        yield (x[0])\n        \n#Get synonyms main\ndef get_synonyms_for(word):        \n    res = {word : []}\n    for x in get_all_synonyms(word):\n        res[word].append(x)\n    return res\n\ndef preprocess_text(document):\n    try:\n        # Remove all the special characters\n        document = re.sub(r'\\W', ' ', str(document))\n\n        # remove all single characters\n        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n\n        # Remove single characters from the start\n        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n\n        # Substituting multiple spaces with single space\n        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n\n        # Removing prefixed 'b'\n        document = re.sub(r'^b\\s+', '', document)\n\n        # Converting to Lowercase\n        document = document.lower()\n\n        # Lemmatization\n        tokens = document.split()\n        tokens = [wordnet_lemmatizer.lemmatize(word) for word in tokens]\n        tokens = [word for word in tokens if word not in en_stop]\n        tokens = [word for word in tokens if len(word) > 3]\n\n        preprocessed_text = ' '.join(tokens)\n\n        return preprocessed_text\n    \n    except:\n        return \"\"\n\ndef read_document_metadata(data_path):\n      #Get the publish time for the paapers\n    vector_dict = {}\n\n    all_files = glob.glob(data_path + '*.csv')\n    \n    metadata_csv = [f for f in all_files if 'metadata' in f]\n    \n    metadata = pd.read_csv(metadata_csv[0])\n\n    metadata = metadata[[\"sha\", \"publish_time\"]]\n    \n    metadata['publish_time'] = pd.to_datetime(metadata.publish_time)\n\n    metadata.rename(columns = {'sha':'paper_id'}, inplace = True) \n    \n    return metadata\n\ndef removeWords(sentence, words_remove):\n    sentence_token = sentence.split()\n    sentence_token = [x for x in sentence_token if 'http' not in x and len(x) <= 25 and x not in words_remove]\n    return(' '.join(sentence_token))\n\ndef show_wordcloud(data, title = None):\n    \"\"\"\n    show word cloud visualization\n    \"\"\"\n    wordcloud = WordCloud(\n        background_color='white',\n        max_words=1000,\n        max_font_size=40, \n        scale=5,\n        random_state=1\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(15,15))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=14)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Parsing raw data:\n\nThis section is to parse the json files and generate data frame with paper_id, title, abstract, text, etc.\n\n- Run generate_clean_df(all_files) function to parse all json files. If parsed and saved before, just skip this and load as clean_df below to save time\n\n- Based on parsed data, we also generated clean_df to save tokens for further analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# !ls '/kaggle/input/CORD-19-research-challenge'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_PATH = '/kaggle/input/CORD-19-research-challenge/'\nW2V_PATH = '/kaggle/input/ericsson-task2-pretrained-models-data/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get all files list\nall_files = glob.glob(DATA_PATH+'**/**/pdf_json/*.json')\nprint(len(all_files))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmerged_df,paper_dict = generate_clean_df(all_files,save_ram=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clean_df = merged_df.drop('text',axis=1).copy()\n# clean_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(paper_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_df = merged_df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nusecols = ['title','abstract']\nfor f in usecols:\n    print(f)\n    clean_df[f] = clean_df[f].astype(str)\n    clean_df[f] = clean_df[f].apply(lambda x:clean_text(x)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(clean_df.shape)\nclean_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.  Models\n\nThis section will show how to categorize risk factors based on different models\n\n- 4.1: Word2vec(w2c): we use multiple pretrained word2vec models to build recommender which shows the most relavant papers given input questions\n\n- 4.2: Topic modeling: we use LDA to train topic modeling and generate rank score for each paper based on risk factors\n\n- 4.3: Pesudo-labeling / adaptive training: we use w2c models in 4.1 to label some papers as risk factors and train a RNN to learn the classifications\n\n"},{"metadata":{},"cell_type":"markdown","source":"## 4.1 train word2vec\n\ntrain three different word2vev models with different sizes, we found it will be robust to capture similar words based on medical domain"},{"metadata":{"trusted":true},"cell_type":"code","source":"#concatenate all text\n# texts = pd.concat([clean_df['title'],clean_df['abstract']]).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n\n# #train w2v with dim=300, epoches should be >=50, if load pretrained ,skip this\n# covid19_w2v_300 = gensim.models.Word2Vec(size=300, window=5, min_count=5, sg=2, workers=4)\n# covid19_w2v_300.build_vocab(texts)\n# covid19_w2v_300.train(sentences=texts,total_examples=len(texts),epochs=100) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# # train w2v with dim=50,if load pretrained ,skip this\n# covid19_w2v_50 = gensim.models.Word2Vec(size=50, window=3, min_count=5, sg=2, workers=4)\n# covid19_w2v_50.build_vocab(texts)\n# covid19_w2v_50.train(sentences=texts,total_examples=len(texts),epochs=100) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# # train w2v with dim=30,if load pretrained ,skip this\n# covid19_w2v_30 = gensim.models.Word2Vec(size=30, window=3, min_count=5, sg=2, workers=4)\n# covid19_w2v_30.build_vocab(texts)\n# covid19_w2v_30.train(sentences=texts,total_examples=len(texts),epochs=100) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# working_directory = W2V_WORKING_DIRECTORY\n# if not os.path.isdir(working_directory):\n#     print(f'Creating directory... {working_directory}')\n#     os.makedirs(working_directory)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# with open(W2V_WORKING_DIRECTORY+'covid19_w2v_300.pickle','wb') as handle:\n#     pickle.dump(covid19_w2v_300,handle)\n# handle.close()\n\n# with open(W2V_WORKING_DIRECTORY+'covid19_w2v_30.pickle','wb') as handle:\n#     pickle.dump(covid19_w2v_30,handle)\n# handle.close()\n\n# with open(W2V_WORKING_DIRECTORY+'covid19_w2v_50.pickle','wb') as handle:\n#     pickle.dump(covid19_w2v_50,handle)\n# handle.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(W2V_PATH+'ericsson_covid19_w2v_300_2.pickle','rb') as handle:\n    covid19_w2v_300 = pickle.load(handle)\nhandle.close()\n\nwith open(W2V_PATH+'ericsson_covid19_w2v_30.pickle','rb') as handle:\n    covid19_w2v_30 = pickle.load(handle)\nhandle.close()\n\nwith open(W2V_PATH+'ericsson_covid19_w2v_50.pickle','rb') as handle:\n    covid19_w2v_50 = pickle.load(handle)\nhandle.close()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Factor_1 = asked_question\n\nprint('=========================================== QUESTION====================================================== \\n')\nprint('Risk Factor: ',Factor_1)\nids = get_answer(Factor_1,['title'],topn=10,showabs=True,print_info=True)\n#check detail text body in top recommended articles\nprint('=========================================== SUMMARY ========================================================  \\n')\nprint('SUMMARY for article: {}'.format(ids[0]))\ntxt = get_summerization(ids[0],sentence_count=5)\nprint('========================================= Visualization ===================================== \\n')\nwordcloud = WordCloud(background_color=\"white\", max_words=500000, contour_width=3, contour_color='steelblue')\n#Generate a word cloud\nwordcloud.generate(txt)\n#Visualize the word cloud\nwordcloud.to_image()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del clean_df # release some memory\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2. Topic modeling\n\nIn this section, we use LDA to perform topic modeling so that we can generate relavant score for each risk factor"},{"metadata":{},"cell_type":"markdown","source":"4.2.1 Configuration parameters to control the way models are built and training data is parsed and loaded\n\n*load_pretrained_models*  - This parameter if false will result in building the models from scratch, otherwise loads the pretrained models. If true  should locate and load the pretrained model for lda topic model and fast text word embeddings. Must have internet enabled, otherwise set *load_pretrained_models* to False (True by default)\n\n*fasttext_model_fname* =  If load_pretrained_models is set to true, the notebook expects the model is added using add data from the url and available in directory with file name \"/kaggle/input/covid19-lda-topics-wv-ft/ericsson_covid19_wv_ft.pickle\"\n\n*topic_model_fname* = If load_pretrained_models is set to true, the notebook expects the model is added using add data from the url and available in directory with file name \"/kaggle/input/covid19-lda-topics-wv-ft/ericsson_covid19_wv_ft_lda_topic.pickle\"\n\n*train_lda_multiple_steps* find an optimum lda topic model in multiple steps and decide the best model based on model coherence value."},{"metadata":{"trusted":true},"cell_type":"code","source":"load_pretrained_models = True\n\ntrain_lda_multiple_steps = False\n\nfasttext_wv_model_fname = W2V_PATH +\"ericsson_covid19_wv_ft.pickle\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**4.2.1 Data loading and preprocessing**\n\nRead and parse the json files and then preprocess and clean the data using lemmetizer and stemmer. Also  remove stopwords punctuation and other unwanted character. We use a custom helper function for cleaning and preprocessing the text."},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_df['abstract'].fillna('', inplace=True)\n    \nmerged_df['abstract_cleansed'] = merged_df['abstract'].apply(lambda x: cleaning_call(x))\n    \nmerged_df['abstract_cleansed'] = merged_df['abstract_cleansed'].apply(lambda x: lemma_main(True,x))\n    \nmerged_df['abstract_cleansed'].fillna(\"\", inplace=True)\n    \nword_to_remove_abstract = ['et al','et all','no info','report','describe','abstract','permission','word','objective','study','background','authorfunder','without','right','reserve','reuse','text','allow','count']\n\nword_to_remove_body = ['introduction','objective','preprint','peerreviewed','copyright','grant','biorxiv','doi','permissionthe','holder','httpsdoiorg10110120200110901801','license',\n    'medrxiv','license','perpetuityis','display','perpetuitythe','et al','et','al','info']\n\nwords_remove = list(set(word_to_remove_body + word_to_remove_abstract))\n\nmerged_df['abstract_cleansed'] = merged_df['abstract_cleansed'].apply(removeWords, words_remove = words_remove)\n\nmerged_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4.2.2 Plot the most common key words in the data set as a word cloud "},{"metadata":{"trusted":true},"cell_type":"code","source":"show_wordcloud(merged_df['abstract_cleansed'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4.2.3 Feed the preprocessed sentences to a tokenizer function and get text tokens using a word punctuation tokenizer.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_corpus = [preprocess_text(sentence) for sentence in merged_df['abstract_cleansed'] if sentence.strip() !='']\n\nword_punctuation_tokenizer = nltk.WordPunctTokenizer()\n\nword_tokenized_corpus = [word_punctuation_tokenizer.tokenize(sent) for sent in final_corpus]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" 4.2.4 Create the dictionary and bag of words corpus to feed to the topic model."},{"metadata":{"trusted":true},"cell_type":"code","source":"id2word = corpora.Dictionary(word_tokenized_corpus)\n\ncorpus = [id2word.doc2bow(text) for text in word_tokenized_corpus]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4.2.5 Build LDA model. The function below is to find an Optimum model by building more than one lda models, varying the number of topics each time and to choose the one with the highest choherence value. Check the model and its coherence values for different k from the plot below."},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n    coherence_values = []\n    model_list = []\n    for num_topics in range(start, limit, step):\n        model = gensim.models.ldamodel.LdaModel(corpus=corpus,  #corpus\n                                           id2word=id2word, #dictionary\n                                           num_topics=num_topics, #num of topics\n                                           random_state=2020, #\n                                           update_every=1, # how often the model paramters needs to be updated \n                                           chunksize=100,   # number of documents to be used in each training chunk\n                                           passes=10,   #total number of training passes\n                                           alpha='auto',  #parameter affecting the sparcity of the topics \n                                           per_word_topics=True)\n        model_list.append(model)\n        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n        coherence_values.append(coherencemodel.get_coherence())\n    return model_list, coherence_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if train_lda_multiple_steps == True:\n    model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=word_tokenized_corpus, start=2, limit=16, step=2)\n    optimal_model = model_list[1] # the model with highest coherence value - 4 is the optimal number of topics being discovered on a sufficently large data set\nelse:\n    optimal_model = gensim.models.ldamodel.LdaModel(corpus=corpus,  #corpus\n                                           id2word=id2word, #dictionary\n                                           num_topics=4, #num of topics\n                                           random_state=2020, #\n                                           update_every=1, # how often the model paramters needs to be updated \n                                           chunksize=100,   # number of documents to be used in each training chunk\n                                           passes=10,   #total number of training passes\n                                           alpha='auto',  #parameter affecting the sparcity of the topics \n                                           per_word_topics=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4.2.6 Plot the coherence values for the models built for different k"},{"metadata":{"trusted":true},"cell_type":"code","source":"if load_pretrained_models == False:\n    limit=16; start=2; step=2;\n    x = range(start, limit, step)\n    plt.plot(x, coherence_values)\n    plt.xlabel(\"Num Topics\")\n    plt.ylabel(\"Coherence score\")\n    plt.legend((\"coherence_values\"), loc='best')\n    plt.show()\n\n    # 3.4 Print Coherence Score (perplexity not printed)\n    coherence_model_lda = CoherenceModel(model=model_list[0], texts=word_tokenized_corpus, dictionary=id2word, coherence='c_v')\n    coherence_lda = coherence_model_lda.get_coherence()\n    print('\\nCoherence Score: ', coherence_lda)\n\n    for m, cv in zip(x, coherence_values):\n        print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4)) # high coherence is better, from the output choose the model with highest confidence or least k","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4.2.7 Get the topics for the optimum model."},{"metadata":{"trusted":true},"cell_type":"code","source":"model_topics = optimal_model.show_topics(formatted=False)\npprint(optimal_model.print_topics(num_words=10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4.2.8 Plot the topics. It might take a while for it to plot based on the size of the data.\n\nNotes:\n****\n1. Each bubble on the left-hand side plot represents a topic. The larger the bubble, the more prevalent is that topic.\n2. A good topic model will have fairly big, non-overlapping bubbles scattered throughout the chart instead of being clustered in one quadrant.\n3. A model with too many topics, will typically have many overlaps, small sized bubbles clustered in one region of the chart."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_topics(optimal_model, corpus, id2word):\n    pyLDAvis.enable_notebook()\n    vis = pyLDAvis.gensim.prepare(optimal_model, corpus, id2word)\n    pyLDAvis.save_html(vis, '/kaggle/working/lda.html')\n    return vis\n\nplot_topics(optimal_model, corpus, id2word)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4.2.9 Add a column to the data set to indicate the topic the document belongs to. The topic info to the data set is useful so that we can use it to test our models accuracy or build a analyser to identify the relevant papers on a particular topic.\nFor example, if the topic talks about virus risks, we can use that topic and the papers corressponding to that for getting the answer to  our query (like whats the potential risks associated to virus on neonates and pregnant women)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_topics_sentences(doc_ids, ldamodel=optimal_model, corpus=corpus, texts=merged_df):\n    \"\"\"Get the main topics for each document, Get the Dominant topic, Perc Contribution and Keywords for each document\n    Add the topics to the original text dataframe\n    \n    Parameters\n    ----------\n    doc_ids: Dataframe\n        The papaer_ids column and its values that is unique\n        \n    ldamodel: LDA\n        The optimum model - which has the highest coherence value , default is first model\n    \n    corpus: corpus\n        The Bag of words representation created from the dictionary of word tokens in the text document\n\n    texts: DataFrame\n        The text to fit the model on\n        \n    Returns\n    -------\n    sent_topics_df: DataFrame\n        The original dataframe text with topic(whichever is dominat) added to it, \n        with addtional columns like percentage of words in the text that belong to the text etc.\n    \"\"\"\n    \n    sent_topics_df = pd.DataFrame()\n\n\n    for i, row in enumerate(ldamodel[corpus]):\n        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n        \n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0: \n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    ids = pd.Series(doc_ids)\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([ids, sent_topics_df, contents], axis=1)\n    return(sent_topics_df)\n\n\ndf_topic_sents_keywords = format_topics_sentences(merged_df['paper_id'], ldamodel=optimal_model, corpus=corpus, texts=merged_df['abstract_cleansed'])\n\ndf_dominant_topic = df_topic_sents_keywords.reset_index()\n\ndf_dominant_topic.columns = ['Row_id','Paper_ids', 'DominantTopic', 'TopicPercContrib', 'Keywords','Text']\n\ndf_dominant_topic.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del model_topics #Do some cleaning\ndel id2word\ndel corpus\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.3 Feature selection using semantic similarity, synonyms and tfidf transform.\n\nIn this section we would create vector representations to find semantic similarities between the words and will couple it with word synonyms and tfidf transform to build a relevant feature extractor. \n\n* This involves vector representations - to find semantic similarities between the words,\n* A utility to find synonyms of features\n* Tf-idf transform and significant feature extractor based on tfidf score.\n* In the end we can use the result to find the cluster of topics(relevant paper ids) that belong to a particular query and validate and compare it against the lda topics in section 4.2"},{"metadata":{},"cell_type":"markdown","source":"4.3.1 Create model for word embeddings and use created vector representations for finding the semantic similarities between words in the text. \n\nWe use 50 iterations and will use the model to find the most similar words for a given word input. \n\nOther parameters for word model include embedding size corressponds to the dimesions of the vector(60), window size of 40 size of number of words occuring  before and after word, minimum frequency of a word in the corpus for which the word representation will be generated(5), and down sampling (1e-2) for most frequently occuring word."},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_size = 60    \nwindow_size = 40   \nmin_word = 5 \ndown_sampling = 1e-2 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#Load pretrained model if available - enable intenet if downloading from external. Otherwise build the model.\nif load_pretrained_models == True:\n    ft_model = pickle.load(open(fasttext_wv_model_fname, 'rb'))\nelse:\n    ft_model = FastText(word_tokenized_corpus,\n                        size=embedding_size,\n                        window=window_size,\n                        min_count=min_word,\n                        sample=down_sampling,\n                        sg=1,\n                        iter=50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4.3.2 Check the word representation for a particular word of interest(virus) in example, which is represented as a 60 dimensional vector."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(ft_model.wv['virus'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4.3.3 Check the similarity metrics(cosine similarity)  for two words that might be related, using the model. High value would indicate high similarity. So we will use this metric as a basis for identifying the most similar words for a given set of words."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(ft_model.wv.similarity(w1='virus', w2='infection'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4.3.4 Helper functions for building a dictionary of related words and functions that does relevant features lookup based on custom dictionary.\n\n* For a given text get all similar words from model and build a dictionary, also extend the dictionary by finding the synonyms for words.\n\n* Use tfidf to  transform text into a meaningful representation of numbers. Use the tfidf weights and then apply a filter based on the weight(frequency of words) to form a dominant subset of text (covid-19 papers) as search target. \n\n* Use the tfidf features and its score together with the custom dictionary (dictionary built using the fasttext model for most similar words, and  wordnet for synonyms) to extract the relevant papers from the covid data set (the tfidf relevant features are matched against the text in input query which is a given sentence or word input) "},{"metadata":{"trusted":true},"cell_type":"code","source":" def fit_idf_get_word_counts(count_vectorizer, fullDocPreprocessed):\n    \"\"\"Build the vocabulary of words from full text - columns as feature names and column values as the word counts (IDF)\n    Compute the IDF(inverse document frequency) using tfidf transform on the word count sparse matrix\n     \n    Parameters\n    ----------\n    count_vectorizer: CountVectorizer\n        The count_vectorizer created with min document frequency of 2, max 95%, and remove stop words, accepts word tokens. \n    \n    Returns\n    -------\n    tfidf_transformer: TfidfTransformer\n        The sparse matrix with IDF counts for each feature name in the vocabulary.\n    \"\"\"\n    sparse_matrix = count_vectorizer.fit_transform(fullDocPreprocessed)\n\n    tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n\n    tfidf_transformer.fit(sparse_matrix)\n    \n    return tfidf_transformer\n\n    \ndef preprocess_tokenize_text(query):\n    \"\"\" Preprocess a line of text and get the word tokens.\n    Parameters\n    ----------\n    query: String\n        The input query string text value. \n    \n    Returns\n    -------\n    word_tokens: List\n        List of word tokens\n    \"\"\"\n    preprocessed_sentences = [preprocess_text(sentence) for sentence in query if sentence.strip() !='']\n    word_punctuation_tokenizer = nltk.WordPunctTokenizer()\n    word_tokens = [word_punctuation_tokenizer.tokenize(sent) for sent in  preprocessed_sentences]\n\n    return word_tokens[0]\n\n\ndef sort_tfid_score(tfidf_vector):\n    \"\"\" Utility function to sort the values of the tfidf vector in descending order.\n    Parameters\n    ----------\n    tfidf_vector: Matrix\n        The tfdif transform of the document text, with feature names as columns and value as tfidf score for each row of docuemnt. \n    \n    Returns\n    -------\n    col_index_score: Tuple\n        column index, score tuple sorted desc\n    \"\"\"\n    col_index_score = zip(tfidf_vector.col, tfidf_vector.data)\n    return sorted(col_index_score, key=lambda x: (x[1], x[0]), reverse=True) \n\n\ndef extract_topn_from_vector(vocabulary, sorted_tfidf_scores,  all_related_features, top_n=1000):\n    \n    \"\"\"Lookup a particular feature name(key) or its synonyms in the vocabulary, and return feature nmae(key) and its tfidf-score as \n    This is a feature extractor to extract relevant feature and its score, \n    will iterate through each feature in tfidf vector and look up the topn feature name and its score,\n    and its score is reported back(tuple - word in input as key and scores and its features as value).\n     \n    Parameters\n    ----------\n    vocabulary: List\n        The list of feature names\n        \n    sorted_tfidf_scores: Tuple\n        List of feature names topn and its score\n        \n    all_related_features: List\n       all related feature for a given feature.\n       \n    topn:int\n        The number of features (topn) from vocabulary of features to consider for search.\n    \n    Returns\n    -------\n    Tuple\n        The tuple with the word/text in input token as key and list of features and its tfidf score summed.\n    \"\"\"\n   \n    sorted_tfidf_scores = sorted_tfidf_scores[:top_n] #Top n items (its important to tune this , so that we can have a decent representation of document)\n\n    scores = []\n    feature_names_found = []\n    \n    for index, score in sorted_tfidf_scores:\n        feature_name = vocabulary[index]\n       \n        for related_feature in all_related_features:\n            if feature_name == related_feature:\n                scores.append(score)\n                feature_names_found.append(feature_name)\n\n\n    results = {}\n    for index in range(len(feature_names_found)):\n        if results.get(feature_names_found[index]) != None:\n            results[feature_names_found[index]]= results.get(feature_names_found[index]) + scores[index] \n        else:\n            results[feature_names_found[index]]=scores[index]\n    \n    return results\n\n\ndef extract_features_from_tfidf_vector(document, look_for, tf_idf_vector, feature_names, topn = 1000):\n    \n    \"\"\"Process the tf-idf scores vector, get the top n feature names by ranking the feature level tf-idf scores.\n    Use the feature extractor to extract relevant feature and its score, \n    will iterate through each row of tfidf vector and look up the topn feature name and its score,\n    and its score is reported back(tuple - word in input as key and scores and its features as value).\n     \n    Parameters\n    ----------\n    documents: DataFrame\n        The input text for training - which has the full vocabulary of feature names\n        \n    look_for: List\n        List of feature names to look for in vector\n        \n    tf_idf_vector: Matrix\n       tfidf transformation of document text\n       \n    topn:int\n        The number of features (topn) from vocabulary of features to consider for search.\n    \n    Returns\n    -------\n    Tuple\n        The tuple with the word/text in input token as key and list of features and its tfidf score summed.\n    \"\"\"\n    \n    scores_combined = []\n\n    matching_scores = []\n\n    results = []\n\n    for i in range(tf_idf_vector.shape[0]):\n    \n        tfidf_vecto_for_doc = tf_idf_vector[i]\n    \n        sorted_items=sort_tfid_score(tfidf_vecto_for_doc.tocoo())\n\n        keywords=extract_topn_from_vector(feature_names,sorted_items,look_for, topn)\n    \n        results.append(sum(keywords.values()))\n        \n    return results\n\ndef get_scores_for_word_group(documents, \n                              words_input,\n                             tf_idf_vector,\n                              feature_names,\n                             topn = 1000):  #extract_relevant_text_on_keywords\n    \n    \"\"\"Use the feature extractor to extract relevant feature and its score, \n    will iterate thtough each word_input and its related words, its the related words thats matched with the tfidf vecotr feature name,\n    and its score is reported back(tuple - word in input as key and scores and its features as value).\n     \n    Parameters\n    ----------\n    documents: DataFrame\n        The input text for training - which has the full vocabulary of feature names\n        \n    words_input: tuple\n        word as key and related words as value\n        \n    feature_names: List\n        List of  features in the vocabulary\n    topn:int\n        The number of features (topn) from vocabulary of features to consider for search.\n    \n    Returns\n    -------\n    results: Tuple\n        The tuple with the word/text in input token as key and list of features and its tfidf score\n    \"\"\"\n    \n    results = {}\n    \n    for word_group, word_list_to_search in  words_input.items():\n        temp_list = word_list_to_search\n        search_for_features = list(set(temp_list + [word_group] ))\n        sum_scores_matching_words = extract_features_from_tfidf_vector(documents, \n                                                           search_for_features, \n                                                           tf_idf_vector, \n                                                           feature_names,\n                                                           topn)\n        \n        results[word_group] = sum_scores_matching_words\n        \n    return pd.DataFrame(results)\n\ndef get_most_similar_keywords_from_model_for_sentence(sentence, topn_similar_words = 5, cosine_similarity = 0.6):\n    \"\"\"Get most similar words (only the topn_similar_words) for the sentence input that has similarity score\n    above(cosine_similarity).\n     \n    Parameters\n    ----------\n    sentence: List\n        The list of tokens- or feature names or sentence text for which to find the most similar words \n        \n    topn_similar_words: int\n        Get the top n similar words out of all the similar words\n        \n    cosine_similarity: float\n        Get the similar words which has a score above\n    \n    Returns\n    -------\n    Tuple\n        The tuple with the word/text in input token as key and list of related words as the value\n    \"\"\"\n    result = {}\n    result[sentence] = [item[0] for item in ft_model.wv.most_similar(sentence)[:topn_similar_words] \n                        if item[1] > cosine_similarity] \n    return result\n\ndef get_most_similar_keywords_from_model_and_wordnet_for_sentence(sentence, topn_similar_words = 5, cosine_similarity = 0.6):\n    \"\"\"Get most similar words (only the topn_similar_words) for the sentence input that has similarity score\n    above(cosine_similarity). Also append the synonyms for the given word to the similar words list.\n     \n    Parameters\n    ----------\n    sentence: List\n        The list of tokens- or feature names or sentence text for which to find the most similar words \n        \n    topn_similar_words: int\n        Get the top n similar words out of all the similar words\n        \n    cosine_similarity: float\n        Get the similar words which has a score above\n    \n    Returns\n    -------\n    Tuple\n        The tuple with the word/text in input token as key and list of related words as the value\n    \"\"\"\n    result = {}\n    result[sentence] = [item[0] for item in ft_model.wv.most_similar(sentence)[:topn_similar_words] \n                        if item[1] > cosine_similarity] \n    for word in preprocess_tokenize_text(sentence):\n        synonyms_set = set(get_synonyms_for(word).get(word)) # Enrich the semantic similar word list further - Combine the fast text model results(semantically similar) with wordnet synonyms and generate a full list of related words    \n    \n        related_words = set(result.get(word))\n    \n        result[word] = list(synonyms_set.union(related_words))  \n        \n    return result\n\ndef get_most_similar_keywords_from_model(word_tokens, topn_similar_words = 5, cosine_similarity = 0.6):\n    \n    \"\"\"Get most similar words (only the topn_similar_words) for the words in the word_tokens which has similarity score\n    above(cosine_similarity).\n     \n    Parameters\n    ----------\n    word_tokens: List\n        The list of tokens- or feature names or words for which to find the most similar words \n        \n    topn_similar_words: int\n        Get the top n similar words out of all the similar words\n        \n    cosine_similarity: float\n        Get the similar words which has a score above\n    \n    Returns\n    -------\n    Tuple\n        The tuple with the word/text in input token as key and list of related words as the value\n    \"\"\"\n    \n    semantically_similar_words = {words: [item[0] \n                                          for item in ft_model.wv.most_similar(words)[:topn_similar_words] \n                                          if item[1] > cosine_similarity] #Uses the fast text model\n                                          for words in word_tokens}\n    return semantically_similar_words\n\ndef get_most_similar_keywords_from_model_and_wordnet(input_tokens, topn_similar_words=5, cosine_similarity = 0.6):\n    \"\"\"Get most similar words (only the topn_similar_words) for the words in the input_tokens which has similarity score\n    above(cosine_similarity). Also append the synonyms for the given word to the similar words list.\n     \n    Parameters\n    ----------\n    input_tokens: List\n        The list of tokens- or feature names or words for which to find the most similar words \n        \n    topn_similar_words: int\n        Get the top n similar words out of all the similar words\n        \n    cosine_similarity: float\n        Get the similar words which has a score above\n    \n    Returns\n    -------\n    Tuple\n        The tuple with the word/text in input token as key and list of related words as the value\n    \"\"\"\n    semantically_similar_words = get_most_similar_keywords_from_model(input_tokens, topn_similar_words, cosine_similarity) \n\n    for word in input_tokens:\n        synonyms_set = set(get_synonyms_for(word).get(word)) # Enrich the semantic similar word list further - Combine the fast text model results(semantically similar) with wordnet synonyms and generate a full list of related words    \n    \n        related_words = set(semantically_similar_words.get(word))\n    \n        semantically_similar_words[word] = list(synonyms_set.union(related_words))  \n        \n    return semantically_similar_words\n\ndef filter_documents_by_score(intermediate_results_df, scores_above = 0):\n    \"\"\"Function to filter scores and get the subset with scores above the threshold.\n     \n    Parameters\n    ----------\n    intermediate_results_df: Dataframe\n        The processed dataframe that has the relevant papers got from the analyser and score per feature\n        \n    scores_above: int\n        score per feature threshold default 0\n    \n    Returns\n    -------\n    DataFrame\n        The dataframe with rows having Score greater than threshold\n    \"\"\"\n    \n    all_numeric_cols_above_threshold = lambda col_name:  col_name > scores_above if np.isreal(col_name) else True \n\n    intermediate_results_df = intermediate_results_df[intermediate_results_df.applymap(all_numeric_cols_above_threshold).all(1)]\n    \n    return intermediate_results_df\n\ndef get_combined_word_scores_sorted(intermediate_results_df):\n    \"\"\"Function to sum up individual scores of individual score and find total score per document.\n     \n    Parameters\n    ----------\n    intermediate_results_df: Dataframe\n        The processed dataframe that has the relevant papers got from the analyser and score per feature\n    \n    Returns\n    -------\n    DataFrame\n        The dataframe with a new column Score, which has the score per document feature summed.\n    \"\"\"\n\n    intermediate_results_df['Score'] = intermediate_results_df[intermediate_results_df.select_dtypes(include=np.number).columns].sum(axis=1)\n\n    intermediate_results_df = intermediate_results_df.sort_values(by = ['Score'],  ascending=False, inplace = False)\n    \n    return intermediate_results_df\n\ndef join_docs_with_scores(merged_df, intermediate_results_df):\n    \"\"\"Function to merge two data frames - based on index match of the right argument.\n    Gets all the documents(papaers and its paper_id) from the original dataframe - that is contained in the intermediate_results_df\n\n    Parameters\n    ----------\n    merged_df : DataFrame\n        The input document text dataframe with paper_id and all relevenat columns thats the training text\n    intermediate_results_df: Dataframe\n        The processed dataframe that has the relevant papers got from the analyser\n    \n    Returns\n    -------\n    DataFrame\n        The relevant papers from original dataframe(left) which is contained in the data frame(on right), merged on papaer_id\n        \n    \"\"\"\n    \n    doc_subset = merged_df.iloc[intermediate_results_df.index][['paper_id','title', 'abstract']] #Choose columns to show to end user\n        \n    return pd.concat([doc_subset.reset_index(drop=True), intermediate_results_df.reset_index(drop=True)], axis=1)\n\n\ndef get_documents_sorted_by_relevance(queries, document, tf_idf_vector, feature_names):\n    \"\"\"Get the documents which has a non zero tfidf score for its features, \n    and features to look for is extracted from  the input query.\n    Input query is parsed and the features to search in documents is \n    its semantically similar features and the feature synonymns.\n    The tfidf score for matching features are then added and returned as total score for the input query.\n\n    Parameters\n    ----------\n    queries : List\n        The input search query list- parsed one by one if list contain multiple queries\n    document: Dataframe\n        The Preprocessed text with all relevant columns\n    tf_idf_vector: Matrix\n        The tfidf transformation of the document text, the vocabulary of features is limited to 10000 default(configurable) \n    feature_names: List\n        The list of feature vocabulary in the text\n    \n    Returns\n    -------\n    DataFrame\n        The relevant papers ordered by score, the papaer_id, text, score and publish time\n        \n    \"\"\"\n    \n    results = {} \n    \n    all_words_dict = {}\n\n    meta_data = read_document_metadata(DATA_PATH)\n    \n    for query in queries:\n        \n        word_tokens = preprocess_tokenize_text([query])\n\n     #   all_related_keywords_dict = get_most_similar_keywords_from_model_for_sentence(query, topn_similar_words = 20, \n      #                                                                              cosine_similarity = 0.5)\n        \n        #all_related_keywords_dict = get_most_similar_keywords_from_model_and_wordnet_for_sentence(query, topn_similar_words = 20, \n         #                                                                           cosine_similarity = 0.5)\n        \n        all_related_keywords_dict = get_most_similar_keywords_from_model_and_wordnet(word_tokens, topn_similar_words = 20, \n                                                                                   cosine_similarity = 0.6)\n        \n        #all_related_keywords_dict = get_most_similar_keywords_from_model(word_tokens, topn_similar_words = 20, \n                                                                             #    cosine_similarity = 0.6)\n \n        scores_for_each_word_group_df = get_scores_for_word_group(document, \n                                                                  all_related_keywords_dict,\n                                                                 tf_idf_vector,\n                                                                 feature_names,\n                                                                 topn = 10000)\n\n        scores_for_each_word_group_df = filter_documents_by_score(scores_for_each_word_group_df, scores_above = 0)\n\n        final_result = get_combined_word_scores_sorted(scores_for_each_word_group_df)\n        \n        final_result = join_docs_with_scores(merged_df, final_result)\n\n        final_result =  final_result.merge(meta_data, how='inner', left_on='paper_id', right_on='paper_id')\n        \n        results[query] = final_result\n        \n        all_words_dict[query] = all_related_keywords_dict\n            \n    return results, all_words_dict\n\n\ndef display_results(relevant_papers, year_filter = 2016):\n    \"\"\"Prints the relevant papers result\n\n    Parameters\n    ----------\n    relevant_papers : DataFrame\n        The relevant papers retrived for search result and order by scores and publish time\n    \n    year_filter : int\n        The filter for publish time - get papers with publish time greater than or equal to the year\n        \n    Returns\n    -------\n    None\n        \n    \"\"\"\n        \n    for query_input, relevant_paper in relevant_papers.items(): \n        #relevant_paper['publish_time'] = relevant_paper[relevant_paper.publish_time.dt.year > year_filter]\n        relevant_paper = relevant_paper[pd.to_datetime(relevant_paper.publish_time).dt.year >= year_filter]\n        print(f\"Retrieved {relevant_paper.shape[0]} papers related to - \\\"{query_input}\\\"\")\n        display(relevant_paper[['paper_id', 'title', 'abstract', 'publish_time', 'Score']].\n                 sort_values(by=['Score'], ascending = False))\n    \n    return None","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4.3.5 Get the list of questions to get the relevant papers from the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"queries = questions_to_ask","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4.3.6 Use tfidf to transform text into a meaningful representation of numbers. Use the tfidf weights and then apply a filter based on the weight(frequency of words) to form a dominant subset of text results(covid-19 papers) for the input query."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ncount_vectorizer = CountVectorizer(max_df=0.95, min_df = 2, stop_words='english') #adjust min_df, max_df accordingly\n\n#Build the vocabulary  database of words from full text(learn) - represent as DTM word counts (IDF)\ntfidf_transformer = fit_idf_get_word_counts(count_vectorizer, merged_df['abstract_cleansed'])\n\n#Compute tfidf weights for a  document subset(test). from which we would extract the relevant features \ntf_idf_vector = tfidf_transformer.transform(count_vectorizer.transform(merged_df['abstract_cleansed']))\n\n#Use the  tfidf vector to extract relevant papaers sort it by tfidf score\nrelevant_papers, all_words_dict = get_documents_sorted_by_relevance(queries, \n                                                     merged_df['abstract_cleansed'], \n                                                     tf_idf_vector, \n                                                     count_vectorizer.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4.3.6 Display the results - the relevant papers for the query - can filter the result and only display the latest papers. Shows all papers that are published on or before year 2018. Change the year_filter accordingly to get the required list of papers."},{"metadata":{"trusted":true},"cell_type":"code","source":"display_results(relevant_papers, year_filter = 2018)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4.3.7 Highlight the key features that match in the relevant papers retrieved from the analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a DF with the top 5 relevant papers\n\nwords_to_color_dict = {}\n\nfor query_input, relevant_paper in relevant_papers.items():\n    # Create a list of the keywords\n    words_to_color_list = []\n    for words in all_words_dict.get(query_input).values():\n            for word in words:\n                words_to_color_list.append(word)\n    #[[words_to_color_list.add(word) for word in words] for words in all_words_dict.get(query_input).values()]\n    \n    words_to_color_dict[query_input] = words_to_color_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the recommended papers.\nfor query_input, words_to_color in words_to_color_dict.items():\n    display(Markdown('# <span style=\"color:#1174E6;font-weight:bold\">Result</span>'))\n\n    articles_topn_df = relevant_papers.get(query_input).head(5)\n    \n    if len(articles_topn_df) == 0:\n        display(Markdown('No relevant papers found on <span style=\"background-color:#FADA5E;font-weight:bold\">' + query_input + '</span>.'))\n    else:\n        if len(articles_topn_df) == 1:\n            display(Markdown('To learn more about <span style=\"background-color:#FADA5E;font-weight:bold\">' + query_input + '</span> we recommend exploring the following paper.'))\n        else:\n            display(Markdown('To learn more about <span style=\"background-color:#FADA5E;font-weight:bold\">' + query_input + '</span> we recommend exploring the following ' + str(len(articles_topn_df)) + ' papers.'))\n\n        for index, row in articles_topn_df.iterrows():\n            print(row['title'])\n\n        for index, row in articles_topn_df.iterrows():\n            display(Markdown('### <span style=\"color:#1174E6;font-weight:bold\"> Paper ' + str(index+1) + ': ' + row['title'] + '</span>'))\n            print(row['abstract'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del relevant_papers # release some memory\ndel articles_topn_df\ndel words_to_color_dict\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.4. Classification of questions based on RNN\n\nThis section is to use pretrained word2vec models done in section 4 to generate tags(classes) for supervised learning based \non RNN. This section has to be run separately because the processing schedule is different. After completing this section, we can save NN model as model.h5\n\nThe general pipeline is:\n\n- Use pretrained w2v to find some relevant key words for basic risk factors. Then we create distionary for each classes\n\n- Label each article with its abstract and conclusion based on the above dictionary\n\n- Build a RNN (LSTM,GRU) to learn the classifications using tensorflow.keras APIs\n\n\n\n`NOTE: this section should be completed individually and save the model.h5`\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define all constants \n# data path\nCL_DATA_NAME = \"CORD-19\"\nCL_TRAINING_DATA_RATIO = 0.8 # Training data ratio\nCL_VALIDATION_DATA_RATIO = 1.0 - CL_TRAINING_DATA_RATIO\n\n# Preprocessing\nCL_SEQUENCE_LENGTH = 256  # Feature fixed sequence length\nCL_SHUFFLE_BUFFER_SIZE = 1024 # Dataset shuffling buffer size\nCL_NUM_PARALLEL_CALLS = 4 # number of CPUs to use for training\n\n# Training\nCL_MODEL_ARCHITECTURE_NAME = \"cls\" # \nCL_MAX_VOCABULARY_SIZE = 500000 # Maximum vocabulary size\nCL_EMBEDDING_DIM = 256 # Embedding dimension\nCL_ACTIVATION_LAYER_UNITS = 128\nCL_BATCH_SIZE = 128 # Bach size\nCL_EPOCHS = 30 # Number of epochs\nCL_NUM_WORKERS = 4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### 4.4.1 Create a dictionary for 8 classes\n\n- Create 8 classes (8 risk factors) labels based on key words mapping using word2vec pretrained in section 4\n\n\n- Save the dictionary for future processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords_classes = OrderedDict()\n\nkeywords_classes['lung disease'] = ['lung','pulmonary','smoke','vascular','smoking','nicotine','asthma'\\\n                            'airflow','cigar','corona','covid','coronavirus','2019ncov']\n\nkeywords_classes['co-infection and co-morbidity'] = ['coinfection','coinfections','coinfected','coinfecting',\\\n                'codetection','copathogen','codetections','comorbidity','comorbidities',\n                'corona','covid','coronavirus','2019ncov']\n\nkeywords_classes['neonate and pregnant women'] = ['woman', 'women','pregnant', 'neonate','newborn',\\\n                             'infant','gestation','neonatal','corona','covid','coronavirus','2019ncov']\n\nkeywords_classes['socioeconomic'] = ['socioeconomic', 'economic', 'social', 'socio','society','demographic',\n    'behavioral', 'finance', 'psychosocial', 'behavioural','corona','covid','coronavirus','2019ncov'\n]\n\nkeywords_classes['transmission dynamics'] = [\n    'transmission', 'transmissible', 'crosstransmission','humantohuman','incubation','persontoperson',\\\n    'geographic', 'geographical','temporal', 'spatial','spatio','prevalence',\\\n    'reproductive','reproduce','asymptomatic','seasonality','airborne','aerosol','corona','covid','coronavirus',\\\n    '2019ncov','droplets','secretions','latent','\\br0\\b'\n]\n\nkeywords_classes['severity of disease'] = ['risk', 'risks','fatality','motality','symptomatic','older','aged','senior',\n                                           'morbidity','death','corona','covid','coronavirus','2019ncov'\n                                           ]\n\n\nkeywords_classes['susceptibility of populations'] = ['susceptibility', 'population','sensitivity','sensitive','susceptible'\n                                                     'corona','covid','coronavirus','2019ncov']\n\n\nkeywords_classes['public control'] = [\n                'public', 'health','control', 'mitigation','mitigating','planning','prevention','publichealth',\\\n                'preparedness','intervention','mitigating','corona','covid','coronavirus','2019ncov'\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4.2 Generate label dictionaries"},{"metadata":{"trusted":true},"cell_type":"code","source":"label_int_to_str = OrderedDict()\nfor i, label in enumerate(keywords_classes.keys()):\n    label_int_to_str[i] = label\n\nlabel_str_to_int = OrderedDict()\nfor i, label in enumerate(keywords_classes.keys()):\n    label_str_to_int[label] = i","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(label_int_to_str,label_str_to_int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_classes = len(label_str_to_int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4.3 Parsing the dataset\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"for dirname, _, _ in os.walk(CL_INPUT_DIRECTORY):\n    print(dirname)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"publication_directory = os.path.join(CL_INPUT_DIRECTORY,'CORD-19-research-challenge')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"publication_names = [\n    'custom_license',\n    'biorxiv_medrxiv',\n    'comm_use_subset',\n    'noncomm_use_subset'\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### 4.4.4 Create publication class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class PublicationModel(object):\n    \"\"\"Publication class to store and process publication information.\"\"\"\n    \n    def __init__(self, name: str, directory: str):\n        \"\"\"Initialize publication object.\n        \n        Arguments:\n            name {str} -- the name of the publication.\n            directory {str} -- the directory of the publication.\n        \"\"\"\n        self.name = name\n        self.directory = directory\n        self.article_filenames: List[str] = []\n        self.articles: ArticleModel = []\n            \n    def collect_articles(self):\n        \"\"\"Collect all article filenames in publication directory.\"\"\"\n        self.article_filenames = os.listdir(os.path.join(self.directory,\n                                                         self.name, self.name,\n                                                         'pdf_json'))\n\n    def __repr__(self):\n        \"\"\"Return publication short information.\"\"\"\n        return (f'{self.name} with {len(self.article_filenames)} articles.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loop and find all articles\npublications: List[PublicationModel] = []\nfor publication_name in publication_names:\n    publication = PublicationModel(publication_name, publication_directory)\n    publication.collect_articles()\n    print(publication.__repr__())\n    publications.append(publication)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_articles = 0\nfor publication in publications:\n    total_articles += len(publication.article_filenames)\n\nprint('total articles:', total_articles)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4.5 Create Article model \n\n- ArticleModel class: to get all information for each paper: paper_id, title,abstract, conclusion if apply, normalized key words frequence and label assigned"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ArticleModel(object):\n    \"\"\"Article class to store the entire article information.\"\"\"\n    \n    def __init__(self, paper_id: str, title: str, abstract: str, conclusion: str):\n        \"\"\"Initialize article object.\n        \n        Arguments:\n            paper_id {str} -- 40-character sha1 of the PDF.\n            title {str} -- the title of the article.\n            abstract {str} -- the text abstract of the article.\n            conclusion {str} -- the text conclusion of the article.\n        \"\"\"\n        self.paper_id = paper_id\n        self.title = title\n        self.abstract = abstract\n        self.conclusion = conclusion\n        self.normalized_keywords_frequency: Dict[str, float] = OrderedDict()\n        self.label_nfreq: (str, float) = (None, 0.0)\n            \n        \n    def get_normalized_keywords_frequency(self, keywords_classes: Dict[str, List[str]]) -> Dict[str, float]:\n        \"\"\"Get normalized keywords frequency.\n        \n        Arguments:\n            keywords_classes {Dict[str, List[str]]} -- keywords classes.\n        Returns:\n            Dict[str, float] -- pair of class and the normalized frequency.\n        \"\"\"\n        for class_name, keywords in keywords_classes.items():\n            frequency = 0\n            for keyword in keywords:\n                if keyword in self.abstract or keyword in self.conclusion:\n                    frequency += 1\n\n            normalized_frequency = float(frequency) / len(keywords)\n            self.normalized_keywords_frequency[class_name] = normalized_frequency\n            \n        return self.normalized_keywords_frequency\n    \n    \n    def assign_label(\n        self, keywords_classes: Dict[str, List[str]]) -> (str, float):\n        \"\"\"Assign label from keywords classes.\n        \n        Arguments:\n            keywords_classes {Dict[str, List[str]]} -- keywords classes.\n        Returns:\n            str -- class label and the normalized frequency.\n        \"\"\"\n        normalized_keywords_frequency = \\\n            self.get_normalized_keywords_frequency(keywords_classes)\n        # get the label based on the highest frequency\n        label, nfreq = sorted(normalized_keywords_frequency.items(),\n                              key=lambda x: x[1],\n                              reverse=True)[0]\n        if nfreq:\n            self.label_nfreq = (label, nfreq)\n        else:\n            self.label_nfreq = (None, 0.0)\n        \n        return self.label_nfreq\n        \n\n    def __repr__(self):\n        \"\"\"Return article in readable string.\"\"\"\n        return (f'{self.paper_id}\\n{self.title}\\n\\n' +\n                f'Abstract:\\n{self.abstract}\\n\\n' +\n                f'Conclusion:\\n{self.conclusion}\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4.6 Article Ingestor \n\n- This class is to parse json for each article and save as instance"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ArticleIngestorInterface(object):\n    \"\"\"Abstract class for ingestor classes.\"\"\"\n\n    allowed_extensions: List[str] = []\n\n    @classmethod\n    def can_ingest(cls, path: str) -> bool:\n        \"\"\"Check if file extension is supported for ingestion.\n\n        Arguments:\n            path {str} -- article file location.\n        Returns:\n            bool -- True if the extension is supported and False otherwise.\n        \"\"\"\n        ext = path.split('.')[-1].lower()\n        return ext in cls.allowed_extensions\n\n    @classmethod\n    @abstractmethod\n    def parse(cls, path: str) -> ArticleModel:\n        \"\"\"Parse article from file and store them in ArticleModel object.\n\n        Arguments:\n            path {str} -- article file location.\n        Returns:\n            ArticleModel -- article object.\n        \"\"\"\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class JsonArticleIngestor(ArticleIngestorInterface):\n    \"\"\"Object that ingests JSON file into article model.\"\"\"\n\n    allowed_extensions: List[str] = ['json']\n\n    @classmethod\n    def parse(cls, path: str) -> ArticleModel:\n        \"\"\"Parse article from file and store them in ArticleModel object.\n        \n        Arguments:\n            path {str} -- article file location.\n        Returns:\n            ArticleModel -- article object.\n        \"\"\"\n        if not cls.can_ingest(path):\n            raise Exception(f'Cannot ingest exception for {path}')\n\n        with open(path, 'r', encoding='utf-8-sig') as fh:\n            article_json = json.load(fh)\n            title = article_json.get('metadata', {}).get('title', '')\n            abstract = ' '.join([\n                p.get('text', '') \n                for p in article_json.get('abstract', '')])\n            conclusion = ' '.join([\n                p.get('text', '')\n                for p in article_json.get('body_text', '')\n                if 'conclusion' in p.get('section', '').lower()])\n            article = ArticleModel(paper_id=article_json.get('paper_id', ''),\n                                   title=title.strip(),\n                                   abstract=abstract.strip(),\n                                   conclusion=conclusion.strip())\n\n        return article","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check one sample json \nsample_json_article_path = os.path.join(\n    publication_directory,\n    'custom_license',\n    'custom_license',\n    'pdf_json',\n    '0a52a3d2793f8ca8a4d6f6630e986ea1da115f80.json')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_article = JsonArticleIngestor.parse(sample_json_article_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check a sample article to show its content: paper_id, title, abstract and conclusion (if applies)\nsample_article","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check normalized keywords, it will show how much weight this sample article has for each category we created \nsample_article.get_normalized_keywords_frequency(keywords_classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#chek assigned label with score\nsample_article.assign_label(keywords_classes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4.7 Ingest articles"},{"metadata":{"trusted":true},"cell_type":"code","source":"def ingest_articles(publications: List[PublicationModel],keywords_classes: Dict[str, List[str]]) -> int:\n    \"\"\"Ingest articles.\n\n    Arguments:\n        publications {List[PublicationModel]} -- list of publications.\n        keywords_classes {Dict[str, List[str]]} -- keywords classes.\n    Returns:\n        int -- total articles.\n    \"\"\"\n    total_articles: int = 0\n    for publication in publications:\n        publication.articles = []\n        article_filenames = publication.article_filenames\n        for article_filename in article_filenames:\n            # Get and parse article\n            article_path = os.path.join(publication.directory,\n                                        publication.name, publication.name,\n                                        'pdf_json',\n                                        article_filename)\n            article = JsonArticleIngestor.parse(article_path)\n            article.assign_label(keywords_classes)\n            publication.articles.append(article)\n            total_articles += 1\n            \n    return total_articles","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntotal_articles = ingest_articles(publications, keywords_classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_articles","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4.8 Calculate class Frequency\n\n- to calculate the frequencies of each assigned class"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_classes_frequency(\n    publications: List[PublicationModel],\n    keywords_classes: Dict[str, List[str]]) -> (int, Dict[str, int]):\n    \"\"\"Get classes frequency.\n\n    Arguments:\n        publications {List[PublicationModel]} -- list of publications.\n        keywords_classes {Dict[str, List[str]]} -- keywords classes.\n    Returns:\n        Dict[str, int] -- class and the frequency pairs.\n    \"\"\"\n    classes_frequency: Dict[str, int] = OrderedDict()\n    for class_name, _ in keywords_classes.items():\n        classes_frequency[class_name] = 0\n    \n    for publication in publications:\n        for article in publication.articles:\n            # Get class label\n            label = article.label_nfreq[0]\n            if label:\n                classes_frequency[label] += 1\n            \n    return classes_frequency","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nclasses_frequency = get_classes_frequency(publications, keywords_classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes_frequency","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_labeled_articles = sum([freq for _, freq in classes_frequency.items()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_labeled_articles","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4.9 Build data set using data generator API in TensorFlow 2.0 "},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data_ratio = CL_TRAINING_DATA_RATIO #use 80% of data as training\nprint(training_data_ratio)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_dataset(publications: List[PublicationModel],keywords_classes: Dict[str, List[str]]):\n    \"\"\"\n    Build dataset with text and label pairs.\n\n    Arguments:\n        publications {List[PublicationModel]} -- list of publications.\n        keywords_classes {Dict[str, List[str]]} -- keywords classes.\n    Returns:\n        (tf.data.Dataset, tf.data.Dataset) -- labeled and unlabeled dataset.\n    \"\"\"\n    training_texts: List[str] = []\n    training_articles: List[ArticleModel] = []\n    validation_texts: List[str] = []\n    validation_articles: List[ArticleModel] = []\n    testing_texts: List[str] = []\n    testing_articles: List[ArticleModel] = []\n\n    for publication in publications:\n        for article in publication.articles:\n            # Combine abstract and conclusion into text\n            text = []\n            if article.abstract:\n                text.append(article.abstract)\n            if article.conclusion:\n                text.append(article.conclusion)\n            if not text:\n                continue\n            text = ' '.join(text)\n            # Get class label\n            label = article.label_nfreq[0]\n            if label in keywords_classes:\n                if np.random.random() <= training_data_ratio:\n                    # Training\n                    training_texts.append((text, label))\n                    training_articles.append(article)\n                else:\n                    # Validation\n                    validation_texts.append((text, label))\n                    validation_articles.append(article)\n            else:\n                # Testing\n                testing_texts.append(text)\n                testing_articles.append(article)\n\n    training_dataset = tf.data.Dataset.from_generator(\n        lambda: training_texts,\n        (tf.string, tf.string),\n        (tf.TensorShape([]), tf.TensorShape([])))\n    validation_dataset = tf.data.Dataset.from_generator(\n        lambda: validation_texts,\n        (tf.string, tf.string),\n        (tf.TensorShape([]), tf.TensorShape([])))\n    testing_dataset = tf.data.Dataset.from_generator(\n        lambda: testing_texts,\n        (tf.string),\n        (tf.TensorShape([])))\n\n    return (training_dataset, training_articles,\n            validation_dataset, validation_articles,\n            testing_dataset, testing_articles)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n(training_dataset, training_articles,validation_dataset, validation_articles,testing_dataset, testing_articles)\\\n    = build_dataset(publications,label_str_to_int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if it workds, takes 2 items from train_dataset\nfor i,(text, label) in enumerate(training_dataset.take(2)):\n#     if label=='woman':\n        print(i,'\\n')\n        print(f'text:\\n{text}\\n')\n        print(f'label:\\n{label}')\n        print('='*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(training_articles)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4.10 Data pipeline \n\n- Tokenize the data\n\n- get sequence for sentences\n\n- pad sequences for sentences"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize(words: str) -> List[str]:\n    \"\"\"Tokenize text of words into a list of individual words.\n\n    Arguments:\n        words {srt} -- tensor of words.\n    Returns:\n        List(str) -- list of words.\n    \"\"\"\n    return text_to_word_sequence(words)\n\ndef get_sequence_lengths(dataset) -> List[int]:\n    \"\"\"Get sequence length of all texts.\n\n    Arguments:\n        dataset {tf.data.Dataset} -- texts dataset.\n    Returns:\n        List(int) -- sequence lengths.\n    \"\"\"\n    sequence_lengths: List[int] = []\n    for text in dataset:\n        if type(text) == tuple:\n            # Remove label\n            text, _ = text\n\n        if type(text) is not str:\n            # Convert tensor to string\n            text = text.numpy().decode('utf-8')\n        sequence_lengths.append(len(tokenize(text)))\n        \n    return sequence_lengths","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for training set\ntic = time.time()\ntraining_sequence_lengths = get_sequence_lengths(training_dataset)\ntoc = time.time()\nprint(f'Runtime: {int(toc-tic)} seconds')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for validation set\ntic = time.time()\nvalidation_sequence_lengths = get_sequence_lengths(validation_dataset)\ntoc = time.time()\nprint(f'Runtime: {int(toc-tic)} seconds')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for test set\ntic = time.time()\ntesting_sequence_lengths = get_sequence_lengths(testing_dataset)\ntoc = time.time()\nprint(f'Runtime: {int(toc-tic)} seconds')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4.11 word count \n\n- build and save word to index dictionary under working space"},{"metadata":{"trusted":true},"cell_type":"code","source":"working_directory = CL_WORKING_DIRECTORY\nprint(working_directory)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not os.path.isdir(working_directory):\n    print(f'Creating directory... {working_directory}')\n    os.makedirs(working_directory)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words_path = os.path.join(working_directory, 'words.txt') # will save tokenization into words.txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def perform_word_count(publications: List[PublicationModel],words_path: str):\n    \"\"\"Perform word count, sort it and save it to file.\"\"\"\n    print(f'Performing word count, sorting and saving to file...')\n    tic = time.time()\n    \n    word_count = Counter()\n    sorted_word_count: List[str, int] = []\n    # Perform word count of dataset\n    text = []\n    for publication in publications:\n        for article in publication.articles:\n            # Combine abstract and conclusion into text\n            if article.abstract:\n                text.append(article.abstract)\n            if article.conclusion:\n                text.append(article.conclusion)\n    text = ' '.join(text)\n    word_count = Counter(tokenize(text))\n    # Sort word count.\n    sorted_word_count = word_count.most_common()\n    # Save words of sorted word count to file\n    sorted_words, sorted_counts = zip(*sorted_word_count)\n    with open(words_path, 'w',encoding=\"utf-8\") as fh:\n        for word in sorted_words:\n            fh.writelines(word + '\\n')\n\n    toc = time.time()\n    print(f'Word count done. Runtime: {int(toc-tic)} seconds.')\n    \n    return word_count, sorted_word_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_count, sorted_word_count = perform_word_count(publications, words_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_unique_words = len(word_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_unique_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for cls, keywords in keywords_classes.items():\n    print(f'{cls}:')\n    for keyword in keywords:\n        print(f'    {keyword} has {word_count.get(keyword, 0)} words')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_words = sum([count for word, count in sorted_word_count])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_words, sorted_counts = zip(*sorted_word_count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### 4.4.12 Words dictionary\n\n- word 2 index \n\n- index 2 word"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_vocabulary_size = CL_MAX_VOCABULARY_SIZE\nprint(max_vocabulary_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_words_dictionaries(sorted_word_count):\n    \"\"\"Generate integer to word dictionary and vice versa.\"\"\"\n    print(f'Generating dictionaries...')\n    sorted_words, sorted_counts = zip(*sorted_word_count)\n    # Dictionary is up to maximum vocabulary size\n    if len(sorted_words) > max_vocabulary_size:\n        sorted_words = sorted_words[:max_vocabulary_size]\n\n    # Vocabulary size with unknown\n    vocabulary_size = len(sorted_words) + 1\n\n    # Generate dictionaries\n    int_to_word = {i+1: word for i, word in enumerate(sorted_words)}\n    int_to_word[0] = '<UNK>'\n    word_to_int = {word: i for i, word in int_to_word.items()}\n    \n    return int_to_word, word_to_int, vocabulary_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"int_to_word, word_to_int, vocabulary_size = generate_words_dictionaries(sorted_word_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert(int_to_word[word_to_int['the']] == 'the')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert(word_to_int[int_to_word[len(sorted_word_count)-1]] == len(sorted_word_count)-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4.13 Preprocess DataSet"},{"metadata":{"trusted":true},"cell_type":"code","source":"sequence_length = CL_SEQUENCE_LENGTH\nprint(sequence_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing(text: str, label: str) -> (List[int], int):\n    \"\"\"Pad/turncate and encode text of words from strings into integers.\n\n    Arguments:\n        text {str} -- input text.\n        label {str} -- input label.\n        \n    Returns:\n        (List[int], int) -- features and label in integers.\n    \"\"\"\n    if type(text) is not str:\n        # Convert tensor to string\n        text = text.numpy().decode('utf-8')\n    # Tokenize text\n    tokenized_text = tokenize(text)\n    # Turncate text\n    if len(tokenized_text) >= sequence_length:\n        features = tokenized_text[:sequence_length]\n    # Pad text\n    else:\n        features = ['<UNK>'] * (sequence_length-len(tokenized_text))\n        features += tokenized_text\n    # Encode text\n    features_int = [word_to_int.get(feature, 0) for feature in features]\n    features_int = np.array(features_int)\n    \n    # Encode Label\n    if label != '':\n        if type(label) is not str:\n            # Convert tensor to string\n            label = label.numpy().decode('utf-8')\n            \n        if label in label_str_to_int:\n            label_int = label_str_to_int[label]\n        else:\n            raise Exception(f'{label} is not in {label_str_to_int}')\n        labels_float = [0.0] * total_classes\n        labels_float[label_int] = 1.0\n        labels_float = np.array(labels_float)\n    else:\n        labels_float = np.array([])\n\n    return features_int, labels_float","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_int, labels_float = preprocessing(text=' '.join(['the'] * (sequence_length-26)), label='lung disease')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4.14 Build data generator for training a NN "},{"metadata":{"trusted":true},"cell_type":"code","source":"num_parallel_calls = CL_NUM_PARALLEL_CALLS\nshuffle_buffer_size = CL_SHUFFLE_BUFFER_SIZE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### For training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"training_preprocessing = lambda x, y: tf.py_function(preprocessing,(x, y), ((tf.int32), tf.float32))\ntraining_features = (\n    training_dataset\n    .shuffle(shuffle_buffer_size)\n    .map(training_preprocessing,\n         num_parallel_calls=num_parallel_calls)\n    .batch(len(training_articles))\n    .prefetch(buffer_size=num_parallel_calls)\n    .cache()\n    .repeat()\n)\ntraining_features_iter = iter(training_features)\nfeatures_int, labels_float = training_features_iter.get_next()\nassert(features_int.shape == (len(training_articles), sequence_length))\nassert(labels_float.shape == (len(training_articles), total_classes))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### For validation data "},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_preprocessing = lambda x, y: tf.py_function(preprocessing,(x, y), ((tf.int32), tf.float32))\nvalidation_features = (\n    validation_dataset\n    .shuffle(shuffle_buffer_size)\n    .map(validation_preprocessing,\n         num_parallel_calls=num_parallel_calls)\n    .batch(len(validation_articles))\n    .prefetch(buffer_size=num_parallel_calls)\n    .cache()\n    .repeat()\n)\nvalidation_features_iter = iter(validation_features)\nfeatures_int, labels_float = validation_features_iter.get_next()\nassert(features_int.shape == (len(validation_articles), sequence_length))\nassert(labels_float.shape == (len(validation_articles), total_classes))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### For test data "},{"metadata":{"trusted":true},"cell_type":"code","source":"testing_preprocessing = lambda x: tf.py_function(preprocessing,(x, ''), ((tf.int32), (tf.float32)))\ntesting_features = (\n    testing_dataset\n    .map(testing_preprocessing,\n         num_parallel_calls=num_parallel_calls)\n    .batch(len(testing_articles))\n    .prefetch(buffer_size=num_parallel_calls)\n    .cache()\n    .repeat()\n)\ntesting_features_iter = iter(testing_features)\nfeatures_int, labels_float = testing_features_iter.get_next()\nfeatures_int, labels_float = testing_features_iter.get_next()\nassert(features_int.shape == (len(testing_articles), sequence_length))\nassert(labels_float.shape == (len(testing_articles), 0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4.15 Create RNN model based on GRU \n\nThe model structure\n\n- Embedding layer\n\n- Bidirectional GRU layer\n\n- Dense Layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim = CL_EMBEDDING_DIM\nprint(embedding_dim)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"activation_layer_units = CL_ACTIVATION_LAYER_UNITS\nprint(activation_layer_units)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create RNN model based on bidirectional GRU layer\nmodel = keras.Sequential([\n    keras.layers.Embedding(vocabulary_size,\n                           embedding_dim,\n                           embeddings_initializer='uniform'),\n    keras.layers.Bidirectional(keras.layers.GRU(128, return_sequences=True)),\n    keras.layers.GlobalAveragePooling1D(),\n    keras.layers.Dense(32, activation='relu'),\n\n    keras.layers.Dense(total_classes)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam',\n              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_directory = os.path.join(working_directory, 'models')\nmodel_directory","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not os.path.isdir(model_directory):\n    print(f'Creating directory... {model_directory}')\n    os.makedirs(model_directory)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_checkpoint_directory = os.path.join(model_directory,\n                                          'checkpoint')\nmodel_checkpoint_directory","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not os.path.isdir(model_checkpoint_directory):\n    print(f'Creating directory... {model_checkpoint_directory}')\n    os.makedirs(model_checkpoint_directory)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_architecture_name = CL_MODEL_ARCHITECTURE_NAME\nmodel_architecture_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_name = CL_DATA_NAME\ndata_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_checkpoint_file_name = \"%s_%s.h5\" % (\n    model_architecture_name, data_name)\nmodel_checkpoint_file_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_checkpoint_path = os.path.join(model_checkpoint_directory,\n                                     model_checkpoint_file_name)\nmodel_checkpoint_path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    filepath=model_checkpoint_path,\n    monitor='val_accuracy', verbose=1, \n    save_best_only=True, save_weights_only=False,\n    mode='max', save_freq='epoch')\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=2,\n                                                           verbose=1,\n                                                           mode='auto')\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1,\n                                                          verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n\ncallbacks = [checkpoint,early_stopping,reduce_lr]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### 4.4.17 Train the model "},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = CL_BATCH_SIZE\nprint(batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = CL_EPOCHS\nprint(epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_workers = CL_NUM_WORKERS\nprint(num_workers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create iterators\ntraining_features_int, training_labels_float =  training_features_iter.get_next()\nvalidation_features_int, validation_labels_float = validation_features_iter.get_next()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"history = model.fit(x=training_features_int, y=training_labels_float, \n                    batch_size=batch_size,\n                    epochs=4,\n                    callbacks=callbacks,\n                    validation_data=(validation_features_int,\n                                     validation_labels_float),\n                    workers=num_workers,\n                    use_multiprocessing=True,                   \n                   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_dict = history.history\nhistory_dict.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = history_dict['loss']\nval_loss = history_dict['val_loss']\nhist_epochs = range(1, len(loss) + 1)\n\nplt.plot(hist_epochs, loss, 'b-.', label='Training Loss')\nplt.plot(hist_epochs, val_loss, 'r-.', label='Validation Loss')\nplt.title('Training Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\nhist_epochs = range(1, len(acc) + 1)\n\nplt.plot(hist_epochs, acc, 'b--', label='Training Accuracy')\nplt.plot(hist_epochs, val_acc, 'r--', label='Validation Accuracy')\nplt.title('Training Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4.18 Evaluate and prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights(model_checkpoint_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, accuracy = model.evaluate(\n    x=validation_features_int, y=validation_labels_float,\n    batch_size=batch_size,\n    workers=num_workers,\n    use_multiprocessing=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Loss: {loss}')\nprint(f'Accuracy: {accuracy}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probability_model = tf.keras.Sequential([model,tf.keras.layers.Softmax()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_prediction(dataset, dataset_size, preprocessing, model):\n    # Make data pipeline\n    features = (\n        dataset\n        .map(preprocessing,\n             num_parallel_calls=num_parallel_calls)\n        .batch(dataset_size)\n        .prefetch(buffer_size=num_parallel_calls)\n        .cache()\n        .repeat()\n    )\n    features_iter = iter(features)\n    features_int, _ = features_iter.get_next()\n    # Make prediction\n    predictions = model.predict(\n        x=features_int,\n        batch_size=batch_size,\n        verbose=1,\n        workers=num_workers, use_multiprocessing=True)\n    \n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_prediction_result_per_class(predictions):\n    predictions_t = predictions.transpose()\n    # Argsort predictions\n    sorted_predictions_indices = np.array([\n        sorted_prediction[::-1] \n        for sorted_prediction in np.argsort(predictions_t)\n    ])\n    # Plot\n    labels = list(label_str_to_int.keys())\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    for i, sorted_predictions_index in enumerate(sorted_predictions_indices):\n        value = [predictions_t[i][j] for j in sorted_predictions_index]\n        ax.plot(value, label=labels[i])\n    plt.legend()\n    plt.title('Sorted Prediction Result per Class')\n    plt.xlabel('Articles')\n    plt.ylabel('Prediction Result')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_top_articles_per_class(predictions, articles, top_k=3):\n    predictions_t = predictions.transpose()\n    # Argsort predictions\n    sorted_predictions_indices = np.array([\n        sorted_prediction[::-1] \n        for sorted_prediction in np.argsort(predictions_t)\n    ])\n    # Print top articles\n    labels = list(label_str_to_int.keys())\n\n \n\n    for i, sorted_predictions_index in enumerate(sorted_predictions_indices):\n#         if i==3:\n#             print(sorted_predictions_index)\n        print(\"*\" * 100)\n        print(f'category: {labels[i]}')\n        print(\"*\" * 100)\n        for j in sorted_predictions_index[:top_k]:\n            print(f'Score: {predictions_t[i][j]:.2f}')\n            print(articles[j])\n            print('\\n')\n            print(\"=\" * 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_top_articles_per_class(predictions, articles, top_k=3):\n    predictions_t = predictions.transpose()\n    # Argsort predictions\n    sorted_predictions_indices = np.array([\n        sorted_prediction[::-1] \n        for sorted_prediction in np.argsort(predictions_t)\n    ])\n    # Print top articles\n    labels = list(label_str_to_int.keys())\n\n \n\n    for i, sorted_predictions_index in enumerate(sorted_predictions_indices):\n#         if i==3:\n#             print(sorted_predictions_index)\n        print(\"*\" * 100)\n        print(f'category: {labels[i]}')\n        print(\"*\" * 100)\n        for j in sorted_predictions_index[:top_k]:\n            print(f'Score: {predictions_t[i][j]:.2f}')\n            print(articles[j])\n            print('\\n')\n            print(\"=\" * 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_dataset_predictions = make_prediction(\n    dataset=training_dataset,\n    dataset_size=len(training_articles),\n    preprocessing=training_preprocessing,\n    model=probability_model\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_top_articles_per_class(\n    predictions=training_dataset_predictions,\n    articles=training_articles,\n    top_k=5\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_dataset_predictions = make_prediction(\n    dataset=validation_dataset,\n    dataset_size=len(validation_articles),\n    preprocessing=validation_preprocessing,\n    model=probability_model\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_top_articles_per_class(\n    predictions=valid_dataset_predictions,\n    articles=validation_articles,\n    top_k=5\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset_predictions = make_prediction(\n    dataset=testing_dataset,\n    dataset_size=len(testing_articles),\n    preprocessing=testing_preprocessing,\n    model=probability_model\n)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print_top_articles_per_class(\n    predictions=test_dataset_predictions,\n    articles=testing_articles,\n    top_k=5\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_k=5\n     \nprint_top_articles_per_class.predictions_t = training_dataset_predictions.transpose()\npredictions_t = training_dataset_predictions.transpose()\nsorted_predictions_indices = np.array([\n    sorted_prediction[::-1] \n    for sorted_prediction in np.argsort(predictions_t)\n])\n    \nprint_top_articles_per_class.labels = list(label_str_to_int.keys())\nlabels = list(label_str_to_int.keys())\n\n \n\nfor i, sorted_predictions_index in enumerate(sorted_predictions_indices):\n    \n    display(Markdown('# <span style=\"color:#1174E6;font-weight:bold;font-size: 20pt\">List of Articles </span>'))\n    for j in sorted_predictions_index[:top_k]:\n        #print(f'Score: {predictions_t[i][j]:.2f}')           \n        #print(training_articles[j])\n        display(Markdown('# <span style=\"color:#FADA5;font-weight:bold;font-size: 10pt\"> Sub Task'+str(i+1)+ ': '+ str(labels[i]) +'</span>'))\n        display(Markdown('# <span style=\"color:#FADA5;font-weight:bold;font-size: 10pt\"> Weightage'+ ': '+ str(predictions_t[i][j]) +'</span>'))\n        display(Markdown('# <span style=\"color:#FADA5;font-weight:bold;font-size: 10pt\"> Paper Id'+ ': '+ str(training_articles[j].paper_id) +'</span>'))\n        display(Markdown('# <span style=\"color:#FADA5;font-weight:bold;font-size: 10pt\"> Title'+ ': '+ str(training_articles[j].title) +'</span>'))\n        display(Markdown('# <span style=\"color:#FADA5;font-size: 10pt;font-weight:normal\"><p style=\"font-weight:bold;\">Abstract'+ ': '+ '</p> '+ str(training_articles[j].abstract) +'</span>'))\n        display(Markdown('# <span style=\"color:#FADA5;font-size: 10pt;font-weight:normal\"><p style=\"font-weight:bold;\">Conclusion'+ ': '+ '</p> '+ str(training_articles[j].conclusion) +'</span>'))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}