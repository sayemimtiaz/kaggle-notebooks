{"cells":[{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Code by Olga Belitskaya https://www.kaggle.com/olgabelitskaya/sequential-data/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https://fonts.googleapis.com/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';</style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s</h1>\"\"\"%string))\n    \n    \ndhtml('Maximum Pooling Exercise by Instructor Ryan Holbrook')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Conclude the feature extraction begun in \"Hugging Tigress Hidden Tree\", explore how invariance is created by maximum pooling, and #then look at a different kind of pooling: average pooling."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nimport learntools.computer_vision.visiontools as visiontools\n\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\nplt.rc('image', cmap='summer')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#We'll use a predefined Kaggle Notebook this time."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read image\nimage_path = '../input/cusersmarildownloadstigerjpg/tiger.jpg'\nimage = tf.io.read_file(image_path)\nimage = tf.io.decode_jpeg(image, channels=1)\nimage = tf.image.resize(image, size=[400, 400])\n\n# Embossing kernel\nkernel = tf.constant([\n    [-2, -1, 0],\n    [-1, 1, 1],\n    [0, 1, 2],\n])\n\n# Reformat for batch compatibility.\nimage = tf.image.convert_image_dtype(image, dtype=tf.float32)\nimage = tf.expand_dims(image, axis=0)\nkernel = tf.reshape(kernel, [*kernel.shape, 1, 1])\nkernel = tf.cast(kernel, dtype=tf.float32)\n\nimage_filter = tf.nn.conv2d(\n    input=image,\n    filters=kernel,\n    strides=1,\n    padding='VALID',\n)\n\nimage_detect = tf.nn.relu(image_filter)\n\n# Show what we have so far\nplt.figure(figsize=(12, 6))\nplt.subplot(131)\nplt.imshow(tf.squeeze(image), cmap='gray')\nplt.axis('off')\nplt.title('Input')\nplt.subplot(132)\nplt.imshow(tf.squeeze(image_filter))\nplt.axis('off')\nplt.title('Filter')\nplt.subplot(133)\nplt.imshow(tf.squeeze(image_detect))\nplt.axis('off')\nplt.title('Detect')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Apply Pooling to Condense\n\nApply Maximum Pooling using a  2×2  Pooling Window"},{"metadata":{"trusted":true},"cell_type":"code","source":"image_condense = image_condense = tf.nn.pool(\n    input=image_detect,\n    window_shape=(2, 2),\n    pooling_type='MAX',\n    strides=(2, 2),\n    padding='SAME',\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#What maximum pooling did to the feature!"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nplt.subplot(121)\nplt.imshow(tf.squeeze(image_detect))\nplt.axis('off')\nplt.title(\"Detect (ReLU)\")\nplt.subplot(122)\nplt.imshow(tf.squeeze(image_condense))\nplt.axis('off')\nplt.title(\"Condense (MaxPool)\")\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We learned about how MaxPool2D layers give a convolutional network the property of translation invariance over small distances. In this exercise, you'll have a chance to observe this in action.\n\nThis next code cell will randomly apply a small shift to a circle and then condense the image several times with maximum pooling.  Make note of the image that results at the end."},{"metadata":{"trusted":true},"cell_type":"code","source":"REPEATS = 4\nSIZE = [64, 64]\n\n# Create a randomly shifted circle\nimage = visiontools.circle(SIZE, r_shrink=4, val=1)\nimage = tf.expand_dims(image, axis=-1)\nimage = visiontools.random_transform(image, jitter=3, fill_method='replicate')\nimage = tf.squeeze(image)\n\nplt.figure(figsize=(16, 4))\nplt.subplot(1, REPEATS+1, 1)\nplt.imshow(image, vmin=0, vmax=1)\nplt.title(\"Original\\nShape: {}x{}\".format(image.shape[0], image.shape[1]))\nplt.axis('off')\n\n# Now condense with maximum pooling several times\nfor i in range(REPEATS):\n    ax = plt.subplot(1, REPEATS+1, i+2)\n    image = tf.reshape(image, [1, *image.shape, 1])\n    image = tf.nn.pool(image, window_shape=(2,2), strides=(2, 2), padding='SAME', pooling_type='MAX')\n    image = tf.squeeze(image)\n    plt.imshow(image, vmin=0, vmax=1)\n    plt.title(\"MaxPool {}\\nShape: {}x{}\".format(i+1, image.shape[0], image.shape[1]))\n    plt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Explore Invariance\n\nSuppose you had made a small shift in a different direction -- what effect would you expect that have on the resulting image? Try running the cell a few more times, if you like, to get a new random shift."},{"metadata":{},"cell_type":"markdown","source":"#How Maximum Pooling creates translation invariance over small distances.\n\nThis means that we would expect small shifts to disappear after repeated maximum pooling. If you run the cell multiple times, you can see the resulting image is always the same; the pooling operation destroys those small translations."},{"metadata":{},"cell_type":"markdown","source":"#Global Average Pooling\n\nAverage pooling has largely been superceeded by maximum pooling within the convolutional base. There is, however, a kind of average pooling that is still widely used in the head of a convnet. This is global average pooling. A GlobalAvgPool2D layer is often used as an alternative to some or all of the hidden Dense layers in the head of the network, like so:\n\n\nmodel = keras.Sequential([\n\n    `pretrained_base`,\n    \n    layers.GlobalAvgPool2D(),\n    \n    layers.Dense(1, activation='sigmoid'),\n])\n\nWhat is this layer doing? Notice that we no longer have the Flatten layer that usually comes after the base to transform the 2D feature data to 1D data needed by the classifier. Now the GlobalAvgPool2D layer is serving this function. But, instead of \"unstacking\" the feature (like Flatten), it simply replaces the entire feature map with its average value. Though very destructive, it often works quite well and has the advantage of reducing the number of parameters in the model.\n\nLet's look at what GlobalAvgPool2D does on some randomly generated feature maps. This will help us to understand how it can \"flatten\" the stack of feature maps produced by the base."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_maps = [visiontools.random_map([5, 5], scale=0.1, decay_power=4) for _ in range(8)]\n\ngs = gridspec.GridSpec(1, 8, wspace=0.01, hspace=0.01)\nplt.figure(figsize=(18, 2))\nfor i, feature_map in enumerate(feature_maps):\n    plt.subplot(gs[i])\n    plt.imshow(feature_map, vmin=0, vmax=1)\n    plt.axis('off')\nplt.suptitle('Feature Maps', size=18, weight='bold', y=1.1)\nplt.show()\n\n# reformat for TensorFlow\nfeature_maps_tf = [tf.reshape(feature_map, [1, *feature_map.shape, 1])\n                   for feature_map in feature_maps]\n\nglobal_avg_pool = tf.keras.layers.GlobalAvgPool2D()\npooled_maps = [global_avg_pool(feature_map) for feature_map in feature_maps_tf]\nimg = np.array(pooled_maps)[:,:,0].T\n\nplt.imshow(img, vmin=0, vmax=1)\nplt.axis('off')\nplt.title('Pooled Feature Maps')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since each of the  5×5  feature maps was reduced to a single value, global pooling reduced the number of parameters needed to represent these features by a factor of 25 -- a substantial savings!\n\nNow we'll move on to understanding the pooled features.\n\nAfter we've pooled the features into just a single value, does the head still have enough information to determine a class? This part of the exercise will investigate that question.\n\nLet's pass some images from our Car or Truck dataset through VGG16 and examine the features that result after pooling. First run this cell to define the model and load the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"#From Exercise4: The Sliding Window\n\nfrom learntools.computer_vision.visiontools import edge, blur, bottom_sobel, emboss, sharpen, circle\n\nimage_dir = '../input/cusersmarildownloadstigerjpg/'\n#circle_64 = tf.expand_dims(circle([64, 64], val=1.0, r_shrink=4), axis=-1)\n#kaggle_k = visiontools.read_image(image_dir + str('k.jpg'), channels=1)\ntigress = visiontools.read_image(image_dir + str('tiger.jpg'), channels=1)\ntigress = tf.image.resize(tigress, size=[200, 200])\nimages = [(tigress, \"Tigress\")]\n\nplt.figure(figsize=(14, 4))\nfor i, (img, title) in enumerate(images):\n    plt.subplot(1, len(images), i+1)\n    plt.imshow(tf.squeeze(img))\n    plt.axis('off')\n    plt.title(title)\nplt.show();\n\nkernels = [(edge, \"edge\"), (blur, \"blur\"), (bottom_sobel, \"bottom_sobel\"),\n           (emboss, \"emboss\"), (sharpen, \"sharpen\")]\nplt.figure(figsize=(14, 4))\nfor i, (krn, title) in enumerate(kernels):\n    plt.subplot(1, len(kernels), i+1)\n    visiontools.show_kernel(krn, digits=2, text_size=20)\n    plt.title(title)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Exercise4: The Sliding Window \n\n# YOUR CODE HERE: choose an image\nimage = tigress\n\n# YOUR CODE HERE: choose a kernel\nKaggleNotebook = bottom_sobel\n\nvisiontools.show_extraction(\n    image, KaggleNotebook,\n\n    # YOUR CODE HERE: set parameters\n    conv_stride=1,\n    conv_padding='valid',\n    pool_size=2,\n    pool_stride=2,\n    pool_padding='same',\n    \n    subplot_shape=(1, 4),\n    figsize=(14, 6),\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#From here we must have binary (like the car-or-truck). Therefore the Tigress is not allowed since it's only One."},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\n\n# Load VGG16\npretrained_base = tf.keras.models.load_model(\n    '../input/cv-course-models/cv-course-models/vgg16-pretrained-base',\n)\n\nmodel = keras.Sequential([\n    pretrained_base,\n    # Attach a global average pooling layer after the base\n    layers.GlobalAvgPool2D(),\n])\n\n# Load dataset\nds = image_dataset_from_directory(\n    '../input/car-or-truck/train',\n    labels='inferred',\n    label_mode='binary',\n    image_size=[128, 128],\n    interpolation='nearest',\n    batch_size=1,\n    shuffle=True,\n)\n\nds_iter = iter(ds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice how we've attached a GlobalAvgPool2D layer after the pretrained VGG16 base. Ordinarily, VGG16 will produce 512 feature maps for each image. The GlobalAvgPool2D layer reduces each of these to a single value, an \"average pixel\", if you like.\n\nThis next cell will run an image from the Car or Truck dataset through VGG16 and show you the 512 average pixels created by GlobalAvgPool2D. Run the cell a few times and observe the pixels produced by cars versus the pixels produced by trucks."},{"metadata":{"trusted":true},"cell_type":"code","source":"car = next(ds_iter)\n\ncar_tf = (tf.image.resize(car[0], size=[192, 192]), car[1])\ncar_features = model(car_tf)\ncar_features = tf.reshape(car_features, shape=(16, 32))\nlabel = int(tf.squeeze(car[1]).numpy())\n\nplt.figure(figsize=(8, 4))\nplt.subplot(121)\nplt.imshow(tf.squeeze(car[0]))\nplt.axis('off')\nplt.title([\"Car\", \"Truck\"][label])\nplt.subplot(122)\nplt.imshow(car_features)\nplt.title('Pooled Feature Maps')\nplt.axis('off')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Why my car is different from Ryan's? Each time I run that cell, the car is different.  "},{"metadata":{},"cell_type":"markdown","source":"#Understand the Pooled Features\n\nWhat do you see? Are the pooled features for cars and trucks different enough to tell them apart? How would you interpret these pooled values? How could this help the classification? \n\nThe VGG16 base produces 512 feature maps. We can think of each feature map as representing some high-level visual feature in the original image -- maybe a wheel or window. Pooling a map gives us a single number, which we could think of as a score for that feature: large if the feature is present, small if it is absent. Cars tend to score high with one set of features, and Trucks score high with another. Now, instead of trying to map raw features to classes, the head only has to work with these scores that GlobalAvgPool2D produced, a much easier problem for it to solve."},{"metadata":{},"cell_type":"markdown","source":"Global average pooling is often used in modern convnets. One big advantage is that it greatly reduces the number of parameters in a model, while still telling you if some feature was present in an image or not -- which for classification is usually all that matters. If you're creating a convolutional classifier it's worth trying out!"},{"metadata":{},"cell_type":"markdown","source":"#Conclusion\n\nIn this lesson we explored the final operation in the feature extraction process: condensing with maximum pooling. Pooling is one of the essential features of convolutional networks and helps provide them with some of their characteristic advantages: efficiency with visual data, reduced parameter size compared to dense networks, translation invariance. We've seen that it's used not only in the base during feature extraction, but also can be used in the head during classification. Understanding it is essential to a full understanding of convnets."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Code by Olga Belitskaya https://www.kaggle.com/olgabelitskaya/sequential-data/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https://fonts.googleapis.com/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';</style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s</h1>\"\"\"%string))\n    \n    \ndhtml('All Exercise by Ryan Holbrook' )","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}