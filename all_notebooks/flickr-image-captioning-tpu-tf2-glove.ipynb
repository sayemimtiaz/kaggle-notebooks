{"cells":[{"metadata":{"id":"K2s1A9eLRPEj"},"cell_type":"markdown","source":"# Flickr Image Captioning with TF2 & TPU!!!\n***with bonus improvement on using Glove***\n\nImage Captioning is a very exciting problem where our model have to translate understanding of images to human readable sentences! However, although Kaggle has been a very rich source of shared codes, I found that there's extremely rare Kaggle kernels on this exciting domain where Vision meets NLP ... \n\n\n![Image Captioning](https://miro.medium.com/max/1400/1*6BFOIdSHlk24Z3DFEakvnQ.png)\n\nWhen I started learning Deep Learning and exploring Kaggle, this kind of tutorial kernel is the one I was looking for, but could not find practical resources at that time. I hope this tutorial to be a starting point and inspiration for those who want to learn this exciting field of Visio-Linguistic domain!!\n\nWith the release of Tensorflow 2 (the latest version now is TF2.2) and Kaggle free TPU + free GCS (Google Cloud Storage), we all now can access a computational **super power** easily than ever before. I think this is simply a revolution, and this super-feature finally allow laymen like us to attack complicated problems like Image Captioning to another level. All of these inspired me to work and share this tutorial kernel!\n\n\n## Reference and Improvement\nAt the moment when I was writing this kernel, the best source to learn image captioning is TF official tutorial : https://www.tensorflow.org/tutorials/text/image_captioning where not only we can learn how to do basic image captioning, we can also learn how to \n\n* How to implement **Attention mechanism** properly! Attention mechanism is a technique to allow us to focus on only-relevant spatial image features when generating each word.\n* How to construct **Keras subclass API** in contrast to normally usage of Function API\n* How to build **custom training loop** in Keras\n\nTo adapt this great tutorial to Kaggle we need to adjust code a bit. For those who are interested in to try this original tutorial in Kaggle, I already did all the jobs for you here : https://www.kaggle.com/ratthachat/image-captioning-by-effnet-attention-in-tf2-1\n\nIn this kernel, we will go beyond the original tutorial in many directions!!\n\n* ***From GPU to TPU*** : we will increase our computational power by *** > 10 folds*** using Kaggle recent TPU feature! With Tensorflow 2, this requires minimal code changing and requires us to transfer data to GCS, which again Kaggle has generously provided us for free! \n* ***Complexity of data***: we changes training data from COCO to Flickr which is more challenging due to quality of texts describing the images! To get decent model, we increase training data ***almost 20 folds*** from around 8K to 150K from the original tutorial, but all training from scratch to convergence can be done by less than 2 hours with the power of TF2 and TPU!!\n* ***Capability of models*** : we upgrade both CNN encoder (for images) and RNN decoder (for texts). On CNN, we use the current SOTA EfficientNet with \"Noisy-Students\" weights generously provided [here](https://github.com/qubvel/efficientnet) . For RNN, instead of learning word vectors from scratch, we adjusted the code to embed the pretrained Glove vectors directly . \n\nWith all these upgrades we got much better results than the original tutorial. (you can compared this kernel result with the original in Kaggle's link above)\n\n# 0. Outline and Pre-requisite\nThis tutorial consists of 3 main sections\n\n1. Setup everything (TPU/GPU and input pipeline)\n2. Build and train Model\n3. Predict and visualize image captions\n\nEach section consists of several sub-steps as described in each section . Before we can begin, install the latest tensorflow stable version (TF 2.2) and Qubvel's efficientnet . These two may be auto set up into Kaggle docker in the future, but for now we have to install it manually.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Try efficientnet in addition to InceptionV3 of the original example.\n!pip install -q tensorflow==2.2 # fix TPU memory issue\n!pip install -q efficientnet\n\nN_VOCABS = 20000 # all vocabs of flickr30k is around 18k, so we choose them all -- if training loss does not work well, change to 5K\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> I also provide the saved weights for this kernel, if you just want to play around image captioning, without model training, you can set the `USE_PREVIOUS_SAVE = True` . If you set it to be `False`, please remember to turn on your TPU where you have a 30 hours/week quota.\n\n**NOTE** In notebook version 7, I set this to true, and just use CPU to predict captions. If you want to use a TPU to train this kernel, please see version 6.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"USE_PREVIOUS_SAVE = True# Set to False if you want to train the model by yourself","execution_count":null,"outputs":[]},{"metadata":{"id":"Cffg2i257iMS"},"cell_type":"markdown","source":"# 1. Setup evertything\n\nWe have to prepare several steps of data pipeline. Note that we have two types of inputs : images and real-captions, and only one output type (predicted captions)\n\n1.1. Choose appropriate TF `strategy` for TPU/GPU, and Transfer dataset into Kaggle GCS in case of TPU\n\n1.2. Propcess Captions , adding `<start>` and `<end>`\n\n1.3. Build a list of images and corresponding captions (image-input and text-output)\n\n1.4. Setup text-input for RNN decoder\n\n - 1.4.1) Load pretrained word vectors (Glove) for each vocabulary\n - 1.4.2) Tokenize captioning to model's vocabulary also pad each caption to have the same length\n \n1.5. Split data into train and valid set\n\n1.6. Employ `tf.dataset` to construct appropriate input pipeline using data from steps 2.-4.\n - We can also add data transformation (e.g. image augmentation like H-flip) easily in this step","execution_count":null},{"metadata":{"id":"U8l4RJ0XRPEm","trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\n# You'll generate plots of attention in order to see which parts of an image\n# our model focuses on during captioning\nimport matplotlib.pyplot as plt\n\n# Scikit-learn includes many helpful utilities\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\nimport re\nimport numpy as np\nimport os\nimport time\nimport json\nimport gc\nfrom glob import glob\nfrom PIL import Image\nimport pickle\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.1 TPU/GPU strategy and GCS transfer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_datasets import KaggleDatasets\nimport efficientnet.tfkeras as efn \nfrom tokenizers import ByteLevelBPETokenizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following code will detect TPU or else use either GPU or CPU. With TF-Keras' power, all complex details of multi-GPUs or TPUs will hide under the hood in the variable `strategy` . In Kaggle, if you enable TPU, you should see REPLICAS = 8 (meaning 8 processors for TPU v3-8)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"id":"b6qbGw8MRPE5"},"cell_type":"markdown","source":"### Upload Flickr into Kaggle GCS\n\nWe will use the [Flickr dataset](http://bryanplummer.com/Flickr30kEntities/) to train our model. Originally, the dataset contains more than 30,000 images, each of which has at least 5 different caption annotations. This great dataset is already in Kaggle's dataset; however, if we want to train our model using TPU, we have to upload this dataset into Kaggle GCS so that TPU can be most effectively access them. Note that TPU cannot directly access normal Kaggle dataset, so this step is a must.\n\nKaggle is so great that it provides free GCS for us, and easy API, see KaggleDatasets below. This will automatically select the best GCS zone with free of charges!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"LOCAL_FLICKR_PATH = '/kaggle/input/flickr-image-dataset/flickr30k_images/'\nannotation_file = LOCAL_FLICKR_PATH + 'results.csv'\nLOCAL_IMG_PATH = LOCAL_FLICKR_PATH + 'flickr30k_images/'\n\n!ls {LOCAL_IMG_PATH} | wc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**IMPORTANT: **. The following cell upload all Flickr images (8GB) into GCS . And it will take around 25 minutes!!!\n\nThe below cell will show an error (HTTP timeout), but actually the transferring process is still ongoing. If using TPU, you have to run this cell again by 25 minutes of time, until it returns the message \"yeah\" . At this point, you can turn off your TPU, wait, and come back again in 25-30 minutes until you can move on.\n\nIf you use GPU or CPU with option `USE_PREVIOUS_SAVE = True`, you can pass all these cells without problems, since you can access normal Kaggle dataset.\n\nAlso note that once the upload finish, it sometimes will remain in Kaggle GCS for a while; therefore, maybe if you play around this kernel in 2-3 days consecutively, you may see that you don't have to upload them again.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n## This steps will take around 25 minutes offline ...\nif strategy.num_replicas_in_sync == 8:\n#     GCS_DS_PATH_FLICKR = KaggleDatasets().get_gcs_path('flickr8k-sau') # 2gb # 5 mins\n    GCS_DS_PATH = KaggleDatasets().get_gcs_path('flickr-image-dataset') # 8gb # 20-25 mins\n    print('yeah')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Once the uploading finish, set up the correct paths for each GPU/TPU\n\nif strategy.num_replicas_in_sync == 8:\n    # print(GCS_DS_PATH_FLICKR)\n    # !gsutil ls $GCS_DS_PATH_FLICKR\n\n    print(GCS_DS_PATH)\n    !gsutil ls $GCS_DS_PATH\n    \n    FLICKR_PATH = GCS_DS_PATH + '/flickr30k_images/'\n    IMG_PATH = FLICKR_PATH + 'flickr30k_images/'\n    # less than 10sec\n    !gsutil ls {IMG_PATH} | wc\nelse: \n    FLICKR_PATH = LOCAL_FLICKR_PATH\n    IMG_PATH = LOCAL_IMG_PATH","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Processing Captions\nAfter finish preparing data, now we begin to process the captions for all images. First, we add the tag `<start>` and `<end>` to all captions to tell the model about the beginning and the end of each caption.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(annotation_file, delimiter='|') #\nprint(df.shape)\nprint(df.columns[2], df.columns[2] == ' comment') # The column name has the front space\ndf[' comment'].values[0]\ndf.head(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm, tqdm_notebook\ntqdm.pandas()\nSTART_TOKEN = '<start> '\nEND_TOKEN = ' <end>'\n\n# tokenizer = ByteLevelBPETokenizer(lowercase=True)\n# tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_start_end(text):\n    return START_TOKEN + str(text) + END_TOKEN\n\ndf['comment'] = df[' comment'].progress_apply(add_start_end)\ndf.comment.values[:6]","execution_count":null,"outputs":[]},{"metadata":{"id":"aANEzb5WwSzg"},"cell_type":"markdown","source":"## 1.3 Lists of input images and corresponding captions\n\nNext, we begin to prepare data pipeline. Lets start first by building a list of input images and corresponding captions (image-input and text-output)\n\nNote that since each image have 5 captions, we simply have 5 duplicated image names in the `full_img_name_list`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"full_img_name_list = [] # include gs path\n\nfor ii in tqdm_notebook(range(len(df))):\n    full_image_path = IMG_PATH + df.image_name.values[ii]\n    full_img_name_list.append(full_image_path)\n                        \nlen(full_img_name_list), full_img_name_list[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_captions_list = list(df.comment.values)\nprint(len(all_captions_list), all_captions_list[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.4 Setup text-input for RNN decoder (text-input)\n\nHere, we have to tokenize the caption labels so that our RNN can use as input and predict as output (word by word). \n\nIn order to generate good sentence, one key is to have good word vectors, so here we choose to employ the pretrained Glove vectors to improve our captioning process\n\n### 1.4.1 Setup Glove word vectors\n\nthe below function `build_matrix` will return the required word vectors. This function will recieve list of words in the dataset's vocabulary as input. We will tokenize all captions and list all vocabs in the dataset in steps 1.4.2\n\nThis funciton is carefully designed to return meaningful word vectors as best as it could as it can fix many unknown words like two examples below :\n\n**Example1.** Note that as Gloves differentiate between upper and lower cases, sometimes Glove may not know word like `john` but it may know `John`. In this case if our dataset have the word `john` which can happen in informal writing, we still want it to have the `John`'s vector (instead of having a random one due to out-of-vocabulary) \n\n**Example2.** For another case, sometimes our dataset may contain some uncommon words like `deforestization` but Glove will not know this word. However, as this complex word may come from the root like `deforest` where Glove really knows, so it may be better to give `deforest`'s vector to this complex word instead of a pure random one.\n\nBoth examples are done automatically in the `build_matrix` function with this line : \n`for candidate in [word, word.lower(), word.upper(), word.capitalize(), ps.stem(word), lc.stem(word), sb.stem(word) ]:` .\n\nNote that all these sub-words mapping can be done automatically in modern decoder model like GPT-2. However, at this moment, GPT-2 cannot get the image input vectors, so unfortunately we cannot employ the GPT-2 super power into our captioning model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer\nps = PorterStemmer()\nfrom nltk.stem.lancaster import LancasterStemmer\nlc = LancasterStemmer()\nfrom nltk.stem import SnowballStemmer\nsb = SnowballStemmer(\"english\")\n\nfrom gensim.models import KeyedVectors\nimport gensim\ndef build_matrix(word_index, embedding_index, vec_dim):\n    \n    num_unk = 0\n    \n    emb_mean, emb_std = -0.0033470048, 0.109855264\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (len(word_index) + 1,vec_dim))\n#     embedding_matrix = np.zeros((len(word_index) + 1, vec_dim))\n    for word, i in word_index.items():\n        known = False\n        for candidate in [word, word.lower(), word.upper(), word.capitalize(), \n                          ps.stem(word), lc.stem(word), sb.stem(word) ]:\n            if candidate in embedding_index:\n                embedding_matrix[i] = embedding_index[candidate]\n                known = True\n                break\n        if known == False: num_unk += 1\n    \n    print('number of unknown words is ', num_unk)\n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# we actually have two choices of pretrained word vectors here : glove and word2vec. You are free to experiment what's best.\nEMBEDDING_FILES = [\n    '../input/gensim-embeddings-dataset/crawl-300d-2M.gensim',\n    '../input/gensim-embeddings-dataset/glove.840B.300d.gensim'\n]\n\nglove_model = gensim.models.KeyedVectors.load(EMBEDDING_FILES[1], mmap='r') # here, we choose glove\ngensim_words = glove_model.index2word\nprint(len(gensim_words), gensim_words[:20])\n# How to use\nprint(glove_model['the'].shape)\n'the' in glove_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.4.2 Tokenization\nHere, we do a usual keras text-processing : \n\n- tokenize all captions into list of words, and remember all vocabulary\n- convert list of words into list of \"word-index\"\n- pad each example in the list to have the same length\n\nNote that we can set the number of Vocab to be small to simplify the training process; however, in this kernel, we already employed pretrained word vector power, so we choose the full set of vocab (around 20K words) so that we can produce most natural sentences.","execution_count":null},{"metadata":{"id":"HZfK8RhQRPFj","trusted":true},"cell_type":"code","source":"# Find the maximum length of any caption in our dataset\ndef calc_max_length(tensor):\n    return max(len(t) for t in tensor)","execution_count":null,"outputs":[]},{"metadata":{"id":"oJGE34aiRPFo","trusted":true},"cell_type":"code","source":"%%time\n# Choose the top_k words from the vocabulary\ntop_k = N_VOCABS \ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n                                                  oov_token=\"<unk>\",\n                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ') # note 'a'\ntokenizer.fit_on_texts(all_captions_list)\ntrain_seqs = tokenizer.texts_to_sequences(all_captions_list)\n\ntokenizer.word_index['<pad>'] = 0\ntokenizer.index_word[0] = '<pad>'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make list from dict\ntokenizer.index2word = [tokenizer.index_word[ii] for ii in range(len(tokenizer.word_index)) ] \nprint(tokenizer.index2word[:20]) # see top-20 most frequent words\nprint(tokenizer.index2word[-20:]) # these all come to <unk>\nlen(tokenizer.index2word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tokenizer.index_word.get(2000, tokenizer.word_index['<end>']))\nprint(tokenizer.index_word.get(19999, tokenizer.word_index['<end>']))\nprint(tokenizer.word_index['<end>'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len_cap = np.array([len(text.split()) for text in all_captions_list])\nprint(len_cap.mean(), len_cap.std(), len_cap.max(), len_cap.min())\nmax_seq_len = int(np.percentile(len_cap,99.9))","execution_count":null,"outputs":[]},{"metadata":{"id":"0fpJb5ojRPFv","trusted":true},"cell_type":"code","source":"%%time\n# Create the tokenized vectors : list of word-indices\ntrain_seqs = tokenizer.texts_to_sequences(all_captions_list)\n\n# Pad each vector to the max_length of the captions\n# If you do not provide a max_length value, pad_sequences calculates it automatically\ncap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post', maxlen = max_seq_len, truncating='post')\n\n# Calculates the max_length, which is used to store the attention weights\nmax_length = calc_max_length(train_seqs) #","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lenx = np.array([len(x) for x in cap_vector])\nprint(lenx.min(), lenx.mean(), cap_vector[0])\nprint(max_length)\nmax_length = np.min([max_seq_len, max_length])\nprint(max_length)","execution_count":null,"outputs":[]},{"metadata":{"id":"M3CD75nDpvTI"},"cell_type":"markdown","source":"## 1.5 Split the data into train and valid set\n\nNote that due to each image have 5 captions, we cannot splitting the data randomly. Here, to avoid knowledge leakage from train-set to valid-set, we must use GroupKFold to carefully make each image stays in the same set.","execution_count":null},{"metadata":{"id":"iS7DDMszRPGF","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold, GroupKFold\n\n# 2.5% valid = 3975 captions = 795 images\nkf = GroupKFold(n_splits=40).split(X=full_img_name_list, groups=full_img_name_list)\n\nfor ind, (tr, val) in enumerate(kf):\n    img_name_train = np.array(full_img_name_list)[tr] # np.array make indexing possible\n    img_name_val = np.array(full_img_name_list)[val]\n    \n    cap_train =  cap_vector[tr]\n    cap_val =  cap_vector[val]\n    break","execution_count":null,"outputs":[]},{"metadata":{"id":"XmViPkRFRPGH","trusted":true},"cell_type":"code","source":"print(img_name_train[:6],'\\n')\nprint(cap_train[:6],'\\n')\nlen(img_name_train), len(cap_train), len(img_name_val), len(cap_val)","execution_count":null,"outputs":[]},{"metadata":{"id":"uEWM9xrYcg45"},"cell_type":"markdown","source":"## 1.6 Create data pipeline tf.data dataset \n\nFinally, we arrive the last step of data preparation : putting everything together in data pipeline.\n\nData pipeline includes reading images/texts from files or from memory, process / transform them (e.g. data augmentation), and feed them into the processors like GPU or TPU. Previously, in Keras & Tensorflow 1, `data_generator` is the method of choice when using Keras but there's cubersome API, and bottleneck on I/O & CPU.\n\nTo optimize training speed, efficient data pipeline is needed when you cannot store all your data in the memory.\n\nIn Tensorflow 2, data pipeline becomes very powerful and simple with `tf.dataset` API. `tf.dataset` not only much simplifies the old mechanism of `data_generator`, but it also address the bottleneck by using parallel machanism, prefetching, processing pipeline in GPU/TPU (instead of a slow CPU) and so on ... For details please see this [tutorial](https://www.youtube.com/watch?v=VeV65oVvoes) ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"target_size = (299, 299,3)\nAUTO = tf.data.experimental.AUTOTUNE\n\ndef decode_image(filename, label=None, image_size=(target_size[0],target_size[1])):\n    means = [0.485, 0.456, 0.406]\n    stds = [0.229, 0.224, 0.225]\n    \n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    \n    image = (tf.cast(image, tf.float32) / 255.0)\n    image = (image - means) / stds # for qubvel EfficientNet\n    \n    image = tf.image.resize(image, image_size)\n    \n    if label is None:\n        return image\n    else:\n        return image, label\n\ndef data_augment(image, label=None):\n    image = tf.image.random_flip_left_right(image)\n#     image = tf.image.random_flip_up_down(image)\n    \n    if label is None:\n        return image\n    else:\n        return image, label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The below specific setting e.g. `drop_remainder = False` is proven to be effective in this tutorial. You can try experiment other options.\n\nOne important thing to note here is that, since we use custom training loop, it's important to convert from normal dataset into distributed dataset for TPU by `strategy.experimental_distribute_dataset()` function.\n\nPlease refer to this official [tutorial](https://www.tensorflow.org/tutorials/distribute/custom_training) and this official [guideline](https://www.tensorflow.org/guide/distributed_training) for more details.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# we need to define BATCH_SIZE for tf.dataset\nBATCH_SIZE = 64 * strategy.num_replicas_in_sync","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_training_dataset(batch_size = BATCH_SIZE):\n    train_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices((img_name_train, cap_train))\n        .map(decode_image, num_parallel_calls=AUTO)\n        .cache()\n        .map(data_augment, num_parallel_calls=AUTO)\n        .repeat() # \n        .shuffle(batch_size*8, reshuffle_each_iteration=True)\n        .batch(batch_size, drop_remainder=False)\n        .prefetch(AUTO)\n    )\n    return strategy.experimental_distribute_dataset(train_dataset)\n\n\n# if use keras.model.fit, no need for repeat and drop_remainder\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((img_name_val, cap_val))\n    .map(decode_image, num_parallel_calls=AUTO)\n#     .repeat()\n    .batch(BATCH_SIZE, drop_remainder=True)\n    .cache()\n    .prefetch(AUTO)\n)\n\nvalid_dist_dataset = strategy.experimental_distribute_dataset(valid_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"nrvoDphgRPGd"},"cell_type":"markdown","source":"# 2.Model\n\nLet me borrow the following model introduction from [official Tensorflow tutorial](https://www.tensorflow.org/tutorials/text/image_captioning ) with little modification for our tutorial: \n\n\nThe model architecture for our Image Captioning model is inspired by the [Show, Attend and Tell](https://arxiv.org/pdf/1502.03044.pdf) paper.\n\n* In this example, we extract the features from the lower convolutional layer of EfficientNet-B3 giving us a vector of shape (10, 10, 1536).\n* We squash that to a shape of (100, 1536). Effectively, we change 2D data from CNN into 1D sequential data for RNN\n* This vector is then passed through the RNN Decoder (which consists of an attention mechanism to look back to this vector in every word prediction).\n* In prediction, RNN (here GRU) using knowledge state (already predicted words) together with original data attended over the image to predict the next word.\n\nGiven an image like the example below, our goal is to generate a caption such as \"a surfer riding on a wave\".\n\n![Man Surfing](https://tensorflow.org/images/surf.jpg)\n\n*[Image Source](https://commons.wikimedia.org/wiki/Surfing#/media/File:Surfing_in_Hawaii.jpg); License: Public Domain*\n\n![Prediction](https://tensorflow.org/images/imcap_prediction.png)","execution_count":null},{"metadata":{"id":"Q3TnZ1ToRPGV","trusted":true},"cell_type":"code","source":"'''\n# Here, we define all relevant parameters for model building and training.\n# Feel free to change these parameters according to your system's configuration\n'''\nLR = 3e-4\nEPOCHS = 10 # For TPU, 1st epoch takes 1hour, after that, with cache power, it's just 3-4 mins /epoch\nif strategy.num_replicas_in_sync == 1:\n    BATCH_SIZE = 1 # in the case of CPU, make thing small just to be able to fit memory\n\nBUFFER_SIZE = 1000\nembedding_dim = 300 #embedding_matrix.shape[1] # 300 for Glove\nunits = 512 # GRU hidden vector #\nvocab_size = top_k + 1 # +1 for <unk>\n\n# attention_features_shape = total number of local parts in images that Decoder can attend. For EfficientNet-B3, it's 10x10 (HxW) = 100\nattention_viz_dim = 10 # \nattention_features_shape = attention_viz_dim**2 \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that to define Attention block, Encoder, Decoder, we use Keras Subclass API, not the usual functional API. This Subclass API is more powerful as it can handle complex model like encoder-decoder & its training process.","execution_count":null},{"metadata":{"id":"ja2LFTMSdeV3","trusted":true},"cell_type":"code","source":"class BahdanauAttention(tf.keras.Model):\n  def __init__(self, units):\n    super(BahdanauAttention, self).__init__()\n    self.W1 = tf.keras.layers.Dense(units)\n    self.W2 = tf.keras.layers.Dense(units)\n    self.V = tf.keras.layers.Dense(1)\n\n  def call(self, features, hidden):\n    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n\n    # hidden shape == (batch_size, hidden_size)\n    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n\n    # score shape == (batch_size, 64, hidden_size)\n    score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n\n    # attention_weights shape == (batch_size, 64, 1)\n    # you get 1 at the last axis because you are applying score to self.V\n    attention_weights = tf.nn.softmax(self.V(score), axis=1)\n\n    # context_vector shape after sum == (batch_size, hidden_size)\n    context_vector = attention_weights * features\n    context_vector = tf.reduce_sum(context_vector, axis=1)\n\n    return context_vector, attention_weights","execution_count":null,"outputs":[]},{"metadata":{"id":"AZ7R1RxHRPGf","trusted":true},"cell_type":"code","source":"class CNN_Encoder(tf.keras.Model):\n    # Since you have already extracted the features and dumped it using pickle\n    # This encoder passes those features through a Fully connected layer\n    def __init__(self, embedding_dim):\n        super(CNN_Encoder, self).__init__()\n        \n        \n        self.cnn0 = efn.EfficientNetB3(weights='noisy-student', \n                                      input_shape=target_size, include_top=False)\n        \n        \n        # e.g. layers[-1].output = TensorShape([None, 10, 10, 1536]) for B3 (not global pooling)\n        self.cnn = tf.keras.Model(self.cnn0.input, self.cnn0.layers[-1].output) \n        self.cnn.trainable = False\n        \n        # shape after fc == (batch_size, attention_features_shape, embedding_dim) >> this is my mistake, should be hidden instead of embedding_dim\n        self.fc = tf.keras.layers.Dense(embedding_dim)\n        \n    # here, x is img-tensor of target_size\n    def call(self, x):\n        x = self.cnn(x) # 4D\n        x = tf.reshape(x, (x.shape[0], -1, x.shape[3]) ) # 3D\n        x = self.fc(x)\n        x = tf.nn.relu(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"id":"V9UbGQmERPGi","trusted":true},"cell_type":"code","source":"class RNN_Decoder(tf.keras.Model):\n  def __init__(self, embedding_matrix, units, vocab_size):\n    super(RNN_Decoder, self).__init__()\n    self.units = units\n    \n    self.vocab_size = embedding_matrix.shape[0]\n    \n    # new interface of pretrained embedding weights : https://github.com/tensorflow/tensorflow/issues/31086\n    # see also : https://stackoverflow.com/questions/55770009/how-to-use-a-pre-trained-embedding-matrix-in-tensorflow-2-0-rnn-as-initial-weigh\n    self.embedding = tf.keras.layers.Embedding(self.vocab_size, embedding_matrix.shape[1], \n                                               embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix), \n                                               trainable=False,\n                                               mask_zero=True)\n    self.gru = tf.keras.layers.GRU(self.units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n    self.fc1 = tf.keras.layers.Dense(self.units)\n    self.fc2 = tf.keras.layers.Dense(vocab_size)\n\n    self.attention = BahdanauAttention(self.units)\n  \n  # x=sequence of words\n  # features=image's extracted features \n  # hidden=GRU's hidden unit\n  def call(self, x, features, hidden):\n    \n    context_vector, attention_weights = self.attention(features, hidden)\n\n    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n    x = self.embedding(x)\n\n    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n\n    # passing the concatenated vector to the GRU\n    output, state = self.gru(x)\n\n    # shape == (batch_size, max_length, hidden_size)\n    x = self.fc1(output)\n\n    # x shape == (batch_size * max_length, hidden_size)\n    x = tf.reshape(x, (-1, x.shape[2]))\n\n    # output shape == (batch_size * max_length, vocab)\n    x = self.fc2(x)\n\n    return x, state, attention_weights\n\n  def reset_state(self, batch_size):\n    return tf.zeros((batch_size, self.units))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we construct the encoder , decoder objects as well as loss function. For TPU custom-training, we have to be careful to create them under  `with strategy.scope():` so that Tensorflow will distribute our training batch to each processor appropriately.\n\nYou can learn more on training strategy [here](https://www.youtube.com/watch?v=jKV53r9-H14&list=UU0rqucBdTuFTjJiefW5t-IQ&index=43).","execution_count":null},{"metadata":{"id":"Qs_Sr03wRPGk","trusted":true},"cell_type":"code","source":"with strategy.scope():\n    # tf.keras.backend.clear_session()\n    embedding_matrix = build_matrix(tokenizer.word_index, glove_model, embedding_dim)\n    print(embedding_matrix.shape) # if not use stop-stem trick, num of unknowns is 495 (vs. current 287)\n    \n    encoder = CNN_Encoder(embedding_dim)\n    decoder = RNN_Decoder(embedding_matrix, units, vocab_size)\n    \n    optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n        from_logits=True, reduction='none') \n    # Set reduction to `none` so we can do the reduction afterwards and divide by\n    # global batch size.\n\n    def loss_function(real, pred):\n        mask = tf.math.logical_not(tf.math.equal(real, 0))\n        loss_ = loss_object(real, pred)\n\n        mask = tf.cast(mask, dtype=loss_.dtype)\n        loss_ *= mask\n        \n        # About why we use `tf.nn.compute_average_loss`, please check this tutorial\n        # https://www.tensorflow.org/tutorials/distribute/custom_training#define_the_loss_function\n#         loss_ = tf.reduce_mean(loss_)\n        loss_ = tf.nn.compute_average_loss(loss_, global_batch_size=BATCH_SIZE)\n        \n        return loss_","execution_count":null,"outputs":[]},{"metadata":{"id":"PHod7t72RPGn"},"cell_type":"markdown","source":"## 2.1 Define Training Step\n\nFor custom training, we have to define the `train_step` function which taking each training batch as input, and return loss as output. For TPU, we also have to make one extra-effort to call `strategy.run` (previously named `strategy.experimental_run_v2`) to divide the global batch into sub-batch for each TPU processor.\n\nIn `train_step`, here's what we do :\n\n* The encoder output, hidden state(initialized to 0) and the decoder input (which is the  `<start>` token) is passed to the decoder.\n* The decoder returns the predictions and the decoder hidden state.\n* The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n\nNote that we use \"teacher-forcing\" training where we input the correct caption for each input. This is different in actual inference stage, where we don't know the real caption, and have to input the previously-predicted word as input. Teacher-forcing make training easier, but will not be very robust. In some advanced research, some papers suggest to transitition from teacher-forcing to actual-inference training in the middle of training process. In this tutorial, we will only do teacher-forcing.\n\nAfter that we define `valid_step` in a similar way.\n","execution_count":null},{"metadata":{"id":"sqgyz2ANKlpU","trusted":true},"cell_type":"code","source":"with strategy.scope():\n    @tf.function\n    def train_step(img_tensor, target):\n        loss = 0\n\n        # initializing the hidden state for each batch\n        # because the captions are not related from image to image\n        hidden = decoder.reset_state(batch_size=target.shape[0])\n\n        dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n\n        with tf.GradientTape() as tape:\n            features = encoder(img_tensor)\n\n            for i in range(1, target.shape[1]):\n                # passing the features through the decoder\n                predictions, hidden, _ = decoder(dec_input, features, hidden)\n\n                loss += loss_function(target[:, i], predictions)\n\n                # using teacher forcing\n                dec_input = tf.expand_dims(target[:, i], 1)\n\n        total_loss = (loss / int(target.shape[1]))\n\n        trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n\n        gradients = tape.gradient(loss, trainable_variables)\n\n        optimizer.apply_gradients(zip(gradients, trainable_variables))\n\n        return loss, total_loss\n    \n    @tf.function\n    def distributed_train_step(inputs):\n\n        (images, labels) = inputs\n#         loss = strategy.experimental_run_v2(train_step, args=(images, labels))\n        loss = strategy.run(train_step, args=(images, labels))\n        \n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    valid_loss = tf.keras.metrics.Sum()\n    \n    @tf.function \n    def val_step(img_tensor, target, teacher_forcing=True):\n        # Non-teacher-forcing val_loss is too complicated at the moment\n        loss = 0\n        batch = target.shape[0] # BATCH_SIZE//strategy.num_replicas_in_sync #\n        hidden = decoder.reset_state(batch_size= batch)\n        \n        dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * batch, 1)\n        features = encoder(img_tensor)\n      #   print(features.shape) # (BATCH_SIZE, IMG_FEAT_LEN, ENCODER_HID) = 64 100 256\n        for i in range(1, target.shape[1]):\n            predictions, hidden, _ = decoder(dec_input, features, hidden)\n            loss += loss_function(target[:, i], predictions)\n\n            # using teacher forcing\n            dec_input = tf.expand_dims(target[:, i], 1)\n\n        avg_loss = (loss / int(target.shape[1]))\n        return loss, avg_loss\n    \n\n    @tf.function\n    def cal_val_loss(val_dataset):\n        # target.shape = (64,49) = (Per Replica BATCH_SIZE?, SEQ_LEN)\n        val_num_steps = len(img_name_val) // BATCH_SIZE\n        valid_data_iter = iter(val_dataset)\n        valid_loss.reset_states()\n        \n        total_loss = 0.0\n        for ii in tf.range(val_num_steps):\n            _, per_replica_val_loss = strategy.run(val_step, args=next(valid_data_iter))\n            t_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_val_loss, axis=None)\n            total_loss += t_loss\n#             print(total_loss)\n            \n        valid_loss.update_state(total_loss/val_num_steps)\n#         tf.print('val loss',valid_loss.result().numpy())\n#             tf.print(total_loss)\n#         tf.print ('Valid Loss -- %4f' % (total_loss.eval()/val_num_steps) )\n        return total_loss/val_num_steps\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Start training!!\n\nFor TPU training, the first epoch takes around 1hour. However, after that, with `tf.dataset.cache` power, it's just 3-4 mins /epoch so we will be able to finish 10 epochs training very fast!\n\nFor TPU, our batch_size is 64x8 =512. We have 154,624 captions to be trained, this will accomplish in 302 steps / epoch. For each 50 steps, we will calculate validation loss, and print out both training and validation loss.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"with strategy.scope():\n    loss_plot = []\n    val_loss_plot = []\n    best_val_loss = 100\n    start_epoch = 0\n    num_steps = len(img_name_train) // (BATCH_SIZE)\n    start = time.time()\n    total_loss = 0\n    epoch = 0\n    train_dist_dataset = get_training_dataset()\n    \n    if USE_PREVIOUS_SAVE: # if we use pretrained checkpoint, just end the train quickly\n        print('Use prev. save weights, so run for few epochs')\n        EPOCHS,num_steps = 1,1\n        \n    num_steps_accum = num_steps\n    print(num_steps, BATCH_SIZE, num_steps*BATCH_SIZE)\n    \n    for (batch, inputs) in tqdm_notebook(enumerate(train_dist_dataset)): # by .repeat() this will indefinitely run\n            \n        if batch >= num_steps_accum:\n            epoch += 1\n            print('end of epoch ', epoch)\n            \n            loss_plot.append(total_loss / num_steps_accum)    \n            print ('Epoch {} Loss {:.6f}'.format(epoch,\n                                         total_loss/num_steps_accum))\n            print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n            \n            if num_steps_accum > num_steps*EPOCHS:\n                print('end of training!!')\n                break\n\n            num_steps_accum += num_steps\n            print('next numsteps ', num_steps_accum)\n\n                \n        # unsupported operand type(s) for +=: 'int' and 'PerReplica'\n        _, per_replica_train_loss = distributed_train_step(inputs)\n        t_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_train_loss,\n                         axis=None)\n            \n        total_loss += t_loss\n            \n        if batch % 50 == 0:\n            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, t_loss.numpy() ))\n\n            val_loss = cal_val_loss(valid_dist_dataset)\n            val_loss_plot.append(val_loss)\n            \n            print('val result', val_loss.numpy())\n            if val_loss.numpy() < best_val_loss:\n                print('update best val loss from %.4f to %.4f' % (best_val_loss, val_loss.numpy()))\n                best_val_loss = val_loss.numpy()\n                encoder.save_weights('encoder_best.h5')\n                decoder.save_weights('decoder_best.h5')\n#                 ckpt_manager.save()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if USE_PREVIOUS_SAVE:\n    %%time\n\nprint(total_loss, t_loss)\n\nplt.plot(loss_plot)\nplt.xlabel('Epochs')\nplt.ylabel('Train Loss')\nplt.title('Loss Plot')\nplt.show()\n\n# plt.plot(loss_plot)\nplt.plot(val_loss_plot)\nplt.xlabel('Epochs')\nplt.ylabel('Val Loss')\nplt.title('Loss Plot')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"1Wm83G-ZBPcC","trusted":true},"cell_type":"code","source":"if USE_PREVIOUS_SAVE:\n    '''\n    ## build construct input_layer, otherwise there is no input_layer and we cannot load weights\n    encoder.build(input_shape = (BATCH_SIZE,299,299,3))\n\n    #>> I don't know how to use model.build with multiple inputs\n    #>> So, I have to use functional API, and manually specify input tensor\n    # >> still error\n    decoder_layer = RNN_Decoder(embedding_matrix, units, vocab_size)\n    inp1 = tf.keras.layers.Input(shape=(1,))\n    inp2 = tf.keras.layers.Input(shape=(attention_features_shape,embedding_dim,))\n    inp3 = tf.keras.layers.Input(shape=(units,))\n    decoder_out = decoder_layer(inp1,inp2,inp3)\n    decoder = tf.keras.Model(inputs=[inp1,inp2,inp3],outputs=decoder_out)\n    '''\n    PATH = '/kaggle/input/image-caption-tf21-v12/'\n    with strategy.scope():\n        try:\n            encoder.load_weights(PATH+'encoder_best.h5')\n            decoder.load_weights(PATH+'decoder_best.h5') \n            # trick still fails due to layer mismatched when call(), have to construct with functional API exactly like subclass\n#             decoder.layers[-1].load_weights(PATH+'decoder_best.h5') # trick to load into layers,see decoder.summary()\n            print(1)\n        except:\n            encoder.load_weights(PATH+'encoder.h5')\n            decoder.load_weights(PATH+'decoder.h5')\n#             decoder.layers[-1].load_weights(PATH+'decoder.h5')\n            print(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder.save_weights('encoder.h5')\ndecoder.save_weights('decoder.h5')\n!ls -sh","execution_count":null,"outputs":[]},{"metadata":{"id":"xGvOcLQKghXN"},"cell_type":"markdown","source":"# 3. Predict Caption!\n\nSo now the real fun time begin!\n\n* The evaluate function is similar to the training loop, except you don't use teacher forcing here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n* Stop predicting when the model predicts the end token.\n* And store the attention weights for every time step.\n\nFirst let us define all helper functions.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def show_image(image,figsize=None,title=None):\n    \n    if figsize is not None:\n        fig = plt.figure(figsize=figsize)\n        \n    if image.ndim == 2:\n        plt.imshow(image,cmap='gray')\n    else:\n        plt.imshow(image)\n        \n    if title is not None:\n        plt.title(title)\n        \ndef show_Nimages(imgs,scale=1):\n\n    N=len(imgs)\n    fig = plt.figure(figsize=(25/scale, 16/scale))\n    for i, img in enumerate(imgs):\n        ax = fig.add_subplot(1, N, i + 1, xticks=[], yticks=[])\n        show_image(img)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"RCWpDtyNRPGs","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def evaluate(image):\n    attention_plot = np.zeros((max_length, attention_features_shape))\n    \n    try:\n        hidden = decoder.reset_state(batch_size=1)\n    except:\n        hidden = decoder.layers[-1].reset_state(batch_size=1)\n        \n    img_tensor_val = tf.expand_dims(decode_image(image), 0)\n#     print(img_tensor_val.shape)\n    features = encoder(img_tensor_val)\n#     print(features.shape)\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n    result = []\n\n    for i in range(max_length):\n        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n\n        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n\n        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n        result.append(tokenizer.index_word[predicted_id])\n\n        if tokenizer.index_word[predicted_id] == '<end>':\n            return result, attention_plot\n\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    attention_plot = attention_plot[:len(result), :]\n    return result, attention_plot","execution_count":null,"outputs":[]},{"metadata":{"id":"fD_y7PD6RPGt","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def plot_attention(image, result, attention_plot):\n    \n    bits = tf.io.read_file(image)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    \n    temp_image = np.array(image)\n\n    fig = plt.figure(figsize=(10, 10))\n\n    len_result = len(result)\n    for l in range(len_result):\n        temp_att = np.resize(attention_plot[l], (attention_viz_dim, attention_viz_dim))\n        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n        ax.set_title(result[l])\n        img = ax.imshow(temp_image)\n        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n\n    plt.tight_layout()\n    plt.show()\n    \n    return temp_image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def print_all_captions(img_list, caps, rid):\n    orig = img_list[rid]\n    for rr in range(rid-5, rid+5):\n        image_name = img_list[rr]\n        if image_name == orig:\n            real_caption = ' '.join([tokenizer.index_word[i] for i in caps[rr] if i not in [0]])\n            print ('Real Caption:', real_caption)\n    return 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.1 Prediction on Training set\n\nFirst, we can see how well our model learn from the training set. Does it overfit or not? \n\nRemember that we have 5 captions per image. To ensure that our model won't memorize some labels, we will print all captions associated with each image.\n\nNote that the ground-truth captions in Flickr are quite mostly quite non-simple. In contrast to other popular dataset like COCO where it can be much easier for our model.\n\nYou can also see on attention visualization that, by predicting each word in the image, our model most of the time focus on the correct part of the pictures. Amazing!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# captions on the train set\nimgs = []\nfor ii in range(10):\n    rid = np.random.randint(0, len(img_name_train))\n    print_all_captions(img_name_train,cap_train,rid)\n    image = img_name_train[rid]\n    result, attention_plot = evaluate(image)\n    print ('Prediction Caption:', ' '.join(result))\n    img = plot_attention(image, result, attention_plot)\n    imgs.append(img)\n    if (ii+1) %2 == 0:\n        show_Nimages(imgs)\n        imgs = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2 Prediction on Validation Set\n\nThe real test for our model is here. Is it able to generalize to unknown images ?\nLet see it by yourself. Nevertheless, from the validation loss, we got, it seems that our model can explain each valid image as well as that of each training image.\n\nOverall, if we run this prediction many times (each times with 10 random new images), we can see that roughly our model understand many images concept like \"many people\", \"computer\", \"field\", \"dog\", \"girl\", \"mouth\", etc. or even action like \"leaning\", \"sitting\", \"jump\". etc. However, it does not know the grammar so the sentence it produces many time look quite strange!\n\nWe may be able to fix this by sending this incomplete sentence to the sentence-expert like GPT-2 and make it correct for us!","execution_count":null},{"metadata":{"id":"7x8RiPHe_4qI","trusted":true},"cell_type":"code","source":"# captions on the validation set\nimgs = []\nfor ii in range(10):\n    rid = np.random.randint(0, len(img_name_val))\n    print_all_captions(img_name_val,cap_val,rid)\n    image = img_name_val[rid]\n    result, attention_plot = evaluate(image)\n    print ('Prediction Caption:', ' '.join(result))\n    img = plot_attention(image, result, attention_plot)\n    imgs.append(img)\n    if (ii+1) %2 == 0:\n        show_Nimages(imgs)\n        imgs = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import gc\n# del dataset\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"id":"Rprk3HEvZuxb"},"cell_type":"markdown","source":"## 3.3 BONUS : Prepare all prediction caps for GPT2\n\nAs mentioned in the previous section, we may further be able to improve the syntactic quality of the captions by feeding these generated sentences into GPT2. Here as a bonus below codes will save the generated sentences into files to be trained by GPT-2. Actually, I also have this GPT-2 kernel, so if anyone read through this last section, and would like to see, please let me know!!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def gen_cap(image):\n    attention_plot = np.zeros((max_length, attention_features_shape))\n    hidden = decoder.reset_state(batch_size=1)\n    img_tensor_val = tf.expand_dims(decode_image(image), 0)\n    features = encoder(img_tensor_val)\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n    result = []\n\n    for i in range(max_length):\n        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n        word = tokenizer.index_word.get(predicted_id, tokenizer.word_index['<end>'])\n        result.append(word)\n        if word == '<end>':\n            return result\n\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(img_name_train),len(img_name_val)\nSTART = 120000\nEND = 150000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# captions on the validation set\nimgs = []\nreal_caps, pred_caps = [], []\nfor rid in tqdm_notebook(range(START, END)): # 100 captions / 1:05 >> 10000 caps / 110mins >> 30,000 / 330mins+30min(preparing) = 6hours\n    image = img_name_train[rid]\n    result = gen_cap(image)\n    \n    real_caps.append(' '.join([tokenizer.index_word[i] for i in cap_train[rid] if i not in [0]]))\n    pred_caps.append(' '.join(result))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# real_caps, pred_caps\nnp.savetxt('real_caps.txt', real_caps, fmt='%s')\nnp.savetxt('pred_caps.txt', pred_caps, fmt='%s')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat real_caps.txt | head\n!cat pred_caps.txt | head","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}