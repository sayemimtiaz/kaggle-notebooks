{"cells":[{"metadata":{},"cell_type":"markdown","source":"# About this Notebook\nHey all,\nmy goal is to write a **compact guide** on feature engineering.\n**I will add new sections to this notebook, whenever I had enough time work on this notebook**, which might take some time since I am currently attending many courses at university.\n\n\n<div class=\"alert alert-danger\" role=\"alert\">\n    <h3>Feel free to <span style=\"color:red\">comment</span> if you have any suggestions   |   motivate me with an <span style=\"color:red\">upvote</span> if you like this project.</h3>\n</div>\n"},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"background-color:DodgerBlue; color:white\" >-> Topics:</h1>\n\n## 1. [Motivation and General Advices](#sec1)\n#### 1.1. [Feature-Target Relations and Monotony](#sec11)\n#### 1.2. [Pearson Correlation and Collinearity](#sec12)\n\n## 2. [Univariate Transformations on Numerical Data](#sec2)\n#### 2.1. [Scaling, Centering and Standardization](#sec21)\n#### 2.2. [Log Transformation](#sec22)\n#### 2.3. [Box-Cox Power Transformation](#sec23)\n#### 2.4. [Logit Transformation](#sec24)\n#### 2.5. [Binning with Decision Trees](#sec25) \n\n## 3. [Encode Categorical Data](#sec3)\n#### 3.1. [Label Encoding](#sec31)\n#### 3.2. [One-Hot Encoding](#sec32)\n#### 3.3. [Target-Mean Encoding](#sec33)\n\n\n## 4. [Combine interacting Features](#sec-2)\n#### 4.1. [Combine Features using Equations](#sec-21)\n#### 4.2. [Combine Features using Groupby](#sec-22)\n\n## 5. [Statistics Vocabulary and Plots](#sec-1)\n#### 5.1. [Distribution Tails](#sec51)\n#### 5.2. [The Quantile-Quantile Plot (qqplot)](#sec52)\n\n## 6. [Further Readings & Helpful Videos](#sec6)"},{"metadata":{},"cell_type":"markdown","source":"Some initial imports.."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom scipy import stats\nimport pylab \nimport matplotlib.pyplot as plt\n\ndf_heart = pd.read_csv('/kaggle/input/heart-disease-uci/heart.csv')\ndf_heart.columns\ntrestbps = df_heart['trestbps']\nchol = df_heart['chol']\ntarget_heart = df_heart['target']\ndf_health = pd.read_csv('/kaggle/input/health-insurance-cross-sell-prediction/train.csv')\nage = df_health['Age']\ntarget_health = df_health['Response']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec1\"></a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 1. Motivation and General Advices</h1>\n\nReworking features to uncover **key relationships** between features and outcome is called **Feature Engineering**. It might be helpful to have some domain in order to understand the data best.\n\nFeature Engineering relies on the resulting insights of [EDA](https://en.wikipedia.org/wiki/Exploratory_data_analysis).\nThe combination of Feature Engineering and EDA occurs in different phases of the whole modeling process, e.g. during post-modeling, based on Residual Analysis. **Residual Analysis** is the process of analysing which feature values lead to false predictions.\n\n\n### Key relationships may be between the outcome and\n* a transformation of a feature\n* a product or ratio of multiple features \n* a functional relationship between features\n* a different representation of a feature\n\n\n### It helps us to obtain a good trade-off between:\n* accuracy\n* simplicity\n* robustness\n\n### A good Mindset for Feature Engineering leads to:\n* Simplifying relationships with the target to either **binary flags** or **monotonic functions, linear if possible**.\n* Treating each transformat on as one model in an Ensemble (just like in [Pipelining](https://www.kaggle.com/milankalkenings/no-pipelines-you-are-probably-doing-it-wrong))"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec11\"></a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 1.1. Feature-Target Relations and Monotony</h1>\n\nAs mentioned above, it is best practice to create features which have a **monotonic relationship** with the target. This is due to:\n\n* The relationship is easy to interpret for Data Analysts\n* Machine Learning algorithms might converge faster\n* Machine Learning algorithms in most cases provide better predictions with features like these\n\n\n\n\nBut what exactly are these monotonic relationships? Let me give you an example:\nImagine having data about some employees of your department, and you want to find out the relationship between years of deployment(*years*) and the *salary* of the employees. You want to predict your income over the next years (which means, that your target variable is the salary)."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"years = [1, 3, 8]\nsalary = [8, 11, 14]\nvalues = list(zip(years, salary))\nnames = ['years', 'salary']\ndf = pd.DataFrame(values, columns=names)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nWe can now plot the data to take a look at the relationship between the feature *years* and the target variable *salary*:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df.plot.line('years', 'salary')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This relationship is called **(strictly) monotonic**, because the hgher the value of *years*, the higher is the value of *salary*. Note: this function of the input variable *years* and the output *salary*  is said to be (strictly) monotonically increasing, but (strictly) monotoniccally decreasing functions are also considered **(strictly) monotonic**.\n\nSo far so good. Let's consider you collected some more data and the relating relationship lookslike this:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"years = [1, 3, 8, 10, 11]\nsalary = [8, 11, 14, 14, 15]\nvalues = list(zip(years, salary))\nnames = ['years', 'salary']\ndf = pd.DataFrame(values, columns=names)\ndf.plot.line('years', 'salary')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This relationship is still called **monotonic**, despite the fact that the *salary* is the same for *years* = 8 **and** *years* = 10. The relationship is just not called **strictly monotonic** anymore. The same holds for monotonically decreasing functions."},{"metadata":{},"cell_type":"markdown","source":"Let's consider you collected even more data and this is the resulting relationship:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"years = [1, 3, 4, 8, 10, 11]\nsalary = [8, 11, 9, 14, 14, 15]\nvalues = list(zip(years, salary))\nnames = ['years', 'salary']\ndf = pd.DataFrame(values, columns=names)\ndf.plot.line('years', 'salary')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The result is a **non-monotonic** function, since the *salary* at *years* = 4 is lower than the *salary* at *years* = 3, **and** lower than the *salary* at *years* = 8. \n\nIn reality, most relationship between your features and your target will not be monotonic, and we will most likely not achieve perfectly monotonic relationships by performing the feature transformations, which we will take a look at within the next sections. \n\nHowever, we should still make them **as monotonic as possible**, and therefore, I suggest using a simple metric for **monotony**, in order to compare the monotony of the original features and our transformed features. \n\nMy very simple approach counts all monotony violations as seen in the last graphical example, and it returns $montony = 1 - \\frac{|\\text{monotony violations}|}{|\\text{samples}|}$."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def monotony(feature, target):\n    '''\n    A simple function for determining the monotony of the feature-target relationship.\n    '''\n    num_samples = len(target)\n    feature_name = feature.name\n    target_name = target.name\n    df = pd.concat([feature, target], axis=1)\n    # sorts with priority 1: feature, priority 2: target\n    df_sorted = df.sort_values([feature_name, target_name], ascending=[True, True])\n    # first target value after sorting:\n    first_target_val = df_sorted.loc[0, [target_name]].values[0] \n    \n    \n    \n    # monotoniccally increasing ? \n    def mon_inc(target_val):\n        nonlocal last_target_val\n        nonlocal violations_inc\n        if (target_val < last_target_val):\n            violations_inc = violations_inc + 1\n        last_target_val = target_val\n            \n    last_target_val = first_target_val\n    violations_inc = 0\n    df_sorted[target_name].apply(mon_inc)\n    \n     # monotoniccally decreasing ? \n    def mon_dec(target_val):\n        nonlocal last_target_val\n        nonlocal violations_dec\n        if (target_val > last_target_val):\n            violations_dec = violations_dec + 1\n        last_target_val = target_val\n            \n    last_target_val = first_target_val\n    violations_dec = 0\n    df_sorted[target_name].apply(mon_dec)\n    \n    \n    \n    # scores:\n    score_inc = 1 - round(violations_inc / num_samples,2)\n    score_dec = 1 - round(violations_dec / num_samples,2)\n    return [score_inc, score_dec, violations_inc, violations_dec]\n        \n    \n    \n    \nmonotony_metrics = monotony(df['years'], df['salary'])\nprint(f'monotonically increasing? violations: {monotony_metrics[2]}, monotony score: {monotony_metrics[0]}\\n' +\n     f'monotonically decreasing? violations: {monotony_metrics[3]}, monotony score: {monotony_metrics[1]}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the relation is an almost **monotonically increasing function**. However, this approach is pretty naive, since it makes too many assumptions about the data. These are some of the reasons, why simply checking for monotony this way might be **problematically**:\n\n* Feature values might be **non-unique** which rises multiple opportunities (E.g. should we use the mean/median target value in these cases?)\n* Functions might have many local minima and maxima, but they could still follow a monotonic trend, when smoothed. In that case, we would have very bad ratios for the decreasing and the incresing case. \n\nA better approach might include checking the **integral** values for varying areas of the feature space. \n\n\nHowever, we can also use another, yet conceptionally pretty similar indicator for approximate (linear) monotony, the so called **Pearson Correlation Coefficient**."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec12\"></a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 1.2. Pearson Correlation and Collinearity</h1>\n\n\nThe Pearson Correlation Coefficient indicates the degree at whcih two variables have a linear relationship. A perfect linear relationship is by definition a [monotonic relationship](#sec11).\nOur goal is to reach as strong correlations between every single feature and the target as possible, and as weak correlations between multiple features as possible. Whenever two features are correlated to each other, and on of them is way more correlated to the target, one should consider dropping the feature, which is less correlated to the target. Correlation between features is also known as **(multi) collinearity**.\n\n\n# Not finished yet\n\nmake sure to motivate me by upvoting this notebook \n\nand feel free to suggest any improvements in the comments, since all of us are using kaggle for studying =) "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec2\"></a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 2. Univariate Transformations on Numerical Data </h1>\n\n\nNumerical Data may..\n* be on different scales\n* follow a [long-tailed distribution](#tail). Long tails might dominate the underlying calcuations in models, which rely on polynomial calculations on the features (most linear models, SVMs and neural networks)\n* have a complex relationship with the outcome\n* be represented inefficiently, sometimes simply **normally distributed respresentations may already improve the performance**\n\n### One often wants data to be **normally distributed**, but why?\n* The whole distribution is defined by the mean(= mode = median) and the variance, which might be of importance\n* the normal distribution is [symmetric](#tail), which has some significant impact on the performance of many models \n* due to the [central limit theorem](https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_probability/BS704_Probability12.html), many machine learning models are [parametric methods](#stats), which [assume the feature values to be drawn from a normally distributed population](https://stackoverflow.com/questions/54071893/a-feature-distribution-is-nearly-normal-what-does-that-imply-for-my-ml-model) like linear regression, logistic regression, LDA, QDA and Gaussian Naive Bayes"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec21\"></a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 2.1. Scaling, Centering and Standardization </h1>\n\nWhen talking about this topic, people tend to mix the following terms:\n* [Centering](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) refers to subtracting the mean of a column\n* [Standardization](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) refers to dividing a centered feature by the standard deviation and leads to a standard deviation of one\n* [Range Scaling](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) refers to using the Minimum and the Maximum value of a feature to rescale the data on a different scale (e.g. between 0 and 1)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec211\"></a>\n## Centering and Standardization\nThe transformed numerical feature will...\n* have a *mean* of $0$\n* have a standard deviation of $1$ \n\nMost Deep Learning methods demand these properties.\n\nTherefore, the standard scaler simply subtracts the mean of the feature and divides each value by the standard deviation of the feature.\n\nLet's take a look at some data containing the age of some *pets* and whether they are *house trained* or not:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"age = [3, 4, 2, 7, 8]\nhouse_trained = [1, 1, 0, 1, 0]\nvalues = list(zip(age, house_trained))\nnames = ['age', 'house_trained']\ndf = pd.DataFrame(values, columns=names)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n# centering = subtract the mean\ncenter = StandardScaler(with_std=False)\ndf['centered'] = center.fit_transform(df['age'].values.reshape((-1,1)))\n\n# standardization = divide a centered feature by its' std\nstd = StandardScaler()\ndf['standardized'] = std.fit_transform(df['age'].values.reshape((-1,1)))\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec211\"></a>\n## Range Scaling\nThe transformed numerical feature will...\n* have a similarly formed distribution as the original feature\n* still contain outliers\n* contain values between $0$ and $1$ by default, which enables further transformations like the [Logit Transformation](#sec24)\n* be especially beneficial for models, which assume the data to be on the same scale (distance based methods like KNN), if applied to all numerical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n# self defined interval after transformation: [-1,1]\nscaler = MinMaxScaler(feature_range=(-1, 1)) \ndf['minmax']  = scaler.fit_transform(df['age'].values.reshape((-1,1)))\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec22\"></a>\n<a id=\"log\"></a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 2.2. Log Transformation </h1>\n\n$\\Large\n     x_{transformed}=ln(x)\n$\n* commonly used\n* suitable for data which approximately follows a [log-normal distribution](https://en.wikipedia.org/wiki/Log-normal_distribution)\n* is a special case of the [Box-Cox Transformation](#box-cox), so take a look at that section if you are interested in this kind of transformations\n"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(8, 10))\n\nskewness = stats.skew(chol)\ntitle = f'original, skewness = {round(skewness, 2)}'\nchol.plot(kind='hist', ax=ax1, color='red', alpha=0.5, title=title)\nchol_t = chol.apply(np.log)\nchol_t = pd.Series(chol_t)\nskewness_t = stats.skew(chol_t)\ntitle_t = f'transformed, skewness = {round(skewness_t, 2)}'\nchol_t.plot(kind='hist', ax=ax2, color='cyan', alpha=0.8, title=title_t)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec23\"></a>\n<a id=\"box-cox\"></a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 2.3. Box-Cox Power Transformation </h1>\n\n$\\Large\n     x_{transformed}=\\left\\{\\begin{array}{ll} \\frac{x^\\lambda}{\\lambda}, & x\\neq 0 \\\\\n         ln(x), & x = 0\\end{array}\\right. \n$\n  \n  \n  \n* transforms the feature into normal shape\n* the paramter $\\lambda$ might be set explicitely or might be estimated in order to obtain **as normally distributed data as possible**\n* different $\\lambda$ cover the Identity Transformation, the [Log Transformation](#log), the Square Root Transformation, the Inverse Transformation, and no-name transformations in between\n* requires the data to be positive\n* is a [variance stabilizing transformation](https://en.wikipedia.org/wiki/Variance-stabilizing_transformation) \n* improves the **validity** of Pearson **Correlation**, and thus **multicollinearity** between features\n* the [scipy implementation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html) allows us to store the best lambda. We can apply a Box-Cox transformation with that lambda value when predicting outcomes for our test data/ validation data\n\nAnother Power Transformation that might be interesting is the [Yeo Johnson transformation](https://www.stat.umn.edu/arc/yjpower.pdf). It allows the feature to contain negative values"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from statsmodels.graphics.gofplots import qqplot\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(nrows=4, ncols=1, figsize=(5, 20))\n\n# original\nskewness = stats.skew(trestbps)\ntitle = f'original, skewness = {round(skewness, 2)}'\ntrestbps.plot(kind='hist', ax=ax1, color='red', alpha=0.5, title=title)\n\n##qqplot\nqqplot(data=trestbps, dist=\"norm\", ax=ax2, line='s')\n\n# transformation\ntrestbps_t, lmbda_best = stats.boxcox(trestbps)\ntrestbps_t = pd.Series(trestbps_t)\nskewness_t = stats.skew(trestbps_t)\ntitle_t = f'transformed, skewness = {round(skewness_t, 2)}, lambda = {round(lmbda_best, 2)}'\ntrestbps_t.plot(kind='hist', ax=ax3, color='cyan', alpha=0.8, title=title_t)\n\n##qqplot\nqqplot(data=trestbps_t, dist=\"norm\", ax=ax4, line='s')\n\n\nplt.tight_layout()\nplt.savefig('.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see in the [qqplots](qqplot), the data used to be [right skewed](#tail) and matches the normal distribution way better now.\n\nhttps://en.wikipedia.org/wiki/Power_transform\n\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html\n\nhttps://www.statisticshowto.com/box-cox-transformation/\n\nhttps://en.wikipedia.org/wiki/Variance-stabilizing_transformation"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec24\"></a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 2.4. Logit Transformation </h1>\n\n\n$\\Large\n     x_{transformed}=ln(\\frac{x}{1-x})\n$\n* useful on continous data between 0 and 1, e.g. proportions, with a **sigmoid distribution** (many values with either very high or very low values)\n* transformed data provides better distinction between the the data with either very high or very low values\n* provides the log odds\n* maps the data to continous values between **-inf** and **inf**\n* the ends of the scale have a larger difference on the logit-transformed scale\n* is a [variance stabilizing transformation](https://en.wikipedia.org/wiki/Variance-stabilizing_transformation) \n* the [scipy implementation](https://docs.scipy.org/doc/scipy-0.13.0/reference/generated/scipy.special.logit.html)\n\n\nThe [Arcsine Transformation](http://strata.uga.edu/8370/rtips/proportions.html) works pretty similar and might be better in some cases, but in general, the Logit Transformation is the better choice."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom scipy.special import logit\nmms = MinMaxScaler()\ntrestbps_mms = pd.Series(mms.fit_transform(trestbps.values.reshape(-1, 1)).flatten())\n\n\nfig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(8, 10))\ntitle = f'original'\ntrestbps_mms.plot(kind='hist', ax=ax1, color='red', alpha=0.5, title=title)\n\ntrestbps_t = pd.Series(logit(trestbps_mms))\ntrestbps_t = trestbps_t.replace(np.Inf, 4) # for the plot\ntrestbps_t = trestbps_t.replace(np.NINF, -4) # for the plot\ntrestbps_t = pd.Series(trestbps_t)\ntitle_t = f'transformed'\ntrestbps_t.plot(kind='hist', ax=ax2, color='cyan', alpha=0.8, title=title_t)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://docs.scipy.org/doc/scipy-0.13.0/reference/generated/scipy.special.logit.html\n\nhttp://strata.uga.edu/8370/rtips/proportions.html\n\nhttps://www.statsdirect.com/help/data_preparation/transform_logit.htm"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec25\"></a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 2.5. Binning with Decision Trees </h1>\n\nBinning Transforms numerical features into categorical features, which we can treat like any other categorical feature. There are several approaches like taking Quantiles as bin limits, or any arbitrary numbers. For example, if your job is to find out, whether patients which are older than 60 have a higher chance to have a specific illness, it might be interesting to bin the numerical age feature using the intervals $(0, 60)$ and $[60,\\text{inf})$. One of the less self explaining methods of binning is **Binning with Decision Trees:**\n* The bins will not necessarily contain equal numbers of cases, but we might end up being lucky, which might improve the model performance even more\n* Each predicted probability will form one category\n* Since Predictions are made in the leaf nodes, and multiple leafs could make the same predictions, we end up having as many categories as leaf nodes or fewer\n* Usually improves the **correlation with the target**, due to having a [monotonical relation with the target](https://www.statisticshowto.com/monotonic-relationship/)\n* handles outliers, since they are assigned to one of the bins\n* Since Deep Decision Trees have a High [Variance](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/), this procedure might lead to overfitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_heart_failure = pd.read_csv('/kaggle/input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')\nc_p = df_heart_failure['creatinine_phosphokinase']\ntarget_heart_failure = df_heart_failure['DEATH_EVENT']\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nx = c_p.values.reshape(-1,1)\n\nhyperparameter = {'max_depth' : [1,2,4, 6, 8]}\nval = GridSearchCV(DecisionTreeClassifier(), \n                         hyperparameter, cv=5, \n                         scoring='roc_auc')\n\nval.fit(x, target_heart_failure)\ndisc_tree = val.best_estimator_\n# do this on bith, train and test set:\nx_binned = pd.Series(disc_tree.predict_proba(x)[:,1], name='x_binned')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's take a look at the Resulting categories, the tree and the correlation improvement."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"x_binned.value_counts().plot.bar(title='resulting feature categories')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.tree import export_graphviz\nimport cv2\nexport_graphviz(disc_tree, 'tree.dot', feature_names = ['c_p'])\n! dot -Tpng tree.dot -o tree.png\nimg = cv2.imread('tree.png')\nplt.figure(figsize = (18, 18))\nplt.imshow(img)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"cor = np.corrcoef(target_heart_failure, x.flatten())[0][1]\ncor_transformed = np.corrcoef(target_heart_failure, x_binned)[0][1]\nprint(f'The Pearson Correaltion between the Target and the numerical feature: {round(cor, 2)}')\nprint(f'The Pearson Correaltion between the Target and the binned feature: {round(cor_transformed, 2)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note: I didn't finetune the other parameters of the decision tree, and I used the *roc_auc* score. One should always use an appropriate score, and feel free to finetune the other hyperparameters in your models, to obtain the best possible features.\n\n\nfurther sources: \n\nhttps://www.youtube.com/watch?v=vsKNxbP8R_8?t=1388\n\nhttps://towardsdatascience.com/discretisation-using-decision-trees-21910483fa4b\n\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec3\"></a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 3. Encode Categorical Data </h1>\n\nCategorical features contain discrete, oftentimes even string values. The number of unique values a categorical feature contains is called **cardinality**. \n\nMost common machine learning models can't handle this kind of data, since they assume data to be numerical. Thus we have to use encoding methods to transform the categorical features into a suitable representation in order to utilize their *predictive abilities*."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec31\"></a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 3.1. Label Encoding </h1>\n\nLabel Encoders are probably the most simple way to encode a categorical feature. The Resulting Encoding has the following properties:\n* Endcodes the feature into one column (so we don't struggle with having too many features)\n* Consecutive integers, starting at 0. \n* Each Category shares the same integer\n* Indicates meaningfull numerical *hierarchies* and *distances* between the categories ($ 1 < 2$ and $1 = 0.5 \\cdot 2$)\n* Some models like *Linear Regression* might assign more meaning to categories with higher integer representation.\n* In most cases, it  **violates** the **key idea** of forcing features to have a monotonic relationship with the target.\n\n\nLet's apply our encodings to some data about *pets*, the *houses* in which they live and whether they are *house trained* or not."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"house_nr = [1, 3, 3, 2, 1, 1, 3, 2, 2, 2, 1, 1]\npet = ['dog', 'cat', 'dog', 'dog', 'rabbit', 'mouse', 'cat', 'rabbit', 'dog', 'cat', 'rat', 'rat']\nhouse_trained = [1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1]\nvalues = list(zip(house_nr, pet, house_trained))\nnames = ['house_nr', 'pet', 'house_trained']\ndf = pd.DataFrame(values, columns=names)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import a label encoder from sklearn\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\ndf_label = df.copy()\n\n# fit -> create parameters for the encoding (which category will be encoded as which integer?)\n# transform -> encode the feature using the parameters\n# fit_transform -> performs both, fit and transform\n# fit_transform on training data, transform on test data\ndf_label['pet'] = le.fit_transform(df_label['pet'])\ndf_label['house_nr'] = le.fit_transform(df_label['house_nr'])\ndf_label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, each *pet* has a new integer representation. Note that *house_nr* contains now values from a consequitive sequence of integers starting at 0.\n\nLet's focus on the feature *pet* and the target *house_trained*. The relationship between these features is not monotonic, as we can see in the following plot:"},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped_by_pet = df_label.groupby('pet')['house_trained'].mean()\n\nfig, ax1 = plt.subplots(nrows=1, ncols=1, figsize=(11,7))\nplt.bar(x=grouped_by_pet.index, height=grouped_by_pet.values)\nplt.xlabel('encoded pet')\nplt.ylabel('chance of being house trained')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, higher feature values don't necessarily relate to higher chances of having a higher target value, so we don't have a *monotonic relationship* with the target. This example covers a *binary classification* task. Thus, our target variable can either have the value $1$ or $0$, wheres $1$ indicates the case to belong to the so called *pisitive class*. \n\nWe could simply define a custom Label Encoding, which enforces our encoding to choose *higher values* for categories, which lead to a *higher chance* of belonging to the *positive class*. Unfortunately, we would have to create one encoding for each class in a *multiclass classification task*, and it might be very complex for regression tasks.\n\nHowever, we would still struggle with the other drawbacks as listed above, and I don't recommend you to rely on this encoding method."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec32\"></a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 3.2. One-Hot Encoder </h1>\n\nAs I already mentioned, label Encodings have some huge drawbacks. Probably the biggest drawback is, that numerical relationships like *distances* and *hierarchies* will be assumed between the categories, because these are solely encoded as discrete numbers inside the same column. Categories, on the other hand, don't have any meaningful numerical relationships.\n\nOne-Hot Encoders evade this problem and the encoded feature will have the following propoerties:\n* Each *category* is stored in a separate, new column.\n* Each of these new columns contains solely zeroes and ones.\n* ones indicate, that the case is of the respective category.\n* Each of these new columns has a *monotonic relationship with the target*.\n* There will be no numerical relationships assumed between the categories\n\nNevertheless, there are still some downsides of using this method:\n* Huge drawback: multiple new columns, which might lead to a worse model due to the [Curse of Dimensionaility](https://en.wikipedia.org/wiki/Curse_of_dimensionality)\n* In many cases, it makes sense to merge very uncommon categories into one category called *'other'* in order to evade having many columns containing very few ones. For example, you could merge all categories together, which occur in less than 5% of your *observations*."},{"metadata":{"trusted":true},"cell_type":"code","source":"# import a one-hot encoder from sklearn\nfrom sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder()\n\n# we will focus on the feature 'pet'\ndf_oh = df.copy().drop(['house_nr'], axis=1)\n\n# creates the new features\ndummies = pd.get_dummies(df_oh['pet'])\n\n# adds the new features to our dataframe\ndf_oh = pd.concat([df_oh, dummies], axis=1)\ndf_oh","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, each row contains just a single $1$ within the new columns, since each case still belongs solely to one of the categories. \n\nMoreover, the common machine learning models can handle this representation of the feature very well, since a category will be either recognized to be absent or not. Last but not least, every new column has either a positive or a negative *monotonic and linear * relationship with the target (for obvious reasons, since there are only two discrete values per column).\n\nLet's for example take a look at the relationship between the column *rabbit* and the target:"},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped_by_rabbit = df_oh.groupby('rabbit')['house_trained'].mean()\n\nfig, ax1 = plt.subplots(nrows=1, ncols=1, figsize=(11,7))\nplt.bar(x=grouped_by_rabbit.index, height=grouped_by_rabbit.values)\nplt.ylabel('chance of being house trained')\n\nplt.xticks([0, 1], ['is a rabbit', 'is not a rabbit'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, being a rabbit provides a higher chance of being house trained than not being a rabbit. This relationship very easy to interpret by the model and thus can be very beneficial. In a regression task, binary columns like this could indicate either higher or lower target values."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec33\"></a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 3.3. Target-Mean Encoding </h1>\n\n$\\Large\n     x_{transformed}=\\frac{|y=1_{X=x}|}{|X=x|} \n$\n\nThis advanced encoding method for classification problems with **binary targets** (i.e. two classes) is an elaborate alternative to the commonly used ones. One should always take a look at this representation of the categorical feature and its *predictive abilities*. \n\n* **Encoding:**  $\n    \\frac{\\text{observations of the  positive class with the respective feature value}}{\\text{observations with the respective feature value}}\n$\n* **Result:** Probability of the target value given each feature value\n* Provides a monotonic relationship between the feature and the target\n* Encodes the feature within **one column** and thus doesn't lead to huge amounts of new columns in contrast to [One-Hot Encoding](#sec31), which might be beneficial for models who can't handle huge amounts of features\n* Might decrease the [cardinality](#sec3) of the categorical feature (e.g. 2 values might be encoded as 0.5 and thus would merge into one category)\n* **Alternative for non-binary classification tasks:** create one Target-Mean encoded column for each target value and treat the respective target value as positive, and all other target values as negative\n* **Regularization:** Instead of using the whole training data to determine the encoding, use K folds and use the average encoding in the final feature representation"},{"metadata":{},"cell_type":"markdown","source":"Assume we have a dataset containing several pets from your friends and whethr they are house trained or not. We want to Target-Mean Encode the categorical feature *pet* with respect to the target *house_trained*."},{"metadata":{"trusted":true},"cell_type":"code","source":"pet = ['dog', 'cat', 'dog', 'dog', 'rabbit', 'mouse', 'cat', 'rabbit', 'dog', 'cat', 'rat', 'rat']\nhouse_trained = [1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1]\nvalues = list(zip(pet, house_trained))\nnames = ['pet', 'house_trained']\ndf = pd.DataFrame(values, columns=names)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def target_mean_encode(feature, target):\n    encoded = feature\n    for val in feature.unique():\n        ser_pure = feature[feature==val]\n        target_pure = target[ser_pure.index].sum()\n        encoded = encoded.replace(val, target_pure/len(ser_pure))\n    return encoded\n\n\n\n\n\npet_encoded = target_mean_encode(feature=df['pet'], target=df['house_trained'])\ndf_with_encoding = pd.concat([df, pet_encoded.rename('pet_encoded')], axis=1)\ndf_with_encoding","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, rats and rabbits end up having the same encoding. Thus, the encoding has a *cardinality* of 4, whereas the original feature had a *cardinality* of 5.\n\nTODO: regularization"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec-2\"></a>\n***\n\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 4. Combine Features </h1>\n\nMany machine learning algorithms utilize feature interactions and combinations implicitely. However, experience has shown that it still might be a good idea to combine features manually, because we can't rely oon our model 'doing all the work'.\n\n\nFinding and combining features could be important for standing out in kaggle competitions, but finding useful combinations might be a non-trivial problem. Suggestions from **domain experts** are oftentimes the best entry point to detecting valuable combinations.\n\nBesides relying on domain experts, we could try every possible combination of features to identify *predictive* (i.e. model improving) ones. This approach would take too much time, but at least we can already find many of the most important combinations if we follow these guidelines: \n* **effect sparsity:** the less features are part of the combination, the higher the chance for the combination to be predictive (including singletons, i.e. uncombined features). We should focus on combinations between 2 or 3 features.\n* **heredity:** the combination $(feat_1, feat_2)$ should only be considered to be predictive, if at least one of the features, $feat_1$ or $feat_2$, is already known to be predictive.[$^{1}$](#note)\n* **priority:** in most cases, the interpretability and the predictivity of a combination is better, when the original features aren't transformed ([scaled](#sec21), encoded, [log-transformed](#log)...). Thus we should create the combinations *prior* to any transformations."},{"metadata":{},"cell_type":"markdown","source":"Note: I will focus on weak heredity. Besides that, strong heredity demands both features to be predictivs"},{"metadata":{},"cell_type":"markdown","source":"As suggested by [@anashamoutni](https://www.kaggle.com/anashamoutni), [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) and otehr [Dimensionality Reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction) methods can be seen as methods for combining features as well, since they merge multiple features together in a more or less meaningful way. I will probably make a separate notebook about that tpopic as well.."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec-21\"></a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 4.1. Combine Features using Equations</h1>\n\n\nThere are multiple ways of combining features as introduced in [this video from Jeff Heaton](https://www.youtube.com/watch?v=X4pWmkxEikM). Combining features demands you to wrap your mind around the given data and to think of new features you could create by combining the given ones. A combination can be seen as a formula/equation for a new feature.\n\nProbably the most common, and most interpretable **building blocks** of these equations are:\n\n* Products of numerical features (E.g. daily cigarettes ⋅ days or area = width ⋅ length)\n* Ratios of numerical features (E.g. $\\frac{price}{gram}$)\n* Sums of numerical features (E.g. weight of passengers + weight of the transported goods)\n* Differences of numerical values (E.g. workdays - sick days). One often uses differences to subtract means\n\nThe whole equation for a new feature parent\nthat describes the likelihood of an employee going on parental leave (*parent*), given the numerical features age, and the years of employment (*empl*), as well as the binary categorical features *sex* (female), and the marital status (*married*) could look like this:\n\n$\\large\n     \\text{parent} =C (1+0.7married)(\\frac{empl}{age^5}+0.5\\frac{empl}{age^5}) \n$\n\nTake a moment to think about the way the binary categorical features affect this equation.\n\nLets apply this equation to some data to see, if it works:"},{"metadata":{"trusted":true},"cell_type":"code","source":"married = [0, 1, 0, 1, 0, 1, 0]\nempl = [2, 5, 4, 15, 2, 6, 1]\nage = [25, 27, 41, 43, 28, 29, 22]\nfemale = [0, 1,  1, 1, 1, 0, 1]\nvalues = list(zip(married, empl, age, female))\nnames = ['married', 'empl', 'age', 'female']\ndf = pd.DataFrame(values, columns=names)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c = 10_000_000 # a constant for better readability\n\ndef parent(row):\n    return  c * (1 + 0.5*row['married'])*(row['empl']/(row['age']**5) + 0.5*row['female']*row['empl']/(row['age']**5))\n    \n\n    \nparent = df.apply(parent, axis=1) # axis=1 for row-wise operation\ndf_with_parent = pd.concat([df, parent.rename('parent')], axis=1)\ndf_with_parent","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, young people who are employed for several years are very likely to go on parental leave. Moreover, the likelihood increases a lot if the employee is female and even more if the employee is married. Note, that this approach benefits from some implicit constraints. Since we are taking a look at employee data, there will be no Kid in this dataset. According to this equation, kids would be extremely likely to become parents within the next year. Moreover, this approach needs some domain knowledge as well. We cant train any (supervised) predictor to obtain the best possible equation/function to create this new feature. We have to wrap our mind around the topic and we might obtain very unpredictable features, but time-consuming creative approaches might provide valuable new features."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec-22\"></a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 4.2. Combine Features using Groupby</h1>\n\nIn some cases, it might be helpful to create a new feature based on one feature grouped by another one. Why should this be helpful?\nImagine having data about your employees' salary and their department. The salary on its own might already be an important feature, but it might be helpful to compare the salary of your employees with the salary of the other employees of the same department when it comes to finding out why some of your employees seem to be less motivated than others, even though they already have high salaries in comparison with employees from other departments. "},{"metadata":{"trusted":true},"cell_type":"code","source":"salary = [40, 42, 30, 32, 45, 44, 31, 44, 29, 33, 46, 50, 33, 39]\ndep =[1, 1, 0, 0, 2, 2, 1, 0, 1, 0, 2, 1, 2, 0]\nvalues = list(zip(salary, dep))\nnames = ['salary', 'dep']\ndf = pd.DataFrame(values, columns=names)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_per_dep = df.groupby('dep')['salary'].mean()\n\nmean_per_dep_rows = df['dep'].replace(mean_per_dep.index, mean_per_dep.values)\nsalary_per_dep = df['salary'] - mean_per_dep_rows\ndf_with_salary_per_dep = pd.concat([df, salary_per_dep.rename('salary_per_dep')], axis=1)\ndf_with_salary_per_dep","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And here we go, we now have the salary of each clerk relative to the salary of his coworkers in the same department. "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec-23\"></a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 4.3. Combine Features using Conditions</h1>\n\nOne more very common method of creating a new feature by combining the original features is using conditions. This method is especially important, if your project demands you to focus on a particular subgroup of our observations, or when you already figured out any frequent sets in your Dataset.\n\n* Use this method for commonly fulfilled conditions \n* The conditions should contain multiple features, to ecade *collinearity*\n* Construct the condition based on your project goals or frequent patterns in your data\n* You can find frequent patterns in your data using the [Apriori Algorithm](https://www.youtube.com/watch?v=guVvtZ7ZClw)\n\n\nImagine your Data Exploration reveals the fact that a particular combination of features occurs frequently with a particular outcome. In such a case, it might be interesting to create a *binary flag*, indicating the particular combination. Of course, the algorithm could find out this relationship automatically, but we can never be sure about it. \n\nFor example, we could have found out, that all young customers, who already bought multiple products from our company are very interested in our new product. Let's call these young people *young fans*. Creating such a feature could look like this:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"age = [23, 24, 51, 41, 72, 35, 21, 64, 29, 27]\nproducts_bought =[1, 4, 3, 2, 1, 5, 1, 2, 7, 4]\nvalues = list(zip(age, products_bought))\nnames = ['age', 'products_bought']\ndf = pd.DataFrame(values, columns=names)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def is_young_fan(df) : \n    if df['age'] < 30 and (df['products_bought'] > 1):\n        return 1\n    else : \n        return 0\n    \ndf['young_fan'] = df.apply(is_young_fan, axis=1)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As mentioned in the Section about [Encodings](#sec3), these binary columns have some huge benefits. Nevertheless, we should only use this method for very common relationships, in order to evade columns mostly containing zeros."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec-1\"></a>\n<a id=\"stats\"></a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 5. Statistics Vocabulary and Plots</h1>\n\n* **population:** \nthe true data one could achieve with immense effort\n* **sample:** \nthe part of the data which is available for the modeling process / the training data \n* **Population Parameter:** \nan aspect of a population (e.g. the ground truth mean of a feature)\n* **statistic:** \nan aspect of a sample (e.g. the mean of a feature in our training data) \n* **parametric statistical test:** \nmakes an assumption about the population parameters(e.g. stdent's T test, ANOVA)\n* **nonparametric statistical test:** \ndoesn't assume anything about the population parameters (e.g. chi-square)\n* **parametric models:**\nmachine learning models which make strong assumptions/have a high bias about the sample on which they are applied (e.g. they assume the data to follow a specific distribution)."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec51\"></a>\n<a id=\"tail\"></a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 5.1. Distribution Tails</h1>\n\n* **tail:** The part on the left side of the modes of the distribution is called the left tail and vice versa.\n* **heavy-tailed distribution:** A Distribution with a bigger area under the curve in the tails than a normal distribution\n* **long-tailed distribution:** A distribution with a long tail has some values which are far away from the mean of the distribution on the respective side of the mean(most long tails are also **\"thin\"** for obvious reasons). long tailed distributions contain many outliers; vice versa:**short and fat** \n* **skewness:** describes the asymmetry of a distribution\n* **negative skew:** distribution tends to have a long tail on the left side\n* **positive skew:** distribution tends to have a long tail on the right side\n* **zero skewness:** both sides of the modes balance out over all. (e.g. symmetry, or one tail is long and thin and the other is short but fat\n* [**kurtosis:**](https://corporatefinanceinstitute.com/resources/knowledge/other/kurtosis/) measures the conformity of a distributions tails with the tails of a normal distribution"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec52\"></a>\n<a id=\"qqplot\"></a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 5.2. The Quantile-Quantile Plot (qqplot)</h1>\n\n* plots the [quantiles](https://en.wikipedia.org/wiki/Quantile) (basically just the data sorted in ascending order) of two variables against each other\n* each axis represents one of these variables\n* the more similar the distributions of the variables are, the more looks the plot like the line formed by $x=y$\n* quantile plots underneath the line have lower $y$-variable values than $x$-variable values and vice versa\n* is oftentimes used to determine graphically, whether the data follows any known distribution like the normal distribution (by plotting these known distributions against the data)\n* take a look at these [typical qqplot results](https://stats.stackexchange.com/questions/101274/how-to-interpret-a-qq-plot) and the respective interpretations regarding [skewness and kurtosis](#tail)."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec6\"></a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 6. Further Readings & Helpful Videos</h1>\n\nI hope that you noticed, that I tried to add some resources for further readings. Maybe I already linked you to some of them, but I want to emphasize the importance of these sources for this notebook:\n\nhttps://www.youtube.com/watch?v=lUg0dRrlsoA\n\nhttps://www.youtube.com/watch?v=vsKNxbP8R_8\n\nhttps://www.youtube.com/watch?v=X4pWmkxEikM\n\nhttps://www.goodreads.com/book/show/45832399-feature-engineering-and-selection"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}