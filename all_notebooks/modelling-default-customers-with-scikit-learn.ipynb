{"cells":[{"source":"# Modelling default customers with scikit-learn\nAuthor : _Vincenzo Pota_\n\nDate : _16 August 2017_","metadata":{"_uuid":"2cf8d778035f7e544177cdfe8251a429bf230f58","_cell_guid":"a6361db0-434f-45ac-b1ce-030fa2762f7f"},"cell_type":"markdown"},{"source":"This notebook describes the code behind the modelling of the credit card dataset. It wants to provide some baseline solutions. Not much effort has been put into performance optimisation.","metadata":{"_uuid":"0968cd1a9406ac4943280036e70001615544ae3a","_cell_guid":"9389bbd3-12cb-4499-a7c5-d6868c3ad798"},"cell_type":"markdown"},{"source":"## Data preparation\nLoad libraries, load the dataset and drop columns we do not need","metadata":{"_uuid":"f98efe3c4568a3e818770786431bb32398a596e3","_cell_guid":"1fe6d083-7efa-41ff-9b6c-b766bd679435"},"cell_type":"markdown"},{"outputs":[],"source":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nfrom matplotlib.pylab import plt\n\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn import preprocessing, metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nimport itertools\n\ndf = pd.read_csv('../input/UCI_Credit_Card.csv')\ndf.columns = df.columns.str.lower()\ndf.drop('id', axis=1, inplace=True) # we do not need id","execution_count":null,"metadata":{"_uuid":"aefecce6dc4466105f8cdbe5e83da4d211c2819e","collapsed":true,"_cell_guid":"607586c1-4ea1-4d27-bbeb-2897ed18b2e7"},"cell_type":"code"},{"source":"Assign English words to categorical variables to make interpretation easier","metadata":{"_uuid":"325f8a87313914578da0df0bb05a3b7f4e359c09","_cell_guid":"daae6881-4b6d-4dbb-8d8b-da731c916d3d"},"cell_type":"markdown"},{"outputs":[],"source":"df['sex'] = df['sex'].map({2:'female', 1:'male'})\ndf['marriage'] = df['marriage'].map({1:'married', 2:'single', 3:'other', 0: 'other'}) \ndf['education'] = df['education'].map({1:'graduate school', 2:'university', 3:'high school', 4:'others', 5:'unknown', 6:'unknown', 0:'unknown'})\ndf['pay_0'] = df['pay_0'].astype(str) \ndf['pay_2'] = df['pay_2'].astype(str) \ndf['pay_3'] = df['pay_3'].astype(str) \ndf['pay_4'] = df['pay_4'].astype(str) \n\ndf.head()","execution_count":null,"metadata":{"_uuid":"4f0c7a52c1b42f655bec0c43f7eb21fd8406acaa","_cell_guid":"1f36b1cf-19db-4a0d-a296-8f68d39f7879"},"cell_type":"code"},{"source":"Let's transform categorical variables into discrete variables using `pd.get_dummies`. Note that `pay_X` metrics are also considered categorical. \n\nLet's create our feature vector `X` and target variable `y`. Let's rescale the metrics to the same mean and standard deviation.","metadata":{"_uuid":"a13a0bb3d50391cb450e4cdf4a3290aa0b140864","_cell_guid":"6a13ffed-9056-4625-befd-e0830c101ec8"},"cell_type":"markdown"},{"outputs":[],"source":"X = pd.get_dummies(df[df.columns[:-1]],columns=['sex','marriage','education','pay_0','pay_2','pay_3','pay_4','pay_5','pay_6'])\ny = df[df.columns[-1]]\nfeatures = X.columns\n\nscaler = preprocessing.StandardScaler()\nX = scaler.fit(X).transform(X)","execution_count":null,"metadata":{"_uuid":"a685ec1f7cb8a94bfb9bd8d8bbf9553d252632ec","collapsed":true,"_cell_guid":"cbd363ed-8ffc-4ece-9f26-dadf653f1d3b"},"cell_type":"code"},{"source":"Split into training and test set with 60% and 40%, respectively. ","metadata":{"_uuid":"c0a7170ccab716a6cc15b0f8006855d5e9562e07","_cell_guid":"3451df6d-0caa-496a-831d-8d4933e8ae66"},"cell_type":"markdown"},{"outputs":[],"source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=2)","execution_count":null,"metadata":{"_uuid":"ebb326fb98c326b6fa29c1ce94eff24064b25c27","collapsed":true,"_cell_guid":"72663fd5-09b6-4f5b-9d6e-9179a0be5375"},"cell_type":"code"},{"source":"## Modelling\nLet's try four models. The parameters of the models have not been optimised, but they work well enough for this task. ","metadata":{"_uuid":"8e4919494bfa1abc25a21f8dc5a1da6ff819d155","_cell_guid":"4513c2e7-b75c-4da6-8c95-67590147d25d"},"cell_type":"markdown"},{"outputs":[],"source":"clfs = {'GradientBoosting': GradientBoostingClassifier(learning_rate= 0.05, max_depth= 6,\n                                                        n_estimators=200, max_features = 0.3,\n                                                        min_samples_leaf = 5),\n        'LogisticRegression' : LogisticRegression(C = 1.0),\n        'GaussianNB': GaussianNB(),\n        'RandomForest': RandomForestClassifier(n_estimators=50)\n        }","execution_count":null,"metadata":{"_uuid":"61965333551535f58e6f3c6b9b0a5f4b1f9889d2","collapsed":true,"_cell_guid":"7fe0ae87-b19a-4070-aca1-8b996451dde3"},"cell_type":"code"},{"source":"The following code will:\n1. fit the four models\n2. calculate the metrics in `cols`\n3. append the results to `models_report`\n4. create a `feature_importance` dataframe for tree-based models\n4. plot a ROC curve","metadata":{"_uuid":"d8188efd8c84260be4f3e0b5dac80b835c99be2c","_cell_guid":"81bbd2cb-9591-4eee-82cb-db7e050ac1d5"},"cell_type":"markdown"},{"outputs":[],"source":"cols = ['model','matthews_corrcoef', 'roc_auc_score', 'precision_score', 'recall_score','f1_score', 'accuracy']\nmodels_report = pd.DataFrame(columns = cols)\nfeature_importance = pd.DataFrame()\n\nconf_matrix = dict()\n\nfor clf, clf_name in zip(clfs.values(), clfs.keys()):\n    clf.fit(X_train,y_train)\n    y_pred = clf.predict(X_test)\n    y_score = clf.predict_proba(X_test)[:,1]\n\n    print('Computing{}'.format(clf_name))\n    \n    if (clf_name == 'RandomForest') | (clf_name == 'GradientBoosting'):\n        tmp_fi = pd.Series(clf.feature_importances_)\n        feature_importance[clf_name] = tmp_fi\n        \n\n    tmp = pd.Series({ \n                     'model': clf_name,\n                     'roc_auc_score' : metrics.roc_auc_score(y_test, y_score),\n                     'matthews_corrcoef': metrics.matthews_corrcoef(y_test, y_pred),\n                     'precision_score': metrics.precision_score(y_test, y_pred),\n                     'recall_score': metrics.recall_score(y_test, y_pred),\n                     'f1_score': metrics.f1_score(y_test, y_pred),\n                     'accuracy': metrics.accuracy_score(y_test, y_pred)},\n                   )\n\n    models_report = models_report.append(tmp, ignore_index = True)\n\n    conf_matrix[clf_name] = pd.crosstab(y_test, y_pred, rownames=['True'], colnames= ['Predicted'], margins=False)\n\n    precision, recall, _ = metrics.precision_recall_curve(y_test, y_score)\n    fpr, tpr, _ = metrics.roc_curve(y_test, y_score, drop_intermediate = False, pos_label = 1)\n\n    plt.figure(1, figsize = (6,5))\n    plt.xlabel('fpr')\n    plt.ylabel('tpr')\n    plt.plot(fpr, tpr, label = clf_name)\n    plt.legend(prop={'size':11})\nplt.plot([0,1], [0,1], c = 'black')\nplt.show()","execution_count":null,"metadata":{"_uuid":"ed59c30d3b6f0a55ea3b862e3824ee042da9d11c","_cell_guid":"d488a208-2c66-4114-9b40-2491dc356337"},"cell_type":"code"},{"outputs":[],"source":"models_report","execution_count":null,"metadata":{"_uuid":"68333689bdd36d330525f309f1e384bb271c90eb","_cell_guid":"2560d201-e805-4c3d-a4c2-e5f34abe81a9"},"cell_type":"code"},{"source":"With an AUC of 0.78, Gradient Boosting seems to win in terms of predictive power, although all models do a pretty nice job.\n\n\nNow, let's plot the confusion matrix for all models using the function `plot_confusion_matrix` which I recycled from [here](http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html)","metadata":{"_uuid":"549e344c59b79cef9e18e36203e201fce9c5aae6","_cell_guid":"f1bceba9-5d1b-47c2-9d92-6f33bf76eea1"},"cell_type":"markdown"},{"outputs":[],"source":"def plot_confusion_matrix(cm, ax, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.set_title(title)\n    #ax.set_colorbar()\n    tick_marks = np.arange(len(classes))\n    ax.set_yticks(tick_marks)\n    ax.set_yticklabels(classes, rotation=35)\n\n    ax.set_xticks(tick_marks)\n    ax.set_xticklabels(classes, rotation=35)\n    \n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        ax.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    #plt.tight_layout()\n    ax.set_ylabel('True label')\n    ax.set_xlabel('Predicted label')","execution_count":null,"metadata":{"_uuid":"6736c2fe190fee0b2c1169e2c9a788778198098e","collapsed":true,"_cell_guid":"d6734847-174b-4094-b040-d8fbddaa7e69"},"cell_type":"code"},{"source":"And now the plots. I used gridspec to place the plots in a 2x2 format.","metadata":{"_uuid":"a8c89978facc285e8021b60a6cbb18ae0aceb347","_cell_guid":"00f164d9-ed54-40b4-b18d-d67811853ff7"},"cell_type":"markdown"},{"outputs":[],"source":"import matplotlib.gridspec as gridspec\n\nfig = plt.figure(figsize=(8, 8)) \ngs = gridspec.GridSpec(2, 2)\n\nax1 = plt.subplot(gs[0,0])\nax2 = plt.subplot(gs[0,1])\nax3 = plt.subplot(gs[1,0])\nax4 = plt.subplot(gs[1,1])\n\nfor c, ax in zip(conf_matrix.keys(), [ax1,ax2,ax3,ax4]):\n    plot_confusion_matrix(conf_matrix[c].values, ax, title = c, classes=['No default','Default'])\n\nplt.tight_layout()\nplt.show()","execution_count":null,"metadata":{"_uuid":"28a4fac36b096c7d2aefab2539f8d98b8c3db388","_cell_guid":"a3c0402b-6f38-485a-81d7-ef2f7efe9a13"},"cell_type":"code"},{"source":"## Feature importance\nLet's which feature is more important. This only works for tree-based models. Note that the results for Gradient Boosting and Random Forest are similar, but there are some differences. For example, `age` is the most predictive metric according to Random Forest, whereas the bill amount from the month before the target month is the most predictive metric according to Gradient Boosting.","metadata":{"_uuid":"811cf4f90080780f42a6297ee72b4452d980a705","_cell_guid":"b1efdeaa-f0be-494d-b439-f338929b97a7"},"cell_type":"markdown"},{"outputs":[],"source":"fi = feature_importance\n\nfi.index = features\nfi = fi.head(15) # Only take the 15 most important metrics\nfi = fi.sort_values('GradientBoosting', ascending=False)\nfi = (fi / fi.sum(axis=0)) * 100\nfi.plot.barh(title = 'Feature importances for Tree algorithms', figsize = (6,9))","execution_count":null,"metadata":{"_uuid":"df4c9ef42df78ab62e553d6a2ef5e8f863a9024f","_cell_guid":"9690d3e9-d5d3-4dd1-ab58-92df8ec97c49"},"cell_type":"code"}],"nbformat":4,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","mimetype":"text/x-python","version":"3.6.1","file_extension":".py","nbconvert_exporter":"python","name":"python"}},"nbformat_minor":1}