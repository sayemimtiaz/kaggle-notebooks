{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nThis notebook will let you build a faster and accurate CORD Search Engine using TransformersðŸ¤— - on top of the research articles. \n\nIn this notebook, we will:\n - First, structure the input data `(.json)` into easy to use `pandas.DataFrame`\n - Then, use `RoBERTa` model to build Q&A model\n - and Finally, use `Haystack` to scale QA model to thousands of documents and build a search engine.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data\n\n - We will take 25,000 articles from `pmc_json` directory and 25000 articles from `pdf_json` - So that it fits into Kaggle's compute limits.\n -  We will extract `paper_id`,`title`,`abstract`,`full_text` and put it in an easy to use `pandas.DataFrame`. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport json\nimport re\nfrom tqdm import tqdm\n\n\ndirs=[\"pmc_json\",\"pdf_json\"]\ndocs=[]\ncounts=0\nfor d in dirs:\n    print(d)\n    counts = 0\n    for file in tqdm(os.listdir(f\"../input/CORD-19-research-challenge/document_parses/{d}\")):#What is an f string?\n        file_path = f\"../input/CORD-19-research-challenge/document_parses/{d}/{file}\"\n        j = json.load(open(file_path,\"rb\"))\n        #Taking last 7 characters. it removes the 'PMC' appended to the beginning\n        #also paperid in pdf_json are guids and hard to plot in the graphs hence the substring\n        paper_id = j['paper_id']\n        paper_id = paper_id[-7:]\n        title = j['metadata']['title']\n\n        try:#sometimes there are no abstracts\n            abstract = j['abstract'][0]['text']\n        except:\n            abstract = \"\"\n            \n        full_text = \"\"\n        bib_entries = []\n        for text in j['body_text']:\n            full_text += text['text']\n                \n        docs.append([paper_id, title, abstract, full_text])\n        #comment this below block if you want to consider all files\n        #comment block start\n        counts = counts + 1\n        if(counts >= 25000):\n            break\n        #comment block end    \ndf=pd.DataFrame(docs,columns=['paper_id','title','abstract','full_text']) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 50,000 articles and columns like `paper_id`, `title`, `abstract` and `full_text`\n\nWe will be interested in `title` and `full_text` columns as these columns will be used to build the engine. Let's setup a Search Engine on top `full_text` - which contains the full content of the research papers.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Welcome Haystack!\n\n![](https://raw.githubusercontent.com/deepset-ai/haystack/master/docs/img/sketched_concepts_white.png)\n\nThe secret sauce behind scaling up is `Haystack`. It lets you scale QA models to large collections of documents! You can read more about this amazing library here https://github.com/deepset-ai/haystack\n\nFor installation: ! `pip install git+https://github.com/deepset-ai/haystack.git`\n\nBut just to give a background, there are 3 major components to `Haystack`.\n\n**Document Store**: Database storing the documents for our search. We recommend `Elasticsearch`, but have also more light-weight options for fast prototyping (SQL or In-Memory).\n\n**Retriever**: Fast, simple algorithm that identifies candidate passages from a large collection of documents. Algorithms include `TF-IDF` or `BM25`, custom `Elasticsearch` queries, and embedding-based approaches. The Retriever helps to narrow down the scope for Reader to smaller units of text where a given question could be answered.\n\n**Reader**: Powerful neural model that reads through texts in detail to find an answer. Use diverse models like `BERT`, `RoBERTa` or `XLNet` trained via `FARM` or `Transformers` on SQuAD like tasks. The Reader takes multiple passages of text as input and returns top-n answers with corresponding confidence scores. You can just load a pretrained model from Hugging Face's model hub or fine-tune it to your own domain data.\n\nAnd then there is **Finder** which glues together a **Reader** and a **Retriever** as a pipeline to provide an easy-to-use question answering interface.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# installing haystack\n\n! pip install git+https://github.com/deepset-ai/haystack.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing necessary dependencies\n\nfrom haystack import Finder\nfrom haystack.indexing.cleaning import clean_wiki_text\nfrom haystack.indexing.utils import convert_files_to_dicts, fetch_archive_from_http\nfrom haystack.reader.farm import FARMReader\nfrom haystack.reader.transformers import TransformersReader\nfrom haystack.utils import print_answers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setting up DocumentStore\n`Haystack` finds answers to queries within the documents stored in a `DocumentStore`. The current implementations of `DocumentStore` include `ElasticsearchDocumentStore`, `SQLDocumentStore`, and `InMemoryDocumentStore`.\n\nBut they recommend `ElasticsearchDocumentStore` because as it comes preloaded with features like full-text queries, BM25 retrieval, and vector storage for text embeddings.\n\nSo - Let's set up a `ElasticsearchDocumentStore`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.6.2-linux-x86_64.tar.gz -q\n! tar -xzf elasticsearch-7.6.2-linux-x86_64.tar.gz\n! chown -R daemon:daemon elasticsearch-7.6.2\n\nimport os\nfrom subprocess import Popen, PIPE, STDOUT\nes_server = Popen(['elasticsearch-7.6.2/bin/elasticsearch'],\n                   stdout=PIPE, stderr=STDOUT,\n                   preexec_fn=lambda: os.setuid(1)  # as daemon\n                  )\n# wait until ES has started\n! sleep 30","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from haystack.database.elasticsearch import ElasticsearchDocumentStore\ndocument_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"document\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once `ElasticsearchDocumentStore` is setup, we will write our documents/texts to the `DocumentStore`.\n\nWriting documents to `ElasticsearchDocumentStore` requires a format - List of dictionaries The default format here is: \n`[{\"name\": \"<some-document-name>, \"text\": \"<the-actual-text>\"},`\n`{\"name\": \"<some-document-name>, \"text\": \"<the-actual-text>\"}`\n`{\"name\": \"<some-document-name>, \"text\": \"<the-actual-text>\"}]`\n\n(Optionally: you can also add more key-value-pairs here, that will be indexed as fields in `Elasticsearch` and can be accessed later for filtering or shown in the responses of the Finder)\n\nWe will use `title` column to pass as `name` and `full_text` column to pass as the `text`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now, let's write the dicts containing documents to our DB.\ndocument_store.write_documents(df[['title', 'full_text']].rename(columns={'title':'name','full_text':'text'}).to_dict(orient='records'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's prepare Retriever, Reader, & FinderÂ¶**\n\nRetrievers help narrowing down the scope for the Reader to smaller units of text where a given question could be answered. They use some simple but fast algorithm.\n\nHere: We use Elasticsearch's default BM25 algorithm","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from haystack.retriever.sparse import ElasticsearchRetriever\nretriever = ElasticsearchRetriever(document_store=document_store)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A **Reader** scans the texts returned by retrievers in detail and extracts the k best answers. They are based on powerful, but slower deep learning models.\n\n`Haystack` currently supports Readers based on the frameworks `FARM` and `Transformers`. With both you can either load a local model or one from `Hugging Face's` model hub (https://huggingface.co/models).\n\nHere: a medium sized `RoBERTa QA` model using a Reader based on `FARM` (https://huggingface.co/deepset/roberta-base-squad2)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2-covid\", use_gpu=True, context_window_size=1000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And finally: The **Finder** sticks together reader and retriever in a pipeline to answer our actual questions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"finder = Finder(reader, retriever)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Voila! We're done!\n\nLet's see, how well our search engine works!\n\nAs a response to our query - \n - we get the `answer`\n - a 1000 words `context` around the answer \n - and the `name` of the research paper","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"question = \"What is the impact of coronavirus on babies?\"\nnumber_of_answers_to_fetch = 2\n\nprediction = finder.get_answers(question=question, top_k_retriever=10, top_k_reader=number_of_answers_to_fetch)\nprint(f\"Question: {prediction['question']}\")\nprint(\"\\n\")\nfor i in range(number_of_answers_to_fetch):\n    print(f\"#{i+1}\")\n    print(f\"Answer: {prediction['answers'][i]['answer']}\")\n    print(f\"Research Paper: {prediction['answers'][i]['meta']['name']}\")\n    print(f\"Context: {prediction['answers'][i]['context']}\")\n    print('\\n\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"question = \"What is the impact of coronavirus on pregnant women?\"\nnumber_of_answers_to_fetch = 2\n\nprediction = finder.get_answers(question=question, top_k_retriever=10, top_k_reader=number_of_answers_to_fetch)\nprint(f\"Question: {prediction['question']}\")\nprint(\"\\n\")\nfor i in range(number_of_answers_to_fetch):\n    print(f\"#{i+1}\")\n    print(f\"Answer: {prediction['answers'][i]['answer']}\")\n    print(f\"Research Paper: {prediction['answers'][i]['meta']['name']}\")\n    print(f\"Context: {prediction['answers'][i]['context']}\")\n    print('\\n\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"question = \"which organ does coronavirus impact?\"\nnumber_of_answers_to_fetch = 2\n\nprediction = finder.get_answers(question=question, top_k_retriever=10, top_k_reader=number_of_answers_to_fetch)\nprint(f\"Question: {prediction['question']}\")\nprint(\"\\n\")\nfor i in range(number_of_answers_to_fetch):\n    print(f\"#{i+1}\")\n    print(f\"Answer: {prediction['answers'][i]['answer']}\")\n    print(f\"Research Paper: {prediction['answers'][i]['meta']['name']}\")\n    print(f\"Context: {prediction['answers'][i]['context']}\")\n    print('\\n\\n')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}