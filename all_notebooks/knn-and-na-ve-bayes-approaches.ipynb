{"cells":[{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix\n\ndf = pd.read_csv(\"../input/data.csv\", index_col = 'id')\ndf.drop('Unnamed: 32',axis = 1 ,inplace = True)\ndf['diagnosis'] = df['diagnosis'].map({'M': 1, 'B':0})\nX = df.drop('diagnosis',axis = 1)\nperimeters = [x for x in df.columns if 'perimeter' in x]\nareas = [x for x in df.columns if 'area' in x]\ndf.drop(perimeters, axis = 1 ,inplace = True)\ndf.drop(areas, axis = 1 ,inplace = True)\nworst = [col for col in df.columns if col.endswith('_worst')]\ndf.drop(worst, axis = 1 ,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hello Kagglers! This work is part of my ongoing project in Predictive Analytics to classify Breast Cancer tumors: whether it's Malignant or Benign.The first part, which is the Explanatory Data Analysis and data visualization, was done [here](http://www.kaggle.com/sulianova/feature-explanation-and-eda). For PCA and Random Forest application please see [this page](http://www.kaggle.com/sulianova/pca-logistic-regression-and-random-forest).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# kNN\n\nFeatures with a larger range of values can dominate the distance metric relative to features that have a smaller range, so feature scaling is important. For continuous data, kNN uses a distance metric like Euclidean or Minkowski distance. As all features are numerical, we do not need to change default metric, which is 'minkowski'.","execution_count":null},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"y = df['diagnosis']\nX = df.drop(['diagnosis'], axis=1).values\nX_scaled = StandardScaler().fit_transform(X)\n\n#Define k-NN classifier and train on a scaled dataset\nknn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(X_scaled, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To assign the class, when neighbors don’t have the same class, we can set 'weights' parameter:\n\n1. = 'uniform' takes a simple majority vote from the neighbors. Whichever class has the greatest number of votes becomes the class for the new data point.\n2. = 'distance' takes a similar vote except gives a heavier weight to those neighbors that are closer. For example, if the neighbor is 5 units away, then weight its vote 1/5. As the neighbor gets further away, the weight gets smaller.\n\nLet's find out which parameter is better for our dataset:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_params = {'n_neighbors': range(1, 11), 'weights':['uniform', 'distance']}\n\nX_scaled_train, X_scaled_holdout, y_train, y_holdout = train_test_split(X_scaled, y, test_size=0.3,\n                                                                        random_state=17)\n\n#knn_grid.best_estimator_.predict(X_scaled_train)\nknn_grid = GridSearchCV(knn, knn_params, cv=10, n_jobs=-1, scoring='recall')\n\nknn_grid.fit(X_scaled_train, y_train)\n\nknn_grid.best_params_, knn_grid.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = knn_grid.best_estimator_.predict(X_scaled_holdout)\n\nprint (\"Accuracy Score : \",accuracy_score(y_holdout, pred))\nprint (\"Recall Score (how much of malignant tumours were predicted correctly) : \",recall_score(y_holdout, pred) )\nprint (\"Precision Score (how much of tumours, which were predicted as 'malignant', were actually 'malignant'): \",precision_score(y_holdout, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_holdout, pred)\ncm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's compare how kNN performs, if we select 3 and 5 closest neighbors:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"from matplotlib.colors import ListedColormap\n\nh = .02  # step size in the mesh\nweights ='uniform'\n# Create color maps\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n\n# we only take the first two features: radius_mean and concave points_mean. We could avoid this ugly\n# slicing by using a two-dim dataset\n\nfor n_neighbors in [3,5]:\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf = KNeighborsClassifier(n_neighbors, weights=weights)\n    clf.fit(X_scaled[:,[0,5]], y)\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = X_scaled[:,0].min() - 1, X_scaled[:,0].max() + 1\n    y_min, y_max = X_scaled[:,5].min() - 1, X_scaled[:,5].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot also the training points\n    plt.scatter(X_scaled[:, 0], X_scaled[:, 5], c=y, cmap=cmap_bold,\n                edgecolor='k', s=20)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.title(\"2-Class classification (k = %i, weights = '%s')\"\n              % (n_neighbors, weights))\n    plt.xlabel(\"radius\")\n    plt.ylabel(\"concave points\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Or we can compare score on train an test sets for different number of neighbors:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\ntest_scores = []\ntrain_scores = []\n\nfor i in range(1,15):\n\n    knn = KNeighborsClassifier(i)\n    train_scores.append(cross_val_score(knn, X_scaled_train,y_train,cv=10, scoring='recall').mean())\n    test_scores.append(cross_val_score(knn, X_scaled_holdout,y_holdout,cv=10, scoring='recall').mean())\n    \nplt.figure(figsize=(12,5))\np = sns.lineplot(range(1,15),train_scores,marker='*',label='Train Score')\np = sns.lineplot(range(1,15),test_scores,marker='o',label='Test Score')\nplt.xlabel(\"Neighbours\")\nplt.ylabel(\"Recall\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the best number of neighbours for the training data is 5.\n\n# Naïve Bayes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import Gaussian Naive Bayes model\nfrom sklearn.naive_bayes import GaussianNB\n\n#Create a Gaussian Classifier\ngnb = GaussianNB()\n\n#Train the model using the training sets\ngnb.fit(X_scaled_train,y_train)\n\n#Predict the response for test dataset\ny_pred = gnb.predict(X_scaled_holdout)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_holdout, y_pred))\nprint (\"Recall Score (how much of malignant tumours were predicted correctly) : \",recall_score(y_holdout, y_pred) )\nprint (\"Precision Score (how much of tumours, which were predicted as 'malignant', were actually 'malignant'): \",precision_score(y_holdout, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Naïve Bayes slightly imporved the accuracy and precision scores.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}