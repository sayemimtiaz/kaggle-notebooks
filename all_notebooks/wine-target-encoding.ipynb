{"cells":[{"metadata":{},"cell_type":"markdown","source":"#This notebook is an exercise in the Feature Engineering course by Ryan Holbrook.\n\nhttps://www.kaggle.com/ryanholbrook/exercise-target-encoding"},{"metadata":{},"cell_type":"markdown","source":"![](https://miro.medium.com/max/2968/1*dFSpzDQJK50ur8ilFN59mQ.png)towardsdatascience.com"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBRegressor\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas\n\ndf = pandas.read_csv('/kaggle/input/cusersmarildownloadswinecsv/wine.csv',  delimiter=';')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def score_dataset(X, y, model=XGBRegressor()):\n    # Label encoding for categoricals\n    for colname in X.select_dtypes([\"category\", \"object\"]):\n        X[colname], _ = X[colname].factorize()\n    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n    score = cross_val_score(\n        model, X, y, cv=5, scoring=\"neg_mean_squared_log_error\",\n    )\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Encoding Categorical Features\n\nFirst you'll need to choose which features you want to apply a target encoding to. Categorical features with a large number of categories are often good candidates. Run this cell to see how many categories each categorical feature in the Portugal Wine dataset has."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.select_dtypes([\"object\"]).nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To see how many times a category occurs in the dataset, you can use the value_counts method. This cell shows the counts for alcohol, but you might want to consider others as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"alcohol\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Choose Features for Encoding\n\nAlmost any of the nominal features would be worth trying because of the prevalence of rare categories."},{"metadata":{},"cell_type":"markdown","source":"Apply a target encoding to your choice of feature. To avoid overfitting, we need to fit the encoder on data heldout from the training set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding split\nX_encode = df.sample(frac=0.20, random_state=0)\ny_encode = X_encode.pop(\"quality\")\n\n# Training split\nX_pretrain = df.drop(X_encode.index)\ny_train = X_pretrain.pop(\"quality\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Apply M-Estimate Encoding\n\nApply a target encoding to your choice of categorical features. Also choose a value for the smoothing parameter m (any value is okay for a correct answer)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Choose a set of features to encode and a value for m\nencoder = MEstimateEncoder(\n    cols=[\"alcohol\"],\n    m=1.0,\n)\n\n\n# Fit the encoder on the encoding split\nencoder.fit(X_encode, y_encode)\n\n# Encode the training split\nX_train = encoder.transform(X_pretrain, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#See how the encoded feature compares to the target\n\nfeature = encoder.cols\n\nplt.figure(dpi=90)\nax = sns.distplot(y_train, kde=True, hist=False)\nax = sns.distplot(X_train[feature], color='r', ax=ax, hist=True, kde=False, norm_hist=True)\nax.set_xlabel(\"quality\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#This cell will show you the score of the encoded set compared to the original set."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.copy()\ny = X.pop(\"quality\")\nscore_base = score_dataset(X, y)\nscore_new = score_dataset(X_train, y_train)\n\nprint(f\"Baseline Score: {score_base:.4f} RMSLE\")\nprint(f\"Score with Encoding: {score_new:.4f} RMSLE\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Depending on which feature or features you chose, you may have ended up with a score significantly worse than the baseline. In that case, it's likely the extra information gained by the encoding couldn't make up for the loss of data used for the encoding."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Try experimenting with the smoothing parameter m\n# Try 0, 1, 5, 50\nm = 0\n\nX = df.copy()\ny = X.pop('quality')\n\n# Create an uninformative feature\nX[\"Count\"] = range(len(X))\nX[\"Count\"][1] = 0  # actually need one duplicate value to circumvent error-checking in MEstimateEncoder\n\n# fit and transform on the same dataset\nencoder = MEstimateEncoder(cols=\"Count\", m=m)\nX = encoder.fit_transform(X, y)\n\n# Results\nscore =  score_dataset(X, y)\nprint(f\"Score: {score:.4f} RMSLE\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Really? 0.0000 RMSLE. Why did I get zero?"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(dpi=90)\nax = sns.distplot(y, kde=True, hist=False)\nax = sns.distplot(X[\"Count\"], color='r', ax=ax, hist=True, kde=False, norm_hist=True)\nax.set_xlabel(\"quality\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since Count never has any duplicate values, the mean-encoded Count is essentially an exact copy of the target. In other words, mean-encoding turned a completely meaningless feature into a perfect feature.\n\nNow, the only reason this worked is because we trained XGBoost on the same set we used to train the encoder. If we had used a hold-out set instead, none of this \"fake\" encoding would have transferred to the training data.\n\nThe lesson is that when using a target encoder it's very important to use separate data sets for training the encoder and training the model. Otherwise the results can be very disappointing!\n\nAs I got 0.0000 RMSLE (input 19)\n\nhttps://www.kaggle.com/ryanholbrook/exercise-target-encoding"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Code by Olga Belitskaya https://www.kaggle.com/olgabelitskaya/sequential-data/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https://fonts.googleapis.com/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';</style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s</h1>\"\"\"%string))\n    \n    \ndhtml('Be patient. Mar√≠lia Prata, @mpwolke Was here' )","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}