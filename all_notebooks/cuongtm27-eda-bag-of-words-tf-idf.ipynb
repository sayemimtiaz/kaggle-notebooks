{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"random_state_split = 2\nDropout_num = 0\nlearning_rate = 6e-6\nvalid = 0.2\nepochs_num = 3\nbatch_size_num = 16\ntarget_corrected = False\ntarget_big_corrected = False","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:53:55.271895Z","iopub.execute_input":"2021-06-10T22:53:55.272168Z","iopub.status.idle":"2021-06-10T22:53:55.278591Z","shell.execute_reply.started":"2021-06-10T22:53:55.272121Z","shell.execute_reply":"2021-06-10T22:53:55.27741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\n\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\n\nfrom wordcloud import WordCloud\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nfrom collections import defaultdict\nfrom collections import Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\n\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\n\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam\n\nimport torch\n\nimport warnings\nwarnings.simplefilter('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:53:55.282498Z","iopub.execute_input":"2021-06-10T22:53:55.282907Z","iopub.status.idle":"2021-06-10T22:54:04.03682Z","shell.execute_reply.started":"2021-06-10T22:53:55.282845Z","shell.execute_reply":"2021-06-10T22:54:04.036145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Download data <a class=\"anchor\" id=\"3\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"tweet= pd.read_csv('../input/nlp-getting-started/train.csv')\ntest=pd.read_csv('../input/nlp-getting-started/test.csv')\nsubmission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:04.038564Z","iopub.execute_input":"2021-06-10T22:54:04.038806Z","iopub.status.idle":"2021-06-10T22:54:04.112648Z","shell.execute_reply.started":"2021-06-10T22:54:04.038764Z","shell.execute_reply":"2021-06-10T22:54:04.111887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # From https://www.kaggle.com/wrrosa/keras-bert-using-tfhub-modified-train-data - \n# # author of this kernel read tweets in training data and figure out that some of them have errors:\n# ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\n# tweet.loc[tweet['id'].isin(ids_with_target_error),'target'] = 0\n# tweet[tweet['id'].isin(ids_with_target_error)]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:04.113914Z","iopub.execute_input":"2021-06-10T22:54:04.114212Z","iopub.status.idle":"2021-06-10T22:54:04.12945Z","shell.execute_reply.started":"2021-06-10T22:54:04.114163Z","shell.execute_reply":"2021-06-10T22:54:04.127096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('There are {} rows and {} columns in train'.format(tweet.shape[0],tweet.shape[1]))\nprint('There are {} rows and {} columns in test'.format(test.shape[0],test.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:04.131501Z","iopub.execute_input":"2021-06-10T22:54:04.131839Z","iopub.status.idle":"2021-06-10T22:54:04.138584Z","shell.execute_reply.started":"2021-06-10T22:54:04.131774Z","shell.execute_reply":"2021-06-10T22:54:04.13753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweet.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:04.144341Z","iopub.execute_input":"2021-06-10T22:54:04.144628Z","iopub.status.idle":"2021-06-10T22:54:04.17578Z","shell.execute_reply.started":"2021-06-10T22:54:04.144577Z","shell.execute_reply":"2021-06-10T22:54:04.174484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. EDA <a class=\"anchor\" id=\"4\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"Thanks to:\n* https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove\n* https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial\n* https://www.kaggle.com/itratrahman/nlp-tutorial-using-python","metadata":{}},{"cell_type":"markdown","source":"### Class distribution","metadata":{}},{"cell_type":"markdown","source":"Before we begin with anything else, let's check the class distribution.","metadata":{}},{"cell_type":"code","source":"# extracting the number of examples of each class\nReal_len = tweet[tweet['target'] == 1].shape[0]\nNot_len = tweet[tweet['target'] == 0].shape[0]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:04.17913Z","iopub.execute_input":"2021-06-10T22:54:04.179561Z","iopub.status.idle":"2021-06-10T22:54:04.189259Z","shell.execute_reply.started":"2021-06-10T22:54:04.179377Z","shell.execute_reply":"2021-06-10T22:54:04.188197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# bar plot of the 3 classes\nplt.rcParams['figure.figsize'] = (7, 5)\nplt.bar(10,Real_len,3, label=\"Real\", color='blue')\nplt.bar(15,Not_len,3, label=\"Not\", color='red')\nplt.legend()\nplt.ylabel('Number of examples')\nplt.title('Propertion of examples')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:04.19109Z","iopub.execute_input":"2021-06-10T22:54:04.19161Z","iopub.status.idle":"2021-06-10T22:54:04.441504Z","shell.execute_reply.started":"2021-06-10T22:54:04.191359Z","shell.execute_reply":"2021-06-10T22:54:04.440711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Number of characters in tweets","metadata":{}},{"cell_type":"code","source":"def length(text):    \n    '''a function which returns the length of text'''\n    return len(text)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:04.445776Z","iopub.execute_input":"2021-06-10T22:54:04.448079Z","iopub.status.idle":"2021-06-10T22:54:04.454964Z","shell.execute_reply.started":"2021-06-10T22:54:04.448016Z","shell.execute_reply":"2021-06-10T22:54:04.454238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweet['length'] = tweet['text'].apply(length)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:04.459658Z","iopub.execute_input":"2021-06-10T22:54:04.462523Z","iopub.status.idle":"2021-06-10T22:54:04.47931Z","shell.execute_reply.started":"2021-06-10T22:54:04.462457Z","shell.execute_reply":"2021-06-10T22:54:04.478567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (18.0, 6.0)\nbins = 150\nplt.hist(tweet[tweet['target'] == 0]['length'], alpha = 0.6, bins=bins, label='Not')\nplt.hist(tweet[tweet['target'] == 1]['length'], alpha = 0.8, bins=bins, label='Real')\nplt.xlabel('length')\nplt.ylabel('numbers')\nplt.legend(loc='upper right')\nplt.xlim(0,150)\nplt.grid()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:04.483649Z","iopub.execute_input":"2021-06-10T22:54:04.485896Z","iopub.status.idle":"2021-06-10T22:54:05.365711Z","shell.execute_reply.started":"2021-06-10T22:54:04.485836Z","shell.execute_reply":"2021-06-10T22:54:05.364379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=tweet[tweet['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='blue')\nax1.set_title('disaster tweets')\ntweet_len=tweet[tweet['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='red')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:05.36735Z","iopub.execute_input":"2021-06-10T22:54:05.367759Z","iopub.status.idle":"2021-06-10T22:54:06.213245Z","shell.execute_reply.started":"2021-06-10T22:54:05.367708Z","shell.execute_reply":"2021-06-10T22:54:06.205855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distribution of both seems to be almost same.120 t0 140 characters in a tweet are the most common among both.","metadata":{}},{"cell_type":"markdown","source":"### Number of words in a tweet","metadata":{}},{"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=tweet[tweet['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='blue')\nax1.set_title('disaster tweets')\ntweet_len=tweet[tweet['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='red')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Words in a tweet')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:06.214369Z","iopub.execute_input":"2021-06-10T22:54:06.214627Z","iopub.status.idle":"2021-06-10T22:54:06.893309Z","shell.execute_reply.started":"2021-06-10T22:54:06.214585Z","shell.execute_reply":"2021-06-10T22:54:06.892419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  Average word length in a tweet","metadata":{}},{"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword=tweet[tweet['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='blue')\nax1.set_title('disaster')\nword=tweet[tweet['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='red')\nax2.set_title('Not disaster')\nfig.suptitle('Average word length in each tweet')","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:06.894829Z","iopub.execute_input":"2021-06-10T22:54:06.896363Z","iopub.status.idle":"2021-06-10T22:54:07.755099Z","shell.execute_reply.started":"2021-06-10T22:54:06.89508Z","shell.execute_reply":"2021-06-10T22:54:07.754114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_corpus(target):\n    corpus=[]\n    \n    for x in tweet[tweet['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:07.759682Z","iopub.execute_input":"2021-06-10T22:54:07.760115Z","iopub.status.idle":"2021-06-10T22:54:07.769925Z","shell.execute_reply.started":"2021-06-10T22:54:07.759937Z","shell.execute_reply":"2021-06-10T22:54:07.769142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_corpus_df(tweet, target):\n    corpus=[]\n    \n    for x in tweet[tweet['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:07.774588Z","iopub.execute_input":"2021-06-10T22:54:07.777117Z","iopub.status.idle":"2021-06-10T22:54:07.785085Z","shell.execute_reply.started":"2021-06-10T22:54:07.777058Z","shell.execute_reply":"2021-06-10T22:54:07.78406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Common stopwords in tweets","metadata":{}},{"cell_type":"markdown","source":"First we  will analyze tweets with class 0.","metadata":{}},{"cell_type":"code","source":"corpus=create_corpus(0)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:07.789938Z","iopub.execute_input":"2021-06-10T22:54:07.792588Z","iopub.status.idle":"2021-06-10T22:54:07.951036Z","shell.execute_reply.started":"2021-06-10T22:54:07.792532Z","shell.execute_reply":"2021-06-10T22:54:07.950336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# displaying the stopwords\nnp.array(stop)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:07.952467Z","iopub.execute_input":"2021-06-10T22:54:07.952729Z","iopub.status.idle":"2021-06-10T22:54:07.960003Z","shell.execute_reply.started":"2021-06-10T22:54:07.952686Z","shell.execute_reply":"2021-06-10T22:54:07.958888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (18.0, 6.0)\nx,y=zip(*top)\nplt.bar(x,y)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:07.961636Z","iopub.execute_input":"2021-06-10T22:54:07.962186Z","iopub.status.idle":"2021-06-10T22:54:08.277312Z","shell.execute_reply.started":"2021-06-10T22:54:07.961924Z","shell.execute_reply":"2021-06-10T22:54:08.276381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now,we will analyze tweets with class 1.","metadata":{}},{"cell_type":"code","source":"corpus=create_corpus(1)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n    \n\nplt.rcParams['figure.figsize'] = (18.0, 6.0)\nx,y=zip(*top)\nplt.bar(x,y)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:08.282007Z","iopub.execute_input":"2021-06-10T22:54:08.284438Z","iopub.status.idle":"2021-06-10T22:54:08.621016Z","shell.execute_reply.started":"2021-06-10T22:54:08.282256Z","shell.execute_reply":"2021-06-10T22:54:08.620224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In both of them,\"the\" dominates which is followed by \"a\" in class 0 and \"in\" in class 1.","metadata":{}},{"cell_type":"markdown","source":"### Analyzing punctuations","metadata":{}},{"cell_type":"markdown","source":"First let's check tweets indicating real disaster.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,5))\ncorpus=create_corpus(1)\n\ndic=defaultdict(int)\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:08.624359Z","iopub.execute_input":"2021-06-10T22:54:08.627502Z","iopub.status.idle":"2021-06-10T22:54:09.029176Z","shell.execute_reply.started":"2021-06-10T22:54:08.627443Z","shell.execute_reply":"2021-06-10T22:54:09.028228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now,we will move on to class 0.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,5))\ncorpus=create_corpus(0)\ndic=defaultdict(int)\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y,color='green')","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:09.033679Z","iopub.execute_input":"2021-06-10T22:54:09.036031Z","iopub.status.idle":"2021-06-10T22:54:09.452789Z","shell.execute_reply.started":"2021-06-10T22:54:09.03393Z","shell.execute_reply":"2021-06-10T22:54:09.452086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Common words","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,5))\ncounter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:09.454067Z","iopub.execute_input":"2021-06-10T22:54:09.454454Z","iopub.status.idle":"2021-06-10T22:54:09.488893Z","shell.execute_reply.started":"2021-06-10T22:54:09.454409Z","shell.execute_reply":"2021-06-10T22:54:09.488089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.barplot(x=y,y=x)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:09.490157Z","iopub.execute_input":"2021-06-10T22:54:09.490616Z","iopub.status.idle":"2021-06-10T22:54:09.775754Z","shell.execute_reply.started":"2021-06-10T22:54:09.490569Z","shell.execute_reply":"2021-06-10T22:54:09.775043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lot of cleaning needed !","metadata":{}},{"cell_type":"markdown","source":"### N-gram analysis","metadata":{}},{"cell_type":"markdown","source":"we will do a bigram (n=2) analysis over the tweets. Let's check the most common bigrams in tweets.","metadata":{}},{"cell_type":"code","source":"def get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:09.77697Z","iopub.execute_input":"2021-06-10T22:54:09.777382Z","iopub.status.idle":"2021-06-10T22:54:09.784779Z","shell.execute_reply.started":"2021-06-10T22:54:09.777335Z","shell.execute_reply":"2021-06-10T22:54:09.78405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,5))\ntop_tweet_bigrams=get_top_tweet_bigrams(tweet['text'])[:10]\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:09.785921Z","iopub.execute_input":"2021-06-10T22:54:09.786384Z","iopub.status.idle":"2021-06-10T22:54:10.839135Z","shell.execute_reply.started":"2021-06-10T22:54:09.786336Z","shell.execute_reply":"2021-06-10T22:54:10.838349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Data Cleaning <a class=\"anchor\" id=\"5\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"Thanks to https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove","metadata":{}},{"cell_type":"code","source":"df=pd.concat([tweet,test])\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:10.843611Z","iopub.execute_input":"2021-06-10T22:54:10.845885Z","iopub.status.idle":"2021-06-10T22:54:10.866216Z","shell.execute_reply.started":"2021-06-10T22:54:10.845829Z","shell.execute_reply":"2021-06-10T22:54:10.86548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing urls","metadata":{}},{"cell_type":"code","source":"example=\"New competition launched :https://www.kaggle.com/c/nlp-getting-started\"","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:10.870387Z","iopub.execute_input":"2021-06-10T22:54:10.872624Z","iopub.status.idle":"2021-06-10T22:54:10.878488Z","shell.execute_reply.started":"2021-06-10T22:54:10.872568Z","shell.execute_reply":"2021-06-10T22:54:10.877677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\nremove_URL(example)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:10.883636Z","iopub.execute_input":"2021-06-10T22:54:10.886664Z","iopub.status.idle":"2021-06-10T22:54:10.898781Z","shell.execute_reply.started":"2021-06-10T22:54:10.885694Z","shell.execute_reply":"2021-06-10T22:54:10.897796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text']=df['text'].apply(lambda x : remove_URL(x))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:10.902534Z","iopub.execute_input":"2021-06-10T22:54:10.906124Z","iopub.status.idle":"2021-06-10T22:54:10.955505Z","shell.execute_reply.started":"2021-06-10T22:54:10.906065Z","shell.execute_reply":"2021-06-10T22:54:10.954942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing HTML tags","metadata":{}},{"cell_type":"code","source":"example = \"\"\"<div>\n<h1>Real or Fake</h1>\n<p>Kaggle </p>\n<a href=\"https://www.kaggle.com/c/nlp-getting-started\">getting started</a>\n</div>\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:10.957187Z","iopub.execute_input":"2021-06-10T22:54:10.957643Z","iopub.status.idle":"2021-06-10T22:54:10.961555Z","shell.execute_reply.started":"2021-06-10T22:54:10.957436Z","shell.execute_reply":"2021-06-10T22:54:10.960886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\nprint(remove_html(example))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:10.962764Z","iopub.execute_input":"2021-06-10T22:54:10.963235Z","iopub.status.idle":"2021-06-10T22:54:10.972255Z","shell.execute_reply.started":"2021-06-10T22:54:10.963185Z","shell.execute_reply":"2021-06-10T22:54:10.971494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text']=df['text'].apply(lambda x : remove_html(x))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:10.980086Z","iopub.execute_input":"2021-06-10T22:54:10.980293Z","iopub.status.idle":"2021-06-10T22:54:11.001941Z","shell.execute_reply.started":"2021-06-10T22:54:10.980254Z","shell.execute_reply":"2021-06-10T22:54:11.001348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing Emojis","metadata":{}},{"cell_type":"code","source":"# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nremove_emoji(\"Omg another Earthquake ðŸ˜”ðŸ˜”\")","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:11.003813Z","iopub.execute_input":"2021-06-10T22:54:11.004257Z","iopub.status.idle":"2021-06-10T22:54:11.016754Z","shell.execute_reply.started":"2021-06-10T22:54:11.004137Z","shell.execute_reply":"2021-06-10T22:54:11.016026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text']=df['text'].apply(lambda x: remove_emoji(x))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:11.018095Z","iopub.execute_input":"2021-06-10T22:54:11.018519Z","iopub.status.idle":"2021-06-10T22:54:11.09526Z","shell.execute_reply.started":"2021-06-10T22:54:11.018338Z","shell.execute_reply":"2021-06-10T22:54:11.094643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing punctuations","metadata":{}},{"cell_type":"code","source":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\nexample=\"I am a #king\"\nprint(remove_punct(example))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:11.096658Z","iopub.execute_input":"2021-06-10T22:54:11.097131Z","iopub.status.idle":"2021-06-10T22:54:11.103237Z","shell.execute_reply.started":"2021-06-10T22:54:11.097035Z","shell.execute_reply":"2021-06-10T22:54:11.102448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text']=df['text'].apply(lambda x : remove_punct(x))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:11.104415Z","iopub.execute_input":"2021-06-10T22:54:11.104951Z","iopub.status.idle":"2021-06-10T22:54:11.169561Z","shell.execute_reply.started":"2021-06-10T22:54:11.104902Z","shell.execute_reply":"2021-06-10T22:54:11.169013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. WordCloud <a class=\"anchor\" id=\"6\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"Thanks to https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial","metadata":{}},{"cell_type":"markdown","source":"### Real Disaster","metadata":{}},{"cell_type":"code","source":"corpus_new1=create_corpus_df(df,1)\nlen(corpus_new1)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:11.171083Z","iopub.execute_input":"2021-06-10T22:54:11.171512Z","iopub.status.idle":"2021-06-10T22:54:11.191799Z","shell.execute_reply.started":"2021-06-10T22:54:11.171342Z","shell.execute_reply":"2021-06-10T22:54:11.190862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus_new1[:10]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:11.193214Z","iopub.execute_input":"2021-06-10T22:54:11.193619Z","iopub.status.idle":"2021-06-10T22:54:11.199013Z","shell.execute_reply.started":"2021-06-10T22:54:11.193455Z","shell.execute_reply":"2021-06-10T22:54:11.198241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generating the wordcloud with the values under the category dataframe\nplt.figure(figsize=(12,8))\nword_cloud = WordCloud(\n                          background_color='black',\n                          max_font_size = 80\n                         ).generate(\" \".join(corpus_new1[:50]))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:11.20037Z","iopub.execute_input":"2021-06-10T22:54:11.200877Z","iopub.status.idle":"2021-06-10T22:54:11.415783Z","shell.execute_reply.started":"2021-06-10T22:54:11.200831Z","shell.execute_reply":"2021-06-10T22:54:11.415042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Not Disaster","metadata":{}},{"cell_type":"code","source":"corpus_new0=create_corpus_df(df,0)\nlen(corpus_new0)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:11.417065Z","iopub.execute_input":"2021-06-10T22:54:11.417337Z","iopub.status.idle":"2021-06-10T22:54:11.451685Z","shell.execute_reply.started":"2021-06-10T22:54:11.417291Z","shell.execute_reply":"2021-06-10T22:54:11.450877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus_new0[:10]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:11.453101Z","iopub.execute_input":"2021-06-10T22:54:11.453581Z","iopub.status.idle":"2021-06-10T22:54:11.459167Z","shell.execute_reply.started":"2021-06-10T22:54:11.453533Z","shell.execute_reply":"2021-06-10T22:54:11.458449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generating the wordcloud with the values under the category dataframe\nplt.figure(figsize=(12,8))\nword_cloud = WordCloud(\n                          background_color='black',\n                          max_font_size = 80\n                         ).generate(\" \".join(corpus_new0[:50]))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:11.460534Z","iopub.execute_input":"2021-06-10T22:54:11.461181Z","iopub.status.idle":"2021-06-10T22:54:11.652038Z","shell.execute_reply.started":"2021-06-10T22:54:11.461131Z","shell.execute_reply":"2021-06-10T22:54:11.651273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:11.653449Z","iopub.execute_input":"2021-06-10T22:54:11.653936Z","iopub.status.idle":"2021-06-10T22:54:11.671865Z","shell.execute_reply.started":"2021-06-10T22:54:11.653888Z","shell.execute_reply":"2021-06-10T22:54:11.671164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Bag of Words Counts <a class=\"anchor\" id=\"7\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"Thanks to https://github.com/hundredblocks/concrete_NLP_tutorial/blob/master/NLP_notebook.ipynb","metadata":{}},{"cell_type":"code","source":"def cv(data):\n    count_vectorizer = CountVectorizer()\n\n    emb = count_vectorizer.fit_transform(data)\n\n    return emb, count_vectorizer\n\nlist_corpus = df[\"text\"].tolist()\nlist_labels = df[\"target\"].tolist()\n\nX_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2, \n                                                                                random_state=random_state_split)\n\nX_train_counts, count_vectorizer = cv(X_train)\nX_test_counts = count_vectorizer.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:11.673328Z","iopub.execute_input":"2021-06-10T22:54:11.673838Z","iopub.status.idle":"2021-06-10T22:54:11.939349Z","shell.execute_reply.started":"2021-06-10T22:54:11.673778Z","shell.execute_reply":"2021-06-10T22:54:11.938531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing the embeddings","metadata":{}},{"cell_type":"code","source":"def plot_LSA(test_data, test_labels, savepath=\"PCA_demo.csv\", plot=True):\n        lsa = TruncatedSVD(n_components=2)\n        lsa.fit(test_data)\n        lsa_scores = lsa.transform(test_data)\n        color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n        color_column = [color_mapper[label] for label in test_labels]\n        colors = ['orange','blue']\n        if plot:\n            plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n            orange_patch = mpatches.Patch(color='orange', label='Not')\n            blue_patch = mpatches.Patch(color='blue', label='Real')\n            plt.legend(handles=[orange_patch, blue_patch], prop={'size': 30})\n\nfig = plt.figure(figsize=(16, 16))          \nplot_LSA(X_train_counts, y_train)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:11.940676Z","iopub.execute_input":"2021-06-10T22:54:11.941042Z","iopub.status.idle":"2021-06-10T22:54:12.883635Z","shell.execute_reply.started":"2021-06-10T22:54:11.940978Z","shell.execute_reply":"2021-06-10T22:54:12.882895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These embeddings don't look very cleanly separated. Let's see if we can still fit a useful model on them.","metadata":{}},{"cell_type":"markdown","source":"## 8. TF IDF <a class=\"anchor\" id=\"8\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"def tfidf(data):\n    tfidf_vectorizer = TfidfVectorizer()\n\n    train = tfidf_vectorizer.fit_transform(data)\n\n    return train, tfidf_vectorizer\n\nX_train_tfidf, tfidf_vectorizer = tfidf(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:12.884815Z","iopub.execute_input":"2021-06-10T22:54:12.885217Z","iopub.status.idle":"2021-06-10T22:54:13.139006Z","shell.execute_reply.started":"2021-06-10T22:54:12.885169Z","shell.execute_reply":"2021-06-10T22:54:13.138331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(16, 16))          \nplot_LSA(X_train_tfidf, y_train)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:13.140556Z","iopub.execute_input":"2021-06-10T22:54:13.141091Z","iopub.status.idle":"2021-06-10T22:54:14.076199Z","shell.execute_reply.started":"2021-06-10T22:54:13.141011Z","shell.execute_reply":"2021-06-10T22:54:14.075552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### tokenizer","metadata":{}},{"cell_type":"code","source":"def create_corpus_new(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet)]\n        corpus.append(words)\n    return corpus   ","metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:49:25.817439Z","iopub.execute_input":"2021-06-10T23:49:25.817731Z","iopub.status.idle":"2021-06-10T23:49:25.823454Z","shell.execute_reply.started":"2021-06-10T23:49:25.817683Z","shell.execute_reply":"2021-06-10T23:49:25.822279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus=create_corpus_new(df)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:49:48.663567Z","iopub.execute_input":"2021-06-10T23:49:48.663873Z","iopub.status.idle":"2021-06-10T23:49:51.062925Z","shell.execute_reply.started":"2021-06-10T23:49:48.663821Z","shell.execute_reply":"2021-06-10T23:49:51.061724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_dict={}\nwith open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word = values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:50:13.716642Z","iopub.execute_input":"2021-06-10T23:50:13.716945Z","iopub.status.idle":"2021-06-10T23:50:29.94856Z","shell.execute_reply.started":"2021-06-10T23:50:13.716891Z","shell.execute_reply":"2021-06-10T23:50:29.947813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:32.720622Z","iopub.execute_input":"2021-06-10T22:54:32.720905Z","iopub.status.idle":"2021-06-10T22:54:33.069723Z","shell.execute_reply.started":"2021-06-10T22:54:32.720861Z","shell.execute_reply":"2021-06-10T22:54:33.0689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:33.071107Z","iopub.execute_input":"2021-06-10T22:54:33.071383Z","iopub.status.idle":"2021-06-10T22:54:33.080579Z","shell.execute_reply.started":"2021-06-10T22:54:33.071339Z","shell.execute_reply":"2021-06-10T22:54:33.076565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i < num_words:\n        emb_vec=embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i]=emb_vec           ","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:33.08226Z","iopub.execute_input":"2021-06-10T22:54:33.082822Z","iopub.status.idle":"2021-06-10T22:54:33.160773Z","shell.execute_reply.started":"2021-06-10T22:54:33.082545Z","shell.execute_reply":"2021-06-10T22:54:33.158928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweet_pad[0][0:]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:33.162281Z","iopub.execute_input":"2021-06-10T22:54:33.162769Z","iopub.status.idle":"2021-06-10T22:54:33.170132Z","shell.execute_reply.started":"2021-06-10T22:54:33.162566Z","shell.execute_reply":"2021-06-10T22:54:33.168956Z"},"trusted":true},"execution_count":null,"outputs":[]}]}