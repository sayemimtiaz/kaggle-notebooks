{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COVID-19 Open Research Dataset Challenge (CORD-19)\n\nChallenge repository at GitHub: https://github.com/chopeen/CORD-19/\n\n## Team\n\n| Name               | Profile                                 |\n|--------------------|-----------------------------------------|\n| Adrianna Safaryn   | https://www.kaggle.com/adriannasafaryn  |\n| Anna Haratym-Rojek | https://www.kaggle.com/annaharatymrojek |\n| Cezary Szulc       | https://www.kaggle.com/cezaryszulc      |\n| Marek Grzenkowicz  | https://www.kaggle.com/chopeen          |\n\n## Goal\n\nWe wanted to use named entity recognition (NER) to highlight names of risk factors (RF). Our goal was\ntraining a **custom NER model for spaCy**, that could later be use to recognize risk factors in medical\npublications.\n\n![RF tags in Prodigy](https://raw.githubusercontent.com/chopeen/CORD-19/master/images/ner.png)\n\n## Pipeline\n\n1. Data preprocessing to extract 'risk factor(s)' sentences\n2. Manual data annotation in Prodigy\n3. Pretraining different models - we experimented with different base models and trained a number of\n   *tok2vec* layers to maximize the F-score\n    - base models: `en_vectors_web_lg`, `en_core_web_lg`, `en_core_sci_lg`\n    - *tok2vec* layers were trained for: RF sentences, subset of abstracts, all abstracts\n4. Labelling more data by correcting the predictions of the top model trained in the previous step\n5. Go back to step #3 to pretrain a new model using more data and then label even more data\n6. Training the final model with all gathered annotations\n\n## Model performance\n\nEach iteration uses all datasets from the previous one and adds more annotations. For detailed information about every\ntrained model, see the notebook [train_experiments_2.ipynb](https://github.com/chopeen/CORD-19/blob/master/train_experiments_2.ipynb).\n\n### Base model `en_core_sci_lg`\n\n| Iteration  | Datasets ([data/annotated/](https://github.com/chopeen/CORD-19/tree/master/data/annotated)) | Best F-score  |\n|------------|-------------------------------------------------|---------------|\n| 1          | `cord_19_rf_sentences`                          |   53.333      |\n| 2          | above + `cord_19_rf_sentences_correct`          | **75.630**    |\n| 3          | above + `cord_19_rf_sentences_correct_2`        |   74.894      |\n| 4          | above + `cord_19_rf_sentences_correct_3`        |   68.770      |\n\n### Base model `en_core_sci_md`\n\n| Iteration  | Datasets ([data/annotated/](https://github.com/chopeen/CORD-19/tree/master/data/annotated)) | Best F-score  | Download |\n|------------|-------------------------------------------------|---------------|------------------------------------------------------------------------------------------------------------|\n| 1          | `cord_19_rf_sentences`                          |   57.778      | [en_ner_rf_i1_md](https://kagglecord19.blob.core.windows.net/risk-factor-ner/en_ner_rf_i1_md-0.0.1.tar.gz) |\n| 2          | above + `cord_19_rf_sentences_correct`          | **74.380**    | [en_ner_rf_i2_md](https://kagglecord19.blob.core.windows.net/risk-factor-ner/en_ner_rf_i2_md-0.0.1.tar.gz) |\n| 3          | above + `cord_19_rf_sentences_correct_2`        |   74.236      | [en_ner_rf_i3_md](https://kagglecord19.blob.core.windows.net/risk-factor-ner/en_ner_rf_i3_md-0.0.1.tar.gz) |\n| 4          | above + `cord_19_rf_sentences_correct_3`        |   69.725      | [en_ner_rf_i4_md](https://kagglecord19.blob.core.windows.net/risk-factor-ner/en_ner_rf_i4_md-0.0.1.tar.gz) |\n\nUsing a smaller base model (`md` instead of `lg`) results in significantly smaller model, while the F-score\nmoves in both directions depending on the iteration.\n\n## Packaged models\n\nMedium models for iterations 1..4 can be installed using the download links from the table above.\n\nThe directory [test/](https://github.com/chopeen/CORD-19/tree/master/test) contains a demo of the models in action (separate Conda environment + notebook).\n\n## Key files and resources\n\n- Data preprocessing: [Kaggle notebook](https://www.kaggle.com/cezaryszulc/kaggle-covid-19-competition)\n- Training of *tok2vec* layers: [Kaggle notebook](https://www.kaggle.com/chopeen/spacy-with-gpu-support)\n- Full set of annotations:\n  - [cord_19_rf_sentences_merged.jsonl](https://github.com/chopeen/CORD-19/blob/master/data/annotated/cord_19_rf_sentences_merged.jsonl) (dump of the Prodigy dataset)\n  - [cord_19_rf_sentences_merged.json](https://github.com/chopeen/CORD-19/blob/master/data/annotated/cord_19_rf_sentences_merged.json) (spaCy JSON format)\n- Log of all experiments (including data annotation and model training): [train_experiments_2.ipynb](https://github.com/chopeen/CORD-19/blob/master/train_experiments_2.ipynb)\n- Early experiments: [train_experiments_1.ipynb](https://github.com/chopeen/CORD-19/blob/master/backup/early_experiments/train_experiments_1.ipynb)\n\n## Challenges\n\n- Detailed discussion posted at the Kaggle forum:\n  [Custom NER model to recognize risk factor names](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/discussion/140451)\n- Question posted to the Prodigy support forum:\n  [Annotating compound entity phrases](https://support.prodi.gy/t/annotating-compound-entity-phrases/2796)\n\n## Tools\n\n- [Prodigy](https://prodi.gy/) - text annotation\n- [spaCy](https://spacy.io/) - NLP and model training\n- [scispaCy](https://allenai.github.io/scispacy/) - specialized spaCy models for biomedical text processing\n- [Miniconda](https://docs.conda.io/en/latest/miniconda.html) - environment setup (you can use\n  `conda env create -f environment.yml` to set up the Python environment with all packages and models)\n\n## Dataset citation\n\nCOVID-19 Open Research Dataset (CORD-19). 2020. Version 2020-03-13.  \nRetrieved from https://pages.semanticscholar.org/coronavirus-research.  \nAccessed 2020-03-26. doi:10.5281/zenodo.3715506\n\n## Notes\n\n1. [When to reject when annotating text for NER?](https://support.prodi.gy/t/when-to-reject-in-ner-manual-or-ner-make-gold/892/2)\n1. [When should I press accept, reject or ignore?](https://prodi.gy/docs/named-entity-recognition#manual-accept-reject)\n1. [`batch-train` is deprecated](https://prodi.gy/docs/recipes#deprecated)"},{"metadata":{},"cell_type":"markdown","source":"# CODE"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -U spacy\n!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import unicode_literals, print_function\nfrom pathlib import Path\nfrom spacy.util import minibatch, compounding\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\nimport itertools\nimport json\nimport nltk.data\nimport numpy as np\nimport os\nimport pandas as pd\nimport random\nimport spacy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CONFIG\n\n# Data\nDIR_DATA_INPUT = os.path.join('/kaggle', 'input', 'CORD-19-research-challenge')\nDIR_BIORXIV = os.path.join(DIR_DATA_INPUT, 'biorxiv_medrxiv', 'biorxiv_medrxiv', 'pdf_json')\nDIR_COMM = os.path.join(DIR_DATA_INPUT, 'comm_use_subset', 'comm_use_subset', 'pdf_json')\nDIR_CUSTOM = os.path.join(DIR_DATA_INPUT, 'custom_license', 'custom_license', 'pdf_json')\nDIR_NONCUSTOM = os.path.join(DIR_DATA_INPUT, 'noncomm_use_subset', 'noncomm_use_subset', 'pdf_json')\n\nDIR_DATA_OUTPUT = os.path.join('/kaggle', 'working')\nPATH_AGG_JSON = os.path.join(DIR_DATA_OUTPUT, 'agg_data.json')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_jsons_to_list(folder):\n    \"\"\"\n    Extracting 4 fields ('abstract', 'text', 'paper_id', 'title') from orginal Json file\n    :folder String, to location with Jsons\n    :return: Lists, with selected params\n    \"\"\"\n    results = []\n\n    files = os.listdir(folder)\n    for filename in tqdm(files, f'parsing {folder}'):\n        json_file = os.path.join(folder, filename)\n        file = json.load(open(json_file, 'rb'))\n        agg_abstract_file = ' '.join(\n            [abstract['text'] for abstract in file['abstract']])\n        text = ' '.join(\n            [text['text'] for text in file['body_text']])\n        results.append({\n            'abstract': agg_abstract_file,\n            'text': text,\n            'paper_id': file['paper_id'], \n            'title': file['metadata']['title']\n        })\n\n    return results\n\n\ndef save_json(file_to_save, path_to_save):\n    \"\"\"\n    Save in relevant Json format\n    :file_to_save DataFrame, file to save\n    :path_to_save String, lacation to save a file\n    \"\"\"\n    df = pd.DataFrame(file_to_save)\n    \n    df['json_output'] = df.apply(lambda x: {\n        'text': x.text, \"meta\":{'paper_id':x.paper_id, 'title': x.title}\n    }, axis=1)\n    df['json_output'].to_json(path_to_save, orient='records', lines=True)\n    \n\ndef filtr_covid_and_risk_factor(file_to_save, path_to_save):\n    \"\"\"\n    List filtering in abstact and text (filters: 'COVID-19' or 'SARS-CoV-2')\n    :file_to_save List, file to save\n    :path_to_save String, lacation to save a file\n    :return: DataFrame, valid data\n    \"\"\"\n    df = pd.DataFrame(file_to_save)\n    mask = df['abstract'].str.contains('COVID-19') | df['text'].str.contains('COVID-19') \\\n     | df['abstract'].str.contains('SARS-CoV-2') | df['text'].str.contains('SARS-CoV-2')\n    \n    abstracts = text_2_sentance(df[mask], 'abstract')\n    text = text_2_sentance(df[mask], 'text')\n    abstracts.extend(text)\n\n    save_json(abstracts, path_to_save)\n    \n    return df\n\n\ntokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\ndef text_2_sentance(df, column):\n    \"\"\"\n    Save 3 senctance before and after sentance which contains `risk factor` expression\n    :df DataFrame, with text data\n    :column String, column name to process\n    :return: List, valid sentance\n    \"\"\"\n    df['sentances'] = df.apply(lambda x: tokenizer.tokenize(x[column]), axis = 1)\n    \n    valid_sentance = []\n    for _, row in tqdm(df.iterrows()):\n        sentance_range = set()\n        for index, singiel_sentance in enumerate(row['sentances']):\n            if 'risk factor' in singiel_sentance.lower():\n                sentance_range.update(\n                    range(index-3, index+4))\n        for valid_index in sentance_range:\n            if valid_index >=0 and valid_index < len(row['sentances']):\n                valid_sentance.append({\n                    'text': row['sentances'][valid_index],\n                    'paper_id': row['paper_id'], \n                    'title': row['title']\n                })\n                \n    return valid_sentance\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate Json for Marek\n\nbio = extract_jsons_to_list(DIR_BIORXIV)\ncomm = extract_jsons_to_list(DIR_COMM)\ncus = extract_jsons_to_list(DIR_CUSTOM)\nnon = extract_jsons_to_list(DIR_NONCUSTOM)\n\nlist_agg = bio + comm + cus + non\nresults = filtr_covid_and_risk_factor(list_agg, PATH_AGG_JSON)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Download data for training"},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget https://raw.githubusercontent.com/chopeen/CORD-19/master/data/annotated/cord_19_rf_sentences_merged.json\n!ls -1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split dataset for train and test sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_list = []\nfile = json.load(open('cord_19_rf_sentences_merged.json', 'rb'))\n\ndf = pd.DataFrame(file)\n\nX_train, X_test = train_test_split(\n    df, test_size=0.2, random_state=42)\n\nX_train.to_json('train_abstract_teach.json', orient='records')\nX_test.to_json('test_abstract_teach.json', orient='records')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train NER model"},{"metadata":{"trusted":true},"cell_type":"code","source":"!spacy train en models/ train_abstract_teach.json test_abstract_teach.json --pipeline ner --base-model en_core_sci_lg  --replace-components","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}