{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"text-align:center\">Git Hub Prediction</h1>\n\n\nIn this work I have carried out detailed EDA for the GitHub Bug Prediction problem. This is the 1st notebook in the end-to-end implementation approach for solving GitHub Bug Prediction problem series.","metadata":{}},{"cell_type":"markdown","source":"<h3> Problem statement : </h3>\n<p> For an issue posted on the GitHub, Predict whether that issue is a bug or a feature or a question based on the issue title and the body text.</p> ","metadata":{"trusted":true}},{"cell_type":"markdown","source":"## Importing Libraries","metadata":{}},{"cell_type":"code","source":"# Basic Libraries\n\nimport numpy as np\nimport pandas as pd\nimport re\nimport string\nimport random\nimport math\nimport time\nimport json\nimport os\nimport itertools\nimport collections\nfrom collections import Counter, defaultdict\nimport nltk\nimport spacy\nimport pickle\nfrom tqdm import tqdm\n\n\n# Visualization\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n%matplotlib inline\nimport seaborn as sns\nsns.set_style('whitegrid')\n\nfrom plotly import tools\nimport plotly.express as px\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom wordcloud import WordCloud,STOPWORDS\n\nfrom sklearn.decomposition import PCA, TruncatedSVD, SparsePCA\nfrom sklearn.manifold import TSNE\n\n\n# Preprocessing\n\nfrom sklearn.preprocessing import normalize\nfrom sklearn.model_selection import train_test_split, cross_val_score,  cross_val_predict\nfrom sklearn.model_selection import StratifiedKFold, KFold, StratifiedShuffleSplit, GridSearchCV\n\nfrom imblearn.over_sampling import ADASYN,SMOTE\nfrom imblearn.under_sampling import NearMiss\n\nfrom bs4 import BeautifulSoup\n\nfrom nltk import word_tokenize\nfrom nltk.probability import FreqDist\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\n\nnlp_spcy = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\",\"ner\"])\nfrom spacy.lang.en.stop_words import STOP_WORDS\nSTOP_WORDS = list(set(STOP_WORDS))\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nfrom gensim.models import Word2Vec,KeyedVectors\nfrom gensim.scripts.glove2word2vec import glove2word2vec\n\n\n\nimport warnings\nwarnings.filterwarnings(action = \"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Description :\n\n\n<p>  Data set contains only three fields as follows :\n    <ul>\n        <li>Title - the title of the GitHub bug/feature/question</li>\n        <li>Body - the body of the GitHub bug/feature/question</li>\n        <li>Label - the label we are trying to Predict for the given GitHub Issue. It contains the various classes of Labels as follows:\n            <ol>\n                <li>Bug - 0</li>\n                <li>Feature - 1</li>\n                <li>Question - 2</li>\n            <ol>\n        </li>\n    </ul>\n</p>","metadata":{}},{"cell_type":"markdown","source":"## Load Train Dataset","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_json('../input/github-bugs-prediction/embold_train.json')\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check the shape of the train data","metadata":{}},{"cell_type":"code","source":"print('Number of data points : ', train_df.shape[0])\nprint('Number of features : ', train_df.shape[1])\nprint('Features : ', train_df.columns.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Extra Train Dataset","metadata":{}},{"cell_type":"code","source":"train_extra_df = pd.read_json('../input/github-bugs-prediction/embold_train_extra.json')\ntrain_extra_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check the shape of the extra train data","metadata":{}},{"cell_type":"code","source":"train_extra_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference:\n\n- Extra train data contains twice the number of data point compared to original train data.\n- If we see any problems of 'Overfitting' while evaluating model on the original train data, this additional train data will be useful in such scenarios.\n- We can use this extra train data when requierd for further analysis. ","metadata":{}},{"cell_type":"markdown","source":"## Check the basic stats of the data","metadata":{}},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the basic stats\ntrain_df.describe(include='all')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the data for null values\ntrain_df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inferences:\n- We can see there are duplicate values present for the 'title' feature with **323 (150000 - 149677)** repeated values.\n- **'add unit tests'** is the most repeated 'title' which is repeated 15 times in the train_data.\n- We can also see there are no duplicate entries present for the 'body' feature.\n- Since the 'body' text is an unique entry for each issue, we can conclude that the underlying 'body' text associated with a GitHub issue is primarily responsible for categorising the issue into either of the bug/feature/question however 'title' is useful too as it briefly sets the context of the issue.\n- For our analysis purpose we will merge both the title and text into a single feature.\n- No null values are present across the train_data.","metadata":{}},{"cell_type":"code","source":"#Checking the duplicate entries for 'title'\n\ntrain_df.loc[train_df['title'] == 'add unit tests']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Combining Title and Body into a Single Feature for further analysis","metadata":{}},{"cell_type":"code","source":"train_df['text'] = train_df.title + ' ' + train_df.body\ntrain_df.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distribution of data points amongst output labels","metadata":{}},{"cell_type":"code","source":"label_counts = train_df.label.value_counts().sort_index()\nlabel_counts","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check the percetage of data points in each category\n\n(train_df.label.value_counts(normalize=True).sort_index())*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of datapoints with label as Bug :',label_counts[0])\nprint('Number of datapoints with label as Feature :',label_counts[1])\nprint('Number of datapoints with label as Question :',label_counts[2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot the distribution of data points amongst output labels","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nlabel_counts.plot(kind='bar', color=['r','g','b'])\n\nB = mpatches.Patch(color='r', label='Bug')\nF = mpatches.Patch(color='g', label='Feature')\nQ = mpatches.Patch(color='b', label='Question')\n\nplt.legend(handles=[B,F,Q], loc='best')\n\nplt.xlabel('Type of Labels')\nplt.ylabel('Count of Data per Label Category')\nplt.title('Distribution of labels in train data')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference:\n\n- we can see the distribution is labels is well balanced between 'Bug' and 'Feature' categories whereas the 'Question' types labels are comparitively very few.","metadata":{}},{"cell_type":"markdown","source":"## Filter the train_data based on each unique label category","metadata":{}},{"cell_type":"code","source":"Bug_data=train_df[train_df['label']== 0]\nFeature_data=train_df[train_df['label']== 1]\nQuestion_data=train_df[train_df['label']== 2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"First 10 rows of the 'text' feature with the Bug_data:\\n\", Bug_data['text'].head(10), \"\\n\")\nprint(\"First 10 rows of the 'text' feature with the Feature_data:\\n\", Feature_data['text'].head(10), \"\\n\")\nprint(\"First 10 rows of the 'text' feature with the Question_data:\\n\", Question_data['text'].head(10), \"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analyse the count of words in 'text' feature for each unique label category","metadata":{}},{"cell_type":"code","source":"count_text_Bug = Bug_data.text.str.split().apply(lambda w : len(w)).sort_values(ascending=True)\ncount_text_Feature = Feature_data.text.str.split().apply(lambda w : len(w)).sort_values(ascending=True)\ncount_text_Question = Question_data.text.str.split().apply(lambda w : len(w)).sort_values(ascending=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Count of words in text feature for Bug Data:\\n\",count_text_Bug,\"\\n\")\nprint(\"Count of words in text feature for Feature Data:\\n\",count_text_Feature,\"\\n\")\nprint(\"Count of words in text feature for Question Data:\\n\",count_text_Question,\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The word count for text for Bug Data varies between a minimum of \", str(np.min(count_text_Bug.values)), \"and maximum of \", str(np.max(count_text_Bug.values)) )\nprint(\"The word count for text for Feature Data varies between a minimum of \", str(np.min(count_text_Feature.values)), \"and maximum of \", str(np.max(count_text_Feature.values)) )\nprint(\"The word count for text for Question Data varies between a minimum of \", str(np.min(count_text_Question.values)), \"and maximum of \", str(np.max(count_text_Question.values)) ) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot the distribution of words in body text for each output labels","metadata":{}},{"cell_type":"code","source":"# Creating a Generic count_plot function with Seaborn\n\ndef plot_count_dist(count_Bug,count_Feature,count_Question,title_1,title_2,title_3,subtitle):\n    fig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(18,6))\n    sns.distplot(count_Bug,ax=ax1,color='r')\n    ax1.set_title(title_1)\n    sns.distplot(count_Feature,ax=ax2,color='g')\n    ax2.set_title(title_2)\n    sns.distplot(count_Question,ax=ax3,color='b')\n    ax3.set_title(title_3)\n    fig.suptitle(subtitle)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_count_dist(count_text_Bug,count_text_Feature,count_text_Question,'Bug','Feature','Question','Text Data Word Count Aanalysis')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference:\n\n- We can observe that the number of words in each of the text feature for Bug, Feature and Question type data mostly varies between 5 to 300 words.","metadata":{}},{"cell_type":"markdown","source":"## Analyse the distribution of Punctuations in 'text' feature for each unique label category","metadata":{}},{"cell_type":"code","source":"#Check the standard punctuation characters\n\nprint(\"Standard Punctuation characters:\",string.punctuation)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_text_punctuations_Bug = Bug_data.text.apply(lambda w : len([p for p in str(w) if p in string.punctuation])).sort_values()\ncount_text_punctuations_Feature = Feature_data.text.apply(lambda w : len([p for p in str(w) if p in string.punctuation])).sort_values()\ncount_text_punctuations_Question = Question_data.text.apply(lambda w : len([p for p in str(w) if p in string.punctuation])).sort_values()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Count of punctuations in text feature for Bug Data:\\n\",count_text_punctuations_Bug,\"\\n\")\nprint(\"Count of punctuations in text feature for Feature Data:\\n\",count_text_punctuations_Feature,\"\\n\")\nprint(\"Count of punctuations in text feature for Question Data:\\n\",count_text_punctuations_Question,\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_count_dist(count_text_punctuations_Bug,count_text_punctuations_Feature,count_text_punctuations_Question,'Bug','Feature','Question','Text Data Punctuations Count Aanalysis')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sample bug data with most number of punctuations\n\nprint('Bug: \\n Sample Bug data with most number of punctuations = ', count_text_punctuations_Bug.max(), '\\n')\nprint(train_df.iloc[count_text_punctuations_Bug.idxmax()]['text'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sample feature data with most number of punctuations\n\nprint('Feature: \\n Sample Feature data with most number of punctuations = ', count_text_punctuations_Feature.max(), '\\n')\nprint(train_df.iloc[count_text_punctuations_Feature.idxmax()]['text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sample Question data with most number of punctuations\n\nprint('Question: \\n Sample Question data with most number of punctuations = ', count_text_punctuations_Question.max(), '\\n')\nprint(train_df.iloc[count_text_punctuations_Question.idxmax()]['text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference:\n\n- we can observe that the punctuations are heavily present across all the data for each of the unique labels.\n* we can also observe that there is some noisy text such **\"\\\\r\"** is present widely in 'text' feature across all the label categories. This noisy data needs to be cleaned explicitly.\n- this implies that we need to do a proper cleaning of punctuation characters in the text pre-processing phase.","metadata":{}},{"cell_type":"markdown","source":"## Plotting WordCloud for Unprocessed Data","metadata":{}},{"cell_type":"code","source":"#Creating a generic function for generating WordCloud across different label categories.\n\ndef generate_wordcloud(df,col,i,label):\n    \n    data = df[df.label == i][col].values\n    \n    wc = WordCloud(stopwords=STOPWORDS, background_color='black',\n                   max_words=10000, min_font_size=6, min_word_length=1)\n    wc.generate(' '.join(data))\n    \n    plt.figure(figsize=(15,15))\n    plt.title('WordCloud for {}'.format(label), fontsize = 24)\n    plt.imshow(wc)\n    plt.axis(\"off\")\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%time\n\nlabels = np.unique(train_df.label.values)\nlabel_names = ['Bug','Feature','Question']\nfor i in tqdm(labels):\n    generate_wordcloud(train_df,'text',i,label_names[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gram Statistic for Unprocessed Data","metadata":{}},{"cell_type":"code","source":"#Creating a generic function for plotting n-Grams across different label categories.\n\ndef gram_analysis(data,gram):\n    stop_words_set = set(stopwords.words('english'))\n    tokens=[t for t in data.lower().split(\" \") if t!=\"\" if t not in stop_words_set]\n    ngrams=zip(*[tokens[i:] for i in range(gram)])\n    final_tokens=[\" \".join(z) for z in ngrams]\n    return final_tokens\n\n\ndef gram_freq(df,gram_type,gram,col,i,label):\n    body_text = \" \".join(df[df.label == i][col].sample(200).values)\n    toks = gram_analysis(body_text, gram)\n    tok_freq = pd.DataFrame(data=[toks, np.ones(len(toks))]).T.groupby(0).sum().reset_index()\n    tok_freq.columns = ['token','frequency']\n    tok_freq = tok_freq.sort_values(by='frequency',ascending=False)\n    \n    #plt.figure(figsize=(15,8))\n    plt.figure(figsize=(10,15))\n    plt.title(\"{0} for most common tokens of {1} type\".format(gram_type, label), fontsize = 24)\n    #sns.barplot(x='token', y='frequency', data=tok_freq.iloc[:30])\n    #plt.xticks(rotation=90)\n    sns.barplot(x='frequency', y='token', data=tok_freq.iloc[:30])\n    plt.show()\n    \n    return ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%time\n\nlabels = np.unique(train_df.label.values)\nlabel_names = ['Bug','Feature','Question']\n\nfor gram_type, n_gram in tqdm(zip(('Bi-Gram','Tri-Gram', 'Penta-Gram'),(2,3,5))):\n    for i in labels:\n        gram_freq(train_df,gram_type,n_gram,'text',i,label_names[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot the Most Common Words Count for each label categories (UnProcessed Data)","metadata":{}},{"cell_type":"code","source":"#Creating a generic function for plotting 50 most common words for each label category\n\ndef plot_common_words(df,col,i,label):\n    \n    data = df[df.label == i][col].values\n    data_string = ' '.join(map(str,data))\n    corpus = word_tokenize(data_string)\n    \n    freq_words = FreqDist(corpus)\n    \n    most_common = freq_words.most_common(50)\n       \n    \"\"\"\n    words = []\n    count = []\n    for w, c in most_common:\n        if w not in stop_words:\n            words.append(w)\n            count.append(c)\n    \n    plt.figure(figsize=(22,6))\n    sns.barplot(y=count, x=words)\n    plt.title('50 Most common words for {}'.format(label), fontsize = 24)\n    plt.show()\n    \"\"\"\n    # plotly graphs are more readable and interactive\n    \n    fig = px.bar(pd.DataFrame(most_common, columns=['Words','Count']), x = \"Words\", y = \"Count\", title='50 Most common words for {}'.format(label),width=1200, height=700)\n    fig.show()  \n    return","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%time\n\nlabels = np.unique(train_df.label.values)\nlabel_names = ['Bug','Feature','Question']\nfor i in tqdm(labels):\n    plot_common_words(train_df,'text',i,label_names[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"#### Closer look at the text data","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(train_df.text.value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text Pre-processing  -- Cleaning Redundant Data\n\nAs we have observed, the 'text' feature contains a lot of redundant entities like punctuations, stop words, url, htlml tags,etc. \nWe will need to clean such data before we proceed with the word embedding and vector transformations. \nRemoving below will sufficiently clean the text and will remove redundancies.\n\n1. HTML codes\n2. URLs\n3. Emojis\n4. Stopwords\n5. Punctuations\n6. Expanding Abbreviations","metadata":{}},{"cell_type":"markdown","source":"### Check the Stop Words list","metadata":{}},{"cell_type":"code","source":"print(\"Total count of standard stop words list from SpaCy :\",len(STOP_WORDS))\nprint(\"\\nStandard stop words list from SpaCy :\\n\", STOP_WORDS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#add some redundant words like 'elif' in the stop words list\nSTOP_WORDS = STOP_WORDS + ['elif']\nprint('elif' in STOP_WORDS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Discard negative words like 'not'and 'no' from this list\n\n#stop_words.remove('not')\n#stop_words.discard('no')\n#print(len(stop_words))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a sigle Generic Function for text cleaning (we can create class as well here)\n\ndef text_cleaning(df,col,clean_col):\n    \n    #cleaning abbreviated words \n    def remove_contractions(data):\n        data = re.sub(r\"he's\", \"he is\", data)\n        data = re.sub(r\"there's\", \"there is\", data)\n        data = re.sub(r\"We're\", \"We are\", data)\n        data = re.sub(r\"That's\", \"That is\", data)\n        data = re.sub(r\"won't\", \"will not\", data)\n        data = re.sub(r\"they're\", \"they are\", data)\n        data = re.sub(r\"Can't\", \"Cannot\", data)\n        data = re.sub(r\"wasn't\", \"was not\", data)\n        data = re.sub(r\"don\\x89Ûªt\", \"do not\", data)\n        data= re.sub(r\"aren't\", \"are not\", data)\n        data = re.sub(r\"isn't\", \"is not\", data)\n        data = re.sub(r\"What's\", \"What is\", data)\n        data = re.sub(r\"haven't\", \"have not\", data)\n        data = re.sub(r\"hasn't\", \"has not\", data)\n        data = re.sub(r\"There's\", \"There is\", data)\n        data = re.sub(r\"He's\", \"He is\", data)\n        data = re.sub(r\"It's\", \"It is\", data)\n        data = re.sub(r\"You're\", \"You are\", data)\n        data = re.sub(r\"I'M\", \"I am\", data)\n        data = re.sub(r\"shouldn't\", \"should not\", data)\n        data = re.sub(r\"wouldn't\", \"would not\", data)\n        data = re.sub(r\"i'm\", \"I am\", data)\n        data = re.sub(r\"I\\x89Ûªm\", \"I am\", data)\n        data = re.sub(r\"I'm\", \"I am\", data)\n        data = re.sub(r\"Isn't\", \"is not\", data)\n        data = re.sub(r\"Here's\", \"Here is\", data)\n        data = re.sub(r\"you've\", \"you have\", data)\n        data = re.sub(r\"you\\x89Ûªve\", \"you have\", data)\n        data = re.sub(r\"we're\", \"we are\", data)\n        data = re.sub(r\"what's\", \"what is\", data)\n        data = re.sub(r\"couldn't\", \"could not\", data)\n        data = re.sub(r\"we've\", \"we have\", data)\n        data = re.sub(r\"it\\x89Ûªs\", \"it is\", data)\n        data = re.sub(r\"doesn\\x89Ûªt\", \"does not\", data)\n        data = re.sub(r\"It\\x89Ûªs\", \"It is\", data)\n        data = re.sub(r\"Here\\x89Ûªs\", \"Here is\", data)\n        data = re.sub(r\"who's\", \"who is\", data)\n        data = re.sub(r\"I\\x89Ûªve\", \"I have\", data)\n        data = re.sub(r\"y'all\", \"you all\", data)\n        data = re.sub(r\"can\\x89Ûªt\", \"cannot\", data)\n        data = re.sub(r\"would've\", \"would have\", data)\n        data = re.sub(r\"it'll\", \"it will\", data)\n        data = re.sub(r\"we'll\", \"we will\", data)\n        data = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", data)\n        data = re.sub(r\"We've\", \"We have\", data)\n        data = re.sub(r\"he'll\", \"he will\", data)\n        data = re.sub(r\"Y'all\", \"You all\", data)\n        data = re.sub(r\"Weren't\", \"Were not\", data)\n        data = re.sub(r\"Didn't\", \"Did not\", data)\n        data = re.sub(r\"they'll\", \"they will\", data)\n        data = re.sub(r\"they'd\", \"they would\", data)\n        data = re.sub(r\"DON'T\", \"DO NOT\", data)\n        data = re.sub(r\"That\\x89Ûªs\", \"That is\", data)\n        data = re.sub(r\"they've\", \"they have\", data)\n        data = re.sub(r\"i'd\", \"I would\", data)\n        data = re.sub(r\"should've\", \"should have\", data)\n        data = re.sub(r\"You\\x89Ûªre\", \"You are\", data)\n        data = re.sub(r\"where's\", \"where is\", data)\n        data = re.sub(r\"Don\\x89Ûªt\", \"Do not\", data)\n        data = re.sub(r\"we'd\", \"we would\", data)\n        data = re.sub(r\"i'll\", \"I will\", data)\n        data = re.sub(r\"weren't\", \"were not\", data)\n        data = re.sub(r\"They're\", \"They are\", data)\n        data = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", data)\n        data = re.sub(r\"you\\x89Ûªll\", \"you will\", data)\n        data = re.sub(r\"I\\x89Ûªd\", \"I would\", data)\n        data = re.sub(r\"let's\", \"let us\", data)\n        data = re.sub(r\"it's\", \"it is\", data)\n        data = re.sub(r\"can't\", \"cannot\", data)\n        data = re.sub(r\"don't\", \"do not\", data)\n        data = re.sub(r\"you're\", \"you are\", data)\n        data = re.sub(r\"i've\", \"I have\", data)\n        data = re.sub(r\"that's\", \"that is\", data)\n        data = re.sub(r\"i'll\", \"I will\", data)\n        data = re.sub(r\"doesn't\", \"does not\",data)\n        data = re.sub(r\"i'd\", \"I would\", data)\n        data = re.sub(r\"didn't\", \"did not\", data)\n        data = re.sub(r\"ain't\", \"am not\", data)\n        data = re.sub(r\"you'll\", \"you will\", data)\n        data = re.sub(r\"I've\", \"I have\", data)\n        data = re.sub(r\"Don't\", \"do not\", data)\n        data = re.sub(r\"I'll\", \"I will\", data)\n        data = re.sub(r\"I'd\", \"I would\", data)\n        data = re.sub(r\"Let's\", \"Let us\", data)\n        data = re.sub(r\"you'd\", \"You would\", data)\n        data = re.sub(r\"It's\", \"It is\", data)\n        data = re.sub(r\"Ain't\", \"am not\", data)\n        data = re.sub(r\"Haven't\", \"Have not\", data)\n        data = re.sub(r\"Could've\", \"Could have\", data)\n        data = re.sub(r\"youve\", \"you have\", data)  \n        data = re.sub(r\"donå«t\", \"do not\", data)\n        \n        return data\n    \n    \n    #cleaning Urls\n    def remove_urls(data):\n        clean_url_regex = re.compile(r\"http\\S+|www\\.\\S+\")\n        data = clean_url_regex.sub(r\"\", data)\n        return data\n    \n    \n    #cleaning noisy data\n    def remove_noisy_char(data):\n        data = data.replace(\"\\\\r\", \"\").strip()\n        return data\n    \n    \n    #cleaning HTML tags\n    def remove_HTML_tags(data):\n        soup = BeautifulSoup(data, 'html.parser') \n        return soup.get_text()\n        \n        \n    #cleaning emojis   \n    def remove_emojis(data):\n        emoji_clean= re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n        \n        data = emoji_clean.sub(r\"\",data)\n        return data\n    \n    \n    #cleaning unicode characters\n    \"\"\"\n    def remove_unicode_chars(data):\n        data = (data.encode('ascii', 'ignore')).decode(\"utf-8\")\n        return data\n    \"\"\"\n    \n    \n    #cleaning punctuations\n    def remove_punctuations(data):\n        #clean_punct_regex = re.compile(r\"[^\\w\\s\\d]+\")\n        clean_punct_regex = re.compile(r\"[^a-zA-Z0-9\\s]+\")\n        data = clean_punct_regex.sub(r\" \", data)\n                        \n        #credits - https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string\n        #data = data.translate(str.maketrans('', '', string.punctuation))   \n        return data\n    \n    \n    #cleaning numeric characters\n    def remove_numerics(data):\n        #clean_num_regex = re.compile(r\"[^A-Za-z]+\")\n        #data = clean_num_regex.sub(r\" \", data)\n        #clean_alphanum_regex = re.compile(r\"\\S*\\d\\S*\")\n        #data = clean_alphanum_regex.sub(r\"\", data)\n        \n        clean_num_regex = re.compile(r\"\\b[0-9]+\\b\")\n        data = clean_num_regex.sub(r\"\", data)\n        return data\n    \n    def remove_single_chars(data):\n        #credits - https://stackoverflow.com/questions/42066352/python-regex-to-replace-all-single-word-characters-in-string\n        clean_single_len_regex = re.compile(r\"\\b[a-zA-Z]\\b\")\n        data = clean_single_len_regex.sub(r\"\", data)\n        return data\n    \n    \n    #cleaning unwanted whitespaces\n    def remove_redundant_whiteSpaces(data):\n        clean_redundant_whitespaces_regex = re.compile(r\"\\s\\s+\") #check for more consecutive spaces\n        data = clean_redundant_whitespaces_regex.sub(r\" \", data) #replace with single space\n        return data\n    \n    \n    #cleaning stopwords except 'not'\n    def remove_stopwords(data):\n        data = ' '.join(word.lower() for word in data.split() if word.lower() != 'not' if word.lower() not in STOP_WORDS)\n        data = data.strip()\n        return data\n    \n    \n    #cleaning long length words (greater than 30 chars)\n    def remove_long_length_tokens(data):\n        data = ' '.join(word.lower() for word in data.split() if len(word) <= 30)\n        data = data.strip()\n        return data\n    \n    \n    df[clean_col]= df[col].apply(remove_contractions)\n    df[clean_col]= df[clean_col].apply(remove_urls)\n    df[clean_col]= df[clean_col].apply(remove_noisy_char)\n    df[clean_col]= df[clean_col].apply(remove_HTML_tags)\n    df[clean_col]= df[clean_col].apply(remove_emojis)\n    #df[clean_col]= df[clean_col].apply(remove_unicode_chars)\n    df[clean_col]= df[clean_col].apply(remove_punctuations)\n    df[clean_col]= df[clean_col].apply(remove_numerics)\n    df[clean_col]= df[clean_col].apply(remove_single_chars)\n    df[clean_col]= df[clean_col].apply(remove_redundant_whiteSpaces)\n    df[clean_col]= df[clean_col].apply(remove_stopwords)\n    df[clean_col]= df[clean_col].apply(remove_long_length_tokens)\n    \n    \n#     df[col]= df[col].apply(lambda text: remove_contractions(text))\n#     df[col]= df[col].apply(lambda text: remove_urls(text))\n#     df[col]= df[col].apply(lambda text: remove_noisy_char(text))\n#     df[col]= df[col].apply(lambda text: remove_HTML_tags(text))\n#     df[col]= df[col].apply(lambda text: remove_emojis(text))\n#     #df[col]= df[col].apply(lambda text: remove_unicode_chars(text))\n#     df[col]= df[col].apply(lambda text: remove_punctuations(text))\n#     df[col]= df[col].apply(lambda text: remove_numerics(text))\n#     df[col]= df[col].apply(lambda text: remove_single_chars(text))\n#     df[col]= df[col].apply(lambda text: remove_redundant_whiteSpaces(text))\n#     df[col]= df[col].apply(lambda text: remove_stopwords(text))\n#     df[col]= df[col].apply(lambda text: remove_long_length_tokens(text))\n\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.clock()\ntrain_df = text_cleaning(train_df, 'text', 'clean_text')\nprint(\"time required text_cleaning :\", time.clock() - start_time, \"sec.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 6\n#check the text data before text-preprocessing\nprint(\"text data at index {0} before text pre-processing : \\n\\n {1}\".format(i, train_df.text.iloc[i]))\nprint(\"\\n\\n\")\n\n#check the cleaned text data post text-preprocessing\nprint(\"text data at index {0} post text pre-processing : \\n\\n {1}\".format(i,train_df.clean_text.iloc[i]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Closely analysing Body text post cleanup","metadata":{}},{"cell_type":"code","source":"# check the sample bug data with most number of punctuations post cleanup\n\nprint('Bug: \\n Sample Bug data (with most number of punctuations = {}) post text cleanup. \\n'.format(count_text_punctuations_Bug.max()))\nprint(train_df.iloc[count_text_punctuations_Bug.idxmax()]['clean_text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check sample feature data with most number of punctuations post cleanup\n\nprint('Feature: \\n Sample Feature data (with most number of punctuations = {}) post text cleanup. \\n'.format(count_text_punctuations_Feature.max()))\nprint(train_df.iloc[count_text_punctuations_Feature.idxmax()]['clean_text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check sample question data with most number of punctuations post cleanup\n\nprint('Feature: \\n Sample Question data (with most number of punctuations = {}) post text cleanup. \\n'.format(count_text_punctuations_Question.max()))\nprint(train_df.iloc[count_text_punctuations_Question.idxmax()]['clean_text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Inference:\n\n- we can also observe repeatation of words in certain data records post cleaning.","metadata":{}},{"cell_type":"code","source":"## Identifying long length words having length > 30 chars in 'Text' data post cleaning\n\n# text_data = train_df.clean_text.values\n# text_data\n\n# # crete a corpus of long length words having length > 20 chars\n\n# long_length_word_corpus = [[w for w in txt.split() if len(w)>20] for txt in text_data]\n# long_length_word_corpus = list(filter(None,long_length_word_corpus))\n# long_length_word_corpus = list(itertools.chain.from_iterable(long_length_word_corpus))\n# long_length_word_corpus = list(set(long_length_word_corpus))\n# long_length_word_corpus\n# print(\"Total number of unique longest length words = {}\".format(len(long_length_word_corpus)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plotting WordClouds for cleaned 'Text' data","metadata":{}},{"cell_type":"code","source":"##%%time\n\nlabels = np.unique(train_df.label.values)\nlabel_names = ['Bug','Feature','Question']\nfor i in tqdm(labels):\n    generate_wordcloud(train_df,'clean_text',i,label_names[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gram Analysis on the cleaned 'Text' data","metadata":{}},{"cell_type":"code","source":"##%%time\n\nlabels = np.unique(train_df.label.values)\nlabel_names = ['Bug','Feature','Question']\n\nfor gram_type, n_gram in tqdm(zip(('Bi-Gram','Tri-Gram','Penta-Gram'),(2,3,5))):\n    for i in labels:\n        gram_freq(train_df,gram_type,n_gram,'clean_text',i,label_names[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot the Most Common Words Count for each label categories on the cleaned 'Text' data","metadata":{}},{"cell_type":"code","source":"##%%time\n\nlabels = np.unique(train_df.label.values)\nlabel_names = ['Bug','Feature','Question']\nfor i in tqdm(labels):\n    plot_common_words(train_df,'clean_text',i,label_names[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference:\n- Thus we can observe that all the noisy data and punctuations characters are removed from the 'text' data.\n- Text preprocessing have resulted in generation of some unusually long fetures.\n- Overall the data looks well cleaned up now and ready for next phase of word embeddings and model building.","metadata":{}},{"cell_type":"markdown","source":"## Lemmatization & Stemming :\n\n- Till now we have cleaned the data and removed all the redundant text and noise from it. This reduced the dimensionality of the data to certain extent.\n- Next up, we will prune some words to their roots which will again reduce the length of sentences.\n- Here we are applying Lemmatization to reduce the words to their morphological roots so as to retain the symantics of the text.\n- We can also apply 'Stemming' in this case, but a stemmer implementation will not retain symantic meaning of the words and will result in reducing the words to their non-dictionary roots.","metadata":{}},{"cell_type":"code","source":"#lemmatizing the text data\ndef lemmatize_corpus(data, method = 'wordnet'):\n    if method == 'spacy':\n        out_data = \" \".join([token.lemma_ for token in nlp_spcy(data)])\n    else:\n        lemmatizer=WordNetLemmatizer()\n        out_data = ' '.join(lemmatizer.lemmatize(word) for word in data.split())\n        \n        return out_data\n\n\n#stemming the text data\ndef stem_traincorpus(data):\n    pstemmer = PorterStemmer()\n    out_data = ' '.join(pstemmer.stem(word) for word in data.split())\n    return out_data ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check the cleaned text data before lemmatization/stemming\ni = 4\n\nprint(\"text data at index {0} before text lemmatization/stemming : \\n\\n {1}\".format(i,train_df.clean_text.iloc[i]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#apply lemmatization\nstart_time = time.clock()\ntrain_df['lemmatized_text'] = train_df['clean_text'].apply(lemmatize_corpus, method='wordnet')\nprint(\"time required lemmatizing text :\", time.clock() - start_time, \"sec.\")\nprint(\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check the cleaned text data post Lemmatization\nprint(\"text data at index {0} post text Lemmatization : \\n\\n {1}\".format(i,train_df.lemmatized_text.iloc[i]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.stem import PorterStemmer\n#apply stemming\nstart_time = time.clock()\ntrain_df['stemmed_text'] = train_df['clean_text'].apply(stem_traincorpus)\nprint(\"time required stemming text :\", time.clock() - start_time, \"sec.\")\nprint(\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check the cleaned text data post Lemmatization\nprint(\"text data at index {0} post text Stemming : \\n\\n {1}\".format(i,train_df.stemmed_text.iloc[i]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check overall train_data\n\ntrain_df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## End Notes :\n\n- In this notebbok, I have tried to uncover the some hiddens insights in the text data by carrying out an in-depth exploratory data analysis (EDA).\n- Next up, I will be implementing the word embeddings and statistical model building.\n- You can refer the 2nd part of the the Git Hub Bug Prediction problem series [here](https://www.kaggle.com/gauravharamkar/github-static-semantic-word-mbeddings).","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}