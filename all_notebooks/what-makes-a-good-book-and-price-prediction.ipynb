{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nsns.set_style('darkgrid')\nfrom wordcloud import WordCloud,STOPWORDS\nimport re\nstopwords = list(STOPWORDS)\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import PolynomialFeatures,StandardScaler\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import train_test_split\n\nplt.rc('figure',figsize=(20,11))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"background-color:skyblue;font-family:newtimeroman;font-size:300%;text-align:center\">Introduction</h3>\n\n![](https://www.bornontario.ca/en/about-born/resources/Images/books-1204029_1920.jpg)\n\n\n\n<p style=\"text-align: center;\"><br></p>\n<p style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><br></span></span></p>\n<p style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\">There are many features that make up a good book, but what are the features which affect most the goodness of a programming or computer science book?</span></span></p>\n<p style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\">In this kernel, we will dive into 270 examples of books and try to understand what makes a book score high review scores, what affects the book&apos;s price, and are there any underlying patterns we are missing, would unraveling them help book writers make better books?</span></span></p>\n<p style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><br></span></span></p>\n<h3 style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\">Questions We Will Try To Answer</span></span></h3>\n<p style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><br></span></span></p>\n<p style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\">1) What features are correlated with the price features and the score feature</span></span></p>\n<p style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><br></span></span></p>\n<p style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\">2) How the number of pages affect the rest of our features.</span></span></p>\n<p style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><br></span></span></p>\n<p style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\">3) Are any programming languages come up more frequently?</span></span></p>\n<p style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><br></span></span></p>\n<p style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\">4) Can we divide the books by categories and understand what category costs more or has higher review scores.</span></span></p>\n<p style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><br></span></span></p>\n<p style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><br></span></span></p>\n<h3 style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\">&nbsp;Feature Engineering Goals</span></span></h3>\n<p style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\">1) Extract description text features like text length, average word length, programming language, etc.</span></span></p>\n<p style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><br></span></span></p>\n<p style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\">2) Extract book title text features like text length, average word length, programming language, etc.</span></span></p>\n<p style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><br></span></span></p>\n<h3 style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\">Prediction Goals</span></span></h3>\n<p style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\">1) Predict the number of potential reviews for a book &nbsp;</span></span></p>\n<p style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><br></span></span></p>\n<p style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\">2) Predict a book&apos;s rating&nbsp;</span></span></p>\n<p style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><br></span></span></p>\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>3) Predict a book&apos;s price</span></p>\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"b_data = pd.read_csv('/kaggle/input/top-270-rated-computer-science-programing-books/prog_book.csv')\nb_data.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"background-color:skyblue;font-family:newtimeroman;font-size:250%;text-align:center\">Feature Engineering</h3>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"b_data['Title_Average_Word_Length'] = b_data.Book_title.apply(lambda x: np.mean([len(word) for word in x.split()]))\nb_data['Title_Number_Of_Words'] = b_data.Book_title.apply(lambda x: len(x.split()))\nb_data['Description_Average_Word_Length'] = b_data.Description.apply(lambda x: np.mean([len(word) for word in x.split()]))\nb_data['Description_Number_Of_Words'] = b_data.Description.apply(lambda x: len(x.split()))\n\n\nlencoder = LabelEncoder()\nlencoder.fit(b_data.Type)\nb_data.Type = lencoder.transform(b_data.Type)\nb_data.Reviews = b_data.Reviews.apply(lambda x : int(x.replace(',','')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"background-color:skyblue;font-family:newtimeroman;font-size:250%;text-align:center\">Exploratory data analysis</h3>\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.subplot(3,1,1)\nax = sns.boxplot(b_data.Rating,color='tab:green',showmeans=True,meanprops={\"marker\":\"+\",\n                       \"markerfacecolor\":\"white\", \n                       \"markeredgecolor\":\"black\",\n                      \"markersize\":\"10\"})\nax.set_title('Distribution Of Rating Scores Across Our Dataset',fontsize=21)\n\nplt.subplot(3,1,2)\nax = sns.distplot(b_data.Rating,label=\"Ratings\",color='green')\nax.set_xlabel(\"Rating\",fontsize=20)\nax.set_ylabel(\"Density\",fontsize=20)\ntextstr = '\\n'.join(\n        (r'$\\mu=%.2f$' % (b_data.Rating.mean(),), r'$\\mathrm{median}=%.2f$' % (b_data.Rating.median(),),\n         r'$\\sigma=%.2f$' % (b_data.Rating.std(),)))\nprops = dict(boxstyle='round', facecolor='green', alpha=0.5)\nax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=14,\n            verticalalignment='top', bbox=props)\n\n\nax.axvline(b_data['Rating'].mean(), color='r', linestyle='--')\nax.axvline(b_data['Rating'].median(), color='tab:orange', linestyle='-')\nax.legend(['Mean','Median'])\nplt.subplot(3,1,3)\nax = sns.kdeplot(b_data.Rating,label=\"Ratings\",color='green',cumulative = True)\nax.set_xlabel(\"Rating\",fontsize=20)\nax.set_ylabel(\"Density\",fontsize=20)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Interesting! We can see that our ratings are normally distributed around 4.0, unlike some datasets where the ratings are usually skewed towards low scores or high scores our sample population is normally distributed which is good news for our future modeling.</span></p>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.subplot(2,1,1)\nax = sns.distplot(b_data.Reviews,label=\"Reviews\",color='teal')\nax.set_xlabel(\"Reviews\",fontsize=20)\nax.set_ylabel(\"Density\",fontsize=20)\ntextstr = '\\n'.join(\n        (r'$\\mu=%.2f$' % (b_data.Reviews.mean(),), r'$\\mathrm{median}=%.2f$' % (b_data.Reviews.median(),),\n         r'$\\sigma=%.2f$' % (b_data.Reviews.std(),)))\nprops = dict(boxstyle='round', facecolor='teal', alpha=0.5)\nax.text(0.85, 0.85, textstr, transform=ax.transAxes, fontsize=14,\n            verticalalignment='center', bbox=props)\n\nax.set_title('Distribution Of Reviews Across Our Dataset',fontsize=21)\nplt.show()\n\n\nb_data.Reviews = b_data.Reviews.replace(0,1)\nb_data.Reviews = np.log(b_data.Reviews)\nplt.subplot(2,1,2)\nax = sns.distplot(b_data.Reviews,label=\"Reviews\",color='teal')\nax.set_xlabel(\"Reviews\",fontsize=20)\nax.set_ylabel(\"Density\",fontsize=20)\ntextstr = '\\n'.join(\n        (r'$\\mu=%.2f$' % (b_data.Reviews.mean(),), r'$\\mathrm{median}=%.2f$' % (b_data.Reviews.median(),),\n         r'$\\sigma=%.2f$' % (b_data.Reviews.std(),)))\nprops = dict(boxstyle='round', facecolor='teal', alpha=0.5)\nax.text(0.85, 0.85, textstr, transform=ax.transAxes, fontsize=14,\n            verticalalignment='center', bbox=props)\n\nax.set_title('Distribution Of Reviews Across Our Dataset After Log Transformation',fontsize=21)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Unlike our ratings, the number of reviews is distributed almost exponentially, we can clearly see that the massive skews is caused by outliers, we will perform a logarithmic transformation of this feature in order to rebalance our data in comparisons to outliers.\nAfter performing a logarithmic transformation we are left with a fairly normal distribution where our mean and median are almost the same as well as a fairly small standard deviation, although we can see two picks which suggest that although the average logarithmic scale number of reviews is 3.3 a large portion of the books have 0 reviews..</span></p>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.subplot(2,1,1)\nax = sns.distplot(b_data.Number_Of_Pages,label=\"Number_Of_Pages\",color='red')\nax.set_xlabel(\"Number Of Pages\",fontsize=20)\nax.set_ylabel(\"Density\",fontsize=20)\ntextstr = '\\n'.join(\n        (r'$\\mu=%.2f$' % (b_data.Number_Of_Pages.mean(),), r'$\\mathrm{median}=%.2f$' % (b_data.Number_Of_Pages.median(),),\n         r'$\\sigma=%.2f$' % (b_data.Number_Of_Pages.std(),)))\nprops = dict(boxstyle='round', facecolor='red', alpha=0.5)\nax.text(0.85, 0.85, textstr, transform=ax.transAxes, fontsize=14,\n            verticalalignment='center', bbox=props)\n\nax.set_title('Distribution Of Book Page Number Counts',fontsize=21)\nplt.show()\n\nb_data = b_data[b_data['Number_Of_Pages']<1500]\n\nplt.subplot(2,1,2)\nax = sns.distplot(b_data.Number_Of_Pages,label=\"Number_Of_Pages\",color='red')\nax.set_xlabel(\"Number Of Pages\",fontsize=20)\nax.set_ylabel(\"Density\",fontsize=20)\ntextstr = '\\n'.join(\n        (r'$\\mu=%.2f$' % (b_data.Number_Of_Pages.mean(),), r'$\\mathrm{median}=%.2f$' % (b_data.Number_Of_Pages.median(),),\n         r'$\\sigma=%.2f$' % (b_data.Number_Of_Pages.std(),)))\nprops = dict(boxstyle='round', facecolor='red', alpha=0.5)\nax.text(0.85, 0.85, textstr, transform=ax.transAxes, fontsize=14,\n            verticalalignment='center', bbox=props)\n\nax.set_title('Distribution Of Book Page Number Counts',fontsize=21)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"ax = sns.countplot(b_data.Type)\nax.set_xticklabels(lencoder.inverse_transform([0,1,2,3,4,5]))\nax.set_xlabel(\"Book Type\",fontsize=20)\nax.set_ylabel(\"Count\",fontsize=20)\nax.set_title('Distibution Of Different Book Types In Our Data',fontsize=22)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Most of the books in our dataset are Paperback which is somewhat surprising taken in mind that we have so many alternatives for reading books using our phones, etc but still most of the books in our data are paperback. This may also suggest that most of these books are of older releases.</span></p>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.subplot(2,1,1)\nax = sns.distplot(b_data.Price,label=\"Price\")\nax.set_xlabel(\"Price\",fontsize=20)\nax.set_ylabel(\"Density\",fontsize=20)\ntextstr = '\\n'.join(\n        (r'$\\mu=%.2f$' % (b_data.Price.mean(),), r'$\\mathrm{median}=%.2f$' % (b_data.Price.median(),),\n         r'$\\sigma=%.2f$' % (b_data.Price.std(),)))\nprops = dict(boxstyle='round', facecolor='red', alpha=0.5)\nax.text(0.85, 0.85, textstr, transform=ax.transAxes, fontsize=14,\n            verticalalignment='center', bbox=props)\n\nax.set_title('Distribution Of Book Prices',fontsize=21)\nplt.show()\nb_data.Price = np.log(b_data.Price)\nplt.subplot(2,1,2)\nax = sns.distplot(b_data.Price,label=\"Price\")\nax.set_xlabel(\"Price\",fontsize=20)\nax.set_ylabel(\"Density\",fontsize=20)\ntextstr = '\\n'.join(\n        (r'$\\mu=%.2f$' % (b_data.Price.mean(),), r'$\\mathrm{median}=%.2f$' % (b_data.Price.median(),),\n         r'$\\sigma=%.2f$' % (b_data.Price.std(),)))\nprops = dict(boxstyle='round', facecolor='red', alpha=0.5)\nax.text(0.85, 0.85, textstr, transform=ax.transAxes, fontsize=14,\n            verticalalignment='center', bbox=props)\n\nax.set_title('Distribution Of Book Prices',fontsize=21)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>As with the other features we have a few outliers in the price range of our books but if we ignore the outliers we can see our distribution is centered around 46 USD and the distribution curve is quite narrow meaning that our standard deviation around the mean price is fairly low when ignoring the outliers.</span></p>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\ntitle_w = ''\n\nfor word in b_data.Book_title:\n    word = word.lower()\n    splited = re.findall(r'\\b[A-Za-z]+\\b',word)\n    splited = [w for w in splited if w not in stopwords]\n    title_w += ' '.join(splited)+ ' '\n\n\nwordcloud = WordCloud(width = 1100, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 8).generate(title_w) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (18, 11), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Not very surprising but we can see that the most common words in our book titles are programing related together with words like Introduction and Guide.\n</span></p>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"desc_w = ''\n\nfor word in b_data.Description:\n    word = word.lower()\n    splited = re.findall(r'\\b[A-Za-z]+\\b',word)\n    splited = [w for w in splited if w not in stopwords]\n    desc_w += ' '.join(splited)+ ' '\n\n\nwordcloud = WordCloud(width = 1100, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 8).generate(desc_w) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (18, 11), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>The description of the book similarly to the title of the book has words that dominate most of the description such as 'programming' and 'language', 'design' and 'computer' which is not surprising although one would expect more references to typical programming language names and we can see that the names of the programming languages are not as visible as the other words stated already.\n</span></p>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(20,11))\nax = sns.jointplot(x=b_data.Price,y=b_data.Rating,height=10,kind='kde',cmap='mako')\n#ax.set_xlabel(\"Price\",fontsize=20)\n#ax.set_ylabel(\"Density\",fontsize=20)\n#ax.set_title('Distribution Of Book Prices',fontsize=21)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Most of the books in or data set are books in the price range 40-50 USD and rating range 4.0-4.5\n</span></p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"correlations = b_data.corr('pearson')\nplt.figure(figsize=(20,11))\nax = sns.clustermap(correlations,annot=True,cmap='coolwarm',figsize=(20,11))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>We can see that we have some interesting correlations between our price and other features such as the number of reviews and the number of pages. we cannot use the number of reviews as a predictor although we have a fair correlation with the price feature it is a classic case of data leakage, we do not know the number of reviews until the book as published, but we can use the number of pages which has a high correlation and the types to estimate a book's price together with the book's type. as for the review scores, there is no significant correlation with any other feature but we will try using nearest neighbor models to approximate the review score.\n</span></p>\n\n"},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"background-color:skyblue;font-family:newtimeroman;font-size:250%;text-align:center\">Model Selection And Evaluation</h3>\n"},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"background-color:skyblue;font-family:newtimeroman;font-size:200%;text-align:center\">Price Prediction</h3>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['Number_Of_Pages','Type','Title_Average_Word_Length']\ntrain_x,test_x,train_y,test_y = train_test_split(b_data[features],b_data.Price)\n\nLR_pipe = Pipeline(steps=[('model',LinearRegression())])\nLR_scores = np.sqrt(-1*cross_val_score(LR_pipe,b_data[features],b_data.Price,cv=6,scoring='neg_mean_squared_error'))\n\nplt.figure(figsize=(20,11))\nax = sns.pointplot(x=np.arange(0,6),y=LR_scores)\nax.set_title('Cross Validation RMSE for LinearRegression',fontsize=20)\nax.set_ylabel('RMSE',fontsize=18)\nax.set_xlabel('Fold Number',fontsize=18)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"LinearRegression Average Cross Validation Score:\",LR_scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Can we lower our RMSE using a different model?\n</span></p>\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"RF_pipe = Pipeline(steps=[('model',RandomForestRegressor(n_estimators=50,max_leaf_nodes=15,random_state=42))])\n\nRF_scores = np.sqrt(-1*cross_val_score(RF_pipe,b_data[features],b_data.Price,cv=6,scoring='neg_mean_squared_error'))\n\nplt.figure(figsize=(20,11))\nax = sns.pointplot(x=np.arange(0,6),y=RF_scores,color='teal')\nax.set_title('Cross Validation RMSE for RandomForest',fontsize=20)\nax.set_ylabel('RMSE',fontsize=18)\nax.set_xlabel('Fold Number',fontsize=18)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"RandomForest Average Cross Validation Score:\",RF_scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,11))\nax = sns.residplot(x=b_data.Number_Of_Pages,y=b_data.Price)\nax.set_title('Absolute residuals vs Price',fontsize=19,fontweight='bold')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>We can clearly see that our plot shows a heteroscedastic pattern meaning that regression will do us not good. We will still try polynomial regression for learning purposes.\n</span></p>\n\n"},{"metadata":{},"cell_type":"markdown","source":"\n\n\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Not a significant change from Linear Regression\nLets plot and check out how our data is spread maybe a polynomial regression will improve are RMSE\n</span></p>\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,11))\nax = sns.scatterplot(x=b_data.Number_Of_Pages,y=b_data.Price,hue=b_data.Type,palette='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>We see that the curve we want to fit is somewhat exponential, lets try and use Polynomial Regression with a polynomial of a 2 degree \n</span></p>\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"pf = PolynomialFeatures(degree = 2)\nPR_pipe = Pipeline(steps = [('scale',StandardScaler()),('pf',pf), ('model',LinearRegression()) ])\n\n\n\nPR_scores = np.sqrt(-1*cross_val_score(PR_pipe,b_data[features],b_data.Price,cv=6,scoring='neg_mean_squared_error'))\n\n\nplt.figure(figsize=(20,11))\nax = sns.pointplot(x=np.arange(0,6),y=PR_scores)\nax.set_title('Cross Validation RMSE for Polynomial Regression',fontsize=20)\nax.set_ylabel('RMSE',fontsize=18)\nax.set_xlabel('Fold Number',fontsize=18)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Polynomial Regression Average Cross Validation Score:\",PR_scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Polynomial Regression didn't improve our RMSE \n</span></p>\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"KNR_pipe = Pipeline(steps=[('model',KNeighborsRegressor(n_neighbors=25))])\n\nKNR_scores = np.sqrt(-1*cross_val_score(KNR_pipe,b_data[features],b_data.Price,cv=6,scoring='neg_mean_squared_error'))\n\nplt.figure(figsize=(20,11))\nax = sns.pointplot(x=np.arange(0,6),y=KNR_scores,color='teal')\nax.set_title('Cross Validation RMSE for KNN',fontsize=20)\nax.set_ylabel('RMSE',fontsize=18)\nax.set_xlabel('Fold Number',fontsize=18)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"KNN Average Cross Validation Score:\",KNR_scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>So we tested a few models we saw that the best RMSE we got was using the good old linear regression, lets train and fit our model and check out will stacking our model result will improve the final RMSE score \n</span></p>\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"b_data = b_data.sample(frac=1)\nLR_pipe.fit(b_data[features],b_data.Price)\nPR_pipe.fit(b_data[features],b_data.Price)\nKNR_pipe.fit(b_data[features],b_data.Price)\nRF_pipe.fit(b_data[features],b_data.Price)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LR_Predict = LR_pipe.predict(b_data[features])\n\nplt.figure(figsize=(20,11))\nax= sns.lineplot(x=np.arange(0,b_data.shape[0]),y=b_data.Price,label='Actual',color='green')\nax= sns.lineplot(x=np.arange(0,b_data.shape[0]),y=LR_Predict,label='LinearReg Prediciton',color='red')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ens_Predict =LR_pipe.predict(b_data[features])*0.1+PR_pipe.predict(b_data[features])*0.3 + RF_pipe.predict(b_data[features])*0.6\n\nax= sns.lineplot(x=np.arange(0,b_data.shape[0]),y=b_data.Price,label='Actual',color='green')\nax= sns.lineplot(x=np.arange(0,b_data.shape[0]),y=ens_Predict,label='Stacked Prediciton',color='red')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Stacked Model RMSE: ',np.sqrt(mean_squared_error(ens_Predict,b_data.Price)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensmble_results = pd.DataFrame({'LR_Pred':LR_pipe.predict(b_data[features]),\n                               'PR_Pred':PR_pipe.predict(b_data[features]),\n                               'KNR_Pred':KNR_pipe.predict(b_data[features]),\n                               'RF_Pred':RF_pipe.predict(b_data[features])})\n\nensmble_results.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain_x,test_x,train_y,test_y = train_test_split(ensmble_results,b_data.Price,random_state=42)\nxgb_m = XGBRegressor(random_state=42)\n\n\nxgb_m.fit(train_x,train_y)\nxgb_predictions = xgb_m.predict(test_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax= sns.lineplot(x=np.arange(0,test_y.shape[0]),y=test_y,label='Actual',color='green')\nax= sns.lineplot(x=np.arange(0,test_y.shape[0]),y=xgb_predictions,label='Stacked Prediciton',color='red')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Ensemble Model RMSE: ',np.sqrt(mean_squared_error(test_y,xgb_predictions)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}