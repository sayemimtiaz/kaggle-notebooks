{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.simplefilter('ignore')\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nfrom pprint import pprint\nfrom datetime import datetime\nimport collections\nimport re\n\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\nfrom nltk.corpus import wordnet, stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\nfrom wordcloud import WordCloud","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_name = '../input/stock-markettweets-lexicon-data/tweets_labelled_09042020_16072020.csv'\ndata = pd.read_csv(file_name, sep=';').set_index('id')\ndata.shape","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Updated 2021/5/11","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Quick view of tweets","metadata":{}},{"cell_type":"code","source":"for i in range(5):\n    pprint(data.iat[i,1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ticker_pattern = re.compile(r'(^\\$[A-Z]+|^\\$ES_F)')\nht_pattern = re.compile(r'#\\w+')\n\nticker_dic = collections.defaultdict(int)\nht_dic = collections.defaultdict(int)\n\nfor text in data['text']:\n    for word in text.split():\n        if ticker_pattern.fullmatch(word) is not None:\n            ticker_dic[word[1:]] += 1\n        \n        word = word.lower()\n        if ht_pattern.fullmatch(word) is not None:\n            ht_dic[word] += 1","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tweet texts contain some ticker symbols  \n> For exsample, $AMD in 5th row","metadata":{}},{"cell_type":"code","source":"ticker_df = pd.DataFrame.from_dict(\n    ticker_dic, orient='index').rename(columns={0:'count'})\\\n    .sort_values('count', ascending=False).head(20)\n    \nht_df = pd.DataFrame.from_dict(\n    ht_dic, orient='index').rename(columns={0:'count'})\\\n    .sort_values('count', ascending=False).head(20)\n\nfig, ax = plt.subplots(1, 2, figsize=(12,8))\nplt.suptitle('Frequent Tickers and Hashtags', fontsize=16)\nplt.subplots_adjust(wspace=0.4)\n\nsns.barplot(x=ticker_df['count'], y=ticker_df.index, orient='h', ax=ax[0])\nax[0].set_title('Top 20 Tickers')\n\nsns.barplot(x=ht_df['count'], y=ht_df.index, orient='h', ax=ax[1])\nax[1].set_title('Top 20 HashTags')\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PreProccess for WordCloud  \n### * delete emoji, handle, URL,...\n### * lemmatize\n### * delete stop_word including 'RT'","metadata":{}},{"cell_type":"code","source":"charonly = re.compile(r'[^a-zA-Z\\s]')\nhandle_pattern = re.compile(r'@\\w+')\nemoji_pattern = re.compile(\"[\"\n                        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                        u\"\\U00002702-\\U000027B0\"\n                        u\"\\U000024C2-\\U0001F251\"\n                        \"]+\", flags=re.UNICODE)\nurl_pattern = re.compile(\n    'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\npic_pattern = re.compile('pic\\.twitter\\.com/.{10}')\nspecial_code = re.compile(r'(&amp;|&gt;|&lt;)')\ntag_pattern = re.compile(r'<.*?>')\n\nSTOPWORDS = set(stopwords.words('english')).union(\n    {'rt', 'retweet', 'RT', 'Retweet', 'RETWEET'})\n\nlemmatizer = WordNetLemmatizer()\n\ndef hashtag(phrase):\n    return ht_pattern.sub(' ', phrase)\n\ndef remove_ticker(phrase):\n    return ticker_pattern.sub('', phrase)\n    \ndef specialcode(phrase):\n    return special_code.sub(' ', phrase)\n\ndef emoji(phrase):\n    return emoji_pattern.sub(' ', phrase)\n\ndef url(phrase):\n    return url_pattern.sub('', phrase)\n\ndef pic(phrase):\n    return pic_pattern.sub('', phrase)\n\ndef html_tag(phrase):\n    return tag_pattern.sub(' ', phrase)\n\ndef handle(phrase):\n    return handle_pattern.sub('', phrase)\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n    \n    # DIS, ticker symbol of Disney, is interpreted as the plural of \"DI\" \n    # in WordCloud, so I converted it to Disney\n    phrase = re.sub('DIS', 'Disney', phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"(he|He)\\'s\", \"he is\", phrase)\n    phrase = re.sub(r\"(she|She)\\'s\", \"she is\", phrase)\n    phrase = re.sub(r\"(it|It)\\'s\", \"it is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"(\\'ve|has)\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\ndef onlychar(phrase):\n    return charonly.sub('', phrase)\n\ndef remove_stopwords(phrase):\n    return \" \".join([word for word in str(phrase).split()\\\n                     if word not in STOPWORDS])\n\ndef tokenize_stem(phrase):   \n    tokens = word_tokenize(phrase)\n    stem_words =[]\n    for token in tokens:\n        word = lemmatizer.lemmatize(token)\n        stem_words.append(word)        \n    buf = ' '.join(stem_words)    \n    return buf","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def arrange_text(ds):\n    ds['text2'] = ds['text'].apply(emoji)\n    ds['text2'] = ds['text2'].apply(handle)\n    ds['text2'] = ds['text2'].apply(specialcode)\n    ds['text2'] = ds['text2'].apply(hashtag)\n    ds['text2'] = ds['text2'].apply(url)\n    ds['text2'] = ds['text2'].apply(pic)\n    ds['text2'] = ds['text2'].apply(html_tag)\n    ds['text2'] = ds['text2'].apply(onlychar)\n    ds['text2'] = ds['text2'].apply(decontracted)\n    ds['text2'] = ds['text2'].apply(onlychar)\n    ds['text2'] = ds['text2'].apply(tokenize_stem)\n    ds['text2'] = ds['text2'].apply(remove_stopwords)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arrange_text(data)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Quick view of preprocessed tweets","metadata":{}},{"cell_type":"code","source":"for i in range(5):\n    pprint(data.iat[i,3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## WordCloud","metadata":{}},{"cell_type":"markdown","source":"> \"DIS\", ticker of The Walt Disney Company, is interpreted as the plural of \"DI\" in WordCloud, so I converted it to \"Disney\"","metadata":{}},{"cell_type":"code","source":"words = ' '.join([text for text in data['text2']])\nwordcloud = WordCloud(\n    width=800, height=400, background_color='white', max_font_size=110)\\\n    .generate(words)\n\nplt.figure(figsize=(14, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('Words in all tweet\\n', fontsize=24)\nplt.axis('off')\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Too much Tickers!!  \n### So I created WordCloud without Ticker because the words that symbolize sentiment are not obvious.","metadata":{}},{"cell_type":"code","source":"not_ticker = [] # list of words except for Ticker\n\nfor text in data['text2']:\n    for word in text.split():\n        if word.upper() not in ticker_dic:\n            not_ticker.append(word)\n            \nwords = ' '.join([word for word in not_ticker])\nwordcloud = WordCloud(\n    width=800, height=400, background_color='white', max_font_size=110).\\\n    generate(words)\nplt.figure(figsize=(14, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('Words in all tweet without Ticker\\n', fontsize=24)\nplt.axis('off')\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### WordCloud by sentiment  \nTickers are excluded, hereafter.","metadata":{}},{"cell_type":"code","source":"def wordcloud_by_sentiment(sentiment):\n    not_ticker = []\n\n    for text in data[data['sentiment']==sentiment]['text2']:\n        for word in text.split():\n            if word.upper() not in ticker_dic:\n                not_ticker.append(word.lower())\n\n    words = ' '.join([word for word in not_ticker])\n    wordcloud = WordCloud(\n        width=800, height=400, background_color='white', max_font_size=110, max_words=100).\\\n        generate(words)\n    plt.figure(figsize=(14, 7))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.title('Words in '+ sentiment.capitalize() + ' tweets\\n', fontsize=32)\n    plt.axis('off')\n    plt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud_by_sentiment('positive')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\"buy\", \"bullish\", \"good\", \"higher\", \"rally\",...","metadata":{}},{"cell_type":"code","source":"wordcloud_by_sentiment('negative')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\"sell\", \"short\", \"loss\", \"lower\", \"bearish\", ...  \nParticularly noteworthy are the words in small fonts, such as \"volatility\", \"risk\", \"short interest\", \"covid\",...","metadata":{}},{"cell_type":"code","source":"wordcloud_by_sentiment('neutral')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]}]}