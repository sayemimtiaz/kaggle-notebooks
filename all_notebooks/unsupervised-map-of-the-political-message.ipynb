{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Unsupervised map of the political message on Twitter\n\n![a](https://i.imgur.com/jPBQ6di.jpg)\n\n\n\nMy goal is to plot a 2D map of the political message broadcasted from the candidates Twitter accounts. \n\n# Method used\n\n- First I clean the tweets and select a subset relevant to my scope.\n- Use a simple GloVe + GRU RNN to identify the tweet author. Very high accuracy is not needed, a few percentage points difference wouldn't matter.\n- Use *predict_proba* score as new features\n- Apply PCA and keep the first 2 components\n\n# Check the results\n\nA good map should cluster together ideologically similar candidates. To check if the map has any value I will plot the Republican candidates in red, and the Progressive Senators in yellow.\n\nThe \"unbiased\" (at least no bias from me) data for progressiveness comes from [progressivepunch](https://progressivepunch.org/scores.htm?house=senate). I would have marked more ideological groups, but this gets more complicated when moving towards the moderates.\n\nIf there is any merit in my approach three groups should emerge. Judge for yourself!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport re\nfrom datetime import datetime\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nimport colorlover as cl\n\nplotly.offline.init_notebook_mode() \n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils.multiclass import unique_labels\nfrom sklearn.decomposition import PCA\n\nfrom keras.models import Model\nfrom keras.optimizers import Adamax\nfrom sklearn.metrics import log_loss\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Embedding, Dense, Bidirectional, CuDNNGRU, GlobalMaxPooling1D\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define functions"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def clean(text):\n    \n    text = re.sub(r'#\\S*', ' ', text) \n    text = re.sub(r'@\\S*', ' ', text) \n    text = re.sub(r'http\\S*', ' ', text)\n    \n    for ch in ['\\\\','`','*','_','{','}','[',']','(',')','>','+','-','.','!','\\'',\"\\”\",'\\\"', '\\“', \"\\’\", \"?\", \":\",\n               \"-\",\",\", \"//t\", \"&amp;\", \"/\", \"'\", \"'\", \"…\",\"-\", \"’\", \"\\—\", \"—\", \"–\", \"“\", \"”\"]:\n        if ch in text:\n            text = text.replace(ch,\" \")\n\n    return(text)  \n\ndef clean_tweet(tweet):\n    return ' '.join((clean(tweet.lower())).split())\n\ndef print_table(header_values, content, colors):\n    data = go.Table(\n    \n      header = dict(\n        values = header_values ,\n        line = dict(color = \"rgb(70,130,180)\"),\n        fill = dict(color = \"rgb(70,130,180)\"),\n        align = 'center',\n        font = dict(color = 'black', size = 9)\n      ),\n      cells = dict(\n        values = content,\n        fill = colors,  \n        align = 'center',\n        font = dict(color = 'black', size = 9),\n        height = 40\n        ))\n\n    plotly.offline.iplot([data])\n\ndef load_glove(word_index):\n    EMBEDDING_FILE = '../input/glove840b300dtxt/glove.840B.300d.txt'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    # word_index = tokenizer.word_index\n    nb_words = min(18000, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= 18000: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n            \n    return embedding_matrix \n\ndef load_data(X, y, train_index, test_index):\n        \n    ## split to train and val    \n    Xtr = X.iloc[train_index]\n    Xte = X.iloc[test_index]\n\n    ## Tokenize the sentences\n    tokenizer = Tokenizer(num_words=18000)\n    tokenizer.fit_on_texts(list(Xtr))\n    Xtr = tokenizer.texts_to_sequences(Xtr)\n    Xte = tokenizer.texts_to_sequences(Xte)\n\n    ## Pad the sentences \n    Xtr = pad_sequences(Xtr, maxlen=50)\n    Xte = pad_sequences(Xte, maxlen=50)\n\n    ## Get the target values\n    ytr = y.iloc[train_index]\n    yte = y.iloc[test_index]\n\n    word_index = tokenizer.word_index\n    \n    return Xtr, ytr, Xte, yte, word_index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clean the Tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_files = os.listdir(\"../input/2020-united-states-presidential-election/twitter\")\ntwitter_users_files = os.listdir(\"../input/2020-united-states-presidential-election/twitter_users\")\npic_files = os.listdir(\"../input/2020-united-states-presidential-election/pics\")\nmetadata = pd.read_csv(\"../input/2020-united-states-presidential-election/candidates_info.csv\")\n\nmetadata[\"filename\"] = metadata[\"handle\"].apply(lambda x: x[1:])\nmetadata[\"age\"] = ((datetime.today() - pd.to_datetime(metadata[\"born\"])).dt.days/365).astype(int)\nmetadata = metadata.sort_values(\"filename\")\nmetadata.reset_index(inplace=True)\n\ndataset = pd.DataFrame()\n\nfor index, row in metadata.iterrows():\n    \n    df = pd.read_csv(\"../input/2020-united-states-presidential-election/twitter/%s.csv\"%row[\"filename\"])\n    dataset = pd.concat([dataset,df],ignore_index=True)\n    \ndataset[\"clean tweet\"] = dataset['Text'].apply(clean_tweet)\ndataset['number of characters'] = dataset[\"clean tweet\"].str.len()\n\nmetadata[\"number of all tweets\"] = dataset.groupby([\"Screen Name\"]).count()[\"Tweet Id\"].values\nmetadata[\"tweets in english\"] = dataset[dataset[\"Language\"] == \"English\"].groupby([\"Screen Name\"]).count()[\"Tweet Id\"].values\nmetadata[\"tweets not in english\"] = dataset[dataset[\"Language\"] != \"English\"].groupby([\"Screen Name\"]).count()[\"Tweet Id\"].values\nmetadata[\"tweets only\"] = dataset[dataset[\"Tweet Type\"] == \"Tweet\"].groupby([\"Screen Name\"]).count()[\"Tweet Id\"].values\nmetadata[\"retweets only\"] = dataset[dataset[\"Tweet Type\"] == \"Retweet\"].groupby([\"Screen Name\"]).count()[\"Tweet Id\"].values\nmetadata[\"replies only\"] = dataset[dataset[\"Tweet Type\"] == \"Reply\"].groupby([\"Screen Name\"]).count()[\"Tweet Id\"].values\nmetadata[\"after 2019\"] = dataset[dataset[\"Created At\"].astype(\"datetime64\").dt.year > 2018].groupby([\"Screen Name\"]).count()[\"Tweet Id\"].values\nmetadata[\"before 2019\"] = metadata[\"number of all tweets\"] - metadata[\"after 2019\"]\nmetadata[\"more than 40 characters\"] = dataset[dataset[\"number of characters\"] > 39].groupby([\"Screen Name\"]).count()[\"Tweet Id\"].values\nmetadata[\"less than 40 characters\"] = dataset[dataset[\"number of characters\"] < 40].groupby([\"Screen Name\"]).count()[\"Tweet Id\"].values\n\ndataset = dataset[dataset[\"Created At\"].astype(\"datetime64\").dt.year > 2018]  \ndataset = dataset[dataset[\"number of characters\"] > 39]\ndataset = dataset[dataset[\"Tweet Type\"] == \"Tweet\"]\ndataset = dataset[dataset[\"Language\"] == \"English\"]\n\nX = dataset[\"clean tweet\"]\ny = dataset[\"Name\"]\n\nmetadata[\"useful tweets\"] = dataset.groupby([\"Screen Name\"]).count()[\"Tweet Id\"].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tweet selection\n \nMost candidates have ~3200 tweets in the database, but not all are used for separating candidates in clusters.\n\n- keep only \"Tweets in English\" (for this purpose it's another way of saying that I only use worded tweets, not picture or links)\n- drop retweets\n- drop replies\n- drop tweets before 2019, irrelevant to the current campaign\n- drop tweets shorter than 40 characters.\n\nWhen all is said and done, I am left with most candidates having a reasonable amount of tweets, but a few without enough data to work on. Going forward I do not account for the imbalance in the dataset, so please keep in mind this fact when interpreting the results."},{"metadata":{"trusted":true},"cell_type":"code","source":"columns =  [\"name\", \"number of all tweets\", \"tweets in english\", \"tweets not in english\",\n           \"tweets only\", \"retweets only\", \"replies only\", \"after 2019\", \"before 2019\", \n           \"more than 40 characters\", \"less than 40 characters\", \"useful tweets\"]\n\nheader_values = ['<b>%s</b>'%x for x in columns]\ncontent = metadata.sort_values(\"useful tweets\", ascending=False)[columns].T\ncolors = dict()\nprint_table(header_values, content, colors)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict the tweet author using GloVe and GRU\n\nAccuracy is ~50%"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nkfold = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\nscores = []\ntest_list = []\npredict_list = []\n\ntest_df = pd.DataFrame()\npred_df = pd.DataFrame()\n\nfor train_index, test_index in kfold.split(X,y):\n    \n    yoh = pd.get_dummies(y)\n    train_X, train_y, test_X, test_y, word_index = load_data(X, yoh, train_index, test_index)\n    \n    embedding = load_glove(word_index)\n    \n    inp = Input(shape=(50,))\n    x = Embedding(18000, 300, weights=[embedding])(inp)\n\n    x = Bidirectional(CuDNNGRU(256, return_sequences=True))(x)\n    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n    x = GlobalMaxPooling1D()(x)\n    x = Dense(train_y.shape[1],activation='softmax')(x)\n    \n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='categorical_crossentropy', optimizer=Adamax(lr=0.002), metrics=['accuracy'])\n\n    model.fit(train_X, train_y, batch_size=1024, epochs=15, validation_data=(test_X, test_y), verbose=0)\n    pred_y = model.predict(test_X)\n\n    \n    test_df = pd.concat([test_df,test_y], axis=0, ignore_index = True)\n    pred_df = pd.concat([pred_df,pd.DataFrame(pred_y)], axis=0, ignore_index = True)\n    \npred_df.columns=test_df.columns.values\ntest_label = test_df.idxmax(axis=1)\npred_label = pred_df.idxmax(axis=1)\n\nprint (\"Accuracy:\", accuracy_score(test_label, pred_label))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Confussion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(test_label, pred_label, test_df.columns.values)\ncm = ((cm.astype('float')*100 / cm.sum(axis=1)[:, np.newaxis] + 0.5).astype('int'))/100\n   \n\nf, ax = plt.subplots(figsize=(14, 12))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(cm, annot=True,annot_kws={\"size\": 8}, xticklabels=test_df.columns.values, yticklabels=test_df.columns.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final Result"},{"metadata":{"trusted":true},"cell_type":"code","source":"progressive_candidates = [\"Bernie Sanders\", \"Elizabeth Warren\", \"Kamala Harris\", 'Cory Booker', 'Kirsten Gillibrand' ]\nrepublican = ['Donald J. Trump', 'Mike Pence', 'Gov. Bill Weld']\nall_others = ['Amy Klobuchar', 'Andrew Yang', \"Beto O'Rourke\",'Bill de Blasio', 'Eric Swalwell', 'Jay Inslee', 'John Delaney', \n              'John Hickenlooper', 'Julián Castro', 'Marianne Williamson', 'Michael Bennet', 'Sen. Mike Gravel', 'Seth Moulton',\n              'Steve Bullock', 'Tim Ryan', 'Tulsi Gabbard', 'Wayne Messam', \"Pete Buttigieg\", \"Joe Biden\"]\n\n\npred_df[\"true_label\"] = test_label\naverage_pred = pred_df.groupby(['true_label']).sum()/pred_df.groupby(['true_label']).count()\n\npca = PCA(n_components=2)\npca.fit(average_pred)\nboiled_down = pd.DataFrame(data=pca.transform(average_pred),index=test_df.columns.values, columns=[\"a\",\"b\"])\n\nfig = {\n    'data': [\n       {'x': boiled_down.loc[progressive_candidates].a, 'y': boiled_down.loc[progressive_candidates].b, 'text': boiled_down.loc[progressive_candidates].index, \n        'marker': {'color': 'rgb(251,169,46)', 'size': 6},\n        'mode': 'markers',\n         'name' :'Progressive Senators' \n       },\n       {'x': boiled_down.loc[all_others].a, 'y': boiled_down.loc[all_others].b, 'text': boiled_down.loc[all_others].index, \n        'marker': {'color': 'rgb(0,138,147)', 'size': 6},\n        'mode': 'markers',\n         'name' :'All Others'\n       },\n       {'x': boiled_down.loc[republican].a, 'y': boiled_down.loc[republican].b, 'text': boiled_down.loc[republican].index, \n        'marker': {'color': 'rgb(143,26,29)', 'size': 6},\n        'mode': 'markers',\n         'name' :'Republican'\n       },\n    ],\n}\n\nplotly.offline.iplot(fig)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}