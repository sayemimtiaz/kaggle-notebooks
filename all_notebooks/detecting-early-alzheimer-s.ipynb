{"cells":[{"metadata":{"_uuid":"31aff975d245f351c6a47a411251c09cf0df142a","colab_type":"text","id":"RPIBkis9u_2Q","_cell_guid":"ee5eaec8-b877-469e-aa1f-2a19dd998356"},"cell_type":"markdown","source":"# DETECTING EARLY ALZHEIMER'S USING MRI DATA AND MACHINE LEARNING\n----\n## TABLE OF CONTENT\n1. Problem Statement\n2. Data\n  1. Dataset Description\n  2. Column Descriptors\n3. Related Work\n4. Exploratory Data Analysis\n5. Data Precrocessing\n  1. Removing rows with missing values\n  2. Imputation\n  3. Splitting Train/Validation/Test Sets\n  4. Cross-validation\n6. Model\n  1. Performance Measure\n  2. Logistic Regression\n  3. Support Vector Machine\n  4. Decision Tree\n  5. Random Forest Classifier\n  6. AdaBoost\n7. Conclusion\n  1. Results\n  2. Unique Approach\n  3. Implementation\n  4. Limitation\n  5. Further Research\n8. Acknowledgements\n\n## TEAM MEMBERS\n1. Hyunseok Choi\n2. Kyuri Song\n3. Saurin Parikh\n"},{"metadata":{"_uuid":"fedeb7d2f5ade1370c0422693aedff04d4d886f8","colab_type":"text","id":"Rqz5sHwoScR7","_cell_guid":"286780e9-d00e-4c73-ba1f-b942ad211133"},"cell_type":"markdown","source":"# 1. PROBLEM STATEMENT\n---\n## ALZHEIMER'S DISEASE\n\n* [Alzheimer's disease (AD)](https://en.wikipedia.org/wiki/Alzheimer%27s_disease) is a neurodegenerative disorder of uncertain cause and pathogenesis that primarily affects older adults and is the most common cause of dementia.\n* The earliest clinical manifestation of AD is selective memory impairment and while treatments are available to ameliorate some symptoms, there is no cure currently available.\n* Brain Imaging via magnetic resonance imaging (MRI), is used for evaluation of patients with suspected AD.\n* MRI findings include both, local and generalized shrinkage of brain tissue. Below is a pictorial representation of tissue shrinkage: ![braintissue](./Alzheimer's_disease_brain_comparison.jpg)\n* Some studies have suggested that MRI features may predict rate of decline of AD and may guide therapy in the future.\n* However in order to reach that stage clinicians and researchers will have to make use of machine learning techniques that can accurately predict progress of a patient from mild cognitive impairment to dementia.\n* We propose to develop a sound model that can help clinicians do that and predict early alzheimer's."},{"metadata":{"_uuid":"6fe31a2ec5929f28eff868ae06f907daeb26fb5c","colab_type":"text","id":"FG0zBhPkScR7","_cell_guid":"199a3108-4520-47f7-982b-2c79a840bffa"},"cell_type":"markdown","source":"# 2. DATA\n---\nThe team has found MRI related data that was generated by the Open Access Series of Imaging Studies (OASIS) project that is available both, on their [website](www.oasis-brains.org) and [kaggle](www.kaggle.com/jboysen/mri-and-alzheimers) that can be utilized for the purpose of training various machine learning models to identify patients with mild to moderate dementia.\n\n## 2.A DATASET DESCRIPTION\n* We will be using the [longitudinal MRI data](http://www.oasis-brains.org/pdf/oasis_longitudinal.csv).\n* The dataset consists of a longitudinal MRI data of 150 subjects aged 60 to 96.\n* Each subject was scanned at least once.\n* Everyone is right-handed.\n* 72 of the subjects were grouped as 'Nondemented' throughout the study.\n* 64 of the subjects were grouped as 'Demented' at the time of their initial visits and remained so throughout the study.\n* 14 subjects were grouped as 'Nondemented' at the time of their initial visit and were subsequently characterized as 'Demented' at a later visit. These fall under the 'Converted' category.\n\n## 2.B COLUMN DESCRIPTORS  \n\n|COL  |FULL-FORMS                          |\n|-----|------------------------------------|\n|EDUC |Years of education                  |\n|SES  |Socioeconomic Status                |\n|MMSE |[Mini Mental State Examination](http://www.dementiatoday.com/wp-content/uploads/2012/06/MiniMentalStateExamination.pdf)       |\n|CDR  |[Clinical Dementia Rating](http://knightadrc.wustl.edu/cdr/PDFs/CDR_Table.pdf)            |\n|eTIV |[Estimated Total Intracranial Volume](https://link.springer.com/article/10.1007/s12021-015-9266-5) |\n|nWBV |[Normalize Whole Brain Volume](https://www.ncbi.nlm.nih.gov/pubmed/11547042)        |\n|ASF  |[Atlas Scaling Factor](http://www.sciencedirect.com/science/article/pii/S1053811904003271)                |\n"},{"metadata":{"_uuid":"984645524fc70c97e6546508e850c27e6bbc96cf","colab_type":"text","id":"NbmbuaAZmMsj","_cell_guid":"195e3926-4b16-482e-8af1-4cff8717d978"},"cell_type":"markdown","source":"# 3. RELATED WORK\n---\n\nThe original publication has only done some preliminary exploration of the MRI data as majority of their work was focused towards data gathering. However, in the recent past there have been multiple efforts that have been made to detect early-alzheimers using MRI data. Some of the work that was found in the literature was as follows:\n\n1) **Machine learning framework for early MRI-based Alzheimer's conversion prediction in MCI subjects.** [3]\n\n   In this paper the authors were interested in identifying mild cognitive impairment(MCI) as a transitional stage between age-related coginitive decline and Alzheimer's. The group proposes a novel MRI-based biomaker that they developed using machine learning techniques. They used data available from the Alzheimer's Disease Neuroimaging Initiative [ADNI](http://adni.loni.usc.edu/) Database. The paper claims that their aggregate biomarker achieved a 10-fold cross-validation area under the curve (AUC) score of 0.9020 in discriminating between progressive MCI (pMCI) and stable MCI (sMCI).\n   \n   Noteworthy Techniques:\n   1. Semi-supervised learning on data available from AD patients and normal controls, without using MCI patients, to help with the sMCI/pMCI classification. Performed feature selection using regularized logistic regression.\n   2. They removed aging effects from MRI data before classifier training to prevent possible confounding between changes due to AD and those due to normal aging.\n   3. Finally constructed an aggregate biomarker by first learning a separate MRI biomarker and then combining age and cognitive measures about MCI subjects by applying a random foresst classifier.\n\n\n2) **Detection of subjects and brain regions related to Alzheimer's disease using 3D MRI scans based on eigenbrain and machine learning.** [4] \n    \n    The authors of this paper have proposed a novel computer-aided diagnosis (CAD) system for MRI images of brains based on eigenbrains [(eg.)](https://www.frontiersin.org/files/Articles/138015/fncom-09-00066-HTML/image_m/fncom-09-00066-t010.jpg) and machine learning. In their approach they use key slices from the 3D volumetric data generated from the MRI and then generate eigenbrain images based on [EEG](https://en.wikipedia.org/wiki/Electroencephalography) data. They then used kernel support-vector-machines with different kernels that were trained by particle swarm optimization. The accuracy of their polynomial kernel (92.36 $\\pm$ 0.94) was better than their linear (91.47 $\\pm$ 1.02) anf radial basis function (86.71 $\\pm$ 1.93) kernels.\n\n\n3) **Support vector machine-based classification of Alzheimer’s disease from whole-brain anatomical MRI.** [5]\n\n    In this paper the authors propose a new method to discriminate patients with AD from elderly controls based on support vector machine (SVM) classification of whole-brain anatomical MRI. The authors used three-dimensional T1-weighted MRI images from 16 patients with AD and 22 elderly controls and parcellated them into regions of interests (ROIs). They then used a SVM algorithm to classify subjects based upon the gray matter characteristics of these ROIs. Based on their results the classifier obtained 94.5% mean correctness.\n    \n    The possible downfalls of their technique might be the fact that they haven't taken age related changes in the gray matter into account and they were working with a small data set.\n    \n    \nWe have described 3 papers over here that we found the most interesting, however there are a few more that have explored the same question. Regardless, it is worthwhile to mention that the above papers were exploring raw MRI data and we, on the other hand, are dealing with 3 to 4 biomarkers that are generated from MRI images."},{"metadata":{"_uuid":"566105ab7a54c9420a4f649dea3fa1f52da1d79d","colab_type":"text","id":"XZY2wtKYtH3h","_cell_guid":"051e92d2-afac-4696-b7d9-1942a6544a5b"},"cell_type":"markdown","source":"# 4. EXPLORATORY DATA ANALYSIS (EDA)\n---\n\nIn this section, we have focused on exploring the relationship between each feature of MRI tests and dementia of the patient. The reason we conducted this Exploratory Data Analysis process is to state the relationship of data explicitly through a graph so that we could assume the correlations before data extraction or data analysis. It might help us to understand the nature of the data and to select the appropriate analysis method for the model later.\n\nThe minimum, maximum, and average values of each feature for graph implementation are as follows.\n\n||Min|Max|Mean|\n|---\n|Educ|6|23|14.6|\n|SES|1|5|2.34\n|MMSE|17|30|27.2|\n|CDR|0|1|0.29|\n|eTIV|1123|1989|1490|\n|nWBV|0.66|0.837|0.73|\n|ASF|0.883|1.563|1.2|"},{"metadata":{"_cell_guid":"dd2db392-d948-4ce9-930b-58c92e65a960","_uuid":"6a720755860b07d7eb93e458306ec1b6fa079ed5","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nsns.set()\n\ndf = pd.read_csv('../input/oasis_longitudinal.csv')\ndf.head()","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"778edfd8-7209-45ef-a872-d8eab2a14ef1","collapsed":true,"_uuid":"08991ab4c6c3aef89ad366662aad97a846aa9d8f","trusted":true},"cell_type":"code","source":"df = df.loc[df['Visit']==1] # use first visit data only because of the analysis we're doing\ndf = df.reset_index(drop=True) # reset index after filtering first visit data\ndf['M/F'] = df['M/F'].replace(['F','M'], [0,1]) # M/F column\ndf['Group'] = df['Group'].replace(['Converted'], ['Demented']) # Target variable\ndf['Group'] = df['Group'].replace(['Demented', 'Nondemented'], [1,0]) # Target variable\ndf = df.drop(['MRI ID', 'Visit', 'Hand'], axis=1) # Drop unnecessary columns","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"bc318bbd-443d-4099-b6ec-28eb9deb3add","colab":{"output_extras":[{"item_id":1}],"autoexec":{"wait_interval":0,"startup":false},"height":919,"base_uri":"https://localhost:8080/"},"_uuid":"9175b97a0b02ff8ace2e3bc8a6327488b3232783","outputId":"5f5cc170-02d6-41d4-f5f2-3b575833b56b","executionInfo":{"user":{"displayName":"Saurin Parikh","photoUrl":"//lh3.googleusercontent.com/-6RG-0wrBKjU/AAAAAAAAAAI/AAAAAAAABpU/h5Zwf5zd3tk/s50-c-k-no/photo.jpg","userId":"104703813675171986785"},"timestamp":1512599691671,"status":"error","user_tz":300,"elapsed":11761},"id":"-4-ZVrHJslSF","colab_type":"code","collapsed":true,"trusted":false},"cell_type":"code","source":"# bar drawing function\ndef bar_chart(feature):\n    Demented = df[df['Group']==1][feature].value_counts()\n    Nondemented = df[df['Group']==0][feature].value_counts()\n    df_bar = pd.DataFrame([Demented,Nondemented])\n    df_bar.index = ['Demented','Nondemented']\n    df_bar.plot(kind='bar',stacked=True, figsize=(8,5))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"adc2e6d4-b622-48dc-a94b-7214534633e6","colab":{"output_extras":[{},{}],"autoexec":{"wait_interval":0,"startup":false}},"_uuid":"5f8599d4f70b78047011217e3bbcaa26311fddc6","outputId":"d9940867-44f6-4d3a-f6fb-ddef5630877e","id":"k-92eJ7pslSL","colab_type":"code","trusted":false,"collapsed":true},"cell_type":"code","source":"# Gender  and  Group ( Femal=0, Male=1)\nbar_chart('M/F')\nplt.xlabel('Group')\nplt.ylabel('Number of patients')\nplt.legend()\nplt.title('Gender and Demented rate')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46c3ccb6893e52a6636e9a764539a92084f90407","colab_type":"text","id":"9m16iCGyslSO","_cell_guid":"0e1779da-b47b-4d1f-b723-c292050e4585"},"cell_type":"markdown","source":"The above graph indicates that men are more likely with dementia than women."},{"metadata":{"_cell_guid":"2efa6bd7-2c72-4b56-aad3-2f6f7b7df847","colab":{"output_extras":[{},{}],"autoexec":{"wait_interval":0,"startup":false}},"_uuid":"e1b8fbe86bbf469971aed915c2b8e9caa9cdbe53","outputId":"b3a6a8a7-5cd6-48eb-e631-20b8d30c3bad","id":"QfZMuTl7slSP","colab_type":"code","trusted":false,"collapsed":true},"cell_type":"code","source":"#MMSE : Mini Mental State Examination\n# Nondemented = 0, Demented =1\n# Nondemented has higher test result ranging from 25 to 30. \n#Min 17 ,MAX 30\nfacet= sns.FacetGrid(df,hue=\"Group\", aspect=3)\nfacet.map(sns.kdeplot,'MMSE',shade= True)\nfacet.set(xlim=(0, df['MMSE'].max()))\nfacet.add_legend()\nplt.xlim(15.30)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d01f7c6b66442f0b1eeb814464cec8654a24945","colab_type":"text","id":"WumyA7d6slSR","_cell_guid":"1c341b9c-86c0-4dcd-a6d0-f35d15e4aedf"},"cell_type":"markdown","source":"The chart shows Nondemented group got much more higher MMSE scores than Demented group."},{"metadata":{"_cell_guid":"dde2291b-7caf-4b9e-bcb6-b56caaec9bbe","colab":{"output_extras":[{},{},{},{}],"autoexec":{"wait_interval":0,"startup":false}},"_uuid":"4dc483b489417595af5d23cf7a44de13c8ba371d","outputId":"4088e3c2-8493-41d2-c61a-c386e784fd37","id":"JPuWkiWGslSS","colab_type":"code","trusted":false,"collapsed":true},"cell_type":"code","source":"#bar_chart('ASF') = Atlas Scaling Factor\nfacet= sns.FacetGrid(df,hue=\"Group\", aspect=3)\nfacet.map(sns.kdeplot,'ASF',shade= True)\nfacet.set(xlim=(0, df['ASF'].max()))\nfacet.add_legend()\nplt.xlim(0.5, 2)\n\n#eTIV = Estimated Total Intracranial Volume\nfacet= sns.FacetGrid(df,hue=\"Group\", aspect=3)\nfacet.map(sns.kdeplot,'eTIV',shade= True)\nfacet.set(xlim=(0, df['eTIV'].max()))\nfacet.add_legend()\nplt.xlim(900, 2100)\n\n#'nWBV' = Normalized Whole Brain Volume\n# Nondemented = 0, Demented =1\nfacet= sns.FacetGrid(df,hue=\"Group\", aspect=3)\nfacet.map(sns.kdeplot,'nWBV',shade= True)\nfacet.set(xlim=(0, df['nWBV'].max()))\nfacet.add_legend()\nplt.xlim(0.6,0.9)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96f2d269e35927b57c5068b2b3dda2d145ae91f4","colab_type":"text","id":"1LC-PdJislSV","_cell_guid":"fba9a62f-d370-4ebd-88fc-ff2f9cd20804"},"cell_type":"markdown","source":"The chart indicates that Nondemented group has higher brain volume ratio than Demented group. This is assumed to be because the diseases affect the brain to be shrinking its tissue. "},{"metadata":{"_cell_guid":"53fec4f8-25af-4a4c-8c63-0843ce2bb6f1","colab":{"output_extras":[{},{}],"autoexec":{"wait_interval":0,"startup":false}},"_uuid":"87a846e38f21e28b3c7ad6bae536b7cd2c3e2466","outputId":"fbb23e63-a926-477c-c6ff-fc87933c1ea6","id":"w6rN7jjSslSW","colab_type":"code","trusted":false,"collapsed":true},"cell_type":"code","source":"#AGE. Nondemented =0, Demented =0\nfacet= sns.FacetGrid(df,hue=\"Group\", aspect=3)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, df['Age'].max()))\nfacet.add_legend()\nplt.xlim(50,100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e491fe3ded880c040a17cde07279b0474b00ff7a","colab_type":"text","id":"VlQHpQRWslSY","_cell_guid":"3eb714d6-be15-4737-a73d-0f4d83474787"},"cell_type":"markdown","source":"There is a higher concentration of 70-80 years old in the Demented patient group than those in the nondemented patients.\nWe guess patients who suffered from that kind of disease has lower survival rate so that there are a few of 90 years old."},{"metadata":{"_cell_guid":"26bc6b18-e52d-4a6c-a899-030c63790147","colab":{"output_extras":[{},{}],"autoexec":{"wait_interval":0,"startup":false}},"_uuid":"8177a32eff0fece26f2220ccc046843347afc41e","outputId":"fbb23e63-a926-477c-c6ff-fc87933c1ea6","id":"w6rN7jjSslSW","colab_type":"code","trusted":false,"collapsed":true},"cell_type":"code","source":"#'EDUC' = Years of Education\n# Nondemented = 0, Demented =1\nfacet= sns.FacetGrid(df,hue=\"Group\", aspect=3)\nfacet.map(sns.kdeplot,'EDUC',shade= True)\nfacet.set(xlim=(df['EDUC'].min(), df['EDUC'].max()))\nfacet.add_legend()\nplt.ylim(0, 0.16)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26e6c99d943c7e371e9fd7c3b962449996e743fd","colab_type":"text","id":"HGKJOeWYslSZ","_cell_guid":"26fdfe0a-6b82-44c1-b83e-f06b884b6abe"},"cell_type":"markdown","source":"## Intermediate Result Summary\n1. Men are more likely with demented, an Alzheimer's Disease, than Women.\n2. Demented patients were less educated in terms of years of education.\n3. Nondemented group has higher brain volume than Demented group.\n4. Higher concentration of 70-80 years old in Demented group than those in the nondemented patients."},{"metadata":{"_uuid":"80a9f0ebb50abd31cfa14ed7d93204d00f313543","colab_type":"text","id":"dR7e2FEuScR8","_cell_guid":"4cdd08b3-fc1e-44c1-b0dc-dd8aa1ce5af4"},"cell_type":"markdown","source":"# 5. Data Preprocessing\n---\nWe identified 8 rows with missing values in SES column. We deal with this issue with 2 approaches. One is just to drop the rows with missing values. The other is to replace the missing values with the corresponing values, also known as 'Imputation'. Since we have only 150 data, I assume imputation would help the performance of our model."},{"metadata":{"_cell_guid":"9e62875b-d870-4572-8b65-ca86c75311af","colab":{"output_extras":[{}],"autoexec":{"wait_interval":0,"startup":false}},"_uuid":"c485e9bdba313a2897fa58aa61406f07c2206c27","outputId":"10613858-0403-4c42-aea7-7bc3635ef60e","id":"crn5DxTUScSI","colab_type":"code","trusted":false,"collapsed":true},"cell_type":"code","source":"# Check missing values by each column\npd.isnull(df).sum() \n# The column, SES has 8 missing values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10bb2143731149217bd59b8ea7fb2f12706dd355","colab_type":"text","id":"v96VUyPYScSL","_cell_guid":"336adc34-e28c-41ab-997b-e56d19c613d3"},"cell_type":"markdown","source":"## 5.A Removing rows with missing values"},{"metadata":{"_cell_guid":"eababf13-6a94-484e-b58a-73349c76afce","colab":{"output_extras":[{}],"autoexec":{"wait_interval":0,"startup":false}},"_uuid":"71f15a1cda750ae72a718d2ec0aa7939ac2b7855","outputId":"29e022db-bb16-44f6-bfc7-3cd3fbb6f930","id":"NCuXVrJtScSM","colab_type":"code","trusted":false,"collapsed":true},"cell_type":"code","source":"# Dropped the 8 rows with missing values in the column, SES\ndf_dropna = df.dropna(axis=0, how='any')\npd.isnull(df_dropna).sum()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5c1bee3c-d5c6-403e-9dce-6a2831c97c79","colab":{"output_extras":[{}],"autoexec":{"wait_interval":0,"startup":false}},"_uuid":"a08f0ff032efa883e45e9b5bd2b6d386dd89f9b3","outputId":"24c17bc4-f411-470d-e483-cd6098eff679","id":"ThXMkCk0ScSQ","colab_type":"code","trusted":false,"collapsed":true},"cell_type":"code","source":"df_dropna['Group'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a81499adca6b212e798a6b9a123c5806d57e087","colab_type":"text","id":"A0ipe9qsScSU","_cell_guid":"5ec7ab3c-597f-4c28-958e-9985ad3afc3e"},"cell_type":"markdown","source":"## 5.B Imputation\n\nScikit-learn provides package for imputation [6], but we do it manually. Since the *SES* is a discrete variable, we use median for the imputation."},{"metadata":{"_cell_guid":"2a1c1ae2-31f8-4001-b801-e416d4346f5c","colab":{"output_extras":[{}],"autoexec":{"wait_interval":0,"startup":false}},"_uuid":"1c2354281b56aaac81b5dd09cee531be85a72b1f","outputId":"a5e8f86e-66d6-4147-83f7-f26b12787245","id":"62IgZtwnScSV","colab_type":"code","trusted":false,"collapsed":true},"cell_type":"code","source":"# Draw scatter plot between EDUC and SES\nx = df['EDUC']\ny = df['SES']\n\nses_not_null_index = y[~y.isnull()].index\nx = x[ses_not_null_index]\ny = y[ses_not_null_index]\n\n# Draw trend line in red\nz = np.polyfit(x, y, 1)\np = np.poly1d(z)\nplt.plot(x, y, 'go', x, p(x), \"r--\")\nplt.xlabel('Education Level(EDUC)')\nplt.ylabel('Social Economic Status(SES)')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e2df717e-f955-4a48-91a7-4bc727a21576","colab":{"output_extras":[{}],"autoexec":{"wait_interval":0,"startup":false}},"_uuid":"d2fcc229a25ba9ad1097de001cd53505ad0ba478","outputId":"f05f5530-b14c-4bb8-abbb-037e2a0838e3","id":"cKaMRSvgScSY","colab_type":"code","trusted":false,"collapsed":true},"cell_type":"code","source":"df.groupby(['EDUC'])['SES'].median()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f13e43a3-810f-48ce-a589-caa0b347e5e7","colab":{"autoexec":{"wait_interval":0,"startup":false}},"_uuid":"b41fdf3fc1f569f482e8a506f0737775e84b338a","collapsed":true,"id":"dj_edAcdScSb","colab_type":"code","trusted":false},"cell_type":"code","source":"df[\"SES\"].fillna(df.groupby(\"EDUC\")[\"SES\"].transform(\"median\"), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cadc4ae5-ecf0-4e9f-ab2c-b020e46644f1","colab":{"output_extras":[{}],"autoexec":{"wait_interval":0,"startup":false}},"_uuid":"63c36b7bc32f5cc65c8bf908113ba919a17fc5b8","outputId":"9325d3dc-7f01-4be2-a932-a39c68d202b7","id":"Y4SgUM58ScSd","colab_type":"code","trusted":false,"collapsed":true},"cell_type":"code","source":"# I confirm there're no more missing values and all the 150 data were used.\npd.isnull(df['SES']).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"59d120be-45c4-47d5-9b9b-4bec206ad8fb","_uuid":"61220f51b25e5efcc5ceaa0d8f133967514b33a5"},"cell_type":"markdown","source":"## 5.C Splitting Train/Validation/Test Sets"},{"metadata":{"_cell_guid":"bea09931-94e4-46f1-8426-7b0ec3d2971e","colab":{"autoexec":{"wait_interval":0,"startup":false}},"_uuid":"b039c111537091add5e04716b4d33501855155e8","collapsed":true,"id":"kJcRjpOIScSj","colab_type":"code","trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler \nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"969584c9-67e7-469f-abf2-ac4f93899b2c","colab":{"autoexec":{"wait_interval":0,"startup":false}},"_uuid":"9c0cd7fedabdd26e4dbd2e5ed19a79c9097511c8","collapsed":true,"id":"eI6EXWT7ScSm","colab_type":"code","trusted":false},"cell_type":"code","source":"# Dataset with imputation\nY = df['Group'].values # Target for the model\nX = df[['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF']] # Features we use\n\n# splitting into three sets\nX_trainval, X_test, Y_trainval, Y_test = train_test_split(\n    X, Y, random_state=0)\n\n# Feature scaling\nscaler = MinMaxScaler().fit(X_trainval)\nX_trainval_scaled = scaler.transform(X_trainval)\nX_test_scaled = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a81defc1-7bd2-4d0a-97ca-53a65c7dc844","colab":{"autoexec":{"wait_interval":0,"startup":false}},"_uuid":"aa47c091672863c3ad5c785a65cf227977cb9c37","collapsed":true,"id":"oQAwHijeScSq","colab_type":"code","trusted":false},"cell_type":"code","source":"# Dataset after dropping missing value rows\nY = df_dropna['Group'].values # Target for the model\nX = df_dropna[['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF']] # Features we use\n\n# splitting into three sets\nX_trainval_dna, X_test_dna, Y_trainval_dna, Y_test_dna = train_test_split(\n    X, Y, random_state=0)\n\n# Feature scaling\nscaler = MinMaxScaler().fit(X_trainval_dna)\nX_trainval_scaled_dna = scaler.transform(X_trainval_dna)\nX_test_scaled_dna = scaler.transform(X_test_dna)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2356d2f3b9b746909c52e98c88c9dd9e20f77bf","colab_type":"text","id":"Hh0VrtWIScSt","_cell_guid":"ab10a488-b6db-46cf-a1c3-d55f78a57cd4"},"cell_type":"markdown","source":"## 5.D Cross-validation\nWe conduct 5-fold cross-validation to figure out the best parameters for each model, Logistic Regression, SVM, Decision Tree, Random Forests, and AdaBoost. Since our performance metric is accuracy, we find the best tuning parameters by *accuracy*. In the end, we compare the accuracy, recall and AUC for each model."},{"metadata":{"_uuid":"27d4266a859c95510a8355df6c98c741a4805224","colab_type":"text","id":"vgYxX0OkScSj","_cell_guid":"3853164a-d4aa-4bb0-a191-04ef6358f624"},"cell_type":"markdown","source":"# 6. MODEL\n---"},{"metadata":{"_uuid":"3bef5c29ca278b2aee2e6d0786e78149ab1bd83d","colab_type":"text","id":"ZmzXtTaFScSs","_cell_guid":"cb521d72-5f43-4259-ba1d-a787fd8e80bb"},"cell_type":"markdown","source":"## 6.A Performance Measures\n\nWe use area under the receiver operating characteristic curve (AUC) as our main performance measure. We believe that in case of medical diagnostics for non-life threatening terminal diseases like most neurodegenerative diseases it is important to have a high true positive rate so that all patients with alzheimer's are identified as early as possible. But we also want to make sure that the false positive rate is as low as possible since we do not want to misdiagnose a healthy adult as demented and begin medical therapy. Hence AUC seemed like a ideal choice for a performance measure.\n\nWe will also be looking at accuracy and recall for each model.\n\nIn the figure below, you can think relevant elements as actually demented subjects.\nPrecision and Recall [12]\n![Precision and Recall](https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg)"},{"metadata":{"_uuid":"52d9efbf3184f96ff3a0767e08d3d1e6f8ea1a73","colab_type":"text","id":"L0zOT1CAScSu","_cell_guid":"d33a8800-7c76-4138-99a4-82ef95352ce2"},"cell_type":"markdown","source":"## 6.B Logistic Regression\nThe parameter C, inverse of regularization strength.\n\nTuning range: [0.001, 0.1, 1, 10, 100]"},{"metadata":{"_cell_guid":"e05124e9-9628-4a00-9ce9-5e9d22ac3af4","collapsed":true,"_uuid":"f8dcc6798d6f0aa777fff85c880a47ec78ad9e5c","trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score, roc_curve, auc","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"77467868-12db-41a2-9ad0-7af244e847e0","colab":{"autoexec":{"wait_interval":0,"startup":false}},"_uuid":"252a6294c3c8754c0e19660e5ad7c0af189fd8e5","collapsed":true,"id":"nvBhRVT_ScSu","colab_type":"code","trusted":false},"cell_type":"code","source":"acc = [] # list to store all performance metric","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4611a76e-9053-4d8c-9cf3-1a8e3c7896e0","colab":{"output_extras":[{}],"autoexec":{"wait_interval":0,"startup":false}},"_uuid":"cb9f7249fa6793ec6e339aeb9936c26fbdfe552d","outputId":"f0c4357e-951e-4078-9356-f01f7326c056","id":"xsWC0JpIScSw","colab_type":"code","trusted":false,"collapsed":true},"cell_type":"code","source":"# Dataset with imputation\nbest_score=0\nkfolds=5 # set the number of folds\n\nfor c in [0.001, 0.1, 1, 10, 100]:\n    logRegModel = LogisticRegression(C=c)\n    # perform cross-validation\n    scores = cross_val_score(logRegModel, X_trainval, Y_trainval, cv=kfolds, scoring='accuracy') # Get recall for each parameter setting\n    \n    # compute mean cross-validation accuracy\n    score = np.mean(scores)\n    \n    # Find the best parameters and score\n    if score > best_score:\n        best_score = score\n        best_parameters = c\n\n# rebuild a model on the combined training and validation set\nSelectedLogRegModel = LogisticRegression(C=best_parameters).fit(X_trainval_scaled, Y_trainval)\n\ntest_score = SelectedLogRegModel.score(X_test_scaled, Y_test)\nPredictedOutput = SelectedLogRegModel.predict(X_test_scaled)\ntest_recall = recall_score(Y_test, PredictedOutput, pos_label=1)\nfpr, tpr, thresholds = roc_curve(Y_test, PredictedOutput, pos_label=1)\ntest_auc = auc(fpr, tpr)\nprint(\"Best accuracy on validation set is:\", best_score)\nprint(\"Best parameter for regularization (C) is: \", best_parameters)\nprint(\"Test accuracy with best C parameter is\", test_score)\nprint(\"Test recall with the best C parameter is\", test_recall)\nprint(\"Test AUC with the best C parameter is\", test_auc)\nm = 'Logistic Regression (w/ imputation)'\nacc.append([m, test_score, test_recall, test_auc, fpr, tpr, thresholds])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b12b7bf1-f29e-4f66-8dcf-00bc1fdf0538","colab":{"output_extras":[{}],"autoexec":{"wait_interval":0,"startup":false}},"_uuid":"200d475f03c493ecc5de448537841771d0817eb9","outputId":"2f1858c0-335a-4720-dc18-bef6e3c10a9e","id":"Zb-VqkXUScSz","colab_type":"code","trusted":false,"collapsed":true},"cell_type":"code","source":"# Dataset after dropping missing value rows\nbest_score=0\nkfolds=5 # set the number of folds\n\nfor c in [0.001, 0.1, 1, 10, 100]:\n    logRegModel = LogisticRegression(C=c)\n    # perform cross-validation\n    scores = cross_val_score(logRegModel, X_trainval_scaled_dna, Y_trainval_dna, cv=kfolds, scoring='accuracy')\n    \n    # compute mean cross-validation accuracy\n    score = np.mean(scores)\n    \n    # Find the best parameters and score\n    if score > best_score:\n        best_score = score\n        best_parameters = c\n\n# rebuild a model on the combined training and validation set\nSelectedLogRegModel = LogisticRegression(C=best_parameters).fit(X_trainval_scaled_dna, Y_trainval_dna)\n\ntest_score = SelectedLogRegModel.score(X_test_scaled_dna, Y_test_dna)\nPredictedOutput = SelectedLogRegModel.predict(X_test_scaled)\ntest_recall = recall_score(Y_test, PredictedOutput, pos_label=1)\nfpr, tpr, thresholds = roc_curve(Y_test, PredictedOutput, pos_label=1)\ntest_auc = auc(fpr, tpr)\nprint(\"Best accuracy on validation set is:\", best_score)\nprint(\"Best parameter for regularization (C) is: \", best_parameters)\nprint(\"Test accuracy with best C parameter is\", test_score)        \nprint(\"Test recall with the best C parameter is\", test_recall)\nprint(\"Test AUC with the best C parameter is\", test_auc)\n\nm = 'Logistic Regression (w/ dropna)'\nacc.append([m, test_score, test_recall, test_recall, fpr, tpr, thresholds])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"42f5a28f-82e8-4259-8d06-991a0c9ff556","_uuid":"9b30f505fa1febaa9b0577ef2943906046f77413"},"cell_type":"markdown","source":"In overall, dataset with imputation outperforms the one without imputation. For the later models, we use dataset without imputation."},{"metadata":{"_uuid":"c3fcf0473df26fdea9ba51953aab1f53b2e522c5","colab_type":"text","id":"Gj3b-ssXScS2","_cell_guid":"3a3de04d-0912-4469-813c-e342c723c501"},"cell_type":"markdown","source":"## 6.C SVM\nC: Penalty parameter C of the error term. [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n\ngamma: kernel coefficient. [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n\nkernel: kernel type. ['rbf', 'linear', 'poly', 'sigmoid']"},{"metadata":{"_cell_guid":"4cf147a3-e548-4c95-a70b-808b24e0d311","colab":{"output_extras":[{}],"autoexec":{"wait_interval":0,"startup":false}},"_uuid":"527bf4c3aaeb65eeb16f929c2c8c82ce4c2263c2","outputId":"ac91ced2-9648-4248-8c62-6babfb401178","id":"Xp5EM__NScS2","colab_type":"code","trusted":false,"collapsed":true},"cell_type":"code","source":"best_score = 0\n\nfor c_paramter in [0.001, 0.01, 0.1, 1, 10, 100, 1000]: #iterate over the values we need to try for the parameter C\n    for gamma_paramter in [0.001, 0.01, 0.1, 1, 10, 100, 1000]: #iterate over the values we need to try for the parameter gamma\n        for k_parameter in ['rbf', 'linear', 'poly', 'sigmoid']: # iterate over the values we need to try for the kernel parameter\n            svmModel = SVC(kernel=k_parameter, C=c_paramter, gamma=gamma_paramter) #define the model\n            # perform cross-validation\n            scores = cross_val_score(svmModel, X_trainval_scaled, Y_trainval, cv=kfolds, scoring='accuracy')\n            # the training set will be split internally into training and cross validation\n\n            # compute mean cross-validation accuracy\n            score = np.mean(scores)\n            # if we got a better score, store the score and parameters\n            if score > best_score:\n                best_score = score #store the score \n                best_parameter_c = c_paramter #store the parameter c\n                best_parameter_gamma = gamma_paramter #store the parameter gamma\n                best_parameter_k = k_parameter\n            \n\n# rebuild a model with best parameters to get score \nSelectedSVMmodel = SVC(C=best_parameter_c, gamma=best_parameter_gamma, kernel=best_parameter_k).fit(X_trainval_scaled, Y_trainval)\n\ntest_score = SelectedSVMmodel.score(X_test_scaled, Y_test)\nPredictedOutput = SelectedSVMmodel.predict(X_test_scaled)\ntest_recall = recall_score(Y_test, PredictedOutput, pos_label=1)\nfpr, tpr, thresholds = roc_curve(Y_test, PredictedOutput, pos_label=1)\ntest_auc = auc(fpr, tpr)\nprint(\"Best accuracy on cross validation set is:\", best_score)\nprint(\"Best parameter for c is: \", best_parameter_c)\nprint(\"Best parameter for gamma is: \", best_parameter_gamma)\nprint(\"Best parameter for kernel is: \", best_parameter_k)\nprint(\"Test accuracy with the best parameters is\", test_score)\nprint(\"Test recall with the best parameters is\", test_recall)\nprint(\"Test recall with the best parameter is\", test_auc)\n\nm = 'SVM'\nacc.append([m, test_score, test_recall, test_auc, fpr, tpr, thresholds])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a111665a3aeefa0449d6e9b86efb45e14e5996d","colab_type":"text","id":"mYGAer5hScS5","_cell_guid":"9ccf7bf6-16b1-4b28-bf4b-20237c3e5c44"},"cell_type":"markdown","source":"## 6.D Decision Tree\nMaximum depth. [1, 2, ..., 8]\n\n8 is the number of features"},{"metadata":{"_cell_guid":"c44cd2e5-9516-4357-b045-3b864df38ccd","colab":{"output_extras":[{}],"autoexec":{"wait_interval":0,"startup":false}},"_uuid":"4c27de17e77f9b986e27d2090113bce38bd67fba","outputId":"86ea4ca6-b57f-47c0-fc4c-a958e1da8f95","id":"jGI1Smg7ScS6","colab_type":"code","trusted":false,"collapsed":true},"cell_type":"code","source":"best_score = 0\n\nfor md in range(1, 9): # iterate different maximum depth values\n    # train the model\n    treeModel = DecisionTreeClassifier(random_state=0, max_depth=md, criterion='gini')\n    # perform cross-validation\n    scores = cross_val_score(treeModel, X_trainval_scaled, Y_trainval, cv=kfolds, scoring='accuracy')\n    \n    # compute mean cross-validation accuracy\n    score = np.mean(scores)\n    \n    # if we got a better score, store the score and parameters\n    if score > best_score:\n        best_score = score\n        best_parameter = md\n\n# Rebuild a model on the combined training and validation set        \nSelectedDTModel = DecisionTreeClassifier(max_depth=best_parameter).fit(X_trainval_scaled, Y_trainval )\n\ntest_score = SelectedDTModel.score(X_test_scaled, Y_test)\nPredictedOutput = SelectedDTModel.predict(X_test_scaled)\ntest_recall = recall_score(Y_test, PredictedOutput, pos_label=1)\nfpr, tpr, thresholds = roc_curve(Y_test, PredictedOutput, pos_label=1)\ntest_auc = auc(fpr, tpr)\nprint(\"Best accuracy on validation set is:\", best_score)\nprint(\"Best parameter for the maximum depth is: \", best_parameter)\nprint(\"Test accuracy with best parameter is \", test_score)\nprint(\"Test recall with best parameters is \", test_recall)\nprint(\"Test AUC with the best parameter is \", test_auc)\n\nm = 'Decision Tree'\nacc.append([m, test_score, test_recall, test_auc, fpr, tpr, thresholds])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b92eeca2-b2c2-4b5d-a6a5-fec44857bf88","colab":{"output_extras":[{},{}],"autoexec":{"wait_interval":0,"startup":false}},"_uuid":"802da5f22fbe73ff4d4ff097408da95100fababe","outputId":"c4d23b31-7cde-459d-c745-df64c9716cad","id":"3k1LzTAOScS9","colab_type":"code","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"Feature importance: \")\nnp.array([X.columns.values.tolist(), list(SelectedDTModel.feature_importances_)]).T","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3a4023e7-3074-4f5f-85a8-31654206138f","colab":{"output_extras":[{}],"autoexec":{"wait_interval":0,"startup":false}},"_uuid":"9df711a59c1470af47f2c5a83f2e9cb11dcb00c7","outputId":"8fccfeea-ec7f-4bad-9029-0b66b4aa09f7","id":"jAXEhs2gScS_","colab_type":"code","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn.tree import export_graphviz\nimport graphviz \ndot_data=export_graphviz(SelectedDTModel, feature_names=X_trainval.columns.values.tolist(),out_file=None)\ngraph = graphviz.Source(dot_data)  \ngraph ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20608c740de79f3ea76598be6c3e3e8fd158fd82","colab_type":"text","id":"ZkIF7600ScTD","_cell_guid":"b41201f7-f5f5-4d0a-931d-1d4e52c1ee1e"},"cell_type":"markdown","source":"## 6.E Random Forest Classifier\nn_estimators(M): the number of trees in the forest\n\nmax_features(d): the number of features to consider when looking for the best split\n\nmax_depth(m): the maximum depth of the tree. "},{"metadata":{"_cell_guid":"a9376afc-7cbf-4be3-a0ab-d579d1f2272b","colab":{"output_extras":[{}],"autoexec":{"wait_interval":0,"startup":false}},"_uuid":"0314f070def4da297b2b8712cea7727f6d5d8112","outputId":"e7adb6b8-ccfd-47b4-f65c-01848c1a0450","id":"zi0Ssns3ScTE","colab_type":"code","trusted":false,"collapsed":true},"cell_type":"code","source":"best_score = 0\n\nfor M in range(2, 15, 2): # combines M trees\n    for d in range(1, 9): # maximum number of features considered at each split\n        for m in range(1, 9): # maximum depth of the tree\n            # train the model\n            # n_jobs(4) is the number of parallel computing\n            forestModel = RandomForestClassifier(n_estimators=M, max_features=d, n_jobs=4,\n                                          max_depth=m, random_state=0)\n        \n            # perform cross-validation\n            scores = cross_val_score(forestModel, X_trainval_scaled, Y_trainval, cv=kfolds, scoring='accuracy')\n\n            # compute mean cross-validation accuracy\n            score = np.mean(scores)\n\n            # if we got a better score, store the score and parameters\n            if score > best_score:\n                best_score = score\n                best_M = M\n                best_d = d\n                best_m = m\n\n# Rebuild a model on the combined training and validation set        \nSelectedRFModel = RandomForestClassifier(n_estimators=M, max_features=d,\n                                          max_depth=m, random_state=0).fit(X_trainval_scaled, Y_trainval )\n\nPredictedOutput = SelectedRFModel.predict(X_test_scaled)\ntest_score = SelectedRFModel.score(X_test_scaled, Y_test)\ntest_recall = recall_score(Y_test, PredictedOutput, pos_label=1)\nfpr, tpr, thresholds = roc_curve(Y_test, PredictedOutput, pos_label=1)\ntest_auc = auc(fpr, tpr)\nprint(\"Best accuracy on validation set is:\", best_score)\nprint(\"Best parameters of M, d, m are: \", best_M, best_d, best_m)\nprint(\"Test accuracy with the best parameters is\", test_score)\nprint(\"Test recall with the best parameters is:\", test_recall)\nprint(\"Test AUC with the best parameters is:\", test_auc)\n\nm = 'Random Forest'\nacc.append([m, test_score, test_recall, test_auc, fpr, tpr, thresholds])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7e6601d4-06ed-495e-b420-532ab0d4620d","colab":{"output_extras":[{},{}],"autoexec":{"wait_interval":0,"startup":false}},"_uuid":"7264a60baa490343e92e743d43f635a18af9fa13","outputId":"f7ba2e6f-1a5b-4495-a9b1-2a0973101259","id":"Mcx6LmzcScTJ","colab_type":"code","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"Feature importance: \")\nnp.array([X.columns.values.tolist(), list(SelectedRFModel.feature_importances_)]).T","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2c7cd77a-9181-4475-9d6b-16a130b56411","_uuid":"ea3b9dd38fd2bbfe32e1cdd35ed6a3cdfa2dbccc"},"cell_type":"markdown","source":"## 6.F AdaBoost"},{"metadata":{"_cell_guid":"df1fc809-135d-4b47-8fdc-18790c232229","_uuid":"9eab03ab2cd59e507dc30b632c60fb0aeb1d2c37","trusted":false,"collapsed":true},"cell_type":"code","source":"best_score = 0\n\nfor M in range(2, 15, 2): # combines M trees\n    for lr in [0.0001, 0.001, 0.01, 0.1, 1]:\n        # train the model\n        boostModel = AdaBoostClassifier(n_estimators=M, learning_rate=lr, random_state=0)\n\n        # perform cross-validation\n        scores = cross_val_score(boostModel, X_trainval_scaled, Y_trainval, cv=kfolds, scoring='accuracy')\n\n        # compute mean cross-validation accuracy\n        score = np.mean(scores)\n\n        # if we got a better score, store the score and parameters\n        if score > best_score:\n            best_score = score\n            best_M = M\n            best_lr = lr\n\n# Rebuild a model on the combined training and validation set        \nSelectedBoostModel = AdaBoostClassifier(n_estimators=M, learning_rate=lr, random_state=0).fit(X_trainval_scaled, Y_trainval )\n\nPredictedOutput = SelectedBoostModel.predict(X_test_scaled)\ntest_score = SelectedRFModel.score(X_test_scaled, Y_test)\ntest_recall = recall_score(Y_test, PredictedOutput, pos_label=1)\nfpr, tpr, thresholds = roc_curve(Y_test, PredictedOutput, pos_label=1)\ntest_auc = auc(fpr, tpr)\nprint(\"Best accuracy on validation set is:\", best_score)\nprint(\"Best parameter of M is: \", best_M)\nprint(\"best parameter of LR is: \", best_lr)\nprint(\"Test accuracy with the best parameter is\", test_score)\nprint(\"Test recall with the best parameters is:\", test_recall)\nprint(\"Test AUC with the best parameters is:\", test_auc)\n\nm = 'AdaBoost'\nacc.append([m, test_score, test_recall, test_auc, fpr, tpr, thresholds])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"387437e6-7dbf-486e-96a5-b2a2fd19642b","_uuid":"a240cd4c2abd5b9bb46c10b21d157e663665a2bb","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"Feature importance: \")\nnp.array([X.columns.values.tolist(), list(SelectedBoostModel.feature_importances_)]).T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5defe4c299e6a47547b1d05a533c90aba26c0365","colab_type":"text","id":"w1vFrwcwScTO","_cell_guid":"61c52e5f-e505-432b-9e2c-44d14fbeb03f"},"cell_type":"markdown","source":"# 7. CONCLUSION\n\n## 7.A RESULTS"},{"metadata":{"_cell_guid":"0b6a7303-e200-420b-aa9a-64c8285510b4","_uuid":"567d27a76de89fd5c9feb715bfc0e58400872f7e","trusted":false,"collapsed":true},"cell_type":"code","source":"# Performance Metric for each model\nresult = pd.DataFrame(acc, columns=['Model', 'Accuracy', 'Recall', 'AUC', 'FPR', 'TPR', 'TH'])\nresult[['Model', 'Accuracy', 'Recall', 'AUC']]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4647d766-ff29-432b-83d4-e609f0f5e38f","_uuid":"97515e1017176438f0b63b12ef68038a396a0778"},"cell_type":"markdown","source":"#### Below is a comparision of our results with those from the papers that were listed previously:\n\n|Sr.No.|Paper|Data|Model|Results|\n|---\n|1.|E. Moradi et al. [3]|Ye et al. [7]|Random Forrest Classifier|AUC = 71.0%|ACC = 55.3%|\n|||Filipovych et al. [8]|Random Forrest Classifier|AUC = 61.0%|ACC = N/A|\n|||Zhang et al. [9]|Random Forrest Classifier|AUC = 94.6%|ACC = N/A|\n|||Batmanghelich et al. [10]|Random Forrest Classifier|AUC = 61.5%|ACC = N/A|\n|2.|Zhang et al. [4]|Ardekani et al. [11]|Support Vector Machine |||\n||||*polynomial kernel*|AUC = N/A|ACC = 92.4%|\n||||*linear kernel*|AUC = N/A|ACC = 91.5%|\n||||*radial basis function*|AUC = N/A|ACC = 86.7%|\n|3.|Hyun, Kyuri, Saurin|Marcus et al. [1]| Logistic Regression (w/ imputation)|AUC = 79.2%|ACC = 78.9%|\n||||Logistic Regression (w/ dropna)|AUC = 70.0%|ACC = 75.0%|\n||||Support Vector Machine|AUC = 82.2%|ACC = 81.6%|\n||||Decision Tree Classifier|AUC = 82.5%|ACC = 81.6%|\n||||Random Forest Classifier|AUC = 84.4%|ACC = 84.2%|\n||||AdaBoost|AUC = 82.5%|ACC = 84.2%|\n\nIt can be noticed that our results are comparable and in certain cases better than those from the previous work. Our Random Forest Classifier was one of the best performing model."},{"metadata":{"_uuid":"d8e735911083847bdff95aac9cf9d76c0cbd730e","colab_type":"text","id":"30ETMyJAszMY","_cell_guid":"922b813f-192d-4469-8281-c0c5e7d0e1a8"},"cell_type":"markdown","source":"## 7.B UNIQUE APPROACH\nThe uniqueness of our approach is the fact that we would be including metrices like MMSE and Education also in our model to train it to differentiate between normal healthy adults and those with Alzheimer's. MMSE is one of the gold standards for determining dementia and hence we think it is an important feature to include.\n\nThe same fact also make our approach flexible enough to be applied to other neurodegenerative diseases which are diagnosed using a combination of MRI features and cognitive tests.\n\n## 7.C IMPLEMENTATION\nThe teams' primary intention was to explore how machine learning can make a difference in the clinical environment. For that we have developed a web program using our algorithm which can be used anyone regardless of their programming experience. By using a [CGI module](https://docs.python.org/2/library/cgi.html) we want to make everyone take advantage of your effort!\n\n\n#### Common Gateway Interface (CGI)\n* CGI can be used make a webserver execute your model.\n* The input arguments for your web program should be the same as the parameters you used to train your model.\n* The idea is that a Clinician should be able to input MRI results, biographic data and other parameters for a patient. Your model should assist them in identifying dementia.\n\n\n#### Here is the [screenshot](https://pitt.box.com/s/a0wvujqqbbtt97qri1pqo06mw28rwjlb) of our web application\n\n\n## 7.D LIMITATIONS\nThere are limitations in implementing a complex model because of the quantity of the dataset. Even though the nature of each feature is evident, the ranges of each group's test value are not classified well. In other words, we should have identified more clearly the differences in the variables which might have played a role in the result.The predicted value using the random forest model is higher than the other models. It implies there is a potential for higher prediction rate if we pay more attention to develop the data cleaning and analysis process. Moreover,  the perfect recall score 1.0 of SVM 1.0. Indicates that the quality and accuracy of the classification might decrease dramatically when we use different dataset.\n\n\n##  7.E FURTHER RESEARCH\nThe main takeaway for us is that there are several key factors which are caused by Dementia and we should continue to check it and clear the process in different ways.For the further study, it is necessary for us to improve our understanding through more sophisticated EDA process with a larger sample size. For instance, we would try not only the age itself but also group it into generation, or grade volume of brain tissue or exam scores. If the results from this process are reflected in the data cleaning process and positively affect the decision making of the model, the accuracy of the prediction model can be further improved."},{"metadata":{"_uuid":"794501cc79cf1a9faa9eef64e8311b48153bb5aa","colab_type":"text","id":"rnyO0OWWmsV-","_cell_guid":"c1f4eb56-a15e-4747-97b7-89d7ce028171"},"cell_type":"markdown","source":"# 8. ACKNOWLEDGEMENTS\n---\n\n## FUNDING SOURCES\nGrant Numbers: P50 AG05681, P01 AG03991, R01 AG021910, P50 MH071616, U24 RR021382, R01 MH56584.\n\n## REFERENCES\n1. Marcus DS, Fotenos AF, Csernansky JG, Morris JC, Buckner RL. Open Access Series of Imaging Studies (OASIS): Longitudinal MRI Data in Nondemented and Demented Older Adults. Journal of cognitive neuroscience. 2010;22(12):2677-2684. doi:10.1162/jocn.2009.21407.\n2. Marcus, DS, Wang, TH, Parker, J, Csernansky, JG, Morris, JC, Buckner, RL. Open Access Series of Imaging Studies (OASIS): Cross-Sectional MRI Data in Young, Middle Aged, Nondemented, and Demented Older Adults. Journal of Cognitive Neuroscience, 19, 1498-1507. doi:10.1162/jocn.2007.19.9.1498\n3. Elaheh Moradi, Antonietta Pepe, Christian Gaser, Heikki Huttunen, Jussi Tohka, Machine learning framework for early MRI-based Alzheimer's conversion prediction in MCI subjects, In NeuroImage, Volume 104, 2015, Pages 398-412, ISSN 1053-8119, doi.org/10.1016/j.neuroimage.2014.10.002.\n4. Zhang Y, Dong Z, Phillips P, et al. Detection of subjects and brain regions related to Alzheimer’s disease using 3D MRI scans based on eigenbrain and machine learning. Frontiers in Computational Neuroscience. 2015;9:66. doi:10.3389/fncom.2015.00066.\n5. Magnin, B., Mesrob, L., Kinkingnéhun, S. et al. Support vector machine-based classification of Alzheimer’s disease from whole-brain anatomical MRI. Neuroradiology (2009) 51: 73. doi.org/10.1007/s00234-008-0463-x\n6. http://scikit-learn.org/stable/modules/preprocessing.html#imputation\n7. Ye, D.H., Pohl, K.M., Davatzikos, C., 2011. Semi-supervised pattern classification: application to structural MRI of Alzheimer's disease. Pattern Recognition in NeuroImaging(PRNI), 2011 International Workshop on. IEEE, pp. 1–4. http://doi:10.1109/PRNI.2011.12.\n8. Filipovych, R., Davatzikos, C., 2011. Semi-supervised pattern classification of medical images: application to mild cognitive impairment (MCI). Neuroimage 55 (3), 1109–1119. https://doi.org/10.1016/j.neuroimage.2010.12.066\n9. Zhang, D., Shen, D., 2012. Predicting future clinical changes ofMCI patients using longitudinal and multimodal biomarkers. PLoS One 7 (3), e33182. https://doi.org/10.1371/journal.pone.0033182\n10. Batmanghelich, K.N., Ye, D.H., Pohl, K.M., Taskar, B., Davatzikos, C., 2011. Disease classification and prediction via semi-supervised dimensionality reduction. Biomedical Imaging: From Nano to Macro, 2011 IEEE International Symposium on. IEEE, pp. 1086–1090. http://10.1109/ISBI.2011.5872590\n11. Ardekani,B.A.,Bachman,A.H.,Figarsky,K.,andSidtis,J.J.(2014).Corpus callosum shape changes in early Alzheimer’s disease: an MRI study using the OASISbraindatabase. BrainStruct.Funct. 219,343–352.doi:10.1007/s00429-013-0503-0\n12. https://en.wikipedia.org/wiki/Precision_and_recall"}],"metadata":{"anaconda-cloud":{},"colab":{"collapsed_sections":[],"version":"0.3.2","provenance":[],"name":"FinalReport_Code.ipynb","default_view":{},"views":{}},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}