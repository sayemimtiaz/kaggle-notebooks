{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a id=\"overview\"></a>\n# Overview ğŸ§\n<img src=\"https://i.imgur.com/HVZezzb.jpg\" width=\"600\"><br>\nIn this notebook, we are going to predict whether a breast mass is benign or malignant based on 30 features in the dataset. This prediction can be useful in diagnosing patients with suspected breast cancer.<br>\n<font color=\"RoyalBlue\">ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å«ã¾ã‚Œã‚‹30ã®ç‰¹å¾´é‡ã‹ã‚‰ä¹³è…ºè…«ç˜¤ãŒè‰¯æ€§ã‹æ‚ªæ€§ã‹ã‚’äºˆæ¸¬ã—ã¾ã™ã€‚ã“ã®äºˆæ¸¬ã¯ã€ä¹³ãŒã‚“ã®ç–‘ã„ãŒã‚ã‚‹æ‚£è€…ã‚’è¨ºæ–­ã™ã‚‹éš›ã«å½¹ç«‹ã¦ã‚‰ã‚Œã‚‹ã§ã—ã‚‡ã†ã€‚</font><br>\n\nI have also run a similar analysis in R ([Breast CancerğŸ¦€EDA & FA / PCA with R (98.2% acc)](https://www.kaggle.com/snowpea8/breast-cancer-eda-fa-pca-with-r-98-2-acc)), \nif you would like to take a look at it.<br>\n<font color=\"RoyalBlue\">åŒæ§˜ã®åˆ†æã‚’ R ã§ã‚‚å®Ÿè¡Œã—ã¦ã„ã¾ã™ã®ã§ã€ãã¡ã‚‰ã‚‚å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚</font><br>\n\nWe will first discover and visualize the data to gain insights. Then we split the data into a training and a test set and use the training set to train some machine learning models. At the same time, we evaluate the performance of the models with cross-validation. Finally, we will ensemble each model to improve its accuracy.<br>\n<font color=\"RoyalBlue\">ã¾ãšæ´å¯Ÿã‚’å¾—ã‚‹ãŸã‚ã«ãƒ‡ãƒ¼ã‚¿ã‚’ç ”ç©¶ã€å¯è¦–åŒ–ã—ã¾ã™ã€‚ãã‚Œã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ã«åˆ†å‰²ã—ã€è¨“ç·´ã‚»ãƒƒãƒˆã‚’ä½¿ã£ã¦ã„ãã¤ã‹ã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¾ã™ã€‚åŒæ™‚ã«ã€äº¤å·®æ¤œè¨¼ã§ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡ã—ã¾ã™ã€‚æœ€å¾Œã«ãã‚Œãã‚Œã®ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã—ã€ç²¾åº¦ã®å‘ä¸Šã‚’ç›®æŒ‡ã—ã¦ã„ãã¾ã™ã€‚</font>\n\n# Table of contents ğŸ“–\n* [Overview ğŸ§](#overview)\n* [Setup ğŸ’»](#setup)\n* [Load CSV data ğŸ“ƒ](#load)\n* [Explore CSV data ğŸ“Š](#explore)\n* [Data preprocessing ğŸ§¹](#preprocessing)\n* [Train models and make predictions ğŸ’­](#models)\n    * [LightGBM ğŸŒ³](#gbm)\n    * [Extremely randomized trees ğŸŒ³](#ert)\n    * [Linear model ğŸ“ˆ](#lm)\n* [Simple ensemble ğŸ¤](#ensemble)\n\n<a id=\"setup\"></a>\n# Setup ğŸ’»\nAll seed values are fixed at zero.<br>\n<font color=\"RoyalBlue\">ã‚·ãƒ¼ãƒ‰å€¤ã¯å…¨ã¦0ã§å›ºå®šã—ã¦ã„ã¾ã™ã€‚</font><br>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport random\nimport pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom mlxtend.plotting import plot_confusion_matrix\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\n\nimport lightgbm as lgb\n\ndef seed_everything(seed=2020):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    # tf.random.set_seed(seed)\nseed_everything(0)\n\nsns.set_style(\"whitegrid\")\npalette_ro = [\"#ee2f35\", \"#fa7211\", \"#fbd600\", \"#75c731\", \"#1fb86e\", \"#0488cf\", \"#7b44ab\"]\n\nROOT = \"../input/breast-cancer-wisconsin-data\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"load\"></a>\n# Load CSV data ğŸ“ƒ"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(ROOT + \"/data.csv\")\n\nprint(\"Data shape: \", df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataset from: [Breast Cancer Wisconsin (Diagnostic) Data Set](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29)\n\n* `id` - ID number\n* `diagnosis` - Diagnosis (`M`: malignant, `B`: benign)\n<br>ã€€<font color=\"RoyalBlue\">ã€ç›®çš„å¤‰æ•°ã€‘è¨ºæ–­ï¼ˆçµæœï¼‰ï¼ˆM : æ‚ªæ€§ï¼ŒB : è‰¯æ€§ï¼‰</font><br>\n\nThe following features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.<br>\n<font color=\"RoyalBlue\">ä»¥ä¸‹ã®ç‰¹å¾´é‡ã¯ã€ä¹³è…ºè…«ç˜¤ã®ç©¿åˆºå¸å¼•ç´°èƒè¨ºï¼ˆFNAï¼‰ã®ãƒ‡ã‚¸ã‚¿ãƒ«ç”»åƒã‹ã‚‰è¨ˆç®—ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚ç”»åƒå†…ã«å­˜åœ¨ã™ã‚‹ç´°èƒæ ¸ã®ç‰¹å¾´ã‚’èª¬æ˜ã—ã¦ã„ã¾ã™ã€‚ä»¥ä¸‹ã®10ã®å„å±æ€§ã«ã¤ã„ã¦ã€ãã‚Œãã‚Œå¹³å‡ï¼ˆmeanï¼‰ã€æ¨™æº–èª¤å·®ï¼ˆseï¼‰ã€æœ€æ‚ªå€¤ï¼ˆworstï¼‰ã®3ç¨®é¡ã€åˆè¨ˆ30ã®ç‰¹å¾´é‡ãŒæ ¼ç´ã•ã‚Œã¦ã„ã¾ã™ã€‚</font><br>\n\n* `radius` - mean of distances from center to points on the perimeter\n<br>ã€€<font color=\"RoyalBlue\">åŠå¾„ - ä¸­å¿ƒã‹ã‚‰å¤–å‘¨ä¸Šã®ç‚¹ã¾ã§ã®è·é›¢ã®å¹³å‡</font>\n* `texture` - standard deviation of gray-scale values\n<br>ã€€<font color=\"RoyalBlue\">ãƒ†ã‚¯ã‚¹ãƒãƒ£ - ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«å€¤ã®æ¨™æº–åå·®</font>\n* `perimeter`\n<br>ã€€<font color=\"RoyalBlue\">å¤–å‘¨é•·</font>\n* `area`\n<br>ã€€<font color=\"RoyalBlue\">é¢ç©</font>\n* `smoothness` - local variation in radius lengths\n<br>ã€€<font color=\"RoyalBlue\">å¹³æ»‘æ€§ - åŠå¾„ã®é•·ã•ã®å±€æ‰€å¤‰å‹•</font>\n* `compactness` - perimeter^2 / area - 1.0\n<br>ã€€<font color=\"RoyalBlue\">ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆæ€§ - å¤–å‘¨é•·^2 / é¢ç© - 1.0</font>\n* `concavity` - severity of concave portions of the contour\n<br>ã€€<font color=\"RoyalBlue\">å‡¹åº¦ - è¼ªéƒ­ã®å‡¹éƒ¨ã®ç¨‹åº¦</font>\n* `concave points` - number of concave portions of the contour\n<br>ã€€<font color=\"RoyalBlue\">å‡¹ç‚¹æ•° - è¼ªéƒ­ã®å‡¹éƒ¨ã®æ•°</font>\n* `symmetry`\n<br>ã€€<font color=\"RoyalBlue\">å¯¾ç§°æ€§</font>\n* `fractal dimension` - \"coastline approximation\" - 1\n<br>ã€€<font color=\"RoyalBlue\">ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒ - è¤‡é›‘ã•ã®ç¨‹åº¦ã‚’è¡¨ã™å°ºåº¦ã€‚è¤‡é›‘ã§ã‚ã‚Œã°ã‚ã‚‹ã»ã©å€¤ãŒå¤§ãããªã‚‹</font>\n\n<a id=\"explore\"></a>\n# Explore CSV data ğŸ“Š\nAcknowledgements: [Feature Selection and Data Visualization](https://www.kaggle.com/kanncaa1/feature-selection-and-data-visualization)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(8, 8))\nsns.countplot(x=\"diagnosis\", ax=ax, data=df, palette=palette_ro[6::-5], alpha=0.9)\n\nax.annotate(len(df[df[\"diagnosis\"]==\"M\"]), xy=(-0.05, len(df[df[\"diagnosis\"]==\"M\"])+5),\n            size=16, color=palette_ro[6])\nax.annotate(len(df[df[\"diagnosis\"]==\"B\"]), xy=(0.95, len(df[df[\"diagnosis\"]==\"B\"])+5),\n            size=16, color=palette_ro[1])\n\nfig.suptitle(\"Distribution of diagnosis\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\ncolumns = df.columns.drop([\"id\", \"Unnamed: 32\", \"diagnosis\"])\n\ndata_s = pd.DataFrame(scaler.fit_transform(df[columns]), columns=columns)\ndata_s = pd.concat([df[\"diagnosis\"], data_s.iloc[:, 0:10]], axis=1)\ndata_s = pd.melt(data_s, id_vars=\"diagnosis\", var_name=\"features\", value_name=\"value\")\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 16))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", ax=ax1,\n               data=data_s, palette=palette_ro[6::-5], split=True,\n               scale=\"count\", inner=\"quartile\")\n\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", ax=ax2,\n              data=data_s, palette=palette_ro[6::-5])\n\nfig.suptitle(\"Mean values distribution\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"data_s = pd.DataFrame(scaler.fit_transform(df[columns]), columns=columns)\ndata_s = pd.concat([df[\"diagnosis\"], data_s.iloc[:, 10:20]], axis=1)\ndata_s = pd.melt(data_s, id_vars=\"diagnosis\", var_name=\"features\", value_name=\"value\")\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 16))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", ax=ax1,\n               data=data_s, palette=palette_ro[6::-5], split=True,\n               scale=\"count\", inner=\"quartile\")\n\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", ax=ax2,\n              data=data_s, palette=palette_ro[6::-5])\n\nfig.suptitle(\"Standard error values distribution\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"data_s = pd.DataFrame(scaler.fit_transform(df[columns]), columns=columns)\ndata_s = pd.concat([df[\"diagnosis\"], data_s.iloc[:, 20:30]], axis=1)\ndata_s = pd.melt(data_s, id_vars=\"diagnosis\", var_name=\"features\", value_name=\"value\")\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 16))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", ax=ax1,\n               data=data_s, palette=palette_ro[6::-5], split=True,\n               scale=\"count\", inner=\"quartile\")\n\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", ax=ax2,\n              data=data_s, palette=palette_ro[6::-5])\n\nfig.suptitle(\"Worst values distribution\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false,"_kg_hide-input":true},"cell_type":"code","source":"df_c = df.reindex(columns=[\"radius_mean\", \"radius_se\", \"radius_worst\", \"texture_mean\", \"texture_se\", \"texture_worst\",\n                           \"perimeter_mean\", \"perimeter_se\", \"perimeter_worst\", \"area_mean\", \"area_se\", \"area_worst\",\n                           \"smoothness_mean\", \"smoothness_se\", \"smoothness_worst\", \"compactness_mean\", \"compactness_se\", \"compactness_worst\",\n                           \"concavity_mean\", \"concavity_se\", \"concavity_worst\", \"concave points_mean\", \"concave points_se\", \"concave points_worst\",\n                           \"symmetry_mean\", \"symmetry_se\", \"symmetry_worst\", \"fractal_dimension_mean\", \"fractal_dimension_se\", \"fractal_dimension_worst\",\n                           \"diagnosis\"])\ndf_c = df_c.replace({\"M\":1, \"B\":0})\n\nprint(\"Correlation coefficient against diagnosis\")\ndf_c.corr().sort_values(\"diagnosis\", ascending=False)[\"diagnosis\"]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(18, 12))\n\nsns.heatmap(df_c.corr(), ax=ax, vmax=1, vmin=-1, center=0,\n            annot=True, fmt=\".2f\",\n            cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            mask=np.triu(np.ones_like(df_c.corr(), dtype=np.bool)))\n\n_, labels = plt.yticks()\nlabels[30].set_color(palette_ro[0])\n\nfig.suptitle(\"Diagonal correlation matrix\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since many of the features in this dataset have high correlation coefficients with each other, feature selection is very important.<br>\n<font color=\"RoyalBlue\">ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç‰¹å¾´é‡ã«ã¯äº’ã„ã«ç›¸é–¢ä¿‚æ•°ã®é«˜ã„ã‚‚ã®ãŒå¤šã„ãŸã‚ã€ç‰¹å¾´é‡é¸æŠãŒéå¸¸ã«é‡è¦ã«ãªã£ã¦ãã¾ã™ã€‚</font><br>\n\n<a id=\"preprocessing\"></a>\n# Data preprocessing ğŸ§¹"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.copy()\ny = X[\"diagnosis\"].replace({\"M\":1, \"B\":0})\nX = X.drop([\"id\", \"Unnamed: 32\", \"diagnosis\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, stratify=y, random_state=0)\nX_train.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"models\"></a>\n# Train models and make predictions ğŸ’­\nNow, let's create some models and check the performance measures. The performance measure for classifiers are as follows.<br>\n<font color=\"RoyalBlue\">ã§ã¯ã€ã„ãã¤ã‹ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã€æ€§èƒ½æŒ‡æ¨™ã‚’ç¢ºèªã—ã¦ã„ãã¾ã—ã‚‡ã†ã€‚åˆ†é¡å™¨ã®æ€§èƒ½æŒ‡æ¨™ã«ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªã‚‚ã®ãŒã‚ã‚Šã¾ã™ã€‚</font>\n\n> Referenced from Hands-On Machine Learning with Scikit-Learn and TensorFlow (Aurelien Geron, 2017).\n* accuracy - the ratio of correct predictions\n<br>ã€€<font color=\"RoyalBlue\">æ­£è§£ç‡ - æ­£ã—ã„äºˆæ¸¬ã®å‰²åˆ</font>\n* confusion matrix - counting the number of times instances of class A are classified as class B\n<br>ã€€<font color=\"RoyalBlue\">æ··åŒè¡Œåˆ— - ã‚¯ãƒ©ã‚¹ï¼¡ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒã‚¯ãƒ©ã‚¹ï¼¢ã«åˆ†é¡ã•ã‚ŒãŸå›æ•°ã‚’æ•°ãˆã‚‹</font>\n* precision - the accuracy of the positive predictions\n<br>ã€€<font color=\"RoyalBlue\">é©åˆç‡ - é™½æ€§ã®äºˆæ¸¬ã®æ­£è§£ç‡ï¼ˆé™½æ€§ã§ã‚ã‚‹ã¨äºˆæ¸¬ã—ãŸã†ã¡ã€å½“ãŸã£ã¦ã„ãŸç‡ï¼‰</font>\n* recall (sensitivity, true positive rate: TPR) - the ratio of positive instances that are correctly detected by the classifier\n<br>ã€€<font color=\"RoyalBlue\">å†ç¾ç‡ï¼ˆæ„Ÿåº¦ã€çœŸé™½æ€§ç‡ï¼‰- åˆ†é¡å™¨ãŒæ­£ã—ãåˆ†é¡ã—ãŸé™½æ€§ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®å‰²åˆï¼ˆæœ¬å½“ã«é™½æ€§ã§ã‚ã‚‹ã‚±ãƒ¼ã‚¹ã®ã†ã¡ã€é™½æ€§ã ã¨åˆ¤å®šã§ããŸç‡ï¼‰</font>\n* F1 score - the harmonic mean of precision and recall\n<br>ã€€<font color=\"RoyalBlue\">F1 ã‚¹ã‚³ã‚¢ï¼ˆF å€¤ï¼‰ - é©åˆç‡ã¨å†ç¾ç‡ã®èª¿å’Œå¹³å‡ï¼ˆç®—è¡“å¹³å‡ã«æ¯”ã¹ã€èª¿å’Œå¹³å‡ã¯ä½ã„å€¤ã«ãã†ã§ãªã„å€¤ã‚ˆã‚Šã‚‚ãšã£ã¨å¤§ããªé‡ã¿ã‚’ç½®ãï¼‰</font>\n* AUC - the area under the ROC curve (plotting the true positive rate (another name for recall) against the false positive rate)\n<br>ã€€<font color=\"RoyalBlue\">AUC - ROC æ›²ç·šï¼ˆå½é™½æ€§ç‡ã«å¯¾ã™ã‚‹çœŸé™½æ€§ç‡ï¼ˆå†ç¾ç‡ï¼‰ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ãŸæ›²ç·šï¼‰ã®ä¸‹ã®é¢ç©</font><br>\n\nIn this notebook, we will look at their accuracy, F1 score, and confusion matrix.<br>\n<font color=\"RoyalBlue\">ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€æ­£è§£ç‡ã€F1 ã‚¹ã‚³ã‚¢ã€ãã—ã¦æ··åŒè¡Œåˆ—ã‚’è¦‹ã¦ã„ãã¾ã™ã€‚</font>\n\n<a id=\"gbm\"></a>\n## LightGBM ğŸŒ³\nFirst, let's try a prediction with all the features using LightGBM.<br>\n<font color=\"RoyalBlue\">ã¾ãšã¯ã€LightGBM ã§å…¨ã¦ã®ç‰¹å¾´é‡ã‚’ä½¿ã£ãŸäºˆæ¸¬ã‚’è©¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"NFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(X_train.columns)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = lgb.LGBMClassifier(objective=\"binary\",\n                             metric=\"binary_logloss\")\n    clf.fit(X_tr, y_tr, eval_set=[(X_va, y_va)],\n            early_stopping_rounds=10,\n            verbose=-1)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.feature_importances_\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test, num_iteration=clf.best_iteration_)\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\n\nprint(f\"\\nOut-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\nprint(f\"Test recall:          {recall_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), X_train.columns), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of default LightGBM\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of default LightGBM\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1-score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16)\nplt.yticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, narrow down the number of features based on EDA and feature importance. Let's choose the following features.<br>\n<font color=\"RoyalBlue\">æ¬¡ã«ã€EDA ã‚„ feature importance ã‚’ã‚‚ã¨ã«ç‰¹å¾´é‡ã®æ•°ã‚’çµã‚Šã¾ã™ã€‚ä¸‹è¨˜ã®ã‚ˆã†ãªç‰¹å¾´é‡ã‚’é¸ã‚“ã§ã„ãã¾ã—ã‚‡ã†ã€‚</font>\n* High correlation coefficient with the objective variable\n<br>ã€€<font color=\"RoyalBlue\">ç›®çš„å¤‰æ•°ã¨ã®ç›¸é–¢ä¿‚æ•°ãŒé«˜ã„</font>\n* Less mixing in data distribution for the objective variable\n<br>ã€€<font color=\"RoyalBlue\">ç›®çš„å¤‰æ•°ã«å¯¾ã™ã‚‹ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã«ãŠã„ã¦æ··åœ¨ãŒå°‘ãªã„</font>\n* High feature importance\n<br>ã€€<font color=\"RoyalBlue\">feature importance ãŒé«˜ã„</font>\n* Features are independent of each other (to eliminate multicollinearity)\n<br>ã€€<font color=\"RoyalBlue\">ç‰¹å¾´é‡åŒå£«ãŒãªã‚‹ã¹ãç‹¬ç«‹ã—ã¦ã„ã‚‹ï¼ˆå¤šé‡å…±ç·šæ€§ã‚’è§£æ¶ˆã™ã‚‹ãŸã‚ï¼‰</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_features1 = [\"radius_mean\", \"radius_se\", \"radius_worst\", \"texture_mean\", \"texture_se\",\n                  \"perimeter_mean\", \"perimeter_se\", \"area_mean\", \"area_worst\",\n                  \"smoothness_mean\", \"smoothness_se\", \"compactness_mean\", \"compactness_se\", \"compactness_worst\",\n                  \"concavity_mean\", \"concavity_se\", \"concavity_worst\", \"concave points_worst\",\n                  \"symmetry_mean\", \"symmetry_se\", \"fractal_dimension_mean\", \"fractal_dimension_se\"]\nX_1 = X.drop(drop_features1, axis=1)\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\nsns.heatmap(pd.concat([X_1, y], axis=1).corr(), ax=ax, vmax=1, vmin=-1, center=0,\n            annot=True, fmt=\".2f\",\n            cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            mask=np.triu(np.ones_like(pd.concat([y, X_1], axis=1).corr(), dtype=np.bool)))\n\n_, labels = plt.yticks()\nlabels[8].set_color(palette_ro[0])\n\nfig.suptitle(\"Diagonal correlation matrix\", fontsize=18);\n\nX_train, X_test, y_train, y_test = train_test_split(X_1, y, test_size=0.3, shuffle=True, stratify=y, random_state=0)\nX_train.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(X_train.columns)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = lgb.LGBMClassifier(objective=\"binary\",\n                             metric=\"binary_logloss\",\n                             min_child_samples=10,\n                             reg_alpha=0.1)\n    clf.fit(X_tr, y_tr, eval_set=[(X_va, y_va)],\n            early_stopping_rounds=10,\n            verbose=-1)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.feature_importances_\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test, num_iteration=clf.best_iteration_)\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\ny_pred_gbm = np.mean(y_preds, axis=1)\n\nprint(f\"\\nOut-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\nprint(f\"Test recall:          {recall_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), X_train.columns), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of optimized LightGBM\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of optimized LightGBM\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16)\nplt.yticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ert\"></a>\n## Extremely randomized trees ğŸŒ³\nWe will also use the Extremely randomized trees.<br>\n<font color=\"RoyalBlue\">Extremely randomized trees ã‚‚ä½¿ã£ã¦ã¿ã¾ã—ã‚‡ã†ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, stratify=y, random_state=0)\nX_train.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(X_train.columns)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = ExtraTreesClassifier(random_state=0)\n    clf.fit(X_tr, y_tr)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.feature_importances_\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test)\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\nprint(f\"Test recall:          {recall_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), X_train.columns), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of default Extremely randomized trees\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of default Extremely randomized trees\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16)\nplt.yticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Do the same feature selection as before.<br>\n<font color=\"RoyalBlue\">å…ˆç¨‹ã¨åŒã˜ã‚ˆã†ã«ã€ç‰¹å¾´é‡é¸æŠã‚’è¡Œã„ã¾ã™ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_features2 = [\"radius_mean\", \"radius_se\", \"radius_worst\", \"texture_mean\", \"texture_se\",\n                  \"perimeter_mean\", \"perimeter_se\", \"area_mean\", \"area_worst\",\n                  \"smoothness_mean\", \"smoothness_se\", \"compactness_mean\", \"compactness_se\", \"compactness_worst\",\n                  \"concavity_mean\",  \"concavity_worst\", \"concave points_mean\", \"concave points_se\",\n                  \"symmetry_mean\", \"symmetry_se\", \"fractal_dimension_mean\", \"fractal_dimension_se\"]\nX_2 = X.drop(drop_features2, axis=1)\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\nsns.heatmap(pd.concat([X_2, y], axis=1).corr(), ax=ax, vmax=1, vmin=-1, center=0,\n            annot=True, fmt=\".2f\",\n            cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            mask=np.triu(np.ones_like(pd.concat([y, X_2], axis=1).corr(), dtype=np.bool)))\n\n_, labels = plt.yticks()\nlabels[8].set_color(palette_ro[0])\n\nfig.suptitle(\"Diagonal correlation matrix\", fontsize=18);\n\nX_train, X_test, y_train, y_test = train_test_split(X_2, y, test_size=0.3, shuffle=True, stratify=y, random_state=0)\nX_train.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(X_train.columns)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = ExtraTreesClassifier(random_state=0,\n                               n_estimators=200,\n                               min_samples_split=5)\n    clf.fit(X_tr, y_tr)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.feature_importances_\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test)\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\ny_pred_ert = np.mean(y_preds, axis=1)\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\nprint(f\"Test recall:          {recall_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), X_train.columns), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of optimized Extremely randomized trees\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of optimized Extremely randomized trees\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16)\nplt.yticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"lm\"></a>\n## Linear model ğŸ“ˆ\nIn order to get diverse models, we will also try linear model as a model without decision trees.<br>\n<font color=\"RoyalBlue\">å¤šæ§˜æ€§ã®ã‚ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’å¾—ã‚‹ãŸã‚ã«ã€æ±ºå®šæœ¨ã‚’ä½¿ã‚ãªã„ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã€ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã‚‚è©¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nX_s = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n\nX_train, X_test, y_train, y_test = train_test_split(X_s, y, test_size=0.3, shuffle=True, stratify=y, random_state=0)\nX_train.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = LogisticRegression(random_state=0)\n    clf.fit(X_tr, y_tr)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test)\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\nprint(f\"Test recall:          {recall_score(y_test, y_pred)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of default linear model\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16)\nplt.yticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Do feature selection.<br>\n<font color=\"RoyalBlue\">ç‰¹å¾´é‡é¸æŠã‚’è¡Œã„ã¾ã™ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_features3 = [\"radius_mean\", \"radius_se\", \"radius_worst\", \"texture_mean\", \"texture_se\",\n                  \"perimeter_mean\", \"perimeter_se\", \"area_mean\", \"area_worst\",\n                  \"smoothness_mean\", \"smoothness_se\", \"compactness_mean\", \"compactness_se\", \n                  \"concavity_se\", \"concavity_worst\", \"concave points_mean\", \"concave points_se\",\n                  \"symmetry_mean\", \"symmetry_se\", \"fractal_dimension_mean\", \"fractal_dimension_se\", \"fractal_dimension_worst\"]\nX_3 = X_s.drop(drop_features3, axis=1)\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\nsns.heatmap(pd.concat([X_3, y], axis=1).corr(), ax=ax, vmax=1, vmin=-1, center=0,\n            annot=True, fmt=\".2f\",\n            cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            mask=np.triu(np.ones_like(pd.concat([y, X_3], axis=1).corr(), dtype=np.bool)))\n\n_, labels = plt.yticks()\nlabels[8].set_color(palette_ro[0])\n\nfig.suptitle(\"Diagonal correlation matrix\", fontsize=18);\n\nX_train, X_test, y_train, y_test = train_test_split(X_3, y, test_size=0.3, shuffle=True, stratify=y, random_state=0)\nX_train.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = LogisticRegression(random_state=0)\n    clf.fit(X_tr, y_tr)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test)\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\ny_pred_lm = np.mean(y_preds, axis=1)\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\nprint(f\"Test recall:          {recall_score(y_test, y_pred)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of optimized linear model\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16)\nplt.yticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ensemble\"></a>\n# Simple ensemble ğŸ¤\nFor better accuracy, ensemble predictions of the three models.<br>\n<font color=\"RoyalBlue\">ç²¾åº¦ã‚’é«˜ã‚ã‚‹ãŸã‚ã«ã€3ã¤ã®ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’çµ„ã¿åˆã‚ã›ã¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚’è¡Œã„ã¾ã—ã‚‡ã†ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_em = y_pred_gbm*2 +  y_pred_ert*2 + y_pred_lm\ny_pred_em = (y_pred_em > 3).astype(int)\n\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred_em)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred_em)}\")\nprint(f\"Test recall:          {recall_score(y_test, y_pred_em)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred_em), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of the ensembled model\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1-score={:0.4f}\".format(accuracy_score(y_test, y_pred_em), f1_score(y_test, y_pred_em)), fontsize=14)\nplt.xticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16)\nplt.yticks(np.arange(2), [\"Benign\", \"Malignant\"], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We were able to get better accuracy by using the ensemble model. Thanks so much for reading!<br>\n<font color=\"RoyalBlue\">è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã™ã‚‹ã“ã¨ã§ã‚ˆã‚Šè‰¯ã„ç²¾åº¦ã‚’å‡ºã™ã“ã¨ãŒã§ãã¾ã—ãŸã€‚ã“ã“ã¾ã§èª­ã‚“ã§ãã ã•ã‚Šã©ã†ã‚‚ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸï¼</font>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}