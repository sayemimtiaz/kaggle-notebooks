{"cells":[{"metadata":{"_uuid":"1a71c7999af3ff4f2d469d9254335bc8f95b013c"},"cell_type":"markdown","source":"**Introduction**\n\nThis kernel relates to the Kaggle competition put on by PASSNYC, a not-for-profit organization which, according to their website, is dedicated to broadening educational opportunities for New York City's talented underserved students.\n\nAs part of that charter, PASSNYC sponsored the Kaggle competition found [here](http://www.kaggle.com/passnyc/data-science-for-good) to encourage NYC 8th graders to participate in the entrance exam (called the Specialized High Schools Admissions Test, or SHSAT) to NYC's eight specialized schools . As one of the PASSNYC representatives, Max, explains:  \n> Only a third (roughly) of eligible students take the SHSAT, and our goal is to drive more test takers (you can't get in if you don't sit for the test!). The education space is full of non-profits like ours with limited resources. So the research question is, given limited resources, where (at which schools) can you target your intervention efforts to make an impact on those participation numbers. [link to quote](http://www.kaggle.com/passnyc/data-science-for-good/discussion/59680#348922).\n\nExisting research from the [Research Alliance for New York City Schools](http://steinhardt.nyu.edu/scmsAdmin/media/users/sg158/PDFs/Pathways_to_elite_education/WorkingPaper_PathwaystoAnEliteEducation.pdf) from 2015 indicated that if you are a child in NYC that is female, black, hispanic or eligible for free lunch, you are underrepresented in SHSAT participation compared to the groups' respective citywide average. \n\n**Datasets**\n\nTo help work towards a solution to this problem, I leaned heavily on the Research Alliance's paper, augmenting their findings by exploring a few other data sets available online. These data sets were:\n* The School Explorer file provided as part of the Kaggle competition\n* New York Times article which supplied data on the number of students who took the SHSAT and subsequently received an offer in 2017, which can be found [here](http://www.nytimes.com/interactive/2018/06/29/nyregion/nyc-high-schools-middle-schools-shsat-students.html). \n* NYC Department of Education data from its website for every school\n* Socrata's NY School Safety Report (2010-2016) data set, found [here](https://www.kaggle.com/new-york-city/ny-2010-2016-school-safety-report)\n* Socrata's NYC Council Discretionary Funding data set, found [here](http://www.kaggle.com/new-york-city/new-york-city-council-discretionary-funding)\n\nIn the case of collecting the NYC Department of Education, the script took a number of hours to complete, as it goes to the Department of Education website for every school in New York City, simulates clicks and other events on the page to subsequently pull data. An example of such a website can be found for the Christa McAuliffe Middle School [here](https://tools.nycenet.edu/guide/2017/#dbn=20K187&report_type=EMS). The output of this script finds data for every NYC school, such as: \n* Student Population and Characteristics (e.g., number of 8th graders enrolled in 2017 and previous years, percentage of students with disabilities, etc.)\n* School Conditions and Practices (e.g., ratings for 2017 and previous years in constructs like how rigorous the instruction is, how collaborative teachers are, etc.)\n* Student Achievement and Outcomes (e.g., student proficiency by different demographics on the New York state-wide tests, percentage of students earning high school credit,etc)\n\nIn many cases, the data available for each school's NYC DoE website overlaps with the standard datasets provided as part of the competition, but the DoE website data provides data from prior years, along with additional data not included in the 2016 School Explorer data set.\n\n#Methodology / Approach\n**Variables**\nWe'll get into the code and output shortly, but first I wanted to summarize the general approach in recommending targeted interventions to specific schools. The output of this process is the predicted number of *additional* students at each NYC school that would have been expected to take the SHSAT test based on the following variables:\n1. Average ELA Performance at each school (from School Explorer dataset)\n2. City-wide difference of the percentage of students at each school that scored a 3 or 4 on the Math statewide exam (from the Dept of Ed data)\n3. Percentage number of students at each school that have an attendance rate greater than 90% (from the Dept of Ed data)\n4. City-wide difference of the percentage of 8th graders at each school that have attained at least some high-school credit (from the Dept of Ed data)\n5. Minimum distance from each school to one of the so-called 'Big Three' specialized high schools -- Stuyvesant HS, Bronx HS of Science, and Brooklyn Tech (calculated using the School Explorer data set)\n\nThese 5 variables were chosen after exploring all variables in the combined data set, including those from the Kaggle competition, external Socrata data, and data collected from the Dept of Education. A couple additional notes on these final variables:\n* To illustrate what the 'city-wide difference' variables (#2 and #4 in the above list) are, let's use an example. If the city-wide average of students that scored a 3 or 4 on the Math statewide exam is 50%, and a particular school has 60% of its students score a 3 or 4, the city-wide difference (referenced as `pct_math_level_3_or_4_2017_city_diff` going forward) variable would be 10%.  Similarly, if another school had 30% of its students score a 3 or 4 on Math, its citywide difference would be -20%.\n* As all the above variables are numerical, they were standardized before usage in the final model. To extend the example from the above bullet, if the mean of the `pct_math_level_3_or_4_2017_city_diff` variable is 0 with a standard deviation of .1 (i.e., 10%), then the standarized version of `pct_math_level_3_or_4_2017_city_diff` (referenced as `pct_math_level_3_or_4_2017_city_diff_stdized` going forward) of the two schools above would be 1 and -2 (since one school had 1 standard deviation above the mean for the raw numerical variable, and the other school had 2 standard deviations below the mean for the raw numerical variable\n* Notice that **none** of the variables take into account any data regarding the demographic population of the school. This was done on purpose; the thinking is that if we only use variables regarding school performance (statewide scores, attendance rate) or logistics (how far would these students have to travel to get to one of the more prestigious specialized school), we can identify schools that have favorable performance/logistics characteristics regardless of whether their populations have been underrepresented in the past.\n\n**Response Variable**\nThese five variables were used to create a linear regression model to predict the percentage of students to take the SHSAT at each school. This variable was easily create using the Grade 8 enrollment at each school from the Dept of Ed, along with the number of testtakers taken from the NYT article\n\n**Model and Output**\nThe model chosen for this purpose was a linear regression model, which, as mentioned previously, outputs a predicted percentage of students expected to take the SHSAT. We can then use the actual number of students enrolled in 8th grade at each school to calculate the number of *additional* (or, in the cases when predicted percentage is lower than the actual, *fewer*) students at each school expected to take the SHSAT. Using these numbers in conjunction with demographic information, we can estimate the number of students in underrepresented groups that might be expected to take the test.\n\n>**OK, apologies for all the words above. Let's begin by looking at the output of the entire process detailed below. We'll go through all the steps needed to created these visualizations in just a minute**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"import os\nimport pandas as pd\nimport scipy.stats\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport plotly\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly import tools\nimport plotly.plotly as py\n\ninput_data_dir = os.getcwd().replace('/working', '/input')\nfinal_data_set = pd.read_csv(\"{}/passnyc-created-data/step5_full_processed_test_dataset.csv\".format(input_data_dir))\nnyc_img=mpimg.imread('{}/passnyc-created-data/cropped_neighbourhoods_new_york_city_map.png'.format(input_data_dir))\n\npos_test_set_df = final_data_set[final_data_set[\"positive_diff_num_testtakers\"] == True]\nneg_test_set_df = final_data_set[final_data_set[\"positive_diff_num_testtakers\"] == True]\n\nfinal_data_set.plot(kind=\"scatter\", \n    x=\"Longitude\", \n    y=\"Latitude\", \n    c=final_data_set[\"pct_poverty_2017_val\"].astype(float), \n    s=final_data_set[\"diff_num_testtakers_all\"].astype(float) * 25, \n    cmap=plt.get_cmap(\"coolwarm\"), \n    title='Additional Expected Number of Testtakers (Color denotes % Below Poverty Level)',\n    figsize=(16.162,16),\n    alpha=0.5)\nplt.imshow(nyc_img, \n           extent=[float(final_data_set[\"Longitude\"].min() - .01) , float(final_data_set[\"Longitude\"].max() + .01), float(final_data_set[\"Latitude\"].min() - .01), float(final_data_set[\"Latitude\"].max() + .01)], \n           alpha = 0.25)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"In the above chart, the size of each bubble represents the number of **additional** students expected to have taken the SHSAT in 2017 at each school, with the color representing the percentage of students at or below the poverty level at each school. One possible intervention to encourage underrepresented populations to take the SHSAT is to focus on the schools that are represented by larger, red bubbles.\n\nThe below chart shows the same exact data as the chart above, except with more interactivity; when you hover a particular bubble, it shows the school name and the number of additional SHSAT testtakers expected, based on the output of the model."},{"metadata":{"trusted":true,"_uuid":"8859ae11cf5868520c531369ef50ca9794561a2f","_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"pos_data = [\n    {\n        'x': pos_test_set_df[\"Longitude\"],\n        'y': pos_test_set_df[\"Latitude\"],\n        'text': pos_test_set_df[\"addtl_testtakers_label\"],\n        'mode': 'markers',\n        'marker': {\n            'color': pos_test_set_df[\"pct_poverty_2017_val\"].astype(float),\n            'size': pos_test_set_df[\"diff_num_testtakers_all\"].astype(float) * 1.2,\n            'showscale': True,\n            'colorscale':'RdBu',\n            'opacity':0.5\n        }\n    }\n]\n\nlayout= go.Layout(\n    autosize=False,\n    width=900,\n    height=750,\n    title= 'Additional Expected Number of Testtakers (Color denotes % Below Poverty Level)',\n    xaxis= dict(\n        title= 'Longitude'\n    ),\n    yaxis=dict(\n        title='Latitude'\n    ))\nfig=go.Figure(data=pos_data,layout=layout)\nplotly.offline.iplot(fig, filename='scatter_hover_labels')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f6101019fa629121320724dc3a558253ddc50c8"},"cell_type":"markdown","source":"# Now, let's take a look at how those visualizations were produced\n## Step 1: Create the dataset\nFirst we will have to create the functions that scrape data from both the New York Times and NY Department of Education. Kaggle doesn't currently support enable internet access within its kernels, but this code is accessible through [my github project found here](http://github.com/b-o-l-l-a/kaggle-passnyc). To explore this data set within this kernel, I have uploaded it into the `../input` folder."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"7cf3cc9510a35a4c454256ed2dfcb842d7032545","collapsed":true},"cell_type":"code","source":"import os\nimport sys\nimport pandas as pd\nimport warnings\nimport requests\nfrom bs4 import BeautifulSoup\nimport numpy as np\nimport re\nfrom selenium import webdriver\nimport time\nimport math\n\ndef nyt_web_scrape(data_dir):\n\n    print(\"\\n*****scraping NYT article with SHSAT stats\")\n    nyt_schools_url = \"https://www.nytimes.com/interactive/2018/06/29/nyregion/nyc-high-schools-middle-schools-shsat-students.html\"\n    csv_drop_path = \"{}/step1_nyt_shsat_article_data.csv\".format(data_dir)\n    \n    schools_html = requests.get(nyt_schools_url, verify=False).content\n    schools_html = schools_html.decode(\"utf-8\")\n    schools_soup = BeautifulSoup(re.sub(\"<!--|-->\",\"\", schools_html), \"lxml\") \n\n    schools_table = schools_soup.find(class_=\"g-schools-table-container\").table.tbody\n    school_rows = schools_table.findAll('tr')\n\n    nyt_article_cols = [\n        \"school_name\",\n        \"dbn\",\n        \"num_testtakers\",\n        \"num_offered\",\n        \"pct_8th_graders_offered\",\n        \"pct_black_hispanic\"\n    ]\n    output_df = pd.DataFrame(columns = nyt_article_cols)\n    \n    for school in school_rows:\n        \n        school_dict = {}        \n        school_name = school['data-name']\n        dbn = school['data-dbn']\n        school_dict['school_name'] = school_name\n        school_dict['dbn'] = dbn\n\n        school_data = school.findAll('td')\n\n        for td in school_data:\n\n            school_stat = td['class']\n\n            if \"g-testers\" in school_stat :\n                school_dict['num_testtakers'] = td.string\n            elif \"g-offers\" in school_stat:\n                school_dict['num_offered'] = td.string\n            elif \"g-offers-per-student\" in school_stat:\n                school_dict['pct_8th_graders_offered'] = td.string\n            elif \"g-pct\" in school_stat:\n                school_dict['pct_black_hispanic'] = td.string\n\n        output_df = output_df.append(school_dict, ignore_index = True)\n\n    \n    merged_df = merge_w_explorer_data(output_df, data_dir)\n    print(\"-- dropping NYT article CSV to {}\".format(csv_drop_path))\n    merged_df.to_csv(csv_drop_path, index = False)\n\n    return merged_df\n\ndef merge_w_explorer_data(nyt_df, data_dir):\n\n    school_explorer_path = data_dir.replace('external', 'raw')\n    print(school_explorer_path)\n    school_explorer_df = pd.read_csv(\"{}/2016 School Explorer.csv\".format(school_explorer_path))\n    \n    nyt_df[[\"num_testtakers\", \"num_offered\"]] = nyt_df[[\"num_testtakers\", \"num_offered\"]].replace(to_replace=\"—\",value=0)\n    nyt_df[\"pct_8th_graders_offered\"] = nyt_df[\"pct_8th_graders_offered\"].replace(to_replace=\"—\",value=\"0%\")\n    \n    school_explorer_df_cols_to_keep = [\n        'School Name',\n        'SED Code',\n        'Location Code',\n        'District',\n        'Latitude',\n        'Longitude',\n        'Address (Full)',\n        'City',\n        'Zip',\n        'Grades',\n        'Grade Low',\n        'Grade High',\n        'Community School?',\n        'Economic Need Index',\n        'School Income Estimate',\n        'Percent ELL',\n        'Percent Asian',\n        'Percent Black',\n        'Percent Hispanic',\n        'Percent Black / Hispanic',\n        'Percent White',\n        'Student Attendance Rate',\n        'Percent of Students Chronically Absent',\n        'Rigorous Instruction %',\n        'Rigorous Instruction Rating',\n        'Collaborative Teachers %',\n        'Collaborative Teachers Rating',\n        'Supportive Environment %',\n        'Supportive Environment Rating',\n        'Effective School Leadership %',\n        'Effective School Leadership Rating',\n        'Strong Family-Community Ties %',\n        'Strong Family-Community Ties Rating',\n        'Trust %',\n        'Trust Rating',\n        'Student Achievement Rating',\n        'Average ELA Proficiency',\n        'Average Math Proficiency'\n    ]\n\n    trimmed_school_explorer_df = school_explorer_df[school_explorer_df_cols_to_keep]\n    \n    merged_df = pd.merge(nyt_df, trimmed_school_explorer_df, left_on=\"dbn\", right_on=\"Location Code\", how=\"outer\")\n    \n    # combine school_name column from nyt and school explorer data\n    merged_df['school_name'] = np.where(merged_df['school_name'].isnull(), merged_df['School Name'], merged_df['school_name'])\n    merged_df['dbn'] = np.where(merged_df['dbn'].isnull(), merged_df['Location Code'], merged_df['dbn'])\n\n    merged_df = merged_df.drop(['School Name', 'Location Code'], axis=1)\n\n    return merged_df\n\nwait_time_after_click = 0.5\nwait_time_after_exception = 30\n\ndef dept_of_ed_web_scrape(school_df, data_dir, start_idx = 0 , debug_flg = False):\n\n    print(\"\\n*****scraping NYC DOE\")\n    if debug_flg == False:\n        output_df = pd.DataFrame()\n    else:\n        output_df = pd.read_csv(\"{data_dir}/step2_in_flight_doe_nyt_data.csv\".format(**locals()))\n        output_df = output_df.drop_duplicates()\n\n    DoE_base_url = \"https://tools.nycenet.edu/guide/{year}/#dbn={dbn}&report_type=EMS\"\n\n    for idx, school in school_df[int(start_idx):].iterrows():\n\n        scrape_school_flg = get_scrape_flg(school, DoE_base_url)\n\n        if scrape_school_flg == False: \n            continue\n\n        row_dict = get_school_info(school, DoE_base_url, idx)\n        output_df = output_df.append(row_dict, ignore_index=True)\n        output_df.to_csv(\"{data_dir}/step2_in_flight_doe_nyt_data.csv\".format(**locals()),index=False)\n    output_df.to_csv(\"{data_dir}/step2_final_doe_nyt_data.csv\".format(**locals()),index=False)\n    \ndef get_school_info(school, DoE_base_url, idx):\n    \n    years_to_scrape = [2017]\n    school_row_dict = school\n\n    school_name = school['school_name']\n    dbn = school['dbn']\n\n    browser = webdriver.Chrome()\n\n    # if there's an issue scraping data from a particular school, \n    # you can just pass in the index and process will continue from there\n    print(\"school: {} / index: {}\".format(school_name, idx))\n    for year in years_to_scrape:\n        school_url = DoE_base_url.format(year=year, dbn=dbn)\n        print(\"--{}\".format(school_url))\n        \n        try:\n            browser.get(school_url)\n            browser.find_element_by_class_name('introjs-skipbutton').click()\n        except:\n            print(\"---in except, waiting {} seconds and retrying\".format(wait_time_after_exception))\n            time.sleep(wait_time_after_exception)\n            browser.get(school_url)\n            time.sleep(wait_time_after_exception)\n            browser.find_element_by_class_name('introjs-skipbutton').click() \n            \n        time.sleep(wait_time_after_click)\n\n        school_row_dict = get_student_achievement_stats(browser, school_row_dict, year)\n        school_row_dict = get_student_characteristic_stats(browser, school_row_dict, year)\n        \n    browser.quit()\n    return school_row_dict\n\ndef uncollapse_all(browser):\n    collapsible_content = browser.find_elements_by_class_name('osp-collapsible-title')\n    for x in range(0, len(collapsible_content)):\n        if collapsible_content[x].is_displayed():\n            collapsible_content[x].click()\n            time.sleep(wait_time_after_click)\n\ndef get_student_characteristic_stats(browser, school_row_dict, year):\n    \n    browser.find_element_by_id('tab-stu-pop').click()\n    time.sleep(wait_time_after_click)\n\n    uncollapse_all(browser)\n\n    student_characteristic_soup = BeautifulSoup(browser.page_source, \"lxml\") \n\n    enrollment_section = student_characteristic_soup.find(id=\"pop-eot\")\n    enrollment_content = enrollment_section.find(class_=\"osp-collapsible-content-wrapper\")\n    \n    for enrollment_grade in enrollment_content.children:\n        enrollment_data = enrollment_grade.find(class_=\"osp-collapsible-title\")\n        grade = enrollment_data.find(class_=\"name\").string.split(\"Grade \")[-1]\n        try:\n            grade = int(grade)\n        except ValueError:\n            pass\n        \n        if grade not in [7, 8]:\n            continue\n        \n        class_str = \"yr-\"\n        regex = re.compile(\".*({class_str}).*\".format(**locals()))\n        for child in enrollment_data.children:\n\n            if any('yr-' in string for string in child['class']):\n                idx = [i for i, s in enumerate(child['class']) if 'yr-' in s][0]\n                class_yr_str = child['class'][idx]\n                class_yr = int(class_yr_str.split('yr-')[-1])\n                enrollment_yr = year - class_yr\n\n                if enrollment_yr == year:\n                    school_row_dict[\"grade_{}_{}_enrollment\".format(grade, enrollment_yr)] = child.string\n\n    addtl_resources_section = student_characteristic_soup.find(id=\"pop-hns\")\n    addtl_resources_content = addtl_resources_section.find(class_=\"cat-collapsibles\")\n    addtl_resources_name_dict = {\n        \"Students in Families Eligible for HRA Assistance\" : \"pct_hra_assistance\",\n        \"Students in Families with Income Below Federal Poverty Level (Estimated)\" : \"pct_poverty\",\n        \"Students in Temporary Housing\" : \"pct_temp_housing\",\n        \"Economic Need Index\" : \"econ_need_index\",\n        \"Students with Disabilities\" : \"pct_students_w_disabilities\",\n        \"English Language Learners\" : \"pct_ell\",\n        \"Avg 5th Grade ELA Rating\" : \"incoming_avg_5th_grade_ela_rating\",\n        \"Avg 5th Grade Math Rating\" : \"incoming_avg_5th_grade_math_rating\",\n        \"Math Level 1\" : \"incoming_math_level_1\",\n        \"Math Level 2\" : \"incoming_math_level_2\",\n        \"Math Level 3\" : \"incoming_math_level_3\",\n        \"Math Level 4\" : \"incoming_math_level_4\",\n        \"ELA Level 1\" : \"incoming_ela_level_1\",\n        \"ELA Level 2\" : \"incoming_ela_level_2\",\n        \"ELA Level 3\" : \"incoming_ela_level_3\",\n        \"ELA Level 4\" : \"incoming_ela_level_4\"\n    }\n\n    for addtl_resource_cat in addtl_resources_content.children:\n        addtl_resource_cat_data = addtl_resource_cat.find(class_=\"osp-collapsible-content-wrapper\")\n    \n        cat_section = addtl_resource_cat.find(class_=\"osp-collapsible-title\")\n        cat_section_name = cat_section.find(class_=\"name\").div.string\n        cat_section_val = cat_section.find(class_=\"val\").string\n        cat_section_dist_diff = cat_section.find(class_=\"dist\").svg.text \n        cat_section_city_diff = cat_section.find(class_=\"city\").svg.text \n        stat_col = addtl_resources_name_dict[cat_section_name]\n        school_row_dict[\"{}_{}_val\".format(stat_col, year)] = cat_section_val\n        school_row_dict[\"{}_{}_dist_diff\".format(stat_col, year)] = cat_section_dist_diff\n        school_row_dict[\"{}_{}_city_diff\".format(stat_col, year)] = cat_section_city_diff\n\n        if cat_section_name == \"Economic Need Index\":\n            for subcat in addtl_resource_cat_data.children:\n                subcat_section = subcat.find(class_=\"osp-collapsible-title\")\n                subcat_section_name = subcat_section.find(class_=\"name\").div.string\n                stat_col = addtl_resources_name_dict[subcat_section_name]\n                subcat_section_val = subcat_section.find(class_=\"val\").string\n                subcat_section_dist_diff = subcat_section.find(class_=\"dist\").svg.text \n                subcat_section_city_diff = subcat_section.find(class_=\"city\").svg.text \n                school_row_dict[\"{}_{}_val\".format(stat_col, year)] = subcat_section_val\n                school_row_dict[\"{}_{}_dist_diff\".format(stat_col, year)] = subcat_section_dist_diff\n                school_row_dict[\"{}_{}_city_diff\".format(stat_col, year)] = subcat_section_city_diff\n\n\n    incoming_proficiency = student_characteristic_soup.find(id=\"pop-ipl\")\n    ipl_content = incoming_proficiency.find(class_=\"cat-collapsibles\")\n\n    for ipl in ipl_content.children:\n        ipl_data = ipl.find(class_=\"osp-collapsible-content-wrapper\")\n        ipl_section = ipl_data.find(class_=\"osp-collapsible-title\")\n        ipl_section_name = ipl_section.find(class_=\"name\").div.string\n        ipl_section_val = ipl_section.find(class_=\"val\").string\n        ipl_section_dist_diff = ipl_section.find(class_=\"dist\").svg.text \n        ipl_section_city_diff = ipl_section.find(class_=\"city\").svg.text \n        stat_col = addtl_resources_name_dict[ipl_section_name]\n        school_row_dict[\"{}_{}_val\".format(stat_col, year)] = ipl_section_val\n        school_row_dict[\"{}_{}_dist_diff\".format(stat_col, year)] = ipl_section_dist_diff\n        school_row_dict[\"{}_{}_city_diff\".format(stat_col, year)] = ipl_section_city_diff\n\n        if ipl_section_name in [\"Avg 5th Grade ELA Rating\", \"Avg 5th Grade Math Rating\"]:\n            if ipl_section_name == \"Avg 5th Grade Math Rating\":\n                stat_col_prepend = \"Math\"\n            elif ipl_section_name == \"Avg 5th Grade ELA Rating\":\n                stat_col_prepend = \"ELA\"\n            ipl_collapsible_children = ipl_data.find(class_=\"osp-collapsible-content-wrapper\")\n            for ipl_sub in ipl_collapsible_children.children:\n                ipl_subsection = ipl_sub.find(class_=\"osp-collapsible-title\")\n                ipl_subsection_name = ipl_subsection.find(class_=\"name\").div.string\n                ipl_subsection_val = ipl_subsection.find(class_=\"val\").string\n                ipl_subsection_size = ipl_subsection.find(class_=\"n\").string\n                ipl_subsection_dist_diff = ipl_subsection.find(class_=\"dist\").svg.text \n                ipl_subsection_city_diff = ipl_subsection.find(class_=\"city\").svg.text\n                stat_col = addtl_resources_name_dict[\"{} {}\".format(stat_col_prepend, ipl_subsection_name)]\n                school_row_dict[\"{}_{}_val\".format(stat_col, year)] = ipl_subsection_val\n                school_row_dict[\"{}_{}_n\".format(stat_col, year)] = ipl_subsection_size\n                school_row_dict[\"{}_{}_dist_diff\".format(stat_col, year)] = ipl_subsection_dist_diff\n                school_row_dict[\"{}_{}_city_diff\".format(stat_col, year)] = ipl_subsection_city_diff\n\n        time.sleep(wait_time_after_click)\n    return school_row_dict\n\ndef get_student_achievement_stats(browser, school_row_dict, year):\n    \n    browser.find_element_by_id('tab-stu-achieve').click()\n    time.sleep(wait_time_after_click)\n\n    uncollapse_all(browser)\n    \n    sa_soup = BeautifulSoup(browser.page_source, \"lxml\") \n\n    sa_name_dict = {\n        \"White\" : \"white\",\n        \"Hispanic\" : \"hispanic\",\n        \"Asian / Pacific Islander\" : \"asian_pacific\",\n        \"Black\" : \"black\",\n        \"Multiracial\" : \"multiracial\",\n        \"American Indian\" : \"amer_indian\",\n        \"ELA - Average Student Proficiency\" : \"avg_ela_proficiency\",\n        \"ELA - Percentage of Students at Level 3 or 4\" : \"pct_ela_level_3_or_4\",\n        \"Math - Average Student Proficiency\" : \"avg_math_proficiency\",\n        \"Math - Percentage of Students at Level 3 or 4\" : \"pct_math_level_3_or_4\",\n        \"Percent of 8th Graders Earning HS Credit\" : \"pct_8th_graders_w_hs_credit\"\n    }\n\n    sa_section = sa_soup.find(id=\"content-stu-achieve\")\n    sa_content = sa_section.find(class_=\"tab-content\")\n\n    sa_title = sa_content.find(class_=\"osp-collapsible-title\")\n    sa_score = sa_title.find(class_=\"score\").string\n    sa_score_dist_diff = sa_title.find(class_=\"dist\").svg.text \n    sa_score_city_diff = sa_title.find(class_=\"city\").svg.text\n\n    sa_metric_collapsibles = sa_content.find(id=\"sa-metric-collapsibles\")\n    for sa_metric in sa_metric_collapsibles.children:\n        sa_metric_content = sa_metric.find(class_=\"osp-collapsible-content-wrapper\")\n\n        for sa_stat in sa_metric_content.children:\n            sa_title = sa_stat.find(class_=\"osp-collapsible-title\") \n            try:\n                stat_name = sa_title.find(class_=\"name\").div.string\n            except AttributeError:\n                stat_name = sa_title.find(class_=\"name\").string\n\n            if stat_name in sa_name_dict.keys():\n                stat_val = sa_title.find(class_=\"value\").string\n                sa_stat_comp_diff = sa_title.find(class_=\"comp\").svg.text \n                sa_stat_city_diff = sa_title.find(class_=\"city\").svg.text\n                stat_col = sa_name_dict[stat_name]\n                school_row_dict[\"{}_{}\".format(stat_col, year)] = stat_val\n                school_row_dict[\"{}_{}_comp_diff\".format(stat_col, year)] = sa_stat_comp_diff\n                school_row_dict[\"{}_{}_city_diff\".format(stat_col, year)] = sa_stat_city_diff\n\n\n    sa_addtl_info = sa_content.find(id=\"sa-add-info\").find(class_=\"osp-collapsible-content-wrapper\")\n    \n    sa_attendance_div = sa_addtl_info.find(id=\"sa-add-info-sg0-m0\").find(class_=\"osp-collapsible-title\")\n    sa_attendance_name = sa_attendance_div.find(class_=\"name\").div.string\n    sa_attendance_val = sa_attendance_div.find(class_=\"value\").string\n\n    try:\n        sa_attendance_dist_diff = sa_attendance_div.find(class_=\"dist\").svg.text \n    except AttributeError:\n        sa_attendance_dist_diff = sa_attendance_div.find(class_=\"comp\").svg.text \n\n    sa_attendance_city_diff = sa_attendance_div.find(class_=\"city\").svg.text \n    school_row_dict[\"sa_attendance_90plus_{}\".format(year)] = sa_attendance_val\n    school_row_dict[\"sa_attendance_90plus_{}_dist_diff\".format(year)] = sa_attendance_dist_diff\n    school_row_dict[\"sa_attendance_90plus_{}_city_diff\".format(year)] = sa_attendance_city_diff\n\n\n    sa_proficiency_scores_by_ethnicity = sa_addtl_info.find(id=\"sa-add-info-re-nonoverlap\").find(class_=\"cat-demog-collapsibles\")\n\n    for sa_proficiency in sa_proficiency_scores_by_ethnicity.children:\n        sa_proficiency_row = sa_proficiency.find(class_=\"osp-collapsible-title\")\n        ethnicity = sa_proficiency_row.find(class_=\"name\").div.string\n        if ethnicity == \"Missing or Invalid Data\":\n            continue\n        ethnicity_col_name = sa_name_dict[ethnicity]\n        ethnicity_sample_size = sa_proficiency_row.find(class_=\"n\").string\n        incoming_ela = sa_proficiency_row.select('div.inc.ela')[0].string\n        avg_ela = sa_proficiency_row.select('div.avg.ela')[0].string\n        incoming_math = sa_proficiency_row.select('div.inc.mth')[0].string\n        avg_math = sa_proficiency_row.select('div.avg.mth')[0].string \n        school_row_dict[\"{}_{}_num_students\".format(ethnicity_col_name, year)] = ethnicity_sample_size\n        school_row_dict[\"{}_{}_incoming_ela\".format(ethnicity_col_name, year)] = incoming_ela\n        school_row_dict[\"{}_{}_avg_ela\".format(ethnicity_col_name, year)] = avg_ela\n        school_row_dict[\"{}_{}_incoming_math\".format(ethnicity_col_name, year)] = incoming_math\n        school_row_dict[\"{}_{}_avg_math\".format(ethnicity_col_name, year)] = avg_math    \n\n    return school_row_dict\n\ndef get_scrape_flg(school_dict, DoE_base_url):\n\n    \n    if isinstance(school_dict['Grades'], float) and math.isnan(school_dict['Grades']):\n        return False\n\n    grade_list = school_dict['Grades'].split(\",\")\n\n    for i, v in enumerate(grade_list): \n        try:\n            grade_list[i] = int(v)\n        except ValueError:\n            grade_list[i] = v\n        \n    if 8 not in grade_list:\n        return False\n\n    else:\n        try:\n            scrape_flg = check_enrollment(school_dict, DoE_base_url)\n        except Exception as e:\n            if type(e).__name__ == \"NoSuchElementException\":\n                url = DoE_base_url.format(year=2017, dbn=school_dict[\"dbn\"])\n                print(\"{} does not have data, returning False for scrape_flg\".format(url))\n                scrape_flg = False\n                \n        return scrape_flg\n\ndef check_enrollment(school_dict, DoE_base_url):\n    \n    dbn = school_dict['dbn']    \n    school_url = DoE_base_url.format(year=2017, dbn=dbn)\n\n    browser = webdriver.Chrome()\n    browser.get(school_url)\n    \n    browser.find_element_by_class_name('introjs-skipbutton').click()\n    time.sleep(wait_time_after_click)\n\n    browser.find_element_by_id('tab-stu-pop').click()\n    time.sleep(wait_time_after_click)\n\n    enrollment_elem = browser.find_element_by_id('pop-eot')\n    enrollment_elem.find_element_by_class_name('osp-collapsible-title').click()\n    time.sleep(wait_time_after_click)\n\n    enrollment_soup = BeautifulSoup(browser.page_source, \"lxml\") \n    enrollment_content = enrollment_soup.find(id=\"pop-eot\").find(class_=\"osp-collapsible-content-wrapper\")\n\n    scrape_flg = False   \n\n    for enrollment_grade in enrollment_content.children:\n        enrollment_data = enrollment_grade.find(class_=\"osp-collapsible-title\")\n        grade = enrollment_data.find(class_=\"name\").string.split(\"Grade \")[-1]\n\n        try:\n            grade = int(grade)\n\n            if grade == 8:\n                scrape_flg = True\n\n        except ValueError:\n            pass\n    \n    return scrape_flg","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75af82b48a7d7c8620021bacd406dd1536416934"},"cell_type":"markdown","source":"Now that we created our functions to pull data from both the New York Times and the New York Department of Education, we can call the controller function (commented out in the snippet below because of Kaggle kernel restrictions on Internet access).  \n\n*Note:* The web_scrape_controller() function can take **a lot** of time to complete, approximately 3-4 hours.\n\nLet's take a quick look at the data frame as it is now:"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"4c0721e58fb66b8e033185b6f7f688a517da1945"},"cell_type":"code","source":"def web_scrape_controller(start_idx = 0, debug_flg = False):\n\n    data_drop_dir = os.getcwd()\n    print(data_drop_dir)\n    nyt_shsat_df = nyt_web_scrape(data_drop_dir)\n    dept_of_ed_df = dept_of_ed_web_scrape(nyt_shsat_df, data_drop_dir, start_idx, debug_flg)\n\n#web_scrape_controller()\n\nfinal_data_set = pd.read_csv(\"{}/passnyc-created-data/step2_final_doe_nyt_data.csv\".format(input_data_dir))\nfinal_data_set.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1204211ddacb17bacf40e2415d20b2061545e2e1"},"cell_type":"markdown","source":"## Step 2: Clean, transform, and build features from the NYT / DOE data\nNow that we have some more raw data to play with, we need to do some work to make it suitable for modeling, as well as add in the Socrata data referenced above. \n\nFor example, the citywide percentage difference columns are strings like '+20%', '-4%', etc. A model doesn't know what to do with that information, so we need to transform them into a numerical variable. We will also need to fill NAs and cast columns appropriately, among other tasks. \n\nThe controller which handles all of these things is found below. Notice that it is a series of functions that return data frames."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f433a426142602d25c93fd91a4e99b357ace524e"},"cell_type":"code","source":"def features_controller(input_data_dir, output_data_dir):\n\n    output_file_path = \"{}/step3_interim_modeling_data.csv\".format(output_data_dir)\n    \n    output_df = pd.read_csv(\"{}/step2_final_doe_nyt_data.csv\".format(input_data_dir))\n    \n    output_df = clean_percentage_cols(output_df)\n    output_df = find_grade_8_flg(output_df)\n    output_df = clean_rows_and_cols(output_df)\n    output_df = get_addtl_columns(output_df)\n    output_df = create_dummy_vars(output_df)\n    output_df = get_socrata_data(output_df, input_data_dir)\n    output_df = cast_as_bool(output_df)\n    output_df = fill_na(output_df)\n    output_df.to_csv(output_file_path, index=False)\n\n    print(\"interim modeling output can be found at: {}\".format(output_file_path))\n          \n    return","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a2c61c89a14a4fc181bf4f57e66abae5f5bc53e"},"cell_type":"markdown","source":"The functions called in the above controller can be found below by clicking on the expandable `Code` button. The tasks these functions perform include:\n* Create a `borough` variable based on a school's zip code\n* Aggregate the Discretionary Funding dataset to create the number of dollars received by the government for each zip code, then assign this amount to each school based on its zip code\n* Aggregate the number of crimes of different types (violent, non-violent, etc) reported at each school from 2010-2016\n* Cast all columns to their appropriate data type\n* Create 'dummy' variables for all the potential categorical variables (like `Trust Rating`, `Collaborative Teacher Rating`, etc.)\n* Trim the dataset of columns that have a high percentage of nulls (in this case, > 25%)\n* Fill NAs of columns that have < 25% with appropriate values (in this case, I used the most common value for categorical variables, and a combination of the median, mean, and 0 for numerical variables)\n* Create additional response variables like perc_testtakers (which will become the response variable we use in our model)\n"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true,"collapsed":true,"_uuid":"0a8d20bae3184cd89e0ed397493b21160cc12b29"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport geopy.distance\n\ndef cast_as_bool(df):\n    \n    # sorry...this is messy. just checks to see if there are columns w/data type of float and 1,0\n    # then casts as bool    \n    for col in df.columns.values:\n        if df[col].dtype == \"float64\":\n            if len(df[col].unique()) == 2 \\\n                and 1 in df[col].unique() \\\n                and 0 in df[col].unique():\n\n                df[col] = df[col].astype(bool)\n            \n    return df\n\ndef fill_na(df):\n\n    response_vars = [\"num_testtakers\", \"num_offered\", \"pct_8th_graders_offered\", \"perc_testtakers\", \"perc_testtakers_quartile\"]\n    for response in response_vars:\n        if response == \"perc_testtakers_quartile\":\n            na_fill_val = 1\n        else:\n            na_fill_val = 0\n\n        df[response] = df[response].fillna(value=na_fill_val)    \n    \n    nobs = float(len(df))\n    \n    for col in df.columns.values:\n\n        num_nulls = float(df[col].isnull().sum())\n\n        if num_nulls / nobs > .1 or len(df[col].unique()) == 1:\n\n            df = df.drop(col, axis = 1)\n\n        elif num_nulls > 0:\n            if df[col].dtype == \"object\":\n                na_fill = df[col].value_counts().idxmax()\n            else:\n                na_fill = df[col].median()\n\n            df[col] = df[col].fillna(value = na_fill)\n    \n    #invalid_preds = [\"school_name\", \"dbn\", \"Address (Full)\", \"City\", \"Grades\", \"Grade Low\", \"Grade High\", \"SED Code\", \"Latitude\", \"Longitude\", \"Zip\"]\n    \n    #invalid_preds.extend(response_vars)\n#     interim_pred_df = interim_modeling_df.drop(invalid_preds, axis=1)\n#     interim_response_df = interim_modeling_df[response_vars]\n    return df\n\ndef transform_pct(col_string):\n    \n    if pd.isnull(col_string):\n        col_val = col_string\n    else:\n        result = re.search('(.*)%', col_string)\n        col_val = float(result.group(1))\n        col_val = col_val / 100\n\n    return col_val\n\ndef transform_pct_diff(col_string):\n\n    #test = col_string.extract('^(\\+|-)+(.*)%')\n    if pd.isnull(col_string):\n        col_val = col_string\n    else:    \n        result = re.search('^(\\+|-)+(.*)%', col_string)\n\n        sign = result.group(1) \n        col_val = float(result.group(2))\n        positive = True if sign == '+' else False\n        col_val = -1 * col_val if positive == False else col_val\n        col_val = col_val / 100\n\n    return col_val\n\ndef clean_percentage_cols(modeling_df):\n\n    modeling_df_cols = modeling_df.columns.values\n\n    for col in modeling_df_cols:\n        df_col = modeling_df[col]\n\n        clean_pct_flg = True if (df_col.dtype == object) and (df_col.str.contains('%').any()) else False\n        if clean_pct_flg:\n\n            # reason why escape char \\ is used is bc of regex underneath the hood of Series.str.contains\n            perc_diff_flg = True if (df_col.str.contains('\\+').any()) and (df_col.str.contains('-').any()) else False\n            \n            if perc_diff_flg == True:\n                df_col = df_col.apply(transform_pct_diff)\n            else:\n                df_col = df_col.apply(transform_pct)\n        modeling_df[col] = df_col\n    return modeling_df\n\ndef find_grade_8_flg(df):\n\n    bool_series = df.apply(lambda row: True if '8' in str(row['Grades']) else False, axis=1)\n    df['grade_8_flg'] = bool_series\n\n    return df\n\ndef clean_rows_and_cols(df):\n\n    # these schools were established in last year or two, and do not yet have 8th graders\n    dbns_to_remove = [\"15K839\", \"03M291\", \"84X492\", \"84X460\", \"28Q358\"]\n    df = df[~df['dbn'].isin(dbns_to_remove)]\n    \n    #TODO: use config to pull years and create incoming_state_score_cols in a better way\n    incoming_state_score_cols = [\n        \"incoming_ela_level_1_2017_n\",\n        \"incoming_ela_level_2_2017_n\",\n        \"incoming_ela_level_3_2017_n\",\n        \"incoming_ela_level_4_2017_n\",\n        \"incoming_math_level_1_2017_n\",\n        \"incoming_math_level_2_2017_n\",\n        \"incoming_math_level_3_2017_n\",\n        \"incoming_math_level_4_2017_n\"\n    ]\n\n    for state_score_col in incoming_state_score_cols:\n        df[state_score_col] = df[state_score_col].replace(to_replace=\"N < 5\", value=0)\n        df[state_score_col] = df[state_score_col].astype('float')\n\n    nobs = float(len(df))\n\n    # remove schools that don't have 8th graders taking the SHSAT\n    df = df[df[\"grade_8_flg\"] == True]\n\n    # remove columns with > 25% nulls\n    for col_name in df.columns.values:\n        \n        col_nulls = float(df[col_name].isnull().sum())\n        perc_nulls = col_nulls / nobs\n        \n        if perc_nulls > 0.25:\n            df = df.drop(col_name, axis=1)\n\n    # remove schools that don't have 8th grade enrollment    \n    df = df.dropna(axis=0, subset=[\"grade_8_2017_enrollment\"])\n\n    return df\n\ndef create_dummy_vars(df):\n\n    categorical_cols = [\n        \"Community School?\",\n        \"Rigorous Instruction Rating\",\n        \"Collaborative Teachers Rating\",\n        \"Supportive Environment Rating\",\n        \"Effective School Leadership Rating\",\n        \"Strong Family-Community Ties Rating\",\n        \"Trust Rating\",\n        \"Student Achievement Rating\",\n        \"borough\"\n    ]\n\n    ref_val_dict = {\n        \"Rigorous Instruction Rating\" : \"Meeting Target\",\n        \"Collaborative Teachers Rating\" : \"Meeting Target\",\n        \"Supportive Environment Rating\" : \"Meeting Target\",\n        \"Effective School Leadership Rating\" : \"Meeting Target\",\n        \"Strong Family-Community Ties Rating\" : \"Meeting Target\",\n        \"Trust Rating\" : \"Meeting Target\",\n        \"Student Achievement Rating\" : \"Meeting Target\"\n    }\n    for cat_col in categorical_cols:\n\n        dummy_df = pd.get_dummies(df[cat_col], prefix=cat_col, dummy_na=True)\n        dummy_df = dummy_df.astype('float')\n        df = pd.concat([df, dummy_df], axis=1)\n\n        drop_val = ref_val_dict.get(cat_col, None)\n        if drop_val is None:\n            drop_val = df.groupby([cat_col]).size().idxmax()\n        \n        drop_col = \"{}_{}\".format(cat_col, drop_val)\n        \n        df = df.drop(drop_col, axis=1)\n\n    return df\n\ndef get_socrata_data(df, data_dir):\n    print(\"--getting socrates data to incorporate into modeling\")\n    disc_funding_data_dir = data_dir.replace('passnyc-created-data', 'new-york-city-council-discretionary-funding')\n    school_safety_df = data_dir.replace('passnyc-created-data', 'ny-2010-2016-school-safety-report')\n    df = get_disc_funding_by_zip(disc_funding_data_dir, df)\n    df = get_school_safety_by_dbn(school_safety_df, df)\n    \n    return df\n\ndef get_school_safety_by_dbn(data_dir, df):\n    \n    school_safety_filename = '2010-2016-school-safety-report.csv'\n    school_safety_df = pd.read_csv(\"{}/{}\".format(data_dir, school_safety_filename))\n    consolidated_loc_df = school_safety_df[school_safety_df[\"DBN\"].isnull()]\n    \n    cols_to_fill = [\"Major N\", \"Oth N\", \"NoCrim N\", \"Prop N\", \"Vio N\", \n        \"AvgOfMajor N\", \"AvgOfOth N\", \"AvgOfNoCrim N\", \"AvgOfProp N\", \"AvgOfVio N\"] \n    \n    consolidated_locations = consolidated_loc_df[\"Building Name\"].unique()\n    school_yrs = consolidated_loc_df[\"School Year\"].unique()\n    \n    consolidated_loc_data = {}\n    for location in consolidated_locations:\n        consolidated_loc_data[location] = {}\n        loc_df = consolidated_loc_df[consolidated_loc_df[\"Building Name\"] == location]\n\n        for year in school_yrs:    \n            loc_yr_row = loc_df[loc_df[\"School Year\"] == year]  \n            if len(loc_yr_row) == 0:\n                # no data for that consolidated loc in that particular school yr\n                continue\n            elif len(loc_yr_row) > 1:\n                raise ValueError(\"duplicate data found for {} / {}\".format(location, year))\n            loc_yr_dict = get_loc_yr_data(loc_yr_row, cols_to_fill)\n            consolidated_loc_data[location][year] = loc_yr_dict    \n    dbn_crimes_df = pd.DataFrame()\n    school_safety_df.head()\n\n    for idx, row in school_safety_df.iterrows():\n\n        dbn = row['DBN']\n        dbn_nan_flg = isinstance(dbn, float) and np.isnan(dbn)\n        building_name = row['Building Name']\n        bldg_nan_flg = isinstance(building_name, float) and np.isnan(building_name)\n        if dbn_nan_flg == False and bldg_nan_flg == False:\n            dbn = dbn.strip()\n            school_yr = row['School Year']      \n            if building_name not in consolidated_loc_data.keys():\n                # no data for consolidated location\n                continue\n            if school_yr not in consolidated_loc_data[building_name].keys():\n                # no data for consolidated location in that school year\n                continue\n            loc_yr_data = consolidated_loc_data[building_name][school_yr]\n\n            #print([row[col] for col in cols_to_fill])\n            for col in cols_to_fill:\n                row[col] = loc_yr_data[col]\n\n        dbn_crimes_df = dbn_crimes_df.append(row)\n    dbn_crimes_df = dbn_crimes_df[~dbn_crimes_df[\"DBN\"].isnull()]\n    dbn_crimes_df = dbn_crimes_df.groupby(['DBN'])[cols_to_fill].agg('sum').reset_index()\n\n    output_df = pd.merge(df, dbn_crimes_df, left_on='dbn', right_on='DBN', how='left').drop(\"DBN\",axis=1)\n    \n    for col in cols_to_fill:\n        col_median = output_df[col].median()\n        output_df[col] = output_df[col].fillna(value=col_median)\n\n    numerator_cols = [\"Major N\", \"Oth N\", \"NoCrim N\", \"Prop N\", \"Vio N\"]\n    for numerator in numerator_cols:\n        denominator = \"AvgOf{}\".format(numerator)\n        new_col_name = \"{}_proportion\".format(numerator)\n        output_df[new_col_name] = output_df[numerator].astype(float) / output_df[denominator].astype(float)\n\n    return output_df\n    \ndef get_loc_yr_data(row, cols_to_fill):\n    \n    row_dict = {}\n    for col in cols_to_fill:\n        row_dict[col] = row[col].values[0]\n\n    return row_dict\n\ndef get_disc_funding_by_zip(data_dir, df):\n    \n    discretionary_funding_filename = 'new-york-city-council-discretionary-funding-2009-2013.csv'\n    disc_fund_df = pd.read_csv(\"{}/{}\".format(data_dir, discretionary_funding_filename))\n    \n    disc_fund_df = disc_fund_df[disc_fund_df[\"Status \"].isin([\"Cleared\", \"Pending\"])]\n    disc_fund_df = disc_fund_df[disc_fund_df[\"Postcode\"].notnull()]\n    disc_fund_df[\"zip\"] = disc_fund_df.apply(clean_zip, axis=1)\n\n    disc_fund_df = disc_fund_df[disc_fund_df['zip'].apply(lambda x: str(x).isdigit())]\n    disc_fund_df[\"zip\"] = disc_fund_df[\"zip\"].astype(int)\n    disc_fund_df = disc_fund_df[disc_fund_df['zip'].apply(lambda x: len(str(x)) == 5)]\n    \n    disc_funds_by_zip = disc_fund_df.groupby(['zip'])['Amount '].agg('sum').to_frame().reset_index()\n    disc_funds_by_zip.columns = [\"zip\", \"discretionary_funding\"]\n    disc_funds_by_zip[\"discretionary_funding\"] = disc_funds_by_zip[\"discretionary_funding\"].astype(float)\n    \n    output_df = pd.merge(df, disc_funds_by_zip, left_on='Zip', right_on='zip', how='left').drop(\"zip\",axis=1)\n    \n    return output_df\n\ndef clean_zip(row):\n\n    raw_zip = row[\"Postcode\"]\n    cleaned_zip = raw_zip.split(\"-\")[0]\n    return cleaned_zip\n\ndef get_addtl_columns(df):\n\n    df = make_continuous_categorical(df)\n    df = get_addtl_response_vars(df)\n    df['borough'] = df.apply(get_borough, axis=1)\n\n    dist_df = df.apply(get_dist_from_specialized_schools, axis=1)\n    df = pd.concat([df, dist_df], axis=1)\n\n    return df\n\ndef get_dist_from_specialized_schools(row):\n   \n    # this only captures distance from feeder school to one of big three specialized schools.\n    # however, other specialized schools are incl in specialized_school_long_lat dictionary, if needed\n    row_long_lat = (float(row['Latitude']), float(row['Longitude']))\n\n    specialized_school_long_lat = {\n        \"bronx_hs_of_science\" : (40.87833, -73.89083),\n        \"brooklyn_latin_school\" : (40.705, -73.9388889),\n        \"brooklyn_tech_hs\" : (40.6888889, -73.9766667),\n        \"hs_for_math_sci_eng\" : (40.8215, -73.9490),\n        \"hs_of_amer_studies\" : (40.8749, -73.8952),\n        \"queens_hs_for_sci\": (40.699, -73.797),\n        \"staten_island_tech\" : (40.5676, -74.1181),\n        \"stuyvesant_hs\" : (40.7178801, -74.0137509)\n    }\n    \n    big_three_schools = [\"bronx_hs_of_science\", \"brooklyn_tech_hs\", \"stuyvesant_hs\"]\n\n    row = {}\n    for specialized_school, specialized_long_lat in specialized_school_long_lat.items():\n        if specialized_school in big_three_schools:\n            row[\"dist_to_{}\".format(specialized_school)] = geopy.distance.vincenty(row_long_lat, specialized_long_lat).miles       \n    row[\"min_dist_to_big_three\"] = row[min(row, key=row.get)]\n    return pd.Series(row)\n\ndef get_addtl_response_vars(df):\n    \n    perc_testtakers = df[\"num_testtakers\"].astype(float) / df[\"grade_8_2017_enrollment\"].astype(float)\n    df[\"perc_testtakers\"] = perc_testtakers\n    df[\"perc_testtakers\"] = df.apply(lambda row: 1 if row[\"perc_testtakers\"] > 1 else row[\"perc_testtakers\"], axis=1)\n    perc_testtakers_quantiles = perc_testtakers.quantile([0.25, 0.5, 0.75])\n    quartile_1_max = perc_testtakers_quantiles[0.25]\n    quartile_2_max = perc_testtakers_quantiles[0.5]\n    quartile_3_max = perc_testtakers_quantiles[0.75]\n\n    df[\"perc_testtakers_quartile\"] = np.nan\n    df[\"perc_testtakers_quartile\"] = np.where(df[\"perc_testtakers\"] <= quartile_1_max, 1, df[\"perc_testtakers_quartile\"])\n    df[\"perc_testtakers_quartile\"] = np.where(\n        (df[\"perc_testtakers\"] > quartile_1_max) & (df[\"perc_testtakers\"] <= quartile_2_max), \n        2, df[\"perc_testtakers_quartile\"])\n    df[\"perc_testtakers_quartile\"] = np.where(\n        (df[\"perc_testtakers\"] > quartile_2_max) & (df[\"perc_testtakers\"] <= quartile_3_max), \n        3, df[\"perc_testtakers_quartile\"])\n    df[\"perc_testtakers_quartile\"] = np.where(df[\"perc_testtakers\"] > quartile_3_max, 4, df[\"perc_testtakers_quartile\"])    \n    \n    return df\n\ndef make_continuous_categorical(df):\n    \n    binary_cols = {\"econ_need_index_2017_city_diff\" : 0}\n\n    for col_to_xfrom, cutoff in binary_cols.items():\n        new_col_name = \"{}_binary\".format(col_to_xfrom)\n        df[new_col_name] = np.nan\n        df[new_col_name] = np.where(df[col_to_xfrom].astype(float) >= cutoff, True, False)\n        \n    return df\n\ndef get_borough(row):\n    \n    # taken from NYC dept of health: https://www.health.ny.gov/statistics/cancer/registry/appendix/neighborhoods.htm\n    borough_zip_dict = {\n        \"bronx\" : [\n            10453, 10457, 10460, 10458, 10467, 10468, 10451, 10452, 10456, \\\n            10454, 10455, 10459, 10474, 10463, 10471, 10466, 10469, 10470, \\\n            10475, 10461, 10462, 10464, 10465, 10472, 10473\n        ],\n        \"brooklyn\": [\n            11212, 11213, 11216, 11233, 11238, 11209, 11214, 11228, 11204, \\\n            11218, 11219, 11230, 11234, 11236, 11239, 11223, 11224, 11229, \\\n            11235, 11201, 11205, 11215, 11217, 11231, 11203, 11210, 11225, \\\n            11226, 11207, 11208, 11211, 11222, 11220, 11232, 11206, 11221, \\\n            11237\n        ],\n        \"manhattan\": [\n            10026, 10027, 10030, 10037, 10039, 10001, 10011, 10018, 10019, \\\n            10020, 10036, 10029, 10035, 10010, 10016, 10017, 10022, 10012, \\\n            10013, 10014, 10004, 10005, 10006, 10007, 10038, 10280, 10002, \\\n            10003, 10009, 10021, 10028, 10044, 10065, 10075, 10128, 10023, \\\n            10024, 10025, 10031, 10032, 10033, 10034, 10040, 10282\n        ],\n        \"queens\": [\n            11361, 11362, 11363, 11364, 11354, 11355, 11356, 11357, 11358, \\\n            11359, 11360, 11365, 11366, 11367, 11412, 11423, 11432, 11433, \\\n            11434, 11435, 11436, 11101, 11102, 11103, 11104, 11105, 11106, \\\n            11374, 11375, 11379, 11385, 11691, 11692, 11693, 11694, 11695, \\\n            11697, 11004, 11005, 11411, 11413, 11422, 11426, 11427, 11428, \\\n            11429, 11414, 11415, 11416, 11417, 11418, 11419, 11420, 11421, \\\n            11368, 11369, 11370, 11372, 11373, 11377, 11378\n        ],\n        \"staten_island\" : [\n            10302, 10303, 10310, 10306, 10307, 10308, 10309, 10312, 10301, \\\n            10304, 10305, 10314, 10311\n        ]\n    } \n\n    school_zip = row[\"Zip\"]\n\n    school_boro = None\n    for boro, borough_zip_list in borough_zip_dict.items():\n        if school_zip in borough_zip_list:\n            school_boro = boro\n            break\n    if school_boro is None:\n        school_boro = \"other\"\n\n    return school_boro","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71ac0f9d3ed32663f960a34fceec06e1e80f4f14"},"cell_type":"markdown","source":"### OK, let's call all of these functions within our features_controller()\nThis will take a few minutes as well. The two arguments we pass in are the input data directory (i.e., where the DoE and NYT data reside), along with an output directory where we can put the transformed data frame."},{"metadata":{"trusted":true,"_uuid":"859e6c31459662a9d89f899358cedd9576731a0e"},"cell_type":"code","source":"cwd = os.getcwd()\ninput_data_dir = \"{}/passnyc-created-data\".format(cwd.replace('/working', '/input'))\nprint(input_data_dir)\nfeatures_controller(input_data_dir, cwd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94aa60a91ba3a3ace3e1c0cfdcac33d52010e9df"},"cell_type":"code","source":"# the line below should work if you have ran the above snippet. For consistency, I'll bring in the data frame from the ../input directory already uploaded\n#interim_modeling_df = pd.read_csv('/kaggle/working/step3_interim_modeling_data.csv')\ninterim_modeling_df = pd.read_csv('{}/step3_interim_modeling_data.csv'.format(input_data_dir))\ninterim_modeling_df.describe()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1135a056406d89a520741ec98be13df1c904a7e2"},"cell_type":"markdown","source":"## Step 3: Exploratory Data Analysis\n\nNow that we have a full dataset to play with, we can start to explore the effects of each predictor variable on the response, `perc_testtakers`. We'll do this by fitting a single linear regression model for each predictor, and examining certain goodness-of-fit metrics like the coefficient of determination (r-squared), adjusted r-squared, and the median absolute error of the model.\n\nI decided to use the **median absolute error** instead of more common metrics in linear regression like mean-squared-error (MSE) because the response is a proportion between 0 and 1. For example, squaring an error of .25 would produce an MSE of .5. In my opinion, median absolute error is more interpretable because you can look at it and know that 50% of model predictions had an error at or below the median absolute error.\n\nTo fit the model, we'll use the `sklearn` library in Python"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true,"_uuid":"36a60f1a07fbc7d16998e4acc1edd0c31d70572a","collapsed":true},"cell_type":"code","source":"from sklearn import metrics\n\ndef adj_r2_score(lm, y, y_pred):\n    adj_r2 = 1 - float(len(y)-1)/(len(y)-len(lm.coef_)-1)*(1 - metrics.r2_score(y,y_pred))\n    return adj_r2\n\ndef get_pred_and_response_dfs(df):\n\n    invalid_preds = [\"school_name\", \"dbn\", \"Address (Full)\", \"City\", \"Grades\", \"Grade Low\", \"Grade High\", \\\n    \"SED Code\", \"Latitude\", \"Longitude\", \"Zip\"]\n    response_vars = [\"num_testtakers\", \"num_offered\", \"pct_8th_graders_offered\", \"perc_testtakers\", \"perc_testtakers_quartile\"]\n    response_var = \"perc_testtakers\"\n    invalid_preds.extend(response_vars)\n    \n    # incl dbn and school_name for convenience and to id each row\n    # incl enrollment to calculate addtl est'd num students\n    response_df_cols = [ \"school_name\", \"dbn\", \"Longitude\", \"Latitude\", response_var, \"num_testtakers\",\"grade_8_2017_enrollment\", \"pct_poverty_2017_val\", \"Percent Black / Hispanic\", ]\n    \n    response_df = df[response_df_cols]\n    pred_df = df.drop(response_df_cols, axis=1)\n    \n    return pred_df, response_df, invalid_preds, response_var","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d76cd9158bf1d626ad67b7ea92c8e89c6614f549","collapsed":true},"cell_type":"code","source":"import os\nimport sklearn\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model, metrics\n\ndef create_single_regressors(input_data_dir, output_data_dir, output_model_summaries_dir):\n\n    interim_modeling_df = pd.read_csv(\"{}/step3_interim_modeling_data.csv\".format(input_data_dir))\n\n    pred_df, response_df, invalid_preds, response_var = get_pred_and_response_dfs(interim_modeling_df)\n   \n    pred_train, pred_test, response_train, response_test = train_test_split(pred_df, response_df, test_size=0.25, random_state=223)\n\n    single_regression_df = pd.DataFrame()\n\n    for col in pred_train.columns.values:\n        if col in invalid_preds:\n            continue\n        else: \n            categorical = True if pred_train[col].dtype == \"object\" else False\n            pred_rows = get_single_regressor_model(col, pred_train, response_train, pred_test, response_test, response_var, categorical)\n            single_regression_df = single_regression_df.append(pred_rows, ignore_index=True)\n            \n    single_regression_df_cols = [\"model\", \"categorical\", \"model_r2\", \"model_adj_r2\", \"median_absolute_error\", \"pred_col\", \"pred_coef\"]\n    single_regression_df = single_regression_df[single_regression_df_cols]\n    output_file_path = \"{}/single_regressor_summary.csv\".format(output_model_summaries_dir)\n    print(\"dropping single regression CSV to {}\".format(output_file_path))\n    \n    single_regression_df.to_csv(output_file_path, index=False)\n    \ndef get_single_regressor_model(col_name, pred_train, response_train, pred_test, response_test, response_var, categorical_bool):\n    model_rows = []\n    \n    col_list = pred_train.columns.values\n    \n    if categorical_bool == True:\n        col_vars = [var for var in col_list if \"{}_\".format(col_name) in var]\n    else:\n        col_vars = [col_name]\n    x_train_df = pred_train[col_vars]\n    x_test_df = pred_test[col_vars]\n    x_model = linear_model.LinearRegression()\n    x_results = x_model.fit(x_train_df, response_train[response_var])\n\n    y_predicted_test = x_results.predict(x_test_df)\n    med_absolute_error = metrics.median_absolute_error(response_test[response_var], y_predicted_test)\n    r2 = metrics.r2_score(response_test[response_var],y_predicted_test)\n    adj_r2 = adj_r2_score(x_results, response_test[response_var],y_predicted_test)\n\n    coef_dict = {}\n    model_rows = []\n    for idx, col in enumerate(x_train_df.columns.values):\n        coef_dict[col] = x_results.coef_[idx]\n        model_row = {}\n        model_row['model'] = col_name\n        pred_coef = x_results.coef_[idx]\n        model_row['model_r2'] = r2\n        model_row['model_adj_r2'] = adj_r2\n        model_row['median_absolute_error'] = med_absolute_error\n        model_row['pred_col'] = col if categorical_bool == True else None\n        model_row['pred_coef'] = pred_coef\n        model_row['categorical'] = categorical_bool\n        model_rows.append(model_row)\n\n    return model_rows","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b63df2379252515b5a6e54feffe47e4f8bc1eda"},"cell_type":"code","source":"# we can use the previously-defined input_ and output_data_dir variables to pass into our function create_single_regressors.\n# we pass in a third directory for the output of these functions. For the purpose of the kernel, we'll set it to our current working dir\ncwd = os.getcwd()\ninput_data_dir = \"{}/passnyc-created-data\".format(cwd.replace('/working', '/input'))\noutput_data_dir = cwd\noutput_model_summaries_dir = cwd\nprint(input_data_dir)\nprint(output_data_dir)\nprint(output_model_summaries_dir)\ncreate_single_regressors(input_data_dir, cwd, cwd)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59c2e47e730cf63db538fede1f34a78153796e4c"},"cell_type":"markdown","source":"Before diving into the results, let's provide some definitions to the columns in this data frame:\n* `model (string)`: variable corresponding to the column name in `interim_modeling_df`\n* `categorical (bool)`: whether or not the variable was categorical\n* `model_r2 (float)`: coefficient of determination of the model\n* `model_adj_r2 (float)`: adjusted r-squared of the model (similar to the coefficient of determination, but takes into account the number of variables in the model. In this case, the number of variables included in the model would vary only if it was based on a categorical column\n* `median_absolute_error`: median absolute error associated with the model's test set\n* `pred_col (string)`: only applicable to categorical variables. If not null, the column `pred_col` corresponds to this dummy variable included in `interim_modeling_df`\n* `pred_coef (float)`: coefficient of the variable (`model` if a numeric column, `pred_col` if a categorical variable)"},{"metadata":{"_uuid":"e65252d059820ce88a3965771dd03a25dbd190f0"},"cell_type":"markdown","source":"Let's look at how the variables did by sorting by each model's r-squared value. Looking at this data frame, you'll notice a few things:\n* The `pred_coef` column can be interpreted as such: If it is positive, the variable has a direct relationship with the percentage of testtakers at a school. If it is negative, the variable has an indirect relationship (i.e., the predictor variable tends to *decrease* as the percentage with the percentage of testtakers *increases*\n* As such, the findings are similar to the Research Alliance for New York City Schools, mainly that variables related to statewide test performance are positively correlated to participation in the SHSAT. Additionally, variables concerning underrepresented groups are negatively correlated with participation in the SHSAT. As an example, in row 9 of the data frame below, `pct_students_w_disabilities_2017_val` has a negative coefficient in the `pred_coef` column. This can be interpreted as the higher percentage of students with disabilities at a school, the lower percentage of SHSAT takers in that school\n* Many of the shared variables included in both the School Explorer CSV and the CSV with data scraped from the Dept of Ed are **very** similar to one another. The variable `Economic Need Index` (line 16) shows the same data as the `econ_need_index_2017_val` variable scraped from the DoE; these two models perform nearly identical to one another"},{"metadata":{"trusted":true,"_uuid":"f027d3f240c795ed4d2c5bf42e0088165ad04c79"},"cell_type":"code","source":"output_model_summaries_dir = os.getcwd()\nsingle_regression_df = pd.read_csv(\"{}/single_regressor_summary.csv\".format(output_model_summaries_dir))\nsingle_regression_df.sort_values(by=\"model_r2\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2996d754312570e3c1dfb7eff96881455c949ab"},"cell_type":"markdown","source":"#### Other variables of interest identified in the Research Alliance for NYC Schools paper\nWhile much of the above trends in a similar direction as the Research Alliance's paper, not everything is the same. The Research Alliance's paper used `borough` and the minimum distance to one of the Big 3 specialized schools (`min_dist_to_big_three` in the data frame). When looking at including *only* those variables in a model, though, their performance was sub-par. In the case of `min_dist_to_big_three`, it found that there was a *direct* relationship to participation rate of the SHSAT (i.e., the further away a school is to one of the Big 3, the more likely that school has a higher SHSAT participation rate. "},{"metadata":{"trusted":true,"_uuid":"e502bb72775aa5faf98c5755bb2ac970563b3146"},"cell_type":"code","source":"single_regression_df[single_regression_df[\"model\"].isin(['borough', 'min_dist_to_big_three'])]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f916627baf2add7e8bc180305efb1698b19fb3ad"},"cell_type":"markdown","source":"#### Socrata data - Discretionary funding and number of crimes occurring at a school\nAs you would expect, the higher number of crimes at a school, the less likely that school has a higher SHSAT participation rate. \n\nAlso as you would expect, the higher amount of discretionary funding from the government to the zip code that shares a school, the higher the SHSAT participation rate.\n\nThe `model`s with `_proportion` appended are actually variables that are a combination of crimes reported at the school / average number of crimes of all buildings in the same \"group\" as defined by the dataset found [here](http://www.kaggle.com/new-york-city/ny-2010-2016-school-safety-report).\n\n> However, looking at the `model_r2` and `median_absolute_error` columns of these variables, they are not as predictive as some of the other variables previously discussed."},{"metadata":{"trusted":true,"_uuid":"d5eb713560bea4805a930f4d82baa7d108b7a469"},"cell_type":"code","source":"socrata_data_cols = ['discretionary_funding', 'Major N', 'Oth N', 'NoCrim N', 'Prop N', 'Vio N', \n                     'Major N_proportion', 'Oth N_proportion', 'NoCrim N_proportion', 'Prop N_proportion', 'Vio N_proportion']\nsingle_regression_df[single_regression_df[\"model\"].isin(socrata_data_cols)].sort_values(by=\"model_r2\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"744357fee7714c376bbe2faca1bd626a0b0709ba"},"cell_type":"markdown","source":"## Step 4: Build our final model\nFinally, we can build our final model. As mentioned, we'll be using a linear regression model to predict the percentage of SHSAT testtakers at every school. We can use this to determine the difference between the number of testtakers we would expect at a school versus how many actually took the SHSAT.\n\nAnother methodology consideration is that the data set is small, in terms of machine learning models -- only ~550 rows (one row for each school). Partly because of this small data set size, I wanted each school to be used in a test set for a model built on data from other schools. There are a few ways to do this, but I chose to divide the data set into two halves, train a model on each half while using the other half as the test set. Combining the results from each school when it was used in the test set, we can get a sense of model performance without worrying about determining model performance based on observations that *were actually used to train the model*.\n\nIf you click on the expandable `Code` button, you'll see the function that builds the model `create_final_model`, along with the variable that standardizes each variable. The reason we didn't standardize the variables when we built our single regression models is that standardization only comes into play when there are multiple variables included in a model. If a variable isn't standardized, more weight may be put on the variables that just happen to have larger units of measurement. For example, `Average ELA Proficiency` has a range of roughly 0-4. Percentage of students with high school credit, `pct_8th_graders_w_hs_credit_2017_city_diff` tends to a range between -0.5 and 0.5. If we were to include both variables in their raw form in the same model, more weight may be given to `Average ELA Proficiency` just because its unit of measurement is larger. Standardizing around the mean and using the standard deviation takes care of this issue. \n\nI went through many iterations going through potential predictors to include in the final model before building out this notebook, but the below code is what split both models into train/test sets, then combined the respective test sets into one comprehensive data set we can use for our visualizations we took a peek at above. I decided to include`min_dist_to_big_three` variable as there may be some effects that this variable has when interacting with others, as opposed to looking at the `min_dist_to_big_three` variable in a model by itself."},{"metadata":{"_kg_hide-output":true,"_kg_hide-input":false,"trusted":true,"collapsed":true,"_uuid":"9fbbaded29182670a05d614311048a83ed87e9fc"},"cell_type":"code","source":"import os\nimport sklearn\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model, metrics, preprocessing\n\ndef create_final_model(input_data_dir, output_data_dir, output_model_summaries_dir):\n\n    interim_modeling_df = pd.read_csv(\"{}/step3_interim_modeling_data.csv\".format(input_data_dir))\n\n    pred_df, response_df, invalid_preds, response_var = get_pred_and_response_dfs(interim_modeling_df)\n    \n    model_1_pred_train, model_1_pred_test, model_1_response_train, model_1_response_test = \\\n    train_test_split(pred_df, response_df, test_size=0.5, random_state=223)\n    \n    model_2_pred_train = model_1_pred_test\n    model_2_pred_test = model_1_pred_train\n    model_2_response_train = model_1_response_test\n    model_2_response_test = model_1_response_train\n    \n    final_preds = [\n    #\"school_name\", \"dbn\", # only incl these two for convenience in ID'ing rows\n    \"Average ELA Proficiency\", \n    \"pct_math_level_3_or_4_2017_city_diff\", \n    \"sa_attendance_90plus_2017\", \n    \"pct_8th_graders_w_hs_credit_2017_city_diff\",\n    \"min_dist_to_big_three\"\n#     \"Collaborative Teachers Rating_Approaching Target\",\n#     \"Collaborative Teachers Rating_Exceeding Target\",\n#     \"Collaborative Teachers Rating_Not Meeting Target\",\n#     \"Collaborative Teachers Rating_nan\",\n#     \"Major N_proportion\"\n]\n    model_1_train_pred_df = model_1_pred_train[final_preds]\n    model_1_test_pred_df = model_1_pred_test[final_preds]\n    \n    model_2_train_pred_df = model_2_pred_train[final_preds]\n    model_2_test_pred_df = model_2_pred_test[final_preds]\n    \n    stdized_model_1_train, stdized_model_1_test = standardize_cols(model_1_train_pred_df, model_1_test_pred_df)\n    stdized_model_2_train, stdized_model_2_test = standardize_cols(model_2_train_pred_df, model_2_test_pred_df)\n    \n    model_1 = linear_model.LinearRegression().fit(stdized_model_1_train, model_1_response_train[response_var])\n    model_1_train_predicted = model_1.predict(stdized_model_1_train) \n    model_1_test_predicted = model_1.predict(stdized_model_1_test)\n    model_1_response_train[\"predicted_perc_testtakers\"] = model_1_train_predicted\n    model_1_response_test[\"predicted_perc_testtakers\"] = model_1_test_predicted\n    model_1_coefficients = pd.concat([pd.DataFrame(stdized_model_1_train.columns),pd.DataFrame(np.transpose(model_1.coef_))], axis = 1)\n    model_1_coefficients.columns = [\"model_1_pred_name\", \"model_1_coef\"]\n    model_1_full_train_df = pd.concat([model_1_response_train, stdized_model_1_train], axis = 1)\n    model_1_full_test_df = pd.concat([model_1_response_test, stdized_model_1_test], axis = 1)\n\n    model_2 = linear_model.LinearRegression().fit(stdized_model_2_train, model_2_response_train[response_var])\n    model_2_train_predicted = model_1.predict(stdized_model_2_train) \n    model_2_test_predicted = model_2.predict(stdized_model_2_test)\n    model_2_response_train[\"predicted_perc_testtakers\"] = model_2_train_predicted\n    model_2_response_test[\"predicted_perc_testtakers\"] = model_2_test_predicted\n    model_2_coefficients = pd.concat([pd.DataFrame(stdized_model_2_train.columns),pd.DataFrame(np.transpose(model_2.coef_))], axis = 1)\n    model_2_coefficients.columns = [\"model_2_pred_name\", \"model_2_coef\"]\n    model_2_full_train_df = pd.concat([model_2_response_train, stdized_model_2_train], axis = 1)\n    model_2_full_test_df = pd.concat([model_2_response_test, stdized_model_2_test], axis = 1)\n    \n    final_train_set = pd.concat([model_1_full_train_df, model_2_full_train_df])\n    final_test_set =  pd.concat([model_1_full_test_df, model_2_full_test_df])\n\n    final_train_set.to_csv(\"{}/full_train_dataset.csv\".format(output_data_dir), index = False)\n    final_test_set.to_csv(\"{}/full_test_dataset.csv\".format(output_data_dir), index = False)\n    model_1_coefficients.to_csv(\"{}/model_1_coefficients.csv\".format(output_model_summaries_dir), index = False)\n    model_2_coefficients.to_csv(\"{}/model_2_coefficients.csv\".format(output_model_summaries_dir), index = False)\n    \n    final_r2 = metrics.r2_score(final_test_set[\"perc_testtakers\"], final_test_set[\"predicted_perc_testtakers\"])\n    final_median_abs_err = metrics.median_absolute_error(final_test_set[\"perc_testtakers\"], final_test_set[\"predicted_perc_testtakers\"])\n    \n    print(\"\\n\\nmodel_1_coefficients: \")\n    print(model_1_coefficients)\n    print(\"\\nmodel_2_coefficients: \")\n    print(model_2_coefficients)\n    print(\"\\n\\n** r2 of entire dataset: {}\".format(final_r2))\n    print(\"** median_absolute_error of entire dataset: {}\".format(final_median_abs_err))\n    \n    print(\"\\n\\ndropped output to folder: {}\".format(output_model_summaries_dir))\n    \n    \ndef standardize_cols(train_df, test_df):\n    \n    standardized_train_df = train_df\n    standardized_test_df = test_df\n    for pred_col in train_df.columns.values:\n        if train_df[pred_col].dtype == \"float64\":\n            scaler = preprocessing.StandardScaler().fit(standardized_train_df[[pred_col]])\n            standardized_train_df[\"{}_stdized\".format(pred_col)] = scaler.transform(standardized_train_df[[pred_col]])\n            standardized_test_df[\"{}_stdized\".format(pred_col)] = scaler.transform(standardized_test_df[[pred_col]])\n            standardized_train_df = standardized_train_df.drop(pred_col, axis=1)\n            standardized_test_df = standardized_test_df.drop(pred_col, axis=1)\n    return standardized_train_df, standardized_test_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8352e563c35df60063e5a077a8d5969026796970"},"cell_type":"markdown","source":"The code below lists the coefficients of both final models, along with performance metrics r-squared and median absolute error. Notice that the coefficients of both models have the same sign (positive or negative). This is important when determining whether the coefficients of your regression model are stable.\n\nAlso notice the median absolute error and r-squared of the models applied to their respective test sets. The two takeaways from these metrics are:\n* 58.7% of the variation in percentage of SHSAT takers at a school can be explained by the variables included in the model (from the `r2` output below)\n* 50% of observations in the test set have a prediction error of <= 8.386% (from the `median_absolute_error` output below)"},{"metadata":{"trusted":true,"_uuid":"568d0222b5dc03e78eb8a9c55cdcb0f22bed8475"},"cell_type":"code","source":"cwd = os.getcwd()\ninput_data_dir = \"{}/passnyc-created-data\".format(cwd.replace('/working', '/input'))\noutput_data_dir = cwd\noutput_model_summaries_dir = cwd\ncreate_final_model(input_data_dir, output_data_dir, output_model_summaries_dir)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb527b64127a43d457674db5a671375e4440e51c"},"cell_type":"markdown","source":"## Step 5: Visualize model results\nLast but not least, let's look at some visualizations from the model to figure out how useful it could be. First things first, let's look at how the **predicted** percentage of testtakers matched with the **actual** percentage of testtakers.\n\n*Note:* Apparently there is a bug within `pandas.DataFrame.plot` that cuts off the x-axis label when using a colormap in a scatter plot. [Link to pandas github issue here](http://github.com/pandas-dev/pandas/issues/10611). The x-axis is comprised of Percent of *Actual* testtakers, with the ticks representing [0, 0.2, 0.4, 0.6, 0.8, 1.0]"},{"metadata":{"trusted":true,"_uuid":"f871e2ff1fef92ed72c9616782a252ee18662493"},"cell_type":"code","source":"\nmodel_performance_df = pd.read_csv(\"{}/passnyc-created-data/step4_full_test_dataset.csv\".format(cwd.replace('/working', '/input')))\npred_actual_scatter = model_performance_df.plot(kind=\"scatter\", \n    x=\"perc_testtakers\", \n    y=\"predicted_perc_testtakers\",\n    c=model_performance_df[\"pct_poverty_2017_val\"].astype(float),\n    cmap=plt.get_cmap(\"coolwarm\"), \n    s=100,\n    colorbar=True,\n    alpha=0.75 ,\n    title='Predicted versus Actual Percentage of SHSAT Testtakers (Color denotes % Below Poverty Level)',\n    figsize=(20,10))\n\npred_actual_scatter.set_ylabel(\"Predicted Percentage of Testtakers\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36c5e6b94655f3cb5e218095024bb22c14e92c8f"},"cell_type":"markdown","source":"Let's re-create the maps we saw earlier, as well. There are a few straightforward calculations to create the data set that we'll visualize. Somehing important to keep in mind is that the `actual_<>_testtakers` columns broken out by demographics (e.g., `actual_testtakers_black_hispanic` and `actual_testtakers_below_poverty_lvl`) are calculated from two known numbers - the number of actual testtakers at a school * percentage of students that are in the given demographic. I do not know of any public resource to find that data directly.\n\nSimilarly, the `predicted_<>_testtakers` columns broken out by demographics are calculated by the number of predicted testtakers at a school * percentage of students that are in the given demographic."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"cb290ce7d5666a76431c9212cc7a16c08e938cc1"},"cell_type":"code","source":"input_data_dir = os.getcwd().replace('/working', '/input')\nmodel_performance_df = pd.read_csv(\"{}/passnyc-created-data/step4_full_test_dataset.csv\".format(input_data_dir))\nnyc_img=mpimg.imread('{}/passnyc-created-data/cropped_neighbourhoods_new_york_city_map.png'.format(input_data_dir))\n\nmodel_performance_df[\"predicted_testtakers_all\"] = model_performance_df[\"predicted_perc_testtakers\"].astype(float) * model_performance_df[\"grade_8_2017_enrollment\"].astype(float)\nmodel_performance_df[\"predicted_testtakers_black_hispanic\"] = model_performance_df[\"predicted_testtakers_all\"].astype(float) * model_performance_df[\"Percent Black / Hispanic\"].astype(float)\nmodel_performance_df[\"predicted_testtakers_below_poverty_lvl\"] = model_performance_df[\"predicted_testtakers_all\"].astype(float) * model_performance_df[\"pct_poverty_2017_val\"].astype(float)\nmodel_performance_df[\"actual_testtakers_black_hispanic\"] = model_performance_df[\"num_testtakers\"].astype(float) * model_performance_df[\"Percent Black / Hispanic\"].astype(float)\nmodel_performance_df[\"actual_testtakers_below_poverty_lvl\"] = model_performance_df[\"num_testtakers\"].astype(float) * model_performance_df[\"pct_poverty_2017_val\"].astype(float)\nmodel_performance_df[\"diff_num_testtakers_all\"] =  model_performance_df[\"predicted_testtakers_all\"] - model_performance_df[\"num_testtakers\"]\nmodel_performance_df[\"diff_num_testtakers_black_hispanic\"] = model_performance_df[\"predicted_testtakers_black_hispanic\"] - model_performance_df[\"actual_testtakers_black_hispanic\"] \nmodel_performance_df[\"diff_num_testtakers_below_poverty_lvl\"] = model_performance_df[\"predicted_testtakers_below_poverty_lvl\"] - model_performance_df[\"actual_testtakers_below_poverty_lvl\"] \nmodel_performance_df[\"percentile_diff_num_testtakers_all\"] = model_performance_df.apply(lambda row: float(scipy.stats.percentileofscore(model_performance_df[\"diff_num_testtakers_all\"], row[\"diff_num_testtakers_all\"])) / float(100), axis=1)\nmodel_performance_df[\"percentile_diff_num_black_hispanic\"] = model_performance_df.apply(lambda row: float(scipy.stats.percentileofscore(model_performance_df[\"diff_num_testtakers_black_hispanic\"], row[\"diff_num_testtakers_black_hispanic\"])) / float(100), axis=1)\nmodel_performance_df[\"percentile_diff_num_below_poverty_lvl\"] = model_performance_df.apply(lambda row: float(scipy.stats.percentileofscore(model_performance_df[\"diff_num_testtakers_black_hispanic\"], row[\"diff_num_testtakers_below_poverty_lvl\"])) / float(100), axis=1)\n\nmodel_performance_df[\"positive_diff_num_testtakers\"] = model_performance_df.apply(lambda row: True if row[\"diff_num_testtakers_all\"] > 0 else False, axis=1)\nmodel_performance_df[\"addtl_testtakers_label\"] = model_performance_df.apply(lambda row: \"{}: {} {} testtakers predicted\".format(row[\"school_name\"], int(row[\"diff_num_testtakers_all\"]), \"fewer\" if int(row[\"diff_num_testtakers_all\"]) < 0 else \"additional\") , axis=1)\nmodel_performance_df[\"addtl_testtakers_blk_hisp_label\"] = model_performance_df.apply(lambda row: \"{}: {} {} testtakers predicted\".format(row[\"school_name\"], int(row[\"diff_num_testtakers_black_hispanic\"]), \"fewer\" if int(row[\"diff_num_testtakers_black_hispanic\"]) < 0 else \"additional\") , axis=1)\nmodel_performance_df[\"addtl_testtakers_below_poverty_lvl_label\"] = model_performance_df.apply(lambda row: \"{}: {} {} testtakers predicted\".format(row[\"school_name\"], int(row[\"diff_num_testtakers_below_poverty_lvl\"]), \"fewer\" if int(row[\"diff_num_testtakers_below_poverty_lvl\"]) < 0 else \"additional\") , axis=1)\n\nnew_int_cols = [\"predicted_testtakers_all\", \"predicted_testtakers_black_hispanic\", \\\n    \"predicted_testtakers_below_poverty_lvl\", \"actual_testtakers_black_hispanic\", \\\n    \"actual_testtakers_below_poverty_lvl\", \"diff_num_testtakers_all\", \\\n    \"diff_num_testtakers_black_hispanic\", \"diff_num_testtakers_below_poverty_lvl\"\n] \nmodel_performance_df[new_int_cols] = model_performance_df[new_int_cols].astype(int)\nmodel_performance_cols_to_show = [\n    \"school_name\", \"dbn\", \"perc_testtakers\", \"predicted_perc_testtakers\", \"diff_num_testtakers_all\",\n    \"predicted_testtakers_all\", \"num_testtakers\",\n    \"predicted_testtakers_black_hispanic\", \"actual_testtakers_black_hispanic\", \n    \"predicted_testtakers_below_poverty_lvl\", \"actual_testtakers_below_poverty_lvl\",\n]\nmodel_performance_df.sort_values(by=\"diff_num_testtakers_all\", ascending = False )\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ade9e43c250d1d13a11e7bb72b8a52ad8df86456"},"cell_type":"markdown","source":"### Expected Additional Testtakers - All"},{"metadata":{"trusted":true,"_uuid":"1d8651b190e6a67e7c6f064ddd0656e3a8130440","_kg_hide-input":true},"cell_type":"code","source":"positive_df = model_performance_df[model_performance_df[\"positive_diff_num_testtakers\"] == True]\npositive_df.plot(kind=\"scatter\", \n    x=\"Longitude\", \n    y=\"Latitude\", \n    c=positive_df[\"pct_poverty_2017_val\"].astype(float), \n    s=positive_df[\"diff_num_testtakers_all\"].astype(float) * 25, \n    cmap=plt.get_cmap(\"coolwarm\"), \n    title='Additional Expected Number of Testtakers (Color denotes % Below Poverty Level)',\n    figsize=(16.162,16),\n    alpha=0.5)\nplt.imshow(nyc_img, \n           extent=[model_performance_df[\"Longitude\"].min() - .01 , model_performance_df[\"Longitude\"].max() + .01, model_performance_df[\"Latitude\"].min() - .01, model_performance_df[\"Latitude\"].max() + .01], \n           alpha = 0.25)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"449ad411043b2a0a410d82a52e65253c3daf2913"},"cell_type":"code","source":"pos_data = [\n    {\n        'x': positive_df[\"Longitude\"],\n        'y': positive_df[\"Latitude\"],\n        'text': positive_df[\"addtl_testtakers_label\"],\n        'mode': 'markers',\n        'marker': {\n            'color': positive_df[\"pct_poverty_2017_val\"].astype(float),\n            'size': positive_df[\"diff_num_testtakers_all\"].astype(float) * 1.2,\n            'showscale': True,\n            'colorscale':'RdBu',\n            'opacity':0.5\n        }\n    }\n]\n\nlayout= go.Layout(\n    autosize=False,\n    width=900,\n    height=750,\n    title= 'Additional Expected Number of Testtakers  (Color denotes % Below Poverty Level)',\n    xaxis= dict(\n        title= 'Longitude'\n    ),\n    yaxis=dict(\n        title='Latitude'\n    ))\nfig=go.Figure(data=pos_data,layout=layout)\nplotly.offline.iplot(fig, filename='scatter_hover_labels')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51a1921f2bc65af8b79de1e2ab0976e014cfcad3"},"cell_type":"markdown","source":"Let's produce similar graphs as the two above, but instead of the size of each bubble representing *all* additional expected testtakers at a school, we can look at the estimated number of additional testtakers from underrepresented groups\n\n### Expected Additional Testtakers - Black / Hispanic"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"c8ef369397d0752a108c625732d5253194b450db"},"cell_type":"code","source":"positive_df.plot(kind=\"scatter\", \n    x=\"Longitude\", \n    y=\"Latitude\", \n    c=positive_df[\"pct_poverty_2017_val\"].astype(float), \n    s=positive_df[\"diff_num_testtakers_black_hispanic\"].astype(float) * 25, \n    cmap=plt.get_cmap(\"coolwarm\"), \n    title='Additional Expected Number of Black/Hispanic Testtakers (Color denotes % Below Poverty Level)',\n    figsize=(16.162,16),\n    alpha=0.5)\nplt.imshow(nyc_img, \n           extent=[model_performance_df[\"Longitude\"].min() - .01 , model_performance_df[\"Longitude\"].max() + .01, model_performance_df[\"Latitude\"].min() - .01, model_performance_df[\"Latitude\"].max() + .01], \n           alpha = 0.25)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"23e31b705dedef7a7d1840875cb5a906977b2b25"},"cell_type":"code","source":"pos_data = [\n    {\n        'x': positive_df[\"Longitude\"],\n        'y': positive_df[\"Latitude\"],\n        'text': positive_df[\"addtl_testtakers_blk_hisp_label\"],\n        'mode': 'markers',\n        'marker': {\n            'color': positive_df[\"pct_poverty_2017_val\"].astype(float),\n            'size': positive_df[\"diff_num_testtakers_black_hispanic\"].astype(float) * 1.2,\n            'showscale': True,\n            'colorscale':'RdBu',\n            'opacity':0.5\n        }\n    }\n]\n\nlayout= go.Layout(\n    autosize=False,\n    width=850,\n    height=750,\n    title= 'Additional Expected Number of Black/Hispanic Testtakers (Color denotes % Below Poverty Level)',\n    xaxis= dict(\n        title= 'Longitude'\n    ),\n    yaxis=dict(\n        title='Latitude'\n    ))\nfig=go.Figure(data=pos_data,layout=layout)\nplotly.offline.iplot(fig, filename='scatter_hover_labels')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3277a0956768e54e9ed944c1a50c11109f014c57"},"cell_type":"markdown","source":"### Expected Additional Testtakers - Below Poverty Level"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"c934d7e077bec186b9d555b5bb5cbe2f7583e813"},"cell_type":"code","source":"positive_df.plot(kind=\"scatter\", \n    x=\"Longitude\", \n    y=\"Latitude\", \n    c=positive_df[\"pct_poverty_2017_val\"].astype(float), \n    s=positive_df[\"diff_num_testtakers_below_poverty_lvl\"].astype(float) * 25, \n    cmap=plt.get_cmap(\"coolwarm\"), \n    title='Additional Expected Number of Testtakers Below Poverty Level (Color denotes % Below Poverty Level)',\n    figsize=(16.162,16),\n    alpha=0.5)\nplt.imshow(nyc_img, \n           extent=[model_performance_df[\"Longitude\"].min() - .01 , model_performance_df[\"Longitude\"].max() + .01, model_performance_df[\"Latitude\"].min() - .01, model_performance_df[\"Latitude\"].max() + .01], \n           alpha = 0.25)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"982c69c26e23cb4ab71df06503959235c2cf665f"},"cell_type":"code","source":"pos_data = [\n    {\n        'x': positive_df[\"Longitude\"],\n        'y': positive_df[\"Latitude\"],\n        'text': positive_df[\"addtl_testtakers_below_poverty_lvl_label\"],\n        'mode': 'markers',\n        'marker': {\n            'color': positive_df[\"pct_poverty_2017_val\"].astype(float),\n            'size': positive_df[\"diff_num_testtakers_below_poverty_lvl\"].astype(float) * 1.2,\n            'showscale': True,\n            'colorscale':'RdBu',\n            'opacity':0.5\n        }\n    }\n]\n\nlayout= go.Layout(\n    autosize=False,\n    width=850,\n    height=750,\n    title= 'Additional Expected Number of Testtakers below Poverty Level (Color denotes % Below Poverty Level)',\n    xaxis= dict(\n        title= 'Longitude'\n    ),\n    yaxis=dict(\n        title='Latitude'\n    ))\nfig=go.Figure(data=pos_data,layout=layout)\nplotly.offline.iplot(fig, filename='scatter_hover_labels')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8cc721454739cb8d75d6fbb2d529f1dee366922"},"cell_type":"markdown","source":"## Step 6: Potential Areas for Additional Exploration and Limitations of the above analysis\nFirst of all - thanks for reading this far into this notebook! Below are some observations as I was completing this analysis, along with a few areas that I would have liked to explore further:\n1. There were no interaction effects taken into any of the above. Potential insights could be yielded by looking when looking at (as an example) crimes reported at a school along with statewide performance metrics. There are countless numbers of possible interactions, however, so it would take a good deal of time and effort,\n2. The web scraper functions from the Department of Education is just scratching the surface. There is a ***ton*** of data available in a standardized format on the Dept of Education site for each school (example for [Stuyvesant High School here](http://tools.nycenet.edu/guide/2017/#dbn=02M475&report_type=HS), all going a few years back\n3. Related to #2, all the data scraped is in regards to the most recent year currently available, 2017. There are no time series information of data taken from the past 2, 3, 4 years that could yield additional information\n4. The 'additional expected testtakers' for black, hispanic students and students below poverty level are taken as a direct percentage of the school's Grade 8 enrollment. In other words, it assume that, within a school, the distribution of students that take the SHSAT and the distribution of students who are part of specialized schools' underrepresented group are the same. This may or may not be the case\n5. Schools whose *actual* SHSAT percentage was much different than *predicted* (i.e., outliers) could be incredibly useful in uncovering some of the dynamics involved in why students decide to take (or not take) the SHSAT. Below is the list of such schools. In the cases of these particular schools, the regression model described above was *not* a good representation of the relationship between school characteristics and SHSAT. Which begs the question - what is going on in these schools that either encourage or discourage students from taking the test?\n\nGiven the areas of additional exploration above, however, much of the analysis is in line with prior research, notably the Research Alliance for NYC Schools' [Pathway to an Elite Education](http://steinhardt.nyu.edu/scmsAdmin/media/users/sg158/PDFs/Pathways_to_elite_education/WorkingPaper_PathwaystoAnEliteEducation.pdf) report from 2015. This lends credence to many of the conclusions that can be reached from the much of what is found in the rest of this analysis."},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"a191abc00a4354bc5452003b1842a5349ae94cb5"},"cell_type":"code","source":"prediction_error = abs(model_performance_df[\"predicted_perc_testtakers\"] - model_performance_df[\"perc_testtakers\"])\npred_error_quantiles = prediction_error.quantile([.1,.25,.5, .75,.95])\nabs_pred_err_95_perc = pred_error_quantiles[.95]\nmodel_performance_df[[\"school_name\",\"grade_8_2017_enrollment\", \"perc_testtakers\", \"predicted_perc_testtakers\"]][abs(model_performance_df[\"predicted_perc_testtakers\"] - model_performance_df[\"perc_testtakers\"]) > abs_pred_err_95_perc].sort_values(by=\"perc_testtakers\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f8a02f050b1c3a80e4a272c5bc3c3db727158af"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}