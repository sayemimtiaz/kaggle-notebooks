{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Life Expectancy Prediction"},{"metadata":{},"cell_type":"markdown","source":"### on data with thousands of missing values"},{"metadata":{},"cell_type":"markdown","source":"The data contain health and socioeconomic indicators of countries, and we will deploy regression techniques to predict life expectancy.\n\nUnfortunately, there are around 2.500 missing values, and although we can impute with the column means, we will implement a more intelligent technique. First, we will inspect correlations between features and pair NaN-containing features with features that correlate highly with them. For every NaN value we will select all the entries whose pair-feature has similar value with that of the instance that corresponds to the missing value, and we will take the mean only of these instances to fill the missing value. \n\nThis technique worked great and resulted in R2-score above 96%."},{"metadata":{},"cell_type":"markdown","source":"### Import Data and Inspect Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../input/life-expectancy-who/Life Expectancy Data.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#visualize NaN values\nsns.heatmap(df.isnull());","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#columns with null values\nnans = pd.DataFrame(data=[], index=None, \n                          columns=['feature_name','missing_values','percentage_of_total'])\nnans['feature_name'] = df.columns[df.isna().sum()>0]\nnans['missing_values'] = np.array(df[nans.iloc[:,0]].isna().sum())\nnans['percentage_of_total'] = np.round(nans['missing_values'] / df.shape[0] * 100)\nnans['var_type']= [df[c].dtype for c in nans['feature_name']]\nnans","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For features with few NaN values, we'll just fill in with the column means"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#make list of columns with up to 50 nan, impute these with column mean\nnan_cols = list(nans['feature_name'][nans['missing_values']<=50])\nfor col in nan_cols:\n    mean_ = df[col].mean()\n    df[col][df[col].isna()==True] = mean_","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#columns with null values\nnans = pd.DataFrame(data=[], index=None, \n                          columns=['feature_name','missing_values','percentage_of_total'])\nnans['feature_name'] = df.columns[df.isna().sum()>0]\nnans['missing_values'] = np.array(df[nans.iloc[:,0]].isna().sum())\nnans['percentage_of_total'] = np.round(nans['missing_values'] / df.shape[0] * 100)\nnans['var_type']= [df[c].dtype for c in nans['feature_name']]\nnans","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the rest columns, we start by looking at the correlations."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"corr_matr = df.corr()\ncorr_matr","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"sns.heatmap(df.corr());","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before we fill the missing values, we will store the feature names and the chosen correlation coefficients for later use, where we will form a dataframe with the linear regression coefficients and random forest feature importances, and compare them"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#for later use, to compare with linear regression coefficients\ncols=[0,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\nfeat_names = corr_matr.iloc[1,cols].keys()\ncorr_coefs = corr_matr.iloc[1,cols].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The function to impute missing values takes the feature with the missing values and the feature to be used as reference, and works like this:\n1. extract indexes where the target feature is NaN and the reference feature isn't. (Instances where both columns are Nan will be dealt with later)\n2. divide the range of the refence feature into ten groups\n3. for each of these ten groups, compute and store means of the target group's instances that correspond to these ranges of the reference column\n4. for each NaN value, fill with mean corresponding to the range within which the same-index instance of reference column falls"},{"metadata":{"trusted":true},"cell_type":"code","source":"#more concise\ndef impute(df, to_impute, reference):\n    index=df[to_impute][(df[to_impute].isna()==True)&\n                    (df[reference].isna()==False)].keys()\n    #df['Total expenditure'][index]\n    var_min = df[reference].min()\n    var_max = df[reference].max()\n    range_filler =  var_max - var_min\n    step = range_filler / 10\n    one = df[to_impute][df[reference] < (var_min+step)].mean()\n    two = df[to_impute][(df[reference] > (var_min+step))&\n              (df[reference] < (var_min+step*2))].mean()\n    three = df[to_impute][(df[reference] > (var_min+step*2))&\n              (df[reference] < (var_min+step*3))].mean()\n    four = df[to_impute][(df[reference] > (var_min+step*3))&\n              (df[reference] < (var_min+step*4))].mean()\n    five = df[to_impute][(df[reference] > (var_min+step*4))&\n              (df[reference] < (var_min+step*5))].mean()\n    six = df[to_impute][(df[reference] > (var_min+step*5))&\n              (df[reference] < (var_min+step*6))].mean()\n    seven = df[to_impute][(df[reference] > (var_min+step*6))&\n              (df[reference] < (var_min+step*7))].mean()\n    eight = df[to_impute][(df[reference] > (var_min+step*7))&\n              (df[reference] < (var_min+step*8))].mean()\n    nine = df[to_impute][(df[reference] > (var_min+step*8))&\n              (df[reference] < (var_min+step**9))].mean()\n    ten = df[to_impute][df[reference] > (var_max-step)].mean()\n    \n    for i in index:\n        if df[reference][i] < (var_min+step):\n            df[to_impute][i]=one\n        elif df[reference][i] < (var_min+step*2):\n                df[to_impute][i]=two\n                continue\n        elif df[reference][i] < (var_min+step*3):\n                df[to_impute][i]=three\n                continue\n        elif df[reference][i] < (var_min+step*4):\n                df[to_impute][i]=four\n                continue\n        elif df[reference][i] < (var_min+step*5):\n                df[to_impute][i]=five\n                continue\n        elif df[reference][i] < (var_min+step*6):\n                df[to_impute][i]=six\n                continue\n        elif df[reference][i] < (var_min+step*7):\n                df[to_impute][i]=seven\n                continue\n        elif df[reference][i] < (var_min+step*8):\n                df[to_impute][i]=eight\n                continue\n        elif df[reference][i] < (var_min+step**9):\n                df[to_impute][i]=nine\n                continue\n        else:\n            df[to_impute][i]=ten","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will apply the function to each pair of columns that correlate highly with each other. One column contains the missing entries to be imputed and the other will be used as reference. Instances where both columns are Nan will be simply filled with the mean of the whole column."},{"metadata":{"trusted":true},"cell_type":"code","source":"impute(df, 'GDP', 'Total expenditure')\nimpute(df, 'Total expenditure', 'GDP')\ndf['GDP'][df['GDP'].isna() == True] =df['GDP'].mean()\ndf['Total expenditure'][df['Total expenditure'].isna()==True]=df['Total expenditure'].mean()\n\nimpute(df, 'Alcohol', 'Schooling')\nimpute(df, 'Schooling', 'Alcohol')\ndf['Alcohol'][df['Alcohol'].isna() == True] =df['Alcohol'].mean()\ndf['Schooling'][df['Schooling'].isna()==True]=df['Schooling'].mean()\n\nimpute(df, 'Hepatitis B', 'Diphtheria ')\ndf['Hepatitis B'][df['Hepatitis B'].isna() == True] =df['Hepatitis B'].mean()\n\nimpute(df, 'Population', 'infant deaths')\ndf['Population'][df['Population'].isna() == True] =df['Population'].mean()\n\nimpute(df, 'Income composition of resources', 'Schooling')\ndf['Income composition of resources'][df['Income composition of resources'].isna() == True] =df['Income composition of resources'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#are there any missing values left?\ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#df.to_csv('imputed_data.csv')\ndf=pd.read_csv('imputed_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#one-hot encode categorical variables\ndummies=pd.get_dummies(df[['Country','Status']])\ndummies.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prepare final dataset, concatenate dummy columns and drop original categorical columns\ndf=pd.concat([df,dummies],axis=1)\ndf.drop(columns=['Country','Status'],inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Machine Learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import RANSACRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\n#prepare train and test data\nx=df.drop(columns='Life expectancy ').values\ny=df['Life expectancy '].values\nx_tr,x_ts,y_tr,y_ts=train_test_split(x,y,test_size=0.2, \n                                     random_state=42, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Linear Regression\nlinreg=LinearRegression()\nlinreg.fit(x_tr,y_tr)\npred=linreg.predict(x_ts)\nprint('mse:',mean_squared_error(y_ts,pred))\nprint('r2_score:',r2_score(y_ts,pred))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#comparison of correlation coefficients and linear regression coefficients, between predictors and target variable\nfeatures = pd.DataFrame(data=[], index=None, \n                          columns=['feature_name','correlation_with_target','lin_reg_coefficient'])\nfeatures['feature_name']=feat_names\nfeatures['correlation_with_target']=corr_coefs\nfeatures['lin_reg_coefficient']=np.round(linreg.coef_[0:19],decimals=3)\nfeatures","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The linear regression coefficients correspond with the feature correlations to a large degree, with occasional misalignments.\n\nLet's try a few more regression models"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#RANSAC regression\nransac=RANSACRegressor(LinearRegression(),max_trials=120,min_samples=50,\n                      loss='absolute_loss',residual_threshold=5.0,\n                       random_state=42)\nransac.fit(x_tr,y_tr)\npred=ransac.predict(x_ts)\nprint('mse:',mean_squared_error(y_ts,pred))\nprint('r2_score:',r2_score(y_ts,pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ridge Regression\nridge=Ridge(alpha=0.1)\nridge.fit(x_tr,y_tr)\npred=ridge.predict(x_ts)\nprint('mse:',mean_squared_error(y_ts,pred))\nprint('r2_score:',r2_score(y_ts,pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lasso Regression\nlasso=Lasso(alpha=0.1)\nlasso.fit(x_tr,y_tr)\npred=lasso.predict(x_ts)\nprint('mse:',mean_squared_error(y_ts,pred))\nprint('r2_score:',r2_score(y_ts,pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ElasticNet\nela=ElasticNet(alpha=0.1, l1_ratio=0.5)\nela.fit(x_tr,y_tr)\npred=ela.predict(x_ts)\nprint('mse:',mean_squared_error(y_ts,pred))\nprint('r2_score:',r2_score(y_ts,pred))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Random Forest Regressor\nrf = RandomForestRegressor(n_estimators=1000, criterion='mse', random_state=42)\nrf.fit(x_tr,y_tr)\ntr_pred=rf.predict(x_tr)\npred=rf.predict(x_ts)\nprint('mse train:',mean_squared_error(y_tr,tr_pred))\nprint('mse test:',mean_squared_error(y_ts,pred))\nprint('')\nprint('r2_score train:',r2_score(y_tr,tr_pred))\nprint('r2_score test:',r2_score(y_ts,pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's throw the feature importances, as identified by the Random Forest, and put in our dataframe to compare with correlations and linear regression coefficients.\n\nAgain, apart from occasional misalignments, there is great covariance among these three quantities"},{"metadata":{"trusted":true},"cell_type":"code","source":"features['Random Forest importances'] = np.round(rf.feature_importances_[0:19], decimals=3)\nfeatures","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}