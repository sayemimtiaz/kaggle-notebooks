{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport json\nfrom IPython.display import Image\nfrom IPython.core.display import HTML\nimport re\nfrom re import finditer\nimport nltk\nimport spacy\nfrom nltk.stem import PorterStemmer\nfrom collections import defaultdict\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Identifying Risk Factors associated with COVID-19\n#### In this task we want to identify unique risk factors associated with COVID-19 from a corpus of research papers. A challenging task here is to automatically consolidate findings from these papers in categories of risk factors. This notebook is built on the intuition that:\n* Paragraphs that discuss risk factors likely also mention the disease it is associated with. E.g. consider the extract below:\n\n![](https://i.ibb.co/Fzzg93Z/covid-pic2.png)\n\n* A simple heuristic here is to find sentences in paragraphs that contain one of the names of the COVID-19 disease along with one of the risk factors of interest (e.g. \"smoking\", \"pregnancy\"). Sentences - or small groups of sentences - that meet both these criteria can be considered valuable information to extract.\n\n* Once we have identified paragraphs or sentences that are relevant, we want to filter down and tag these by type of factors for ease of categorizing.\n---\n\n![](https://i.ibb.co/j5r1v0F/covid-pic4.png)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"base_path = \"/kaggle/input/CORD-19-research-challenge/\"\nsources = pd.read_csv(base_path + \"metadata.csv\",\n                     dtype={\"pubmed_id\":str,\n                           \"Microsoft Academic Paper ID\":str})\nsources.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Specify the sets of unigrams to look for.\n#### Separately specify the set of unigrams for Risk Factors (e.g. \"smoke\", \"pregnancy\") as well as the unigram variants of how authors discuss COVID-19 (e.g. \"COVID-19\", \"2019-nCoV\", \"SARS-CoV-2\")"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nset the global variable.\n\nsent_tokens : for each paper_id, keep a cache of the sentence and word tokenized and stemmed paragraphs.\nsent_fulls  : for each paper_id, keep a cache of the sentence but NOT word tokenized paragraphs.\n\njdict       : for each paragraph segment found to contain valuable data, store the paper_id and desired segment of text.\n'''\n\nsent_tokens = defaultdict(lambda: defaultdict(lambda: \"\")) # for each document, cache the tokenized sentences for easy revisits\nsent_fulls = defaultdict(lambda: defaultdict(lambda: \"\"))\n\n\n\nvalid_ids = set(sources[sources[\"has_full_text\"]==True][\"sha\"].unique().tolist())\n\nstemmer = PorterStemmer()\n\ntargs = [set({\"smoke\", \"pulmonary\", \"pre-existing\", \"neo-natal\", \"natal\", \"pregnancy\", \"pregnant\", \"economic\", \"social\", \"socio-economic\"}),\n         set({\"covid-19\", \"covid19\", \"sars-cov-2\", \"2019-ncov\"})]\n\nreverse_map = {}\n# convert our target terms into their stemmed versions for compatibility in the matching stage\nfor i in range(0, len(targs)):\n    newterms = set()\n    for ele in targs[i]:\n        st = stemmer.stem(ele)\n        newterms.add(st)\n        reverse_map[st] = ele\n    targs[i] = newterms","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Process each paragraph in the JSON file."},{"metadata":{"trusted":true},"cell_type":"code","source":"\n'''\nvalidate_segment() is our main function. Each paragraph in each JSON file is passed to this function.\nWe process and tokenize the text and then look for sentences that mention the desired terms.\nSince we are also interested in numeric data, we specifically put a filter to only include segments that have numeric values in them.\n'''\n\ndef validate_segment(segment, paper_id=None, cnt=None):\n    global targs\n    '''\n    so the thinking here is that a paragraph that mentions a risk factor related to COVID-19 will mention both in short word-order proximity.\n    Particularly, we will work with the hypothesis that the mention of the risk factor and mention of COVID will be no greater than 2\n    sentences apart.\n    These are the sentences we will keep.\n    '''\n    \n    # quick heuristic to get rid of paragraphs that don't even discuss COVID-19 (or SARS-CoV-2)\n    if not \"19\" in segment and not \"-2\" in segment:\n        return False, \"\", set()\n        \n    # first convert this into tokens\n    jtxt = None\n    # check if we have already cached the tokenized paragraph.\n    # if so, just pick it up and move on.\n    if paper_id:\n        if paper_id in sent_tokens:\n            if cnt in sent_tokens[paper_id]:\n                jtxt = sent_tokens[paper_id][cnt]\n                jtxt_base = sent_fulls[paper_id][cnt]\n            \n            \n    # if this particular paragraph has not already been cached,\n    # perform sentence and word tokenization as well as stemming.\n    # then cache it for much faster subsequent processing.\n    if jtxt is None:\n        jtxt_base = nltk.sent_tokenize(segment)\n        jtxt = [[stemmer.stem(y.lower()) for y in nltk.word_tokenize(x)] for x in jtxt_base]\n        if not paper_id is None:\n            sent_tokens[paper_id][cnt] = jtxt\n            sent_fulls[paper_id][cnt] = jtxt_base\n\n    \n    \n    # for each sentence, determine if the two categories of targets have been matched. If not, try checking the preceding\n    # and succeeding sentence.\n    \n    sent_founds = []\n    sent_numerics = []\n    for i in range(0, len(jtxt)):\n        # for each sentence, check if at least one number is mentioned (stats)\n        \n        # don't count citations as numeric values (e.g. \"according to [12,13] etc.\")\n        no_bracks = re.sub(r\"\\[\\s*\\d+((\\s*\\,\\s*\\d+)+)?\\]\", \"\", jtxt_base[i])\n        matchers = re.search(r\"[^A-Z-a-z0-9\\-](\\d+)[^A-Z-a-z0-9\\-]\", no_bracks)\n        is_numeric = False\n        if matchers:\n            # as a simple heuristic, we ignore values that might be years.\n            # highly unlikely these values will be less than 1900 or greater than 2020.\n            if int(matchers.group(1)) < 1900 or int(matchers.group(1)) > 2020:\n                is_numeric = True\n                    \n        # for each sentence, check if any of the words are target words\n        tempy = set()\n        for k in range(0, len(jtxt[i])):\n            word = jtxt[i][k]                    \n            # check for match\n            for q in range(0, len(targs)):\n                if word in targs[q]:\n                    tempy.add(q)\n        sent_numerics.append(is_numeric)\n        sent_founds.append(tempy)\n    \n    \n    # we now thave the list of found words. now let's run the heuristic.\n    # for each sentence, we check if all terms were located. If not, then we check if the missing terms were in either the preceding\n    # of following sentence.\n    val_sent = None\n    val_tags = None\n    tagset = set()\n    for i in range(0, len(sent_founds)):\n        if len(sent_founds[i])==len(targs):\n            if sent_numerics[i]:\n                val_sent = jtxt_base[i]\n                val_tags = jtxt[i]\n                break\n        \n        # at least one target is missing. check the neighbors\n        is_numeric = sent_numerics[i]\n        tempset = sent_founds[i].copy()\n        if i > 0:\n            tempset.update(sent_founds[i-1])\n            is_numeric = True if sent_numerics[i] or sent_numerics[i-1] else False\n            if len(tempset)==len(targs) and is_numeric:\n                val_sent = jtxt_base[i-1] + \" \" + jtxt_base[i]\n                val_tags = jtxt[i] + (jtxt[i-1])\n                break\n                \n        is_numeric = sent_numerics[i]\n        tempset = sent_founds[i].copy()\n        if i < (len(sent_founds) - 1):\n            tempset.update(sent_founds[i+1])\n            is_numeric = True if sent_numerics[i] or sent_numerics[i+1] else False\n            if len(tempset)==len(targs) and is_numeric:\n                val_sent = jtxt_base[i] + \" \" + jtxt_base[i+1]\n                val_tags = jtxt[i] + (jtxt[i+1])\n                break          \n    \n    if not val_sent:\n        return False, \"\", set()\n    \n    # find the set of tags that were matches\n    matchset = set()\n    vbase = val_tags\n    val_tags = set(val_tags)\n    for q in range(0, len(targs)-1):\n        matchset = matchset.union(targs[q])\n    val_tags = val_tags.intersection(matchset)\n    \n    return True, val_sent, val_tags","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Find the Basic Reproduction Number (R_0)\n- Heuristic: look for string \"basic reproduction number followed closely by a number that is \"reasonable\": above 0 but below 12.\n- This heuristic helps avoid cases of mistakenly identifying citation numbers that appear near mentions of R_0 as the actual values."},{"metadata":{"trusted":true},"cell_type":"code","source":"def is_number(s):\n    try:\n        float(s)\n        return True\n    except ValueError:\n        return False\n\n\nBRN = defaultdict(lambda: [])\n\n\n'''\ngiven the raw paragraph text, see if there is mention of the basic reproduction number and what that number is.\n'''\ndef find_reproduction_number(segment, paper_id=None, cnt=None, paper_title=None):\n    # quick heuristic to get rid of paragraphs that don't even discuss COVID-19 (or SARS-CoV-2)\n    if not \"19\" in segment and not \"-2\" in segment:\n        return False, \"\", set()\n        \n    # first convert this into tokens\n    jtxt = None\n    # check if we have already cached the tokenized paragraph.\n    # if so, just pick it up and move on.\n    if paper_id:\n        if paper_id in sent_tokens:\n            if cnt in sent_tokens[paper_id]:\n                jtxt = sent_tokens[paper_id][cnt]\n                jtxt_base = sent_fulls[paper_id][cnt]\n            \n            \n    # if this particular paragraph has not already been cached,\n    # perform sentence and word tokenization as well as stemming.\n    # then cache it for much faster subsequent processing.\n    if jtxt is None:\n        jtxt_base = nltk.sent_tokenize(segment)\n        jtxt = [[stemmer.stem(y.lower()) for y in nltk.word_tokenize(x)] for x in jtxt_base]\n        if not paper_id is None:\n            sent_tokens[paper_id][cnt] = jtxt\n            sent_fulls[paper_id][cnt] = jtxt_base    \n            \n    # check if this paragraph is talking about COVID-19\n    tempy = set()\n    is_relevant = False\n    for i in range(0, len(jtxt)):\n        for k in range(0, len(jtxt[i])):\n            word = jtxt[i][k]                    \n            # check for match\n            if word in targs[-1]:\n                is_relevant = True\n                break\n    if not is_relevant:\n        return\n            \n    \n    # for each sentence in the unsplit paragraph, check if there is mention of the basic reproduction number\n    # also check that it is still in the context of covid-19.\n    \n    \n    # format the paragraph text to be alphanumeric, with at most one consecuteve space\n    # and entirely lower case.\n    # this makes it easier to search for strings.\n    brn_stem = [stemmer.stem(x) for x in \"basic reproduction number\".split(\" \")]\n    for q,sent in enumerate(jtxt):\n        for i in range(2, len(sent)):\n            if sent[i]==brn_stem[-1] and sent[i-1]==brn_stem[-2] and sent[i-2]==brn_stem[-3]:\n                # we found a match. is there a number that follows?\n                for k in range(0, 3):\n                    if (i + k) >= len(sent):\n                        break\n                    if is_number(sent[i + k]):\n                        if float(sent[i+k])==0:\n                            continue\n                        if float(sent[i+k]) > 12:\n                            # highly unlikely and is likely a broken reference bracket\n                            continue\n                        # we found a numeric match as well.\n                        BRN[paper_id].append({\"sentence\": jtxt_base[q], \"value\": float(sent[i+k]),\n                                             \"paper_id\": paper_id, \"title\":paper_title})\n                        \n    #heuristics = [r\"basic reproduction number (?:(?:\\w+\\s+){1,3})(\\d+(?:\\.\\d+)?)\"]\n    \n#     for sent in jtxt_base:\n#         seglow = sent.lower()\n#         seglow = re.sub(r\"[^A-Za-z0-9 ]\", \"\", seglow)\n#         seglow = re.sub(r\" {2,}\", \" \", seglow)\n        \n#         all_matches = [m.start(0) for m in re.finditer(target, seglow)]\n#         if len(all_matches) > 0:\n#             BRN.append({\"sentence\": sent, \"\"})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# For each JSON file, send body text paragraph to validate_segment()"},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_file(jstruct):\n    if \"paper_id\" in jstruct:\n        if jstruct[\"paper_id\"] in valid_ids:\n            # consolidate the document text and see if there's a match.\n            jbod = jstruct[\"body_text\"]\n            temp = defaultdict(lambda x: \"\")\n                        \n            #.... for now, let's keep things simple. We assume that if risk factors are mentioned, it will be at the paragraph level\n            \n            # loop through each paragraph\n            for cnt, x in enumerate(jbod):\n                is_valid, val_seg, val_tags = validate_segment(x[\"text\"], jstruct[\"paper_id\"], cnt)\n                find_reproduction_number(x[\"text\"], jstruct[\"paper_id\"], cnt, jstruct[\"metadata\"][\"title\"])\n                if is_valid:\n                    #print (\"found match: {} : {} => {}\".format(jstruct[\"paper_id\"], val_seg, val_tags))\n                    jdict[jstruct[\"paper_id\"]].append({\"text\":x[\"text\"], \"tags\":val_tags, \"segment\":val_seg, \"paper_id\":jstruct[\"paper_id\"], \"title\":jstruct[\"metadata\"][\"title\"]})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loop through each JSON file. Send it to process_file()"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# jdict will hold the tokenized sentences for each paragraph in each document\njdict = defaultdict(lambda:[])\n\n# BRN holds the list of sentences that mention a COVID-19 basic reproduction number \nBRN = defaultdict(lambda: [])\n\n\n\ncounter = 0\nfile_list = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        if filename[-5:]==\".json\":\n            file_list.append(os.path.join(dirname, filename))\n\nfile_list.sort()\ntotal_files = len(file_list)\n\nuseds = set()\nfor file in file_list:\n    #if counter > 1000:\n    #    break\n    process_file(json.load(open(file, \"r\")))\n    counter += 1\n    perc_complete = round((counter/total_files)*100)\n    if perc_complete%5==0:\n        if perc_complete in useds:\n            continue\n        useds.add(perc_complete)\n        print (\"{} / {} => {}% complete\".format(counter, total_files, perc_complete))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results completed. Display the findings.\n- For each of the Risk Factor tags, compile all the segments we found.\n- Color-code the key terms for easier understanding"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_brns = set()\nfor paper_id, found_objs in BRN.items():\n    for entry in found_objs:\n        all_brns.add(entry[\"value\"])\nall_brns = sorted(all_brns)\n\nbrn_objs = []\nfor paper_id, found_objs in BRN.items():\n    for entry in found_objs:\n        brn_objs.append({\"value\":entry[\"value\"], \"sentence\": entry[\"sentence\"], \"title\": entry[\"title\"]})\n\nbrn_sort = sorted(brn_objs, key=lambda x: x[\"value\"], reverse=False)\n\nhtmlstr = \"<span style='font-weight:bold;font-size:18px;'>Basic Reproduction Numbers (R<sub>0</sub>)</span><br />\"\nhtmlstr += \"<span style='font-size:16px;'>\"\nhtmlstr += \"<span style='font-weight:normal;padding-right:20px;'>Average R<sub>0</sub>: </span><span>{}</span><br />\".format(round(np.mean(all_brns),3))\nhtmlstr += \"<span style='font-weight:normal;padding-right:20px;'>Median R<sub>0</sub>: </span><span>{}</span><br />\".format(round(np.median(all_brns),3))\nhtmlstr += \"<span style='font-weight:normal;padding-right:20px;'>Minimum R<sub>0</sub>: </span><span>{}</span><br />\".format(min(all_brns))\nhtmlstr += \"<span style='font-weight:normal;padding-right:20px;'>Maximum R<sub>0</sub>: </span><span>{}</span><br />\".format(max(all_brns))\nhtmlstr += \"<br /><span style='font-size:18px;font-weight:bold;'>All R<sub>0</sub> values found</span><br />\"\nhtmlstr += \"<span>{}</span>\".format(\",  \".join(map(str, all_brns))) + \"<br />\"\nhtmlstr += \"<br /><span style='font-size:18px;font-weight:bold;'>References</span><br />\"\n\ntempstr = \"<br /><div style='display:table;'>\"\ntempstr += \"<div style='display:table-row;'>\\\n<div style='display:table-cell;'>&nbsp;</div>\\\n<div style='display:table-cell;font-weight:bold;font-size:18px;padding-bottom:10px;'>R<sub>0</sub></div>\\\n<div style='display:table-cell;min-width:50px;'>&nbsp;</div>\\\n<div style='display:table-cell;font-weight:bold;font-size:18px;'>Extract</sub></div>\\\n</div>\"\nfor result in brn_sort:\n    tempstr += \"<div style='display:table-row;'>\"\n    tempstr += \"<div style='display:table-cell;padding-right:30px;font-size:20px;'>•</div>\"\n    tempstr += \"<div style='display:table-cell;'>{}</div>\".format(result[\"value\"])\n    tempstr += \"<div style='display:table-cell;'></div>\"    \n    tempstr += \"<div style='display:table-cell;'>{}<span style='color:#0099cc'>[{}]</span></div>\".format(result[\"sentence\"], result[\"title\"])\n    tempstr += \"</div>\"\n    \ntempstr += \"</div>\"\nhtmlstr += tempstr\n\nhtmlstr += \"</span>\"\ndisplay(HTML(htmlstr))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntopics = defaultdict(lambda: {\"text\":[], \"title\":[], \"rawtag\":\"\"})\n\nfor paper_id, found_objs in jdict.items():\n    \n    for ele in found_objs:\n        \n        # for each tag (usually only one) see which topic this falls under\n        for tag in ele[\"tags\"]:\n            topics[reverse_map[tag]][\"text\"].append(ele[\"segment\"])\n            topics[reverse_map[tag]][\"title\"].append(ele[\"title\"])\n            topics[reverse_map[tag]][\"rawtag\"] = tag\n            \nhtmls = defaultdict(lambda: \"\")\nfor topic_name in topics:\n    htmlstr = \"<div class='test_output'>\"\n    htmlstr += \"<br /><div style='font-weight:bold;'>{}</div><br />\".format(topic_name)\n    htmlstr += \"<div style='display:table;'>\"\n    \n    for q, entry in enumerate(topics[topic_name][\"text\"]):\n        splinter = nltk.word_tokenize(entry)\n        \n        for i in range(0, len(splinter)):\n            if stemmer.stem(splinter[i])==topics[topic_name][\"rawtag\"]:\n                splinter[i] = \"<span style='background-color:#FFCC33;'>\" + splinter[i] + \"</span>\"\n            elif stemmer.stem(splinter[i]) in targs[-1]:\n                splinter[i] = \"<span style='background-color:#FF99FF;'>\" + splinter[i] + \"</span>\"\n                \n        formatted = \" \".join(splinter) + \"<span style='color:#0099cc;'> [\" + topics[topic_name][\"title\"][q] + \"]</span>\"\n        htmlstr += \"<div style='display:table-row;'>\"\n        htmlstr += \"<div style='display:table-cell;padding-right:15px;font-size:20px;'>•</div><div style='display:table-cell;'>\" + formatted + \"</div>\"\n        htmlstr += \"</div>\"\n        \n    htmlstr += \"</div>\"\n    htmlstr += \"</div>\"\n    htmls[topic_name] = htmlstr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(HTML(htmls[\"social\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(HTML(htmls[\"smoke\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(HTML(htmls[\"pregnant\"]))\ndisplay(HTML(htmls[\"pregnancy\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(HTML(htmls[\"pre-existing\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(HTML(htmls[\"economic\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(HTML(htmls[\"neo-natal\"]))\ndisplay(HTML(htmls[\"natal\"]))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}