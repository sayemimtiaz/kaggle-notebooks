{"cells":[{"metadata":{},"cell_type":"markdown","source":"<center><h1>Recommendations at Scale !! </h1>\n<img src=\"https://i.gifer.com/DTtv.gif\" width=\"800\">\n</center>\n<br><br>\n\n* [Collaborative Filtering](#section-one)\n* [Paradox of Choice](#section-two)\n* [Pre-Processing](#section-four)\n    - [Candidate Generation](#section-four-one)\n    - [Gaussian Normalization](#section-four-two)\n* [Machine Learning and Matrix Factorization Models](#section-five)\n    - [Machine Learning based Model](#section-five-one)\n    - [Matrix Factorization](#section-five-two)\n* [Recommendation Evaluation](#section-six)\n* [Scoring](#section-seven)\n* [Neural Net Recommendation](#section-eight)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n# Collaborative Filtering  ðŸ‘¥\n\n**To address some of the limitations of content-based filtering, It relies on the concept that similar users has similar taste or choices of reading  similar books.** \n\n> ðŸ“Œ **If a user named *Akhil* liked certain genre of books, authors and etc.. and a similar user *Ram* also like the same genre of books, authors.. then both *Akhil* and *Ram* are categorized into similar users. If *Akhil* reads a book and if he likes it then the same book will be recommended to *Ram***\n\n<img src=\"https://socital.com/wp-content/uploads/2019/09/Collaborative-filtering.jpg\" width=\"400\">"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-two\"></a>\n# Paradox of Choice\n\n### Can One Desire Too Much of a Good Thing?\n\nRecommedations have to be few out of a large corpora of dataset. And common architecture for recommendation systems consists of the following components:\n* candidate generation\n* scoring\n* re-ranking\n<center><img src=\"https://i0.wp.com/doist.com/blog/wp-content/uploads/sites/3/2015/07/paradox-of-choice.jpg?quality=85&strip=all&ssl=1\" width=\"400\"></center>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-four\"></a>\n\n## Importing Necessary Libraries\n\n#### Here I am trying to use SURPRISE library which is like scikit library for recommendation algorithms\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from surprise.model_selection import train_test_split\nfrom surprise import Reader\nfrom surprise import Dataset\nfrom surprise.model_selection import cross_validate\nfrom surprise import NormalPredictor,KNNBasic,KNNWithMeans,KNNWithZScore,KNNBaseline,SVD,BaselineOnly,SVDpp,NMF,SlopeOne,CoClustering\nfrom surprise.accuracy import rmse\nfrom surprise import accuracy\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset Loading.."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"users = pd.read_csv('../input/bookcrossing-dataset/Book reviews/BX-Users.csv', sep='\\\";\\\"', names=['User-ID', 'Location', 'Age'], encoding='latin-1', skiprows=1)\nbooks = pd.read_csv('../input/bookcrossing-dataset/Book reviews/BX-Books.csv', sep='\\\";\\\"', names=['ISBN', 'Book-Title' ,'Book-Author','Year-Of-Publication', 'Publisher', 'Image-Url-S', 'Image-Url-M', 'Image-Url-L'], encoding='latin-1', skiprows=1)\nratings = pd.read_csv('../input/bookcrossing-dataset/Book reviews/BX-Book-Ratings.csv', sep='\\\";\\\"', names=['User-ID', 'ISBN', 'Book-Rating'], encoding='latin-1', skiprows=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning\n\n* Replacing NULL values\n* Removing Unnecessary characters"},{"metadata":{"trusted":true},"cell_type":"code","source":"users['User-ID'] = users['User-ID'].str.replace(\"\\\"\",\"\")\nusers['Location'] = users['Location'].str.replace(\"\\\";NULL\",\"\")\nusers['Age'] = users['Age'].fillna(\"0\")\nusers['Age'] = users['Age'].str.replace(\"\\\"\",\"\")\nbooks['ISBN'] = books['ISBN'].str.replace(\"\\\"\",\"\")\nbooks['Book-Title'] = books['Book-Title'].str.replace(\"\\\"\",\"\")\nratings['User-ID'] = ratings['User-ID'].str.replace(\"\\\"\",\"\")\nratings['Book-Rating'] = ratings['Book-Rating'].str.replace(\"\\\"\",\"\").astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-four-one\"></a>\n\n## Candidate Generation\n\n**This is the first stage of the Recommender Systems. Not all books and users are taken as quality books and users. There will be few stringent and lenient users.**\n\nStringent Users: They are insensitive towards ratings, they won't rate higher ratings and mostly give medium ratings for books\n\nLenient Users: They are very sensitive towards ratings, they will rate higher ratings as 9, 10 always for most of the books\n\n### Normalization of users ratings is required"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Quality books having atleast 5 reviews\n\nquality_ratings = ratings[ratings['Book-Rating']!=0]\nquality_book = quality_ratings['ISBN'].value_counts().rename_axis('ISBN').reset_index(name = 'Count')\nquality_book = quality_book[quality_book['Count']>5]['ISBN'].to_list()\nquality_ratings = quality_ratings[quality_ratings['ISBN'].isin(quality_book)]\nquality_ratings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Quality Users making atleast 5 reviews\n\nquality_user = quality_ratings['User-ID'].value_counts().rename_axis('User-ID').reset_index(name = 'Count')\nquality_user = quality_user[quality_user['Count']>5]['User-ID'].to_list()\nquality_ratings = quality_ratings[quality_ratings['User-ID'].isin(quality_user)]\nquality_ratings","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-four-two\"></a>\n## Gaussian Normalization\n\n* All ratings are normalized as gaussian distribution \n* Gaussian Ratings are scaled on (0-5) Rating scale\n\n\n\n\\begin{equation*}\nR_{norm}^{u_i}(b) = \\frac{R_b - R_{mean}^{u_i}}{\\sqrt{\\sum_{j} (R_{b_j} - R_{mean}^{u_i})^2}}\n\\end{equation*}"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Normalizing the Ratings\n\nmean_rating_user = quality_ratings.groupby('User-ID')['Book-Rating'].mean().reset_index(name='Mean-Rating-User')\nmean_data = pd.merge(quality_ratings, mean_rating_user, on='User-ID')\nmean_data['Diff'] = mean_data['Book-Rating'] - mean_data['Mean-Rating-User']\nmean_data['Square'] = (mean_data['Diff'])**2\nnorm_data = mean_data.groupby('User-ID')['Square'].sum().reset_index(name='Mean-Square')\nnorm_data['Root-Mean-Square'] = np.sqrt(norm_data['Mean-Square'])\nmean_data = pd.merge(norm_data, mean_data, on='User-ID')\nmean_data['Norm-Rating'] = mean_data['Diff']/(mean_data['Root-Mean-Square'])  \nmean_data['Norm-Rating'] = mean_data['Norm-Rating'].fillna(0)\nmax_rating = mean_data.sort_values('Norm-Rating')['Norm-Rating'].to_list()[-1]\nmin_rating = mean_data.sort_values('Norm-Rating')['Norm-Rating'].to_list()[0]\nmean_data['Norm-Rating'] = 5*(mean_data['Norm-Rating'] - min_rating)/(max_rating-min_rating)\nmean_data['Norm-Rating'] = np.ceil(mean_data['Norm-Rating']).astype(int)\nnorm_ratings = mean_data[['User-ID','ISBN','Norm-Rating']]\nmean_data.sort_values('Norm-Rating')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\nreader = Reader(rating_scale=(0, 5))\ndata = Dataset.load_from_df(norm_ratings[['User-ID', 'ISBN', 'Norm-Rating']], reader)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-five\"></a>\n# Machine Learning and Matrix Factorization Models \n\n\nPerforming Cross validation and checking RMSE of all Machine Learning and Matrix Factorization algorithms available in surprise library\n"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"benchmark = []\nfor algorithm in [SVD(), \n                  SVDpp(), \n                  SlopeOne(), \n                  NMF(), \n                  NormalPredictor(), \n                  KNNBaseline(), \n                  KNNBasic(), \n                  KNNWithMeans(),\n                  KNNWithZScore(), \n                  BaselineOnly(),\n                  CoClustering()]:\n    \n    results = cross_validate(algorithm, data, measures=['RMSE'], cv=3, verbose=False)\n    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n    tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n    benchmark.append(tmp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can observe that Baseline ML algorithm and SVD based Matrix Factorization has last RMSE.\n\n> ðŸ“Œ  0.62 of RMSE says that predicted rating may have an error of 0.62"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"surprise_results = pd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse')\nsurprise_results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-five-one\"></a>\n# Machine Learning based Model\n\n### BaselineOnly\n\nAlgorithm predicting the baseline estimate for given user and item.\n\n\\begin{equation*}\nb_{ui}=Î¼+b_u+b_i\n\\end{equation*}\n\nIf user u is unknown, then the bias b<sub>u</sub> is assumed to be zero. The same applies for item i with b<sub>i</sub>.\n\nusing SGD: Stocahstic Gradient Descent to minimize the loss with regularization parameter 0.5"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Baseline\n\ntrain_set, test_set = train_test_split(data, test_size=0.25)\nalgo = BaselineOnly(bsl_options={'method': 'sgd','learning_rate': .00005, 'n_epochs':30, 'reg':0.5})\nfit = algo.fit(train_set)\npred = fit.test(test_set)\naccuracy.rmse(pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-five-two\"></a>\n# Matrix Factorization Method\n\n### SVD\n\nThe famous SVD algorithm, as popularized by Simon Funk during the Netflix Prize. When baselines are not used, This is equivalent to Probabilistic Matrix.\n\nThe prediction r<sup>ui</sup> is set as:\n\\begin{equation*}\nr^{ui}=Î¼+b_u+b_i+q_i^Tp_u\n\\end{equation*}\n\n\nTo estimate all the unknown, we minimize the following regularized squared error:\n\n\\begin{equation*}\n\\sum_{r_{ui}âˆˆR_{train}}(r_{ui}âˆ’r^{ui})^2+Î»(b^2_i+b^2_u+||q_i||^2+||p_u||^2)\n\\end{equation*}\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# SVD \n\nalgo = SVD(reg_bi = 0.5, lr_bi=0.005)\nfit = algo.fit(train_set)\npred = fit.test(test_set)\naccuracy.rmse(pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"recommend = algo.trainset\nusers_norm = list(set(norm_ratings['User-ID'].to_list()))\nbooks_norm = list(set(norm_ratings['ISBN'].to_list()))\nnorm_ratings['User-ID'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pred_users = [user for user in users_norm if recommend.knows_user(recommend.to_inner_uid(user))]\npred_books = []\nfor book in books_norm:\n    try:\n        if recommend.knows_item(recommend.to_inner_iid(book)):\n            pred_books.append(book)\n    except:\n        pass\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"pred_users[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-six\"></a>\n# Recommendation Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def recommend_books(user_id, count):\n    result=[]\n    for b in pred_books:\n        result.append([b,algo.predict(user_id,b,r_ui=4).est])\n    recom = pd.DataFrame(result, columns=['ISBN','Rating'])\n    merge = pd.merge(recom,books, on='ISBN' )\n    return merge.sort_values('Rating', ascending=False).head(count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recommendation = recommend_books('36938', 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-seven\"></a>\n# Scoring \n\nAfter candidate generation, another model scores and ranks the generated candidates to select the set of items to display. The recommendation system may have multiple candidate generators that use different sources, such as the following:\n\n* User features that account for personalization.\n* geographic information into account.\n* Popular or trending items.\n\n\nHere scoring is done based on published year"},{"metadata":{"trusted":true},"cell_type":"code","source":"scoring = recommendation.sort_values('Year-Of-Publication')\nview = \"\".join([\"<span><img src='\"+a+\"'></span>\" for a in scoring['Image-Url-M'].to_list()])\nscoring[['Book-Title']]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"view","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<center><h1>My Top 5 Recommendations</h1></center>\n<span><img src='http://images.amazon.com/images/P/0446310786.01.MZZZZZZZ.jpg'></span><span><img src='http://images.amazon.com/images/P/059035342X.01.MZZZZZZZ.jpg'></span><span><img src='http://images.amazon.com/images/P/0316666343.01.MZZZZZZZ.jpg'></span><span><img src='http://images.amazon.com/images/P/0385504209.01.MZZZZZZZ.jpg'></span><span><img src='http://images.amazon.com/images/P/0142001740.01.MZZZZZZZ.jpg'></span>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-eight\"></a>\n# Neural Net Model\n\nWill be updated !!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}