{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predicting flight delay\nKarthik Sunil, Aug 2019\n\n-------\nIn this notebook, I develop a model aimed at predicting flight delays at take-off. I show how to build linear and polynomial models for univariate or multivariate regressions and also, I give some insight on the reason why regularisation helps us in developing models that generalize well.\n\nThe entire notebook is divided into 3 Major sections. \n1. Understanding and Cleaning Data\n2. Re-sampling of the data\n3. Exploratory Data Analysis\n4. Building various model to detect the delays\n\nIn the begenning let us import all the libraries we need for this analysis purpose"},{"metadata":{"_uuid":"f67230a81f9466b6c8d2c5d603a8e8615c2e99a7","_cell_guid":"42d80b40-c268-44ae-be4c-1b0660b8a0c6","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import datetime, warnings, scipy \nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib.patches import ConnectionPatch\nfrom collections import OrderedDict\nfrom matplotlib.gridspec import GridSpec\nfrom mpl_toolkits.basemap import Basemap\nfrom sklearn import metrics, linear_model\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\nfrom scipy.optimize import curve_fit\nplt.rcParams[\"patch.force_edgecolor\"] = True\nplt.style.use('fivethirtyeight')\nmpl.rc('patch', edgecolor = 'dimgray', linewidth=1)\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"last_expr\"\npd.options.display.max_columns = 50\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Loading the data \nIn this dataset, we have 3 input files and 1 kernel output file. You can see the details in the workspace to the right\n\nInput files:\n\n- **flights.csv**: Contains entire dataset of the flights data for 2015. Contains around 58 million data records\n- **aiports.csv**: Contains the lookup table for Airports\n- **airlines.csv**: Contains the lookup table for Airlines\n\nKernel output file:\n- **flights_date**: Contains pre-processed output file that can be readily used for data analysis. It contains the data after the clearning of date format and added lookup data to flights data. This avoids to perform costly operations again and again. However, in this kernel the cells which cleans up the data has been commented, but kept for future purposes. "},{"metadata":{"_uuid":"43d6714ff1fdfe7465f9ad69f5fdc8754cc88f82","_cell_guid":"6f858b64-000b-4b5e-a2a0-c827afb59bd3","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"airports = pd.read_csv(\"../input/flight-delays/airports.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e16a4dda060923da07c99eca1d0fc941c3ddd46","scrolled":true,"_cell_guid":"f1181648-1d4e-434c-b9b6-58240e4f0787","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/flight-delays/flights.csv', low_memory=False)\nprint('Dataframe dimensions:', df.shape)\n#____________________________________________________________\n# gives some infos on columns types and number of null values\ntab_info=pd.DataFrame(df.dtypes).T.rename(index={0:'column type'})\ntab_info=tab_info.append(pd.DataFrame(df.isnull().sum()).T.rename(index={0:'null values (nb)'}))\ntab_info=tab_info.append(pd.DataFrame(df.isnull().sum()/df.shape[0]*100)\n                         .T.rename(index={0:'null values (%)'}))\ntab_info","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2226b359e705a8396004cd4d5fe0b0a6dc062189","_cell_guid":"d7ee2a3f-f4c5-4dff-bfc0-46c9a7d1e9d3"},"cell_type":"markdown","source":"___\n## 1. Data Acquaintance and Cleaning\n___\n### 1.1 Dates and times\n\nIn the initial dataframe, dates are coded according to 4 variables: **YEAR, MONTH, DAY**, and **DAY_OF_WEEK**. In fact, python offers the **_datetime_** format which is really convenient to work with dates and times and I thus convert the dates in this format:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#!ls ../input/*","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**NOTE**\nThe cleaning for date is already done and stored in another dataset called flights_date.csv. I suggest to jump to the cell with heading *Load flight_date.csv* . The file path. [Link to cell](#load_file)\n\n```\n../input/analysis-of-flight-delay/flights_date.csv\n```\n"},{"metadata":{"_uuid":"8652a730699b241163bdbe13396004e442f92baf","_cell_guid":"d6861e13-aa9a-4790-b768-40c54717c62f","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df['DATE'] = pd.to_datetime(df[['YEAR','MONTH', 'DAY']])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2497f787a027f453cae5b6434542fc3ad2a9ff0d","_cell_guid":"9cccabe5-ae57-4814-b11a-6437b521e516"},"cell_type":"markdown","source":"Moreover, in the **SCHEDULED_DEPARTURE** variable, the hour of the take-off is coded as a float where the two first digits indicate the hour and the two last, the minutes. This format is not convenient and I thus convert it. Finally, I merge the take-off hour with the flight date. To proceed with these transformations, I define a few functions:"},{"metadata":{"_uuid":"d84175d06e4bd797bd125ce71f4629573d1d8092","_cell_guid":"fe712458-abe5-4f40-a100-80dcddd69ad6","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#_________________________________________________________\n# Function that convert the 'HHMM' string to datetime.time\ndef format_heure(chaine):\n    if pd.isnull(chaine):\n        return np.nan\n    else:\n        if chaine == 2400: chaine = 0\n        chaine = \"{0:04d}\".format(int(chaine))\n        heure = datetime.time(int(chaine[0:2]), int(chaine[2:4]))\n        return heure\n#_____________________________________________________________________\n# Function that combines a date and time to produce a datetime.datetime\ndef combine_date_heure(x):\n    if pd.isnull(x[0]) or pd.isnull(x[1]):\n        return np.nan\n    else:\n        return datetime.datetime.combine(x[0],x[1])\n#_______________________________________________________________________________\n# Function that combine two columns of the dataframe to create a datetime format\ndef create_flight_time(df, col):    \n    liste = []\n    for index, cols in df[['DATE', col]].iterrows():    \n        if pd.isnull(cols[1]):\n            liste.append(np.nan)\n        elif float(cols[1]) == 2400:\n            cols[0] += datetime.timedelta(days=1)\n            cols[1] = datetime.time(0,0)\n            liste.append(combine_date_heure(cols))\n        else:\n            cols[1] = format_heure(cols[1])\n            liste.append(combine_date_heure(cols))\n    return pd.Series(liste)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0ca2fe6b79c14a9b4ff90e7655bdf7cc6354c76","_cell_guid":"a1b9f76b-ca73-4f30-a3b9-649dadf5decd"},"cell_type":"markdown","source":"and I call them to modify the dataframe variables:\n\n*** \nNOTE: Following code is very expensive and takes a lot of time. Please uncomment and run if you need. Or else you can load already processed file\n\n`../input/flights-delay-date/flights_date.csv`\n"},{"metadata":{"_uuid":"b7ecec66f3d495e2e50b82e61d5dc550b8179b6a","_cell_guid":"356ec2ff-3fe5-4ab9-8f4d-235fec6ffb63","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# df['SCHEDULED_DEPARTURE'] = create_flight_time(df, 'SCHEDULED_DEPARTURE')\n# df['DEPARTURE_TIME'] = df['DEPARTURE_TIME'].apply(format_heure)\n# df['SCHEDULED_ARRIVAL'] = df['SCHEDULED_ARRIVAL'].apply(format_heure)\n# df['ARRIVAL_TIME'] = df['ARRIVAL_TIME'].apply(format_heure)\n# #__________________________________________________________________________\n# df.loc[:5, ['SCHEDULED_DEPARTURE', 'SCHEDULED_ARRIVAL', 'DEPARTURE_TIME',\n#              'ARRIVAL_TIME', 'DEPARTURE_DELAY', 'ARRIVAL_DELAY']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**NOTE**\nThe above cell is very Costly Operation, hence saving the DF to a file, so that the prrocessed data can be loaded \n\nPlease run the following code to save the file to local and upload back to kernel for future usage if needed. As of now the file is processed and loaded\n\n`../input/flights-delay-date/flights_date.csv`\n\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\n# df.to_csv('./flights_date.csv')\n\n# from IPython.display import FileLink\n# FileLink(r'./flights_date.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### End of Data Processing"},{"metadata":{},"cell_type":"markdown","source":"<a id='load_file'> </a>\n#### LOAD flight_date.csv\nAs the date conversion and other pre-processing steps are expensive for 5.8 million records, I am loading already pre-processed file"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df = pd.read_csv('../input/flights-delay-date/flights_date.csv', low_memory=False)\ndf['SCHEDULED_DEPARTURE']= pd.to_datetime(df['SCHEDULED_DEPARTURE']) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is how the data looks like. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.2 Analysis of cancelled flights\nAs we are analysing the delays, we dont need to consider the flights those have been cancelled. In this section, let us see how many such flights are there and remove them"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"total_count = df['AIRLINE'].count()\ncancelled_count = df[df['CANCELLED'] == 1]['AIRLINE'].count()\n\nprint('Total flights', total_count, 'and Cancelled flights',cancelled_count, '. So % of cancelled flights: ', round(cancelled_count/total_count*100,2) ,'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us remove those cancelled flights, as they will not add any value in performing prediction of flight delays"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df = df[df['CANCELLED'] == 0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb1bc01410ecd9c32e4955eba5ec974a6cb41afe","_cell_guid":"7636e373-e2d9-4478-9270-2b29b436801a"},"cell_type":"markdown","source":"Note that in practice, the content of the **DEPARTURE_TIME** and **ARRIVAL_TIME** variables can be a bit misleading since they don't contain the dates. For exemple, in the first entry of the dataframe, the scheduled departure is at 0h05 the 1st of January. The **DEPARTURE_TIME** variable indicates 23h54 and we thus don't know if the flight leaved before time or if there was a large delay. Hence, the **DEPARTURE_DELAY** and **ARRIVAL_DELAY** variables proves more useful since they directly provides the delays in minutes. Hence, in what follows, I will not use the **DEPARTURE_TIME** and **ARRIVAL_TIME** variables.\n\n### 1.3 Filling factor\n\nFinally, I clean the dataframe throwing the variables I won't use and re-organize the columns to ease its reading:"},{"metadata":{"_uuid":"b4daa5a7f345e3fc5b847823f542e2bdf5a178a0","_cell_guid":"4d8faabc-9b14-4598-a1ec-8c8e6277db7d","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"variables_to_remove = ['TAXI_OUT', 'TAXI_IN', 'WHEELS_ON', 'WHEELS_OFF','DATE', 'AIR_SYSTEM_DELAY',\n                       'SECURITY_DELAY', 'AIRLINE_DELAY', 'LATE_AIRCRAFT_DELAY',\n                       'WEATHER_DELAY', 'DIVERTED', 'CANCELLED', 'CANCELLATION_REASON',\n                       'FLIGHT_NUMBER', 'TAIL_NUMBER', 'AIR_TIME']\ndf.drop(variables_to_remove, axis = 1, inplace = True)\ndf = df[['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT',\n        'SCHEDULED_DEPARTURE', 'DEPARTURE_TIME', 'DEPARTURE_DELAY',\n        'SCHEDULED_ARRIVAL', 'ARRIVAL_TIME', 'ARRIVAL_DELAY',\n        'SCHEDULED_TIME',  'DAY_OF_WEEK', 'ELAPSED_TIME', 'MONTH']]\ndf[:5]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f8c3345c720f977e5120d3d40b929a1fe8f1f2c","_cell_guid":"208ba5d6-a305-4458-ad95-c4499135e34f"},"cell_type":"markdown","source":"At this stage, I examine how complete the dataset is:"},{"metadata":{"_uuid":"6b5db35ee0badf50e777df2bfff70f19b0bda3bc","_cell_guid":"f9d7ccec-ad93-491a-b114-ab9a6db73b56","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"missing_df = df.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['variable', 'missing values']\nmissing_df['filling factor (%)']=(df.shape[0]-missing_df['missing values'])/df.shape[0]*100\nmissing_df.sort_values('filling factor (%)').reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4bc8737a5da6714ce6cc49c8688619656865fb02","_cell_guid":"d910dffa-ce25-4c92-8610-04efff80d47e"},"cell_type":"markdown","source":"We see that the variables filling factor is quite good (> 99%). Since we have pretty good number of samples, I decide to proceed without trying to impute what's missing and I simply remove the entries that contain missing values. The shape of our dataset is: "},{"metadata":{"_uuid":"60bceb5e13bf5af8d460ec96f87341c7dc407a84","_cell_guid":"374ed8ec-e66d-4964-a0fd-1960fe7a4ea5","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df.dropna(inplace = True)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.4 Perform resampling for analysis\nWe can see that the size of the dataset is 5.7 million records and any operations on such a huge dataset will be very expensive. So, we see  if we can re-sample the dataset, for analysis purposes. \n\nAs we know this dataset it for entire 2015 year, let us see if we can take few months data. \nLet us see if the MONTH feature affects the delay by large way or not"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,8))\nax = sns.barplot(data=df, x='MONTH',y='DEPARTURE_DELAY', ax=ax, ci=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above chart, we see that MONTH feature affects the delay time. We see Jun is most affected month, followed by Dec, Feb and Jul. We also see that Sep and Oct months witnessed least delay. \n\nWe can take one month from each Quarter for analysis purpose. Let us consider follwoing months - Feb, Jun, Aug and Dec. This will help in saving time during EDA and also modelling. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# df_original contains data with all 12 months for later analysis\n# df_original = df\ndf = df[df['MONTH'].apply(lambda x:x in [2,6,8,12])  ]\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5997da5d8af46951ce7408399e0b39dbe09477b8","_cell_guid":"f3697145-55d1-4943-a7f2-a9c56c636a86"},"cell_type":"markdown","source":"___\n## 2. Explortory Data Analysis\nIn this section we perform various steps to understand the data. We will derive some intuitions which will help us in preparing the model\n\n\n### 2.1 Comparing airlines\nLet us compare different Airlines. As said earlier, the **AIRLINE** variable contains the airline abreviations. Their full names can be retrieved from the `airlines.csv` file.\n"},{"metadata":{"_uuid":"8608cd6e56aa3663cdc23ea325a9f3760e53eddf","_cell_guid":"cfff7486-c495-4d6b-ba81-429508942f56","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"airlines_names = pd.read_csv('../input/flight-delays/airlines.csv')\nairlines_names","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93223fb59244741a45e672ca8f054ea5316028ef","_cell_guid":"df1d2a0c-ff18-4a01-a25d-e8430831900b"},"cell_type":"markdown","source":"For further use, I put the content of this this dataframe in a dictionary:"},{"metadata":{"_uuid":"d0cf6529149105eac1ba48b0ba3e1d246f51b2a0","_cell_guid":"3bae09a6-4d88-4457-8758-2b7b4836083d","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"abbr_companies = airlines_names.set_index('IATA_CODE')['AIRLINE'].to_dict()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d13334e56d7e7ef54b1828066a5ecd5c0b8e4fb3","_cell_guid":"2ac6e2cc-3948-4cd2-88d1-2d0d853aae0d"},"cell_type":"markdown","source":"___\n### 2.2 Basic statistical description of airlines\n\nAs a first step, I consider all the flights from all carriers. Here, the aim is to classify the airlines with respect to their punctuality and for that purpose, I compute a few basic statisticial parameters:"},{"metadata":{"_uuid":"4bf03db2985c11552fc1ff40f658a272475b4b26","_cell_guid":"4dc47175-b9e5-4c1c-8e7d-e9f4c4a2a0c1","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#__________________________________________________________________\n# function that extract statistical parameters from a grouby objet:\ndef get_stats(group):\n    return {'min': group.min(), 'max': group.max(),\n            'count': group.count(), 'mean': group.mean()}\n#_______________________________________________________________\n# Creation of a dataframe with statitical infos on each airline:\nglobal_stats = df['DEPARTURE_DELAY'].groupby(df['AIRLINE']).apply(get_stats).unstack()\nglobal_stats = global_stats.sort_values('count')\nglobal_stats","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38a3613c045702d9da671254f06d9d113a7e4ed3","_cell_guid":"a0b2064a-b3a7-42a8-971c-b45cb0dfbe31"},"cell_type":"markdown","source":"Now, in order to understand the data, let us see some graphics:"},{"metadata":{"_uuid":"c01ea5f23d0df63a6b1e48f97096e52a6b677f45","_cell_guid":"ae7f3ede-a9ef-4db4-aae0-f4ff5829b887","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"font = {'family' : 'normal', 'weight' : 'bold', 'size'   : 15}\nmpl.rc('font', **font)\nimport matplotlib.patches as mpatches\n#__________________________________________________________________\n# I extract a subset of columns and redefine the airlines labeling \ndf2 = df.loc[:, ['AIRLINE', 'DEPARTURE_DELAY']]\ndf2['AIRLINE'] = df2['AIRLINE'].replace(abbr_companies)\n#________________________________________________________________________\ncolors = ['royalblue', 'grey', 'wheat', 'c', 'firebrick', 'seagreen', 'lightskyblue',\n          'lightcoral', 'yellowgreen', 'gold', 'tomato', 'violet', 'aquamarine', 'chartreuse']\n#___________________________________\nfig = plt.figure(1, figsize=(16,15))\ngs=GridSpec(2,2)             \nax1=fig.add_subplot(gs[0,0]) \nax2=fig.add_subplot(gs[0,1]) \nax3=fig.add_subplot(gs[1,:]) \n#------------------------------\n# Pie chart n0. 1: nb of flights\n#------------------------------\nlabels = [s for s in  global_stats.index]\nsizes  = global_stats['count'].values\nexplode = [0.3 if sizes[i] < 20000 else 0.0 for i in range(len(abbr_companies))]\npatches, texts, autotexts = ax1.pie(sizes, explode = explode,\n                                labels=labels, colors = colors,  autopct='%1.0f%%',\n                                shadow=False, startangle=0)\nfor i in range(len(abbr_companies)): \n    texts[i].set_fontsize(14)\nax1.axis('equal')\nax1.set_title('% of flights per company', bbox={'facecolor':'midnightblue', 'pad':5},\n              color = 'w',fontsize=18)\n#_______________________________________________\n# I set the legend: abreviation -> airline name\ncomp_handler = []\nfor i in range(len(abbr_companies)):\n    comp_handler.append(mpatches.Patch(color=colors[i],\n            label = global_stats.index[i] + ': ' + abbr_companies[global_stats.index[i]]))\nax1.legend(handles=comp_handler, bbox_to_anchor=(0.2, 0.9), \n           fontsize = 13, bbox_transform=plt.gcf().transFigure)\n#----------------------------------------\n# Pie chart nÂº2: mean delay at departure\n#----------------------------------------\nsizes  = global_stats['mean'].values\nsizes  = [max(s,0) for s in sizes]\nexplode = [0.0 if sizes[i] < 20000 else 0.01 for i in range(len(abbr_companies))]\npatches, texts, autotexts = ax2.pie(sizes, explode = explode, labels = labels,\n                                colors = colors, shadow=False, startangle=0,\n                                autopct = lambda p :  '{:.0f}'.format(p * sum(sizes) / 100))\nfor i in range(len(abbr_companies)): \n    texts[i].set_fontsize(14)\nax2.axis('equal')\nax2.set_title('Mean delay at origin', bbox={'facecolor':'midnightblue', 'pad':5},\n              color='w', fontsize=18)\n#------------------------------------------------------\n# striplot with all the values reported for the delays\n#___________________________________________________________________\n# I redefine the colors for correspondance with the pie charts\ncolors = ['firebrick', 'gold', 'lightcoral', 'aquamarine', 'c', 'yellowgreen', 'grey',\n          'seagreen', 'tomato', 'violet', 'wheat', 'chartreuse', 'lightskyblue', 'royalblue']\n#___________________________________________________________________\nax3 = sns.stripplot(y=\"AIRLINE\", x=\"DEPARTURE_DELAY\", size = 4, palette = colors,\n                    data=df2, linewidth = 0.5,  jitter=True)\nplt.setp(ax3.get_xticklabels(), fontsize=14)\nplt.setp(ax3.get_yticklabels(), fontsize=14)\nax3.set_xticklabels(['{:2.0f}h{:2.0f}m'.format(*[int(y) for y in divmod(x,60)])\n                         for x in ax3.get_xticks()])\nplt.xlabel('Departure delay', fontsize=18, bbox={'facecolor':'midnightblue', 'pad':5},\n           color='w', labelpad=20)\nax3.yaxis.label.set_visible(False)\n#________________________\nplt.tight_layout(w_pad=3) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dda13bff5d30fdfa309309c492a1a1bef8be916b","_cell_guid":"015986bc-c6e0-4a91-91e2-0180127e21b0"},"cell_type":"markdown","source":"Considering the first pie chart that gives the percentage of flights per airline, we see that there is some disparity between the carriers. For exemple, *Southwest Airlines* accounts for $\\sim$20% of the flights which is similar to the number of flights chartered by the 7 tiniest airlines. However, if we have a look at the second pie chart, we see that here, on the contrary, the differences among airlines are less pronounced. Excluding *Hawaiian Airlines* and *Alaska Airlines* that report extremely low mean delays, we obtain that a value of **$\\sim$11$\\pm$7 minutes** would correctly represent all mean delays. Note that this value is quite low which mean that the standard for every airline is to respect the schedule !\n\nFinally, the figure at the bottom makes a census of all the delays that were measured in January 2015. This representation gives a feeling on the dispersion of data and put in perspective the relative homogeneity that appeared in the second pie chart. Indeed, we see that while all mean delays are around 10 minutes, this low value is a consequence of the fact that a majority of flights take off on time. However, we see that occasionally, we can face really large delays that can reach a few tens of hours !\n\n---\n### 2.3 Delays Categorization\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"\nThe two numbers match up, which infers that only the flights with arrival delay >= 15 minutes having a detailed delay breakdown (e.g. air system, airline, weather).\n\nSo, let us try to categorize the delays as:\n- Anything <15 min as **On time**\n- 15 min to 1 hour as **Short Delay**\n- Beyond 1 hour as **Long Delay** \n\nThe different type of delays are visible in the next figure:"},{"metadata":{"_uuid":"1a4d6c1e05eee52f1c92766c213a7a3ab50cb1bd","_cell_guid":"ed1b8da1-052a-4114-bb6f-4bd0b832aa3b","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#_____________________________________________\n# Function that define how delays are grouped\ndelay_type = lambda x:((0,1)[x > 15],2)[x > 60]\ndf['DELAY_LEVEL'] = df['DEPARTURE_DELAY'].apply(delay_type)\n#____________________________________________________\nfig = plt.figure(1, figsize=(10,7))\nax = sns.countplot(y=\"AIRLINE\", hue='DELAY_LEVEL', data=df)\n#____________________________________________________________________________________\n# We replace the abbreviations by the full names of the companies and set the labels\nlabels = [abbr_companies[item.get_text()] for item in ax.get_yticklabels()]\nax.set_yticklabels(labels)\nplt.setp(ax.get_xticklabels(), fontsize=12, weight = 'normal', rotation = 0);\nplt.setp(ax.get_yticklabels(), fontsize=12, weight = 'bold', rotation = 0);\nax.yaxis.label.set_visible(False)\nplt.xlabel('Flight count', fontsize=16, weight = 'bold', labelpad=10)\n#________________\n# Set the legend\nL = plt.legend()\nL.get_texts()[0].set_text('on time (t < 15 min)')\nL.get_texts()[1].set_text('small delay (15 < t < 60 min)')\nL.get_texts()[2].set_text('large delay (t > 60 min)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b562b408c5b91ebc500ca2124e6796866886bee","_cell_guid":"23b373a6-41d8-4c20-8f29-4929c502792c"},"cell_type":"markdown","source":"This figure gives a count of the delays of less than 15 minutes, those in the range 15 < t < 60 min and finally, the delays greater than 60 minutes. Hence, we see that independently of the airline, delays greater than 60 minutes only account for a few percents. However, the proportion of delays in these three groups depends on the airline: as an exemple, in the case of *SkyWest Airlines*, the flights with delay greater than 60 minutes are more than $\\sim$50% with respect to delays in the range 15 < t < 60 min. Things are better for *SoutWest Airlines*  since delays greater than 60 minutes are around $\\sim$35% of that of in the range 15 < t < 60 min.\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"total = df['AIRLINE'].count()\nontime = df[df['DELAY_LEVEL'] == 0]['AIRLINE'].count()\npct_ontime = ontime/total*100\nprint('Ontime%: ',pct_ontime)\n\nsmall_delay = df[df['DELAY_LEVEL'] == 1]['AIRLINE'].count()\npct_small_delay = small_delay/total*100\nprint('Small delay%: ', pct_small_delay)\n\n\nlarge = df[df['DELAY_LEVEL'] == 2]['AIRLINE'].count()\npct_large = large/total*100\nprint('Large delay%: ', pct_large)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fdfa7e9a201fdbb18244ef140cfa3943274baa05","_cell_guid":"1f6cd47c-d910-4720-93d3-ab55c5db7804"},"cell_type":"markdown","source":"---\n### 2.4 Delays distribution: establishing the ranking of airlines\n\nIt was shown in the previous section that mean delays behave homogeneously among airlines (apart from two extrem cases) and is around 11$\\pm$7 minutes. Then, we saw that this low value is a consequence of the large proportion of flights that take off on time. However, occasionally, large delays can be registred. In this section, I examine more in details the distribution of delays for every airlines:"},{"metadata":{"_uuid":"d83bea7754ac53b3110e2a9cdda802274e82b3c6","_cell_guid":"d451bacd-b2b5-40ce-8b47-8c77902f6c2a","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#___________________________________________\n# Model function used to fit the histograms\ndef func(x, a, b):\n    return a * np.exp(-x/b)\n#-------------------------------------------\npoints = [] ; label_company = []\nfig = plt.figure(1, figsize=(11,11))\ni = 0\nfor carrier_name in [abbr_companies[x] for x in global_stats.index]:\n    i += 1\n    ax = fig.add_subplot(5,3,i)    \n    #_________________________\n    # Fit of the distribution\n    n, bins, patches = plt.hist(x = df2[df2['AIRLINE']==carrier_name]['DEPARTURE_DELAY'],\n                                range = (15,180), normed=True, bins= 60)\n    bin_centers = bins[:-1] + 0.5 * (bins[1:] - bins[:-1])    \n    popt, pcov = curve_fit(func, bin_centers, n, p0 = [1, 2])\n    #___________________________\n    # bookeeping of the results\n    points.append(popt)\n    label_company.append(carrier_name)\n    #______________________\n    # draw the fit curve\n    plt.plot(bin_centers, func(bin_centers, *popt), 'r-', linewidth=3)    \n    #_____________________________________\n    # define tick labels for each subplot\n    if i < 10:\n        ax.set_xticklabels(['' for x in ax.get_xticks()])\n    else:\n        ax.set_xticklabels(['{:2.0f}h{:2.0f}m'.format(*[int(y) for y in divmod(x,60)])\n                            for x in ax.get_xticks()])\n    #______________\n    # subplot title\n    plt.title(carrier_name, fontsize = 14, fontweight = 'bold', color = 'darkblue')\n    #____________\n    # axes labels \n    if i == 4:\n        ax.text(-0.3,0.9,'Normalized count of flights', fontsize=16, rotation=90,\n            color='k', horizontalalignment='center', transform = ax.transAxes)\n    if i == 14:\n        ax.text( 0.5, -0.5 ,'Delay at origin', fontsize=16, rotation=0,\n            color='k', horizontalalignment='center', transform = ax.transAxes)\n    #___________________________________________\n    # Legend: values of the a and b coefficients\n    ax.text(0.68, 0.7, 'a = {}\\nb = {}'.format(round(popt[0],2), round(popt[1],1)),\n            style='italic', transform=ax.transAxes, fontsize = 12, family='fantasy',\n            bbox={'facecolor':'tomato', 'alpha':0.8, 'pad':5})\n    \nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dac4f7c0db7e999fdf680f7bee9e0110169f8aa0","_cell_guid":"210eea4c-6e2c-43f8-a24d-f10889c0ea3b"},"cell_type":"markdown","source":"This figure shows the normalised distribution of delays that I modelised with an exponential distribution $ f(x) = a \\, \\mathrm{exp} (-x/b)$. The $a$ and $b$ parameters obtained to describe each airline are given in the upper right corner of each panel. Note that the normalisation of the distribution implies that $\\int f(x) \\, dx \\sim 1$. Here, we do not have a strict equality since the normalisation applies the histograms but not to the model function. However, this relation entails that the $a$ and $b$ coefficients will be correlated with $a \\propto 1/b$ and hence, only one of these two values is necessary to describe the distributions. Finally, according to the value of either $a$ or $b$, it is possible to establish a ranking of the companies: the low values of $a$ will correspond to airlines with a large proportion of important delays and, on the contrary, airlines that shine from their punctuality will admit hight $a$ values:\n\nBased on above explanation, we can decide that Hawaiin Airlines is most punctual followed by Alaska, Southwest and Delta Airlines. But, we can recall here that Hawaiin and Alaska Airliens have very less volume of flights."},{"metadata":{"_uuid":"1057fcfe8613fd870e02aa2097354ca63985bf9b","_cell_guid":"0fabc61c-3b7b-4aca-ade0-4839400d28b9"},"cell_type":"markdown","source":"___\n### 2.5 Delays: take-off or landing ?\nIn the previous section, all the discussion was done on departure delays. However, these delays differ somewhat from the delays recorded at arrival:"},{"metadata":{"_uuid":"d6d18dcdf529df297c7d2c3e363b4ccf79912dfc","_cell_guid":"379a4681-413f-4c81-a43b-8284660e39c7","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"mpl.rcParams.update(mpl.rcParamsDefault)\nmpl.rcParams['hatch.linewidth'] = 2.0  \n\nfig = plt.figure(1, figsize=(11,6))\nax = sns.barplot(x=\"DEPARTURE_DELAY\", y=\"AIRLINE\", data=df, color=\"lightskyblue\", ci=None)\nax = sns.barplot(x=\"ARRIVAL_DELAY\", y=\"AIRLINE\", data=df, color=\"r\", hatch = '///',\n                 alpha = 0.0, ci=None)\nlabels = [abbr_companies[item.get_text()] for item in ax.get_yticklabels()]\nax.set_yticklabels(labels)\nax.yaxis.label.set_visible(False)\nplt.xlabel('Mean delay [min] (@departure: blue, @arrival: hatch lines)',\n           fontsize=14, weight = 'bold', labelpad=10);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e517ce5240d519a852e48264a3931c050844d0d","_cell_guid":"5a0e7958-4c7b-4512-a02c-b6a29192f23c"},"cell_type":"markdown","source":"On this figure, we can see that delays at arrival are generally lower than at departure. This indicates that airlines adjust their flight speed in order to reduce the delays at arrival. In what follows, I will just consider the delays at departure but one has to keep in mind that this can differ from arrival delays."},{"metadata":{},"cell_type":"markdown","source":"---\n### 2.6 Relationship with delays and weekday\nI am hypothesising that delay can be associated with the day of the week. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"ax = sns.barplot(x=\"DAY_OF_WEEK\", y=\"DEPARTURE_DELAY\", data=df, palette=\"husl\", ci=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that avg  delays are more on Mondays and Tuesday. But variation is not too much. We will not consider this as important feature that will impact delays"},{"metadata":{"_uuid":"0c59461c5d64f4479779f2591c0f18c15dda6b00","_cell_guid":"df6a500d-e61e-495d-9e2b-d2445fb9bee3"},"cell_type":"markdown","source":"___\n### 2.7. Relation between the origin airport and delays\n\nI will now try to define if there is a correlation between the delays registered and the airport of origin. I recall that in the dataset, the number of airports considered is: "},{"metadata":{"_uuid":"4e48ddde536f319230619c786e9780bc7b25d4ad","_cell_guid":"043c3b7d-06b0-4785-ba63-6ff0f4bd495f","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"No. of airports: {}\".format(len(df['ORIGIN_AIRPORT'].unique())))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6110299e5f3920e097bd0dd0669315c5b7ab2cd0","_cell_guid":"39b8aac0-5def-4a9b-8578-369e66eee662","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"origin_nb = dict()\nfor carrier in abbr_companies.keys():\n    liste_origin_airport = df[df['AIRLINE'] == carrier]['ORIGIN_AIRPORT'].unique()\n    origin_nb[carrier] = len(liste_origin_airport)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0dae75ac6bfb237065cd066ab393a5421ea70bd4","_cell_guid":"268b73f5-79df-4341-8576-901b61c5fa8d","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"test_df = pd.DataFrame.from_dict(origin_nb, orient='index')\ntest_df.rename(columns = {0:'count'}, inplace = True)\nax = test_df.plot(kind='bar', figsize = (8,3))\nlabels = [abbr_companies[item.get_text()] for item in ax.get_xticklabels()]\nax.set_xticklabels(labels)\nplt.ylabel('Number of airports visited', fontsize=14, weight = 'bold', labelpad=12)\nplt.setp(ax.get_xticklabels(), fontsize=11, ha = 'right', rotation = 80)\nax.legend().set_visible(False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b905c9fae552b570fa4abc22770e5c028b99644","_cell_guid":"032c19e6-a518-4eba-a260-4f06ee7c09af"},"cell_type":"markdown","source":"___\n#### 2.7.1 How the origin airport impact delays\n\nIn this section, I will have a look at the variations of the delays with respect to the origin airport and for every airline. The first step thus consists in determining the mean delays per airport:"},{"metadata":{"_uuid":"9edca92438d893df9caafd9b066d547f79c4756c","_cell_guid":"a47b96fd-5650-4fe2-916a-54734816033f","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"airport_mean_delays = pd.DataFrame(pd.Series(df['ORIGIN_AIRPORT'].unique()))\nairport_mean_delays.set_index(0, drop = True, inplace = True)\n\nfor carrier in abbr_companies.keys():\n    df1 = df[df['AIRLINE'] == carrier]\n    test = df1['DEPARTURE_DELAY'].groupby(df['ORIGIN_AIRPORT']).apply(get_stats).unstack()\n    airport_mean_delays[carrier] = test.loc[:, 'mean'] ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"618a41aa1dcd0c49c1b37cd826ba0a5f4c71b886","_cell_guid":"33ebba80-45d9-4d9a-a292-56ace32a995e"},"cell_type":"markdown","source":"Since the number of airports is quite large, a graph showing all the information at once would be a bit messy, since it would represent around 4400 values (i.e. 312 airports $\\times$ 14 airlines). Hence, I just represent a subset of the data:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"identify_airport = airports.set_index('IATA_CODE')['CITY'].to_dict()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7e8d24ce06a878fbfb8a6bc405d560331c679ed","_cell_guid":"b029d5eb-50e6-4461-87fc-7effed9e5bd2","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.set(context=\"paper\")\nfig = plt.figure(1, figsize=(12,12))\n\nax = fig.add_subplot(1,2,1)\nsubset = airport_mean_delays.iloc[:50,:].rename(columns = abbr_companies)\nsubset = subset.rename(index = identify_airport)\nmask = subset.isnull()\nsns.heatmap(subset, linewidths=0.01, cmap=\"YlGnBu\", mask=mask, vmin = 0, vmax = 35)\nplt.setp(ax.get_xticklabels(), fontsize=10, rotation = 85) ;\nax.yaxis.label.set_visible(False)\n\nax = fig.add_subplot(1,2,2)    \nsubset = airport_mean_delays.iloc[50:100,:].rename(columns = abbr_companies)\nsubset = subset.rename(index = identify_airport)\nfig.text(0.5, 1.02, \"Delays: impact of the origin airport\", ha='center', fontsize = 18)\nmask = subset.isnull()\nsns.heatmap(subset, linewidths=0.01, cmap=\"YlGnBu\", mask=mask, vmin = 0, vmax = 35)\nplt.setp(ax.get_xticklabels(), fontsize=10, rotation = 85) ;\nax.yaxis.label.set_visible(False)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e48d2a37cf3cafb4da3b4172fe4116dcdb1ea96c","_cell_guid":"fe346c75-a242-47c9-8e5f-d304a86a34cc"},"cell_type":"markdown","source":"This figure allows to draw some conclusions. First, by looking at the data associated with the different airlines, we find the behavior we previously observed: for example, if we consider the right panel, it will be seen that the column associated with  *American Eagle Airlines* mostly reports large delays, while the column associated with *Delta Airlines* is mainly associated  with delays of less than 5 minutes. If we now look at the airports of origin, we will see that some airports favor late departures: see e.g. Denver, Chicago or New York. Conversely, other airports will mainly know on time departures such as Portland or Oakland.\n\nFinally, we can deduce from these observations that there is a high variability in average delays, both between the different airports but also between the different airlines. This is important because it implies that in order to accurately model the delays, it will be necessary to adopt a model that is ** specific to the company and the home airport **. "},{"metadata":{},"cell_type":"markdown","source":"___\n### 2.8 Temporal variability of delays\n\nIn this section, I look at the way delays vary with time. Considering the case of a specific airline and airport, delays can be easily represented by day and time "},{"metadata":{"_uuid":"3ede65bbb79cce631cdfc487ca9efa9c9b47e805","_cell_guid":"8cd7160a-5f42-45e8-8931-b0b768f83317","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class Figure_style():\n    #_________________________________________________________________\n    def __init__(self, size_x = 11, size_y = 5, nrows = 1, ncols = 1):\n        sns.set_style(\"white\")\n        sns.set_context(\"notebook\", font_scale=1.2, rc={\"lines.linewidth\": 2.5})\n        self.fig, axs = plt.subplots(nrows = nrows, ncols = ncols, figsize=(size_x,size_y,))\n        #________________________________\n        # convert self.axs to 2D array\n        if nrows == 1 and ncols == 1:\n            self.axs = np.reshape(axs, (1, -1))\n        elif nrows == 1:\n            self.axs = np.reshape(axs, (1, -1))\n        elif ncols == 1:\n            self.axs = np.reshape(axs, (-1, 1))\n    #_____________________________\n    def pos_update(self, ix, iy):\n        self.ix, self.iy = ix, iy\n    #_______________\n    def style(self):\n        self.axs[self.ix, self.iy].spines['right'].set_visible(False)\n        self.axs[self.ix, self.iy].spines['top'].set_visible(False)\n        self.axs[self.ix, self.iy].yaxis.grid(color='lightgray', linestyle=':')\n        self.axs[self.ix, self.iy].xaxis.grid(color='lightgray', linestyle=':')\n        self.axs[self.ix, self.iy].tick_params(axis='both', which='major',\n                                               labelsize=10, size = 5)\n    #________________________________________\n    def draw_legend(self, location='upper right'):\n        legend = self.axs[self.ix, self.iy].legend(loc = location, shadow=True,\n                                        facecolor = 'g', frameon = True)\n        legend.get_frame().set_facecolor('whitesmoke')\n    #_________________________________________________________________________________\n    def cust_plot(self, x, y, color='b', linestyle='-', linewidth=1, marker=None, label=''):\n        if marker:\n            markerfacecolor, marker, markersize = marker[:]\n            self.axs[self.ix, self.iy].plot(x, y, color = color, linestyle = linestyle,\n                                linewidth = linewidth, marker = marker, label = label,\n                                markerfacecolor = markerfacecolor, markersize = markersize)\n        else:\n            self.axs[self.ix, self.iy].plot(x, y, color = color, linestyle = linestyle,\n                                        linewidth = linewidth, label=label)\n        self.fig.autofmt_xdate()\n    #________________________________________________________________________\n    def cust_plot_date(self, x, y, color='lightblue', linestyle='-',\n                       linewidth=1, markeredge=False, label=''):\n        markeredgewidth = 1 if markeredge else 0\n        self.axs[self.ix, self.iy].plot_date(x, y, color='lightblue', markeredgecolor='grey',\n                                  markeredgewidth = markeredgewidth, label=label)\n    #________________________________________________________________________\n    def cust_scatter(self, x, y, color = 'lightblue', markeredge = False, label=''):\n        markeredgewidth = 1 if markeredge else 0\n        self.axs[self.ix, self.iy].scatter(x, y, color=color,  edgecolor='grey',\n                                  linewidths = markeredgewidth, label=label)    \n    #___________________________________________\n    def set_xlabel(self, label, fontsize = 14):\n        self.axs[self.ix, self.iy].set_xlabel(label, fontsize = fontsize)\n    #___________________________________________\n    def set_ylabel(self, label, fontsize = 14):\n        self.axs[self.ix, self.iy].set_ylabel(label, fontsize = fontsize)\n    #____________________________________\n    def set_xlim(self, lim_inf, lim_sup):\n        self.axs[self.ix, self.iy].set_xlim([lim_inf, lim_sup])\n    #____________________________________\n    def set_ylim(self, lim_inf, lim_sup):\n        self.axs[self.ix, self.iy].set_ylim([lim_inf, lim_sup])           ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9b7f73a58def8517dfaae518fbb3fe9ab498796","_cell_guid":"4a4e7ec9-f5f4-4f30-850f-9b10288a23e3","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#_______________________________\ndef func2(x, a, b, c):\n    return a * x**2 +  b*x + c\n#_______________________________\ndf['heure_depart'] =  df['SCHEDULED_DEPARTURE'].apply(lambda x:x.time())\ntest2 = df['DEPARTURE_DELAY'].groupby(df['heure_depart']).apply(get_stats).unstack()\nfct = lambda x:x.hour*3600+x.minute*60+x.second\nx_val = np.array([fct(s) for s in test2.index]) \ny_val = test2['mean']\npopt, pcov = curve_fit(func2, x_val, y_val, p0 = [1, 2, 3])\ntest2['fit'] = pd.Series(func2(x_val, *popt), index = test2.index)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2dba27e34e280498d85a873851fd8e2fb7572990","_cell_guid":"f66716ae-73a9-464b-8301-a65f92bd15d2"},"cell_type":"markdown","source":"which visually gives:"},{"metadata":{"_uuid":"8133d47cb1bb2fc42bc68413cb6244f0fb7c8c78","_cell_guid":"397a7b48-7026-47de-8a98-c8e00d7826f5","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig1 = Figure_style(8, 4, 1, 1)\nfig1.pos_update(0, 0)\nfig1.cust_plot_date(df['heure_depart'], df['DEPARTURE_DELAY'],\n                    markeredge=False, label='initial data points')\nfig1.cust_plot(test2.index, test2['mean'], linestyle='--', linewidth=2, label='mean')\nfig1.cust_plot(test2.index, test2['fit'], color='r', linestyle='-', linewidth=3, label='fit')\nfig1.style() ; fig1.draw_legend('upper left')\nfig1.set_ylabel('Delay (minutes)', fontsize = 14)\nfig1.set_xlabel('Departure time', fontsize = 14)\nfig1.set_ylim(-15, 210)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b68cd855f24407af4b0907f024e7f7f7f4b1f4c","_cell_guid":"c968a42d-d155-430f-a929-631571f1713b"},"cell_type":"markdown","source":"Here, we can see that the average delay tends to increase with the departure time of day: flights leave on time in the morning  and the delay grows almost monotonously up to 30 minutes at the end of the day. In fact, this behavior is quite general and looking at other aiports or companies, we would find similar trends.\n\n---\nLet us now try to plot the avg delay by hour of the day to visualize in better way. So, using the SCHEDULED_DEPARTURE  field we create a new column called *dep_hour* which contains the hour of the day. \n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df['dep_hour'] = df['SCHEDULED_DEPARTURE'].apply(lambda x:x.time().hour)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us now see scatter plot of DEPARTURE_DELAY against dep_hour and see how temporal value is impacting the DEPARTURE_DELAY"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8,6))\nsns.lineplot(data=df, x='dep_hour',y='DEPARTURE_DELAY', ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that, till 5 AM, delay is more and reduces suddenly and keeps increasing till around 8PM and drops down"},{"metadata":{},"cell_type":"markdown","source":"---\n## 3. Feature Engineering\nWith all the data analysis done in Section 2, we can deduce that the DEPARTURE_DELAY target variable has following feature variables\n1. Airline\n2. Airport\n3. Departure time\n\nOut of these above features, Airline and Airport are categorical variables. In this section, we convert these categorical valriables in numerical variables using encoding techiques. We can use Label encoding or OneHot Encoding. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fct = lambda x:x.hour*3600+x.minute*60+x.second\ndf['heure_depart'] = df['SCHEDULED_DEPARTURE'].apply(lambda x:x.time())\ndf['heure_depart'] = df['heure_depart'].apply(fct)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We encode Airline and Airport using Label Encodign "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df = df[['AIRLINE','ORIGIN_AIRPORT', 'heure_depart', 'dep_hour', 'DEPARTURE_DELAY' ]]\nle = LabelEncoder()\ndf['AIRLINE'] = le.fit_transform(df['AIRLINE'])\ndf['ORIGIN_AIRPORT'] = le.fit_transform(df['ORIGIN_AIRPORT'])\ndf.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are now ready for building our models\n\n---\n## 4.0 Model Building and Testing"},{"metadata":{},"cell_type":"markdown","source":"Due to the fact that any delay which are more than 60 min is considered as **Large Delay**, let us remove that from our original data set. We can consider them as outliers."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df2 = df\ndf2['DEPARTURE_DELAY'] = df2['DEPARTURE_DELAY'].apply(lambda x:x if x < 60 else np.nan)\ndf2 = df2.dropna(how = 'any')\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now split the data into X and Y. X which are the features of the model are - *AIRLINE*, *ORIGIN_AIRPORT* AND departure hour (*dep_hour*). The mean of DEPARTURE_DELAY  is our Y. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"features = ['AIRLINE', 'ORIGIN_AIRPORT', 'dep_hour']\ndf2 = df2.groupby(features ).mean()\n\ndf2 = df2.drop([ 'heure_depart'], axis=1)\ndf2 = df2.dropna(how='any')\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df2 = df2.reset_index()\ny = df2['DEPARTURE_DELAY']\n# X = df.drop(['DEPARTURE_DELAY'], axis=1)\nX = df2[features]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we split the data into train and test data. 70% train and 30% test data"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Baseline Model\n\nLet us now build a Naive Linear Model"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"model = linear_model.LinearRegression()\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use MSE and R2 as Metrics"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"results = model.predict(X_test)\nscore1 = metrics.mean_squared_error(y_test,results )\nscore2 = metrics.r2_score(y_test,results )\nprint('MSE: ',score1, '  R2 Score: ', score2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is our baseline model. Let us try other models to beat this metric\n\n---\n### 4.2 Polynomial Model\n\nWe will try to improve the model by using polynomial model. In the section below, we create a grid search to get best parameter for Polynomial model"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for i in range(1,10):\n    poly = PolynomialFeatures(degree = i)\n    regr = linear_model.LinearRegression()\n    X_ = poly.fit_transform(X_train)\n    regr.fit(X_, y_train)\n    X_ = poly.fit_transform(X_test)\n    results = regr.predict(X_)\n    score1 = metrics.mean_squared_error(y_test,results )\n    score2 = metrics.r2_score(y_test,results )\n    print('Model with Polynominal Degree', i, 'MSE: ',score1, '  R2 Score: ', score2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that polynomial order 4 gives best MSE result.\n\n---\n#### 4.2.1 Cross Validation for Polynomial Model\n\nNow let use Cross Validation for Polynomial Model"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"nb_folds = 10\nfor i in range(1,10):\n    poly = PolynomialFeatures(degree = i)\n    regr = linear_model.LinearRegression()\n    X_ = poly.fit_transform(X )\n    results = cross_val_predict(regr, X_, y, cv = nb_folds)\n    \n    score1 = np.mean(cross_val_score(regr, X_, y,cv = nb_folds, scoring = 'neg_mean_squared_error'))   \n    score2 = np.mean(cross_val_score(regr, X_, y,cv = nb_folds, scoring = 'r2'))\n    print('Model with Polynominal Degree', i, 'MSE: ',score1, '  R2 Score: ', score2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n### 4.3 Random Forest Regression\nAs we have seen, the data is not linear, so let us try some tree based algorithm. Let us try Random Forest Algorithm"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nmodel2 = RandomForestRegressor(n_estimators=50, max_depth=1000 )\nmodel2.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us try to predict and evaluate the values "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"results2 = model2.predict(X_test)\nscore1 = metrics.mean_squared_error(y_test,results2 )\nscore2 = metrics.r2_score(y_test,results2 )\nprint('MSE: ',score1, '  R2 Score: ', score2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe that this has not improves much compared to baseline model.\n\n---\n### 4.4 Ridge Regression\nLet us try to use Ridge technique, which helps to improve the model by regularization."},{"metadata":{"_uuid":"48889e55fc5aef282ba1a652dd7962c8a7a94cbc","_cell_guid":"534b513a-b5ec-42f4-bfaf-b01eb98c90a2","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nridgereg = Ridge(alpha=0.0001,normalize=True)\npoly = PolynomialFeatures(degree = 3)\nX_ = poly.fit_transform(X_train)\nridgereg.fit(X_, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2ccca02f1fa3ef58eb66d6bb953c071c0eda660","_cell_guid":"abbc5e64-dab1-4972-a0d1-2f7f2de5813c","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"X_ = poly.fit_transform(X_test)\nresults = ridgereg.predict(X_)\nscore1 = metrics.mean_squared_error(y_test,results )\nscore2 = metrics.r2_score(y_test,results )\nprint('MSE: ',score1, '  R2 Score: ', score2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dcb0a8a9fafe20fce3291981cdd0d81b1898d31e","_cell_guid":"022f00b3-ec66-4622-875f-78fb8f85f5a3"},"cell_type":"markdown","source":"Now, if we calculate the score associated to the predictions made with a regularization technique\n\nAnd we can see that we obtain a reasonable score. Hence, with the current procedure, to determine the best model, we have two free parameters to adjust: the polynomial order and the $\\alpha$ coefficient of the * 'Ridge Regression' *:"},{"metadata":{"_uuid":"7a327687b2b5b0aa92230445547aaa01ecaafddf","_cell_guid":"a18e4c13-a083-432b-8140-2e0e6cfd06c7","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"score_min = 10000\nfor pol_order in range(1, 5):\n    for alpha in range(0, 20, 2):\n        ridgereg = Ridge(alpha = alpha/10000, normalize=True)\n        poly = PolynomialFeatures(degree = pol_order)\n        regr = linear_model.LinearRegression()\n        X_ = poly.fit_transform(X_train)\n        ridgereg.fit(X_, y_train)        \n        X_ = poly.fit_transform(X_test)\n        result = ridgereg.predict(X_)\n        score = metrics.mean_squared_error(result, y_test)        \n        if score < score_min:\n            score_min = score\n            parameters = [alpha/10, pol_order]\n        print(\"n={}Â alpha={} , MSE = {:<0.5}\".format(pol_order, alpha, score))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b31399eef2c7f75f7360142af755d54c35a62d21","_cell_guid":"c6f3ac04-85ef-472f-897d-4ccf687c3fb8"},"cell_type":"markdown","source":"This grid search allows to find the best set of $\\alpha$ and $n$ parameters. Let us note, however, that for this model, the estimates obtained with a linear regression or a polynomial of order 3 are quite close. Also we can see the best results are obtained for $\\alpha$ value to be *0*. It means that model is ordinary least square regression model"},{"metadata":{},"cell_type":"markdown","source":"---\n### 4.5 LightGBM algorithm\n\n**LightGBM** is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n\n* Faster training speed and higher efficiency.\n* Lower memory usage.\n* Better accuracy.\n* Support of parallel and GPU learning.\n* Capable of handling large-scale data."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import lightgbm as lgb\nd_train = lgb.Dataset(X_train, label=y_train)\nparams = {}\nparams['learning_rate'] = 0.08\nparams['boosting_type'] = 'gbdt'\n# params['boosting_type'] = 'dart'\nparams['objective'] = 'regression'\nparams['metric'] = 'mse'\nparams['sub_feature'] = 0.5\nparams['num_leaves'] = 100\nparams['min_data'] = 5\nparams['max_depth'] = 100\ny_train=y_train.ravel()\nreg= lgb.train(params, d_train, 100)\nresults=reg.predict(X_test)\nscore1 = metrics.mean_squared_error(y_test,results )\nscore2 = metrics.r2_score(y_test,results )\nprint('MSE: ',score1, '  R2 Score: ', score2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can tweak many of the LightGBM hyperparameters for better results, I observed that ```boosting_type``` and ```learning_rate``` affected the most for the results. "},{"metadata":{},"cell_type":"markdown","source":"## 5.0 Model Evaluation"},{"metadata":{},"cell_type":"markdown","source":"So far LightGBM regression with ```boosting_type = gbdt``` with ```learning_rate = 0.08``` has fit the model the best way with the above MSE and R2 score."},{"metadata":{"_uuid":"2e506fc52610ff75f59bed100a3450fae01adc62","_cell_guid":"7a4dc48b-f0a3-4d4d-aa4b-51d7903a2687"},"cell_type":"markdown","source":"To get an idea of the meaning of such a value for the MSE, we can assume a constant error on each point of the dataset. In which case, at each point $ i $, we have:\n\n\\begin{eqnarray}\ny_i - f(x_i) = cste = \\sqrt{MSE}\n\\end{eqnarray}\n\nthus giving the difference in minutes between the predicted delay and the actual delay. In this case, the difference between the model and the observations is thus typically:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"'Estimation Error = {:.2f} min'.format(np.sqrt(score1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6db9f7f318636eede2be5d3d6bdfd60507ed89cb","_cell_guid":"6369dc6e-6179-47e1-a581-fbae6576843d","collapsed":true},"cell_type":"markdown","source":"## 6.0 Conclusion\n\nBy keeping the original problem which is to predict the flight departure delays, we have considered multiple features and selected top 3 features which has major impacts on delays. (Features)\n\n- Airport\n- Airline\n- Time of departure (Hour)\n\nWe then constructed base model and then improvised multiple other models. At the end we found the best model based on best Metric output which is **LightGBM**. We can use that model to predict any flight delays provided Airport, Airline and time of departure. We can estimate the delay with approximation of *5~6 min*.\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Improvements\nDefinitely, we can improve the solution for this problem. Following are some of the improvement ideas:\n\n* First of all, we have eliminated Cancelled flights, however in the improved model we can also predict the possibiltiy of cancellation\n* We have removed all the flights which have delays more than 1 hour. We can build a 2 step soltution, first one, which classifies if the flight gets **cancelled**, **short_delay**, **large_delay**  or  **ontime**. Then in the second step, it can predict the actual delay using regression\n* We can perform an ensemble of LightGBM and Ridge Regression models for better generalization \n* We can also explore more hyperparameter tuning for LightGBM to improve the model\n* In this solution, we took only 4 months of data due to low power kernels, we can get more powerful kernel and consider all 12 months data\n* We can also model the solution using Neural Network with multiple layers"},{"metadata":{},"cell_type":"markdown","source":"---\n## References\n\n* Most of the graphical analysis in this notebook is inspired by the excellent Tutorial of Fabien Daniel. https://www.kaggle.com/fabiendaniel/predicting-flight-delays-tutorial\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}