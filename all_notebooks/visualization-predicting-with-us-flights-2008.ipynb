{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Visualization of US Flights Data 2008 & Predicting The Take Off Delays"},{"metadata":{},"cell_type":"markdown","source":"In this notebook, we try to visualize the US Flights Data 2008 and try to develop a model to predict the flights delays at take-off. aimed at predicting flight delays at take-off. In visualization, not all information of data will be caught by only some. Also for predicting, I did not try to develope the best model to predict but rather to emphasize on the various steps needed to build such a model.\n\n\nThis notebook is composed of three parts: cleaning (section 1), exploration (section 2-5) and modeling (section 6).\n\n**_Preamble_:** _overview, visualization, and descriptif information of the dataset_ <br>\n\n**1. Cleaning** \n- 1.1 Dates and times\n- 1.2 Filling factor\n\n**2. Comparing airlines**\n- 2.1 Basic statistical description of airlines\n- 2.2 Delays distribution: establishing the ranking of airlines \n\n** 3. Delays: take-off or landing ?** <br>\n** 4.  Relation between the origin airport and delays** <br>\n- 4.1 Geographical area covered by airlines  <br>\n- 4.2 How the origin airport impact delays <br>\n- 4.3 Flights with usual delays ? <br>\n\n** 5. Temporal variability of delays** <br>\n** 6. Predicting flight delays** <br>\n- 6.1 Model nº1: one airline, one airport \n  * 6.1.1 Pitfalls\n  * 6.1.2 Polynomial degree: splitting the dataset\n  * 6.1.3 Model test: prediction of end-January delays\n- 6.2 Model nº2: one airline, all airports\n  * 6.2.1 Linear regression  \n  * 6.2.2 Polynomial regression\n  * 6.2.3 Setting the free parameters\n  * 6.2.4 Model test: prediction of end-January delays\n- 6.3 Model nº3: Accounting for destinations\n   * 6.3.1 Choice of the free parameters\n   * 6.3.2 Model test: prediction of end-January delays \n   \n**Conclusion**\n\n\n**Acknowledge:**\n1. Plotly Tutorial: https://www.kaggle.com/hakkisimsek/plotly-tutorial-2\n2. Predicting flight delays [Tutorial]: https://www.kaggle.com/fabiendaniel/predicting-flight-delays-tutorial\n"},{"metadata":{},"cell_type":"markdown","source":"# _Preamble_: overview of the dataset\n\nFirst, we load all the packages that will be needed during this project:"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"ee455980b770b609f232023472fa024602060085","scrolled":false,"trusted":true},"cell_type":"code","source":"import datetime, warnings, scipy \nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib.patches import ConnectionPatch\nfrom collections import OrderedDict\nfrom matplotlib.gridspec import GridSpec\nfrom mpl_toolkits.basemap import Basemap\nfrom sklearn import metrics, linear_model\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\nfrom scipy.optimize import curve_fit\nplt.rcParams[\"patch.force_edgecolor\"] = True\nplt.style.use('fivethirtyeight')\nmpl.rc('patch', edgecolor = 'dimgray', linewidth=1)\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"last_expr\"\npd.options.display.max_columns = 50\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")\n\nimport plotly.offline as py\nimport plotly.figure_factory as ff\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom plotly import tools\nimport datetime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"conda install -c conda-forge basemap-data-hires ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/us-flights-data-2008/2008.csv', low_memory=False)\ndf.head(3).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Dataframe dimensions:', df.shape)\n#____________________________________________________________\n# gives some infos on columns types and number of null values\ntab_info=pd.DataFrame(df.dtypes).T.rename(index={0:'column type'})\ntab_info=tab_info.append(pd.DataFrame(df.isnull().sum()).T.rename(index={0:'null values (nb)'}))\ntab_info=tab_info.append(pd.DataFrame(df.isnull().sum()/df.shape[0]*100)\n                         .T.rename(index={0:'null values (%)'}))\ntab_info","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f62087f4cbafa4dc30b2796b8388be3b67d1774c"},"cell_type":"markdown","source":"Each entry of the `2008.csv` file corresponds to a flight and we see that more than 7,009,728 flights have been recorded in 2018. These flights are described according to 29 variables and I briefly recall the meaning of the variables that will be used in this notebook:\n\n- **Year, Month, DayofMonth, DayOfWeek**: dates of the flight <br/>\n- **UniqueCarrier**: An identification number assigned by US DOT to identify a unique airline <br/>\n- **Origin** and **Dest**: code attributed by IATA to identify the airports <br/>\n- **CRSDepTime** and **CRSArrTime** : scheduled times of take-off and landing <br/> \n- **DepTime** and **ArrTime**: real times at which take-off and landing took place <br/> \n- **DepDelay** and **ArrDelay**: difference (in minutes) between planned and real times <br/> \n- **Distance**: distance (in miles)  <br/>\n\nNext we'll see the number of null in data."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"8885c951b8b32240d2dce166c2f1763ad56c2420","trusted":true},"cell_type":"code","source":"pd.concat([df.isnull().sum(), 100 * df.isnull().sum()/len(df)], \n              axis=1).rename(columns={0:'Missing Records', 1:'Percentage (%)'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The highest null at the Cancelation Code, the information about the reason for Cancelation, about 98%.\n\n\nAn additional file of this dataset, the `airports.csv` and `airlines.csv` file, gives a more exhaustive description of the airports and the airlines:"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"d38f681f3187f7ec229baad5ec334138793eaf11","trusted":true},"cell_type":"code","source":"airlines_names = pd.read_csv('../input/airport-and-airlines-for-us-flights-data-2008/airlines.csv')\nairlines_names.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"769c5585c4e5da3873976847c563c0269f02ea04","trusted":true},"cell_type":"code","source":"df = pd.merge(df,airlines_names, left_on='UniqueCarrier', right_on = 'IATA_CODE')\ndf = df.drop(['IATA_CODE'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airports = pd.read_csv('../input/airport-and-airlines-for-us-flights-data-2008/airports.csv')\nairports.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"b785856d45df865bb55738cc3a55c49a5d442ce4","trusted":true},"cell_type":"code","source":"df = pd.merge(df,airports[['IATA_CODE','AIRPORT','CITY']], left_on='Origin', right_on = 'IATA_CODE')\ndf = df.drop(['IATA_CODE'], axis=1)\ndf = pd.merge(df,airports[['IATA_CODE','AIRPORT','CITY']], left_on='Dest', right_on = 'IATA_CODE')\ndf = df.drop(['IATA_CODE'], axis=1)\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualization and Description of US Flights Data"},{"metadata":{"_kg_hide-input":true,"_uuid":"e0385432f0db346fd75dbb8a28c2e2e9e8f94720","scrolled":false,"trusted":true},"cell_type":"code","source":"dff = df['AIRPORT_x'].value_counts()[:10]\nlabel = dff.index\nsize = dff.values\n\ncolors = ['skyblue', '#FEBFB3', '#96D38C', '#D0F9B1', 'gold', 'orange', 'lightgrey', \n          'lightblue','lightgreen','aqua']\ntrace = go.Pie(labels=label, values=size, marker=dict(colors=colors),hole = .2)\n\ndata = [trace]\nlayout = go.Layout(\n    title='Origin Airport Distribution'\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**By the pie chart above we know that Hartsfield-Jackson International Airport is the croudest one followed by Chicago O'Hare International Airport.**"},{"metadata":{"_kg_hide-input":true,"_uuid":"131a074bde551507481e00c50530ac0fda084ac5","scrolled":false,"trusted":true},"cell_type":"code","source":"dff = df.CITY_x.value_counts()[:10]\n\ntrace = go.Bar(\n    x=dff.index,\n    y=dff.values,\n    marker=dict(\n        color = dff.values,\n        colorscale='Jet',\n        showscale=True\n    )\n)\n\ndata = [trace]\nlayout = go.Layout(\n    title='Origin City Distribution', \n    yaxis = dict(title = '# of Flights')\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**By the distribution bar above we get that most flights fly from Chicago about 400k number of flights and folowed by Atlanta. The most quiet city is Detroit**"},{"metadata":{"_kg_hide-input":true,"_uuid":"92b639dcf9ce295bde1f4451aa3ea728a3e93255","trusted":true},"cell_type":"code","source":"dff = df.AIRLINE.value_counts()[:10]\n\ntrace = go.Bar(\n    x=dff.index,\n    y=dff.values,\n    marker=dict(\n        color = dff.values,\n        colorscale='Jet',\n        showscale=True)\n)\n\ndata = [trace]\nlayout = go.Layout(xaxis=dict(tickangle=15),\n    title='Airline distribution', \n                   yaxis = dict(title = '# of Flights'))\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**By the chart above, the most booked airlines by people is Southwest Airlines. It is twice as much as the second most booked airlines, American Airlines.** "},{"metadata":{"_kg_hide-input":true,"_uuid":"8ab47d4543556b48b5a795892f40f0391ee4d798","scrolled":false,"trusted":true},"cell_type":"code","source":"dff = df.Month.value_counts().to_frame().reset_index().sort_values(by='index')\ndff.columns = ['month', 'flight_num']\nmonth = {1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May',\n         6: 'Jun', 7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'}\ndff.month = dff.month.map(month)\n\ntrace = go.Bar(\n    x=dff.month,\n    y=dff.flight_num,\n    marker=dict(\n        color = dff.flight_num,\n        colorscale='Reds',\n        showscale=True)\n)\n\ndata = [trace]\nlayout = go.Layout(\n    title='# of Flights (monthly)', \n    yaxis = dict(title = '# of Flights'\n                                                )\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The distribution bar above provide information about the number of flights per month at 2018. The highest one is July. however, if we compare with others, there is no significant different number of flights. All months is about 610k flights.**"},{"metadata":{"_kg_hide-input":true,"_uuid":"c6ebfeb10e58bfb626c0c5258871f2635eb729fb","scrolled":false,"trusted":true},"cell_type":"code","source":"dayOfWeek={1:'Monday', 2:'Tuesday', 3:'Wednesday', 4:'Thursday', 5:'Friday', 6:'Saturday', 7:'Sunday'}\ndff = df.DayOfWeek.value_counts()\ndff = dff.to_frame().sort_index()\ndff.index = dff.index.map(dayOfWeek)\n\ntrace1 = go.Bar(\n    x=dff.index,\n    y=dff.DayOfWeek,\n    name = 'Weather',\n    marker=dict(\n        color = dff.DayOfWeek,\n        colorscale='Jet',\n        showscale=True\n    )\n)\n\ndata = [trace1]\nlayout = go.Layout(\n    title='# of Flights (Day of Week)', \n    yaxis = dict(title = '# of Flights'\n                                                    )\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**By chart above, Monday until Friday have the similar number of flights and the lowest one is Saturday**"},{"metadata":{"_kg_hide-input":true,"_uuid":"650f42299bcd411a12c8c9c3c65e1436e74b9810","scrolled":false,"trusted":true},"cell_type":"code","source":"df['dep_delay'] = np.where(df.DepDelay>0,1,0)\ndf['arr_delay'] = np.where(df.ArrDelay>0,1,0)\ndff = df.groupby('Month').dep_delay.mean().round(2)\n\ndff.index = dff.index.map(month)\ntrace1 = go.Bar(\n    x=dff.index,\n    y=dff.values,\n    name = 'Departure_delay',\n    marker = dict(\n        color = 'aqua'\n    )\n)\n\ndff = df.groupby('Month').arr_delay.mean().round(2)\ndff.index = dff.index.map(month)\n\ntrace2 = go.Bar(\n    x=dff.index,\n    y=dff.values,\n    name='Arrival_delay',\n    marker=dict(\n        color = 'red'\n    )\n)\n\ndata = [trace1,trace2]\nlayout = go.Layout(\n    title='% Delay (Months)', \n    yaxis = dict(title = '%')\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**By chart above Arrival Delay is more than that of Departure Delay. And both occured in December 2008** "},{"metadata":{"_kg_hide-input":true,"_uuid":"e7f31ffa67fa05009e1b4bc9b69062dbc7080faa","scrolled":false,"trusted":true},"cell_type":"code","source":"dff = df.groupby('DayOfWeek').DepDelay.mean().round(2)\ndff.index = dff.index.map(dayOfWeek)\n\ntrace1 = go.Bar(\n    x=dff.index,\n    y=dff.values,\n    name = 'Departure_delay',\n    marker=dict(\n        color = 'cyan'\n    )\n)\n\ndff = df.groupby('DayOfWeek').ArrDelay.mean().round(2)\ndff.index = dff.index.map(dayOfWeek)\n\ntrace2 = go.Bar(\n    x=dff.index,\n    y=dff.values,\n    name='Arrival_delay',\n    marker=dict(\n        color = 'indigo'\n    )\n)\n\ndata = [trace1,trace2]\nlayout = go.Layout(\n    title='% Delay (Day of Week)', \n    yaxis = dict(title = '%')\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**By chart above, most of dalay occured at Friday and Sunday. However, different with by month which is the number of Arrival Delay is more than that Departure Delay, when we compare by day of week, the number of Departure Delay is more than that of Arrival Delay.**"},{"metadata":{"_kg_hide-input":true,"_uuid":"e4867ace433dacb3f556eae1bb3c3ba350832436","trusted":true},"cell_type":"code","source":"flight_volume = df.pivot_table(index=\"CITY_x\",columns=\"DayOfWeek\",\n                               values=\"DayofMonth\",aggfunc=lambda x:x.count())\nfv = flight_volume.sort_values(by=1, ascending=False)[:7]\nfv = fv.iloc[::-1]\n\nfig = plt.figure(figsize=(16,9))\nsns.heatmap(fv, cmap='RdBu',linecolor=\"w\", linewidths=2)\n\nplt.title('Air Traffic by Cities',size=16)\nplt.ylabel('CITY')\nplt.xticks(rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**By the headmap above, we get that at Saturday, many people in Atlanta and Chicago prefer to not leave their city. Different with other cities where most of their people choose to leave at Saturday.**"},{"metadata":{"_uuid":"a07fb3bf3fc7f9c429cfaec73346f61da761018d"},"cell_type":"markdown","source":"\n## AIRLINES COMPARISON\n\n\n\n### 1. Arrival & Departure Delay\n"},{"metadata":{"_kg_hide-input":true,"_uuid":"cdfb3bcfa46fccc31868ee2faf46601b64a80e7b","scrolled":false,"trusted":true},"cell_type":"code","source":"dff = df.groupby('AIRLINE').DepDelay.mean().to_frame().sort_values(by='DepDelay',\n                                                    ascending=False).round(2)\ntrace1 = go.Bar(\n    x=dff.index,\n    y=dff.DepDelay,\n    name='departure_delay',\n    marker=dict(\n        color = 'navy'\n    )\n)\n\ndff = df.groupby('AIRLINE').ArrDelay.mean().to_frame().sort_values(by='ArrDelay',\n                                                    ascending=False).round(2)\ntrace2 = go.Bar(\n    x=dff.index,\n    y=dff.ArrDelay,\n    name='arrival_delay',\n    marker=dict(\n        color = 'red'\n    )\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(xaxis=dict(tickangle=15), title='Mean Arrival & Departure Delay by Airlines',\n    yaxis = dict(title = 'minute'), \n                   barmode='stack')\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3844ead866ec0989fe114eef4d461a06b25955f"},"cell_type":"markdown","source":"**As we expect the number of the mean of departure delay id higher than the mean of arrival delay. Except by some airlines as we can see clearly at the chart below.**"},{"metadata":{"_kg_hide-input":true,"_uuid":"494300d14e988b0dc096d524d984ef119a21f788","trusted":true},"cell_type":"code","source":"df['DEP_ARR_DIFF'] = df['DepDelay'] - df['ArrDelay']\ndff = df.groupby('AIRLINE').DEP_ARR_DIFF.mean().to_frame().sort_values(by='DEP_ARR_DIFF',\n                                                    ascending=False).round(2)\n\ntrace = go.Bar(\n    x=dff.index,\n    y=dff.DEP_ARR_DIFF,\n    marker=dict(\n        color = dff.DEP_ARR_DIFF,\n        colorscale='Jet',\n        showscale=True\n    )\n)\n\ndata = [trace]\nlayout = go.Layout(xaxis=dict(tickangle=15),\n    title='Mean (Departure Delay - Arrival Delay) by Airlines', \n                   yaxis = dict(title = 'minute')\n                  )\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac68970886e68e69898c20dbf084bde24e19d605"},"cell_type":"markdown","source":"**We can see that there are four airlines that their number of arrival delay is higher than that departure delay: Frontier Airlines, PASA Airlines, Hawaiian Airlines, and Spirit Airlines.**"},{"metadata":{"_uuid":"22f7db75bc7b959b660b555021ff86cc921d1669"},"cell_type":"markdown","source":"## 2. Taxi Out vs. Taxi In"},{"metadata":{"_uuid":"6656890e9d147468c503ff67e3e8a41e31f52820"},"cell_type":"markdown","source":"#### Reminder: ####\n- taxi_out = wheels_off - departure_time\n- taxi_in = arrival_time - wheels_on"},{"metadata":{"_kg_hide-input":true,"_uuid":"f8fe3f058abea838ce0ec8703553adc5a6148d62","trusted":true},"cell_type":"code","source":"dff = df.groupby('AIRLINE').TaxiOut.mean().to_frame().sort_values(by='TaxiOut',\n                                                    ascending=False)[:8].round(2)\n\ntrace1 = go.Bar(\n    x=dff.index,\n    y=dff.TaxiOut,name='TAXI_OUT',\n    marker=dict(\n        color = 'aqua'\n    )\n)\n\ndff = df.groupby('AIRLINE').TaxiIn.mean().to_frame().sort_values(by='TaxiIn',\n                                                        ascending=False)[:8].round(2)\n\ntrace2 = go.Bar(\n    x=dff.index,\n    y=dff.TaxiIn, name='TAXI_IN',\n    marker=dict(\n       color = 'indigo'\n    )\n)\n\nfig = tools.make_subplots(rows=1, cols=2, subplot_titles=('Median Taxi Out', 'Median Taxi In'))\n\nfig.append_trace(trace1, 1,1)\nfig.append_trace(trace2, 1,2)\n\nfig['layout'].update(yaxis = dict(title = 'minute'), height=500, width=850, \n                     title='Which is hard whell-off or whell-on?',  \n                     showlegend=False)               \npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"b0f49ee5ba7b3e237465a6fd527c13fdfe4bf110","trusted":true},"cell_type":"code","source":"df['OUT_IN_DIFF'] = df['TaxiOut'] - df['TaxiIn']\ndff = df.groupby('AIRLINE').OUT_IN_DIFF.mean().to_frame().sort_values(by='OUT_IN_DIFF',\n                                                    ascending=False).round(2)\n\ntrace = go.Bar(\n    x=dff.index,\n    y=dff.OUT_IN_DIFF,\n    marker=dict(\n        color = dff.OUT_IN_DIFF,\n        colorscale='Jet',\n        showscale=True\n    )\n)\n\ndata = [trace]\nlayout = go.Layout(xaxis=dict(tickangle=15),\n    title='Mean (Taxi Out - Taxi In) by Airlines', \n                   yaxis = dict(title = 'minute'\n                                                               )\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**By both chart above, we know that all airlines spend most of their time in air rather than that on airport**."},{"metadata":{"_uuid":"c930e84d3d92ec34e2e61ddc5a4486caf3c7214a"},"cell_type":"markdown","source":"### 3. Speed"},{"metadata":{"_kg_hide-input":true,"_uuid":"1653b92d310f147f0dceaa90d2ba52d1af7ba5ac","trusted":true},"cell_type":"code","source":"df['SPEED'] = 60*df['Distance']/df['AirTime']\ndff = df.groupby('AIRLINE').SPEED.mean().to_frame().sort_values(by='SPEED',\n                                                    ascending=False).round(2)\n\ntrace = go.Scatter(\n    x=dff.index,\n    y=dff.SPEED,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 30,\n        color = dff.SPEED.values,\n        colorscale='Jet',\n        showscale=True\n    )\n)\n\ndata = [trace]\nlayout = go.Layout(xaxis=dict(tickangle=-20),\n    title='Mean Speed by Airlines', \n                   yaxis = dict(title = 'Speed')\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**By the scatter chart, we get that American Eagle Airlines, Hawaiian Airlines, and Mesa Airlines are speedest speed flight at 2018. And the slowest one is American Airlines.**"},{"metadata":{"_uuid":"c05b10fac9b7a2a397faa2e4c255644f0685134a"},"cell_type":"markdown","source":"### 4. Cancellation Rates"},{"metadata":{"_kg_hide-input":true,"_uuid":"eaf547d03a647cd4200532691558228cdb46933b","trusted":true},"cell_type":"code","source":"dff = df.groupby('AIRLINE')[['Cancelled']].mean().sort_values(by='Cancelled', \n                                                    ascending=False).round(3)\n\ntrace1 = go.Scatter(\n    x=dff.index,\n    y=dff.Cancelled,\n    mode='markers',\n    marker=dict(\n        symbol = 'star-square',\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 30,\n        color = dff.Cancelled,\n        colorscale='Portland',\n        showscale=True\n    )\n)\n\ndata = [trace1]\nlayout = go.Layout(xaxis=dict(tickangle=20),\n    title='Cancellation Rate by Airlines', yaxis = dict(title = 'Cancellation Rate'\n                                                       )\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"age\")\n\ndff = df.groupby('CITY_x')[['Cancelled']].mean().sort_values(by='Cancelled', \n                                            ascending=False)[:10].round(3)\ntrace2 = go.Scatter(\n    x=dff.index,\n    y=dff.Cancelled,\n    mode='markers',\n    marker=dict(symbol = 'diamond',\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 30,\n        color = dff.Cancelled,\n        colorscale='Portland',\n        showscale=True\n    )\n)\n\ndata = [trace2]\nlayout = go.Layout(xaxis=dict(tickangle=20),\n    title='Cancellation Rate by Cities', \n                   yaxis = dict(title = 'Cancellation Rate'\n                                                     )\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**American Eagle Airlines and Mesa Airlines are the speedest, however they both also are the airlines with the highest cancelation rate. Besides that, if we compared by city, Waterloo is the city with the highest cancelation rate followed by Hancock. The lowest one are Beamount and Dubuque.**"},{"metadata":{"_uuid":"ab8869ec7794653a273373f810536f17a7466a18"},"cell_type":"markdown","source":"### 5. Cancellation Reasons\n*Do you wonder why flights are cancelled wheather, security or anything else?*"},{"metadata":{"_kg_hide-input":true,"_uuid":"0ad91a0ba04934c6e9a6d4a4b208605bdcfbc8e5","trusted":true},"cell_type":"code","source":"reason={'A':'Airline/Carrier', 'B':'Weather', 'C':'National Air System', 'D':'Security'}\ndf.CancellationCode = df.CancellationCode.map(reason)\n\ndff = df[df.Cancelled==1]['Month'].value_counts().reset_index().sort_values(by='index')\ndff.columns = ['month', 'flight_num']\ndff.month = dff.month.map(month)\n\ntrace = go.Bar(\n    x=dff.month,\n    y=dff.flight_num,\n    marker=dict(\n        color = dff.flight_num,\n        colorscale='Reds',\n        showscale=True\n    )\n)\n\ndata = [trace]\nlayout = go.Layout(\n    title='# of Cancelled Flights (monthly)', \n    yaxis = dict(title = '# of Flights'\n                                                          )\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"ec5893b00ca7217cfe8d0dcd7a773dd3f6189ec1","trusted":true},"cell_type":"code","source":"dff = df[df.CancellationCode=='Weather'].Month.value_counts()\ndff = dff.to_frame().sort_index()\ndff.index = dff.index.map(month)\n\ntrace1 = go.Bar(\n    x=dff.index,\n    y=dff.Month,\n    name = 'Weather',\n    marker=dict(\n        color = 'aqua'\n    )\n)\n\ndff = df[df.CancellationCode=='Airline/Carrier'].Month.value_counts()\ndff = dff.to_frame().sort_index()\ndff.index = dff.index.map(month)\n\ntrace2 = go.Bar(\n    x=dff.index,\n    y=dff.Month,\n    name='Airline/Carrier',\n    marker=dict(\n        color = 'red'\n    )\n)\n\ndff = df[df.CancellationCode=='National Air System'].Month.value_counts()\ndff = dff.to_frame().sort_index()\ndff.index = dff.index.map(month)\n\ntrace3 = go.Bar(\n    x=dff.index,\n    y=dff.Month,\n    name='National Air System',\n    marker=dict(\n        color = 'navy'\n    )\n)\n\ndata = [trace1,trace2,trace3]\nlayout = go.Layout(\n    title='Cancellation Reasons (Monthly)', \n    yaxis = dict(title = '# of Flights'\n                                                        )\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80524fdb06d7862baf6fe0c69795378c4ac9f9af"},"cell_type":"markdown","source":"**The highest number of cancelation occured in Dec, Jan, Feb, and March and most of them happened due to weather followed by Airline/Carrier. On May until Nov except Sep, Ailine/Carrier becomes the most coused factor of delay.**"},{"metadata":{"_kg_hide-input":true,"_uuid":"bc1ceb6bc13bcfd4e2d1630c95d873417255ee16","trusted":true},"cell_type":"code","source":"dff = df[df.CancellationCode == 'Weather'].DayOfWeek.value_counts()\ndff = dff.to_frame().sort_index()\ndff.index = dff.index.map(dayOfWeek)\n\ntrace1 = go.Bar(\n    x=dff.index,\n    y=dff.DayOfWeek,\n    name = 'Weather',\n    marker=dict(\n        color = 'aqua'\n    )\n)\n\ndff = df[df.CancellationCode=='Airline/Carrier'].DayOfWeek.value_counts()\ndff = dff.to_frame().sort_index()\ndff.index = dff.index.map(dayOfWeek)\n\ntrace2 = go.Bar(\n    x=dff.index,\n    y=dff.DayOfWeek,\n    name='Airline/Carrier',\n    marker=dict(\n        color = 'red'\n    )\n)\n\ndff = df[df.CancellationCode=='National Air System'].DayOfWeek.value_counts()\ndff = dff.to_frame().sort_index()\ndff.index = dff.index.map(dayOfWeek)\n\ntrace3 = go.Bar(\n    x=dff.index,\n    y=dff.DayOfWeek,\n    name='National Air System',\n    marker=dict(\n        color = 'navy'\n    )\n)\n\ndata = [trace1,trace2,trace3]\nlayout = go.Layout(\n    title='Cancellation Reasons (Day of Week)', \n    yaxis = dict(title = '# of Flights'\n                                                            )\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Also if we compared by day of week, weather and Airline/Carrier become the most caused factor of delay.** "},{"metadata":{"_kg_hide-input":true,"_uuid":"6fbde2fdce06acbacb7d6712528dcd2a36f21bce","trusted":true},"cell_type":"code","source":"correlation = df[['DayOfWeek','Month','ActualElapsedTime', 'AirTime', 'Distance',\n       'TaxiIn', 'CRSArrTime', 'ArrTime',\n       'ArrDelay','SPEED']].fillna(0).corr()\ncols = correlation.columns.values\ncorr  = correlation.values\n\ntrace = go.Heatmap(z = corr,\n                   x = cols,\n                   y = cols,\n                   colorscale = \"YlOrRd\",reversescale = True\n                                    ) \n\ndata = [trace]\nlayout = go.Layout(dict(title = \"Correlation Matrix for variables\",\n                        autosize = False,\n                        height  = 600,\n                        width   = 800,\n                        margin  = dict(l = 200\n                                      ),\n                        yaxis   = dict(tickfont = dict(size = 8)),\n                        xaxis   = dict(tickfont = dict(size = 8))\n                       )\n                  )\n\nfig = go.Figure(data=data,layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b1e3fc4094a567bfcecce00e318719f113ed7c3"},"cell_type":"markdown","source":"**By the heatmap, Distance, ActualElapsedTIme, and Airtime have high correlation and ArrTIme has high corelation with CRSArrTime.**\n\n\n\n**Next we plot the map of airport.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"airports=airports[airports.IATA_CODE.isin(df.Origin.unique())]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_flights = df['Origin'].value_counts()\n#___________________________\nplt.figure(figsize=(15,15))\n#________________________________________\n# define properties of markers and labels\ncolors = ['yellow', 'red', 'lightblue', 'purple', 'green', 'orange']\nsize_limits = [1, 100, 1000, 10000, 100000, 1000000]\nlabels = []\nfor i in range(len(size_limits)-1):\n    labels.append(\"{} <.< {}\".format(size_limits[i], size_limits[i+1])) \n#____________________________________________________________\nmap = Basemap(resolution='i',llcrnrlon=-180, urcrnrlon=-50,\n              llcrnrlat=10, urcrnrlat=75, lat_0=0, lon_0=0,)\nmap.shadedrelief()\nmap.drawcoastlines()\nmap.drawcountries(linewidth = 3)\nmap.drawstates(color='0.3')\n#_____________________\n# put airports on map\nfor index, (code, y,x) in airports[['IATA_CODE', 'LATITUDE', 'LONGITUDE']].iterrows():\n    x, y = map(x, y)\n    isize = [i for i, val in enumerate(size_limits) if val < count_flights[code]]\n    ind = isize[-1]\n    map.plot(x, y, marker='o', markersize = ind+5, markeredgewidth = 1, color = colors[ind],\n             markeredgecolor='k', label = labels[ind])\n#_____________________________________________\n# remove duplicate labels and set their order\nhandles, labels = plt.gca().get_legend_handles_labels()\nby_label = OrderedDict(zip(labels, handles))\nkey_order = ('1 <.< 100', '100 <.< 1000', '1000 <.< 10000',\n             '10000 <.< 100000', '100000 <.< 1000000')\nnew_label = OrderedDict()\nfor key in key_order:\n    new_label[key] = by_label[key]\nplt.legend(new_label.values(), new_label.keys(), loc = 1, prop= {'size':11},\n           title='Number of flights per year', frameon = True, framealpha = 1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"___\n## 1. Cleaning\n___\n### 1.1 Dates and times\n\nIn the initial dataframe, dates are coded according to 4 variables: **Year, Month, DayofMonth**, and **DayOfWeek**. In fact, python offers the **_datetime_** format which is really convenient to work with dates and times and I thus convert the dates in this format:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Date'] = pd.to_datetime(df.Year.map(str)+'-'+df.Month.map(str)+'-'+df.DayofMonth.map(str))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#_________________________________________________________\n# Function that convert the 'HHMM' string to datetime.time\ndef format_heure(chaine):\n    if pd.isnull(chaine):\n        return np.nan\n    else:\n        if chaine == 2400: chaine = 0\n        chaine = \"{0:04d}\".format(int(chaine))\n        heure = datetime.time(int(chaine[0:2]), int(chaine[2:4]))\n        return heure\n#_____________________________________________________________________\n# Function that combines a date and time to produce a datetime.datetime\ndef combine_date_heure(x):\n    if pd.isnull(x[0]) or pd.isnull(x[1]):\n        return np.nan\n    else:\n        return datetime.datetime.combine(x[0],x[1])\n#_______________________________________________________________________________\n# Function that combine two columns of the dataframe to create a datetime format\ndef create_flight_time(df, col):    \n    liste = []\n    for index, cols in df[['Date', col]].iterrows():    \n        if pd.isnull(cols[1]):\n            liste.append(np.nan)\n        elif float(cols[1]) == 2400:\n            cols[0] += datetime.timedelta(days=1)\n            cols[1] = datetime.time(0,0)\n            liste.append(combine_date_heure(cols))\n        else:\n            cols[1] = format_heure(cols[1])\n            liste.append(combine_date_heure(cols))\n    return pd.Series(liste)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['CRSDepTime'] = create_flight_time(df, 'CRSDepTime')\ndf['DepTime'] = df['DepTime'].apply(format_heure)\ndf['CRSArrTime'] = df['CRSArrTime'].apply(format_heure)\ndf['ArrTime'] = df['ArrTime'].apply(format_heure)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[:5, ['CRSDepTime', 'CRSArrTime', 'DepTime',\n             'ArrTime', 'DepDelay', 'ArrDelay']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.2 Filling factor\n\nNow, we clean all columns that will not be used."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[['UniqueCarrier', 'Origin', 'Dest',\n        'CRSDepTime', 'DepTime', 'DepDelay',\n        'CRSArrTime', 'ArrTime', 'ArrDelay',\n        'CRSElapsedTime', 'AirTime']]\ndf[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_df = df.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['variable', 'missing values']\nmissing_df['filling factor (%)']=(df.shape[0]-missing_df['missing values'])/df.shape[0]*100\nmissing_df.sort_values('filling factor (%)').reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the variables filling factor is quite good (> 97%). Since the scope of this work is not to establish the state-of-the-art in predicting flight delays, I decide to proceed without trying to impute what's missing and I simply remove the entries that contain missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"___\n## 2. Comparing airlines\n"},{"metadata":{},"cell_type":"markdown","source":"As said earlier, the **AIRLINE** variable contains the airline abreviations. Now we put it to dictionary."},{"metadata":{"trusted":true},"cell_type":"code","source":"abbr_companies = airlines_names.set_index('IATA_CODE')['AIRLINE'].to_dict()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"___\n### 2.1 Basic statistical description of airlines\n\nAs a first step, we calculate some basic statistical information."},{"metadata":{"trusted":true},"cell_type":"code","source":"#__________________________________________________________________\n# function that extract statistical parameters from a grouby objet:\ndef get_stats(group):\n    return {'min': group.min(), 'max': group.max(),\n            'count': group.count(), 'mean': group.mean()}\n#_______________________________________________________________\n# Creation of a dataframe with statitical infos on each airline:\nglobal_stats = df['DepDelay'].groupby(df['UniqueCarrier']).apply(get_stats).unstack()\nglobal_stats = global_stats.sort_values('count')\nglobal_stats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, in order to facilitate the lecture of that information, we construct some graphics:"},{"metadata":{"trusted":true},"cell_type":"code","source":"font = {'family' : 'DejaVu Sans', 'weight' : 'bold', 'size'   : 15}\nmpl.rc('font', **font)\nimport matplotlib.patches as mpatches\n#__________________________________________________________________\n# I extract a subset of columns and redefine the airlines labeling \ndf2 = df.loc[:, ['UniqueCarrier', 'DepDelay']]\ndf2['UniqueCarrier'] = df2['UniqueCarrier'].replace(abbr_companies)\n#________________________________________________________________________\ncolors = ['royalblue', 'grey', 'wheat', 'c', 'firebrick', 'seagreen', 'lightskyblue','blue','yellow','red','green','brown','pink',\n          'lightcoral', 'yellowgreen', 'gold', 'tomato', 'violet', 'aquamarine', 'chartreuse']\n#___________________________________\nfig = plt.figure(1, figsize=(16,15))\ngs=GridSpec(2,2)             \nax1=fig.add_subplot(gs[0,0]) \nax2=fig.add_subplot(gs[0,1]) \nax3=fig.add_subplot(gs[1,:]) \n#------------------------------\n# Pie chart nº1: nb of flights\n#------------------------------\nlabels = [s for s in  global_stats.index]\nsizes  = global_stats['count'].values\nexplode = [0.3 if sizes[i] < 20000 else 0.0 for i in range(len(abbr_companies))]\npatches, texts, autotexts = ax1.pie(sizes, explode = explode,\n                                labels=labels, colors = colors,  autopct='%1.0f%%',\n                                shadow=False, startangle=0)\nfor i in range(len(abbr_companies)): \n    texts[i].set_fontsize(14)\nax1.axis('equal')\nax1.set_title('% of flights per company', bbox={'facecolor':'midnightblue', 'pad':5},\n              color = 'w',fontsize=18)\n#_______________________________________________\n# I set the legend: abreviation -> airline name\ncomp_handler = []\nfor i in range(len(abbr_companies)):\n    comp_handler.append(mpatches.Patch(color=colors[i],\n            label = global_stats.index[i] + ': ' + abbr_companies[global_stats.index[i]]))\nax1.legend(handles=comp_handler, bbox_to_anchor=(0.2, 0.9), \n           fontsize = 13, bbox_transform=plt.gcf().transFigure)\n#----------------------------------------\n# Pie chart nº2: mean delay at departure\n#----------------------------------------\nsizes  = global_stats['mean'].values\nsizes  = [max(s,0) for s in sizes]\nexplode = [0.0 if sizes[i] < 20000 else 0.01 for i in range(len(abbr_companies))]\npatches, texts, autotexts = ax2.pie(sizes, explode = explode, labels = labels,\n                                colors = colors, shadow=False, startangle=0,\n                                autopct = lambda p :  '{:.0f}'.format(p * sum(sizes) / 100))\nfor i in range(len(abbr_companies)): \n    texts[i].set_fontsize(14)\nax2.axis('equal')\nax2.set_title('Mean delay at origin', bbox={'facecolor':'midnightblue', 'pad':5},\n              color='w', fontsize=18)\n#------------------------------------------------------\n# striplot with all the values reported for the delays\n#___________________________________________________________________\n# I redefine the colors for correspondance with the pie charts\ncolors = ['firebrick', 'gold', 'lightcoral', 'aquamarine', 'c', 'yellowgreen', 'grey','blue','yellow','red','green','brown','pink',\n          'seagreen', 'tomato', 'violet', 'wheat', 'chartreuse', 'lightskyblue', 'royalblue']\n#___________________________________________________________________\nax3 = sns.stripplot(y=\"UniqueCarrier\", x=\"DepDelay\", size = 4, palette = colors,\n                    data=df2, linewidth = 0.5,  jitter=True)\nplt.setp(ax3.get_xticklabels(), fontsize=14)\nplt.setp(ax3.get_yticklabels(), fontsize=14)\nax3.set_xticklabels(['{:2.0f}h{:2.0f}m'.format(*[int(y) for y in divmod(x,60)])\n                         for x in ax3.get_xticks()])\nplt.xlabel('Departure delay', fontsize=18, bbox={'facecolor':'midnightblue', 'pad':5},\n           color='w', labelpad=20)\nax3.yaxis.label.set_visible(False)\n#________________________\nplt.tight_layout(w_pad=3) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Considering the first pie chart that gives the percentage of flights per airline, we see that there is some disparity between the carriers. For exemple, *Southwest Airlines* accounts for $\\sim$17% of the flights which is similar to the number of flights chartered by the 7 tiniest airlines. However, if we have a look at the second pie chart, we see that here, on the contrary, the differences among airlines are less pronounced. The delay is only between  **$\\sim$9$\\pm$4 minutes** . The higest one is United Airlines.\n\nFinally, the figure at the bottom makes a census of all the delays. This representation gives a feeling on the dispersion of data and put in perspective the relative homogeneity that appeared in the second pie chart. Indeed, we see that while all mean delays are around 10 minutes, this low value is a consequence of the fact that a majority of flights take off on time. However, we see that occasionally, we can face really large delays that can reach a few tens of hours!\n\nThe large majority of short delays is visible in the next figure:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#_____________________________________________\n# Function that define how delays are grouped\ndelay_type = lambda x:((0,1)[x > 5],2)[x > 45]\ndf['DelayLevel'] = df['DepDelay'].apply(delay_type)\n#____________________________________________________\nfig = plt.figure(1, figsize=(10,7))\nax = sns.countplot(y=\"UniqueCarrier\", hue='DelayLevel', data=df)\n#____________________________________________________________________________________\n# We replace the abbreviations by the full names of the companies and set the labels\nlabels = [abbr_companies[item.get_text()] for item in ax.get_yticklabels()]\nax.set_yticklabels(labels)\nplt.setp(ax.get_xticklabels(), fontsize=12, weight = 'normal', rotation = 0);\nplt.setp(ax.get_yticklabels(), fontsize=12, weight = 'bold', rotation = 0);\nax.yaxis.label.set_visible(False)\nplt.xlabel('Flight count', fontsize=16, weight = 'bold', labelpad=10)\n#________________\n# Set the legend\nL = plt.legend()\nL.get_texts()[0].set_text('on time (t < 5 min)')\nL.get_texts()[1].set_text('small delay (5 < t < 45 min)')\nL.get_texts()[2].set_text('large delay (t > 45 min)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This figure gives a count of the delays of less than 5 minutes, those in the range 5 < t < 45 min and finally, the delays greater than 45 minutes. Hence, we wee that independently of the airline, delays greater than 45 minutes only account for a few percents. However, the proportion of delays in these three groups depends on the airline: as an exemple, in the case of \n*SkyWest Airlines*, the delays greater than 45 minutes are only lower by $\\sim$30% with respect to delays in the range 5 < t < 45 min. Things are better for *SoutWest Airlines*  since delays greater than 45 minutes are than 3 times less frequent than delays in the range 5 < t < 45 min.\n"},{"metadata":{},"cell_type":"markdown","source":"### 2.2 Delays distribution: establishing the ranking of airlines\n\nIt was shown in the previous section that mean delays behave homogeneously among airlines (apart from two extrem cases) and is around $\\sim$9$\\pm$4 minutes. Then, we saw that this low value is a consequence of the large proportion of flights that take off on time. However, occasionally, large delays can be registred. In this section, I examine more in details the distribution of delays for every airlines:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#___________________________________________\n# Model function used to fit the histograms\ndef func(x, a, b):\n    return a * np.exp(-x/b)\n#-------------------------------------------\npoints = [] ; label_company = []\nfig = plt.figure(1, figsize=(20,20))\ni = 0\nfor carrier_name in [abbr_companies[x] for x in global_stats.index]:\n    i += 1\n    ax = fig.add_subplot(5,4,i)    \n    #_________________________\n    # Fit of the distribution\n    n, bins, patches = plt.hist(x = df2[df2['UniqueCarrier']==carrier_name]['DepDelay'],\n                                range = (15,180), normed=True, bins= 60)\n    bin_centers = bins[:-1] + 0.5 * (bins[1:] - bins[:-1])    \n    popt, pcov = curve_fit(func, bin_centers, n, p0 = [1, 2])\n    #___________________________\n    # bookeeping of the results\n    points.append(popt)\n    label_company.append(carrier_name)\n    #______________________\n    # draw the fit curve\n    plt.plot(bin_centers, func(bin_centers, *popt), 'r-', linewidth=3)    \n    #_____________________________________\n    # define tick labels for each subplot\n    if i < 10:\n        ax.set_xticklabels(['' for x in ax.get_xticks()])\n    else:\n        ax.set_xticklabels(['{:2.0f}h{:2.0f}m'.format(*[int(y) for y in divmod(x,60)])\n                            for x in ax.get_xticks()])\n    #______________\n    # subplot title\n    plt.title(carrier_name, fontsize = 14, fontweight = 'bold', color = 'darkblue')\n    plt.xlabel('Delay at origin', fontsize = 14, fontweight = 'bold', color = 'k')\n    plt.ylabel('Normalized count of flights', fontsize = 14, fontweight = 'bold', color = 'k')\n    #____________\n    # axes labels \n    #if i == 5:\n    #    ax.text(-0.3,0.9,'Normalized count of flights', fontsize=16, rotation=90,\n    #        color='k', horizontalalignment='center', transform = ax.transAxes)\n    #if i == 19:\n    #    ax.text( 0.5, -0.5 ,'Delay at origin', fontsize=16, rotation=0,\n    #        color='k', horizontalalignment='center', transform = ax.transAxes)\n    #___________________________________________\n    # Legend: values of the a and b coefficients\n    ax.text(0.68, 0.7, 'a = {}\\nb = {}'.format(round(popt[0],2), round(popt[1],1)),\n            style='italic', transform=ax.transAxes, fontsize = 12, family='fantasy',\n            bbox={'facecolor':'tomato', 'alpha':0.8, 'pad':5})\n    \nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This figure shows the normalised distribution of delays that I modelised with an exponential distribution $ f(x) = a \\, \\mathrm{exp} (-x/b)$. The $a$ et $b$ parameters obtained to describe each airline are given in the upper right corner of each panel. Note that the normalisation of the distribution implies that $\\int f(x) \\, dx \\sim 1$. Here, we do not have a strict equality since the normalisation applies the histograms but not to the model function. However, this relation entails that the $a$ et $b$ coefficients will be correlated with $a \\propto 1/b$ and hence, only one of these two values is necessary to describe the distributions. Finally, according to the value of either $a$ or $b$, it is possible to establish a ranking of the companies: the low values of $a$ will correspond to airlines with a large proportion of important delays and, on the contrary, airlines that shine from their punctuality will admit hight $a$ values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"mpl.rcParams.update(mpl.rcParamsDefault)\nsns.set_context('paper')\nimport matplotlib.patches as patches\n\nfig = plt.figure(1, figsize=(11,5))\ny_shift = [0 for _ in range(20)]\ny_shift[3] = 0.5/1000\ny_shift[12] = 2.5/1000\ny_shift[11] = -0.5/1000\ny_shift[8] = -2.5/1000\ny_shift[5] = 1/1000\nx_val = [s[1] for s in points]\ny_val = [s[0] for s in points]\n\ngs=GridSpec(2,7)\n#_______________________________\n# 1/ Plot overview (left panel)\nax1=fig.add_subplot(gs[1,0:2]) \nplt.scatter(x=x_val, y=y_val, marker = 's', edgecolor='black', linewidth = '1')\n#__________________________________\n# Company label: Hawaiian airlines\ni= 1\nax1.annotate(label_company[i], xy=(x_val[i]+1.5, y_val[i]+y_shift[i]),\n             xycoords='data', fontsize = 10)\nplt.xlabel(\"$b$ parameter\", fontsize=16, labelpad=20)\nplt.ylabel(\"$a$ parameter\", fontsize=16, labelpad=20)\n#__________________________________\n# Company label: Hawaiian airlines\ni= 12\nax1.annotate(label_company[i], xy=(x_val[i]+1.5, y_val[i]+y_shift[i]),\n             xycoords='data', fontsize = 10)\nplt.xlabel(\"$b$ parameter\", fontsize=16, labelpad=20)\nplt.ylabel(\"$a$ parameter\", fontsize=16, labelpad=20)\n#____________\n# Main Title\nax1.text(.5,1.5,'Characterizing delays \\n among companies', fontsize=16,\n        bbox={'facecolor':'midnightblue', 'pad':5}, color='w',\n        horizontalalignment='center',\n        transform=ax1.transAxes)\n#________________________\n# plot border parameters\nfor k in ['top', 'bottom', 'right', 'left']:\n    ax1.spines[k].set_visible(True)\n    ax1.spines[k].set_linewidth(0.5)\n    ax1.spines[k].set_color('k')\n#____________________\n# Create a Rectangle \nrect = patches.Rectangle((21,0.025), 19, 0.07, linewidth=2,\n                         edgecolor='r', linestyle=':', facecolor='none')\nax1.add_patch(rect)\n#_______________________________________________\n# 2/ Zoom on the bulk of carriers (right panel)\nax2=fig.add_subplot(gs[0:2,2:])\nplt.scatter(x=x_val, y=y_val, marker = 's', edgecolor='black', linewidth = '1')\nplt.setp(ax1.get_xticklabels(), fontsize=12)\nplt.setp(ax1.get_yticklabels(), fontsize=12)\nax2.set_xlim(21,45)\nax2.set_ylim(0.025,0.095)\n#________________\n# Company labels\nfor i in range(len(abbr_companies)):\n    ax2.annotate(label_company[i], xy=(x_val[i]+0.5, y_val[i]+y_shift[i]),\n                 xycoords='data', fontsize = 10)\n#____________________________\n# Increasing delay direction\nax2.arrow(30, 0.09, 8, -0.03, head_width=0.005,\n          shape = 'full', head_length=2, fc='k', ec='k')\nax2.annotate('increasing \\n  delays', fontsize= 20, color = 'r',\n          xy=(35, 0.075), xycoords='data')\n#________________________________\n# position and size of the ticks\nplt.tick_params(labelleft=False, labelright=True)\nplt.setp(ax2.get_xticklabels(), fontsize=14)\nplt.setp(ax2.get_yticklabels(), fontsize=14)\n#________________________\n# plot border parameters\nfor k in ['top', 'bottom', 'right', 'left']:\n    ax2.spines[k].set_visible(True)\n    ax2.spines[k].set_linewidth(0.5)\n    ax2.spines[k].set_color('k')    \n#________________________________\n# Connection between the 2 plots\nxy2 = (40, 0.09) ; xy1 = (21, 0.095)\ncon = ConnectionPatch(xyA=xy1, xyB=xy2, coordsA=\"data\", coordsB=\"data\",\n                      axesA=ax2, axesB=ax1,\n                      linestyle=':', linewidth = 2, color=\"red\")\nax2.add_artist(con)\nxy2 = (40, 0.025) ; xy1 = (21, 0.025)\ncon = ConnectionPatch(xyA=xy1, xyB=xy2, coordsA=\"data\", coordsB=\"data\",\n                      axesA=ax2, axesB=ax1,\n                      linestyle=':', linewidth = 2, color=\"red\")\nax2.add_artist(con)\nplt.xlabel(\"$b$ parameter\", fontsize=16, labelpad=20)\n#--------------------------------\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The left panel of this figure gives an overview of the $a$ and $b$ coefficients of the 20 airlines showing that *Hawaiian Airlines* and *Frontier Airlnes* occupy the first two places. The right panel zooms on 18 other airlines. We can see that *SouthWest Airlines* and *Delta Airlines*, which represent $\\sim$20% of the total number of flights is well ranked and occupy the third position. According to this ranking, *United Airlines* is the worst carrier."},{"metadata":{},"cell_type":"markdown","source":"___\n## 3. Delays: take-off or landing ?\nIn the previous section, all the discussion was done on departure delays. However, these delays differ somewhat from the delays recorded at arrival:"},{"metadata":{"trusted":true},"cell_type":"code","source":"mpl.rcParams.update(mpl.rcParamsDefault)\nmpl.rcParams['hatch.linewidth'] = 2.0  \n\nfig = plt.figure(1, figsize=(11,6))\nax = sns.barplot(x=\"DepDelay\", y=\"UniqueCarrier\", data=df, color=\"lightskyblue\", ci=None)\nax = sns.barplot(x=\"ArrDelay\", y=\"UniqueCarrier\", data=df, color=\"r\", hatch = '///',\n                 alpha = 0.0, ci=None)\nlabels = [abbr_companies[item.get_text()] for item in ax.get_yticklabels()]\nax.set_yticklabels(labels)\nax.yaxis.label.set_visible(False)\nplt.xlabel('Mean delay [min] (@departure: blue, @arrival: hatch lines)',\n           fontsize=14, weight = 'bold', labelpad=10);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again on this figure, we can see that delays at arrival are generally lower than at departure. This indicates that airlines adjust their flight speed in order to reduce the delays at arrival. In what follows, I will just consider the delays at departure but one has to keep in mind that this can differ from arrival delays."},{"metadata":{},"cell_type":"markdown","source":"___\n## 4. Relation between the origin airport and delays\n\nWe will now try to define if there is a correlation between the delays registered and the airport of origin. We recall that in the dataset, the number of airports considered is: "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Nb of airports: {}\".format(len(df['Origin'].unique())))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### 4.1 Geographical area covered by airlines \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"origin_nb = dict()\nfor carrier in abbr_companies.keys():\n    liste_origin_airport = df[df['UniqueCarrier'] == carrier]['Origin'].unique()\n    origin_nb[carrier] = len(liste_origin_airport)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.DataFrame.from_dict(origin_nb, orient='index')\ntest_df.rename(columns = {0:'count'}, inplace = True)\nax = test_df.plot(kind='bar', figsize = (8,3))\nlabels = [abbr_companies[item.get_text()] for item in ax.get_xticklabels()]\nax.set_xticklabels(labels)\nplt.ylabel('Number of airports visited', fontsize=14, weight = 'bold', labelpad=12)\nplt.setp(ax.get_xticklabels(), fontsize=11, ha = 'right', rotation = 80)\nax.legend().set_visible(False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = pd.read_csv('../input/airport-and-airlines-for-us-flights-data-2008/airports.csv')\nidentify_airport = temp.set_index('IATA_CODE')['CITY'].to_dict()\nlatitude_airport = temp.set_index('IATA_CODE')['LATITUDE'].to_dict()\nlongitude_airport = temp.set_index('IATA_CODE')['LONGITUDE'].to_dict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_map(df, carrier, long_min, long_max, lat_min, lat_max):\n    fig=plt.figure(figsize=(7,3))\n    ax=fig.add_axes([0.,0.,1.,1.])\n    m = Basemap(resolution='i',llcrnrlon=long_min, urcrnrlon=long_max,\n                  llcrnrlat=lat_min, urcrnrlat=lat_max, lat_0=0, lon_0=0,)\n    df2 = df[df['UniqueCarrier'] == carrier]\n    count_trajectories = df2.groupby(['Origin', 'Dest']).size()\n    count_trajectories.sort_values(inplace = True)\n    \n    for (origin, dest), s in count_trajectories.iteritems():\n        nylat,   nylon = latitude_airport[origin], longitude_airport[origin]\n        m.plot(nylon, nylat, marker='o', markersize = 10, markeredgewidth = 1,\n                   color = 'seagreen', markeredgecolor='k')\n\n    for (origin, dest), s in count_trajectories.iteritems():\n        nylat,   nylon = latitude_airport[origin], longitude_airport[origin]\n        lonlat, lonlon = latitude_airport[dest], longitude_airport[dest]\n        if pd.isnull(nylat) or pd.isnull(nylon) or \\\n                pd.isnull(lonlat) or pd.isnull(lonlon): continue\n        if s < 100:\n            m.drawgreatcircle(nylon, nylat, lonlon, lonlat, linewidth=0.5, color='b',\n                             label = '< 100')\n        elif s < 200:\n            m.drawgreatcircle(nylon, nylat, lonlon, lonlat, linewidth=2, color='r',\n                             label = '100 <.< 200')\n        else:\n            m.drawgreatcircle(nylon, nylat, lonlon, lonlat, linewidth=2, color='gold',\n                              label = '> 200')    \n    #_____________________________________________\n    # remove duplicate labels and set their order\n    handles, labels = plt.gca().get_legend_handles_labels()\n    by_label = OrderedDict(zip(labels, handles))\n    key_order = ('< 100', '100 <.< 200', '> 200')                \n    new_label = OrderedDict()\n    for key in key_order:\n        if key not in by_label.keys(): continue\n        new_label[key] = by_label[key]\n    plt.legend(new_label.values(), new_label.keys(), loc = 'best', prop= {'size':8},\n               title='flights per month', facecolor = 'palegreen', \n               shadow = True, frameon = True, framealpha = 1)    \n    m.drawcoastlines()\n    m.fillcontinents()\n    ax.set_title('{} flights'.format(abbr_companies[carrier]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coord = dict()\ncoord['AA'] = [-165, -60, 10, 55]\ncoord['AS'] = [-182, -63, 10, 75]\ncoord['HA'] = [-180, -65, 10, 52]\nfor carrier in ['AA', 'AS', 'HA']: \n    make_map(df, carrier, *coord[carrier])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"___\n### 4.2 How the origin airport impact delays\n\nIn this section, we will have a look at the variations of the delays with respect to the origin airport and for every airline. The first step thus consists in determining the mean delays per airport:"},{"metadata":{"trusted":true},"cell_type":"code","source":"airport_mean_delays = pd.DataFrame(pd.Series(df['Origin'].unique()))\nairport_mean_delays.set_index(0, drop = True, inplace = True)\n\nfor carrier in abbr_companies.keys():\n    df1 = df[df['UniqueCarrier'] == carrier]\n    test = df1['DepDelay'].groupby(df['Origin']).apply(get_stats).unstack()\n    airport_mean_delays[carrier] = test.loc[:, 'mean'] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(context=\"paper\")\nfig = plt.figure(1, figsize=(8,8))\n\nax = fig.add_subplot(1,2,1)\nsubset = airport_mean_delays.iloc[:50,:].rename(columns = abbr_companies)\nsubset = subset.rename(index = identify_airport)\nmask = subset.isnull()\nsns.heatmap(subset, linewidths=0.01, cmap=\"Accent\", mask=mask, vmin = 0, vmax = 35)\nplt.setp(ax.get_xticklabels(), fontsize=10, rotation = 85) ;\nax.yaxis.label.set_visible(False)\n\nax = fig.add_subplot(1,2,2)    \nsubset = airport_mean_delays.iloc[50:100,:].rename(columns = abbr_companies)\nsubset = subset.rename(index = identify_airport)\nfig.text(0.5, 1.02, \"Delays: impact of the origin airport\", ha='center', fontsize = 18)\nmask = subset.isnull()\nsns.heatmap(subset, linewidths=0.01, cmap=\"Accent\", mask=mask, vmin = 0, vmax = 35)\nplt.setp(ax.get_xticklabels(), fontsize=10, rotation = 85) ;\nax.yaxis.label.set_visible(False)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"___\n### 4.3 Flights with usual delays ?\n\nIn the previous section, it has been seen that there is variability in delays when considering the different airlines and the different airports of origin. we're now going to add a level of granularity by focusing not just on the original airports but on flights: origin $\\to$ destination. The objective here is to see if some flights are systematically delayed or if, on the contrary, there are flights that would always be on time.\n\nIn the following, we consider the case of a single airline. We list all the flights A $\\to$ B carried out by this company and for each of them, we create the list of delays that have been recorded:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#_________________________________________________________________\n# We select the company and create a subset of the main dataframe\ncarrier = 'AA'\ndf1 = df[df['UniqueCarrier']==carrier][['Origin','Dest','DepDelay']]\n#___________________________________________________________\n# I collect the routes and list the delays for each of them\ntrajet = dict()\nfor ind, col in df1.iterrows():\n    if pd.isnull(col['DepDelay']): continue\n    route = str(col['Origin'])+'-'+str(col['Dest'])\n    if route in trajet.keys():\n        trajet[route].append(col['DepDelay'])\n    else:\n        trajet[route] = [col['DepDelay']]\n#____________________________________________________________________        \n# I transpose the dictionary in a list to sort the routes by origins        \nliste_trajet = []\nfor key, value in trajet.items():\n    liste_trajet.append([key, value])\nliste_trajet.sort()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_val = [] ; std_val = [] ; x_label = []\n\ni = 0\nfor route, liste_retards in liste_trajet:\n    #_____________________________________________\n    # I set the labels as the airport from origin\n    index = route.split('-')[0]\n    x_label.append(identify_airport[index])\n    #______________________________________________________________________________\n    # I put a threshold on delays to prevent that high values take too much weight\n    trajet2 = [min(90, s) for s in liste_retards]\n    #________________________________________\n    # I compute mean and standard deviations\n    mean_val.append(scipy.mean(trajet2))\n    std_val.append(scipy.std(trajet2))\n    i += 1\n#________________\n# Plot the graph\nfig, ax = plt.subplots(figsize=(10,4))\nstd_min = [ min(15 + mean_val[i], s) for i,s in enumerate(std_val)] \nax.errorbar(list(range(i)), mean_val, yerr = [std_min, std_val], fmt='o') \nax.set_title('Mean route delays for \"{}\"'.format(abbr_companies[carrier]),\n             fontsize=14, weight = 'bold')\nplt.ylabel('Mean delay at origin (minutes)', fontsize=14, weight = 'bold', labelpad=12)\n#___________________________________________________\n# I define the x,y range and positions of the ticks\nimin, imax = 145, 230\nplt.xlim(imin, imax) ; plt.ylim(-20, 45)\nliste_ticks = [imin]\nfor j in range(imin+1,imax):\n    if x_label[j] == x_label[j-1]: continue\n    liste_ticks.append(j)\n#_____________________________\n# and set the tick parameters  \nax.set_xticks(liste_ticks)\nax.set_xticklabels([x_label[int(x)] for x in ax.get_xticks()], rotation = 90, fontsize = 8)\nplt.setp(ax.get_yticklabels(), fontsize=12, rotation = 0)\nax.tick_params(axis='y', which='major', pad=15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This figure gives the average delays for *American Airlines*, according to the city of origin and the destination (note that on the abscissa axis, only the origin is indicated for the sake of clarity). The error bars associated with the different paths correspond to the standard deviations.\n\n___\n## 5. Temporal variability of delays\n\nIn this section, we look at the way delays vary with time. Considering the case of a specific airline and airport, delays can be easily represented by day and time:"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Figure_style():\n    #_________________________________________________________________\n    def __init__(self, size_x = 11, size_y = 5, nrows = 1, ncols = 1):\n        sns.set_style(\"white\")\n        sns.set_context(\"notebook\", font_scale=1.2, rc={\"lines.linewidth\": 2.5})\n        self.fig, axs = plt.subplots(nrows = nrows, ncols = ncols, figsize=(size_x,size_y,))\n        #________________________________\n        # convert self.axs to 2D array\n        if nrows == 1 and ncols == 1:\n            self.axs = np.reshape(axs, (1, -1))\n        elif nrows == 1:\n            self.axs = np.reshape(axs, (1, -1))\n        elif ncols == 1:\n            self.axs = np.reshape(axs, (-1, 1))\n    #_____________________________\n    def pos_update(self, ix, iy):\n        self.ix, self.iy = ix, iy\n    #_______________\n    def style(self):\n        self.axs[self.ix, self.iy].spines['right'].set_visible(False)\n        self.axs[self.ix, self.iy].spines['top'].set_visible(False)\n        self.axs[self.ix, self.iy].yaxis.grid(color='lightgray', linestyle=':')\n        self.axs[self.ix, self.iy].xaxis.grid(color='lightgray', linestyle=':')\n        self.axs[self.ix, self.iy].tick_params(axis='both', which='major',\n                                               labelsize=10, size = 5)\n    #________________________________________\n    def draw_legend(self, location='upper right'):\n        legend = self.axs[self.ix, self.iy].legend(loc = location, shadow=True,\n                                        facecolor = 'g', frameon = True)\n        legend.get_frame().set_facecolor('whitesmoke')\n    #_________________________________________________________________________________\n    def cust_plot(self, x, y, color='b', linestyle='-', linewidth=1, marker=None, label=''):\n        if marker:\n            markerfacecolor, marker, markersize = marker[:]\n            self.axs[self.ix, self.iy].plot(x, y, color = color, linestyle = linestyle,\n                                linewidth = linewidth, marker = marker, label = label,\n                                markerfacecolor = markerfacecolor, markersize = markersize)\n        else:\n            self.axs[self.ix, self.iy].plot(x, y, color = color, linestyle = linestyle,\n                                        linewidth = linewidth, label=label)\n        self.fig.autofmt_xdate()\n    #________________________________________________________________________\n    def cust_plot_date(self, x, y, color='lightblue', linestyle='-',\n                       linewidth=1, markeredge=False, label=''):\n        markeredgewidth = 1 if markeredge else 0\n        self.axs[self.ix, self.iy].plot_date(x, y, color='lightblue', markeredgecolor='grey',\n                                  markeredgewidth = markeredgewidth, label=label)\n    #________________________________________________________________________\n    def cust_scatter(self, x, y, color = 'lightblue', markeredge = False, label=''):\n        markeredgewidth = 1 if markeredge else 0\n        self.axs[self.ix, self.iy].scatter(x, y, color=color,  edgecolor='grey',\n                                  linewidths = markeredgewidth, label=label)    \n    #___________________________________________\n    def set_xlabel(self, label, fontsize = 14):\n        self.axs[self.ix, self.iy].set_xlabel(label, fontsize = fontsize)\n    #___________________________________________\n    def set_ylabel(self, label, fontsize = 14):\n        self.axs[self.ix, self.iy].set_ylabel(label, fontsize = fontsize)\n    #____________________________________\n    def set_xlim(self, lim_inf, lim_sup):\n        self.axs[self.ix, self.iy].set_xlim([lim_inf, lim_sup])\n    #____________________________________\n    def set_ylim(self, lim_inf, lim_sup):\n        self.axs[self.ix, self.iy].set_ylim([lim_inf, lim_sup])           ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"carrier = 'WN'\nid_airport = 2\nliste_origin_airport = df[df['UniqueCarrier'] == carrier]['Origin'].unique()\ndf2 = df[(df['UniqueCarrier'] == carrier) & (df['ArrDelay'] > 0)\n         & (df['Origin'] == 'IND')]\ndf2.sort_values('CRSDepTime', inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1 = Figure_style(11, 5, 1, 1)\nfig1.pos_update(0, 0)\nfig1.cust_plot(df2['CRSDepTime'], df2['DepDelay'], linestyle='-')\nfig1.style() \nfig1.set_ylabel('Delay (minutes)', fontsize = 14)\nfig1.set_xlabel('Departure date', fontsize = 14)\ndate_1 = datetime.datetime(2008,1,1)\ndate_2 = datetime.datetime(2008,1,31)\n#fig1.set_xlim(date_1, date_2)\nfig1.set_ylim(-15, 217)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This figure shows the existence of cycles, both in the frequency of the delays but also in their magnitude. In fact, intuitively, it seems quite logical to observe such cycles since they will be a consequence of the day-night alternation and the fact that the airport activity will be greatly reduced (if not inexistent) during the night. This suggests that a **important  variable** in the modeling of delays will be **take-off time**. To check this hypothesis, I look at the behavior of the mean delay as a function of departure time, aggregating the data of the current month:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#_______________________________\ndef func2(x, a, b, c):\n    return a * x**2 +  b*x + c\n#_______________________________\ndf2['heure_depart'] =  df2['CRSDepTime'].apply(lambda x:x.time())\ntest2 = df2['DepDelay'].groupby(df2['heure_depart']).apply(get_stats).unstack()\nfct = lambda x:x.hour*3600+x.minute*60+x.second\nx_val = np.array([fct(s) for s in test2.index]) \ny_val = test2['mean']\npopt, pcov = curve_fit(func2, x_val, y_val, p0 = [1, 2, 3])\ntest2['fit'] = pd.Series(func2(x_val, *popt), index = test2.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1 = Figure_style(8, 4, 1, 1)\nfig1.pos_update(0, 0)\nfig1.cust_plot_date(df2['heure_depart'], df2['DepDelay'],\n                    markeredge=False, label='initial data points')\nfig1.cust_plot(test2.index, test2['mean'], linestyle='--', linewidth=2, label='mean')\nfig1.cust_plot(test2.index, test2['fit'], color='r', linestyle='-', linewidth=3, label='fit')\nfig1.style() ; fig1.draw_legend('upper left')\nfig1.set_ylabel('Delay (minutes)', fontsize = 14)\nfig1.set_xlabel('Departure time', fontsize = 14)\nfig1.set_ylim(-15, 210)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we can see that the average delay tends to increase with the departure time of day: flights leave on time in the morning  and the delay grows almost monotonously up to 30 minutes at the end of the day. In fact, this behavior is quite general and looking at other aiports or companies, we would find similar trends.\n\n___\n## 6. Predicting flight delays \n\nThe previsous sections dealt with an exploration of the dataset. Here, we start with the modeling of flight delays.\nIn this section, my goal is to create a model that uses a window of 11 months to predict the delays of the following month.\nHence, we use January until Nopember to predict December."},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\ndf_train = df[df['CRSDepTime'] < datetime.datetime(2008,12,1)]\ndf_test  = df[df['CRSDepTime'] >= datetime.datetime(2008,12,1)]\ndf=df_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"___\n### 6.1 Model nº1: one airline, one airport\n\nWe first decide to model the delays by considering separately the different airlines and by splitting the data according to the different home airports. This first model can be seen as a *\"toy-model\"*  that enables to identify problems that may arise at the  production stage. When treating the whole dataset,  the number of fits will be large. Hence we have to be sure that the automation of the whole process is robust enough to insure the quality of the fits.\n\n\n#### 6.1.1 Pitfalls <br>\n\n\n**a) Unsufficient statistics**\n\nFirst of all, we consider the *American Airlines* flights and make a census of the number of flights that left each airport:"},{"metadata":{"trusted":true},"cell_type":"code","source":"carrier = 'AA'\ncheck_airports = df[(df['UniqueCarrier'] == carrier)]['DepDelay'].groupby(\n                         df['Origin']).apply(get_stats).unstack()\ncheck_airports.sort_values('count', ascending = False, inplace = True)\ncheck_airports[-5:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at this list, we can see that the less visited aiports only have a few flights in a month.\nThus, in the least favorable case, it is impossible to perform a regression.\n\n**b) Extreme delays**\n\nAnother pitfall to avoid is that of \"accidental\" delays: a particular attention should be paid to extreme delays. Indeed, during the exploration, it was seen that occasionally, delays of several hours (even tens of hours) could be recorded. This type of delay is however marginal (a few %) and the cause of these delays is probably linked to unpredictable events (weather, breakdown, accident, ...). On the other hand, taking into account a delay of this type will likely introduce a bias in the analysis. Moreover, the weight taken by large values will be significant if we have a small statistics.\n\nIn order to illustrate this, I first define a function that calculates the mean flights delay per airline and per airport: "},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_flight_delays(df, carrier, id_airport, extrem_values = False):\n    df2 = df[(df['UniqueCarrier'] == carrier) & (df['Origin'] == id_airport)]\n    #_______________________________________\n    # remove extreme values before fitting\n    if extrem_values:\n        df2['DepDelay'] = df2['DepDelay'].apply(lambda x:x if x < 60 else np.nan)\n        df2.dropna(how = 'any')\n    #__________________________________\n    # Conversion: date + heure -> heure\n    df2.sort_values('CRSDepTime', inplace = True)\n    df2['heure_depart'] =  df2['CRSDepTime'].apply(lambda x:x.time())\n    #___________________________________________________________________\n    # regroupement des vols par heure de départ et calcul de la moyenne\n    test2 = df2['DepDelay'].groupby(df2['heure_depart']).apply(get_stats).unstack()\n    test2.reset_index(inplace=True)\n    #___________________________________\n    # conversion de l'heure en secondes\n    fct = lambda x:x.hour*3600+x.minute*60+x.second\n    test2.reset_index(inplace=True)\n    test2['heure_depart_min'] = test2['heure_depart'].apply(fct)\n    return test2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def linear_regression(test2):\n    test = test2[['mean', 'heure_depart_min']].dropna(how='any', axis = 0)\n    X = np.array(test['heure_depart_min'])\n    Y = np.array(test['mean'])\n    X = X.reshape(len(X),1)\n    Y = Y.reshape(len(Y),1)\n    regr = linear_model.LinearRegression()\n    regr.fit(X, Y)\n    result = regr.predict(X)\n    return X, Y, result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id_airport = 'PHL'\ndf2 = df[(df['UniqueCarrier'] == carrier) & (df['Origin'] == id_airport)]\ndf2['heure_depart'] =  df2['CRSDepTime'].apply(lambda x:x.time())\ndf2['heure_depart'] = df2['heure_depart'].apply(lambda x:x.hour*3600+x.minute*60+x.second)\n#___________________\n# first case\ntest2 = get_flight_delays(df, carrier, id_airport, False)\nX1, Y1, result2 = linear_regression(test2)\n#___________________\n# second case\ntest3 = get_flight_delays(df, carrier, id_airport, True)\nX2, Y2, result3 = linear_regression(test3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1 = Figure_style(8, 4, 1, 1)\nfig1.pos_update(0, 0)\nfig1.cust_scatter(df2['heure_depart'], df2['DepDelay'], markeredge = True)\nfig1.cust_plot(X1, Y1, color = 'b', linestyle = ':', linewidth = 2, marker = ('b','s', 10))\nfig1.cust_plot(X2, Y2, color = 'g', linestyle = ':', linewidth = 2, marker = ('g','X', 12))\nfig1.cust_plot(X1, result2, color = 'b', linewidth = 3)\nfig1.cust_plot(X2, result3, color = 'g', linewidth = 3)\nfig1.style()\nfig1.set_ylabel('Delay (minutes)', fontsize = 14)\nfig1.set_xlabel('Departure time', fontsize = 14)\n#____________________________________\n# convert and set the x ticks labels\nfct_convert = lambda x: (int(x/3600) , int(divmod(x,3600)[1]/60))\nfig1.axs[fig1.ix, fig1.iy].set_xticklabels(['{:2.0f}h{:2.0f}m'.format(*fct_convert(x))\n                                            for x in fig1.axs[fig1.ix, fig1.iy].get_xticks()]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First of all, in this figure, the points corresponding to the individual flights are represented by the points in gray.\nThe mean of these points gives the mean delays and the mean of the set of initial points corresponds to the blue squares. By removing extreme delays (> 1h), one obtains the average delays represented by the green crosses.\nThus, in the first case, the fit (solid blue curve) leads to a prediction which corresponds to an average delay of $\\sim$ 10 minutes larger than the predicton obtained in the second case (green curve), and this, at any hour of the day.\n\nIn conclusion, we see in this example that the way in which we manage the extreme delays will have an important impact on the modeling. Note, however, that the current example corresponds to a *chosen case* where the impact of extreme delays is magnified by the limited number of flights. Presumably, the impact of such delays will be less pronounced in the majority of cases."},{"metadata":{},"cell_type":"markdown","source":"___\n#### 6.1.2 Polynomial degree: splitting the dataset\n\n\nIn practice, rather than performing a simple linear regression, we can improve the model doing a fit with a polynomial of order $N$. Doing so, it is necessary to define the degree $N$ which is optimal to represent the data. When increasing the polynomial order, it is important ** to prevent over-fitting** and we do this by splitting the dataset in **test and training sets**. A problem that may arise with this procedure is that the model ends by *indirectly* learning the contents of the test set and is thus biased. To avoid this, the data can be re-separated into 3 sets: *train*, *test* and *validation*. An alternative to this technique, which is often more robust, is the so-called cross-validation method. This method consists of performing a first separation of the data in *training* and *test* sets. As always, learning is done on the training set, but to avoid over-learning, it is split into several pieces that are used alternately for training and testing.\n\nNote that if the data set is small, the separation in test & training sets can introduce a bias in the estimation of the parameters. In practice, the *cross-validation* method avoids such bias. In fact, in the current model, we will encounter this type of problem and in what follows, I will highlight this. For example, we can consider an extreme case where, after separation, the training set would contain only hours $<$20h and the test set would have hours$>$ 20h. The model would then be unable to reproduce precisely this data, of which it would not have seen equivalent during the training. The cross-validation method avoids this bias because all the data are used successively to drive the model.\n\n** a) Bias introduced by the separation of the data set **\n\nIn order to test the impact of data separation on model determination, I first define the class * fit_polynome *:"},{"metadata":{"trusted":true},"cell_type":"code","source":"class fit_polynome:\n\n    def __init__(self, data):\n        self.data = data[['mean', 'heure_depart_min']].dropna(how='any', axis = 0)\n\n    def split(self, method):        \n        self.method = method        \n        self.X = np.array(self.data['heure_depart_min'])\n        self.Y = np.array(self.data['mean'])\n        self.X = self.X.reshape(len(self.X),1)\n        self.Y = self.Y.reshape(len(self.Y),1)\n\n        if method == 'all':\n            self.X_train = self.X\n            self.Y_train = self.Y\n            self.X_test  = self.X\n            self.Y_test  = self.Y                        \n        elif method == 'split':            \n            self.X_train, self.X_test, self.Y_train, self.Y_test = \\\n                train_test_split(self.X, self.Y, test_size=0.3)\n    \n    def train(self, pol_order):\n        self.poly = PolynomialFeatures(degree = pol_order)\n        self.regr = linear_model.LinearRegression()\n        self.X_ = self.poly.fit_transform(self.X_train)\n        self.regr.fit(self.X_, self.Y_train)\n    \n    def predict(self, X):\n        self.X_ = self.poly.fit_transform(X)\n        self.result = self.regr.predict(self.X_)\n    \n    def calc_score(self):        \n        X_ = self.poly.fit_transform(self.X_test)\n        result = self.regr.predict(X_)\n        self.score = metrics.mean_squared_error(result, self.Y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The * fit_polynome * class allows you to perform all operations related to a fit and to save the results. When calling the  **split()** method, the variable '*method*' defines how the initial data is separated:\n- *method = 'all' *: all input data is used to train and then test the model\n- *method = 'split' *: we use the * train_test_split() * method of sklearn to define test & training sets\n \nThen, the other methods of the class have the following functions:\n- ** train (n) **: drives the data on the training set and makes a polynomial of order n\n- ** predict (X) **: calculates the Y points associated with the X input and for the previously driven model\n- ** calc_score () **: calculates the model score in relation to the test set data\n\nIn order to illustrate the bias introduced by the selection of the test set, I proceed in the following way: I carry out several \"train / test\" separation of a data set and for each case, I fit polynomials of orders ** n = 1, 2 and 3 **, by calculating their respective scores. Then, I show that according to the choice of separation, the best score can be obtained with any of the values ​​of ** n **. In practice, it is enough to carry out a dozen models to obtain this result. Moreover, this bias is introduced by the choice of the separation \"train / test\" and results from the small size of the dataset to be modeled. In fact, in the following, I take as an example the case of the airline * American Airlines * (the second biggest airline) and the airport of id 1129804, which is the airport with the most registered flights for that company. This is one of the least favorable scenarios for the emergence of this kind of bias, which, nevertheless, is present:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(1, figsize=(10,4))\n\nax = ['_' for _ in range(4)]\nax[1]=fig.add_subplot(131) \nax[2]=fig.add_subplot(132) \nax[3]=fig.add_subplot(133) \n\nid_airport = 'BNA'\ntest2 = get_flight_delays(df, carrier, id_airport, True)\n\nresult = ['_' for _ in range(4)]\nscore = [10000 for _ in range(4)]\nfound = [False for _ in range(4)]\nfit = fit_polynome(test2)\n\ncolor = '.rgbyc'\n\ninc = 0\nwhile True:\n    inc += 1\n    fit.split('split')\n    for i in range(1,4):\n        fit.train(pol_order = i)\n        fit.predict(fit.X)\n        result[i] = fit.result\n        fit.calc_score()\n        score[i]  = fit.score\n\n    [ind_min] = [j for j,val in enumerate(score) if min(score) == val]\n    print(\"modèle nº{:<2}, min. pour n = {}, score = {:.1f}\".format(inc, ind_min,score[ind_min]))\n    \n    if not found[ind_min]:            \n        for i in range(1,4):\n            ax[ind_min].plot(fit.X, result[i], color[i], linewidth = 4 if i == ind_min else 1)\n        ax[ind_min].scatter(fit.X, fit.Y)                \n        ax[ind_min].text(0.05, 0.95, 'MSE = {:.1f}, {:.1f}, {:.1f}'.format(*score[1:4]),\n                         style='italic', transform=ax[ind_min].transAxes, fontsize = 8,\n                         bbox={'facecolor':'tomato', 'alpha':0.8, 'pad':5})                \n        found[ind_min] = True\n\n    shift = 0.5\n    plt.text(-1+shift, 1.05, \"polynomial order:\", color = 'k',\n                transform=ax[2].transAxes, fontsize = 16, family='fantasy')\n    plt.text(0+shift, 1.05, \"n = 1\", color = 'r', \n                transform=ax[2].transAxes, fontsize = 16, family='fantasy')\n    plt.text(0.4+shift, 1.05, \"n = 2\", color = 'g', \n                transform=ax[2].transAxes, fontsize = 16, family='fantasy')\n    plt.text(0.8+shift, 1.05, \"n = 3\", color = 'b',\n                transform=ax[2].transAxes, fontsize = 16, family='fantasy')\n   \n    if inc == 40 or all(found[1:4]): break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this figure, the panels from left to right correspond to 3 separations of the data in train and test sets, for which the best models are obtained respectively with polynomials of order 1, 2 and 3. On each of these panels the 3 fits polynomials have been represented and the best model corresponds to the thick curve.\n\n** b) Selection by cross-validation**\n\nOne of the advantages of the cross-validation method is that it avoids the bias that has just been put forward when choosing the polynomial degree. In order to use this method, I define a new class that I will use later to perform the fits:"},{"metadata":{"trusted":true},"cell_type":"code","source":"class fit_polynome_cv:\n\n    def __init__(self, data):\n        self.data = data[['mean', 'heure_depart_min']].dropna(how='any', axis = 0)\n        self.X = np.array(self.data['heure_depart_min'])\n        self.Y = np.array(self.data['mean'])\n        self.X = self.X.reshape(len(self.X),1)\n        self.Y = self.Y.reshape(len(self.Y),1)\n\n    def train(self, pol_order, nb_folds):\n        self.poly = PolynomialFeatures(degree = pol_order)\n        self.regr = linear_model.LinearRegression()\n        self.X_ = self.poly.fit_transform(self.X)\n        self.result = cross_val_predict(self.regr, self.X_, self.Y, cv = nb_folds)\n    \n    def calc_score(self, pol_order, nb_folds):\n        self.poly = PolynomialFeatures(degree = pol_order)\n        self.regr = linear_model.LinearRegression()\n        self.X_ = self.poly.fit_transform(self.X)\n        self.score = np.mean(cross_val_score(self.regr, self.X_, self.Y,\n                                             cv = nb_folds,scoring='neg_mean_squared_error'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This class has two methods:\n- ** train (n, nb_folds) **: defined 'nb_folds' training sets from the initial dataset and drives a 'n' order polynomial on each of these sets. This method returns as a result the Y predictions obtained for the different test sets.\n- ** calc_score (n, nb_folds) **: performs the same procedure as a ** train ** method except that this method calculates the fit score and not the predicted values ​​on the different test data.\n\nBy default, the *'K-fold'* method is used by sklearn * cross_val_predict () * and * cross_val_score () * methods. These methods are deterministic in the choice of the K folds, which implies that for a fixed K value, the results obtained using these methods will always be identical. As seen in the previous example, this was not the case when using the *train_test_split()* method. Thus, if we take the same dataset as in the previous example, the method of cross validation makes it possible to choose the best polynomial degree:"},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_folds = 10\nprint('Max possible number of folds: {} \\n'.format(test2.shape[0]-1))\nfit2 = fit_polynome_cv(test2)\nfor i in range(1, 8):\n    fit2.calc_score(i, nb_folds)\n    print('n={} -> MSE = {}'.format(i, round(abs(fit2.score),3)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that using this method gives us that the best model (ie the best generalized model) is obtained with a polynomial of order 2. At this stage of the procedure, the choice of the polynomial order a has been validated and we can now use all the data in order to perform the fit:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fit = fit_polynome(test2)\nfit.split('all')\nfit.train(pol_order = 2)\nfit.predict(fit.X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus, in the following figure, the juxtaposition of the K = 50 polynomial fits corresponding to the cross validation calculation leads to the red curve. The polynomial fit corresponding to the final model corresponds to the blue curve."},{"metadata":{"trusted":true},"cell_type":"code","source":"fit2.train(pol_order = 2, nb_folds = nb_folds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1 = Figure_style(8, 4, 1, 1) ; fig1.pos_update(0, 0)\nfig1.cust_scatter(fit2.X, fit2.Y, markeredge = True, label = 'initial data points')\nfig1.cust_plot(fit.X,fit2.result,color=u'#1f77b4',linestyle='--',linewidth=2,label='CV output')\nfig1.cust_plot(fit.X,fit.result,color=u'#ff7f0e',linewidth = 3,label='final fit')\nfig1.style(); fig1.draw_legend('upper left')\nfig1.set_ylabel('Delay (minutes)') ; fig1.set_xlabel('Departure time')\n#____________________________________\n# convert and set the x ticks labels\nfct_convert = lambda x: (int(x/3600) , int(divmod(x,3600)[1]/60))\nfig1.axs[fig1.ix, fig1.iy].set_xticklabels(['{:2.0f}h{:2.0f}m'.format(*fct_convert(x))\n                                            for x in fig1.axs[fig1.ix, fig1.iy].get_xticks()]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = metrics.mean_squared_error(fit.result, fit2.Y)\nscore","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 6.1.3 Model test: prediction of end-January delays"},{"metadata":{},"cell_type":"markdown","source":"At this stage, the model was driven is tested on the training set which include the data of the first 3 weeks of January. We now look at the comparison of predictions and observations for the fourth week of January:"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = get_flight_delays(df_test, carrier, id_airport, True)\ntest_data = test_data[['mean', 'heure_depart_min']].dropna(how='any', axis = 0)\nX_test = np.array(test_data['heure_depart_min'])\nY_test = np.array(test_data['mean'])\nX_test = X_test.reshape(len(X_test),1)\nY_test = Y_test.reshape(len(Y_test),1)\nfit.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = metrics.mean_squared_error(fit.result, Y_test)\nscore","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'Ecart = {:.2f} min'.format(np.sqrt(score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"___\n### 6.2 Model nº2: One airline, all airports\n\nIn the previous section, the model only considered one airport. This procedure is potentially inefficient because it is likely that some of the observations can be extrapolated from an ariport to another. Thus, it may be advantageous to make a single fit, which would take all the airports into account. In particular, this will allow to predict delays on airports for which the number of data is low with a better accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_merged_delays(df, carrier):\n    liste_airports = df[df['UniqueCarrier'] == carrier]['Origin'].unique()\n    i = 0\n    liste_columns = ['AIRPORT_ID', 'heure_depart_min', 'mean']\n    for id_airport in liste_airports:\n        test2 = get_flight_delays(df, carrier, id_airport, True)\n        test2.loc[:, 'AIRPORT_ID'] = id_airport\n        test2 = test2[liste_columns]\n        test2.dropna(how = 'any', inplace = True)\n        if i == 0:\n            merged_df = test2.copy()\n        else:\n            merged_df = pd.concat([merged_df, test2], ignore_index = True)\n        i += 1    \n    return merged_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"carrier = 'AA'\nmerged_df = get_merged_delays(df, carrier)\nmerged_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_encoder = LabelEncoder()\ninteger_encoded = label_encoder.fit_transform(merged_df['AIRPORT_ID'])\n#__________________________________________________________\n# correspondance between the codes and tags of the airports\nzipped = zip(integer_encoded, merged_df['AIRPORT_ID'])\nlabel_airports = list(set(list(zipped)))\nlabel_airports.sort(key = lambda x:x[0])\nlabel_airports[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above, I have assigned a label to each airport. The correspondence between the label and the original identifier has been saved in the *label_airport* list. Now I proceed with the \"One Hot Encoding\" by creating a matrix where instead of the **Origin** variable that contained $M$ labels, we build a matrix with $M$ columns, filled of 0 and 1 depending on the correspondance with particular airports:"},{"metadata":{"trusted":true},"cell_type":"code","source":"onehot_encoder = OneHotEncoder(sparse=False)\ninteger_encoded = integer_encoded.reshape(len(integer_encoded), 1)\nonehot_encoded = onehot_encoder.fit_transform(integer_encoded)\nb = np.array(merged_df['heure_depart_min'])\nb = b.reshape(len(b),1)\nX = np.hstack((onehot_encoded, b))\nY = np.array(merged_df['mean'])\nY = Y.reshape(len(Y), 1)\nprint(X.shape, Y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"___\n#### 5.2.1 Linear regression\n\nThe matrices X and Y thus created can be used to perform a linear regression:"},{"metadata":{"trusted":true},"cell_type":"code","source":"lm = linear_model.LinearRegression()\nmodel = lm.fit(X,Y)\npredictions = lm.predict(X)\nprint(\"MSE =\", metrics.mean_squared_error(predictions, Y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, I calculated the MSE score of the fit. In practice, we can have a feeling of the quality of the fit by considering the number of predictions where the differences with real values is greater than 15 minutes:"},{"metadata":{"trusted":true},"cell_type":"code","source":"icount = 0\nfor i, val in enumerate(Y):\n    if abs(val-predictions[i]) > 15: icount += 1\n'{:.2f}%'.format(icount / len(predictions) * 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tips = pd.DataFrame()\ntips[\"prediction\"] = pd.Series([float(s) for s in predictions]) \ntips[\"original_data\"] = pd.Series([float(s) for s in Y]) \nsns.jointplot(x=\"original_data\", y=\"prediction\", data=tips, size = 6, ratio = 7,\n              joint_kws={'line_kws':{'color':'limegreen'}}, kind='reg')\nplt.xlabel('Mean delays (min)', fontsize = 15)\nplt.ylabel('Predictions (min)', fontsize = 15)\nplt.plot(list(range(-10,25)), list(range(-10,25)), linestyle = ':', color = 'r')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"___\n#### 5.2.2 Polynomial regression\n\nWe will now extend the previous fit by using a polynomial rather than a linear function:"},{"metadata":{"trusted":true},"cell_type":"code","source":"poly = PolynomialFeatures(degree = 2)\nregr = linear_model.LinearRegression()\nX_ = poly.fit_transform(X)\nregr.fit(X_, Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = regr.predict(X_)\nprint(\"MSE =\", metrics.mean_squared_error(result, Y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that a polynomial fit improves slightly the MSE score. In practice, the percentage of values where the difference between predictions and real delays is greater than 15 minutes is:"},{"metadata":{"trusted":true},"cell_type":"code","source":"icount = 0\nfor i, val in enumerate(Y):\n    if abs(val-result[i]) > 15: icount += 1\n'{:.2f}%'.format(icount / len(result) * 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tips = pd.DataFrame()\ntips[\"prediction\"] = pd.Series([float(s) for s in result]) \ntips[\"original_data\"] = pd.Series([float(s) for s in Y]) \nsns.jointplot(x=\"original_data\", y=\"prediction\", data=tips, size = 6, ratio = 7,\n              joint_kws={'line_kws':{'color':'limegreen'}}, kind='reg')\nplt.xlabel('Mean delays (min)', fontsize = 15)\nplt.ylabel('Predictions (min)', fontsize = 15)\nplt.plot(list(range(-10,25)), list(range(-10,25)), linestyle = ':', color = 'r')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"___\n#### 5.2.3 Setting the free parameters\n\nAbove, the two models were fit and tested on the training set. In practice, as mentioned above, there is a risk of overfitting by proceeding that way and the free parameters of the model will be biased. Hence,  the model will not allow a good generalization. In what follows, I will therefore split the datas in order to train and then test the model. The purpose will be to determine the polynomial degree which allows the best generalization of the predictions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"poly = PolynomialFeatures(degree = 2)\nregr = linear_model.LinearRegression()\nX_ = poly.fit_transform(X_train)\nregr.fit(X_, Y_train)\nresult = regr.predict(X_)\nscore = metrics.mean_squared_error(result, Y_train)\nprint(\"Mean squared error = \", score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_ = poly.fit_transform(X_test)\nresult = regr.predict(X_)\nscore = metrics.mean_squared_error(result, Y_test)\nprint(\"Mean squared error = \", score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we see that the **fit is particularly bad with a MSE > 100** , which means that the fit performs poorly when generalyzing to other data. Now let's examine in detail the reasons why we have such a bad score. Below, I examing all the terms of the MSE calculation and identify the largest terms:"},{"metadata":{"trusted":true},"cell_type":"code","source":"somme = 0\nfor valeurs in zip(result, Y_test):\n    ajout = (float(valeurs[0]) - float(valeurs[1]))**2\n    somme += ajout\n    if ajout > 10**4:\n        print(\"{:<.1f} {:<.1f} {:<.1f}\".format(ajout, float(valeurs[0]), float(valeurs[1])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that some predictions show very large errors. In practice, this can be explained by the fact that during the separation in train and test sets, **data with no equivalent in the training set was put in the test data**. Thus, when calculating the prediction, the model has to **perform an extrapolation**. If the coefficients of the fit are large (which is often the case when overfitting), extrapolated values will show important values, as in the present case. In order to have a control over this phenomenon, we can use a **regularization method** which will put a penalty to the models whose coefficients are the most important:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nridgereg = Ridge(alpha=0.3,normalize=True)\npoly = PolynomialFeatures(degree = 2)\nX_ = poly.fit_transform(X_train)\nridgereg.fit(X_, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_ = poly.fit_transform(X_test)\nresult = ridgereg.predict(X_)\nscore = metrics.mean_squared_error(result, Y_test)\nprint(\"Mean squared error = \", score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_min = 10000\nfor pol_order in range(1, 3):\n    for alpha in range(0, 20, 2):\n        ridgereg = Ridge(alpha = alpha/10, normalize=True)\n        poly = PolynomialFeatures(degree = pol_order)\n        regr = linear_model.LinearRegression()\n        X_ = poly.fit_transform(X_train)\n        ridgereg.fit(X_, Y_train)        \n        X_ = poly.fit_transform(X_test)\n        result = ridgereg.predict(X_)\n        score = metrics.mean_squared_error(result, Y_test)        \n        if score < score_min:\n            score_min = score\n            parameters = [alpha/10, pol_order]\n        print(\"n={} alpha={} , MSE = {:<0.5}\".format(pol_order, alpha, score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridgereg = Ridge(alpha = parameters[0], normalize=True)\npoly = PolynomialFeatures(degree = parameters[1])\nX_ = poly.fit_transform(X)\nridgereg.fit(X_, Y)\nresult = ridgereg.predict(X_)\nscore = metrics.mean_squared_error(result, Y)        \nprint(score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 6.2.4 Testing the model: delays of end-january\n\n\nAt this stage, model predictions are tested against end-January data. These data are first extracted:"},{"metadata":{"trusted":true},"cell_type":"code","source":"carrier = 'AA'\nmerged_df_test = get_merged_delays(df_test, carrier)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_conversion = dict()\nfor s in label_airports:\n    label_conversion[s[1]] = s[0]\n\nmerged_df_test['AIRPORT_ID'].replace(label_conversion, inplace = True)\n\nfor index, label in label_airports:\n    temp = merged_df_test['AIRPORT_ID'] == index\n    temp = temp.apply(lambda x:1.0 if x else 0.0)\n    if index == 0:\n        matrix = np.array(temp)\n    else:\n        matrix = np.vstack((matrix, temp))\nmatrix = matrix.T\n\nb = np.array(merged_df_test['heure_depart_min'])\nb = b.reshape(len(b),1)\nX_test = np.hstack((matrix, b))\nY_test = np.array(merged_df_test['mean'])\nY_test = Y_test.reshape(len(Y_test), 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_ = poly.fit_transform(X_test)\nresult = ridgereg.predict(X_)\nscore = metrics.mean_squared_error(result, Y_test)\n'MSE = {:.2f}'.format(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'Ecart = {:.2f} min'.format(np.sqrt(score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The current MSE score is calculated on all the airports served by _American Airlines_, whereas previously it was calculated on the data of a single airport. The current model is therefore more general. Moreover, considering the previous model, it is likely that predictions will be poor for airports with low statistics.\n____\n## 6.3 Model nº3: Accounting for destinations"},{"metadata":{},"cell_type":"markdown","source":"In the previous model, I grouped the flights per departure time. Thus, flights with different destinations were grouped as soon as they leave at the same time. Now I make a model that accounts for both departure and arrival times:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_df(df, carrier):\n    df2 = df[df['UniqueCarrier'] == carrier][['CRSDepTime','CRSArrTime',\n                                    'Origin','Dest','DepDelay']]\n    df2.dropna(how = 'any', inplace = True)\n    df2['weekday'] = df2['CRSDepTime'].apply(lambda x:x.weekday())\n    #____________________\n    # delete delays > 1h\n    df2['Dep[Delay]'] = df2['DepDelay'].apply(lambda x:x if x < 60 else np.nan)\n    df2.dropna(how = 'any', inplace = True)\n    #_________________\n    # formating times\n    fct = lambda x:x.hour*3600+x.minute*60+x.second\n    df2['heure_depart'] = df2['CRSDepTime'].apply(lambda x:x.time())\n    df2['heure_depart'] = df2['heure_depart'].apply(fct)\n    df2['heure_arrivee'] = df2['CRSArrTime'].apply(fct)\n    df3 = df2.groupby(['heure_depart', 'heure_arrivee', 'Origin'],\n                      as_index = False).mean()\n    return df3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3 = create_df(df, carrier)    \ndf3[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_encoder = LabelEncoder()\ninteger_encoded = label_encoder.fit_transform(df3['Origin'])\n#_________________________________________________________\nzipped = zip(integer_encoded, df3['Origin'])\nlabel_airports = list(set(list(zipped)))\nlabel_airports.sort(key = lambda x:x[0])\n#_________________________________________________\nonehot_encoder = OneHotEncoder(sparse=False)\ninteger_encoded = integer_encoded.reshape(len(integer_encoded), 1)\nonehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n#_________________________________________________\nb = np.array(df3[['heure_depart', 'heure_arrivee']])\nX = np.hstack((onehot_encoded, b))\nY = np.array(df3['DepDelay'])\nY = Y.reshape(len(Y), 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"___\n#### 6.3.1 Choice of model parameters\n\nAs before, we will perform a regression with regularization and I will have to define the value to attribute to the parameter $\\alpha$. we therefore separate the data to train and then test the model to select the best value for $\\alpha$:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_min = 10000\nfor pol_order in range(1, 3):\n    for alpha in range(0, 20, 2):\n        ridgereg = Ridge(alpha = alpha/10, normalize=True)\n        poly = PolynomialFeatures(degree = pol_order)\n        regr = linear_model.LinearRegression()\n        X_ = poly.fit_transform(X_train)\n        ridgereg.fit(X_, Y_train)\n        \n        X_ = poly.fit_transform(X_test)\n        result = ridgereg.predict(X_)\n        score = metrics.mean_squared_error(result, Y_test)\n        \n        if score < score_min:\n            score_min = score\n            parameters = [alpha, pol_order]\n\n        print(\"n={} alpha={} , MSE = {:<0.5}\".format(pol_order, alpha/10, score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridgereg = Ridge(alpha = parameters[0], normalize=True)\npoly = PolynomialFeatures(degree = parameters[1])\nX_ = poly.fit_transform(X)\nridgereg.fit(X_, Y)\nresult = ridgereg.predict(X_)\nscore = metrics.mean_squared_error(result, Y)        \nprint(score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 6.3.2 Test of the model: late January delays\n\nNow I test the quality of the predictions on the data of the last week of January. I first extract these data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df3 = create_df(df_test, carrier)    \ndf3[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_conversion = dict()\nfor s in label_airports:\n    label_conversion[s[1]] = s[0]\n\ndf3['Origin'].replace(label_conversion, inplace = True)\n\nfor index, label in label_airports:\n    temp = df3['Origin'] == index\n    temp = temp.apply(lambda x:1.0 if x else 0.0)\n    if index == 0:\n        matrix = np.array(temp)\n    else:\n        matrix = np.vstack((matrix, temp))\nmatrix = matrix.T\n\nb = np.array(df3[['heure_depart', 'heure_arrivee']])\nX_test = np.hstack((matrix, b))\nY_test = np.array(df3['DepDelay'])\nY_test = Y_test.reshape(len(Y_test), 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_ = poly.fit_transform(X_test)\nresult = ridgereg.predict(X_)\nscore = metrics.mean_squared_error(result, Y_test)\nprint('MSE = {}'.format(round(score, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'Ecart = {:.2f} min'.format(np.sqrt(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"icount = 0\nfor i, val in enumerate(Y_test):\n    if abs(val-predictions[i]) > 15: icount += 1\nprint(\"ecarts > 15 minutes: {}%\".format(round((icount / len(predictions))*100,3)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tips = pd.DataFrame()\ntips[\"prediction\"] = pd.Series([float(s) for s in predictions]) \ntips[\"original_data\"] = pd.Series([float(s) for s in Y_test]) \nsns.jointplot(x=\"original_data\", y=\"prediction\", data=tips, size = 6, ratio = 7,\n              joint_kws={'line_kws':{'color':'limegreen'}}, kind='reg')\nplt.xlabel('Mean delays (min)', fontsize = 15)\nplt.ylabel('Predictions (min)', fontsize = 15)\nplt.plot(list(range(-10,25)), list(range(-10,25)), linestyle = ':', color = 'r')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\nThese notebook was two-fold. The first part dealt with an exploration of the dataset, with the aim of understanding some properties of the delays registered by flights. This exploration gave me the occasion of using various vizualization tools offered by python. The second part of the notebook consisted in the elaboration of a model aimed at predicting flight delays. For that purpose, we used polynomial regressions and showed the importance of regularisation techniques. In fact, we only used ridge regression but it is important to keep in mind that other regularisations techniques could be more appropriate ( e.g Lasso or Elastic net).\n\nFurthermore, for the model to predict the fligths delay, we still need to improve the accuration by trying other kind model with some kind of their parameters."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}