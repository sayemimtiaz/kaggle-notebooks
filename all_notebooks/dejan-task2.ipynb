{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this example, we will assess the risk of a student failing a course module based on student characterstics (gender, age, etc.) and information about their activity (studied credits, number of previous attempts to pass the course). To do that, we will train a student model using logistic regression.\n\nThen, we will try to improve the model's performance in terms of accuracy by using the assignments' grades as an additional factor."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#read student information\nstudentInfo = pd.read_csv(\"/kaggle/input/open-university-learning-analytics-dataset/anonymiseddata/studentInfo.csv\")\nvle = pd.read_csv(\"/kaggle/input/open-university-learning-analytics-dataset/anonymiseddata/vle.csv\")\n#print out the 10 first rows of the data\nstudentInfo.head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"studentInfo[\"disability\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"studentInfo[\"imd_band\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# lets see all potential final results\nstudentInfo[\"final_result\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create a new column to classify final results. classify studets with a pass or distinction as \"1\", the rest as \"0\"\nstudentInfo[\"result.class\"] = 1\n\n#studentInfo[\"result.class\"] = studentInfo[\"final_result\"].apply(lambda x: 0 if (x == 'Fail') | x == \"Withdrawn\") else 1)\nstudentInfo[\"result.class\"].loc[(studentInfo[\"final_result\"] == \"Withdrawn\") | (studentInfo[\"final_result\"] == \"Fail\")] = 0\nstudentInfo[\"result.class\"].loc[(studentInfo[\"imd_band\"] == \"90-100%\") | (studentInfo[\"imd_band\"] == \"80-90%\") | (studentInfo[\"imd_band\"] == \"70-80%\") | (studentInfo[\"imd_band\"] == \"nan\")] = 0\nstudentInfo[\"result.class\"].loc[(studentInfo[\"disability\"] == \"N\")] = 0\n#and look at the dataset again\nstudentInfo.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We create one dataframe (Xfactors) with all the factors (variables) that we will use to assess whether a student will pass of fail the course"},{"metadata":{"trusted":true},"cell_type":"code","source":"Xfactors = studentInfo[[\"gender\", \"region\", \"highest_education\", \"imd_band\", \"age_band\", \"num_of_prev_attempts\", \"studied_credits\", \"disability\"]]\nX_noncat = pd.get_dummies(Xfactors)\n\nX_noncat.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we create another variable (Youtcome) which represents the outcome, that is what we want to assess. Here, we want to assess whether a student will pass the course successfully or not - which is represented by the variable \"result.class\". Please remember, 1 means the student passes the course, 0 means the student fails the course."},{"metadata":{"trusted":true},"cell_type":"code","source":"Youtcome = studentInfo[\"result.class\"].values\nYoutcome","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now its time to fit our model! This means that we will use \"old\" data - where we already know the outcome - to train the model. We will also keep a part of the old data to test our model's performance - that is whether the model learned to an acceptable degree to assess student performance.\nThe datasets used for training have the suffix \"_train\" while the datasets saved for testing have the suffix \"_test\".\nThe model is trained as a logistic regression binary classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"#fit the model\nX_train, X_test, y_train, y_test = train_test_split(X_noncat, Youtcome, test_size=0.3, random_state=0)\nOurModel = LogisticRegression()\nOurModel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will use our model (OurModel) to assess student performance using the test dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"#predict on a testset\ny_pred = OurModel.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(OurModel.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see from the results, our model can assess student performance correctly with a 61% accuracy. \nThis is not really good, is it? \nLets try to improve the accuracy by adding one more variable to the predictive features (the Xfactors dataframe): the students'average grade of the Teacher Marked Asssessments (TMA) assignments of the course.\n\nTo do that, we will need the data contained in the tables: assessment and studentAssessment.\nThe analysis follows."},{"metadata":{"trusted":true},"cell_type":"code","source":"#read additional data\nstudentAssessments = pd.read_csv(\"/kaggle/input/open-university-learning-analytics-dataset/anonymiseddata/studentAssessment.csv\")\nassessments = pd.read_csv(\"/kaggle/input/open-university-learning-analytics-dataset/anonymiseddata/assessments.csv\")\n\n#retrieve the ids only of the teacher assessments (TMA)\nTAM = assessments.loc[assessments['assessment_type'] == \"TMA\"]\n\n\n#then keep the students assessments (grades) that were only given by the teacher (TAM) and remove unknown entries (\"?\")\nTAM_student_grades = studentAssessments.loc[studentAssessments.id_assessment.isin(TAM[\"id_assessment\"])]\nTAM_student_grades = TAM_student_grades.loc[TAM_student_grades['score'] != '?']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create an empty list where we will save the average grade for each and every student\navg_grades = [] \n#for each student find all TMA scores for the course we are interested, and get the mean value\n\nfor i in range (0, len(studentInfo['id_student'])):\n    \n    this_student = studentAssessments.loc[(studentAssessments['id_student'] == studentInfo['id_student'][i]) &\n                                          (studentAssessments['score'] != '?')]\n    \n    assmt = list(this_student['id_assessment'])\n    score = list(this_student['score'])\n                 \n    #score = list(this_student['score'].astype(float))\n    \n    final_score = 0\n    for j in range(0, len(assmt)):\n        idx = assessments.loc[assessments.id_assessment == assmt[j]].index[0]\n        if((assessments.code_module[idx] == studentInfo['code_module'][i]) & (assessments.assessment_type[idx] == \"TMA\")):\n            final_score = final_score + (float(assessments.weight[idx])*score[j])/100\n            \n    avg_grades.append(final_score)\n    \n#add the new information about average TAM grades to the student information dataframe\n\nstudentInfo['avg_TMA_assessment'] = avg_grades","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#add the new information about average TAM grades to our model\n\nXfactors_updated = studentInfo[[\"gender\", \"region\", \"highest_education\", \"imd_band\", \"age_band\", \"num_of_prev_attempts\", \"studied_credits\", \"disability\", \"avg_TMA_assessment\"]]\nX_noncat_updated = pd.get_dummies(Xfactors_updated)\nX_noncat_updated = X_noncat_updated.fillna(0)\nX_noncat_updated.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fit the model again\nX_train, X_test, y_train, y_test = train_test_split(X_noncat_updated, Youtcome, test_size=0.3, random_state=0)\nOurModelUpdated = LogisticRegression()\nOurModelUpdated.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will use the updated model (OurModelUpdated) to assess student performance using the test dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"#predict on a testset\ny_predUpdated = OurModelUpdated.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(OurModelUpdated.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, what about that! The model now can assess students' performance with 83% accuracy!\nLooks like the grades of Teacher Marked Assignments really helped us to improve the performance of our model :)\nI wonder what else could help.... ;)"},{"metadata":{},"cell_type":"markdown","source":"How to evaluate the models and choose \"THE BEST\"?\n\nThe answer to this question is a bit like \"42\" (if you don't know what \"42\" is, look here: https://en.wikipedia.org/wiki/Phrases_from_The_Hitchhiker%27s_Guide_to_the_Galaxy#Answer_to_the_Ultimate_Question_of_Life,_the_Universe,_and_Everything_(42))\n\nThe important thing here is not the answer, but the QUESTION. How do we define \"the best\"? The \"best\" in terms of what? And is even our best, good enough (and how to deal with it if not)?\n\nLets say that we need to choose \"the best\" from the two models we built last week: OurModel and OurModelUpdated (amazing names, i know.....)\n\nHow can we do that?"},{"metadata":{},"cell_type":"markdown","source":"**Confusion Matrix**\nA confusion matrix is a table that is often used to describe the performance of a classification model - like our model! The confusion matrix allows us to identify which classes were predicted correctly and which ones were misspredicted - for example, if our model tends to identify students as at risk while they actually are not. You can read more about confusion matrices, here:\n\nhttps://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/"},{"metadata":{"trusted":true},"cell_type":"code","source":"#in our example...\n#lets calculate the confusion matrix for the first model: OurModel\nfrom sklearn.metrics import confusion_matrix\n\nOurModelCM = confusion_matrix(y_test, y_pred)\nprint(OurModelCM)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results above show the following: a) 3390 people were predicted as failing the course and they indeed failed b) 1727 people were predicted as passing the course and they failed c) 2081 people were predicted as failing the course and they passed d) 2580 people were predicted as passing the course and they indeed passed\n\nOverall, 1727 + 2081 = 3808 people were misclassified! (not good...)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#now lets calculate the confusion matrix for the second model: OurModelUpdated\n\nOurModelUpdatedCM = confusion_matrix(y_test, y_predUpdated)\nprint(OurModelUpdatedCM)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results above show the following: a) 4354 people were predicted as failing the course and they indeed failed b) 763 people were predicted as passing the course and they failed c) 866 people were predicted as failing the course and they passed d) 3795 people were predicted as passing the course and they indeed passed\n\nOverall, 763 + 866 = 1629 people were misclassified! (better than before but is if acceptable?)"},{"metadata":{},"cell_type":"markdown","source":"**Precision, recall, F-measure and support metrics**\nThe precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier to not label a sample as positive if it is negative.\n\nThe recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n\nThe F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n\nThe F-beta score weights the recall more than the precision by a factor of beta. beta = 1.0 means recall and precision are equally important.\n\nThe support is the number of occurrences of each class in y_test."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\n#compute the above metrics for the first model (OurModel)\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#compute the above metrics for the second model (OurModelUpdated)\nprint(classification_report(y_test, y_predUpdated))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ROC curves**\nThe receiver operating characteristic (ROC) curve is another common tool used with binary classifiers. The dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner)."},{"metadata":{"trusted":true},"cell_type":"code","source":"#try out the ROC curve of the first model (OurModel)\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nimport matplotlib.pyplot as plt \n\nX_train, X_test, y_train, y_test = train_test_split(X_noncat, Youtcome, test_size=0.3, random_state=0)\nOurModel = LogisticRegression()\nOurModel.fit(X_train, y_train)\n\nlogit_roc_auc = roc_auc_score(y_test, OurModel.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, OurModel.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic for Original Model')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#try out the ROC curve of the second model (OurModelUpdated)\n\nX_train, X_test, y_train, y_test = train_test_split(X_noncat_updated, Youtcome, test_size=0.3, random_state=0)\nOurModelUpdated = LogisticRegression()\nOurModelUpdated.fit(X_train, y_train)\n\nlogit_roc_auc = roc_auc_score(y_test, OurModelUpdated.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, OurModelUpdated.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic for Updated Model')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Last thing to discuss...**\n\nI've tried to demontstrate a few alternatives that can help you choose the \"best model\" for the needs of the second assignment but also for our own research :)\n\nPlease let me know your thoughts, questions or ideas about this topic!\n\nCheers :)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}