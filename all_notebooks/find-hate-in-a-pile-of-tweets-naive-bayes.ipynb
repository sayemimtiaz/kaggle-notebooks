{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://www.pewtrusts.org/-/media/post-launch-images/2018/01/sln_jan23_1/sln_jan23_1_16x9.jpg)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nThe Me Too (or #MeToo) movement, with variations of related local or international names, is a movement against sexual harassment and sexual abuse where people publicize their allegations of sex crimes committed by powerful and/or prominent men. <br>\n\nThe dataset contains Twitter posts (tweets) made during the MeToo movement by various Twitter accounts and some of them as been classifed as hateful (1), whilst others are more benign (0). Our job is to build a simple classifier that can distingiush hateful and non-hateful tweets.\n\nThis notebook is based mostly on Sklearn documention and subsequent TDS article:\n\nhttps://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html <br>\nhttps://towardsdatascience.com/naive-bayes-document-classification-in-python-e33ff50f937e <br>\n\nKey takeaways:\n\n* Naive-Bayes is simple and does at good job at classifying text when looking at overall precession.\n* The data is imbalanced (lot more non-hate and than hate tweets), so NB has a low recall score (many false negatives)\n* Version 1 used ONLY the text of the tweet\n* Version 2+ uses the text of the tweet AND statistics about the tweet (likes, retweets etc).\n* Version 2+ slightly improves classification score using Naive-Bayes.\n\nWe start by loading the data and do some simple EDA to get the lay of the land. Loading all tweets (about 700000) causes the kernel here on Kaggle to run out of memory, so instead we load about half the dataset (300000) which will be enough for a demonstration.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\npd.reset_option('^display.', silent=True)\n\n# Load half the data and separate target from predictors\nX = pd.read_csv('../input/hatred-on-twitter-during-metoo-movement/MeTooHate.csv', nrows=300000)\nX.dropna(axis=0, subset=['text', 'category'], inplace=True)\ny = X.category\nX.drop(['category'], axis=1, inplace=True)\n\n# Drop columns not used for modelling\ncols_to_drop = ['status_id', 'created_at', 'location']\nX.drop(cols_to_drop, axis=1, inplace=True)\n\n# Split the data while maintaining the proportion of hate/non-hate (stratify) \nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25)\n\n# Reset the index\nX_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\nX_test_stats = X_test.copy()\n\nprint(\"Total training samples:\", len(X_train))\nprint(\"Total test samples:\", len(X_test))\n\nX_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show descriptive statistics of training set\nX_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show how many values are non-null for each feature\nX_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print a random tweet as a sample\nsample_index = 25\nprint(X_train.iloc[sample_index])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the target label and notice that it is imbalanced\n\ny_train.value_counts().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature encoding\n\nOur job here is to transform the text feature (the tweet) into vectors, that a classifier can understand. In this notebook we use the Naive Bayes classifier. Our classifier needs to be able to calculate how many times each word appears in each document and how many times it appears in each category. To make this possible, the data needs to look something like this: <br>\n\n[0, 1, 0, …] <br>\n[1, 1, 1, …] <br>\n[0, 2, 0, …] <br>\n\nEach row represents a document, and each column represents a word. The first row might be a document that contains a zero for “dumb,” a one for “the” and a zero for “hate”. That means that the document contains one instance of the word “the”, but no “dumb” or “hate.” <br>\n\nWe'll use Scikit Learn’s CountVectorizer to turn the tweets into count vectors. CountVectorizer creates a vector of word counts for each abstract to form a matrix. Each index corresponds to a word and every word appearing in the texts is represented.\n\nSource: https://towardsdatascience.com/naive-bayes-document-classification-in-python-e33ff50f937e","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert the text feature into a vectors of tokens\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(strip_accents='ascii', token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b',\n                             lowercase=True, stop_words='english')\nX_train_cv = cv.fit_transform(X_train.text)\nX_test_cv = cv.transform(X_test.text)\n\n# Scale numerical features (followers, retweets etc.)\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ncols = ['favorite_count', 'retweet_count', 'followers_count', 'friends_count', 'statuses_count']\nX_train_sc = scaler.fit_transform(X_train[cols])\nX_test_sc = scaler.transform(X_test[cols])\n\n# Merge the numerical features with our count vectors\nimport scipy.sparse as sp\ntrain_count = sp.csr_matrix(X_train_cv)\ntrain_num = sp.csr_matrix(X_train_sc)\nX_train = sp.hstack([train_count, train_num])\n\ntest_count = sp.csr_matrix(X_test_cv)\ntest_num = sp.csr_matrix(X_test_sc)\nX_test = sp.hstack([test_count, test_num])\n\n# Save top words for training set\nword_freq_df = pd.DataFrame(X_train_cv.toarray(), columns=cv.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we'll print the top 20 words occuring our tweets. Not surprisingly, women and movement are the two most occuring words.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top 20 words occuring in tweets\npd.DataFrame(word_freq_df.sum()).sort_values(0, ascending=False).head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model\n\nNaive Bayes classification makes use of Bayes theorem to determine the probability that each sample (tweet) belongs to a certain category. If my tweet contains the words \"hate\", \"go\" and \"away\", what’s the probability that it falls in the category “hate” rather than “non-hate\"? Naive Bayes sorts the samples in two groups based on the highest probability for each sample.\n\nWe'll use the **MultinomialNB** since it is suitable for classifying a multinomial (binomial) model from discreate features (e.g., word counts for text).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train a Naive-Bayes classifier to classify hate/non-hate tweets\n\nfrom sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB()\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot scores and make a confusion matrix for non-hate/hate predictions\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.metrics import confusion_matrix\nn_classes = 2\ncm = confusion_matrix(y_test, predictions, labels=range(n_classes))\n\nprint(f'Number of samples to classify: {len(X_test.toarray())}\\n')\nprint(f'Accuracy score: {accuracy_score(y_test, predictions)}')\nprint(f'Precision score: {precision_score(y_test, predictions)}')\nprint(f'Recall score: {recall_score(y_test, predictions)}\\n')\nprint(f'Confusion matrix: \\n{cm}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A few remarks on the notation and the scores:\n\n**Accuracy score:** Out of all tweets, how many did we label correctly? <br>\n(True positives + true negatives) / total observations: (4866 + 64190) / 74712 <br>\n\n**Precision score:** Out of all hate tweets, how many did we get right? <br>\nTrue positives / (true positives + false positives): 4866 / (4866 + 1537)\n\n**Recall score:** Out of all true hate tweets, how many did we label correctly? <br>\nTrue positives / (true positives + false negatives): 4866 / (4866 + 4119)\n\nSince most of the training tweets are true-positive (non-hate) tweets with about a 80-20 ratio in favor, our classifer is good at classifying the non-hate ones, but struggles to classify the ones with hatred (notice how many false positives we have).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize the confusion matrix and plot it\n\nplt.figure(figsize=(6,6))\ncm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nsns.heatmap(cm, square=True, annot=True, cbar=False,\n            xticklabels=['non-hate', 'hate'], yticklabels=['non-hate', 'hate'])\nplt.xlabel('Predicted label')\nplt.ylabel('True label')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It shows that **MultinomialNB** struggles with tweets that are in fact hateful and oftentimes we predict tweets to be hateful, when they are in fact not. Note taht the second confusion matrix is normalized (rows sum to one), which makes our model looks more dire than it actually is. We do get most tweets right (91%).\n\nNext we plot a simple ROC curve that shows the true positive rate vs the false positive rate.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the ROC curve for the MNB classifier\nfrom sklearn.metrics import roc_curve\nfpr, tpr, _ = roc_curve(y_test, predictions)\nplt.figure(figsize=(8,8))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr, label='MNB')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show how the first 50 test tweets were classified and their true label\ntesting_predictions = []\nfor i in range(len(X_test.toarray())):\n    if predictions[i] == 1:\n        testing_predictions.append('Hate')\n    else:\n        testing_predictions.append('Non-hate')\ncheck_df = pd.DataFrame({'actual_label': list(y_test), 'prediction': testing_predictions, 'text':list(X_test_stats.text)})\ncheck_df.replace(to_replace=0, value='Non-hate', inplace=True)\ncheck_df.replace(to_replace=1, value='Hate', inplace=True)\ncheck_df.iloc[:50]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}