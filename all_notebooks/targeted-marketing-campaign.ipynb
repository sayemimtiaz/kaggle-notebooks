{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_excel(\"/kaggle/input/arketing-campaign/marketing_campaign.xlsx\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **EDA**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing data\ndef missing_rep(df):\n    miss = df.isna().sum()\n    miss = miss[miss>0]\n    miss_p = miss/df.shape[0]\n    miss_t = miss_p>0.03\n\n    return pd.DataFrame({\"Missings\" : miss, \"Proportion of Missings\" : miss_p, \"Higher than 3%\" : miss_t})\n\n\nmissing_rep(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def highlight_above_threshold(val):\n    if val < .15:\n        color = 'red'\n    else:\n        color = 'black'\n    #color = 'red' if val < .15 else 'black'\n    return 'color: %s' % color","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_c = [\"Education\", \"Marital_Status\", \"Kidhome\", \"Teenhome\", \"AcceptedCmp1\", \"AcceptedCmp2\", \"AcceptedCmp3\", \"AcceptedCmp4\",\n         \"AcceptedCmp5\", \"Complain\"]\n\n# Categorical features analysis\ndef cat_feat_describe(df, fc, target, n, thresh):\n\n    fl = []\n    if (type(fc)==list):\n    \n        for feature in fc:\n            fl.append(df.groupby([feature]).agg({target : [\"count\", \"mean\"]}))    \n\n            fm = pd.concat(fl, keys=fc)\n\n            fm = pd.DataFrame({\"Number of observations\" : fm.iloc[:,0], \"Discrimination ability\" : fm.iloc[:,1],\n                                 \"More than n observations\" : fm.iloc[:,0]>n})\n    else:\n        fm = (df.groupby(fc).agg({target : [\"count\", \"mean\"]}))\n        \n        fm = pd.DataFrame({\"Number of observations\" : fm.iloc[:,0], \"Discrimination ability\" : fm.iloc[:,1],\n                                 \"More than n observations\" : fm.iloc[:,0]>n})\n        \n    return fm\n\n\nfeat_sum = cat_feat_describe(data, feat_c, \"Response\", 40, 0.15)\nfeat_sum.style.applymap(highlight_above_threshold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_ = data.copy()\n\nlow_discriminability_cat = [\"Absurd\", \"Alone\", \"YOLO\", \"Married\", \"Together\"]\ndata_['Marital_Status'].loc[data_['Marital_Status'].isin(low_discriminability_cat)] = 'Other'\ndata_.groupby(\"Marital_Status\").count().index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"low_discriminability_cat = [\"Graduation\", \"2n Cycle\", \"Basic\"]\ndata_['Education'].loc[data_['Education'].isin(low_discriminability_cat)] = 'Other'\ndata_.groupby(\"Education\").count().index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_['NumberOff'] = data_['Kidhome'] + data_['Teenhome']\nfeat_c.append(\"NumberOff\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cat_feat_plot(df, fc, target, thresh):\n\n    sns.set_style(\"whitegrid\")    \n    fig = plt.figure()\n    fig.subplots_adjust(hspace=0.2, wspace=0.4)\n    i=1\n    \n    for feat in fc:\n        plot_df = cat_feat_describe(df, feat, target, 50, thresh).iloc[:,1]\n        plot_df = plot_df.sort_values(ascending=False)\n        plot_df = pd.DataFrame(plot_df)\n        plot_df = pd.DataFrame(plot_df.reset_index())\n        ax =sns.barplot(plot_df[feat], plot_df['Discrimination ability'], palette='magma')\n        ax.set_ylabel(\"\", size = 10)\n        ax.axhline(y=thresh, color=\"black\", ls='--')\n        ax.set_xticklabels(plot_df.index, size=10)\n        \n        if i<11:\n            plt.subplot(2, 5, i)\n            i+=1\n        \n        fig.suptitle('Discrimination ability of categories', ha='center',\n                     va='center', fontsize=15, y=0.92, fontweight='bold')\n        fig.yaxis_title='Discrimination ability'\n        fig.text(0.07, 0.5,'Discrimination ability', ha='center', va='center',\n                 rotation='vertical', fontsize=15)\n        fig.set_figheight(9)\n        fig.set_figwidth(16)\n        \ncat_feat_plot(data_, feat_c, \"Response\", 0.15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Transforms date format in days","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def days_since(dates_series, date_format):\n    n = len(dates_series)\n    result = [0] * n\n\n    for i in range(n):\n        result[i] = (datetime.today()-datetime.strptime(dates_series[i], date_format)).days\n    \n    return result\n\ndata_[\"Days_Customer\"] = days_since(list(data_.Dt_Customer), \"%Y-%m-%d\")\ndata_ = data_.drop(columns=\"Dt_Customer\")\ndata_[\"Days_Customer\"].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Describes numerical attributes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_n = list(data_.columns)\nfeat_n = list(filter(lambda x: x not in feat_c, feat_n))\n\nfeat_n.remove(\"ID\") # Removing ID column\n\ndata_[feat_n].describe() # Describing only Numerical Variables","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Drops constant variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"std = data_[feat_n].describe().iloc[2,:]\nconst_lab = [std[std<0.05].index[0], std[std<0.05].index[1]]\nstd[std<0.05]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_.drop(labels=const_lab, axis=1, inplace=True)\nfeat_n = list(filter(lambda x: x not in const_lab, feat_n))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Removes inconsistant age","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_[(2020 - data_[\"Year_Birth\"])>90]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_ = data_[(2020 - data_[\"Year_Birth\"])<=90]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Drops rows with less missing *income* (about 1% of the data)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_ = data_[~data_['Income'].isna()]\ndata_.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Correlation matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# The function to \"zoom\" in the correlation matrix.\ndef magnify():\n    return [dict(selector=\"th\",\n                 props=[(\"font-size\", \"7pt\")]),\n            dict(selector=\"td\",\n                 props=[('padding', \"0em 0em\")]),\n            dict(selector=\"th:hover\",\n                 props=[(\"font-size\", \"12pt\")]),\n            dict(selector=\"tr:hover td:hover\",\n                 props=[('max-width', '200px'),\n                        ('font-size', '12pt')])]\n\ndef corr_matrix(df):\n    # Compute the correlation matrix\n    corr = df.corr()\n    cmap = sns.diverging_palette(5, 250, as_cmap=True)\n    \n    vis = corr.style.background_gradient(cmap, axis=1)\\\n            .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\\\n            .set_caption(\"Hover to magify\")\\\n            .set_precision(2)\\\n            .set_table_styles(magnify())\n\n    return vis\n\nfeat_n_ = feat_n.copy()\nfeat_n.remove(\"Response\") # Removing the Targer variable from the list of numerical features to be analyzed by correlation.\ncorr_matrix(data_[feat_n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = data_[feat_n].corr()\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\nplt.figure(figsize=(11,9))\n\ncmap = sns.diverging_palette(5, 250, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.6, vmin=-.6, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.title(\"Numerical features correlation matrix\", fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Distribution according to response","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def num_feat_plot(df, feat_nlist, target, feat_clist = None):\n    \n    sns.set_style(\"darkgrid\")    \n    fig = plt.figure()\n    fig.subplots_adjust(hspace=0.2, wspace=0.4)\n    i=1\n    \n    if(target in feat_nlist):\n        feat_nl = feat_nlist.copy()\n        feat_nl.remove(target)\n\n    \n    for feat in feat_nl:\n        sns.violinplot(data = df, y = feat, x = target)\n        ax=sns.violinplot(data = df, y = feat, x = target, palette=\"Reds\")\n        ax.set_xlabel(\"\", size = 10)\n        \n        if i<15:\n            plt.subplot(3, 5, i)\n            i+=1\n        \n        fig.suptitle('Distribution by response', ha='center',\n                     va='center', fontsize=15, y=0.92, fontweight='bold')\n        fig.text(0.5, 0.06, 'Response', ha='center', va='center', fontsize=14)\n        fig.set_figheight(10)\n        fig.set_figwidth(17)\n\n    return\n\nnum_feat_plot(data_, feat_n_, \"Response\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Outlier detection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import IsolationForest\n\ndef anom_plot(df, num_feat_list, l, c):\n    sns.set_style(\"whitegrid\") \n    fig, axs = plt.subplots(l, c, figsize=(22, 12), facecolor='w', edgecolor='k')\n    axs = axs.ravel()\n\n    for i, column in enumerate(num_feat_list):\n        isolation_forest = IsolationForest(n_estimators=500, contamination=\"auto\")\n        isolation_forest.fit(df[column].values.reshape(-1,1))\n\n        xx = np.linspace(df[column].min(), df[column].max(), len(df)).reshape(-1,1)\n        anomaly_score = isolation_forest.decision_function(xx)\n        outlier = isolation_forest.predict(xx)\n    \n        axs[i].plot(xx, anomaly_score, label='anomaly score')\n        axs[i].fill_between(xx.T[0], np.min(anomaly_score), np.max(anomaly_score), \n                     where=outlier==-1, color='r', \n                     alpha=.4, label='outlier region')\n        axs[i].legend()\n        axs[i].set_title(column)\n        \n    fig.suptitle('Anomaly detection', ha='center',\n                     va='center', fontsize=20, y=0.92, fontweight='bold')\n        \n    return\n    \nanom_plot(data_, feat_n, 3, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following cell defines two utility functions to semi-automatically identify outliers, through univariate perspective, in a pandas.Series using standard deviation from the mean (filter_by_std) and interquartile range (filter_by_iqr).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def filter_by_iqr(series_, k=1.5, return_thresholds=False):\n    q25, q75 = np.percentile(series_, 25), np.percentile(series_, 75)\n    iqr = q75-q25\n    \n    cutoff = iqr*k\n    lower_bound, upper_bound = q25-cutoff, q75+cutoff\n    \n    if return_thresholds:\n        return lower_bound, upper_bound\n    else:\n        return [True if i < lower_bound or i > upper_bound else False for i in series_]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import KBinsDiscretizer\n\ndef bivariate_outlier_id_plot(df, list_num_features, target, n_bins=20):\n    sns.set_style(\"darkgrid\") \n    fig = plt.figure(figsize=(22, 12))\n    color = \"floralwhite\"\n    i=1\n    for feature in list_num_features:\n        if feature == \"Income\":\n            ser = df[feature].copy()\n            ser.dropna(inplace=True)\n        else:\n            ser = df[feature]\n          \n        # box plots\n        thresholds = filter_by_iqr(ser, 1.5, True)\n        outliers = df[[feature, target]][df[feature]>thresholds[1]]\n\n        ax = fig.add_subplot(5, 3, i)\n\n        box = ax.boxplot(ser, flierprops=dict(markerfacecolor='r', marker='s'), \n                         vert=False, patch_artist=True, sym=\"w\")                                                                  \n        ax.plot(outliers.iloc[:, 0][outliers.iloc[:, 1]==0], np.ones(sum(outliers.iloc[:, 1]==0)), color=\"black\", marker = \"o\", markersize=4)\n        ax.plot(outliers.iloc[:, 0][outliers.iloc[:, 1]==1], np.ones(sum(outliers.iloc[:, 1]==1)), color=\"red\", marker = \"D\", markersize=6)\n        ax.set_title(feature)\n        box['boxes'][0].set_facecolor(color)\n\n        i+=1\n        \n    fig.suptitle('Variable Distribution',  ha='center',\n                     va='center', fontsize=20, y=1.03, fontweight='bold')\n\n    plt.tight_layout()\n    plt.show()\n\nbivariate_outlier_id_plot(data_, feat_n, \"Response\", n_bins=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Split dataset**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"seeds = [4, 56, 92, 105, 400]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data_,\n                                                    data_[\"Response\"],\n                                                    test_size=0.3,\n                                                    random_state=seeds[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Multivariate outlier detection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Simple function to check if the matrix is positive definite \n#(for example, it will return False if the matrix contains NaN).\ndef is_pos_def(A):\n    if np.allclose(A, A.T):\n        try:\n            np.linalg.cholesky(A)\n            return True\n        except np.linalg.LinAlgError:\n            return False\n    else:\n        return False \n\n# The function to calculate the Mahalanobis Distance. Returns a list of distances.\ndef MahalanobisDist(data):\n    covariance_matrix = np.cov(data, rowvar=False)\n    if is_pos_def(covariance_matrix):\n        inv_covariance_matrix = np.linalg.inv(covariance_matrix)\n        if is_pos_def(inv_covariance_matrix):\n            vars_mean = []\n            for i in range(data.shape[0]):\n                vars_mean.append(list(data.mean(axis=0)))\n            diff = data - vars_mean\n            md = []\n            for i in range(len(diff)):\n                md.append(np.sqrt(diff[i].dot(inv_covariance_matrix).dot(diff[i])))\n            return md\n        else:\n            print(\"Error: Inverse of Covariance Matrix is not positive definite!\")\n    else:\n        print(\"Error: Covariance Matrix is not positive definite!\")\n        \ndef MD_detectOutliers(data, extreme=False):\n    MD = MahalanobisDist(data)\n\n    std = np.std(MD)\n    k = 3. * std if extreme else 2. * std\n    m = np.mean(MD)\n    up_t = m + k\n    low_t = m - k\n    outliers = []\n    for i in range(len(MD)):\n        if (MD[i] >= up_t) or (MD[i] <= low_t):\n            outliers.append(i)  # index of the outlier\n    return np.array(outliers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_aux = X_train[feat_n_]\noutliers_i = MD_detectOutliers(np.array(data_aux))\nlen(outliers_i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outliers = pd.DataFrame()\nfor i in outliers_i:\n    outliers = outliers.append(data_aux.iloc[i,:])\n    \noutliers.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drops the outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train[~X_train.index.isin(outliers.index)]\ny_train = y_train[~y_train.index.isin(outliers.index)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Feature Engineering**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Business-oriented features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Percentage of Monetary Units spent on gold products out of the total spent\naux = [0]* X_train.shape[0]\n\nfor i in range(X_train.shape[0]):\n    aux[i] = X_train[\"MntGoldProds\"].iloc[i]/sum(X_train[['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts']].iloc[i,:])\n    \n    \nX_train[\"PrpGoldProds\"] = aux\nX_train[\"PrpGoldProds\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aux = [0]* X_test.shape[0]\n\nfor i in range(X_test.shape[0]):\n    aux[i] = X_test[\"MntGoldProds\"].iloc[i]/sum(X_test[['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts']].iloc[i,:])\n    \n    \nX_test[\"PrpGoldProds\"] = aux\nX_test[\"PrpGoldProds\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of Accepted Campaigns out of the last 5 Campaigns\naux = [0]* X_train.shape[0]\n\n\nfor i in range(X_train.shape[0]):\n    aux[i] = sum(X_train[['AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1', 'AcceptedCmp2']].iloc[i,:])\n    \n    \nX_train[\"NmbAccCmps\"] = aux\nX_train[\"NmbAccCmps\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of Accepted Campaigns out of the last 5 Campaigns\naux = [0]* X_test.shape[0]\n\n\nfor i in range(X_test.shape[0]):\n    aux[i] = sum(X_test[['AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1', 'AcceptedCmp2']].iloc[i,:])\n    \n    \nX_test[\"NmbAccCmps\"] = aux\nX_test[\"NmbAccCmps\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Proportion of Accepted Campaigns out of the last 5 Campaigns\naux = [0]* X_train.shape[0]\n\nfor i in range(X_train.shape[0]):\n    aux[i] = sum(X_train[['AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1', 'AcceptedCmp2']].iloc[i,:])/5\n    \nX_train[\"PrpAccCmps\"] = aux\nX_train[\"PrpAccCmps\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Proportion of Accepted Campaigns out of the last 5 Campaigns\naux = [0]* X_test.shape[0]\n\nfor i in range(X_test.shape[0]):\n    aux[i] = sum(X_test[['AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1', 'AcceptedCmp2']].iloc[i,:])/5\n    \nX_test[\"PrpAccCmps\"] = aux\nX_test[\"PrpAccCmps\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Proportion of Monetary Units spent on Wine out of the total spent\naux = [0]* X_train.shape[0]\n\nfor i in range(X_train.shape[0]):\n    aux[i] = float(X_train[[\"MntWines\"]].iloc[i,:]/sum(X_train[['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts']].iloc[i,:]))\n    \nX_train[\"PrpWines\"] = aux\nX_train[\"PrpWines\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Proportion of Monetary Units spent on Wine out of the total spent\naux = [0]* X_test.shape[0]\n\nfor i in range(X_test.shape[0]):\n    aux[i] = float(X_test[[\"MntWines\"]].iloc[i,:]/sum(X_test[['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts']].iloc[i,:]))\n    \nX_test[\"PrpWines\"] = aux\nX_test[\"PrpWines\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Proportion of Monetary Units spent on Fruits out of the total spent\naux = [0]* X_train.shape[0]\n\nfor i in range(X_train.shape[0]):\n    aux[i] = float(X_train[[\"MntFruits\"]].iloc[i,:]/sum(X_train[['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts']].iloc[i,:]))\n    \nX_train[\"PrpFruits\"] = aux\nX_train[\"PrpFruits\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Proportion of Monetary Units spent on Fruits out of the total spent\naux = [0]* X_test.shape[0]\n\n\nfor i in range(X_test.shape[0]):\n    aux[i] = float(X_test[[\"MntFruits\"]].iloc[i,:]/sum(X_test[['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts']].iloc[i,:]))\n    \nX_test[\"PrpFruits\"] = aux\nX_test[\"PrpFruits\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Proportion of Monetary Units spent on Meat out of the total spent\naux = [0]* X_train.shape[0]\n\nfor i in range(X_train.shape[0]):\n    aux[i] = float(X_train[[\"MntMeatProducts\"]].iloc[i,:]/sum(X_train[['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts']].iloc[i,:]))\n    \nX_train[\"PrpMeat\"] = aux\nX_train[\"PrpMeat\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Proportion of Monetary Units spent on Meat out of the total spent\naux = [0]* X_test.shape[0]\n\n\nfor i in range(X_test.shape[0]):\n    aux[i] = float(X_test[[\"MntMeatProducts\"]].iloc[i,:]/sum(X_test[['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts']].iloc[i,:]))\n    \nX_test[\"PrpMeat\"] = aux\nX_test[\"PrpMeat\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Proportion of Monetary Units spent on Fish out of the total spent\naux = [0]* X_train.shape[0]\n\nfor i in range(X_train.shape[0]):\n    aux[i] = float(X_train[[\"MntFishProducts\"]].iloc[i,:]/sum(X_train[['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts']].iloc[i,:]))\n    \nX_train[\"PrpFish\"] = aux\nX_train[\"PrpFish\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Proportion of Monetary Units spent on Fish out of the total spent\naux = [0]* X_test.shape[0]\n\nfor i in range(X_test.shape[0]):\n    aux[i] = float(X_test[[\"MntFishProducts\"]].iloc[i,:]/sum(X_test[['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts']].iloc[i,:]))\n    \nX_test[\"PrpFish\"] = aux\nX_test[\"PrpFish\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Monetary\naux = [0]* X_train.shape[0]\n\nfor i in range(X_train.shape[0]):\n    aux[i] = sum(X_train[['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts']].iloc[i,:])\n    \nX_train[\"Mnt\"] = aux\nX_train[\"Mnt\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Monetary\naux = [0]* X_test.shape[0]\n\nfor i in range(X_test.shape[0]):\n    aux[i] = sum(X_test[['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts']].iloc[i,:])\n    \nX_test[\"Mnt\"] = aux\nX_test[\"Mnt\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Buy Potential\naux = [0]* X_train.shape[0]\n\nfor i in range(X_train.shape[0]):\n    aux[i] = float(sum(X_train[['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts']].iloc[i,:])/((X_train[[\"Income\"]].iloc[i,:])*2))   \n    \nX_train[\"BuyPot\"] = aux\nX_train[\"BuyPot\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Buy Potential\naux = [0]* X_test.shape[0]\n\nfor i in range(X_test.shape[0]):\n    aux[i] = float(sum(X_test[['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts']].iloc[i,:])/((X_test[[\"Income\"]].iloc[i,:])*2))\n    \nX_test[\"BuyPot\"] = aux\nX_test[\"BuyPot\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Frequency\naux = [0]* X_train.shape[0]\n\nfor i in range(X_train.shape[0]):\n    aux[i] = sum(X_train[['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']].iloc[i,:])\n    \nX_train[\"Freq\"] = aux\nX_train[\"Freq\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Frequency\naux = [0]* X_test.shape[0]\n\nfor i in range(X_test.shape[0]):\n    aux[i] = sum(X_test[['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']].iloc[i,:])\n    \nX_test[\"Freq\"] = aux\nX_test[\"Freq\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating RFM feature using Recency, Freq and Mnt:\nfeature_list, n_bins = [\"Recency\", \"Freq\", \"Mnt\"], 5\nrfb_dict = {}\nfor feature in feature_list:\n    bindisc = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy=\"quantile\")\n    feature_bin = bindisc.fit_transform(X_train[feature].values[:, np.newaxis])\n    feature_bin = pd.Series(feature_bin[:, 0], index=X_train.index)\n    feature_bin += 1\n    \n    if feature == \"Recency\":\n        feature_bin = feature_bin.sub(5).abs() + 1\n    rfb_dict[feature+\"_bin\"] = feature_bin.astype(int).astype(str)\n\nX_train[\"RFM\"] = (rfb_dict['Recency_bin'] + rfb_dict['Freq_bin'] + rfb_dict['Mnt_bin']).astype(int)\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating RFM feature using Recency, Freq and Mnt:\nfeature_list, n_bins = [\"Recency\", \"Freq\", \"Mnt\"], 5\nrfb_dict = {}\nfor feature in feature_list:\n    bindisc = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy=\"quantile\")\n    feature_bin = bindisc.fit_transform(X_test[feature].values[:, np.newaxis])\n    feature_bin = pd.Series(feature_bin[:, 0], index=X_test.index)\n    feature_bin += 1\n    \n    if feature == \"Recency\":\n        feature_bin = feature_bin.sub(5).abs() + 1\n    rfb_dict[feature+\"_bin\"] = feature_bin.astype(int).astype(str)\n\nX_test[\"RFM\"] = (rfb_dict['Recency_bin'] + rfb_dict['Freq_bin'] + rfb_dict['Mnt_bin']).astype(int)\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scales the features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_n.extend(('PrpGoldProds',\n       'NmbAccCmps', 'PrpAccCmps', 'PrpWines', 'PrpFruits', 'PrpMeat',\n       'PrpFish', 'Mnt', 'BuyPot', 'Freq', 'RFM'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nsuffix = \"_t\"\n\ndata_scaler = X_train[feat_n]\ndata_scaler_test = X_test[feat_n]\n\nfscaler = MinMaxScaler()\nscaled_d = fscaler.fit_transform(data_scaler.values)\nscaled_d_test = fscaler.transform(data_scaler_test.values)\n\ncolnames = [s + suffix for s in data_scaler.columns]\n\nX_train = pd.concat([X_train, pd.DataFrame(scaled_d, index=data_scaler.index, columns=colnames)], axis=1)\nX_test = pd.concat([X_test, pd.DataFrame(scaled_d_test, index=data_scaler_test.index, columns=colnames)], axis=1)\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Box-Cox transformation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\n# Receives a dataframe consisting only of scaled features and the target, and the name of the target feature.\n# Returns both the dataframe with the features already transformed to the best transformation and a dictionary\n# containing the name of each feature with its best transformation name.\ndef power_transf(df, target_feat):\n\n    # define a set of transformations\n    trans_dict = {\"x\": lambda x: x, \"log\": np.log, \"sqrt\": np.sqrt, \n                  \"exp\": np.exp, \"**1/4\": lambda x: np.power(x, 0.25), \n                  \"**2\": lambda x: np.power(x, 2), \"**4\": lambda x: np.power(x, 4)}\n\n    target = target_feat\n    best_power_dict = {}\n    for feature in df.columns[:-1]:\n        max_test_value, max_trans, best_power_trans = 0, \"\", None\n        for trans_key, trans_value in trans_dict.items():\n            # apply transformation\n            feature_trans = trans_value(df[feature])\n            if trans_key == \"log\":\n                feature_trans.loc[np.isfinite(feature_trans)==False] = -50.\n\n            # bin feature\n            bindisc = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy=\"uniform\")\n            feature_bin = bindisc.fit_transform(feature_trans.values[:, np.newaxis])\n            feature_bin = pd.Series(feature_bin[:, 0], index=df.index)\n\n            # obtain contingency table\n            df_ = pd.DataFrame(data={feature: feature_bin, target: df[target]})\n            cont_tab = pd.crosstab(df_[feature], df_[target], margins = False)        \n\n            # compute Chi-Squared\n            chi_test_value = stats.chi2_contingency(cont_tab)[0]\n            if chi_test_value > max_test_value:\n                max_test_value, max_trans, best_power_trans = chi_test_value, trans_key, feature_trans      \n\n        best_power_dict[feature] = (max_test_value, max_trans, best_power_trans)\n        df[feature] = best_power_trans\n        \n    return df, best_power_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def power_transf(X_train, X_test, target_feat):\n\n    # define a set of transformations\n    trans_dict = {\"x\": lambda x: x, \"log\": np.log, \"sqrt\": np.sqrt, \n                  \"exp\": np.exp, \"**1/4\": lambda x: np.power(x, 0.25), \n                  \"**2\": lambda x: np.power(x, 2), \"**4\": lambda x: np.power(x, 4)}\n\n    target = target_feat\n    best_power_dict = {}\n    for feature in X_train.columns[:-1]:\n        max_test_value, max_trans, best_power_trans = 0, \"\", None\n        for trans_key, trans_value in trans_dict.items():\n            # apply transformation\n            feature_trans = trans_value(X_train[feature])\n            feature_trans_test = trans_value(X_test[feature])\n            if trans_key == \"log\":\n                feature_trans.loc[np.isfinite(feature_trans)==False] = -50.\n                feature_trans_test.loc[np.isfinite(feature_trans_test)==False] = -50.\n\n            # bin feature\n            bindisc = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy=\"uniform\")\n            feature_bin = bindisc.fit_transform(feature_trans.values[:, np.newaxis])\n            feature_bin = pd.Series(feature_bin[:, 0], index=X_train.index)\n\n            # obtain contingency table\n            df_ = pd.DataFrame(data={feature: feature_bin, target: X_train[target]})\n            cont_tab = pd.crosstab(df_[feature], df_[target], margins = False)        \n\n            # compute Chi-Squared\n            chi_test_value = stats.chi2_contingency(cont_tab)[0]\n            if chi_test_value > max_test_value:\n                max_test_value, max_trans, best_power_trans = chi_test_value, trans_key, feature_trans      \n\n        best_power_dict[feature] = (max_test_value, max_trans, best_power_trans)\n        X_train[feature] = best_power_trans\n        \n    return X_train, X_test, best_power_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We need to create a dataframe containing only the scaled features with the Response.\ndf_pt = X_train.iloc[:,-15:]\ndf_pt_test = X_test.iloc[:,-15:]\n\ndf_pt[\"Response\"] = X_train[\"Response\"]\n\ndata_pt, data_pt_test, best_pt = power_transf(df_pt, df_pt_test, \"Response\")\n\nprint(\"Best Power Transformation for each feature:\")\nfor key in best_pt:\n    print(\"\\t->\", key, best_pt[key][1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing the old columns of scaled features with the features transformed according to the best transformation\ncoln = data_pt.columns[:-1]\n\nX_train.drop(columns=coln, inplace=True)\nX_train[coln] = data_pt[coln]\n\nX_test.drop(columns=coln, inplace=True)\nX_test[coln] = data_pt_test[coln]\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Merges categories with low discrimination ability","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging Categories\n# in Marital_Status:  \"Single\" as 3, \"Widow\" as 2, \"Divorced\" as 1 and [\"Married\", \"Together\"] as 0\nX_train[\"Marital_Status_bin\"] = X_train['Marital_Status'].apply(lambda x: 3 if x == \"Single\" else\n                                                            (2 if x == \"Widow\" else\n                                                             (1 if x == \"Divorced\" else 0))).astype(int)\n\nX_test[\"Marital_Status_bin\"] = X_test['Marital_Status'].apply(lambda x: 3 if x == \"Single\" else\n                                                            (2 if x == \"Widow\" else\n                                                             (1 if x == \"Divorced\" else 0))).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# in Education: \"Phd\" as 2, \"Master\" as 1 and ['Graduation', 'Basic', '2n Cycle'] as 0\nX_train[\"Education_bin\"] = X_train['Education'].apply(lambda x: 2 if x == \"PhD\" else (1 if x == \"Master\" else 0)).astype(int)\n\nX_test[\"Education_bin\"] = X_test['Education'].apply(lambda x: 2 if x == \"PhD\" else (1 if x == \"Master\" else 0)).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[\"Kidhome\"] = X_train['Kidhome'].astype(int)\nX_train[\"Teenhome\"] = X_train['Teenhome'].astype(int)\nX_train[\"NumberOff\"] = X_train['NumberOff'].astype(int)\n\nX_test[\"Kidhome\"] = X_test['Kidhome'].astype(int)\nX_test[\"Teenhome\"] = X_test['Teenhome'].astype(int)\nX_test[\"NumberOff\"] = X_test['NumberOff'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aux = [0]* X_train.shape[0]\n\nfor i in range(X_train.shape[0]):\n    if(int(X_train[[\"Kidhome\"]].iloc[i,:])+int(X_train[[\"Teenhome\"]].iloc[i,:])>0):\n        aux[i] = 1\n    else:\n        aux[i] = 0\n    \nX_train[\"HasOffspring\"] = aux\nX_train[\"HasOffspring\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aux = [0]* X_test.shape[0]\n\nfor i in range(X_test.shape[0]):\n    if(int(X_test[[\"Kidhome\"]].iloc[i,:])+int(X_test[[\"Teenhome\"]].iloc[i,:])>0):\n        aux[i] = 1\n    else:\n        aux[i] = 0\n    \nX_test[\"HasOffspring\"] = aux\nX_test[\"HasOffspring\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding these new categorical features to the list:\nfeat_c.append(\"Marital_Status_bin\")\nfeat_c.append(\"Education_bin\")\nfeat_c.append(\"HasOffspring\")\n\n# Our data now:\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"PCA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.drop([\"ID\",'Education','Marital_Status','Year_Birth','Year_Birth',\n            'Income', 'Recency', 'MntWines', 'MntFruits',\n            'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts',\n            'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases',\n            'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth',\n            'Days_Customer', 'PrpGoldProds','NmbAccCmps', 'PrpAccCmps', 'PrpWines',\n            'PrpFruits', 'PrpMeat','PrpFish', 'Mnt', 'BuyPot', 'Freq', 'RFM'], \n           axis=1, inplace=True)\n\nX_test.drop([\"ID\",'Education','Marital_Status','Year_Birth','Year_Birth',\n            'Income', 'Recency', 'MntWines', 'MntFruits',\n            'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts',\n            'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases',\n            'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth',\n            'Days_Customer', 'PrpGoldProds','NmbAccCmps', 'PrpAccCmps', 'PrpWines',\n            'PrpFruits', 'PrpMeat','PrpFish', 'Mnt', 'BuyPot', 'Freq', 'RFM'], \n           axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\ncolumns = X_train.columns\ncolumns = columns.drop(['Kidhome','Teenhome','NumberOff','AcceptedCmp3', 'AcceptedCmp4',\n                        'AcceptedCmp5', 'AcceptedCmp1','AcceptedCmp2', 'Complain', 'Response',\n                        'Marital_Status_bin', 'Education_bin', 'HasOffspring'])\n\n\npca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(X_train[columns])\nprincipalComponents_test = pca.transform(X_test[columns])\n\nX_train[\"pc1\"] = principalComponents[:,0]\nX_train[\"pc2\"] = principalComponents[:,1]\nX_test[\"pc1\"] = principalComponents_test[:,0]\nX_test[\"pc2\"] = principalComponents_test[:,1]\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_n =['Year_Birth_t', 'Income_t', 'Recency_t', 'MntWines_t', 'MntFruits_t',\n       'MntMeatProducts_t', 'MntFishProducts_t', 'MntSweetProducts_t',\n       'MntGoldProds_t', 'NumDealsPurchases_t', 'NumWebPurchases_t',\n       'NumCatalogPurchases_t', 'NumStorePurchases_t', 'NumWebVisitsMonth_t',\n       'Days_Customer_t', 'PrpGoldProds_t', 'NmbAccCmps_t', 'PrpAccCmps_t',\n       'PrpWines_t', 'PrpFruits_t', 'PrpMeat_t', 'PrpFish_t', 'Mnt_t',\n       'BuyPot_t', 'Freq_t', 'RFM_t', 'pc1', 'pc2']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_c.remove('Education')\nfeat_c.remove('Marital_Status')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Gets dummies for categories","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_columns = ['Kidhome', 'Teenhome', 'NumberOff','Marital_Status_bin','Education_bin']\nX_train = pd.get_dummies(X_train, prefix_sep=\"_\",\n                              columns=cat_columns)\nX_test = pd.get_dummies(X_test, prefix_sep=\"_\",\n                              columns=cat_columns)\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_c.remove('Kidhome')\nfeat_c.remove('Teenhome')\nfeat_c.remove('NumberOff')\nfeat_c.remove('Marital_Status_bin')\nfeat_c.remove('Education_bin')\n\nfeat_c.extend(('Kidhome_0', 'Kidhome_1',\n       'Kidhome_2', 'Teenhome_0', 'Teenhome_1', 'Teenhome_2', 'NumberOff_0',\n       'NumberOff_1', 'NumberOff_2', 'NumberOff_3', 'Marital_Status_bin_0',\n       'Marital_Status_bin_1', 'Marital_Status_bin_2', 'Marital_Status_bin_3',\n       'Education_bin_0', 'Education_bin_1', 'Education_bin_2'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Feature Selection**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Is given as input a dataframe, a list of continuous features names, a list of categorical features names,\n# the name of the target feature and returns a dataframe with the discrimination ability of each feature and if\n# its p-value is lower than 0.05.\n# 10 is the default number of bins and uniform is the strategy used in the binning of continuous features.\ndef chisq_ranker(df, continuous_flist, categorical_flist, target, n_bins=10, binning_strategy=\"uniform\"):\n    chisq_dict = {}\n    if  continuous_flist:\n        bindisc = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', \n                               strategy=binning_strategy)\n        for feature in continuous_flist:            \n            feature_bin = bindisc.fit_transform(df[feature].values[:, np.newaxis])\n            feature_bin = pd.Series(feature_bin[:, 0], index=df.index)\n            cont_tab = pd.crosstab(feature_bin, df[target], margins = False)\n            chisq_dict[feature] = stats.chi2_contingency(cont_tab.values)[0:2] \n    if  categorical_flist:\n        for feature in categorical_flist:  \n            cont_tab = pd.crosstab(df[feature], df[target], margins = False)          \n            chisq_dict[feature] = stats.chi2_contingency(cont_tab.values)[0:2]       \n    \n    df_chi = pd.DataFrame(chisq_dict, index=[\"Chi-Squared\", \"p-value\"]).transpose()\n    df_chi.sort_values(\"Chi-Squared\", ascending=False, inplace=True)\n    df_chi[\"valid\"]=df_chi[\"p-value\"]<=0.05\n    \n    \n    return df_chi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_chisq_rank = chisq_ranker(X_train, feat_n, feat_c, \"Response\")\ndf_chisq_rank.head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('whitegrid') \n\nplt.subplots(figsize=(13,12))\npal = sns.color_palette(\"RdBu_r\", len(df_chisq_rank))\nrank = df_chisq_rank['Chi-Squared'].argsort().argsort()  \n\nsns.barplot(y=df_chisq_rank.index,x=df_chisq_rank['Chi-Squared'], palette=np.array(pal[::-1])[rank])\nplt.title(\"Features' worth by Chi-Squared statistic test\", fontsize=18)\nplt.ylabel(\"Input feature\", fontsize=14)\nplt.xlabel(\"Chi-square\", fontsize=14)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Balance training set**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.value_counts().plot(kind='bar', title='Count (target)');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Oversampling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE()\nX_train, y_train = smote.fit_resample(X_train, y_train)\n\ny_train.value_counts().plot(kind='bar', title='Count (target)');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler()\nX_train = pd.DataFrame(scaler.fit_transform(X_train.values))\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\n\nLR = LogisticRegression()\nLR.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = LR.predict(X_test)\nprint('ROC score: {}'.format(roc_auc_score(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Naive Bayes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"NB = GaussianNB()\nNB.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = LR.predict(X_test)\nprint('ROC score: {}'.format(roc_auc_score(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"DNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import models\nfrom keras import layers\nimport tensorflow as tf\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(5, activation='relu', input_dim=53))\nmodel.add(layers.Dense(5, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train, epochs=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn_model = model\nmetrics = cnn_model.evaluate(X_test, y_test)\nprint(\"{}: {}\".format(cnn_model.metrics_names[1], metrics[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nprint(confusion_matrix)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}