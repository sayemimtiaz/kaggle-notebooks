{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is an analysis of covid vaccine tweets, as well as correlations between tweet sentiment and account size/engagement. An unsupervised clustering algorithm was used based off of this article\nhttps://towardsdatascience.com/unsupervised-sentiment-analysis-a38bf1906483?gi=fcf9c329e93d","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('../input/pfizer-vaccine-tweets/vaccination_tweets.csv')\n#data = pd.read_csv('../input/all-covid19-vaccines-tweets/vaccination_all_tweets.csv')\ndata = data.dropna().drop_duplicates()\ndata = data[data.text.str.len()>1]\ndata.head()\n\n#Reading data and removing duplicates\n","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['text'] = data['text'].str.lower()\ndata['text'] = data['text'].str.replace(\"[^A-Za-z0-9^,!?.\\/'+]\", \" \")\ndata['text'] = data['text'].str.replace(r\"https?.*\", \" \")\ndata['text'] = data['text'].str.replace(r\"\\+\", \" plus \")\ndata['text']= data['text'].str.replace(r\",\", \" \")\ndata['text']= data['text'].str.replace(r\"\\.\", \" \")\ndata['text'] = data['text'].str.replace(r\"!\", \" ! \")\ndata['text'] = data['text'].str.replace(r\"\\?\", \" ? \")\ndata['text'] = data['text'].str.replace(r\"'\", \" \")\ndata['text'] = data['text'].str.replace(r\":\", \" : \")\ndata['text'] = data['text'].str.replace(r\"\\s{2,}\", \" \")\n\ntext = data['text']\ntext.head()\n#Data cleaning","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.models import Word2Vec\nimport multiprocessing\nw2v_model = Word2Vec(min_count=3,window=4,\n                     size=300)\nprint(\"done word2vec\")\n#Defining word2vec model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.models.phrases import Phrases, Phraser\nsent = [row for row in data.text]\nphrases = Phrases(sent, min_count=1, progress_per=50000)\nbigram = Phraser(phrases)\nsentences = bigram[sent]\nsentences[1]\n#converting tweets into bigrams for word2vec","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences2 = []\nfor i in sentences:\n    str(i)\n    i = i.split(' ')\n    sentences2.append(i)\n\n\nfor j in sentences2:\n    for k in j:\n        if(len(k) < 2):\n            j.remove(k)\n#splitting sentences by word to create word2vec vocabulary\n    \n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model.build_vocab(sentences2, progress_per=50000)\nprint(\"done vocab\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nw2v_model.train(sentences2, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n\nprint('Done Training')\n\nw2v_model.init_sims(replace=True)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model.save(\"word2vec.model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nword_vectors = Word2Vec.load(\"word2vec.model\").wv\nprint('loaded vectors')\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\n\nmodel = KMeans(n_clusters=2, max_iter=1000, random_state=False, n_init=50).fit(X=word_vectors.vectors.astype('double'))\n\n\n#K means clustering model fit to word2vec model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_vectors.similar_by_vector(model.cluster_centers_[0], topn=10, restrict_vocab=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_vectors.similar_by_vector(model.cluster_centers_[1], topn=10, restrict_vocab=None)\n#These blocks show top 10 words in each cluster to identify the positive one","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\npositive_cluster_index = 1\npositive_cluster_center = model.cluster_centers_[positive_cluster_index]\nnegative_cluster_center = model.cluster_centers_[1-positive_cluster_index]\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words = pd.DataFrame(word_vectors.vocab.keys())\nwords.columns = ['words']\nwords['vectors'] = words.words.apply(lambda x: word_vectors[f'{x}'])\nwords['cluster'] = words.vectors.apply(lambda x: model.predict([np.array(x)]))\nwords.cluster = words.cluster.apply(lambda x: x[0])\n#fits words to clusters","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nwords['cluster_value'] = [1 if i==positive_cluster_index else -1 for i in words.cluster]\nwords['closeness_score'] = words.apply(lambda x: 1/(model.transform([x.vectors]).min()), axis=1)\nwords['sentiment_coeff'] = words.closeness_score * words.cluster_value\n\n#displays cluster and sentiment score of each word (closeness score multiplied by pos or neg 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words[['words', 'sentiment_coeff']].to_csv('sentiment_dictionary.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_file = text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_file.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentiment_map = pd.read_csv('sentiment_dictionary.csv')\nsentiment_map.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentiment_dict = dict(zip(sentiment_map.words.values, sentiment_map.sentiment_coeff.values))\n#combine words with sentiment scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_weighting2 = final_file.copy()\nfile_weighting2 = file_weighting2.to_frame()\nfile_weighting2[\"weight\"] = 1\nfile_weighting2 = file_weighting2.rename(columns={\"text\": \"title\", \"weight\": \"rate\"})\nfile_weighting2[['title', 'rate']].to_csv(\"cleaned_dataset.csv\",index=False)\nfile_weighting = pd.read_csv(\"cleaned_dataset.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\nfrom IPython.display import display\n\n\n\ntfidf = TfidfVectorizer(tokenizer=lambda y: y.split(), norm=None)\ntfidf.fit(file_weighting.title)\nfeatures = pd.Series(tfidf.get_feature_names())\ntransformed = tfidf.transform(file_weighting.title)\n#prepare tfidf score for adjusted weighting\n#TFIDF takes into account frequency, so the score for a common word like 'the' will be scaled down\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_tfidf_dictionary(x, transformed_file, features):\n#create TFIDF dictionary, method from https://towardsdatascience.com/unsupervised-sentiment-analysis-a38bf1906483\n    vector_coo = transformed_file[x.name].tocoo()\n    vector_coo.col = features.iloc[vector_coo.col].values\n    dict_from_coo = dict(zip(vector_coo.col, vector_coo.data))\n    return dict_from_coo\n\ndef replace_tfidf_words(x, transformed_file, features):\n    dictionary = create_tfidf_dictionary(x, transformed_file, features)   \n    return list(map(lambda y:dictionary[f'{y}'], x.title.split()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"replaced_tfidf_scores = file_weighting.apply(lambda x: replace_tfidf_words(x, transformed, features), axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def replace_sentiment_words(word, sentiment_dict):\n    try:\n        out = sentiment_dict[word]\n    except KeyError:\n        out = 0\n    return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nreplaced_closeness_scores = file_weighting.title.apply(lambda x: list(map(lambda y: replace_sentiment_words(y, sentiment_dict), x.split())))\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nreplacement_df = pd.DataFrame(data=[replaced_closeness_scores, replaced_tfidf_scores, file_weighting.title]).T\nreplacement_df.columns = ['sentiment_coeff', 'tfidf_scores', 'sentence']\nreplacement_df['sentiment_rate'] = replacement_df.apply(lambda x: np.array(x.loc['sentiment_coeff']) @ np.array(x.loc['tfidf_scores']), axis=1)\nreplacement_df['prediction'] = (replacement_df.sentiment_rate>0).astype('int8')\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install vaderSentiment","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nanalyser = SentimentIntensityAnalyzer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef sentiment_analyzer_scores(sentence):\n    score = analyser.polarity_scores(sentence)\n    if score['pos']>score['neg']:\n        return 1\n    return 0\n\n#calculate sentiment with VADER engine to compare to our clustering","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"replacement_df['vader'] = replacement_df.apply(lambda x: sentiment_analyzer_scores(x.loc['sentence']), axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"replacement_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\npredicted_classes = replacement_df.prediction\ny_test = replacement_df.vader\n\nconf_matrix = pd.DataFrame(confusion_matrix(replacement_df.vader, replacement_df.prediction))\nprint('Confusion Matrix')\ndisplay(conf_matrix)\n\ntest_scores = accuracy_score(y_test,predicted_classes), precision_score(y_test, predicted_classes), recall_score(y_test, predicted_classes), f1_score(y_test, predicted_classes)\n\nprint('\\n \\n Scores')\nscores = pd.DataFrame(data=[test_scores])\nscores.columns = ['accuracy', 'precision', 'recall', 'f1']\nscores = scores.T\nscores.columns = ['scores']\ndisplay(scores)\n#confusion matrix for comparison","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_classes.value_counts()\n#sanity check for number of positive and negative in cluster algorithm + vader\n#a large difference probably means something is wrong","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = replacement_df[\"vader\"]\ndata = data.reset_index()\ndata[\"sentiment\"] = preds\ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ncorrmatrix = data[['favorites', 'retweets','user_verified','user_followers','sentiment']].corr()\nprint(sns.heatmap(corrmatrix))\n#heatmap of correlations between interesting columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the zero correlations in sentiment are a bit strange, so this is separating the positive and negative\n#tweets for further analysis\ndata['date'] = pd.to_datetime(data['date'])\npos = data.loc[data['sentiment'] == 1]\nneg = data.loc[data['sentiment'] == 0]\npos.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Deeper correlation analysis: comparing tweet engagement and account size on positive vs negative tweets\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Comparing user followers\nfrom scipy.stats import ttest_ind\ndef correlation_analysis(param):\n    print(\"positive sentiment mean\",np.mean(pos[param]))\n    print(\"positive sentiment standard deviation\",np.std(pos[param]))\n    print(\"negative sentiment mean\",np.mean(neg[param]))\n    print(\"negative sentiment standard deviation\",np.std(neg[param]))\n    ttest,pval = ttest_ind(pos[param],neg[param])\n    print(\"p-value\",pval)\n    plt.plot(pos[param],label=\"positive\")\n    plt.plot(neg[param],label=\"negative\")\n    plt.legend()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation_analysis(\"user_followers\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation_analysis(\"user_friends\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation_analysis(\"retweets\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation_analysis(\"favorites\")\n#only significant p value here, positive tweets are significantly more likely to get more likes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}