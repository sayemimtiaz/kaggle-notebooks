{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"reated by: Sangwook Cheon\n\nDate: Dec 24, 2018\n\nThis is step-by-step guide to Reinforcement Learning using scikit-learn, which I created for reference. I added some useful notes along the way to clarify things. This notebook's content is from A-Z Datascience course, and I hope this will be useful to those who want to review materials covered, or anyone who wants to learn the basics of Reinforcement Learning.\n\n## Content:\n### 1. Upper Confidence Bound (UCB)\n### 2. Thomson Sampling"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# Upper Confidence Bound (UCB)\n\n## The multi-armed Bandit Problem\nTrying to find the optimal option among many possible options as quickly as possible, with minimal exploration of other options which have high disadvantage in money (resources) and time. For example, if an Advertising company has 5 advertisement options, they need to find the best one to publish. In order to do this, they might to AB testing. But if they do too much AB testing, then it is just as same as utilizing all 5 options which is not ideal. Therefore, through reinforcement learning, the company needs to find the optimal option quickly. \n**Steps of solving Multi-armed Bandit problem**\n![](https://i.imgur.com/Dn3n0ri.png)\n\n**What's happening behind the scene**\n![](https://i.imgur.com/n22tQ3o.png)\nThe algorithm starts with an initial expecte value. Then, select a column and exploit it one time. The expected value might go down or up as new observation is taken. As there are more observations, the confidence bound gets smaller as the algorithm is more confident in the result. Then, select the column with the hiest upper confidence bound and exploit it once, which also makes the bound gets smaller and shifts the observation. Repeat these steps until the algorithm consistently exploits the same column, which is an indicator that this column is the optimal model. \n\n### This is different from simple Supervised Learning, as this model starts with no data. When we start experimenting to collect data, reinforcement learning determines what to do with that existing data. \n#### This dataset is for simulation purposes only. In real world, we cannot expect behaviors of each customer, and there is our job to start experimental strategically using UCB.\n\n**Algorithm:**\n![](https://i.imgur.com/NNIwROs.png)"},{"metadata":{"trusted":true,"_uuid":"aa784efa4766b4f386ce6d6d6579ebac2e4b158c"},"cell_type":"code","source":"#implementing Upper Confidence Bound (UCB)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport math\n\ndataset = pd.read_csv('../input/Ads_CTR_Optimisation.csv')\ndataset.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d293949b03e0e0afdce33b6b40c81d8407599517"},"cell_type":"code","source":"#UCB needs to be implemented from scratch without using any package, as there is no easy library to use.\n\nN = 10000\nd = 10\nads_selected = []\nnumbers_of_selections = [0] * d\nsums_of_rewards = [0] * d\ntotal_reward = 0\n\nfor n in range(0, N):\n    ad = 0\n    max_upper_bound = 0\n    for i in range(0, d):\n        if numbers_of_selections[i] > 0:\n            # 3 lines below is the algorithm shown above\n            average_reward = sums_of_rewards[i] / numbers_of_selections[i]\n            delta_i = math.sqrt(3/2 * math.log(n + 1) / numbers_of_selections[i])\n            upper_bound = average_reward + delta_i\n        else:\n            upper_bound = 1e400 #makes this large so that the first round gives every category a chance \n        if upper_bound > max_upper_bound:\n            ad = i\n            max_upper_bound = upper_bound\n    ads_selected.append(ad)\n    numbers_of_selections[ad] = numbers_of_selections[ad] + 1\n    reward = dataset.values[n, ad]\n    sums_of_rewards[ad] = sums_of_rewards[ad] + reward\n    total_reward = total_reward + reward\n\n#Visualizing the result\nplt.hist(ads_selected) #histogram of number of times each ad is clicked\nplt.title('Histogram of ads_selected')\nplt.xlabel('Ad No')\nplt.ylabel('Number of times each add is selected')\nplt.show()\n\n#to view the reward\nprint(total_reward)\n            ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"09bf7a573a29aaf1c0db518c3a3347a657e1664e"},"cell_type":"markdown","source":"As can be noticed on the graph, 4 is chosen the most times which shows that it is the best option. As it was the best option, the algorithm converged towards 4 as the number of iterations increased. The first iteration went over all the ads in order: 0, 1, 2, 3, --- ,9 so that the algorithm has at least one observation to work with.\n\nThe aim of Reinforcement learning is to maximize the total reward. Let's compare this model to Thomson Sampling."},{"metadata":{"_uuid":"e31a523c622051927aca75bc24ce30cbcf03a881"},"cell_type":"markdown","source":"# Thomson Sampling\n![](https://i.imgur.com/h63JGov.png)\nFirst of all, in Thomson Sampling, we are trying to guess the expected value for each distribution (which would be a bandit). Therefore, this is a probabilistic algorithm. According to initial distributions created, we then generate three random points from each distribution (creating a bandit configuration, which is sampling)\n\n![](https://i.imgur.com/fntToWs.png)\nThen, pick the best point which is the point that is on the far right side of the plot, as it has the highest return. Calculate it on the existing distribution and adjust the expected value and refine the distribution according to this. After iterating these steps many times, the graph will look like this:\n\n![](https://i.imgur.com/gXLQqBm.png)\nThe distributions are refined because of more observations present. Mathematics behind this is shown below when working with code.\n\n## Comparison with Upper Confidence Bound (UCB)\n* While UCB is deterministic, Thomson Sampling is probabilistic\n* In UCB, the result needs to be updated every time. On the other hand, many sanples can be chosen at once, and update the model later, which is computationally more efficient. \n* Thomson Sampling has better empirical evidence. \n\n\n## Mathematical details\n\n"},{"metadata":{"trusted":true,"_uuid":"a1f4fb7fd77fda9f59c85a67ec7d89a639f36518"},"cell_type":"code","source":"import random\n\nN = 10000\nd = 10\nads_selected = []\nnumbers_of_rewards_1 = [0] * d\nnumbers_of_rewards_0 = [0] * d #number of 0 rewards for each ad\ntotal_reward = 0\n\nfor n in range(0, N):\n    ad = 0\n    max_random = 0 #maximum random draw\n    for i in range(0, d):\n        random_beta = random.betavariate(numbers_of_rewards_1[i] + 1, numbers_of_rewards_0[i] + 1)\n        if random_beta > max_random:\n            ad = i\n            max_random = random_beta\n    ads_selected.append(ad)\n    reward = dataset.values[n, ad]\n    if reward == 1:\n        numbers_of_rewards_1[ad] = numbers_of_rewards_1[ad] + 1\n    else:\n        numbers_of_rewards_0[ad] = numbers_of_rewards_0[ad] + 1\n    total_reward = total_reward + reward\n\nplt.hist(ads_selected) #histogram of number of times each ad is clicked\nplt.title('Histogram of ads_selected')\nplt.xlabel('Ad No')\nplt.ylabel('Number of times each add is selected')\nplt.show()\n\n#to view the reward\nprint(total_reward)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9837bd2e131048acf9b00bd325fa1a955eca703"},"cell_type":"markdown","source":"Thomson Sampling is clearly better than UCB. It gave the same result: Ad number 4 is the optimal option."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}