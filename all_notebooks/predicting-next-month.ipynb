{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"8ff9901d-97f0-cdbc-fda2-bd583869ad71"},"source":"**Import dependencies**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ad51bff4-3832-d2e2-5649-f8fed94ada4d"},"outputs":[],"source":"import tensorflow as tf\nimport numpy as np\nimport time\nimport pandas as pd\nfrom sklearn.cross_validation import train_test_split"},{"cell_type":"markdown","metadata":{"_cell_guid":"84335cfc-17f9-d23f-9bd9-552684a8672a"},"source":"**Function to clean the data set and get X and Y. X is hyper parameters, Y is last column in csv**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"427b25a0-13bd-157d-ec66-65c061054631"},"outputs":[],"source":"def get_data(data_location, split_dataset):\n    dataset = pd.read_csv(data_location)\n\n    # 0 shape to get total of rows, 1 to get total of columns\n    rows = dataset.shape[0]\n    print (\"there are \", rows, \" rows before cleaning\\n\")\n\n    # removing unimportant columns\n    columns = ['ID']\n    for text in columns:\n        del dataset[text]\n\n    # get all data except last column\n    x = dataset.ix[: , :-1].values\n\n    # get all data on last column only\n    y = dataset.ix[: , -1:].values\n\n    # split our dataset to reduce overfitting\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = split_dataset)\n    \n    return x_train, x_test, y_train, y_test"},{"cell_type":"markdown","metadata":{"_cell_guid":"93332ce4-33bb-ddfd-56d1-4168204d8e16"},"source":"**Function to return one-hot-label our Y for softmax cross entropy**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"27e4ac66-5ea3-24a6-4f7c-d2cd4ffb313d"},"outputs":[],"source":"def return_embedded(x):\n\n    data = np.zeros((x.shape[0], np.unique(x).shape[0]), dtype = np.float32)\n    \n    for i in range(x.shape[0]):\n        data[i][x[i][0]] = 1.0\n    \n    return data"},{"cell_type":"markdown","metadata":{"_cell_guid":"dff3fcbf-8edf-9319-ba32-8a5e069ccfda"},"source":"**Our global variables**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"38811c3c-4fad-dbda-8c45-ec4876811fe4"},"outputs":[],"source":"data_location = '../input/UCI_Credit_Card.csv'\n\n# not included input and output layer\nnum_layers = 5\n# all hidden layers are same wide size\nsize_layer = 64\nlearning_rate = 0.01\n# batch mini size for training\nbatch_size = 100\n\n# beta for regularizer, learn from penalty value\nbeta = 0.05\n\n# probability to disconnect connection between nodes\nprob_dropout = 1.0\n\nbiased_node = True\n\nsplit_dataset = 0.7\n\n# iteration for training\nepoch = 100"},{"cell_type":"markdown","metadata":{"_cell_guid":"92b6df36-7dcc-05e7-b1cc-fee4e919428e"},"source":"**You can choose what type of activation function, I prefer RELu because it does not have upper boundary, and to put penalty later**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a2a3e567-7c9d-751f-5a26-64b6aff1fa48"},"outputs":[],"source":"# got sigmoid, softmax, tanh\nactivation = 'relu'\n\nif activation == 'sigmoid':\n    activation = tf.nn.sigmoid\nelif activation == 'tanh':\n    activation = tf.nn.tanh\nelif activation == 'relu':\n    activation = tf.nn.relu\nelse:\n    raise Exception(\"model type not supported\")\n    \nx_train, x_test, y_train, y_test = get_data(data_location, split_dataset)\n\ny_train = return_embedded(y_train)\ny_test = return_embedded(y_test)"},{"cell_type":"markdown","metadata":{"_cell_guid":"ba705cc7-26b9-d089-1241-5edf921cca0e"},"source":"**Our Deep Dynamic Neural Network**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1e0b7720-4928-f66a-79cf-73942e501546"},"outputs":[],"source":"# Neural Network pipelining ===========================================================================\n\nX = tf.placeholder(\"float\", [None, x_train.shape[1]])\nY = tf.placeholder(\"float\", [None, y_train.shape[1]])\n        \ninput_layer = tf.Variable(tf.random_normal([x_train.shape[1], size_layer]))\n\nif biased_node:\n    biased_input_layer = tf.Variable(tf.random_normal([size_layer]))\n    biased = []\n    for i in range(num_layers):\n        biased.append(tf.Variable(tf.random_normal([size_layer])))\n\nlayers = []\nfor i in range(num_layers):\n    layers.append(tf.Variable(tf.random_normal([size_layer, size_layer])))\n\noutput_layer = tf.Variable(tf.random_normal([size_layer, y_train.shape[1]]))\n\nif biased_node:\n    first_l = activation(tf.add(tf.matmul(X, input_layer), biased_input_layer))\n    \n    # reduce nodes connection\n    first_l = tf.nn.dropout(first_l, prob_dropout)\n    \n    next_l = activation(tf.add(tf.matmul(first_l, layers[0]), biased[0]))\n    # reduce nodes connection\n    next_l = tf.nn.dropout(next_l, prob_dropout)\n    \n    for i in range(1, num_layers - 1):\n        next_l = activation(tf.add(tf.matmul(next_l, layers[i]), biased[i]))\n        \n        # reduce nodes connection\n        next_l = tf.nn.dropout(next_l, prob_dropout)\nelse:\n    first_l = activation(tf.matmul(X, input_layer))\n    \n    # reduce nodes connection\n    first_l = tf.nn.dropout(first_l, prob_dropout)\n    \n    next_l = activation(tf.matmul(first_l, layers[0]))\n    \n    # reduce nodes connection\n    next_l = tf.nn.dropout(next_l, prob_dropout)\n    \n    for i in range(1, num_layers - 1):\n        next_l = activation(tf.matmul(next_l, layers[i]))\n        \n        # reduce nodes connection\n        next_l = tf.nn.dropout(next_l, prob_dropout)\n    \nlast_l = tf.matmul(next_l, output_layer)\n\n# adding up all penalties values\nregularizers = tf.nn.l2_loss(input_layer) + sum(map(lambda x: tf.nn.l2_loss(x), layers)) + tf.nn.l2_loss(output_layer)\n\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = last_l, labels = Y))\n\n# included penalty values\ncost = tf.reduce_mean(cost + beta * regularizers)\n\noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n\ncorrect_prediction = tf.equal(tf.argmax(last_l, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"},{"cell_type":"markdown","metadata":{"_cell_guid":"416db8c3-636d-2c40-3782-c6bae62cb92b"},"source":"**Start our session**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c0349549-b1bc-69b2-81b7-a4021a94ffae"},"outputs":[],"source":"# start the session graph\nsess = tf.InteractiveSession()\n    \n# initialize global variables\nsess.run(tf.global_variables_initializer())\n\nprint (\"Train for \", epoch, \" iteration\")\nprint (\"There are \", x_train.shape[0], \" of rows for training\")\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"87177496-63c7-1c39-caa3-d17cb27f3255"},"source":"**Training session begin**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"89c6ba56-4943-d80b-4ec8-4d381dbcb9eb"},"outputs":[],"source":"for i in range(epoch):\n    last_time = time.time()\n    total_lost = 0\n    total_accuracy = 0\n    \n    for n in range(0, x_train.shape[0], batch_size):\n        out, _, loss = sess.run([accuracy, optimizer, cost], feed_dict={X: x_train[n : n + batch_size, :], Y: y_train[n : n + batch_size, :]})\n        total_accuracy += out\n        total_lost += loss\n    \n    print (\"total accuracy: \", total_accuracy / (x_train.shape[0] / batch_size * 1.0))\n    diff = time.time() - last_time\n    print (\"batch: \", i + 1, \", loss: \", total_lost/x_train.shape[0], \", speed: \", diff, \" s / epoch\")\n    total_lost = 0\n    total_accuracy = 0"},{"cell_type":"markdown","metadata":{"_cell_guid":"ff2a35c6-1769-8b80-ae84-a3b8943e0d8b"},"source":"Benchmarking"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e01fe1c7-d463-76b9-01d1-edf7eb2684d3"},"outputs":[],"source":"total_correct = 0\ntotal_positive = 0\ntotal_correct_positive = 0\nfor n in range(x_test.shape[0]):\n    \n    correct = sess.run(accuracy, feed_dict={X: x_test[n : n + 1, :], Y: y_test[n : n + 1 , :]})\n    total_correct += correct\n    if y_test[n][1] == 1:\n        total_positive += 1\n        if correct == 1:\n            total_correct_positive += 1\n    \nprint (\"total correct positive: \", total_correct_positive, \" / \", total_positive)\nprint (\"total correct: \", int(total_correct), \" / \", x_test.shape[0]) \nprint (\"total accuracy: \", total_correct / (x_test.shape[0] * 1.0))"},{"cell_type":"markdown","metadata":{"_cell_guid":"4c2899b8-1966-36e8-d6c5-f8f866f02b19"},"source":"**The output generated overly biased/under fitted. You need to increase the iteration atleast one thousand if want better result.**"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}