{"cells":[{"metadata":{},"cell_type":"markdown","source":"# CORD-19 - Data extraction functions\n\nWithin the arising COVID-19 pandemia, Kaggle has launched the COVID-19 Open Research Dataset Challenge (CORD-19) dataset as a general call for any data scientist that is able to contribute extracting relevant information to deal with the virus. Since we are still in the first stages of this analytics challenge, the idea of this kernel is to provide quality of life functions to extract certain information about COVID-19 papers. The content is far from being particularly creative or perfect, but it will hopefully save time to other people interested in the challenge.\n\nTABLE OF CONTENTS\n\n1. [Filter papers by word occurrences](#section1)\n2. [Extract the conclusions section](#section2)\n\nDisclaimer: This kernel is still under construction. "},{"metadata":{},"cell_type":"markdown","source":"Import required libraries:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\npd.set_option('display.width', 30) \nimport matplotlib.pyplot as plt\nimport time\nimport warnings \nwarnings.filterwarnings('ignore')\nfrom collections import Counter\nfrom nltk.corpus import stopwords\n\n# NLP libraries\nimport spacy\nfrom spacy.lang.en import English\n\n# There's a large number of input files, it's nice to check out the list\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I load the output files from [xhlulu's kernel](https://www.kaggle.com/xhlulu/cord-19-eda-parse-json-and-generate-clean-csv), which contains a useful transformation of the json files in dictionaries to csv readable format. Go check it to give some credit!"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"biorxiv = pd.read_csv(\"/kaggle/input/cord-19-eda-parse-json-and-generate-clean-csv/biorxiv_clean.csv\")\nbiorxiv.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"biorxiv.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Filter papers by word occurrences <a id=\"section1\"></a>\n\nGeneral studies like word frequency and such do require the full set of scientific papers. However, when dealing with specific tasks or topics, it's useful to select the subset of papers containing only certain words. Despite being very simple, the function defined in this section provides a list of paper_id containing a desired set of words."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter papers containing all words in list\ndef filter_papers_word_list(word_list):\n    papers_id_list = []\n    for idx, paper in biorxiv.iterrows():\n        if all(x in paper.text for x in word_list):\n            papers_id_list.append(paper.paper_id)\n\n    return papers_id_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see an example related to the challenge task [What is known about transmission, incubation, and environmental stability?](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks?taskId=568), tackling the last bullet: **Role of the environment in transmission**. We look for  papers that contain the words \"coronavirus\", \"environment\" and \"transmission\". (Thanks to [Mar√≠lia](https://www.kaggle.com/mpwolke) for her insightful comment!)"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option(\"display.max_colwidth\", 100000) # Extend the display width to prevent split functions to not cover full text\n\nbiorxiv_environment = filter_papers_word_list([\"coronavirus\"])\nprint(\"Papers containing coronavirus: \", len(biorxiv_environment))\n\nbiorxiv_environment = filter_papers_word_list([\"environment\"])\nprint(\"Papers containing environment: \", len(biorxiv_environment))\n\nbiorxiv_environmental = filter_papers_word_list([\"environmental\"])\nprint(\"Papers containing environmental: \", len(biorxiv_environmental))\n\nprint(\"Intersection of environment and environmental: \", len(set(biorxiv_environment)-(set(biorxiv_environment)-set(biorxiv_environmental))))\n\nbiorxiv_environment_transmission = filter_papers_word_list([\"coronavirus\", \"environment\", \"transmission\"])\nprint(\"Number of papers containing coronavirus, environment and transmission: \", len(biorxiv_environment_transmission))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations**:\n\n* From the 803 biorxiv papers from this challenge, the word \"coronavirus\" appears only in half of them (412). This suggests that a large number of papers are not strictly related to COVID-19, despite they may involve useful information to answer some of the challenge tasks.\n* Papers with \"environmental\" are a subset of those containing \"environment\". However, this could not be the case for other words' families. In the next subsection we cover an alternative to find all papers based on the lemma of words. "},{"metadata":{},"cell_type":"markdown","source":"### 1.1. Alternative filter: word lemmatization\n\nFiltering by literal words may lead to loosing some papers just because the word appeared in an alternative version. For example, above we looked for the word \"environment\", and we had to check if the word \"environmental\" was present in some papers where \"environment\" was not. To deal with this, we can **transform all words into their lemma**, and then filter the papers. \n\nNotice that **this procedure is very time consuming**, since we need to first transform all texts (including the ones that do not contain any information about our word list).\n\nTo do this, we will use the spaCy library:"},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load('en_core_web_lg')\n\ndef lemmatizer(text):\n    tokens = [token.lemma_ for token in text]\n    return ' '.join([token for token in tokens])\n\n# Filter papers containing all words in list\ndef filter_papers_word_list_lemma(word_list):\n    papers_id_list = []\n    word_list_lemma = lemmatizer(nlp(str(' '.join([token for token in word_list]))))\n    for idx, paper in biorxiv.iterrows():\n        if all(w in lemmatizer(nlp(paper.text)) for w in word_list_lemma):\n            papers_id_list.append(paper.paper_id)\n\n    return papers_id_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ts = time.time() # I comment this part so that commit time does not skyrocket\n\n#biorxiv_environment_lemma = filter_papers_word_list_lemma([\"coronavirus\",\"environment\"])\n#print(\"Papers containing environment: \", len(biorxiv_environment_lemma))\n\n#print(\"Time spent: \", time.time() - ts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Extract the conclusions section <a id=\"section2\"></a>\n\nMost scientific papers contain a Conclusion section, which consists on a summary of the main observations and results from the study. In order to reduce the amount of data to analyze, it may prove useful to focus on the conclusions instead of performing a full search in the paper. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_conclusion(dataset, papers_id_list):\n    data = dataset.loc[dataset['paper_id'].isin(papers_id_list)]\n    conclusion = []\n    for idx, paper in data.iterrows():\n        paper_text = paper.text\n        if \"\\nConclusion\\n\" in paper.text:\n            conclusion.append(paper_text.split('\\nConclusion\\n')[1])\n        else:\n            conclusion.append(\"No Conclusion section\")\n    data['conclusion'] = conclusion\n        \n    return data\n\npd.reset_option('^display.', silent=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's  now extract the Conclusion section from all papers containing the words \"environment\" and \"transmission\":"},{"metadata":{"trusted":true},"cell_type":"code","source":"environ_trans_conclusion = extract_conclusion(biorxiv, biorxiv_environment_transmission)\nenviron_trans_conclusion.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Done, our DataFrame has now a conclusion column. We are able to process conclusions independently, but remember this is convenient only incertainsituations. With the help of the word_bar_function from [Paul Mooney](https://www.kaggle.com/paultimothymooney/most-common-words-in-the-cord-19-dataset), let's study which are the most frequent words in the Conclusion section of papers containing \"environment\" and \"transmission\":"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.width', 100000)\n\n\n\ndef word_bar_graph_function(df,column,title):\n    # adapted from https://www.kaggle.com/benhamner/most-common-forum-topic-words\n    topic_words = [ z.lower() for y in\n                       [ x.split() for x in df[column] if isinstance(x, str)]\n                       for z in y]\n    word_count_dict = dict(Counter(topic_words))\n    popular_words = sorted(word_count_dict, key = word_count_dict.get, reverse = True)\n    popular_words_nonstop = [w for w in popular_words if w not in stopwords.words(\"english\")]\n    plt.barh(range(50), [word_count_dict[w] for w in reversed(popular_words_nonstop[0:50])])\n    plt.yticks([x + 0.5 for x in range(50)], reversed(popular_words_nonstop[0:50]))\n    plt.title(title)\n    plt.show()\n\nplt.figure(figsize=(10,10))\nword_bar_graph_function(environ_trans_conclusion, \"conclusion\", \"Most common words in papers with environment & transmission\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that some of these words are merely situational (i.e. conclusion, section, medrxiv), but others may be particularly common in the subset of papers we have filtered. For example, words like \"epicenter\", \"humidity\", \"city\", \"distance\" and \"lockdown\" seem particularly related to the transmission of the virus and the environmental effects, and they probably won't be that frequent in other articles. \n\nLet's compare this case with papers containing the word \"susceptibility\":"},{"metadata":{"trusted":true},"cell_type":"code","source":"biorxiv_susceptibility = filter_papers_word_list([\"coronavirus\", \"susceptibility\"])\nsusceptibility_conclusion = extract_conclusion(biorxiv, biorxiv_susceptibility)\nplt.figure(figsize=(10,10))\nword_bar_graph_function(susceptibility_conclusion, \"conclusion\", \"Most common words in papers with susceptibility\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, in this case there is no trace of the words \"epicenter\", \"humidity\", \"city\", \"distance\" or \"lockdown\". Instead, we now see large frequencies for words like \"reporting\", \"rate\", \"scenarios\", \"diagnostic\" and \"protocol\", which are more related to the contagion suscpetibility of the population."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}