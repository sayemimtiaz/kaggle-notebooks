{"cells":[{"metadata":{},"cell_type":"markdown","source":"# CNN Architecture Using PyTorch\nName: Vijay Vignesh P<br><br>\nLinkedIn: https://www.linkedin.com/in/vijay-vignesh-0002/ <br><br>\nGitHub: https://github.com/VijayVignesh1 <br><br>\nEmail: vijayvigneshp02@gmail.com <br><br>\n***Please Upvote if you like it*** <br><br>","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the required packages\nimport torch\nimport cv2\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset\nimport torch.nn as nn\nfrom sklearn.metrics import f1_score\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Read the csv file and print the first 10 rows\ncsv=pd.read_csv(\"/kaggle/input/sign-language-mnist/sign_mnist_train.csv\")\nprint(csv.head(10))\nlabels=csv['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the number of data in each label as a countplot.\nplt.figure(figsize=(10,10))\nsns.countplot(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the label (1st Column) and Image Pixels (2nd-785th Column)\ntext=\"pixel\"\nimages=torch.zeros((csv.shape[0],1))\nfor i in range(1,785):\n    temp_text=text+str(i)\n    temp=csv[temp_text]\n    temp=torch.FloatTensor(temp).unsqueeze(1)\n    images=torch.cat((images,temp),1)\nimages_final=torch.FloatTensor(images[:,1:]).view(-1,28,28)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Displaying Data:** <br><br>\nLoop through the first 12 images and display it using Matplotlib. <br><br>\nImages are arranged in 4 rows of three columns each. <br><br>\n***Images are resized to (224,224).*** <br><br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig=plt.figure(figsize=(10,10))\ncolumns=3\nrows=4\nfor i in range(12):\n    img=images_final[i,:]\n    img=img.numpy()\n    img=cv2.resize(img,(224,224))\n    fig.add_subplot(columns, rows, i + 1)\n    plt.imshow(img)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>**GestureDataset class:**</b> <br><br>\nDefining GestureDataset class, which inherits ***Dataset class*** and overrides the two methods ***\\__getitem__*** and ***\\__len__***. <br><br>\nThe main purpose of this class is to process the images and labels into a format which can be directly used for training.<br><br>\n***\\__init__:*** \n*     Reads csv.<br>\n*     Splits Labels and Images.<br>\n*     Converts given 1-D vectors to 2-D images.<br><br>\n***\\__getitem__:*** <br><br>\n*     Reads each image and resizes them to the size (224,224).<br>\n*     The image is then converted to Tensor of type Float.<br>\n*     Finally, the tensor values are normalized to the range (0,1).<br><br>\n***\\__len__:***<br><br>\n*     Returns the number of images in the dataset.<br><br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class GestureDataset(Dataset):\n    def __init__(self,csv,train=True):\n        self.csv=pd.read_csv(csv)\n        self.img_size=224\n        # print(self.csv['image_names'][:5])\n        self.train=train\n        text=\"pixel\"\n        self.images=torch.zeros((self.csv.shape[0],1))\n        for i in range(1,785):\n            temp_text=text+str(i)\n            temp=self.csv[temp_text]\n            temp=torch.FloatTensor(temp).unsqueeze(1)\n            self.images=torch.cat((self.images,temp),1)\n        self.labels=self.csv['label']\n        self.images=self.images[:,1:]\n        self.images=self.images.view(-1,28,28)\n        \n    def __getitem__(self,index):\n        img=self.images[index]\n        img=img.numpy()\n        img=cv2.resize(img,(self.img_size,self.img_size))\n        tensor_image=torch.FloatTensor(img)\n        tensor_image=tensor_image.unsqueeze(0)\n        tensor_image/=255.\n        if self.train:\n            return tensor_image,self.labels[index]\n        else:\n            return tensor_image\n    def __len__(self):\n        return self.images.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using custom GestureDataset class to load train and test data respectively.\ndata=GestureDataset(\"/kaggle/input/sign-language-mnist/sign_mnist_train.csv\")\ndata_val=GestureDataset(\"/kaggle/input/sign-language-mnist/sign_mnist_test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using the in-built DataLoader to create batches of images and labels for training validation respectively. \ntrain_loader=torch.utils.data.DataLoader(dataset=data,batch_size=128,num_workers=4,shuffle=True)\nval_loader=torch.utils.data.DataLoader(dataset=data_val,batch_size=64,num_workers=0,shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining the Model:\nDefining the Classifier class for Classification. <br><br>\nThe ***\\__init__*** method is used to initialize the network layers. <br><br>\nThe ***forward*** method is used to process the input through the initialized layers and return the final output. <br> <br>\n\n<b>Classifier:</b><br>\n1. The model contains 5 Convolutional modules. <br>\n    i) Each Convolutional module consists of a 2D -Convolutional layer, followed by MaxPooling, REctified Linear Unit and Batch Normalization layers.<br>\n    ii) The first convolutional layer takes the input image of size (224,224) and convolves using a 32 channeled kernel of size (5,5).<br> (224,224,1) * (5,5,32) --> (220,220,32) -->MaxPool--> (110,110,32).<br><br>\n    iii) The second convolutional layer takes the output of the first conv layer of size (110,110,32) and convolves using a 64 channeled kernel of size (5,5).<br> (110,110,32) * (5,5,64) --> (106,106,64) -->MaxPool--> (53,53,64).<br><br>\n    iv) The third convolutional layer takes the output of the second conv layer of size (53,53,64) and convolves using a 128 channeled kernel of size (3,3).<br> (53,53,64) * (3,3,128) --> (51,51,128) -->MaxPool--> (25,25,128).<br><br>\n    v) The fourth convolutional layer takes the output of the third conv layer of size (25,25,128) and convolves using a 256 channeled kernel of size (3,3).<br> (25,25,128) * (3,3,256) --> (23,23,256) -->MaxPool--> (11,11,256).<br><br>\n    iii) The fifth convolutional layer takes the output of the fourth conv layer of size (11,11,256) and convolves using a 512 channeled kernel of size (3,3).<br> (11,11,256) * (3,3,512) --> (9,9,512) -->MaxPool--> (4,4,512).<br><br>\n\n2. The model contains two fully connected layers for classification.<br><br>\n    i) The first FC layer takes the flattened output of the final Conv layer of size (512\\*4\\*4,1) reduces the dimension to 256. This layer is follwed by a dropout layer with a dropout probability of 10% <br><br>\n    ii) The second FC layer takes the output of the first and reduces the dimension to 25, which is the number of classes. <br><br>\n    ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self):\n        super(Classifier, self).__init__()\n        self.Conv1 = nn.Sequential(\n        nn.Conv2d(1, 32, 5), # 220, 220\n        nn.MaxPool2d(2), # 110, 110\n        nn.ReLU(),\n        nn.BatchNorm2d(32)\n        )\n        self.Conv2 = nn.Sequential(\n        nn.Conv2d(32, 64, 5), # 106, 106\n        nn.MaxPool2d(2),  # 53,53\n        nn.ReLU(),\n        nn.BatchNorm2d(64)\n        )\n        self.Conv3 = nn.Sequential(\n        nn.Conv2d(64, 128, 3), # 51, 51\n        nn.MaxPool2d(2), # 25, 25\n        nn.ReLU(),\n        nn.BatchNorm2d(128)\n        )\n        self.Conv4 = nn.Sequential(\n        nn.Conv2d(128, 256, 3), # 23, 23\n        nn.MaxPool2d(2), # 11, 11\n        nn.ReLU(),            \n        nn.BatchNorm2d(256)\n        )\n        self.Conv5 = nn.Sequential(\n        nn.Conv2d(256, 512, 3), # 9, 9\n        nn.MaxPool2d(2), # 4, 4\n        nn.ReLU(),\n        nn.BatchNorm2d(512)\n        )\n        \n        self.Linear1 = nn.Linear(512 * 4 * 4, 256)\n        self.dropout=nn.Dropout(0.1)\n        self.Linear3 = nn.Linear(256, 25)\n    def forward(self, x):\n        x = self.Conv1(x)\n        x = self.Conv2(x)\n        x = self.Conv3(x)\n        x = self.Conv4(x)\n        x=self.dropout(x)\n        x = self.Conv5(x)\n        x = x.view(x.size(0), -1)\n        x = self.Linear1(x)\n        x = self.dropout(x)\n        x = self.Linear3(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Validating the model against the validation dataset and generate the accuracy and F1-Score.\ndef validate(val_loader,model):\n    model.eval()\n    test_labels=[0]\n    test_pred=[0]\n    for i, (images,labels) in enumerate(val_loader):\n        outputs=model(images.to(device))\n        predicted = torch.softmax(outputs,dim=1)\n        _,predicted=torch.max(predicted, 1)\n        test_pred.extend(list(predicted.data.cpu().numpy()))\n        test_labels.extend(list(labels.data.cpu().numpy()))\n\n    test_pred=np.array(test_pred[1:])\n    test_labels=np.array(test_labels[1:])\n    correct=(test_pred==test_labels).sum()\n    accuracy=correct/len(test_labels)\n    f1_test=f1_score(test_labels,test_pred,average='weighted')\n    model.train()\n    return accuracy,f1_test ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the model \n1. Define the Cross Entopy loss function. <br>\n2. Define the Adam Optimizer with a learning rate of 1e-3.<br>\n3. Finally, define the learning rate scheduler which reduces the learning rate by a factor of 0.5 (i.e. lr\\*0.5), if the validation accuracy does not reduce after 2 epochs.<br>\n4. Train the model for 20 epochs.<br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Classifier()\nmodel=model.to(\"cuda\")\nmodel.train()\ncheckpoint=None\ndevice=\"cuda\"\nlearning_rate=1e-3\nstart_epoch=0\nend_epoch=20\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, factor=0.5, verbose= True, min_lr=1e-6)\nif checkpoint:\n    model.load_state_dict(torch.load(checkpoint)['state_dict'])\n    start_epoch=torch.load(checkpoint)['epoch']\nfor epoch in range(start_epoch,end_epoch+1):\n    for i, (images,labels) in enumerate(train_loader):\n        outputs=model(images.to(device))\n        loss=criterion(outputs.to(device),labels.to(device))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        predicted = torch.softmax(outputs,dim=1)\n        _,predicted=torch.max(predicted, 1)\n        f1=f1_score(labels.cpu().numpy(),predicted.cpu().numpy(),average='weighted')\n    val_accuracy, val_f1=validate(val_loader,model)\n    print(\"------------------------------------------------------------------------------------------------------\")\n    print(\"Epoch [{}/{}], Training F1: {:.4f}, Validation Accuracy: {:.4f}, Validation F1: {:.4f}\".format(epoch,end_epoch,f1,val_accuracy,val_f1))\n    scheduler.step(val_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the model for future use and optimization.\ntorch.save({\n'epoch': epoch,\n'state_dict': model.state_dict(),\n'optimizer' : optimizer.state_dict()},\n'checkpoint.epoch.1.{}.pth.tar'.format(epoch))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}