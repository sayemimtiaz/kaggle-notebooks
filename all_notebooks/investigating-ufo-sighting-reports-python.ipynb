{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Investigating UFO Sighting Reports\n## - By Sakshat Rao\n\nFor this analysis, I am using a dataset containing approximately 90000 UFO Sighting details. The dataset contains the textual report, geographical details of the sighting spot and other additional information like duration and shape of the possible UFO.\n\nIt is possible to analyze the geography and time series of UFO sightings; in fact there is this [nice analysis](https://www.kaggle.com/ricardoxp1/spatial-analysis-of-ufo-sightings-on-us) which looks at spatial autocorrelation among different states of USA for different time periods. However, for my purpose, I wanted to analyze the textual reports and try to gain some insights about the UFO sightings through these reports.\n\nThis is a small analysis aimed at making use of NLP techniques to extract relevant information from this huge stack of textual reports. The analysis is not perfect as I have made and mentioned few assumptions during the course of this analysis. However, my main objective is to show how NLP can possibly be used with the reports to answer certain valid questions.","metadata":{}},{"cell_type":"markdown","source":"## Table of Content\n1. [Extracting UFO-specific Word Embeddings](#1)\n2. [Visualizing the shape and color of UFO sightings](#2)\n3. [Measuring level of detail of reports](#3)\n4. [Quantifying Durations](#4)","metadata":{}},{"cell_type":"code","source":"!pip install word2number\n!pip install webcolors==1.3\n\nimport numpy as np\nimport pandas as pd\n\nimport os\nimport re\nimport math\nfrom word2number import w2n\nimport webcolors\nfrom time import time\nfrom collections import defaultdict, Counter\nfrom tqdm.notebook import tqdm\n\nimport scipy.stats as stats\n\nimport spacy\nimport multiprocessing\nfrom gensim.models.phrases import Phrases, Phraser\nfrom gensim.models import Word2Vec\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.patches import Rectangle, Circle, Ellipse, Polygon","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-06T17:58:31.102638Z","iopub.execute_input":"2021-06-06T17:58:31.103296Z","iopub.status.idle":"2021-06-06T17:58:56.938087Z","shell.execute_reply.started":"2021-06-06T17:58:31.10319Z","shell.execute_reply":"2021-06-06T17:58:56.936544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_data = pd.read_csv('/kaggle/input/ufo-sightings-approx-100000/nuforc_reports.csv', usecols = ['city', 'state', 'date_time', 'shape', 'duration', 'text', 'city_latitude', 'city_longitude'], parse_dates = ['date_time'], infer_datetime_format = True)\nraw_data.head()","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word2Vec Embeddings <a id=\"1\"></a>\n### Extracting UFO-specific Word Embeddings\n---\nThe main idea here is to obtain embeddings or vector representations of different words and thus find relationships, similarities and commonness among all words. One option is to use pre-trained word embeddings like GloVe, FastText, etc. These embeddings have been obtained after training on extraordinary amounts of data. However, it might be a better idea to create our own word embeddings because of three reasons - \n1. UFO Sightings are reported by common people, hence the vocabulary used may not necessarily be the same as in news or scientific reports. In other words, several informal/slang terms may not be present in the pre-trained word embeddings.\n2. There may be several words specific to UFOs which may not be present in the pre-trained word embeddings. Examples could include shortforms like 'et' (standing for extraterrestrial).\n3. There are around 90000 reports available. Although not large, it is a decent enough amount to train word embeddings and obtain satisfactory performance.","metadata":{}},{"cell_type":"code","source":"# nlp = spacy.load('en', disable=['ner', 'parser'])\n\n# def cleaning(doc):\n#     txt = [token.lemma_ for token in doc if not token.is_stop]\n#     if len(txt) > 2:\n#         return ' '.join(txt)\n\n# brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in raw_data['text'])\n\n# t = time()\n# txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000, n_threads=-1)]\n# print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))\n\n# df_clean = pd.DataFrame({'clean': txt})\n# df_clean = df_clean.dropna().drop_duplicates()\n\n# df_clean = df_clean.reset_index(drop = False)\n# df_clean.to_csv('cleaned_ufo_texts.csv', index = False)\n\n# sent = [row.split() for row in df_clean['clean']]\n# phrases = Phrases(sent, min_count=30, progress_per=10000)\n# bigram = Phraser(phrases)\n# sentences = bigram[sent]\n\n# word_freq = defaultdict(int)\n# for sent in sentences:\n#     for i in sent:\n#         word_freq[i] += 1\n# print(f\"No. of words - {len(word_freq)}\")\n# print(\"Most frequent words - \")\n# print(sorted(word_freq, key=word_freq.get, reverse=True)[:10])\n\n# cores = multiprocessing.cpu_count()\n# w2v_model = Word2Vec(\n#     min_count=20,\n#     window=3,\n#     vector_size=300,\n#     sample=6e-5, \n#     alpha=0.03, \n#     min_alpha=0.0007, \n#     negative=20,\n#     workers=cores-1\n# )\n\n# t = time()\n# w2v_model.build_vocab(sentences, progress_per=10000)\n# print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n\n# t = time()\n# w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n# print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n\n# w2v_model.init_sims(replace=True)\n# w2v_model.save(\"ufo_text.model\")\n\nw2v_model_new = Word2Vec.load(\"/kaggle/input/ufo-word2vec/ufo_text.model\")\n\ndef show_similar_words(words, target_word):\n    print(f\"Words most similar to {target_word}:\")\n    for word_idx, word in enumerate(words):\n        print(f\"{word_idx+1}. {word[0]}\", end = '')\n        for x in range(20 - len(word[0])):\n            print(' ', end = '')\n        if(word_idx % 3 == 2):\n            print()\n    print('\\n')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-06T18:02:29.323171Z","iopub.execute_input":"2021-06-06T18:02:29.323577Z","iopub.status.idle":"2021-06-06T18:02:29.525608Z","shell.execute_reply.started":"2021-06-06T18:02:29.323542Z","shell.execute_reply":"2021-06-06T18:02:29.524644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We extract the Word2Vec word embeddings and try to find which words are similar to different keywords. The keywords I have used here are 'ufo', 'alien', 'spaceship' and 'light'.","metadata":{}},{"cell_type":"code","source":"show_similar_words(w2v_model_new.wv.most_similar(positive=[\"ufo\"]), \"ufo\")","metadata":{"execution":{"iopub.status.busy":"2021-06-06T18:02:30.222878Z","iopub.execute_input":"2021-06-06T18:02:30.223263Z","iopub.status.idle":"2021-06-06T18:02:30.252913Z","shell.execute_reply.started":"2021-06-06T18:02:30.223229Z","shell.execute_reply":"2021-06-06T18:02:30.251853Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We find two really interesting words which are apparently similar to UFO - documentary and history_channel.\n\nFor those unaware, **History Channel** has run shows like 'Ancient Aliens' and a few more which deal with the possibility of humans already contacting aliens. There are several other shows and **documentaries** which explore this possibility.\n\nSo is it fair to say that UFO sightings could have been boosted by the increased attention towards these shows? After all, these word embeddings were trained on UFO reports and they mention history_channel and documentary as being similar to UFOs.\n\nThere is a condition in medical science that I would like to mention here called [Medical Student's Disease](https://en.wikipedia.org/wiki/Medical_students%27_disease). It is a condition where medical students feel they are having a disease which they are currently studying. This happens because students have studied about the disease and its symptoms very well and this causes increased awareness and fear about this disease. As a result, sometimes they could confuse their normal bodily sensations as symptoms of the disease.\n\nCould it be possible that the public also face a similar condition? Watching shows related to UFOs raises their awareness and makes them fearful about it. As a result, they may confuse normal and innocent situations/objects as being weird and extraterrestrial.","metadata":{}},{"cell_type":"code","source":"show_similar_words(w2v_model_new.wv.most_similar(positive=[\"alien\"]), \"alien\")","metadata":{"execution":{"iopub.status.busy":"2021-06-06T18:23:14.579794Z","iopub.execute_input":"2021-06-06T18:23:14.580335Z","iopub.status.idle":"2021-06-06T18:23:14.62085Z","shell.execute_reply.started":"2021-06-06T18:23:14.580299Z","shell.execute_reply":"2021-06-06T18:23:14.619456Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Abduct seems to be similar to alien\n\nThis holds well with several reports of people claiming they were abducted by aliens","metadata":{}},{"cell_type":"code","source":"show_similar_words(w2v_model_new.wv.most_similar(positive=[\"spaceship\"]), \"spaceship\")","metadata":{"execution":{"iopub.status.busy":"2021-06-06T18:24:22.836903Z","iopub.execute_input":"2021-06-06T18:24:22.837248Z","iopub.status.idle":"2021-06-06T18:24:22.858891Z","shell.execute_reply.started":"2021-06-06T18:24:22.837218Z","shell.execute_reply":"2021-06-06T18:24:22.857838Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sci-fi movies seem to have similarities with spaceship\n\nWe find two movies similar to spaceship - Independence Day & Star Trek. [Independence Day](https://en.wikipedia.org/wiki/Independence_Day_(1996_film)) is a 1996 film with an alien invasion and an enormous alien mothership at the core of its plot. [Star Trek](https://en.wikipedia.org/wiki/Star_Trek) on the other hand is not exactly about alien invasion, but nonetheless involves several spaceships like the famous starship USS Enterprise.\n\nIt makes sense that both movies involve spaceships and are apparently similar to 'spaceship'. Again, we can make a similar argument here about the popularity of such sci-fi films and the depiction of UFOs in them boosting UFO sightings.","metadata":{}},{"cell_type":"code","source":"show_similar_words(w2v_model_new.wv.most_similar(positive=[\"light\"]), \"light\")","metadata":{"execution":{"iopub.status.busy":"2021-06-06T18:31:42.6047Z","iopub.execute_input":"2021-06-06T18:31:42.605053Z","iopub.status.idle":"2021-06-06T18:31:42.637362Z","shell.execute_reply.started":"2021-06-06T18:31:42.605021Z","shell.execute_reply":"2021-06-06T18:31:42.636387Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Red, white, yellow and green seem to be the most common colors of 'UFO lights' while bright, flash, blink and dim seem to be the most common characteristics of 'UFO lights'\n\nIt is fascinating to see so much information about UFO lights being obtained from these word embeddings. We can apparently infer not only the colour but also the motion of these UFO lights.\n\nIn the dataset, the colour of UFO sightings is not mentioned as a column. However, it seems we can extract that from the textual reports. We will be doing that for the next section.","metadata":{}},{"cell_type":"markdown","source":"## UFO Color & Shape Analysis <a id=\"2\"></a>\n### Visualizing the shape and color of UFO sightings\n---\nWe shall now extract colors of all UFO sightings from the corresponding report and after pairing them with the shape of the UFO, we can look at the most common UFO sightings.","metadata":{}},{"cell_type":"code","source":"df_clean = pd.read_csv('/kaggle/input/cleaned-ufo-texts/cleaned_ufo_texts.csv')\ndf_clean = df_clean.set_index('index')\n\npossible_colors = webcolors.css3_names_to_hex\n# colors = np.zeros((df_clean.shape[0], len(possible_colors)), dtype = np.bool)\n# for idx, row in tqdm(df_clean.iterrows(), total = df_clean.shape[0]):\n#     for color_idx, color in enumerate(possible_colors):\n#         if(pd.isnull(row['clean'])):\n#             continue\n#         if(re.search(r\"\\b\" + re.escape(color) + r\"\\b\", row['clean'], flags = re.I)):\n#             colors[idx, color_idx] = 1\n\n# with open('ufo_colors.npy', 'wb') as save_file:\n#     np.save(save_file, colors)\n\nwith open('/kaggle/input/ufo-colors-list/ufo_colors.npy', 'rb') as load_file:\n    colors = np.load(load_file)\n\ndef show_frequent_colors(colors):\n    print(\"Most frequent colors:\")\n    for color_idx, color in enumerate(colors):\n        print(f\"{color_idx+1}. {color[0]}\", end = '')\n        for x in range(20 - len(color[0])):\n            print(' ', end = '')\n        if(color_idx % 3 == 2):\n            print()\n    print('\\n')\nshow_frequent_colors(sorted(list(zip(possible_colors, np.sum(colors, axis = 0))), key = lambda x: x[1], reverse = True)[:10])\n\nshape_mapping = {\n    'light': 'circle',\n    'circle': 'circle',\n    'teardrop': 'teardrop',\n    'cigar': 'rectangle',\n    'disk': 'disk',\n    'unknown': 'unknown',\n    'oval': 'oval',\n    'other': 'unknown',\n    'sphere': 'circle',\n    'changing': 'unknown',\n    'formation': 'unknown',\n    'flash': 'circle',\n    'chevron': 'triangle',\n    'triangle': 'triangle',\n    'cylinder': 'rectangle',\n    'fireball': 'circle',\n    'diamond': 'diamond',\n    'egg': 'oval',\n    'cross': 'cross',\n    'rectangle': 'rectangle',\n    'cone': 'triangle'\n}\nraw_data['shape'] = raw_data['shape'].fillna('unknown')\nraw_data['shape'] = raw_data['shape'].map(shape_mapping)\n\ncolors_df = pd.DataFrame(colors, index = df_clean.index, columns = possible_colors)\nshape_color_df = pd.merge(colors_df, raw_data, how = 'left', left_index = True, right_index = True)\n\npossible_shape_color_pairs = []\nfor idx, row in shape_color_df.iterrows():\n    for color in possible_colors:\n        if(row[color] == True):\n            possible_shape_color_pairs.append((row['shape'], color))\npossible_shape_color_pairs = Counter([x for x in possible_shape_color_pairs if x[0] != 'unknown']).most_common()[:25]\ntot_sum = np.sum([x[1] for x in possible_shape_color_pairs])\n\nfig, ax = plt.subplots(1, 1, figsize = (20, 20))\n\nfor y_pos in range(4, -1, -1):\n    for x_pos in range(5):\n        idx = (4 - y_pos) * 5 + x_pos\n        if(possible_shape_color_pairs[idx][0][0] == 'circle'):\n            # Circle\n            ax.add_patch(Circle((x_pos * 4 + 2, y_pos * 4 + 2), 0.25, color = possible_shape_color_pairs[idx][0][1]))\n        elif(possible_shape_color_pairs[idx][0][0] == 'rectangle'):\n            # Rectange\n            ax.add_patch(Rectangle((x_pos * 4 + 2 - 0.25, y_pos * 4 + 2 - 0.25 / 2), 0.5, 0.25, color = possible_shape_color_pairs[idx][0][1]))\n        elif(possible_shape_color_pairs[idx][0][0] == 'triangle'):\n            # Triangle\n            ax.add_patch(Polygon([[x_pos * 4 + 2 - 0.25, y_pos * 4 + 2 - 0.25], [x_pos * 4 + 2, y_pos * 4 + 2 + 0.25], [x_pos * 4 + 2 + 0.25, y_pos * 4 + 2 - 0.25]], color = possible_shape_color_pairs[idx][0][1]))\n        elif(possible_shape_color_pairs[idx][0][0] == 'oval'):\n            # Oval\n            ax.add_patch(Ellipse((x_pos * 4 + 2, y_pos * 4 + 2), 0.5, 0.25, color = possible_shape_color_pairs[idx][0][1]))\n        elif(possible_shape_color_pairs[idx][0][0] == 'disk'):\n            # Disk\n            ax.add_patch(Ellipse((x_pos * 4 + 2, y_pos * 4 + 2), 0.5, 0.25, color = possible_shape_color_pairs[idx][0][1]))\n            ax.add_patch(Ellipse((x_pos * 4 + 2, y_pos * 4 + 2), 0.75 / 2, 0.75 / 4, color = '#38393b'))\n        else:\n            pass\n        ax.annotate(f\"{possible_shape_color_pairs[idx][0][1]} {possible_shape_color_pairs[idx][0][0]}\", (x_pos * 4 + 2, y_pos * 4 + 4 - 1.25), ha = 'center', va = 'center', color = 'white', fontsize = 'xx-large')\n        ax.annotate(f\"({possible_shape_color_pairs[idx][1] * 100 / tot_sum: .1f}%)\", (x_pos * 4 + 2, y_pos * 4 + 1.25), ha = 'center', va = 'center', color = 'white', fontsize = 'large')\n\nstars = np.random.uniform(low = 0, high = 20, size = (200, 2))\nax.scatter(stars[:, 0], stars[:, 1], color = 'white', s = 0.25)\n\nax.set_facecolor('#38393b')\nax.set_xlim(0, 20)\nax.set_ylim(0, 20)\nax.set_xticks([])\nax.set_yticks([])\nax.set_title(\"Most Common UFO Sightings\", fontsize = 20)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-06T18:39:12.978051Z","iopub.execute_input":"2021-06-06T18:39:12.978582Z","iopub.status.idle":"2021-06-06T18:40:34.665739Z","shell.execute_reply.started":"2021-06-06T18:39:12.978537Z","shell.execute_reply":"2021-06-06T18:40:34.664399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The most frequent colors are white, red & orange\n\nThis partially holds true with respect to the UFO colours obtained from the word embeddings since red and white are common to both of them.\n\n### Orange Circles seem to be the most common UFO sighting\n\nI have plotted the top 25 most common UFO sightings. Do note that this is not perfect since solely searching for colors in the textual reports does not guarantee that the color is being used to describe UFOs. For example, if the report contains a text like - \"Through the green leaves, I suddenly saw a bright white light\", then simply searching for colors in the text will make it appear as the UFO sighting was green & white in color. One solution for that is to use advanced NLP and only choose those colors which seem to describe UFOs.","metadata":{}},{"cell_type":"markdown","source":"## Level of Detail in Reports <a id=\"3\"></a>\n---\nNow unfortunately, there is no information available about which UFO sighting is true and which is false. And it doesn't look possible to even have this information.\n\nOne method of trying to predict fakeness in UFO sightings is to analyze the level of detail in the report. Often fake news contains content which is not descriptive, informative or comprehensive. Detailed content is often a decent indicator of true news. Applying a similar idea, we can analyze the level of detail in the UFO sighting reports. Having a vague, non-descriptive report could be a decent indicator of a fake UFO sighting. A descriptive report obviously cannot guarantee an actual UFO sighting and additional investigation would definitely be required.\n\nFor this analysis, I am keeping it simple and equating level of detail to the addition of two terms - \n1. Size of report (number of non-stopwords)\n2. Number of adjectives in report","metadata":{}},{"cell_type":"code","source":"# df_clean['length'] = df_clean['clean'].apply(lambda x: len(x.split(' ')))\n# nlp = spacy.load('en', disable=['ner', 'parser'])\n# # adjective_pos = ['JJ', 'JJR', 'JJS']\n# adjectives = np.zeros(raw_data.shape[0])\n# for idx, row in tqdm(raw_data.iterrows(), total = raw_data.shape[0]):\n#     if(pd.isnull(row['text'])):\n#         continue\n#     adjectives[idx] = len([x.string for x in nlp(row['text']) if x.pos_ == 'ADJ'])\n# raw_data['num_adjectives'] = pd.Series(adjectives, index = raw_data.index)\n# detail_df = pd.merge(df_clean, raw_data, how = 'left', left_index = True, right_index = True)[['length', 'num_adjectives']]\n# detail_df = detail_df.reset_index(drop = False)\n# detail_df.to_csv('detail.csv', index = False)\n\ndetail_df = pd.read_csv('/kaggle/input/ufo-sighting-description-detail/detail.csv')\ncorr_val = detail_df[['length', 'num_adjectives']].corr().values[0, 1]\n\nfig, ax = plt.subplots(1, 1, figsize = (10, 5))\nax.scatter(x = detail_df['length'], y = detail_df['num_adjectives'], alpha = 0.5)\nax.set_xlabel(\"Length of Description\", fontsize = 10)\nax.set_ylabel(\"Number of adjectives in Description\", fontsize = 10)\nax.set_title(f\"Correlation between Length of Description & no. of adjectives: {corr_val:.2f}\", fontsize = 15)\nplt.show()\n\nlength_mean = detail_df['length'].mean(); length_std = detail_df['length'].std()\nnum_adj_mean = detail_df['num_adjectives'].mean(); num_adj_std = detail_df['num_adjectives'].std()\ndetail_df['detail_score'] = detail_df.apply(lambda x: ((x['length'] - length_mean) / length_std) + (x['num_adjectives'] - num_adj_mean) / num_adj_std, axis = 1)\n\nprint(\"\\n\\nUFO Sighting Descriptions with least detail:\\n\")\nfor idx in detail_df[detail_df['detail_score'] < np.quantile(detail_df['detail_score'], 0.001)].sort_values('length', ascending = False).iloc[:10]['index']:\n    print('\" ' + raw_data.loc[idx]['text'] + ' \"')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-06T18:54:57.756924Z","iopub.execute_input":"2021-06-06T18:54:57.757393Z","iopub.status.idle":"2021-06-06T18:54:59.871524Z","shell.execute_reply.started":"2021-06-06T18:54:57.757356Z","shell.execute_reply":"2021-06-06T18:54:59.870217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As I was expecting, the length of the report and the number of adjectives in it is highly correlated which probably means we don't even need to use both of them. It is also evident from the graph that almost all reports have less than 2000 words with certain exceptions having as many as 5000 words.\n\nI have also displayed few reports which I found to have least level of detail. They contain very few non-stopwords and almost no adjectives which really does put a question mark over their validity.\n\nA few ways to better capture level of detail could be to also count proper nouns or even by checking for missing values for other data variables like time of reporting, shape of UFO and location of sighting.","metadata":{}},{"cell_type":"markdown","source":"## Quantifying Duration <a id=\"4\"></a>\n---\nAs some of you might have noticed, the duration variable is actually a string. We do not have numerical values like 5, 60 & 300; instead we have strings like 'about 5 seconds', '1 minute roughly' & '5min'. Converting those strings to numbers is extremely difficult since there are a lot of cases to consider. With basic regular expression knowledge, I was able to approximately quantify most of the 90000 durations (except ~6500). I treated words like 'many', 'few', etc. to be treated as 2 for convenience - that is, I treated durations like 'few minutes' as 2 minutes to keep it simple.","metadata":{}},{"cell_type":"code","source":"def convert_text_into_numbers(x):\n    if(type(x) == float):\n        return x\n    else:\n        x = x.strip()\n        x = re.sub(r\"\\+\", \"\", x, flags = re.I)\n        x = re.sub(r\",\", \"\", x, flags = re.I)\n        x = re.sub(r\"\\?\", \"\", x, flags = re.I)\n        x = re.sub(r\"approx.\", \"\", x, flags = re.I)\n        x = re.sub(r\"approximately\", \"\", x, flags = re.I)\n        x = x.strip()\n        \n        x = re.sub(r\"secound\", 'second', x, flags = re.I)\n        x = re.sub(r\"secend\", 'second', x, flags = re.I)\n        x = re.sub(r\"secopnd\", 'second', x, flags = re.I)\n        x = re.sub(r\"minet\", 'minute', x, flags = re.I)\n        x = re.sub(r\"minit\", 'minute', x, flags = re.I)\n        x = re.sub(r\"miute\", 'minute', x, flags = re.I)\n        x = re.sub(r\"mnute\", \"minute\", x, flags = re.I)\n        x = re.sub(r\"minuet\", \"minute\", x, flags = re.I)\n        x = re.sub(r\"minuite\", \"minute\", x, flags = re.I)\n        x = re.sub(r\"mintue\", \"minute\", x, flags = re.I)\n        x = re.sub(r\"minut[s]*$\", \"minute\", x, flags = re.I)\n        x = re.sub(r\"minuto\", \"minute\", x, flags = re.I)\n        x = re.sub(r\"mineut\", \"minute\", x, flags = re.I)\n        x = re.sub(r\"mimit\", \"minute\", x, flags = re.I)\n        x = re.sub(r\"minte\", \"minute\", x, flags = re.I)\n        x = re.sub(r\"minuit\", \"minute\", x, flags = re.I)\n        \n        x = re.sub(r\"sec[s]*[\\.]*$\", 'second', x, flags = re.I)\n        x = re.sub(r\"(\\d+s)$\", r'\\1econd', x, flags = re.I)\n        x = re.sub(r\"min[s]*[\\.]*$\", \"minute\", x, flags = re.I)\n        x = re.sub(r\"(\\d+m)$\", r'\\1inute', x, flags = re.I)\n        x = re.sub(r\"hr[s]*[\\.]*$\", \"hour\", x, flags = re.I)\n        x = re.sub(r\"(\\d+h)$\", r'\\1our', x, flags = re.I)\n        \n        x = re.sub(r\"zero\", \"0\", x, flags = re.I)\n        x = re.sub(r\"one\", \"1\", x, flags = re.I)\n        x = re.sub(r\"two\", \"2\", x, flags = re.I)\n        x = re.sub(r\"three\", \"3\", x, flags = re.I)\n        x = re.sub(r\"four\", \"4\", x, flags = re.I)\n        x = re.sub(r\"five\", \"5\", x, flags = re.I)\n        x = re.sub(r\"six\", \"6\", x, flags = re.I)\n        x = re.sub(r\"seven\", \"7\", x, flags = re.I)\n        x = re.sub(r\"eight\", \"8\", x, flags = re.I)\n        x = re.sub(r\"nine\", \"9\", x, flags = re.I)\n        \n        x = re.sub(r\"few\", \"2\", x, flags = re.I)\n        x = re.sub(r\"many\", \"2\", x, flags = re.I)\n        x = re.sub(r\"several\", \"2\", x, flags = re.I)\n        x = re.sub(r\"couple\", \"2\", x, flags = re.I)\n        x = re.sub(r\"mere\", \"2\", x, flags = re.I)\n        x = re.sub(r\"^a \", \"1\", x, flags = re.I)\n        x = re.sub(r\"^an \", \"1\", x, flags = re.I)\n        x = re.sub(r\" a \", \"1\", x, flags = re.I)\n        x = re.sub(r\" an \", \"1\", x, flags = re.I)\n        \n        x = re.sub(r\"^second$\", \"1 second\", x, flags = re.I)\n        x = re.sub(r\"^minute$\", \"1 minute\", x, flags = re.I)\n        x = re.sub(r\"^hour$\", \"1 hour\", x, flags = re.I)\n        x = re.sub(r\"^seconds$\", \"2 second\", x, flags = re.I)\n        x = re.sub(r\"^minutes$\", \"2 minute\", x, flags = re.I)\n        x = re.sub(r\"^hours$\", \"2 hour\", x, flags = re.I)\n        \n        new_x = []\n        add_words = False\n        for word in x.split(' '):\n            try:\n                new_word = str(w2n.word_to_num(word))\n            except:\n                new_word = word\n            if(add_words == True):\n                new_x.append(new_word)\n            elif(new_word.isdigit()):\n                add_words = True\n                new_x.append(new_word)\n        if(add_words == True):\n            x = ' '.join(new_x)\n        \n        return x\n\ndef convert_seconds_into_numbers(x):\n    if(type(x) == float):\n        return x\n    else:\n        search_x = re.search(r\"\\d+[ ]*seconds*\", x, flags = re.I)\n        if(search_x):\n            return float(''.join([x for x in search_x.group(0) if x.isdigit()]))\n        else:\n            return x\n\ndef convert_minutes_into_numbers(x):\n    if(type(x) == float):\n        return x\n    else:\n        search_x = re.search(r\"\\d+[ ]*minutes*\", x, flags = re.I)\n        if(search_x):\n            return float(''.join([x for x in search_x.group(0) if x.isdigit()])) * 60\n        else:\n            return x\n\ndef convert_hours_into_numbers(x):\n    if(type(x) == float):\n        return x\n    else:\n        search_x = re.search(r\"\\d+[ ]*hours*\", x, flags = re.I)\n        if(search_x):\n            return float(''.join([x for x in search_x.group(0) if x.isdigit()])) * 3600\n        else:\n            return x\n\nduration = [convert_text_into_numbers(x) for x in raw_data['duration']]\nduration = [convert_seconds_into_numbers(x) for x in duration]\nduration = [convert_minutes_into_numbers(x) for x in duration]\nduration = [convert_hours_into_numbers(x) for x in duration]\n\nnumerical_duration = [x for x in duration if type(x) != str]\nnumerical_duration = [x for x in numerical_duration if math.isnan(x) == False]\nupper_limit = np.quantile(numerical_duration, 0.99)\nnumerical_duration = [x for x in numerical_duration if x <= upper_limit]\n\nfig, ax = plt.subplots(1, 1, figsize = (15, 5))\nax.hist(numerical_duration, bins = 150)\nax.set_xlabel(\"Duration (secs)\", fontsize = 10)\nax.set_ylabel(\"Frequency\", fontsize = 10)\nax.set_title(\"Duration Histogram\", fontsize = 15)\nplt.show()\n\nprint(f\"\\nAverage duration: {np.mean(numerical_duration) / 60: .1f} minutes\")\nprint(f\"Most frequent duration: {stats.mode(numerical_duration).mode[0] / 60: .1f} minutes (appearing {stats.mode(numerical_duration).count[0] / len(numerical_duration) * 100: .1f}% times)\")","metadata":{"execution":{"iopub.status.busy":"2021-06-06T19:08:13.848567Z","iopub.execute_input":"2021-06-06T19:08:13.848999Z","iopub.status.idle":"2021-06-06T19:08:26.055272Z","shell.execute_reply.started":"2021-06-06T19:08:13.848962Z","shell.execute_reply":"2021-06-06T19:08:26.05422Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The durations do have a rather large range of values, ranging from a few seconds to almost 3 hours. In fact, among the ~6500 durations which I wasn't able to quantify, I noticed a few ones like 'overnight' and 'three months' which indicates that the range shown here actually extends way beyond. Also, the most frequent duration mentioned in the reports was 5 minutes (300 seconds).","metadata":{}},{"cell_type":"markdown","source":"# Thank You!\n---\nI hope you enjoyed the analysis! Feel free to check out my code if interested. If you have any suggestions or you wish to point out any mistake which I may have done, do let me know.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}