{"cells":[{"metadata":{"_uuid":"3ac7f0874545c6cbd9aac0f0ccb80e61432b5d0d"},"cell_type":"markdown","source":"# Kaggle around the world\n\n\n![worldClock](https://raw.githubusercontent.com/felipessalvatore/kaggle4life/master/img/kaggle_cover.jpg)\n\na kernel by [Felipe Salvatore](https://twitter.com/Felipessalvador)\n\n## Introduction\n\nIt goes without saying that the number of data scientists, machine learning engineers and AI researchers are not equally distributed around the world. There is a variety of metrics that we can use to understand how big is the machine learning gap between countries and continents. For example, the folks from the Deep Learning Indaba used the number of accepted papers on the annual Conference on Neural Information Processing Systems (NeurIPS - formerly NIPS) to observe that [two entire continents are missing from the contemporary machine learning landscape: South America and Africa](https://goo.gl/yec14p).\n\nWe can do a similar job by looking at the available data about Kaggle (since Kaggle is very popular among data science people, it’s not a stretch to use it as a proxy for the data science community of each country). The main question that we want to address here is the following: *can we see clear disparities between the machine learning/data science communities from the different continents? Are these disparities only performance related? Or are they only socio-economic differences, reflecting the existing inequalities between countries and continents?*\n\nIn order to answer these questions we will use two sources:\n- **Ranked Users Dataset**: Kaggle user data obtained by a crawler and intelligently displayed by [Norconsult](http://kagglerank.azurewebsites.net/).\n\n- **Kaggle’s second annual Machine Learning and Data Science Survey (ML & DS Survey)**: a survey conducted by Kaggle with 23,859 participants from different countries.\n\nThe first dataset will help us understand how each continent *performs* in Kaggle ; the second will guide us to see the *relevant features* of the different ML/DS communities. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"# Here is all the imports you need to run this kernel\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom collections import Counter\nimport seaborn as sns\nimport scipy\nimport itertools\nimport os\nfrom datetime import date\n\npd.options.mode.chained_assignment = None \n\n% matplotlib inline\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"168e8b3faf52e89a121c75e3714810eba5e3b79e","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# For the sake of organization, let's start by defining all the plot functions.\n\ndef bar_plot(Xaxis,\n             Yaxis,\n             df_,\n             title,\n             figsize=(9, 9),\n             decimals=1,\n             color=None,\n             palette=None):\n    \"\"\"\n    Plot a barplot with values on the top of each bar.\n\n    palette reference:\n    https://matplotlib.org/examples/color/colormaps_reference.html\n\n    :param Xaxis: column used to be the x axis\n    :type Xaxis: str\n    :param Yaxis: column used to be the y axis\n    :type Xaxis: str\n    :param df_: data frame\n    :type df_: pd.DataFrame\n    :param title: plot's title\n    :type title: str\n    :param path: path to save plot\n    :type path: str\n    :param figsize: plot's size\n    :type figsize: tuple\n    :param color: color for all of the elements\n    :type color: srt\n    :param palette: matplotlib color palette\n    :type palette: srt\n    \"\"\"\n    fig, ax = plt.subplots(1, 1, figsize=figsize)\n    ax = sns.barplot(x=Xaxis, y=Yaxis, data=df_, color=color, palette=palette)  # noqa\n    ax.set_xlabel(Xaxis, fontsize=20)\n    ax.set_ylabel(Yaxis, fontsize=20)\n    for p in ax.patches:\n        ax.annotate(np.round(p.get_height(), decimals=decimals),\n                    (p.get_x() + p.get_width() / 2.,\n                     p.get_height()),\n                    ha='center',\n                    va='center',\n                    xytext=(0, 10),\n                    textcoords='offset points',\n                    fontweight='bold',\n                    color='black')\n    fig.suptitle(title, fontsize=18, fontweight='bold')\n\n\ndef series_plot(place_list,\n                place2series,\n                time_series,\n                title,\n                xlabel,\n                ylabel,\n                path,\n                figsize=(15, 12)):\n    \"\"\"\n    Plot a time series\n\n    :param place_list: list of places for reference (countries, continents)\n    :type place_list: [str]\n    :param place2series: dict mapping places to time series\n    :type place2series: {str:[tuple]}\n    :param time_series: list of dates\n    :type time_series: [datetime.date]\n    :param title: plot's title\n    :type title: str\n    :param xlabel: x axis label\n    :type xlabel: str\n    :param ylabel: y axis label\n    :type ylabel: str\n    :param path: path to save plot\n    :type path: str\n    :param figsize: plot's size\n    :type figsize: tuple\n    \"\"\"\n    lines = []\n    fig, ax = plt.subplots(1, 1, figsize=figsize)\n    ax.grid(linewidth=0.4)\n    for place in place_list:\n        series = place2series[place]\n        place_y = [cdr for car, cdr in series]\n        line, = ax.plot_date(x=time_series, y=place_y, ls='-', label=place)\n        lines.append(line)\n    plt.legend(handles=lines, loc=2, fontsize=18)\n    fig.suptitle(title, fontsize=24, fontweight='bold')\n    ax.set_xlabel(xlabel, fontsize=20)\n    ax.set_ylabel(ylabel, fontsize=20)\n    plt.savefig(path)\n\n\ndef plot_stacked_bar(df_,\n                     place_list,\n                     base_column,\n                     target_column,\n                     palette,\n                     value_list,\n                     title,\n                     figsize=(12, 6),\n                     ylabel='number of responses'):\n    \"\"\"\n    Plot a barplot stacking different bars.\n    The bars are defined by a list of values from the target_column\n    \n    :param df_: data frame\n    :type df_: pd.DataFrame\n    :param place_list: list of places for reference (countries, continents)\n    :type place_list: [str]\n    :param base_column: df_ column used to define x axis\n    :type base_column: str\n    :param target_column: df_ column used to define the different bars\n    :type target_column: str\n    :param palette: matplotlib color palette\n    :type palette: srt\n    :param value_list: list of the different values appearing in target_column\n    :type value_list: [str]\n    :param title: plot's title\n    :type title: str\n    :param figsize: plot's size\n    :type figsize: tuple\n    :param ylabel: y axis label\n    :type ylabel: str\n\n    :return: target DataFrame\n    :rtype: pd.DataFrame\n    \"\"\"\n    all_entries = []\n    for place in place_list:\n        target_dict = Counter(df_[df_[base_column] == place][target_column])\n        entry = [place] + [target_dict[value] for value in value_list]\n        all_entries.append(entry)\n    target_columns = [base_column] + value_list\n    df_target = pd.DataFrame(columns=target_columns,\n                             data=all_entries)\n    values_number = len(value_list)\n    colormap = ListedColormap(sns.color_palette(palette, values_number))\n    ax = df_target.set_index(base_column)\\\n        .reindex(df_target.set_index(base_column).sum().sort_values().index, axis=1)\\\n        .plot(kind='bar',\n              rot=45,\n              stacked=True,\n              colormap=colormap,\n              figsize=figsize)\n    ax.set_xlabel(base_column, fontsize=20, x=0.45, y=2)\n    ax.set_ylabel(ylabel, fontsize=20)\n    ax.set_title(title, fontsize=18, fontweight='bold')\n    return df_target\n\n\ndef plot_stacked_bar_simpl(df_,\n                           base_column,\n                           values_number,\n                           palette,\n                           title,\n                           figsize=(12, 6),\n                           ylabel='number of responses'):\n    \"\"\"\n    Plot a barplot stacking different bars.\n    The bars are defined by a list of values from the target_column\n    uses a already prepared df_\n    \n    :param df_: data frame\n    :type df_: pd.DataFrame\n    :param base_column: df_ column used to define x axis\n    :type base_column: str\n    :param values_number: number of values\n    :type values_number: int\n    :param palette: matplotlib color palette\n    :type palette: srt\n    :param title: plot's title\n    :type title: str\n    :param figsize: plot's size\n    :type figsize: tuple\n    :param ylabel: y axis label\n    :type ylabel: str\n    \"\"\"\n    df_target = df_\n    colormap = ListedColormap(sns.color_palette(palette, values_number))\n    ax = df_target.set_index(base_column)\\\n        .reindex(df_target.set_index(base_column).sum().sort_values().index, axis=1)\\\n        .plot(kind='bar',\n              rot=45,\n              stacked=True,\n              colormap=colormap,\n              figsize=figsize)\n    ax.set_xlabel(base_column, fontsize=20, x=0.45, y=2)\n    ax.set_ylabel(ylabel, fontsize=20)\n    ax.set_title(title, fontsize=18, fontweight='bold')\n    \n    \ndef plot_distances(names_,\n                   distances_,\n                   title,\n                   cmap=plt.cm.Oranges,\n                   figsize=(9, 9)):\n    \"\"\"\n    Plot a matrix with KL-distances.\n    \n    cmap reference:\n    https://matplotlib.org/examples/color/colormaps_reference.html\n    \n    :param names_: row/collum names\n    :type names_: [str]\n    :param distances_: matrix with distances\n    :type distances_: np.array\n    :param title: image title\n    :type title: str\n    :param cmap: plt color map\n    :type cmap: plt.cm\n    :param figsize: plot's size\n    :type figsize: tuple\n    \"\"\"\n    plt.figure(figsize=figsize)\n    plt.imshow(distances_, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=24, fontweight='bold')\n    plt.colorbar()\n    tick_marks = np.arange(len(names_))\n    plt.xticks(tick_marks, names_, rotation=45)\n    plt.yticks(tick_marks, names_)\n    thresh = distances_.max() / 2.\n    for i, j in itertools.product(range(distances_.shape[0]), range(distances_.shape[1])):\n        plt.text(j, i, format(distances_[i, j], '.2f'),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if distances_[i, j] > thresh else \"black\")\n\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33f83c66027401e38b47936db9c3fa8e9f25a18c"},"cell_type":"markdown","source":"## Measuring performance\n\nThe Ranked Users Dataset was obtained by scraping the location data off Kaggle's websites (you can find the dataset [here](https://www.kaggle.com/felsal/ranked-users-kaggle-data)).  Kaggle has different categories of expertise. We will work with only one of them: *competition*. Inside this category there are five performance tiers: novice, contributor, expert, master and grandmaster. *Ranked users are users that are expert tier or higher*.\n\nLooking at the data we can see that each row represent a ranked user. The columns are: *register date*, *current points*, *current ranking*, *highest ranking*, *country* and *continent*.  In Kaggle,  points and ranking change over time.  So, all the positions represented here correspond only to a specific point in time (around August 2018). "},{"metadata":{"trusted":true,"_uuid":"320892d8f8eafa646c299f0283998c140cc4c720","_kg_hide-input":true},"cell_type":"code","source":"# Let's see some examples of this csv and used it to organize the countries in continents. \ndf_rank = pd.read_csv(\"../input/ranked-users-kaggle-data/ranked_users_kaggle.csv\")\ndf_rank.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7624b8a73186ef33ca0286d72e1ba0ccaaac969","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"country2continent  = {k:v for k,v in zip(df_rank[\"Country\"].values, df_rank[\"Continent\"].values)}\n\nSouth_America = [i for i in list(country2continent.keys()) if country2continent[i]==\"South America\"]\nNorth_America = [i for i in list(country2continent.keys()) if country2continent[i]==\"North America\"]\nCentral_America = [i for i in list(country2continent.keys()) if country2continent[i]==\"Central America\"]\nAfrica = [i for i in list(country2continent.keys()) if country2continent[i]==\"Africa\"]\nEurope = [i for i in list(country2continent.keys()) if country2continent[i]==\"Europe\"]\nAsia =[i for i in list(country2continent.keys()) if country2continent[i]==\"Asia\"]\nOceania = [i for i in list(country2continent.keys()) if country2continent[i]==\"Oceania\"]\n\n\ncontinent2country = {\"South America\": South_America,\n                     \"North America\": North_America,\n                     \"Central America\": Central_America,\n                     \"Africa\": Africa,\n                     \"Europe\": Europe,\n                     \"Asia\": Asia,\n                     \"Oceania\": Oceania}\n\nall_countries = list(set(df_rank[\"Country\"].values))\nall_continents = list(set(df_rank[\"Continent\"].values))\n\nall_continents = [c for c in all_continents if c!='None']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74a2de1453ff2814f821c71e0fdbf3f067b50e8b"},"cell_type":"markdown","source":"### Ranked users around the world"},{"metadata":{"trusted":true,"_uuid":"7bf0ccc50f18f03a8a411028f0e8d5ce81e2cef6","_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"def get_top_n(df_, column, new_name, n=10, column_2=\"Points\", mode=\"count\"):\n    \"\"\"\n    Get top n places (as defined in column) from DataFrame df_\n    by using the values from column_2 as reference.\n\n    :param df_: data frame\n    :type df_: pd.DataFrame\n    :param column: df_ column used to groub values from column_2\n    :type column: str\n    :param new_name: name of the new aggregated value\n    :type new_name: str\n    :param n: number of top places\n    :type n: int\n    :param column_2: df_ column with reference values \n    :type column_2: str\n    :param mode: mode to aggregate values\n    :type mode: str\n    :return: top n DataFrame\n    :rtype: pd.DataFrame\n    \"\"\"\n    if mode == \"count\":\n        place_counts = df_.groupby(column)[column_2].count()\n    elif mode == \"mean\":\n        place_counts = df_.groupby(column)[column_2].mean()\n    elif mode == \"sum\":\n        place_counts = df_.groupby(column)[column_2].sum()\n    else:\n        return None\n    place_counts = place_counts.sort_values(ascending=False)\n    place2count = dict(place_counts)\n    place2count = {k: v for k, v in place2count.items() if k != 'None'}\n    place2count_keys = list(place2count.keys())\n    place2count_values = list(place2count.values())\n    df_place = pd.DataFrame({column: place2count_keys, new_name: place2count_values})\n    top_n = df_place.head(n)\n    return top_n\n\n\ntop10_countries = get_top_n(df_=df_rank,\n                            column='Country',\n                            new_name='Number of ranked users')\n\ntop10_continents = get_top_n(df_=df_rank,\n                             column='Continent',\n                             new_name='Number of ranked users')\n\nbar_plot(\"Country\",\n         'Number of ranked users',\n         df_=top10_countries,\n         title=\"Plot 1: Top 10 countries by number of ranked users\",\n         figsize=(12.6, 9))\n\n\nbar_plot(\"Continent\",\n         'Number of ranked users',\n         df_=top10_continents,\n         title=\"Plot 2: Ranked users by continent\",\n         figsize=(12.6, 9))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d00f726e94e9c79be7e15067c4a465407448201"},"cell_type":"markdown","source":"It should be noted that these plots are not complete, **26.6%** of total ranked users (1267 users) don’t disclose their location.\n\nLooking at **Plot 1** it’s clear that United States is the country with most ranked users by a large margin. If you remove the United States from the list of countries presented here the disparity is not so big.\n\n**Plot 2** shows that there are two distinct groups of continents: on one side, North America, Asia and Europe (**Group 1**); on the other side, Oceania, South America, Africa and Central America (**Group 2**). The first group is responsible for **69.1%** of all ranked users from Kaggle, hence the second group retain only **4.3%** of ranked users.\n\nUsing the register date of each user we can see how the different regions in the world evolve inside Kaggle:"},{"metadata":{"trusted":true,"_uuid":"7db75694174fd270db5b7b101cef9dcc18f20b23","_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"# Let's extract the time series from the DataFrame.\n\ndf_rank[\"Year\"] = [int(i.split(\"/\")[2]) for i in df_rank[\"RegisterDate\"].values]\n\nyears = list(set(df_rank[\"Year\"].values))\nyears.sort()\nyears_d = [date(y, 1, 1) for y in years]\n\ndef get_series(df_, column, place_list, time_series):\n    \"\"\"\n    Get a dictionary of series from a DataFrame\n    using the values of column as reference\n    \n    :param df_: data frame\n    :type df_: pd.DataFrame\n    :param column: df_ column for the kind of place that will be used\n                   (countries, continents)\n    :type column: str\n    :param place_list: list of places for reference (countries, continents)\n    :type place_list: [str]\n    :param time_series: list of dates\n    :type time_series: [datetime.date]\n    :return: dict mapping places to time series\n    :rtype: {str:[tuple]}\n    \"\"\"\n    place2series = {}\n    for place in place_list:\n        df_time_place = df_[df_[column] == place]\n        df_time_per_place = df_time_place.groupby('Year')\n        df_time_per_place = df_time_per_place.count()\n        df_time_per_place.reset_index(inplace=True)\n        dict_ = {k:v for k,v in zip(df_time_per_place[\"Year\"].values, df_time_per_place[\"Points\"].values)}\n        tuples = []\n        total = 0\n        for year in time_series:\n            if year in dict_:\n                current = dict_[year]\n            else:\n                current = 0\n            total += current\n            tuples.append((year, total))\n        place2series[place] = tuples\n    \n    return place2series\n\ncontinent2series = get_series(df_=df_rank,\n                              column=\"Continent\",\n                              place_list=all_continents,\n                              time_series=years)\n\ncountry2series = get_series(df_=df_rank,\n                            column='Country',\n                            place_list=all_countries,\n                            time_series=years)\n\n# To help visualization let's change the order of the continents\n\nall_continents = [\"North America\",\n                  \"Asia\",\n                  \"Europe\",\n                  \"Oceania\",\n                  \"South America\",\n                  \"Africa\",\n                  \"Central America\"]\n\n\nseries_plot(place_list=all_continents,\n            place2series=continent2series,\n            time_series=years_d,\n            title=\"Plot 3: Ranked users in Kaggle from 2010 to 2018\",\n            xlabel=\"Year\",\n            ylabel='Number of ranked users',\n            path=\"ranked_users_region.png\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a02071d406c0181c0c57dbcd83ce4b21d53e98f6"},"cell_type":"markdown","source":"And we can also see the evolution of each country: "},{"metadata":{"trusted":true,"_uuid":"445677550cb1caca6b1d119ccfd3fddf66ae8a1b","_kg_hide-input":true},"cell_type":"code","source":"# To help visualization let's use only the countries\n# with most ranked users from Asia and Europe\n\nAsia_ = [\"China\",\n         \"Russia\",\n         \"India\",\n         \"Vietnam\",\n         \"Singapore\",\n         \"Hong Kong\",\n         \"Taiwan\",\n         \"Japan\",\n         \"Israel\",\n         \"South Korea\"]\n\nEurope_ = [\"France\",\n           \"United Kingdom\",\n           \"Germany\",\n           \"Ukraine\",\n           \"Netherlands\",\n           \"Italy\",\n           \"Spain\",\n           \"Poland\",\n           \"Belgium\",\n           \"Belarus\"]\n\ncontinent2country[\"Asia\"] = Asia_\ncontinent2country[\"Europe\"] = Europe_\n\nfor i, continent in enumerate(all_continents):\n    i += 4\n    if continent != \"None\":\n        \n        series_plot(place_list=continent2country[continent],\n            place2series=country2series,\n            time_series=years_d,\n            title=\"Plot {}: Ranked users in Kaggle from 2010 to 2018 ({})\".format(i, continent),\n            xlabel=\"Year\",\n            ylabel='Number of ranked users',\n            path=\"ranked_users_series_{}.png\".format(continent))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b658da55761818356903cb2ce714e7522be3283d"},"cell_type":"markdown","source":"**Plot 3** shows clearly the difference of the two groups of continents. One thing to be noted here is that although Europe was consistently dominating Asia, in 2016 the later began to close the gap between them, and from 2017 onwards Asia start having more ranked users than Europe.\n\n**Plots 4–10** show the dynamics of each region. In North America, Asia, Oceania, South America and Africa we can see clearly one country dominating the others: United States, Russia, Australia, Brazil and South Africa, respectively. Central America countries have equal number of users (only one each). And in Europe we can see France, United Kingdom and Germany competing for the first place.\n\n### Points and ranks\n\nKaggle has a point system. They are designed to [decay over time](https://www.kaggle.com/progression). So they only reflect the Kaggle landscape at the time the data was collected. Summing the points for each user of a specific country we got the following plots:"},{"metadata":{"trusted":true,"_uuid":"1b106f8ecb98f61b908eac3110c0e15210c5040b","_kg_hide-input":true},"cell_type":"code","source":"df_rank[\"Points\"] = list(map(lambda x: float(x), df_rank[\"Points\"].values))\n\ntop_countries_points = get_top_n(df_=df_rank,\n                                 column='Country',\n                                 new_name=\"Points\",\n                                 n=10,\n                                 column_2=\"Points\",\n                                 mode=\"sum\")\n\ntop_continents_points = get_top_n(df_=df_rank,\n                                 column='Continent',\n                                 new_name=\"Points\",\n                                 n=10,\n                                 column_2=\"Points\",\n                                 mode=\"sum\")\n\nbar_plot(\"Country\",\n         \"Points\",\n         df_=top_countries_points,\n         title=\"Plot 11: Top 10 countries by total points\",\n         figsize=(12.6, 9))\n\nbar_plot(\"Continent\",\n         \"Points\",\n         df_=top_continents_points,\n         title=\"Plot 12: Total points by continent\",\n         figsize=(12.6, 9))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6466ea0c68326ba49dc27d657d6a9089af94403"},"cell_type":"markdown","source":"**Plot 11** show a scenario similar to **Plot 1**, the only big difference here is the appearance of Brazil (leaving Australia out of the list). This is the effect of some high ranked users from this country (at the time the data was collected the first place belonged to a Brazilian user).\n\nOne thing that can be seen in **Plot 12** is how both Asia and Europe have more points than North America. Remember what we saw in **Plot 2**, North America is the continent with most ranked users. The points decay explains the difference between **Plots 2** and **12**. This will become clear when we analyze the ranking information.  \n\nIn this data we have for each ranked user his/her *current rank* and also the *highest rank* him/her have ever achieved. In order to compare countries and continents, we can use the following score function:\n\n<img src=\"https://raw.githubusercontent.com/felipessalvatore/kaggle4life/master/img/score.png\" width=\"350px\"/>\n\nThis function returns a score such that the user with the highest rank receives 1 and the user with lowest rank receives 0, and for anyone in between it returns a real number in the interval (0,1).\n\nSumming the score by continent we get:"},{"metadata":{"trusted":true,"_uuid":"22ffa51dee59c22de8656fa07d0af30341f9e23d","_kg_hide-input":true},"cell_type":"code","source":"# Creating two new collumns with the score fuction\n\ndf_rank[\"CurrentRanking\"] = list(map(lambda x: int(x), df_rank[\"CurrentRanking\"].values))\nlowest_rank = np.max(df_rank[\"CurrentRanking\"].values)\nf_score = lambda x : np.abs((np.log(x/lowest_rank)) / (np.log(1/lowest_rank)))\ndf_rank[\"CurrentRankingScore\"] = list(map(f_score, df_rank[\"CurrentRanking\"].values))\n\n\ndf_rank[\"HighestRanking\"] = list(map(lambda x: int(x), df_rank[\"HighestRanking\"].values))\nlowest_rank = np.max(df_rank[\"HighestRanking\"].values)\nf_score = lambda x : np.abs((np.log(x/lowest_rank)) / (np.log(1/lowest_rank)))\ndf_rank[\"HighestRankingScore\"] = list(map(f_score, df_rank[\"HighestRanking\"].values))\n\ntop_continents_HR = get_top_n(df_=df_rank,\n                              column='Continent',\n                              new_name=\"Highest ranking score\",\n                              n=10,\n                              column_2=\"HighestRankingScore\",\n                              mode=\"sum\")\n\ntop_continents_CR = get_top_n(df_=df_rank,\n                              column='Continent',\n                              new_name=\"Current ranking score\",\n                              n=10,\n                              column_2=\"CurrentRankingScore\",\n                              mode=\"sum\")\n\n\nbar_plot(\"Continent\",\n         \"Highest ranking score\",\n         df_=top_continents_HR,\n         title=\"Plot 13: Highest ranking score by continent\",\n         figsize=(12.6, 9))\n\n\nbar_plot(\"Continent\",\n         \"Current ranking score\",\n         df_=top_continents_CR,\n         title=\"Plot 14: Current ranking score by continent\",\n         figsize=(12.6, 9))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25b5b5f74746919254791e56f3d86fa9229a2b59"},"cell_type":"markdown","source":"If we compare the differences from **Plots 13** and **14** we see the following: North America presents the highest *highest rank score* because many ranked users from this continent have achieved good ranks in the *past* but *nowadays* when we look at the *current rank score* Asia and Europe have a better performance. Similarly, when we look at **Group 2** we can see that South America have *currently* the best results among the continents from this group, leaving Oceania at the second position.\n\n*The key fact here is that at some point in time the users from Asia and Europe become better at machine learning competitions and took the place from North America. From this data we can’t say exactly when. But the shift is clear.*\n\n## Taking a closer look to each community\n\nBy asking a variety of interesting questions, the ML & DS Survey have generated a rich description of the machine learning community of each country. For the sake of brevity, we have concentrated our focus on some specific questions only. The selected questions reflect subjects were the geographic differences are usually more prominent: *age*, *formal education*, *years using machine learning*,  *yearly compensation* and  *use of technology*.\n\n### ML & DS Survey's geographical distribution"},{"metadata":{"trusted":true,"_uuid":"923deb73991c56b9c8f3bd7c349af9b7175fdb35","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Before plotting anything, let's re-organize the data.\n# To do so we need to standardize some countries names, and add new countries to our country2continent dict\n\ncountry2continent[\"Kenya\"] = \"Africa\"\ncountry2continent['Tunisia'] = \"Africa\"\ncountry2continent['Bangladesh'] = \"Asia\"\n\ncountry2continent_f = lambda x: country2continent[x] if x in country2continent else x\n\nSouth_America = [i for i in list(country2continent.keys()) if country2continent[i]==\"South America\"]\nNorth_America = [i for i in list(country2continent.keys()) if country2continent[i]==\"North America\"]\nAfrica = [i for i in list(country2continent.keys()) if country2continent[i]==\"Africa\"]\nEurope = [i for i in list(country2continent.keys()) if country2continent[i]==\"Europe\"]\nAsia =[i for i in list(country2continent.keys()) if country2continent[i]==\"Asia\"]\nOceania = [i for i in list(country2continent.keys()) if country2continent[i]==\"Oceania\"]\n\n\ncontinent2country = {\"South America\": South_America,\n                     \"North America\": North_America,\n                     \"Africa\": Africa,\n                     \"Europe\": Europe,\n                     \"Asia\": Asia,\n                     \"Oceania\": Oceania}\n\ndf_mult = pd.read_csv('../input/kaggle-survey-2018/multipleChoiceResponses.csv',low_memory=False)\n\nselected_question = [\"Q2\",\n                     \"Q3\",\n                     \"Q4\",\n                     \"Q9\",\n                     \"Q25\",\n                     \"Q15_Part_1\",\n                     \"Q15_Part_2\",\n                     \"Q15_Part_3\",\n                     \"Q15_Part_4\",\n                     \"Q15_Part_5\",\n                     \"Q15_Part_6\",\n                     \"Q15_Part_7\",\n                     \"Q19_Part_1\",\n                     \"Q19_Part_2\",\n                     \"Q19_Part_3\",\n                     \"Q19_Part_4\",\n                     \"Q19_Part_5\",\n                     \"Q19_Part_6\",\n                     \"Q19_Part_7\",\n                     \"Q19_Part_8\",\n                     \"Q19_Part_9\",\n                     \"Q19_Part_10\",\n                     \"Q19_Part_11\",\n                     \"Q19_Part_12\",\n                     \"Q19_Part_13\",\n                     \"Q19_Part_14\",\n                     \"Q19_Part_15\",\n                     \"Q19_Part_16\",\n                     \"Q19_Part_17\",\n                     \"Q19_Part_18\",\n                     \"Q19_Part_19\"] \n\ndf_less = df_mult[selected_question]\n\n# Let's simplify the answers to help visualization.\n\nsimple_questions = [\"Q2\",\n                     \"Q3\",\n                     \"Q4\",\n                     \"Q9\",\n                     \"Q25\"]\n\nmultiple_questions = [\"Q15_Part_1\",\n                     \"Q15_Part_2\",\n                     \"Q15_Part_3\",\n                     \"Q15_Part_4\",\n                     \"Q15_Part_5\",\n                     \"Q15_Part_6\",\n                     \"Q15_Part_7\",\n                     \"Q19_Part_1\",\n                     \"Q19_Part_2\",\n                     \"Q19_Part_3\",\n                     \"Q19_Part_4\",\n                     \"Q19_Part_5\",\n                     \"Q19_Part_6\",\n                     \"Q19_Part_7\",\n                     \"Q19_Part_8\",\n                     \"Q19_Part_9\",\n                     \"Q19_Part_10\",\n                     \"Q19_Part_11\",\n                     \"Q19_Part_12\",\n                     \"Q19_Part_13\",\n                     \"Q19_Part_14\",\n                     \"Q19_Part_15\",\n                     \"Q19_Part_16\",\n                     \"Q19_Part_17\",\n                     \"Q19_Part_18\",\n                     \"Q19_Part_19\"] \n\nnan_dict_simple = {q: \"Other\" for q in simple_questions}\nnan_dict_multiple = {q: \" \" for q in multiple_questions}\n\nnan_dict = {**nan_dict_simple, **nan_dict_multiple}\n\ndf_less.fillna(nan_dict, inplace=True)\n\n# Simplifying Q2\n\nq2_simpl = {'25-29':\"22-29\",\n            '22-24': \"22-29\",\n            '30-34': \"30-39\",\n            '18-21': '18-21',\n            '35-39': \"30-39\",\n            '40-44': \"40-49\",\n            '45-49': \"40-49\",\n            '50-54': \"50-59\",\n            '55-59': \"50-59\",\n            '60-69': \"60+\",\n            '70-79': \"60+\",\n            '80+': \"60+\"}\n\n\nq2_simpl_f = lambda x: q2_simpl[x] if x in q2_simpl else x\n\ndf_less[\"Age\"] = list(map(q2_simpl_f, df_less[\"Q2\"]))\n\n# Simplifying Q3 and adding a column for to indicate the user's continent\n\nq3_simpl = {'United States of America': \"United States\",\n            'Other': \"None\",\n            'Iran, Islamic Republic of...': \"Iran\",\n            'United Kingdom of Great Britain and Northern Ireland': \"United Kingdom\",\n            'I do not wish to disclose my location': \"None\",\n            'Hong Kong (S.A.R.)': \"Hong Kong\",\n            'Viet Nam': \"Vietnam\",\n            'Republic of Korea': \"South Korea\"}\n\nq3_simpl_f = lambda x: q3_simpl[x] if x in q3_simpl else x\n\ndf_less[\"Country\"] = list(map(q3_simpl_f, df_less[\"Q3\"]))\n\ndf_less[\"Continent\"] = list(map(country2continent_f, df_less[\"Country\"]))\n\n# Simplifying Q4\n\nq4_simpl = {'I prefer not to answer': \"Other\"}\nq4_simpl_f = lambda x: q4_simpl[x] if x in q4_simpl else x\n\ndf_less[\"FormalEducation\"] = list(map(q4_simpl_f, df_less[\"Q4\"]))\n\n# Simplifying Q9\n\nq9_simpl = {'200-250,000': '200,000+',\n            '250-300,000': '200,000+',\n            '300-400,000': '200,000+',\n            '400-500,000': '200,000+',\n            '500,000+': '200,000+',\n            'I do not wish to disclose my approximate yearly compensation': \"Other\"}\nq9_simpl_f = lambda x: q9_simpl[x] if x in q9_simpl else x\n\ndf_less[\"CurrentYearlyCompensation$USD\"] = list(map(q9_simpl_f, df_less[\"Q9\"]))\n\n# Simplifying Q15 and adding a column to each cloud computing service\n\ndf_less[\"CloudComputing\"] =  df_less[\"Q15_Part_1\"] + df_less[\"Q15_Part_2\"] + df_less[\"Q15_Part_3\"] + df_less[\"Q15_Part_4\"] + df_less[\"Q15_Part_5\"] + df_less[\"Q15_Part_6\"] + df_less[\"Q15_Part_7\"] \n\nf_strip = lambda x: x.strip()\n\ndf_less[\"CloudComputing\"] = list(map(f_strip, df_less[\"CloudComputing\"].values))\n\ndef cloud_f(service): return lambda x: int(x.find(service) != -1)\n\nall_services = [\"Azure\",\n                \"GCP\",\n                \"IBM\",\n                \"AWS\",\n                \"Alibaba\"]\n\nall_f_cloud = list(map(cloud_f, all_services))\n\ndef apply(y): return lambda f: f(y)\n\ndef sum_f(x): return str(np.sum(list(map(apply(x), all_f_cloud))))\n\nfor service in all_services:\n\n    df_less[service] = list(map(cloud_f(service), df_less[\"CloudComputing\"].values))\n    \ndf_less[\"CloudComputing_num\"] = list(map(sum_f, df_less[\"CloudComputing\"].values))\n\n# Simplifying Q19 and adding a column to each machine learning framework\n\ndf_less[\"Framework\"] = df_less[\"Q19_Part_1\"] + df_less[\"Q19_Part_2\"] + df_less[\"Q19_Part_3\"] + df_less[\"Q19_Part_4\"] + df_less[\"Q19_Part_5\"] + df_less[\"Q19_Part_6\"] + df_less[\"Q19_Part_7\"] + df_less[\"Q19_Part_8\"] + df_less[\"Q19_Part_9\"] + \\\n    df_less[\"Q19_Part_10\"] + df_less[\"Q19_Part_11\"] + df_less[\"Q19_Part_12\"] + df_less[\"Q19_Part_13\"] + df_less[\"Q19_Part_14\"] + \\\n    df_less[\"Q19_Part_15\"] + df_less[\"Q19_Part_16\"] + \\\n    df_less[\"Q19_Part_17\"] + df_less[\"Q19_Part_18\"] + df_less[\"Q19_Part_19\"]\n\ndef f_strip(x): return x.strip()\n\ndf_less[\"Framework\"] = list(map(f_strip, df_less[\"Framework\"].values))\n\nall_frameworks = [\"Scikit-Learn\",\n                  \"TensorFlow\",\n                  \"Keras\",\n                  \"PyTorch\",\n                  \"Spark MLlib\",\n                  \"H20\",\n                  \"Fastai\",\n                  \"Mxnet\",\n                  'Caret',\n                  \"Xgboost\",\n                  \"mlr\",\n                  \"Prophet\",\n                  \"randomForest\",\n                  \"lightgbm\",\n                  \"catboost\",\n                  \"CNTK\",\n                  \"Caffe\"]\n\n\ndef framework_f(framework): return lambda x: int(x.find(framework) != -1)\n\nall_f = list(map(framework_f, all_frameworks))\n\ndef apply(y): return lambda f: f(y)\n\ndef sum_f(x): return str(np.sum(list(map(apply(x), all_f))))\n\nfor frame in all_frameworks:\n    df_less[frame] = list(map(framework_f(frame), df_less[\"Framework\"].values))\n\ndf_less[\"Framework_num\"] = list(map(sum_f, df_less[\"Framework\"].values))\n\nq19_simpl = {'10': '10+',\n             '11': '10+',\n             '12': '10+',\n             '13': '10+',\n             '14': '10+',\n             '15': '10+',\n             '16': '10+',\n             '17': '10+'}\n\nq19_simpl_f = lambda x: q19_simpl[x] if x in q19_simpl else x\n\ndf_less[\"Framework_num\"] = list(map(q19_simpl_f, df_less[\"Framework_num\"]))\n\n# Simplifying Q25\n\nq25_simpl = {'I have never studied machine learning and I do not plan to': \"Other\",\n             'I have never studied machine learning but plan to learn in the future': 'I have never studied, plan to learn in the future'}\nq25_simpl_f = lambda x: q25_simpl[x] if x in q25_simpl else x\n\ndf_less[\"YearsUsingML\"] = list(map(q25_simpl_f, df_less[\"Q25\"]))\n\nnew_col = [\"Country\",\n           \"Continent\",\n           \"Age\",\n           \"FormalEducation\",\n           \"YearsUsingML\",\n           \"CurrentYearlyCompensation$USD\",\n           \"CloudComputing_num\",\n           \"Framework_num\"]\n\nnew_col += all_frameworks + all_services\n\ndf = df_less[new_col]\n\ndf.drop(df.index[0], inplace=True)\n\ntop10_countries = get_top_n(df_=df,\n                            column='Country',\n                            column_2='YearsUsingML',\n                            new_name='Number of paticipants')\n\ntop10_continents = get_top_n(df_=df,\n                             column='Continent',\n                             column_2='YearsUsingML',\n                             new_name='Number of paticipants')\n\nselected_continents = list(top10_continents.Continent.values)\nselected_countries = list(top10_countries.Country.values)\ngroup1 = [\"Asia\", \"North America\", \"Europe\"]\ngroup2 = [\"South America\", \"Africa\", \"Oceania\"]\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true,"_uuid":"8ed1aa1e7aef5e1fc37beed5922dbe9c18c81af2"},"cell_type":"code","source":"bar_plot(\"Country\",\n         'Number of paticipants',\n         df_=top10_countries,\n         title=\"Plot 15: Top 10 countries by number of paticipants in the ML & DS Survey\",\n         figsize=(12.6, 9))\n\nbar_plot(\"Continent\",\n         'Number of paticipants',\n         df_=top10_continents,\n         title=\"Plot 16: Paticipants in the ML & DS Survey by continent\",\n         figsize=(12.6, 9))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4739a5be7863844bd4579d1e93e934c91dafe5aa"},"cell_type":"markdown","source":"When compared to **Plots 1** and **2**, **Plots 15** and **16** show a different distribution. We can see that there is a great number of participants from India, making Asia the continent most represented in the survey. Note that *no Central American country was represented here*. And compared to the ranked users data, we have a lesser percentage of users that didn't disclose their location:  **6%** of the participants (1430).\n\n### Analyzing the responses as distributions\n\nThe participants were asked a series of multiple choice questions (we didn't analyze the open-ended questions on the survey). Here we will display some of the answers by continent and, to aid visualization, we will break plots displaying continents for **Group 1** (Asia, North America and Europe) and for **Group 2** (South America, Africa and Oceania).\n\nWhen we aggregate the multiple answers by continent, we construct a *distribution* over these answers.  One way to compare the difference between continents is by comparing the *divergence* between their respective distributions. To do so we will make use of the  [Kullback–Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence):\n\n<img src=\"https://github.com/felipessalvatore/kaggle4life/raw/master/img/KL.png\" width=\"450px\"/>\n\n\nThe value of $D_{KL}(p \\,||\\, q)$ is $0$ when $p$ is the same distribution as $q$, and $D_{KL}(p \\,||\\, q) > 0$ when they are different. The value of $D_{KL}(p \\,||\\, q)$  grows as the divergence between $p$ and $q$ grows. We can define a *distance metric* on top of $D_{KL}(p \\,||\\, q)$ as follows:\n\n$$ distance_{KL}(p,q) =  D_{KL}(p \\,||\\, q) +  D_{KL}(q \\,||\\, p) $$\n\nHence, for each question, we can construct a distance matrix to observe how each continent differs from each other.\n"},{"metadata":{"trusted":true,"_uuid":"c907ab66d76ee78367ca076e4d54e0ec0f3e501b","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# The functions to calculate the KL distance matrix\n\ndef smooth(array_, epsilon=0.003):\n    \"\"\"\n    function smooth a distribution.\n    \n    :param array_: input array\n    :type array_: np.array\n    :param epsilon: smoothing value\n    :type epsilon: float\n    \n    :return: output array\n    :rtype: np.array\n    \"\"\"\n    id_zeros = []\n    for i,v in enumerate(array_):\n        if v == 0:\n            id_zeros.append(i)\n    p_max = np.argmax(array_)\n    minus_value = (len(id_zeros) * - epsilon)\n    array_[p_max] += minus_value\n    for i in id_zeros:\n        array_[i] += epsilon\n    return array_\n\ndef KLdistance(p,q):\n    \"\"\"\n    Computes KL distance \n        \n    :param p: left distribution\n    :type p: np.array\n    :param q: right distribution\n    :type q: np.array\n\n    :return: distance\n    :rtype: float\n    \"\"\"\n    if np.min(p)==0:\n        p = smooth(p)\n    if np.min(q)==0:\n        q = smooth(q)\n    return scipy.stats.entropy(p, q) + scipy.stats.entropy(q, p) \n\ndef compare_distr(df_):\n    \"\"\"\n    Function to compare the distribution for each place\n    (the first column of the DataFrame) in df_\n    \n    :param df_: data frame\n    :type df_: pd.DataFrame\n\n    :return: first column values, distance matrix\n    :rtype: [str], np.array\n    \"\"\"\n    distr_ = []\n    names_ = []\n    n_lines = df_.shape[0] \n    for i in range(n_lines):\n        name = list(df_.iloc[i])[0]\n        names_.append(name)\n        array_ = list(df_.iloc[i])[1:]\n        array_ = np.array(array_) * (1 / np.sum(array_))\n        distr_.append(array_)\n    n = len(distr_)\n    all_distances = []\n    for i in range(n):\n        distance = []\n        for j in range(n):\n            distance.append(KLdistance(distr_[i],distr_[j]))\n        all_distances.append(distance)\n    all_distances = np.array(all_distances)    \n    return names_, all_distances","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f18e221a2d8d759d8cc4b5a6ff464c035c23c7d"},"cell_type":"markdown","source":"### Q: “What is your age (# years)?”"},{"metadata":{"trusted":true,"_uuid":"df9105ffec426d3541b610d8fa6f4114a2d012dc","_kg_hide-input":true},"cell_type":"code","source":"age_value =['18-21',\n            '22-29',\n            '30-39',\n            '40-49',\n            '50-59',\n            '60+']\n\ndf_age = plot_stacked_bar(df_=df,\n                          place_list=selected_continents,\n                          base_column=\"Continent\",\n                          target_column=\"Age\",\n                          palette=\"Set2\",\n                          value_list=age_value,\n                          title=\"Plot 17: Age (all continents)\",\n                          figsize=(12, 6),\n                          ylabel='Number of responses')\n\n_ = plot_stacked_bar(df_=df,\n                     place_list=group1,\n                     base_column=\"Continent\",\n                     target_column=\"Age\",\n                     palette=\"Set2\",\n                     value_list=age_value,\n                     title=\"Plot 18: Age (Group 1)\",\n                     figsize=(12, 6),\n                     ylabel='Number of responses')\n\n_ = plot_stacked_bar(df_=df,\n                     place_list=group2,\n                     base_column=\"Continent\",\n                     target_column=\"Age\",\n                     palette=\"Set2\",\n                     value_list=age_value,\n                     title=\"Plot 19: Age (Group 2)\",\n                     figsize=(12, 6),\n                     ylabel='Number of responses')\n\nnames_age, distances_age = compare_distr(df_age)\n\nplot_distances(names_=names_age,\n               distances_=distances_age,\n               title=\"Plot 20: Distances (Age)\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e57fe5cf05c8e33cfb622461d7e9ac82e7426b13"},"cell_type":"markdown","source":"As can be seen in **Plots 17–19** the vast majority of young people in this survey are from Asia (mostly from India). To put things in perspective, the number of people from India who claim to be 18 to 21 years old (1225) is bigger than the number of all participants from South America (1140).\n\nOther fact to be noted is how few people who reported to be 40 years old or older are from Africa (only **1.8%** of the 40+ group are from this continent). When we observe **Plot 20** we can see that the great disparity in age occurs between Oceania and Asia. The former has relatively more older kaggle users than the latter.\n\n### Q: “What is the highest level of formal education that you have attained or plan to attain within the next 2 years?”"},{"metadata":{"trusted":true,"_uuid":"dce46af5b23e5eed937646214a0f380a000f5902","_kg_hide-input":true},"cell_type":"code","source":"formal_education_values = ['Bachelor’s degree',\n                           'Master’s degree',\n                           'Doctoral degree',\n                           'Some college/university study without earning a bachelor’s degree',\n                           'Professional degree',\n                           'No formal education past high school']\n\n\ndf_formal_education = plot_stacked_bar(df_=df,\n                                       place_list=selected_continents,\n                                       base_column=\"Continent\",\n                                       target_column=\"FormalEducation\",\n                                       palette=\"tab20c\",\n                                       value_list=formal_education_values,\n                                       title=\"Plot 21: Formal education (all continents)\",\n                                       figsize=(12, 6),\n                                       ylabel='Number of responses')\n\n_ = plot_stacked_bar(df_=df,\n                     place_list=group1,\n                     base_column=\"Continent\",\n                     target_column=\"FormalEducation\",\n                     palette=\"tab20c\",\n                     value_list=formal_education_values,\n                     title=\"Plot 22: Formal education (Group 1)\",\n                     figsize=(12, 6),\n                     ylabel='Number of responses')\n\n_ = plot_stacked_bar(df_=df,\n                     place_list=group2,\n                     base_column=\"Continent\",\n                     target_column=\"FormalEducation\",\n                     palette=\"tab20c\",\n                     value_list=formal_education_values,\n                     title=\"Plot 23: Formal education (Group 2)\",\n                     figsize=(12, 6),\n                     ylabel='Number of responses')\n\nnames_formal_education, distances_formal_education = compare_distr(df_formal_education)\n\nplot_distances(names_=names_formal_education,\n               distances_=distances_formal_education,\n               title=\"Plot 24: Distances\\n(Formal education)\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"735dd1ebd564a2576a9d33474343b56dfc75f543"},"cell_type":"markdown","source":"The most striking fact in **Plots 21–23** is the number of Kaggle users from Europe that have, or plan to have, some postgraduate degree. Europe concentrates the majority of users with doctoral degree (1083), and **77.5%** of the European users have or plan to have a master or a doctoral degree.  As **Plot 24** shows Europe diverges a lot from almost all continents (North America is the only exception).\n\n### Q: “For how many years have you used machine learning methods (at work or in school)?”"},{"metadata":{"trusted":true,"_uuid":"22a455250b59b757ddadafe995ebc68b1d98096d","_kg_hide-input":true},"cell_type":"code","source":"yearsML_values = ['< 1 year',\n                  '1-2 years',\n                  '2-3 years',\n                  '3-4 years',\n                  '4-5 years',\n                  '5-10 years',\n                  '10-15 years',\n                  '20+ years',\n                  'I have never studied, plan to learn in the future']\n\ndf_yearsML = plot_stacked_bar(df_=df,\n                              place_list=selected_continents,\n                              base_column=\"Continent\",\n                              target_column=\"YearsUsingML\",\n                              palette=\"tab20\",\n                              value_list=yearsML_values,\n                              title=\"Plot 25: Years using machine learning (all continents)\",\n                              figsize=(12, 6),\n                              ylabel='Number of responses')\n\n_ = plot_stacked_bar(df_=df,\n                     place_list=group1,\n                     base_column=\"Continent\",\n                     target_column=\"YearsUsingML\",\n                     palette=\"tab20\",\n                     value_list=yearsML_values,\n                     title=\"Plot 26: Years using machine learning (Group 1)\",\n                     figsize=(12, 7),\n                     ylabel='Number of responses')\n\n_ = plot_stacked_bar(df_=df,\n                     place_list=group2,\n                     base_column=\"Continent\",\n                     target_column=\"YearsUsingML\",\n                     palette=\"tab20\",\n                     value_list=yearsML_values,\n                     title=\"Plot 27: Years using machine learning (Group 2)\",\n                     figsize=(12, 7),\n                     ylabel='Number of responses')\n\nnames_yearsML, distances_yearsML = compare_distr(df_yearsML)\n\nplot_distances(names_=names_yearsML,\n               distances_=distances_yearsML,\n               title=\"Plot 28: Distances\\n(Years in ML)\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"538cd0d8feee72c55b84735ef820262281172122"},"cell_type":"markdown","source":"**Plots 25–27** show a scenario similar to the one presented in **Plots 17–19**: Asia concentrates a big number of novice ML users, mainly due to India; North America and Europe have the most experience users (of all people with 5+ years of experience in ML **39,7%** are in North America and **36.6%** are in Europe); and Africa have only **1.2%** of ML veterans. The divergence between Europe and Africa that we first spotted in **Plot 24** appear again in **Plot 28**. This can be explained by the positive correlation between formal education and years using machine learning.\n\n### Q: “What is your current yearly compensation (approximate $USD)?”"},{"metadata":{"trusted":true,"_uuid":"964bf6ed2cca7e99f5b82f5daa5af72e98c27030","_kg_hide-input":true},"cell_type":"code","source":"comp_values = ['0-10,000',\n               '10-20,000',\n               '20-30,000',\n               '30-40,000',\n               '40-50,000',\n               '50-60,000',\n               '60-70,000',\n               '70-80,000',\n               '80-90,000',\n               '90-100,000',\n               '100-125,000',\n               '125-150,000',\n               '150-200,000',\n               '200,000+']\n\ndf_compensation = plot_stacked_bar(df_=df,\n                                   place_list=selected_continents,\n                                   base_column=\"Continent\",\n                                   target_column=\"CurrentYearlyCompensation$USD\",\n                                   palette=\"tab20b\",\n                                   value_list=comp_values,\n                                   title=\"Plot 29: Yearly compensation in $USD (all continents)\",\n                                   figsize=(12, 6),\n                                   ylabel='Number of responses')\n\n_ = plot_stacked_bar(df_=df,\n                     place_list=group1,\n                     base_column=\"Continent\",\n                     target_column=\"CurrentYearlyCompensation$USD\",\n                     palette=\"tab20b\",\n                     value_list=comp_values,\n                     title=\"Plot 30: Yearly compensation in $USD (Group 1)\",\n                     figsize=(14, 12),\n                     ylabel='Number of responses')\n\n_ = plot_stacked_bar(df_=df,\n                     place_list=group2,\n                     base_column=\"Continent\",\n                     target_column=\"CurrentYearlyCompensation$USD\",\n                     palette=\"tab20b\",\n                     value_list=comp_values,\n                     title=\"Plot 31: Yearly compensation in $USD (Group 2)\",\n                     figsize=(12, 8),\n                     ylabel='Number of responses')\n\n_ = plot_stacked_bar(df_=df,\n                     place_list=selected_countries,\n                     base_column=\"Country\",\n                     target_column=\"CurrentYearlyCompensation$USD\",\n                     palette=\"tab20b\",\n                     value_list=comp_values,\n                     title=\"Plot 32: Yearly compensation in $USD (selected countries)\",\n                     figsize=(12, 8),\n                     ylabel='Number of responses')\n\nnames_compensation, distances_compensation = compare_distr(df_compensation)\n\nplot_distances(names_=names_compensation,\n               distances_=distances_compensation,\n               title=\"Plot 33: Distances\\n(Yearly compensation)\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3b53c61bbeb75802c45f1f038b884a79cc7aacf"},"cell_type":"markdown","source":"The disparity between continents is crystal clear when we look at difference in compensation (**Plots 29-33**). **Plot 33** presents the highest values for KL distance in this whole kernel! North America clearly diverges from the rest of the world. In terms of KL distance from this specific answer, we can also see two distant groups: on one side, North America, Europe and Oceania; on the other South America, Asia and Africa. Other way to see this is by looking at the numbers. On one extreme, you have high-paying jobs concentrated in North America, **68.8%** of people who earn 125,000+ are from United States. On the other, South America, Asia and Africa together concentrate **75.9%** of all users who reported the lowest yearly compensation.\n\n### Q: Which of the following cloud computing services have you used at work or school in the last 5 years?"},{"metadata":{"trusted":true,"_uuid":"41cef3aad004ac11c6b596d6abe5048a2d281412","_kg_hide-input":true},"cell_type":"code","source":"cloud_values = ['0',\n                '1',\n                '2',\n                '3',\n                '4',\n                '5',\n                \"6\"]\n\n\nAzure = df.groupby(\"Continent\")[\"Azure\"].sum().to_frame().reset_index()\nIBM = df.groupby(\"Continent\")[\"IBM\"].sum().to_frame().reset_index()\nAWS = df.groupby(\"Continent\")[\"AWS\"].sum().to_frame().reset_index()\nGCP = df.groupby(\"Continent\")[\"GCP\"].sum().to_frame().reset_index()\nAlibaba = df.groupby(\"Continent\")[\"Alibaba\"].sum().to_frame().reset_index()\n\n\ndf_cloud = pd.merge(Azure, IBM, on=['Continent'])\ndf_cloud = pd.merge(df_cloud, AWS, on=['Continent'])\ndf_cloud = pd.merge(df_cloud, GCP, on=['Continent'])\ndf_cloud = pd.merge(df_cloud, Alibaba, on=['Continent'])\n\ndf_cloud[\"Total\"] = df_cloud[\"Azure\"] + df_cloud[\"IBM\"] + df_cloud[\"AWS\"] + df_cloud[\"GCP\"] + df_cloud[\"Alibaba\"]\ndf_cloud.sort_values(by=\"Total\", inplace=True, ascending=False)\ndf_cloud.drop(\"Total\", axis=1, inplace=True)\ndf_cloud.drop(axis=0, index=3, inplace=True)\ndf_cloud.reset_index(drop=True, inplace=True)\n\n\ndf_cloud_n = plot_stacked_bar(df_=df,\n                              place_list=selected_continents,\n                              base_column=\"Continent\",\n                              target_column=\"CloudComputing_num\",\n                              palette=\"tab20\",\n                              value_list=cloud_values,\n                              title=\"Plot 34: Number of used cloud computing plataforms (all continents)\",\n                              figsize=(12, 6),\n                              ylabel='Number of responses')\n\n\nnames_cloud_n, distances_cloud_n = compare_distr(df_cloud_n)\n\nplot_distances(names_=names_cloud_n,\n               distances_=distances_cloud_n,\n               title=\"Plot 35: Distances\\n(Number of cloud services)\")\n\n\nplot_stacked_bar_simpl(df_=df_cloud,\n                       base_column=\"Continent\",\n                       values_number=5,\n                       palette=\"tab20c\",\n                       title=\"Plot 36: Cloud computing plataforms by continent (all continents)\",\n                       figsize=(12, 6),\n                       ylabel='number of responses')\n\nnames_cloud, distances_cloud = compare_distr(df_cloud)\n\nplot_distances(names_=names_cloud,\n               distances_=distances_cloud,\n               title=\"Plot 37: Distances\\n(Type of cloud services)\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19ddd2e57af1fca4a0dafe495a12c5fe12ef6be2"},"cell_type":"markdown","source":"The access to technology shapes the potential of each ML/DS community. Fortunately, regarding the number of cloud computing services we see the same pattern around the world (**Plots 34** and **35**). The only notable difference that we can observe is that there is a small variance in the type of service used in Asia, the vast majority of users that report using Alibaba cloud service is from this continent (**Plots 36** and **37**).\n\n### Q: What machine learning frameworks have you used in the past 5 years? \n\n"},{"metadata":{"trusted":true,"_uuid":"7d7cac409e8da137f80c8850d3a459cc050a83e0","_kg_hide-input":true},"cell_type":"code","source":"framework_values = ['0',\n                    '1',\n                    '2',\n                    '3',\n                    '4',\n                    '5',\n                    '6',\n                    '7',\n                    '8',\n                    '9',\n                    '10+']\n\n\nScikitLearn = df.groupby(\"Continent\")[\"Scikit-Learn\"].sum().to_frame().reset_index()\nTensorFlow = df.groupby(\"Continent\")[\"TensorFlow\"].sum().to_frame().reset_index()\nKeras = df.groupby(\"Continent\")[\"Keras\"].sum().to_frame().reset_index()\nPyTorch = df.groupby(\"Continent\")[\"PyTorch\"].sum().to_frame().reset_index()\nSparkMLlib = df.groupby(\"Continent\")[\"Spark MLlib\"].sum().to_frame().reset_index()\n\nH20 = df.groupby(\"Continent\")[\"H20\"].sum().to_frame().reset_index()\nFastai = df.groupby(\"Continent\")[\"Fastai\"].sum().to_frame().reset_index()\nMxnet = df.groupby(\"Continent\")[\"Mxnet\"].sum().to_frame().reset_index()\nCaret = df.groupby(\"Continent\")[\"Caret\"].sum().to_frame().reset_index()\n\nXgboost = df.groupby(\"Continent\")[\"Xgboost\"].sum().to_frame().reset_index()\nmlr = df.groupby(\"Continent\")[\"mlr\"].sum().to_frame().reset_index()\nProphet = df.groupby(\"Continent\")[\"Prophet\"].sum().to_frame().reset_index()\nrandomForest = df.groupby(\"Continent\")[\"randomForest\"].sum().to_frame().reset_index()\n\nlightgbm = df.groupby(\"Continent\")[\"lightgbm\"].sum().to_frame().reset_index()\ncatboost = df.groupby(\"Continent\")[\"catboost\"].sum().to_frame().reset_index()\nCNTK = df.groupby(\"Continent\")[\"CNTK\"].sum().to_frame().reset_index()\nCaffe = df.groupby(\"Continent\")[\"Caffe\"].sum().to_frame().reset_index()\n\ndf_frame = pd.merge(ScikitLearn, TensorFlow, on=['Continent'])\ndf_frame = pd.merge(df_frame, Keras, on=['Continent'])\ndf_frame = pd.merge(df_frame, PyTorch, on=['Continent'])\ndf_frame = pd.merge(df_frame, SparkMLlib, on=['Continent'])\ndf_frame = pd.merge(df_frame, H20, on=['Continent'])\ndf_frame = pd.merge(df_frame, Fastai, on=['Continent'])\n\ndf_frame = pd.merge(df_frame, Mxnet, on=['Continent'])\ndf_frame = pd.merge(df_frame, Caret, on=['Continent'])\ndf_frame = pd.merge(df_frame, Xgboost, on=['Continent'])\ndf_frame = pd.merge(df_frame, mlr, on=['Continent'])\ndf_frame = pd.merge(df_frame, Prophet, on=['Continent'])\ndf_frame = pd.merge(df_frame, randomForest, on=['Continent'])\ndf_frame = pd.merge(df_frame, lightgbm, on=['Continent'])\ndf_frame = pd.merge(df_frame, catboost, on=['Continent'])\ndf_frame = pd.merge(df_frame, CNTK, on=['Continent'])\ndf_frame = pd.merge(df_frame, Caffe, on=['Continent'])\n\n\ndf_frame[\"Total\"] = df_frame[\"Scikit-Learn\"] + df_frame[\"TensorFlow\"] + df_frame[\"Keras\"] + df_frame[\"Spark MLlib\"] + df_frame[\"PyTorch\"] + df_frame[\"H20\"] + df_frame[\"Fastai\"] + df_frame[\"Mxnet\"] + \\\n    df_frame['Caret'] + df_frame[\"Xgboost\"] + df_frame[\"mlr\"] + df_frame[\"Prophet\"] + df_frame[\"randomForest\"] + \\\n    df_frame[\"lightgbm\"] + df_frame[\"catboost\"] + \\\n    df_frame[\"CNTK\"] + df_frame[\"Caffe\"]\n\n\ndf_frame.sort_values(by=\"Total\", inplace=True, ascending=False)\ndf_frame.drop(\"Total\", axis=1, inplace=True)\ndf_frame.drop(axis=0, index=3, inplace=True)\ndf_frame.reset_index(drop=True, inplace=True)\n\ndf_framework = plot_stacked_bar(df_=df,\n                                place_list=selected_continents,\n                                base_column=\"Continent\",\n                                target_column=\"Framework_num\",\n                                palette=\"tab20b\",\n                                value_list=framework_values,\n                                title=\"Plot 38: Number of frameworks used (all continents)\",\n                                figsize=(12, 6),\n                                ylabel='Number of responses')\n\nnames_framework, distances_framework = compare_distr(df_framework)\n\nplot_distances(names_=names_framework,\n               distances_=distances_framework,\n               title=\"Plot 39: Distances\\n(Number of frameworks)\")\n\nplot_stacked_bar_simpl(df_=df_frame,\n                       base_column=\"Continent\",\n                       values_number=17,\n                       palette=\"tab20b\",\n                       title=\"Plot 40: Frameworks by continent (all continents)\",\n                       figsize=(15, 7),\n                       ylabel='number of responses')\n\nnames_frame, distances_frame = compare_distr(df_frame)\n\nplot_distances(names_=names_frame,\n               distances_=distances_frame,\n               title=\"Plot 41: Distances\\n(Types of frameworks)\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aee134b6b864d7c9f54253e653448a4669c17eea"},"cell_type":"markdown","source":"Similar to the results presented in **Plots 34-37**, the information conveyed in **Plots 38-41** is that there is small divergence over the continents in regard to the use of machine learning frameworks.\n\n## Final thoughts\n\nFrom our analysis two distinct groups of continents have emerged: **Group 1** (Asia, North America and Europe) and **Group 2** (South America, Africa, Oceania and Central America). This distinction is based on *number of users*, *points* and *rank position* (a *performance-based* distinction). But, as the ML & DS Survey shows, *there is no socio-economic unity between the continents of these groups*. If we take our KL distance metric, we can define two other more coherent groups: **Group 3** (North America, Europe and Oceania) and  **Group 4** (Africa, Asia, South and Central Americas). \n\n| Continent group | Total KL distance within the group |\n|-------|-------------------|\n| **1**: Asia, North America, Europe     | 11.47            |\n|**2**:  South America, Africa, Oceania, Central America    | 12.64            |\n|**3**:  North America, Europe, Oceania    | 5.08             |\n| **4**:  Africa, Asia, South and Central Americas   | 3.87             |\n\nTable 1: KL distance table by continent groups\n\nThis shouldn't be seen as a surprise,  **Groups 3** and **4** also emerge when we rank continents by [GDP per capita](https://www.imf.org/external/datamapper/NGDPDPC@WEO/OEMDC/ADVEC/WEOWORLD).  It seems that the ML & DS Survey is a reflection of the world reality, as it should be."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"48a9c5c615d0e59a72f05f170d824632cfcb4877"},"cell_type":"code","source":"# Let's sum the KL divergence for each type of group mentioned above\n\ndef get_distance_groups(input_names, input_matrix):\n    \"\"\"\n    Get the sum of the KL divergence for 4 types of groups:\n\n    Group 1 = [Asia, North America, Europe]\n    Group 2 = [South America, Africa, Oceania]\n    Group 3 = [North America, Europe, Oceania]\n    Group 4 = [Africa, Asia, South America]\n\n    :param input_names: list of continents\n    :type input_names: [str]\n    :param input_matrix: matrix with KL divergences\n    :type input_matrix: np.array\n    :return: matrix with KL divergences for groups 1, 2, 3 and 4\n    :rtype: np.array, np.array, np.array, np.array\n    \"\"\"\n    together = list(zip(input_names, input_matrix))\n    together.sort()\n\n    g1 = [\"Asia\", \"Europe\", 'North America']\n    g1_i_sorted = [1, 2, 3]\n    g2 = [\"Africa\", 'Oceania', 'South America']\n    g2_i_sorted = [0, 4, 5]\n    g3 = [\"Europe\", 'North America', 'Oceania']\n    g3_i_sorted = [2, 3, 4]\n    g4 = [\"Africa\", 'Asia', 'South America']\n    g4_i_sorted = [0, 1, 5]\n\n    g1_i = [i for i, v in enumerate(input_names) if v in g1]\n    g2_i = [i for i, v in enumerate(input_names) if v in g2]\n    g3_i = [i for i, v in enumerate(input_names) if v in g3]\n    g4_i = [i for i, v in enumerate(input_names) if v in g4]\n\n    g1_d = [together[i][1][g1_i] for i in g1_i_sorted]\n    g2_d = [together[i][1][g2_i] for i in g2_i_sorted]\n    g3_d = [together[i][1][g3_i] for i in g3_i_sorted]\n    g4_d = [together[i][1][g4_i] for i in g4_i_sorted]\n\n    return np.array(g1_d), np.array(g2_d), np.array(g3_d), np.array(g4_d)\n\n\nall_distances = [(names_frame, distances_frame),\n                 (names_framework, distances_framework),\n                 (names_cloud, distances_cloud),\n                 (names_cloud_n, distances_cloud_n),\n                 (names_compensation, distances_compensation),\n                 (names_yearsML, distances_yearsML),\n                 (names_formal_education, distances_formal_education),\n                 (names_age, distances_age)]\n\n\ng1_d = np.zeros((3, 3))\ng2_d = np.zeros((3, 3))\ng3_d = np.zeros((3, 3))\ng4_d = np.zeros((3, 3))\n\nfor names, distances in all_distances:\n    g1_d_, g2_d_, g3_d_, g4_d_ = get_distance_groups(names, distances)\n    g1_d += g1_d_\n    g2_d += g2_d_\n    g3_d += g3_d_\n    g4_d += g4_d_\n\n\nprint(\"Group 1 = {:.2f}\".format(np.sum(g1_d)))\nprint(\"Group 2 = {:.2f}\".format(np.sum(g2_d)))\nprint(\"Group 3 = {:.2f}\".format(np.sum(g3_d)))\nprint(\"Group 4 = {:.2f}\".format(np.sum(g4_d)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec0bf43abec51f6f9f1d35449ce26c35d5b3ba3c"},"cell_type":"markdown","source":"Fortunately, we can also derive some *non-trivial conclusions*.\n\nThe first one is that Asia currently plays a central role in Kaggle (**Plots 12** and **14**) and, as the ML & DS Survey shows, *it will continue to be a big player in the future. A large contingent of young data scientist belong to this continent, mostly from India.* Although India was well represented in the survey it shouldn't be overestimated. As **Plot 11** shows, Russia, Japan and China currently perform better in competitions if compared to India.\n\nThe second is that *the use of technology is quite homogeneous along the different ML/DS communities*. Although access to electricity, computers, and internet vary from continent to continent, we didn't find huge gaps regarding the number and types of cloud services and machine learning frameworks (**Plots 34-41**).  It seems that the access of these technologies is quite democratized, at least among the Kaggle users.\n\nGoing back to the blog post that has inspired our analysis (the one from Deep Learning Indaba), we see a brighter picture regarding South America and Africa. These continents are not missing in the Kaggle landscape (the only one that is really missing is Central America). *They are present and they are competitive*, South America is ahead of **Group 3** (**Plots 12** and **14**). But they still have a lot of problems (and since I am from Brazil, these problems cut close to the bone). South America is a big continent with a considerable economy (the GDP per capita of this continent is greater than Asia), yet *the number of ranked users from this continent is the same as the one from the Netherlands*  (64). It is a shockingly small number.  Africa still face a lot of difficulties. When we look at some important socio-economic features like formal education, years working with machine learning and yearly compensation, *the highest KL distance is between one continent in **Group 3** and Africa* (**Plots 24**, **28** and **33**).  \n\nAt the end of the day, the greatest challenge that we face isn't the newest Kaggle competition, it's the improvement and nurturing of our communities.\n"},{"metadata":{"_uuid":"398330a6b9e0c454a635da22eb327e205ffe7405"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}