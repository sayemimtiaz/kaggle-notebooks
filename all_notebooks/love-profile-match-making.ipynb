{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport nltk\nimport spacy\nimport string\npd.options.mode.chained_assignment = None\n#nltk.download('punkt')\n#nltk.download('stopwords')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get Eassay 0 data \nfull_df = pd.read_csv(\"../input/okcupid-profiles/okcupid_profiles.csv\")\nfull_df['essay'] = full_df[full_df.columns[21:]].apply(\n    lambda x: ' '.join(x.astype(str)),\n    axis=1\n)\ndf = full_df[[\"essay\"]]\ndf[\"essay\"] = df[\"essay\"].astype(str)\ndf.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom collections import Counter\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nimport numpy as np\nimport pandas as pd\nimport re\nimport nltk\nimport spacy\nimport string\npd.options.mode.chained_assignment = None\n\n\nclass TextPreprocessor:\n    APOSTROPHE = '\\u2019'\n    EMOTICONS_REGEX = r'[\\U0001f600-\\U0001f64f]+'\n    DINGBATS_REGEX = r'[\\U00002702-\\U000027b0]+'\n    TRANSPORT_AND_MAP_REGEX = r'[\\U0001f680-\\U0001f6c0]+'\n    ENCLOSED_CHARS_REGEX = r'[\\U000024c2-\\U0001f251]+'\n    MISC_REGEX = r'[\\U000000a9-\\U0001f999]'\n\n    def make_lowercase(self, data_frame, column_name):\n        data_frame[column_name] = data_frame[column_name].str.lower()\n        #data_frame[column_name] = data_frame[column_name].apply(lambda texts: \n        print('make_lowercase applied')\n        #print(data_frame[column_name])\n        return data_frame\n\n    def remove_punctuation(self, data_frame, column_name):\n        PUNCT_TO_REMOVE = string.punctuation\n        data_frame[column_name] = data_frame[column_name].apply(\n            lambda text: text.translate(str.maketrans('', '', PUNCT_TO_REMOVE)))\n        print('remove_punctuation applied')\n        #print(data_frame[column_name])\n        return data_frame\n\n    def remove_stop_words(self, data_frame, column_name):\n        STOPWORDS = set(stopwords.words('english'))\n        data_frame[column_name] = data_frame[column_name].apply(\n            lambda text: \" \".join([word for word in str(text).split() if word not in STOPWORDS]))\n        print('remove_stop_words applied')\n        #print(data_frame[column_name])\n        return data_frame\n\n    def remove_frequent_words(self, data_frame, column_name):\n        cnt = Counter()\n        for text in data_frame[column_name].values:\n            for word in text.split():\n                cnt[word] += 1\n        FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\n        data_frame[column_name] = data_frame[column_name].apply(\n            lambda text: \" \".join([word for word in str(text).split() if word not in FREQWORDS]))\n        print('remove_frequent_words applied')\n        #print(data_frame[column_name])\n        return data_frame\n\n    def remove_rare_words(self, data_frame, column_name, max_rare_words_count=10):\n        cnt = Counter()\n        for text in data_frame[column_name].values:\n            for word in text.split():\n                cnt[word] += 1\n        RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-max_rare_words_count - 1:-1]])\n        data_frame[column_name] = data_frame[column_name].apply(\n            lambda text: \" \".join([word for word in str(text).split() if word not in RAREWORDS]))\n        print('remove_rare_words applied')\n        #data_frame.head(5)\n        return data_frame\n\n    def stem_words(self, data_frame, column_name):\n        stemmer = PorterStemmer()\n        data_frame[column_name] = data_frame[column_name].apply(\n            lambda text: \" \".join([stemmer.stem(word) for word in text.split()]))\n        print('stem_words applied')\n        #print(data_frame[column_name])\n        return data_frame\n\n    def lemmatize_words(self, data_frame, column_name):\n        lemmatizer = WordNetLemmatizer()\n        data_frame[column_name] = data_frame[column_name].apply(\n            lambda text: \" \".join([lemmatizer.lemmatize(word) for word in text.split()]))\n        print('lemmatize_words applied')\n        # print(data_frame[column_name])\n        return data_frame\n    \n    def remove_numbers(self, data_frame, column_name):\n        number_pattern = r'\\d+'\n        data_frame[column_name] = data_frame[column_name].apply(\n            lambda text: re.sub(pattern=number_pattern, repl=\" \", string=text))\n        print('remove_numbers applied')\n        return data_frame\n\n\n    def lemmatize_words_v2(self, data_frame, column_name):\n        lemmatizer = WordNetLemmatizer()\n        wordnet_map = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"J\": wordnet.ADJ, \"R\": wordnet.ADV}\n        # pos_tagged_text = nltk.pos_tag(text.split())\n        data_frame[column_name] = data_frame[column_name].apply(lambda text: \" \".join(\n            [lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in\n             nltk.pos_tag(text.split())]))\n        print('lemmatize_words_v2 applied')\n        #print(data_frame[column_name])\n        return data_frame\n\n    def tokenize(self, data_frame, column_name):\n        data_frame[column_name] = data_frame[column_name].apply(lambda text: nltk.tokenize.word_tokenize(text))\n        print('tokenize applied')\n        #print(data_frame[column_name])\n        return data_frame\n\n\n    def clean_text(self, data_frame, column_name):     \n        data_frame_local = self.make_lowercase(data_frame, column_name)\n        data_frame_local = self.remove_punctuation(data_frame_local, column_name)\n        data_frame_local = self.remove_numbers(data_frame_local, column_name)\n        data_frame_local = self.remove_stop_words(data_frame_local, column_name)\n        data_frame_local = self.remove_rare_words(data_frame_local, column_name)\n        data_frame_local = self.remove_frequent_words(data_frame_local, column_name)\n        data_frame_local = self.lemmatize_words_v2(data_frame_local, column_name)\n        #data_frame_local = self.tokenize(data_frame_local, column_name)\n        return data_frame_local\n\n\n    def remove_emojis(data):\n        result = []\n        for word in data:\n            match = []\n            match += re.findall(EMOTICONS_REGEX, word)\n            match += re.findall(ENCLOSED_CHARS_REGEX, word)\n            match += re.findall(DINGBATS_REGEX, word)\n            match += re.findall(TRANSPORT_AND_MAP_REGEX, word)\n            match += re.findall(MISC_REGEX, word)\n            if not match == []:\n                for item in match:\n                    word = word.replace(item, '')\n            result.append(word)\n        return result\n\n\n    def remove_empty_strings(data):\n        return [word for word in data if word != '']\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = df.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_processor = TextPreprocessor()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#raw_data = df_test\ndf_test = df_test.sample(frac=0.01,random_state=10)\n#raw_data = raw_data.drop(['age', 'status', 'sex', 'orientation', 'body_type', 'diet', 'drinks', 'drugs', 'education', 'ethnicity', 'height', 'income', 'job', 'last_online', 'location', 'offspring', 'pets', 'religion', 'sign', 'smokes', 'speaks', 'essay0', 'essay1', 'essay2', 'essay3', 'essay4', 'essay5', 'essay6', 'essay7', 'essay8', 'essay9'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_processor = TextPreprocessor()\ndf_clean = text_processor.clean_text(df_test, 'essay')\ndf_clean.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Test code","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom IPython.display import display\nfrom tqdm import tqdm\nfrom collections import Counter\nimport ast\n\nimport matplotlib.pyplot as plt\nimport matplotlib.mlab as mlab\nimport seaborn as sb\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom textblob import TextBlob\nimport scipy.stats as stats\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.manifold import TSNE\n\nfrom bokeh.plotting import figure, output_file, show\nfrom bokeh.models import Label\nfrom bokeh.io import output_notebook\noutput_notebook()\n\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define helper functions\ndef get_top_n_words(n_top_words, count_vectorizer, text_data):\n    '''\n    returns a tuple of the top n words in a sample and their \n    accompanying counts, given a CountVectorizer object and text sample\n    '''\n    vectorized_headlines = count_vectorizer.fit_transform(text_data.values)\n    vectorized_total = np.sum(vectorized_headlines, axis=0)\n    word_indices = np.flip(np.argsort(vectorized_total)[0,:], 1)\n    word_values = np.flip(np.sort(vectorized_total)[0,:],1)\n    \n    word_vectors = np.zeros((n_top_words, vectorized_headlines.shape[1]))\n    for i in range(n_top_words):\n        word_vectors[i,word_indices[0,i]] = 1\n\n    words = [word[0].encode('ascii').decode('utf-8') for \n             word in count_vectorizer.inverse_transform(word_vectors)]\n\n    return (words, word_values[0,:n_top_words].tolist()[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_vectorizer = CountVectorizer(stop_words='english')\nwords, word_values = get_top_n_words(n_top_words=15,\n                                     count_vectorizer=count_vectorizer, \n                                     text_data=df_clean['essay'])\n\nfig, ax = plt.subplots(figsize=(16,8))\nax.bar(range(len(words)), word_values);\nax.set_xticks(range(len(words)));\nax.set_xticklabels(words, rotation='vertical');\nax.set_title('Top words in headlines dataset (excluding stop words)');\nax.set_xlabel('Word');\nax.set_ylabel('Number of occurences');\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nfrom yellowbrick.text import FreqDistVisualizer\n#from yellowbrick.datasets import load_hobbies\n\n# Load the text data\n#corpus = load_hobbies()\n\nvectorizer = CountVectorizer()\ndocs       = vectorizer.fit_transform(df_clean['essay'])\nfeatures   = vectorizer.get_feature_names()\n\nvisualizer = FreqDistVisualizer(features=features, orient='v')\nvisualizer.fit(docs)\nvisualizer.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(stop_words='english', \nmax_features= 1000, # keep top 1000 terms \nmax_df = 0.5, \nsmooth_idf=True)\n\n#Replace NaN with an empty string\n#df_clean['essay'] = df_clean['essay'].fillna('')\ndf_clean = df_clean.dropna()\n\nX = vectorizer.fit_transform(df_clean['essay'])\n\nX.shape # check shape of the document-term matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.getrow(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\nnum_clusters = 10\nnum_seeds = 10\nmax_iterations = 300\nlabels_color_map = {\n    0: '#20b2aa', 1: '#ff7373', 2: '#ffe4e1', 3: '#005073', 4: '#4d0404',\n    5: '#ccc0ba', 6: '#4700f9', 7: '#f6f900', 8: '#00f91d', 9: '#da8c49'\n}\npca_num_components = 2\ntsne_num_components = 2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# texts_list = some array of strings for which TF-IDF is being computed\n\n# calculate tf-idf of texts\ntf_idf_vectorizer = TfidfVectorizer(analyzer=\"word\", use_idf=True, smooth_idf=True, ngram_range=(1, 2))\ntf_idf_matrix = tf_idf_vectorizer.fit_transform(df_clean['essay'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dense = tf_idf_matrix.todense()\ndenselist = dense.tolist()\ndf = pd.DataFrame(\n    denselist,columns=tf_idf_vectorizer.get_feature_names())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# create k-means model with custom config\nclustering_model = KMeans(\n    n_clusters=num_clusters,\n    max_iter=max_iterations,\n    precompute_distances=\"auto\",\n    n_jobs=-1\n)\n\nlabels = clustering_model.fit_predict(tf_idf_matrix)\n# print labels\nX = tf_idf_matrix.todense()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ----------------------------------------------------------------------------------------------------------------------\n\nreduced_data = PCA(n_components=pca_num_components).fit_transform(X)\n# print reduced_data\n\nfig, ax = plt.subplots()\nfor index, instance in enumerate(reduced_data):\n    # print instance, index, labels[index]\n    pca_comp_1, pca_comp_2 = reduced_data[index]\n    color = labels_color_map[labels[index]]\n    ax.scatter(pca_comp_1, pca_comp_2, c=color)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# t-SNE plot\nembeddings = TSNE(n_components=tsne_num_components)\nY = embeddings.fit_transform(X)\nplt.scatter(Y[:, 0], Y[:, 1], cmap=plt.cm.Spectral)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import linear_kernel\nfrom sklearn.metrics.pairwise import linear_kernel\n\n# Compute the cosine similarity matrix\ncosine_sim = linear_kernel(X, X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\n\n# SVD represent documents and terms in vectors \nsvd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\n\nsvd_model.fit(X)\n\nlen(svd_model.components_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"terms = vectorizer.get_feature_names()\n\nfor i, comp in enumerate(svd_model.components_):\n    terms_comp = zip(terms, comp)\n    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n    print(\"Topic \"+str(i)+\": \")\n    topic_words = []\n    for t in sorted_terms:\n        topic_words.append(t[0])\n    print(*topic_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import umap\n\n# X_topics = svd_model.fit_transform(X)\n# embedding = umap.UMAP(n_neighbors=150, min_dist=0.5, random_state=12).fit_transform(X_topics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_clean.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# import nltk\n# nltk.download('stopwords')\n# from nltk.corpus import stopwords\n# from sklearn.metrics.pairwise import linear_kernel\n# from sklearn.feature_extraction.text import CountVectorizer\n# from sklearn.feature_extraction.text import TfidfVectorizer\n# from nltk.tokenize import RegexpTokenizer\n# import re\n# import string\n# import random\n# from PIL import Image\n# import requests\n# from io import BytesIO\n# import matplotlib.pyplot as plt\n# %matplotlib inline\n# from sklearn.metrics.pairwise import cosine_similarity\n# from gensim.models import Word2Vec\n# from gensim.models.phrases import Phrases, Phraser\n# from matplotlib import pyplot\n# from gensim.models import KeyedVectors\n\n# plt.figure(figsize=(7,5))\n# plt.scatter(embedding[:, 0], embedding[:, 1], \n# c = df_clean.essay,\n# s = 10, # size\n# edgecolor='none'\n# )\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.datasets import fetch_20newsgroups\n\n# dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(dataset.target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}