{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Covid19 Text-Classification - NLP Bert - TensorFlow**\n\nreferences:\n- Huggingface Bert model: https://huggingface.co/bert-base-uncased\n- https://www.tensorflow.org/tutorials/text/classify_text_with_bert\n- https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# pip install tensorflow-text (preprocessing for BERT inputs)\n# pip install -q tf-models-official\n# pip install --upgrade tensorflow_hub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"pip install tensorflow-text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"pip install -q tf-models-official","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# -- Import Libraries -- \nimport os\nimport numpy as np\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import model_selection\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix\n\nfrom official.nlp import optimization\nfrom nltk.corpus import stopwords\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nfrom tensorflow.keras import layers, losses, preprocessing\n\ntf.get_logger().setLevel('ERROR')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# -- Global Variables -- \nTRAIN_PATH = '../input/covid-19-nlp-text-classification/Corona_NLP_train.csv'  \nTEST_PATH = '../input/covid-19-nlp-text-classification/Corona_NLP_test.csv' \nCLASSES = ['Extremely Negative', 'Negative', 'Positive', 'Extremely Positive', 'Neutral']\nBATCH_SIZE = 128\nEPOCHS = 16\nLEARNING_RATE = 1e-05 #small gradient steps to prevent forgetting in transfer learning.\n\n# MODEL_NAME = 'bert-base-uncased'\ntfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\ntfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# -- Load Data -- \ntrain_data = pd.read_csv(TRAIN_PATH, encoding='L1')\ntest_data = pd.read_csv(TEST_PATH,encoding='L1')\n\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# -- Data distribution -- \nfig = plt.figure(figsize=(16, 4))\n\nx=train_data.Sentiment.value_counts()\n\nplt.bar(x=x.index,\n        height=x.values,\n        width=0.5)\n\nplt.title('Data Distribution')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# -- Pre-processing -- \n\n# drop empty tweets, and unclassified tweets.\ntrain_data.OriginalTweet.dropna()\ntrain_data.Sentiment.dropna()\ntest_data.OriginalTweet.dropna()\ntest_data.Sentiment.dropna()\n\n# remove stop-words\nsw_nltk = stopwords.words('english')\nfunc = lambda text : \" \".join([word for word in str(text).split() if word.lower() not in sw_nltk])\ntrain_data['OriginalTweet'] = train_data['OriginalTweet'].apply(func)\n\n# TODO: remove Urls and HTML links","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# -- Split Data to train, validation and test -- \ntrain_X, val_X, train_y, val_y = model_selection.train_test_split(train_data['OriginalTweet'],\n                                                                  train_data['Sentiment'], \n                                                                  test_size=0.3)\n\ntest_X, test_y = test_data['OriginalTweet'],test_data['Sentiment']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# -- convert labels to one hot --\nlabel_encoder = LabelEncoder()\n\nvec = label_encoder.fit_transform(train_y)\ntrain_y = tf.keras.utils.to_categorical(vec)\n\nvec = label_encoder.fit_transform(val_y)\nval_y = tf.keras.utils.to_categorical(vec)\n\nvec = label_encoder.fit_transform(test_y)\ntest_y = tf.keras.utils.to_categorical(vec)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# -- Creating the Model for Fine Tuning -- \ndef bert_text_classification():\n\n    # - text input -\n    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n        \n    # - preprocessing layer - \n    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n        \n    # - encoding - \n    encoder_inputs = preprocessing_layer(text_input)\n    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n        \n    # - output -\n    outputs = encoder(encoder_inputs)\n        \n    # - classifier layer -\n    net = outputs['pooled_output']\n    net = tf.keras.layers.Dropout(0.2)(net)\n    net = tf.keras.layers.Dense(5, activation='softmax', name='classifier')(net)\n    \n    model = tf.keras.Model(text_input, net)\n    return model\n        \nmodel = bert_text_classification()     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# running the model on some random text, of course that doesn't mean anything, since we haven't trained the model yet, \n# it just so we will be able to print the model structure \ntest_text = ['some random tweet']\nbert_raw_result = model(tf.constant(test_text))\n\n# -- Model structure -- \ntf.keras.utils.plot_model(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# -- Loss -- \nloss = tf.keras.losses.CategoricalCrossentropy()\n\n# -- Optimizer -- \n# will use the same optimizer that BERT was originally trained with: the \"Adaptive Moments\" (Adam). \ntrain_data_size = len(train_X)\nsteps_per_epoch = int(train_data_size/BATCH_SIZE)\nnum_train_steps = steps_per_epoch * EPOCHS\nnum_warmup_steps = int(0.1*num_train_steps/BATCH_SIZE)\n\noptimizer = optimization.create_optimizer(init_lr=LEARNING_RATE,\n                                          num_train_steps=num_train_steps,\n                                          num_warmup_steps=num_warmup_steps,\n                                          optimizer_type='adamw')\n\n# -- compile the model --\nmodel.compile(optimizer=optimizer,\n              loss=loss,\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# -- Fine Tuning the Model --\nhistory = model.fit(x=train_X,\n                    y=train_y,\n                    validation_data=(val_X, val_y),\n                    epochs=EPOCHS,\n                    validation_steps=1,\n                    verbose=1,\n                    batch_size=BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# -- Plot training and validation loss and accuracy --\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n    \nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.figure(figsize=(12,8))\n\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\n    \nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Loss - Cross Entropy')\nplt.xlabel('epoch')\nplt.ylim([0,1.0])\nplt.title('Training and Validation Loss')\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# -- Testing --\nloss, acc = model.evaluate(x=test_X,\n                           y=test_y)\nprint(\"test loss: \", loss, \", test acc: \", 100*acc, \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# -- Save the Model -- \nmodel.save('classifier_model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# -- Confusion matrix -- \nplt.title('confusion matrix - train data')\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\npreds = model.predict(train_X)\n\n# Convert the one-hot vectors to labels\ntrain_y_labels = tf.argmax(train_y, axis = 1)\npreds_labels = tf.argmax(preds, axis = 1)\n\ncm_train = tf.math.confusion_matrix(train_y_labels,\n                                    preds_labels,5,\n                                    dtype=tf.dtypes.float32)\n\n# Normalize the confusion matrix so that each row sums to 1.\ncm_train = cm_train/cm_train.numpy().sum(axis=1)[:, tf.newaxis]\n\nsns.heatmap(data=cm_train,\n            annot=True,\n            xticklabels=CLASSES,\n            yticklabels=CLASSES)\n\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}