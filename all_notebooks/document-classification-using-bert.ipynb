{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\n**BERT (Bidirectional Encoder Representations from Transformers)** is a state-of-the-art NLP model from Google for the purpose of this classification problem due to its efficiency, accuracy, and speed. BERT is inspired by the concept of Transfer Learning that is commonly used in the field of Computer Vision. The use of pre-trained models on massive unlabelled datasets such as that of Wikipedia and Book Corpus has helped speed up the training process and excellent results for BERT. It uses a multi-layer bidirectional transformer encoder that can be fine-tuned using an additional output layer to generate state=of-the-art models for various NLP tasks. The bi-directionality of the model helps capturing both the left and right context for truly understanding the meaning of a language.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Dataset\n\nThe dataset used is text from [BBC full-text dataset](https://www.kaggle.com/shivamkushwaha/bbc-full-text-document-classification). The model attempts to predict which category the text is about. So is a multi-class classification problem.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here we require Tensorflow version 1.15","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip uninstall tensorflow -y \n!pip install tensorflow==1.15","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !wget https://raw.githubusercontent.com/google-research/bert/master/modeling.py \n# !wget https://raw.githubusercontent.com/merishnaSuwal/bert/master/optimization.py\n# !wget https://raw.githubusercontent.com/merishnaSuwal/bert/master/run_classifier.py\n# !wget https://raw.githubusercontent.com/merishnaSuwal/bert/master/tokenization.py","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Getting bert functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget https://raw.githubusercontent.com/google-research/bert/master/modeling.py \n!wget https://raw.githubusercontent.com/google-research/bert/master/optimization.py\n!wget https://raw.githubusercontent.com/google-research/bert/master/run_classifier.py\n!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Import necessary libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport re, os\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\n\nimport datetime\nfrom datetime import datetime\n\n# BERT\nimport optimization\nimport run_classifier\nimport tokenization\nimport tensorflow_hub as hub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking tensorflow version (should be 1.15)\ntf.__version__","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Read the training dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the file details\ndirectory = []\nfile = []\ntitle = []\ntext = []\nlabel = []\ndatapath = '../input/bbc-full-text-document-classification/bbc-fulltext (document classification)/bbc/' \nfor dirname, _ , filenames in os.walk(datapath):\n    #print('Directory: ', dirname)\n    #print('Subdir: ', dirname.split('/')[-1])\n    # remove the Readme.txt file\n    # will not find file in the second iteration so we skip the error\n    try:\n        filenames.remove('README.TXT')\n    except:\n        pass\n    for filename in filenames:\n        directory.append(dirname)\n        file.append(filename)\n        label.append(dirname.split('/')[-1])\n        fullpathfile = os.path.join(dirname,filename)\n        with open(fullpathfile, 'r', encoding=\"utf8\", errors='ignore') as infile:\n            intext = ''\n            firstline = True\n            for line in infile:\n                if firstline:\n                    title.append(line.replace('\\n',''))\n                    firstline = False\n                else:\n                    intext = intext + ' ' + line.replace('\\n','')\n            text.append(intext)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Get the data as dataframe","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"DATA_COLUMN = 'text'\nLABEL_COLUMN = 'label'\n\nfulldf = pd.DataFrame(list(zip(directory, file, title, text, label)), \n               columns =['directory', 'file', 'title', 'text', 'label'])\n\ndf = fulldf.filter(['text','label'], axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking the shape of the dataframe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking if null values exist","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get unique labels","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for label in np.unique(df['label']):\n    print(label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking number of records of each label\ndf['label'].value_counts().sort_values(ascending=False).plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encode the labels into numeric","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\ndf['label'] = le.fit_transform(df['label'])\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Perform preprocessing to text","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\nBAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\nSTOPWORDS = set(stopwords.words('english'))\n\ndef clean_text(text):\n    \"\"\"\n        text: a string\n        \n        return: modified initial string\n    \"\"\"\n    text = text.lower() # lowercase text\n    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n#     text = re.sub(r'\\W+', '', text)\n    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['text'].apply(clean_text)\ndf['text'] = df['text'].str.replace('\\d+', '')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Processing text for BERT model\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_split(text1):\n    l_total = []\n    l_parcial = []\n    if len(text1.split())//150 >0:\n        n = len(text1.split())//150\n    else: \n        n = 1\n    for w in range(n):\n        if w == 0:\n            l_parcial = text1.split()[:200]\n            l_total.append(\" \".join(l_parcial))\n        else:\n            l_parcial = text1.split()[w*150:w*150 + 200]\n            l_total.append(\" \".join(l_parcial))\n    return l_total","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text_split'] = df['text'].apply(get_split)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Setting output directory for BERT","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the output directory for saving model file\nOUTPUT_DIR = '/bert_news_category'\n\n#@markdown Whether or not to clear/delete the directory and create a new one\nDO_DELETE = True #@param {type:\"boolean\"}\n\nif DO_DELETE:\n    try:\n        tf.gfile.DeleteRecursively(OUTPUT_DIR)\n#         tf.compat.v1.gfile.DeleteRecursively(OUTPUT_DIR)\n    except:\n        pass\n\ntf.gfile.MakeDirs(OUTPUT_DIR)\nprint('***** Model output directory: {} *****'.format(OUTPUT_DIR))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split into 80% training and 20% validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train, val = train_test_split(df, test_size=0.2, random_state=35)\ntrain.reset_index(drop=True, inplace=True)\ntrain.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get labels\nlabel_list = [x for x in np.unique(train.label)]\nlabel_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val.reset_index(drop=True, inplace=True)\nval.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check shape of train and validation data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"val.shape, train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_l = []\nlabel_l = []\nindex_l =[]\nfor idx,row in train.iterrows():\n    for l in row['text_split']:\n        train_l.append(l)\n        label_l.append(row['label'])\n        index_l.append(idx)\nlen(train_l), len(label_l), len(index_l)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_l = []\nval_label_l = []\nval_index_l = []\nfor idx,row in val.iterrows():\n    for l in row['text_split']:\n        val_l.append(l)\n        val_label_l.append(row['label'])\n        val_index_l.append(idx)\nlen(val_l), len(val_label_l), len(val_index_l)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Getting train and validation set as dataframe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.DataFrame({DATA_COLUMN:train_l, LABEL_COLUMN:label_l})\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_df = pd.DataFrame({DATA_COLUMN:val_l, LABEL_COLUMN:val_label_l})\nval_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train, X_val, y_train, y_val = train_test_split(df['text'], df['label'], test_size=0.20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fine tuning the BERT model\n\nThe BERT model can be applied for any kind of classification task by fine-tuning it.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 1. Preparing the input data, i.e create **InputExample** using the BERTâ€™s constructor.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the InputExample class from BERT's run_classifier code to create examples from the data\ntrain_InputExamples = train.apply(lambda x: run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n                                                                   text_a = x[DATA_COLUMN], \n                                                                   text_b = None, \n                                                                   label = x[LABEL_COLUMN]), axis = 1)\n\nval_InputExamples = val.apply(lambda x: run_classifier.InputExample(guid=None, \n                                                                   text_a = x[DATA_COLUMN], \n                                                                   text_b = None, \n                                                                   label = x[LABEL_COLUMN]), axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_InputExamples\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Row 0 - guid of training set : \", train_InputExamples.iloc[0].guid)\nprint(\"\\n__________\\nRow 0 - text_a of training set : \", train_InputExamples.iloc[0].text_a)\nprint(\"\\n__________\\nRow 0 - text_b of training set : \", train_InputExamples.iloc[0].text_b)\nprint(\"\\n__________\\nRow 0 - label of training set : \", train_InputExamples.iloc[0].label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n\ndef create_tokenizer_from_hub_module():\n  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n  with tf.Graph().as_default():\n    bert_module = hub.Module(BERT_MODEL_HUB)\n    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n    with tf.Session() as sess:\n#     with tf.compat.v1.Session() as sess:\n        vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n                                            tokenization_info[\"do_lower_case\"]])\n      \n    return tokenization.FullTokenizer(\n      vocab_file=vocab_file, do_lower_case=do_lower_case)\n\ntokenizer = create_tokenizer_from_hub_module()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(tokenizer.vocab.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here is what the tokenised sample of the first training set observation looks like\nprint(tokenizer.tokenize(train_InputExamples.iloc[0].text_a))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2. Converting the train and validation features to InputFeatures that BERT understands.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SEQ_LENGTH = 200\n\ntrain_features = run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n\nval_features = run_classifier.convert_examples_to_features(val_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Example on first observation in the training set\nprint(\"Sentence : \", train_InputExamples.iloc[0].text_a)\nprint(\"-\"*30)\nprint(\"Tokens : \", tokenizer.tokenize(train_InputExamples.iloc[0].text_a))\nprint(\"-\"*30)\nprint(\"Input IDs : \", train_features[0].input_ids)\nprint(\"-\"*30)\nprint(\"Input Masks : \", train_features[0].input_mask)\nprint(\"-\"*30)\nprint(\"Segment IDs : \", train_features[0].segment_ids)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating prediction model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n                 num_labels):\n  \n    bert_module = hub.Module(\n        BERT_MODEL_HUB,\n        trainable=True)\n    bert_inputs = dict(\n      input_ids=input_ids,\n      input_mask=input_mask,\n      segment_ids=segment_ids)\n    bert_outputs = bert_module(\n      inputs=bert_inputs,\n      signature=\"tokens\",\n      as_dict=True)\n\n    # Use \"pooled_output\" for classification tasks on an entire sentence.\n    # Use \"sequence_outputs\" for token-level output.\n    output_layer = bert_outputs[\"pooled_output\"]\n    # with tf.Session() as sess:\n    output_layer1 = bert_outputs[\"pooled_output\"]\n    # output_layer1 = 999\n    hidden_size = output_layer.shape[-1].value\n\n    # Create our own layer to tune for politeness data.\n    output_weights = tf.get_variable(\n      \"output_weights\", [num_labels, hidden_size],\n      initializer=tf.truncated_normal_initializer(stddev=0.02))\n\n    output_bias = tf.get_variable(\n      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n\n    with tf.variable_scope(\"loss\"):\n\n        # Dropout helps prevent overfitting\n        output_layer = tf.nn.dropout(output_layer, keep_prob=0.8)\n\n        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n        # Convert labels into one-hot encoding\n        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n\n        predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n        # If we're predicting, we want predicted labels and the probabiltiies.\n        if is_predicting:\n            return (predicted_labels, log_probs, output_layer1)\n\n        # If we're train/eval, compute loss between predicted and actual label\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n        \n        return (loss, predicted_labels, log_probs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_fn_builder(num_labels, learning_rate, num_train_steps,\n                     num_warmup_steps):\n    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n    \n    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n\n        input_ids = features[\"input_ids\"]\n        input_mask = features[\"input_mask\"]\n        segment_ids = features[\"segment_ids\"]\n        label_ids = features[\"label_ids\"]\n\n        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n\n        # TRAIN and EVAL\n        if not is_predicting:\n\n            (loss, predicted_labels, log_probs) = create_model(\n            is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n\n            train_op = optimization.create_optimizer(\n              loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n\n            # Calculate evaluation metrics. \n            def metric_fn(label_ids, predicted_labels):\n                accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n                true_pos = tf.metrics.true_positives(\n                    label_ids,\n                    predicted_labels)\n                true_neg = tf.metrics.true_negatives(\n                    label_ids,\n                    predicted_labels)   \n                false_pos = tf.metrics.false_positives(\n                    label_ids,\n                    predicted_labels)  \n                false_neg = tf.metrics.false_negatives(\n                    label_ids,\n                    predicted_labels)\n\n                return {\n                    \"eval_accuracy\": accuracy,\n                    \"true_positives\": true_pos,\n                    \"true_negatives\": true_neg,\n                    \"false_positives\": false_pos,\n                    \"false_negatives\": false_neg,\n                    }\n\n            eval_metrics = metric_fn(label_ids, predicted_labels)\n\n            if mode == tf.estimator.ModeKeys.TRAIN:\n                return tf.estimator.EstimatorSpec(mode=mode,\n                  loss=loss,\n                  train_op=train_op)\n            else:\n                return tf.estimator.EstimatorSpec(mode=mode,\n                    loss=loss,\n                    eval_metric_ops=eval_metrics)\n        else:\n            (predicted_labels, log_probs, output_layer) = create_model(\n            is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n            predictions = {\n              'probabilities': log_probs,\n              'labels': predicted_labels,\n              'pooled_output': output_layer\n            }\n        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n\n    # Return the actual model function in the closure\n    return model_fn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 16\nLEARNING_RATE = 2e-5\nNUM_TRAIN_EPOCHS = 1.0\n# Warmup is a period of time where the learning rate is small and gradually increases--usually helps training.\nWARMUP_PROPORTION = 0.1\n# Model configs\nSAVE_CHECKPOINTS_STEPS = 300\nSAVE_SUMMARY_STEPS = 100\n\n# Compute train and warmup steps from batch size\nnum_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\nnum_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n\n# Specify output directory and number of checkpoint steps to save\nrun_config = tf.estimator.RunConfig(\n    model_dir=OUTPUT_DIR,\n    save_summary_steps=SAVE_SUMMARY_STEPS,\n    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n\n# Specify output directory and number of checkpoint steps to save\nrun_config = tf.estimator.RunConfig(\n    model_dir=OUTPUT_DIR,\n    save_summary_steps=SAVE_SUMMARY_STEPS,\n    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_train_steps, len(label_list)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Initializing the model and the estimator\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_fn = model_fn_builder(\n  num_labels=len(label_list),\n  learning_rate=LEARNING_RATE,\n  num_train_steps=num_train_steps,\n  num_warmup_steps=num_warmup_steps)\n\nestimator = tf.estimator.Estimator(\n  model_fn=model_fn,\n  config=run_config,\n  params={\"batch_size\": BATCH_SIZE})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_input_fn = run_classifier.input_fn_builder(\n    features=train_features,\n    seq_length=MAX_SEQ_LENGTH,\n    is_training=True,\n    drop_remainder=False)\n\n# Create an input function for validating. drop_remainder = True for using TPUs.\nval_input_fn = run_classifier.input_fn_builder(\n    features=val_features,\n    seq_length=MAX_SEQ_LENGTH,\n    is_training=False,\n    drop_remainder=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training\nprint(f'Beginning Training!')\ncurrent_time = datetime.now()\nestimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\nprint(\"Training took time \", datetime.now() - current_time)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluating the model with Validation set\nestimator.evaluate(input_fn=val_input_fn, steps=None)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}