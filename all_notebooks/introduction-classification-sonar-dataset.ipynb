{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('../input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## First look at the csv file\n\nUsing Pandas library as pd we use the method [read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) to open the file and take a look at how the data is displayed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.read_csv('../input/mines-vs-rocks/sonar.all-data.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. This data has no header, so we must open it using the parameter header=None parameter.\n\n2. There are 207 observations with 61 columns, and the last column is the data we wish to predict.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"main_df = pd.read_csv('/kaggle/input/mines-vs-rocks/sonar.all-data.csv',header=None)\nmain_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the columns are enumerated from 0 to 60.\n\nThe last column have values for \"R\" and \"M\", wich stands for \"Rock\" or \"Mine\" observation.\n\nWe will try to predict it based on the columns from 0 to 59 while the column 60 says if its a \"Rock\" or \"Mine\" observation.\n\nLets check the classes balance.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"main_df[60].value_counts().plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are not much difference between the classes proportion, so I will not apply any rebalance to it.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I choose to split the inputs (first 59 columns) and targets (column 60 dummie data) for then use it as the model inputs and outputs.\n\nFirst the inputs.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs_df = main_df.drop(60, axis=1)\ninputs_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get the [dummy data for our classification column](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html), creates a new column for each class representing if the row belongs to the column class (1) or not (0).\n\nThen the outputs.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"targets_df = pd.get_dummies(main_df[60])\ntargets_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the targets we have now the columns 'R' wich stand for Rock and 'M' for Mine.\n\nFor these columns we have the values 1 for \"belongs to\" and 0 for \"doesn't belongs to\" the column class.\n\nI choose to split it into two Series object in a way for me to test the classification results for each one.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rock_y_df = targets_df['R']\nmine_y_df = targets_df['M']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We must then [split our data into train and test](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for we to be able to measure the model generalization as we predict unseen data by the model.\n\nThis step has a great impact on the model selection stage.\n\nI choose to predict 1 if its a mine and 0 if its a rock.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(inputs_df, mine_y_df, test_size=0.30, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) for feature creation as we are dealing with numerical data.\n\nThen we will use [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) with the feature creator and the classifier we choose ahead to get our predictions done.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# For feature creation\npoly = PolynomialFeatures(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will import some sklearn classifiers, test them and select the best one to use in our problem.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing classifiers\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import SGDClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Time to declare our classification models for testing and then choose the one with better generalization.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classifiers_ = [\n    (\"AdaBoost\",AdaBoostClassifier()),\n    (\"Decision Tree\", DecisionTreeClassifier(max_depth=10)),\n    (\"Gaussian Process\", GaussianProcessClassifier(1.0 * RBF(1.0))),\n    (\"Linear SVM\", SVC(kernel=\"linear\", C=0.025,probability=True)),\n    (\"Naive Bayes\",GaussianNB()),\n    (\"Nearest Neighbors\",KNeighborsClassifier(3)),\n    (\"Neural Net\",MLPClassifier(alpha=1)),\n    (\"QDA\", QuadraticDiscriminantAnalysis()),\n    (\"Random Forest\",RandomForestClassifier(n_jobs=2, random_state=1)),\n    (\"RBF SVM\",SVC(gamma=2, C=1,probability=True)),\n    (\"SGDClassifier\", SGDClassifier(max_iter=1000, tol=10e-3,penalty='elasticnet'))\n    ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train each Classifier to take its training results.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_names = []\ntrain_scores = []\ntest_scores = []\nfor n,clf in classifiers_:\n    clf_names.append(n)\n    # Model declaration with pipeline\n    clf = Pipeline([('POLY', poly),('CLF',clf)])\n    \n    # Model training\n    clf.fit(X_train, y_train)\n    print(n+\" training done!\")\n    \n    # Measure training accuracy and score\n    train_scores.append(clf.score(X_train, y_train))\n    print(n+\" training score done!\")\n    \n    # Measure test accuracy and score\n    test_scores.append(clf.score(X_test, y_test))\n    print(n+\" testing score done!\")\n    print(\"---\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can plot each one results for comparing.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot results\nplt.title('Accuracy Training Score')\nplt.grid()\nplt.plot(train_scores,clf_names)\nplt.show()\n\nplt.title('Accuraccy Test Score')\nplt.grid()\nplt.plot(test_scores,clf_names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the 11 classifiers we used, 7 got overfitting with 100% accuracy on the train data, but the test score shows us that only a few of them was able to generalize the problem.\n\nAs seen in the Test Score results, the Gaussian Process shows better generalization, followed by the methods of Artificial Neural Networks, K-Neares Neighbors then SGD.\n\nWe will then train a model using Gaussian Process method together with Polynomial Features as it shows better results for this experiment.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rng = np.random.RandomState(1)\n\nclf = GaussianProcessClassifier(1.0 * RBF(1.0))\n\nclf = Pipeline([('POLY', poly),\n                ('ADABOOST', clf)])\n\n# Training our model\n%time clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Measure its performance on the training set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It shows a kind of overfitting, where its high complexity makes it fit the the whole training dataset.\n\nIt can become a problem depending on the context that you're dealing with, but first lets check its score on the test dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accuraccy of 92% on the test dataset shows that the model was able to generalize well for the task of classifying if the observation is a rock or a mine.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Lets count how much mines our classifier points in the test dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.predict(X_test).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets count how much there really is.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nFor better conclusions is a good choice to plot a [confusion matrix](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html) for better describing our model accuracy on both: train and test data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import plot_confusion_matrix\n\ndisp = plot_confusion_matrix(clf, X_train, y_train,\n                             display_labels=['ROCK','MINE'],\n                             cmap=plt.cm.Blues,\n                             normalize=None)\ndisp.ax_.set_title('Confusion matrix')\n\nprint('Train results: confusion matrix')\nprint(disp.confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The training dataset confusion matrix shows that it has 100% accuray, correct classifying each observation.\n\nWe must then take measurements on the confusion matrix of the test data:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"disp = plot_confusion_matrix(clf, X_test, y_test,\n                             display_labels=['ROCK','MINE'],\n                             cmap=plt.cm.Blues,\n                             normalize=None)\ndisp.ax_.set_title('Confusion matrix')\n\nprint('Test results: confusion matrix')\nprint(disp.confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our model wrong predicted some samples on the test dataset.\n\nThe model predicted some rocks as mines, and no one would be in danger, no stone would explode by being carefull to disarm it.\n\nThe other hand it predicted some mines as rock and it may put people life in danger if dealing with the task of predicting real mines even though the chances are low.\n\nOne available solution, if the risk is not worth, is to use this predictor attached to a robot to avoid injuries.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}