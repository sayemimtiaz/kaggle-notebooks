{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Hyperparameter grid search sample code\nThis is a sample code for performing a hyperparameter grid search using the [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) routine from scikit-learn. We shall use the default 5-fold [cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics&#41;). Finally, for the classifier we shall use the [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), also from scikit-learn.\n","metadata":{}},{"cell_type":"code","source":"#!/usr/bin/python3\n# coding=utf-8\n#===========================================================================\n# This is a simple script to perform a classification on the kaggle \n# 'Titanic' data set using a grid search, in conjunction with a \n# random forest classifier\n# Carl McBride Ellis (1.V.2020)\n#===========================================================================\n#===========================================================================\n# load up the libraries\n#===========================================================================\nimport pandas as pd\nimport numpy  as np\n\n#===========================================================================\n# read in the data\n#===========================================================================\ntrain_data = pd.read_csv('../input/titanic/train.csv')\ntest_data  = pd.read_csv('../input/titanic/test.csv')\n\n#===========================================================================\n# select some features of interest (\"ay, there's the rub\", Shakespeare)\n#===========================================================================\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\"]\n\n#===========================================================================\n# for the features that are categorical we use pd.get_dummies:\n# \"Convert categorical variable into dummy/indicator variables.\"\n#===========================================================================\nX_train       = pd.get_dummies(train_data[features])\ny_train       = train_data[\"Survived\"]\nfinal_X_test  = pd.get_dummies(test_data[features])\n\n#===========================================================================\n# hyperparameter grid search using scikit-learn GridSearchCV\n# we use the default 5-fold cross validation\n#===========================================================================\nfrom sklearn.model_selection import GridSearchCV\n# we use the random forest classifier\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(criterion='gini', max_features='auto')\ngs = GridSearchCV(cv=5, error_score=np.nan, estimator=classifier,\n# dictionaries containing values to try for the parameters\nparam_grid={'min_samples_leaf':  [10, 15, 20],\n            'max_depth':         [3, 4, 5, 6],\n            'n_estimators':      [10, 20, 30]})\ngs.fit(X_train, y_train)\n\n# grid search has finished, now echo the results to the screen\nprint(\"The best score is %.5f\"  %gs.best_score_)\nprint(\"The best parameters are \",gs.best_params_)\nthe_best_parameters = gs.best_params_\n\n#===========================================================================\n# now perform the final fit, using the best values from the grid search\n#===========================================================================\nclassifier = RandomForestClassifier(criterion='gini', max_features='auto',\n             min_samples_leaf  = the_best_parameters[\"min_samples_leaf\"],\n             max_depth         = the_best_parameters[\"max_depth\"],\n             n_estimators      = the_best_parameters[\"n_estimators\"])\nclassifier.fit(X_train, y_train)\n\n#===========================================================================\n# use the model to predict 'Survived' for the test data\n#===========================================================================\npredictions = classifier.predict(final_X_test)\n\n#===========================================================================\n# write out CSV submission file\n#===========================================================================\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, \n                       'Survived': predictions})\noutput.to_csv('submission.csv', index=False)","metadata":{"_uuid":"69c9d0fd-8ab0-4f0e-b431-0755b8f7adf8","_cell_guid":"9251ec7a-15b5-4b3a-89f8-87e94dea8b5f","execution":{"iopub.status.busy":"2021-08-08T05:51:34.799922Z","iopub.execute_input":"2021-08-08T05:51:34.80039Z","iopub.status.idle":"2021-08-08T05:51:43.440972Z","shell.execute_reply.started":"2021-08-08T05:51:34.800333Z","shell.execute_reply":"2021-08-08T05:51:43.439798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let us compare our score with the final leaderboard score","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nsolution   = pd.read_csv('../input/submission-solution/submission_solution.csv')\nprint(\"The test (i.e. leaderboard) score is %.5f\" % accuracy_score(solution['Survived'],predictions))","metadata":{"execution":{"iopub.status.busy":"2021-08-08T05:51:43.445372Z","iopub.execute_input":"2021-08-08T05:51:43.445802Z","iopub.status.idle":"2021-08-08T05:51:43.46226Z","shell.execute_reply.started":"2021-08-08T05:51:43.445748Z","shell.execute_reply":"2021-08-08T05:51:43.46054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that our score is significantly higher than the final leaderboard score. This is a clear symptom of overfitting, which is *very* easy to do on the Titanic dataset. For more about this see [\"*Overfitting and underfitting the Titanic*\"](https://www.kaggle.com/carlmcbrideellis/overfitting-and-underfitting-the-titanic). Part of the reason for this is simply that the Titanic dataset is very small, and splitting it up makes it even smaller, with each small piece being even less representative of the whole. Another reason is due to the technique of cross-validation itself, whose variance estimate is a bit too small due to the correlation between the error estimates in different folds. This in turn means that the confidence intervals for prediction error are too small, leading to an overly-optimistic hyperparameter selection.\n\nA better solution, although more computationally expensive, is to use **nested cross-validation**. To learn more about nested-CV see:\n\n* [\"*Nested versus non-nested cross-validation*\"](https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html) on scikit-learn\n* [Stephen Bates, Trevor Hastie and Robert Tibshirani \"*Cross-validation: what does it estimate and how well does it do it?*\", arXiv:2104.00673 (2021)](https://arxiv.org/pdf/2104.00673.pdf)\n* [\"*Nested Cross-Validation for Machine Learning with Python*\"](https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/) on *Machine Learning Mastery*\n* [\"*A step by step guide to Nested Cross-Validation*\"](https://www.analyticsvidhya.com/blog/2021/03/a-step-by-step-guide-to-nested-cross-validation/) on *Analytics Vidhya*","metadata":{}}]}