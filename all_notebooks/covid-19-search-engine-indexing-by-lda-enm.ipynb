{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1><span class=\"tocSkip\"></span>Table of Contents</h1>\n<div id=\"toc-wrapper\"></div>\n<div id=\"toc\"></div>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Introduction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The **COVID-19 pandemic**, is an ongoing global pandemic of coronavirus disease 2019 (COVID‑19), caused by severe acute respiratory syndrome coronavirus 2 (SARS‑CoV‑2).\n\nThe pandemic has caused global social and economic disruption, including the largest global recession since the Great Depression. [[Wikipedia]](https://en.wikipedia.org/wiki/COVID-19_pandemic)\n\nIn response to the **COVID-19 pandemic**, the White House and a coalition of leading research groups have prepared the COVID-19 Open Research Dataset (CORD-19). [CORD-19](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) is a resource of over 158,000 scholarly articles, including over 75,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. \n\nBy filtering the CORD-19 dataset, collecting only papers published after the december 2019 and related to COVID-19/SARS-CoV-2, we build a corpus of 5900 papers.\n\nOur goal is to build a Search Engine (SE) over the COVID-19 corpus taking into account linguistic phenomena, synonymy and polysemy.\n\n\n*Before we move on the next section, just let us prepare our framework, by loading libraries, data and setting some queries...*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Configuration class\n\nWe set variables like from where we load, where to store and some parameters (Explained later).","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"class config():\n    OUTPUT_DIR=\"/kaggle/working/\"\n    CORPUS_FN='/kaggle/input/cord-19-step2-corpus/corpus.pkl'\n    ENM_FN='/kaggle/working/ranker_enm.pickle'\n    TOC2_FN='/kaggle/input/toc2js/toc2.js'\n    \n    n_relevant = 150 # |R|\n    ql_lambda = 0.4\n    rm1_lambda = 0.6\n    rm3_lambda = 0.8\n    \n    coherence_model = 'c_v'\n    coherence_win = 110\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Libraries","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Our libraries\n\nAll our libraries are made public under open source.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"import cord_19_container as container\nimport cord_19_rankers as rankers\nimport cord_19_lm as lm\nimport cord_19_vis as vis\n\nfrom cord_19_container import Sentence, Document, Paper, Corpus\n\nfrom cord_19_metrics import compute_queries_perf\n\nfrom cord_19_helpers import load, save\nfrom cord_19_text_cleaner import Cleaner\nfrom cord_19_wn_phrases import wn_phrases","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Commun libraries","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from gensim import matutils\nfrom gensim.models import (LdaModel, LsiModel, TfidfModel,\n                           CoherenceModel, LogEntropyModel)\n\nimport numpy as np\nimport pandas as pd\n\nimport math\nimport operator\nimport logging","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualization libraries","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"%matplotlib inline\n\nfrom IPython.display import display, HTML, Markdown, Latex\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport pyLDAvis.gensim\n\nHTML(\"\"\"\n<style>\n.output_png {\n    text-align: center;\n    vertical-align: middle;\n}\n\n.rendered_html table{\n    display: table;\n}\n</style>\n\"\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Load the corpus, papers talking about COVID-19/SARS-CoV-2, done in our previous kernel.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"corpus = load(config.CORPUS_FN)\ndictionary = corpus.dictionary\n\n# Rebuild id2token from token2id, only token2id is saved\nfor k,v in dictionary.token2id.items():\n    dictionary.id2token[v]=k\n\n# Set the dictionary as global, we have to find better way\ncontainer.dictionary = dictionary\nrankers.dictionary = dictionary\nvis.dictionary = dictionary\n\nprint(f'#Papers {len(corpus)}, #Tokens {len(dictionary)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Queries\n\nWe define a bunch of [queries](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks?taskId=568) to train our model, we are using 17 queries, higher is better.\n","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"queries = [\n    'Range of incubation periods for the disease in humans.',\n    'Range of incubation periods for the disease in humans and how this varies across age and health status.',\n    'How long individuals are contagious, even after recovery?',\n    'Prevalence of asymptomatic shedding and transmission e.g. particularly children.',\n    'Seasonality of transmission e.g. climate, humidity, temperature.',\n    'Physical science of the coronavirus: Charge distribution, adhesion to hydrophilic hydrophobic surfaces.',\n    'Physical science of the coronavirus: Environmental survival to inform \\\ndecontamination efforts for affected areas and provide information about viral shedding.',\n    'Persistence and stability on a multitude of substrates and sources \\\ne.g. nasal discharge, sputum, urine, fecal matter, blood.',\n    'Persistence of virus on surfaces of different materials e.g. copper, stainless steel, plastic.',\n    'Natural history of the virus and shedding of it from an infected person.',\n    'Implementation of diagnostics and products to improve clinical processes.',\n    'Disease models, including animal models for infection, disease and transmission.',\n    'Tools and studies to monitor phenotypic change and potential adaptation of the virus.',\n    'Immune response and immunity.',\n    'Effectiveness of movement control strategies to prevent secondary transmission in health care and \\\ncommunity settings.',\n    'Effectiveness of personal protective equipment PPE and its usefulness to reduce risk of \\\ntransmission in health care and community settings.',\n    'Role of the environment in transmission.'\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define the queries corpus, foreach query in queries do:\n- Clean;\n- Tokenize;\n- Merge multi-word expressions using WordNet.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"queries_corpus = container.Corpus([container.Document([Cleaner(True).clean(q)]) for q in queries])\nfor doc in queries_corpus:\n    doc.tokenize()\n    wn_phrases(doc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Language models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Language modeling is a quite general formal approach to Information Retrieval (IR), with many variant realizations. The original and basic method for using language models in IR is the *query likelihood model*. [[4]](#r4)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Query likelihood model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We estimate the likelihood of an individual document model generating the query as:\n\n$\\hat{P}(Q|D) = \\prod_{q \\in Q}\\hat{P}(q|D)$\n\nUsing Bayes rule (**Query generating document**), we have:\n\n$P(D|Q) = \\frac{P(Q|D)P(D)}{P(Q)}$\n\nwhere:\n- $P(Q)$ = is constant for all documents, and so can be ignored\n- $P(D)$ = is the prior of relevant document\n- $Q$ = Query sequence of terms\n- $q$ = term of query\n- $D$ = Document sequence of terms\n\n$P(D)$ is uniform across all $D$ and is ignored for the moment, **<span style=\"color:red\">but we can improve our SE by including it as authority/newness</span>**.\n\n\nThe classic problem with using language models is one of estimation (the $\\hat{}$ symbol is used above to stress that the model is estimated): terms appear very sparsely in documents.\n\nIn particular, some words will not have appeared in the document at all, but are possible words for the information need, which the user may have used in the query. \n\nIf we estimate $\\hat{P}(q|D) = 0$ for a term missing from a document $D$, then we get a strict conjunctive semantics: documents will only give a query non-zero probability if all of the query terms appear in the document.\nRegardless of the approach here, there is a more general problem of estimation: occurring words are also badly estimated; in particular,\nthe probability of words occurring once in the document is normally overestimated, since their one occurrence was partly by chance.\n\nThe answer to this is smoothing. There's a wide space of approaches (three of them are presented in [[9]](#r9)) to smoothing probability distributions to deal with this problem, we are using the simplist form *Jelinek-Mercer*.\n\n\n$P(q|D) = \\lambda P_{MLE}(q|D) + (1-\\lambda)P_{coll}(w)$\n\nwhere:\n- $\\lambda$ free parameter [0,1] was set to 0.6 as many papers `config.ql_lambda`\n\n$P_{MLE}(q|D)$ defined as $P_{MLE}(w|D, w \\notin Q) = 0$\n\nwhere:\n- $P_{coll}(w)$ = relative frequency of the term in the collection as a whole\n\n\n*Most if not all we said in this section comes from well known online IR Standford course \"Introduction to Information Retrieval (Section 12)\" [[4]](#r4)*\n\nFunction `cord_19_lm::compute_ql`","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"ranker_ql = rankers.ranker_QL(corpus, config.ql_lambda)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Relevance Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Relevance models are a form of massive query expansion through blind feedback. Constructing a relevance model entails first ranking the collection according to the maximum likelihood query model. [[13 Section 3.1.4]](#r6) [[6]](#r6)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Relevance Model (RM1)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<br>\n$P_{RM1}(w|Q) = \\sum_{D \\in R}P(w|D)P(D|Q)$\n<br><br>\nWe do for $P(w|D)$ as we did previously for $P(q|D)$.\n\nwhere:\n- $R$ is a set of relevant documents the $|R|$ defined at `config.n_relevant`\n\nFunction `cord_19_lm::compute_rm1`","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Relevance Model (RM3)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"RM3 is the most popular of the four Relevance Model variants (RM1, RM2, RM3 and RM4)\n\nSome times the original query terms do not have the highest weights in the expanded query model, seems risky and problematic.\n\nRM3 solve this behavior by interpolating the RM1 with the original query's MLE. [[13 Section 3.1.4]](#r6) [[6]](#r6)\n\n<br>\n$P_{RM3}(w|Q) = \\lambda P_{RM1}(w|Q) + (1-\\lambda)P_{MLE}(w|Q)$\n\nwhere:\n- $\\lambda$ is a free parameter in range [0,1] defined at `config.rm3_lambda`\n\n\nFunction `cord_19_lm::compute_rm3`","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Topic models RM","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Combine topics models with RM query expansion method [[5]](#r5)\n\n$P(w|Q) = \\sum_{D \\in R}(\\lambda P_{RM3}(w|D)+(1-\\lambda) P_{TM}(w|D,Q) P(D|Q)$\n\n$P_{TM}(w|D,Q) = \\sum_{t_m}P(w|t_m)P(t_m|D,Q) \\propto \\sum_{t_m}P(w|t_m)P(t_m|D)P(Q|t_m)$\n\nNote: we can use $P_{RM3}$ or $P_{RM1}$ depending on the problem. e.g., $P_{RM3}$ for query expansion.\n\nwhere:\n- $P(w|t_m)$ = word probality given a topic\n- $P(t_m|D)$ = topic probality given a Document\n- $P(Q|t_m) \\propto P(t_m|Q)$ = Query likelihood given a topic\n\nFunction `cord_19_lm::compute_tm_rm`","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Metrics\n\nWe need metrics for estimating the effectiveness of a search performed in response to a query in lack of relevance judgments. In case we have labeled data usually we use [Average Precision](https://en.wikipedia.org/wiki/Evaluation_measures_%28information_retrieval%29#Mean_average_precision) (AP).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Clarity score","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*Clarity score* for predicting query performance by computing the relative entropy between a query language model and the corresponding collection language model. The resulting *clarity score* measures the coherence of the language usage in documents whose models are likely to generate the query. \n\nThe *clarity score* for the query is simply the relative entropy, or Kullback-Leibler divergence (we use [Jensen–Shannon divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence) ), between the query and collection language models. [[2]](#r2)\n\n<br>\n$clarity\\;score=\\sum_{w \\in V}P_{RM1}(w|Q)log_2\\frac{P_{RM1}(w|Q)}{P_{coll}(w)}$\n\nwhere:\n- $V$ = the entire collection vocabulary\n\nFunction `cord_19_metrics::compute_cs`","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## UEF(Clarity)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Utility estimation framework (UEF) is a framework for the query-performance prediction task. The approach [[3]](#r3) is based on using statistical decision theory for estimating the utility that a document ranking provides with respect to an information need expressed by the query. To address the uncertainty in inferring the information need, we estimate utility by the expected similarity between the given ranking and those induced by relevance models; the impact of a relevance model is based on its presumed representativeness of the information need.\n\n$U(R|I_Q) \\approx Sim(\\pi(R, \\hat{R_Q}), R)P(\\hat{R_Q}|I_Q)$\n\nThe re-ranking function $\\pi(R, \\hat{R_Q})$, foreach $D$ in $R$ using negative cross entropy (CE) is:\n\n$Score_{CE} = \\sum_{w \\in D} P_{RM3}(w|Q) log P(w|D)$\n\nwhere:\n- $Sim$ is similarity between ranked lists, we use Pearson's r correlation\n- $\\hat{R_Q}$ = estimates for the true relevance model\n- $P(\\hat{R_Q}|I_Q)$ Quantifying the extent to which a relevance-model estimate, represents the information need $I_Q$ in our case *Clarity scrore*\n\nFunction `cord_19_metrics::compute_uef_cs`","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Topic Coherence\n\nWe evaluate the performance of the topic models we will use by their coherences. Using the $C_v$ model (window size 110) [[11]](#r11), the $C_v$ model produce a strong average correlation of **0.73** with human ratings. Fortunately already implemented into *Gensim*.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"def get_coherence(model):\n    \"\"\"Topics coherence sorted in ascending order\n    return: (topic,coherence)\n    \"\"\"\n    cm = CoherenceModel(model=model, texts=corpus.text,\n                        coherence=config.coherence_model,\n                        window_size=config.coherence_win)\n    \n    coherence_values = np.asarray(cm.get_coherence_per_topic())\n    coherence = {k:v for k,v in enumerate(coherence_values)}\n    \n    coherence = sorted(coherence.items(), key=operator.itemgetter(1), reverse=True)\n    \n    return coherence","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Indexing by Latent Dirichlet Allocation (LDI)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this section (the master piece of the kernel), we propose an LDA-based probabilistic topic search model that identifies a set of documents that closely matches a given set of query terms in a topic space. This method is called Indexing by Latent Dirichlet Allocation (LDI). [[1]](#r1)\n\nSince LDA models documents as a mixture of topics, it provides a new approach for representing documents in a topic space where the topics can be seen as index terms for indexing. \n\nSince we aim to construct explicit document representations associated with topics, the method directly uses the $\\beta$ matrix in the LDA model. The conditional probability $\\beta_{jk}$ in LDA represents the selection probability of the word $w^j$ given a topic (concept) $z^k$. This value represents the probability of a word given a specific topic and it is used in identifying words that are associated with a topic. However, it may not be used as the probability of a\ntopic given a word. Thus, for characterization, we define word representation in topic space, $W_j \\in \\mathbb{R} ^K$. The $kth$ component $W_{j}^{k}$\nof $W_j$ represents the probability of word $w^j$ embodying the $kth$ concept $z^k$. This quantity can be obtained by Bayes’ rule as:\n\n$$W_{j}^{k} = p(z^k=1|w^j=1)=\\frac {p(w^j=1|z^k=1)p(z^k=1)} {\\sum \\limits _{h=1} ^{K} p(w^j=1|z^h=1)p(z^h=1) }.$$\n\nWe assume that the probability of a topic selection is uniformly distributed. It is conceivable that more sophisticated adaptive techniques for the probability of topic selection will result in a more accurate model. With this assumption, we obtain the probality of a word $w^j$ corresponding to a concept $z^k$ as\n\n$$W^{k}_{j}=\\frac {\\beta_{jk}} {\\sum \\limits _{h=1} ^K \\beta_{jh}}.$$\n\nFurthermore, the documents can be represented in the topic space as well, $D_i \\in \\mathbb{R} ^K$. The $kth$ component $D^{k}_{i}$ of $D_i$ represents the probability of a concept $z^k$ given a document $d_i$ and it is expressed as\n\n$$D^{k}_{i} \\approx \\tilde{D^{k}_{i}} = \\frac {\\sum _{w^j \\in d_i} W ^{k} _{j} n_{ij}} {N_{d_i}}$$\n\nwhere:\n- $n_{ij}$ is the count of word $j$ in document $d_i$;\n- $N_{d_i}$ is the number of words in document $d_i$;\n- $K$ number of topics\n\n$\\beta$ can be computed using: `B = lda_model.state.get_lambda(); B = B / B.sum(axis=1)[:, None]`\n\nThe computation of $W$ and $D$, are not restricted to LDA but can be done for many others models, provided that they can produce $\\beta$ matrix (probability of the word $w^j$ given a topic $z^k$).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### LDA Training","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We train our LDA model in unusual way, not necessary better, but mathematically simpler with less stochasticity and we believe it's better for small corpus. Please refere to [[10 Section 2.2]](#r10)\n\nWe train using standard (batch) variational Bayes (VB) , we set decay () to zero and chunksize=len(corpus) , \n- Set $\\kappa$ to zero (*Gensim*:lda:decay=0 means 'learning rate'=1);\n- Set $S=D$ (*Gensim*:lda:chunksize=|corpus|).\n\nWe can set the LDA priors in five different ways Please refere to [[12]](#r12):\n- Symmetric priors over both $\\Theta$ and $\\Phi$ denoted **SS**, default option in *Gensim*:lda;\n- Symmetric prior over $\\Theta$ and asymmetric over $\\Phi$ denoted **SA**;\n- Asymmetric prior over $\\Theta$ and symmetric over $\\Phi$ denoted **AS**;\n- Asymmetric learned prior over $\\Theta$ and symmetric over $\\Phi$ denoted **ASO**;\n- Aymmetric priors over both $\\Theta$ and $\\Phi$ denoted **AA**.\n\nWe can say that $\\Theta$ and $\\Phi$ are Topics importance and Words importance prior.\n\n1. **SA** and **AA** produce the worst results, that's why not included into *Gensim*;\n2. **AS** and **ASO** produce better than **SS**;\n3. There's no clear winner between **AS** and **ASO**\n4. The learned asymmetricity from **ASO** can improve the uniformity assumption in the previous section, also coherence can do.\n5. We use **ASO**\n\nIn the production mode we need to train LDA until convergence, the relative improvement in maximization step lower than 1e-5. In this kernel we stop around 5e-2.\n","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"def train_lda(k):\n    gensimLogger = logging.getLogger('gensim.models.ldamodel')\n    gensimLogger.setLevel(logging.DEBUG)\n    \n    ch = logging.FileHandler(config.OUTPUT_DIR+f'lda.log', mode='w')\n    formatter = logging.Formatter('%(asctime)s : %(levelname)s : %(message)s')\n    ch.setFormatter(formatter)\n    gensimLogger.addHandler(ch)\n\n    lda_params = {'num_topics':k,\n                  'eval_every':0,\n                  'gamma_threshold':1e-5,\n                  'iterations':5000,\n                  'passes':50,\n                  'offset':1,\n                  'chunksize':len(corpus),\n                  'decay':0., # kappa\n                  'update_every':0,\n                  'random_state':1,\n                  'alpha':'auto', 'eta':'symmetric' # ASO\n                 }\n\n    lda_model = LdaModel(corpus.bow,\n                        id2word=dictionary,\n                        **lda_params)\n\n    ch.close()\n    gensimLogger.removeHandler(ch)\n        \n    return lda_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n\nlda_model = train_lda(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Better in the future to use corpus.TRF\nranker_ldi = rankers.Ranker_LDI(lda_model, corpus.TF)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n\nlda_top_topics = get_coherence(lda_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LDA Intertopic Distance Map (Top 25)","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"top_topics_ids = [k for k,v in lda_top_topics[:25]]\npyvis = vis.prepare(ranker_ldi, corpus, dictionary=dictionary, top_topics=top_topics_ids)\n\nhtml = vis.prepared_data_to_html(pyvis, visid='pylda_ldi')\ndisplay(HTML(html))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Non-negative matrix factorization (NMF)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Non-negative matrix factorization (NMF or NNMF) is a group of algorithms in multivariate analysis and linear algebra where a matrix $V$ is factorized into (usually) two matrices $W$ and $H$, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect instead of LSI results [[15]](#r15).\n\nNMF is a NP-Hard problem, discussing the methods to solve it is beyond this scope. Fortunately we have `sklearn.decomposition.NMF`.\n\nWe are interested in the non-negativity of both input and the output:\n- Our input $V$ matrix terms-documents relative frequency all elements are **positive**;\n- The output is **positive** we can easily transform to probality, then apply the same process as LDI.\n\nEmpirical studies with LSI report that the Log and Entropy weighting functions work well, in practice, with many data sets [[14]](#r14).  We do the same here.\n\n$V$ now is defined as\n\n$g_i = 1 + \\sum _j \\frac {P_{ij} log(P_{ij})} {log(n)}$  \n$V_{ij} = g_{i} log(tf_{ij} + 1)$\n\nWhere:\n- $tf_{ij}$ term frequency;\n- $P_{ij}$ = $\\frac{tf_{ij}}{\\sum_j tf_{ij}}$\n- $i$ document index;\n- $j$ term index;\n\nThanks again to *Gensim* no need to develop from scratch, but use `gensim.models.LogEntropyModel`","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### NMF Training","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will train our NMF model, using default *sklearn* parameters.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"def train_nmf(k, nmf_pre, max_iter=400):\n    X_csr = matutils.corpus2csc(nmf_pre[corpus.bow])\n    X = X_csr.T.toarray()\n    \n    from sklearn import decomposition\n    \n    clf = decomposition.NMF(n_components=k, max_iter=max_iter, random_state=1, verbose=0)\n    \n    W = clf.fit_transform(X)\n    H = clf.components_\n    \n    return H","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n\nnmf_pre = LogEntropyModel(corpus.bow)\n\nH1 = train_nmf(200, nmf_pre, max_iter=500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# We need an interface compatible with Gensim, used for coherence\nnmf_model = rankers.NMFModel(H1.T)\nranker_nmf = rankers.Ranker_NMF(H1, corpus, nmf_pre)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n\nnmf_top_topics = get_coherence(nmf_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### NMF Intertopic Distance Map (Top 25)","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"top_topics_ids = [k for k,v in nmf_top_topics[:25]]\npyvis = vis.prepare(ranker_nmf, corpus, dictionary=dictionary, top_topics=top_topics_ids)\n\nhtml = vis.prepared_data_to_html(pyvis, visid='pylda_nmf')\ndisplay(HTML(html))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Latent semantic indexing (LSI)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"LSI [[14]](#r14) uses common linear algebra techniques to learn the conceptual correlations in a collection of text. In general, the process involves constructing a weighted term-document matrix, performing a Singular Value Decomposition on the matrix, and using the matrix to identify the concepts contained in the text.\n\n$A=U \\Sigma V^T$\n\nTruncated version to r (number of topics):\n\n$A \\approx A_{r} = U_{r} \\Sigma_{r} V_{r}^T $\n\nwhere:\n- $A$ = matrix [m,n] of term frequencies, m is the number of unique terms, and n is the number of documents\n- $U$ = term-concept vector matrix [m,r], when truncated r is number of topics\n- $S$ = Singular values [r,r], sorted by magnitude\n- $V$ = concept-document vector matrix [n,r]\n\nThe problem with LSI the $U$ resulting dimensions might be difficult to interpret, and contains positive and negative values, but usually we use the magnitude as the importance of word topics.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"lsi_pre = LogEntropyModel(corpus.bow)\nlsimodel = LsiModel(lsi_pre[corpus.bow], id2word=dictionary, power_iters=10,\n                    num_topics=200, onepass=False)\n\nranker_lsi = rankers.Ranker_LSI(corpus, lsimodel, lsi_pre)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n\nlsi_top_topics = get_coherence(lsimodel)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TF-IDF","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. [[Wikipedia]](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n\nThere is many weighting variants in the vector space model [SMART](https://en.wikipedia.org/wiki/SMART_Information_Retrieval_System), we use the 'nfc' form.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"ranker_tfidf = rankers.Ranker_TFIDF(corpus, smartirs='nfc')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Topic models comparative","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Clusters of documents","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"# https://stackoverflow.com/questions/25812255/row-and-column-headers-in-matplotlibs-subplots\n\nfig, axes = plt.subplots(3, 3, gridspec_kw={\"height_ratios\":[0.01,1,1],\n                                            \"width_ratios\": [0.02,1,1]},\n                        figsize=(14,14))\n\naxes = axes.flatten()\n\"\"\"\n0 1 2\n3 4 5\n6 7 8\n\"\"\"\n\n# Columns titles\n# 1st column\naxes[1].axis('off')\naxes[1].set_xticks([]); axes[1].set_yticks([])\naxes[1].set_title('NMF', loc='left', fontweight='bold')\n# 2nd column\naxes[2].axis('off')\naxes[2].set_title('LDI', loc='left', fontweight='bold')\n\n# Rows titles\n# Empty\naxes[0].axis(\"off\")\n# 1st row\naxes[3].axis(\"off\")\naxes[3].annotate('The visualization of the clustered data.',\n                 xy=(0, 0), xycoords='data',\n                 rotation='vertical',\n                 fontsize='large', fontweight='bold')\n# 2nd row\naxes[6].axis(\"off\")\naxes[6].annotate('The silhouette plot for the various clusters.',\n                 xy=(0, 0), xycoords='data',\n                 rotation='vertical',\n                 fontsize='large', fontweight='bold')\n\nvis.plot_tm_clusters(ranker_nmf, axes[4], axes[7], set_y_label=False)\nvis.plot_tm_clusters(ranker_ldi, axes[5], axes[8], set_y_label=True)\n\nfig.suptitle(f'20 Clusters of documents projected into t-SNE (2 subspaces)')\n\n#fig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dominant topics","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"fig_1, (ax1_nmf, ax1_ldi) = plt.subplots(1, 2, figsize=(14,6), sharey=True)\nfig_2, (ax2_nmf, ax2_ldi) = plt.subplots(1, 2, figsize=(14,6), sharey=True)\n\nvis.plot_topics_dist(ranker_nmf, corpus, ax1_nmf, ax2_nmf, \"NMF\", set_y_label=True)\nfig_1.suptitle(f'Number of Documents by Dominant Topic.')\n\nvis.plot_topics_dist(ranker_ldi, corpus, ax1_ldi, ax2_ldi, \"LDI\", set_y_label=False)\nfig_2.suptitle(f'Mean topic probability over corpus.')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Coherence","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"nmf_coh = np.asarray([coherence for _, coherence in nmf_top_topics])\nlda_coh = np.asarray([coherence for _, coherence in lda_top_topics])\nlsi_coh = np.asarray([coherence for _, coherence in lsi_top_topics])\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[10, 4])\n\nax1.plot(nmf_coh, label=f'NMF \\u03BC={nmf_coh.mean():.2f}')\nax1.plot(lda_coh, label=f'LDA \\u03BC={lda_coh.mean():.2f}')\nax1.plot(lsi_coh, label=f'LSI \\u03BC={lsi_coh.mean():.2f}')\nax1.axhline(y=0.5, color='grey', linestyle='dashed')\nax1.set_xlabel('topic')\nax1.set_ylabel('coherence')\nax1.set_title('Topic/Model sorted by coherence')\nax1.legend()\n\nsns.kdeplot(nmf_coh, shade=True, cut=0, ax=ax2, label=f'NMF')\nsns.kdeplot(lda_coh, shade=True, cut=0, ax=ax2, label=f'LDA')\nsns.kdeplot(lsi_coh, shade=True, cut=0, ax=ax2, label=f'LSI')\nax2.set_title('Topic/Model KDE')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## High ranked topics","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"def get_topics_tbl(X, n_topics=10, n_words=5, best=True):\n    columns = []\n    tbl_data = dict()\n    for name, top_topics, model in X:\n        if best:\n            top_topics = top_topics[:n_topics]\n        else:\n            top_topics = top_topics[-n_topics:]\n            \n        for tid, coherence in top_topics:\n            words = '\\n'.join([w for w,p in model.show_topic(tid, topn=n_words)])\n            \n            if not name in tbl_data:\n                tbl_data[name] = {'Coherence': [],\n                                  'Topics': []}\n            \n            tbl_data[name]['Coherence'].append(coherence)\n            tbl_data[name]['Topics'].append(words)\n        \n    # Pandas: Multilevel column names\n    # https://stackoverflow.com/questions/21443963/pandas-multilevel-column-names\n    d = {k:pd.DataFrame(v) for k,v in tbl_data.items()}\n    \n    return pd.concat(d, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"df = get_topics_tbl([('NMF', nmf_top_topics, nmf_model),\n                     ('LDI', lda_top_topics, lda_model),\n                     ('LSI', lsi_top_topics, lsimodel)],\n                   best=True)\n\ndf_style = (df.style\n            .set_properties(**{\n                'text-align': 'left',\n                'white-space': 'pre-wrap'})\n            .set_table_styles([dict(selector='th',\n                                    props=[('text-align', 'center')])]))\n\nHTML('<center>' + df_style.render() + '</center>')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Low ranked topics","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"df = get_topics_tbl([('NMF', nmf_top_topics, nmf_model),\n                     ('LDI', lda_top_topics, lda_model),\n                     ('LSI', lsi_top_topics, lsimodel)],\n                   best=False)\n\ndf_style = (df.style\n            .set_properties(**{\n                'text-align': 'left',\n                'white-space': 'pre-wrap'})\n            .set_table_styles([dict(selector='th',\n                                    props=[('text-align', 'center')])]))\n\nHTML('<center>' + df_style.render() + '</center>')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble Model (EnM)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The Ensemble Model (EnM) ranks documents according to the summation of weighted similarity values that are computed by constituent indexing models. [[1]](#r1)\n\n","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"ranker_enm = rankers.Ranker_EnM({#'QL':ranker_ql,\n                                 'TFIDF':ranker_tfidf,\n                                 'LDI':ranker_ldi,\n                                 #'LSI':ranker_lsi,\n                                 'NMF':ranker_nmf}, renorm=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ensemble Model Boosting (EnM.B)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Algorithm EnM.B is developed within the boosting scheme that utilizes the competition between constituent models and training data sets to iteratively update the weights until the game reaches equilibrium. [[1]](#r1)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"***\n$\\mathbf{\\text{EnM.B: A boosting algorithm for training the EnM.}}$<br>\n***\n**Require:** Query set $Q$, a set of basis models $\\Phi$ a duplicate set of models $\\Phi^\\prime = \\Phi$.  \n&emsp;Initialized weights $\\mathfrak{D}^1$ over all queries with uniform distribution, i.e., $\\mathfrak{D}^1 = 1/|Q|$.  \n&emsp;Initialize $\\alpha's$ with zeros.  \n&emsp;Set the initial performance measure $E_0$.\n\n\n&emsp;**while** $|E^t - E^{t-1}| > \\epsilon$ **do**  \n&emsp;&emsp;**if** $\\Phi^\\prime = \\emptyset$ **then**  \n&emsp;&emsp;&emsp;$\\Phi^\\prime = \\Phi$;  \n&emsp;&emsp;**end if**  \n&emsp;&emsp;Select basis models $\\phi^t \\in \\Phi^\\prime $ with weights $\\mathfrak{D}^t$ on training queries using:  \n$$j^{*} = \\underset{j}{\\textrm{arg max}}\\sum \\limits_{i=1} ^{|Q|} \\mathfrak{D}_i P(\\phi_{ji});$$  \n\n&emsp;&emsp;Update the weight $\\alpha = \\alpha + \\delta_{i}*e_{j^{*}}$ using using:  \n$$\\delta_{j} = \\frac{1}{2} log \\frac{\\sum \\limits_{i=1} ^{|Q|} \\mathfrak{D}_i (1+P(\\phi_{ji}))}{\\sum \\limits_{i=1} ^{|Q|} \\mathfrak{D}_i (1-P(\\phi_{ji}))};$$  \n\n&emsp;&emsp;Compute the $MP$ $E^t$ with $EnM$ $H^t$;  \n\n&emsp;&emsp;**if** $|E^t - E^{t-1}| > \\epsilon$ **then**  \n&emsp;&emsp;&emsp;$\\Phi^\\prime = \\Phi^\\prime \\setminus \\phi^\\prime;$  \n&emsp;&emsp;&emsp;Update $\\mathfrak{D}^{t+1}$ using:  \n$$\\mathfrak{D}_{i} = \\frac {exp(-P(h_i))}{Z}, Z = \\sum \\limits_{i=1} ^{|Q|} exp(-P(h_i));$$\n&emsp;&emsp;**end if**  \n&emsp;**end while**  \n&emsp;return $EnM$ $H$.\n***","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Useful links:\n- [Writing Math Equations in Jupyter Notebook: A Naive Introduction](https://medium.com/analytics-vidhya/writing-math-equations-in-jupyter-notebook-a-naive-introduction-a5ce87b9a214)\n- [Math symbols defined by LaTeX package «amsfonts»](http://milde.users.sourceforge.net/LUCR/Math/mathpackages/amsfonts-symbols.pdf)\n- [7 Essential Tips for Writing With Jupyter Notebook](https://towardsdatascience.com/7-essential-tips-for-writing-with-jupyter-notebook-60972a1a8901)","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"def EnM_b(ranker_enm, queries_corpus, kind='cs', max_iters=150, eps=1e-6):\n    models = ranker_enm.models_name\n    \n    models_dup = models.copy()\n    \n    alpha = np.zeros(len(models))\n    q_weight = np.ones(len(queries_corpus)) / len(queries_corpus)\n    \n    query_likelihood = ranker_ql[queries_corpus]\n    _ = ranker_enm[queries_corpus]\n    \n    performances = compute_queries_perf(corpus, queries_corpus, ranker_enm.scores, query_likelihood,\n                                        kind=kind,\n                                        rm1_lambda=config.rm1_lambda, rm3_lambda=config.rm3_lambda)\n    print('Models:', models)\n    print('Queries perf:\\n')\n    with np.printoptions(precision=2):\n        print(performances)\n    print('Winner:', np.argmax(performances, 0))\n    print('')\n    \n    E_prev = 0\n    E_best = 0\n    E_hist = []\n    delta = []\n    iteration = 0\n    \n    while True:\n        if len(models_dup) == 0:\n            models_dup = models.copy()\n        \n        #Eq 37\n        dup_performances = performances[ranker_enm.get_models_index(models_dup)]\n        e = (dup_performances*q_weight).sum(1)\n        \n        j_star = np.argmax(e)\n        \n        #Eq 35\n        sigma = 0.5*math.log((q_weight*(1+dup_performances[j_star,:])).sum() / \n                             (q_weight*(1-dup_performances[j_star,:])).sum() )\n        \n        g_j_star = ranker_enm.get_model_index(models_dup[j_star])\n        alpha[g_j_star] += sigma\n        \n        ranker_enm.set_alpha(alpha)\n        enm_perf = compute_queries_perf(corpus, queries_corpus, ranker_enm.combine_scores(), \n                                        query_likelihood,\n                                        kind=kind,\n                                        rm1_lambda=config.rm1_lambda, rm3_lambda=config.rm3_lambda)\n        \n        E = enm_perf.mean()\n        E_hist.append(E)\n        \n        if E>E_best:\n            E_best = E\n        \n        print(f'iter:{iteration:4d} E:{E:.6f} E_best:{E_best:.6f} j_star:{g_j_star} alpha:{alpha}')\n        \n        if abs(E - E_prev) > eps and iteration<max_iters:\n            E_prev = E\n            del models_dup[j_star]\n            #Eq 33\n            q_weight = np.exp(-enm_perf)\n            q_weight[:] /= q_weight.sum()\n        else:\n            # Maybe (E - E_prev) < eps should be true for all models, before we break the loop\n            break\n        \n        iteration += 1\n        \n    alpha = alpha/sum(alpha)\n    \n    return alpha, E_hist","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"%%time\n\nalpha, E_hist = EnM_b(ranker_enm, queries_corpus, kind='uef', max_iters=151)\nranker_enm.set_alpha(alpha)\n\nsave(ranker_enm, config.ENM_FN)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"df=pd.DataFrame([alpha], columns=ranker_enm.models_name)\n\nhtml = (df.style\n        .format('{:.2f}')\n        .hide_index()\n        .set_caption(\"Normalized alpha\")\n        .render())\ndisplay(HTML('<center>'+html+'</center>'))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"plt.figure(figsize=[5,4])\nplt.plot(np.asarray(E_hist))\nplt.xlabel('round')\nplt.ylabel('mean UEF(cs)')\nplt.title('Learning curve of EnM.B')\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results\n\nWe put some links of the SE results into this [page](https://www.kaggle.com/atmarouane/covid-19-se-results-dispatcher), it will be updated frequently.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# TODO\n\n1. There is a serious high quality list of human judgements about relevancy on more than 40 queries on CORD-19, at [TREC-COVID](https://ir.nist.gov/covidSubmit/index.html), it's better to switch the metric from **UEF(Clarity) to MAP**;\n2. Review the code, to more simple and packed one;\n3. Use best practices to rearange and comment the code.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# References\n\n1. <a id=\"r1\"></a>[Indexing by Latent Dirichlet Allocation and Ensemble Model](https://arxiv.org/pdf/1309.3421.pdf)\n2. <a id=\"r2\"></a>[Predicting Query Performance](https://dl.acm.org/doi/pdf/10.1145/564376.564429)\n3. <a id=\"r3\"></a>[Using Statistical Decision Theory and Relevance Models\nfor Query-Performance Prediction](https://dl.acm.org/doi/pdf/10.1145/1835449.1835494)\n4. <a id=\"r4\"></a>[Language models for information retrieval](https://nlp.stanford.edu/IR-book/pdf/12lmodel.pdf)\n5. <a id=\"r5\"></a>[Topic Models and Its Applications](https://staff.fnwi.uva.nl/e.kanoulas/wp-content/uploads/Lecture-5-1-Topic-Models-for-IR.pdf)\n6. <a id=\"r6\"></a>[Lecture 10: Query Expansion & Relevance Feedback](http://people.cs.vt.edu/~jiepu/cs5604_fall2018/10_qm.pdf)\n7. <a id=\"r7\"></a>[Relevance-Based Language Models](https://dl.acm.org/doi/pdf/10.1145/383952.383972)\n8. <a id=\"r8\"></a>[LDA-Based Document Models for Ad-hoc Retrieval](http://ciir.cs.umass.edu/pubfiles/ir-464.pdf)\n9. <a id=\"r9\"></a>[A Study of Smoothing Methods for Language Models\nApplied to Ad Hoc Information Retrieval](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.94.8019&rep=rep1&type=pdf)\n10. <a id=\"r10\"></a>[Online Learning for Latent Dirichlet Allocation](https://www.di.ens.fr/~fbach/mdhnips2010.pdf)\n11. <a id=\"r11\"></a>[Exploring the Space of Topic Coherence Measures](http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf)\n12. <a id=\"r12\"></a>[Rethinking LDA: Why Priors Matter](https://people.cs.umass.edu/~wallach/publications/wallach09rethinking.pdf)\n13. <a id=\"r13\"></a>[UMass at TREC 2004: Novelty and HARD](https://trec.nist.gov/pubs/trec13/papers/umass.novelty.hard.pdf)\n14. <a id=\"r14\"></a>[Latent semantic indexing](https://en.wikipedia.org/wiki/Latent_semantic_analysis#Latent_semantic_indexing)\n15. <a id=\"r15\"></a>[Non-negative matrix factorization](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization)\n","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"from IPython.display import HTML\n\nwith open(config.TOC2_FN, 'r') as file:\n    js = file.read()\n\n    display(HTML('<script type=\"text/Javascript\">'+js+'</script>'))\n    \n    del js","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"%%javascript\n\n// Autonumbering & Table of Contents\n// Using: https://github.com/ipython-contrib/jupyter_contrib_nbextensions/tree/master/src/jupyter_contrib_nbextensions/nbextensions/toc2\ntable_of_contents(default_cfg);\n\n// We solve a unknow problem, sometimes in ‘notebookviewer.js’ in the function ‘setAnchorsToScrollIntoView’, 't.hash' is undefined.\nArray.from(document.getElementsByTagName(\"a\")).forEach(function(t) {\n    var e = t.hash;\n    if (e === undefined) {\n        t.hash = '';\n    }\n})","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}