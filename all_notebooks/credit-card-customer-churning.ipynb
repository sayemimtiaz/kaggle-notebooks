{"cells":[{"metadata":{},"cell_type":"markdown","source":"**1. Introduction**\n\nThis report will showcase my data science skills by analysing data from Kaggle published recently where a bank manager is disturbed by the trend where more customers leaving their credit card services. They want to know how they can predict this in the future so they can proactively approach customers to prevent them from leaving. \n\nThis is a classic business issue that happens in all industries and I will explore the customer behaviours to get more insights as well as applying the predictive modelling.\n\n"},{"metadata":{},"cell_type":"markdown","source":"**2. Dataset and Features**\n\nThe dataset is not large with 10,127 rows and 24 columns. There are 23 features consisting of 6 objects and the remaining are either integer or float. It is a clean dataset with no null values encountered. \n\nFrom the information given, we also know the attrition rate is 16% in the dataset. This presents a problem with imbalance data to do accurate predictive modelling. I will consider this issue later after applying the predictive modelling and reviewing the final accuracy score. \n\nSome column descriptions are self-explanatory and the rest can be seen below :  \n\n- **Attrition_Flag**           : flagging existing-customers vs lost-customers \n- **Card_Category**            : card products\n- **Months_on_book**           : months customers with the bank\n- **Total_Relationship_Count** : total products owned by customers \n- **Months_Inactive_12_mon**   : months of inactivity by customers \n- **Contacts_Count_12_mon**    : frequency of customer contacts in the last 12 months\n- **Total_Revolving_Bal**      : outstanding card balance\n- **Avg_Open_To_Buy**          : opportunity to open new credit limit\n- **Total_Trans_Amt**          : total transactions in the last 12 months\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ndataset = pd.read_csv('/kaggle/input/credit-card-customers/BankChurners.csv')\ndataset = dataset.drop([ 'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\n        'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['Customer_Age', 'Credit_Limit' ,'Total_Revolving_Bal'\n           ,'Total_Trans_Amt','Avg_Utilization_Ratio','Months_Inactive_12_mon'\n          ]\n\n\nnorow = int(round(len(columns)/3,0))\n\n\nfiq, ax = plt.subplots(norow, 3, figsize=(15,9))\n\n\nplt.suptitle('Histograms of Numerical Columns\\n',horizontalalignment=\"center\",fontstyle = \"normal\", fontsize = 24, fontfamily = \"sans-serif\")\n\nfor i,d in enumerate(columns):\n     #print (i,d)\n     if i < 3:\n         chart = sns.histplot(x=dataset.loc[:,d], data=dataset, ax=ax[0,i], hue='Attrition_Flag')\n     else:\n         chart = sns.histplot(x=dataset.loc[:,d], data=dataset, ax=ax[1,i-6], hue='Attrition_Flag')   \n        \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2.1 Exploratory Data Analysis Numerical Columns**\n\nFurther exploration from numerical columns showing some insights on the relationships between attrition flag and other features.\n\n- Customer age does not show any specific behaviours as both histograms are in line in terms of distribution.\n- The credit limit, however, shows attrition tends to occur to those customers with credit limit less 10,000.\n- Customers with zero revolving balance also have high attrition as well as customers with 2,500 revolving balance. \n- There is no attrition for customers with total transaction amounts greater than 12,500.\n- Attrition is high for those customers with zero utilisation ratio.\n- Attrition is also high when customers have been inactive for two and three months.\n\n\n    \nFrom the above observations, we can see some interesting patterns and make some assumptions why they have more customers leaving the bank. Those customers seem to have not used their cards for more than two months and they make up the numbers in zero utilisation ratio as well as zero revolving balance. \n\n\n\nThere are still more questions to ask based on the facts we see above such as :\n\n- Those customers who left, how long have they been with the bank?\n- How many products do they have?\n- Have we made enough contacts with them to retain them?\n- Do they response well with the column average open to buy?\n- Is there specific cohort based on age, dependent?\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# step 1 split the tables\natt_cust = dataset[dataset['Attrition_Flag']=='Attrited Customer']\natt = att_cust[['Customer_Age','Months_on_book','Total_Relationship_Count'\n         ,'Months_Inactive_12_mon','Contacts_Count_12_mon','Credit_Limit','Avg_Open_To_Buy'\n        ,'Dependent_count']]\natt.columns=['Customer Age','Months on Book','Number of Products','Months Inactive','Contacts 12 Mths','Credit Limit'\n          ,'Avg Open to New Credit','No of Dependents']\n\n\nreg = dataset[dataset['Attrition_Flag']!='Attrited Customer']\nreg = reg[['Customer_Age','Months_on_book','Total_Relationship_Count'\n         ,'Months_Inactive_12_mon','Contacts_Count_12_mon','Credit_Limit','Avg_Open_To_Buy'\n        ,'Dependent_count']]\nreg.columns=['Customer Age','Months on Book','Number of Products','Months Inactive','Contacts 12 Mths','Credit Limit'\n          ,'Avg Open to New Credit','No of Dependents']\n\n\n# step 2 create two panda series\na = att.describe()\na =a.loc['mean',['Customer Age','Months on Book','Number of Products','Months Inactive','Contacts 12 Mths','Credit Limit'\n          ,'Avg Open to New Credit','No of Dependents']]\n\n\nb = reg.describe()\nb =b.loc['mean',['Customer Age','Months on Book','Number of Products','Months Inactive','Contacts 12 Mths','Credit Limit'\n          ,'Avg Open to New Credit','No of Dependents']]\n        \nframe = { 'Lost-Customers': a, 'Existing-Customers': b } \n  \nresult = pd.DataFrame(frame) \npd.options.display.float_format = '{:,.2f}'.format\nresult\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The table above shows the average for different features between existing-customers and lost-customers to help us answering some of the previous questions.\n\n- On average, lost-customers have been with the bank about 36 months and in line with existing ones.\n- We actually make similar contacts with these customers compared to existing ones. \n- They are also open to similar new credit limit. \n- And on average, they also have similar dependent so there is no specific cohort on attrition.\n\nAs per previous assumption, we do know these customers have been inactive for quite a while before they close their cards. Another interesting fact from the above comparison, the existing customers also have an average inactivity of 2.27 months. \n\n>The bank manager needs to be wary there are some existing customers that might follow to leave the bank.\n"},{"metadata":{},"cell_type":"markdown","source":"**Customers Churned Transactions vs Revolving Balance**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(12,9))\nsns.scatterplot(y='Total_Revolving_Bal',x='Total_Trans_Amt',data=att_cust)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.mode.chained_assignment = None\n\natt_cust.loc[att_cust.Total_Revolving_Bal==0,'Status'] = 'Nil Balance'\natt_cust.loc[att_cust.Total_Revolving_Bal!=0,'Status'] = 'Default'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above, we can see a cluster among the attrited customers where a lot of them made less than 4,000 transaction amounts and they have revolving balances in their account. That means these customers have defaulted and left the bank. \n\nFurther grouping between zero and non-zero balances showing 734 of these customers have defaulted in their payments whilst remaining 893 customers have left with no outstanding amounts due. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(att_cust.groupby('Status')['Total_Trans_Ct'].count().reset_index())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">This give an insight to bank manager that almost half of the attrited customers have left because they have defaulted in their overdue payments. Higher attrition due to bad customers leaving probably is not a bad thing."},{"metadata":{},"cell_type":"markdown","source":"**2.2 Exploratory Data Analysis Categorical Columns**"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['Gender','Education_Level','Marital_Status','Income_Category','Card_Category','Attrition_Flag']\n\n\nnorow = int(round(len(columns)/3,0))\n\nfiq, ax = plt.subplots(norow, 3, figsize=(16,13))\n\n\nplt.suptitle('Histograms of Categorical Columns\\n',horizontalalignment=\"center\",fontstyle = \"normal\", fontsize = 24, fontfamily = \"sans-serif\")\n\nfor i,d in enumerate(columns):\n     #print (i,d)\n     if i < 3:\n         chart = sns.countplot(x=dataset.loc[:,d], data=dataset, order = dataset.loc[:,d].value_counts().index, ax=ax[0,i])\n         sns.set(font_scale = 1)\n         if dataset.loc[:,d].nunique() > 3: \n             chart.set_xticklabels(chart.get_xticklabels(), rotation=90)  \n\n     else:\n         chart = sns.countplot(x=dataset.loc[:,d], data=dataset, order = dataset.loc[:,d].value_counts().index, ax=ax[1,i-6])\n         sns.set(font_scale = 1) \n         if dataset.loc[:,d].nunique() > 3: \n             chart.set_xticklabels(chart.get_xticklabels(), rotation=90)\n            \n        \n        #chart.set(title='Tips Bar Plot Default (Avg)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see further information about the customers from the categorical columns.\n- Based on gender, we have almost equal split between males and females.\n- Based on education, we can also see we have significant customers with graduate degrees.\n- Marital status is also closely split between single and married.\n- Another interesting fact, we have significant customers with salary less than 40,000.\n- We have four product categories but majority of customers signed up with Blue.\n- The actual customer that moved out is 1,627 customers out 8,500 existing customers (16% attrition).\n    \nWe are going to look at further the profiles of the past-customers vs existing-customers to see the if compositions are different. \n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"default = att_cust[att_cust['Status']=='Default']\nzerobal = att_cust[att_cust['Status']!='Default']\ncolumns = ['Gender','Education_Level','Income_Category','Marital_Status']\n\n\nfiq, ax = plt.subplots(4, 2, figsize=(13,20))\n\nfor i,d in enumerate(columns):\n       \n        #### create pctg table 1 and plot to col 1####\n        a = default[[d,'Customer_Age']].groupby(d,as_index=False).count()\n        total = default[d].count()\n        a['Total'] = total\n        a['Pctg'] = a['Customer_Age']/a['Total']\n        sizes = a['Pctg']\n        labels = a[d]\n        ax[i,0].pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90)\n        ax[i,0].set_title(d + '- Default Cust',fontweight=\"bold\")\n        \n        \nfor i,d in enumerate(columns):\n       \n        #### create pctg table 2 and plot to col 2####\n        a = zerobal[[d,'Customer_Age']].groupby(d,as_index=False).count()\n        total = zerobal[d].count()\n        a['Total'] = total\n        a['Pctg'] = a['Customer_Age']/a['Total']\n        sizes = a['Pctg']\n        labels = a[d]\n        ax[i,1].pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90)  \n        ax[i,1].set_title(d + '- Zero Bal Cust', fontweight=\"bold\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the observations above there are some interesting findings for customers who churned.\n\n- Male composition is higher by ~2% from customers with zero revolving balance.\n- Some percentage increases in graduate, postgraduate and doctorate.\n- Customers with 80k-120k salary goes up by ~3%\n"},{"metadata":{},"cell_type":"markdown","source":"**3. Prediction Machine Learning**\n\n\nWe have seen some of the insights and assumptions regarding why the customers have left. We know half of those customers have defaulted in their payments but we also know there are existing customers who have been inactive which may end up leaving the bank. \n\nWe are going to predict those customers who will churn using random forests which are widely used for classifiction problem like this one. We will see if the final prediction of random observations, rows as well as multiple trees will tackle the imbalance issue.\n\nThe dataset will be split 70% for training and remaining 30% will be data without label to determine the accurancy of the model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\n\n\n# Step 1 Convert object columns to category and converting them to numerical values\n\nstart = time.time()\n\ndf = dataset\nother = df.select_dtypes(exclude=['category','int64','float64']).columns\n\n# convert other to category\nfor i in other:\n    df[i]=df[i].astype('category')\n\n# pick up those category columns only and apply the cat codes (numeric)    \ncat_columns = df.select_dtypes(include='category').columns\ndf[cat_columns]=df[cat_columns].apply(lambda x:x.cat.codes)\n\n\n\n# Step 2 Split the dataset to train/test\n\nfrom sklearn.model_selection import train_test_split\n\nnumeric_col = ['Customer_Age','Dependent_count','Months_on_book','Total_Relationship_Count'\n               ,'Months_Inactive_12_mon','Contacts_Count_12_mon','Credit_Limit'\n               ,'Total_Revolving_Bal','Total_Trans_Amt','Total_Trans_Ct','Avg_Utilization_Ratio']\n\nX = dataset[numeric_col]\ny = dataset['Attrition_Flag']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.7, random_state=42)\n\n\n\n# Step 3 Apply Random Forest using some random variables (finetuning later)\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(min_samples_split=5,n_estimators=100, random_state=20)\nmodel.fit(X_train, y_train)\n\nprediction = model.predict(X_test)\n\nduration = round(time.time() - start,2)\n\nprint ('It took {} seconds to process'.format(duration))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\ntarget_names = ['Lost-Customers','Existing-Customers']   # remember the order is 0,1  hence the labelling follow that order\n\nprint(classification_report(y_test,prediction,target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The result of the random forest is good with 95% accuracy and F1 score of 84% for the determining customers who will churn. It seems that we do not have imbalance problem with our model at this stage.\nYou can also see in the following confusion matrix where the model made incorrect prediction. We have 40 customers incorrectly categorised as existing-customers and 108 customers categorised as lost-customers."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport numpy as np\n\ncm = confusion_matrix(y_test, prediction) \ndf_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))\n#plt.figure(figsize = (28,20))\nfig, ax = plt.subplots()\nsns.set(font_scale=1.4)\nsns.heatmap(df_cm, annot=True, fmt='g'#,cmap=\"YlGnBu\" \n           )\nclass_names=[0,1]\ntick_marks = np.arange(len(class_names))\nplt.tight_layout()\nplt.title('Confusion matrix\\n', y=1.1)\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\nax.xaxis.set_label_position(\"top\")\nplt.ylabel('Actual label\\n')\nplt.xlabel('Predicted label\\n')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will try to fine-tune this model by finding the optimal values in some of the parameters used. The fine-tuning will look at the best parameters for min samples split and the numbers of trees in the model. In the following result, fine-tuning improves the result to 96% and recommends to use min samples split of ten and using multiple trees of 50. Now, we are going to use the fine-tuned model to predict X_test and see if the accuracy will improve."},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nfrom sklearn.model_selection import GridSearchCV\nparameters = {'min_samples_split':[5,10,15,20], 'n_estimators':[50, 100, 150]}\n\n\nstart = time.time()\n\ntuning = GridSearchCV(model,parameters,cv=10)\ntuning.fit(X_train,y_train)\n\nduration = round(time.time() - start)\n\nprint ('It took {} seconds'.format(duration))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_accuracy = round(tuning.best_score_,2)\nbest_parameters = tuning.best_params_\nbest_accuracy, best_parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction2 =  tuning.predict(X_test)\n\nfrom sklearn.metrics import classification_report\n\ntarget_names = ['Lost-Customers','Existing-Customers']   # remember the order is 0,1  hence the labelling follow that order\n\nprint(classification_report(y_test,prediction2,target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\ncm = confusion_matrix(y_test, prediction2) \ndf_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))\n#plt.figure(figsize = (28,20))\nfig, ax = plt.subplots()\nsns.set(font_scale=1.4)\nsns.heatmap(df_cm, annot=True, fmt='g'#,cmap=\"YlGnBu\" \n           )\nclass_names=[0,1]\ntick_marks = np.arange(len(class_names))\nplt.tight_layout()\nplt.title('Confusion matrix\\n', y=1.1)\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\nax.xaxis.set_label_position(\"top\")\nplt.ylabel('Actual label\\n')\nplt.xlabel('Predicted label\\n')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the results pretty much in line with previous model before fine-tuning but this has been cross-validated and can generalise better against a new dataset.\nYou can also see from the ROC curve below that our model works well and trending above the mid-point which means it can distinguish the classification problems well."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve\nmodel.fit(X_train, y_train) \nprobs = model.predict_proba(X_test) \nprobs = probs[:, 1] \nclassifier_roc_auc = accuracy_score(y_test, prediction )\nrf_fpr, rf_tpr, rf_thresholds = roc_curve(y_test, model.predict_proba(X_test)[:,1])\n\n\n# plot the figures\nplt.figure(figsize=(14, 6))\n\nplt.plot(rf_fpr, rf_tpr, \nlabel='Random Forest Area (area = %0.2f)' % classifier_roc_auc)\n\n# Plot Base Rate ROC\nplt.plot([0,1], [0,1],label='Mid Point')\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC Curve')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}