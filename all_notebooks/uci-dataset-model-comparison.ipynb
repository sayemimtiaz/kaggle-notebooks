{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Plotting some aspects of data and comparing classification models on UCI Heart Disease Dataset**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#import necessary packages\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nprint(tf.__version__)\nfrom keras.regularizers import l2\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\n#from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\n\n# import warnings filter\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We import the data and explore it by making some graphics using seaborn"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"file_path =\"../input/heart.csv\"\ndata = pd.read_csv(file_path) \nprint(data.shape)\nd=data.transpose()\nprint(data.describe())\n\nprint(data.head())\nprint(data.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The columns represent the following information.\n\n**age**: age in years \n\n**sex**: (1 = male; 0 = female) \n\n**cp**: chest pain type (0-4)\n\n**trestbps**: resting blood pressure (in mm Hg on admission to the hospital) \n\n**chol**: serum cholestoral in mg/dl \n\n**fbs**: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) \n\n**restecg**: resting electrocardiographic results \n\n**thalach**: maximum heart rate achieved \n\n**exang**: exercise induced angina (1 = yes; 0 = no) \n\n**oldpeak**: ST depression induced by exercise relative to rest \n\n**slope**: Slope of the peak exercise ST segment \n\n**ca**: number of major vessels (0-3) colored by flourosopy \n\n**thal3** = normal; 6 = fixed defect; 7 = reversable defect \n\n**target**: 1 or 0 (presence or absence of heart disease)"},{"metadata":{},"cell_type":"markdown","source":"Lets use Seaborn's pair plot to get an idea of how certain columns are correlated and distributed against each other"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotdata=data[['age','chol','thalach','trestbps','target']]\n\nplotdata['target'][plotdata['target']==0]='no heart disease present (0)'\nplotdata['target'][plotdata['target']==1]='heart disease present (1)'\n\nsns.pairplot(plotdata, hue='target')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We continue with a relative plot of resting blood pressure (Trestbps), maximum heart rate achieved (Thalach), and serum cholestoral (Chol)"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.relplot(x='trestbps', y=\"thalach\", size=\"chol\", sizes=(15, 200),data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Lets do a factor plot between age and chest pain type.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot('cp','age',data=data,\n                   hue='target',\n                   size=6,\n                   aspect=0.8,\n                   palette='magma',\n                   join=False,\n              )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems like chest pain of types 1 and 2 are the best at differentiating the target with respect to age."},{"metadata":{},"cell_type":"markdown","source":"Let's check how the age is distributed in the data. we will only consider the 15 highest ranking positions."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsns.barplot(x=data.age.value_counts()[:15].index,y=data.age.value_counts()[:15].values)\nplt.xlabel('Age')\nplt.ylabel('Number of people')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally let's do a correlation heat map"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(9,9))\nsns.heatmap(data.corr(),annot=True,fmt='.1f',cmap=\"YlGnBu\",annot_kws = {'size':10})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It appears that no two different features are highly correlated, so lets just proceed with building the models."},{"metadata":{},"cell_type":"markdown","source":"Data preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"#separate Data versus target\ny = data.target\ny = y.astype('float64') \n#We drop the sex as it is a binary categorical feature that interferes with the training of the model\nX=data.drop(['target','sex'], axis=1)\n# 'age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n#        'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'\n#Are there missing values?\nprint('Is there any missing data: %s' %data.isnull().values.any())\n\n\n#Separate training data from test data\nX_train, X_test, y_train, y_test =train_test_split(X, y, train_size=0.8, test_size=0.2, shuffle=True,\n                                                      random_state=0)\nX_stats=X_train.describe()\nX_stats=X_stats.transpose()\n\n\n\n#I would like to start with a simple neural network, so lets renormalize the data we will feed to it. \ndef norm(x):\n    return ((x-X_stats['mean'])/X_stats['std'])\nn_X_train=norm(X_train)\nn_X_test=norm(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Neural network  model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#We use l2 regularization to prevent overfitting\ndef Build_Model():\n    model=keras.Sequential([\n        layers.Dense(64,activation=tf.nn.relu,kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), input_shape=[len(n_X_train.keys())]),\n        layers.Dense(64,activation=tf.nn.relu,kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)),\n        layers.Dense(1,activation=tf.nn.sigmoid)\n    ])\n\n\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['binary_accuracy'])\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Build_Model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"We train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS=500\nhistory=model.fit(n_X_train,y_train,epochs=EPOCHS,validation_split=0.2,verbose=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting the history of training"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef plot_history(history):\n    hist=pd.DataFrame(history.history)\n    hist['epoch']=history.epoch\n    \n    plt.figure()\n    plt.xlabel('Epoch')\n    plt.ylabel('cross_entropy')\n    plt.plot(hist['epoch'],hist['loss'],label='Train Error')\n    plt.plot(hist['epoch'],hist['val_loss'], label='Val Error')\n    plt.legend()\n    \n    plt.figure()\n    \nplot_history(history)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reduce Overfitting with early stopping"},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Build_Model()\nearly_stop=keras.callbacks.EarlyStopping(monitor='val_loss',patience=10)\nhistory=model.fit(n_X_train,y_train,epochs=EPOCHS,validation_split=0.2,verbose=0,callbacks=[early_stop])\nplot_history(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, accu=model.evaluate(n_X_test,y_test,verbose=1)\nprint(accu)\nprint('Testing accuracy:%0.2f'%(accu))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pretty Good! Now lets make some predictions and get a confusion matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions=model.predict_classes(n_X_test)\ntest_predictions_probability=model.predict(n_X_test)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Recall that the confusion matrix is given by \n\\begin{pmatrix} \n\\textbf{True Positives} & \\textbf{False Positives} \\\\\n\\textbf{False Negatives} & \\textbf{True Negatives}\n\\end{pmatrix} "},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nprint('Confusion matrix:')\nprint(metrics.confusion_matrix(y_test, test_predictions))\nconfusion=metrics.confusion_matrix(y_test, test_predictions)\nTP = confusion[1, 1]\nTN = confusion[0, 0]\nFP = confusion[0, 1]\nFN = confusion[1, 0]\n\n#accuracy\nprint('Accuracy:')\nprint((TP + TN) / float(TP + TN + FP + FN))\n#print(metrics.accuracy_score(y_test, test_predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We plot an ROC curve to evaluate the model. Recall that $$\\textbf{False positive rate}=\\frac{FP}{FP+TN}$$ and $$\\textbf{True positive rate}=\\frac{TP}{TP+FN}$$"},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = metrics.roc_curve(y_test, test_predictions_probability)\n\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for heart disease')\nplt.xlabel('False Positive Rate ')\nplt.ylabel('True Positive Rate ')\nplt.grid(True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We find the area under the curve for this model"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.roc_auc_score(y_test, test_predictions_probability))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is very close to 1 which is a good indicative."},{"metadata":{},"cell_type":"markdown","source":"Finally we compare the following models with our neural network: XG Boost, Logistic Regression, Naive Bayes,\nRandom Forest and Decision Tree Bagging."},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef Build_Model_and_ROC(model,name,  X_train, X_test, y_train, y_test):\n    model.fit(X_train, y_train)\n    y_pred=model.predict(X_test)\n    test_predictions_probability=model.predict_proba(X_test)[:,1]\n    \n    #Get the confusion matrix of the classifier\n    confusion=metrics.confusion_matrix(y_test, y_pred)\n    TP = confusion[1, 1]\n    TN = confusion[0, 0]\n    FP = confusion[0, 1]\n    FN = confusion[1, 0]\n    print('Confusion Matrix for '+ name +':')\n    print(confusion)\n   \n    #ROC curve\n    fpr, tpr, thresholds = metrics.roc_curve(y_test, test_predictions_probability)\n\n    #Plotting ROC curve\n  \n    plt.plot(fpr, tpr, label=name)\n    \n\n    print(name+\" Number of mislabeled points out of a total %d points  : %d\" %(X_test.shape[0],(y_test != y_pred).sum()))\n    print( 'Accuracy of '+ name+ ' : %f'%(metrics.accuracy_score(y_test, y_pred)))\n    print('auc score of '+ name+ ' : %f'%( metrics.roc_auc_score(y_test, test_predictions_probability)))\n    print('\\n')\n  \n   \n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.rcParams['font.size'] = 12\n    plt.title('ROC curve for heart disease')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.grid(True)\n    plt.legend(loc=4) \n\nmodels={'XG Boost': XGBClassifier(), 'Logistic Regression': LogisticRegression(), 'Naive Bayes': GaussianNB(),\n        'Random Forest':RandomForestClassifier(max_depth=5), 'Decision Tree Bagging' : BaggingClassifier(DecisionTreeClassifier(),n_estimators=500,max_samples=100 ,\n                         bootstrap=True, n_jobs=-1,oob_score=True)} \nplt.figure(figsize=(20,10))\nfor name, model in models.items():\n    Build_Model_and_ROC( model, name,  X_train, X_test, y_train, y_test)    \n\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All in all these models give similar results  to those of a simple neural network.\n\nThank you for taking the time of reading my Kernel, any comments are very welcome !"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}