{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> A type of image-based sequence recognition problem is an optical character recognition problem. And, for a sequence recognition problem, recurrent neural networks (RNN) are best suited, whereas convolution neural networks (CNN) are best suited for an image-based problem (CNN). To deal with OCR issues, we need to combine both of these CNN and RNN.","metadata":{}},{"cell_type":"markdown","source":"**We can break the implementation of CRNN network into following steps:**\n\n1. Import Data\n\n2. Data Preprocessing\n\n3. Model Building\n\n4. Defining Loss Function\n\n5. Training Model\n\n6. Decoding Outputs from Prediction","metadata":{}},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport os\nimport pandas as pd\nimport string\nimport matplotlib.pyplot as plt\n\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom keras.layers import Dense, LSTM, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional\nfrom keras.models import Model\nfrom keras.activations import relu, sigmoid, softmax\nimport keras.backend as K\n\nfrom keras.utils import to_categorical\nfrom keras.callbacks import ModelCheckpoint\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom PIL import Image\n\nimport tensorflow as tf\n\n#ignore warnings in the output\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Intialize the variable","metadata":{}},{"cell_type":"code","source":"images = []\nlabels = []\n\nRECORDS_COUNT = 25000","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images = []\ntrain_labels = []\ntrain_input_length = []\ntrain_label_length = []\ntrain_original_text = []\n\nvalid_images = []\nvalid_labels = []\nvalid_input_length = []\nvalid_label_length = []\nvalid_original_text = []\n\ninputs_length = []\nlabels_length = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Data","metadata":{}},{"cell_type":"markdown","source":"1. **Personlized Data**","metadata":{}},{"cell_type":"code","source":"engl=pd.read_csv('../input/english-handwritten-characters-dataset/english.csv')\nengl.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. **IAM Dataset**","metadata":{}},{"cell_type":"code","source":"\nwith open('../input/iam-dataset/IAM/words.txt') as f:\n    contents = f.readlines()[18:22539]\n\nlines = [line.strip() for line in contents] \nlines[-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data-Preprocessing","metadata":{}},{"cell_type":"markdown","source":"**1. Image Pre-processing**","metadata":{}},{"cell_type":"code","source":"def process_image(img):\n    \"\"\"\n    Converts image to shape (32, 128, 1) & normalize\n    \"\"\"\n    w, h = img.shape\n    \n#     _, img = cv2.threshold(img, \n#                            128, \n#                            255, \n#                            cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n    \n    # Aspect Ratio Calculation\n    new_w = 32\n    new_h = int(h * (new_w / w))\n    img = cv2.resize(img, (new_h, new_w))\n    w, h = img.shape\n    \n    img = img.astype('float32')\n    \n    # Converts each to (32, 128, 1)\n    if w < 32:\n        add_zeros = np.full((32-w, h), 255)\n        img = np.concatenate((img, add_zeros))\n        w, h = img.shape\n    \n    if h < 128:\n        add_zeros = np.full((w, 128-h), 255)\n        img = np.concatenate((img, add_zeros), axis=1)\n        w, h = img.shape\n        \n    if h > 128 or w > 32:\n        dim = (128,32)\n        img = cv2.resize(img, dim)\n    \n    img = cv2.subtract(255, img)\n    \n    img = np.expand_dims(img, axis=2)\n    \n    # Normalize \n    img = img / 255\n    \n    return img","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2. Label-Encoding**","metadata":{}},{"cell_type":"code","source":"max_label_len = 0\n\nchar_list = \"!\\\"#&'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\" \n\n# string.ascii_letters + string.digits (Chars & Digits)\n# or \n# \"!\\\"#&'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n\nprint(char_list, len(char_list))\n\ndef encode_to_labels(txt):\n    # encoding each output word into digits\n    dig_lst = []\n    for index, chara in enumerate(txt):\n        dig_lst.append(char_list.index(chara))\n        \n    return dig_lst","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Personalized Data","metadata":{}},{"cell_type":"code","source":"for i in range(len(engl)):\n \n        splits_id=engl[\"image\"][i].split('/')\n        filepath = '../input/english-handwritten-characters-dataset/{}/{}'.format(splits_id[0], \n                                                  splits_id[1])\n       \n        # process image\n        img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n        try:\n            img = process_image(img)\n        except:\n            continue\n            \n        # process label\n        try:\n            word=engl['label'][i]\n            label = encode_to_labels(word)\n        except:\n            continue\n        \n        if i % 10 == 0:\n            valid_images.append(img)\n            valid_labels.append(label)\n            valid_input_length.append(31)\n            valid_label_length.append(len(word))\n            valid_original_text.append(word)\n        else:\n            train_images.append(img)\n            train_labels.append(label)\n            train_input_length.append(31)\n            train_label_length.append(len(word))\n            train_original_text.append(word)\n        \n        if len(word) > max_label_len:\n            max_label_len = len(word)\n    \n        if i >= RECORDS_COUNT:\n            break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# IAM-Dataset","metadata":{}},{"cell_type":"code","source":"for index, line in enumerate(lines):\n    splits = line.split(' ')\n    status = splits[1]\n    \n    if status == 'ok':\n        word_id = splits[0]\n        word = \"\".join(splits[8:])\n        \n        splits_id = word_id.split('-')\n        filepath = '../input/iam-dataset/IAM/words/{}/{}-{}/{}.png'.format(splits_id[0], \n                                                  splits_id[0], \n                                                  splits_id[1], \n                                                  word_id)\n      \n        # process image\n        img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n        try:\n            img = process_image(img)\n        except:\n            continue\n            \n        # process label\n        try:\n            label = encode_to_labels(word)\n        except:\n            continue\n        \n        if index % 10 == 0:\n            valid_images.append(img)\n            valid_labels.append(label)\n            valid_input_length.append(31)\n            valid_label_length.append(len(word))\n            valid_original_text.append(word)\n        else:\n            train_images.append(img)\n            train_labels.append(label)\n            train_input_length.append(31)\n            train_label_length.append(len(word))\n            train_original_text.append(word)\n        \n        if len(word) > max_label_len:\n            max_label_len = len(word)\n    \n    if index >= RECORDS_COUNT:\n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_padded_label = pad_sequences(train_labels, \n                             maxlen=max_label_len, \n                             padding='post',\n                             value=len(char_list))\n\nvalid_padded_label = pad_sequences(valid_labels, \n                             maxlen=max_label_len, \n                             padding='post',\n                             value=len(char_list))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels[3101]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n> pad each output label to maximum text length","metadata":{}},{"cell_type":"code","source":"train_padded_label[3101]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_padded_label.shape, valid_padded_label.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images = np.asarray(train_images)\ntrain_input_length = np.asarray(train_input_length)\ntrain_label_length = np.asarray(train_label_length)\n\nvalid_images = np.asarray(valid_images)\nvalid_input_length = np.asarray(valid_input_length)\nvalid_label_length = np.asarray(valid_label_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"markdown","source":"**Model Description**\n1. Input shape for our architecture having an input image of height 32 and width 128.\n2. Here we used seven convolution layers of which 6 are having kernel size (3,3) and the last one is of size (2.2). And the number of filters is increased from 64 to 512 layer by layer.\n3. Two max-pooling layers are added with size (2,2) and then two max-pooling layers of size (2,1) are added to extract features with a larger width to predict long texts.\n4. Also, we used batch normalization layers after fifth and sixth convolution layers which accelerates the training process.\n5. Then we used a lambda function to squeeze the output from conv layer and make it compatible with LSTM layer.\n6. Then used two Bidirectional LSTM layers each of which has 128 units. This RNN layer gives the output of size (batch_size, 31, 79). Where 79 is the total number of output classes including blank character.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import layers\nfrom tensorflow.keras import Model\nfrom tensorflow.keras import backend as tf_keras_backend\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n\ntf_keras_backend.set_image_data_format('channels_last')\ntf_keras_backend.image_data_format()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Model1():\n    # input with shape of height=32 and width=128 \n    inputs = Input(shape=(32,128,1))\n\n    # convolution layer with kernel size (3,3)\n    conv_1 = Conv2D(64, (3,3), activation = 'relu', padding='same')(inputs)\n    # poolig layer with kernel size (2,2)\n    pool_1 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_1)\n\n    conv_2 = Conv2D(128, (3,3), activation = 'relu', padding='same')(pool_1)\n    pool_2 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_2)\n\n    conv_3 = Conv2D(256, (3,3), activation = 'relu', padding='same')(pool_2)\n\n    conv_4 = Conv2D(256, (3,3), activation = 'relu', padding='same')(conv_3)\n    # poolig layer with kernel size (2,1)\n    pool_4 = MaxPool2D(pool_size=(2, 1))(conv_4)\n\n    conv_5 = Conv2D(512, (3,3), activation = 'relu', padding='same')(pool_4)\n    # Batch normalization layer\n    batch_norm_5 = BatchNormalization()(conv_5)\n\n    conv_6 = Conv2D(512, (3,3), activation = 'relu', padding='same')(batch_norm_5)\n    batch_norm_6 = BatchNormalization()(conv_6)\n    pool_6 = MaxPool2D(pool_size=(2, 1))(batch_norm_6)\n\n    conv_7 = Conv2D(512, (2,2), activation = 'relu')(pool_6)\n\n    squeezed = Lambda(lambda x: K.squeeze(x, 1))(conv_7)\n\n    # bidirectional LSTM layers with units=128\n    blstm_1 = Bidirectional(LSTM(256, return_sequences=True, dropout = 0.2))(squeezed)\n    blstm_2 = Bidirectional(LSTM(256, return_sequences=True, dropout = 0.2))(blstm_1)\n\n    outputs = Dense(len(char_list)+1, activation = 'softmax')(blstm_2)\n\n    # model to be used at test time\n    act_model = Model(inputs, outputs)\n    \n    return act_model,outputs,inputs\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"act_model,outputs,inputs=Model1()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"act_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"the_labels = Input(name='the_labels', shape=[max_label_len], dtype='float32')\ninput_length = Input(name='input_length', shape=[1], dtype='int64')\nlabel_length = Input(name='label_length', shape=[1], dtype='int64')\n\ndef ctc_lambda_func(args):\n    y_pred, labels, input_length, label_length = args\n    \n    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n\nloss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([outputs, the_labels, input_length, label_length])\n\n#model to be used at training time\nmodel = Model(inputs=[inputs, the_labels, input_length, label_length], outputs=loss_out)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Model","metadata":{}},{"cell_type":"code","source":"batch_size = 5\nepochs = 20\ne = str(epochs)\noptimizer_name = 'sgd'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = optimizer_name, metrics=['accuracy'])\n\nfilepath=\"{}o-{}r-{}e-{}t-{}v.hdf5\".format(optimizer_name,\n                                          str(RECORDS_COUNT),\n                                          str(epochs),\n                                          str(train_images.shape[0]),\n                                          str(valid_images.shape[0]))\n\ncheckpoint = ModelCheckpoint(filepath=filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='auto')\ncallbacks_list = [checkpoint]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(x=[train_images, train_padded_label, train_input_length, train_label_length],\n                    y=np.zeros(len(train_images)),\n                    batch_size=batch_size, \n                    epochs=epochs, \n                    validation_data=([valid_images, valid_padded_label, valid_input_length, valid_label_length], [np.zeros(len(valid_images))]),\n                    verbose=1,\n                    callbacks=callbacks_list)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model.save(filepath='./model3.h5', overwrite=False, include_optimizer=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Performance check","metadata":{}},{"cell_type":"markdown","source":"For computing the performance, I will be using the **Jaro-Winkler** algorithm to detect similarity between the captured text and the actual text.","metadata":{}},{"cell_type":"code","source":"\nfilepath='../input/my-own/sgdo-25000r-20e-21143t-2348v.hdf5'\n# load the saved best model weights\nact_model.load_weights(filepath)\n\n# predict outputs on validation images\nprediction = act_model.predict(valid_images)\n \n# use CTC decoder\ndecoded = K.ctc_decode(prediction, \n                       input_length=np.ones(prediction.shape[0]) * prediction.shape[1],\n                       greedy=True)[0][0]\nout = K.get_value(decoded)\n\nimport Levenshtein as lv\n\ntotal_jaro = 0\n\n# see the results\nfor i, x in enumerate(out):\n    letters=''\n    for p in x:\n        if int(p) != -1:\n            letters+=char_list[int(p)]\n    total_jaro+=lv.jaro(letters, valid_original_text[i])\n  \nprint('jaro :', total_jaro/len(out))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict outputs on validation images\ni=2000\nj=2005\nprediction = act_model.predict(valid_images[i:j])\n \n# use CTC decoder\ndecoded = K.ctc_decode(prediction,   \n                       input_length=np.ones(prediction.shape[0]) * prediction.shape[1],\n                       greedy=True)[0][0]\n\nout = K.get_value(decoded)\n\n# see the results\nfor _, x in enumerate(out):\n    \n    print(\"original_text =  \", valid_original_text[i])\n    print(\"predicted text = \", end = '')\n    for p in x:\n        if int(p) != -1:\n            print(char_list[int(p)], end = '')\n    plt.imshow(valid_images[i].reshape(32,128), cmap=plt.cm.gray)\n    plt.show()\n    i+=1\n    print('\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization of Loss and Acurracy","metadata":{}},{"cell_type":"code","source":"#plot accuracy and loss\ndef plotgraph(epochs, acc, val_acc):\n    # Plot training & validation accuracy values\n    plt.plot(epochs, acc, 'b')\n    plt.plot(epochs, val_acc, 'r')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Val'], loc='upper left')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1,len(loss)+1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotgraph(epochs, loss, val_loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotgraph(epochs, acc, val_acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get best model index\nminimum_val_loss = np.min(history.history['val_loss'])\nbest_model_index = np.where(history.history['val_loss'] == minimum_val_loss)[0][0]\n\nbest_loss = str(history.history['loss'][best_model_index])\nbest_acc = str(history.history['accuracy'][best_model_index])\nbest_val_loss = str(history.history['val_loss'][best_model_index])\nbest_val_acc = str(history.history['val_accuracy'][best_model_index])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(best_loss)\nprint(best_acc)\nprint(best_val_acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting My Own handwriting","metadata":{}},{"cell_type":"code","source":"pip install fpdf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mkdir ./word1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(directory):\n    dir=directory\n    for filename in os.listdir(dir):\n        test=[]\n        filepath=os.path.join(dir, filename)\n        img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n    \n        try:\n            test_img = process_image(img)\n            test.append(test_img)\n            test_img=np.asarray(test)\n        except:\n            print('hi')\n        \n        try:\n            prediction = act_model.predict(test_img)\n\n            # use CTC decoder\n            decoded = K.ctc_decode(prediction,   \n                               input_length=np.ones(prediction.shape[0]) * prediction.shape[1],\n                               greedy=True)[0][0]\n            out = K.get_value(decoded)\n\n            for i, x in enumerate(out):\n                print(\"predicted text = \", end = '')\n                for p in x:\n                    if int(p) != -1:\n                        s=char_list[int(p)]\n                        print(s)\n            img = Image.open(filepath)\n            name = './word1/' + str(s) + '.png'\n            img.save(name, 'PNG')\n            \n            del test[-1]\n        \n        except:\n            print(filename)\n\n        \n    \npredict('../input/my-own/alphabet/char')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Converting voice text to handwritten","metadata":{}},{"cell_type":"code","source":"mkdir ./img2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cp ../input/my-own/space.png  ./word1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\n\nfrom fpdf import FPDF\n\nimg=Image.open(\"../input/my-own/bg.jpg\")\nsizeOfSheet=img.width\ngap,_=0,0\nallowedchar='ABCDEFGHIJKLMNOPQRSTUVWXYZqwertyuiopasdfghjklzxcvbnm(),.?;1234567890'\n\ndef Write(char):\n    if char=='\\n':\n        pass\n    else:\n        global gap,_\n        cases=Image.open(\"./word1/%s.png\"%char)\n        cases=cases.resize((50,100))\n        img.paste(cases,(gap,_))\n        size=cases.width\n        gap+=size\n        del cases\n\ndef Letters(word):\n    global gap,_\n    if gap > sizeOfSheet-95*(len(word)):\n        gap=0\n        _+=200\n    for letter in word:\n        if letter in allowedchar:\n            Write(letter)\n\n\ndef Word(Input):\n    wordlist=Input.split(' ')\n    for i in wordlist:\n        Letters(i)\n        Write('space')\n\n\n\n\nif __name__=='__main__':\n\n    try:\n        with open(\"../input/my-own/data.txt\",'r') as file:\n            data=file.read().replace('\\n','')\n            l=len(data)\n            nn=len(data)//l\n            print(nn)\n            print(l)\n            chunks,chunk_size=len(data),len(data)//nn+1\n            p=[data[i:i+chunk_size] for i in range(0,chunks,chunk_size)]\n\n            for i in range(0,len(p)):\n                Word(p[i])\n                Write('\\n')\n                img.save(\"./img2/%doutt.png\"%i)\n                img1=Image.open(\"../input/my-own/bg.jpg\")\n                img=img1\n                gap,_=0,0\n    except ValueError as E:\n        print(\"{}\\nTry again\",format(E))\n    imageList=[]\n    for i in range(0,len(p)):\n        imageList.append(\"./img2/%doutt.png\"%i)\n\n    cover=Image.open(imageList[0])\n    width,height=cover.size\n    pdf=FPDF(unit=\"pt\",format=[width,height])\n    for i in range(0,len(imageList)):\n        pdf.add_page()\n        pdf.image(imageList[i],0,0)\n    pdf.output(\"./img2/word1.pdf\",\"F\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}