{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Electricity Price Forecasting Model with LightGBM**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This notebook presents the visualization, treatment and modeling of electrical consumption, generation, princing and weather data in Spain.\n\n##### **The goal of this project is to predict the electricity distribution price hourly, providing local households a service where they can monitorize their consumption levels, as well as additional features. Moreover, this application can be extended to a broader range of users, like market investors or power generation and management companies.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Table of Contents\n\n\n* [1. Data Cleaning and Processing](#section_one)\n* [2. Exploratory Data Analysis](#section_two)\n* [3. Feature Generation and Selection](#section_three)\n* [4. Model](#section_four)\n* [5. Model Metrics](#section_five)\n* [6. Remarks](#section_a)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport datetime\nfrom pylab import rcParams\nimport seaborn as sn\nfrom scipy import stats\nfrom geopy.geocoders import Nominatim\nimport plotly as pl\nimport plotly.express as px\nfrom sklearn.preprocessing import LabelEncoder\nfrom functools import reduce\nimport requests\nimport json\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.features import Rank1D\nfrom yellowbrick.features import Rank2D\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nimport lightgbm as lgb\nfrom sklearn.metrics import r2_score\nfrom sklearn import metrics\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.linear_model import LassoLarsCV\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.preprocessing import PolynomialFeatures","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Set numpy output options \n\nnp.set_printoptions(edgeitems=3)\nnp.core.arrayprint._line_width = 30\n\n# Pandas output options\npd.set_option('display.max_columns', 20)\npd.set_option('display.max_rows', 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section_one\"></a>\n## **1. Data Cleaning and Processing**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\n\n# Data\nenergyData = pd.read_csv('/kaggle/input/energy-consumption-generation-prices-and-weather/energy_dataset.csv')\nweatherData = pd.read_csv('/kaggle/input/energy-consumption-generation-prices-and-weather/weather_features.csv')\n\n# Set time as index\nenergyData.set_index('time', inplace = True)\nweatherData.set_index('dt_iso', inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Energy Data Overview\n\nenergyData.head(15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### As seen before, the energy dataset contains energy generation from different sources, all in MWh, since it is being provided hourly. Also, the goal of our study is set (price - €/MWh). It is important to refer that this is nationally gathered information.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rename all columns\n\nenergyData.columns = energyData.columns.map(lambda x : x+'_MWh' if x !='price day ahead' and x!='price actual' else x)\n\ncolumns = energyData.columns[energyData.columns.str.contains('price day ahead|price actual')]\nenergyData.rename(columns = dict(zip(columns, columns + '_€/Mwh')), inplace=True)\n\n\n# Check all nan values\n\nprint('Energy Data NaN values: \\n', energyData.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the features that have the same number of NaN values as the lenght of the dataframe\n\ndef CheckNull(data_frame):\n    for i in data_frame.columns.values:\n        if data_frame[i].isna().sum() == len(data_frame):\n            print('This column is empty: ', i)\n            \nCheckNull(energyData)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the NaN columns\nenergyData = energyData.drop(['generation hydro pumped storage aggregated_MWh', 'forecast wind offshore eday ahead_MWh'], axis=1)\n\n# Substituting NaN values in energy dataset with linear interpolation\nenergyData.interpolate(method='linear', inplace=True, axis=0)\n\n# Checking for duplicated values\n\nduplicatedEnergy_values = energyData.duplicated().sum()\nprint('There is {} duplicated values.'.format(duplicatedEnergy_values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check dataset dtype\n\nprint('Dataset Type \\n', energyData.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for the values distribution for more cleaning\n\nenergyData.hist(figsize=(25, 30), bins=50, xlabelsize=10, ylabelsize=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### From the previous cell, we can clearly see a unique distribution for the features 'generation fossil coal-derived gas', 'generation fossil oil shale', 'generation fossil peat', 'generation geothermal', 'generation marine' and 'generation wind offshore'. For this reason, we can also ignore this features from our energy dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping least relevant columns\n\nenergyData = energyData.drop(['generation fossil coal-derived gas_MWh', 'generation fossil oil shale_MWh', 'generation fossil peat_MWh', 'generation geothermal_MWh', 'generation marine_MWh', 'generation wind offshore_MWh'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Weather data overview\n\nweatherData.head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verifying NaN values at each column\n\nif CheckNull(weatherData) == None:\n    print('All collumns have values')\n    \nprint('Weather Data: \\n', weatherData.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check dataset type \nprint('Dataset Type \\n', weatherData.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Column values distribution\n\nweatherData.hist(figsize=(25, 30), bins=50, xlabelsize=10, ylabelsize=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Unlike the energy dataset format, the weather information contains object type data that can be treated properly. We will let the feature 'city_name' unchanged, as we have different weather information to each different city.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cities = weatherData['city_name'].unique().tolist()\n\nprint('Weather Cities: \\n', cities)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can have a better notion of our data distribution, with geographical representation of our data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining our locator function\ngeolocator = Nominatim()\n\n# Function for latitude and longitude information\ndef geo_locator(city, country):\n    loc = geolocator.geocode(str(city + ',' + country))\n    return (loc.latitude, loc.longitude)\n\n# Coordinates\nlatitudes = []\nlongitudes = []\n\n# Geolocate from city list\nfor i in cities:\n    location = geo_locator(i,'Spain')\n    latitudes.append(location[0])\n    longitudes.append(location[1])\n    \n    \nweatherData['Latitude'] = 0\nweatherData['Longitude'] = 0\n\n# Filling latitude and longitude for each city\n\nweatherData['Latitude'].loc[weatherData['city_name']=='Valencia'] = latitudes[0]\nweatherData['Latitude'].loc[weatherData['city_name']=='Madrid'] = latitudes[1]\nweatherData['Latitude'].loc[weatherData['city_name']=='Bilbao'] = latitudes[2]\nweatherData['Latitude'].loc[weatherData['city_name']==' Barcelona'] = latitudes[3]\nweatherData['Latitude'].loc[weatherData['city_name']=='Seville'] = latitudes[4]\n\nweatherData['Longitude'].loc[weatherData['city_name']=='Valencia'] = longitudes[0]\nweatherData['Longitude'].loc[weatherData['city_name']=='Madrid'] = longitudes[1]\nweatherData['Longitude'].loc[weatherData['city_name']=='Bilbao'] = longitudes[2]\nweatherData['Longitude'].loc[weatherData['city_name']==' Barcelona'] = longitudes[3]\nweatherData['Longitude'].loc[weatherData['city_name']=='Seville'] = longitudes[4]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Our weather dataset contains some features with categorical data. We can convert this information to other dataset type, using the Label Encoder method, converting each unique value to a number.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking unique values in categorical features\n\nweatherMain_values = weatherData['weather_main'].unique().tolist()\nweatherDescription_values = weatherData['weather_description'].unique().tolist()\nweatherIcon_values = weatherData['weather_icon'].unique().tolist()\n\nprint('Weather Main unique values: \\n', weatherMain_values)\nprint('Weather Description unique values: \\n', weatherDescription_values)\nprint('Weather Icon unique values: \\n', weatherIcon_values)\n\n\n# Setting Label Encoder for categorical values\n\nlabel_encoder = LabelEncoder()\n\nweatherData['weather_main'] = label_encoder.fit_transform(weatherData['weather_main'])\nweatherData['weather_description'] = label_encoder.fit_transform(weatherData['weather_description'])\nweatherData['weather_icon'] = label_encoder.fit_transform(weatherData['weather_icon'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### There is weather information regarding 5 different cities. We have to make sure that number of energy records in energy dataset is equal to the number of records in each city.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Energy Data Lenght:', len(energyData))\nprint('Weather Data Lenght:', len(weatherData))\n\nif (len(energyData) != (len(weatherData)/5)):\n    print('There are duplicate values in weather data')\n    \n    \nduplicatedWeather_values = weatherData.duplicated().sum()\nprint('There are {} duplicated values.'.format(duplicatedWeather_values))\n\n# Dropping duplicated values by city and time\nweatherData = weatherData.reset_index().drop_duplicates(subset=['dt_iso', 'city_name']).set_index('dt_iso')\n\n# Renaming index\nweatherData = weatherData.reset_index()\nweatherData = weatherData.rename(columns = {'dt_iso':'time'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Before merging the energy and weather information, we must first differentiate each weather information to each city, eliminating the city feature.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Subdividing weather information by city\nweatherData1, weatherData2, weatherData3, weatherData4, weatherData5 = [y for _, y in weatherData.groupby('city_name')]\n\n# Add sufix to each feature function\ndef addcity(dataframe):\n    city_name = dataframe.iloc[0]['city_name']\n    dataframe = dataframe.set_index(['time'])\n    dataframe = dataframe.drop(['city_name'], axis = 1)\n    dataframe = dataframe.add_suffix(city_name)\n    return dataframe\n\nweatherData_list = [weatherData1, weatherData2, weatherData3, weatherData4, weatherData5]\n\nweatherData_result = []\n\n# Applying the function to all weather data sets\nfor i in weatherData_list:\n    weatherData_result.append(addcity(i))\n    \n# For merging purposes   \nenergyData = energyData.reset_index()\n\n# For merging purposes\nfor i in range(0, len(weatherData_result)):\n    weatherData_result[i] = weatherData_result[i].reset_index()\n    \n    \n# Joining weather and energy data\ncompleteDataset = reduce(lambda x,y: pd.merge(x,y, on='time'), [energyData, weatherData_result[0], weatherData_result[1], weatherData_result[2], weatherData_result[3], weatherData_result[4]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Complete dataset\n\ncompleteDataset.head(15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's now detect possible outliers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot parameters\nrcParams['figure.figsize'] = 10, 5\n\n# Seaborn boxplot \nsn.boxplot(x=completeDataset['price actual_€/Mwh'])\nplt.title('Dataset Outliers')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We will first clean the outliers through Z_score method.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining Z_score\nz = np.abs(stats.zscore(completeDataset['price actual_€/Mwh']))\n\n# Removing outliers\ncompleteDataset = completeDataset[(z < 3)]\n\nsn.boxplot(x=completeDataset['price actual_€/Mwh'])\nplt.title('Dataset Outliers after first removal')\nplt.show()\n\nprint('Lenght after 1st removal:', len(completeDataset))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Using the Z_score method, we still have a considerable number of outliers. We will now complete this process with the quantile IQR method.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the quantile IQR\n\nQ1 = completeDataset['price actual_€/Mwh'].quantile(0.25)\nQ3 = completeDataset['price actual_€/Mwh'].quantile(0.75)\nIQR = Q3 - Q1\n\ncompleteDataset = completeDataset[~((completeDataset['price actual_€/Mwh'] < (Q1 - 1.5 * IQR)) | (completeDataset['price actual_€/Mwh'] > (Q3 + 1.5 * IQR)))]\n\nsn.boxplot(x=completeDataset['price actual_€/Mwh'])\nplt.title('Dataset Outliers after second removal')\nplt.show()\n\nprint('Lenght after 2nd removal:', len(completeDataset))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### To finalize this process, we can now get the date and time separatly as features (before defining our model), as well as the week day, to EDA purposes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"completeDataset.reset_index(drop=True)\n\n# Taking format\ncompleteDataset['time'] = completeDataset['time'].str[:-9]\n\ncompleteDataset['time'] = completeDataset['time'].apply(lambda x: pd.to_datetime(str(x), format='%Y-%m-%d %H:%M'))\ncompleteDataset['time'] = completeDataset['time'].dt.strftime('%d-%m-%Y %H:%M')\ncompleteDataset['time'] = completeDataset['time'].apply(lambda x: pd.to_datetime(str(x), format='%d-%m-%Y %H:%M'))\n\n# Getting date feature\ncompleteDataset['date'] = completeDataset['time'].dt.date\n\n\n# Getting hour_minute feature\ncompleteDataset['Hour_Minute'] = completeDataset['time'].dt.time\n\n\n# Week day\ncompleteDataset['Week_Day'] = completeDataset['time'].dt.weekday","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section_two\"></a>\n## **2. Exploratory Data Analysis**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# General Statistics\n\nprint('Data Statistics \\n', completeDataset.describe())\n\nprint(completeDataset.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We have different energy production sources through Spain's territory. It would be interesting to measure which source has the higher contribution to the overall electricity demand.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Figure subplot size\nfig = plt.figure(figsize=(15,13))\nax = fig.add_subplot(111)\n\n\ncompleteDataset = completeDataset.set_index('time', drop = False)\n\n# Weekend example\nstartDate = '2015-01-01 00:00:00'\nendDate = '2015-03-31 00:00:00'\n\n\n\n\nax.plot(completeDataset['date'][startDate:endDate],completeDataset['generation biomass_MWh'][startDate:endDate], color='r', label='biomass')\nax.plot(completeDataset['date'][startDate:endDate],completeDataset['generation fossil brown coal/lignite_MWh'][startDate:endDate], color='g', label='fossil brown coal/lignite')\nax.plot(completeDataset['date'][startDate:endDate],completeDataset['generation fossil gas_MWh'][startDate:endDate], color='grey', label='g')\nax.plot(completeDataset['date'][startDate:endDate],completeDataset['generation fossil hard coal_MWh'][startDate:endDate], color='y', label='hard coal')\nax.plot(completeDataset['date'][startDate:endDate],completeDataset['generation fossil oil_MWh'][startDate:endDate], color='c', label='oil')\nax.plot(completeDataset['date'][startDate:endDate],completeDataset['generation hydro pumped storage consumption_MWh'][startDate:endDate], color='m', label='hydro pumped storage')\n\n\nplt.legend(loc='upper right')\nplt.title('Energy sources contribution 1')\nplt.xlabel('Date')\nplt.ylabel('MW/h')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15,13))\nax = fig.add_subplot(111)\n\ncompleteDataset = completeDataset.set_index('time', drop = False)\n\n# Weekend example\nstartDate = '2015-01-01 00:00:00'\nendDate = '2015-03-31 00:00:00'\n\n\nax.plot(completeDataset['date'][startDate:endDate],completeDataset['generation hydro run-of-river and poundage_MWh'][startDate:endDate], color='r', label='hydro run-of-river and poundage')\nax.plot(completeDataset['date'][startDate:endDate],completeDataset['generation hydro water reservoir_MWh'][startDate:endDate], color='g', label='hydro water reservoir')\nax.plot(completeDataset['date'][startDate:endDate],completeDataset['generation nuclear_MWh'][startDate:endDate], color='grey', label='nuclear')\nax.plot(completeDataset['date'][startDate:endDate],completeDataset['generation other_MWh'][startDate:endDate], color='y', label='other')\nax.plot(completeDataset['date'][startDate:endDate],completeDataset['generation other renewable_MWh'][startDate:endDate], color='c', label='other renewable')\nax.plot(completeDataset['date'][startDate:endDate],completeDataset['generation solar_MWh'][startDate:endDate], color='m', label='solar')\n\nplt.legend(loc='upper right')\nplt.title('Energy sources contribution 2')\nplt.xlabel('Date')\nplt.ylabel('MW/h')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Figure subplot size\nfig = plt.figure(figsize=(15,13))\nax = fig.add_subplot(111)\n\nax.plot(completeDataset['date'][startDate:endDate],completeDataset['generation waste_MWh'][startDate:endDate], color='r', label='w')\nax.plot(completeDataset['date'][startDate:endDate],completeDataset['generation wind onshore_MWh'][startDate:endDate], color='g', label='wind')\n\nplt.legend(loc='upper right')\nplt.title('Energy sources contribution 3')\nplt.xlabel('Date')\nplt.ylabel('MW/h')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Figure subplot size\nfig = plt.figure(figsize=(20,14))\nax = fig.add_subplot(111)\n\naset = '2015-01-01 00:00:00'\nabset = '2015-12-31 00:00:00'\n\nacset = '2016-01-01 00:00:00'\nadset = '2016-12-31 00:00:00'\n\naeset = '2017-01-01 00:00:00'\nafset = '2017-12-31 00:00:00'\n\nagset = '2018-01-01 00:00:00'\nahset = '2018-12-31 00:00:00'\n\n\n\n\nax.plot(completeDataset['date'][aset:abset] ,completeDataset['total load actual_MWh'][aset:abset], color='c', label = '2015')\nax.plot(completeDataset['date'][acset:adset] ,completeDataset['total load actual_MWh'][acset:adset], color='r', label = '2016')\nax.plot(completeDataset['date'][aeset:afset] ,completeDataset['total load actual_MWh'][aeset:afset], color='b', label = '2017')\nax.plot(completeDataset['date'][agset:ahset] ,completeDataset['total load actual_MWh'][agset:ahset], color='g', label = '2018')\n\n\nplt.legend(loc='upper right')\nplt.title('Energy demand at each year')\nplt.xlabel('Date')\nplt.ylabel('MW/h')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can also measure the correlation metrics for both energy demand and price with other features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"energy_metrics = ['total load actual_MWh', 'price actual_€/Mwh']\n\nweather_metrics = completeDataset.loc[:, 'temp Barcelona':'LongitudeValencia']\n\nweather_metrics = weather_metrics.drop(['LatitudeBilbao', 'LongitudeBilbao', 'LatitudeValencia', 'LongitudeValencia', 'LatitudeMadrid', 'LongitudeMadrid', 'Latitude Barcelona', 'Longitude Barcelona', 'LatitudeSeville', 'LongitudeSeville'], axis=1)\n\ncont = pd.merge(completeDataset[energy_metrics], weather_metrics, left_index=True, right_index=True)\n\n# Correlation Matrix\n\ncalculation = cont.corr()\n\n\nprint('Energy matrix \\n', calculation['total load actual_MWh'])\n\nprint('Price matrix \\n', calculation['price actual_€/Mwh'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Price distribution\n\nsn.distplot(completeDataset['price actual_€/Mwh'])\nplt.title('Price Distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Figure subplot size\nfig = plt.figure(figsize=(20,14))\nax = fig.add_subplot(111)\n\naset = '2015-01-01 00:00:00'\nabset = '2015-12-31 00:00:00'\n\nacset = '2016-01-01 00:00:00'\nadset = '2016-12-31 00:00:00'\n\naeset = '2017-01-01 00:00:00'\nafset = '2017-12-31 00:00:00'\n\nagset = '2018-01-01 00:00:00'\nahset = '2018-12-31 00:00:00'\n\n\n\n\nax.plot(completeDataset['date'][aset:abset] ,completeDataset['price actual_€/Mwh'][aset:abset], color='c', label = '2015')\nax.plot(completeDataset['date'][acset:adset] ,completeDataset['price actual_€/Mwh'][acset:adset], color='r', label = '2016')\nax.plot(completeDataset['date'][aeset:afset] ,completeDataset['price actual_€/Mwh'][aeset:afset], color='b', label = '2017')\nax.plot(completeDataset['date'][agset:ahset] ,completeDataset['price actual_€/Mwh'][agset:ahset], color='g', label = '2018')\n\n\nplt.legend(loc='upper right')\nplt.title('Energy demand at each year')\nplt.xlabel('Date')\nplt.ylabel('MW/h')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section_three\"></a>\n## **3. Feature Generation and Selection**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### We could create other interesting features to our dataset. We can generate a city weight in demand, according to their population number (data from 2020, assuming a constant growth rate). We will give a classification, from 1 to 4, from the least populated city, to the highest.\n\nSource: https://worldpopulationreview.com/countries/spain-population/cities/.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Demand Weight Feature\n\nBilbao_weight = 1\nSeville_weight = 2\nValencia_weight = 3\nBarcelona_weight = 4\nMadrid_weight = 5\n\n\ncompleteDataset['Bilbao_weight'] = Bilbao_weight\ncompleteDataset['Seville_weight'] = Seville_weight\ncompleteDataset['Valencia_weight'] = Valencia_weight\ncompleteDataset['Barcelona_weight'] = Barcelona_weight\ncompleteDataset['Madrid_weight'] = Madrid_weight","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### To reduce the dimensions of the dataset, we can also join the same type of energy sources.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"completeDataset['coal_oil_fossil_MWh'] = completeDataset['generation fossil brown coal/lignite_MWh'] + completeDataset['generation fossil gas_MWh'] + completeDataset['generation fossil hard coal_MWh'] + completeDataset['generation fossil oil_MWh']\n\ncompleteDataset['renewables_MWh'] = completeDataset['generation hydro pumped storage consumption_MWh'] + completeDataset['generation hydro run-of-river and poundage_MWh'] + completeDataset['generation hydro water reservoir_MWh'] + completeDataset['generation other renewable_MWh'] + completeDataset['generation solar_MWh'] + completeDataset['generation wind onshore_MWh']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Using the previous information, we can set a value from which we select the relevant features regarding the weather information.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"set_value = 0.05\n\nweather_features = []\n\nfor index, value in calculation['price actual_€/Mwh'].items():\n    if value > 0.05:\n        weather_features.append(index)\n        \nprint('Relevant Features: \\n', weather_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"relevant_features = ['date', 'Hour_Minute', 'Week_Day', 'total load forecast_MWh', 'total load actual_MWh', 'price day ahead_€/Mwh', 'price actual_€/Mwh', 'LatitudeBilbao', 'LongitudeBilbao', 'LatitudeValencia', 'LongitudeValencia', 'LatitudeMadrid', 'LongitudeMadrid', 'Latitude Barcelona', 'Longitude Barcelona', 'LatitudeSeville', 'LongitudeSeville', 'temp Barcelona', 'temp_min Barcelona', 'temp_max Barcelona', 'weather_description Barcelona', 'tempBilbao', 'temp_minBilbao', 'temp_maxBilbao', 'pressureBilbao', 'weather_idBilbao', 'tempMadrid', 'temp_minMadrid', 'temp_maxMadrid', 'temp_minSeville', 'pressureSeville', 'weather_idSeville', 'tempValencia', 'temp_minValencia', 'coal_oil_fossil_MWh', 'renewables_MWh', 'generation biomass_MWh', 'forecast solar day ahead_MWh', 'forecast wind onshore day ahead_MWh']\n\ncompleteDataset = completeDataset[relevant_features]\n\n\n# Using seaborn for heatmap correlation matrix\n\n# Plot size\nfig, ax = plt.subplots(figsize=(40,20))\n\n# This method will only be used for continuous variables\ncontinuousVariables = completeDataset.select_dtypes('float64','int64')\nheatmap = sn.heatmap(completeDataset.corr(), annot=True, fmt='.2f')\n\nplt.title('Heatmap for continuous variables', fontsize=20)\nplt.savefig('Heatmap.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### From the previous analysis, we see that the latitude and longitude features. We can then discard that information.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Discarding features\n\ncompleteDataset = completeDataset.drop(['LatitudeBilbao', 'LongitudeBilbao', 'LatitudeValencia', 'LongitudeValencia', 'LatitudeMadrid', 'LongitudeMadrid', 'Latitude Barcelona', 'Longitude Barcelona', 'LatitudeSeville', 'LongitudeSeville'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rank 2D Pearson Algorithm\n\n# Plot Size\nplt.figure(figsize=(20, 15))\n\n\ncontinuousVariables = completeDataset.select_dtypes('float64','int64')\n\n\n# Definition of the algorithm\nvisualizer = Rank2D(algorithm='pearson')\nvisualizer.fit_transform(continuousVariables)\nvisualizer.poof()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Definition of the covariance\nvisualizer = Rank2D(features=continuousVariables.columns, algorithm='covariance')\n\n# Plot Size\nplt.figure(figsize=(20, 15))\n\n\nlabel = continuousVariables['price actual_€/Mwh']\n\nvisualizer.fit(continuousVariables, label) \nvisualizer.transform(continuousVariables) \nvisualizer.poof()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the algorithm\nvisualizer = Rank1D(features=continuousVariables.columns, algorithm='shapiro')\nvisualizer.fit(continuousVariables, label) \n# Transforming data\nvisualizer.transform(continuousVariables)\nvisualizer.poof()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting all relevant features \n\ncompleteDataset = completeDataset.drop(['total load forecast_MWh', 'price day ahead_€/Mwh', 'forecast solar day ahead_MWh', 'forecast wind onshore day ahead_MWh', 'date', 'Hour_Minute'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section_four\"></a>\n## **4. Model**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Let's use LightGBM regressor for our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing the model\nmodel = lgb.LGBMRegressor(objective= 'regression')\nprint('LightGBM Parameters:', np.array(model.get_params))\n\n# Define features and label\n\nfeatures = completeDataset.drop(['price actual_€/Mwh'], axis=1)\nlabel = completeDataset['price actual_€/Mwh']\n\n\n# Train and test split\nX_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.2)\n\n\n\n# Fit with train set\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section_five\"></a>\n## **5. Model Metrics**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### In this section, we evaluate the performance of the algorithm and procede to model parameter tuning, in order to improve the results.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predictions to test data\nExpectedValues  = y_test\nPredictedValues = model.predict(X_test)\n\n# R2 and Mean Square Error for LightGBM Regressor\n\nprint('R2 Score: \\n', metrics.r2_score(ExpectedValues, PredictedValues))\nprint('Mean Square Error: \\n', metrics.mean_squared_log_error(ExpectedValues, PredictedValues))\n\n# Expected vs Predicted Plot in seaborn values regressor plot\nsn.regplot(ExpectedValues, PredictedValues, fit_reg=True, scatter_kws={'s': 100})\n\nplt.title ('Expected vs Predicted Values with LightGBM model')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameter Tuning\n\n# Nº of CV folds\nnumberFolds = 5\n\n# Validation CV function\ndef validationcv(parameterTuning):\n    folds = KFold(numberFolds, shuffle=True).get_n_splits(X_train.values)\n    rsquare= np.sqrt(-cross_val_score(model, X_train.values, y_train, scoring='neg_mean_squared_error', cv=folds))\n    return(rsquare)\n\n\n# Choosing parameters\nparameters = {\n    'objective':'regression',\n    'boosting_type':'gbdt', \n    'max_bin':50,\n    'num_leaves':3,\n    'max_depth':10,\n    'learning_rate':0.5, \n    'bagging_fraction':0.7,\n    'bagging_freq':6,\n    'bagging_seed':7,\n    'min_data_in_leaf':5, \n    'min_sum_hessian_in_leaf':7}\n\n# Setting model with the chosen parameters\nparameterTuning = lgb.LGBMRegressor(**parameters)\n\n# Fitting with new parameters\nparameterTuning.fit(X_train, y_train)\n\n\n# Expected and Predicted Values\nExpectedValues =  y_test\nPredictedValues = parameterTuning.predict(X_test)\n\n# Results\nprint('New R2 Score: \\n', metrics.r2_score(ExpectedValues, PredictedValues))\nprint('New Mean Square Error: \\n', metrics.mean_squared_log_error(ExpectedValues, PredictedValues))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Deploy\nprint('Saving the model...')\nmodel.booster_.save_model('LightgbmEnergyPricePrediction_Project.txt')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### <a id=\"section_a\"></a>\n## **6. Remarks**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Model performance after parameter tuning was actually lower. However, the presented process may not be the optimal one. It can be improved by, for example, tuning in a set interval of values for each value, until the best score is achieved. Overall, obtained results obtained by the model for test set where not completely satisfactory, and they can upgraded to better performance values. However, they can give a overall picture to the purpose of this project.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}