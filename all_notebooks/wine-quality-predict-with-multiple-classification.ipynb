{"cells":[{"metadata":{},"cell_type":"markdown","source":"## 1. Import Libraries","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Load DATA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading dataset\ndf = pd.read_csv('../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Feature Engineering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['quality'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df.select_dtypes([np.int, np.float])\n\nfor i, col in enumerate(df1.columns):\n    plt.figure(i)\n    sns.barplot(x='quality',y=col, data=df1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# quality > 6 is good and less is bad\nbins = [2, 6.5, 8] \nprint(bins)\ngroup_names = ['bad', 'good']\ndf['quality'] = pd.cut(df['quality'], bins = bins, labels = group_names)\nprint(df['quality'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# converting 'bad' and 'good' to labels 0 and 1\nfrom sklearn.preprocessing import LabelEncoder\nencode = LabelEncoder()\ndf.quality = encode.fit_transform(df.quality)\ndf['quality'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature Selection**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_data = df['quality']\nx_data = df\nx_data.drop(['residual sugar', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'quality'], axis=1, inplace=True)\n#x_data.drop(['quality'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Trying different Classification Techniques\n### a) Linear classifier\n### b) Support Vector Machine\n### c) Kernel Estimation\n### d) Bagging","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.15, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### a) Linear classifier","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### (i) Logistic Regression - Logit is a function through which the binary distribution is associated with the linear equation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\n\ny_pred = logreg.predict(x_test)\n\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\ncnf_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(cnf_matrix, annot=True)\nplt.title(\"Confusion Matrix\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (ii) DECISION TREE CLASSIFIER - A decision tree is a map of the possible outcomes of a series of related choices.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()\ndt.fit(x_train, y_train)\n\ny_pred = dt.predict(x_test)\n\naccuracy_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### b) Support Vector Machine - SVM is a supervised machine learning algorithm which can be used for classification or regression problems. It uses a technique called the kernel trick to transform your data and then based on these transformations it finds an optimal boundary between the possible outputs.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nclf = SVC(kernel='linear')\nclf.fit(x_train, y_train)\n\ny_pred = clf.predict(x_test)\n\naccuracy_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### c) Kernel estimation -> k-nearest neighbor - K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nk = 2\n#Train Model and Predict  \nneigh = KNeighborsClassifier(n_neighbors = k).fit(x_train,y_train)\nyhat = neigh.predict(x_test)\n\nfrom sklearn import metrics\nprint(\"Train set Accuracy: \", metrics.accuracy_score(y_train, neigh.predict(x_train)))\nprint(\"Test set Accuracy (real acc): \", metrics.accuracy_score(y_test, yhat))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### d) Bagging - Bagging (or Bootstrap Aggregating) technique uses these subsets (bags) to get a fair idea of the distribution (complete set).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### (i) Bagging meta-estimator","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn import tree\nmodel = BaggingClassifier(tree.DecisionTreeClassifier(random_state=1))\nmodel.fit(x_train, y_train)\nmodel.score(x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (ii) Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nmodel = RandomForestClassifier()\ntrain_x = x_train\ntrain_y = y_train\ntest_x = x_test\ntest_y = y_test\n# fit the model with the training data\nmodel.fit(train_x,train_y)\n\n# number of trees used\nprint('Number of Trees used : ', model.n_estimators)\n\n# predict the target on the train dataset\npredict_train = model.predict(train_x)\nprint('\\nTarget on train data',predict_train) \n\n# Accuray Score on train dataset\naccuracy_train = accuracy_score(train_y,predict_train)\nprint('\\naccuracy_score on train dataset : ', accuracy_train)\n\n# predict the target on the test dataset\npredict_test = model.predict(test_x)\nprint('\\nTarget on test data',predict_test) \n\n# Accuracy Score on test dataset\naccuracy_test = accuracy_score(test_y,predict_test)\nprint('\\naccuracy_score on test dataset : ', accuracy_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion \n## WE tried different technnique for classification and finally saw that Random Forest gives best results from all of them.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}