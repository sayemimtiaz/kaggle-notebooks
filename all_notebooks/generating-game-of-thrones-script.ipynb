{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Introduction\nOn this kernel, I will explain step by step I done in generating Script for Game of Thrones series.\n\nThe site I used for scrapping is genius.com. This site contains a whole scripts of Game of Thrones series for all episodes."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## Scrapping Episodes Urls\n### Loading the Required Packages\nPackages used for scrapping the episodes list are:\n* requests\n* BeautifulSoup"},{"metadata":{"trusted":true},"cell_type":"code","source":"import requests # Used for doing the http request\nfrom bs4 import BeautifulSoup # Used for assisting the HTML read","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Urls Scrapping\nExample URL for an album in genius.com is as following (https://genius.com/albums/Game-of-thrones/Season-1-scripts ). In our case, album means season of a series. Each album contains a full list of URLs used to access the lyrics of all the songs in the album. Then again, songs can be interpreted as episodes of a series in a single season (album).\n\nFrom the example above we can see that the season URLs are structured following this pattern below:\n> 'https://genius.com/albums/Game-of-thrones/Season-' + [Season Number] + '-scripts'\n\nWe know that Game of Thrones has already finished with 8 seasons in total. Therefore we will have 8 different URLs for each season.\n\nIn order to capture the URLs for each season, we can make a simple list comprehension to generate the URL for each season which follows the pattern we've discovered before."},{"metadata":{"trusted":true},"cell_type":"code","source":"season_urls = ['https://genius.com/albums/Game-of-thrones/Season-' + str(season_number) + '-scripts' for season_number in range(1,9)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at our season URLs."},{"metadata":{"trusted":true},"cell_type":"code","source":"for season_url in season_urls:\n    print(season_url)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we already have the URLs for each season, we can take a look at the inner HTML from one of those URLs.\nWe can do HTTP request using requests package that we imported, and then save the HTML result as a BeautifulSoup object.\n\nBeautifulSoup will wrap the result of a requests from raw string/text format to a structured data type known as BeautifulSoup object."},{"metadata":{"trusted":true},"cell_type":"code","source":"r = requests.get(season_urls[0])\nhtml_doc = r.text\nsoup = BeautifulSoup(html_doc)\n\n# only view snippet because the result is too large\nstr(soup)[:1000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"If we look at the HTML text comprehensively, we can see that the URLs for each episode in a season are wrapped in an `a` tag with class name of `u-display_block`.\n\nIn BeautifulSoup object, we can easily access each of html tag with specific attribute using a method called `.find_all()`."},{"metadata":{"trusted":true},"cell_type":"code","source":"url_containers = soup.find_all('a', class_='u-display_block')\n# take a look at one of the items\nurl_containers[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From one of the html `a` tag above we know that the URL for each episode is stored in the attribute `href`. Tag in `BeautifulSoup` is kind of similar to `dictionary` in python in term of (`attribute`, `value of attribute`) from a tag can be treated as (`key`, `value`) from a dictionary. Therefore we can get the URL of the episode by simply accessing the `href` attribute of an `a` tag using the same method as accessing value of a python dictionary."},{"metadata":{"trusted":true},"cell_type":"code","source":"urls = [url_container['href'] for url_container in url_containers]\n\n# Take a look at the URLs inside\nfor url in urls:\n    print(url)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we know how to get the URL for each episode in a season of Game of Thrones. Next we need to do is extracting the URLs for all episodes from all seasons. We can do this by making simple loop of the season URLs and wrap all of what we have done before in order to get the episode URLs inside the loop."},{"metadata":{"trusted":true},"cell_type":"code","source":"urls = []\nfor season_url in season_urls:\n    \n    r = requests.get(season_url)\n    html_doc = r.text\n    soup = BeautifulSoup(html_doc)\n    \n    url_containers = soup.find_all('a', class_='u-display_block')\n    \n    for url_container in url_containers:\n        urls.append(url_container['href'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show number of episodes\nlen(urls)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are some anomalies in the URLs that we got. We know that Game of Thrones only consists of 73 episodes in total, but based on our scrap we captured more than 73 episodes."},{"metadata":{"trusted":true},"cell_type":"code","source":"for url in urls:\n    print(url)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After looking comprehensively at each item in our URL list, we could see there are preview and trailer episodes for both season 4 & 5 which are not needed. Therefore, we will remove those episodes from the list."},{"metadata":{"trusted":true},"cell_type":"code","source":"urls = [url for url in urls if 'season' not in url]\nlen(urls)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we have clean and complete list of Game of Thrones' all episodes. Now we can move to scrap the script/conversations from each of those episodes."},{"metadata":{},"cell_type":"markdown","source":"## Raw Text Scrapping\nIn this part we will start scrapping the content contained on each episode URLs that we've found. The data that we want to retrieve from each URL simply consists of:\n* Episode Number\n* Episode Title\n* Season Number\n* Release Date\n* Conversations"},{"metadata":{},"cell_type":"markdown","source":"### Path Finding\nWhat we gonna do in finding those data is starting with finding the html paths containing each one of the data. After the path found we will transform the data which naturally will be on raw html format to more readable datatypes and store them on predefined variables.\n\nBefore we do web scrapping from all the episode URLs, It is better to do the process on one of the URL. Therefore we can find the processes and methods that we can apply to other URLs.\n\nSo, we need to save one of our URL in one single variable for further use."},{"metadata":{"trusted":true},"cell_type":"code","source":"url = urls[0]\nurl","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similar to previous process, we need to get the HTML response from the URL and store them as `BeautifulSoup` object."},{"metadata":{"trusted":true},"cell_type":"code","source":"r = requests.get(url)\nhtml_doc = r.text\nsoup = BeautifulSoup(html_doc)\n\n# take a look inside\n# again only snippet because the result too large\nstr(soup)[:2000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Episode Number & Title\nBased on the HTML text that we loaded above, the episode number and title are wrapped in a `div` tag with class name of `track_listing-track track_listing-track--current`.\nOnce again we can easily access this tag using `.find_all()` method."},{"metadata":{"trusted":true},"cell_type":"code","source":"episode = soup.find_all('div', class_='track_listing-track track_listing-track--current')\n# take a look inside\nepisode","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the result is a list containing a single element of inner HTML from a span. However, the inner HTML itself is still in HTML format. We can get all the texts inside HTML using `.text` attribute of a soup."},{"metadata":{"trusted":true},"cell_type":"code","source":"episode = episode[0].text\n# take a look inside\nepisode","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Further text processing is needed to get the `number` and `title` of an episode."},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a list by splitting the string using '\\n'\nepisode = episode.split('\\n')\n\n# remove unused and empty strings\nepisode = ''.join(e + ' ' for e in episode)\nepisode = episode.split(' ')\nepisode = list(filter(None, episode))\n\n# assign episode number and episode title to different variables\nepisode_number = ''.join('Episode ' + episode[0].split('.')[0])\nepisode_title = ''.join(e + ' ' for e in episode[1:])[:-1]\n\n# show the results\nprint(episode_number)\nprint(episode_title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Season Number\nSeason number is wrapped in an `a` tag with class name of `song_album-info-title`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# get all elements inside 'a' tag, remove enters, convert to list splitted by empty space\nseason = soup.find_all('a', class_='song_album-info-title')[0].text.replace('\\n','').split(' ')\n\n# remove empty strings and concat all the remaining\nseason = list(filter(None, season))\nseason = ''.join(s + ' ' for s in season[:-1])[:-1]\n\nprint(season)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Release Date\nRelease date is wrapped in an `span` tag with class name of `metadata_unit-info metadata_unit-info--text_only`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# get all elements inside 'a' tag\nrelease_date = soup.find_all('span', class_='metadata_unit-info metadata_unit-info--text_only')\nrelease_date = release_date[0].text\n\nprint(release_date)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We want to make the date stored in a more simplified format. We can use method `.strptime()` and `.strftime()` from `datetime`. To do this we need to import `datetime` from package `datetime`."},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\n\nrelease_date = datetime.strptime(release_date, '%B %d, %Y')\nrelease_date = datetime.strftime(release_date, '%Y-%m-%d')\n\nprint(release_date)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Conversations\nScrapping the conversations part will have complex and long processes. These processes include getting the raw html text, removing unused tags, filtering the tags needed, and so many text cleansing processes.\n\nAs a start, we kno that the conversation part is stored in a `div` tag with class name of `lyrics`"},{"metadata":{"trusted":true},"cell_type":"code","source":"lyrics = soup.find_all(\"div\", class_=\"lyrics\")[0]\n\n# again only snippet because the result too large\nstr(lyrics)[:2000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see there are a lot of tags on our soup. We can easily remove those unused tags using method `.extract()` of a soup. In order to do that we need to convert our soup to a BeautifulSoup object, and then apply the `.extract()` method for each unused tag."},{"metadata":{"trusted":true},"cell_type":"code","source":"lyrics = BeautifulSoup(str(lyrics))\n[s.extract() for s in lyrics('br')]\n[s.extract() for s in lyrics('i')]\n[s.extract() for s in lyrics('hr')]\n[s.extract() for s in lyrics('h1')]\n[s.extract() for s in lyrics('h2')]\n[s.extract() for s in lyrics('h3')]\n\n# take a look inside\n# again only snippet because the result too large\nprint(str(lyrics)[:3000])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we already removed some of unused tags in our `lyrics`. From this we can see that all of the conversations are wrapped in `p` tag and they are clearly written by matching with this pattern `[Person]:[Sentences]`.\n\nHowever, other text that is not considered as conversation also stored on this tag. So, we need to clean the data again later."},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the 'p' tags inner HTML\nparagraphs = lyrics.find_all('p')\n\n# create variable to store the conversations\nconversations = []\n\n# iterating all 'p' tags found\nfor p in paragraphs:\n    # get the inner text of p, create list by splitting text using '\\n', and extend them to list outside the loop\n    conversations.extend(p.text.split('\\n'))\n    \n# remove empty strings\nconversations = list(filter(None, conversations))\n\n# by following the [person]:[sentences] pattern, convert the string inside list to tuple format\nconversations = [tuple(s.split(':')) for s in conversations]\n\nfor conversation in conversations[:10]:\n    print(conversation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have all the conversations stored in a list containing tuple of (`person`,`sentence`) format. Unfortunately, some of the entries of our list don't match with the format. This indicates that those values are not considered as a conversation. Therefore we need to remove them.\n\nNow we have two different types of tuple on our list which are tuple consisting 2 values, and tuple consisting only one value. Let's take a look on those two types of tuple."},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, conversation in enumerate(conversations[255:265]):\n    if len(conversation) >= 2:\n        print(str(index) + ' | 2 values | ' + ''.join(str(c) + ':' for c in conversation)[:-1])\n    else:\n        print(str(index) + ' | 1 value | ' + ''.join(str(c) + ':' for c in conversation)[:-1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ideally, we can just remove the tuple that only consisting 1 value and store the rest of tuples as our clean data. However, It turns out that some of the conversations actually are not following the `[person]:[sentences]` format. We need to do some `regex` matching in order not to lose those conversations.\n\nBut before doing that, we need to remove tuples that represent background situation. Those tuples have elements written inside a bracket `[]`. We can remove those tuples using some `regex` matching. For a better understanding about `regex`, you can do exercise here: https://regexr.com/"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n# regex to find conversations in [ some text ] format\nregex = '(.+)\\[.+\\](.+)|(.+)\\[.+\\]|\\[.+\\]'\npattern = re.compile(regex)\n\nfor index, conversation in enumerate(conversations):\n    if len(conversation) <= 1:\n        match = pattern.findall(conversation[0])\n        if len(match) > 0:\n            conversations[index] = tuple((''.join(e + ' ' for e in list(filter(None, match[0]))).replace('    ',' ').replace('   ',' ').replace('  ', ' ')).split('\\n'))\n\nconversations = list(filter(None, conversations))\nconversations = [c for c in conversations if len(c[0]) > 0]\n\n# show\nconversations[15:25]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Eventhough we already filtered some of the background situations, there are some cases in which the background situation is divided to two different lines which are not captured by our regex before. We need to clean the list again."},{"metadata":{"trusted":true},"cell_type":"code","source":"# regex that match for '[ some text' and 'some text ]' format\nregex = '^\\[.+|.+\\]$'\npattern = re.compile(regex)\n\nfor index, conversation in enumerate(conversations):\n    if len(conversation) <= 1:\n        match = pattern.search(conversation[0])\n        if match:\n            conversations[index] = None\n\nconversations = list(filter(None, conversations))\nconversations[15:25]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have filtered all of the background situations, our list now should contains conversations only. However, we still have not do anything about the conversations that is not following the `[person]:[sentence]` format.\n\nLet's take a look at them."},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, conversation in enumerate(conversations):\n    if len(conversation) < 2:\n        print(str(index) + ' | ' + conversation[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we look closely, these conversations match the pattern `[person in uppercase] (some text) [sentences]`. Besides, there can also be some conversations that do not have the `(some text)` part.\n\nOnce again, we will extract the `person` and `sentence` from those conversations using `regex`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# regex to match with '[person in uppercase] [rest of the text]' and '[person in uppercase] (some text) [rest of the text]' format\nregex = '^([A-Z]{2,})(.+)'\npattern = re.compile(regex)\n\nfor index, conversation in enumerate(conversations):\n    if len(conversation) <= 1:\n        match = pattern.findall(conversation[0])\n        if len(match) > 0:\n            conversations[index] = (match[0][0], match[0][-1])\n    \n# take a look\nconversations[125:135]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this point we have all the conversations on desired format. Now our list of `conversations` should only have conversation on two valued tuple."},{"metadata":{"trusted":true},"cell_type":"code","source":"for conversation in conversations:\n    if len(conversation) < 2:\n        print(conversation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now take out all of the one valued tuples from our list."},{"metadata":{"trusted":true},"cell_type":"code","source":"conversations = [conversation for conversation in conversations if len(conversation) > 1]\n\n# take a look\nconversations[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we have our desired list of conversations which is the last piece of data that we want to scrap. From now on we will combine all of the data that we already gathered which are `episode number`, `episode title`, `season number`, `release date`, and `conversation`. We will put all these data together and store them in a `dataframe`."},{"metadata":{},"cell_type":"markdown","source":"#### Create Dataframe\nFor creating dataframe we need to import the required package."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To make a better and tidier dataframe, we can not put raw tuples as our entry. Therefore we need to separate our conversations data as two different set of values which are `person` and `sentence`. We are going to create pandas `Series` for each of them."},{"metadata":{"trusted":true},"cell_type":"code","source":"person = pd.Series([c[0] for c in conversations])\nsentence = pd.Series([c[1] for c in conversations])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's have a quick look."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(person.head())\nprint(sentence.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have all the separated data in different variables. We can now wrap all of these variables to a single dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"script = pd.DataFrame({\n    'Season': season,\n    'Episode': episode_number,\n    'Episode Title': episode_title,\n    'Sentence': sentence,\n    'Name': person,\n    'Release Date': release_date\n})\nscript = script[['Release Date','Season','Episode','Episode Title','Name','Sentence']]\nprint(script.info())\nscript.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can still see there are some data that considered dirty in this dataframe. On the top position we have data with Name `EPISODE` and Sentence `1 - WINTER IS COMING`. Other case is different representation of same person, such as `DAENERYS` and `DAENERYS TARGARYEN`. There are still some other cases as well. However, we will clean this later on `Post Scrapping Data Cleansing` part. For now we already have at least all of the conversation with no loss and in our desired format."},{"metadata":{},"cell_type":"markdown","source":"### Wrap The Process in Functions\nBased on what we have done before, we already know how to scrap and gather all the data required to make a dataframe of conversations from one episode of Game of Thrones. We can now iterate all of the episode `URLs` and do the whole process to each of them to get all of the scripts.\n\nHowever, instead of putting the whole process inside a single loop, it is better to wrap each of the independent process in different functions."},{"metadata":{},"cell_type":"markdown","source":"#### Get Episode"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_episode(soup):\n    episode = soup.find_all('div', class_='track_listing-track track_listing-track--current')[0].text.split('\\n')\n    episode = ''.join(e + ' ' for e in episode)\n    episode = episode.split(' ')\n    episode = list(filter(None, episode))\n\n    episode_number = ''.join('Episode ' + episode[0].split('.')[0])\n    episode_title = ''.join(e + ' ' for e in episode[1:])[:-1]\n    \n    return episode_number, episode_title","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Get Season"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_season(soup):\n    season = soup.find_all('a', class_='song_album-info-title')[0].text.replace('\\n','').split(' ')\n    season = list(filter(None, season))\n    season = ''.join(s + ' ' for s in season[:-1])[:-1]\n    \n    return season","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Get Release Date"},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\n\ndef get_release_date(soup):\n    release_date = soup.find_all('span', class_='metadata_unit-info metadata_unit-info--text_only')[0].text\n    release_date = datetime.strptime(release_date, '%B %d, %Y')\n    release_date = datetime.strftime(release_date, '%Y-%m-%d')\n    \n    return release_date","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Get Conversations"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\ndef get_conversations(soup):\n    lyrics = soup.find_all(\"div\", class_=\"lyrics\")[0]\n\n    lyrics = BeautifulSoup(str(lyrics))\n    [s.extract() for s in lyrics('br')]\n    [s.extract() for s in lyrics('i')]\n    [s.extract() for s in lyrics('hr')]\n    [s.extract() for s in lyrics('h1')]\n    [s.extract() for s in lyrics('h2')]\n    [s.extract() for s in lyrics('h3')]\n\n    paragraphs = lyrics.find_all('p')\n\n    conversations = []\n\n    for p in paragraphs:\n        conversations.extend(p.text.split('\\n'))\n\n    conversations = list(filter(None, conversations))\n    conversations = [tuple(s.split(':')) for s in conversations]\n    \n    regex = '(.+)\\[.+\\](.+)|(.+)\\[.+\\]|\\[.+\\]'\n    pattern = re.compile(regex)\n    \n    for index, conversation in enumerate(conversations):\n        if len(conversation) <= 1:\n            match = pattern.findall(conversation[0])\n            if len(match) > 0:\n                conversations[index] = tuple((''.join(e + ' ' for e in list(filter(None, match[0]))).replace('    ',' ').replace('   ',' ').replace('  ', ' ')).split('\\n'))\n                \n    conversations = list(filter(None, conversations))\n    conversations = [c for c in conversations if len(c[0]) > 0]\n    \n    regex = '^\\[.+|.+\\]$'\n    pattern = re.compile(regex)\n    \n    for index, conversation in enumerate(conversations):\n        if len(conversation) <= 1:\n            match = pattern.search(conversation[0])\n            if match:\n                conversations[index] = None\n                \n    conversations = list(filter(None, conversations))\n    \n    regex = '^([A-Z]{2,})(.+)'\n    pattern = re.compile(regex)\n    \n    for index, conversation in enumerate(conversations):\n        if len(conversation) <= 1:\n            match = pattern.findall(conversation[0])\n            if len(match) > 0:\n                conversations[index] = (match[0][0], match[0][-1])\n                \n    conversations = [conversation for conversation in conversations if len(conversation) > 1]\n    \n    return conversations","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create Dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_dataframe(**kwargs):\n    \n    person = pd.Series([c[0] for c in conversations])\n    sentence = pd.Series([c[1] for c in conversations])\n    \n    script = pd.DataFrame({\n        'Season': season,\n        'Episode': episode_number,\n        'Episode Title': episode_title,\n        'Sentence': sentence,\n        'Name': person,\n        'Release Date': release_date\n    })\n    \n    script = script[['Release Date','Season','Episode','Episode Title','Name','Sentence']]\n    \n    return script","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Iterate All Episodes\nAfter wrapping all of our independent processes in different functions, next thing we should do is applying these functions to all of our episode `URLs` to get the whole script of Game of Thrones. To do this we will make a simple for loop to iterate all of our `URLs`, and put the functions we have made before inside the loop."},{"metadata":{"trusted":true},"cell_type":"code","source":"# initiate an empty list to store dataframes from each episode\nscripts = []\nfor url in urls:\n    r = requests.get(url)\n    html_doc = r.text\n    soup = BeautifulSoup(html_doc)\n    \n    episode_number, episode_title = get_episode(soup)\n    season = get_season(soup)\n    release_date = get_release_date(soup)\n    conversations = get_conversations(soup)\n    \n    df_scripts = create_dataframe(episode_number = episode_number, \n                                  episode_title = episode_title,\n                                  season = season,\n                                  release_date = release_date,\n                                  conversations = conversations)\n    \n    scripts.append(df_scripts)\n    print('Script from: ' + url + ' added')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"script_dataframe = pd.concat(scripts)\nscript_dataframe.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we have collected all scripts for the whole season of Game of Thrones in a single dataframe. However, we still keep in mind that these data are not completely clean yet. That is why what we are going to do next is doing the `Post Scrapping Data Cleansing` to make sure the dataset is safe to use."},{"metadata":{},"cell_type":"markdown","source":"## Post-Scrapping Data Cleansing"},{"metadata":{"trusted":true},"cell_type":"code","source":"script_dataframe = script_dataframe.dropna()\nscript_dataframe.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have encountered some `Name` and `Sentence` that contain some bracketed strings in previous sections. This can describe about what the person is thinking or refers to the audience of the person talking. We don't need these to ruin our data, therefore we need to remove them."},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef remove_bracketed(text):\n    regex = '\\([^)]*\\)'\n    text = re.sub(regex, '', text).replace('  ',' ')\n    return text\n\nscript_dataframe['Name'] = script_dataframe['Name'].apply(remove_bracketed)\nscript_dataframe['Sentence'] = script_dataframe['Sentence'].apply(remove_bracketed)\n\nscript_dataframe.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have already eliminated null values on our dataframe and also eliminated bracketed text from column `Name` and `Sentence` on our dataframe.\n\nFor further cleansing, we need to make values of our `Name` column homogenous. First thing we need to do to achieve this is making the `Name` column in lowercase text format."},{"metadata":{"trusted":true},"cell_type":"code","source":"script_dataframe['Name'] = script_dataframe['Name'].apply(lambda x: str(x).lower())\nscript_dataframe.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After making all names in lowercase format, we need to remove all non-aphabetical character in `Name` column. We can make a simple function to do regex substitution."},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef remove_non_alphabetic(text):\n    regex = '[^A-Za-z\\s]'\n    text = re.sub(regex, '', text).replace('  ',' ')\n    text = text if text[-1] != ' ' else text[:-1]\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"script_dataframe['Name'] = script_dataframe['Name'].apply(remove_non_alphabetic)\nscript_dataframe.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have all names in similar format, in lowercase and only consist of alphabetic characters.The first thing we need to do now is removing the naration text from our dataframe. As we previously known, some of the background conditions which categorized as narations are written in the same format as conversation. Some examples for this case are `EPISODE` and `CUT TO`.\n\nEasiest way to do this is by extracting the first word of each names and put them into a new column. This column will later be used to filter the names."},{"metadata":{"trusted":true},"cell_type":"code","source":"script_dataframe['First Token'] = script_dataframe['Name'].apply(lambda x: str(x).split(' ')[0])\nscript_dataframe.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To continue, we will now filter the background conditions from our dataframe. These entries will have `First Token` such as `episode`, `cut`, `int`, and `ext`."},{"metadata":{"trusted":true},"cell_type":"code","source":"script_dataframe = script_dataframe[(script_dataframe['First Token'] != 'cut') &\n                                    (script_dataframe['First Token'] != 'int') &\n                                    (script_dataframe['First Token'] != 'ext') &\n                                    (script_dataframe['First Token'] != 'episode')]\nscript_dataframe.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we have now filtered all of the backgoud conditions from our dataframe. Let's focus on column `Name` again.\n\nWe should check if this column only has normal name as its entries. There are many cases that can be considered as anomaly in person's name, such as name length. For now we will see are there any anomaly length on our `Name` column."},{"metadata":{"trusted":true},"cell_type":"code","source":"script_dataframe['Name Length'] = script_dataframe['Name'].apply(lambda x: len(str(x)))\nprint(script_dataframe['Name Length'].describe(percentiles=[.8,.9,.95,.99,.999,.9999,.99999,.999999]))\nprint(script_dataframe.info())\nprint(script_dataframe['Name Length'].value_counts().sort_values().head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on information above, we know that most of our value in column `Name` only have length no longer than 28 characters. In fact, as an outlier we have one name that consists of more than 300 characters.\n\nOnce again, just like null values on data, there are also many ways to handle outlier values on data. But for this case we will just eliminate the outlier because it only has small number, just one entry to be precise."},{"metadata":{"trusted":true},"cell_type":"code","source":"script_dataframe = script_dataframe[script_dataframe['Name Length'] <= 28]\nscript_dataframe.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Name Homogenization\nAs the next step, we previously know that our `Name` column may have different values for a same person. This can happen because of aliases, family names, nicknames, and others. However, it will take a lot of effort if we make all of our `Name` value homogen. Instead of doing that, we will just make homogen name of characters that matter the most in the data. Now let's see who are those characters that matter the most."},{"metadata":{"trusted":true},"cell_type":"code","source":"appearance_counts = script_dataframe.groupby(['Name'])['Sentence'].count().reset_index()\nappearance_counts.Sentence.describe(percentiles=[.8,.9,.95,.99,.999])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on information above, we figured that only 10% of the characters have more than 80 different sentences. These characters will we be focused on and made homogen."},{"metadata":{"trusted":true},"cell_type":"code","source":"most_sentence_characters = appearance_counts[appearance_counts['Sentence'] > 80].sort_values(by=['Sentence'], ascending=[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Take a peek on the dataset, we will see that characters on this dataset still consist of some universal aliases like `man` and `soldier` which are not owned by a single character, instead used by many different characters. These aliases should be removed from dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"most_sentence_characters = most_sentence_characters[(most_sentence_characters['Name'] != 'man') &\n                                                    (most_sentence_characters['Name'] != 'soldier')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we already removed all of the universal aliases from our character dataframes. Let's take a look of its unique values."},{"metadata":{"trusted":true},"cell_type":"code","source":"char_names = most_sentence_characters['Name'].unique()\nprint('total: ' + str(len(char_names)))\nchar_names","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only 63 characters with more than 80 sentences (from now on we will address this as `important characters`) are left on our character dataframe. It takes not a big effort for us to manually homogenize the names on this list. Also, notice that some of the characters on this list have other name or alias in the series. We are going to make a mapping for these aliases too.\n\nFor now, we will make a new dataframe containing unique name and alias from these characters."},{"metadata":{"trusted":true},"cell_type":"code","source":"char_names = ['tyrion lannister', 'jon snow', 'jaime lannister', 'sansa stark', 'arya stark', 'davos',\n              'theon greyjoy', 'bronn', 'varys', 'brienne', 'bran stark', 'tywin lannister', 'jorah mormont', 'stannis baratheon',\n              'margaery tyrell', 'ramsay bolton', 'melisandre', 'robb stark', 'jon snow', 'shae', 'gendry baratheon',\n              'tormund', 'gilly', 'tyrion lannister', 'missandei', 'catelyn stark', 'ygritte', 'olenna tyrell', 'daario',\n              'podrick', 'yara greyjoy', 'osha', 'oberyn martell', 'jaqen hghar','grey worm', 'qyburn', 'talisa', 'meera', 'catelyn stark',\n              'thoros','robert baratheon', 'arya stark', 'shireen', 'sparrow', 'beric', 'euron greyjoy','sansa stark', 'grenn', 'jorah mormont']\n\nalias_mapper = ['sandor clegane','petyr baelish','petyr baelish','sam tarly','eddard stark','cersei lannister','joffrey lannister',\n                'tommen lannister','daenerys targaryen','daenerys targaryen']\n\nalias = ['hound','littlefinger','baelish','samwell tarly','ned stark','cersei baratheon','joffrey baratheon',\n         'tommen baratheon','daenerys stormborn','dany']\n\nchar_names = sorted(list(pd.Series(char_names).unique()))\nchar_alias = [None for i in range(0, len(char_names))]\nchar_names.extend(alias_mapper)\nchar_alias.extend(alias)\nname_dictionary = pd.DataFrame({\n    \"Base Name\": char_names,\n    \"Alias\": char_alias\n})\n\nname_dictionary = name_dictionary[['Base Name','Alias']]\nname_dictionary = name_dictionary.sort_values(by=['Base Name'])\nname_dictionary.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After completing the name dictionary for important characters, the next thing we do is mapping these name to our main dataframe. We will first make a mapper dataframe to be used later for mapping purpose. To do this we need to make a copy of our main dataframe.\n\nBut before we make a copy, there are some cases that need to be highlighted. Some of the character aliases contain word like `high` and `the`, for example `high sparrow` and `the hound`. Keeping these words will get us into trouble when scoring the string similarity later because these words will increase the scores two different names that have prefix of these words. Therefore, we need to remove these words from our character names."},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_words(x):\n    new_name = x.replace('the ','')\n    new_name = new_name.replace('high ', '')\n    return new_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"script_dataframe['Name'] = script_dataframe['Name'].apply(clean_words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can start making the mapper for our important characters starting with making a copy of our main dataframe as a new dataframe object."},{"metadata":{"trusted":true},"cell_type":"code","source":"script_for_mapper = script_dataframe.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first step of creating this mapper is generating a cartesian product of our new dataframe and our important characters dataframe. The easiest way to do this is by creating a column with similar name and values for both dataframe. This column will be used for merging in which we will use this column as a key for doing dataframe left outer merge. We add column `Cartesian Key` with value of `0` for both data frame, and then we do outer merge on those dataframes using the column `Cartesian Key`."},{"metadata":{"trusted":true},"cell_type":"code","source":"script_for_mapper['Cartesian Key'] = 0\nname_dictionary['Cartesian Key'] = 0\nscript_for_mapper = script_for_mapper.merge(name_dictionary, on=['Cartesian Key'], how='outer')\nscript_for_mapper.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice that there is massive increase on number of rows of our dataframe. This is happened because of cartesian product basically mapping every rows in first dataframe to every rows in second dataframe. It increases total row by multiplying number of rows of first dataframe by number of rows of second dataframe.\n\nIn order to create a proper mapper, we need to make sure that each name of our dataframe is the really the name of our important characters. In this process we will also tackle case of typo writing on our dataframe. To do this we will get similarity score between our character names and the important characters dataset, either the name or the alias.\n\nThis algorithm below will do the scoring process. As for the string similarity, after some researches I found that the most suitable algorithm for this is the `Jaro Winkler` algorithm. I will use package from `jellyfish` that contains `Jaro Winkler` algorithm. You can read more about `Jaro Winkler` algorithm here https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance, and the `jellyfish` documentation here https://jellyfish.readthedocs.io/."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install jellyfish","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from jellyfish import jaro_winkler\n\ndef get_similarity(row):\n    current_name = row['Name']\n    base_name = row['Base Name']\n    alias = row['Alias']\n    \n    score_base_name = 0\n    score_alias = 0\n    \n    if current_name == base_name:\n        score_base_name = 1\n    else:\n        listed_current_name = current_name.split(' ')\n        listed_base_name = base_name.split(' ')\n        \n        if len(listed_current_name) > 1 and len(listed_base_name) > 1:\n            family_name_similarity = jaro_winkler(listed_current_name[1], listed_base_name[1])\n            if family_name_similarity > .9:\n                score_base_name = jaro_winkler(listed_current_name[0], listed_base_name[0])\n            else:\n                score_base_name = jaro_winkler(current_name, base_name)\n        elif len(listed_base_name) > 1:\n            score_base_name = jaro_winkler(current_name, listed_base_name[0])\n        else:\n            score_base_name = jaro_winkler(current_name, base_name)\n        \n        if alias != None:\n            listed_alias = alias.split(' ')\n            if len(listed_current_name) > 1 and len(listed_alias) > 1:\n                family_name_similarity = jaro_winkler(listed_current_name[1], listed_alias[1])\n                if family_name_similarity > .9:\n                    score_base_name = jaro_winkler(listed_current_name[0], listed_alias[0])\n                else:\n                    score_base_name = jaro_winkler(current_name, alias)\n            elif len(listed_alias) > 1:\n                score_base_name = jaro_winkler(current_name, listed_alias[0])\n            else:\n                score_base_name = jaro_winkler(current_name, alias)\n    \n    return score_base_name if score_base_name > score_alias else score_alias","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"script_for_mapper['Name Similarity'] = script_for_mapper.apply(get_similarity, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After some data exploration I found that minimum score for the name similarity that can be accepted as a same character is `0.89`. Therefore, we will make a new name column named `Homogenized Name` and fill them with condition if the similarity score is greater than `0.89` use name from the important character dataset, which in this dataframe stored as `Base Name` column, and for rows with similarity score les than `0.89` use `None`."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_homogenized_name(x):\n    similarity = x['Name Similarity']\n    name = x['Name']\n    base_name = x['Base Name']\n    \n    if similarity > .89:\n        return base_name\n    else:\n        return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"script_for_mapper['Homogenized Name'] = script_for_mapper.apply(get_homogenized_name, axis=1)\nscript_for_mapper.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By now, we should have our important characters with both of their `Name` and `Homogenized Name` filled with non null object. Next we will do is extracting the `Name` and `Homogenized Name` columns and dropping the `None` values so that the dataframe only contains name of our important characters."},{"metadata":{"trusted":true},"cell_type":"code","source":"script_for_mapper = script_for_mapper[['Name','Homogenized Name']].dropna().drop_duplicates()\nscript_for_mapper.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this point, there is a special case on this mapper. A character named `Robett Glover` is mapped as `Robert Baratheon` on our mapper. This is happened because on the script from `genius.com`, they wrote the name `robett` without the family name. If you read our algorithm for scoring the string similarity, after comparing name of `robett` and `robert` the algorithm will give a high similarity score as result. For this case, I will manually delete the row from our mapper since it is the fastest way for cleaning the table."},{"metadata":{"trusted":true},"cell_type":"code","source":"script_for_mapper = script_for_mapper.drop(1031097)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have a clean mapper for our important characters, we can finally map the name in our main dataframe to our important characters mapper dataframe. We can do the mapping by doing a simple pandas left merge on column `Name` of each dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"script_dataframe = script_dataframe.merge(script_for_mapper, on=['Name'], how='left')\nscript_dataframe.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now have successfully mapped the name of our important characters to their name on main dataframe. We will now clean the `Name` column by filling them with available value in `Homogenized Name` column, or we can say changing the name of our important characters to their clean name."},{"metadata":{"trusted":true},"cell_type":"code","source":"script_dataframe['Homogenized Name'] = script_dataframe['Homogenized Name'].fillna('')\nscript_dataframe['Name'] = script_dataframe[['Name','Homogenized Name']].apply(lambda x: x[1] if x[1] != '' else x[0], axis=1)\nscript_dataframe.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we managed to map and homogenize the name of our important characters. Let's restore our main dataframe to its orginal format and remove the duplicated values."},{"metadata":{"trusted":true},"cell_type":"code","source":"script_dataframe = script_dataframe[['Release Date','Season','Episode','Episode Title','Name','Sentence']].drop_duplicates()\nscript_dataframe.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sentence Cleansing\nCase that most likely occured on column `Sentence` is not as much as in `Name` column. One of the reason is because we already clean some of them on `Name` cleansing process.\n\nHowever there are still cases that make our the `Sentence` column still contains dirty data. First case is the sentence is not properly started meaning that first character on the sentence is a non-aphanumeric character. This can happen because of our previous cleansing in which we remove the bracketed text on our `Name` and `Sentence` columns. Next case is the sentence written in differen format than most of them, or to be specific some sentences are written inside quote `''` or double quote `\"\"`, while others are not. The last case is sentences that contain empty string which might also happen because of removing of the bracketed texts.\n\nThe function below contains algorithm that is going to handle the non-proper form sentences."},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef clean_sentence(text):\n    \n    text_list = text.split(' ')\n    \n    if len(text_list) > 1:\n        text = ''.join(' ' + word for word in text_list if word != '')[1:].replace('    ',' ').replace('   ',' ').replace('  ',' ')\n        if len(text) > 1:\n            text = text[:-1] if text[-1] == ' ' else text\n            if text[0] == '\"' and text[-1] == '\"':\n                text = text[1:-1]\n            if text[0] == '\\'' and text[-1] == '\\'':\n                text = text[1:-1]\n\n        regex = '^[^A-Za-z0-9]*'\n        text = re.sub(regex, '', text).replace('  ',' ')\n        if len(text) > 0:\n            text = text if text[-1] != ' ' else text[:-1]\n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"script_dataframe['Clean Sentence'] = script_dataframe['Sentence'].apply(clean_sentence)\nscript_dataframe.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the `Sentence` column values should all be in the same format. The last case that needed to be handled is the empty string. We can easily handle this by making a new column that contains value of the lenght of each `Sentence` value in our main dataframe. And then use the column to filter empty strings."},{"metadata":{"trusted":true},"cell_type":"code","source":"script_dataframe['Length Sentence'] = script_dataframe['Clean Sentence'].apply(len)\nscript_dataframe = script_dataframe[script_dataframe['Length Sentence'] > 1]\nscript_dataframe.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We already have a clean sentence and also removed all empty strings on our dataframe. Now we should replace values on `Sentence` column using the values on `Clean Sentence` column and then restore our dataframe to its original structure."},{"metadata":{"trusted":true},"cell_type":"code","source":"script_dataframe['Sentence'] = script_dataframe['Clean Sentence']\nscript_dataframe = script_dataframe[['Release Date','Season','Episode','Episode Title','Name','Sentence']].drop_duplicates()\nscript_dataframe.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the final touch, it might be hard to notice but quote symbol in `Sentence` column is using `` instead of `'`. A simple regex replace can fix this problem. While we are doing this, we can as well replace the `d` word that can easily be seen on the first row. This is actually a simple alternative for word `do`. Again, we will use regex replace to fix them."},{"metadata":{"trusted":true},"cell_type":"code","source":"script_dataframe['Sentence'] = script_dataframe['Sentence'].apply(lambda x: str(x).replace('', '\\''))\nscript_dataframe['Sentence'] = script_dataframe['Sentence'].apply(lambda x: str(x).replace('d\\'', 'do '))\nscript_dataframe.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"script_dataframe.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, our process in scrapping and cleansing the dataset for `Game of Thrones` has finished. The last thing to do is export the dataframe to an external file."},{"metadata":{"trusted":true},"cell_type":"code","source":"script_dataframe.to_csv('Game_of_Thrones_Script.csv', encoding='utf-8', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nScrapping web to extract the data scattered around them may take a lot of effort. We need to do bunch of manual inspect element just to get the exact position of data or information that we want to collect from all over the html parts. BeautifulSoup functionality is good, but for case like this we need to combine this package with manual inspect element to produce the desired result faster. Some of you might have different and even a better solution on extracting data from online sources, and that is a great thing. As for me, I am still learning and experimenting different methodologies to find my best practice on doing so.\n\nAs for the closing, you can use this data and mine the information provided there as you please. Also, give me feedback both on this dataset and the process of getting them if you have the time. \n\nThank you! \n\nAnd, Have a nice day:)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}