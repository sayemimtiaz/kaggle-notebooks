{"cells":[{"metadata":{},"cell_type":"markdown","source":"# OVERVIEW OF STUDY","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The **GOAL** of this study is building linear and non-linear models by using Hitters dataset. \nAll built models will be tuned to minimize prediction errors - test dataset errors- by RMSE metric.\nHold-out method will be used to split dataset for validation. \n\n**Content of Study:**\n1. Business Understanding\n1. Data Understanding\n2. Data Preparation\n    - Checking Outliers (Quantile Method, Suppressing)\n    - Missing Values (Filling Missing Values)\n    - Feature Engineering (Label Encoding)\n    - Feature Scaling (Normalization)\n3. Modeling (Linear, Non-Linear Modeling and Tuning)\n    - Linear Regression Models: Linear, Ridge, Lasso, ElasticNet\n    - Non-Linear Models: KNN, SVR, ANN, CART, RF, GBM, XGB, LGBM, CatBoost\n4. Evaluation & Proposals","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# BUSINESS UNDERSTANDING","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Dataset: Hitters\n* Description: Major League Baseball Data from the 1986 and 1987 seasons.\n* Source: This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n* A data frame with 322 observations of major league players on the following 20 variables.\n----\n* AtBat: Number of times at bat in 1986\n* Hits: Number of hits in 1986\n* HmRun: Number of home runs in 1986\n* Runs: Number of runs in 1986\n* RBI: Number of runs batted in in 1986\n* Walks: Number of walks in 1986\n----\n* PutOuts: Number of put outs in 1986\n* Assists: Number of assists in 1986\n* Errors: Number of errors in 1986\n---- \n* CAtBat: Number of times at bat during his career\n* CHits: Number of hits during his career\n* CHmRun: Number of home runs during his career\n* CRuns: Number of runs during his career\n* CRBI: Number of runs batted in during his career\n* CWalks: Number of walks during his career\n---- \n* Years:Number of years in the major leagues\n* League: A factor with levels A and N indicating player's league at the end of 1986\n* Division: A factor with levels E and W indicating player's division at the end of 1986\n* NewLeague: A factor with levels A and N indicating player's league at the beginning of 1987\n* Salary: 1987 annual salary on opening day in thousands of dollars\n----","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# DATA UNDERSTANDING","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install xgboost\n#!pip install lightgbm\n#!pip install catboost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#close warnings\nimport warnings\nwarnings.simplefilter(action='ignore')\n\n#import libraries for linear models\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, RidgeCV, LassoCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\n#import libraries for non-linear models (additional to linear models)\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#read dataset\ndf_hitters=pd.read_csv(\"../input/hitters-baseball-data/Hitters.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#copy dataset in case of reloading dataset immediately\ndf=df_hitters.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get a summary of descriptive statistics\ndf.describe([0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99]).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check number of missing values\ndf.isnull().sum().sort_values(ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get columns names according to variable types >> int64, float64, object\n#it helps us to use fancy indexes, manipulate different types separately and relatively reduce memory usage instead of creating two dataframes\ncat_cols=[col for col in df.columns if df[col].dtype=='object']\nnum_cols=[col for col in df.columns if col not in cat_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#as a summary, there are 20 variables and 'Salary' is target variable. \n#rest 19 variables (16 are numerical, 3 are categorical/object) are independent variables.\n#there are 59 null values only in target variable. ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATA PREPARATION","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#detect outliers for target variable\nsns.boxplot(x=df['Salary'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#before filling missing values in salary, trim/correct them slightly\nupper_limit=df['Salary'].quantile(0.95)\noutliers_upper=df[df[\"Salary\"] > upper_limit]\ndf.loc[df[\"Salary\"] > upper_limit, \"Salary\"] = upper_limit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check outliers again\nsns.boxplot(x=df['Salary'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#make a decision about handling NaN values: 1. Drop them 2. Fill them.\n#there are 59 missing values in 322 rows. 18% is not a small ratio and can affect results. Filling seems a better way.\n#if dropping is being preferred, code: df.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fill missing values according to categorical variables and mean\ndf['Salary']=df.groupby(['League','Division'])['Salary'].transform(lambda x: x.fillna(x.mean()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#no missing values:)\ndf.isnull().sum().sort_values(ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check categorical variables and number of subcategories\nprint(df['League'].value_counts())\nprint(df['Division'].value_counts())\nprint(df['NewLeague'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#all categorical variables consist of two subcategories. Then, use Label Encoding (LE).\n#LE assigns values as 0-1 (means to model: coequal variables)\nle_League=LabelEncoder()\nle_Division=LabelEncoder()\nle_NewLeague=LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['League']=le_League.fit_transform(df['League'])\ndf['Division']=le_Division.fit_transform(df['Division'])\ndf['NewLeague']=le_NewLeague.fit_transform(df['NewLeague'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#to get original categorical values below inverse code can be used\n#le_League.inverse_transform(df['League'])\n#le_Division.inverse_transform(df['League'])\n#le_NewLeague.inverse_transform(df['League'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Scaling: Normalization of Numerical Variables (Except from target variable)\n#First remove Salary variable from numerical columns - you don't want to normalize it\nnum_cols.remove('Salary')\nnorm_num_df=preprocessing.normalize(df[num_cols])\nnorm_num_df=pd.DataFrame(norm_num_df, columns=num_cols)\nnorm_num_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#change dataframe with normalized variables\ndf=pd.concat([norm_num_df, df[cat_cols], df['Salary']], axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MODELING (LINEAR MODELS)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#get dependent and independent values\ny=df[['Salary']] #dependent/target variable\nX=df.drop(['Salary'], axis=1)  #independent variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#divide dataset to train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=46)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for linear models, according to models' documentation and logic of penalty, it is advised that alpha, l1_ratio values are between 0 and 1.\n#if you define alpha=0 it turns models to linear regression w/o penalties:)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lineer: Primitive Model\nlin_reg=LinearRegression().fit(X_train, y_train)\ny_pred_lin_reg=lin_reg.predict(X_test)\nlin_reg_rmse=np.sqrt(mean_squared_error(y_test, y_pred_lin_reg))\nlin_reg_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ridge: Primitive Model\nrid_reg=Ridge().fit(X_train, y_train)\ny_pred_rid_reg=rid_reg.predict(X_test)\nrid_reg_rmse=np.sqrt(mean_squared_error(y_test, y_pred_rid_reg))\nprint(rid_reg_rmse)\n\n#Ridge: CV Model\nalpha_rid = [0.001, 0.005, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1]\nrid_reg_cv=RidgeCV(alphas = alpha_rid, scoring = \"neg_mean_squared_error\", cv = 10, normalize = True)\nrid_reg_cv.fit(X_train, y_train)\nprint(rid_reg_cv.alpha_)\n\n#Ridge: Tuned Model\nrid_reg_tuned=Ridge(rid_reg_cv.alpha_).fit(X_train, y_train)\ny_pred_rid_reg_tuned=rid_reg_tuned.predict(X_test)\nrid_reg_tuned_rmse=np.sqrt(mean_squared_error(y_test, y_pred_rid_reg_tuned))\nrid_reg_tuned_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lasso: Primitive Model\nlas_reg=Lasso().fit(X_train, y_train)\ny_pred_las_reg=las_reg.predict(X_test)\nlas_reg_rmse=np.sqrt(mean_squared_error(y_test, y_pred_las_reg))\nprint(las_reg_rmse)\n\n#Lasso: CV Model\nalpha_las = [0.001, 0.005, 0.01, 0.02, 0.03, 0.05, 0.1, 0.5, 1, 1.1]\nlas_reg_cv = LassoCV(alphas = alpha_las, cv = 10, normalize = True)\nlas_reg_cv.fit(X_train, y_train)\nprint(las_reg_cv.alpha_)\n\n#Lasso: Tuned Model\nlas_reg_tuned = Lasso(alpha = las_reg_cv.alpha_).fit(X_train,y_train)\ny_pred_las_reg_tuned = las_reg_tuned.predict(X_test)\nlas_reg_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred_las_reg_tuned))\nlas_reg_tuned_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ElasticNet: Primitive Model\nenet_reg=ElasticNet().fit(X_train, y_train)\ny_pred_enet_reg=enet_reg.predict(X_test)\nenet_reg_rmse=np.sqrt(mean_squared_error(y_test, y_pred_enet_reg))\nprint(enet_reg_rmse)\n\n#ElasticNet: CV Model\nenet_reg_params = {\"l1_ratio\": [0.001, 0.01, 0.1, 0.5, 0.9, 1, 1.1],\n              \"alpha\":[0.001, 0.01, 0.05, 0.1, 0.2, 0.5, 1, 1.1]}\nenet_reg_cv = GridSearchCV(enet_reg, enet_reg_params, cv = 10).fit(X, y)\nprint(enet_reg_cv.best_params_)\n\n#ElasticNet: Tuned Model\nenet_reg_tuned = ElasticNet(**enet_reg_cv.best_params_).fit(X_train,y_train)\ny_pred_enet_reg_tuned = enet_reg_tuned.predict(X_test)\nenet_reg_tuned_rmse = np.sqrt(mean_squared_error(y_test,y_pred_enet_reg_tuned))\nenet_reg_tuned_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MODELING (NON-LINEAR MODELS)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#KNN: Primitive Model\nknn_model=KNeighborsRegressor().fit(X_train, y_train)\nprint(knn_model)\ny_pred_knn_model=knn_model.predict(X_test)\ny_pred_knn_model_rmse=np.sqrt(mean_squared_error(y_test, y_pred_knn_model))\ny_pred_knn_model_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#KNN: CV Model\nknn_params={\"n_neighbors\": np.arange(2,20,1)}\nknn_cv_model=GridSearchCV(knn_model, knn_params, cv=10, n_jobs=-1, verbose=2).fit(X_train, y_train)\nprint(knn_cv_model.best_params_)\n#KNN: Tuned Model\nknn_tuned=KNeighborsRegressor(**knn_cv_model.best_params_).fit(X_train, y_train)\ny_pred_knn_tuned=knn_tuned.predict(X_test)\ny_pred_knn_tuned_rmse=np.sqrt(mean_squared_error(y_test, y_pred_knn_tuned))\ny_pred_knn_tuned_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SVR: Primitive Model\nsvr_model=SVR().fit(X_train, y_train)\nprint(svr_model)\ny_pred_svr_model=svr_model.predict(X_test)\ny_pred_svr_model_rmse=np.sqrt(mean_squared_error(y_test, y_pred_svr_model))\ny_pred_svr_model_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SVR: CV Model\nsvr_params={\"C\": (0.01, 0.1, 0.5, 0.9, 1),\n           \"kernel\": ('rbf', 'linear')}\nsvr_cv_model=GridSearchCV(svr_model, svr_params, cv=10, n_jobs=-1, verbose=2).fit(X_train, y_train)\nprint(svr_cv_model.best_params_)\n#SVR: Tuned Model\nsvr_tuned=SVR(**svr_cv_model.best_params_).fit(X_train, y_train)\ny_pred_svr_tuned=svr_tuned.predict(X_test)\ny_pred_svr_tuned_rmse=np.sqrt(mean_squared_error(y_test, y_pred_svr_tuned))\ny_pred_svr_tuned_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ANN: Primitive Model\n#independent variables were already scaled/normalized, then no need to be scaled again\nann_model=MLPRegressor(random_state=42).fit(X_train, y_train)\nprint(ann_model)\ny_pred_ann_model=ann_model.predict(X_test)\ny_pred_ann_model_rmse=np.sqrt(mean_squared_error(y_test, y_pred_ann_model))\ny_pred_ann_model_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ANN: CV Model\nann_params = {\"alpha\": [0.001, 0.01, 0.1, 0.2, 0.3, 0.5], \n             \"hidden_layer_sizes\": [(5,5), (10,10), (20,20), (100,100)],\n             \"solver\": ['lbfgs', 'sgd', 'adam']}\nann_cv_model=GridSearchCV(ann_model, ann_params, cv=10, n_jobs=-1, verbose=2).fit(X_train, y_train)\nprint(ann_cv_model.best_params_)\n#ANN: Tuned Model\nann_tuned=MLPRegressor(**ann_cv_model.best_params_, random_state=42).fit(X_train, y_train)\ny_pred_ann_tuned=ann_tuned.predict(X_test)\ny_pred_ann_tuned_rmse=np.sqrt(mean_squared_error(y_test, y_pred_ann_tuned))\ny_pred_ann_tuned_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CART: Primitive Model\ncart_model = DecisionTreeRegressor(random_state=42).fit(X_train,y_train)\nprint(cart_model)\ny_pred_cart_model=cart_model.predict(X_test)\ny_pred_cart_model_rmse=np.sqrt(mean_squared_error(y_test, y_pred_cart_model))\ny_pred_cart_model_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CART: CV Model\ncart_params = {\"max_depth\": [2, 3, 4, 5, 10, None],\n              \"min_samples_split\": [2, 5, 10, 12, 20]}\ncart_cv_model = GridSearchCV(cart_model, cart_params, cv = 10, n_jobs = -1, verbose=2).fit(X_train, y_train)\nprint(cart_cv_model.best_params_)\n\n#CART: Tuned Model\ncart_tuned=DecisionTreeRegressor(**cart_cv_model.best_params_, random_state=42).fit(X_train, y_train)\ny_pred_cart_tuned=cart_tuned.predict(X_test)\ny_pred_cart_tuned_rmse=np.sqrt(mean_squared_error(y_test, y_pred_cart_tuned))\ny_pred_cart_tuned_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RF: Primitive Model\nrf_model = RandomForestRegressor(random_state=42).fit(X_train,y_train)\nprint(rf_model)\ny_pred_rf_model=rf_model.predict(X_test)\ny_pred_rf_model_rmse=np.sqrt(mean_squared_error(y_test, y_pred_rf_model))\ny_pred_rf_model_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RF: CV Model\nrf_params = {\"max_depth\": [5, 8, 10, None],\n             \"max_features\": [3, 5, 10, 15, 17],\n             \"min_samples_split\": [2, 3, 5, 10],\n             \"n_estimators\": [100, 200, 500]}\nrf_cv_model = GridSearchCV(rf_model, rf_params, cv = 10, n_jobs = -1, verbose=2).fit(X_train, y_train)\nprint(rf_cv_model.best_params_)\n\n#RF: Tuned Model\nrf_tuned=RandomForestRegressor(**rf_cv_model.best_params_, random_state=42).fit(X_train, y_train)\ny_pred_rf_tuned=rf_tuned.predict(X_test)\ny_pred_rf_tuned_rmse=np.sqrt(mean_squared_error(y_test, y_pred_rf_tuned))\ny_pred_rf_tuned_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#GBM: Primitive Model\ngbm_model = GradientBoostingRegressor(random_state=42).fit(X_train,y_train)\nprint(gbm_model)\ny_pred_gbm_model=gbm_model.predict(X_test)\ny_pred_gbm_model_rmse=np.sqrt(mean_squared_error(y_test, y_pred_gbm_model))\ny_pred_gbm_model_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#GBM: CV Model\ngbm_params = {\"learning_rate\": [0.01, 0.1, 0.5],\n             \"max_depth\": [2, 3, 4],\n             \"n_estimators\": [1000, 1500, 2000],\n             \"subsample\": [0.2, 0.3, 0.5],\n             \"loss\": [\"ls\",\"lad\",\"quantile\"]}\ngbm_cv_model = GridSearchCV(gbm_model, gbm_params, cv = 10, n_jobs = -1, verbose=2).fit(X_train, y_train)\nprint(gbm_cv_model.best_params_)\n\n#GBM: Tuned Model\ngbm_tuned=GradientBoostingRegressor(**gbm_cv_model.best_params_, random_state=42).fit(X_train, y_train)\ny_pred_gbm_tuned=gbm_tuned.predict(X_test)\ny_pred_gbm_tuned_rmse=np.sqrt(mean_squared_error(y_test, y_pred_gbm_tuned))\ny_pred_gbm_tuned_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#XGB: Primitive Model\nxgb_model = XGBRegressor(random_state=42).fit(X_train,y_train)\nprint(xgb_model)\ny_pred_xgb_model=xgb_model.predict(X_test)\ny_pred_xgb_model_rmse=np.sqrt(mean_squared_error(y_test, y_pred_xgb_model))\ny_pred_xgb_model_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#XGB: CV Model\nxgb_params = {\"learning_rate\": [0.01, 0.1, 0.5],\n             \"max_depth\": [2, 3, 5, 8],\n             \"n_estimators\": [100, 200, 1000],\n             \"colsample_bytree\": [0.5, 0.8, 1]}\nxgb_cv_model = GridSearchCV(xgb_model, xgb_params, cv = 10, n_jobs = -1, verbose=2).fit(X_train, y_train)\nprint(xgb_cv_model.best_params_)\n\n#XGB: Tuned Model\nxgb_tuned=XGBRegressor(**xgb_cv_model.best_params_, random_state=42).fit(X_train, y_train)\ny_pred_xgb_tuned=xgb_tuned.predict(X_test)\ny_pred_xgb_tuned_rmse=np.sqrt(mean_squared_error(y_test, y_pred_xgb_tuned))\ny_pred_xgb_tuned_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LGBM: Primitive Model\nlgbm_model = LGBMRegressor(random_state=42).fit(X_train,y_train)\nprint(lgbm_model)\ny_pred_lgbm_model=lgbm_model.predict(X_test)\ny_pred_lgbm_model_rmse=np.sqrt(mean_squared_error(y_test, y_pred_lgbm_model))\ny_pred_lgbm_model_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LGBM: CV Model\nlgbm_params = {\"learning_rate\": [0.01, 0.1, 0.5],\n             \"max_depth\": [2, 3, 4, 5],\n             \"n_estimators\": [200, 500, 700, 1000],\n             \"colsample_bytree\": [0.6, 0.7, 0.8, 1]}\nlgbm_cv_model = GridSearchCV(lgbm_model, lgbm_params, cv = 10, n_jobs = -1, verbose=2).fit(X_train, y_train)\nprint(lgbm_cv_model.best_params_)\n\n#LGBM: Tuned Model\nlgbm_tuned=LGBMRegressor(**lgbm_cv_model.best_params_, random_state=42).fit(X_train, y_train)\ny_pred_lgbm_tuned=lgbm_tuned.predict(X_test)\ny_pred_lgbm_tuned_rmse=np.sqrt(mean_squared_error(y_test, y_pred_lgbm_tuned))\ny_pred_lgbm_tuned_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CATB: Primitive Model\ncatb_model = CatBoostRegressor(verbose=False, random_state=42).fit(X_train,y_train)\nprint(catb_model)\ny_pred_catb_model=catb_model.predict(X_test)\ny_pred_catb_model_rmse=np.sqrt(mean_squared_error(y_test, y_pred_catb_model))\ny_pred_catb_model_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CATB: CV Model\ncatb_params = {\"learning_rate\": [0.01, 0.1, 0.5],\n               \"iterations\": [100, 200, 500],\n              \"depth\": [3, 5, 8]}\ncatb_cv_model = GridSearchCV(catb_model, catb_params, cv = 10, n_jobs = -1).fit(X_train, y_train)\nprint(catb_cv_model.best_params_)\n\n#CATB: Tuned Model\ncatb_tuned=CatBoostRegressor(**catb_cv_model.best_params_, verbose=False, random_state=42).fit(X_train, y_train)\ny_pred_catb_tuned=lgbm_tuned.predict(X_test)\ny_pred_catb_tuned_rmse=np.sqrt(mean_squared_error(y_test, y_pred_catb_tuned))\ny_pred_catb_tuned_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EVALUATING","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_colwidth', -1)\nresults = pd.DataFrame({\"Model Name\": [\"Primitive Test Errors\", \"Tuning Params\", \"Tuned Test Errors\"],\n                        \"Linear Reg\": [lin_reg_rmse, np.nan, np.nan],\n                        \"Ridge Reg\": [rid_reg_rmse, rid_reg_cv.alpha_, rid_reg_tuned_rmse],\n                        \"Lasso Reg\": [las_reg_rmse, las_reg_cv.alpha_, las_reg_tuned_rmse],\n                        \"ElasticNet Reg\": [enet_reg_rmse, enet_reg_cv.best_params_, las_reg_tuned_rmse],\n                        \"KNN\": [y_pred_knn_model_rmse, knn_cv_model.best_params_, y_pred_knn_tuned_rmse],\n                        \"SVR\": [y_pred_svr_model_rmse, svr_cv_model.best_params_, y_pred_svr_tuned_rmse],\n                        \"ANN\": [y_pred_ann_model_rmse, ann_cv_model.best_params_, y_pred_ann_tuned_rmse],\n                        \"CART\": [y_pred_cart_model_rmse, cart_cv_model.best_params_, y_pred_cart_tuned_rmse],\n                        \"RF\": [y_pred_rf_model_rmse, rf_cv_model.best_params_, y_pred_rf_tuned_rmse],\n                        \"GBM\": [y_pred_gbm_model_rmse, gbm_cv_model.best_params_, y_pred_gbm_tuned_rmse],\n                        \"XGB\": [y_pred_xgb_model_rmse, xgb_cv_model.best_params_, y_pred_xgb_tuned_rmse],\n                        \"LGBM\": [y_pred_lgbm_model_rmse, lgbm_cv_model.best_params_, y_pred_lgbm_tuned_rmse],\n                        \"CATB\": [y_pred_catb_model_rmse, catb_cv_model.best_params_, y_pred_catb_tuned_rmse]\n                        })\n\nresults.set_index(\"Model Name\", inplace=True)\nresults.T.sort_values(by=\"Tuned Test Errors\", ascending=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#GBM: Feature Importances & Visualization\nimportance=pd.DataFrame({'importance': gbm_tuned.feature_importances_ * 100},\n                       index=X_train.columns)\n\nimportance.sort_values(by='importance', axis=0, ascending=True). plot(kind='barh', color='g')\n\nplt.xlabel('Variable Importances')\nplt.gca().legend_=None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RF: Feature Importances & Visualization\nimportance=pd.DataFrame({'importance': rf_tuned.feature_importances_ * 100},\n                       index=X_train.columns)\n\nimportance.sort_values(by='importance', axis=0, ascending=True). plot(kind='barh', color='g')\n\nplt.xlabel('Variable Importances')\nplt.gca().legend_=None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#XGB: Feature Importances & Visualization\nimportance=pd.DataFrame({'importance': xgb_tuned.feature_importances_ * 100},\n                       index=X_train.columns)\n\nimportance.sort_values(by='importance', axis=0, ascending=True). plot(kind='barh', color='g')\n\nplt.xlabel('Variable Importances')\nplt.gca().legend_=None","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### As a conlusion, best model seems like GBM. Top models' test errors are similar to each other.\n\n### When we look at feature importances of top 3 models (GBM, RF, XGB), we see that \"AtBat, Years, Chits\" features are important. \n### We can back to data understanding and preparation steps and focus on important features of top models (AtBat, Years, Chits).","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}