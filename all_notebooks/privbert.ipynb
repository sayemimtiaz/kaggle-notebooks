{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Training the models\n\nThis is the notebook that was used to train the models for my capstone. Original code is meant to be used on kaggle:\nhttps://www.kaggle.com/lukasbusch/privbert\nPart is mainly modelled after:\nhttps://towardsdatascience.com/transformers-for-multilabel-classification-71a1a0daf5e1\nIn this notebook:\n- All clasifier models are trained for the 5-fold sets\n- A gridsearch is performed for each classifier and the results are saved","metadata":{"id":"RE04qAVhTZNE"}},{"cell_type":"code","source":"#importing libraries\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport torch\nfrom torch.nn import BCEWithLogitsLoss, BCELoss\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score\nimport pickle\nfrom transformers import *\nfrom tqdm import tqdm, trange\nfrom ast import literal_eval\n","metadata":{"id":"Cthi9q5doY7U","outputId":"f4875ec4-2d18-4f73-b013-f95f651fc05d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# using GPU (don't bother without)\ndevice_name = tf.test.gpu_device_name()\nif device_name != '/device:GPU:0':\n  raise SystemError('GPU device not found')\nprint('Found GPU at: {}'.format(device_name))\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)","metadata":{"id":"Znf8tV3WX76_","outputId":"21f17602-5b51-42cb-c751-007ccab09454","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import module we'll need to import our custom module (needed for kaggle)\nfrom shutil import copyfile\n\n# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"../input/privbert-data/data_processing.py\", dst = \"../working/data_processing.py\")\ncopyfile(src = \"../input/privbert-data/pytorch_classifier.py\", dst = \"../working/pytorch_classifier.py\")\ncopyfile(src = \"../input/privbert-data/privbert_gridsearches3.csv\", dst = \"../working/privbert_gridsearches.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#initiate our data\nfrom data_processing import Op115OneHots\nimport random\nTRAIN_PATH = '../input/privbert-data/op115_data/op115_train_k0.csv' # here k0 is used, for each k this had to be changed\nVAL_PATH = '../input/privbert-data/op115_data/op115_val_k0.csv'\nALL_PATH = '../input/privbert-data/op115_processed.csv'\nop115_train = pd.read_csv(TRAIN_PATH)\nop115_val = pd.read_csv(VAL_PATH)\nop115_all = pd.read_csv(ALL_PATH)\n\n#For more information on the Op115OneHots class please look atht the notebook on data_processing. \n# i.e:  data_processing_explanation.ipynb\n\n#Initiating data for respective k-set\nop115_all_c = Op115OneHots(op115_all)\nop115_all_c.go2()\nuniques = op115_all_c.return_oh_names()\n\nop115_t_c = Op115OneHots(op115_train)\nop115_t_c.go2(majority = True, class_tup = uniques)\n\nop115_v_c = Op115OneHots(op115_val)\nop115_v_c.go2(majority = True, class_tup = uniques)\n\n\nv_catsub,v_catval,v_subval,v_cats,v_subs,v_vals,v_my_texts = op115_v_c.new_onehots() # validation data\ncatsub,catval,subval,cats,subs,vals,my_texts = op115_t_c.new_onehots() # training data","metadata":{"id":"EOg_5kt1nnUA","outputId":"38296ecc-ee4b-4c8c-edc1-acc489da7ca0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# small function that creates a dictionary that links the subcategory classes to the respective \na,b,c,inds = op115_t_c.len_onehots()\nall_inds = list(inds.keys())\nonly2 = [ind for ind in all_inds if len(ind) == 2 and not isinstance(ind[0], int) and isinstance(ind[0], str)]\n\nsub_dict = {}\nfor item in only2:\n    cat = item[0]\n    sub = item[1]\n    cat_ind = inds[cat]\n    if cat in sub_dict:\n        sub_dict[cat].append(inds[(cat_ind,cat,sub)])\n    else:\n        sub_dict[cat] = [inds[(cat_ind,cat,sub)]]\n        \nprint(sub_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Performing a grid search for the classifiers\n\n#Setting the paramaters over which I want to classify\nparams = {  'max_length' : [128,512],\n            'learning_rate': [5e-6, 1e-5, 2e-5],\n            'batch_size' : [16],\n            'n_epochs' : [3,6,9],\n            'treshold' : [0.5]}\n\n# Initiate list with the 'category' and 'all subcategory' texts and labels\ncat_pair = [my_texts,cats]\nsub_pair = [my_texts, subs]\nnames = [\"Categories\",\"Subcategories\"]\ntext_label_pairs = [cat_pair,sub_pair]\n\n# now for each other classifier we want to train we create a pair with the respective text and labels\n\n# for the category -> subcategory classifier\nfor akey in sorted(list(catsub.keys())):\n  valtup = catsub[akey]\n  text_value_pair = (valtup[1],valtup[2])\n  name = 'cs_'+ akey\n  names.append(name)\n  text_label_pairs.append(text_value_pair)\n\n# for the category -> value classifier\nfor akey in sorted(list(catval.keys())):\n  valtup = catval[akey]\n  text_value_pair = (valtup[1],valtup[2])\n  name = 'cv_'+ akey\n  names.append(name)\n  text_label_pairs.append(text_value_pair)\n\n\n# list with the texts and respective labels for the validation test \nval_cat_pair = [v_my_texts,v_cats]\nval_sub_pair = [v_my_texts, v_subs]\nval_names = [\"Categories\",\"Subcategories\"]\nval_text_label_pairs = [val_cat_pair,val_sub_pair]\n\nfor akey in sorted(list(v_catsub.keys())):\n  valtup = v_catsub[akey]\n  v_text_value_pair = (valtup[1],valtup[2])\n\n  val_text_label_pairs.append(v_text_value_pair)\n\nfor akey in sorted(list(v_catval.keys())):\n  valtup = v_catval[akey]\n  v_text_value_pair = (valtup[1],valtup[2])\n  val_text_label_pairs.append(v_text_value_pair)","metadata":{"id":"1m_E3LwzQTma","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(train_dataload, val_data_load, optimizer, model, nb_labels, epochs = 3, threshold = 0.5, ret_macro = False):\n        \"\"\"\n        Function that trains the model. Pretty straightforward,takes a base model, in case of this proejct that is the \n        model as found in the 'privbert-model' dataset. This model was trained on a privacy policy corpus. \n        \n        ret_macro is an option for returning not only the model but also the final training result on the validation set.\n        \n        \"\"\"\n        \n        \n        # Store our loss and accuracy for plotting\n        train_loss_set = []\n        \n        # trange is a tqdm wrapper around the normal python range\n        for _ in trange(epochs, desc=\"Epoch\"):\n        \n          # Training\n          \n          # Set our model to training mode (as opposed to evaluation mode)\n          model.train()\n        \n          # Tracking variables\n          tr_loss = 0 #running loss\n          nb_tr_examples, nb_tr_steps = 0, 0\n          \n          # Train the data for one epoch\n          for step, batch in enumerate(train_dataload):\n            # Add batch to GPU\n            batch = tuple(t.to(device) for t in batch)\n\n            # Unpack the inputs from our dataloader\n            b_input_ids, b_input_mask, b_labels, b_token_types = batch\n            # Clear out the gradients (by default they accumulate)\n            optimizer.zero_grad()\n        \n            # # Forward pass for multiclass classification\n            # outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n            # loss = outputs[0]\n            # logits = outputs[1]\n        \n            # Forward pass for multilabel classification\n            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n            logits = outputs[0]\n            loss_func = BCEWithLogitsLoss() \n            loss = loss_func(logits.view(-1,nb_labels),b_labels.type_as(logits).view(-1,nb_labels)) #convert labels to float for calculation\n            # loss_func = BCELoss() \n            # loss = loss_func(torch.sigmoid(logits.view(-1,num_labels)),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n            train_loss_set.append(loss.item())    \n        \n            # Backward pass\n            loss.backward()\n            # Update parameters and take a step using the computed gradient\n            optimizer.step()\n            # scheduler.step()\n            # Update tracking variables\n            tr_loss += loss.item()\n            nb_tr_examples += b_input_ids.size(0)\n            nb_tr_steps += 1\n        \n          print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n        \n        ###############################################################################\n        \n          # Validation\n        \n          # Put model in evaluation mode to evaluate loss on the validation set\n          model.eval()\n        \n          # Variables to gather full output\n          logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n        \n          # Predict\n          for i, batch in enumerate(val_data_load):\n            # Add batch to GPU\n            batch = tuple(t.to(device) for t in batch)\n            # Unpack the inputs from our dataloader\n            b_input_ids, b_input_mask, b_labels, b_token_types = batch\n            with torch.no_grad():\n              # Forward pass\n              outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n              b_logit_pred = outs[0]\n              pred_label = torch.sigmoid(b_logit_pred)\n        \n              b_logit_pred = b_logit_pred.detach().cpu().numpy()\n              pred_label = pred_label.to('cpu').numpy()\n              b_labels = b_labels.to('cpu').numpy()\n        \n            tokenized_texts.append(b_input_ids)\n            logit_preds.append(b_logit_pred)\n            true_labels.append(b_labels)\n            pred_labels.append(pred_label)\n        \n          # Flatten outputs\n          pred_labels = [item for sublist in pred_labels for item in sublist]\n          true_labels = [item for sublist in true_labels for item in sublist]\n        \n          # Calculate Accuracy\n          pred_bools = [pl>threshold for pl in pred_labels]\n          true_bools = [tl==1 for tl in true_labels]\n          val_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')*100\n          val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n          \n        \n          print('F1 Validation Accuracy: ', val_f1_accuracy)\n          print('Flat Validation Accuracy: ', val_flat_accuracy)\n        if ret_macro:\n            val_f1_macro_accuracy = f1_score(true_bools,pred_bools,average='macro')*100\n            return val_f1_accuracy, val_f1_macro_accuracy\n        return val_f1_accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pytorch_classifier import BertClassification\nfrom keras import backend as K\n\n\ndef RunBertOneLoop(labels,texts,labels_val, texts_val, max_length,batch_size,learning_rate,n_epochs,nb_labels, return_model = False, ret_macro = False):\n    \"\"\"\n    Function that takes the train function and the raw texts and raw labels and creates the dataloaders needed for \n    the training model. \n    \n    return_model can be toggled to true if one wants to not only return the f1 score of the model but also the model itself\n    ret_macro can be toggled on to return both the macro and micro f1 scores\n    \"\"\"   \n    tryout = BertClassification(device=device)\n    tryout.train = False\n    tryout.test = True\n    tryout.init_tokenizer(TOK_NAME)\n    tryout.input_labels(labels)\n    tryout.input_texts(texts)\n    \n    tryout_v = BertClassification(gpu = False)\n    tryout_v.test = True\n    tryout_v.train = False\n    tryout_v.init_tokenizer(TOK_NAME)\n    tryout_v.input_labels(labels_val)\n    tryout_v.input_texts(texts_val)\n\n\n    model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=nb_labels)\n    model.cuda()\n    train_data=tryout.encode_texts(max_length,batchsize = batch_size)\n    val_data=tryout_v.encode_texts(max_length,batchsize = batch_size)\n    optimizer = tryout.init_optimizer(model,learning_rate)\n    if ret_macro:\n        mic_f1, mac_f1 = train_model(train_data, val_data, optimizer, model, nb_labels,n_epochs, ret_macro = ret_macro)\n        return mic_f1, mac_f1\n    acc = train_model(train_data, val_data, optimizer, model, nb_labels,n_epochs)\n    if return_model:\n        return acc,model\n    else:\n        return acc","metadata":{"id":"5uFdEgbn2uIt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Running the girdsearch\n\nin the code below the gridsearch is performed using the paramters that were set above.","metadata":{}},{"cell_type":"code","source":"# set file name and column names\nSAVEFILE = 'privbert_gridsearches.csv'\nTOK_NAME = 'bert-base-uncased'\nMODEL_NAME = '../input/privbert-model'\nPARAM_NAMES = ['max_length','learning_rate','batch_size','epochs','treshold']\n\nfor i in range(1):\n    cur_name = names[12]\n    cur_texts,cur_labels = text_label_pairs[12]\n    nb_labels = len(cur_labels[0])\n    print(nb_labels)\n    if nb_labels > 1:    \n        # if statements are in case that a parameter is not defined, it then sets a default variable\n        if 'max_length' in params:\n            max_length = params['max_length']\n        else:\n            max_length = [128]\n        if 'learning_rate' in params:\n            learning_rate = params['learning_rate']\n        else:\n            learning_rate = [2e-5]\n        if 'batch_size' in params:\n            batch_size = params['batch_size']\n        else:\n            batch_size = [32]\n        if 'n_epochs' in params:\n            n_epochs = params['n_epochs']\n        else:\n            n_epochs = [3]\n        if 'treshold' in params:\n            treshold = params['treshold']\n        else:\n            treshold = [0.5]\n        \n        param_list = []\n        acc_list = []\n        # go through all parameters\n        for ml in max_length:\n            for lr in learning_rate:\n                for bs in batch_size:\n                    for n in n_epochs:\n                        for tr in treshold:\n                            \n                            # run the training function for ever variable, returing only the accuaracy (return_model = False)\n                            acc = RunBertOneLoop(cur_labels,cur_texts,ml,bs,lr,n,nb_labels)\n                            acc_list.append(acc)\n                            pars = [ml,lr,bs,n,tr]\n                            param_list.append(pars)\n                            \n                            # clear memory after each loop\n                            K.clear_session()\n                            torch.cuda.empty_cache()\n\n        \n        try:\n            existing_df = pd.read_csv(SAVEFILE)\n            existing_df[cur_name] = acc_list\n            existing_df.to_csv(SAVEFILE,index=False)\n        except:\n            df = pd.DataFrame(param_list,columns=PARAM_NAMES)\n            df[cur_name] = acc_list\n            df.to_csv(SAVEFILE)\n            print(\"Saved\")\n\n                        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the models\n\nAfter finding the optimal parameters for each classifier it is now time to actually train the models\nFor this we will train a model with the optimal parameters 10 times and choose the one that yielded the best results on the validation set","metadata":{}},{"cell_type":"code","source":"# setting some of the parameters again. Some are copied from above, this is done so that I dont have to run th whole code\n# everytime\nPARAM_NAMES = ['max_length','learning_rate','batch_size','epochs','treshold']\nSAVEFILE = 'privbert_gridsearches.csv'\ngridsearch_results = pd.read_csv(SAVEFILE)\ngr_cols = list(gridsearch_results.columns)\nparam_df = gridsearch_results[PARAM_NAMES]\nmodel_names = gr_cols[5:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Pick the best parameters from the parameter df that was created ealier\nbest_params = {}\nfor model_name in model_names:\n  col_results = gridsearch_results[model_name].to_list()\n  best_res = col_results.index(max(col_results))\n  best_param = param_df.iloc[best_res].to_list()\n  best_params[model_name] = best_param\n\n#dictionary that links the model name to the index, remember that names was defined earlier. \n# Example name: 'cs_Data_Retention'\nnames_index = {}\nfor i,name in enumerate(names):\n    names_index[name] = i\n    \ncat_names = sorted(op115_t_c.unique_cats)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#set parameters and the folder where the models will be stored as well as the language model used to train upon\nN_TRIES = 10\nSAVEFOLDER = 'trained_models'\nTOK_NAME = 'bert-base-uncased'\nMODEL_NAME = '../input/privbert-model'\n\nfor amodel in model_names:\n    #set the parameters for each moel\n    ml,lr,bs,epoch,tr = best_params[amodel]\n    ml = int(ml)\n    bs = int(bs)\n    epoch = int(epoch)\n    \n    # get the texts and labels per model\n    texts, labels = text_label_pairs[names_index[amodel]]\n    val_texts, val_labels = val_text_label_pairs[names_index[amodel]]\n    nb_labels = len(labels[0])\n    \n    n = 0\n    highest_accuracy = 0\n    while n < N_TRIES and highest_accuracy < 100:\n        # return both the model and the score for a trained model\n        acc,model = RunBertOneLoop(labels,texts,val_labels, val_texts,ml,bs,lr,epoch,nb_labels, return_model = True)\n        saved = False\n        #only save the model if the accuracy is higher than all previous ones\n        if acc > highest_accuracy:\n            path = SAVEFOLDER + '/' + amodel\n            model.save_pretrained(path)\n            highest_accuracy = acc\n            saved = True\n        n+=1\n        \n        print(\"finshed try-{} for {} \\n accuracy={} \\n saved={}\".format(n,amodel,acc,saved))\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tar -zcvf trained_modelsk3.tar.gz /kaggle/working/trained_models #create a zipped folder","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'./trained_modelsk0.tar.gz') # use this to donwload the respective folder","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing different stratificaton methods\n\nTo test the effect of choosing different stratification methods I perform a 15 fold test using either stratfication on the segments or on the policies. The code used for that is found below. Some code might be exta so that not the whole notebook has to be loaded","metadata":{}},{"cell_type":"code","source":"from data_processing import Op115OneHots\nimport pandas as pd\nimport numpy as np\n\n# load all data\nALL_POLS = '../input/privbert-data/op115_processed.csv'\nall_pols_df = pd.read_csv(ALL_POLS)\npoll_uids = sorted(all_pols_df.policy_uid.unique())\nlabels_per_pol = []\n\nop115_all_c = Op115OneHots(all_pols_df)\nop115_all_c.go2(majority = True)\nuniques = op115_all_c.return_oh_names()\ncatsub_index, catval_index, subval_index, inds = op115_all_c.len_onehots()\n\ncatsub,catval,subval,cats,subs,vals,my_texts = op115_all_c.new_onehots()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the indexes for the sgements that have a respective category labeled\ncat_arry = np.array(cats)\nindexes = []\nfor i in range(10):\n    colrow= cat_arry[:,i]\n    inds = np.where(colrow == 1)\n    indexes.append(inds)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"policies = []\npolsegs = op115_all_c.pol_seg()\nfor i in range(10):\n    pol_uids = []\n    for ind in indexes[i][0].astype(int):\n        pol_uids.append(polsegs[ind][0])\n    policies.append(pol_uids)\n\nspecials = [0,2,6,8] # the 'specials' are the categories with few labels, these I want to make sure are somewhat divided\nspecial_pols = []\nfor special in specials:\n    print(set(policies[special]))\n    special_pols = special_pols + policies[special]\n\nspecial_pols = list(set(special_pols))\nprint(special_pols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from skmultilearn.model_selection import IterativeStratification\n\ndef iterative_train_test_split(X, y, train_size):\n    \"\"\"Custom iterative train test split which\n    'maintains balanced representation with respect\n    to order-th label combinations.'\n    \"\"\"\n    stratifier = IterativeStratification(\n        n_splits=2, order=1, sample_distribution_per_fold=[1.0-train_size, train_size, ])\n    train_indices, test_indices = next(stratifier.split(X, y))\n    X_train, y_train = X[train_indices], y[train_indices]\n    X_test, y_test = X[test_indices], y[test_indices]\n    return X_train, y_train, X_test, y_test\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_poluids = all_pols_df['policy_uid'].unique()\nwithout_special = [poluid for poluid in all_poluids if poluid not in special_pols]\ndef split_per_uid(X, y, train_size, special_pols = special_pols, without_special = without_special):\n    \"\"\"\n    Function that splits data based on the policies, differentiates between the 'special' and 'other' policies.\n    The special policies have to distributed a bit more manually because if one were to randomly pick policies there is\n    a decent chance on ends up with no support for certain labels\n    \"\"\"\n    test_size = 1 - train_size\n    train_inds, val_inds = train_test_split(special_pols, test_size = 0.5) # split the special pols 50 - 50\n    \n    train_pols, val_pols = train_test_split(without_special, test_size = test_size - 0.04) # split the other pols based on user choice\n    \n    train_pols = train_pols + train_inds # policy uids for training\n    val_pols = val_pols + val_inds # policy uids for validating\n    train_df = all_pols_df[all_pols_df['policy_uid'].isin(train_pols)]\n    val_df = all_pols_df[all_pols_df['policy_uid'].isin(val_pols)]\n    \n    # get respective labels and texts as done earlier\n    op115_train = Op115OneHots(train_df)\n    op115_train.go2(majority = True, class_tup=uniques)\n    catsub,catval,subval,train_labels,subs,vals,train_texts = op115_train.new_onehots()\n    \n    op115_val = Op115OneHots(val_df)\n    op115_val.go2(majority = True, class_tup=uniques)\n    catsub,catval,subval,val_labels,subs,vals,val_texts = op115_val.new_onehots()\n    \n    return train_texts, train_labels, val_texts, val_labels\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Perform the 15 tests","metadata":{}},{"cell_type":"code","source":"TOK_NAME = 'bert-base-uncased'\nMODEL_NAME = '../input/privbert-model'\n\nn_per_strat = 15\n\ntr_cats = ['tr_' + cat for cat in cat_names]\nte_cats = ['te_' + cat for cat in cat_names]\n\nml,lr,bs,epoch,tr = best_params['Categories']\nml = int(ml)\nbs = int(bs)\nepoch = 5 #less epochs as it is not as important\ntrain_size = 0.75\n\nX = np.array(my_texts)\nY = np.array(cats)\nnb_labels = len(Y[0])\n\nCOLNAMES = ['k', 'split_method', 'train_segments', 'micro_f1', 'macro_f1'] + tr_cats + te_cats\noutdf = pd.DataFrame(columns = COLNAMES)\nfor i in range(n_per_strat):\n    # perform tests for the standard stratification method\n    \n    train_texts, train_labels, val_texts, val_labels = iterative_train_test_split(X,Y, train_size)\n    tr_labels = list(np.sum(train_labels, axis = 0)) #number of training labels per category\n    te_labels = list(np.sum(val_labels, axis = 0)) #number of testing labels per category\n    #micro and macro scores for this set (based on validation/testing set) ret_macro = True\n    micro_f1,macro_f1 = RunBertOneLoop(list(train_labels),list(train_texts),list(val_labels), list(val_texts),ml,bs,lr,epoch,nb_labels, ret_macro = True)\n    row = [i,'iterative', len(train_texts)] + [micro_f1, macro_f1] + tr_labels + te_labels\n    outdf.loc[len(outdf)] = row\n\n    print(\"finished with {} for iterative\".format(i))\n\nfor i in range(n_per_strat):\n    #perform tests for policy stratification method\n    \n    train_texts, train_labels, val_texts, val_labels = split_per_uid(X,Y, train_size+ 0.03)\n    tr_labels = list(np.sum(train_labels, axis = 0))\n    te_labels = list(np.sum(val_labels, axis = 0))\n    micro_f1,macro_f1 = RunBertOneLoop(list(train_labels),list(train_texts),list(val_labels), list(val_texts),ml,bs,lr,epoch,nb_labels, ret_macro = True)\n    row = [i,'uid', len(train_texts)] + [micro_f1, macro_f1] + tr_labels + te_labels\n    outdf.loc[len(outdf)] = row\n\n    print(\"finished with {} for uid\".format(i))\n\noutdf.to_csv('stratification_comparison.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outdf.to_csv('new_strats.csv')\ngrpd = outdf.groupby(['split_method']).mean()\ngrpd.to_csv('strat_grouped.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}