{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GPT-2 Fine-Tuning","metadata":{}},{"cell_type":"markdown","source":"#### This is the code I wrote at the company, but I think it would be nice to share it here, so I post it.\n\n#### With this data, we will fine tune GPT-2 to make a sentence generation model. \n\n#### This code is for AI beginners.","metadata":{}},{"cell_type":"markdown","source":"## Step 1. Data preprocessing","metadata":{}},{"cell_type":"markdown","source":"#### the data contains unnecessary newlines, tags, and URLs it will be necessary to remove them before preprocessing.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cleaning(s):\n    s = str(s)\n    s = re.sub('\\s\\W',' ',s)\n    s = re.sub('\\W,\\s',' ',s)\n    s = re.sub(\"\\d+\", \"\", s)\n    s = re.sub('\\s+',' ',s)\n    s = re.sub('[!@#$_]', '', s)\n    s = s.replace(\"co\",\"\")\n    s = s.replace(\"https\",\"\")\n    s = s.replace(\"[\\w*\",\" \")\n    return s","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"Articles.csv\", encoding=\"ISO-8859-1\") \ndf = df.dropna()\ntext_data = open('Articles.txt', 'w')\nfor idx, item in df.iterrows():\n  article = cleaning(item[\"Article\"])\n  text_data.write(article)\ntext_data.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 2. Model Training","metadata":{}},{"cell_type":"code","source":"!pip install transformers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TextDataset, DataCollatorForLanguageModeling\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom transformers import Trainer, TrainingArguments","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_dataset(file_path, tokenizer, block_size = 128):\n    dataset = TextDataset(\n        tokenizer = tokenizer,\n        file_path = file_path,\n        block_size = block_size,\n    )\n    return dataset\n\n\ndef load_data_collator(tokenizer, mlm = False):\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer, \n        mlm=mlm,\n    )\n    return data_collator\n\n\ndef train(train_file_path,model_name,\n          output_dir,\n          overwrite_output_dir,\n          per_device_train_batch_size,\n          num_train_epochs,\n          save_steps):\n  tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n  train_dataset = load_dataset(train_file_path, tokenizer)\n  data_collator = load_data_collator(tokenizer)\n\n  tokenizer.save_pretrained(output_dir)\n      \n  model = GPT2LMHeadModel.from_pretrained(model_name)\n\n  model.save_pretrained(output_dir)\n\n  training_args = TrainingArguments(\n          output_dir=output_dir,\n          overwrite_output_dir=overwrite_output_dir,\n          per_device_train_batch_size=per_device_train_batch_size,\n          num_train_epochs=num_train_epochs,\n      )\n\n  trainer = Trainer(\n          model=model,\n          args=training_args,\n          data_collator=data_collator,\n          train_dataset=train_dataset,\n  )\n      \n  trainer.train()\n  trainer.save_model()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# you need to set parameters \ntrain_file_path = \"/content/drive/MyDrive/Articles.txt\"\nmodel_name = 'gpt2'\noutput_dir = '/content/drive/MyDrive/result'\noverwrite_output_dir = False\nper_device_train_batch_size = 8\nnum_train_epochs = 5.0\nsave_steps = 500","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# It takes about 30 minutes to train in colab.\ntrain(\n    train_file_path=train_file_path,\n    model_name=model_name,\n    output_dir=output_dir,\n    overwrite_output_dir=overwrite_output_dir,\n    per_device_train_batch_size=per_device_train_batch_size,\n    num_train_epochs=num_train_epochs,\n    save_steps=save_steps\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 3. Inference","metadata":{}},{"cell_type":"code","source":"from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel, GPT2TokenizerFast, GPT2Tokenizer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_model(model_path):\n    model = GPT2LMHeadModel.from_pretrained(model_path)\n    return model\n\n\ndef load_tokenizer(tokenizer_path):\n    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n    return tokenizer\n\n\ndef generate_text(sequence, max_length):\n    model_path = \"/content/drive/MyDrive/result\"\n    model = load_model(model_path)\n    tokenizer = load_tokenizer(model_path)\n    ids = tokenizer.encode(f'{sequence}', return_tensors='pt')\n    final_outputs = model.generate(\n        ids,\n        do_sample=True,\n        max_length=max_length,\n        pad_token_id=model.config.eos_token_id,\n        top_k=50,\n        top_p=0.95,\n    )\n    print(tokenizer.decode(final_outputs[0], skip_special_tokens=True))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequence = input() # oil price\nmax_len = int(input()) # 20\ngenerate_text(sequence, max_len) # oil price for July June which had been low at as low as was originally stated Prices have since resumed","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following process may be a little more complicated or tedious because you have to write the code one by one, and it takes a long time if you don't have a personal GPU.\n\nThen, how about use Ainize's Teachable NLP? Teachable NLP provides an API to use the model so when data is input it will automatically learn quickly.\n\nTeachable NLP : [https://ainize.ai/teachable-nlp](https://link.ainize.ai/3tJVRD1)\n\nTeachable NLP Tutorial : [https://forum.ainetwork.ai/t/teachable-nlp-how-to-use-teachable-nlp/65](https://link.ainize.ai/3tATaUh)","metadata":{}}]}