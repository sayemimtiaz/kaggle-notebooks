{"cells":[{"cell_type":"code","source":"import numpy as np\nimport os\nimport pandas as pd\nimport sys\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import PorterStemmer\nimport nltk\nfrom nltk import word_tokenize, ngrams\nfrom nltk.classify import SklearnClassifier\nfrom wordcloud import WordCloud,STOPWORDS","execution_count":null,"outputs":[],"metadata":{"_uuid":"ed96364a4b5d324e846fc30271e1ecc40193c8f1","_cell_guid":"2b6498bc-1819-4e1b-baa8-8abfeb327c5b","collapsed":true}},{"cell_type":"code","source":"np.random.seed(25)\nfrom keras.models import Sequential\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\nfrom keras.layers import Dense, Input, Flatten, merge, LSTM, Lambda, Dropout\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom keras.models import Model\nfrom keras.layers.wrappers import TimeDistributed, Bidirectional\nfrom keras.layers.normalization import BatchNormalization\nfrom keras import backend as K\nfrom keras.layers import Convolution1D, GlobalMaxPooling1D\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.layers.merge import concatenate\nfrom keras.layers.core import Dense, Activation, Dropout\nimport codecs","execution_count":null,"outputs":[],"metadata":{"_uuid":"c3a3aeba7fed3c0f052ce0b6a33dda4871461210","_cell_guid":"e03a701f-e6e6-43a2-ab16-116a9325aa41","collapsed":true}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/hotel-review/train.csv\")\ntest = pd.read_csv(\"../input/hotel-review/test.csv\")","execution_count":null,"outputs":[],"metadata":{"_uuid":"a6e702d9499646ace4f34f6f910eec6e3f50727e","_cell_guid":"a67672df-445e-408f-a163-5e257237f258","collapsed":true}},{"cell_type":"code","source":"# Target Mapping\nmapping_target = {'happy':0, 'not happy':1}\ntrain = train.replace({'Is_Response':mapping_target})\n\n# Browser Mapping\nmapping_browser = {'Firefox':0, 'Mozilla':0, 'Mozilla Firefox':0,\n                  'Edge': 1, 'Internet Explorer': 1 , 'InternetExplorer': 1, 'IE':1,\n                   'Google Chrome':2, 'Chrome':2,\n                   'Safari': 3, 'Opera': 4\n                  }\ntrain = train.replace({'Browser_Used':mapping_browser})\ntest = test.replace({'Browser_Used':mapping_browser})\n# Device mapping\nmapping_device = {'Desktop':0, 'Mobile':1, 'Tablet':2}\ntrain = train.replace({'Device_Used':mapping_device})\ntest = test.replace({'Device_Used':mapping_device})","execution_count":null,"outputs":[],"metadata":{"_uuid":"795f67e2de5aa5d47e81656c8cd6daa6495dd4f3","_cell_guid":"1848ba0b-ec54-4556-9b83-851b050e16e7","collapsed":true}},{"cell_type":"code","source":"GLOVE_DIR = '../input/glove-global-vectors-for-word-representation/'\nMAX_SEQUENCE_LENGTH = 300\nMAX_NB_WORDS = 10000\nEMBEDDING_DIM = 32\nVALIDATION_SPLIT = 0.3","execution_count":null,"outputs":[],"metadata":{"_uuid":"f8c8968476ec2f9e8112d4b62d4a0d1ee6e4b28e","_cell_guid":"67421ecc-a85f-41fb-9d3b-10f9fe212db0","collapsed":true}},{"cell_type":"code","source":"test_id = test['User_ID']\ntarget = train['Is_Response']","execution_count":null,"outputs":[],"metadata":{"_uuid":"10d63310d5571c0152ed0504674b517cffee13d9","_cell_guid":"514a7aa0-c812-4638-842d-286f788dd8f4","collapsed":true}},{"cell_type":"code","source":"# function to clean data\nimport string\nimport itertools \nimport re\nfrom nltk.stem import WordNetLemmatizer\nfrom string import punctuation\n\nstops = ['also','on','the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n              'Is','If','While','This','january','february','march','april','may','june','july','august','september','october',\n        'november','december','monday','tuesday','wednesday','thursday','friday','saturday','sunday','india','tripadviser','usa',\n        'hundred','thousand','today','tomorrow','yesterday','etc','delhi','mumbai','chennai','kolkata','room','hotel','even',\n        'front desk','new york','san francisco','however','time square','canada','review','us','uk','china','staff','found'\n        ,'one','area','although','walking', 'distance','though','th','floor','really','got','people','lobby','location'] \n# punct = list(string.punctuation)\n# punct.append(\"''\")\n# punct.append(\":\")\n# punct.append(\"...\")\n# punct.append(\"@\")\n# punct.append('\"\"')\ndef cleanData(text, lowercase = False, remove_stops = False, stemming = False, lemmatization = False):\n    txt = str(text)\n    \n    # Replace apostrophes with standard lexicons\n    txt = txt.replace(\"isn't\", \"is not\")\n    txt = txt.replace(\"aren't\", \"are not\")\n    txt = txt.replace(\"ain't\", \"am not\")\n    txt = txt.replace(\"won't\", \"will not\")\n    txt = txt.replace(\"didn't\", \"did not\")\n    txt = txt.replace(\"shan't\", \"shall not\")\n    txt = txt.replace(\"haven't\", \"have not\")\n    txt = txt.replace(\"hadn't\", \"had not\")\n    txt = txt.replace(\"hasn't\", \"has not\")\n    txt = txt.replace(\"don't\", \"do not\")\n    txt = txt.replace(\"wasn't\", \"was not\")\n    txt = txt.replace(\"weren't\", \"were not\")\n    txt = txt.replace(\"doesn't\", \"does not\")\n    txt = txt.replace(\"'s\", \" is\")\n    txt = txt.replace(\"'re\", \" are\")\n    txt = txt.replace(\"'m\", \" am\")\n    txt = txt.replace(\"'d\", \" would\")\n    txt = txt.replace(\"'ll\", \" will\")\n    txt = txt.replace(\"--th\", \" \")\n    \n    # More cleaning\n    txt = re.sub(r\"alot\", \"a lot\", txt)\n    txt = re.sub(r\"what's\", \"\", txt)\n    txt = re.sub(r\"What's\", \"\", txt)\n    txt = re.sub(r\"\\'s\", \" \", txt)\n    txt = txt.replace(\"pic\", \"picture\")\n    txt = re.sub(r\"\\'ve\", \" have \", txt)\n    txt = re.sub(r\"can't\", \"cannot \", txt)\n    txt = re.sub(r\"n't\", \" not \", txt)\n    txt = re.sub(r\"I'm\", \"I am\", txt)\n    txt = re.sub(r\" m \", \" am \", txt)\n    txt = re.sub(r\"\\'re\", \" are \", txt)\n    txt = re.sub(r\"\\'d\", \" would \", txt)\n    txt = re.sub(r\"\\'ll\", \" will \", txt)\n    txt = re.sub(r\"60k\", \" 60000 \", txt)\n    txt = re.sub(r\" e g \", \" eg \", txt)\n    txt = re.sub(r\" b g \", \" bg \", txt)\n    txt = re.sub(r\"\\0s\", \"0\", txt)\n    txt = re.sub(r\" 9 11 \", \"911\", txt)\n    txt = re.sub(r\"e-mail\", \"email\", txt)\n    txt = re.sub(r\"\\s{2,}\", \" \", txt)\n    txt = re.sub(r\"quikly\", \"quickly\", txt)\n    txt = re.sub(r\"imrovement\", \"improvement\", txt)\n    txt = re.sub(r\"intially\", \"initially\", txt)\n    txt = re.sub(r\"quora\", \"Quora\", txt)\n    txt = re.sub(r\" dms \", \"direct messages \", txt)  \n    txt = re.sub(r\"demonitization\", \"demonetization\", txt) \n    txt = re.sub(r\"actived\", \"active\", txt)\n    txt = re.sub(r\"kms\", \" kilometers \", txt)\n    txt = re.sub(r\"KMs\", \" kilometers \", txt)\n    txt = re.sub(r\" cs \", \" computer science \", txt) \n    txt = re.sub(r\" upvotes \", \" up votes \", txt)\n    txt = re.sub(r\" iPhone \", \" phone \", txt)\n    txt = re.sub(r\"\\0rs \", \" rs \", txt) \n    txt = re.sub(r\"calender\", \"calendar\", txt)\n    txt = re.sub(r\"ios\", \"operating system\", txt)\n    txt = re.sub(r\"gps\", \"GPS\", txt)\n    txt = re.sub(r\"gst\", \"GST\", txt)\n    txt = re.sub(r\"programing\", \"programming\", txt)\n    txt = re.sub(r\"bestfriend\", \"best friend\", txt)\n    txt = re.sub(r\"dna\", \"DNA\", txt)\n    txt = re.sub(r\"III\", \"3\", txt) \n    txt = re.sub(r\"the US\", \"America\", txt)\n    txt = re.sub(r\"Astrology\", \"astrology\", txt)\n    txt = re.sub(r\"Method\", \"method\", txt)\n    txt = re.sub(r\"Find\", \"find\", txt) \n    txt = re.sub(r\"banglore\", \"Banglore\", txt)\n    txt = re.sub(r\" J K \", \" JK \", txt)\n    txt = re.sub(r\"comfy\", \"comfortable\", txt)\n    txt = re.sub(r\"colour\", \"color\", txt)\n    txt = re.sub(r\"travellers\", \"travelers\", txt)\n\n    # Emoji replacement\n#     txt = re.sub(r':\\)',r' Happy ',txt)\n#     txt = re.sub(r':D',r' Happy ',txt)\n#     txt = re.sub(r':P',r' Happy ',txt)\n#     txt = re.sub(r':\\(',r' Sad ',txt)\n    \n    # Remove urls and emails\n    txt = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', ' ', txt, flags=re.MULTILINE)\n    txt = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', ' ', txt, flags=re.MULTILINE)\n    \n    # Remove punctuation from text\n    txt = ''.join([c for c in text if c not in punctuation])\n#     txt = txt.replace(\".\", \" \")\n#     txt = txt.replace(\":\", \" \")\n#     txt = txt.replace(\"!\", \" \")\n#     txt = txt.replace(\"&\", \" \")\n#     txt = txt.replace(\"#\", \" \")\n    \n    # Remove all symbols\n    txt = re.sub(r'[^A-Za-z0-9\\s]',r' ',txt)\n    txt = re.sub(r'\\n',r' ',txt)\n    \n    txt = re.sub(r'[0-9]',r' ',txt)\n    \n    # Replace words like sooooooo with so\n    txt = ''.join(''.join(s)[:2] for _, s in itertools.groupby(txt))\n    \n    # Split attached words\n    #txt = \" \".join(re.findall('[A-Z][^A-Z]*', txt))   \n    \n    if lowercase:\n        txt = \" \".join([w.lower() for w in txt.split()])\n        \n    if remove_stops:\n        txt = \" \".join([w for w in txt.split() if w not in stops])\n    if stemming:\n        st = PorterStemmer()\n#         print (len(txt.split()))\n#         print (txt)\n        txt = \" \".join([st.stem(w) for w in txt.split()])\n    \n    if lemmatization:\n        wordnet_lemmatizer = WordNetLemmatizer()\n        txt = \" \".join([wordnet_lemmatizer.lemmatize(w, pos='v') for w in txt.split()])\n\n    return txt","execution_count":null,"outputs":[],"metadata":{"_uuid":"ad230a9ab93055fceb2ce47faaa65466b7dfbd04","_cell_guid":"623fa441-cb24-4b47-8a33-861d524b92aa","collapsed":true}},{"cell_type":"code","source":"# clean description\ntrain['Description'] = train['Description'].map(lambda x: cleanData(x, lowercase=True, remove_stops=True, stemming=True, lemmatization = True))\ntest['Description'] = test['Description'].map(lambda x: cleanData(x, lowercase=True, remove_stops=True, stemming=True, lemmatization = True))","execution_count":null,"outputs":[],"metadata":{"_uuid":"741ae2e989fe3d7b7357152521a7aba17cbc912a","_cell_guid":"4301258e-f9a2-44cc-80d7-b3d5e48bdd08","collapsed":true}},{"cell_type":"code","source":"# print('Indexing word vectors.')\n# embeddings_index = {}\n# f = codecs.open(os.path.join(GLOVE_DIR, 'glove.6B.50d.txt'), encoding='utf-8')\n# for line in f:\n#     values = line.split(' ')\n#     word = values[0]\n#     coefs = np.asarray(values[1:], dtype='float32')\n#     embeddings_index[word] = coefs\n# f.close()\n# print('Found %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[],"metadata":{"_uuid":"a1e1f4e46f251413c8c09fc1080e3ec332e1791e","_cell_guid":"04808af0-7ab5-4f25-8839-f0b1b9552f88","collapsed":true}},{"cell_type":"code","source":"print('Processing text dataset')\ntexts_1 = []\nfor text in train['Description']:\n    texts_1.append(text)\n\nlabels = train['Is_Response']  # list of label ids\n\nprint('Found %s texts.' % len(texts_1))\ntest_texts_1 = []\nfor text in test['Description']:\n    test_texts_1.append(text)\nprint('Found %s texts.' % len(test_texts_1))","execution_count":null,"outputs":[],"metadata":{"_uuid":"d20ae39f850520b7951c06d0c9c0a7619d38da9c","_cell_guid":"b8c9fc43-b05b-4e0d-9f13-69de619fc32c","collapsed":true}},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\ntokenizer.fit_on_texts(texts_1 + test_texts_1)\nsequences_1 = tokenizer.texts_to_sequences(texts_1)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\ntest_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n\ndata_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\nlabels = np.array(labels)\nprint('Shape of data tensor:', data_1.shape)\nprint('Shape of label tensor:', labels.shape)\n\ntest_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n#test_labels = np.array(test_labels)\ndel test_sequences_1\ndel sequences_1\nimport gc\ngc.collect()","execution_count":null,"outputs":[],"metadata":{"_uuid":"4e0e02b46b496dc501b9a33eace07451fd71a2ad","_cell_guid":"42a906d8-63ff-43dd-a3f3-f5f0048e6a80","collapsed":true}},{"cell_type":"code","source":"# print('Preparing embedding matrix.')\n# # prepare embedding matrix\nnb_words = min(MAX_NB_WORDS, len(word_index)) + 1\n\n# embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n# for word, i in word_index.items():\n#     if i >= nb_words:\n#         continue\n#     embedding_vector = embeddings_index.get(word)\n#     if embedding_vector is not None:\n#         # words not found in embedding index will be all-zeros.\n#         embedding_matrix[i] = embedding_vector\n# print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))","execution_count":null,"outputs":[],"metadata":{"_uuid":"4ee7f1fccb674bc122a1156db47af203a1cffb08","_cell_guid":"0bfeff32-b26e-4098-882a-c137ff7a4906","collapsed":true}},{"cell_type":"code","source":"# embedding_layer = Embedding(nb_words,\n#                             EMBEDDING_DIM,\n#                             weights=[embedding_matrix],\n#                             input_length=MAX_SEQUENCE_LENGTH,\n#                             trainable=False)","execution_count":null,"outputs":[],"metadata":{"_uuid":"1240a25bd3598ef16d4d762f8721db526b1b9b6a","_cell_guid":"326e37b2-6f01-4a30-99e4-5c88301879a6","collapsed":true}},{"cell_type":"code","source":"from keras.layers.recurrent import LSTM, GRU\nmodel = Sequential()\nmodel.add(Embedding(nb_words,32,input_length=MAX_SEQUENCE_LENGTH))\n# model.add(Flatten())\n# model.add(Dense(800, activation='relu'))\n# model.add(Dropout(0.2))\n# model.add(Conv1D(64,\n#                  5,\n#                  padding='valid',\n#                  activation='relu'))\n# model.add(Dropout(0.2))\n# model.add(MaxPooling1D())\n# model.add(Flatten())\n# model.add(Dense(400, activation='relu'))\n# model.add(Dropout(0.7))\n\nmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(2, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])","execution_count":null,"outputs":[],"metadata":{"_uuid":"33a1fc019ea95e7e3e50fc0cbdd1f4af1c41276c","_cell_guid":"953a4976-9394-4a42-9e43-f63f6302cad9","collapsed":true}},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ModelCheckpoint\n# early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n# class_weight= {0: 1.309028344, 1: 0.472001959}\nmodel.fit(data_1, to_categorical(labels), validation_split=0.1, nb_epoch=2, batch_size=64)","execution_count":null,"outputs":[],"metadata":{"_uuid":"b4921cceda4819b3da785476c7ff0ab8f2d4d7ea","_cell_guid":"7bae83ad-7ee8-4d9a-b95e-da0008022cac","collapsed":true}},{"cell_type":"code","source":"pred = model.predict(test_data_1)\n","execution_count":null,"outputs":[],"metadata":{"_uuid":"2ce16ea1ecc9b3526f948065c913802470dc59a1","_cell_guid":"264d0c90-9937-497c-a8b2-5bb4f3766035","collapsed":true}},{"cell_type":"code","source":"pred.shape","execution_count":null,"outputs":[],"metadata":{"_uuid":"da149b733b6e51e2ab3bc8908a0a12fe70b970a5","_cell_guid":"d8d6b3a0-e1cb-431d-97af-e809a9cefacd","collapsed":true}},{"cell_type":"code","source":"preds = []\n\nfor i in pred:\n    if i[0] >= i[1]:\n        preds.append('happy')\n    else:\n        preds.append('not_happy')","execution_count":null,"outputs":[],"metadata":{"_uuid":"5b3ea92595c4dbf5d51dacb7ecdc640d2fdec4c0","_cell_guid":"75f0847c-882e-4c24-b7a0-3f6233fe0728","collapsed":true}},{"cell_type":"code","source":"result = pd.DataFrame()\nresult['User_ID'] = test_id\nresult['Is_Response'] = preds\nmapping = {0:'happy', 1:'not_happy'}\nresult = result.replace({'Is_Response':mapping})\n\nresult.to_csv(\"nn_predicted_result_1.csv\", index=False)","execution_count":null,"outputs":[],"metadata":{"_uuid":"2a596f0d6fc28d7e28d2ecd022c4c07f57f803be","_cell_guid":"962cbcc8-42a0-48c0-8481-c8d124b7e70d","collapsed":true}},{"cell_type":"code","source":"","execution_count":null,"outputs":[],"metadata":{"_uuid":"26988305c0b7aa50fa32dd43e37fe29087168929","_cell_guid":"30ad9d32-a11f-4b40-aa7a-226a09b92444","collapsed":true}}],"nbformat_minor":1,"nbformat":4,"metadata":{"language_info":{"version":"3.6.3","nbconvert_exporter":"python","file_extension":".py","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","name":"python"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}}}