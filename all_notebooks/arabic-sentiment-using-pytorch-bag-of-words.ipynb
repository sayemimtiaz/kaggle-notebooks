{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\ndf = pd.read_csv('../input/reviews/5556-ar-reviews.csv')\ndf = df.sample(frac=1)\ndf.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-09T12:19:24.3659Z","iopub.execute_input":"2021-07-09T12:19:24.366334Z","iopub.status.idle":"2021-07-09T12:19:25.561762Z","shell.execute_reply.started":"2021-07-09T12:19:24.366248Z","shell.execute_reply":"2021-07-09T12:19:25.56065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass NeuralNet(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(NeuralNet, self).__init__()\n        self.l1 = nn.Linear(input_size, hidden_size)\n        self.l2 = nn.Linear(hidden_size, hidden_size)\n        self.l3 = nn.Linear(hidden_size, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        out = self.l1(x)\n        out = self.relu(out)\n        out = self.l2(out)\n        out = self.relu(out)\n        out = self.l3(out)\n        # no activation and no softmax at the end\n        return out","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:19:25.563694Z","iopub.execute_input":"2021-07-09T12:19:25.564097Z","iopub.status.idle":"2021-07-09T12:19:25.572964Z","shell.execute_reply.started":"2021-07-09T12:19:25.564056Z","shell.execute_reply":"2021-07-09T12:19:25.571844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef bag_of_words(tokenized_sentence, words):\n    \"\"\"\n    return bag of words array:\n    1 for each known word that exists in the sentence, 0 otherwise\n    example:\n    sentence = [\"hello\", \"how\", \"are\", \"you\"]\n    words = [\"hi\", \"hello\", \"I\", \"you\", \"bye\", \"thank\", \"cool\"]\n    bog   = [  0 ,    1 ,    0 ,   1 ,    0 ,    0 ,      0]\n    \"\"\"\n    # stem each word\n    sentence_words = [word for word in tokenized_sentence]\n    # initialize bag with 0 for each word\n    bag = np.zeros(len(words), dtype=np.float32)\n    for idx, w in enumerate(words):\n        if w in sentence_words:\n            bag[idx] = 1\n\n    return bag\n\n\nstopwords = {'فإذا', 'أنى', 'بمن', 'حتى', 'لم', 'أنتما', 'هناك', 'تينك', 'بل', 'إي', 'عن', 'ولكن', 'وإذا', 'دون', 'إنا', 'إذن', 'بكم', 'حين', 'عند', 'هل', 'إلا', 'هاته', 'ذينك', 'اللواتي', 'كذا', 'لستما', 'هي', 'اللتان', 'أكثر', 'كلتا', 'لكن', 'ليستا', 'هكذا', 'عسى', 'إذ', 'إن', 'اللاتي', 'إذا', 'بهم', 'نحن', 'فيما', 'ذاك', 'بكن', 'بيد', 'لهن', 'هذي', 'كأي', 'ذوا', 'أي', 'كلاهما', 'هذين', 'أينما', 'كي', 'إليكن', 'ماذا', 'هيا', 'هنالك', 'بي', 'بما', 'تلكما', 'بعض', 'بهن', 'تين', 'ريث', 'على', 'غير', 'حيثما', 'كأن', 'بخ', 'هاتان', 'هاهنا', 'ما', 'هيهات', 'لدى', 'شتان', 'لسنا', 'كيفما', 'مع', 'ممن', 'كما', 'إنما', 'يا', 'عليه', 'لك', 'ذه', 'ذان', 'لهما', 'ليست', 'لنا', 'مه', 'أنتن', 'في', 'لولا', 'بس', 'لها', 'أقل', 'عليك', 'فلا', 'مهما', 'ليسا', 'ذين', 'ذات', 'كلما', 'ذا', 'ذو', 'فيه', 'تي', 'هنا', 'هاتين', 'ها', 'هم', 'ألا', 'لا', 'سوى', 'وإذ', 'كم', 'لست', 'حيث', 'إليكما', 'لوما', 'الذين', 'كلا',\n             'التي', 'كأين', 'ذواتي', 'لستم', 'هذا', 'فمن', 'ذلكم', 'وما', 'كيف', 'لكم', 'حاشا', 'بك', 'والذي', 'أن', 'لهم', 'لسن', 'ثمة', 'ذي', 'وإن', 'ومن', 'أيها', 'له', 'متى', 'بلى', 'اللتين', 'لستن', 'بكما', 'قد', 'كليكما', 'لكما', 'هلا', 'آي', 'لكنما', 'اللذين', 'اللائي', 'ذلكن', 'لاسيما', 'ذلك', 'مذ', 'اللتيا', 'هما', 'إليك', 'سوف', 'منها', 'والذين', 'أنتم', 'هاتي', 'لكي', 'اللذان', 'ذواتا', 'عما', 'فيها', 'إلى', 'تلك', 'كل', 'لي', 'هو', 'فيم', 'إليكم', 'بها', 'ذانك', 'إنه', 'هؤلاء', 'أولئك', 'إذما', 'بنا', 'من', 'خلا', 'ليسوا', 'ثم', 'لعل', 'وهو', 'نحو', 'أين', 'لئن', 'عدا', 'آه', 'كأنما', 'كليهما', 'الذي', 'لن', 'نعم', 'هذه', 'بهما', 'ليت', 'تلكم', 'أما', 'منذ', 'أو', 'هاك', 'بماذا', 'كذلك', 'أنا', 'آها', 'فإن', 'عل', 'منه', 'هيت', 'أف', 'أم', 'إيه', 'كيت', 'ته', 'لكيلا', 'ليس', 'مما', 'هذان', 'أنت', 'حبذا', 'ولو', 'أوه', 'إما', 'لو', 'بين', 'به', 'ولا', 'لما', 'بعد', 'هن', 'ذلكما', 'أولاء', 'و'}\n\n\ndef tokenize(sentence):\n    tmpTokens = sentence.lower().split()\n    tokens = [token for token in tmpTokens if (\n        (token not in stopwords) and (len(token) > 0))]\n\n    return tokens","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:19:25.575483Z","iopub.execute_input":"2021-07-09T12:19:25.576106Z","iopub.status.idle":"2021-07-09T12:19:25.595366Z","shell.execute_reply.started":"2021-07-09T12:19:25.576058Z","shell.execute_reply":"2021-07-09T12:19:25.594512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nall_words = []\ntags = [0, 1]\nxy = []\n# loop through each sentence in our intents patterns\nintents = df[[\"label\", \"text\"]].values\n\nfor pattern in intents:\n    tag = pattern[0]\n    # tokenize each word in the sentence\n    w = tokenize(pattern[1])\n    # add to our words list\n    all_words.extend(w)\n    # add to xy pair\n    xy.append((w, tag))\n\n# stem and lower each word\nignore_words = ['?', '.', '!']\nall_words = [w for w in all_words if w not in ignore_words]\n# remove duplicates and sort\nall_words = sorted(set(all_words))\ntags = sorted(set(tags))\n\nX_train = []\ny_train = []\nfor (pattern_sentence, tag) in xy:\n    # X: bag of words for each pattern_sentence\n    bag = bag_of_words(pattern_sentence, all_words)\n    X_train.append(bag)\n    # y: PyTorch CrossEntropyLoss needs only class labels, not one-hot\n    label = tags.index(tag)\n    y_train.append(label)\n\nX_train = np.array(X_train)\ny_train = np.array(y_train)\n\n# Hyper-parameters\nnum_epochs = 100\nbatch_size = 64\nlearning_rate = 0.001\ninput_size = len(X_train[0])\nhidden_size = 8\noutput_size = len(tags)\nprint(input_size, output_size)  # 59915 2\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-09T12:19:25.596786Z","iopub.execute_input":"2021-07-09T12:19:25.597447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass ReviewsDataset(Dataset):\n\n    def __init__(self):\n        self.n_samples = len(X_train)\n        self.x_data = X_train\n        self.y_data = y_train\n\n    # support indexing such that dataset[i] can be used to get i-th sample\n    def __getitem__(self, index):\n        return self.x_data[index], self.y_data[index]\n\n    # we can call len(dataset) to return the size\n    def __len__(self):\n        return self.n_samples\n\n\ndataset = ReviewsDataset()\ntrain_loader = DataLoader(dataset=dataset,\n                          batch_size=batch_size,\n                          shuffle=True,\n                          num_workers=0)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = NeuralNet(input_size, hidden_size, output_size).to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# Train the model\nfor epoch in range(num_epochs):\n    for idx, (words, labels) in enumerate(train_loader):\n        words = words.to(device)\n        labels = labels.to(dtype=torch.long).to(device)\n\n        # Forward pass\n        outputs = model(words)\n        # if y would be one-hot, we must apply\n        # labels = torch.max(labels, 1)[1]\n        loss = criterion(outputs, labels)\n\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    if (epoch+1) % 5 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()/idx:.4f}')\n\n\nprint(f'final loss: {loss.item():.4f}')\n\ndata = {\n    \"model_state\": model.state_dict(),\n    \"input_size\": input_size,\n    \"hidden_size\": hidden_size,\n    \"output_size\": output_size,\n    \"all_words\": all_words,\n    \"tags\": tags\n}\n\nFILE = \"data2.pth\"\ntorch.save(data, FILE)\n\nprint(f'training complete. file saved to {FILE}')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"sentence= \"رحلاتي القادمة الى الرياض عندهم أن شاء الله . . طاقم العمل رائع كلهم ا.وائل و اثنين شباب في فترة الصباح ناسي اسمهم . تقديم ضيافة عند القدوم صباحا . تقديم ضيافة في المساء شاي و قهوة عربي مع تمر . راحة و نظافة المكان جدا جدا نظيف و خدمة الغرف سريعة . موقعه مناسب للأشخاص اهتمامهم السفارات او الجامعة قريب منها و لكن بعيد عن شرق الرياض . سهولة الوصول الى الدائري الشمالي . هدوء المكان . جميع الخدمات حولك . المسجد مقابل الفندق. لا يوجد شي سي ابدا\" \n\nsentence = tokenize(sentence)\nX = bag_of_words(sentence, all_words)\nX = X.reshape(1, X.shape[0])\nX = torch.from_numpy(X).to(device)\n\noutput = model(X)\n_, predicted = torch.max(output, dim=1)\n\ntag = tags[predicted.item()]\n\nprobs = torch.softmax(output, dim=1)\nprob = probs[0][predicted.item()]\nif prob.item() > 0.75:\n    for intent in intents:\n        if tag == intent[0]:\n            print(f\" {tag} \")\n            break\nelse:\n    print(\"I do not understand...\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}