{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Coronavirus Research Engine # \n\nThis project is made for the CORD-19 challenge. \n\nUsing the dataset provided, I have developed a search engine that responds to querys and returns the most relevant articles long with a summary of the main points.\n\nThe application is hosted using Flask web server and React as a front-end framework. Due to file upload limit, the app is currently only responding to 1000 articles.\n\nThe index and summary of all articles are cached in seperate JSON files to help return the most relevant results quickly.\n\nLive demo: https://c-ovid-19.technology\n\nBM250kapi was used to build an index/corpus of the data as well as nltk features to clean the data, remove stop words, tokenize and calculate sentences score to produce a summary from the article.\n\nGithub repo of the live demo: https://github.com/samikhalildev/COVID-19-Research-Engine"},{"metadata":{},"cell_type":"markdown","source":"# Dependencies: BM250kapi and NLTK"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install rank_bm25 nltk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom rank_bm25 import BM25Okapi\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, SnowballStemmer\nfrom nltk.cluster.util import cosine_distance\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\nimport json\nimport re\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Helper functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# cleans the text and section property of the article\ndef getText(x):\n    d = []\n    for t in x:\n        section = t['section'].lower().strip()\n        section = re.sub(' +', ' ', section)\n        \n        if section.isnumeric():\n            section = ''\n        \n        if len(section) > 2:\n            section = section[0].upper() + section[1:]\n            \n        text = t['text']\n        text = cleanText(text)\n\n        d.append({ 'text': text, 'section': section })\n    return d\n\n\n# regex to remove characters and extra spaces\ndef cleanText(text):\n    text = re.sub(r'\\(.*?\\)', '', text)\n    text = re.sub(r'\\[.*?\\]', '', text)\n    text = re.sub(' +', ' ', text)\n    t = ''\n\n    sentences = text.split('. ')\n    for i in range(len(sentences)):\n        sent = sentences[i]\n        sent = sent.strip()\n        if len(sent) >= 10:\n            if sent[-1] == ' ':\n                sent = sent[:-1]\n            sent += '. '\n            t += sent\n            \n    return t\n\n\ndef lastElement(i, arr):\n    return i == len(arr) - 1\n\n\ndef getAuthors(metadata):\n    authors = []\n    for author in metadata['authors']:\n        name = author['first'] + ' ' + author['last']\n        name = name.lstrip('[').rstrip(']').strip()\n        if len(name) >= 3:\n            authors.append(name)\n\n    return authors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# filters out junk data and returns a list of unique words\ndef preProcess(paragraph, article_id=False, tokenized_words=False):\n    \n    # Removing stop words\n    stop_words = set(stopwords.words(\"english\"))\n    \n    # Remove special characters\n    paragraph = re.sub('\\(|\\)|:|,|;|’|”|“|\\?|%|>|<', '', paragraph)\n    paragraph = re.sub('/', ' ', paragraph)\n    paragraph = paragraph.replace(\"'\",'')\n\n    # Tokenize paragraph\n    paragraph = word_tokenize(paragraph.lower())\n    \n    words = []\n    \n    # filter out stop words\n    for word in paragraph:\n        if (word not in stop_words and not tokenized_words) or (word not in stop_words and tokenized_words and word not in tokenized_words) and word not in words:\n            if len(word.strip()) > 2:\n                words.append(word)\n                \n    if article_id and not tokenized_words:\n        words.insert(0, article_id)\n        \n    return words\n\n    # Reducing words to their root form\n    #stemmer = SnowballStemmer(\"english\")\n    # words = [stemmer.stem(word) for word in words]\n    \n    # tokenizing the sentences\n    #sentences = sent_tokenize(text)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_frequency(article) -> dict:\n\n    # Removing stop words\n    stop_words = set(stopwords.words(\"english\"))\n    \n    tokenized_words = []\n    tokenized_sentences = []\n    \n    for para in article['body'][:3]:\n        text = para['text']\n        sent = sent_tokenize(text)\n        \n        # tokenizing the sentences\n        if sent not in tokenized_sentences:\n            tokenized_sentences.extend(sent)\n            \n        tokenized_words.extend(preProcess(text, False, tokenized_words))\n        \n    # Reducing words to their root form\n    stem = PorterStemmer()\n\n    # Creating dictionary for the word frequency table\n    frequency_table = dict()\n    for wd in tokenized_words:\n        wd = stem.stem(wd)\n        if wd in stop_words:\n            continue\n        if wd in frequency_table:\n            frequency_table[wd] += 1\n        else:\n            frequency_table[wd] = 1\n\n    return frequency_table, tokenized_sentences\n\n\ndef calculate_sentence_scores(sentences, frequency_table) -> dict:\n\n    # Algorithm for scoring a sentence by its words\n    sentence_weight = dict()\n\n    for sentence in sentences:\n        sentence_wordcount = (len(word_tokenize(sentence)))\n        sentence_wordcount_without_stop_words = 0\n        for word_weight in frequency_table:\n            if word_weight in sentence.lower():\n                sentence_wordcount_without_stop_words += 1\n                if sentence[:7] in sentence_weight:\n                    sentence_weight[sentence[:7]] += frequency_table[word_weight]\n                else:\n                    sentence_weight[sentence[:7]] = frequency_table[word_weight]\n    try:\n        sentence_weight[sentence[:7]] = round(sentence_weight[sentence[:7]] / sentence_wordcount_without_stop_words, 5)\n    except:\n        return sentence_weight\n    \n    return sentence_weight\n\n\ndef calculate_average_score(sentence_weight) -> int:\n\n    # Calculating the average score for the sentences\n    sum_values = 0\n    for entry in sentence_weight:\n        sum_values += sentence_weight[entry]\n\n    # Getting sentence average value from source text\n    try:\n        average_score = (sum_values / len(sentence_weight))\n    except:\n        return 0\n\n    return average_score\n\n\ndef produce_article_summary(sentences, sentence_weight, threshold):\n    sentence_counter = 0\n    article_summary = ''\n\n    for sentence in sentences:\n        if sentence[:7] in sentence_weight and sentence_weight[sentence[:7]] >= (threshold):\n            article_summary += \" \" + sentence\n            sentence_counter += 1\n\n    return article_summary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getArticleSummary(article):\n\n    # creating a dictionary for the word frequency table and tokenized sentences\n    frequency_table, sentences = get_frequency(article)\n    \n    # algorithm for scoring a sentence by its words\n    sentence_scores = calculate_sentence_scores(sentences, frequency_table)\n\n    # getting the threshold\n    threshold = calculate_average_score(sentence_scores)\n    \n    # producing the summary\n    article_summary = produce_article_summary(sentences, sentence_scores, 1.5 * threshold)\n\n    if len(article_summary.split(' ')) > 5:\n        if article_summary[0] == ' ':\n            article_summary = article_summary[1:]\n        return article_summary        \n    else:\n        return False\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Extract JSON data"},{"metadata":{"trusted":true},"cell_type":"code","source":"dirname = '/kaggle/input/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv/'\narticles = []\n    \n# Iterate over all JSON files and capture the title, id, authors, abstract, body and produce a sumamry\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if filename.split('.')[-1] == 'json':\n            \n            # Convert to JSON\n            path = os.path.join(dirname, filename)\n            d = json.load(open(path, 'rb'))\n            metadata = d['metadata']\n\n            # Get relevant data\n            title = metadata['title'].strip()\n\n            if len(title) < 3:\n                continue\n\n            authors = getAuthors(metadata)\n            abstract = []\n            \n            if 'abstract' in d:\n                abstract = d['abstract']\n                \n            body = d['body_text']\n\n            # Clean data\n            abstract = getText(abstract)\n            body = getText(body)\n\n            data = {\n                'paper_id': d['paper_id'],\n                'title': title,\n                'authors': authors,\n                'abstract': abstract,\n                'body': body,\n                'summary': ''\n            }\n            \n            # Get article summary \n            article_summary = getArticleSummary(data)\n            \n            if article_summary:\n                data['summary'] = article_summary\n                \n            articles.append(data)\n            \n            # NOTE: It takes way too long to run through the entire dataset, we will only capture and summarise 1000 papers\n            if len(articles) > 1000:\n                break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(articles)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenizeArticles(articles):\n    tokenized_corpus = []\n\n    for article in articles:\n        article_id = article['paper_id']\n        abstract = article['abstract']\n        body = article['body']\n        title = article['title']\n\n        paragraph = title + ' '\n\n        if len(abstract) > 0:\n            paragraph += abstract[0]['text']\n        elif len(body) > 0:\n            paragraph += body[0]['text']\n\n        tokenized_words = preProcess(paragraph, article_id)\n        tokenized_corpus.append(tokenized_words)\n        \n    return tokenized_corpus","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build a searchable Index"},{"metadata":{"trusted":true},"cell_type":"code","source":"def buildIndex(tokenized_corpus):\n    return BM25Okapi(tokenized_corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def searchArticles(bm25, tokenized_corpus, articles, query):\n    query = query.split(' ')\n    \n    doc_scores = bm25.get_scores(query)\n    results = bm25.get_top_n(query, tokenized_corpus, n=15)\n\n    return getArticles(articles, results)\n\ndef getArticles(articles, results):\n    result_articles = []\n\n    for words in results:\n        article_id = words[0]\n        sentences = ' '.join(words[1:])\n\n        for article in articles:\n            if article_id == article['paper_id']:\n                result_articles.append(article)\n                break\n\n    return result_articles","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenize articles by words\ntokenized_corpus = tokenizeArticles(articles)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build the index using BM25 Search algorithm\nbm25_index = buildIndex(tokenized_corpus)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's test it"},{"metadata":{"trusted":true},"cell_type":"code","source":"query = 'COVID 19 risk factors'\nfound_articles = searchArticles(bm25_index, tokenized_corpus, articles, query)\nfor article in found_articles:\n    print(f'Title: {article[\"title\"]}\\n')\n    \n    if len(article['summary']):\n        print(f'Summary: {article[\"summary\"]}\\n\\n')\n    elif len(article['abstract']):\n        print(f'Abstract: {article[\"abstract\"][0][\"text\"]}\\n\\n')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Time to answer CORD-19 questions!"},{"metadata":{"trusted":true},"cell_type":"code","source":"tasks = [\n    {\n        'name': 'What is known about transmission, incubation and environmental stability?',\n        'questions': [\n            'incubation periods for the disease in humans',\n            'Prevalence of asymptomatic shedding and transmission'\n            'Natural history of the virus',\n            'Implementation of diagnostics and products to improve clinical processes',\n            'Immune response and immunity',\n            'Role of the environment in transmission'\n        ]\n    },\n    {\n        'name': 'What do we know about COVID-19 risk factors?',\n        'questions': [\n            'COVID 19 risk factors',\n            'Smoking, pre-existing pulmonary disease',\n            'Co-infections and other co-morbidities',\n            'Neonates and pregnant women',\n            'Socio-economic and behavioral factors',\n            'Transmission dynamics of the virus serial interval, modes of transmission and environmental factors',\n            'risk of fatality among symptomatic hospitalized patients, and high-risk patient groups',\n            'Susceptibility of populations',\n            'Public health mitigation measures'\n        ]\n    },\n    {\n        'name': 'What do we know about virus genetics, origin, and evolution?',\n        'questions': [\n            'Real-time tracking of whole genomes and a mechanism for coordinating the rapid dissemination',\n            'geographic and temporal diverse sample sets',\n            'field surveillance, genetic sequencing, receptor binding',\n            'Surveillance of mixed wildlife- livestock farms for SARS-CoV-2 and other coronaviruses in Southeast Asia',\n            'Experimental infections to test host range for this pathogen',\n            'Animal host(s) and any evidence of continued spill-over to humans',\n            'Socioeconomic and behavioral risk factors for this spill-over',\n            'Sustainable risk reduction strategies'\n        ]\n    },\n    {\n        'name': 'What do we know about vaccines and therapeutics?',\n        'questions': [\n            'Effectiveness of drugs being developed and tried to treat COVID-19 patients.',\n            'Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients',\n            'Exploration of use of best animal models and their predictive value for a human vaccine',\n            'Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents',\n            'Alternative models to aid decision makers',\n            'Efforts targeted at a universal coronavirus vaccine',\n            'Approaches to evaluate risk for enhanced disease after vaccination'\n        ]\n    }\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for task in tasks:\n    print(f'Task: {task[\"name\"]}\\n')\n    for question in task['questions']:\n        found_articles = searchArticles(bm25_index, tokenized_corpus, articles, question)\n        print(f'\\nQuestion: {question}\\n')\n        for article in found_articles[:3]:\n            print(f'Title: {article[\"title\"]}\\n')\n            if len(article['summary']):\n                print(f'Summary: {article[\"summary\"]}')\n            elif len(article['abstract']):\n                print(f'Abstract: {article[\"abstract\"][0][\"text\"]}')\n\n            print()\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Try it out!"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Search for a query, seperate words by a space')\n\nwhile True:\n    try:\n        query = input('Search: ')\n\n        # Query the index and to get a list of articles a long with a summary\n        found_articles = searchArticles(bm25_index, tokenized_corpus, articles, query)\n\n        print(f'\\n{len(found_articles)} Articles found:')\n\n        for article in found_articles[:3]:\n            print(f'\\t- Title: {article[\"title\"]}')\n            if article['summary']:\n                print(f'\\t- Summary: {article[\"summary\"]}')\n            elif len(article['abstract']):\n                print(f'\\t- Abstract: {article[\"abstract\"][0][\"text\"]}')\n                print()\n\n        print()\n        \n    except:\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I hope this can help anyone in any shape or form. Thank you!\n\nhttp://c-ovid-19.technology/\n\nhttps://github.com/samikhalildev/COVID-19-Research-Engine"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}