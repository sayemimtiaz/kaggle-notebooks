{"cells":[{"metadata":{},"cell_type":"markdown","source":"#Imbalanced Data\n\nImbalanced data typically refers to a problem with classification problems where the classes are not represented equally.\n\nFor example, you may have a 2-class (binary) classification problem with 100 instances (rows). A total of 80 instances are labeled with Class-1 and the remaining 20 instances are labeled with Class-2.\n\nThis is an imbalanced dataset and the ratio of Class-1 to Class-2 instances is 80:20 or more concisely 4:1.\n\nYou can have a class imbalance problem on two-class classification problems as well as multi-class classification problems. Most techniques can be used on either.\nhttps://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/"},{"metadata":{},"cell_type":"markdown","source":"![](https://miro.medium.com/max/450/1*zsyN08VVrgHbAEdvv27Pyw.png)medium.com"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"nRowsRead = 1000  # specify 'None' if want to read whole file\ndata = pd.read_csv(\n    \"../input/cusersmarildownloadsfostercsv/foster.csv\",\n    delimiter=\";\",\n    encoding=\"utf8\",\n    nrows=nRowsRead,\n)\ndata.dataframeName = \"foster.csv\"\nnRow, nCol = data.shape\nprint(f\"There are {nRow} rows and {nCol} columns\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Codes by Marco Carujo. https://www.kaggle.com/mcarujo/churn-prediction-ann-over-under-sampling"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install nb_black -q","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%load_ext nb_black","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\nimport plotly.figure_factory as ff\nimport os\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To plot numerical column\ndef plot_hist(data, column):\n    fig = px.histogram(data, x=column, color=\"RegionID\")\n    fig.show()\n    fig = ff.create_table(pd.DataFrame(data[column].describe()).T)\n    fig.show()\n\n\n# To plot categorical column\ndef plot_count(data, column):\n    df = data.groupby(column)[\"RegionID\"].value_counts()\n    df = pd.DataFrame(df)\n    df.columns = [\"Count\"]\n    df.reset_index(inplace=True)\n    fig = px.bar(\n        df, x=column, y=\"Count\", color=\"RegionID\", text=\"Count\", barmode=\"group\"\n    )\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_hist(data, \"RegionID\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_count(data, \"AreaID\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_count(data, \"Area\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_hist(data, \"Jan\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_hist(data, \"Dec\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n\nenc = OneHotEncoder(handle_unknown=\"ignore\")\nstander_scaler = StandardScaler()\nlabel_encoder = LabelEncoder()\n\nX = np.concatenate(\n    (\n        ## OneHotEncoder\n        enc.fit_transform(data[[\"Area\"]]).toarray(),\n        ## Stander Scaler\n        stander_scaler.fit_transform(\n            data[\n                [\n                    \"Jan\",\n                    \"Feb\",\n                    \"Mar\",\n                    \"Apr\",\n                    \"May\",\n                    \"Jun\",\n                    \"Jul\",\n                    \"Aug\",\n                    \"Sep\",\n                    \"Oct\",\n                    \"Nov\",\n                    \"Dec\",\n                ]\n            ]\n        ),\n        ## LabelEncoder\n        label_encoder.fit_transform(data[[\"Area\"]]).reshape(-1, 1),\n        ## No formatation\n        data[[\"RegionID\", \"AreaID\"]].values,\n    ),\n    axis=1,\n)\n\ny = data.RegionID.values\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = (\n    [el for el in enc.categories_[0]]\n    + [\n        \"Jan\",\n        \"Feb\",\n        \"Mar\",\n        \"Apr\",\n        \"May\",\n        \"Jun\",\n        \"Jul\",\n        \"Aug\",\n        \"Sep\",\n        \"Oct\",\n        \"Nov\",\n        \"Dec\",\n    ]\n    + [\"Area\"]\n    + [\"RegionID\", \"AreaID\"]\n    + [\"RegionID\"]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ntable = pd.DataFrame(np.concatenate([X, y.reshape(-1, 1)], axis=1))\ntable.columns = columns\ntable = table.corr()\nwith sns.axes_style(\"white\"):\n    mask = np.zeros_like(table)\n    mask[np.triu_indices_from(mask)] = True\n    plt.figure(figsize=(10, 10))\n    sns.heatmap(\n        round(table, 2),\n        cmap=\"Reds\",\n        mask=mask,\n        vmax=table.max().max(),\n        vmin=table.min().min(),\n        linewidths=0.5,\n        annot=True,\n        annot_kws={\"size\": 12},\n    ).set_title(\"Correlation Matrix App behavior dataset\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Conv2D\nimport keras\n\n\ndef get_model():\n    return Sequential(\n        [\n            Dense(units=200, input_dim=12, activation=\"relu\"),\n            Dense(150, activation=\"relu\"),\n            Dropout(0.2),\n            Dense(100, activation=\"relu\"),\n            Dense(100, activation=\"relu\"),\n            Dropout(0.2),\n            Dense(100, activation=\"relu\"),\n            Dense(100, activation=\"relu\"),\n            Dense(100, activation=\"relu\"),\n            Dropout(0.2),\n            Dense(100, activation=\"relu\"),\n            Dense(1, activation=\"sigmoid\"),\n        ]\n    )\n\n\ndef train_ann(X, y):\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.1, random_state=42\n    )\n\n    model = get_model()\n\n    model.compile(\n        optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"mse\", \"accuracy\"],\n    )\n\n    # Trainig and returning back the results.\n    history = model.fit(\n        X_train,\n        y_train,\n        batch_size=10,\n        epochs=50,\n        verbose=0,\n        validation_data=(X_test, y_test),\n    )\n    loss, mse, acc = model.evaluate(X_test, y_test, verbose=0)\n    fig = ff.create_table(\n        pd.DataFrame([(loss, mse, acc)], columns=[\"Loss\", \"MSE\", \"Accuracy\"]),\n    )\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\nX, y = SMOTE(random_state=42).fit_resample(X, y)\n\ntrain_ann(X, y)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler\n\nX, y = RandomOverSampler(random_state=42).fit_resample(X, y)\n\ntrain_ann(X, y)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"from imblearn.over_sampling import BorderlineSMOTE\n\nX, y = BorderlineSMOTE(random_state=42).fit_resample(X, y)\n\ntrain_ann(X, y)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"from imblearn.over_sampling import ADASYN\n\nX, y = ADASYN(random_state=42).fit_resample(X, y)\n\ntrain_ann(X, y)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"from imblearn.over_sampling import KMeansSMOTE\n\nX, y = KMeansSMOTE(random_state=42).fit_resample(X, y)\n\ntrain_ann(X, y)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"from imblearn.over_sampling import SVMSMOTE\n\nX, y = SVMSMOTE(random_state=42).fit_resample(X, y)\n\ntrain_ann(X, y)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"from imblearn.under_sampling import ClusterCentroids\n\nX, y = ClusterCentroids(random_state=42).fit_resample(X, y)\n\ntrain_ann(X, y)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"from imblearn.under_sampling import AllKNN\n\nX, y = AllKNN().fit_resample(X, y)\n\ntrain_ann(X, y)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"from imblearn.under_sampling import NeighbourhoodCleaningRule\n\nX, y = NeighbourhoodCleaningRule().fit_resample(X, y)\n\ntrain_ann(X, y)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"from imblearn.under_sampling import RandomUnderSampler\n\nX, y = RandomUnderSampler().fit_resample(X, y)\n\ntrain_ann(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Added to my list: learn how to deal with imbalanced data."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}