{"cells":[{"metadata":{},"cell_type":"markdown","source":"## KMeans Clustering vs LDA Topic Modeling \n### -- using Polymerase subset of 1943 papers \n\n\n#### My hypothesis was instead of LDA, a visual method using wordcloud after KMeans Clustering could be helpful to find topics for each group.\n#### I did the wordcloud before LDA to keep me blinded and prevent bias.\n\n#### The results are positive. When the size of papers in collection is small enough, topics stand out of wordcloud plot for each group. \n#### I run LDA topic modeling later to verify my previous semi-manual topic modeling. \n#### Kind of see the similar results on the top-30 Most Salient Terms on pyLDAvis dashboard. \n#### For example, for group 1 of the papers, flu and children are the most salient terms for topic 1 and topic 2 for this pile of papers.\n\n## Results\n\n\n### Results from WordCloud visualizations after clustering (optimized cluster number=6): \n#### Topics for each group (see appendix plot for more details):\n\n##### G1 flu children clinical, 456 papers  = topic 5\n##### G2 gene protein cell IFN, 679 papers  = topic 6 ?\n##### G3 bat host phylogenetic transmission spike, 41 papers  = topic 4\n##### G4 SARS COV MERS COV, 133 papers = topic 1\n##### G5 diganosis asssay detection sensitivity specificity, 222 papers  = topic 2\n##### G6 RdRp RNA replication RNA synthesis, 412 papers = topic 3 ?\n\n--------------------\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"\n### Results from LDA topic modeling for 6 groups\n#### Topics for each group:\n\n| Topic 1 | Topic 2 | Topic 3 | Topic 4 | Topic 5 | Topic 6 | \n|------|------|------|------|------|:-:|\n|sars|pcr|sequence|viruses|respiratory|rna|\n|cov|rt|gene|human|patients|viral|\n|patients|detection|rna|hcov|viral|replication|\n|infection|assay|de|species|children|protein|\n|respiratory|samples|ibv|strains|influenza|cells|\n|coronavirus|time|genome|bats|viruses|expression|\n|mers|real|sequences|rna|infections|proteins|\n|syndrome|using|strain|analysis|infection|cell|\n\n\n----------------------------\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n#### KMeans Clustering and LDA Topic Modeling produced very similar results! \n#### From the above results, most of the clustering results match the topic modeling results! \n#### The only exceptions are topic 3 and 6 kind of mixed clustering group 2 and 6. So the two methods can use interchangeably. \n#### The advantage of LDA is it return you the topics automately while KMeans clustering required manually check the top words cloud.\n#### However, the clustering method give you the exact papers for each cluster.\n\n#### To compare LDA topic modeling with the WordCloud method, see code of the clustering method on GitHub \nhttps://github.com/lj89/CORD19/blob/master/Polymerase_subset_find_topic_for_groups_after_clustering2.ipynb\n\n\n#### Additional, I shoud add protein and proteins, sequence and sequences, etc, to CONTRACTION_MAP"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Appendix - WordCloud plots for each clustering group \n#### (6 groups in total)\n\n### All papers\n#### From wordcloud of all papers we do not see any information helpful to group the papers.\n\n\n  <img src=\"https://i2.wp.com/parrotbike.wpcomstaging.com/wp-content/uploads/2020/04/all1943updated.png?resize=768%2C430&ssl=1\" width=\"800px\" align=\"left\"> \n","attachments":{}},{"metadata":{},"cell_type":"markdown","source":"### Group 1\n\n#### In wordcloud of group 1, we can see the words \"children\" and \"influenza\" stand out, the word \"clinical\" is also pretty Salient .\n\n<img src=\"https://i1.wp.com/parrotbike.wpcomstaging.com/wp-content/uploads/2020/04/G1-topic-flu-children-clinical.png?resize=768%2C380&ssl=1\" width=\"800px\" align=\"left\"> \n\n\n\n","attachments":{}},{"metadata":{},"cell_type":"markdown","source":"### Group 2\n<img src=\"https://i1.wp.com/parrotbike.wpcomstaging.com/wp-content/uploads/2020/04/G2-topic-gene-protein-cell-IFN-IBV.png?resize=768%2C380&ssl=1\" width=\"800px\" align=\"left\"> "},{"metadata":{},"cell_type":"markdown","source":"### Group 3\n\n<img src=\"https://i2.wp.com/parrotbike.wpcomstaging.com/wp-content/uploads/2020/04/G3-topic-bat-host-phylogenetic-transmission.png?resize=768%2C386&ssl=1\" width=\"800px\" align=\"left\"> \n\n"},{"metadata":{},"cell_type":"markdown","source":"### Group 4\n<img src=\"https://i1.wp.com/parrotbike.wpcomstaging.com/wp-content/uploads/2020/04/G4-SARS-COV-MERS-COV.png?resize=768%2C380&ssl=1\" width=\"800px\" align=\"left\"> ","attachments":{}},{"metadata":{},"cell_type":"markdown","source":"### Group 5\n<img src=\"https://i1.wp.com/parrotbike.wpcomstaging.com/wp-content/uploads/2020/04/G5-diganosis-asssay-detection-sensitivity-specificity.png?resize=768%2C380&ssl=1\" width=\"800px\" align=\"left\"> "},{"metadata":{},"cell_type":"markdown","source":"### Group 6\n<img src=\"https://i1.wp.com/parrotbike.wpcomstaging.com/wp-content/uploads/2020/04/G6-topic-RdRp-RNA-replication-RNA-synthesis.png?resize=768%2C380&ssl=1\" width=\"800px\" align=\"left\"> "},{"metadata":{},"cell_type":"markdown","source":"### KMeans clustering results (optimal 6 clusters)\n\n<img src=\"https://i1.wp.com/parrotbike.wpcomstaging.com/wp-content/uploads/2020/04/6clusters.png?w=446&ssl=1\" width=\"800px\" align=\"left\"> "},{"metadata":{},"cell_type":"markdown","source":"## Code\n### Content\n\nFor a quicker access to each part, please use the below links. \n\n[Libraries and System Information](#Libraries-and-System-Information)\n\n[Text Processing](#Text-Processing)\n\n[Feature Extraction](#Feature-Extraction)\n\n[Topic Modelling](#Topic-Modelling)\n\n[Visualization](#Visualization)\n\n[Findings](#Findings)\n\n[References](#References)"},{"metadata":{},"cell_type":"markdown","source":"### Libraries and System Information"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install mglearn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport numpy as np\nimport nltk\nfrom nltk.tag import UnigramTagger\nfrom nltk.corpus import words, stopwords\nfrom nltk import word_tokenize, sent_tokenize, pos_tag\nfrom nltk.corpus import wordnet\nfrom bs4 import BeautifulSoup\nfrom collections import Counter\nimport requests\nimport os\nimport pandas as pd\nimport bs4\nimport sys\nimport sklearn\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.manifold import MDS\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\nimport pyLDAvis.sklearn\n\nfrom html.parser import HTMLParser\nimport spacy\nfrom itertools import combinations \nimport warnings\nwarnings.filterwarnings('ignore')\nfrom operator import itemgetter\nfrom nltk.corpus import wordnet as wn\nfrom html.parser import HTMLParser\nimport pprint\nimport string\nimport statistics\nimport matplotlib.pyplot as plt\nfrom tabulate import tabulate\nimport seaborn as sns\nimport time\nimport mglearn\nfrom wordcloud import WordCloud\nprint(\"Below, I am providing system information.\")\nprint(\"=========================================\")\nprint(\"Sklearn version -\",sklearn.__version__)\nprint(\"NLTK version -\",nltk.__version__)\nprint(\"SpaCy version -\",spacy.__version__)\nprint(\"BeautifulSoup version -\",bs4.__version__)\nprint(\"Numpy version -\",np.__version__)\nprint(\"Pandas version -\",pd.__version__)\nprint(\"Python and Anaconda version -\",sys.version)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Text Processing"},{"metadata":{},"cell_type":"markdown","source":"As a part of standardizing the text data, I will do text processing in this part. The functions I have used to normalize the text comes from the book Text Analytics with Python A Practical Real-World Approach to Gaining Actionable Insights from Your Data by Dipanjan Sarkar[1]. The functions below are to tokenize the text, expand contractions, remove special characters, add POS tags and remove stop words. The contraction map is from GitHub of Dipanjan Sarkar[2].\n\nIn this part, I will perform HTML tags removal, special characters removal, contraction expansion, and stopwords removal. Since the data is origianlly from online, the texts have the possibilities that they contain HTML tags that do not provide much valyue to this analysis. They need to be removed before putting into feature extraction process. In case that there are special charaters in the reviews, I will remove special characters. The contraction expansion and stopwords removal are used to expand the contractions and remove the stopwords. I have added other words based on the stopwords list provided by NLTK package. "},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = nltk.corpus.stopwords.words('english')\n\n# page 268 adding more words into the original list\nstopwords = stopwords + ['mr', 'mrs', 'family','come', 'go', 'get', 'tell', 'listen', 'one', 'two', 'three', 'four', \n                         'five', 'six', 'seven', 'eight', 'nine', 'zero', 'join', 'find', 'make', 'say', \n                         'ask', 'tell', 'see', 'try', 'back', 'also','movie',\n                         '1','2','3','4','5','6','7','8','9','10','0',\n                         'film', 'movie', 'watch', 'cinema', 'scene','action', 'fighting','story', '3D'\n                         'show', 'get','tell', 'listen']\n\n# github of dipanjanS [2]\nCONTRACTION_MAP = {\"ain't\": \"is not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'cause\": \"because\",\n                   \"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\n                   \"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\n                   \"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\n                   \"he'll've\": \"he he will have\",\"he's\": \"he is\",\"how'd\": \"how did\",\"how'd'y\": \"how do you\",\n                   \"how'll\": \"how will\",\"how's\": \"how is\",\"I'd\": \"I would\",\"I'd've\": \"I would have\",\"I'll\": \"I will\",\n                   \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\"i'd\": \"i would\",\"i'd've\": \"i would have\",\n                   \"i'll\": \"i will\",\"i'll've\": \"i will have\",\"i'm\": \"i am\",\"i've\": \"i have\",\"isn't\": \"is not\",\n                   \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\"it's\": \"it is\",\n                   \"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\n                   \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\n                   \"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n                   \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"shan't've\": \"shall not have\",\n                   \"she'd\": \"she would\",\"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\n                   \"she's\": \"she is\",\"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\n                   \"so've\": \"so have\",\"so's\": \"so as\",\"that'd\": \"that would\",\"that'd've\": \"that would have\",\"that's\": \"that is\",\n                   \"there'd\": \"there would\",\"there'd've\": \"there would have\",\"there's\": \"there is\",\"they'd\": \"they would\",\n                   \"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\n                   \"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\"we'd've\": \"we would have\",\n                   \"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\"weren't\": \"were not\",\n                   \"what'll\": \"what will\",\"what'll've\": \"what will have\",\"what're\": \"what are\",\"what's\": \"what is\",\n                   \"what've\": \"what have\",\"when's\": \"when is\",\"when've\": \"when have\",\"where'd\": \"where did\",\n                   \"where's\": \"where is\",\"where've\": \"where have\",\"who'll\": \"who will\",\"who'll've\": \"who will have\",\n                   \"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\"why've\": \"why have\",\"will've\": \"will have\",\n                   \"won't\": \"will not\",\"won't've\": \"will not have\",\"would've\": \"would have\",\"wouldn't\": \"would not\",\n                   \"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\"y'all'd\": \"you all would\",\n                   \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\",\n                   \"you'd've\": \"you would have\",\"you'll\": \"you will\",\"you'll've\": \"you will have\",\"you're\": \"you are\",\n                   \"you've\": \"you have\"}\n\n# Ben Brock's Analyzing Movie Reviews - Sentiment Analysis notebook\ndef strip_html_tags(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    stripped_text = soup.get_text()\n    return stripped_text\n\n# page 175 to tokenize the text\ndef tokenize(text):\n    tokens = nltk.word_tokenize(text) \n    tokens = [token.strip() for token in tokens]\n    return tokens\n\n# page 118 to expand the contractions\ndef expand_contractions(sentence, CONTRACTION_MAP):\n    contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())),flags=re.IGNORECASE|re.DOTALL)\n    \n    def expand_match(contraction):\n        match = contraction.group(0)\n        first_char = match[0]\n        expanded_contraction = CONTRACTION_MAP.get(match)\\\n                                if CONTRACTION_MAP.get(match)\\\n                                else CONTRACTION_MAP.get(match.lower())\n        expanded_contraction = first_char + expanded_contraction[1:]\n        return expanded_contraction\n    expanded_sentence = contractions_pattern.sub(expand_match, sentence)\n    return expanded_sentence\n\n# page 176 remove the special symbols and characters\ndef remove_special_characters(text):\n    tokens = tokenize(text)\n    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n    filtered_tokens = filter(None, [pattern.sub(' ', token) for token in tokens])\n    filtered_text = ' '.join(filtered_tokens)\n    return filtered_text\n\n# page 120 to remove the stop words\ndef remove_stopwords(sentence):\n    tokens = nltk.word_tokenize(sentence)\n    stopword_list = nltk.corpus.stopwords.words('english')\n    filtered_tokens = [token for token in tokens if token not in stopword_list]\n    return filtered_tokens\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now functions have been established for the text processing, I will load the abstracts data. The data is mentioned in the book Text Analytics with Python A Practical Real-World Approach to Gaining Actionable Insights from Your Data by Dipanjan Sarkar[1]. The data is from Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies by Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher[3]. The moview review data has 50,000 rows and 2 columns. After data import, I will spilt the data into 70% training and 30% testing dataset correspondingly. "},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load in \n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the \"../input/\" directory.\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd \n\nreview_dataset = pd.read_csv('/kaggle/input/1943polymerase/1943tibbleABSTRACT_polymeraseTitleABSGroup.csv')\nprint('Let us take a brief look at the data:\\n',review_dataset.head())\n\nreview = np.array(review_dataset['ABS'])\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# normalizing the train data\nprint('Normalize dataset needs:')\nhtml_parser = HTMLParser()\nnormed_review = []\nstart = time.time()\nfor text in review:\n    text = html_parser.unescape(text)\n    text = expand_contractions(text, CONTRACTION_MAP)\n    text = text.lower()\n    text = remove_special_characters(text)\n    text = remove_stopwords(text)\n    normed_review.append(text)\nnormed_review = [str (item) for item in normed_review]\nnormed_review = [item for item in normed_review if not isinstance(item, int)]\n\nend = time.time()\nprint(end - start,'seconds')\nprint(normed_review)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[Back to Content](#Content)"},{"metadata":{},"cell_type":"markdown","source":"### Feature Extraction"},{"metadata":{},"cell_type":"markdown","source":"Now we have normalized them for future use. Let me establish the feature matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"# page 345 to extract features\ndef build_feature_matrix(documents, feature_type='frequency', ngram_range=(1, 1), min_df=0.0, max_df=1.0):\n    feature_type = feature_type.lower().strip()\n    if feature_type == 'binary':\n        vectorizer = CountVectorizer(binary=True, min_df=min_df,\n                                     max_df=max_df, ngram_range=ngram_range)\n    elif feature_type == 'frequency':\n        vectorizer = CountVectorizer(binary=False, min_df=min_df,max_df=max_df, ngram_range=ngram_range)\n    elif feature_type == 'tfidf': \n        vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df, ngram_range=ngram_range)\n    else: \n        raise Exception(\"Wrong feature type entered. Possible values:'binary', 'frequency', 'tfidf'\")\n    feature_matrix = vectorizer.fit_transform(documents).astype(float)\n    return vectorizer, feature_matrix\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer, feature_matrix = build_feature_matrix(normed_review, feature_type='frequency',min_df=0.01, max_df=0.55)\nprint(\"The dimension of the feature matrix is\",feature_matrix.shape)\nfeature_names = vectorizer.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[Back to Content](#Content)"},{"metadata":{},"cell_type":"markdown","source":"### Topic Modelling"},{"metadata":{},"cell_type":"markdown","source":"> In NLP, Latent Dirichlet Allocation(LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's presence is attributable to one of the document's topics. LDA is an example of a topic model.[4] Simply saying, just like pizza and toppings, in terms of topic modelling, the composites are documents and the parts are words and/or phrases(phrases n words in length are referred to as n-grams).\n\n\n> If you view the number of topics as a number of clusters and the probabilities as the proportion of cluster membership, then using LDA is a way of soft-clustering composites and parts. Similar to k-Means clustering, each instance can only belong to one specific cluster but LDA allows a fuzzy membership. It adds more nuances. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets set a base number of components\nn_comp = 6\nstart = time.time()\n# LDA set up\nlda = LatentDirichletAllocation(n_components = n_comp,\n                                random_state= 2019,\n                                learning_method= 'online',\n                                verbose = True)\n\n\ndata_lda_p = lda.fit_transform(feature_matrix)\nend = time.time()\nprint(end - start,'seconds')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print out the topics using a for loop for reviews\nfor idx, topic in enumerate(lda.components_):\n    print(\"Topic\",idx+1)\n    print([(vectorizer.get_feature_names()[i], topic[i])\n           for i in topic.argsort()[:-8 - 1:-1]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[Back to Content](#Content)"},{"metadata":{},"cell_type":"markdown","source":"## Visualize Topic Modelling Results - interactive plot"},{"metadata":{},"cell_type":"markdown","source":"Using pyLDAvis to illustration the topic modelling result. Feel free to explore the interactive results."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"pyLDAvis.enable_notebook()\n\ndashboard = pyLDAvis.sklearn.prepare(lda, feature_matrix, vectorizer)\n\ndashboard","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review_wordcloud = ' '.join(normed_review)\n\nwordcloud = WordCloud().generate(review_wordcloud)\nplt.figure(figsize = (16, 9))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Findings\n\nTopic modeling is a statistical modeling for discovering the topics that occur in a collection of documents. LDA is modeled as Dirichlet distributions and is one way of topic modelling. LDA is used to classify the text in a document to a specific topic.\n\n\n| Topic 1 | Topic 2 | Topic 3 | Topic 4 | Topic 5 | Topic 6 | \n|------|------|------|------|------|:-:|\n|sars|pcr|sequence|viruses|respiratory|rna|\n|cov|rt|gene|human|patients|viral|\n|patients|detection|rna|hcov|viral|replication|\n|infection|assay|de|species|children|protein|\n|respiratory|samples|ibv|strains|influenza|cells|\n|coronavirus|time|genome|bats|viruses|expression|\n|mers|real|sequences|rna|infections|proteins|\n|syndrome|using|strain|analysis|infection|cell|"},{"metadata":{"trusted":true},"cell_type":"code","source":"# more components\nn_comp = 10\nstart = time.time()\n# LDA set up\nlda = LatentDirichletAllocation(n_components = n_comp,\n                                random_state= 2019,\n                                learning_method= 'online',\n                                verbose = True)\n\n\ndata_lda_p = lda.fit_transform(feature_matrix)\nend = time.time()\nprint(end - start,'seconds')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print out the topics using a for loop for reviews\nfor idx, topic in enumerate(lda.components_):\n    print(\"Topic\",idx+1)\n    print([(vectorizer.get_feature_names()[i], topic[i])\n           for i in topic.argsort()[:-10 - 1:-1]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pyLDAvis.enable_notebook()\n\ndashboard = pyLDAvis.sklearn.prepare(lda, feature_matrix, vectorizer)\n\ndashboard","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 6 groups seem to have better separations among groups","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### References\n\n[1] Text Analytics with Python A Practical Real-World Approach to Gaining Actionable Insights from Your Data by Dipanjan Sarkar.\n\n[2] https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/notebooks/Ch07_Analyzing_Movie_Reviews_Sentiment/contractions.py\n\n[3] http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.bib\n\n[4] https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\n\n[5] https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d\n\n[6] Nuoya_Rezsonya_Topic_Modelling_code"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":4}