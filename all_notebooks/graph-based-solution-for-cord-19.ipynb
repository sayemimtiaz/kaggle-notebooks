{"cells":[{"metadata":{},"cell_type":"markdown","source":"# About this notebook\nThis notebook is focused on the task: \"What has been published about ethical and social science considerations?\" of the COVID-19 Open Research Dataset Challenge (CORD-19). For calculation purposes, a smaller part of the CORD-19 corpus will be used, filtered by social and ethical terms. Nevertheless, the whole approach can be perfectly replicated for the whole dataset resulting in meaningfull outcomes.\n\nThe **objective** of this submission is to map the CORD-19 articles' corpus in a network. The graph structure will allow the scientific community to reveal 'hidden' properties otherwise hardly distinguishable. We believe that articles could be mapped closer together in the topological space of a graph when sharing many and different attributes. This characteristic of graphs could be of great use in future analysis and exploitation.\n\n<a id=\"section-one\"></a>\n# 1. Contents\n* [1. Contents](#section-one)\n* [2. The idea](#section-two)\n* [3. The data](#section-three)\n* [4. Building the graph](#section-four)\n    - [4.1 Similarity-based links](#section-four-one)\n    - [4.2 Other attribute-based links](#section-four-two)\n    - [4.3 Transforming into a multigraph](#section-four-three)\n* [5. Common neighbours](#section-five)\n* [6. Future work](#section-six)\n\n<a id=\"section-two\"></a>\n# 2. The idea\nThe initial, most adecuate idea as a response to the challenge was to build a graph database containing all information related to the corpus as visualised in the following diagram. This approach would allow the use of graph algorithms and metrics to infer more information on the corpus."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nImage(\"../input/cord19images/GraphDatabase.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Of course, space, time and means were limited, so the decision was to explore the functionalities of [networkx](http://https://networkx.github.io/) library of Python and transform the initial idea of a graph database into a graph of type Multigraph. A multigraph permits the use of multiple connections between elements. For example, article A can share with B different attributes, such as being written by the same author, published in the same journal, talking about the same subject, etc. In this approach, all those sharing attributes will be represented as links and with the articles forming the nodes. The following diagram gives an example of possible connections between articles. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Image(\"../input/cord19images/Mulltigraph.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you may have observed, links between articles in the previous diagram differ in thicknesses. The idea is to assign a weight in each link and work with weighted networks. Higher weights will allow articles to be mapped closer together in the topological space of the graph. This distribution in the \"graph's space\" will allow in the future the application of metrics and algorithms that will reveal the important properties of the nodes (articles). Some examples are: centrality measures,  communities or clusters, closer neighbours, etc."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-three\"></a>\n# 3. The data\n\nAs mentioned previously, the focus here will be on articles related to COVID and to social and ethical considerations. The imported dataset is a filtered, by social and ethical terms, version of the initial CORD-19 corpus. It includes the full text of the articles and also contains further information. This information is a result of a previous [notebook](http://www.kaggle.com/jredondopizarro/cord-19-semantic-similarities-with-use-and-doc2vec) containing similarities between titels, abstracts and full texts. The similarities between titles and between abstracts were calculated using the Universal Sentence Encoder (USE) model applying cosine distance between vector embeddings and the ones between the full texts using a doc2vec model."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"# Import libraries\nimport covid19_tools as cv19\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pickle\nimport networkx as nx\nfrom networkx.algorithms import bipartite\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"with open(r\"/kaggle/input/semantic-similarities-with-use-and-doc2vec/output_data/data.pkl\", \"rb\") as input_file:\n    df = pickle.load(input_file)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-four\"></a>\n# 4. Building the graph\n<a id=\"section-four-one\"></a>\n## 4.1 Similarity-based links\nWe will start building the graph using the three similarities calculated in the previous [notebook](http://www.kaggle.com/jredondopizarro/cord-19-semantic-similarities-with-use-and-doc2vec). The similarity scores will be used as weights for the graphs."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading the similarity matrices\nuse_title_sim_matrix = np.load(\"/kaggle/input/semantic-similarities-with-use-and-doc2vec/output_data/title_sim.npy\")\nuse_abstract_sim_matrix = np.load(\"/kaggle/input/semantic-similarities-with-use-and-doc2vec/output_data/abstract_sim.npy\")\ndoc2vec_full_text_sim_matrix = np.load(\"/kaggle/input/semantic-similarities-with-use-and-doc2vec/output_data/full_text_sim.npy\")\n\n#Extracting the articles' ids\nindex_id = df['cord_uid'].values\n\n#Preparing the dfs that include the ids both as indexes and columns\ntitle_sim_df = pd.DataFrame(use_title_sim_matrix, index = index_id, columns = index_id)\nabstract_sim_df = pd.DataFrame(use_abstract_sim_matrix, index = index_id, columns = index_id)\ntext_sim_df = pd.DataFrame(doc2vec_full_text_sim_matrix, index = index_id, columns = index_id)\ntitle_sim_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We may now start building the individual networks, one for each similarity type."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a dictionary with the nodes (articles)\nnodes_dict = dict([x for x in enumerate(index_id)])\n\n#Initialise graph of title similarities\nG_title = nx.from_numpy_matrix(np.matrix(use_title_sim_matrix), create_using=nx.Graph) # Creates a graph from a numpy matrix\nG_title = nx.relabel_nodes(G_title,nodes_dict) # Relabels the nodes using the Ids\nG_title.remove_edges_from(nx.selfloop_edges(G_title)) # Removes selfloops\nprint(\"Number of title-graph nodes: {0}, Number of graph edges: {1} \".format(len(G_title.nodes()), G_title.size()))\n\n#Initialise graph of abstract similarities\nG_abstract = nx.from_numpy_matrix(np.matrix(use_abstract_sim_matrix), create_using=nx.Graph) # Creates a graph from a numpy matrix\nG_abstract = nx.relabel_nodes(G_abstract,nodes_dict) # Relabels the nodes using the Ids\nG_abstract.remove_edges_from(nx.selfloop_edges(G_abstract)) # Removes selfloops\nprint(\"Number of abstract-graph nodes: {0}, Number of graph edges: {1} \".format(len(G_abstract.nodes()), G_abstract.size()))\n\n#Initialise graph of abstract similarities\nG_text = nx.from_numpy_matrix(np.matrix(doc2vec_full_text_sim_matrix), create_using=nx.Graph) # Creates a graph from a numpy matrix\nG_text = nx.relabel_nodes(G_text,nodes_dict) # Relabels the nodes using the Ids\nG_text.remove_edges_from(nx.selfloop_edges(G_text)) # Removes selfloops\nprint(\"Number of text-graph nodes: {0}, Number of graph edges: {1} \".format(len(G_text.nodes()), G_text.size()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As an example, we may check the edges of a concrete node to see its connections. "},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Check edges of node 'wyz5jyjh'\nG_title.edges('wyz5jyjh', data = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now save the individual graphs to dfs. We will use those dfs later to build the multigraph."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Save title-graph to df\ndf_G_title = nx.to_pandas_edgelist(G_title)\ndf_G_title = df_G_title.dropna()\ndf_G_title['sharing'] = 'title_similarity'\nprint(df_G_title.shape)\n\n#Save abstract-graph to df\ndf_G_abstract = nx.to_pandas_edgelist(G_abstract)\ndf_G_abstract = df_G_abstract.dropna()\ndf_G_abstract['sharing'] = 'abstract_similarity'\nprint(df_G_abstract.shape)\n\n#Save text-graph to df\ndf_G_text = nx.to_pandas_edgelist(G_text)\ndf_G_text = df_G_text.dropna()\ndf_G_text['sharing'] = 'text_similarity'\nprint(df_G_text.shape)\ndf_G_text.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-four-two\"></a>\n## 4.2 Other attribute-based links\n\nThe similarity links where the only direct links between articles of the graph. All the other links are projections of bipartite graphs.\n\nThere is a long list of existing attributes in the dataset that could be used as sharing properties of the articles and translated into links in our graph. For example, articles could share Journal, Authors, Citations, Tag, etc."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-four-two-one\"></a>\n### 4.2.1 Sharing Journal attribute\nOf the previously mentioned sharing attribute we selected to represent the sharing journal one. The process can be repeated for the rest of the attributes. \n\nWe believe that articles appearing in the same Journal usually share similar context and should be mapped closer together. The process is similar to the similarities' graphs but here we will start working with bipartite networks, i.e., networks that include two types of nodes. In our case the types of nodes will be articles and journals.\n\nWe detected 649 journals missing, those rows will be exluded."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create article-journal df and drop na\narticle_journal_df = df[['cord_uid', 'journal']].dropna()\nprint(article_journal_df.shape)\narticle_journal_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Initialise the article journal graph\nG_article_journal = nx.Graph()\n\n#Add nodes from df\nG_article_journal.add_nodes_from(article_journal_df['cord_uid'].unique(), bipartite = 'articles')\nG_article_journal.add_nodes_from(article_journal_df['journal'].unique(), bipartite = 'journals')\n\n#Add edges from df\nG_article_journal.add_edges_from(zip(article_journal_df['cord_uid'], article_journal_df['journal']))\n\nprint(\"Number of graph nodes: {0}, Number of graph edges: {1} \".format(len(G_article_journal.nodes()), G_article_journal.size()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's important here to understand the difference between a normal and a bipartite graph: The bipartite nodes contain an extra attribute indicating the type. In our case this would be 'article' or 'journal'."},{"metadata":{"trusted":true},"cell_type":"code","source":"G_article_journal.nodes['wyz5jyjh']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have the bipartite graph created we will check again for our favourite first node. We see that it is only connected to one journal: Clinical eHealth."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check edges of node 'wyz5jyjh'\nG_article_journal.edges('wyz5jyjh', data = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will now extract the projection of the bipartite article - journal graph on the articles type of nodes. Extracting the projection on journals makes no sense as they are not connected. They would be if an article (or more) was written in two journals which is not possible. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare the nodelists needed for computing projections: articles, journals\narticles = [n for n in G_article_journal.nodes() if G_article_journal.nodes[n]['bipartite'] == 'articles']\njournals = [n for n, d in G_article_journal.nodes(data=True) if d['bipartite'] == 'journals']\n\n# Compute the article projections: articlesG\narticlesG = nx.bipartite.projected_graph(G_article_journal, articles)\nprint(\"Number of articles graph nodes: {0}, Number of graph edges: {1} \".format(len(articlesG.nodes()), articlesG.size()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While we create those graphs we can play with and check different metrics, for example, the degree centrality distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the degree centrality using nx.degree_centrality: dcs\ndcs = nx.degree_centrality(articlesG)\n# Plot the histogram of degree centrality values\nplt.hist(list(dcs.values()))\n#plt.yscale('log')  \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will now transform the projection of the bipartite graph on articles to a simple graph. We'll do that because we want to add the weight attribute to the edges. As mentioned previously, weights are important as they indicate if articles should be mapped closer together. In this case, the weight is not trivial as it was with the similarity scores but could try with a weight of 0.2. In any case it can be changed in the future."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform graph into a simple graph (not bipartite) and add a weight of 0.2\nG_journals = nx.Graph()\nfor (u, v) in articlesG.edges():\n    G_journals.add_edge(u,v,weight=0.2)\n\nprint(\"Number of articles graph nodes: {0}, Number of graph edges: {1} \".format(len(G_journals.nodes()), G_journals.size()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"G_journals.edges('aeogp8c7', data = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now save the graph in a df format as we deed with the similarities ones. We will use it to build the multigraph."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Save text-graph to df\ndf_G_journals = nx.to_pandas_edgelist(G_journals)\n#df_G_journals = df_G_text.dropna()\ndf_G_journals['sharing'] = 'journal'\nprint(df_G_journals.shape)\ndf_G_journals.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-four-three\"></a>\n## 4.3 Transforming into a Multigraph\nTo build the multifgraph we will first append all individual dfs we created from the graphs."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Append the edgelists dfs\ndf_G_similarities = df_G_title.append(df_G_abstract, ignore_index=True)\ndf_G_similarities = df_G_similarities.append(df_G_text, ignore_index=True)\ndf_G_similarities = df_G_similarities.append(df_G_journals, ignore_index=True)\nprint(df_G_similarities.shape)\ndf_G_similarities.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And then initialise a graph of type Multigraph. We can see that although the number of nodes haven't increased, the number of edges is almost the triple."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Multigraph\nM = nx.to_networkx_graph(df_G_similarities, create_using=nx.MultiGraph)\nprint(\"Number of multigraph nodes: {0}, Number of graph edges: {1} \".format(len(M.nodes()), M.size()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we now check again the initial example node we will see that it has multiple edges with different weights and sharing attributes with other nodes (articles)."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Check edges of node 'wyz5jyjh'\nM.edges('wyz5jyjh', data = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will now export the multigraph in a pickle for future use."},{"metadata":{"trusted":true},"cell_type":"code","source":"nx.write_gpickle(M, '/kaggle/working/cord19_multigraph.gpickle')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-five\"></a>\n# 5. Common neighbours\nA simple analysis towards building a recommender system is that of a node's neighbours and common neighbours between two nodes. "},{"metadata":{"trusted":true},"cell_type":"code","source":"nbrs1 = M.neighbors('wyz5jyjh')\nprint(len(list(nbrs1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def shared_nodes(G, node1, node2):\n\n    # Get neighbors of node 1: nbrs1\n    nbrs1 = G.neighbors(node1)\n    # Get neighbors of node 2: nbrs2\n    nbrs2 = G.neighbors(node2)\n\n    # Compute the overlap using set intersections\n    overlap = set(nbrs1).intersection(nbrs2)\n    return overlap\n\n#Check the number of shared nodes between the first and second article\nprint(len(shared_nodes(M, 'wyz5jyjh', 'aeogp8c7')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Of course, as we have already added 4 different graphs in our multigraph, almost all nodes will be connected to each other. What we should create is a recommender system using and based on the weights of the edges."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-six\"></a>\n# 6. Future work\n* Build visual representations of parts of the network\n* Recommender systems\n* Integrate more data sources as attributes in the graph (sharing authors, citations, tags...)\n* Apply a clustering or community detection algorithm to find the articles closely related together\n* Apply topic modelling and extract main concepts of each cluster\n* Extend analysis to the complete corpus"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}