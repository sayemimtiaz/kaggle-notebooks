{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Smart Home Commands Classification"},{"metadata":{},"cell_type":"markdown","source":"I noticed that it's hard to find a decent dataset with all sorts of commands so I build my own tiny dataset that I could start with just for fun. Upside of a small dataset is the training time, it's possible to try a lot of different machine learning methods.\n"},{"metadata":{},"cell_type":"markdown","source":"Following machine learning classifiers will be tested:\n- random forests\n- support vector machines\n- xgboost\n- multi-layer perceptrons\n- catboost\n- lightgbm\n- TPOT"},{"metadata":{},"cell_type":"markdown","source":"What we are going to test in this notebook is the following:\n- averaged sentence representation vs TFIDF (term frequency inverse document frequency) representation\n- random forests vs support vector machines vs xgboost vs neural networks vs catboost vs lightgbm vs TPOT\n- radial basis function kernel vs linear kernel in SVM's"},{"metadata":{},"cell_type":"markdown","source":"## <center style=\"background-color: #6dc8b5; width:30%;\">Contents</center>\n* [Data Statistics](#data_statistics)\n* [Data Preperation](#data_preperation)\n* [Training Base Model](#training_base_model)\n* [Improved Models](#improved_models)\n* [Further Improvements](#further_improvements)\n* [Testing](#testing)"},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"data_statistics\"></a>\n# Data Statistics"},{"metadata":{},"cell_type":"markdown","source":"Some statistics about our small dataset of commands."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/smart-home-commands-dataset/dataset.csv\")\ndel df[\"Number\"]\ndf.sample(frac=1).head(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,10))\nsns.countplot(x=\"Category\", palette=\"rocket\", data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,10))\nsns.countplot(x=\"Subcategory\", palette=\"rocket\", data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.countplot(x=\"Question\", palette=\"rocket\", data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.countplot(x=\"Action_needed\", palette=\"rocket\", data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.countplot(x=\"Time\", palette=\"rocket\", data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.countplot(x=\"Action\", palette=\"rocket\", data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"data_preperation\"></a>\n# Data Preperation"},{"metadata":{},"cell_type":"markdown","source":"> One-hot word representation can be simply done with taking a vector initialize with all zeros and putting a one on the place of that word in the vocabulary.\n"},{"metadata":{},"cell_type":"markdown","source":"> Distributed word representations are a bit trickier, a neural network has to help us with that. To obtain these distributed word embeddings, word2vec is used. Word2vec uses the skip-gram model. It takes the weight vector between the input layer and the hidden layer after training each word with its closest neighbours. By taking the weight matrices of this neural network, hidden representations of the words have been encapsulated in the vector representations of each word. These hidden representations between words are already embedded in the word vector."},{"metadata":{},"cell_type":"markdown","source":"> In order to obtain a trainable vector for the machine learning methods to learn, a sentence has to be transformed to a vector. Each sentence consists of several words, that each have their own word representation. The word representations can be combined into a sentence representation, there are several ways to do this, either with a simple averaging of the word vectors or first multiplying with the TFIDF (Term Frequency Inverse Document Frequency) score and then averaging. This  score  is  obtained  bymultiplying the term frequency with the inverse document frequency.  The termfrequency  is  the  likelihood  of  a  word  occurring  in  a  sentence  and  the  inversedocument frequency is used to indicate how rare a word is in a sentence. This to avoid giving more importancy to sentences where the same word appears multiple times."},{"metadata":{},"cell_type":"markdown","source":"> Experience has shown that the dataset has way too little data to use distributed word vectors, so we will continue with the one-hot encoded words and sentences."},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import word_tokenize\nfrom sklearn.model_selection import train_test_split\nimport itertools\nimport math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = df['Sentence']\ncategories = df['Category']\nsubcategories = df['Subcategory']\nactions = df['Action']\n\nuniquecategories = list(set(categories))\nuniquesubcategories = list(set(subcategories))\nuniqueactions = list(set(actions))\n\nmergesentences = list(itertools.chain.from_iterable([word_tokenize(sentence.lower()) for sentence in sentences]))\nvocabulary = list(set(mergesentences))\nprint(vocabulary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculates how often the word appears in the sentence\ndef term_frequency(word, sentence):\n    return sentence.split().count(word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculates how often the word appears in the entire vocabulary\ndef document_frequency(word):\n    return vocabulary.count(word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# will make sure that unimportant words such as \"and\" that occur often will have lower weights\n# log taken to avoid exploding of IDF with words such as 'is' that can occur a lot\ndef inverse_document_frequency(word):\n    return math.log(len(vocabulary) / (document_frequency(word) + 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get term frequency inverse document frequency value\ndef calculate_tfidf(word, sentence):\n    return term_frequency(word, sentence) * inverse_document_frequency(word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get one-hot encoded vectors for the targets\ndef one_hot_class_vector(uniqueclasses, w):\n    emptyvector = [0 for i in range(len(uniqueclasses))]\n    emptyvector[uniqueclasses.index(w)] = 1\n    return emptyvector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get one-hot encoded vectors for the words\ndef one_hot_vector(w):\n    emptyvector = [0 for i in range(len(vocabulary))]\n    emptyvector[vocabulary.index(w)] = 1\n    return emptyvector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get one-hot encdoded sentence vector\ndef sentence_vector(sentence, tfidf=False):\n    tokenizedlist = word_tokenize(sentence.lower())\n    sentencevector = [0 for i in range(len(vocabulary))]\n    count = 0\n\n    for word in tokenizedlist:\n        if word in vocabulary:\n            count = count + 1\n            if tfidf:\n                sentencevector = [x + y for x, y in zip(sentencevector, [e * calculate_tfidf(word, sentence) for e in one_hot_vector(word)])] \n            else:\n                sentencevector = [x + y for x, y in zip(sentencevector, one_hot_vector(word))]\n\n    if count == 0:\n        return sentencevector\n    else:\n        return [(el / count) for el in sentencevector]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's construct the sentence vectors now, these are needed to start training on."},{"metadata":{"trusted":true},"cell_type":"code","source":"# wordvectors = [one_hot_vector(w) for w in vocabulary] # not needed\ncategoryvectors = [cv.index(1) for cv in [one_hot_class_vector(uniquecategories, w) for w in categories]]\nsubcategoryvectors = [cv.index(1) for cv in [one_hot_class_vector(uniquesubcategories, w) for w in subcategories]]\nactionvectors = [cv.index(1) for cv in [one_hot_class_vector(uniqueactions, w) for w in actions]]\nsentencevectors = [sentence_vector(sentence) for sentence in sentences]\nsentencevectorstfidf = [sentence_vector(sentence, True) for sentence in sentences]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(sentencevectors, categoryvectors, test_size=0.25, random_state=42)\nX_train_cat_tfidf, X_test_cat_tfidf, y_train_cat_tfidf, y_test_cat_tfidf = train_test_split(sentencevectorstfidf, categoryvectors, test_size=0.25, random_state=42)\nX_train_subcat, X_test_subcat, y_train_subcat, y_test_subcat = train_test_split(sentencevectors, subcategoryvectors, test_size=0.25, random_state=42)\nX_train_action, X_test_action, y_train_action, y_test_action = train_test_split(sentencevectors, actionvectors, test_size=0.25, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"training_base_model\"></a>\n# Training Base Model"},{"metadata":{},"cell_type":"markdown","source":"Training a Random Foreset baseline models to start from."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom catboost import CatBoostClassifier\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nimport xgboost as xgb\nfrom tpot import TPOTClassifier\nfrom sklearn.metrics import accuracy_score\nfrom numpy import random\n\nrandom.seed(2020)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fit(model_name, model, X, y, X_test, y_test):\n    model.fit(X, y)\n    y_preds = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_preds)\n    print(f\"{model_name}: {accuracy}\")\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest_model = RandomForestClassifier()\nrandom_forest_model = train_fit(\"RandomForestClassifier\", random_forest_model, X_train_cat, y_train_cat, X_test_cat, y_test_cat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"improved_models\"></a>\n# Improved Models"},{"metadata":{},"cell_type":"markdown","source":"Starting with RandomForestClassifier, SVC (linear + rbf kernel), XGBClassifier, MLPClassifier, CatBoostClassifier and TPOT. Time to do some improvements and explain why we see what we see."},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_model_linear = SVC(kernel='linear', decision_function_shape='ovo')\nsvc_model_linear = train_fit(\"SVC (linear)\", svc_model_linear, X_train_cat, y_train_cat, X_test_cat, y_test_cat)\n\nsvc_model_rbf = SVC(kernel='rbf', decision_function_shape='ovo')\nsvc_model_rbf = train_fit(\"SVC (rbf)\", svc_model_rbf, X_train_cat, y_train_cat, X_test_cat, y_test_cat)\n\nxgb_model = xgb.XGBClassifier()\nxgb_model = train_fit(\"XGBClassifier\", xgb_model, np.array(X_train_cat), np.array(y_train_cat), X_test_cat, y_test_cat)\n\ncatboost_model = CatBoostClassifier(verbose=False)\ncatboost_model = train_fit(\"CatBoostClassifier\", catboost_model, X_train_cat, y_train_cat, X_test_cat, y_test_cat)\n\nmlp_model = MLPClassifier()\nmlp_model = train_fit(\"MLPClassifier\", mlp_model, X_train_cat, y_train_cat, X_test_cat, y_test_cat)\n\nlgbm_model = lgb.LGBMClassifier()\nlgbm_model = train_fit(\"LGBMClassifier\", lgbm_model, X_train_cat, y_train_cat, X_test_cat, y_test_cat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"further_improvements\"></a>\n# Further Improvements"},{"metadata":{},"cell_type":"markdown","source":"## Linear kernel vs RBF kernel SVM"},{"metadata":{},"cell_type":"markdown","source":"The support vector classifier didn't score so well, but what if we tweak the parameters, such as `cost`. With a higher value it even outperforms the radial basis function kernel."},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_linear_cost_model = SVC(kernel='linear', decision_function_shape='ovo', C=52)\nsvc_linear_cost_model = train_fit(\"SVC (linear) + cost\", svc_linear_cost_model, X_train_cat, y_train_cat, X_test_cat, y_test_cat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The positive sides of a linear function kernel outweigh the positive sides of a radial basis function kernel mainly based on the argument of computational intensity. A linear kernel is faster to train than a radial basis function kernel.The linear function kernel also has a parameter less to tune which is the gamma parameter. Text classification is a problem that can be handled in a linearly separable way because of the already high dimensional space in which text resides, it doesnâ€™t really help the performance to transform the data into even higher dimensional space. "},{"metadata":{},"cell_type":"markdown","source":"## Tweaking MLPClassifier"},{"metadata":{},"cell_type":"markdown","source":"The MLPClassifier gave a convergence warning: `ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.`. Let's see what happens when we increase the max iterations."},{"metadata":{"trusted":true},"cell_type":"code","source":"mlp_max_iter_model = MLPClassifier(max_iter=10000)\nmlp_max_iter_model = train_fit(\"MLPClassifier\", mlp_max_iter_model, X_train_cat, y_train_cat, X_test_cat, y_test_cat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## AutoML with TPOT"},{"metadata":{},"cell_type":"markdown","source":"Last but not least, let's try TPOT (https://github.com/epistasislab/tpot/). It should find the most optimal solution. To avoid really long training times, we're setting the generations, population_size and cross-validation to 5."},{"metadata":{"trusted":true},"cell_type":"code","source":"tpot_model = TPOTClassifier(generations=5, population_size=5, cv=5, verbosity=3)\ntpot_model = train_fit(\"TPOTClassifier\", tpot_model, np.array(X_train_cat), np.array(y_train_cat), X_test_cat, y_test_cat)\ntpot_model.export(\"tpot_pipeline.py\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! cat tpot_pipeline.py","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Averaged vs TFIDF+averaged"},{"metadata":{},"cell_type":"markdown","source":"The MLPClassifier got the highest accuracy in the most test runs and was one of the fastest to train. This was with the averaged sentence approach, let's see how it does with the TFIDF approach.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"mlp_max_iter_model_tfidf = MLPClassifier(max_iter=10000)\nmlp_max_iter_model_tfidf = train_fit(\"MLPClassifier\", mlp_max_iter_model_tfidf, X_train_cat_tfidf, y_train_cat_tfidf, X_test_cat_tfidf, y_test_cat_tfidf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Results did not approve with TFIDF, reason for this is the length of the 'documents' and in our case, a short sentence. TFIDF approach is less useful in small length text because chances of multiple occurences of words is slim. It hurts more in this case than it helps."},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"testing\"></a>\n# Testing"},{"metadata":{},"cell_type":"markdown","source":"The multi-layer perceptron classifier has the best score with a high max_iter value. Let's use this for the predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"mlp_max_iter_model_cat = MLPClassifier(max_iter=10000)\nmlp_max_iter_model_cat = train_fit(\"MLPClassifier\", mlp_max_iter_model_cat, X_train_cat, y_train_cat, X_test_cat, y_test_cat)\nmlp_max_iter_model_subcat = MLPClassifier(max_iter=10000)\nmlp_max_iter_model_subcat = train_fit(\"MLPClassifier\", mlp_max_iter_model_subcat, X_train_subcat, y_train_subcat, X_test_subcat, y_test_subcat)\nmlp_max_iter_model_action = MLPClassifier(max_iter=10000)\nmlp_max_iter_model_action = train_fit(\"MLPClassifier\", mlp_max_iter_model_action, X_train_action, y_train_action, X_test_action, y_test_action)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(model, classes, sentence):\n    y_preds = model.predict([sentence_vector(sentence)])\n    return classes[y_preds[0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence = \"Hi Google, please turn off the lights.\"\nprint(predict(mlp_max_iter_model, uniquecategories, sentence))\nprint(predict(mlp_max_iter_model_subcat, uniquesubcategories, sentence))\nprint(predict(mlp_max_iter_model_action, uniqueactions, sentence))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence = \"Turn the lights off in the kitchen.\"\nprint(predict(mlp_max_iter_model, uniquecategories, sentence))\nprint(predict(mlp_max_iter_model_subcat, uniquesubcategories, sentence))\nprint(predict(mlp_max_iter_model_action, uniqueactions, sentence))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence = \"Random sentence.\"\nprint(predict(mlp_max_iter_model, uniquecategories, sentence))\nprint(predict(mlp_max_iter_model_subcat, uniquesubcategories, sentence))\nprint(predict(mlp_max_iter_model_action, uniqueactions, sentence))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence = \"Find the furthest bus stop.\"\nprint(predict(mlp_max_iter_model, uniquecategories, sentence))\nprint(predict(mlp_max_iter_model_subcat, uniquesubcategories, sentence))\nprint(predict(mlp_max_iter_model_action, uniqueactions, sentence))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence = \"Lower the door.\"\nprint(predict(mlp_max_iter_model, uniquecategories, sentence))\nprint(predict(mlp_max_iter_model_subcat, uniquesubcategories, sentence))\nprint(predict(mlp_max_iter_model_action, uniqueactions, sentence))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}