{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h2> Which populations are at risk of contracting COVID-19? Part 2 </h2>\n<h3> dependencies between countries in Europe </h3>\n\n<h3>Data Analyzer</h3>\n\nThe script collects and analyze the data preprocessed by the data creator \"Which populations are at risk ? part 1\" (https://www.kaggle.com/user1001/which-populations-are-at-risk-part-1). \n\n "},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"%matplotlib inline\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt \nimport numpy as np \nimport os \nimport pandas as pd \nfrom pandas.api.types import is_string_dtype\nfrom pandas.api.types import is_numeric_dtype\nimport re\nfrom sklearn.preprocessing import OneHotEncoder\nfrom scipy import spatial\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport sklearn\nimport community\nimport networkx as nx\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> functions:</h3> "},{"metadata":{"trusted":true},"cell_type":"code","source":"# functions: \n\n# check duplicated cols:\ndef GetDuplicatedColumns(df):\n    '''\n    Get a list of duplicate columns.\n    It will iterate over all the columns in dataframe and find the columns whose contents are duplicate.\n    :param df: Dataframe object\n    :return: List of columns whose contents are duplicates.\n    '''\n    duplicateColumnNames = set()\n    # Iterate over all the columns in dataframe\n    for x in range(df.shape[1]):\n        # Select column at xth index.\n        col = df.iloc[:, x]\n        # Iterate over all the columns in DataFrame from (x+1)th index till end\n        for y in range(x + 1, df.shape[1]):\n            # Select column at yth index.\n            otherCol = df.iloc[:, y]\n            # Check if two columns at x 7 y index are equal\n            if col.equals(otherCol):\n                duplicateColumnNames.add(df.columns.values[y])\n \n    return list(duplicateColumnNames)\n\n# select only numerical cols:\ndef get_num_cols(df):\n    myCols = ['Country']\n    for item_ in df.columns.tolist():\n        if df[item_].dtype == 'float64':\n            myCols.append(item_) \n    return myCols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Read files prepared by the data creator \"data_preparation_part_1.ipynb\". \nAll files are stored in directory defined by the \"myDir\" variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"# read files:\nmyDir = '../input/preprocessed-data-stat/' \ndf = pd.DataFrame()\nfor dirname, _, filenames in os.walk(myDir):\n    for filename in filenames:\n        myFile = os.path.join(dirname, filename)\n        tmp_ = pd.read_csv(myFile)\n        df = df.append(tmp_)\n        #print(myFile, tmp_.shape, df.shape)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>  Check the data quality:</h3> \ncount NaN values, replace them by template values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# count nans:\n#print ('1:\\n',df.isna().sum())\n\n# replacements for Nan values:\nobj_replacement = 'Not Existing'\nfloat_replacement = -1.\nint_replacement = -10","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"replace NaN values by the replacements:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for item_ in df.columns.tolist():\n    type_ = df[item_].dtype\n    #print (item_, type_)\n    if type_ == 'object':\n        df[item_].fillna(value=obj_replacement, inplace=True)\n    if type_ == 'float64':\n        df[item_].fillna(value=float_replacement, inplace=True)\n    if type_ == 'int64':\n        df[item_].fillna(value=int_replacement, inplace=True)\n        \n## count nans:\n#print ('Check NaN values after replacement:\\n',df.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lot of columns are very uniform. \nWe will remove all columns which contain less than 3 different values. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# select cols to delete:\ncols_to_remove = []\nfor item_ in df.columns.tolist():\n    #print ('col = ',item_)\n    #print (df[item_].unique())\n    if len(df[item_].unique()) < 3:\n        cols_to_remove.append(item_)\n    #print ('==========')\n    \nprint ('Nr of columns to remove: ',len(cols_to_remove))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"removal of columns with too uniform range of values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ = df.copy()\ndf_.drop(cols_to_remove, axis=1, inplace=True)\nprint ('Shape of resulting Data Frame: ',df_.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next stage of the data cleaning: removal of duplicated columns:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_remove_ = GetDuplicatedColumns(df_)\n# drop duplicated cols:\nprint ('Nr of columns to be removed: ',len(cols_to_remove_))\ndf__ = df_.drop(columns=cols_to_remove_).copy()\nprint ('Shape of the Data Frame after removal of the duplicated coilumns: ',df__.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> The present analysis is limited only to the numerical columns. </h3> \nTherefore, we remove all not non-numerical columns, keeping only the column \"Country\" as an ID of the population"},{"metadata":{},"cell_type":"markdown","source":"Before that step we have to move the column \"Country\" to the first position:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# take into account only numerical cols:\ncols = df__.columns.tolist()\ncols.insert(0, cols.pop(cols.index('Country')))\ndf__ = df__.reindex(columns= cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Selecting only numerical columns:"},{"metadata":{"trusted":true},"cell_type":"code","source":"myCols = get_num_cols(df__)\ndf_num = df__[myCols].copy()\nprint ('The shape of resulting Data Frame (with numercial columns)', df_num.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next step in data preparation is based on a manual analysis of the available columns.\nThe purpose of this procedure is to separate the statistical data collected in the available data from files available in the directory \"world_data\" from the data specified for the period of pandemic development available from all other analyzed data (i.e. in other directories than \"world_data\")."},{"metadata":{},"cell_type":"markdown","source":"### 1. List of available Countries"},{"metadata":{"trusted":true},"cell_type":"code","source":"myCountries = df_num.Country.values\nprint ('Available Countries: ',myCountries)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-  2.Selection of columns related to the geographical positions (which have to be treated as a categorical one):\nthose columns will be removed from the data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"additional_to_delete = ['hq_zip_cod','lat','latitude_d','long','longitude']\ndf_num.drop(additional_to_delete, axis=1, inplace=True)\nprint ('Shape of the current Data Frame: ',df_num.shape)\nmyCols_num = df_num.columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- 3.Selection of columns related directly to measures of the virus infection:\nAs a set of columns defining an action agains the virus infections. "},{"metadata":{"trusted":true},"cell_type":"code","source":"dic_part2 = {'AUC__cases':4068, 'AUC__deaths':4069, 'AUC__ratio_death_to_cases':4070, \n 'active':4071, 'active_cases':4072, 'bed_utiliz':4073, 'cnty_fips':4074, 'data_value':4075, 'elevation':4076,\n 'fips':4077, 'high_confidence_limit':4078, 'id':4079, \n 'inform_epidemic_lack_of_coping_capacity':4080, 'inform_epidemic_vulnerability':4081,\n 'inform_lack_of_coping_capacity':4082, 'inform_p2p_hazard_and_exposure_dimension':4083, \n 'inform_risk':4084, 'inform_vulnerability':4085, 'locationid':4086, 'low_confidence_limit':4087, \n 'new_deaths':4088, 'num_licens':4089, 'num_staffe':4090, 'objectid':4091, \n 'people_using_at_least_basic_sanitation_services':4092,'percent_yoy_change':4093,'popdata2018':4094, \n 'population_density':4095,'population_living_in_urban_areas':4096,'potential':4097,\n 'prevalence_of_undernourishment':4098, 'recovered':4099,'sample_size':4100, \n 'serious_critical_cases':4101, 'state_fips':4102, 'total_cases':4103, 'total_cases_per_1m_pop':4104, \n 'total_confirmed_cases_of_covid_19_cases':4105, \n 'total_confirmed_cases_of_covid_19_per_million_people_cases_per_million':4106, \n 'total_covid_19_tests':4107, 'total_covid_19_tests_per_million_people':4108, \n 'total_deaths':4109, 'total_deaths_per_1m_pop':4110, 'total_recovered':4111}\nprint (list(dic_part2.keys()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- 4.Final selection of statistical and action parts of columns:\n    - part 1: columns[:4068] - statistical description of countries,\n    - part 2: columns[4068:] - action and results of actions related to the virus illness. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# save \nfile_ = './df_num__data_analyzer_v4_global.csv'\ndf_num.to_csv(file_,index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> Graph analysis of the data:<h2> "},{"metadata":{},"cell_type":"markdown","source":"### Define product records"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define product records\nSelected_countries = myCountries \nproducts = df_num.iloc[0:,0:].values\nprint ('Shape of products: ',products[:,:].shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Set of common functions: "},{"metadata":{"trusted":true},"cell_type":"code","source":"keys = list(dic_part2.keys())\n\ndef GetDictionary(keys, attrs):\n    d = dict((key, value) for (key, value) in zip(keys, attrs))\n    return d\n\n#\ndef get_cos_dist(a,b):\n    # cos dist:\n    offset = 1. # 1. is added in order to avoid negative values !!\n    myDist = offset + ((1. - spatial.distance.cosine(a, b)))\n    return (myDist)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> Set of functions used for creation of the graphs. </h3>\n\nOur goal is to create a graph showing similarities between different countries (populations) based on statistical data mapped by data from the \"world_data\" catalog and a set of measurements directly related to virus infection. \n\nBasic functions used for creation of the graphs are similar to those presented on the blog presented in https://jobs.zalando.com/en/tech/blog/exploring-fashion-catalog/ .\nDescription the entire graphs is the following: our graphs consists of countries (Nodes = [df_num.Country.values]) and connections between nodes that represent a set of measures based on the Cos distance between statistical description of a node (columns of the part 1) and Cos distance calculated for the virus's actions (defined by columns of the part 2). \n\nEach node (Country) has a record of all product attributes for this product, Country_i = {(product[0]), product[1:4068], product[4068:]}, where:  \n\n1. product[0] is the country, \n2. product[1:4068] - describes statistical set of values (collected by the \"world_data\" entries, columns - part 1), and\n3. product[4068:] - is a vector of all measures related to the virus infection (columns, part 2).\n\nAll these data are used to generate navigation markers.  Description of the navigation markers:\n- preparation of the basic Cosine distances between county:\nis performed by the functions: product_similarity_matrix_part1 for the statistical description and by the  product_similarity_matrix_part2 for the virus actions. Both function return Cos distance between  both parts of data (part 1 and part 2 separately).\n\n- additional markers:\n    - diff_dist_part1 : Cos distance between Countries calculated for the statistical descriprion (part 1 only),\n    - diff_dist_part2 : Cos distance between Countries calculated for the virus action descriprion (part 2 only),\n    - diff_dist_ratio : The ratio between diff_dist_part1 and diff_dist_part2,\n    - diff_ratio_env_action_part1: ratio between euclidean norm of the part 1 data calculated for each country pair,\n    - diff_ratio_env_action_part2: ratio between euclidean norm of the part 2 data calculated for each country pair,\n    - cos_dist_eucl: the euclidean norm calculated for cos_dist_2 and cos_dist_1 $\\sqrt{cos\\_dist\\_1^2 + cos\\_dist\\_2^2}$,\n    - cos_dist_eta: factor calculated from the euclidean norm cos_dist_eucl.  The value cos_dist_eucl could be expresssed by: $\\sqrt{2}*cos\\_dist\\_1*eta$ with $eta = \\sqrt{1 + (delta/cos\\_dist\\_1) + (delta/cos\\_dist\\_1)^2}$ with \n    $delta = cos\\_dist\\_2 - cos\\_dist\\_1$. This eta value is considered as a measure of a difference between countires.\n\n\n\nThe algorithm cycle is:- The single node (country) on the graph is selected as the anchor node, country_i.\n- A set of all connected nodes to country_i, country_rest = {country_1, ..., country_N} is created.\n- A mapping of the differences in attributes, metrices and measures between the anchor node, country_i, and the attributes, metrices and measures of the products contained in the set of connected nodes is constructed.\n- The set of navigation markers, Markers = {diff_dist_part1, diff_dist_part2, diff_dist_ratio, diff_ratio_env_action_part1, diff_ratio_env_action_part2, cos_dist_eucl, cos_dist_eta} for the anchor: country_i is calculated.\n- The algorithm returns to step 1 and the process repeats.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Returns a Product Similarity Matrix for specified products &\n# similarity score, sim_score.\n# Part 1 - related to the statistical description of the countries\ndef product_similarity_matrix_part1(products, sim_score=get_cos_dist):\n    prod_sim_mat = {}\n    # n(n-1)/2 scores\n    for i, product_i in enumerate(products[:]):\n        for product_j in products[:i]:\n            idi, attr_i =  (product_i[0]), [float(i) for i in product_i[1:4068]] \n            idj, attr_j =  (product_j[0]), [float(i) for i in product_j[1:4068]] \n            prod_sim_mat[(idi, idj)] = sim_score(attr_i, attr_j)\n            \n    return prod_sim_mat\n\n# Returns a Product Similarity Matrix for specified products \n# similarity score, sim_score.\n# Part 2 - related to the virus measures per countries\ndef product_similarity_matrix_part2(products, sim_score=get_cos_dist): \n    prod_sim_mat = {}\n    # n(n-1)/2 scores\n    for i, product_i in enumerate(products[:]):\n        for product_j in products[:i]:\n            idi, attr_i =  (product_i[0]), [float(i) for i in product_i[4068:]] \n            idj, attr_j =  (product_j[0]), [float(i) for i in product_j[4068:]] \n            prod_sim_mat[(idi, idj)] = sim_score(attr_i, attr_j)\n            \n    return prod_sim_mat\n\n# Combine Product Similarity Matrix and products to construct a \n# Product Similarity Graph.\ndef product_similarity_graph(prod_sim_mat_1, prod_sim_mat_2, products):\n    # Create networkx graph\n    PSG = nx.Graph() #nx.DiGraph()\n\n    # Add nodes and attrs to graph\n    for product in products:\n        id_, attrs1, attrs2 = (product[0]), product[1:4068], product[4068:]\n        attrs_dict = GetDictionary(keys, attrs2)\n        PSG.add_node(id_, attrs1=attrs1, attrs2=attrs2, attr3=attrs_dict) \n        \n    # Add edges and scores to nodes\n    for ind, score1 in prod_sim_mat_1.items():\n        if score1 > 0:\n            start, end = ind\n            PSG.add_edge(start, end, score1=score1)\n            PSG.add_edge(end, start, score1=score1)\n            \n    # Add edges and scores to nodes\n    for ind, score2 in prod_sim_mat_2.items():\n        if score2 > 0:\n            start, end = ind\n            PSG.add_edge(start, end, score2=score2)\n            PSG.add_edge(end, start, score2=score2)\n            \n    return PSG\n\n# Generate a set of attribute differences between each pair of connected \n# nodes in prod_sim_graph and add to edges.\ndef generate_diff_attrs(prod_sim_graph):\n    for anchor, neighbour in prod_sim_graph.edges():\n        \n        anchor_attrs1, anchor_attrs2 = prod_sim_graph.nodes[anchor]['attrs1'],prod_sim_graph.nodes[anchor]['attrs2']\n        neighbour_attrs1, neighbour_attrs2 = prod_sim_graph.nodes[neighbour]['attrs1'],prod_sim_graph.nodes[neighbour]['attrs2']\n        \n        # cos dist:\n        offset = 1.\n        cos_dist_1 = offset + (1. - spatial.distance.cosine([float(i) for i in anchor_attrs1], \n                                                   [float(i) for i in neighbour_attrs1]))\n        cos_dist_2 = offset + (1. - spatial.distance.cosine([float(i) for i in anchor_attrs2], \n                                                   [float(i) for i in neighbour_attrs2]))\n        #cos_dist_euclidean = np.sqrt(cos_dist_1**(2.) + cos_dist_2**(2.))\n        delta = cos_dist_2 - cos_dist_1\n        eta = np.sqrt(1. + (delta/cos_dist_1) + (delta/cos_dist_1)**(2.))\n        cos_dist_euclidean = np.sqrt(2.)*cos_dist_1*eta\n        # we will calculate provide cos_dist_euclidean & eta \n        dist_ratio = (offset + cos_dist_1)/(offset + cos_dist_2)\n        dist_anchor_1 = np.linalg.norm([float(i) for i in anchor_attrs1])\n        dist_anchor_2 = np.linalg.norm([float(i) for i in anchor_attrs2])\n        dist_neighbour_1 = np.linalg.norm(np.array([float(i) for i in neighbour_attrs1]))\n        dist_neighbour_2 = np.linalg.norm(np.array([float(i) for i in neighbour_attrs2]))\n        dist_env_1 = dist_anchor_1/dist_neighbour_1 # ratio\n        dist_action_2 = dist_anchor_2/dist_neighbour_2 # ratio\n        \n        prod_sim_graph.edges[anchor,neighbour]['diff_tags_1'] = cos_dist_1\n        prod_sim_graph.edges[anchor,neighbour]['diff_tags_2'] = cos_dist_2\n        prod_sim_graph.edges[anchor,neighbour]['diff_dist_ratio'] = dist_ratio\n        prod_sim_graph.edges[anchor,neighbour]['diff_ratio_env_action_part1'] = dist_env_1\n        prod_sim_graph.edges[anchor,neighbour]['diff_ratio_env_action_part2'] = dist_action_2\n        prod_sim_graph.edges[anchor,neighbour]['cos_dist_eucl'] = cos_dist_euclidean\n        prod_sim_graph.edges[anchor,neighbour]['cos_dist_eta'] = eta\n            \n    return prod_sim_graph\n\n# Generate a navigation tag map for each node of the prod_sim_graph.\ndef generate_nav_tags(prod_sim_graph):\n    \n    for anchor in prod_sim_graph.nodes():\n        \n        tag_map_1 = {}\n        tag_map_2 = {}\n        tag_map_3 = {}\n        tag_map_4 = {}\n        tag_map_5 = {}\n        tag_map_6 = {}\n        tag_map_7 = {}\n        for neighbour in prod_sim_graph.neighbors(anchor):\n        \n            tag1 = prod_sim_graph[anchor][neighbour]['diff_tags_1']\n            tag2 = prod_sim_graph[anchor][neighbour]['diff_tags_2']\n            tag3 = prod_sim_graph[anchor][neighbour]['diff_dist_ratio']\n            tag4 = prod_sim_graph[anchor][neighbour]['diff_ratio_env_action_part1']\n            tag5 = prod_sim_graph[anchor][neighbour]['diff_ratio_env_action_part2']\n            tag6 = prod_sim_graph[anchor][neighbour]['cos_dist_eucl']\n            tag7 = prod_sim_graph[anchor][neighbour]['cos_dist_eta']\n            \n            tag_map_1[tag1] = tag_map_1.get(tag1, []) + [neighbour]\n            tag_map_2[tag2] = tag_map_2.get(tag2, []) + [neighbour]\n            tag_map_3[tag3] = tag_map_3.get(tag3, []) + [neighbour]\n            tag_map_4[tag4] = tag_map_4.get(tag4, []) + [neighbour]\n            tag_map_5[tag5] = tag_map_5.get(tag5, []) + [neighbour]\n            tag_map_6[tag6] = tag_map_6.get(tag6, []) + [neighbour]\n            tag_map_7[tag7] = tag_map_7.get(tag7, []) + [neighbour]\n                \n        prod_sim_graph.nodes[anchor]['nav_tags_1'] = tag_map_1\n        prod_sim_graph.nodes[anchor]['nav_tags_2'] = tag_map_2\n        prod_sim_graph.nodes[anchor]['nav_dist_ratio'] = tag_map_3\n        prod_sim_graph.nodes[anchor]['nav_ratio_env_action_part1'] = tag_map_4\n        prod_sim_graph.nodes[anchor]['nav_ratio_env_action_part2'] = tag_map_5\n        prod_sim_graph.nodes[anchor]['nav_cos_dist_eucl'] = tag_map_6\n        prod_sim_graph.nodes[anchor]['nav_cos_dist_eta'] = tag_map_7\n        \n    return prod_sim_graph           \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Calculation of the graphs</h3>"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"if True:\n    \n    # Construct Product Similarity Matrix\n    prod_sim_mat_1 = product_similarity_matrix_part1(products)\n    prod_sim_mat_2 = product_similarity_matrix_part2(products)\n    \n    # Construct Product Similarity Graph\n    PSG = product_similarity_graph(prod_sim_mat_1, prod_sim_mat_2, products)\n    \n    # Generate difference attributes and attach to edges\n    PSG = generate_diff_attrs(PSG)\n    \n    # Generate navigation tags and attach to nodes\n    PSG = generate_nav_tags(PSG)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Example of connections</h3>\n \nIn this case, we analyse the disparities between the two countries expressed in the cos_dist_eta measurement. \n\nLet us assume that the difference describing the general state development between the two countries is A \n(determined by the Cos distance). \nAnd the Cos distance calculated for the part related to the response to the virus will be B.\n  \nNow, we could consider 3 cases:\n- If both countries react to virus-related situations according to their respective capabilities, \nwe should expect that $B=A$, which corresponds to $eta = 1$ .\n- If one of the countries shows a better performance, then $eta < 1$ .\n- When the fight against the virus is 2x worse than possible, we expect that $eta = \\sqrt{3}$ .\nA completely inefficient situation is expressed by $eta > \\sqrt{3}$ .\n\nOther examples could be checked using another available metrics:\n'diff_tags_1','diff_tags_2','_ratio_env_action_part1','diff_ratio_env_action_part2','diff_dist_ratio','cos_dist_eucl','cos_dist_eta'.\nIt is enought to change the parameter value param_ in the cell 'parameters' below.\n\nIn the example below, we used the following colours (and widths) for lines between countires:\n- if val_ < 1.  : color = Green, width = 10\n- if val_ == 1. : color = Black, width = 5\n- if (val_ > 1.) & (val_ <= np.sqrt(3.)): color = Blue, width = 2\n- if (val_ > np.sqrt(3.)): color = Red, width = 5\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# parameters:\n# Draw the resulting graph\nparam_ = 'cos_dist_eta'\n# all possibilities for variable 'param_: 'diff_tags_1', 'diff_tags_2','_ratio_env_action_part1','diff_ratio_env_action_part2',\n# 'diff_dist_ratio','cos_dist_eucl','cos_dist_eta'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if True:\n    # Setup simulated user inputs\n    anchor = myCountries[1] # random choice\n    tag_selections = list(dic_part2.keys()) \n    product_selections = myCountries[2] \n    \n    # Run through simulated inputs    \n    for selection in tag_selections:\n        try:\n            products = PSG.node[anchor]['attr3'][selection]\n            if len(products)>1:\n                product_selection = product_selections.pop(0)\n                assert product_selection in products, \"Bad selection\"\n                anchor = product_selection\n            else:\n                anchor, = products\n        except:\n            pass\n        \n\n    edge_labels=dict([((u,v,),'{:0.4f}'.format(d[param_]))\n                 for u,v,d in PSG.edges(data=True)])\n\n    edge_colors = []\n    edge_widths = []\n    for edge in edge_labels:\n        val_ = np.float(edge_labels.get(edge))\n        \n        if val_ < 1.:\n            edge_colors.append('green')\n            edge_widths.append(10)\n        if val_ == 1.:\n            edge_colors.append('black')\n            edge_widths.append(5)\n        if (val_ > 1.) & (val_ <= np.sqrt(3.)):\n            edge_colors.append('blue')\n            edge_widths.append(2)\n        if (val_ > np.sqrt(3.)):\n            edge_colors.append('red')\n            edge_widths.append(5)\n\n    pos=nx.spring_layout(PSG, k=0.02, weight=param_, seed=2)\n    #pos=nx.spectral_layout(PSG, weight=param_, scale=0.1)\n    \n    val_map = { myCountries[i] : 123 for i in range(len(myCountries)) } # the same color of node\n    values = [val_map.get(node, 0.) for node in PSG.nodes()]\n    \n    plt.figure(figsize=(20,20))\n    #nx.draw_networkx_edge_labels(PSG,pos,alpha=0.8, edge_labels=edge_labels)\n    nx.draw(PSG,pos,with_labels=True, node_color = values, node_size=3500, font_size=28, \n            edge_color=edge_colors, width=edge_widths, edge_cmap=plt.cm.Reds, alpha=0.6)\n    plt.show()\n\n    print(\"\\n\\tDone !.\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Numerical values of the coefficient eta calculated for different pairs of countries:</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# it provides a long list, it disturbs the evaluation of the code\n#for edge in edge_labels:\n#    print (edge)\n#    print (np.float(edge_labels.get(edge)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Clustering:</h2>"},{"metadata":{},"cell_type":"markdown","source":"<h3>1 Clustering: spectral clustering:</h3>\n\nThe quality of the clustering is determined by the silhouette score"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# spectral clustering:\nfrom sklearn.cluster import SpectralClustering\nfrom sklearn import metrics\nnp.random.seed(1)\n\n# Get adjacency-matrix as numpy-array\nweight_ = param_ \nadj_mat = nx.adjacency_matrix(PSG, nodelist=list(PSG.nodes()), weight=weight_).todense()\nnode_list = list(PSG.nodes())\n\nIndexArray = []\nNrOfElem = []\nmyXList = range(2,15,1)\naffinity_ = 'precomputed'\nn = 5\nfor NrOfClusters in myXList:\n    #print (NrOfClusters)\n    sc = SpectralClustering(NrOfClusters, affinity= affinity_, n_init=100, assign_labels='discretize',random_state=0)\n    clusters = sc.fit_predict(adj_mat)\n    labels = sc.labels_    \n    # plot each nth result:\n    if NrOfClusters % n == 0:\n        plt.scatter(node_list,clusters,c=clusters, s=50, cmap='viridis')\n        plt.xticks(node_list,node_list, rotation='vertical')\n        plt.show()\n    # Compare ground-truth and clustering-results\n    print('spectral clustering')\n    print ('nr of clusters: ', np.unique(labels))\n    \n    indexValue = metrics.silhouette_score(adj_mat, labels)\n    IndexArray.append(indexValue) \n    NrOfElem.append(len(labels[labels == np.unique(labels)[-1]]))\n    print ('nr of elements in the last class:',len(labels[labels == np.unique(labels)[-1]]),indexValue)\n    \nplt.figure(figsize=(12,12))\nplt.title('silhouette_score')\nplt.plot(myXList,IndexArray,'o')\nplt.xlabel('nr of clusters')\nplt.ylabel('silhouette_score')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>single run for the best cluster</h3> "},{"metadata":{"trusted":true},"cell_type":"code","source":"# single run for the best cluster\n# Spectral Clustering\nclusters_ = np.argmax(IndexArray) + 2\nclusters = SpectralClustering(affinity = affinity_, assign_labels=\"discretize\",\n                              random_state=0,n_clusters=clusters_).fit_predict(adj_mat)\nplt.figure(figsize=(10,10))\nplt.scatter(node_list,clusters,c=clusters, s=50, cmap='viridis')\nplt.xticks(node_list,node_list, rotation='vertical')\nplt.xlabel('Country')\nplt.ylabel('Cluster')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>2 Clustering: AgglomerativeClustering:</h3>\n\nThe quality of the clustering is determined by the silhouette score"},{"metadata":{"trusted":true},"cell_type":"code","source":"# AgglomerativeClustering\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Get adjacency-matrix as numpy-array\nweight_ = param_ \nadj_mat = nx.adjacency_matrix(PSG, nodelist=list(PSG.nodes()), weight=weight_).todense()\nnode_list = list(PSG.nodes())\n\nIndexArray = []\nNrOfElem = []\nmyXList = range(2,15,1)\naffinity_ = 'precomputed' \nn = 5\nfor NrOfClusters in myXList:\n    print (NrOfClusters)\n    sc = AgglomerativeClustering(n_clusters=NrOfClusters,linkage='average',affinity=affinity_)\n    clusters = sc.fit_predict(adj_mat)\n    labels = sc.labels_    \n    # plot each nth result:\n    if NrOfClusters % n == 0:\n        plt.scatter(node_list,clusters,c=clusters, s=50, cmap='viridis')\n        plt.xticks(node_list,node_list, rotation='vertical')\n        plt.show()\n    # Compare ground-truth and clustering-results\n    print('spectral clustering')\n    print ('nr of clusters: ', np.unique(labels))\n    indexValue = metrics.silhouette_score(adj_mat, labels)\n    IndexArray.append(indexValue) \n    NrOfElem.append(len(labels[labels == np.unique(labels)[-1]]))\n    print ('nr of elements in the last class:',len(labels[labels == np.unique(labels)[-1]]),indexValue)\n    \nplt.figure(figsize=(10,10))\nplt.title('silhouette_score')\nplt.plot(myXList,IndexArray,'o')\nplt.xlabel('nr of clusters')\nplt.ylabel('silhouette_score')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>single run for the best cluster (agglomerative Clustering)</h3> "},{"metadata":{"trusted":true},"cell_type":"code","source":"# single run\n# agglomerative Clustering \nclusters_ = np.argmax(IndexArray) + 2\naffinity_ = 'precomputed'\nclusters = SpectralClustering(affinity = affinity_, assign_labels=\"discretize\",\n                              random_state=0,n_clusters=clusters_).fit_predict(adj_mat)\nplt.figure(figsize=(10,10))\nplt.scatter(node_list,clusters,c=clusters, s=50, cmap='viridis')\nplt.xticks(node_list,node_list, rotation='vertical')\nplt.xlabel('Country')\nplt.ylabel('Cluster')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>3 Clustering: PageRank Algorithm </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# PageRank Algorithm\n# PageRank estimates a current nodeâ€™s importance from its linked neighbors and \n# then again from their respective neighbors.\n\nweight_ = param_ \nrank_list = nx.pagerank(PSG, weight=weight_, alpha=0.9)\nlists = sorted(rank_list.items()) \nx, y = zip(*lists) \n\nplt.plot(x, y,'o--')\nplt.xticks(x,x, rotation='vertical')\nplt.ylabel('rank_value')\nplt.xlabel('country')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>4 Clustering: Kernighan-Lin Paritioning</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Kernighan-Lin Paritioning\nweight_ = param_ \nparts = nx.community.kernighan_lin_bisection(PSG, weight=weight_, max_iter=100, seed=1234)\nprint ('parts=',parts)\nnode_colors_map = {}\nfor i, lg in enumerate(parts):\n    for node in lg:\n        node_colors_map[node] = i\nnode_colors = [node_colors_map[n] for n in PSG.nodes]\n\npos_=nx.spring_layout(PSG, weight=param_)\nnode_list = list(PSG.nodes())\nplt.figure(figsize=(12, 12))  \nplt.axis('off')\nnx.draw_networkx_nodes(PSG, pos=pos_, with_labels=False, node_size=600, node_color=node_colors)\nnx.draw_networkx_labels(PSG,pos_,font_size=16,font_color='r',alpha=0.9)\nnx.draw_networkx_edges(PSG, pos_, alpha=0.1)\nplt.show(PSG)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Final conclusions:</h2>\n    \nThe overall analysis of the data is very difficult because of:\n1. the data are extremely differentiated in both record format and meaning \n(the same column names are used in different sets from the different indications).\n2. Incomplete data.\n3. Difficulties in determining the correct measurements for different countries (differences in determining the infection rates).\n\nIn my opinion, the analysis I have presented above may be rather useful in determining countries \nthat deviate more from the average behaviour of other countries.\nSince the fight against the virus is based on the individual behaviour of each country individually, \nit should be possible to determine the best and worst tactics with more careful data preparation. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":4}