{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Capstone Project: BikeShare Toronto\n## Part 1: Data Clean & EDA\n---\n**Author:** Jason Ho<br>\n**Date:** July 1st, 2021","metadata":{}},{"cell_type":"markdown","source":"### Changelog\n\n#### Version 1.0\n**New**<br>\n    - Added datasets for November and December of 2020\n\n#### Version 1.1\n**New**<br>\n    - Added comments throughout notebook explaining thought process<br>\n    - Added Conclusions<br>\n**Changes**<br>\n    - Changed the introduction by including background information on data sets<br>\n    - Changed Business Case Question<br>\n    - Changed API requests for Nominatim code<br>\n**Fixes**<br>\n    - Removed station names relating to maintenance and repair<br>\n    - Removed feature cleaning for `physical_configuration`, `address`, `rental_methods`, `groups`, `obcn`, `nearby_distance`, `cross_street`<br>\n    - Removed unnecessary API requests<br>\n    - Removed Map Visualizations<br>\n    \n #### Version 1.2\n**New**<br>\n    - Added sections for data cleaning steps for easier reading<br>\n    - Added option to keep trip duration in seconds\n    - Added more comments throughtout notebook<br>\n**Changes**<br>\n    - Changed the layout of notebook and condensed sections so that the reader can follow along easier<br>\n    - Removed redundant code<br>\n**Fixes**<br>\n    - Fixed coding for imputing missing station ids in trips17 & missing station names in trip 20 dataset<br>\n    - Fixed issue regarding datetime datatype change in trips17 dataset<br>\n    - Fixed classifer for User Types<br>\n_____________________________________________","metadata":{}},{"cell_type":"markdown","source":"## Introduction\n\nThis Jupyter Notebook is Part 1 of my Capstone Project for the Data Science Program at BrainStation. I will be using the BikeShare Toronto data set and hourly weather data gathered from Environment Canada, between the dates of 2017-01-01 to 2020-12-31.\n\n**Background**<br>\nBikeShare Toronto is a bike-share program that features more than 6,850 bicycles and 625 docking stations across 200sqkm of the city. BikeShare Toronto provides a flexible and cost-effective way to navigate Toronto with their traditional bikes and e-bikes. Both options cost the same rate at \\$3.25 per trip and the service offers short-term pass options (Single Day (24 hours), or 3 Day (72 Hours)) as well as long-term memberships (Annual 30 (30min rides), or Annual 45 (45min rides)).\n\n**Objective**<br>\nThe objective of my Capstone Project is to answer the *Business Case Question* of:\n\n**<center>Accurately predict bike usage numbers based on past usage patterns</center>**\n\nThe ability to predict usage levels are beneficial to many industries in terms of strategic planning and logistics where you can plan and adjust inventory and distribution across the system accordingly so that you can maximize efficiency and revenue generation.\n\n**Purpose**<br>\nThe purpose of this notebook is to consolidate the downloaded BikeShare Toronto data and hourly weather data into a single dataframe, then conducting an exploratory data analysis, and produce some visualizations that we can gain some insights on **HOW** riders use the bikeshare service. Then we will export the cleaned data set to use in Part 2 to create a machine learning model to answer our *Business Case Question*.\n\n**Data Sources**<br>\nThe data is to be gathered is from the following sources:\n\n1. [City of Toronto: Open Data](https://open.toronto.ca/dataset/bike-share-toronto-ridership-data/): City of Toronto has an open data catalogue where I obtained Toronto BikeShare trip datasets from 2017 to 2020. This will be the main source of trip data that will be analyzed.\n\n2. [CKAN](https://ckan0.cf.opendata.inter.prod-toronto.ca/en/dataset/bike-share-toronto): CKAN is an open-source data portal platform where I obtained the links to the Toronto BikeShare API to request up-to-date data regarding BikeShare station information. I will be querying the API for any more useful information that can help with my objective.\n\n3. [Government of Canada](https://climate.weather.gc.ca/historical_data/search_historic_data_e.html): Government of Canada has a catalogue of historical weather data throughout various weather stations across Canada. Weather affects bike usage, where incliment weather causes people to ride bikes less.\n\nAside from the data I am gathering above, I will also add in calendar information such as weekends and holidays as it may have an affect the number of riders will use the bike share service.\n\n**Next Steps**<br>\nAfter loading in the data sets, I will be checking the data and then cleaning it by imputation and/or removing any null values then combining the datasets together as my main working dataset.\n\nIf you wish to follow along, you will need to create a new conda environment by using the package list included in the zip folder by running the script below.\n\n```bash\nconda create -n jho_capstone_env --file jho_capstone_env.txt\n```\nYou may change jho_capstone_env to any name you prefer.","metadata":{}},{"cell_type":"code","source":"# Import packages\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Timer\nfrom tqdm import tqdm \nimport time\n\n# global settings\npd.set_option('display.max_columns', None) #Set max columns view to unlimited","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:21:39.642107Z","iopub.execute_input":"2021-07-01T22:21:39.642399Z","iopub.status.idle":"2021-07-01T22:21:40.052372Z","shell.execute_reply.started":"2021-07-01T22:21:39.642336Z","shell.execute_reply":"2021-07-01T22:21:40.051673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Table of Contents\n[Section 1: Loading and Inspecting the Data](#Section-1:-Loading-and-Inspecting-the-Data)<br>\n- [Part 1: Trips Datasets](#Part-1:-Trips-Datasets)\n\n- [Part 2: Weather Datasets](#Part-2:-Weather-Datasets)\n\n[Section 2: Data Wrangling](#Section-2:-Data-Wrangling)<br>\n- [Part 1: Trip Datasets](#Part-1:-Trip-Datasets)\n\n- [Part 2: Weather Datasets](#Part-2:-Weather-Datasets)\n\n- [Part 3: Loading API Datasets](#Part-3:-Loading-API-Datasets)\n\n- [Part 4: Creating Master Dataframe](#Part-4:-Creating-Master-Dataframe)\n\n[Section 3: Exploratory Data Analysis](#Section-3:-Exploratory-Data-Analysis)<br>\n- [Histograms](#Histograms)<br>\n- [Time Scale Graphs](#Time-Scale-Graphs)<br>\n\n[Section 4: Conclusions](#Section-4:-Conclusions)<br>","metadata":{}},{"cell_type":"markdown","source":"---\n## Step 1: Loading and Inspecting the Data","metadata":{}},{"cell_type":"code","source":"# Condense below load data code into loops.\n# [Code Source](https://swcarpentry.github.io/python-novice-gapminder/14-looping-data-sets/)\n\n# `for filename in glob.glob('data/gapminder_*.csv'):\n#     data = pd.read_csv(filename)`","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:21:40.053368Z","iopub.execute_input":"2021-07-01T22:21:40.053758Z","iopub.status.idle":"2021-07-01T22:21:40.056238Z","shell.execute_reply.started":"2021-07-01T22:21:40.05373Z","shell.execute_reply":"2021-07-01T22:21:40.055636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Part 1: Trip Data\n#### **2017 & 2018 Trip Data**\n**_About the Data_**\n\nThe 2017 trip dataset contains 1,492,369 bike trip data taken between more than 600 stations across the City of Toronto. The 2018 trip dataset contains 1,922,955 bike trip data. Both datasets contains 9 features. The description of each feature is listed below:\n\n- `trip_id`: Unique ID code for individual trip taken.\n- `trip_start_time`: Trip start time.\n- `trip_end_time`: Trip end time.\n- `trip_duration_seconds`: Duration of the trip in seconds.\n- `from_station_id`: Unique ID code for the start station.\n- `from_station_name`: Name of start station.\n- `to_station_id`: Unique ID code for the end station.\n- `to_station_name`: Name of end station.\n- `user_type`: Type of user, either Member or Casual.","metadata":{}},{"cell_type":"code","source":"# Load in 2017 & 2018 datasets\n# Change directory accordingly to where you saved the data files in relation to this notebook\n\nQ1_17 = pd.read_csv('../input/bikeshare-toronto-ridership-data-20172020/Bikeshare 2017/2017-Q1.csv')\nQ2_17 = pd.read_csv('../input/bikeshare-toronto-ridership-data-20172020/Bikeshare 2017/2017-Q2.csv')\nQ3_17 = pd.read_csv('../input/bikeshare-toronto-ridership-data-20172020/Bikeshare 2017/2017-Q3.csv')\nQ4_17 = pd.read_csv('../input/bikeshare-toronto-ridership-data-20172020/Bikeshare 2017/2017-Q4.csv')\n\nQ1_18 = pd.read_csv('../input/bikeshare-toronto-ridership-data-20172020/Bikeshare 2018/2018-Q1.csv')\nQ2_18 = pd.read_csv('../input/bikeshare-toronto-ridership-data-20172020/Bikeshare 2018/2018-Q2.csv')\nQ3_18 = pd.read_csv('../input/bikeshare-toronto-ridership-data-20172020/Bikeshare 2018/2018-Q3.csv')\nQ4_18 = pd.read_csv('../input/bikeshare-toronto-ridership-data-20172020/Bikeshare 2018/2018-Q4.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:21:40.05723Z","iopub.execute_input":"2021-07-01T22:21:40.057623Z","iopub.status.idle":"2021-07-01T22:21:49.722274Z","shell.execute_reply.started":"2021-07-01T22:21:40.057595Z","shell.execute_reply":"2021-07-01T22:21:49.720693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **2017 & 2018 Trip Data**\n**_About the Data_**\n\nThe 2019 trip dataset contains 2,439,517 bike trip data taken between more than 600 stations across the City of Toronto. The 2020 trip dataset contains 2,612,704 bike trip data. Starting 2019, the data feature names changed and 2 additional features were added making it 11 total. The description of each feature is listed below:\n\n- `Trip Id`: Unique ID code for individual trip taken.\n- `Subscription Id`: Unique ID code for the individual member, this can be used to track Annual Member usage.\n- `Trip  Duration`: Duration of the trip in seconds.\n- `Start Station Id`: Unique ID code for the start station.\n- `Start Time`: Trip start time.\n- `Start Station Name`: Name of start station.\n- `End Station Id`: Unique ID code for the end station.\n- `End Time`: Trip end time.\n- `End Station Name`: Name of end station.\n- `Bike Id`: Unique ID for the individual bike used.\n- `User Type`: Type of user, either Annual or Casual.","metadata":{}},{"cell_type":"code","source":"# Load in 2019 & 2020 datasets\n# Change directory accordingly to where you saved the data files in relation to this notebook\n\nQ1_19 = pd.read_csv('../input/bikeshare-toronto-ridership-data-20172020/Bikeshare 2019/2019-Q1.csv')\nQ2_19 = pd.read_csv('../input/bikeshare-toronto-ridership-data-20172020/Bikeshare 2019/2019-Q2.csv')\nQ3_19 = pd.read_csv('../input/bikeshare-toronto-ridership-data-20172020/Bikeshare 2019/2019-Q3.csv')\nQ4_19 = pd.read_csv('../input/bikeshare-toronto-ridership-data-20172020/Bikeshare 2019/2019-Q4.csv')\n\nM1_20 = pd.read_csv('../input/bikeshare-toronto-ridership-data-20172020/Bikeshare 2020/2020-01.csv')\nM2_20 = pd.read_csv('../input/bikeshare-toronto-ridership-data-20172020/Bikeshare 2020/2020-02.csv')\nM3_20 = pd.read_csv('../input/bikeshare-toronto-ridership-data-20172020/Bikeshare 2020/2020-03.csv')\nM4_20 = pd.read_csv('../input/bikeshare-toronto-ridership-data-20172020/Bikeshare 2020/2020-04.csv')\nM5_20 = pd.read_csv('../input/bikeshare-toronto-ridership-data-20172020/Bikeshare 2020/2020-05.csv')\nM6_20 = pd.read_csv('../input/bikeshare-toronto-ridership-data-20172020/Bikeshare 2020/2020-06.csv')\nM7_20 = pd.read_csv('../input/bikeshare-toronto-ridership-data-20172020/Bikeshare 2020/2020-07.csv')\nM8_20 = pd.read_csv('../input/bikeshare-toronto-ridership-data-20172020/Bikeshare 2020/2020-08.csv')\nM9_20 = pd.read_csv('../input/bikeshare-toronto-ridership-data-20172020/Bikeshare 2020/2020-09.csv')\nM10_20 = pd.read_csv('../input/bikeshare-toronto-ridership-data-20172020/Bikeshare 2020/2020-10.csv')\nM11_20 = pd.read_csv('../input/bikeshare-toronto-ridership-data-20172020/Bikeshare 2020/2020-11.csv')\nM12_20 = pd.read_csv('../input/bikeshare-toronto-ridership-data-20172020/Bikeshare 2020/2020-12.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:21:49.723638Z","iopub.execute_input":"2021-07-01T22:21:49.72401Z","iopub.status.idle":"2021-07-01T22:22:08.179372Z","shell.execute_reply.started":"2021-07-01T22:21:49.723971Z","shell.execute_reply":"2021-07-01T22:22:08.178787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **2017-2020 Weather Data**\n**_About the Data_**\n\nThe following datasets contains 8,760 weather datapoints taken during the each year between 2017-2020 recorded from a weather station located on the Toronto Island. The csv contains 28 features. The description of each feature is listed below:\n\n- `Longitude (x)`: The longitude geo-coordinate of the weather station\n- `Latitude (y)`: The longitude geo-coordinate of the weather station\n- `Station Name`: The name of the weather station\n- `Climate ID`: The Climate ID is a 7 digit number assigned by the Meteorological Service of Canada to a site where official weather observations are taken, and serves as a permanent, unique identifier.\n- `Date/Time`: The date and time of when the weather data was collected.\n- `Year`: The year the data was collected.\n- `Month`: The month the data was collected.\n- `Day`: The day the data was collected.\n- `Time`: The time of day the data was collected.\n- `Temp (째C)`: The temperature of the air in degrees Celsius (째C). \n- `Temp Flag`: Flag for unique events for temperature.\n- `Dew Point Temp (째C)`: The dew point temperature in degrees Celsius (째C), a measure of the humidity of the air, is the temperature to which the air would have to be cooled to reach saturation with respect to liquid water. Saturation occurs when the air is holding the maximum water vapour possible at that temperature and atmospheric pressure.\n- `Dew Point Temp Flag`: Flag for unique events for dew point temp.\n- `Rel Hum (%)`: Relative humidity in percent (%) is the ratio of the quantity of water vapour the air contains compared to the maximum amount it can hold at that particular temperature.\n- `Rel Hum Flag`: Flag for unique events for relative humidity.\n- `Wind Dir (10s deg)`: The speed of motion of air in kilometres per hour (km/h) usually observed at 10 metres above the ground. It represents the average speed during the one-, two- or ten-minute period ending at the time of observation.\n- `Wind Dir Flag`: Flag for unique events for wind direction.\n- `Wind Spd (km/h)`: The speed of motion of air in kilometres per hour (km/h) usually observed at 10 metres above the ground. It represents the average speed during the one-, two- or ten-minute period ending at the time of observation.\n- `Wind Spd Flag`: Flag for unique events for wind speed.\n- `Visibility (km)`: Visibility in kilometres (km) is the distance at which objects of suitable size can be seen and identified.\n- `Visibility Flag`: Flag for unique events for visibility.\n- `Stn Press (kPa)`: The atmospheric pressure in kilopascals (kPa) at the station elevation. Atmospheric pressure is the force per unit area exerted by the atmosphere as a consequence of the mass of air in a vertical column from the elevation of the observing station to the top of the atmosphere.\n- `Stn Press Flag`: Flag for unique events for station atmospheric pressure.\n- `Hmdx`: Humidex is an index to indicate how hot or humid the weather feels to the average person. It is derived by combining temperature and humidity values into one number to reflect the perceived temperature.\n- `Hmdx Flag`: Flag for unique events for humidex.\n- `Wind Chill`: Wind chill is an index to indicate how cold the weather feels to the average person. It is derived by combining temperature and wind velocity values into one number to reflect the perceived temperature.\n- `Wind Chill Flag`: Flag for unique events for wind chill index.\n- `Weather`: Observations of atmospheric phenomenon including the occurrence of weather and obstructions to vision have been taken at many hourly reporting stations.","metadata":{}},{"cell_type":"code","source":"# Load in Weather datasets\n# Change directory accordingly to where you saved the data files in relation to this notebook\n\nW1_17 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2017/en_climate_hourly_ON_6158359_01-2017_P1H.csv')\nW2_17 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2017/en_climate_hourly_ON_6158359_02-2017_P1H.csv')\nW3_17 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2017/en_climate_hourly_ON_6158359_03-2017_P1H.csv')\nW4_17 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2017/en_climate_hourly_ON_6158359_04-2017_P1H.csv')\nW5_17 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2017/en_climate_hourly_ON_6158359_05-2017_P1H.csv')\nW6_17 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2017/en_climate_hourly_ON_6158359_06-2017_P1H.csv')\nW7_17 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2017/en_climate_hourly_ON_6158359_07-2017_P1H.csv')\nW8_17 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2017/en_climate_hourly_ON_6158359_08-2017_P1H.csv')\nW9_17 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2017/en_climate_hourly_ON_6158359_09-2017_P1H.csv')\nW10_17 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2017/en_climate_hourly_ON_6158359_10-2017_P1H.csv')\nW11_17 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2017/en_climate_hourly_ON_6158359_11-2017_P1H.csv')\nW12_17 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2017/en_climate_hourly_ON_6158359_12-2017_P1H.csv')\n\nW1_18 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2018/en_climate_hourly_ON_6158359_01-2018_P1H.csv')\nW2_18 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2018/en_climate_hourly_ON_6158359_02-2018_P1H.csv')\nW3_18 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2018/en_climate_hourly_ON_6158359_03-2018_P1H.csv')\nW4_18 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2018/en_climate_hourly_ON_6158359_04-2018_P1H.csv')\nW5_18 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2018/en_climate_hourly_ON_6158359_05-2018_P1H.csv')\nW6_18 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2018/en_climate_hourly_ON_6158359_06-2018_P1H.csv')\nW7_18 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2018/en_climate_hourly_ON_6158359_07-2018_P1H.csv')\nW8_18 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2018/en_climate_hourly_ON_6158359_08-2018_P1H.csv')\nW9_18 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2018/en_climate_hourly_ON_6158359_09-2018_P1H.csv')\nW10_18 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2018/en_climate_hourly_ON_6158359_10-2018_P1H.csv')\nW11_18 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2018/en_climate_hourly_ON_6158359_11-2018_P1H.csv')\nW12_18 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2018/en_climate_hourly_ON_6158359_12-2018_P1H.csv')\n\nW1_19 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2019/en_climate_hourly_ON_6158359_01-2019_P1H.csv')\nW2_19 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2019/en_climate_hourly_ON_6158359_02-2019_P1H.csv')\nW3_19 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2019/en_climate_hourly_ON_6158359_03-2019_P1H.csv')\nW4_19 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2019/en_climate_hourly_ON_6158359_04-2019_P1H.csv')\nW5_19 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2019/en_climate_hourly_ON_6158359_05-2019_P1H.csv')\nW6_19 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2019/en_climate_hourly_ON_6158359_06-2019_P1H.csv')\nW7_19 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2019/en_climate_hourly_ON_6158359_07-2019_P1H.csv')\nW8_19 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2019/en_climate_hourly_ON_6158359_08-2019_P1H.csv')\nW9_19 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2019/en_climate_hourly_ON_6158359_09-2019_P1H.csv')\nW10_19 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2019/en_climate_hourly_ON_6158359_10-2019_P1H.csv')\nW11_19 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2019/en_climate_hourly_ON_6158359_11-2019_P1H.csv')\nW12_19 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2019/en_climate_hourly_ON_6158359_12-2019_P1H.csv')\n\nW1_20 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2020/en_climate_hourly_ON_6158359_01-2020_P1H.csv')\nW2_20 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2020/en_climate_hourly_ON_6158359_02-2020_P1H.csv')\nW3_20 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2020/en_climate_hourly_ON_6158359_03-2020_P1H.csv')\nW4_20 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2020/en_climate_hourly_ON_6158359_04-2020_P1H.csv')\nW5_20 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2020/en_climate_hourly_ON_6158359_05-2020_P1H.csv')\nW6_20 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2020/en_climate_hourly_ON_6158359_06-2020_P1H.csv')\nW7_20 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2020/en_climate_hourly_ON_6158359_07-2020_P1H.csv')\nW8_20 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2020/en_climate_hourly_ON_6158359_08-2020_P1H.csv')\nW9_20 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2020/en_climate_hourly_ON_6158359_09-2020_P1H.csv')\nW10_20 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2020/en_climate_hourly_ON_6158359_10-2020_P1H.csv')\nW11_20 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2020/en_climate_hourly_ON_6158359_11-2020_P1H.csv')\nW12_20 = pd.read_csv('../input/toronto-city-centre-hourly-weather-data-20172020/Weather 2020/en_climate_hourly_ON_6158359_12-2020_P1H.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:22:08.180482Z","iopub.execute_input":"2021-07-01T22:22:08.180813Z","iopub.status.idle":"2021-07-01T22:22:08.452975Z","shell.execute_reply.started":"2021-07-01T22:22:08.180779Z","shell.execute_reply":"2021-07-01T22:22:08.45175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Section 2: Data Wrangling","metadata":{}},{"cell_type":"markdown","source":"Back to [Table of Contents](#Table-of-Contents).<br>\n\nNow that we have successfully loaded all the datasets, let's start by combining the datasets by year, then inspecting the dataframes and perform any data cleaning if required.\n\n1. Trip Datasets\n2. Weather Datasets\n3. API Datasets","metadata":{}},{"cell_type":"code","source":"# Add in code to loop through repetitive tasks.\n\n# list[trips17,trips18,trips19,trips20]\n# for i in list:","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:22:08.456985Z","iopub.execute_input":"2021-07-01T22:22:08.45721Z","iopub.status.idle":"2021-07-01T22:22:08.460524Z","shell.execute_reply.started":"2021-07-01T22:22:08.457188Z","shell.execute_reply":"2021-07-01T22:22:08.459804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Part 1: Trip Datasets**","metadata":{}},{"cell_type":"code","source":"# Combine csvs into yearly dataframes\ntrips17 = pd.concat([Q1_17,Q2_17,Q3_17,Q4_17]).reset_index(drop=True)\ntrips18 = pd.concat([Q1_18,Q2_18,Q3_18,Q4_18]).reset_index(drop=True)\ntrips19 = pd.concat([Q1_19,Q2_19,Q3_19,Q4_19]).reset_index(drop=True)\ntrips20 = pd.concat([M1_20, M2_20, M3_20, M4_20, M5_20, M6_20, M7_20, M8_20, M9_20, M10_20, M11_20, M12_20]).reset_index(drop=True)\n\n# Display dataframe shape\nprint(f\"\"\"\n2017 Trip dataframe shape: {trips17.shape}\n2018 Trip dataframe shape: {trips18.shape}\n2019 Trip dataframe shape: {trips19.shape}\n2020 Trip dataframe shape: {trips20.shape}\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:22:08.462498Z","iopub.execute_input":"2021-07-01T22:22:08.462801Z","iopub.status.idle":"2021-07-01T22:22:09.782287Z","shell.execute_reply.started":"2021-07-01T22:22:08.462771Z","shell.execute_reply":"2021-07-01T22:22:09.780782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Observations_**\n- In 2017 there are almost 1.5 million recorded trips with 9 column features.\n- In 2018 there are over 1.9 million recorded trips with 9 column features, 400k more trips than 2017.\n- In 2019 there are over 2.4 million recorded trips with 11 column features, 500k more trips than 2018.\n- In 2020 there are over 2.9 million recorded trips with 11 column features, 500k more trips than 2019.\n\n**_Next Step_**<br>\nBecause there are 2 additonal column features in 2019-2020 trip datasets, we need to take a closer look into the datasets into what the differences are. Next we will display the first 3 records(rows) of each dataset.","metadata":{}},{"cell_type":"code","source":"# Display first 3 rows of each dataframe\nprint(\"\\n**trips17 dataset**\")\ndisplay(trips17.head(3))\nprint(\"\\n**trips18 dataset**\")\ndisplay(trips18.head(3))\nprint(\"\\n**trips19 dataset**\")\ndisplay(trips19.head(3))\nprint(\"\\n**trips20 dataset**\")\ndisplay(trips20.head(3))","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:22:09.783782Z","iopub.execute_input":"2021-07-01T22:22:09.784086Z","iopub.status.idle":"2021-07-01T22:22:09.843251Z","shell.execute_reply.started":"2021-07-01T22:22:09.78406Z","shell.execute_reply":"2021-07-01T22:22:09.842108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Observations_**\n\n- It appears that in 2019-2020, *BikeShare Toronto* added in 2 column features `Subscription Id` & `Bike Id` allowing for more granularity in analyzing their trip data. We may need to create two separate dataframes to make use of both the new data features and older datasets in the analysis phase\n- Feature headings in 2019-2020 datasets looks more presentable therefore we will be changing the earlier datasets to fit this format later on before EDA is performed\n- We will need to investigate if `user_type`/`User Type` can be converted to boolean classifier\n\n**_Next Steps_**\n1. Inspect each datasets' datatype\n2. Check for any duplicates & `null` values in the datasets","metadata":{}},{"cell_type":"code","source":"# Check each dataframe's datatypes\nprint(f\"\"\"\n**trips17 dataset**\\n\n{trips17.dtypes}\\n\n========================================\\n\n**trips18 dataset**\\n\n{trips18.dtypes}\\n\n========================================\\n\n**trips19 dataset**\\n\n{trips19.dtypes}\\n\n========================================\\n\n**trips20 dataset**\\n\n{trips20.dtypes}\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:22:09.844699Z","iopub.execute_input":"2021-07-01T22:22:09.845039Z","iopub.status.idle":"2021-07-01T22:22:09.854987Z","shell.execute_reply.started":"2021-07-01T22:22:09.844994Z","shell.execute_reply":"2021-07-01T22:22:09.853209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Observations_**\n- `trip_start_time` & `trip_stop_time` data type needs to be changed to `datetime64`\n- `from_station_id` & `to_station_id` data type needs to be changed to `int64`\n- `trip_duration_seconds` can be simplified down to `trip_duration_minutes`\n\n**_Next Steps_**<br>\n~1. Inspect each datasets' datatype~<br>\n2. Check for any duplicates & null values in the datasets","metadata":{}},{"cell_type":"code","source":"# Check for duplicates\nprint(f\"\"\"\nDuplicates found in trips17: {trips17.duplicated().sum()}\nDuplicates found in trips18: {trips18.duplicated().sum()}\nDuplicates found in trips19: {trips19.duplicated().sum()}\nDuplicates found in trips20: {trips20.duplicated().sum()}\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:22:09.856606Z","iopub.execute_input":"2021-07-01T22:22:09.856913Z","iopub.status.idle":"2021-07-01T22:22:17.868954Z","shell.execute_reply.started":"2021-07-01T22:22:09.856881Z","shell.execute_reply":"2021-07-01T22:22:17.867592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for null values\nprint(f\"\"\"\n**trips17 dataset**\\n\nTotal Number of Null values:\\n\\n{trips17.isna().sum()}\\n\n----------------------------------------\\n\nTotal Percentage of Null values:\\n\\n{trips17.isna().mean()*100}\\n\n========================================\\n\n**trips18 dataset**\\n\nTotal Number of Null values:\\n\\n{trips18.isna().sum()}\\n\n----------------------------------------\\n\nTotal Percentage of Null values:\\n\\n{trips18.isna().mean()*100}\\n\n========================================\\n\n**trips19 dataset**\\n\nTotal Number of Null values:\\n\\n{trips19.isna().sum()}\\n\n----------------------------------------\\n\nTotal Percentage of Null values:\\n\\n{trips19.isna().mean()*100}\\n\n========================================\\n\n**trips20 dataset**\\n\nTotal Number of Null values:\\n\\n{trips20.isna().sum()}\\n\n----------------------------------------\\n\nTotal Percentage of Null values:\\n\\n{trips20.isna().mean()*100}\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:22:17.87105Z","iopub.execute_input":"2021-07-01T22:22:17.871291Z","iopub.status.idle":"2021-07-01T22:22:20.594378Z","shell.execute_reply.started":"2021-07-01T22:22:17.871268Z","shell.execute_reply":"2021-07-01T22:22:20.593116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Observations_**\n\n_2017 Trip Dataset_\n\nThere are ~1 million (68.8%) `from_station_id` and `to_station_id` missing records, we will need to look into what caused this and resolve the issue but the fact that `from_station_name` and `to_station_name` are not missing means that we can possibly impute the missing values by matching the `name` and `station id` from where it was not missing.\n\n**Later investigation revealed that the date format in Q1 & Q2 of 2017 datasets were structurally different than Q3 & Q4 of 2017. Not only is it missing `from_station_id` and `to_station_id` but the date format is also different. I have not found an easy way to fix the date format issue other than instantiating 2017 trips data into two separate dataframes and changing the datatypes separately then joining it afterwards.**\n\n_2018 Trip Dataset_\n\nThere are no duplicates or any null values found! \n\n_2019 Trip Dataset_\n\nThere are 454 (0.018%) `End Station Id` and `End Station Name` missing. Lets take a closer look into these null values to see if we can impute the missing information, if not then we can drop these datapoints since it is less than 0.1% of total datapoints.\n\n_2020 Trip Dataset_\n\nThere are 691 (0.036%) `End Station Id` and `End Station Name` missing similar to the 2019 Trips Dataset. Therefore I need to take a closer look into these null values as well, if we cannot impute the missing data we will need to drop these datapoints since it is less than 0.1% of total datapoints. `Start Station Name` can be imputed easily as there are no missing `Start Station Id`. `Bike Id` will need to be dropped as there is no way to impute that data.\n\n**_Next Steps_**\n1. Fix the datetime issue in the 2017 dataset\n2. Impute missing `from_station_id` & `to_station_id` values in the 2017 trip dataset\n3. Investigate missing `End Station Id` & `End Station Name` values in 2019 & 2020 datasets, then impute/drop missing values\n4. Change/Fix datatypes & column names\n5. Rename columns/features for consistency\n6. Add new features for weekend and holiday classifiers\n7. Concatenate dataframes","metadata":{}},{"cell_type":"markdown","source":"#### *Issue 1: Fix Datetime Issue in 2017*","metadata":{}},{"cell_type":"markdown","source":"In the Time Series Analysis phase I noticed an anomaly in the first half of 2017 where the data skips near the end of every month. On closer inspection I realized that the first half of 2017 the date format is different from the rest of the dataset where it is formatted %d/%m/%Y, then changes to %m/%d/%Y July of 2017 onward.\n\nI have not found a siple way to fix this issue because of this, I will need to re-instantiate the 2017 trip data into separated dataframes, change the datetime separately then join them together again once the date format is fixed.","metadata":{}},{"cell_type":"code","source":"# Combine 2017 csvs into 2 dataframes\ntrips17_1 = pd.concat([Q1_17,Q2_17]).reset_index(drop=True)\ntrips17_2 = pd.concat([Q3_17,Q4_17]).reset_index(drop=True)\n\n# Display dataframe shape\nprint(f\"\"\"\ntrips17_1 dataframe shape: {trips17_1.shape}\ntrips17_2 dataframe shape: {trips17_2.shape}\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:22:20.595403Z","iopub.execute_input":"2021-07-01T22:22:20.59568Z","iopub.status.idle":"2021-07-01T22:22:20.960047Z","shell.execute_reply.started":"2021-07-01T22:22:20.595656Z","shell.execute_reply":"2021-07-01T22:22:20.959174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Observations_**\n\nIt appears that the second half of 2017 has 2 fewer column features, we will need to take a closer look into this but this may be why there were over 1 million (68.8%) `from_station_id` and `to_station_id` missing records when it was initially combined as one dataframe.\n\n**_Next Step_**<br>\nPerform a check for `null` values in the dataset","metadata":{"execution":{"iopub.execute_input":"2021-06-29T16:34:06.108153Z","iopub.status.busy":"2021-06-29T16:34:06.107889Z","iopub.status.idle":"2021-06-29T16:34:06.112365Z","shell.execute_reply":"2021-06-29T16:34:06.111241Z","shell.execute_reply.started":"2021-06-29T16:34:06.108126Z"}}},{"cell_type":"code","source":"# Check for null values\nprint(f\"\"\"\n**trips17_1 dataset**\\n\nTotal Number of Null values:\\n\\n{trips17_1.isna().sum()}\\n\n----------------------------------------\\n\nTotal Percentage of Null values:\\n\\n{trips17_1.isna().mean()*100}\\n\n========================================\\n\n**trips17_2 dataset**\\n\nTotal Number of Null values:\\n\\n{trips17_2.isna().sum()}\\n\n----------------------------------------\\n\nTotal Percentage of Null values:\\n\\n{trips17_2.isna().mean()*100}\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:22:20.961602Z","iopub.execute_input":"2021-07-01T22:22:20.961976Z","iopub.status.idle":"2021-07-01T22:22:21.597996Z","shell.execute_reply.started":"2021-07-01T22:22:20.961936Z","shell.execute_reply":"2021-07-01T22:22:21.596785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There was only 1 `null` value found, therefore we found our reason as to why there were so many missing values in `trips17`. Because we still have the first half the year of data to draw on, we can simply impute the missing station ids by grouping and matching the station names to station ids once we merge the dataframes back together.\n\nFirst we will need to see if we can impute the 1 missing value or if we need to drop it then change the datetime format.","metadata":{}},{"cell_type":"code","source":"# Display row with null vlaue\ntrips17_2[trips17_2.isna().any(axis=1)]","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:22:21.59908Z","iopub.execute_input":"2021-07-01T22:22:21.599358Z","iopub.status.idle":"2021-07-01T22:22:22.082643Z","shell.execute_reply.started":"2021-07-01T22:22:21.59933Z","shell.execute_reply":"2021-07-01T22:22:22.079145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the`to_station_name` is missing, there is no way to impute the `null` values and therefore needs to be dropped.","metadata":{}},{"cell_type":"code","source":"# Drop row with null value\ntrips17_2.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:22:22.086835Z","iopub.execute_input":"2021-07-01T22:22:22.087506Z","iopub.status.idle":"2021-07-01T22:22:22.386747Z","shell.execute_reply.started":"2021-07-01T22:22:22.087327Z","shell.execute_reply":"2021-07-01T22:22:22.385678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that the `NULL` value has been dropped we can run the code to change the data type for `trip_start_time`/`Start Time` & `trip_stop_time`/`End Time`","metadata":{}},{"cell_type":"code","source":"%%time\n# Changing `trip_start_time` & `trip_stop_time` dtype from object to datetime\n# WARNING: Will take a while to run \n# Runtime on my machine: ~2min\ntrips17_1['trip_start_time'] = pd.to_datetime(trips17_1['trip_start_time'], dayfirst=True, infer_datetime_format=True)\ntrips17_2['trip_start_time'] = pd.to_datetime(trips17_2['trip_start_time'], infer_datetime_format=True)\n\ntrips17_1['trip_stop_time'] = pd.to_datetime(trips17_1['trip_stop_time'], dayfirst=True, infer_datetime_format=True)\ntrips17_2['trip_stop_time'] = pd.to_datetime(trips17_2['trip_stop_time'], infer_datetime_format=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:22:22.387878Z","iopub.execute_input":"2021-07-01T22:22:22.388367Z","iopub.status.idle":"2021-07-01T22:24:06.180057Z","shell.execute_reply.started":"2021-07-01T22:22:22.388336Z","shell.execute_reply":"2021-07-01T22:24:06.179339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine the separate 2017 csvs into 1 dataframe\ntrips17 = pd.concat([trips17_1,trips17_2]).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:24:06.181264Z","iopub.execute_input":"2021-07-01T22:24:06.181549Z","iopub.status.idle":"2021-07-01T22:24:06.450516Z","shell.execute_reply.started":"2021-07-01T22:24:06.18152Z","shell.execute_reply":"2021-07-01T22:24:06.449539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### *Issue 2: Impute Missing Values in 2017*","metadata":{}},{"cell_type":"markdown","source":"Code taken from [StackOverflow Forum Post](https://stackoverflow.com/questions/48359091/fill-nulls-with-values-in-another-dataframe-in-pandas).\n\nI tried to just use the first half of 2017 trip data to impute the missing `station_ids` but it was not sufficient to impute all the missing values. Therefore I will be importing a `station_ids` dataset as reference to fill in the `null` values in `trips17`. The dataset contains the proper Station Ids and Station Names used between 2017 and 2020. \n\nIn certain years, BikeShare Toronto would move Bike Stations to different locations while retaining the same Station Id. There is also some spelling and type errors in some station names that need to be addressed in the future using *Levenshtein Distance*.","metadata":{}},{"cell_type":"code","source":"# Load station_ids dataset as reference for imputing\nstation_ids17_20 = pd.read_csv('../input/bikeshare-toronto-station-ids/station_ids17_20.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:24:06.451663Z","iopub.execute_input":"2021-07-01T22:24:06.451942Z","iopub.status.idle":"2021-07-01T22:24:06.463919Z","shell.execute_reply.started":"2021-07-01T22:24:06.451911Z","shell.execute_reply":"2021-07-01T22:24:06.462888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display dataframe\nstation_ids17_20","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:24:06.465297Z","iopub.execute_input":"2021-07-01T22:24:06.465602Z","iopub.status.idle":"2021-07-01T22:24:06.482566Z","shell.execute_reply.started":"2021-07-01T22:24:06.465573Z","shell.execute_reply":"2021-07-01T22:24:06.481226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 1 - Fill NA using station_ids as reference\ntrips17['from_station_id'].fillna(trips17['from_station_name'].map(station_ids17_20.set_index('name')['station_id']), inplace=True)\ntrips17['to_station_id'].fillna(trips17['to_station_name'].map(station_ids17_20.set_index('name')['station_id']), inplace=True)\n\n# Step 2 - Forward fill any missed NA\ntrips17['from_station_id'] = (trips17.groupby('from_station_name')['from_station_id']).ffill()\ntrips17['to_station_id'] = (trips17.groupby('to_station_name')['to_station_id']).ffill()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:24:06.484817Z","iopub.execute_input":"2021-07-01T22:24:06.485141Z","iopub.status.idle":"2021-07-01T22:24:07.818491Z","shell.execute_reply.started":"2021-07-01T22:24:06.485111Z","shell.execute_reply":"2021-07-01T22:24:07.816834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if imputation worked correctly\ntrips17.groupby(['from_station_id','from_station_name']).size().reset_index(name='count')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:24:07.820013Z","iopub.execute_input":"2021-07-01T22:24:07.820308Z","iopub.status.idle":"2021-07-01T22:24:08.097554Z","shell.execute_reply.started":"2021-07-01T22:24:07.820278Z","shell.execute_reply":"2021-07-01T22:24:08.096168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Future Cleaning Task_**<br>\nUse Levenshtein_distance to fix typos in Station Names such as Station Id 7002 & St. George St / Bloor St W.\nWe cannot just use a simple transform as there may be instances where BikeShare Toronto would reuse the same Station Id for a different location or they simply moved the station down a street such as Station Id 7029 with it being named Bay St / Bloor St W (East Side) and St. James Park (King St. E.).\n\nFor now we will skip this step and continue forward to with the current data cleaning tasks.","metadata":{}},{"cell_type":"code","source":"# Check to see if the imputation worked\nprint(f\"\"\"\nTotal Number of Null values:\\n\\n{trips17.isna().sum()}\\n\n----------------------------------------\\n\nTotal Percentage of Null values:\\n\\n{trips17.isna().mean()*100}\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:24:08.0999Z","iopub.execute_input":"2021-07-01T22:24:08.100125Z","iopub.status.idle":"2021-07-01T22:24:08.4062Z","shell.execute_reply.started":"2021-07-01T22:24:08.100101Z","shell.execute_reply":"2021-07-01T22:24:08.40521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display rows with null values\ntrips17[trips17.isna().any(axis=1)]","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:24:08.407507Z","iopub.execute_input":"2021-07-01T22:24:08.407719Z","iopub.status.idle":"2021-07-01T22:24:08.681321Z","shell.execute_reply.started":"2021-07-01T22:24:08.407697Z","shell.execute_reply":"2021-07-01T22:24:08.680035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Observation_**<br>\nSince `Base Station` appears to be used for Maintenance and not Public Use. I will be dropping these records.","metadata":{"execution":{"iopub.execute_input":"2021-07-01T16:38:06.358387Z","iopub.status.busy":"2021-07-01T16:38:06.358124Z","iopub.status.idle":"2021-07-01T16:38:06.372278Z","shell.execute_reply":"2021-07-01T16:38:06.37116Z","shell.execute_reply.started":"2021-07-01T16:38:06.35836Z"}}},{"cell_type":"code","source":"# Drop `NULL` values\ntrips17.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:24:08.682823Z","iopub.execute_input":"2021-07-01T22:24:08.683251Z","iopub.status.idle":"2021-07-01T22:24:09.174724Z","shell.execute_reply.started":"2021-07-01T22:24:08.683208Z","shell.execute_reply":"2021-07-01T22:24:09.173622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### *Issue 3: Investigate & Impute/Drop Missing Values in 2019 & 2020*","metadata":{}},{"cell_type":"code","source":"# Display rows with null values\nprint(\"\\n**trips19 dataset**\")\ndisplay(trips19[trips19.isna().any(axis=1)].head(3))\nprint(\"\\n**trips20 dataset**\")\ndisplay(trips20[trips20.isna().any(axis=1)].head(3))","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:24:09.175722Z","iopub.execute_input":"2021-07-01T22:24:09.175916Z","iopub.status.idle":"2021-07-01T22:24:10.668385Z","shell.execute_reply.started":"2021-07-01T22:24:09.175895Z","shell.execute_reply":"2021-07-01T22:24:10.667545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Observations_**\n\nUnfortunately it appears that there are a number of trips where none of the End Station information was recorded and there is no way to accurate impute the missing information. We will have try and impute the rows we can and drop the rows we cannot impute. Luckily it only comprises of less than 0.1% of total data each year so either way it will not have a statistical significance.\n\nBut first we will need to impute the missing values in `Start Station Name` in `trip20` and then drop the 73 records for missing `Bike Id`. This will not have any significant statistical impact as it comprises less than 0.003% of the data in 2020.","metadata":{}},{"cell_type":"code","source":"# Drop row if both `End Station Id` & `End Station Name` are missing\ntrips19.dropna(subset=['End Station Id', 'End Station Name'], thresh=1, inplace=True)\ntrips20.dropna(subset=['End Station Id', 'End Station Name'], thresh=1, inplace=True)\n\n# Different way to drop rows with missing Bike Ids \ntrips20 = trips20[trips20['Bike Id'].notna()]\n\n# Reset index\ntrips19.reset_index(inplace=True, drop=True)\ntrips20.reset_index(inplace=True, drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:24:10.669339Z","iopub.execute_input":"2021-07-01T22:24:10.669559Z","iopub.status.idle":"2021-07-01T22:24:11.81422Z","shell.execute_reply.started":"2021-07-01T22:24:10.669538Z","shell.execute_reply":"2021-07-01T22:24:11.813262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Recheck for null values\nprint(f\"\"\"\n**trips19 dataset**\\n\nTotal Number of Null values:\\n\\n{trips19.isna().sum()}\\n\n----------------------------------------\\n\nTotal Percentage of Null values:\\n\\n{trips19.isna().mean()*100}\\n\n========================================\\n\n**trips20 dataset**\\n\nTotal Number of Null values:\\n\\n{trips20.isna().sum()}\\n\n----------------------------------------\\n\nTotal Percentage of Null values:\\n\\n{trips20.isna().mean()*100}\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:24:11.816224Z","iopub.execute_input":"2021-07-01T22:24:11.816523Z","iopub.status.idle":"2021-07-01T22:24:13.454819Z","shell.execute_reply.started":"2021-07-01T22:24:11.816491Z","shell.execute_reply":"2021-07-01T22:24:13.453276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Next Step_**<br>\nSimilar to `trips17` I will be using a reference dataset to impute the missing Station Names. However to avoid multiple names using the same Station Id, I am using a limited selection of Stations used in 2020.","metadata":{"execution":{"iopub.execute_input":"2021-07-01T20:01:21.244391Z","iopub.status.busy":"2021-07-01T20:01:21.244179Z","iopub.status.idle":"2021-07-01T20:01:21.257287Z","shell.execute_reply":"2021-07-01T20:01:21.248489Z","shell.execute_reply.started":"2021-07-01T20:01:21.244366Z"}}},{"cell_type":"code","source":"# Load station_ids dataset as reference for imputing\nstation_ids20 = pd.read_csv('../input/bikeshare-toronto-station-ids/station_ids20.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:24:13.456364Z","iopub.execute_input":"2021-07-01T22:24:13.456723Z","iopub.status.idle":"2021-07-01T22:24:13.468912Z","shell.execute_reply.started":"2021-07-01T22:24:13.456689Z","shell.execute_reply":"2021-07-01T22:24:13.467453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display dataframe\nstation_ids20","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:24:13.470239Z","iopub.execute_input":"2021-07-01T22:24:13.470514Z","iopub.status.idle":"2021-07-01T22:24:13.487888Z","shell.execute_reply.started":"2021-07-01T22:24:13.470488Z","shell.execute_reply":"2021-07-01T22:24:13.486704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use reference to impute missing Station Names\ntrips20['Start Station Name'].fillna(trips20['Start Station Id'].map(station_ids20.set_index('station_id')['name']), inplace=True)\ntrips20['End Station Name'].fillna(trips20['Start Station Id'].map(station_ids20.set_index('station_id')['name']), inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:24:13.488933Z","iopub.execute_input":"2021-07-01T22:24:13.489232Z","iopub.status.idle":"2021-07-01T22:24:14.390184Z","shell.execute_reply.started":"2021-07-01T22:24:13.489203Z","shell.execute_reply":"2021-07-01T22:24:14.389168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for null values\nprint(f\"\"\"\n**trips20 dataset**\\n\nTotal Number of Null values:\\n\\n{trips20.isna().sum()}\\n\n----------------------------------------\\n\nTotal Percentage of Null values:\\n\\n{trips20.isna().mean()*100}\\\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:24:14.391488Z","iopub.execute_input":"2021-07-01T22:24:14.391827Z","iopub.status.idle":"2021-07-01T22:24:17.166762Z","shell.execute_reply.started":"2021-07-01T22:24:14.39179Z","shell.execute_reply":"2021-07-01T22:24:17.165549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### *Issue 4: Change Datatypes*","metadata":{}},{"cell_type":"markdown","source":"**_Next Steps_**\n- `trip_start_time`/`Start Time` & `trip_stop_time`/`End Time` datatype needs to be changed to datetime64\n- `from_station_id`/`Start Station Id` & `to_station_id`/`End Station Id` datatype needs to be changed to int64\n- **OPTIONAL**: `trip_duration_seconds`/`Trip  Duration` can be simplified down to `trip_duration_minutes`/`Trip Duration Minutes`\n- `user_type`/`User Type` to be changed to boolean classifier","metadata":{}},{"cell_type":"code","source":"%%time\n# Changing `trip_start_time`/`Start Time` & `trip_stop_time`/`End Time` dtype from object to datetime\n\ntrips18['trip_start_time'] = pd.to_datetime(trips18['trip_start_time'], infer_datetime_format=True)\ntrips19['Start Time'] = pd.to_datetime(trips19['Start Time'], infer_datetime_format=True)\ntrips20['Start Time'] = pd.to_datetime(trips20['Start Time'], infer_datetime_format=True)\n\ntrips18['trip_stop_time'] = pd.to_datetime(trips18['trip_stop_time'], infer_datetime_format=True)\ntrips19['End Time'] = pd.to_datetime(trips19['End Time'], infer_datetime_format=True)\ntrips20['End Time'] = pd.to_datetime(trips20['End Time'], infer_datetime_format=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:24:17.167992Z","iopub.execute_input":"2021-07-01T22:24:17.168351Z","iopub.status.idle":"2021-07-01T22:24:33.430929Z","shell.execute_reply.started":"2021-07-01T22:24:17.168312Z","shell.execute_reply":"2021-07-01T22:24:33.430023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Change `from_station_id` & 'to_station_id' dtype to int\ntrips17['from_station_id'] = trips17['from_station_id'].astype(int)\ntrips18['from_station_id'] = trips18['from_station_id'].astype(int)\n\ntrips17['to_station_id'] = trips17['to_station_id'].astype(int)\ntrips18['to_station_id'] = trips18['to_station_id'].astype(int)\ntrips19['End Station Id'] = trips19['End Station Id'].astype(int)\ntrips20['End Station Id'] = trips20['End Station Id'].astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:24:33.431874Z","iopub.execute_input":"2021-07-01T22:24:33.432103Z","iopub.status.idle":"2021-07-01T22:24:33.476601Z","shell.execute_reply.started":"2021-07-01T22:24:33.432069Z","shell.execute_reply":"2021-07-01T22:24:33.475734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next datatype issue to resolve is the trip duration metric. We have two options available to us:\n1. We can round the duration time into minutes or;\n2. We can leave the duration time in seconds and have the option to change it later.\n\nCurrently I have the option to keep the duration time in seconds active but left the code to drop it commented out.","metadata":{}},{"cell_type":"code","source":"# OPTIONAL TO SKIP in order to keep `trip_duration_seconds`/`Trip  Duration` \n# for more granularity in future analysis\n\n# # Adding new column for `trip_duration_mins`\n# trips17['Trip Duration (Minutes)'] = (trips17['trip_duration_seconds']/60).round(2).astype(int)\n# trips18['Trip Duration (Minutes)'] = (trips18['trip_duration_seconds']/60).round(2).astype(int)\n# trips19['Trip Duration (Minutes)'] = (trips19['Trip  Duration']/60).round(2).astype(int)\n# trips20['Trip Duration (Minutes)'] = (trips20['Trip  Duration']/60).round(2).astype(int)\n\n# # Dropping `trip_duration_seconds`\n# trips17.drop(['trip_duration_seconds'], axis=1, inplace=True)\n# trips18.drop(['trip_duration_seconds'], axis=1, inplace=True)\n# trips19.drop(['Trip  Duration'], axis=1, inplace=True)\n# trips20.drop(['Trip  Duration'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:24:33.47794Z","iopub.execute_input":"2021-07-01T22:24:33.478268Z","iopub.status.idle":"2021-07-01T22:24:33.482331Z","shell.execute_reply.started":"2021-07-01T22:24:33.478236Z","shell.execute_reply":"2021-07-01T22:24:33.481356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verify if we can make column into boolean variable\nprint(f\"\"\"\nUser Types found in trips17: {trips17['user_type'].unique()}\nUser Types found in trips18: {trips18['user_type'].unique()}\nUser Types found in trips19: {trips19['User Type'].unique()}\nUser Types found in trips20: {trips20['User Type'].unique()}\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:24:33.483499Z","iopub.execute_input":"2021-07-01T22:24:33.483783Z","iopub.status.idle":"2021-07-01T22:24:34.005209Z","shell.execute_reply.started":"2021-07-01T22:24:33.483753Z","shell.execute_reply":"2021-07-01T22:24:34.004021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dictionary for converting membership status to binary\nmember = {'Member': 1, 'Casual': 0}\nmember2 = {'Annual Member': 1, 'Casual Member': 0}\n\n# Create new column for `is_member` (1= Members, 0 = Casual)\ntrips17['Member'] = trips17['user_type'].map(member)\ntrips18['Member'] = trips18['user_type'].map(member2)\ntrips19['Member'] = trips19['User Type'].map(member2)\ntrips20['Member'] = trips20['User Type'].map(member2)\n\n# Dropping `user_type`\ntrips17.drop(['user_type'], axis=1, inplace=True)\ntrips18.drop(['user_type'], axis=1, inplace=True)\ntrips19.drop(['User Type'], axis=1, inplace=True)\ntrips20.drop(['User Type'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:24:34.00657Z","iopub.execute_input":"2021-07-01T22:24:34.006854Z","iopub.status.idle":"2021-07-01T22:24:36.247998Z","shell.execute_reply.started":"2021-07-01T22:24:34.00683Z","shell.execute_reply":"2021-07-01T22:24:36.24678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### *Issue 5: Add New Weekend & Holiday Features*","metadata":{}},{"cell_type":"code","source":"# Create new `Weekend` Boolean Column\ntrips17['Weekend'] = ((trips17['trip_start_time'].dt.dayofweek) > 5).astype(int)\ntrips18['Weekend'] = ((trips18['trip_start_time'].dt.dayofweek) > 5).astype(int)\ntrips19['Weekend'] = ((trips19['Start Time'].dt.dayofweek) > 5).astype(int)\ntrips20['Weekend'] = ((trips20['Start Time'].dt.dayofweek) > 5).astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:24:36.249848Z","iopub.execute_input":"2021-07-01T22:24:36.250274Z","iopub.status.idle":"2021-07-01T22:24:37.224708Z","shell.execute_reply.started":"2021-07-01T22:24:36.250241Z","shell.execute_reply":"2021-07-01T22:24:37.222862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Weekend Value Counts\nprint(f\"\"\"\ntrips17['Weekend'] value counts \\n\n{trips17['Weekend'].value_counts()}\\n\n========================================\\n\ntrips18['Weekend'] value counts \\n\n{trips18['Weekend'].value_counts()}\\n\n========================================\\n\ntrips19['Weekend'] value counts \\n\n{trips19['Weekend'].value_counts()}\\n\n========================================\\n\ntrips20['Weekend'] value counts \\n\n{trips20['Weekend'].value_counts()}\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:24:37.226482Z","iopub.execute_input":"2021-07-01T22:24:37.226814Z","iopub.status.idle":"2021-07-01T22:24:37.324978Z","shell.execute_reply.started":"2021-07-01T22:24:37.226778Z","shell.execute_reply":"2021-07-01T22:24:37.324047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import holidays\nfrom datetime import date\n\n# Instantiate Ontario Holidays\nont_holidays = holidays.CA(years=[2017,2018,2019,2020])\n\n# Create new `Holiday` Boolean Column\ntrips17['Holiday'] = pd.to_datetime(trips17['trip_start_time']).dt.date.isin(ont_holidays).astype(int)\ntrips18['Holiday'] = pd.to_datetime(trips18['trip_start_time']).dt.date.isin(ont_holidays).astype(int)\ntrips19['Holiday'] = pd.to_datetime(trips19['Start Time']).dt.date.isin(ont_holidays).astype(int)\ntrips20['Holiday'] = pd.to_datetime(trips20['Start Time']).dt.date.isin(ont_holidays).astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:24:37.326069Z","iopub.execute_input":"2021-07-01T22:24:37.326296Z","iopub.status.idle":"2021-07-01T22:24:44.098907Z","shell.execute_reply.started":"2021-07-01T22:24:37.326273Z","shell.execute_reply":"2021-07-01T22:24:44.098146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Holidays Value Counts\nprint(f\"\"\"\ntrips17['Holiday'] value counts \\n\n{trips17['Holiday'].value_counts()}\\n\n========================================\\n\ntrips18['Holiday'] value counts \\n\n{trips18['Holiday'].value_counts()}\\n\n========================================\\n\ntrips19['Holiday'] value counts \\n\n{trips19['Holiday'].value_counts()}\\n\n========================================\\n\ntrips20['Holiday'] value counts \\n\n{trips20['Holiday'].value_counts()}\\\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:24:44.099982Z","iopub.execute_input":"2021-07-01T22:24:44.100364Z","iopub.status.idle":"2021-07-01T22:24:44.201177Z","shell.execute_reply.started":"2021-07-01T22:24:44.100336Z","shell.execute_reply":"2021-07-01T22:24:44.199694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### *Issue 6: Rename Columns for Consistency*","metadata":{}},{"cell_type":"code","source":"# OPTION 1 (If duration seconds was dropped)\n# Renaming the columns to match trips19 & trips20 datasets\n\n# trips17.rename(columns={'trip_id':'Trip Id',\n#                         'trip_start_time':'Start Time',\n#                         'trip_stop_time':'End Time',\n#                         'from_station_id':'Start Station Id',\n#                         'from_station_name':'Start Station Name',\n#                         'to_station_id':'End Station Id',\n#                         'to_station_name':'End Station Name'}, inplace=True)\n\n# trips18.rename(columns={'trip_id':'Trip Id',\n#                         'trip_start_time':'Start Time',\n#                         'trip_stop_time':'End Time',\n#                         'from_station_id':'Start Station Id',\n#                         'from_station_name':'Start Station Name',\n#                         'to_station_id':'End Station Id',\n#                         'to_station_name':'End Station Name'}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:24:44.202755Z","iopub.execute_input":"2021-07-01T22:24:44.203039Z","iopub.status.idle":"2021-07-01T22:24:44.20762Z","shell.execute_reply.started":"2021-07-01T22:24:44.203011Z","shell.execute_reply":"2021-07-01T22:24:44.206363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# OPTION 2 (Keep trip duration seconds)\n# Renaming the columns to match trips19 & trips20 datasets\n\ntrips17.rename(columns={'trip_id':'Trip Id',\n                        'trip_start_time':'Start Time',\n                        'trip_stop_time':'End Time',\n                        'from_station_id':'Start Station Id',\n                        'from_station_name':'Start Station Name',\n                        'to_station_id':'End Station Id',\n                        'to_station_name':'End Station Name',\n                        'trip_duration_seconds':'Trip Duration (Seconds)'}, inplace=True)\n\ntrips18.rename(columns={'trip_id':'Trip Id',\n                        'trip_start_time':'Start Time',\n                        'trip_stop_time':'End Time',\n                        'from_station_id':'Start Station Id',\n                        'from_station_name':'Start Station Name',\n                        'to_station_id':'End Station Id',\n                        'to_station_name':'End Station Name',\n                        'trip_duration_seconds':'Trip Duration (Seconds)'}, inplace=True)\n\ntrips19.rename(columns={'Trip  Duration':'Trip Duration (Seconds)'}, inplace=True)\n\ntrips20.rename(columns={'Trip  Duration':'Trip Duration (Seconds)'}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:24:44.209016Z","iopub.execute_input":"2021-07-01T22:24:44.209305Z","iopub.status.idle":"2021-07-01T22:24:44.225823Z","shell.execute_reply.started":"2021-07-01T22:24:44.209277Z","shell.execute_reply":"2021-07-01T22:24:44.224635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final check on datatypes\nprint(f\"\"\"\\\n**trips17 dataset**\\n\n{trips17.dtypes}\\n\n========================================\\n\n**trips18 dataset**\\n\n{trips18.dtypes}\\n\n========================================\\n\n**trips19 dataset**\\n\n{trips19.dtypes}\\n\n========================================\\n\n**trips20 dataset**\\n\n{trips20.dtypes}\\\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:24:44.226911Z","iopub.execute_input":"2021-07-01T22:24:44.227252Z","iopub.status.idle":"2021-07-01T22:24:44.253553Z","shell.execute_reply.started":"2021-07-01T22:24:44.227224Z","shell.execute_reply":"2021-07-01T22:24:44.251773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### *Issue 7: Concatenate Dataframes*","metadata":{}},{"cell_type":"code","source":"# Combine the trip datasets\ntrips17_20 = pd.concat([trips17,trips18,trips19,trips20], ignore_index=True)\n\n# Dropping columns from trips19 & trips20 datasets where we have no data in trips17 & trips18\n# i.e. 'Subscription Id' & 'Bike Id'\ntrips17_20.drop(['Subscription Id', 'Bike Id'], axis=1, inplace=True)\n\n# Note: We have future opportunity to explore further into tracking 'Subscritption ID' & 'Bike Id\n#       by creating a separate dataframe for trip19 & trip20\ntrips19_20 = pd.concat([trips19, trips20], ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:24:44.25466Z","iopub.execute_input":"2021-07-01T22:24:44.255029Z","iopub.status.idle":"2021-07-01T22:24:46.068691Z","shell.execute_reply.started":"2021-07-01T22:24:44.254986Z","shell.execute_reply":"2021-07-01T22:24:46.067709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reorganize columns\ntrips17_20 = trips17_20[['Trip Id',\n                         'Trip Duration (Seconds)',\n#                          `Trip Duration (Minutes),\n                         'Start Time',\n                         'Start Station Id',\n                         'Start Station Name',\n                         'End Time',\n                         'End Station Id',\n                         'End Station Name',\n                         'Member',\n                         'Weekend',\n                         'Holiday']]\n\ntrips19_20 = trips19_20[['Trip Id',\n                         'Subscription Id',\n                         'Bike Id',\n                         'Trip Duration (Seconds)',\n#                          `Trip Duration (Minutes),\n                         'Start Time',\n                         'Start Station Id',\n                         'Start Station Name',\n                         'End Time',\n                         'End Station Id',\n                         'End Station Name',\n                         'Member',\n                         'Weekend',\n                         'Holiday']]","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:24:46.069852Z","iopub.execute_input":"2021-07-01T22:24:46.070139Z","iopub.status.idle":"2021-07-01T22:24:46.903757Z","shell.execute_reply.started":"2021-07-01T22:24:46.07011Z","shell.execute_reply":"2021-07-01T22:24:46.903042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if concatting is successful\nprint(f\"\"\"\ntrips17_20 dataframe shape: {trips17_20.shape}\ntrips19_20 dataframe shape: {trips19_20.shape}\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:24:46.904821Z","iopub.execute_input":"2021-07-01T22:24:46.905216Z","iopub.status.idle":"2021-07-01T22:24:46.909694Z","shell.execute_reply.started":"2021-07-01T22:24:46.905181Z","shell.execute_reply":"2021-07-01T22:24:46.908718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final check for duplicates\nprint(f\"\"\"\nDuplicates found in trips17_20: {trips17_20.duplicated().sum()}\nDuplicates found in trips19_20: {trips19_20.duplicated().sum()}\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:24:46.910806Z","iopub.execute_input":"2021-07-01T22:24:46.9111Z","iopub.status.idle":"2021-07-01T22:25:02.969206Z","shell.execute_reply.started":"2021-07-01T22:24:46.911069Z","shell.execute_reply":"2021-07-01T22:25:02.968265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final check for null values\nprint(f\"\"\"\n**trips17_20 dataset**\\n\nTotal Number of Null values:\\n\\n{trips17_20.isna().sum()}\\n\n========================================\\n\n**trips19_20 dataset**\\n\nTotal Number of Null values:\\n\\n{trips19_20.isna().sum()}\\\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:25:02.982652Z","iopub.execute_input":"2021-07-01T22:25:02.983014Z","iopub.status.idle":"2021-07-01T22:25:05.779408Z","shell.execute_reply.started":"2021-07-01T22:25:02.982981Z","shell.execute_reply":"2021-07-01T22:25:05.778689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Next Step_**<br>\nNow that the datasets are cleaned, lets run a quick visualization of each column feature to see what kind of insights we can find.","metadata":{}},{"cell_type":"code","source":"# Ignore runtime warning\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set plot style\nsns.set_style(\"white\")\n\n# Show histogram for each numeric column\nfor col in trips17_20.columns[1:]:\n    try:\n        print(f\"\"\"\n---------------------------------------------\n{col}\n---------------------------------------------\n        \"\"\")\n        sns.set_style(\"white\")\n        plt.figure(figsize=(6, 4))\n        plt.title(col)\n        plt.hist(trips17_20[col], color='cornflowerblue', bins=100)\n        plt.xlabel(col)\n        sns.despine()\n        plt.show()\n    except KeyError:\n        pass","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:25:05.781399Z","iopub.execute_input":"2021-07-01T22:25:05.78169Z","iopub.status.idle":"2021-07-01T22:26:13.512443Z","shell.execute_reply.started":"2021-07-01T22:25:05.78166Z","shell.execute_reply":"2021-07-01T22:26:13.511361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Observations_**\n- `Trip Duration Seconds` is right skewed with a long tail which resulted in the visualization tightly binned to one large bin on the left, we may need to limit of cut off the long tail with a max value to produce a usable visualization in the EDA phase.\n- There is a clear seasonality displayed in the Start Time and End Time columns based on bike usage trend with summer highs and winter lows and steady upward trend over the years.\n- Most usage happens in the older stations and tapers off with the newer stations\n- There are noticeable differences between Station Ids and Station Names possibly because of the moving and renaming of stations or typos. This will need to be looked into in the future.","metadata":{}},{"cell_type":"code","source":"# Save cleaned datasets to new csv\n# trips17_20.to_csv('../data/processed/trips17_20.csv') \n# trips19_20.to_csv('../data/processed/trips19_20.csv') ","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:13.51379Z","iopub.execute_input":"2021-07-01T22:26:13.514079Z","iopub.status.idle":"2021-07-01T22:26:13.519811Z","shell.execute_reply.started":"2021-07-01T22:26:13.514054Z","shell.execute_reply":"2021-07-01T22:26:13.518865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Part 2: Weather Datasets**\n\nBack to [Table of Contents](#Table-of-Contents).<br>\n\n~1. Trip Datasets~<br>\n2. Weather Datasets<br>\n3. API Datasets","metadata":{}},{"cell_type":"code","source":"# Combine csvs into one dataframe 'weather17'\nweather17 = pd.concat([W1_17, W2_17, W3_17, W4_17, W5_17, W6_17, W7_17, W8_17, W9_17, W10_17, W11_17, W12_17]).reset_index(drop=True)\nweather18 = pd.concat([W1_18, W2_18, W3_18, W4_18, W5_18, W6_18, W7_18, W8_18, W9_18, W10_18, W11_18, W12_18]).reset_index(drop=True)\nweather19 = pd.concat([W1_19, W2_19, W3_19, W4_19, W5_19, W6_19, W7_19, W8_19, W9_19, W10_19, W11_19, W12_19]).reset_index(drop=True)\nweather20 = pd.concat([W1_20, W2_20, W3_20, W4_20, W5_20, W6_20, W7_20, W8_20, W9_20, W10_20, W11_20, W12_20]).reset_index(drop=True)\n\n# Display dataset size\nprint(f\"\"\"\n2017 Weather dataframe shape: {weather17.shape}\n2018 Weather dataframe shape: {weather18.shape}\n2019 Weather dataframe shape: {weather19.shape}\n2020 Weather dataframe shape: {weather20.shape}\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:13.520745Z","iopub.execute_input":"2021-07-01T22:26:13.520946Z","iopub.status.idle":"2021-07-01T22:26:13.636519Z","shell.execute_reply.started":"2021-07-01T22:26:13.520923Z","shell.execute_reply":"2021-07-01T22:26:13.635244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Observation_**\n\n2020 has 24 more records because it was a leap year.","metadata":{}},{"cell_type":"code","source":"# Display first 3 rows\nprint(\"\\n**weather17 dataset**\")\ndisplay(weather17.head(3))\nprint(\"\\n**weather18 dataset**\")\ndisplay(weather18.head(3))\nprint(\"\\n**weather19 dataset**\")\ndisplay(weather19.head(3))\nprint(\"\\n**weather20 dataset**\")\ndisplay(weather20.head(3))","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:13.638343Z","iopub.execute_input":"2021-07-01T22:26:13.638801Z","iopub.status.idle":"2021-07-01T22:26:13.745217Z","shell.execute_reply.started":"2021-07-01T22:26:13.638759Z","shell.execute_reply":"2021-07-01T22:26:13.743677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Observation_**\n\nIt appears that there are a lot of `null` values in multiple column features. But lets take a look at the datatypes before making any further observations and next steps.","metadata":{}},{"cell_type":"code","source":"# Check each dataframe's datatypes\nprint(f\"\"\"\\\n**weather17 dataset**\\n\n{weather17.dtypes}\\n\n========================================\\n\n**weather18 dataset**\\n\n{weather18.dtypes}\\n\n========================================\\n\n**weather19 dataset**\\n\n{weather19.dtypes}\\n\n========================================\\n\n**weather20 dataset**\\n\n{weather20.dtypes}\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:13.746206Z","iopub.execute_input":"2021-07-01T22:26:13.746403Z","iopub.status.idle":"2021-07-01T22:26:13.757594Z","shell.execute_reply.started":"2021-07-01T22:26:13.746381Z","shell.execute_reply":"2021-07-01T22:26:13.756879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Initial Observations**\n- We can drop the `Longitude (x)`, `Latitude (y)`, `Station Name`, `Climate ID` as they relate specifically with weather station\n- We can drop the *Flag* features as they are indicators for missing data which is useful but not necessary for us\n- We can also drop `Wind Dir (10s deg)` & Wind speed `Wind Spd (km/h)` because this metric will vary in different parts of the city greatly depending on terrain, i.e. open field vs city streets. However, we can create a boolean classifier for windy conditions over 60km/h sustained before we drop these features\n- We will conconcating this weather data with the trips data via `Date/Time`, therefore the `Year`, `Month`, `Day`, `Time` can be dropped as it is redundant\n- `null` values for Temperature `Temp (째C)`, Dew Point `Dew Point Temp (째C)`, Relative Humidity `Rel Hum (%)`, and Atmopheric Pressure `Stn Press (kPa)` can be imputed via ffill to keep it simple\n- According to Meteorological Standards, visibility reported as 10 miles is normally a stand-in for \"unlimited\" visibility, therefore Visibility (km) can have a maximum of 16.1 on a clear day. Therefore we will be replacing all `null` values with the max value of 16.1\n- `Hmdx` & `Wind Chill` impute 0 for `null` as they are no longer measured in cold/hot weather respectively\n- `Weather` has multiple weather observations that can be broken down into boolean classifier using `CountVectorizer` to create sparse (boolean) matrix \n\n**Next Steps**<br>\n1. Since all the weather dataframes are the same, we can concat them together before doing any data cleaning.\n2. Create boolean variable for `Strong Winds`\n3. Drop unnecassary/redundant features\n4. Create Sparse Matrix for `Weather` Observations\n5. Check for duplicates or `null` values\n6. Impute `null` values\n7. Change the dtypes for `Date/Time` from object strings to datetime","metadata":{}},{"cell_type":"markdown","source":"#### *Issue 1: Combine dataframes*","metadata":{}},{"cell_type":"code","source":"# Combine weather dataframes into 1\nweather_df = pd.concat([weather17,weather18,weather19,weather20], ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:13.759396Z","iopub.execute_input":"2021-07-01T22:26:13.759844Z","iopub.status.idle":"2021-07-01T22:26:13.808325Z","shell.execute_reply.started":"2021-07-01T22:26:13.759801Z","shell.execute_reply":"2021-07-01T22:26:13.807296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### *Issue 2: Create Feature for Strong Winds*","metadata":{}},{"cell_type":"code","source":"# Create a new binary column with a 1 if Wind Spd (km/h) >= 60, 0 if Wind Spd (km/h) < 60\nweather_df['Strong Wind'] = (weather_df['Wind Spd (km/h)']>=60).astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:13.809637Z","iopub.execute_input":"2021-07-01T22:26:13.809981Z","iopub.status.idle":"2021-07-01T22:26:13.817007Z","shell.execute_reply.started":"2021-07-01T22:26:13.809944Z","shell.execute_reply":"2021-07-01T22:26:13.816047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### *Issue 3: Drop Features*","metadata":{}},{"cell_type":"code","source":"# Dropping features\nweather_df.drop(['Longitude (x)', \n                 'Latitude (y)', \n                 'Station Name', \n                 'Climate ID',\n                 'Year', \n                 'Month', \n                 'Day', \n                 'Time',\n                 'Temp Flag', \n                 'Dew Point Temp Flag', \n                 'Rel Hum Flag', \n                 'Wind Dir (10s deg)',\n                 'Wind Dir Flag',\n                 'Wind Spd (km/h)',\n                 'Wind Spd Flag', \n                 'Visibility Flag',\n                 'Stn Press Flag', \n                 'Hmdx Flag',\n                 'Wind Chill Flag'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:13.817893Z","iopub.execute_input":"2021-07-01T22:26:13.818162Z","iopub.status.idle":"2021-07-01T22:26:13.845456Z","shell.execute_reply.started":"2021-07-01T22:26:13.818134Z","shell.execute_reply":"2021-07-01T22:26:13.843896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### *Issue 4: Create Sparse Matrix for Weather Observations*","metadata":{}},{"cell_type":"code","source":"# Verify if we can make column into boolean variable\nprint(f\"\"\"\nVariables found in Weather:\\n {weather_df['Weather'].unique()}\\\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:13.848274Z","iopub.execute_input":"2021-07-01T22:26:13.848767Z","iopub.status.idle":"2021-07-01T22:26:13.855506Z","shell.execute_reply.started":"2021-07-01T22:26:13.848742Z","shell.execute_reply":"2021-07-01T22:26:13.854279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Observation_**\n\nLooks like there are definitely several re-occuring variables that can be transformed into a sparse matrix. Next I will import a NLP tool that will help in transforming the column feature into a sparse matrix then concating it back into the weather dataframe.","metadata":{}},{"cell_type":"code","source":"# Import CountVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:13.856454Z","iopub.execute_input":"2021-07-01T22:26:13.856755Z","iopub.status.idle":"2021-07-01T22:26:13.907542Z","shell.execute_reply.started":"2021-07-01T22:26:13.856723Z","shell.execute_reply":"2021-07-01T22:26:13.906616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fill in `NULL` values with a stand in value\nweather_df['Weather'].fillna(value='clear', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:13.908702Z","iopub.execute_input":"2021-07-01T22:26:13.908986Z","iopub.status.idle":"2021-07-01T22:26:13.915753Z","shell.execute_reply.started":"2021-07-01T22:26:13.908956Z","shell.execute_reply":"2021-07-01T22:26:13.914675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1. Instantiate the CountVectorizer with a token pattern\nweather_code = CountVectorizer(token_pattern=r'(?u)[a-zA-Z][a-z ]+')\n\n# 2. Fit & Transform column feature with the CountVectorizer\nweather_code_f_t = weather_code.fit_transform(weather_df['Weather'])\n\n# 3. Print Results\nprint(\"Size of weather_code_f_t\\n\")\ndisplay(weather_code_f_t)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:13.916845Z","iopub.execute_input":"2021-07-01T22:26:13.917145Z","iopub.status.idle":"2021-07-01T22:26:14.17779Z","shell.execute_reply.started":"2021-07-01T22:26:13.917105Z","shell.execute_reply":"2021-07-01T22:26:14.176821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List of feature names\nprint(weather_code.get_feature_names())","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:14.178807Z","iopub.execute_input":"2021-07-01T22:26:14.179027Z","iopub.status.idle":"2021-07-01T22:26:14.183176Z","shell.execute_reply.started":"2021-07-01T22:26:14.179003Z","shell.execute_reply":"2021-07-01T22:26:14.182596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate into dataframe\nweather_code_df = pd.DataFrame(columns=weather_code.get_feature_names(), \n                               data=weather_code_f_t.toarray())\n\n# Drop stand in value\nweather_code_df.drop(['clear'], axis=1, inplace=True)\n\n# Change Titlecase on Column Names\nweather_code_df.columns = map(str.title, weather_code_df.columns)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:14.18426Z","iopub.execute_input":"2021-07-01T22:26:14.184649Z","iopub.status.idle":"2021-07-01T22:26:14.204995Z","shell.execute_reply.started":"2021-07-01T22:26:14.184598Z","shell.execute_reply":"2021-07-01T22:26:14.20358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Overwrite previous weather_df with additional weather_code\nweather_df = pd.concat([weather_df, weather_code_df], join='inner', axis=1)\n\n# Drop `Weather` feature\nweather_df.drop(['Weather'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:14.2064Z","iopub.execute_input":"2021-07-01T22:26:14.20689Z","iopub.status.idle":"2021-07-01T22:26:14.222865Z","shell.execute_reply.started":"2021-07-01T22:26:14.206853Z","shell.execute_reply":"2021-07-01T22:26:14.221609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### *Issue 5: Check for Duplicates and Missing Values*","metadata":{}},{"cell_type":"code","source":"# Check for duplicates\nprint(f\"Duplicates found in weather_df: {weather_df.duplicated().sum()}\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:14.223713Z","iopub.execute_input":"2021-07-01T22:26:14.223907Z","iopub.status.idle":"2021-07-01T22:26:14.254806Z","shell.execute_reply.started":"2021-07-01T22:26:14.223885Z","shell.execute_reply":"2021-07-01T22:26:14.253314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for null values\nprint(f\"\"\"\n**weather_df dataset**\\n\nTotal Number of Null values:\\n\\n{weather_df.isna().sum()}\\n\n----------------------------------------\\n\nTotal Percentage of Null values:\\n\\n{weather_df.isna().mean()*100}\\\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:14.256131Z","iopub.execute_input":"2021-07-01T22:26:14.256473Z","iopub.status.idle":"2021-07-01T22:26:14.271271Z","shell.execute_reply.started":"2021-07-01T22:26:14.256427Z","shell.execute_reply":"2021-07-01T22:26:14.270261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### *Issue 6: Impute Missing Values*\n- Impute `Visibility (km)` with 16.1\n- Impute `Hmdx` & `Wind Chill` with 0\n- Impute `Temp (째C)`, `Dew Point Temp (째C)`, `Rel Hum (%)`, and `Stn Press (kPa)` via ffill","metadata":{}},{"cell_type":"code","source":"# Impute `Visibility (km)` with 16.1; `Hmdx` & `Wind Chill` with 0\nweather_df.fillna({'Visibility (km)': 16.1, 'Hmdx': 0, 'Wind Chill': 0}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:14.272305Z","iopub.execute_input":"2021-07-01T22:26:14.272547Z","iopub.status.idle":"2021-07-01T22:26:14.283059Z","shell.execute_reply.started":"2021-07-01T22:26:14.272523Z","shell.execute_reply":"2021-07-01T22:26:14.281613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Forwardfill any null values\nweather_df.fillna(method='ffill', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:14.284461Z","iopub.execute_input":"2021-07-01T22:26:14.284788Z","iopub.status.idle":"2021-07-01T22:26:14.299063Z","shell.execute_reply.started":"2021-07-01T22:26:14.284759Z","shell.execute_reply":"2021-07-01T22:26:14.297978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final check for null values\nprint(f\"\"\"\n**weather_df dataset**\\n\nTotal Number of Null values:\\n\\n{weather_df.isna().sum()}\\\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:14.300297Z","iopub.execute_input":"2021-07-01T22:26:14.300645Z","iopub.status.idle":"2021-07-01T22:26:14.321877Z","shell.execute_reply.started":"2021-07-01T22:26:14.300614Z","shell.execute_reply":"2021-07-01T22:26:14.320555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### *Issue 7: Change Datatypes*","metadata":{}},{"cell_type":"code","source":"# Datatype check\nweather_df.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:14.323125Z","iopub.execute_input":"2021-07-01T22:26:14.323388Z","iopub.status.idle":"2021-07-01T22:26:14.337053Z","shell.execute_reply.started":"2021-07-01T22:26:14.323361Z","shell.execute_reply":"2021-07-01T22:26:14.335576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Changing `Date/Time` and `Time` dtype from object to datetime\nweather_df['Date/Time'] = pd.to_datetime(weather_df['Date/Time'], yearfirst=True, infer_datetime_format=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:14.338831Z","iopub.execute_input":"2021-07-01T22:26:14.33921Z","iopub.status.idle":"2021-07-01T22:26:14.371413Z","shell.execute_reply.started":"2021-07-01T22:26:14.339176Z","shell.execute_reply":"2021-07-01T22:26:14.370384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final datatype check\nweather_df.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:14.372507Z","iopub.execute_input":"2021-07-01T22:26:14.372722Z","iopub.status.idle":"2021-07-01T22:26:14.389562Z","shell.execute_reply.started":"2021-07-01T22:26:14.372698Z","shell.execute_reply":"2021-07-01T22:26:14.388344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Next Step_**<br>\nNow that the dataset are cleaned, lets run a quick visualization of each column feature to see what kind of insights we can find.","metadata":{}},{"cell_type":"code","source":"for col in weather_df.columns[1:]:\n    try:\n        print(f\"\"\"\n---------------------------------------------\n{col}\n---------------------------------------------\n        \"\"\")\n        plt.figure(figsize=(6, 4))\n        plt.title(col)\n        plt.hist(weather_df[col], color='cornflowerblue', bins=80)\n        plt.xlabel(col)\n        sns.despine()\n        plt.show()\n    except KeyError:\n        pass","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:14.390525Z","iopub.execute_input":"2021-07-01T22:26:14.390716Z","iopub.status.idle":"2021-07-01T22:26:19.976333Z","shell.execute_reply.started":"2021-07-01T22:26:14.390695Z","shell.execute_reply":"2021-07-01T22:26:19.975482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Observations_**\n- Most common `Temp (째C)` peaks around 2째C and around 21째C\n- Most common `Dew Point Temp (째C)` peaks around 0째C and drops off above 20째C and distribution has a left skew\n- Most common `Rel Hum (%)` peaks around 75% and distribution has a left skew\n- `Stn Press (kPa)` has a normal distribution with the peak between 100-101kPa\n- Normal Rain, Snow and Fog are the most common Weather Events","metadata":{}},{"cell_type":"code","source":"# Save cleaned dataset to new csv\n# weather_df.to_csv('../data/processed/weather17_20.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:19.977496Z","iopub.execute_input":"2021-07-01T22:26:19.977771Z","iopub.status.idle":"2021-07-01T22:26:19.981135Z","shell.execute_reply.started":"2021-07-01T22:26:19.977742Z","shell.execute_reply":"2021-07-01T22:26:19.98031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Part 3: Loading API Datasets\n\nBack to [Table of Contents](#Table-of-Contents).<br>\n\n~1. Trip Datasets~<br>\n~2. Weather Datasets~<br>\n3. API Datasets\n\n**_Datasets_**<br>\n\nToronto BikeShare API Links and description listed below:<br>\n\n[System Regions](https://tor.publicbikesystem.net/ube/gbfs/v1/en/system_regions)<br>\nJSON on Toronto BikeShare System Region\n\n[System Information](https://tor.publicbikesystem.net/ube/gbfs/v1/en/system_information)<br>\nJSON on Toronto BikeShare System Information\n\n[Station Information](https://tor.publicbikesystem.net/ube/gbfs/v1/en/station_information)<br>\nJSON on the current Toronto BikeShare Station Information including the geo-coordinates, altitude, and capacity\n\n[Station Status](https://tor.publicbikesystem.net/ube/gbfs/v1/en/station_status)<br>\nJSON on the current Toronto BikeShare Station Status including on the number of bikes docks\n\n[System Pricing Plans](https://tor.publicbikesystem.net/ube/gbfs/v1/en/system_pricing_plans)<br>\nJSON on the current Toronto BikeShare Pricing Plan\n\n\nOut of all the different APIs listed, I believe the ones that are the most useful would be `station_information` and `system_pricing_plans`. We will be requesting them individually, cleaning the datasets, and determine if they are useful in our analysis.\n\nThe [requests](https://requests.readthedocs.io/en/master/) module allows us to write a custom wrapper around a web API. Many web sites provide a way of interacting with them programmatically through an API. The way it generally works is that you send a request to a certain URL, and the data is returned as a JSON response.\n\n**_Next Steps_**\n1. Request & Clean Station Information\n2. Request & Clean System Pricing Plans","metadata":{}},{"cell_type":"markdown","source":"### API 1: Station Information","metadata":{}},{"cell_type":"code","source":"# Import API requests and time\nimport requests\nimport time\n\n# Counter\ni = 1\nprint(f'API Request attempt {i}:')\n\n# Request BikeShare Stations Information\nrequest1 = requests.get('https://tor.publicbikesystem.net/ube/gbfs/v1/en/station_information')\n\n# Loop if status code != 200 then retry with delay.\nwhile request1.status_code != 200:\n    print('Failed!')\n    i += 1\n    print(f'API Request attempt {i}:')\n    request1 = requests.get('https://tor.publicbikesystem.net/ube/gbfs/v1/en/station_information')\n    if request1.status_code == 200:\n        break\n    else:\n        continue\n    time.sleep(5)\n    \nprint(f\"Successful!\\nContent-Type is: {request1.headers['Content-Type']}\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:19.982126Z","iopub.execute_input":"2021-07-01T22:26:19.982504Z","iopub.status.idle":"2021-07-01T22:26:20.614708Z","shell.execute_reply.started":"2021-07-01T22:26:19.98235Z","shell.execute_reply":"2021-07-01T22:26:20.613274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# If unable to reach API on mutiple attempts, then \n# load in the following csv from when it was last successful.\n\n# station_info = pd.read_csv('..data/raw/station_info.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:20.615971Z","iopub.execute_input":"2021-07-01T22:26:20.616257Z","iopub.status.idle":"2021-07-01T22:26:20.620111Z","shell.execute_reply.started":"2021-07-01T22:26:20.616228Z","shell.execute_reply":"2021-07-01T22:26:20.618909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert json to a dataframe\nstation_info = pd.DataFrame(request1.json())\nstation_info","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:20.621288Z","iopub.execute_input":"2021-07-01T22:26:20.621714Z","iopub.status.idle":"2021-07-01T22:26:20.792004Z","shell.execute_reply.started":"2021-07-01T22:26:20.621678Z","shell.execute_reply":"2021-07-01T22:26:20.791336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check last updated time\nfrom datetime import datetime\n\ntime = station_info.iloc[0]['last_updated']\n\nprint(f\"Data last updated:\\n{datetime.fromtimestamp(time).strftime('%Y-%m-%d %I:%M:%S %p')}\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:20.793085Z","iopub.execute_input":"2021-07-01T22:26:20.793528Z","iopub.status.idle":"2021-07-01T22:26:20.799164Z","shell.execute_reply.started":"2021-07-01T22:26:20.793493Z","shell.execute_reply":"2021-07-01T22:26:20.797924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Observation_**<br>\nIt appears that all the data is in in a single dictionary record under the data column. Lets extract/transform that dictionary into a workable data frame.","metadata":{}},{"cell_type":"code","source":"# Transform dictionary to dataframe\nstation_info = pd.DataFrame(station_info.iloc[0]['data'])\n\n# Check the data set\nstation_info.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:20.800548Z","iopub.execute_input":"2021-07-01T22:26:20.80089Z","iopub.status.idle":"2021-07-01T22:26:20.839674Z","shell.execute_reply.started":"2021-07-01T22:26:20.800859Z","shell.execute_reply":"2021-07-01T22:26:20.838625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Observations_**<br>\nIt appears that the transformation worked. However some data features are ambigious and needs more in-depth analysis to ascertain the nature of each feature. But first, lets export this dataframe in case. weare unable to reach the API source.","metadata":{}},{"cell_type":"code","source":"# Export to csv in case unable to reach API\n# station_info.to_csv('../data/raw/station_info.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:20.841002Z","iopub.execute_input":"2021-07-01T22:26:20.841387Z","iopub.status.idle":"2021-07-01T22:26:20.853279Z","shell.execute_reply.started":"2021-07-01T22:26:20.841356Z","shell.execute_reply":"2021-07-01T22:26:20.851844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the size of data set\nstation_info.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:20.854957Z","iopub.execute_input":"2021-07-01T22:26:20.855382Z","iopub.status.idle":"2021-07-01T22:26:20.869966Z","shell.execute_reply.started":"2021-07-01T22:26:20.855342Z","shell.execute_reply":"2021-07-01T22:26:20.867879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print out value counts for each data column.\n\nfor col in station_info.columns:\n    try:\n        print(f\"\"\"\n------------------------------\n{col}\n------------------------------\n\n{station_info[col].value_counts()}\n        \"\"\")\n    except TypeError:\n        pass","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:20.871241Z","iopub.execute_input":"2021-07-01T22:26:20.871533Z","iopub.status.idle":"2021-07-01T22:26:20.917011Z","shell.execute_reply.started":"2021-07-01T22:26:20.871504Z","shell.execute_reply":"2021-07-01T22:26:20.915862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As of July 1 2021, there are 608 active bike stations at the time of API request across Toronto with 14 features(columns). This may vary depending on when the data is requested. The description of each feature is listed below:\n\n- `station_id`: Unique Station ID\n- `name`: Name of Bike Station\n- `physical_configuration`: Type of Bike Station\n- `lat`: Latitude coordinates of Bike Station\n- `lon`: Longitude coordinates of Bike Station\n- `address`: Address of where the Bike Station is located\n- `capacity`: Max number of bikes the station can accommodate\n- `rental_methods`: Payment Methods Available\n- `groups`: Bike Station Group ID\n- `obcn`: On-board Communication Number, the unique phone number at which the station communicates remotely\n- `nearby_distance`: Based on heading, it is the distance nearby some unknown reference point\n- `post_code`: Postal Code of where the Bike Station is located\n- `cross_street`: Based on heading, it may be the intersection at which the Bikestation is located\n\nOut of the features listed above, we are only interested in `station_id`, `name`, `lat`, `lon`, `capacity` `post_code` as these can help us identify the locations of the stations and their corresponding capacity currently. All other features will be dropped.\n\n`physical_configuration`<br>\nPhysical Configuration seems to refer to the type of bike station it is and what type of map frame is attached to the station itself which may or maynot help riders navigate through the city if they are not familiar. Since there is only 5 distinct physical configurations for stations but majority of which are regular configuration I do not believe there is any bearing on bike usage and therefore can be dropped.\n\n`address`<br>\nIt appears that the `address` is just a copy of the `name` feature and can be dropped.\n\n`rental_methods`<br>\nIt appears all the stations take phone, key and transitcard as rental methods they are not useful values to differentiate the stations however 92 stations do not take creditcard so that we can encode that feature and change it to binary classification. \n\n`groups`<br>\nIt appears that this feature is mainly used for internal usage to classify station groupings and have no relation in terms of location, and most likly have no bearing into bike usage statistics, therefore can be dropped.\n\n`obcn`<br>\nIt appears to be the internal device communication number and has no bearing on trip rentals and can be dropped.<br>\n\n`nearby_distance`<br>\nThe measure is ambiguous in terms of reference point, therefore unusable and can be dropped.\n\n`cross_street`<br>\nIt is not clear as to what this feature refers to or classifies as cross_street. As it may seem redundant to the address, it can be dropped.<br>\n\n**Next Steps**\n1. Drop features\n2. Check for duplicates and nulls\n3. Create binary classifier for Rental Methods\n4. Check/Change dtypes","metadata":{}},{"cell_type":"markdown","source":"#### *Issue 1: Drop Features*","metadata":{}},{"cell_type":"code","source":"# Dropping features\nstation_info.drop(['address',\n                   'physical_configuration', \n                   'groups', \n                   'obcn', \n                   'nearby_distance', \n                   'cross_street'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:20.918382Z","iopub.execute_input":"2021-07-01T22:26:20.918754Z","iopub.status.idle":"2021-07-01T22:26:20.925689Z","shell.execute_reply.started":"2021-07-01T22:26:20.918715Z","shell.execute_reply":"2021-07-01T22:26:20.924479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### *Issue 2: Check Duplicates & Missing Values*","metadata":{}},{"cell_type":"code","source":"# Check for duplicates\nprint(f\"Duplicates found: {station_info['station_id'].duplicated().sum()}\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:20.926881Z","iopub.execute_input":"2021-07-01T22:26:20.927164Z","iopub.status.idle":"2021-07-01T22:26:20.940055Z","shell.execute_reply.started":"2021-07-01T22:26:20.927131Z","shell.execute_reply":"2021-07-01T22:26:20.939331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for null values\nprint(f\"\"\"\nTotal Number of Null values:\\n\\n{station_info.isna().sum()}\\n\n----------------------------------------\\n\nTotal Percentage of Null values:\\n\\n{station_info.isna().mean()*100}\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:20.94123Z","iopub.execute_input":"2021-07-01T22:26:20.941509Z","iopub.status.idle":"2021-07-01T22:26:20.95938Z","shell.execute_reply.started":"2021-07-01T22:26:20.941477Z","shell.execute_reply":"2021-07-01T22:26:20.957717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Initial Observations**\n\n- 10 (~1.6%) missing data for `altitude`\n- 326 (~53.6%) missing data for `post_code`\n\n**Next Steps**\n\n~1. Drop features.~<br>\n~2. Check for duplicates and null values.~<br>\n3. Create binary classifier for Rental Methods.<br>\n4. Impute `altitude`. We can use [Open Elevation API](https://api.open-elevation.com/api/v1/lookup) to request elevation information based on `lat`, `lon` information. More information/documentation is available [here](https://github.com/Jorl17/open-elevation/blob/master/docs/api.md). Altitude and elevation are often used interchangeably where both are vertical measures using sea level as the start point.<br>\n5. Impute `post_code`/`FSA`.  We can use [GeoPy](https://geopy.readthedocs.io/en/stable/) to impute the missing postal code values by using the geo-coordinates, but also group it together by FSA (Forward Sortation Area) to gain more insight by higher level more aggregated neighbourhood area.<br>\n6. Check/Change dtypes","metadata":{}},{"cell_type":"markdown","source":"#### *Issue 3: Create Binary Classifier for Rental Methods*","metadata":{}},{"cell_type":"code","source":"# Check if value type is string or list\ntype(station_info['rental_methods'].iloc[0,])","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:20.960622Z","iopub.execute_input":"2021-07-01T22:26:20.961008Z","iopub.status.idle":"2021-07-01T22:26:20.975724Z","shell.execute_reply.started":"2021-07-01T22:26:20.960979Z","shell.execute_reply":"2021-07-01T22:26:20.974616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert to lowercase\nstation_info['rental_methods'] = station_info['rental_methods'].apply(lambda x: [rental_methods.lower() for rental_methods in x])\n\n# Combine all tags into a single list, using nested list comprehension\nmethod_list = [x for sublist in station_info['rental_methods'] for x in sublist]\n\n# Display the list & count\npd.Series(method_list).value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:20.977145Z","iopub.execute_input":"2021-07-01T22:26:20.977521Z","iopub.status.idle":"2021-07-01T22:26:20.995885Z","shell.execute_reply.started":"2021-07-01T22:26:20.977487Z","shell.execute_reply":"2021-07-01T22:26:20.994964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Observations_**<br>\nIt appears all the stations take phone, key and transitcard as rental methods they are not useful values to differentiate the stations however 92 stations do not take creditcard so that we can encode that feature and change it to binary classification. ","metadata":{}},{"cell_type":"code","source":"# Encode rental_methods to binary change to integer\nstation_info['creditcard_payment'] = station_info['rental_methods'].apply(lambda x: 'creditcard' in x).astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:20.997161Z","iopub.execute_input":"2021-07-01T22:26:20.997505Z","iopub.status.idle":"2021-07-01T22:26:21.011978Z","shell.execute_reply.started":"2021-07-01T22:26:20.997474Z","shell.execute_reply":"2021-07-01T22:26:21.010884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check column value counts\nstation_info['creditcard_payment'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:21.01341Z","iopub.execute_input":"2021-07-01T22:26:21.01403Z","iopub.status.idle":"2021-07-01T22:26:21.031955Z","shell.execute_reply.started":"2021-07-01T22:26:21.013994Z","shell.execute_reply":"2021-07-01T22:26:21.030219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop `rental_methods`\nstation_info.drop(['rental_methods'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:21.033336Z","iopub.execute_input":"2021-07-01T22:26:21.033634Z","iopub.status.idle":"2021-07-01T22:26:21.046693Z","shell.execute_reply.started":"2021-07-01T22:26:21.033606Z","shell.execute_reply":"2021-07-01T22:26:21.045683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### *Issue 4: Impute Altitude Values*","metadata":{}},{"cell_type":"markdown","source":"Before we go ahead and impute the missing data using the Open Elevation API, lets check the current altitude distribution in the data set.","metadata":{}},{"cell_type":"code","source":"# Checking altitude distribution\nstation_info.groupby(['altitude']).size().reset_index(name='count')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:21.047933Z","iopub.execute_input":"2021-07-01T22:26:21.048387Z","iopub.status.idle":"2021-07-01T22:26:21.066515Z","shell.execute_reply.started":"2021-07-01T22:26:21.048358Z","shell.execute_reply":"2021-07-01T22:26:21.064605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Observations_**<br>\nIt appears that most of the recorded station altitude are actually 0 with one at 102.08, assuming in meters. Because majority of the data is missing, we will need to replace it all using the Open Elevation API.","metadata":{}},{"cell_type":"code","source":"# Rename columns\nstation_info.rename(columns={'lat':'latitude', 'lon':'longitude'}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:21.06802Z","iopub.execute_input":"2021-07-01T22:26:21.068268Z","iopub.status.idle":"2021-07-01T22:26:21.07332Z","shell.execute_reply.started":"2021-07-01T22:26:21.068244Z","shell.execute_reply":"2021-07-01T22:26:21.07215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Separate geocoordinates to be used for API query\ngeocoordinates = station_info[['latitude','longitude']]","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:21.074599Z","iopub.execute_input":"2021-07-01T22:26:21.074871Z","iopub.status.idle":"2021-07-01T22:26:21.090114Z","shell.execute_reply.started":"2021-07-01T22:26:21.074841Z","shell.execute_reply":"2021-07-01T22:26:21.088471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setup a dictionary from 2 pandas array\ngeo_dict = {\"locations\":geocoordinates.to_dict('records')}","metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-07-01T22:26:21.093359Z","iopub.execute_input":"2021-07-01T22:26:21.093743Z","iopub.status.idle":"2021-07-01T22:26:21.107209Z","shell.execute_reply.started":"2021-07-01T22:26:21.093707Z","shell.execute_reply":"2021-07-01T22:26:21.10584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import json\nimport json\n\n# Convert dict to json\njsonData = json.dumps(geo_dict)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:21.108834Z","iopub.execute_input":"2021-07-01T22:26:21.109198Z","iopub.status.idle":"2021-07-01T22:26:21.126007Z","shell.execute_reply.started":"2021-07-01T22:26:21.109162Z","shell.execute_reply":"2021-07-01T22:26:21.125386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# WARNING API can be unreliable with it routinely timing out, \n# that is why I have written a loop here with a time delay to retry every 2 seconds.\n\n# Counter\ni = 1\nprint(f'API Request attempt {i}:')\nrequest2 = requests.post('https://api.open-elevation.com/api/v1/lookup',\n                         data=jsonData,\n                         headers={'Accept': 'application/json', 'Content-type': 'application/json'})\n\n# loop if status code != 200 then retry with delay.\nwhile request2.status_code != 200:\n    print('Failed!')\n    i += 1\n    print(f'API Request attempt {i}:')\n    request2 = requests.post('https://api.open-elevation.com/api/v1/lookup',\n                         data=jsonData,\n                         headers={'Accept': 'application/json', 'Content-type': 'application/json'})\n    if request2.status_code == 200:\n        break\n    else:\n        continue\n    time.sleep(5)\n    \nprint(f\"Successful!\\nContent-Type is: {request2.headers['Content-Type']}\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-07-01T22:26:21.126896Z","iopub.execute_input":"2021-07-01T22:26:21.127087Z","iopub.status.idle":"2021-07-01T22:26:21.751811Z","shell.execute_reply.started":"2021-07-01T22:26:21.127066Z","shell.execute_reply":"2021-07-01T22:26:21.750521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Observation_**<br>\nIt appears that all the data is in in a single dictionary record under the data column. Lets extract/transform that dictionary into a workable data frame.","metadata":{}},{"cell_type":"code","source":"# Convert json to a dataframe\nelevation_info = pd.DataFrame(request2.json())\nelevation_info","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:21.7526Z","iopub.execute_input":"2021-07-01T22:26:21.752794Z","iopub.status.idle":"2021-07-01T22:26:21.768385Z","shell.execute_reply.started":"2021-07-01T22:26:21.752763Z","shell.execute_reply":"2021-07-01T22:26:21.76694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Observation_**<br>\nBecause each station's elevation information is in each record, we can simply extract that information and add it into `station_info` data frame.","metadata":{}},{"cell_type":"code","source":"# Extract `elevation` values to `altitude` column in station_info data frame\nstation_info['altitude'] = elevation_info['results'].apply(lambda x: x['elevation'])","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:21.769875Z","iopub.execute_input":"2021-07-01T22:26:21.770165Z","iopub.status.idle":"2021-07-01T22:26:21.783319Z","shell.execute_reply.started":"2021-07-01T22:26:21.770135Z","shell.execute_reply":"2021-07-01T22:26:21.782345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Rename column from `altitude` to `elevation`\nstation_info.rename(columns={'altitude':'elevation'}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:21.78437Z","iopub.execute_input":"2021-07-01T22:26:21.784664Z","iopub.status.idle":"2021-07-01T22:26:21.79793Z","shell.execute_reply.started":"2021-07-01T22:26:21.784635Z","shell.execute_reply":"2021-07-01T22:26:21.797105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check dataset\nstation_info.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:21.798813Z","iopub.execute_input":"2021-07-01T22:26:21.79905Z","iopub.status.idle":"2021-07-01T22:26:21.821587Z","shell.execute_reply.started":"2021-07-01T22:26:21.799023Z","shell.execute_reply":"2021-07-01T22:26:21.820154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Rechecking elevation distribution\nstation_info.groupby(['elevation']).size().reset_index(name='count')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:21.823056Z","iopub.execute_input":"2021-07-01T22:26:21.823485Z","iopub.status.idle":"2021-07-01T22:26:21.845153Z","shell.execute_reply.started":"2021-07-01T22:26:21.823455Z","shell.execute_reply":"2021-07-01T22:26:21.844502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### *Issue 5: Postal Code & FSA*","metadata":{}},{"cell_type":"code","source":"# Split `postal code` into two columns\npostal_code = station_info[\"post_code\"].str.split(\" \", n = 1, expand = True)\n\n# Check split\npostal_code","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:21.846173Z","iopub.execute_input":"2021-07-01T22:26:21.846456Z","iopub.status.idle":"2021-07-01T22:26:21.861368Z","shell.execute_reply.started":"2021-07-01T22:26:21.846413Z","shell.execute_reply":"2021-07-01T22:26:21.860676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create new `fsa` column and place current known FSAs into it\nstation_info[\"fsa\"] = postal_code[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:21.862785Z","iopub.execute_input":"2021-07-01T22:26:21.863086Z","iopub.status.idle":"2021-07-01T22:26:21.881299Z","shell.execute_reply.started":"2021-07-01T22:26:21.863054Z","shell.execute_reply":"2021-07-01T22:26:21.879574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for any more null `fsa`\nprint(f'Number of Null values in `fsa`:\\n\\n{station_info[\"fsa\"].isna().sum()}')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:21.883335Z","iopub.execute_input":"2021-07-01T22:26:21.883812Z","iopub.status.idle":"2021-07-01T22:26:21.895772Z","shell.execute_reply.started":"2021-07-01T22:26:21.883781Z","shell.execute_reply":"2021-07-01T22:26:21.894481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Next STeps_**<br>\nNow that we extracted all the `FSA` we could, we need to use Nominatim to search OpenStreetMap and query the postal codes of the stations using the geo-coordinates to try and fill the missing `FSA` values.","metadata":{}},{"cell_type":"code","source":"# Import Nominatim from GeoPy\nfrom geopy.geocoders import Nominatim","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:21.897344Z","iopub.execute_input":"2021-07-01T22:26:21.897645Z","iopub.status.idle":"2021-07-01T22:26:22.008095Z","shell.execute_reply.started":"2021-07-01T22:26:21.897617Z","shell.execute_reply":"2021-07-01T22:26:22.006892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating separate dataframe for `latitude` and `longitude` string searches\nlocations = station_info[['latitude', 'longitude']].astype(str).agg(','.join, axis=1)\n\n# Display locations\ndisplay(locations)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:22.009369Z","iopub.execute_input":"2021-07-01T22:26:22.009673Z","iopub.status.idle":"2021-07-01T22:26:22.05313Z","shell.execute_reply.started":"2021-07-01T22:26:22.009643Z","shell.execute_reply":"2021-07-01T22:26:22.052144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Requesting address information for all 608 Toronto BikeShare Stations\n# WARNING: This will take several minutes to run.\n\n# Instantiate empty list for query\naddress = []\n\n# Must set user_agent id in order to access API, user_agent id can be anything\n# You may need to tweak timeout value if code does not run properly\ngeolocator = Nominatim(user_agent=\"bstn_jho\", timeout=3)\n\n# iterate through the address query list\nfor i in tqdm(range(len(locations))):\n    # search each address and retrieve lat and lan\n    location = geolocator.geocode(locations[i])\n    # append lat and long data into empty lists\n    address.append(location.address)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:26:22.05396Z","iopub.execute_input":"2021-07-01T22:26:22.054171Z","iopub.status.idle":"2021-07-01T22:31:26.309897Z","shell.execute_reply.started":"2021-07-01T22:26:22.054149Z","shell.execute_reply":"2021-07-01T22:31:26.309003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import regex package\nimport re\n\n# regex the FSA postal code and save to new list\nfsa = []\nfor i in range(len(address)):\n    fsa.append(re.findall('[A-Z]\\d[A-Z]', address[i]))","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:26.311075Z","iopub.execute_input":"2021-07-01T22:31:26.311292Z","iopub.status.idle":"2021-07-01T22:31:26.318498Z","shell.execute_reply.started":"2021-07-01T22:31:26.31127Z","shell.execute_reply":"2021-07-01T22:31:26.317724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transform list in to data frame\nfsa = pd.DataFrame(fsa)\n\n# Check data frame\nfsa","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:26.319269Z","iopub.execute_input":"2021-07-01T22:31:26.319605Z","iopub.status.idle":"2021-07-01T22:31:26.345198Z","shell.execute_reply.started":"2021-07-01T22:31:26.31958Z","shell.execute_reply":"2021-07-01T22:31:26.343951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fill null values with FSA list obtained from OpenStreetMap API\nstation_info[\"fsa\"] = station_info[\"fsa\"].fillna(fsa[0])","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:26.3463Z","iopub.execute_input":"2021-07-01T22:31:26.346637Z","iopub.status.idle":"2021-07-01T22:31:26.360045Z","shell.execute_reply.started":"2021-07-01T22:31:26.346609Z","shell.execute_reply":"2021-07-01T22:31:26.358856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for any nulls `fsa`\nprint(f'Number of Null Values in fsa:\\n\\n{fsa[0].isna().sum()}')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:26.362491Z","iopub.execute_input":"2021-07-01T22:31:26.362795Z","iopub.status.idle":"2021-07-01T22:31:26.376776Z","shell.execute_reply.started":"2021-07-01T22:31:26.362765Z","shell.execute_reply":"2021-07-01T22:31:26.375077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Observation_**<br>\nLooks like we missed a few`FSA` value lets take alook into which station it is and we will have to impute the missing values manually via Google search because using the Google Maps API costs money and private API key.","metadata":{}},{"cell_type":"code","source":"# Looks like we missed an`FSA` value lets take alook into which station it is and \n# we will have to imput the missing value manually via Google search\nnull_fsa = station_info[station_info[\"fsa\"].isna()]","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:26.378531Z","iopub.execute_input":"2021-07-01T22:31:26.37883Z","iopub.status.idle":"2021-07-01T22:31:26.391403Z","shell.execute_reply.started":"2021-07-01T22:31:26.378801Z","shell.execute_reply":"2021-07-01T22:31:26.390275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display null_fsa\nnull_fsa","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:26.393198Z","iopub.execute_input":"2021-07-01T22:31:26.393541Z","iopub.status.idle":"2021-07-01T22:31:26.416163Z","shell.execute_reply.started":"2021-07-01T22:31:26.393512Z","shell.execute_reply":"2021-07-01T22:31:26.415403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using Google Maps search, for the geo-coordinates provided in the datapoints above, I will manually replace the `fsa` and fill in the current `null` value.","metadata":{}},{"cell_type":"code","source":"# Manually create fsa_dict from Google Maps search\nfsa_dict = {6: \"M5G\", 14: \"M5A\", 21: \"M5G\", 75: \"M5V\", 76: \"M5J\", 77: \"M5G\", 166: \"M5V\", 356: \"M5G\"}","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:26.417366Z","iopub.execute_input":"2021-07-01T22:31:26.417777Z","iopub.status.idle":"2021-07-01T22:31:26.432042Z","shell.execute_reply.started":"2021-07-01T22:31:26.417748Z","shell.execute_reply":"2021-07-01T22:31:26.431268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Update/Impute fsa_dict onto station_info\nstation_info['fsa'].update(pd.Series(fsa_dict))","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:26.433075Z","iopub.execute_input":"2021-07-01T22:31:26.433421Z","iopub.status.idle":"2021-07-01T22:31:26.448686Z","shell.execute_reply.started":"2021-07-01T22:31:26.433394Z","shell.execute_reply":"2021-07-01T22:31:26.447902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for any more null `fsa`\nprint(f'Number of Null values in `fsa`:\\n\\n{station_info[\"fsa\"].isna().sum()}')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:26.450148Z","iopub.execute_input":"2021-07-01T22:31:26.450422Z","iopub.status.idle":"2021-07-01T22:31:26.46652Z","shell.execute_reply.started":"2021-07-01T22:31:26.450395Z","shell.execute_reply":"2021-07-01T22:31:26.465566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Next Step_**<br>\nJust to be sure that all the FSAs were extracted properly from before, we will use a regex search on `fsa` column to see if any were extracted from `post_code` wrong.","metadata":{}},{"cell_type":"code","source":"# Check for any FSA not 3 characters long with letter 'M', number, letter combo.\nbad_fsa = station_info[~station_info['fsa'].str.match('(?:^|\\W)M\\d[A-Z](?:$|\\W)')]\n\n# View dataframe\nbad_fsa.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:26.46738Z","iopub.execute_input":"2021-07-01T22:31:26.467636Z","iopub.status.idle":"2021-07-01T22:31:26.491603Z","shell.execute_reply.started":"2021-07-01T22:31:26.467611Z","shell.execute_reply":"2021-07-01T22:31:26.490402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the size of data set\nbad_fsa.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:26.492822Z","iopub.execute_input":"2021-07-01T22:31:26.493098Z","iopub.status.idle":"2021-07-01T22:31:26.50811Z","shell.execute_reply.started":"2021-07-01T22:31:26.493071Z","shell.execute_reply":"2021-07-01T22:31:26.507249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Observation_**<br>\nNow that we have found 24 stations incorrectly extracted their FSAs, we will need to create a dictionary from the master fsa dataframe we created earlier from using the Nominatim API then updating the station_info dataset with the correct FSAs.","metadata":{}},{"cell_type":"code","source":"# Create dictionary from FSAs found by API earlier\nfsa_dict2 = fsa[0].to_dict()\n\n# Replace bad FSAs with ones in dictionary\nstation_info['fsa'].update(pd.Series(fsa_dict2))","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:26.509192Z","iopub.execute_input":"2021-07-01T22:31:26.509596Z","iopub.status.idle":"2021-07-01T22:31:26.523981Z","shell.execute_reply.started":"2021-07-01T22:31:26.509567Z","shell.execute_reply":"2021-07-01T22:31:26.523333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop `post_code` after fixing FSAs as we no longer need postal codes\nstation_info.drop(['post_code'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:26.524994Z","iopub.execute_input":"2021-07-01T22:31:26.525376Z","iopub.status.idle":"2021-07-01T22:31:26.535237Z","shell.execute_reply.started":"2021-07-01T22:31:26.525348Z","shell.execute_reply":"2021-07-01T22:31:26.534288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final check for any FSA not 3 characters long with letter 'M', number, letter combo.\nstation_info[~station_info['fsa'].str.match('(?:^|\\W)M\\d[A-Z](?:$|\\W)')]","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:26.536289Z","iopub.execute_input":"2021-07-01T22:31:26.53656Z","iopub.status.idle":"2021-07-01T22:31:26.554043Z","shell.execute_reply.started":"2021-07-01T22:31:26.536534Z","shell.execute_reply":"2021-07-01T22:31:26.552912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### *Issue 6: Check/Change Datatypes*","metadata":{}},{"cell_type":"code","source":"# Datatype check\nstation_info.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:26.555372Z","iopub.execute_input":"2021-07-01T22:31:26.55588Z","iopub.status.idle":"2021-07-01T22:31:26.56316Z","shell.execute_reply.started":"2021-07-01T22:31:26.555849Z","shell.execute_reply":"2021-07-01T22:31:26.561863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Change `station_id` to `int`\nstation_info['station_id'] = station_info['station_id'].astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:26.564687Z","iopub.execute_input":"2021-07-01T22:31:26.565127Z","iopub.status.idle":"2021-07-01T22:31:26.575704Z","shell.execute_reply.started":"2021-07-01T22:31:26.565097Z","shell.execute_reply":"2021-07-01T22:31:26.574504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final datatype check\nstation_info.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:26.576918Z","iopub.execute_input":"2021-07-01T22:31:26.577205Z","iopub.status.idle":"2021-07-01T22:31:26.594041Z","shell.execute_reply.started":"2021-07-01T22:31:26.577169Z","shell.execute_reply":"2021-07-01T22:31:26.592464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save cleaned dataset to new csv\n# station_info.to_csv('../data/processed/station_info_clean.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:26.595319Z","iopub.execute_input":"2021-07-01T22:31:26.595606Z","iopub.status.idle":"2021-07-01T22:31:26.603021Z","shell.execute_reply.started":"2021-07-01T22:31:26.595578Z","shell.execute_reply":"2021-07-01T22:31:26.602233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### API 2: Pricing Plan\n\nBack to [Table of Contents](#Table-of-Contents).<br>\n\n~1. Request & Clean Station Information~<br>\n2. Request & Clean System Pricing Plans","metadata":{}},{"cell_type":"code","source":"# Counter\ni = 1\nprint(f'API Request attempt {i}:')\n\n# Request BikeShare Pricing Plan\nrequest3 = requests.get('https://tor.publicbikesystem.net/ube/gbfs/v1/en/system_pricing_plans')\n\n# Loop if status code != 200 then retry with delay.\nwhile request3.status_code != 200:\n    print('Failed!')\n    i += 1\n    print(f'API Request attempt {i}:')\n    request3 = requests.get('https://tor.publicbikesystem.net/ube/gbfs/v1/en/system_pricing_plans')\n    if request3.status_code == 200:\n        break\n    else:\n        continue\n    time.sleep(5)\n    \nprint(f\"Successful!\\nContent-Type is: {request2.headers['Content-Type']}\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:26.603997Z","iopub.execute_input":"2021-07-01T22:31:26.604261Z","iopub.status.idle":"2021-07-01T22:31:27.156126Z","shell.execute_reply.started":"2021-07-01T22:31:26.604236Z","shell.execute_reply":"2021-07-01T22:31:27.155038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert json to a dataframe\npricing = pd.DataFrame(request3.json())\npricing","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:27.157071Z","iopub.execute_input":"2021-07-01T22:31:27.157286Z","iopub.status.idle":"2021-07-01T22:31:27.173287Z","shell.execute_reply.started":"2021-07-01T22:31:27.157263Z","shell.execute_reply":"2021-07-01T22:31:27.172064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check last updated time\ntime = pricing.iloc[0]['last_updated']\nprint(f\"Data last updated:\\n{datetime.fromtimestamp(time).strftime('%Y-%m-%d %I:%M:%S %p')}\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:27.174381Z","iopub.execute_input":"2021-07-01T22:31:27.174715Z","iopub.status.idle":"2021-07-01T22:31:27.191873Z","shell.execute_reply.started":"2021-07-01T22:31:27.174684Z","shell.execute_reply":"2021-07-01T22:31:27.189849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transform dictionary into a dataset\npricing = pd.DataFrame(pricing.iloc[0]['data'])\n\n# Check dataset\npricing.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:27.19362Z","iopub.execute_input":"2021-07-01T22:31:27.193998Z","iopub.status.idle":"2021-07-01T22:31:27.220677Z","shell.execute_reply.started":"2021-07-01T22:31:27.193959Z","shell.execute_reply":"2021-07-01T22:31:27.219296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the size of dataset\npricing.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:27.222041Z","iopub.execute_input":"2021-07-01T22:31:27.222268Z","iopub.status.idle":"2021-07-01T22:31:27.239014Z","shell.execute_reply.started":"2021-07-01T22:31:27.222245Z","shell.execute_reply":"2021-07-01T22:31:27.237703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for duplicates\nprint(f\"Duplicates found: {pricing['plan_id'].duplicated().sum()}\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:27.239974Z","iopub.execute_input":"2021-07-01T22:31:27.240212Z","iopub.status.idle":"2021-07-01T22:31:27.25295Z","shell.execute_reply.started":"2021-07-01T22:31:27.240188Z","shell.execute_reply":"2021-07-01T22:31:27.251153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for null values\nprint(f\"\"\"\nTotal Number of Null values:\\n\\n{pricing.isna().sum()}\\n\n----------------------------------------\\n\nTotal Percentage of Null values:\\n\\n{pricing.isna().mean()*100}\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:27.254164Z","iopub.execute_input":"2021-07-01T22:31:27.254546Z","iopub.status.idle":"2021-07-01T22:31:27.27273Z","shell.execute_reply.started":"2021-07-01T22:31:27.254517Z","shell.execute_reply":"2021-07-01T22:31:27.271572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check datatypes\npricing.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:27.274359Z","iopub.execute_input":"2021-07-01T22:31:27.27465Z","iopub.status.idle":"2021-07-01T22:31:27.28873Z","shell.execute_reply.started":"2021-07-01T22:31:27.274621Z","shell.execute_reply":"2021-07-01T22:31:27.287681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataset small enough to display fully\npricing","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:27.289941Z","iopub.execute_input":"2021-07-01T22:31:27.29033Z","iopub.status.idle":"2021-07-01T22:31:27.313084Z","shell.execute_reply.started":"2021-07-01T22:31:27.290288Z","shell.execute_reply":"2021-07-01T22:31:27.311854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Observations_**<br>\n- `plan_id`: change to int\n- `price`: change to float","metadata":{}},{"cell_type":"code","source":"# Change `station_id` dtype to int\npricing['plan_id'] = pricing['plan_id'].astype(int)\npricing['price'] = pricing['price'].astype(float)\n# Last check on datatypes\npricing.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:27.314619Z","iopub.execute_input":"2021-07-01T22:31:27.31492Z","iopub.status.idle":"2021-07-01T22:31:27.332075Z","shell.execute_reply.started":"2021-07-01T22:31:27.314892Z","shell.execute_reply":"2021-07-01T22:31:27.330859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save cleaned dataset to new csv\n# pricing.to_csv('../data/processed/pricing_clean.csv') ","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:27.333216Z","iopub.execute_input":"2021-07-01T22:31:27.333484Z","iopub.status.idle":"2021-07-01T22:31:27.344254Z","shell.execute_reply.started":"2021-07-01T22:31:27.333457Z","shell.execute_reply":"2021-07-01T22:31:27.343524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Part 4: Creating Master Dataframe\n\nBack to [Table of Contents](#Table-of-Contents).<br>\n\n~1. Trip Datasets~<br>\n~2. Weather Datasets~<br>\n~3. API Datasets~\n\nAfter cleaning all the necessary datasets, I will now combine them into one final dataset to be used for EDA and Modelling in the next notebook. \n\n**_Next Steps_**:\n1. Merge the Trip & Weather Datasets\n2. Merge `station_info`\n3. Clean/Reorganize","metadata":{}},{"cell_type":"markdown","source":"#### *Issue 1: Merge Trip & Weather Datasets*","metadata":{}},{"cell_type":"code","source":"# To prep the trips data set to be merged with the weather dataset, we need to round the start time\n# to nearest hour and save it to a temp column to use as key to join weather df\ntrips17_20['start_hour'] = trips17_20['Start Time'].dt.round('H')\ntrips19_20['start_hour'] = trips19_20['Start Time'].dt.round('H')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:27.345707Z","iopub.execute_input":"2021-07-01T22:31:27.346246Z","iopub.status.idle":"2021-07-01T22:31:27.759297Z","shell.execute_reply.started":"2021-07-01T22:31:27.346206Z","shell.execute_reply":"2021-07-01T22:31:27.758612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge trip and weather data sets\ndf17_20 = pd.merge(trips17_20, weather_df, \n              how='left', \n              left_on='start_hour', right_on='Date/Time')\n\ndf19_20 = pd.merge(trips19_20, weather_df, \n              how='left', \n              left_on='start_hour', right_on='Date/Time')\n\n# Check if merge is successful\nprint(\"\\n**df17_20 dataset**\")\ndisplay(df17_20.head(3))\nprint(\"\\n**df19_20 dataset**\")\ndisplay(df19_20.head(3))","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:27.760351Z","iopub.execute_input":"2021-07-01T22:31:27.760839Z","iopub.status.idle":"2021-07-01T22:31:34.129226Z","shell.execute_reply.started":"2021-07-01T22:31:27.760801Z","shell.execute_reply":"2021-07-01T22:31:34.128277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Observations_**<br>\nThere seems to be a display glitch for the `start_time` and `Date/Time` to not show the time when it is 00:00:00, I have not found a solution to this yet but I have double-checked the values by inspecting the csv file and the merge was successful.\n\n**_Next Step_**<br>\nNow that the two data sets have been concatted, we can drop the redundant time features.","metadata":{}},{"cell_type":"code","source":"# Dropping `start_time` and `Date/Time` as these are not necessary features going forward in the analysis\ndf17_20.drop(['start_hour', 'Date/Time'], axis=1, inplace=True)\ndf19_20.drop(['start_hour', 'Date/Time'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:34.13152Z","iopub.execute_input":"2021-07-01T22:31:34.132225Z","iopub.status.idle":"2021-07-01T22:31:36.627113Z","shell.execute_reply.started":"2021-07-01T22:31:34.132181Z","shell.execute_reply":"2021-07-01T22:31:36.626137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### *Issue 2: Merge Station Info*","metadata":{}},{"cell_type":"code","source":"# Combining station latitude, longitude, elevation, capacity, creditcard_payment, and fsa information\ndf17_20 = pd.merge(df17_20, station_info[['station_id', 'latitude', 'longitude', 'elevation', \n                                          'capacity', 'creditcard_payment','fsa']].add_prefix('start_'), \n                   how='left', \n                   left_on='Start Station Id', \n                   right_on='start_station_id')\n\ndf17_20 = pd.merge(df17_20, station_info[['station_id', 'latitude', 'longitude', 'elevation', \n                                          'capacity', 'creditcard_payment', 'fsa']].add_prefix('end_'), \n                   how='left', \n                   left_on='End Station Id', \n                   right_on='end_station_id')\n\ndf19_20 = pd.merge(df19_20, station_info[['station_id', 'latitude', 'longitude', 'elevation', \n                                          'capacity', 'creditcard_payment', 'fsa']].add_prefix('start_'), \n                   how='left', \n                   left_on='Start Station Id', \n                   right_on='start_station_id')\n\ndf19_20 = pd.merge(df19_20, station_info[['station_id', 'latitude', 'longitude', 'elevation', \n                                          'capacity', 'creditcard_payment', 'fsa']].add_prefix('end_'), \n                   how='left', \n                   left_on='End Station Id', \n                   right_on='end_station_id')\n\n# Check if merge successful\nprint(\"\\n**df17_20 dataset**\")\ndisplay(df17_20.head(3))\nprint(\"\\n**df19_20 dataset**\")\ndisplay(df19_20.head(3))","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:36.628555Z","iopub.execute_input":"2021-07-01T22:31:36.629142Z","iopub.status.idle":"2021-07-01T22:31:50.68082Z","shell.execute_reply.started":"2021-07-01T22:31:36.629102Z","shell.execute_reply":"2021-07-01T22:31:50.679641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Observation_**<br>\nNow that it was merged successfully, we can drop the redundant station ids.","metadata":{}},{"cell_type":"code","source":"# Dropping duplicate columns `start_station_id`, `end_station_id`\ndf17_20.drop(['start_station_id', 'end_station_id'], axis=1, inplace=True)\ndf19_20.drop(['start_station_id', 'end_station_id'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:50.681926Z","iopub.execute_input":"2021-07-01T22:31:50.68227Z","iopub.status.idle":"2021-07-01T22:31:54.632341Z","shell.execute_reply.started":"2021-07-01T22:31:50.682234Z","shell.execute_reply":"2021-07-01T22:31:54.631313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### *Issue 3: Clean & Reorganize Dataframe*","metadata":{}},{"cell_type":"code","source":"# Renaming columns\ndf17_20.rename(columns={'start_latitude':'Start Station Latitude',\n                        'start_longitude':'Start Station Longitude',\n                        'start_elevation':'Start Station Elevation',\n                        'start_capacity':'Start Station Capacity',\n                        'start_creditcard_payment':'Start Station Credit Card',\n                        'start_fsa':'Start Station FSA',\n                        'end_latitude':'End Station Latitude',\n                        'end_longitude':'End Station Longitude',\n                        'end_elevation':'End Station Elevation',\n                        'end_capacity':'End Station Capacity',\n                        'end_creditcard_payment':'End Station Credit Card',\n                        'end_fsa':'End Station FSA',\n                        'Stn Press (kPa)':'Atm Pressure (kPa)',\n                        'Hmdx':'Humidex'}, inplace=True)\n\n# Reorganize columns\ndf17_20 = df17_20[['Trip Id',\n                   'Start Time',\n                   'Start Station Id',\n                   'Start Station Name',\n                   'Start Station Latitude',\n                   'Start Station Longitude',\n                   'Start Station Elevation',\n                   'Start Station Capacity',\n                   'Start Station Credit Card',\n                   'Start Station FSA',\n                   'End Time',\n                   'End Station Id',\n                   'End Station Name',\n                   'End Station Latitude',\n                   'End Station Longitude',\n                   'End Station Elevation',\n                   'End Station Capacity',\n                   'End Station Credit Card',\n                   'End Station FSA',\n                   'Trip Duration (Seconds)',\n#                    'Trip Duration (Minutes)',\n                   'Member',\n                   'Weekend',\n                   'Holiday',\n                   'Temp (째C)',\n                   'Dew Point Temp (째C)',\n                   'Rel Hum (%)',\n                   'Visibility (km)',\n                   'Atm Pressure (kPa)',\n                   'Humidex',          \n                   'Wind Chill',     \n                   'Strong Wind',    \n                   'Blowing Snow',   \n                   'Fog',            \n                   'Freezing Rain',  \n                   'Haze',           \n                   'Heavy Rain',     \n                   'Heavy Snow',     \n                   'Moderate Rain',  \n                   'Moderate Snow',  \n                   'Rain',           \n                   'Snow',           \n                   'Thunderstorms']]\n\n# Column Label Check\ndisplay(df17_20.columns)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:54.63333Z","iopub.execute_input":"2021-07-01T22:31:54.633581Z","iopub.status.idle":"2021-07-01T22:31:55.515149Z","shell.execute_reply.started":"2021-07-01T22:31:54.633557Z","shell.execute_reply":"2021-07-01T22:31:55.514316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Renaming columns\ndf19_20.rename(columns={'start_latitude':'Start Station Latitude',\n                        'start_longitude':'Start Station Longitude',\n                        'start_elevation':'Start Station Elevation',\n                        'start_capacity':'Start Station Capacity',\n                        'start_creditcard_payment':'Start Station Credit Card',\n                        'start_fsa':'Start Station FSA',\n                        'end_latitude':'End Station Latitude',\n                        'end_longitude':'End Station Longitude',\n                        'end_elevation':'End Station Elevation',\n                        'end_capacity':'End Station Capacity',\n                        'end_creditcard_payment':'End Station Credit Card',\n                        'end_fsa':'End Station FSA',\n                        'Stn Press (kPa)':'Atm Pressure (kPa)',\n                        'Hmdx':'Humidex'}, inplace=True)\n\n\n# Reorganize columns\ndf19_20 = df19_20[['Trip Id',\n                   'Subscription Id',\n                   'Bike Id',\n                   'Start Time',\n                   'Start Station Id',\n                   'Start Station Name',\n                   'Start Station Latitude',\n                   'Start Station Longitude',\n                   'Start Station Elevation',\n                   'Start Station Capacity',\n                   'Start Station Credit Card',\n                   'Start Station FSA',\n                   'End Time',\n                   'End Station Id',\n                   'End Station Name',\n                   'End Station Latitude',\n                   'End Station Longitude',\n                   'End Station Elevation',\n                   'End Station Capacity',\n                   'End Station Credit Card',\n                   'End Station FSA',\n                   'Trip Duration (Seconds)',\n#                    'Trip Duration (Minutes)',\n                   'Member',\n                   'Weekend',\n                   'Holiday',\n                   'Temp (째C)',\n                   'Dew Point Temp (째C)',\n                   'Rel Hum (%)',\n                   'Visibility (km)',\n                   'Atm Pressure (kPa)',\n                   'Humidex',          \n                   'Wind Chill',     \n                   'Strong Wind',    \n                   'Blowing Snow',   \n                   'Fog',            \n                   'Freezing Rain',  \n                   'Haze',           \n                   'Heavy Rain',     \n                   'Heavy Snow',     \n                   'Moderate Rain',  \n                   'Moderate Snow',  \n                   'Rain',           \n                   'Snow',           \n                   'Thunderstorms']]\n\n# Column Label Check\ndisplay(df19_20.columns)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:55.516104Z","iopub.execute_input":"2021-07-01T22:31:55.51631Z","iopub.status.idle":"2021-07-01T22:31:56.068888Z","shell.execute_reply.started":"2021-07-01T22:31:55.516288Z","shell.execute_reply":"2021-07-01T22:31:56.068262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for null values\nprint(f\"\"\"\nTotal Number of Null values:\\n\\n{df17_20.isna().sum()}\\n\n----------------------------------------\\n\nTotal Percentage of Null values:\\n\\n{df17_20.isna().mean()*100}\\n\n========================================\\n\nTotal Number of Null values:\\n\\n{df19_20.isna().sum()}\\n\n----------------------------------------\\n\nTotal Percentage of Null values:\\n\\n{df19_20.isna().mean()*100}\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:31:56.069743Z","iopub.execute_input":"2021-07-01T22:31:56.070047Z","iopub.status.idle":"2021-07-01T22:32:00.752209Z","shell.execute_reply.started":"2021-07-01T22:31:56.070016Z","shell.execute_reply":"2021-07-01T22:32:00.751299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View rows with null values\ndf19_20[df19_20.isna().any(axis=1)].head()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:32:00.75321Z","iopub.execute_input":"2021-07-01T22:32:00.75345Z","iopub.status.idle":"2021-07-01T22:32:01.683906Z","shell.execute_reply.started":"2021-07-01T22:32:00.753409Z","shell.execute_reply":"2021-07-01T22:32:01.682738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Observations_**<br>\nIt appears that there are null values create int Start Station and End Station information because we do no have any info for these discontinued stations. Since there is no way to impute this information, we will need to drop these rows.\n\nBut before we drop the null values, we first it looks like there are some weather null values that was created from rounding to the nearest hour and since we do not have weather information for `2020-11-01 00:00:00` we will need to forward fill this information.","metadata":{"execution":{"iopub.execute_input":"2020-12-13T04:55:16.9448Z","iopub.status.busy":"2020-12-13T04:55:16.944488Z","iopub.status.idle":"2020-12-13T04:55:16.961651Z","shell.execute_reply":"2020-12-13T04:55:16.960742Z","shell.execute_reply.started":"2020-12-13T04:55:16.944772Z"}}},{"cell_type":"code","source":"# Forwardfill any weather null values\ndf17_20[['Temp (째C)', \n         'Dew Point Temp (째C)',\n         'Rel Hum (%)',        \n         'Visibility (km)',    \n         'Atm Pressure (kPa)', \n         'Humidex',            \n         'Wind Chill',         \n         'Strong Wind',        \n         'Blowing Snow',       \n         'Fog',                \n         'Freezing Rain',      \n         'Haze',               \n         'Heavy Rain',         \n         'Heavy Snow',         \n         'Moderate Rain',      \n         'Moderate Snow',      \n         'Rain',               \n         'Snow',               \n         'Thunderstorms']] = df17_20[['Temp (째C)', \n                                      'Dew Point Temp (째C)',\n                                      'Rel Hum (%)',        \n                                      'Visibility (km)',    \n                                      'Atm Pressure (kPa)', \n                                      'Humidex',            \n                                      'Wind Chill',         \n                                      'Strong Wind',        \n                                      'Blowing Snow',       \n                                      'Fog',                \n                                      'Freezing Rain',      \n                                      'Haze',               \n                                      'Heavy Rain',         \n                                      'Heavy Snow',         \n                                      'Moderate Rain',      \n                                      'Moderate Snow',      \n                                      'Rain',               \n                                      'Snow',               \n                                      'Thunderstorms']].fillna(method='ffill')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:32:01.684787Z","iopub.execute_input":"2021-07-01T22:32:01.685044Z","iopub.status.idle":"2021-07-01T22:32:02.713632Z","shell.execute_reply.started":"2021-07-01T22:32:01.685015Z","shell.execute_reply":"2021-07-01T22:32:02.712791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Forwardfill any weather null values\ndf19_20[['Temp (째C)', \n         'Dew Point Temp (째C)',\n         'Rel Hum (%)',        \n         'Visibility (km)',    \n         'Atm Pressure (kPa)', \n         'Humidex',            \n         'Wind Chill',         \n         'Strong Wind',        \n         'Blowing Snow',       \n         'Fog',                \n         'Freezing Rain',      \n         'Haze',               \n         'Heavy Rain',         \n         'Heavy Snow',         \n         'Moderate Rain',      \n         'Moderate Snow',      \n         'Rain',               \n         'Snow',               \n         'Thunderstorms']] = df19_20[['Temp (째C)', \n                                      'Dew Point Temp (째C)',\n                                      'Rel Hum (%)',        \n                                      'Visibility (km)',    \n                                      'Atm Pressure (kPa)', \n                                      'Humidex',            \n                                      'Wind Chill',         \n                                      'Strong Wind',        \n                                      'Blowing Snow',       \n                                      'Fog',                \n                                      'Freezing Rain',      \n                                      'Haze',               \n                                      'Heavy Rain',         \n                                      'Heavy Snow',         \n                                      'Moderate Rain',      \n                                      'Moderate Snow',      \n                                      'Rain',               \n                                      'Snow',               \n                                      'Thunderstorms']].fillna(method='ffill')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:32:02.714618Z","iopub.execute_input":"2021-07-01T22:32:02.714954Z","iopub.status.idle":"2021-07-01T22:32:03.316992Z","shell.execute_reply.started":"2021-07-01T22:32:02.714929Z","shell.execute_reply":"2021-07-01T22:32:03.316025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop null values\ndf17_20.dropna(inplace=True)\ndf19_20.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:32:03.318184Z","iopub.execute_input":"2021-07-01T22:32:03.318844Z","iopub.status.idle":"2021-07-01T22:32:12.694899Z","shell.execute_reply.started":"2021-07-01T22:32:03.318802Z","shell.execute_reply":"2021-07-01T22:32:12.694088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final check for null values\nprint(f\"\"\"\n**df17_20 dataset**\\n\nTotal Number of Null values:\\n\\n{df17_20.isna().sum()}\\n\n----------------------------------------\\n\nTotal Percentage of Null values:\\n\\n{df17_20.isna().mean()*100}\\n\n========================================\\n\n**df19_20 dataset**\\n\nTotal Number of Null values:\\n\\n{df19_20.isna().sum()}\\n\n----------------------------------------\\n\nTotal Percentage of Null values:\\n\\n{df19_20.isna().mean()*100}\\\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:32:12.696035Z","iopub.execute_input":"2021-07-01T22:32:12.69646Z","iopub.status.idle":"2021-07-01T22:32:24.461982Z","shell.execute_reply.started":"2021-07-01T22:32:12.696409Z","shell.execute_reply":"2021-07-01T22:32:24.460996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save cleaned datasets to new csv\n# df17_20.to_csv('../data/processed/df17_20.csv')\n# df19_20.to_csv('../data/processed/df19_20.csv') ","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:32:24.462813Z","iopub.execute_input":"2021-07-01T22:32:24.463019Z","iopub.status.idle":"2021-07-01T22:32:24.46733Z","shell.execute_reply.started":"2021-07-01T22:32:24.462997Z","shell.execute_reply":"2021-07-01T22:32:24.46551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Section 3: Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"Back to [Table of Contents](#Table-of-Contents).<br>\n\nThis section will be devoted to an exploratory data analysis of our cleaned dataset and will have a conclusion of all the insights we can find in the data.","metadata":{}},{"cell_type":"code","source":"# Display most and least used stations of 2017 \ntrips17.groupby(['Start Station Id','Start Station Name']).size().reset_index(name='count').sort_values(by = 'count', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:32:24.469005Z","iopub.execute_input":"2021-07-01T22:32:24.469296Z","iopub.status.idle":"2021-07-01T22:32:24.741749Z","shell.execute_reply.started":"2021-07-01T22:32:24.469267Z","shell.execute_reply":"2021-07-01T22:32:24.740667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display most and least used stations of 2018\ntrips18.groupby(['Start Station Id','Start Station Name']).size().reset_index(name='count').sort_values(by = 'count', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:32:24.742926Z","iopub.execute_input":"2021-07-01T22:32:24.743219Z","iopub.status.idle":"2021-07-01T22:32:25.089819Z","shell.execute_reply.started":"2021-07-01T22:32:24.74319Z","shell.execute_reply":"2021-07-01T22:32:25.088923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display most and least used stations of 2019\ntrips19.groupby(['Start Station Id','Start Station Name']).size().reset_index(name='count').sort_values(by = 'count', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:32:25.090917Z","iopub.execute_input":"2021-07-01T22:32:25.0912Z","iopub.status.idle":"2021-07-01T22:32:25.546826Z","shell.execute_reply.started":"2021-07-01T22:32:25.091171Z","shell.execute_reply":"2021-07-01T22:32:25.546005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display most and least used stations of 2020\ntrips20.groupby(['Start Station Id','Start Station Name']).size().reset_index(name='count').sort_values(by = 'count', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:32:25.548484Z","iopub.execute_input":"2021-07-01T22:32:25.548799Z","iopub.status.idle":"2021-07-01T22:32:25.931514Z","shell.execute_reply.started":"2021-07-01T22:32:25.548766Z","shell.execute_reply":"2021-07-01T22:32:25.930583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display most and least used stations of the last 4 years\ntrips17_20.groupby(['Start Station Id','Start Station Name']).size().reset_index(name='count').sort_values(by = 'count', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:32:25.932463Z","iopub.execute_input":"2021-07-01T22:32:25.932761Z","iopub.status.idle":"2021-07-01T22:32:27.037315Z","shell.execute_reply.started":"2021-07-01T22:32:25.932729Z","shell.execute_reply":"2021-07-01T22:32:27.036506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Observations_**<br>\n- Union Station used to be one of the top used stations between 2017-2019 but was knocked off most likely due to the city being locked down and people working from home.\n- In the beginning the service may have been most frequently used by commuters judging by the most used locations but gradually it became more leisure focussed with most used stations now located along the waterfront.","metadata":{}},{"cell_type":"code","source":"# Display feature statistics\nprint(\"\\n**df17_20 dataset**\")\ndisplay(df17_20.describe().apply(lambda x: x.apply('{0:.2f}'.format)))\nprint(\"\\n**df19_20 dataset**\")\ndisplay(df19_20.describe().apply(lambda x: x.apply('{0:.2f}'.format)))","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:32:27.038678Z","iopub.execute_input":"2021-07-01T22:32:27.038976Z","iopub.status.idle":"2021-07-01T22:32:38.624368Z","shell.execute_reply.started":"2021-07-01T22:32:27.038943Z","shell.execute_reply":"2021-07-01T22:32:38.623068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Observations_**\n\n- Longest Trip Duration is 206,730 minutes\n- Mean Trip Duration is 17.8  minutes with a standard deviation of 191\n- 50% of all trips end at 12 minutes\n- The hottest temperature the last 4 years was 34.1째C and the coldest was -21.9째C\n- The highest relative humidity recorded in the last 4 years was 81% and the lowest was 16%\n- The windiest speed recorded was 76 km/h","metadata":{}},{"cell_type":"markdown","source":"As suspected from the previously observations before running this graph that it would be heavily skewed to the right where we see that the majority of rides are concentrated in the 12 min mark with the long tail of rides that stretch out to the longest trip length of ~200,000 min.\n\nWe will need to address this imbalance by either binning the trips into categories or dropping datapoints where trips exceed a threshold.\n\nBased on the Toronto BikeShare Usage Fees\nhttps://bikesharetoronto.com/get-pass/\n\nRiders only get overfee charges after the trip length is >30min so we can categorize the trips into 3 categories to simplify the ride stats. Trips <10min would be categorized as short trips, >10min to <30min as medium trips and >30min for long trips.\n\nCapping the max trip duration to 540,000 Seconds or (9,000 Minutes) because that is the same cost that BikeShare Toronto will charge a member for the bike to be replaced.\n\n\\\\$1,200 for bike replacement \\\\$3.75 for first 30min(1,800sec) + \\\\$ 4.00 each 30min(1,800sec) over.","metadata":{}},{"cell_type":"code","source":"# Replacing trip duration greater than 540,000 to 540,000\ndf17_20['Trip Duration (Seconds)'].where(df17_20['Trip Duration (Seconds)'] <= 540000, 540000, inplace=True)\ndf19_20['Trip Duration (Seconds)'].where(df19_20['Trip Duration (Seconds)'] <= 540000, 540000, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:32:38.626468Z","iopub.execute_input":"2021-07-01T22:32:38.626824Z","iopub.status.idle":"2021-07-01T22:32:38.676872Z","shell.execute_reply.started":"2021-07-01T22:32:38.626792Z","shell.execute_reply":"2021-07-01T22:32:38.67569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Histograms","metadata":{}},{"cell_type":"markdown","source":"Back to [Table of Contents](#Table-of-Contents).<br>","metadata":{}},{"cell_type":"code","source":"# Ignore runtime warning\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Display histogram for each feature in dataframe\nfor col in df17_20:\n    print(f\"\"\"\n------------------------------------------------------------\n{col}\n------------------------------------------------------------\n    \"\"\")\n    plt.figure(figsize=(16, 9))\n    plt.title(col)\n    plt.hist(df17_20[col], color='cornflowerblue', bins=100)\n    plt.xlabel(col)\n    sns.despine()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:32:38.678103Z","iopub.execute_input":"2021-07-01T22:32:38.678387Z","iopub.status.idle":"2021-07-01T22:34:02.876888Z","shell.execute_reply.started":"2021-07-01T22:32:38.678357Z","shell.execute_reply":"2021-07-01T22:34:02.875645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Observations_**<br>\n- There is a clear seasonality displayed in the Start Time and End Time columns based on bike usage trend. \n- There tight binning happening in the Latitude and Longitude columns that may connote tight clustering of stations or that users take trips in a small geographic area, this can be better displayed on a map or network graph.\n- Most of thr rides happen at the lower range of elevation between 80-100m.\n- Most common trips are taken from stations with capacity around 20 bikes then 15 bikes and thirdly 10 bikes.\n- Most common temperature at the start of trip is around 18-25째C which can indicate that is the most ideal temperature bike users prefer to take trips.\n- Most common relative humidity at the start of trip is between 60% to 80% which can indicate that is the most ideal relative humidity bike users prefer to take trips.\n- While majority of rides happen when there are no weather events, rain seems the most common for riders to ride in compared to other events.","metadata":{}},{"cell_type":"code","source":"# Display most active FSA in the last 4 years\ndf17_20.groupby(['Start Station FSA']).size().reset_index(name='count').sort_values(by = 'count', ascending=False).head()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:34:02.878791Z","iopub.execute_input":"2021-07-01T22:34:02.879144Z","iopub.status.idle":"2021-07-01T22:34:03.983536Z","shell.execute_reply.started":"2021-07-01T22:34:02.879113Z","shell.execute_reply":"2021-07-01T22:34:03.982334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**_Observations_**<br>\nThe top 5 most active FSA neighbourhoods are all downtown areas with M5V being the most active and also most dense in population as well as encompassing a number of tourist attractions.","metadata":{}},{"cell_type":"markdown","source":"### Time Scale Graphs\n\nBack to [Table of Contents](#Table-of-Contents).<br>\n\nI will now chart to time series graphs of the bike usage over the 4 years of dataset.","metadata":{}},{"cell_type":"code","source":"# Saving all trips `Start Time` to a temp series\nt_series = pd.to_datetime(df17_20['Start Time'])\n\n# Creating dataframe and groupby day with count of trips\ndtrips_df = t_series.groupby(t_series.dt.floor('d')).size().reset_index(name='Trip Count')\n\n# Check\ndisplay(dtrips_df.head(5))","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:34:03.984797Z","iopub.execute_input":"2021-07-01T22:34:03.985077Z","iopub.status.idle":"2021-07-01T22:34:04.260835Z","shell.execute_reply.started":"2021-07-01T22:34:03.985048Z","shell.execute_reply":"2021-07-01T22:34:04.260043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib.dates import DateFormatter\n\n# Plot time series chart for all daily trips\nplt.figure(figsize=(20,8))\nax = plt.gca()\nplt.plot(dtrips_df['Start Time'], dtrips_df['Trip Count'], marker='.')\nplt.xlabel('Date')\nplt.ylabel('Trips')\nplt.title('Total Daily Trips over the Last 4 Years')\nax.autoscale(enable=True, axis='x', tight=True)\nax.xaxis.set_major_formatter(DateFormatter(\"%b %Y\"))\n\nsns.despine()\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:34:04.262155Z","iopub.execute_input":"2021-07-01T22:34:04.262688Z","iopub.status.idle":"2021-07-01T22:34:04.767286Z","shell.execute_reply.started":"2021-07-01T22:34:04.262646Z","shell.execute_reply":"2021-07-01T22:34:04.766347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**<br>\nTrips have been on the upward trend over the years with the last few months having the most variation between each day compared to previous years.","metadata":{}},{"cell_type":"code","source":"# Closer look into each year\ndtrips_df17 = dtrips_df[dtrips_df['Start Time'].dt.year==2017]\ndtrips_df18 = dtrips_df[dtrips_df['Start Time'].dt.year==2018]\ndtrips_df19 = dtrips_df[dtrips_df['Start Time'].dt.year==2019]\ndtrips_df20 = dtrips_df[dtrips_df['Start Time'].dt.year==2020]","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:34:04.768599Z","iopub.execute_input":"2021-07-01T22:34:04.768903Z","iopub.status.idle":"2021-07-01T22:34:04.782963Z","shell.execute_reply.started":"2021-07-01T22:34:04.768874Z","shell.execute_reply":"2021-07-01T22:34:04.781807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Subplots of the time series chart of each year\n# Can possibly shorten the code in the fture by writing a loop function.\n\nplt.subplots(4, 2, figsize=(14, 20))\n\n# Daily trips for 2017\nplt.subplot(4, 1, 1)\nax = plt.gca()\nplt.plot(dtrips_df17['Start Time'], dtrips_df17['Trip Count'], marker='.')\nplt.xlabel('Date')\nplt.ylabel('Trips')\nplt.title('Total Daily Trips over 2017')\nax.autoscale(enable=True, axis='x', tight=True)\nax.xaxis.set_major_formatter(DateFormatter(\"%b %Y\"))\n\n# Daily trips for 2018\nplt.subplot(4, 1, 2)\nax = plt.gca()\nplt.plot(dtrips_df18['Start Time'], dtrips_df18['Trip Count'], marker='.')\nplt.xlabel('Date')\nplt.ylabel('Trips')\nplt.title('Total Daily Trips over 2018')\nax.autoscale(enable=True, axis='x', tight=True)\nax.xaxis.set_major_formatter(DateFormatter(\"%b %Y\"))\n\n# Daily trips for 2019\nplt.subplot(4, 1, 3)\nax = plt.gca()\nplt.plot(dtrips_df19['Start Time'], dtrips_df19['Trip Count'], marker='.')\nplt.xlabel('Date')\nplt.ylabel('Trips')\nplt.title('Total Daily Trips over 2019')\nax.autoscale(enable=True, axis='x', tight=True)\nax.xaxis.set_major_formatter(DateFormatter(\"%b %Y\"))\n\n# Daily trips for 2020\nplt.subplot(4, 1, 4)\nax = plt.gca()\nplt.plot(dtrips_df20['Start Time'], dtrips_df20['Trip Count'], marker='.')\nplt.xlabel('Date')\nplt.ylabel('Trips')\nplt.title('Total Daily Trips over 2020')\nax.autoscale(enable=True, axis='x', tight=True)\nax.xaxis.set_major_formatter(DateFormatter(\"%b %Y\"))\n\nsns.despine()\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:34:04.784539Z","iopub.execute_input":"2021-07-01T22:34:04.784951Z","iopub.status.idle":"2021-07-01T22:34:06.446736Z","shell.execute_reply.started":"2021-07-01T22:34:04.784918Z","shell.execute_reply":"2021-07-01T22:34:06.445978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating dataframe and groupby `Start Station Id` and `Start Station Name` with count of trips\n# trips17_20.groupby(['Start Station Id','Start Station Name']).size().reset_index(name='count')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T22:34:06.447801Z","iopub.execute_input":"2021-07-01T22:34:06.448187Z","iopub.status.idle":"2021-07-01T22:34:06.451176Z","shell.execute_reply.started":"2021-07-01T22:34:06.448156Z","shell.execute_reply":"2021-07-01T22:34:06.450499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n## Section 4: Conclusions\n\n#### Observations & Trends\nJust to reinterate the observations and trends from before:\n- Trips have been on the upward trend over the years with the 2020 having the most weekly variation compared to previous years.\n- There is also a clear seasonality each year where the most rides happen in the heart of summer and least rides in the winter months. \n-  Station usage pattern have been relatively consistant the previous 3 years where the top 5 start and stop stations have been a mix of downtown stations and harbourfront where in 2020 all the top start and stop stations are along the harbourfront.\n- Most of thr rides happen at the lower range of elevation between 80-100m, indicating this to be places closer to the water.\n- Most common trips are taken from stations with capacity around 20 bikes then 15 bikes and thirdly 10 bikes.\n- Most common temperature at the start of trip is around 18-25째C which can indicate that is the most ideal temperature bike users prefer to take trips.\n- Most common relative humidity at the start of trip is between 60% to 80% which can indicate that is the most ideal relative humidity bike users prefer to take trips.\n- Most trips happen outside of weather events with rain being the most common if there were to be a ride when a weather event is occurring.\n- The top 5 most active FSA neighbourhoods are all downtown areas with M5V being the most active and also most dense in population as well as encompassing a number of tourist attractions.\n\n#### Suggestions\n- While 2020 was an atypical year from others there is a clear trend for higher weekend usage and that was amplified in 2020 and could well continue into the 2021 as people get more used to the service as tourism ramps back up.\n\n#### Next Steps\n- Complete Machine Learning Models, Time Series Analysis, and Network Analysis in subsequent notebooks.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}