{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import keras\nimport tensorflow as tf\nimport keras.layers as layers\nfrom keras.preprocessing.image import load_img, img_to_array, array_to_img\nimport tensorflow_probability as tfp\nimport numpy as np\nfrom math import floor, ceil\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow import math, random, shape\nimport os\nfrom keras.losses import MeanSquaredError, BinaryCrossentropy\nfrom keras.optimizers import Nadam, SGD, Adam, Adamax\nfrom keras.activations import sigmoid\nfrom tensorflow import convert_to_tensor as tens\nfrom keras import backend as K\nfrom cv2 import getGaborKernel as Gabor\nfrom functools import reduce\nfrom matplotlib import pyplot as plt\nfrom math import sqrt\nimport itertools\nimport re\nfrom random import shuffle, seed\nfrom tensorflow.keras.utils import Sequence\nfrom keras.constraints import NonNeg\nfrom keras.regularizers import l1,l2,l1_l2\nfrom keras.initializers import RandomNormal\nimport pickle\nimport time\nimport pandas as pd","metadata":{"id":"imv7hlmyTW7X","executionInfo":{"status":"ok","timestamp":1608812549446,"user_tz":-120,"elapsed":2997,"user":{"displayName":"Roy Urbach","photoUrl":"","userId":"03979889048736445362"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 32\nEXCITATORY_SYNAPSES_WANTED = 8\nINHIBITORY_SYNAPSES_WANTED = 6\nPRESYNAPTIC_THRESHOLD = .001\nN_EXC = 639\nLABELS = [\"EVEN\", \"ODD\"]\nLABELS = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\nNLABELS = 2","metadata":{"id":"cEiFIsPuIAzE","executionInfo":{"status":"ok","timestamp":1608822940208,"user_tz":-120,"elapsed":671,"user":{"displayName":"Roy Urbach","photoUrl":"","userId":"03979889048736445362"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transform data to dataset","metadata":{"id":"nUlxq41-TW7e"}},{"cell_type":"code","source":"class SHDDirectoryGeneratorSequence(Sequence):\n    def __init__(self, dct_label, dtype='.jpeg', balance=True, randomize=True, noise=0, delay=0, random_seed=1331, validation_split=None, is_validation=False, batch_size=32):\n        self.directories = dct_label\n        self.dtype = dtype\n        self.randomize = randomize\n        self.seed = random_seed\n        self.validation_split = validation_split\n        self.is_validation = is_validation\n        self.batch_size = batch_size\n        self.x = self.y = None\n        self.balance = True\n        self.noise = None if not noise else NoiseLayer(noise)\n        self.delay = None if not delay else GeometricDelay(delay)\n        if set(dct_label.values()) != {0,1}: raise Exception(\"Labels should be 0 and 1.\")\n            \n        self._create_files()\n        print(self)\n        \n    def _create_files(self):\n        lbls_dct = {0:[], 1:[]}\n        files = []\n        for directory,label in self.directories.items():\n            lbls_dct[label] += [(directory+ ('' if directory[-1] == '/' else '/') +i, label) \n                                for i in os.listdir(directory) if re.findall(self.dtype, i)]\n        if self.balance:\n\n            seed(self.seed)\n            l0 = len(lbls_dct[0])\n            l1 = len(lbls_dct[1])\n            if l0 > l1:\n                arr = np.array([1]*l1+[0]*(l0-l1))\n                shuffle(arr)\n                new = []\n                for i, j in zip(lbls_dct[0], list(arr)):\n                    if j: new.append(i)\n                lbls_dct[0] = new\n            elif l0<l1:\n                arr = np.array([1]*l0+[0]*(l1-l0))\n                shuffle(arr)\n                new = []\n                for i, j in zip(lbls_dct[1], list(arr)):\n                    if j: new.append(i)\n                lbls_dct[1] = new\n        files = lbls_dct[0] + lbls_dct[1]\n                \n        if self.randomize:\n            seed(self.seed)\n            shuffle(files)\n        if self.validation_split:\n            if self.is_validation:\n                files = files[floor(len(files) - len(files)*self.validation_split):]\n            else:\n                files = files[:floor(len(files) - len(files)*self.validation_split)]\n                \n        self.x, self.y = zip(*files)\n        self.y = np.array(self.y)\n    \n    def __len__(self):\n        return ceil(self.y.shape[0] / self.batch_size)\n    \n    def __getitem__(self, idx):\n        batch_x_pre, batch_y = self.x[idx * self.batch_size:(idx + 1) * self.batch_size], self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_x = [self.getSpikeTrain(file) for file in batch_x_pre]\n        return np.stack(batch_x, axis=0), batch_y\n    \n    def getSpikeTrain(self, file):\n        image = load_img(file)\n        image = img_to_array(image)[:,:,np.newaxis,0] / 255\n        if self.noise is not None:\n            image = self.noise(image, training=not self.is_validation)\n        if self.delay is not None:\n            image = self.delay(image, training=not self.is_validation)\n        return image\n    \n    def __str__(self):\n        return (\"Training\" if not self.is_validation else \"Validation\") + f\" generator: Got {len(self.x)} files and 2 categories\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SHDDirectoryGeneratorSequenceCategories(Sequence):\n    def __init__(self, dct_label, dtype='.jpeg', randomize=True, random_seed=1331, validation_split=None, is_validation=False, batch_size=32):\n        self.directories = dct_label\n        self.categories = sorted(list({cat for cat in dct_label.values()}))\n        self.dtype = dtype\n        self.randomize = randomize\n        self.seed = random_seed\n        self.validation_split = validation_split\n        self.is_validation = is_validation\n        self.batch_size = batch_size\n        self.x = self.y = None\n        self._create_files()\n        \n    def _create_files(self):\n        files = []\n        for directory,label in self.directories.items():\n            files += [(directory+ ('' if directory[-1] == '/' else '/') +i, \n                       [1. if cat==label else 0. for cat in self.categories]) \n                           for i in os.listdir(directory) \n                           if re.findall(self.dtype, i)]\n        if self.randomize:\n            seed(self.seed)\n            shuffle(files)\n        if self.validation_split:\n            if self.is_validation:\n                files = files[floor(len(files) - len(files)*self.validation_split):]\n            else:\n                files = files[:floor(len(files) - len(files)*self.validation_split)]\n                \n        self.x, self.y = zip(*files)\n        self.y = np.array(self.y)\n        print(self)\n    \n    def __len__(self):\n        return ceil(self.y.shape[0] / self.batch_size)\n    \n    def __getitem__(self, idx):\n        batch_x_pre, batch_y = self.x[idx * self.batch_size:(idx + 1) * self.batch_size], self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_x = [self.getSpikeTrain(file) for file in batch_x_pre]\n        return np.array(batch_x), batch_y\n    \n    def getSpikeTrain(self, file):\n        image = load_img(file)\n        image = img_to_array(image)[:,:,np.newaxis,0] / 255\n        return image\n    \n    def __str__(self):\n        return (\"Training\" if not self.is_validation else \"Validation\") + f\" generator: Got {len(self.x)} files and {len(self.categories)} categories\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NoiseLayer(keras.layers.Layer):\n    def __init__(self, p=.003):\n        super().__init__(name=\"NoiseLayer\")\n        self.p = p\n        self.shape = None\n    def call(self, inputs, training=None):\n        if training is not True:\n            allshape = 1\n            for i in inputs.shape: allshape *=i\n            noise = (np.random.rand(allshape) > (1-self.p)).reshape(inputs.shape)[np.newaxis]\n            together = np.concatenate([inputs[np.newaxis], noise])\n            return together.max(axis=0)\n        else: return inputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GeometricDelay(keras.layers.Layer):\n    def __init__(self, p=.02, to_plot=False):\n        super().__init__(name=\"GeometricDelay\")\n        self.p = p\n        self.to_plot = to_plot\n        \n    def call(self, inputs, training=None):\n        if training is True or self.to_plot:\n            fulltime = inputs.shape[-2]\n            delay = tf.keras.layers.Lambda(lambda x: self.get_delay())(None)\n#             delay = self.get_delay()\n            if len(inputs.shape) == 4:\n                inputs = layers.ZeroPadding2D(((0,0), (delay,0)))(inputs)[:, :, :fulltime, :]\n            elif len(inputs.shape) == 3:\n                inputs = layers.ZeroPadding1D((delay,0))(inputs)[:, :fulltime, :]\n            return inputs\n        else: return inputs\n        \n    def get_delay(self):\n#         return np.random.geometric(self.p, 1)\n        return tf.cast(tfp.distributions.Geometric(probs=[self.p]).sample()[0], dtype=tf.int32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LABELS = [\"NoI\", \"I\"]\nINums = {3, 5, 6, 8, 9}\ntrain_ds = SHDDirectoryGeneratorSequence({f\"../input/matrixshd/train/{i}\": int(i in INums) for i in range(10)}, validation_split=0.2, batch_size=BATCH_SIZE, noise=False, delay=.01)\nvalid_ds = SHDDirectoryGeneratorSequence({f\"../input/matrixshd/train/{i}\": int(i in INums) for i in range(10)}, validation_split=0.2, is_validation=True, batch_size=BATCH_SIZE)\ntest_ds = SHDDirectoryGeneratorSequence({f\"../input/matrixshd/test/{i}\": int(i in INums) for i in range(10)}, validation_split=0, is_validation=False, batch_size=BATCH_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_ds = SHDDirectoryGeneratorSequenceCategories({f\"../input/matrixshd/train/{i}\": i%10 for i in range(10)}, validation_split=0.2, batch_size=BATCH_SIZE)\n# valid_ds = SHDDirectoryGeneratorSequenceCategories({f\"../input/matrixshd/train/{i}\": i%10 for i in range(10)}, validation_split=0.2, is_validation=True, batch_size=BATCH_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_ds = SHDDirectoryGeneratorSequenceCategories({f\"../input/matrixshd/train/{i}\": i==0 for i in range(NLABELS)}, validation_split=0.2, batch_size=BATCH_SIZE)\n# valid_ds = SHDDirectoryGeneratorSequenceCategories({f\"../input/matrixshd/train/{i}\": i==0 for i in range(NLABELS)}, validation_split=0.2, is_validation=True, batch_size=BATCH_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Binary","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(40,40))\nimage, label = train_ds.__getitem__(0)\nfor i in range(BATCH_SIZE):\n    plt.subplot(BATCH_SIZE//4, 4, i+1)\n    plt.imshow(image[i,:,:,0],aspect='auto',origin='lower', cmap=\"binary\")\n    plt.title(LABELS[label[i]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Categorical","metadata":{}},{"cell_type":"code","source":"# plt.figure(figsize=(40,40))\n# for _, (image, label) in enumerate(train_ds):\n#     for i in range(BATCH_SIZE):\n#         plt.subplot(BATCH_SIZE//4, 4, i+1)\n#         plt.imshow(image[i,:,:,0],aspect='auto',origin='lower', cmap=\"binary\")\n#         plt.title(LABELS[np.where(label[i] == 1.)[0][0]])\n#     break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Use David Beniaguev's (selfishgene) trained L5PC model","metadata":{"id":"P-VtQx1GTW7g"}},{"cell_type":"code","source":"class Module:\n    def __init__(self, path, module_name, name):\n        self.name = name\n        self.module = self.create_module(path, module_name)\n    \n    def create_module(self, path, module_name):\n        models_folder  = os.path.join(path, 'Models')\n        model_filename  = os.path.join(models_folder, module_name)\n        old_model = keras.models.load_model(model_filename)\n        inp = keras.Input(shape=old_model.layers[0].input.shape[1:])\n        x = old_model.layers[1](inp)\n        for layer in old_model.layers[2:-2-1]:\n            x = layer(x)\n        output = old_model.layers[-3](x)\n        module = keras.Model(inp, output)\n#         L5PC_model.compile(optimizer=Nadam(lr=0.0001), loss='binary_crossentropy', loss_weights=[1.])\n\n        for layer in module.layers:\n            layer.trainable = False\n\n        module.summary()\n        return module  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multiple_neurons = False\ndataset_folder = '/kaggle/input/single-neurons-as-deep-nets-nmda-test-data'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if multiple_neurons:\n    neurons = [Module(dataset_folder, \"NMDA_TCN__DWT_8_224_217__model.h5\", \"Module_8_layers\"), \n           Module(dataset_folder, \"NMDA_TCN__DWT_7_292_169__model.h5\", \"Module_7_layers\"), \n           Module(dataset_folder, \"NMDA_TCN__DWT_9_256_241__model.h5\", \"Module_9_layers\")]\nelse:\n    neurons = Module(dataset_folder, \"NMDA_TCN__DWT_7_128_153__model.h5\", \"module_7_layers\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Some custom layers and funcs to use","metadata":{"id":"fv097qMmTW7g"}},{"cell_type":"code","source":"class ToBoolLayer(keras.layers.Layer):\n    def __init__(self, threshold=0.5, mult=15, use_sigmoid=True, use_special_sigmoid=None, name=\"ToBoolLayer\"):\n        super().__init__(name=name)\n        self.use_sigmoid = use_sigmoid\n        self.special_sigmoid = use_special_sigmoid\n        self.threshold = threshold\n        self.mult = mult\n    def build(self, shape):\n        if self.special_sigmoid is not None:\n            if not len(self.special_sigmoid) == 2:\n                raise Exception(\"Special sigmoid should be in format (pos_mult, neg_mult).\")\n            else:\n                self.sigmoid = SigmoidThreshold(*self.special_sigmoid, self.threshold)\n        elif self.use_sigmoid:\n            self.sigmoid = SigmoidThresholdEasier(threshold=self.threshold, mult=self.mult)\n    \n    def call(self, inputs, training=None):\n        if not training:\n            inputs = inputs - self.threshold\n            inputs = math_ops.ceil(inputs)\n        elif self.use_sigmoid:\n            inputs = self.sigmoid(inputs)\n        return inputs","metadata":{"id":"DDTEqC7ZTW7h","executionInfo":{"status":"ok","timestamp":1608812566737,"user_tz":-120,"elapsed":540,"user":{"displayName":"Roy Urbach","photoUrl":"","userId":"03979889048736445362"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SpikeProcessor(keras.layers.Layer):\n    def __init__(self, nSynapses, start=None, end=None, name=\"SpikeProcessor\"):\n        super().__init__(name=name)\n        self.nSynapses = nSynapses\n        self.start = start\n        self.end = end\n\n    def build(self, shape):\n        pass\n    \n    def call(self, inputs):\n        if self.start is not None:\n            if self.end is not None:\n                inputs = inputs[:,:,self.start:self.end]\n            else:\n                inputs = inputs[:,:,self.start:]\n        elif self.end is not None:\n            inputs = inputs[:,:,:self.end]\n        preds_sum = math.reduce_sum(inputs, axis=-1, keepdims=True)\n        return_value = preds_sum/self.nSynapses\n        return layers.Flatten()(return_value)","metadata":{"id":"BZT3ZQ5ATW7h","executionInfo":{"status":"ok","timestamp":1608812567060,"user_tz":-120,"elapsed":854,"user":{"displayName":"Roy Urbach","photoUrl":"","userId":"03979889048736445362"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PredictNeuron(keras.layers.Layer):\n    \"\"\"flattens the temporal dimensions, order by new_order and pads with 0's to fill\"\"\"\n    def __init__(self, neuron_module, name=\"NeuronPrediction\"):\n        super().__init__(name=name)\n        self.neurons = neuron_module\n        self.single = not isinstance(self.neurons, list)\n        if self.single:\n            print(\"Neuron module is\", self.neurons.name)\n        else:\n            message = f\"Averaging between {len(self.neurons)} neurons:\\n\"\n            for neuron in self.neurons:\n                message += f\"\\t- {neuron.name}\\n\"\n            print(message)\n                \n    def call(self, inputs):\n        if not self.single:\n            outputs = math_ops.reduce_mean(tf.stack([module.module(inputs) for module in self.neurons], axis=-1), axis=-1)\n        else:\n            outputs = self.neurons.module(inputs)\n        return outputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loss_for_me(y_true, y_preds):\n    return MeanSquaredError()(1, y_preds)","metadata":{"id":"pZ2H3wGyrbUd","executionInfo":{"status":"ok","timestamp":1608812568706,"user_tz":-120,"elapsed":639,"user":{"displayName":"Roy Urbach","photoUrl":"","userId":"03979889048736445362"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def spikes_preds_processing(preds, n_spikes=50):\n    preds_sum = math.reduce_sum(preds, axis=-1, keepdims=True)\n    return preds_sum/(n_spikes)","metadata":{"id":"5f3B1SWXTW7i","executionInfo":{"status":"ok","timestamp":1608812569049,"user_tz":-120,"elapsed":971,"user":{"displayName":"Roy Urbach","photoUrl":"","userId":"03979889048736445362"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def SigmoidThreshold(pos_mult=1, neg_mult=1, threshold=0):\n    \"\"\"returns a sigmoid function [0,1]->[0,1] with the center at threshold given, and slope multified\"\"\"\n    def sigmoid_threshold(x):\n        new_x = -x+threshold\n        multiplier = (pos_mult + neg_mult + (-pos_mult + neg_mult)*K.sign(new_x)) / 2 \n        return 1/(1+math.exp(multiplier*(new_x)))\n    return sigmoid_threshold","metadata":{"id":"Wl7uhvKsTW7i","executionInfo":{"status":"ok","timestamp":1608812569050,"user_tz":-120,"elapsed":964,"user":{"displayName":"Roy Urbach","photoUrl":"","userId":"03979889048736445362"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = np.arange(0,1,0.01)\nthreshold = 1/16\npos_mult = 10\nneg_mult = 50\nplt.plot(x, SigmoidThreshold(pos_mult, neg_mult, threshold)(x))\nplt.ylim(0,1)\nplt.axhline(0.5, color='r', linestyle='--')\nplt.axvline(threshold, color='g', linestyle='--')\nplt.show()","metadata":{"id":"AgCnGTiqDqLR","executionInfo":{"status":"ok","timestamp":1608812569371,"user_tz":-120,"elapsed":1278,"user":{"displayName":"Roy Urbach","photoUrl":"","userId":"03979889048736445362"}},"outputId":"79b44f37-3a2c-4e87-d1a3-9b8231a9eaa1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def SigmoidThresholdEasier(mult=1, threshold=0.5):\n    \"\"\"returns a sigmoid function [0,1]->[0,1] with the center at threshold given, and slope multified\"\"\"\n    def sigmoid_threshold(x):\n        return 1/(1+math.exp(mult*(threshold-x)))\n    return sigmoid_threshold","metadata":{"id":"jYlYdJfVTW7j","executionInfo":{"status":"ok","timestamp":1608812569372,"user_tz":-120,"elapsed":1269,"user":{"displayName":"Roy Urbach","photoUrl":"","userId":"03979889048736445362"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"how the new sigmoid looks like","metadata":{"id":"awHJyR8-TW7j"}},{"cell_type":"code","source":"x = np.arange(0, 1, 0.01)\nthreshold = 0.01\nmult = 75\nplt.plot(x, SigmoidThresholdEasier(mult, threshold)(x))\nplt.ylim(0,1)\nplt.xlim(0,1)\nplt.axhline(0.5, color='r', linestyle='--')\nplt.axvline(threshold, color='g', linestyle='--')\nplt.show()","metadata":{"id":"ClSb-6fHTW7j","executionInfo":{"status":"ok","timestamp":1608812569373,"user_tz":-120,"elapsed":1263,"user":{"displayName":"Roy Urbach","photoUrl":"","userId":"03979889048736445362"}},"outputId":"5b4d512b-45b3-4eb6-c1d1-d9576e029bd5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"will be in use later","metadata":{"id":"hBjjPRoETW7k"}},{"cell_type":"code","source":"def set_num_syn_loss(syns_wanted_per_ms=50):\n    def num_syn_loss(y_true, y_preds):\n        return MeanSquaredError()(1,y_preds/syns_wanted_per_ms)\n    return num_syn_loss","metadata":{"id":"LymD9MkFTW7k","executionInfo":{"status":"ok","timestamp":1608812569374,"user_tz":-120,"elapsed":1255,"user":{"displayName":"Roy Urbach","photoUrl":"","userId":"03979889048736445362"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ToNeuronInput(keras.layers.Layer):\n    \"\"\"flattens the temporal dimensions, order by new_order and pads with 0's to fill\"\"\"\n    def __init__(self, full, padding=0, new_order=None, name=\"NeuronInput\", part=200, batch_size=BATCH_SIZE):\n        super().__init__(name=name)\n        self.full = full\n        self.new_order = None\n        self.zero_padding = isinstance(padding, int) and padding == 0\n        self.padding = padding\n        self.part = part\n        self.batch_size = batch_size\n    \n    def build(self, shape):\n        self.ms = shape[1]*shape[2]\n        self.times = self.part // self.ms\n        print(self.padding.shape)\n        self.padding = tf.reshape(self.padding, (self.padding.shape[0]*self.padding.shape[1], self.padding.shape[-1]))\n        self.padding = K.variable(self.padding[np.newaxis, :self.full-self.ms*self.times])\n    \n    def call(self, inputs):\n        new_inp = layers.Reshape((self.ms, inputs.shape[-1]))(inputs)\n        if self.new_order is not None:\n            new_inp = gather(new_inp, self.new_order, axis=-2)\n        if self.times > 1:\n            new_inp = layers.Concatenate(axis=1)([new_inp for _ in range(self.times)])\n        if self.zero_padding:\n            new_inp = layers.ZeroPadding1D(padding=(self.full - self.ms*self.times, 0))(new_inp)\n        else:\n            new_inp = layers.Concatenate(axis=-2)([tf.tile(self.padding, [tf.shape(new_inp)[0], 1, 1]), new_inp])\n        return new_inp\n    \n    @tf.function\n    def gather(x, ind):\n        return tf.gather(x + 0, ind)","metadata":{"id":"gFrBZHkyTW7q","executionInfo":{"status":"ok","timestamp":1608812569375,"user_tz":-120,"elapsed":1247,"user":{"displayName":"Roy Urbach","photoUrl":"","userId":"03979889048736445362"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss","metadata":{}},{"cell_type":"code","source":"def MSE_RMS_SynapsesPerMS(wanted, size=N_EXC, batch_size=32, eps=1e-3):\n    def mse_rms(X, mu):\n        return tf.math.reduce_mean(tf.math.reduce_sum(tf.square(tf.math.sqrt(tf.math.reduce_mean(tf.square(X+eps), axis=-1)) - mu), axis=-1))\n    real_wanted = np.sqrt(wanted / size)\n    zeros_wanted = np.sqrt(1. - wanted / size)\n    def mse_rms_synapses_per_ms(y_true, y_preds):\n        result = tf.math.sqrt((mse_rms(y_preds, real_wanted)**2 + mse_rms(1. - y_preds, zeros_wanted)**2) / 2)\n        return result\n    return mse_rms_synapses_per_ms","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def MeanSquaredErrorSynapsesPerMS(batch_size=32, y_true_for_real=False):\n    def mean_squared_error_synapses_per_ms(y_true, y_preds):\n        squared_difference = tf.square((y_true if y_true_for_real else 1)-y_preds)\n        mean = tf.reduce_mean(squared_difference, axis=-1)\n        return mean\n    return mean_squared_error_synapses_per_ms","metadata":{"id":"Mq8hSWFxTW7r","executionInfo":{"status":"ok","timestamp":1608812569375,"user_tz":-120,"elapsed":1241,"user":{"displayName":"Roy Urbach","photoUrl":"","userId":"03979889048736445362"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def MaxErrorSynapsesPerMS(batch_size=32, y_true_for_real=False):\n    def max_error_synapses_per_ms(y_true, y_preds):\n        maxes = tf.reduce_max(y_preds, axis=-1)\n        squared_difference = tf.square(1-y_preds)\n        mean = tf.reduce_mean(squared_difference, axis=-1)\n        return mean\n    return max_error_synapses_per_ms","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_acceptable_spikes_per_ms = 3.0\nmax_acceptable_spikes_deviation = 20.0\nactivity_reg_constant = 0.2 * 0.0028\n\n\ndef pre_synaptic_spike_regularization(activation_map):\n    # sum over all dendritic locations\n    x = K.sum(activation_map, axis=2)\n\n    # ask if it's above 'max_acceptable_spikes_per_ms'\n    x = K.relu(x - max_acceptable_spikes_per_ms)\n\n    # if above threshold, apply quadratic penelty\n    x = K.square(x / max_acceptable_spikes_deviation)\n\n    # average everything\n    x = activity_reg_constant * K.mean(x)\n\n    return x","metadata":{"id":"a6NtHpNJTW7t","executionInfo":{"status":"ok","timestamp":1608812605586,"user_tz":-120,"elapsed":1334,"user":{"displayName":"Roy Urbach","photoUrl":"","userId":"03979889048736445362"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Noise Augmentation","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(90,30))\nsigma = .01\nnoise = NoiseLayer(sigma)\nfor _, (image, label) in enumerate(train_ds):\n    imageNoi = noise(image[:,:,:,0])\n    for i in range(3):\n        x = image[i,:,:,0]\n        plt.subplot(3,5,i+1)\n        plt.title(\"Original\")\n        plt.imshow(x, cmap=\"binary\")\n        plt.axis(\"off\")\n        plt.subplot(3,5,5+i+1)\n        plt.title(r\"Noise ($\\sigma$={})\".format(sigma))\n        plt.imshow(imageNoi[i], cmap=\"binary\")\n        plt.axis(\"off\")\n        plt.subplot(3,5, 11+i)\n        plt.title(\"Difference\")\n        plt.imshow(imageNoi[i]-image[i,:,:,0], cmap=\"binary\")\n        plt.axis(\"off\")\n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Delay Augmentation","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(40,10))\nfor _, (image, label) in enumerate(train_ds):\n    layer = GeometricDelay(.01, True)\n    for i in range(5):\n        imageDel = layer(image[np.newaxis, i])\n        plt.subplot(2,5,i+1)\n        plt.title(\"Original\")\n        plt.imshow(image[i,:,:,0], cmap=\"binary\")\n#         plt.axis(\"off\")\n        plt.subplot(2,5,5+i+1)\n        plt.title(\"Delay\")\n        plt.imshow(imageDel[0,:,:,0], cmap=\"binary\")\n#         plt.axis(\"off\")\n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Noisy Optimizer\nhttps://arxiv.org/abs/1511.06807\n\nhttps://github.com/cpury/keras_gradient_noise","metadata":{"id":"DCaWcWciTW7p"}},{"cell_type":"code","source":"import inspect\nimport importlib\n\ndef add_gradient_noise(BaseOptimizer, keras=None):\n    \"\"\"\n    Given a Keras-compatible optimizer class, returns a modified class that\n    supports adding gradient noise as introduced in this paper:\n    https://arxiv.org/abs/1511.06807\n    The relevant parameters from equation 1 in the paper can be set via\n    noise_eta and noise_gamma, set by default to 0.3 and 0.55 respectively.\n    By default, tries to guess whether to use default Keras or tf.keras based\n    on where the optimizer was imported from. You can also specify which Keras\n    to use by passing the imported module.\n    \"\"\"\n    if keras is None:\n        # Import it automatically. Try to guess from the optimizer's module\n        if hasattr(BaseOptimizer, '__module__') and BaseOptimizer.__module__.startswith('keras'):\n            keras = importlib.import_module('keras')\n        else:\n            keras = importlib.import_module('tensorflow.keras')\n\n    K = keras.backend\n\n    if not (\n        inspect.isclass(BaseOptimizer) and\n        issubclass(BaseOptimizer, keras.optimizers.Optimizer)\n    ):\n        raise ValueError(\n            'add_gradient_noise() expects a valid Keras optimizer'\n        )\n\n    def _get_shape(x):\n        if hasattr(x, 'dense_shape'):\n            return x.dense_shape\n\n        return K.shape(x)\n\n    class NoisyOptimizer(BaseOptimizer):\n        def __init__(self, noise_eta=0.3, noise_gamma=0.55, **kwargs):\n            super(NoisyOptimizer, self).__init__(**kwargs)\n            with K.name_scope(self.__class__.__name__):\n                self.noise_eta = K.variable(noise_eta, name='noise_eta')\n                self.noise_gamma = K.variable(noise_gamma, name='noise_gamma')\n\n        def get_gradients(self, loss, params):\n            grads = super(NoisyOptimizer, self).get_gradients(loss, params)\n\n            # Add decayed gaussian noise\n            t = K.cast(self.iterations, K.dtype(grads[0]))\n            variance = self.noise_eta / ((1 + t) ** self.noise_gamma)\n\n            grads = [\n                grad + K.random_normal(\n                    _get_shape(grad),\n                    mean=0.0,\n                    stddev=K.sqrt(variance),\n                    dtype=K.dtype(grads[0])\n                )\n                for grad in grads\n            ]\n\n            return grads\n\n        def get_config(self):\n            config = {'noise_eta': float(K.get_value(self.noise_eta)),\n                      'noise_gamma': float(K.get_value(self.noise_gamma))}\n            base_config = super(NoisyOptimizer, self).get_config()\n            return dict(list(base_config.items()) + list(config.items()))\n\n    NoisyOptimizer.__name__ = 'Noisy{}'.format(BaseOptimizer.__name__)\n\n    return NoisyOptimizer","metadata":{"id":"Z58q6uw_TW7p","executionInfo":{"status":"ok","timestamp":1608812586779,"user_tz":-120,"elapsed":9237,"user":{"displayName":"Roy Urbach","photoUrl":"","userId":"03979889048736445362"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NoisySGD = add_gradient_noise(SGD)\nNoisyNadam = add_gradient_noise(Nadam)\nNoisyAdamax = add_gradient_noise(Adamax)","metadata":{"id":"HVVeWEnsTW7p","executionInfo":{"status":"ok","timestamp":1608812586780,"user_tz":-120,"elapsed":9226,"user":{"displayName":"Roy Urbach","photoUrl":"","userId":"03979889048736445362"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eta = 0.03\ngamma = 0.55\n\nn = 150\n\nvar = []\nfor t in range(n):\n    var.append(eta/(1+t)**gamma)\nplt.plot(np.arange(n), var)\nplt.title(r'Noise $\\sigma^{2} $ through epochs')\nplt.xlabel('ephocs')\nplt.ylabel(r'noise $\\sigma^{2}$')\nplt.show()","metadata":{"id":"HzrjIvE0TW7p","executionInfo":{"status":"ok","timestamp":1608812586781,"user_tz":-120,"elapsed":9219,"user":{"displayName":"Roy Urbach","photoUrl":"","userId":"03979889048736445362"}},"outputId":"247350a7-ced5-4613-df00-3c911f6de4e0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SynapsePruner(keras.callbacks.Callback):\n    def __init__(self, rate1=0.9921752738654147, rate2=0.9765258215962441, iterations=8, axis=-1, splitPoint = N_EXC, prune_layer=\"WiringLayer\"):\n        super().__init__()\n        self.rate1 = rate1 * 100\n        self.rate2 = rate2 * 100\n        self.iterations = iterations\n        self.curIterations = iterations\n        self.split = splitPoint\n        self.layer_to_prune = prune_layer\n        print(self)\n    \n    def __str__(self):\n        string = f\"\\nSynapsePruner callback:\\nrate1 = {self.rate1} ({str(round((1-self.rate1/100)*N_EXC,2))} : {N_EXC})\\n\"\n        string += f\"rate2 = {self.rate2} ({str(round((1-self.rate2/100)*N_EXC, 2))} : {N_EXC})\\n\"\n        string += f\"every {self.iterations} iterations\"\n        return string\n        \n    def build(self, shape):\n        pass\n        \n#     @tf.autograph.experimental.do_not_convert\n    def on_train_batch_end(self, batch, logs=None):\n        if self.curIterations: self.curIterations -= 1\n        else: \n            self.curIterations = self.iterations\n            self.prune()\n    \n    def prune(self):\n        layer = self.model.get_layer(self.layer_to_prune)\n        kernels, bias = layer.get_weights()\n        kernels = kernels[:,0,0,:]\n        p1_x = kernels[:, :self.split]\n        p2_x = kernels[:, self.split:]\n        percentsP1 = np.percentile(p1_x, self.rate1, axis=-1)\n        percentsP2 = np.percentile(p2_x, self.rate2, axis=-1)\n        arr = np.concatenate([(p1_x.T>percentsP1).T, (p2_x.T > percentsP2).T], axis=-1)\n        arr = kernels * arr\n        assert arr.shape == kernels.shape, print(\"Oh no. something went wrong\")\n        layer.set_weights([arr[:,np.newaxis, np.newaxis, :], bias])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SynapsePruner(keras.callbacks.Callback):\n    def __init__(self, kmax1=5, kmax2=15, iterations=8, axis=-1, splitPoint = N_EXC, prune_layer=\"WiringLayer\"):\n        super().__init__()\n        self.kmax1 = kmax1\n        self.kmax2 = kmax2\n        self.iterations = iterations\n        self.curIterations = iterations\n        self.split = splitPoint\n        self.layer_to_prune = prune_layer\n        print(self)\n    \n    def __str__(self):\n        string = f\"\\nSynapsePruner callback:\\nkmax1 = {self.kmax1}\\n\"\n        string += f\"kmax2 = {self.kmax2}\\n\"\n        string += f\"every {self.iterations} iterations\\n Layer to Prune: {self.layer_to_prune}\"\n        return string\n        \n    def build(self, shape):\n        pass\n        \n#     @tf.autograph.experimental.do_not_convert\n    def on_train_batch_end(self, batch, logs=None):\n        if self.curIterations: self.curIterations -= 1\n        else: \n            self.curIterations = self.iterations\n            self.prune()\n    \n    def prune(self):\n        layer = self.model.get_layer(self.layer_to_prune)\n        kernels, bias = layer.get_weights()\n        kernels = kernels[:,0,0,:]\n        p1_x = kernels[:, :self.split]\n        p2_x = kernels[:, self.split:]\n        kmax1 = np.partition(p1_x, -self.kmax1, axis=-2)[-self.kmax1]\n        kmax2 = np.partition(p2_x, -self.kmax2, axis=-2)[-self.kmax2]\n        arr1 = p1_x * (p1_x >= kmax1)\n        arr2 = p2_x * (p2_x >= kmax2)\n        arr = np.concatenate([arr1, arr2], axis=-1)\n        assert arr.shape == kernels.shape, tf.print(\"Oh no. something went wrong\")\n        layer.set_weights([arr[:,np.newaxis, np.newaxis,:], bias])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NSynapseRegularizer(keras.regularizers.Regularizer):\n\n    def __init__(self, strength=0.01, nSynapseEx=5, nSynapseIn=5, threshold = 0.07, mult = 75):\n        self.strength = strength\n        self.sigmoid = SigmoidThresholdEasier(mult, threshold)\n        self.loss = lambda nSynapse, x: tf.reduce_mean(tf.math.pow(1 - x/nSynapse, 2))\n        self.nSynapseEx = nSynapseEx\n        self.nSynapseIn = nSynapseIn\n        self.percentEx = (1 - nSynapseEx / N_EXC) * 100\n        self.percentIn = (1 - nSynapseIn / N_EXC) * 100\n        print(self.percentEx)\n\n    def __call__(self, x):\n        \n#         tf.print(cur_max, tf.reduce_mean(x))\n        x = x[:,0,0,:] # self.sigmoid(x)[:,0,0,:]\n#         tf.print(tf.reduce_mean(x))\n        ex = x[:, :N_EXC]\n        inh = x[:, N_EXC:]\n        percentsEx = tfp.stats.percentile(ex, self.percentEx, axis=-1)\n        percentsIn = tfp.stats.percentile(inh, self.percentIn, axis=-1)\n#         tf.print(percentsEx.shape)\n#         tf.print((tf.transpose(x[:,:N_EXC])*percentsEx).shape)\n        dist_from_half_ex = tf.reduce_mean(1-tf.math.pow(percentsEx-tf.transpose(ex), 2))\n        dist_from_half_in = tf.reduce_mean(1-tf.math.pow(percentsIn-tf.transpose(inh), 2))\n#         exX = tf.reduce_sum(x[:,:N_EXC], axis=-1)\n#         inX = tf.reduce_sum(x[:,N_EXC:], axis=-1)\n#         lossEx = self.loss(self.nSynapseEx, exX)\n#         lossIn = self.loss(self.nSynapseIn, inX)\n#         return self.strength * (lossEx + lossIn + dist_from_half)\n        return self.strength * (dist_from_half_ex + dist_from_half_in)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CallNeuron(keras.layers.Layer):\n    def __init__(self, neuronModules, spare=150, name=\"CallNeuronLayer\"):\n        super().__init__(name=name)\n        self.neurons = neuronModules\n        self.spare = spare\n        self.neuronTime =  neuronModules.module.input.shape[-2] if not isinstance(neuronModules, list) else neuronModules[0].module.input.shape[-2]\n        self.synapses = neuronModules.module.input.shape[-1] if not isinstance(neuronModules, list) else neuronModules[0].module.input.shape[-1]\n        self.fullTime = None\n        self.timePerRun = self.neuronTime-self.spare\n        self.times = None\n        self.paddingSize = 0\n        self.neuron = None\n        \n    def build(self, shape):\n        self.neuron = PredictNeuron(self.neurons)\n#         print(\"Module is:\", self.neuron)\n        if shape[-1] != self.synapses:\n            raise Exception(f\"Wrong number of synapses! Neuron synapse number is {self.synapses} and input shape is {shape}\")\n        self.fullTime = shape[-2]\n        self.paddingSize = self.fullTime%self.timePerRun\n        self.times = [((self.timePerRun)*i, (self.timePerRun)*i+self.neuronTime) for i in range(self.fullTime//self.timePerRun)]    \n        \n    @tf.autograph.experimental.do_not_convert\n    def call(self, inputs, training=None):\n        inputs = layers.ZeroPadding1D(padding=((0,self.paddingSize)))(inputs)\n        outputs = layers.Concatenate(axis=-2)([self.neuron(inputs[:,self.times[0][0]:self.times[0][1]])]+[self.neuron(inputs[:,i:j,])[:,self.spare:] for i,j in self.times[1:]])\n        return outputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"id":"ruv8yAnaTW7r"}},{"cell_type":"code","source":"def create_model(optimizer, sigmoid_threshold=0.8, use_sigmoid=True, sigmoid_mult=25, dropout=.2, \n                 excitatory_wanted=EXCITATORY_SYNAPSES_WANTED, inhibitory_wanted=INHIBITORY_SYNAPSES_WANTED, qSynapse=(0.05, 0.05)):\n    \n    inp = keras.Input(shape=(700, 1400,1))\n    if dropout: x = layers.Dropout(dropout)(inp)\n    x = layers.Conv2D(1278, (inp.shape[-3], 1), activity_regularizer=pre_synaptic_spike_regularization, name=\"WiringLayer\")(inp)\n    x = K.squeeze(x, axis=-3)\n    x = layers.BatchNormalization()(x)\n    x = sigmoid(x)\n    x = ToBoolLayer(threshold=sigmoid_threshold, \n                    use_sigmoid=use_sigmoid, \n                    mult=sigmoid_mult, \n                    name='preNeuronBool')(x)\n    if dropout: x = layers.Dropout(dropout)(x)\n    nExcitatorySynapsesPerMS = SpikeProcessor(excitatory_wanted, name='nExcitatory', end=N_EXC)(x)\n    nInhibitorySynapsesPerMS = SpikeProcessor(inhibitory_wanted, name='nInhibitory', start=N_EXC)(x)\n    x = CallNeuron(L5PC_model, name=\"SpikeTrain\")(x)\n    x = ToBoolLayer(threshold=.2, \n                    use_sigmoid=False, \n                    name='postNeuronBool')(x)\n    output = layers.Dense(1, activation=\"sigmoid\", name=\"nSpikes\")(x)\n    model = keras.Model(inp, [output, nExcitatorySynapsesPerMS, nInhibitorySynapsesPerMS])\n    model.compile(optimizer=optimizer, \n                  loss={'nSpikes': MeanSquaredError(), 'nExcitatory': MeanSquaredErrorSynapsesPerMS(), 'nInhibitory': MeanSquaredErrorSynapsesPerMS()}, \n                  metrics={'nSpikes': keras.metrics.BinaryAccuracy()}, \n                  loss_weights=[1-sum(qSynapse), *qSynapse])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model(optimizer, sigmoid_threshold=0.8, use_sigmoid=True, sigmoid_mult=25, dropout=.2, nSynapse=True, delay=.02, dropout2=False,\n                     excitatory_wanted=EXCITATORY_SYNAPSES_WANTED, inhibitory_wanted=INHIBITORY_SYNAPSES_WANTED, qSynapse=(0.05, 0.05)):\n    \n    inp = keras.Input(shape=(700, 1400,1))\n    x = inp\n    if delay: x = GeometricDelay(delay)(x)    \n    if dropout: x = layers.Dropout(dropout)(x)\n    \n    # Wiring\n    x = layers.Conv2D(1278, (inp.shape[-3], 1), activity_regularizer=pre_synaptic_spike_regularization, kernel_constraint=keras.constraints.NonNeg(), \n                      bias_constraint=keras.constraints.NonNeg(), name=\"WiringLayer\")(x)\n    x = layers.Reshape((x.shape[-2], x.shape[-1]))(x)\n    \n    # Regularize synapse number\n#     x = layers.BatchNormalization()(x)\n#     x = sigmoid(x)\n    x = ToBoolLayer(threshold=sigmoid_threshold, use_sigmoid=use_sigmoid, mult=sigmoid_mult, name='preNeuronBool')(x)\n    \n    if dropout2: x = layers.Dropout(dropout2, name=\"preNeuronDrop\")(x)\n    \n    # For number of synapse loss regularization\n    if nSynapse:\n        nExcitatorySynapsesPerMS = SpikeProcessor(excitatory_wanted, name='nExcitatory', end=N_EXC)(x)\n        nInhibitorySynapsesPerMS = SpikeProcessor(inhibitory_wanted, name='nInhibitory', start=N_EXC)(x)\n    \n    # Neuron\n    x = CallNeuron(L5PC_model, name=\"SpikeTrain\")(x)\n#     x = ToBoolLayer(threshold=.2, use_sigmoid=False, name='postNeuronBool')(x)\n    \n    x = layers.Reshape((x.shape[-1], 1))(x)#[:, :800, :]\n#     x = layers.Conv1D(NLABELS, 150, 100)(x)\n#     x = K.sum(x, axis=-2)\n    x = layers.MaxPooling1D(x.shape[-2], strides=x.shape[-2], name='maxPool')(x)\n#     x = layers.AveragePooling1D(x.shape[-2], name='avePool')(x)\n    output = layers.Flatten(name=\"nSpikes\")(x)\n#     print(x.shape)\n#     output = layers.Softmax(name=\"nSpikes\")(x)\n#     output = layers.Dense(1, activation=\"softmax\", name=\"nSpikes\")(x)\n    \n    # create and compile model:\n#     output = layers.Dense(1, activation=\"sigmoid\", name=\"nSpikes\")(x)\n    if nSynapse: \n        model = keras.Model(inp, [output, nExcitatorySynapsesPerMS, nInhibitorySynapsesPerMS])\n        model.compile(optimizer=optimizer, \n                      loss={'nSpikes': MeanSquaredError(), 'nExcitatory': MaxErrorSynapsesPerMS(), 'nInhibitory': MaxErrorSynapsesPerMS()}, \n                      metrics={'nSpikes': keras.metrics.BinaryAccuracy()}, \n                      loss_weights=[1-sum(qSynapse), *qSynapse])\n    else:\n        model = keras.Model(inp, output)\n        model.compile(optimizer=optimizer, loss=MeanSquaredError(), metrics=keras.metrics.BinaryAccuracy())\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model_cat(categories, optimizer, sigmoid_threshold=0.8, use_sigmoid=True, sigmoid_mult=25, dropout=.2, \n                     excitatory_wanted=EXCITATORY_SYNAPSES_WANTED, inhibitory_wanted=INHIBITORY_SYNAPSES_WANTED, qSynapse=(0.05, 0.05)):\n    \n    inp = keras.Input(shape=(700, 1400,1))\n    \n    if dropout: x = layers.Dropout(dropout)(inp)\n    \n    # Wiring\n    x = layers.Conv2D(1278, (inp.shape[-3], 1), activity_regularizer=pre_synaptic_spike_regularization, bias_constraint=keras.constraints.non_neg(),\n                      kernel_constraint=keras.constraints.non_neg(), name=\"WiringLayer\")(x)\n#     x = layers.Conv2D(1278, (inp.shape[-3], 1),use_bias=False, \n#                       kernel_constraint=keras.constraints.NonNeg(), kernel_regularizer = NSynapseRegularizer(kernel_regularizer_strength),\n#                       name=\"WiringLayer\")(inp if not dropout else x)\n    x = K.squeeze(x, axis=-3)\n    \n    # Regularize synapse number\n#     x = layers.BatchNormalization()(x)\n#     x = sigmoid(x)\n    x = ToBoolLayer(threshold=sigmoid_threshold, use_sigmoid=use_sigmoid, mult=sigmoid_mult, name='preNeuronBool')(x)\n    \n#     if dropout: x = layers.Dropout(dropout, name=\"preNeuronDrop\")(x)\n    \n    # For number of synapse loss regularization\n    nExcitatorySynapsesPerMS = SpikeProcessor(excitatory_wanted, name='nExcitatory', end=N_EXC)(x)\n    nInhibitorySynapsesPerMS = SpikeProcessor(inhibitory_wanted, name='nInhibitory', start=N_EXC)(x)\n    \n    # Neuron\n    x = CallNeuron(L5PC_model, name=\"SpikeTrain\")(x)\n#     x = ToBoolLayer(threshold=.2, use_sigmoid=False, name='postNeuronBool')(x)\n    \n    # to categorical prediction:\n    x = tf.expand_dims(x, -1)\n    x = layers.Conv1D(NLABELS, 150, 100)(x)\n#     x = K.sum(x, axis=-2)\n    x = layers.MaxPooling1D(x.shape[-2], name='maxPool')(x)\n    x = layers.Flatten()(x)\n    output = layers.Softmax(name=\"nSpikes\")(x)\n#     output = layers.Dense(categories, activation=\"softmax\", name=\"nSpikes\")(x)\n    \n    # create and compile model:\n    model = keras.Model(inp, [output, nExcitatorySynapsesPerMS, nInhibitorySynapsesPerMS])\n    model.compile(optimizer=optimizer, \n                  loss={'nSpikes': 'categorical_crossentropy', \n                        'nExcitatory': MeanSquaredErrorSynapsesPerMS(), \n                        'nInhibitory': MeanSquaredErrorSynapsesPerMS()}, \n                  metrics={'nSpikes': keras.metrics.CategoricalAccuracy()}, \n                  loss_weights=[1-sum(qSynapse), *qSynapse])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = create_model(Nadam(), sigmoid_mult=50, nSynapse=True, use_sigmoid=True, sigmoid_threshold=0.2, dropout=.1, excitatory_wanted=8, inhibitory_wanted=6, qSynapse=(.08, .05))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = create_model(Nadam(), sigmoid_mult=50, nSynapse=True, use_sigmoid=True, sigmoid_threshold=0.2, dropout=.1, delay=False, excitatory_wanted=10, inhibitory_wanted=8, qSynapse=(.08, .05))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Current Module","metadata":{}},{"cell_type":"code","source":"def create_model(optimizer, sigmoid_threshold=0.8, use_sigmoid=True, sigmoid_mult=25, dropout=.2, delay=.02, dropout2=False):\n    \n    inp = keras.Input(shape=(700, 1400,1))\n    x = inp\n    if delay: x = GeometricDelay(delay)(x)    \n    if dropout: x = layers.Dropout(dropout)(x)\n    \n    # Wiring\n    x = layers.Conv2D(1278, (inp.shape[-3], 1), activity_regularizer=pre_synaptic_spike_regularization, kernel_constraint=keras.constraints.NonNeg(), \n                      bias_constraint=keras.constraints.NonNeg(), name=\"WiringLayer\")(x)\n    x = layers.Reshape((x.shape[-2], x.shape[-1]))(x)\n    \n    # Regularize synapse number\n    x = layers.BatchNormalization()(x)\n    x = sigmoid(x)\n    x = ToBoolLayer(threshold=sigmoid_threshold, use_sigmoid=use_sigmoid, mult=sigmoid_mult, name='preNeuronBool')(x)\n    \n    if dropout2: x = layers.Dropout(dropout2, name=\"preNeuronDrop\")(x)\n\n    # Neuron\n    x = CallNeuron(L5PC_model, name=\"SpikeTrain\")(x)\n    \n    x = layers.Reshape((x.shape[-1], 1))(x)\n\n    x = layers.MaxPooling1D(x.shape[-2], strides=x.shape[-2], name='maxPool')(x)\n    output = layers.Flatten(name=\"nSpikes\")(x)\n\n    model = keras.Model(inp, output)\n    model.compile(optimizer=optimizer, loss=MeanSquaredError(), metrics=keras.metrics.BinaryAccuracy())\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SliceLayer(tf.keras.layers.Layer):\n    def __init__(self, start=None, end=None, name=\"SliceLayer\"):\n        super().__init__(name=name)\n        self.start = start\n        self.end = end\n\n    def build(self, shape):\n        self.dims = len(shape)\n        pass\n    \n    def call(self, inputs):\n        if self.start is not None:\n            if self.end is not None:\n                return inputs[:,:,:,self.start:self.end] if self.dims==4 else inputs[:,:,self.start:self.end]\n            else:\n                return inputs[:,:,:,self.start:] if self.dims==4 else inputs[:,:,self.start:]\n        elif self.end is not None:\n            return inputs[:,:,:,:self.end] if self.dims==4 else inputs[:,:,:self.end]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MeanSynapsesPerMsMetric:\n    def __init__(self, name=\"nSynapses\"):\n        self.name=name\n    def __call__(self, y_true, y_pred):\n        booleans = tf.cast(tf.math.greater(y_pred, 0.5), tf.float32)\n        summed = tf.math.reduce_sum(booleans, axis=-1)\n        mean_by_ms = tf.math.reduce_mean(summed, axis=-1)\n        meaned_batches = tf.math.reduce_mean(mean_by_ms, axis=-1)\n        return meaned_batches","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def printModule(dropout1, dropout2, sigmoid_threshold, sigmoid_mult, exWanted, inWanted, qSynapse, synLoss):\n    print(\"~*~ Visual Module ~*~\")\n#     if augment: print(f\"Images are augmented (rotated by {augment})\")\n    print(f\"Image Dropout Rate: {dropout1}\")\n    print(f\"Synapse Threshold: {sigmoid_threshold}\")\n    print(f\"Synapse Training Sigmoid Multiplication: {sigmoid_mult}\")\n    print(f\"Synapses Wanted: Exc-{exWanted}; Inh-{inWanted}; Sum-{exWanted+inWanted}\")\n    print(f\"Synapses Loss Rate: Exc-{qSynapse[0]}; Inh-{qSynapse[1]}\")\n    print(f\"Synapses Dropout Rate: {dropout2}\")\n    print(f\"Synapse Loss Function: {synLoss.__name__}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def different_nSynapses(saccades=None, xaxis=True, use_sigmoid=True, pruning=True, dropout1=False, dropout2=False,\n                        sigmoid_threshold=0.9, sigmoid_mult=15, to_bool=True, dropout=False, \n                        excitatory_wanted=EXCITATORY_SYNAPSES_WANTED, inhibitory_wanted=INHIBITORY_SYNAPSES_WANTED,\n                        qSynapse=(tf.Variable(0.1, trainable=False), tf.Variable(0.1, trainable=False)), \n                        optimizer=SGD(momentum=.9), conv_shape=(16,16), non_neg=False,\n                        synLoss=MeanSquaredErrorSynapsesPerMS, nSynapse=True, neurons=neurons, regular_sigmoid=True):\n    printModule(dropout1, dropout2, sigmoid_threshold, sigmoid_mult, excitatory_wanted, inhibitory_wanted, qSynapse, synLoss)\n    module_name = \"module_syn_\"\n    for i in [dropout1, dropout2, sigmoid_threshold, sigmoid_mult, excitatory_wanted, inhibitory_wanted, qSynapse, synLoss]:\n        module_name += str(i)\n    inp = keras.Input(shape=(700, 1400,1))\n    x = inp\n    if dropout: x = layers.Dropout(dropout)(x)\n    \n    # Wiring\n    if non_neg: x = layers.Conv2D(1278, (inp.shape[-3], 1), activity_regularizer=pre_synaptic_spike_regularization, kernel_constraint=keras.constraints.NonNeg(), \n                      bias_constraint=keras.constraints.NonNeg(), name=\"WiringLayer\")(x)\n    else: x = layers.Conv2D(1278, (inp.shape[-3], 1),\n                      bias_constraint=keras.constraints.NonNeg(), name=\"WiringLayer\")(x)\n    x = layers.Reshape((x.shape[-2], x.shape[-1]))(x)\n#     inp = keras.Input(shape=(256,256,1))\n    \n#     padding = tf.Variable(lambda: padding, trainable=False)\n#     x = inp\n#     if augment: x = data_augmentation(augment)(x)\n#     x = processor(x)\n#     if dropout1: x = layers.Dropout(dropout1)(x)\n#     conv_shape = (x.shape[-3], 1) if xaxis else conv_shape\n#     x = Pad(padding)(x)\n#     x = layers.Conv2D(2*N_EXC, conv_shape, strides=conv_shape, use_bias=True, kernel_constraint=keras.constraints.NonNeg(), name=\"WiringLayer\")(x)#, activity_regularizer=pre_synaptic_spike_regularization)(x)\n    x = layers.BatchNormalization(name=\"BatchNorm\")(x)\n    if regular_sigmoid: x = layers.Activation(sigmoid)(x)\n    x = ToBoolLayer(threshold=sigmoid_threshold, use_sigmoid=use_sigmoid, mult=sigmoid_mult, name='preNeuronBool')(x)\n    \n#     x = K.squeeze(x, axis=-3)\n        \n    ExcitatorySynapses = SliceLayer(end=N_EXC, name=\"ExcSyns\")(x)#[:, :, :N_EXC]  #SpikeProcessor(excitatory_wanted, name='nExcitatory', end=639)(x)\n    InhibitorySynapses = SliceLayer(start=N_EXC, name=\"InhSyns\")(x)#[:, :, N_EXC:]  #SpikeProcessor(inhibitory_wanted, name='nInhibitory', start=639)(x)\n\n#     if dropout2: x = layers.Dropout(dropout2)(x)\n    x = CallNeuron(neurons, name=\"SpikeTrain\")(x)\n\n#     x = PredictNeuron(neurons)(x)[:,-198:,:]   # run through david's model, take only the post-noise time (32 ms X 6 times)\n    x = layers.MaxPooling1D(x.shape[-2], strides=x.shape[-2], name=\"MaxPooling\")(x)\n    output = layers.Flatten(name=\"nSpikes\")(x)\n    \n    model = keras.Model(inp, [output, ExcitatorySynapses, InhibitorySynapses])\n    if not nSynapse:\n        model.compile(optimizer=optimizer, loss={'nSpikes': MeanSquaredError()}, \n                      metrics={\"nSpikes\": keras.metrics.BinaryAccuracy(), \"ExcSyns\": MeanSynapsesPerMsMetric(), \"InhSyns\": MeanSynapsesPerMsMetric()})\n    else:\n        model.compile(optimizer=optimizer, \n                  loss=[MeanSquaredError(), synLoss(excitatory_wanted), synLoss(inhibitory_wanted)], \n                  metrics={\"nSpikes\": keras.metrics.BinaryAccuracy(), \"ExcSyns\": MeanSynapsesPerMsMetric(), \"InhSyns\": MeanSynapsesPerMsMetric()},\n                  loss_weights=[1.0 -qSynapse[0] - qSynapse[1], qSynapse[0], qSynapse[1]])\n    return model, module_name","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_synapses, module_name = different_nSynapses(nSynapse=True, non_neg=True, dropout1=False, dropout2=False, use_sigmoid=True, regular_sigmoid=False, sigmoid_mult=2, synLoss=MSE_RMS_SynapsesPerMS, sigmoid_threshold=2, optimizer=Nadam(), qSynapse=(.1, .1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_synapses.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# history = model_synapses.fit_generator(train_ds, epochs=50, validation_data=valid_ds)#, callbacks=SynapsePruner(iterations=20))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Benchmark","metadata":{}},{"cell_type":"code","source":"def build_defected_FCN_model(cat=1, depth=1, width=32, filters=1, kernel_size=400, stride_size=150, dropout=0, optimizer=SGD(momentum=.9), l2_reg=1e-3):\n    inp = keras.Input(shape=(700, 1400,1))\n    x = inp\n#     x = GeometricDelay(delay, False)(inp) if delay else inp\n#     x = layers.Flatten()(x)\n    if dropout != 0: x = layers.Dropout(dropout)(x)\n#     x = layers.Conv2D(1278, (700, 1), (700, 1), activation=\"relu\")(x)\n#     x = layers.BatchNormalization()(x)\n    x = layers.Conv2D(filters, (700, kernel_size), (700, stride_size), activation=\"relu\")(x)\n    x = layers.BatchNormalization()(x)\n    for d in range(depth):\n#         if dropout != 0: x = layers.Dropout(dropout)(x)\n        x = layers.Dense(units=width, activation='linear', kernel_regularizer=keras.regularizers.l2(l2_reg), name='FC_layer_%d' %(d + 1))(x)\n        x = layers.LeakyReLU(alpha=0.3, name='LReLU_%d' %(d + 1))(x)\n        x = layers.BatchNormalization(name='BN_layer_%d' %(d + 1))(x)\n    if filters > 1: x = layers.Dense(units=1, activation='sigmoid', kernel_regularizer=keras.regularizers.l2(l2_reg), name='Logits')(x)\n\n#     x = layers.Flatten()(x)\n    x = layers.Reshape((x.shape[-2], 1))(x)\n    x = layers.MaxPooling1D(x.shape[-2], strides=x.shape[-2])(x)\n    output = layers.Flatten()(x)\n#     output = keras.activations.sigmoid(x)\n#     output = layers.Dense(cat, activation='sigmoid', name='logits')(x)\n    model = keras.Model(inp, output)\n    model.compile(optimizer=optimizer, loss=MeanSquaredError(), metrics=keras.metrics.BinaryAccuracy() if cat==1 else keras.metrics.CategoricalAccuracy())\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bench = build_defected_FCN_model(cat=1, depth=0, width=5, filters=10, dropout=.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bench.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bench.fit_generator(train_ds, epochs=5, validation_data=valid_ds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# David's Benchamrk Check","metadata":{}},{"cell_type":"code","source":"short_run = False\n\nif short_run:\n    # simple cells feature extraction is extreemly fast\n    batch_size = 64\n    num_epochs = 12\n    l2_reg = 1e-2\n\n    depths_list = [1,2,3]\n    widths_list = [1,2,4,8,16,32,64]\n    num_random_repetitions = [1,2,3,4,5,6,7,8]\nelse:\n    batch_size = 64\n    num_epochs = 7\n    l2_reg = 1e-3\n\n    depths_list = [1,2,3]\n    widths_list = [1,2,4,8,16]\n    num_random_repetitions = [1,2,3,4]\n\n    \nresults_list = []\nfor depth in depths_list:\n    for width in widths_list:\n        for rand_rep in num_random_repetitions:\n            training_start_time = time.time()\n\n            # create model \n            FCN_model = build_defected_FCN_model(depth=depth, width=width, l2_reg=l2_reg, dropout=.5, filters=10)\n\n            # fit model\n            print('-----------------------------------------------------------')\n            train_history = FCN_model.fit_generator(train_ds, epochs=num_epochs)\n            print('----------------------------------------')\n\n            # evaluate performace\n            train_loss_and_metrics = FCN_model.evaluate_generator(train_ds)\n            test_loss_and_metrics  = FCN_model.evaluate_generator(test_ds)\n            print('----------------------------------------')\n            training_time_min = (time.time() - training_start_time) / 60\n            # print results\n            print('----------------------------------------')\n            print(\"training took %.2f minutes\" % (training_time_min))\n            print('----------------------------------------')\n            print('model name is \"\"%s\"' %(FCN_model.name))\n            print('----------------------------------------')\n            print(\"Train loss = %.5f\" %(train_loss_and_metrics[0]))\n            print(\"Train accuracy = %.2f%s\" %(100 * train_loss_and_metrics[1], '%'))\n            print('----------------------------------------')\n            print(\"Test loss = %.5f\" %(test_loss_and_metrics[0]))\n            print(\"Test accuracy = %.2f%s\" %(100 * test_loss_and_metrics[1], '%'))\n            print('-----------------------------------------------------------')\n\n            # store results in dict\n            results_dict = {}\n            results_dict['depth']              = depth\n            results_dict['width']              = width\n            results_dict['rand_rep']           = rand_rep\n\n            results_dict['model_name']         = FCN_model.name\n            results_dict['training_time_min']  = training_time_min\n            results_dict['train_history']      = train_history.history\n            results_dict['Train loss']         = train_loss_and_metrics[0]\n            results_dict['Train accuracy']     = 100 * train_loss_and_metrics[1]\n            results_dict['Test loss']          = test_loss_and_metrics[0]\n            results_dict['Test accuracy']      = 100 * test_loss_and_metrics[1]\n\n            # store in results list\n            results_list.append(results_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The total number of models that were trained is %d' %(len(results_list)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pickle.dump(results_list, open('results_list_%d_models_%d.pickle' %(len(results_list), np.random.randint(100)), \"wb\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"column_names = ['preprocessing_type','subsample','depth','width','Test accuracy','rand_rep']\n\nresults_df = pd.DataFrame(index=range(len(results_list)), columns=column_names)\n\nfor k, results_dict in enumerate(results_list):\n    results_df.loc[k,'depth'] = results_dict['depth']\n    results_df.loc[k,'width'] = results_dict['width']\n    results_df.loc[k,'Test accuracy'] = results_dict['Test accuracy']\n    results_df.loc[k,'rand_rep'] = results_dict['rand_rep']\n\nresults_df = results_df.astype({\"Test accuracy\": float})\nresults_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grouped_results_df = results_df.groupby(['depth','width']).agg({'Test accuracy': ['max','mean']})\ngrouped_results_df.columns = ['Best Test accuracy','Mean Test accuracy']\ngrouped_results_df = grouped_results_df.reset_index()\ngrouped_results_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot","metadata":{"id":"7dGfCp8zTW7u"}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def plot_examples(model, plot_start=None, write_last=False, show_spikes=False, ds=valid_ds):\n    plt.figure(figsize=(256, 200))\n    how_many = 10\n    for img, label in ds:\n#         inputs = K.function(model.input, model.get_layer(bool_layers[0]).output)([img])\n        inputs = K.function(model.input, model.get_layer(\"preNeuronBool\").output)([img])\n        outputs = K.function(model.input, model.get_layer(\"SpikeTrain\").output)([img])\n        if write_last: nAP = K.function(model.input, model.get_layer(\"nSpikes\").output)([img])\n\n        for i in range(how_many):\n            plt.subplot(how_many, 3, 3*i+1)\n            plt.imshow(img[i,:,:,0]/255, cmap='binary')\n            plt.title(LABELS[label[i]], fontdict={'fontsize':200})\n            plt.axis(\"off\")\n            plt.subplot(how_many, 3, 3*i+2)\n            if len(inputs[i].shape)==3:\n                curr_input = np.squeeze(inputs[i], axis=-3)\n            else:\n                curr_input = inputs[i]\n            num_of_synapses = np.sum(curr_input, axis=-1)\n            mean_synapses = round(num_of_synapses.mean(), 2)\n            std_synapses = round(num_of_synapses.std(), 2)\n            plt.title(f\"\\u03BC: {str(mean_synapses)},  \\u03C3: {str(std_synapses)}\", fontdict={'fontsize':200})\n            plt.imshow(curr_input, cmap='binary', vmin=0, vmax=1)\n            plt.axis(\"off\")\n            plt.subplot(how_many, 3, 3*i+3)\n            curr_output = outputs[i]            \n            if show_spikes:\n                spikes = []\n                curr_index = 0\n                while True:\n                    start = np.where(curr_output[curr_index:] > 0.25)[0]\n                    if not start.shape[0]: break\n                    start = curr_index+start[0]\n                    end = np.where(curr_output[start:] < 0.1)[0]\n                    if not end.shape[0]: break\n                    end = end[0] + start\n                    spikes.append((start,end))\n                    curr_index = end + 1\n            plt.title((f\"Score: {str(round(nAP[i][0],2))},\" if write_last else \"\") +f\"\\u03A3: {str(round(curr_output.sum(),2))}\" + (f\", spikes:{spikes}\" if show_spikes else \"\"), fontdict={'fontsize':150})\n            plt.plot(curr_output, linewidth=5)\n            plt.ylim(0,1)\n            if plot_start: plt.axvline(plot_start, color='g', linestyle=':', linewidth=10.)\n            plt.axis('on')\n        break","metadata":{"id":"uciiuFqPTW7v","executionInfo":{"status":"ok","timestamp":1608815880827,"user_tz":-120,"elapsed":795,"user":{"displayName":"Roy Urbach","photoUrl":"","userId":"03979889048736445362"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_examples_cat(model, plot_start=None, write_last=True, show_spikes=False, input_layer = \"preNeuronBool\"):\n    plt.figure(figsize=(256, 200))\n    \n    how_many = 10\n    for img, label in valid_ds:\n#         inputs = K.function(model.input, model.get_layer(bool_layers[0]).output)([img])\n        inputs = K.function(model.input, model.get_layer(input_layer).output)([img])\n        outputs = K.function(model.input, model.get_layer(\"SpikeTrain\").output)([img])\n        prediction = K.function(model.input, model.get_layer(\"nSpikes\").output)([img])\n\n        for i in range(how_many):\n            img_summed = img[i,:,:,0].sum(axis=0)\n            for k in range(img_summed.shape[0]-1):\n                if img_summed[k] > 0 and (img_summed[k+1:]==0).all():\n                    end_time = k+1\n                    break\n            plt.subplot(how_many, 4, 4*i+1)\n            plt.imshow(img[i,:,:,0]/255, cmap='binary')\n            plt.axvline(end_time, color='r', linestyle=\"--\")\n            correct = label[i].tolist().index(1)\n            plt.title(LABELS[correct], fontdict={'fontsize':200})\n            plt.axis(\"off\")\n            \n            plt.subplot(how_many, 4, 4*i+2)\n            if len(inputs[i].shape)==3:\n                curr_input = np.squeeze(inputs[i], axis=-3)\n            else:\n                curr_input = inputs[i]\n            num_of_synapses = np.sum(curr_input, axis=-1)\n            mean_synapses = round(num_of_synapses.mean(), 2)\n            std_synapses = round(num_of_synapses.std(), 2)\n            plt.title(f\"\\u03BC: {str(mean_synapses)},  \\u03C3: {str(std_synapses)}\", fontdict={'fontsize':200})\n            plt.imshow(curr_input, cmap='binary', vmin=0, vmax=1)\n            plt.axhline(end_time, color='r', linestyle=\"--\")\n            plt.axis(\"off\")\n            \n            plt.subplot(how_many, 4, 4*i+3)\n            curr_output = outputs[i]\n            if show_spikes:\n                spikes = []\n                curr_index = 0\n                while True:\n                    start = np.where(curr_output[curr_index:] > 0.25)[0]\n                    if not start.shape[0]: break\n                    start = curr_index+start[0]\n                    end = np.where(curr_output[start:] < 0.1)[0]\n                    if not end.shape[0]: break\n                    end = end[0] + start\n                    spikes.append((start,end))\n                    curr_index = end + 1\n            plt.title(f\"\\u03A3: {str(round(curr_output.sum(),2))}\" + (f\", spikes:{spikes}\" if show_spikes else \"\"), fontdict={'fontsize':150})\n            plt.plot(curr_output, linewidth=5)\n            plt.axvline(end_time, color='r', linestyle=\"--\")\n            plt.ylim(0,1)\n            if plot_start: plt.axvline(plot_start, color='g', linestyle=':', linewidth=10.)\n            plt.axis('on')\n            \n            plt.subplot(how_many, 4, 4*i+4)\n            cur_prediction = prediction[i].tolist().index(max(prediction[i]))\n            cur_prediction_list = [j==max(prediction[i]) for j in prediction[i]]\n            plt.title(f\"Correct: {LABELS[correct]} ({str(round(prediction[i][correct], 2))}), Prediction: {LABELS[cur_prediction]} ({str(round(max(prediction[i]),2))})\", fontdict={'fontsize':150})\n            colors = [\"g\" if (lbl and pred) else \"r\" if pred else \"y\" if lbl else \"b\" for lbl, pred in zip(label[i], cur_prediction_list)]\n            plt.bar(range(label[i].shape[0]), prediction[i], tick_label = LABELS[:NLABELS], color=colors)\n#             plt.xticks(range(label[i].shape[0]), LABELS)\n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_statistics(model, input_layer=\"preNeuronBool\"):\n    \n    all_inputs = []\n    n_synapses = []\n    synapses_mu = []\n    synapses_std = []\n    synapses_max = []\n    synapses_min = []\n#     sum_output = []\n    all_outputs = []\n#     excitatory_synapses = []\n#     inhibitory_synapses = []\n    label_dct = {}\n    synapses_mean_per_label = {}\n    \n    for i, (img, label) in enumerate(train_ds):\n        if i > 3: break\n        inputs = K.function(model.input, model.get_layer(input_layer).output)([img])\n        outputs = K.function(model.input, model.get_layer(\"SpikeTrain\").output)([img])\n        all_inputs.append(inputs)\n        labels = [LABELS[lbl] for lbl in label]\n        for j in range(len(inputs)):\n            if len(inputs[j].shape)==3:\n                curr_input = np.squeeze(inputs[j], axis=-3)\n            else:\n                curr_input = inputs[j]\n            img_summed = img[j,:,:,0].sum(axis=0)\n            for k in range(img_summed.shape[0]-1):\n                if img_summed[k] > 0 and (img_summed[k+1:]==0).all():\n                    end_time = k+1\n                    break\n#             plt.imshow(img[j,:,:,0], cmap=\"binary\")\n#             plt.axvline(end_time)\n            curr_input = curr_input[:end_time,:]\n            num_of_synapses = np.sum(curr_input, axis=-1)\n#             excitatory_synapses.append(np.sum(curr_input[:, :N_EXC], axis=-1).mean())\n#             inhibitory_synapses.append(np.sum(curr_input[:, N_EXC:], axis=-1).mean())\n            n_synapses.append(num_of_synapses)\n            synapses_mu.append(num_of_synapses.mean())\n            synapses_std.append(num_of_synapses.std())\n            synapses_max.append(num_of_synapses.max())\n            synapses_min.append(num_of_synapses.min())\n            curr_output = outputs[j]\n#             sum_output.append(curr_output.sum())\n            all_outputs.append(curr_output)\n            if labels[j] not in label_dct:\n                label_dct[labels[j]] = []\n                synapses_mean_per_label[labels[j]] = {\"Sum\": [], \"Ex\":[], \"Inh\":[]}\n            label_dct[labels[j]].append(curr_output)\n            synapses_mean_per_label[labels[j]][\"Sum\"].append(np.sum(curr_input, axis=-1).mean())\n            synapses_mean_per_label[labels[j]][\"Ex\"].append(np.sum(curr_input[:, :N_EXC], axis=-1).mean())\n            synapses_mean_per_label[labels[j]][\"Inh\"].append(np.sum(curr_input[:, N_EXC:], axis=-1).mean())\n\n    mean_dct = {}\n    for lbl in label_dct.keys():\n        mean_dct[lbl] = np.stack(label_dct[lbl]).mean(axis=0)\n\n    plt.figure(figsize=(15,10))\n\n    plt.subplot(3,3,1)\n    plt.title('input values')\n    plt.hist(np.array(all_inputs).ravel())\n\n    plt.subplot(3,3,2)\n    plt.title('mean of sum synapses')\n    plt.hist(synapses_mu)\n\n    plt.subplot(3,3,3)\n    plt.title('std of sum synapses')\n    plt.hist(synapses_std)\n\n    plt.subplot(3,3,4)\n    plt.title('max sum synapses')\n    plt.hist(synapses_max)\n\n    plt.subplot(3,3,5)\n    plt.title('min sum synapses')\n    plt.hist(synapses_min)\n\n    plt.subplot(3,3,6)\n    plt.title(f'mean output of neuron (AP)')\n    for lbl, value in mean_dct.items():\n        plt.plot(value, label=lbl, linewidth=2.)\n    plt.ylim(0,1)\n    plt.legend()\n\n    plt.subplot(3,3,7)\n    plt.title('Excitatory Synapses Mean per Label')\n    plt.hist([synapses_mean_per_label[lbl][\"Ex\"] for lbl in synapses_mean_per_label.keys()], label=list(synapses_mean_per_label.keys()))\n    plt.legend()\n    plt.xlim((0,15))\n\n\n    plt.subplot(3,3,8)\n    plt.title('Inhibitory Synapses Mean per Label')\n    plt.hist([synapses_mean_per_label[lbl][\"Inh\"] for lbl in synapses_mean_per_label.keys()], label=list(synapses_mean_per_label.keys()))\n    plt.legend()\n    plt.xlim((0,15))\n\n    \n    plt.subplot(3,3,9)\n    plt.title('All Synapses Mean per Label')\n    plt.hist([synapses_mean_per_label[lbl][\"Sum\"] for lbl in synapses_mean_per_label.keys()], label=list(synapses_mean_per_label.keys()))\n    plt.legend()\n\n    \n    \n    # plt.axvline(400-257, color='g', linestyle=':', linewidth=2.)\n\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_statistics_cat(model, input_layer=\"preNeuronBool\"):\n    \n    all_inputs = []\n    n_synapses = []\n    synapses_mu = []\n    synapses_std = []\n    synapses_max = []\n    synapses_min = []\n    sum_output = []\n    all_outputs = []\n    excitatory_synapses = []\n    inhibitory_synapses = []\n    label_dct = {lbl:[] for lbl in LABELS[:NLABELS]}\n    synapses_mean_per_label = {lbl:[] for lbl in LABELS[:NLABELS]}\n    \n    for i, (img, label) in enumerate(train_ds):\n        if i > 3: break\n        inputs = K.function(model.input, model.get_layer(input_layer).output)([img])\n        outputs = K.function(model.input, model.get_layer(\"SpikeTrain\").output)([img])\n        all_inputs.append(inputs)\n        labels = [LABELS[lbl] for lbl in np.where(label == 1)[1].tolist()]\n        for j in range(len(inputs)):\n            if len(inputs[j].shape)==3:\n                curr_input = np.squeeze(inputs[j], axis=-3)\n            else:\n                curr_input = inputs[j]\n            img_summed = img[j,:,:,0].sum(axis=0)\n            for k in range(img_summed.shape[0]-1):\n                if img_summed[k] > 0 and (img_summed[k+1:]==0).all():\n                    end_time = k+1\n                    break\n#             plt.imshow(img[j,:,:,0], cmap=\"binary\")\n#             plt.axvline(end_time)\n            curr_input = curr_input[:end_time,:]\n            num_of_synapses = np.sum(curr_input, axis=-1)\n            excitatory_synapses.append(np.sum(curr_input[:, :N_EXC], axis=-1).mean())\n            inhibitory_synapses.append(np.sum(curr_input[:, N_EXC:], axis=-1).mean())\n            n_synapses.append(num_of_synapses)\n            synapses_mu.append(num_of_synapses.mean())\n            synapses_std.append(num_of_synapses.std())\n            synapses_max.append(num_of_synapses.max())\n            synapses_min.append(num_of_synapses.min())\n            curr_output = outputs[j]\n            sum_output.append(curr_output.sum())\n            all_outputs.append(curr_output)\n            label_dct[labels[j]].append(curr_output)\n            \n    mean_dct = {}\n    for lbl in label_dct.keys():\n        mean_dct[lbl] = np.stack(label_dct[lbl]).mean(axis=0)\n\n    plt.figure(figsize=(15,10))\n\n    plt.subplot(3,3,1)\n    plt.title('input values')\n    plt.hist(np.array(all_inputs).ravel())\n\n    plt.subplot(3,3,2)\n    plt.title('mean of sum synapses')\n    plt.hist(synapses_mu)\n\n    plt.subplot(3,3,3)\n    plt.title('std of sum synapses')\n    plt.hist(synapses_std)\n\n    plt.subplot(3,3,4)\n    plt.title('max sum synapses')\n    plt.hist(synapses_max)\n\n    plt.subplot(3,3,5)\n    plt.title('min sum synapses')\n    plt.hist(synapses_min)\n\n    plt.subplot(3,3,6)\n    plt.title(f'mean output of neuron (AP)')\n    for lbl, value in mean_dct.items():\n        plt.plot(value, label=lbl, linewidth=2.)\n    plt.ylim(0,1)\n    plt.legend()\n\n\n    plt.subplot(3,3,7)\n    plt.title('Synapses Mean (per ms per image)')\n    plt.hist(excitatory_synapses, label='excitatory', color='b')\n    plt.hist(inhibitory_synapses, label='inhibitiroy', color='r')\n\n\n    plt.subplot(3,3,8)\n    plt.title('Dense Layer Weights')\n    plt.plot(model.get_layer(\"nSpikes\").get_weights()[0][:,0], color='r')\n\n    plt.subplot(3,3,9)\n    \n    plt.show()","metadata":{"id":"FR_2SW-jTW7v","executionInfo":{"status":"ok","timestamp":1608815881171,"user_tz":-120,"elapsed":721,"user":{"displayName":"Roy Urbach","photoUrl":"","userId":"03979889048736445362"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PLOT = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PLOT: plot_examples(model, write_last=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PLOT: plot_examples_cat(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PLOT: plot_statistics(model)","metadata":{"id":"weTSTPMeTW7v","executionInfo":{"status":"ok","timestamp":1608822145474,"user_tz":-120,"elapsed":3517478,"user":{"displayName":"Roy Urbach","photoUrl":"","userId":"03979889048736445362"}},"outputId":"5ed8097b-25c9-4d0b-da11-c041fb2b64bf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PLOT: \n    plt.figure(figsize=(10,20))\n    weights = model.get_layer(\"WiringLayer\").get_weights()[0][:,0,0,:]\n    print(f\"min: {weights.min()}, max: {weights.max()}\\nmean: {weights.mean()}, sd: {weights.std()}\\n\")\n    exc = weights[:,:N_EXC]\n    inh = weights[:,N_EXC:]\n    # xExc = K.flatten(tf.transpose(exc) / tf.reduce_max(exc, axis=-1))\n    # xInh = K.flatten(tf.transpose(inh) / tf.reduce_max(exc, axis=-1))\n    # print(f\"Exc: mean per pre-synaptic: {xExc.mean()}, mean binary: {(xExc>PRESYNAPTIC_THRESHOLD).sum(axis=-1).mean()}\")\n    # print(f\"Inh: mean per pre-synaptic: {weights[:,N_EXC:].mean()}, mean binary: {(weights[:,N_EXC:]>PRESYNAPTIC_THRESHOLD).sum(axis=-1).mean()}\")\n    amountExc = [(exc>0.01).sum(axis=-1)]\n    amountInh = [(inh>0.01).sum(axis=-1)]\n\n    plt.subplot(3,1,1)\n    plt.hist(amountExc)\n    plt.title(\"Historgram of Excitatory Synapses per Neuron\")\n    plt.ylabel(\"Synapses per Neuron\")\n\n    plt.subplot(3,1,2)\n    plt.hist(amountInh)\n    plt.title(\"Historgram of Inhibitiory Synapses per Neuron\")\n    plt.ylabel(\"Synapses per Neuron\")\n\n\n\n    # plt.subplot(3,1,1)\n    # plt.title(\"Histogram Synapses per Presynaptic Neuron (Exc synapse)\")\n    # plt.hist((weights[:,:N_EXC]>PRESYNAPTIC_THRESHOLD).sum(axis=-1))\n    # plt.ylabel(\"nSynapses\")\n\n    # plt.subplot(3,1,2)\n    # plt.title(\"Histogram Synapses per Presynaptic Neuron (Inh synapse)\")\n    # plt.hist((weights[:,N_EXC:]>PRESYNAPTIC_THRESHOLD).sum(axis=-1))\n    # plt.ylabel(\"nSynapses\")\n\n    plt.subplot(3,1,3)\n    plt.title(\"Weights\")\n    plt.imshow(weights, cmap=\"Reds\", vmin=weights.min(), vmax=weights.max())\n    plt.axvline(N_EXC, color='r')\n    plt.xlabel(\"synapse\")\n    plt.ylabel(\"neuron\")\n\n    plt.show()","metadata":{"id":"m6tSi907qDRa","executionInfo":{"status":"ok","timestamp":1608822151511,"user_tz":-120,"elapsed":4948,"user":{"displayName":"Roy Urbach","photoUrl":"","userId":"03979889048736445362"}},"outputId":"80358f7c-3142-4dfb-a268-dc854a4f17b1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save Weights","metadata":{}},{"cell_type":"code","source":"if PLOT:\n    with open(\"weights_Nadam_84.npy\", 'wb') as f:\n        np.save(f, model.get_layer(\"WiringLayer\").get_weights()[0])\n    with open(\"bias_Nadam_84.npy\", 'wb') as f:\n        np.save(f, model.get_layer(\"WiringLayer\").get_weights()[1])","metadata":{"id":"IER4lsMqtnYB","executionInfo":{"status":"ok","timestamp":1608822147219,"user_tz":-120,"elapsed":672,"user":{"displayName":"Roy Urbach","photoUrl":"","userId":"03979889048736445362"}},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evolution","metadata":{"id":"-Wv6yELpTW7v"}},{"cell_type":"markdown","source":"Never actually ran it. But it's an idea.","metadata":{}},{"cell_type":"code","source":"class EvolutionModule:\n    generations = {}\n    results = {}\n    alive = set()\n    \n    def __init__(self, ID, generation, create_func, noise=.1, parent=None):\n        self.module = None\n        \n        self.generation = generation\n        self.cur_generation = generation\n        self.ID = ID\n        self.parent = parent\n        self.name = \"Module\" + (f\"({parent.__str__()[6:]})-\" if parent else \"\") + f\"[{generation}.{ID}]\"\n        \n        if generation not in EvolutionModule.generations: EvolutionModule.generations[generation] = [self]\n        else: EvolutionModule.generations[generation].append(self)\n        EvolutionModule.alive.add(self)\n                   \n        self.history = None\n        \n        self.children = set()\n        self.status = True\n        \n        self.create_func = create_func\n        self.original_weights = None\n        self.noise = noise\n        \n        self._create_model()\n        \n    \n    def _create_model(self):\n        self.module = self.create_func()\n        if self.parent:\n            self.original_weights = [weight for weight in self.parent.get_weights()]\n            self.original_weights[2] = self.original_weights[2] + np.random.rand(*self.original_weights[2].shape) * self.original_weights[2].std() * self.noise\n            self.module.set_weights(self.original_weights)\n    \n    def give_birth(self, n):\n        self.cur_generation += 1\n        for i in range(n):\n            self.children.add(EvolutionModule(i, self.cur_generation, self.create_func, self.noise, self))\n    \n    def is_alive(self):\n        return self.status\n    \n    def fit(self, training_ds, epochs, validation_data):\n        self.to_print()\n        history = self.module.fit(train_ds, epochs=epochs, validation_data=validation_data).history\n        if not self.history: self.history = history\n        else: \n            for key, value in history.items(): \n                self.history[key].extend(value)\n    \n    def die(self):\n        self.status = False\n        EvolutionModule.alive.remove(self)\n    \n    def value(self, n):\n        if not self.history: return None\n        return np.mean(self.history['loss'][-n:])\n    \n    def accuracy(self):\n        return self.history['val_logits_binary_accuracy'][-1]\n    \n    def value_to_print(self, n):\n        return self.history['loss'][-1], np.mean(self.history['loss'][-n:]), self.history['val_loss'][-1], np.mean(self.history['val_loss'][-n:]), self.history['val_logits_binary_accuracy'][-1], np.mean(self.history['val_logits_binary_accuracy'][-n:])\n    \n    def to_print(self):\n        print(f\"\\n Beginning fitting for {self.name}...\\n\")\n    \n    def __str__(self):\n        return self.name\n    \n    def get_weights(self):\n        return self.module.get_weights()","metadata":{"id":"XaveiKPHTW7v","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model():\n    return playable(saccades=None, xaxis=True, use_sigmoid=True, sigmoid_mult=50, to_bool=True, useSynapse=True, qSynapse=0.1, sigmoid_threshold=0.8, augment=False, optimizer=SGD(5e-3))","metadata":{"id":"XxnIaN8jTW7w","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_evolution(nStart=10, nEpochs=10, nMax=2, nPerMax=3, nGenerations=5, rand=0.1, measure_by=2):\n    EvolutionModule.generations.clear()\n    EvolutionModule.results.clear()\n    EvolutionModule.alive.clear()\n    \n    print(\"\\nGeneration 0:\\n\")\n    for i in range(nStart):\n        module = EvolutionModule(i, 0, create_model, rand)\n        module.fit(train_ds, epochs=nEpochs, validation_data=valid_ds)\n    EvolutionModule.results[0] = sorted(EvolutionModule.alive, key=lambda x: x.value(measure_by))\n    print(\"\\n\" + \"* \"*10)\n    print(f'Generation: 0:\\n Best accuracy (by loss): {EvolutionModule.results[0][0].accuracy()} by {EvolutionModule.results[0][0]}\\n')\n    for module in EvolutionModule.alive:\n        print(module, end=': ')\n        print(\"Last loss: {0}, Mean loss: {1}, Last val_loss: {2}, Mean val_loss: {3}, Last val accuracy: {4}, Mean val acc: {5}\".format(*module.value_to_print(measure_by)))\n    print(\"\\n\" + \"* \"*10+'\\n')\n    \n    \n    for generation in range(1, nGenerations):    \n        print(f\"\\nGeneration {generation}:\\n\")\n        for module, indicator in zip(EvolutionModule.results[generation-1], [True]*nMax+[False]*len(EvolutionModule.alive)):\n            if indicator: module.give_birth(nPerMax)\n            else: module.die()\n        for module in EvolutionModule.alive:\n            module.fit(train_ds, epochs=nEpochs, validation_data=valid_ds)\n        \n        EvolutionModule.results[generation] = sorted(EvolutionModule.alive, key=lambda x: x.value(measure_by))\n        print(\"\\n\" + \"* \"*10)\n        print(f'Generation: {generation}:\\n Best accuracy (by loss): {EvolutionModule.results[generation][0].accuracy()}\\n')\n        for module in EvolutionModule.alive:\n            print(module, end=': ')\n            print(\"Last loss: {0}, Mean loss: {1}, Last val_loss: {2}, Mean val_loss: {3}, Last val accuracy: {4}, Mean val acc: {5}\".format(*module.value_to_print(measure_by)))\n        print(\"\\n\" + \"* \"*10+'\\n')\n        \n    print(\"\\n~ ~ ~ ~ ~ ~ Done! ~ ~ ~ ~ ~ ~\\n\")\n    return EvolutionModule.results[-1][0], EvolutionModule.generations, EvolutionModule.results, EvolutionModule.alive","metadata":{"id":"HpQE59lnTW7w","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evolution(result_dct={}, nStart=10, nEpochs=10, nMax=2, nPerMax=3, nGenerations=5, rand=0.1, measure_by=3):\n    result_dct[0] = []\n    for i in range(nStart):\n        print(\"\\nStarting first generation module number\", i, \"...\\n\")\n        model = create_model()\n        history = model.fit(train_ds, epochs=nEpochs, validation_data=valid_ds)\n        result_dct[0].append((model, history.history['loss'], history.history['val_loss'], history.history['val_logits_binary_accuracy']))\n    for generation in range(1,nGenerations):\n        result_dct[generation] = []\n        max_chosen = sorted(result_dct[generation-1], key=lambda x: sum(x[1][-measure_by:]))[:nMax]\n        \n        print(\"\\n\" + \"* \"*10)\n        print(f'Generation: {generation}:\\n Best accuracy (by loss): {max_chosen[0][-1][-1]}\\n')\n        for result in result_dct[generation-1]:\n            print(\"last loss:\", result[1][-1], \" loss mean:\", np.mean(result[1][-measure_by:]),\"   last accuracy:\", result[-1][-1], \"  accuracy mean:\", np.mean(result[-1][-measure_by:]))\n        print(\"\\n\" + \"* \"*10+'\\n')\n        \n        print(\"Starting generation \", generation+1,\"...\\n\")\n        i = 1\n        for max_model, _, _ in max_chosen:\n            old_weights = max_model.get_weights()\n            for _ in range(nPerMax):\n                print(\"Starting module number \", i, \" out of\", nMax*(nPerMax+1), \"\\n\")\n                new_weights = [weight for weight in old_weights]\n                old_conv_weights = new_weights[2]\n                new_weights[2] = old_conv_weights + np.random.rand(*old_conv_weights.shape) * old_conv_weights.std() * rand\n                model = create_model()\n                model.set_weights(new_weights)\n                history = model.fit(train_ds, epochs=nEpochs, validation_data=valid_ds)\n                result_dct[generation].append((model, history.history['loss'], history.history['val_loss'], history.history['val_logits_binary_accuracy']))\n                i+=1\n            print(\"\\nStarting father...\\n\")\n            history = max_model.fit(train_ds, epochs=nEpochs, validation_data=valid_ds)\n            result_dct[generation].append((model, history.history['loss'], history.history['val_loss'], history.history['val_logits_binary_accuracy']))\n    max_chosen = sorted(result_dct[generation-1], key=lambda x: sum(x[1][-measure_by:]))[:nMax]\n        \n    print(\"\\n\" + \"* \"*10)\n    print(f'Generation: {nGenerations}:\\n Best accuracy (by loss): {max_chosen[0][-1][-1]}\\n')\n    for result in result_dct[nGenerations-1]:\n        print(\"last loss:\", result[2][-1], \" loss mean:\", np.mean(result[2][-measure_by:]),\"   last accuracy:\", result[-1][-1], \"  accuracy mean:\", np.mean(result[-1][-measure_by:]))\n    print(\"\\n\" + \"* \"*10+'\\n')\n    return result_dct","metadata":{"id":"R9SDvX_PTW7w","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best_module, generations, results, alive = run_evolution(nStart=10, nEpochs=10, nMax=3, nPerMax=2, nGenerations=6)\n# print(best_module.value_to_print(1))","metadata":{"id":"jxFh6AtQTW7w","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot_examples(best_module)","metadata":{"id":"1NDh5i2eTW7x","outputId":"87545486-88dc-4cce-8c22-5d011461d2ec","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot_statistics(best_module)","metadata":{"id":"zUo2VSI5TW7y","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cur_model = best_module\n# while cur_model:\n#     print(\" * * * * * * *\"*2)\n#     print(\"Current module:\", cur_model)\n#     print(\"current weights:\", best_module.get_weights()[2].std())\n#     print(\"Original weights:\", best_module.original_weights[2].std())\n#     cur_model = cur_model.parent","metadata":{"id":"165eAukhTW7y","trusted":true},"execution_count":null,"outputs":[]}]}