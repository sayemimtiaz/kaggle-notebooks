{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Welcome to this Kernel\n\n***This kernel is a compilation of tricks of sklearn published weekly by Kevin Markham.***\n\nYou can find the original tricks and tips on the GitHub repo:\n\nhttps://github.com/justmarkham/scikit-learn-tips","metadata":{}},{"cell_type":"markdown","source":"<a id = \"table_of_contents\"></a>\n# Table of contents\n\n[Importing libraries and setting some helper functions](#Imports)\n\n[Trick 50: General pattern to many ML problems](#trick50)\n\n[Trick 49: Tune multiple models simultaneously with GridSearchCV](#trick49)\n\n[Trick 48: You can access a part of a pipeline using Python slicing](#trick48)\n\n[Trick 46: Ensemble multiple methods using VotingClassifier or VotingRegressor](#trick46)\n\n[Trick 45: Create polynomial features](#trick45)\n\n[Trick 44: Speed up the GridSearchCV](#trick44)\n\n[Trick 43: OrdinalEncoder vs OneHotEncoder for tree based models](#trick43)\n\n[Trick 42: ColumnTransformer use cases of 'passthrough' and 'drop'](#trick42)\n\n[Trick 41: OneHotEncoder, drop colums if it's binary (new in 0.23 and above)](#trick41)\n\n[Trick 40: Estimators only print the parameters that are *not* set to their default values (new in 0.23 and above)](#trick40)\n\n[Trick 39: Load a toy dataset into a DataFrame (new in 0.23 and above)](#trick39)\n\n[Trick 38: Get the features names of a ColumnTransformer (new in 0.23 and above)](#trick38)\n\n[Trick 37: Generating interactive pipelines (new in 0.23 and above)](#trick37)\n\n[Trick 36: Passing parameters as keyword arguments (new in 0.23 and above)](#trick36)\n\n[Trick 35: Passing a df directly to sklearn](#trick35)\n\n[Trick 34: Feature selection with Pipeline](#trick34)\n\n[Trick 33: Using custom and existing function in a ColumnTransformer](#trick33)\n\n[Trick 32: Area Under Curve (AUC) for binary classification: ovo and ovr strategies](#trick32)\n\n[Trick 31: Shuffle when using cross_val_score](#trick31)\n\n[Trick 30: Four ways of displaying the model coefficients](#trick30)\n\n[Trick 29: Vectorize two text columns using ColumnTransformer](#trick29)\n\n[Trick 28: Save a model of pipeline using joblib](#trick28)\n\n[Trick 27: Imputing missing values for categorical values](#trick27)\n\n[Trick 26: Use of stratify when performing classification problems](#trick26)\n\n[Trick 25: Prunning decision trees (new in 0.22 and above).](#trick25)\n\n[Trick 24: Plotting the decision tree with sklearn (new in 0.21 and above)](#trick24)\n\n[Trick 23: Display the intercept & coefficients for a liner model](#trick23)\n\n[Trick 22: Two types of Pipelines](#trick22)\n\n[Trick 21: Several ROC curves in a single plot (new in sklearn 0.22)](#trick21)\n\n[Trick 20: Plot confusion matrix (new in sklearn 0.22)](#trick20)\n\n[Trick 19: Most important parameters of a LogisticRegression](#trick19)\n\n[Trick 18: Convert your GridSearchCV or RandomizedGridSearch results into a pandas DataFrame](#trick18)\n\n[Trick 17: RandomizedGridSearch](#trick17)\n\n[Trick 16: Crossvalidate and gridsearch a sklearn pipeline](#trick16)\n\n[Trick 15: OneHotEncoder: tips using it](#trick15)\n\n[Trick 14: Handling missing values](#trick14)\n\n[Trick 13: Examine each step of a Pipeline](#trick13)\n\n[Trick 12: Difference between Pipeline and make_pipeline](#trick12)\n\n[Trick 11: KNNImputer](#trick11)\n\n[Trick 10: Using random_state to reproduce results](#trick10)\n\n[Trick 9: Using missing values as a feature: SimpleImputer & add_indicator = True](#trick9)\n\n[Trick 8: Using make_pipeline in a ML project](#trick8)\n\n[Trick 7: Handle new data while using OneHotEncoder](#trick7)\n\n[Trick 6: Common ways to encode categorical features: OneHotEncoder, OrdinalEncoder](#trick6)\n\n[Trick 5: Benefits of using sklearn for preprocessing and not pandas](#trick5)\n\n[Trick 4: When to use fit_transform and transform methods](#trick4)\n\n[Trick 3: Difference between fit and transform method](#trick3)\n\n[Trick 2: Seven ways to select columns using ColumnsTransformer](#trick2)\n\n[Trick 1: Using column selector to transform different columns](#trick1)\n","metadata":{}},{"cell_type":"markdown","source":"<a id = \"Imports\"></a>\n# Importing libraries and setting some helper functions\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"# basic libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython.display import Image\n\n# this will allow us to print all the files as we generate more in the kernel\ndef print_files(directory = \"output\"):\n    if directory.lower() == \"input\":\n        for dirname, _, filenames in os.walk('/kaggle/input'):\n            for filename in filenames:\n                print(os.path.join(dirname, filename))\n    else:\n        for dirname, _, filenames in os.walk('/kaggle/working'):\n            for filename in filenames:\n                print(os.path.join(dirname, filename))\n\n\ndef generate_sample_data(): # creates a fake df for testing\n    number_or_rows = 20\n    num_cols = 7\n    cols = list(\"ABCDEFG\")\n    df = pd.DataFrame(np.random.randint(1, 20, size = (number_or_rows, num_cols)), columns=cols)\n    df.index = pd.util.testing.makeIntIndex(number_or_rows)\n    return df\n\n\ndef generate_sample_data_datetime(): # creates a fake df for testing\n    number_or_rows = 365*24\n    num_cols = 2\n    cols = [\"sales\", \"customers\"]\n    df = pd.DataFrame(np.random.randint(1, 20, size = (number_or_rows, num_cols)), columns=cols)\n    df.index = pd.util.testing.makeDateIndex(number_or_rows, freq=\"H\")\n    return df\n\n# show several prints in one cell. This will allow us to condence every trick in one cell.\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\nprint_files(\"input\")\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = \"../input/learn-sklearn\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick50\"></a>\n# Trick 50: General pattern to solve many ML problems\n[Go back to the Table of Contents](#table_of_contents)\n\nLet's build in this cell a general pattern to solve many ML problems\n\nSome of the shortcomings it has are:\n\n* Assumes all columns have proper data types\n\n* May include irrelevant or improper features\n\n* Does not handle text or date columns well\n\n* Does not include feature engineering\n\n* Ordinal encoding may be better\n\n* Other imputation strategies may be better\n\n* Numeric features may not need scaling\n\n* A different model may be better\n\n* And so on...","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import make_column_selector, make_column_transformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\n#------------------------------------------------------------\n# get the data\nlc = ['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\ndf = pd.read_csv(os.path.join(PATH, \"train.csv\"))\nX = df[lc]\ny = df['Survived']\n\nX_test = pd.read_csv(os.path.join(PATH, \"train.csv\"), usecols = lc)\n\n# first let's define the steps that will allow us to filter our columns\nnumeric_select = make_column_selector(dtype_include = \"number\")\nno_numeric_select = make_column_selector(dtype_exclude = 'number')\n\npreprocessor = make_column_transformer(\n    (make_pipeline(SimpleImputer(strategy = \"mean\"), StandardScaler()), numeric_select), # select all numeric columns, impute mean and scale the data\n    (make_pipeline(SimpleImputer(strategy = \"constant\"), OneHotEncoder(handle_unknown='ignore')), no_numeric_select) # select all non numeric columns, impute most frequent value and OneHotEncode\n) \n\npipe = make_pipeline(preprocessor, LogisticRegression())\n\n# see the crossvalidation score\nprint(\"CV score is {}\".format(cross_val_score(pipe, X, y).mean()))\n\n# apply the same pipeline and predict\n# fit the pipeline and make predictions\npipe.fit(X, y)\ny_pred = pipe.predict(X_test)\n\nprint(\"Here we have our predictions\",y_pred[:10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick49\"></a>\n# Trick 49: Tune multiple models simultaneously with GridSearchCV\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\n#------------------------------------------------------------\n# get the data\ndf = pd.read_csv(os.path.join(PATH, \"train.csv\"))\nX = df[['Sex', 'Name', 'Age']]\ny = df['Survived']\n\n# this will be the first Pipeline step\nct = ColumnTransformer(\n    [('ohe', OneHotEncoder(), ['Sex']),\n     ('vectorizer', CountVectorizer(), 'Name'),\n     ('imputer', SimpleImputer(), ['Age'])])\n\n# each of these models will take a turn as the second Pipeline step\nclf1 = LogisticRegression(solver='liblinear', random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\n\n# create the Pipeline\npipe = Pipeline([('preprocessor', ct), ('classifier', clf1)])\n\n# create the parameter dictionary for clf1\nparams1 = {}\nparams1['preprocessor__vectorizer__ngram_range'] = [(1, 1), (1, 2)]\nparams1['classifier__penalty'] = ['l1', 'l2']\nparams1['classifier__C'] = [0.1, 1, 10]\nparams1['classifier'] = [clf1]\n\n# create the parameter dictionary for clf2\nparams2 = {}\nparams2['preprocessor__vectorizer__ngram_range'] = [(1, 1), (1, 2)]\nparams2['classifier__n_estimators'] = [100, 200]\nparams2['classifier__min_samples_leaf'] = [1, 2]\nparams2['classifier'] = [clf2]\n\n# create a list of parameter dictionaries\nparams = [params1, params2]\n\n# this will search every parameter combination within each dictionary\ngrid = GridSearchCV(pipe, params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the best parameters for each method\ngrid.fit(X, y)\ngrid.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick48\"></a>\n# Trick 48: You can access a part of a pipeline using Python slicing\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_selection import SelectPercentile, chi2\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\n\n#------------------------------------------------------------\n# get the data\ndf = pd.read_csv(os.path.join(PATH, \"train.csv\"))\nX = df[['Parch', 'Fare', 'Embarked', 'Sex', 'Name', 'Age']]\ny = df['Survived']\n\nimp_constant = SimpleImputer(strategy='constant')\nohe = OneHotEncoder()\n\nimp_ohe = make_pipeline(imp_constant, ohe)\nvect = CountVectorizer()\nimp = SimpleImputer()\n\n# pipeline step 1\nct = make_column_transformer(\n    (imp_ohe, ['Embarked', 'Sex']),\n    (vect, 'Name'),\n    (imp, ['Age', 'Fare']),\n    ('passthrough', ['Parch']))\n\n# pipeline step 2\nselection = SelectPercentile(chi2, percentile=50)\n\n# pipeline step 3\nlogreg = LogisticRegression(solver='liblinear')\n\n# display estimators as diagrams\nfrom sklearn import set_config\nset_config('diagram')\n\npipe = Pipeline([('preprocessor', ct), ('feature selector', selection), ('classifier', logreg)])\n\n# now we can use our pipeline to fit_transform everything\nX_all = pipe.fit(X, y)\npipe.score(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# but can also access just a part of the pipeline\npipe[1]\nX_parcial = pipe[0].fit_transform(X)\nX_parcial","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick47\"></a>\n# Trick 47: Tune the parameters of a VotingClassifier\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\n\n#------------------------------------------------------------\n# get the data\ndf = pd.read_csv(os.path.join(PATH, \"train.csv\"), usecols = ['Survived', 'Pclass', 'Parch', 'SibSp', 'Fare'])\ndf.dropna(inplace = True)\n# separate X and y\nX = df.drop(\"Survived\", axis = \"columns\")\ny = df[\"Survived\"]\n\n#------------------------------------------------------------\n# instanciate individual models\nlr = LogisticRegression()\nrf = RandomForestClassifier()\nmnb = MultinomialNB()\ndt = DecisionTreeClassifier()\n\n#------------------------------------------------------------\n# create an ensemble for improved accuracy\nvc = VotingClassifier([('clf1', lr), ('clf2', rf), (\"clf3\", mnb), (\"clf4\", dt)], voting = 'soft')\nprint(\"CV Score of a VotingClassifier is {}\".format(cross_val_score(vc, X, y).mean()))\n\n#------------------------------------------------------------\n# GridSearch the best parameters\nparams = {'voting':['hard', 'soft'],\n          'weights':[(1,1,1,1), (2,1,1,1), (1,2,1,1), (1,1,2,1), (1,1,1,2)]}\n\n# find the best set of parameters\ngrid = GridSearchCV(vc, params)\ngrid.fit(X, y)\ngrid.best_params_\n\n# accuracy has improved\nprint(\"CV Score of a VotingClassifier with GridSearchCV is {}\".format(grid.best_score_))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick46\"></a>\n# Trick 46: Ensemble multiple methods using VotingClassifier or VotingRegressor\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.model_selection import cross_val_score\n\n#------------------------------------------------------------\n# get the data\ndf = pd.read_csv(os.path.join(PATH, \"train.csv\"), usecols = ['Survived', 'Pclass', 'Parch', 'SibSp', 'Fare'])\ndf.dropna(inplace = True)\n# separate X and y\nX = df.drop(\"Survived\", axis = \"columns\")\ny = df[\"Survived\"]\n\n#------------------------------------------------------------\n# let's fit the individual models\nlr = LogisticRegression()\nprint(\"CV Score of LogisticRegression is {}\".format(cross_val_score(lr, X, y).mean()))\n\nrf = RandomForestClassifier()\nprint(\"CV Score of RandomForest is {}\".format(cross_val_score(rf, X, y).mean()))\n\n#------------------------------------------------------------\n# create an ensemble for improved accuracy\nvc = VotingClassifier([('clf1', lr), ('clf2', rf)], voting = 'soft')\nprint(\"CV Score of a VotingClassifier is {}\".format(cross_val_score(vc, X, y).mean()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick45\"></a>\n# Trick 45: Create polynomial features\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"# a very fast way to create features in sklearn\n# but be careful, it might be time consuming and impracticale for some algorithms\n\n#------------------------------------------------------------\n# import libraries\nfrom sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(include_bias=False, interaction_only=True)\n\n#------------------------------------------------------------\n# get the data\nX = pd.DataFrame({'A':[1, 2, 3], 'B':[4, 4, 4], 'C':[0, 10, 100]})\n\n\n#------------------------------------------------------------\n# create new features\n# Output columns: A, B, C, A*B, A*C, B*C\npoly.fit_transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick44\"></a>\n# Trick 44: Speed up the GridSearchCV\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\n#------------------------------------------------------------\n# get the data\ndf = pd.read_csv(os.path.join(PATH, \"train.csv\"), usecols = [\"Survived\",\"Sex\",\"Name\", \"Age\"])\ndf.dropna(inplace = True)\n# separate X and y\nX = df.drop(\"Survived\", axis = \"columns\")\ny = df[\"Survived\"]\n\n#------------------------------------------------------------\n# make a ColumTransformer, instaciate a model and build a pipeline\n\nct = ColumnTransformer([(\"ohe\", OneHotEncoder(), [\"Sex\"]),\n                       (\"vectorizer\", CountVectorizer(), \"Name\"),\n                       (\"imputer\", SimpleImputer(), [\"Age\"])])\n\nclf = LogisticRegression(solver='liblinear', random_state=1)\n\npipe = Pipeline([(\"preprocessor\", ct), (\"classifier\", clf)])\n\n# set the parameters for the GridSearch\n\nparams = {}\nparams['preprocessor__ohe__drop'] = [None, 'first']\nparams['preprocessor__vectorizer__min_df'] = [1, 2, 3]\nparams['preprocessor__vectorizer__ngram_range'] = [(1, 1), (1, 2)]\nparams['classifier__C'] = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\nparams['classifier__penalty'] = ['l2']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's time the GridSearch without all CPU\n\ngrid = GridSearchCV(pipe, params)\n%time grid.fit(X, y);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's time the GridSearch with all CPU\n# as you can see, when using -1, it's much faster\ngrid = GridSearchCV(pipe, params, n_jobs = -1)\n%time grid.fit(X, y);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick43\"></a>\n# Trick 43: OrdinalEncoder vs OneHotEncoder for tree based models\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\nfrom sklearn.pipeline import make_pipeline\n\n#------------------------------------------------------------\n# get the data\ndf = pd.read_csv(os.path.join(PATH, \"train.csv\"), usecols = [\"Survived\", \"Pclass\", \"Sex\", \"Embarked\"])\ndf.dropna(inplace = True)\n# separate X and y\nX = df.drop(\"Survived\", axis = \"columns\")\ny = df[[\"Survived\"]]\n\n#------------------------------------------------------------\n# instanciate 2 encoders\noe = OrdinalEncoder()\nohe = OneHotEncoder()\n\nX_oe = oe.fit_transform(df)\nX_ohe = ohe.fit_transform(df)\n\n#------------------------------------------------------------\n# let's see the shape of a resulting DataFrame using OE or OHE\nprint('The dataframe transformed with OrdinalEncoder has a shape of {}'.format(X_oe.shape))\nprint('The dataframe transformed with OneHotEncoder has a shape of {}'.format(X_ohe.shape))\n\n#------------------------------------------------------------\n# let's measure the time needed for training and the corresponing cv score\noe_pipe = make_pipeline(oe, RandomForestClassifier(random_state = 175))\nohe_pipe = make_pipeline(ohe, RandomForestClassifier(random_state = 175))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%time cross_val_score(oe_pipe, X, y).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# as you can see, oe is slightly faster and accuracy doesn't suffer that much.\n%time cross_val_score(ohe_pipe, X, y).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick42\"></a>\n# Trick 42: ColumnTransformer use cases of 'passthrough' and 'drop'\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.compose import make_column_transformer\n\n#------------------------------------------------------------\n# load some data\nX = pd.DataFrame({'A':[1, 2, np.nan],\n                  'B':[10, 20, 30],\n                  'C':[100, 200, 300],\n                  'D':[1000, 2000, 3000],\n                  'E':[10000, 20000, 30000]})\n\n# use ColumnTransformer to\n# 1. imputer A\n# 2. ignore/passthrough B,C\n# 3. drop D, E\n\nct = make_column_transformer((SimpleImputer(), [\"A\"]),\n                            (\"passthrough\", [\"B\", \"C\"]),\n                             remainder = \"drop\")\n\nct.fit_transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use ColumnTransformer to\n# 1. imputer A\n# 2. drop D, C\n# 3. ignore/passthrough B, E\n\nct = make_column_transformer((SimpleImputer(), [\"A\"]),\n                            (\"drop\", [\"D\", \"C\"]),\n                             remainder = \"passthrough\")\n\nct.fit_transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick41\"></a>\n# Trick 41: OneHotEncoder, drop colums if it's binary (new in 0.23 and above)\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.preprocessing import OneHotEncoder\n\n#------------------------------------------------------------\n# load some data\ndf = pd.DataFrame({'Shape':['circle', 'oval', 'square', 'square'],\n                  'Color': ['pink', 'yellow', 'pink', 'yellow']})\n\n# as you can see, color is a binary columns\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the default behaviour creates a colum per category\n# Note: we change sparse = False, to see our matrix\nohe_default = OneHotEncoder(sparse = False).fit_transform(df)\nohe_default","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the default behaviour creates a colum per category\n# add drop = \"first\" to drop first category in each columns\nohe_drop_first = OneHotEncoder(sparse = False, drop = \"first\").fit_transform(df)\nohe_drop_first","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the default behaviour creates a colum per category\n# new in 0.23 allows you to drop if it's binary\n# ohe_if_binary = OneHotEncoder(sparse = False, drop = \"if_binary\").fit_transform(df)\n# ohe_if_binary\n\n# in older version you get this error\n# ValueError: Wrong input for parameter `drop`. Expected 'first', None or array of objects, got <class 'str'>","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick40\"></a>\n# Trick 40: Estimators only print the parameters that are *not* set to their default values (new in 0.23 and above)\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.linear_model import LogisticRegression\n\n#------------------------------------------------------------\n# in old versions of sklearn, when you instanciate an estimator class you will see all it's parameters\n# in 0.23 and above, you will only see the parameters that have been changed\n\nclf_old = LogisticRegression(C = 0.1, solver = \"liblinear\")\nclf_old\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# in the new version however, you will only see:\n# LogisticRegression(C=0.1, solver='liblinear')\n\n# to see all parameters\nclf_old.get_params()\n\n# restore old default behaviour\n# makes sense in 0.23 and above\nfrom sklearn import set_config\nset_config(print_changed_only = False)\nclf_old","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick39\"></a>\n# Trick 39: Load a toy dataset into a DataFrame (new in 0.23 and above)\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.datasets import load_iris\n\n#------------------------------------------------------------\n# load the iris dataset as a DataFrame\n# this will work in 0.23: as_frame = True\n# df = load_iris(as_frame = True)[\"frame\"]\n\n# return DataFrame with features and Series with target\n# X, y = load_iris(as_frame=True, return_X_y=True)\n\n#------------------------------------------------------------\n# load a dataframe \"old school\" method\ndata = load_iris()\n\nX = data[\"data\"]\ny = data[\"target\"]\ntarget_names = data[\"target_names\"] # original values of the target columns\ncolumns = data[\"feature_names\"]\n\ndf = pd.DataFrame(X, columns = columns)\ndf[\"target\"] = y\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick38\"></a>\n# Trick 38: Get the features names of a ColumnTransformer (new in 0.23 and above)\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import make_column_transformer\n\n#------------------------------------------------------------\n# get the data: select only 4 columns and drop all na\nX = pd.read_csv(os.path.join(PATH, \"train.csv\"), usecols = ['Embarked', 'Sex', 'Parch', 'Fare']).dropna()\n\n#------------------------------------------------------------\n# make a column transform\nct = make_column_transformer((OneHotEncoder(), [\"Embarked\", \"Sex\"]),\n                            remainder = \"passthrough\")\n\n#------------------------------------------------------------\n# fit transform our columns transformer and see the resulting shape\nct.fit_transform(X).shape\n\n#------------------------------------------------------------\n# get the names of the of the features we have created\n# in sklearn 0.23 you won't see this error\n# NotImplementedError: get_feature_names is not yet supported when using a 'passthrough' transformer.\n# ct.get_feature_names()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick37\"></a>\n# Trick 37: Generating interactive pipelines (new in 0.23 and above)\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"# When creating large pipelines, create a pipeline to visualize it easier\n# New in sklearn 0.23\n\n#------------------------------------------------------------\n# import libraries\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_selection import SelectPercentile, chi2\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\n\n#------------------------------------------------------------\n# get the data\ndf = pd.read_csv(os.path.join(PATH, \"train.csv\"))\nX = df[['Parch', 'Fare', 'Embarked', 'Sex', 'Name', 'Age']]\ny = df['Survived']\n\nimp_constant = SimpleImputer(strategy='constant')\nohe = OneHotEncoder()\n\nimp_ohe = make_pipeline(imp_constant, ohe)\nvect = CountVectorizer()\nimp = SimpleImputer()\n\n# pipeline step 1\nct = make_column_transformer(\n    (imp_ohe, ['Embarked', 'Sex']),\n    (vect, 'Name'),\n    (imp, ['Age', 'Fare']),\n    ('passthrough', ['Parch']))\n\n# pipeline step 2\nselection = SelectPercentile(chi2, percentile=50)\n\n# pipeline step 3\nlogreg = LogisticRegression(solver='liblinear')\n\n# display estimators as diagrams\nfrom sklearn import set_config\nset_config('diagram')\n\npipe = make_pipeline(ct, selection, logreg)\npipe","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick36\"></a>\n# Trick 36: Passing parameters as keyword arguments (new in 0.23 and above)\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"# Starting from sklearn 0.23 we must pass the parameters for the functions and classes as \n# keyword and not as positional argument. Otherwise a warning will be raised\n\nimport sklearn\nfrom sklearn.svm import SVC\n\n# here we won't see because, Kaggle uses a previous version\nprint(sklearn.__version__)\n\n# positional argument\nclf = SVC(0.1, 'linear')\n\n# keyword arguments\nclf = SVC(C=0.1, kernel='linear')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick35\"></a>\n# Trick 35: Passing a df directly to sklearn\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"# We don't need to use .values when passing a df or a pandas series to sklearn\n# It knows internally how to acess the values and deal with them\n\n#------------------------------------------------------------\n# import libraries\nfrom sklearn.linear_model import LogisticRegression\n\n#------------------------------------------------------------\n# get the data\ndf = pd.read_csv(os.path.join(PATH, \"train.csv\"))\nX = df[['Pclass', 'Fare']]\ny = df[\"Survived\"]\n\n#------------------------------------------------------------\n# check the X and y types\nprint(type(X))\nprint(type(y))\n\n#------------------------------------------------------------\n# instanciate our classes\nmodel = LogisticRegression()\n\n# we fit directly a df and a series and sklearn deals with the rest\nmodel.fit(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick34\"></a>\n# Trick 34: Feature selection with Pipeline\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import SelectPercentile, chi2\n\n#------------------------------------------------------------\n# get the data\ndf = pd.read_csv(os.path.join(PATH, \"train.csv\"))\nX = df[\"Name\"]\ny = df[\"Survived\"]\n\n#------------------------------------------------------------\n# instanciate our classes\nvectorizer = CountVectorizer()\nmodel = LogisticRegression()\n\n#------------------------------------------------------------\n# make the pipeline without feature selection\npipe = make_pipeline(vectorizer, model)\nscore = cross_val_score(pipe, X, y, scoring = 'accuracy').mean()\nprint(\"Score of pipeline without feature selection is {}\".format(score))\n\n#------------------------------------------------------------\n# make the pipeline without feature selection\n\n# keep 50% of features with the best chi-squared scores\nselection = SelectPercentile(chi2, percentile = 50)\n\n# add the selection after preprocessing but before model\npipe = make_pipeline(vectorizer, selection, model)\nscore = cross_val_score(pipe, X, y, scoring = 'accuracy').mean()\nprint(\"Score of pipeline with feature selection is {}\".format(score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick33\"></a>\n# Trick 33: Using custom and existing function in a ColumnTransformer\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import FunctionTransformer\n\n#------------------------------------------------------------\n# get some data\nX = pd.DataFrame({'Fare':[200, 300, 50, 900],\n                  'Code':['X12', 'Y20', 'Z7', np.nan],\n                  'Deck':['A101', 'C102', 'A200', 'C300']})\n\n#------------------------------------------------------------\n# use an existing column and make the function compatible with Pipeline\nclip_values = FunctionTransformer(np.clip, kw_args={'a_min':100, 'a_max':600})\n\n#------------------------------------------------------------\n# create a custom function\ndef first_letter(string_column):\n    return string_column.apply(lambda x: x.str.slice(0, 1))\n\n# now use FunctionTransformer to make the function compatible with Pipeline\nget_first_letter = FunctionTransformer(first_letter)\n\n#------------------------------------------------------------\n# create the column Transformer\nct = make_column_transformer(\n    (clip_values, ['Fare']),\n    (get_first_letter, ['Code', 'Deck']))\n\n#------------------------------------------------------------\n# Original X\nprint(\"Original X\")\nX\n\n#------------------------------------------------------------\n# Modified X\nprint(\"Modified X\")\nct.fit_transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick32\"></a>\n# Trick 32: Area Under Curve (AUC) for binary classification: ovo and ovr strategies\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.datasets import load_wine\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_val_score\n\n# get X and y for classification\nX, y = load_wine(return_X_y=True)\n\n# select only a few features\nX = X[:, 0:2]\n\n# instanciate the model for regression\nmodel_clf = LogisticRegression()\n\n# Multiclass AUC with train/test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nmodel_clf.fit(X_train, y_train)\ny_score = model_clf.predict_proba(X_test)\n\n# use 'ovo' (One-vs-One) or 'ovr' (One-vs-Rest)\nprint(\"roc_auc_score is {} for one vs one strategy with train test split\".format(roc_auc_score(y_test, y_score, multi_class = 'ovo')))\nprint(\"roc_auc_score is {} for one vs rest strategy with train test split\".format(roc_auc_score(y_test, y_score, multi_class = 'ovr')))\nprint(\"-----------------------------------------\")\n\n# Multiclass AUC with cross-validation\n# use 'roc_auc_ovo' (One-vs-One) or 'roc_auc_ovr' (One-vs-Rest)\nprint(\"cross_val_score is {} for one vs one strategy with cross validation\".format(cross_val_score(model_clf, X, y, cv = 5, scoring = 'roc_auc_ovo').mean()))\nprint(\"cross_val_score is {} for one vs rest strategy with cross validation\".format(cross_val_score(model_clf, X, y, cv = 5, scoring = 'roc_auc_ovr').mean()));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick31\"></a>\n# Trick 31: Shuffle when using cross_val_score\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\n# get X and y for regression\nX_reg, y_reg = load_diabetes(return_X_y = True)\n\n# get X and y for classification\ndf = pd.read_csv(os.path.join(PATH, \"train.csv\"), usecols = ['Pclass', 'Fare', 'SibSp', 'Survived']).dropna()\n\n# separate X and y\nX_clf = df[['Pclass', 'Fare', 'SibSp']]\ny_clf = df[[\"Survived\"]]\n\n# instanciate the model for regression\nmodel_reg = LinearRegression()\n\n# instanciate the model for classification\nmodel_clf = LogisticRegression()\n\n# Use KFold for regression\nkf = KFold(5, shuffle = True, random_state = 1)\nprint(\"cross_val_score for regression model\")\ncross_val_score(model_reg, # the regression model, in our case, LinearRegression\n                X_reg, # X: features to learn from\n                y_reg, # y: what the predict\n                cv = kf, # cross_validation scheme we have created earlier\n                scoring = \"r2\") # metric to use to validate the quality of the model\n\n# Use StratifiedKFold for classification\nskf = StratifiedKFold(5, shuffle = True, random_state = 1)\nprint(\"cross_val_score for classification model\")\ncross_val_score(model_clf, # the model, in our case, LogisticRegression\n                X_clf, # X: features to learn from\n                y_clf, # y: what the predict\n                cv = skf, # cross_validation scheme we have created earlier\n                scoring = \"accuracy\") # metric to use to validate the quality of the model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick30\"></a>\n# Trick 30: Four ways of displaying the model coefficients\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\n\n#------------------------------------------------------------\n# instanciate the classes\nohe = OneHotEncoder()\nmodel = LogisticRegression()\n\n#------------------------------------------------------------\n# get the data\ndf = pd.read_csv(os.path.join(PATH, \"train.csv\"), usecols = ['Embarked', 'Survived']).dropna()\n\n# separate X and y\nX = df[[\"Embarked\"]]\ny = df[[\"Survived\"]]\n\n# create a pipeline and fit the X and y\npipe = Pipeline([(\"ohe\", ohe),\n                 (\"clf\", model)])\npipe.fit(X, y)\n\n# inspect the coefficients\nprint(\"1 way to show model coefficients\")\npipe.named_steps.clf.coef_\nprint(\"2 way to show model coefficients\")\npipe.named_steps[\"clf\"].coef_\nprint(\"3 way to show model coefficients\")\npipe[\"clf\"].coef_\nprint(\"4 way to show model coefficients\")\npipe[1].coef_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick29\"></a>\n# Trick 29: Vectorize two text columns using ColumnTransformer\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer\n\n#------------------------------------------------------------\n# get train data\ndf_train = pd.read_csv(os.path.join(PATH, \"train.csv\"))\n# drop any nans\nX = df_train[[\"Name\", \"Cabin\"]].dropna()\n\n#------------------------------------------------------------\n# instanciate CountVectorizer\ncount_vect = CountVectorizer()\n\n#------------------------------------------------------------\n# instanciate CountVectorizer\n# You can pass the CountVectorizer multiple times and it will learn\n# separate vocabularies.\n# to do so, you must use make_column_transformer\nct = make_column_transformer((count_vect, 'Name'), (count_vect, 'Cabin'))\nX_transform = ct.fit_transform(X)\nX_transform","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick28\"></a>\n# Trick 28: Save a model of pipeline using joblib\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nimport joblib\n\n#------------------------------------------------------------\n# get train data\ndf_train = pd.read_csv(os.path.join(PATH, \"train.csv\"))\n\n# drop any nans\ndf_train.dropna(axis = \"rows\", inplace = True)\n\n# separate X and y\ncols_for_x = [\"Embarked\", \"Sex\"]\nX_train = df_train[cols_for_x]\ny_train = df_train[\"Survived\"]\n\n#------------------------------------------------------------\n# get test data\ndf_test = pd.read_csv(os.path.join(PATH, \"test.csv\"))\n# drop any nans\ndf_test.dropna(axis = \"rows\", inplace = True)\n\nX_test = df_test[cols_for_x]\n\n#------------------------------------------------------------\n# instanciate Ohe and make_pipeline\nohe = OneHotEncoder()\nmodel = LogisticRegression()\n\n#------------------------------------------------------------\n# create the pipeline\npipe = make_pipeline(ohe, model)\n\n#------------------------------------------------------------\n# predict_using pipeline\npipe.fit(X_train, y_train)\n\n#------------------------------------------------------------\n# save pipeline\njoblib.dump(pipe, 'pipe.joblib')\n\n# print our newly saved pipeline\nprint_files()\n\n#------------------------------------------------------------\n# save pipeline\nnew_pipe = joblib.load('/kaggle/working/pipe.joblib')\n\n#------------------------------------------------------------\n# predict using the same pipe and the old pipe\nprint(\"------------\")\nprint(\"Old pipe.\")\npipe.predict(X_test)\nprint(\"------------\")\nprint(\"Old pipe.\")\nnew_pipe.predict(X_test)\nprint(\"Notice that both pipes predict the same result.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick27\"></a>\n# Trick 27: Imputing missing values for categorical values\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# get some fake data\nd = {\"Shape_Original\":[\"square\", \"square\", \"square\", \"oval\", \"circle\", np.nan]}\ndf = pd.DataFrame(d)\n\n#------------------------------------------------------------\n# import libraries\nfrom sklearn.impute import SimpleImputer\n\n#------------------------------------------------------------\n# impute values using most frequent\ndf[\"most_frequent\"] = SimpleImputer(strategy = \"most_frequent\").fit_transform(df[[\"Shape_Original\"]])\n\n#------------------------------------------------------------\n# impute values using most constant\ndf[\"constant\"]  = SimpleImputer(strategy = \"constant\", fill_value = \"missing\").fit_transform(df[[\"Shape_Original\"]])\n\n#------------------------------------------------------------\n# the result of our imputation\ndf.style.apply(lambda x: ['background: lightgreen' if x.name == 5 else '' for i in x], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick26\"></a>\n# Trick 26: Use of stratify when performing classification problems\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.model_selection import train_test_split\n\n#------------------------------------------------------------\n# generate some data and separate X and y\ndf = pd.DataFrame({'feature':list(range(8)), 'target':['not fraud']*6 + ['fraud']*2})\nX = df[['feature']]\ny = df['target']\n\n#------------------------------------------------------------\n# train and test without stratify\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state = 1)\n\nprint(\"y_train withous stratify\")\ny_train\nprint(\"y_test withous stratify\")\ny_test\n\n\n#------------------------------------------------------------\n# train and test with stratify\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, stratify = y, random_state = 1)\n\nprint(\"Notice how using statify preserves the fraud and not fraud percentage.\")\nprint(\"y_train with stratify\")\ny_train\nprint(\"y_test with stratify\")\ny_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick25\"></a>\n# Trick 25: Prunning decision trees (new in 0.22 and above).\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"import sklearn\nprint(sklearn.__version__)\n\n#------------------------------------------------------------\n# import libraries\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\n#------------------------------------------------------------\n# import data\ndf = pd.read_csv(os.path.join(PATH, \"train.csv\"))\ndf['Sex'] = df['Sex'].map({'male':0, 'female':1})\nX = df[[\"Pclass\", \"Fare\", \"Sex\"]]\ny = df[\"Survived\"]\n\n#------------------------------------------------------------\n# basic model and evaluation\nmodel = DecisionTreeClassifier(random_state = 175)\nmodel.fit(X, y)\nscore = cross_val_score(model, X, y, scoring = \"accuracy\")\nprint(\"Our DecissionTree with {} nodes has scored {}\".format(model.tree_.node_count, score.mean()))\n\n#------------------------------------------------------------\n# prun the tree and see cross validation score\n# Notice that the score went up. Prunnig trees has a lot of benefits, the main one is reducing overfitting.\n# ccp_alpha is the parameter that controls the decision tree complexity (cost complexity parameter).\n# Greater values of ccp_alpha increase the number of nodes pruned.\n# documentation \n# https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html\n\nmodel = DecisionTreeClassifier(ccp_alpha = 0.001, random_state = 175)\nmodel.fit(X, y)\nscore = cross_val_score(model, X, y, scoring = \"accuracy\")\nprint(\"Our prunned DecissionTree with {} nodes has scored {}\".format(model.tree_.node_count, score.mean()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick24\"></a>\n# Trick 24: Plotting the decision tree with sklearn (new in 0.21 and above).\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"import sklearn\nprint(sklearn.__version__)\n\n#------------------------------------------------------------\n# import libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n\n#------------------------------------------------------------\n# create our instances\nmodel = DecisionTreeClassifier()\n\n#------------------------------------------------------------\n# import data\ndf = pd.read_csv(os.path.join(PATH, \"train.csv\"))\ndf['Sex'] = df['Sex'].map({'male':0, 'female':1})\nX = df[[\"Pclass\", \"Fare\", \"Sex\"]]\ny = df[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Fare\", \"Sex\"]\nclasses = [\"Survived\"]\n\n#------------------------------------------------------------\n# train test split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n\n#------------------------------------------------------------\n# fit and predict\n\nmodel.fit(X_train, y_train)\n\n#------------------------------------------------------------\n# plot the tree\n\nplt.figure(figsize = (20, 10))\nplot_tree(model, feature_names = features, filled = True);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#------------------------------------------------------------\n# show the text\n# I will plot only the first 200 characters of the tree since it grows rapidly\nprint(export_text(model, feature_names = features, show_weights=True)[:200]);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick23\"></a>\n# Trick 23: Display the intercept & coefficients for a liner model\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.linear_model import LinearRegression\n\n#------------------------------------------------------------\n# load data and separate X and y\ndataset = load_diabetes()\nX, y = dataset.data, dataset.target\nfeatures = dataset.feature_names\n\n#------------------------------------------------------------\n# fit model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n#------------------------------------------------------------\n# intercept and coef\nmodel.intercept_\nmodel.coef_\nlist(zip(features, model.coef_))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick22\"></a>\n# Trick 22: Two types of Pipelines\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# Two types of ROC Curve\n\n# If the pipeline ends in a classifier or regressor, you use the fit and predict methods\n# If the pipeline ends in a transformer you use the fit_transform and transform methods\n\npath = os.path.join(PATH, \"ROC Curve.jpeg\")\nImage(path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick21\"></a>\n# Trick 21: Several ROC curves in a single plot (new in sklearn 0.22 and above)\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"import sklearn\nprint(sklearn.__version__)\n\n#------------------------------------------------------------\n# import libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n#------------------------------------------------------------\n# create our instances\nlr = LogisticRegression()\ndt = DecisionTreeClassifier()\nrf = RandomForestClassifier()\n\n#------------------------------------------------------------\n# import data\ndf = pd.read_csv(os.path.join(PATH, \"train.csv\"))\nX = df[[\"Pclass\", \"Fare\"]]\ny = df[\"Survived\"]\n\n#------------------------------------------------------------\n# train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n\n#------------------------------------------------------------\n# fit and predict\nlr.fit(X_train, y_train)\ndt.fit(X_train, y_train)\nrf.fit(X_train, y_train)\n\n#------------------------------------------------------------\n# plot roc curve\ndisp = plot_roc_curve(lr, X_test, y_test)\nplot_roc_curve(dt, X_test, y_test, ax = disp.ax_) # ax = disp.ax_ this line will share the x axis\nplot_roc_curve(rf, X_test, y_test, ax = disp.ax_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick20\"></a>\n# Trick 20: Plot confusion matrix (new in sklearn 0.22 and above)\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"import sklearn\nprint(sklearn.__version__)\n\n#------------------------------------------------------------\n# import libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\n\n#------------------------------------------------------------\n# create our instances\nmodel = LogisticRegression(random_state = 1)\n\n#------------------------------------------------------------\n# import data\ndf = pd.read_csv(os.path.join(PATH, \"train.csv\"))\nX = df[[\"Pclass\", \"Fare\"]]\ny = df[\"Survived\"]\n\n#------------------------------------------------------------\n# train test split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n\n#------------------------------------------------------------\n# fit and predict\n\nmodel.fit(X_train, y_train)\n\n#------------------------------------------------------------\n# plot confusion matrix\n# notice that you have to pass the model, X_test and y_test\n# plot_confusion_matrix predicts with the model and plots the values\n\ndisp = plot_confusion_matrix(model, X_test, y_test, cmap = \"Blues\", values_format = \".3g\")\n\nprint(\"The classical confusion matrix\")\ndisp.confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick19\"></a>\n# Trick 19: Most important parameters of a LogisticRegression\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# C: inverse of regularization strength\n# penalty: type of regularization\n# solver: algorithm used for optimization\n\npath = os.path.join(PATH, \"LogisticRegression.jpg\")\nImage(path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick18\"></a>\n# Trick 18: Convert your GridSearchCV or RandomizedGridSearch results into a pandas DataFrame\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\n#------------------------------------------------------------\n# create our instances\nohe = OneHotEncoder()\nvect = CountVectorizer()\nct = make_column_transformer((ohe, [\"Sex\"]), (vect, \"Name\"))\nmodel = LogisticRegression(solver = \"liblinear\", random_state = 1)\n\n#------------------------------------------------------------\n# import data\ndf = pd.read_csv(os.path.join(PATH, \"train.csv\"))\nX = df[[\"Sex\", \"Name\", \"Fare\"]]\ny = df[\"Survived\"]\n\n#------------------------------------------------------------\n# make pipeline\npipeline = make_pipeline(ct, model)\n\n#------------------------------------------------------------\n# cross validate the entire pipeline\nprint(\"Notice the score of our entire pipeline is {}\".format(cross_val_score(pipeline, X, y, cv = 5, scoring = \"accuracy\").mean()))\ncross_val_score(pipeline, X, y, cv = 5, scoring = \"accuracy\").mean()\n\n#------------------------------------------------------------\n# gridsearch the entire pipeline\n\n# set the parameters\nparams = {\"columntransformer__countvectorizer__min_df\":[1, 2],\n         \"logisticregression__C\":[0.1, 1, 10],\n         \"logisticregression__penalty\":[\"l1\", \"l2\"]}\n\ngrid = GridSearchCV(pipeline, params, cv = 5, scoring = \"accuracy\")\ngrid.fit(X, y)\n\n# convert to a pandas DataFrame\n\nresults = pd.DataFrame(grid.cv_results_)[[\"params\", \"mean_test_score\", \"rank_test_score\"]]\nresults.sort_values(\"rank_test_score\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick17\"></a>\n# Trick 17: RandomizedGridSearch\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import RandomizedSearchCV, cross_val_score\nfrom sklearn.naive_bayes import MultinomialNB\nimport scipy as sp\n\n#------------------------------------------------------------\n# create our instances\nvect = CountVectorizer()\nmodel = MultinomialNB()\n\n#------------------------------------------------------------\n# make pipeline\npipeline = make_pipeline(vect, model)\n\n#------------------------------------------------------------\n# import data\ndf = pd.read_csv(os.path.join(PATH, \"train.csv\"))\nX = df['Name']\ny = df['Survived']\n\n#------------------------------------------------------------\n# set the params to optimize\n\nparams = {}\n\nparams[\"countvectorizer__min_df\"] = [1, 2, 3, 4]\nparams[\"countvectorizer__lowercase\"] = [True, False]\nparams[\"multinomialnb__alpha\"] = sp.stats.uniform(scale = 1)\n\n#------------------------------------------------------------\n# optimize\n\nrand = RandomizedSearchCV(pipeline, params, n_iter = 10, cv = 5, scoring = \"accuracy\", random_state = 1)\nrand.fit(X, y)\n\n#------------------------------------------------------------\n# best score and params\n\nprint(\"Best score achieved with our search is:\")\nrand.best_score_\n\nprint(\"Best params are:\")\nrand.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick16\"></a>\n# Trick 16: Crossvalidate and gridsearch a sklearn pipeline\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\n#------------------------------------------------------------\n# create our instances\nohe = OneHotEncoder()\nvect = CountVectorizer()\nct = make_column_transformer((ohe, [\"Sex\"]), (vect, \"Name\"))\nmodel = LogisticRegression(solver = \"liblinear\", random_state = 1)\n\n#------------------------------------------------------------\n# import data\ndf = pd.read_csv(os.path.join(PATH, \"train.csv\"))\nX = df[[\"Sex\", \"Name\", \"Fare\"]]\ny = df[\"Survived\"]\n\n#------------------------------------------------------------\n# make pipeline\npipeline = make_pipeline(ct, model)\n\n#------------------------------------------------------------\n# cross validate the entire pipeline\nprint(\"Notice the score of our entire pipeline is {}\".format(cross_val_score(pipeline, X, y, cv = 5, scoring = \"accuracy\").mean()))\ncross_val_score(pipeline, X, y, cv = 5, scoring = \"accuracy\").mean()\n\n#------------------------------------------------------------\n# gridsearch the entire pipeline\n\n# set the parameters\nparams = {\"columntransformer__countvectorizer__min_df\":[1, 2],\n         \"logisticregression__C\":[0.1, 1, 10],\n         \"logisticregression__penalty\":[\"l1\", \"l2\"]}\n\ngrid = GridSearchCV(pipeline, params, cv = 5, scoring = \"accuracy\")\ngrid.fit(X, y)\n\n# see the best score\nprint(\"#-------------------------------------------------------------------------\")\nprint(\"Best score of the GridSearchCV is \")\ngrid.best_score_\n\n# see the best params\nprint(\"#-------------------------------------------------------------------------\")\nprint(\"Best parameters of the GridSearchCV are \")\ngrid.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick15\"></a>\n# Trick 15: OneHotEncoder: tips using it\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"markdown","source":"If you use OneHotEncoder, don't \"drop = 'first'\":\n1. Multicollinearity is rarely and issue with sklearn models\n2. drop = 'first' is incompatible with handle_unknown = 'ignore'\n3. May cause you problems if yoy standarize all features or use a regularized model","metadata":{}},{"cell_type":"markdown","source":"<a id = \"trick14\"></a>\n# Trick 14: Handling missing values\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# If you have missing values you can:\n# 1. Drop all rows with missing values\n# 2. Drop all colmns with missing values\n# 3. Impute missing values\n# 4. Use a model that handles missing values\n\n#------------------------------------------------------------\n# import libraries\nfrom sklearn.experimental import enable_hist_gradient_boosting # this import enables \"experimental\" packages and clases in sklearn\nfrom sklearn.ensemble import HistGradientBoostingClassifier # this is an experimental package\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n#------------------------------------------------------------\n# import data\ndf = pd.read_csv(os.path.join(PATH, \"train.csv\"))\nprint(\"We can see that we have missing values\")\n\ndf.isnull().sum()\n\n#------------------------------------------------------------\n# split target and feature\n\nfeatures = [col for col in df.columns if df[col].dtype != \"object\"] # select only numerical columns\nfeatures.remove(\"Survived\")\nfeatures.remove(\"PassengerId\")\n\nX = df[features]\ny = df[\"Survived\"]\n\n#------------------------------------------------------------\n# Train a model that handles missing values\n\nmodel = HistGradientBoostingClassifier()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 175)\n\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nprint(\"Accuracy of our model while having missing values is {}%\".format(round(accuracy_score(y_test, y_pred), 2)*100))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick13\"></a>\n# Trick 13: Examine each step of a Pipeline\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\n\n#------------------------------------------------------------\n# create each instance\nsi = SimpleImputer()\nmodel = LogisticRegression()\n\n#------------------------------------------------------------\n# import data\ndf = pd.read_csv(os.path.join(PATH, \"train.csv\"))\n\n#------------------------------------------------------------\n# select columns to transform\nX = df[[\"Fare\", \"Age\"]].head()\nX[\"Age\"].iloc[0] = np.nan # create a missing values\nX.head()\n\ny = df[[\"Survived\"]].head()\n\n#------------------------------------------------------------\n# use make_pipeline\n\npipeline = make_pipeline(si, model)\n\npipeline.fit(X, y)\n\n#------------------------------------------------------------\n# let's see the statistics of each step\nprint(\"These are the imputed values with the SimpleImputer\")\npipeline.named_steps.simpleimputer.statistics_\n\nprint(\"Display the coefficients of the linear model\")\npipeline.named_steps.logisticregression.coef_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick12\"></a>\n# Trick 12: Difference between Pipeline and make_pipeline\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.compose import make_column_transformer, ColumnTransformer\nfrom sklearn.pipeline import make_pipeline, Pipeline\n\n#------------------------------------------------------------\n# create each instance\nohe = OneHotEncoder()\nsi = SimpleImputer()\nmodel = LogisticRegression()\n\n#------------------------------------------------------------\n# import data\ndf = pd.read_csv(os.path.join(PATH, \"train.csv\"))\n\n#------------------------------------------------------------\n# select columns to transform\nX = df[[\"Fare\", \"Embarked\", \"Sex\", \"Age\"]].head()\nX[\"Age\"].iloc[0] = np.nan # create a missing values\nX.head()\n\ny = df[[\"Survived\"]].head()\n\n#------------------------------------------------------------\n# use make_pipeline\n\ncolumn_transformer = make_column_transformer(\n(ohe, [\"Embarked\", \"Sex\"]),\n(si, [\"Age\"]),\nremainder = \"passthrough\"\n)\n\npipeline = make_pipeline(column_transformer, model)\n\npipeline.fit(X, y)\n\n#------------------------------------------------------------\n# use Pipeline\n# The main difference is that we must name each step\n\ncolumn_transformer = ColumnTransformer(\n[(\"encoder\", ohe, [\"Embarked\", \"Sex\"]), # notice how we must name each step\n(\"imputer\", si, [\"Age\"])],\nremainder = \"passthrough\"\n)\n\npipeline = Pipeline([(\"preprocessing\", column_transformer), (\"model\", model)]) # notice how we must name each step\npipeline.fit(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick11\"></a>\n# Trick 11: KNNImputer\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.impute import KNNImputer\n\n#------------------------------------------------------------\n# create some random data\ndf = pd.read_csv(os.path.join(PATH, \"train.csv\"))\ndf.isnull().sum() # inspect nulls\ndf.head()\n\n#------------------------------------------------------------\n# let's see KNNInputer in action\nknn_inputer = KNNImputer()\n\nX = df[[\"SibSp\", \"Fare\", \"Age\"]]\nnan_index = X[X[\"Age\"].isnull()].index\n\nprint(\"Data with nans\")\nX[X.index.isin(nan_index)].head(10)\n\nprint(\"Transformed data with no nans, and the values are based on the NN.\")\nX_transformed = pd.DataFrame(knn_inputer.fit_transform(X), columns = [\"SibSp\", \"Fare\", \"Age\"], index = X.index)\nX_transformed[X.index.isin(nan_index)].head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick10\"></a>\n# Trick 10: Using random_state to reproduce results\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.model_selection import train_test_split\n\n#------------------------------------------------------------\n# create some random data\n\ndf = generate_sample_data()\ntarget = [np.random.choice([0, 1]) for i in range(len(df))]\ndf[\"target\"] = target\nprint(\"A sneak peak at our df.\")\ndf.head(3)\n\n#------------------------------------------------------------\n# train and target columns\nfeatures = list(df.columns)[:-1] # all except the last one\n\n#------------------------------------------------------------\n# split nr 1 using random_state to reproduce results\nX_train1, X_test1, y_train1, y_test1 = train_test_split(df[features], df[\"target\"], test_size = 0.1, random_state = 175)\n\n#------------------------------------------------------------\n# split nr 2 using random_state to reproduce results\nX_train2, X_test2, y_train2, y_test2 = train_test_split(df[features], df[\"target\"], test_size = 0.1, random_state = 175)\n\n#------------------------------------------------------------\n# split nr 3 using with no random_state\nX_train3, X_test3, y_train3, y_test3 = train_test_split(df[features], df[\"target\"], test_size = 0.1)\n\n#------------------------------------------------------------\n# let's look at our results\nprint(\"First train test split.\")\nX_train1.head()\nprint(\"Second train test split. Equal to the first one.\")\nX_train2.head()\nX_train1.index == X_train2.index\nprint(\"Third train test split. Different than the others.\")\nX_train3.head()\nX_train1.index == X_train3.index\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick9\"></a>\n# Trick 9: Using missing values as a feature: SimpleImputer & add_indicator = True\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import the libraries\nfrom sklearn.impute import SimpleImputer\n\n#------------------------------------------------------------\n# create some train and test data\n\n# create a train df\nd = {\n\"Age\":[10, 20, np.nan, 30, 15, 10, 40, 10, np.nan]\n}\n\nprint(\"Train data\")\ndf_train = pd.DataFrame(d)\ndf_train\n\n#------------------------------------------------------------\n# Sometimes there is a relathionship between missing values and the target\n# We can use this information creating a new features while performing an imputation of a missing values\n\n# normal SimpleImputer()\nimputer = SimpleImputer()\ndf_transformed = imputer.fit_transform(df_train)\ndf_transformed\n\n# SimpleImputer() with the parameter add_indicator\nprint(\"Notice aditional column with an indicator of 1 next to the previously missing values\")\nimputer = SimpleImputer(add_indicator = True)\ndf_transformed = imputer.fit_transform(df_train)\ndf_transformed\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick8\"></a>\n# Trick 8: Using make_pipeline in a ML project\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import the libraries\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\n\n#------------------------------------------------------------\n# create some train and test data\n\n# create a train df\nd = {\n\"feat1\":[10, 20, np.nan, 2],\n\"feat2\":[25, 20, 5, 3],\n\"target\":[\"A\", \"A\", \"B\", \"B\"]\n}\n\nprint(\"Train data\")\ndf_train = pd.DataFrame(d)\ndf_train\n\n# create a test df\nd = {\n\"feat1\":[30, 5, 15],\n\"feat2\":[12, 10, np.nan]\n}\n\nprint(\"Test data\")\ndf_test = pd.DataFrame(d)\ndf_test\n\n#------------------------------------------------------------\n# simple ML project step by step\n\n# create each instance\nsi = SimpleImputer()\nmodel = LogisticRegression()\npipeline = make_pipeline(si, model)\n\n# separate the data between target and features\nfeatures = [\"feat1\", \"feat2\"]\nX_train, y_train = df_train[features], df_train[\"target\"]\nX_test = df_test[features]\n\n#------------------------------------------------------------\n# use pipeline to fit and predict\n\n# First pipeline will use the SimpleImputer to imputer the missing values\n# Then it will train using LogisticRegression\npipeline.fit(X_train, y_train)\n\n# When used pipeline to predict, it will do the same steps as in fit\npipeline.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick7\"></a>\n# Trick 7: Handle new data while using OneHotEncoder\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# create a train df\nd = {\n\"Categorical\":[\"A\", \"A\", \"B\", \"C\"]\n}\n\ndf_train = pd.DataFrame(d)\ndf_train\n\n#------------------------------------------------------------\n# import the libraries\nfrom sklearn.preprocessing import OneHotEncoder\n\n#------------------------------------------------------------\n# transform data during train part\nprint(\"fit_transform using OneHotEncoder.\")\nohe = OneHotEncoder(sparse = False, handle_unknown = \"ignore\") # if you don't put false, you will get a sparse matrix object\nX_train = ohe.fit_transform(df_train[[\"Categorical\"]])\nX_train\n\n#------------------------------------------------------------\n# create a test df\n\nd = {\n\"Categorical\":[\"A\", \"A\", \"B\", \"C\", \"D\"] # new value, D, previously not seen in train\n}\n\ndf_test = pd.DataFrame(d)\ndf_test\n\n\nprint(\"transform using OneHotEncoder. Notice that we have a line with zeros for categorical value of D\")\nX_test = ohe.transform(df_test[[\"Categorical\"]])\nX_test\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick6\"></a>\n# Trick 6: Common ways to encode categorical features: OneHotEncoder, OrdinalEncoder\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# create a df\nd = {\n\"Shape\":[\"square\", \"square\", \"oval\", \"circle\"],\n\"Class\":[\"third\", \"first\", \"second\", \"third\"],\n\"Size\":[\"S\", \"S\", \"L\", \"XL\"]\n}\n\ndf = pd.DataFrame(d)\ndf\n\n#------------------------------------------------------------\n# import the libraries\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\n\n#------------------------------------------------------------\n# transform data using OneHotEncoder\nprint(\"Transform categorical data using OneHotEncoder\")\nohe = OneHotEncoder(sparse = False) # if you don't put false, you will get a sparse matrix object\nshaped_transformed = ohe.fit_transform(df[[\"Shape\"]]) # if you pass as a series, you will need to reshape the data. Notice the double square bracket\nshaped_transformed\n\n#------------------------------------------------------------\n# transform data using OrdinalEncoder\nprint(\"Transform categorical data using OrdinalEncoder\")\nprint(\"When using OrdinalEncoder, your data has to have a order: like first class, second class, third class\")\noe = OrdinalEncoder(categories = [[\"first\", \"second\", \"third\"], # order for the column Class\n                                  [\"S\", \"M\", \"L\", \"XL\"]]) # order for the column Size\ncategorical_ordinal_transformed = oe.fit_transform(df[[\"Class\", \"Size\"]])\ncategorical_ordinal_transformed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick5\"></a>\n# Trick 5: Benefits of using sklearn for preprocessing and not pandas\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"markdown","source":"### Reasons to use sklearn to ML preprocessing and not pandas\n\n1. You can cross-validate the entire workflow\n2. You can grid search model & preprocessing hyperparameters\n3. Avoids adding new columns to the source DataFrame\n4. pandas lacks separate fit/transform steps to prevent data leakage","metadata":{}},{"cell_type":"markdown","source":"<a id = \"trick4\"></a>\n# Trick 4: When to use fit_transform and transform methods\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"markdown","source":"### ***Use \"fit_transform\" on training data, but \"transform\" (only) on testing/new data.***\n\nApplies the same transformations to both sets of data, which creates consistent columns and prevents data leakage!","metadata":{}},{"cell_type":"markdown","source":"<a id = \"trick3\"></a>\n# Trick 3: Difference between fit and transform method\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"markdown","source":"### Q: What is the difference between the \"fit\" and \"transform\" methods?\n\n#### ***\"fit\"***: transformer learns something about the data\n\n####***\"transform\":*** it uses what it learned to do the data transformation\n\n------\n\n***CountVectorizer: ***\n\nfit: learns the vocabulary\n\ntransform: creates a document term matrix using the vocabulary\n\n------\n\n***SimpleImputer: ***\n\nfit: learns the value to impute\n\ntransform: fills the missing value with the value to impute\n\n------\n\n***StandartScaler: ***\n\nfit: learns the mean and scale of each feature\n\ntransform: standarizes the features using the mean and scale\n\n------\n\n***HashingVectorizer: ***\n\nfit: if not used, then it's known as \"stateless\" transformer\n\ntransform: creates a document term matrix using a hash of the token\n","metadata":{}},{"cell_type":"markdown","source":"<a id = \"trick2\"></a>\n# Trick 2: Seven ways to select columns using ColumnsTransformer\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import data\ndf = pd.read_csv(os.path.join(PATH, \"train.csv\"))\n\n#------------------------------------------------------------\n# select columns to transform\nX = df[[\"Fare\", \"Embarked\", \"Sex\", \"Age\"]].head()\nX[\"Age\"].iloc[0] = np.nan # create a missing values\nX\n\n#------------------------------------------------------------\n# import the libraries\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import make_column_selector\nfrom sklearn.compose import make_column_transformer\n\n#------------------------------------------------------------\n# instanciate the classes\nohe = OneHotEncoder()\n\n#------------------------------------------------------------\n# create the pipeline and select the columns by name\n\nprint(\"Select the column by name\")\n\nct = make_column_transformer(\n(ohe, [\"Embarked\", \"Sex\"])# if you have null values it will give and error. You must first fill those values before doing an ohe\n)\n\n# fit_transform the columns\nX_transformed = ct.fit_transform(X) \nX_transformed\n\n#------------------------------------------------------------\n# create the pipeline and select the columns by position\n\nprint(\"Select the column by position\")\n\nct = make_column_transformer(\n(ohe, [1, 2])# if you have null values it will give and error. You must first fill those values before doing an ohe\n)\n\n# fit_transform the columns\nX_transformed = ct.fit_transform(X) \nX_transformed\n\n#------------------------------------------------------------\n# create the pipeline and select the columns using slice\n\nprint(\"Select the column using slice\")\n\nct = make_column_transformer(\n(ohe, slice(1, 3))# if you have null values it will give and error. You must first fill those values before doing an ohe\n)\n\n# fit_transform the columns\nX_transformed = ct.fit_transform(X) \nX_transformed\n\n#------------------------------------------------------------\n# create the pipeline and select the columns using a boolean mask\n\nprint(\"Select the column using a boolean mask\")\n\nct = make_column_transformer(\n(ohe, [False, True, True, False])# if you have null values it will give and error. You must first fill those values before doing an ohe\n)\n\n# fit_transform the columns\nX_transformed = ct.fit_transform(X) \nX_transformed\n\n#------------------------------------------------------------\n# create the pipeline and select the columns using make_column_selector and regex\n\nprint(\"Select the column using using make_column_selector and regex. New in pandas 0.22\")\n\nct = make_column_transformer(\n(ohe, make_column_selector(pattern = \"E|S\"))# if you have null values it will give and error. You must first fill those values before doing an ohe\n)\n\n# fit_transform the columns\nX_transformed = ct.fit_transform(X) \nX_transformed\n\n#------------------------------------------------------------\n# create the pipeline and select the columns using make_column_selector and dtype_include\n\nprint(\"Select the column using using make_column_selector and dtype_include. New in pandas 0.22\")\n\nct = make_column_transformer(\n(ohe, make_column_selector(dtype_include = object))# if you have null values it will give and error. You must first fill those values before doing an ohe\n)\n\n# fit_transform the columns\nX_transformed = ct.fit_transform(X) \nX_transformed\n\n#------------------------------------------------------------\n# create the pipeline and select the columns using make_column_selector and dtype_exclude\n\nprint(\"Select the column using using make_column_selector and dtype_exclude. New in pandas 0.22\")\n\nct = make_column_transformer(\n(ohe, make_column_selector(dtype_exclude = \"number\"))# if you have null values it will give and error. You must first fill those values before doing an ohe\n)\n\n# fit_transform the columns\nX_transformed = ct.fit_transform(X) \nX_transformed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trick1\"></a>\n# Trick 1: Using ColumnTransformer to manipulate different columns\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------\n# import data\ndf = pd.read_csv(os.path.join(PATH, \"train.csv\"))\ndf.head()\n\n#------------------------------------------------------------\n# select columns to transform\nX = df[[\"Fare\", \"Embarked\", \"Sex\", \"Age\"]].head()\nX[\"Age\"].iloc[0] = np.nan # create a missing values\nX\n\n#------------------------------------------------------------\n# import the libraries\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import make_column_transformer\n\n#------------------------------------------------------------\n# instanciate the classes\nohe = OneHotEncoder()\nimp = SimpleImputer()\n\n#------------------------------------------------------------\n# create the pipeline\nct = make_column_transformer(\n(ohe, [\"Embarked\", \"Sex\"]), # if you have null values it will give and error. You must first fill those values before doing an ohe\n(imp, [\"Age\"]), \nremainder = \"passthrough\" #this means that the column Fare will appear the last one.\n)\n\n#------------------------------------------------------------\n# fit_transform the columns\nX_transformed = ct.fit_transform(X) \nX_transformed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# That's all, thanks a lot. I hope you learned a lot of sklearn.\n[Go back to the Table of Contents](#table_of_contents)","metadata":{}}]}