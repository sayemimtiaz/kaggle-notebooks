{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics as sm\nfrom sklearn import metrics\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **PREPROCESSING**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/environmental-sensor-data-132k/iot_telemetry_data.csv')\nprint(data.isnull().sum())\ndevices = data.device.unique()\ndata = data.replace(devices, ['device 1', 'device 2', 'device 3'])\ndata['time'] = pd.to_datetime(data['ts'], unit='s')\ndata.drop('ts', inplace=True, axis=1)\ndata['light'] = data['light'].astype('int')\ndata['motion'] = data['motion'].astype('int')\nprint(data.corr())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dev_1 = data[data.device == 'device 1']\ndev_2 = data[data.device == 'device 2']\ndev_3 = data[data.device == 'device 3']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## FUNCTIONS"},{"metadata":{"trusted":true},"cell_type":"code","source":"def FindLocalMin(numbers):\n    minima = []\n    length = len(numbers)\n    for n in range(1, length - 1):\n        if numbers[n] <= numbers[n - 1] and numbers[n] <= numbers[n + 1]:\n            minima.append(numbers[n])\n        if numbers[length - 1] <= numbers[length - 2]:\n            minima.append(numbers[length - 1])\n    return min(minima)\n\ndef data_clean(x, y, x_label, y_label, graphs):\n    MSE = []\n    rn = range(1000, 3100, 100)\n    for s in rn:\n        EMA = y.ewm(span=s, adjust=False).mean()\n        weight = abs(1 / (.1 + y - EMA))\n        # 0.1 is a small constant offset used to avoid division by zero\n        mse = round(sm.mean_squared_error(y, EMA, sample_weight=weight), 2)\n        MSE.append(mse)\n\n    loc_min = FindLocalMin(MSE)\n    idx_first_loc_min = MSE.index(loc_min)\n    best_ema = y.ewm(span=rn[idx_first_loc_min], adjust=False).mean()\n\n    if graphs == True:\n        plt.figure(figsize=(25, 10))\n        plt.subplot(1, 2, 1)\n        plt.plot(x, y, color='lime', label='Data')\n        plt.plot(x, best_ema, color='blue', linewidth=2, label='EMA')\n        plt.xlabel(x_label, fontsize=18)\n        plt.ylabel(y_label, fontsize=18)\n        plt.legend(loc='upper right', prop={'size': 20})\n        plt.tight_layout()\n\n        plt.subplot(1, 2, 2)\n        plt.plot(rn, MSE, label='MSE')\n        plt.plot(rn[idx_first_loc_min], loc_min, marker='o', color='red', label='ideal n of data')\n        plt.xlabel('number of data', fontsize=18)\n        plt.ylabel('MSE', fontsize=18)\n        plt.legend(loc='upper left', prop={'size': 20})\n\n    plt.subplots_adjust(wspace=.1)\n    plt.show()\n\n    return best_ema\n\ndef conf_mat(test, pred):\n    print('Accuracy=', round(metrics.accuracy_score(test, pred), 2))\n    c_m = metrics.confusion_matrix(test, pred)\n    plt.figure(figsize=(10, 5))\n    sns.heatmap(c_m/np.sum(c_m), annot=True, cmap='Blues_r', fmt='.2%',\n                xticklabels=['absence', 'presence'],\n                yticklabels=['absence', 'presence'])\n    plt.xlabel('Predicted labels', fontsize=18)\n    plt.ylabel('True labels', fontsize=18)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data_clean function can be divided into three steps:\n1. calculates EMA (exponential moving average) considering different amounts of data; it has the characteristic of taking into account more the recent values than previous ones. Furthermore, the original data contains outliers that differ significantly from the others, so a weight is considered in such a way that the farther the value is from the moving average, the less it is considered (0.1 is a small constant offset used to avoid division by zero);\n\n2. finds the ideal amount of EMA data via the MSE method;\n\n3.  constructs two adjacent graphs representing the original data superimposed on EMA (left) and the MSE values for the different amounts of data and the value in red for the ideal one (right)."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dev_1.corr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I apply the data_clean function on the 'humidity', 'smoke' and 'temperature' parameters no taking into account the 'co' and 'gpl' parameters as, together with the 'smoke' parameter, they have a covariance value close to 1 as proof which are highly correlated."},{"metadata":{"trusted":true},"cell_type":"code","source":"# dc_1_co = data_clean(dev_1.time, dev_1.co, x_label='time', y_label='co [ppm]', graphs=True)\ndc_1_hum = data_clean(dev_1.time, dev_1.humidity, x_label='time', y_label='humidity [%]', graphs=True)\n# dc_1_lpg = data_clean(dev_1.time, dev_1.lpg, x_label='time', y_label='LPG [ppm]', graphs=True)\ndc_1_smoke = data_clean(dev_1.time, dev_1.smoke, x_label='time', y_label='smoke [ppm]', graphs=True)\ndc_1_temp = data_clean(dev_1.time, dev_1.temp, x_label='time', y_label='temp [°F]', graphs=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame({'time': dev_1.time,\n                   # 'co': dc_1_co,\n                   'humidity': dc_1_hum,\n                   # 'lpg': dc_1_lpg,\n                   'smoke': dc_1_smoke,\n                   'temp': dc_1_temp,\n                   'light': dev_1.light,\n                   'motion': dev_1.motion})\n\ndf['pres'] = np.zeros(len(df), dtype=int)\ndf = df.sample(frac=.05, random_state=154)\ndf = df.sort_values(by='time', ascending=True)\ndf.index = range(0, len(df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I consider an increase in parameters and the activation of the light and motion sensor as the fact that there is at least one person near the device."},{"metadata":{"trusted":true},"cell_type":"code","source":"name_col = df.columns[1:]\n\nfor i in name_col[:-1]:\n    if df[i].dtype == 'float':\n        for j in df.index:\n            if j == 0 or df.pres[j] == 1:\n                pass\n            elif df[i][j - 1] < df[i][j] and df[i][j] - df[i][j - 1] > .0008:\n                # .0008 is a random number but it must be replaced with other values such as, for example, the error of each sensor (error device[i]), in order not to consider an increase in the value below a certain threshold.\n                df.loc[j, 'pres'] = 1\n    else:\n        for j in df.index:\n            if df.pres[j] == 1:\n                pass\n            elif df[i][j] == 1:\n                df.loc[j, 'pres'] = 1\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RANDOM FOREST"},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = df[name_col[:-1]], df[name_col[-1]]\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=123)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I look for the optimal number of estimators that allows me to lower the OOB errors as much as possible."},{"metadata":{"trusted":true},"cell_type":"code","source":"oob_error = []\nrn_est = list(range(30, 205, 5))\nfor i in rn_est:\n    clf = RandomForestClassifier(max_features='sqrt', oob_score=True, n_estimators=i,\n                                 random_state=123)\n    clf.fit(X_train, y_train)\n    oob_err = 1 - clf.oob_score_\n    oob_error.append(oob_err)\n\nbest_n_est = rn_est[oob_error.index(min(oob_error))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nplt.plot(rn_est, oob_error, label='OOB error rate')\nplt.plot(best_n_est, min(oob_error), marker='o', color='red', label='ideal n estimator')\nplt.xlabel('n estimators', fontsize=18)\nplt.ylabel('OOB error rate', fontsize=18)\nplt.legend(loc='upper right', prop={'size': 10})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RF_classifier = RandomForestClassifier(max_features='sqrt', oob_score=True,\n                                       n_estimators=best_n_est, random_state=123)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RF_classifier.fit(X_train, y_train)\ny_RF_pred = RF_classifier.predict(X_test)\nconf_mat(y_test, y_RF_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance = RF_classifier.feature_importances_\nfeature_names = name_col[:-1]\nfeature_importance = 100 * (feature_importance / max(feature_importance))\n\nindex_sorted = np.flipud(np.argsort(feature_importance))\npos = np.arange(index_sorted.shape[0]) + 0.5\n\nplt.figure(figsize=(10, 5))\nplt.bar(pos, feature_importance[index_sorted], align='center')\nplt.xticks(pos, feature_names[index_sorted], fontsize=18)\nplt.ylabel('Relative importance', fontsize=18)\nplt.title('Feature Importance', fontsize=23)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# APPLICATION OF THE CLASSIFIER ON THE SECOND DATASET"},{"metadata":{"trusted":true},"cell_type":"code","source":"# dc_2_co = data_clean(dev_2.time, dev_2.co, x_label='time', y_label='co [ppm]', graphs=False)\ndc_2_hum = data_clean(dev_2.time, dev_2.humidity, x_label='time', y_label='humidity [%]', graphs=False)\n# dc_2_lpg = data_clean(dev_2.time, dev_2.lpg, x_label='time', y_label='LPG [ppm]', graphs=False)\ndc_2_smoke = data_clean(dev_2.time, dev_2.smoke, x_label='time', y_label='smoke [ppm]', graphs=False)\ndc_2_temp = data_clean(dev_2.time, dev_2.temp, x_label='time', y_label='temp [°F]', graphs=False)\n\ndf_2 = pd.DataFrame({'time': dev_2.time,\n                     # 'co': dc_2_co,\n                     'humidity': dc_2_hum,\n                     # 'lpg': dc_2_lpg,\n                     'smoke': dc_2_smoke,\n                     'temp': dc_2_temp,\n                     'light': dev_2.light,\n                     'motion': dev_2.motion})\n\ndf_2['pres'] = np.zeros(len(df_2), dtype=int)\ndf_2 = df_2.sample(frac=0.5, random_state=154)\ndf_2 = df_2.sort_values(by='time', ascending=True)\ndf_2.index = range(0, len(df_2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_RF_pred_2 = RF_classifier.predict(df_2[name_col[:-1]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nplt.plot(df_2.time, y_RF_pred_2, linewidth=2)\nplt.xlabel('time', fontsize=18)\nplt.ylabel('presence', fontsize=18)\nplt.yticks([0, 1], fontsize=15)\nplt.ylim(0, 1.1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# APPLICATION OF THE CLASSIFIER ON THE THIRD DATASET"},{"metadata":{"trusted":true},"cell_type":"code","source":"# dc_3_co = data_clean(dev_3.time, dev_3.co, x_label='time', y_label='co [ppm]', graphs=False)\ndc_3_hum = data_clean(dev_3.time, dev_3.humidity, x_label='time', y_label='humidity [%]', graphs=False)\n# dc_3_lpg = data_clean(dev_3.time, dev_3.lpg, x_label='time', y_label='LPG [ppm]', graphs=False)\ndc_3_smoke = data_clean(dev_3.time, dev_3.smoke, x_label='time', y_label='smoke [ppm]', graphs=False)\ndc_3_temp = data_clean(dev_3.time, dev_3.temp, x_label='time', y_label='temp [°F]', graphs=False)\n\ndf_3 = pd.DataFrame({'time': dev_3.time,\n                     # 'co': dc_3_co,\n                     'humidity': dc_3_hum,\n                     # 'lpg': dc_3_lpg,\n                     'smoke': dc_3_smoke,\n                     'temp': dc_3_temp,\n                     'light': dev_3.light,\n                     'motion': dev_3.motion})\n\ndf_3['pres'] = np.zeros(len(df_3), dtype=int)\ndf_3 = df_3.sample(frac=0.5, random_state=154)\ndf_3 = df_3.sort_values(by='time', ascending=True)\ndf_3.index = range(0, len(df_3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_RF_pred_3 = RF_classifier.predict(df_3[name_col[:-1]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nplt.plot(df_3.time, y_RF_pred_3, linewidth=2)\nplt.xlabel('time', fontsize=18)\nplt.ylabel('presence', fontsize=18)\nplt.yticks([0, 1], fontsize=15)\nplt.ylim(0, 1.1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another way to implement this program would be to consider the number of increasing parameters such as, if they are more than a certain percentage of the number of parameters, the device considers that fact to be someone's presence. Each parameter must be assigned a probability such as, if the light or motion sensor is activated, I'm sure there is someone near the device, so I can assign them a P = 1. For the CO sensor a different one can be assigned because increasing that parameter may mean that the heating system is faulty. This approach can be applied for the other sensors:\n* smoke and temperature sensors: it can mean that something is taking fire;\n* LPG sensor: if the device is mounted on an LPG car, it may mean that the LPG cylinder is leaking gas."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}