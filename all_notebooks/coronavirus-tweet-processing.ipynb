{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Natural Language Processing Project: Coronavirus Tweets"},{"metadata":{},"cell_type":"markdown","source":"Coded by Luna McBride, following ideas in the Kaggle NLP course"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport spacy # NLP\nfrom sklearn.svm import LinearSVC\nimport re # regular expressions\nimport html # HTML content, like &amp;\nfrom spacy.lang.en.stop_words import STOP_WORDS # stopwords\nfrom sklearn.model_selection import train_test_split # training and testing a model\nfrom spacy.util import minibatch # batches for training\nimport random # randomizing for training\n\nnlp = spacy.load('en_core_web_lg') #Load spacy, up here so I do not have to load it constantly\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read in the data files"},{"metadata":{},"cell_type":"markdown","source":"Note: just read_csv(\"file\") causes error here. Source for fix: https://stackoverflow.com/questions/18171739/unicodedecodeerror-when-reading-csv-file-in-pandas-with-python"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/covid-19-nlp-text-classification/Corona_NLP_train.csv\", encoding = \"ISO-8859-1\") #Load the training set\ntrain.head() #Take a peek at the training set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/covid-19-nlp-text-classification/Corona_NLP_test.csv\", encoding = \"ISO-8859-1\") #Load the testing set\ntest.head() #Take a peek at the testing set","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Check for Nulls"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for nulls in all columns in Train\nprint(\"Train CSV: \\n\")\nprint(train[\"UserName\"].isnull().any())\nprint(train[\"ScreenName\"].isnull().any())\nprint(train[\"Location\"].isnull().any())\nprint(train[\"TweetAt\"].isnull().any())\nprint(train[\"OriginalTweet\"].isnull().any())\nprint(train[\"Sentiment\"].isnull().any())\n    \n#Location has a null\ntrain[\"Location\"] = train[\"Location\"].fillna(\"Unknown\") #Fill the null values with \"Unknown\"\nprint(\"Location: \", train[\"Location\"].isnull().any(), \"\\n\") #Print the now fixed location to make sure it is truly fixed\n\n# Check for nulls in all columns in Test\nprint(\"Test CSV: \\n\")\nprint(test[\"UserName\"].isnull().any())\nprint(test[\"ScreenName\"].isnull().any())\nprint(test[\"Location\"].isnull().any())\nprint(test[\"TweetAt\"].isnull().any())\nprint(test[\"OriginalTweet\"].isnull().any())\nprint(test[\"Sentiment\"].isnull().any())\n\n#Location has a null\ntest[\"Location\"] = test[\"Location\"].fillna(\"Unknown\") #Fill the null values with \"Unknown\"\nprint(\"Location: \", test[\"Location\"].isnull().any(), \"\\n\") #Print the now fixed location to make sure it is truly fixed","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All nulls removed"},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Check OriginalTweet for empty strings"},{"metadata":{"trusted":true},"cell_type":"code","source":"empty = train[\"OriginalTweet\"].apply(lambda x: print(\"One\") if not x else x) #Prints \"One\" if there are any empty strings\nempty2 = test[\"OriginalTweet\"].apply(lambda x: print(\"One\") if not x else x) #Prints \"One\" if there are any empty strings","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No print statements. No empty strings here."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Tweet Processing"},{"metadata":{},"cell_type":"markdown","source":"Sources for Tweet Processing: https://towardsdatascience.com/basic-tweet-preprocessing-in-python-efd8360d529e , https://medium.com/analytics-vidhya/working-with-twitter-data-b0aa5419532 , https://www.analyticsvidhya.com/blog/2019/08/how-to-remove-stopwords-text-normalization-nltk-spacy-gensim-python/ ,  https://stackoverflow.com/questions/2087370/decode-html-entities-in-python-string , plus some general regex searches."},{"metadata":{"trusted":true},"cell_type":"code","source":"punctuations = \"\"\"!()-![]{};:+'\"\\,<>./?@#$%^&*_~Ã‚\"\"\" #List of punctuations to remove, including a weird A that will not process out any other way\n\n#CleanTweets: parces the tweets and removes punctuation, stop words, digits, and links.\n#Input: the list of tweets that need parsing\n#Output: the parsed tweets\ndef cleanTweets(tweetParse):\n    for i in range(0,len(tweetParse)):\n        tweet = tweetParse[i] #Putting the tweet into a variable so that it is not calling tweetParse[i] over and over\n        tweet = html.unescape(tweet) #Removes leftover HTML elements, such as &amp;\n        tweet = re.sub(r\"@\\w+\", ' ', tweet) #Completely removes @'s, as other peoples' usernames mean nothing\n        tweet = re.sub(r'https\\S+', ' ', tweet) #Removes links, as links provide no data in tweet analysis in themselves\n        tweet = re.sub(r\"\\d+\\S+\", ' ', tweet) #Removes numbers, as well as cases like the \"th\" in \"14th\"\n        tweet = ''.join([punc for punc in tweet if not punc in punctuations]) #Removes the punctuation defined above\n        tweet = tweet.lower() #Turning the tweets lowercase real quick for later use\n    \n        tweetWord = tweet.split() #Splits the tweet into individual words\n        tweetParse[i] = ''.join([word + \" \" for word in tweetWord if nlp.vocab[word].is_stop == False]) #Checks if the words are stop words\n        \n    return tweetParse #Returns the parsed tweets\n\n#Jeez, this whole NLP project (plus the kaggle course) has thrown a lot of use of making a list via _ for _ if _\n\ntrainCopy = train[\"OriginalTweet\"].copy() #Copies the train tweets, using a copy to ensure I do not screw it up\ntestCopy = test[\"OriginalTweet\"].copy() #Copies the test tweets, using a copy to ensure I do not screw it up\n\ntrainTweets = cleanTweets(trainCopy) #Calls the cleanTweets method to clean the train tweets\ntestTweets = cleanTweets(testCopy) #Calls the cleanTweets method to clean the test tweets\n\ntrain[\"CleanTweet\"] = trainTweets #Puts the clean train tweets into a new column\ntest[\"CleanTweet\"] = testTweets #Puts the clean test tweets into a new column\ntrain.head() #Take a peek at the new addition to the data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Important note***: there are various rows that become blank strings after processing. After a bit of exploration, these blank strings have different sentiments (ie. the train set's 186 and 13777, which are neutral and negative respectively). Below is the list of the indecies of empty strings after preprocessing. These will be removed further below. It is important to note this, as empty tweets give no information and could skew the model with different sentiments."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(trainTweets.loc[trainTweets == \"\"], \"\\n \\n\") #Print the row numbers with empty clean train tweets\nprint(testTweets.loc[testTweets == \"\"]) #Print the row number with empty clean test tweets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RemoveBlanks: removes tweets that became blank after processing\n#Input: the dataframe to look at\n#Output: none\ndef removeBlanks(df):\n    df[\"CleanTweet\"] = df[\"CleanTweet\"].apply(lambda x: np.nan if not x else x) #Changes blank strings to nan\n    df.dropna(subset = [\"CleanTweet\"], inplace = True) #Drops the rows newly assigned to nan\n    df.reset_index(drop=True, inplace=True) #Reset indecies so we can still loop through without error\n\nremoveBlanks(train) #Removes the blanks from the train set\nremoveBlanks(test) #Removes the blanks from the test set\ntrain.head() #Opens up the train to take a peek, as the first one was blank in the training set","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And to check if that caught everything"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train[\"CleanTweet\"].loc[train[\"CleanTweet\"] == \"\"], \"\\n \\n\") #Print the row number that still has empty clean train tweets\nprint(test[\"CleanTweet\"].loc[test[\"CleanTweet\"] == \"\"]) #Print the row number that still has empty clean test tweets","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Adding Numeric Sentiments and Remove Extremes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sentiments: A function to turn the word sentiments into numerical values for the Train set, 0, 1, 2, 0 being negative, 2 being positive.\n# This function also makes incorrect values in labels -1, as nothing else is -1\ndef sentiments(x):\n    if x == \"Negative\":\n        return 0\n    if x == \"Neutral\":\n        return 1\n    return 2\n\ndef removeExtremes(x):\n    if x == \"Extremely Negative\":\n        return \"Negative\"\n    if x == \"Extremely Positive\":\n        return \"Positive\"\n    return x\n\n#Extremes were causing problems in the model, as it is hard to exemplify extreme to a computer\n#These change the extremes to just their counterparts so it is not a necessary hurdle\ntrain[\"Sentiment\"] = train[\"Sentiment\"].apply(removeExtremes)\ntest[\"Sentiment\"] = test[\"Sentiment\"].apply(removeExtremes)\n\ntrain[\"NumSentiment\"] = train[\"Sentiment\"].apply(sentiments) #Add a row into train for numerical sentiment\ntest[\"NumSentiment\"] = test[\"Sentiment\"].apply(sentiments) #Add a row into test for numerical sentiment\ntest.head() #Display the test and see if it has numerical sentiment","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Begin the pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pipe for processing, copied from the kaggle course\ntextcat = nlp.create_pipe(\n              \"textcat\",\n              config={\n                \"exclusive_classes\": True,\n                \"architecture\": \"bow\"})\ntry:\n    nlp.add_pipe(textcat) #Add the pipe\n    print(\"Pipeline loaded\") #Print for if the pipeline is loaded\nexcept:\n    nlp.remove_pipe(\"textcat\") #delete the pipe to reload\n    nlp.add_pipe(textcat) #Add the pipe\n    print(\"Pipeline now loaded\") #Print for if the pipeline is loaded\n\n#Adding labels for the tweets\ntextcat.add_label(\"Negative\")\ntextcat.add_label(\"Neutral\")\ntextcat.add_label(\"Positive\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Training via the training set"},{"metadata":{},"cell_type":"markdown","source":"Based on the code from the kaggle course Text Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"#TrainData: a function to train the model to the train data. Modeled after the one in the kaggle class\n#Input: the model, the training data, and an optimizer\n#Output: losses\ndef trainData(model, data, optimize):\n    losses = {} #A set for the losses data\n    random.seed() #Randomizing the seed of shuffling data\n    random.shuffle(data) #Shuffles the data\n    \n    batches = minibatch(data, size=10) #Creates batches of texts\n    \n    #For each batch of texts\n    for batch in batches:\n        text, label = zip(*batch) #Unzip the labels and text\n        model.update(text, label, sgd = optimize, losses = losses) #Update the model with the new data\n    \n    return losses #Return the losses","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I had initially been trying to count sentiments like Extremely Negative and Negative separately, but that gave a loss of 32 with batches of 10. It also took over an hour. To compare, this took a few minutes to finish with a loss of about 1 at batches of 30. I guess it is hard to quantify \"Extremely\".\n\nAlso, I initially trained the model here and then predicted later, but I decided to have all the heavy lifting in one place."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Prediction on Test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"#PredictTexts: predicts the sentiment of the tweet, from negative to positive\n#Input: the model and the tweets\n#Output: predictions\ndef predictTexts(model, texts):\n    predicText = [model.tokenizer(text) for text in texts] #Tokenizes the test tweets\n    model.get_pipe(\"textcat\") #Gets the trained textcat pipe\n    scores,_ = textcat.predict(predicText) #Gets the scores from the predictions, ignoring other outputs\n    classes = scores.argmax(axis = 1) #Get the highest ranked prediction score for each tweet\n    return classes #Returns the predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CheckAccuracy: checks the accuracy compared to the predictions.\n#Input: the NLP model, the tweets to predict, their pre-determined labels\n#Output: the accuracy of the predictions\ndef checkAccuracy(model, texts, labels):\n    predicted = predictTexts(model, texts) #Creates predictions on the tweets\n    trueVal = [2*int(label[\"cats\"][\"Positive\"]) + int(label[\"cats\"][\"Neutral\"]) for label in labels] #Gets the actual value of the tweets provided\n    correct = 0 #A holder variable for how many predictions are correct\n    total = len(predicted) #The total number of analyzed tweets\n    \n    #For loop, comparing predictions to their values\n    for i in range(0,total):\n        if trueVal[i] == predicted[i]: #If the prediction is correct\n            correct+=1  #Add a point to the correct pile\n    \n    accuracy = correct/total #Get the accuracy of the number correct over the number total\n    return accuracy #Returns the accuracy of the model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Adding labels for the categories"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = [] #Labels for the cleaned training tweet\nlabelsT = [] #The labels for the cleaned test tweet\n\n#For loop to add true and false to classifications for the train set\nfor i in range(0,len(train)): \n    label = train[\"Sentiment\"][i] #Get the sentiment\n    \n    #Categorize true false based on the labels\n    if label == \"Negative\":\n        cats = {\"Negative\" : True, \"Neutral\" : False, \"Positive\" : False}\n    elif label == \"Neutral\":\n        cats = {\"Negative\" : False, \"Neutral\" : True, \"Positive\" : False}\n    else:\n        cats = {\"Negative\" : False, \"Neutral\" : False, \"Positive\" : True}\n    labels.append({'cats' : cats})\n\n#For loop to add true and false to classifications for the test set\nfor i in range(0,len(test)):\n    label = test[\"Sentiment\"][i] #Get the sentiment\n    \n    #Categorize true false based on the labels\n    if label == \"Negative\":\n        cats = {\"Negative\" : True, \"Neutral\" : False, \"Positive\" : False}\n    elif label == \"Neutral\":\n        cats = {\"Negative\" : False, \"Neutral\" : True, \"Positive\" : False}\n    else:\n        cats = {\"Negative\" : False, \"Neutral\" : False, \"Positive\" : True}\n    labelsT.append({'cats' : cats})\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Run the training and prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"texts = train[\"CleanTweet\"].copy() #Get the clean tweets\ntokenTexts = [nlp.tokenizer(tweet) for tweet in texts] #Tokenize the training tweets\noptimize = nlp.begin_training() #The optimizer, using spacy\ndata = list(zip(tokenTexts, labels)) #Zipping the labels and texts together\n\nlosses = trainData(nlp, data, optimize) #Train the model\naccuracy = checkAccuracy(nlp, test[\"CleanTweet\"].copy(), labelsT) #Gets the accuracy of predictions for the trained model\nprint(\"Losses: \", losses[\"textcat\"], \"Accuracy: \", accuracy) #Prints the loss when training and the accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The initial accuracy was 19%, however, it was when I was giving labels before I deleted the blank tweets. Just a note about the process."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"The highest accuracy I achieved was 76.2% with a loss of 17.025, that being with a batch size of 10. This is honestly a little lower than I was hoping. I used Spacy for this first NLP project in order following the Kaggle course, but I believe it was a limiting factor in this case. When trying to look up methods and other items, I would notice NLTK and Keras pop up way more commonly even with \"Spacy\" in the search term. This makes me think these are more common and possibly more powerful, but I think I will do a little bit more research before trying another project with one of those. Overall, however, I do think it was a good learning experience."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}