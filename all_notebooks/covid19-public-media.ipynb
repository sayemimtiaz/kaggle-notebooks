{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COVID-19 Open Research Dataset Challenge (CORD-19)\n# What has been published about information sharing and inter-sectoral collaboration?\n\nhttps://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks?taskId=583\n\n\n# Results and discussion\n\nThe following results were manually selected to highlight research and news articles that are relevant to information-sharing and collaboration:\n- [SOCRATES: An online tool leveraging a social contact data sharing initiative to assess mitigation strategies for COVID-19](https://doi.org/10.1101/2020.03.03.20030627)\n- [Clinical AI Leader Jvion Launches COVID Community Vulnerability Map.](https://www.globalbankingandfinance.com/category/news/clinical-ai-leader-jvion-launches-covid-community-vulnerability-map/)\n- [Sharing public health data and information across borders: lessons from Southeast](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6162912/)\n- [U.S. county-level characteristics to inform equitable COVID-19 response](https://doi.org/10.1101/2020.04.08.20058248)\n- [Age-dependent risks of Incidence and Mortality of COVID-19 in Hubei Province and Other Parts of China](https://doi.org/10.1101/2020.02.25.20027672)\n- [Contributing to communicable diseases intelligence management in Canada: CACMID meeting, March 2007, Halifax, Nova Scotia](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2533573/)\n- [ClinMicroNet â€” Sharing experiences and building knowledge virtually](https://doi.org/10.1016/s0196-4399%2803%2980038-7)\n- [TAXONOMY, CLASSIFICATION AND NOMENCLATURE OF VIRUSES](https://doi.org/10.1006/rwvi.1999.0277)\n- [DATA, DATA, AND MORE DATA](https://doi.org/10.1006/rwvi.1999.0277)\n\n\n# Highlights and suggestions for improvements\n\nThe advantages of this work includes:\n- Use of news content for greater information coverage. For example, the \"[Clinical AI Leader Jvion Launches COVID Community Vulnerability Map.](https://www.globalbankingandfinance.com/category/news/clinical-ai-leader-jvion-launches-covid-community-vulnerability-map/)\" resource was obtained from the news dataset. Another example includes \"[UPDATE 1-G20 finance leaders pledge 'appropriate' fiscal, monetary actions in coronavirus response.](https://www.cnbc.com/2020/03/06/reuters-america-update-1-g20-finance-leaders-pledge-appropriate-fiscal-monetary-actions-in-coronavirus-response.html)\".\n- Accurate results\n- Data table fromatting with clickable URLs\n- Simple data pipeline to understand and re-use: Users can easily enter a query and find relevant documents by simply calling\n> q='equity considerations and problems of inequity'\n<br>\n> search(q, 'inequity')\n- Results are summarized as a WordCloud image\n\nSuggestions for improvements:\n- To speed up the process and minimize computation, only the title and part of the abstract / news content are used in the analysis. Analyzing full-text may reveal additional information.\n- Results can be fed into a document summarization algorithm so results can be more focussed even further\n- Further improvements in how data is presented with a more advanced user interface can be beneficial\n\n\n# Methodology\nData sources:\n- [CORD-19 Research Papers](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge)\n- [COVID-19 Public Media Dataset](https://www.kaggle.com/jannalipenkova/covid19-public-media-dataset)\n\nTo build the corpus, the title and the content from both data soucres are combined (only the first 3,000 characters are used). The task questions were used as search queries for relevant documents in the corpus. For each document in the corpus and search query pair, the word vectors are retrieved from the GoogleNews word embeddings and the cosine distance is calculated. The top documents that match each query are shown in this notebook together with a summary of results presented as a WordCloud image. Additional results are available as CSV files.\n\nI experimented with using the Word Mover's Distance measure as an alternative similarity measure to cosine distance however, it proved to be a lot slower and it was difficult to objectively decide if the results were better than cosine distance. I also tried using LDA (latent dirichlet allocation) to identify the major topics in the results but I decided to use WordClouds instead because it is more visually appealing and more keywords are highlighted.\n\n\nI hope that the findings from this notebook can help inform researchers and curious minds about the ongoing COVID-19 research. I want to thank the researchers, competition organizers, Kaggle and the dataset providers for making this work possible."},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport json\nimport requests\nimport io\nimport gc\nimport re\n\nimport logging\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n\n# from tqdm import tqdm\nfrom tqdm.notebook import tqdm\n\n\n# pandas settings\npd.set_option('display.max_colwidth', -1)\npd.set_option('display.max_rows', 1000)\nplt.rcParams['figure.figsize'] = [12, 8]\n\n\nfrom nltk import download\ndownload('stopwords')  # Download stopwords list.\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nfrom nltk import word_tokenize\ndownload('punkt')  # Download data for tokenizer.\n\nplt.rcParams['figure.figsize'] = [12, 8]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare data\n\nGather title and abstract from COVID19 articles and titles from news upto a maximum number of characters.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 3000   # 3000 chars\n\nresearch = pd.read_csv('/kaggle/input/CORD-19-research-challenge/metadata.csv')\nresearch['title_abstract'] = [str(research.loc[i,'title']) + ' ' + str(research.loc[i,'abstract']) for i in research.index ]\nresearch['source'] = 'research'\nresearch\n\nnews = pd.read_csv('/kaggle/input/covid19-public-media-dataset/covid19_articles.csv')\ndel news[\"Unnamed: 0\"]\nnews['source'] = 'news'\nnews['title_abstract'] = [ news.loc[i,'title'] + '. ' + news.loc[i,'content'][:(MAX_LEN-len(news.loc[i,'title']))] for i in news.index  ]\nnews\n\ndata = pd.concat([research[['title_abstract','source', 'url']], news[['title_abstract', 'source', 'url']]]).rename(columns={'title_abstract':'title'}).drop_duplicates().reset_index(drop=True)\n\nprint('News:',news.shape)\nprint('Research:',research.shape)\nprint('Combined data:',data.shape)\n\ndel research\ndel news\ngc.collect()\n\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Title similarity search and Topic Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gensim word embeddings\n# https://www.kaggle.com/raymishra/sentence-similarity-match\n# https://radimrehurek.com/gensim/models/fasttext.html\n# https://radimrehurek.com/gensim/models/keyedvectors.html\n\nimport gensim\nfrom gensim.models import Word2Vec\nfrom gensim.utils import simple_preprocess\n\nfrom gensim.models.keyedvectors import KeyedVectors\n\nfilepath = \"../input/gnewsvector/GoogleNews-vectors-negative300.bin\"\n\n\nfrom gensim.models import KeyedVectors\nwv_from_bin = KeyedVectors.load_word2vec_format(filepath, binary=True) \n\n#extracting words7 vectors from google news vector\nembeddings_index = {}\nfor word, vector in zip(wv_from_bin.vocab, wv_from_bin.vectors):\n    coefs = np.asarray(vector, dtype='float32')\n    embeddings_index[word] = coefs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# helper functions\n\ndef preprocess(doc):\n#     doc = re.sub(r'[\\W\\d]+',' ',doc)  # Remove numbers and punctuation.\n    doc = doc.lower()  # Lower the text.\n    doc = word_tokenize(doc)  # Split into words.\n    doc = [w for w in doc if not w in stop_words]  # Remove stopwords.\n    doc = [w for w in doc if w.isalpha()]  # Remove numbers and punctuation.\n    return doc\n\ndef avg_feature_vector(sentence, model, num_features):\n#     words = sentence.lower().split()\n#     words = preprocess(sentence)\n    words = simple_preprocess(sentence)\n    #feature vector is initialized as an empty array\n    feature_vec = np.zeros((num_features, ), dtype='float32')\n    n_words = 0\n    for word in words:\n        if word in embeddings_index.keys():\n            n_words += 1\n            feature_vec = np.add(feature_vec, model[word])\n    if (n_words > 0):\n        feature_vec = np.divide(feature_vec, n_words)\n    return feature_vec\n\nfrom scipy.spatial import distance\ndef calc_dist_cosine(s1, target, max_dist=0.5):\n    ret = []\n    for t in tqdm(target):\n        tv = avg_feature_vector(t,model= embeddings_index, num_features=300)\n        qv = avg_feature_vector(q,model= embeddings_index, num_features=300)\n        dist = distance.cosine(tv, qv)\n        if dist <= max_dist:\n            ret.append([dist, t])\n    df = pd.DataFrame(ret,columns=['dist','title']).reset_index(drop=True)\n    return pd.merge(df, data, on='title', how='left').sort_values(by='dist', ascending=True).reset_index(drop=True)\n\n\n# wv_from_bin.init_sims(replace=True)  # Normalizes the vectors in the word2vec class before calculating wmdistance\ndef calc_dist_wm(s1, target, max_dist=5.0):\n    \"\"\"\n    Word mover distance. Slower than cosine similarity.\n    https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html\n    \"\"\"\n    ret = []\n    for t in tqdm(target):\n#         print(t)\n        dist = wv_from_bin.wmdistance(preprocess(s1), preprocess(t))\n        if dist <= max_dist:\n            ret.append([dist, t])\n    df = pd.DataFrame(ret,columns=['dist','title']).reset_index(drop=True)\n    return pd.merge(df, data, on='title', how='left').sort_values(by='dist', ascending=True).reset_index(drop=True)\n       \ndef calc_dist(s1, target):\n    \"\"\"\n    Dist interface\n    \"\"\"\n    return calc_dist_cosine(s1, target)\n#     return calc_dist_wm(s1, target)\n\n# usage\n# s1_afv = avg_feature_vector('Why the second proforma does not coincide with the first, what has changed', model= embeddings_index, num_features=300 )\n# s2_afv = avg_feature_vector('Again came the proforma double.In the morning there was already a proforma with the same positions, but under a different number',model= embeddings_index, num_features=300)\n# cos = distance.cosine(s1_afv, s2_afv)\n# print(cos)\n# calc_dist_wm('Why the second proforma does not coincide with the first, what has changed', ['Again came the proforma double.In the morning there was already a proforma with the same positions, but under a different number'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LDA\n# https://www.kaggle.com/monsterspy/topic-modeling-with-lda\n# https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n\nfrom gensim.models import ldamodel\nimport gensim.corpora\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\nstop.update(['href','br'])\nfrom nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer(r'\\w+')\n\nnum_topics = 5\n\ndef train_lda(data_text):\n    train_ = []\n    for i in range(len(data_text)):\n        train_.append([word for word in tokenizer.tokenize(data_text[i].lower()) if word not in stop])\n\n    id2word = gensim.corpora.Dictionary(train_)\n    corpus = [id2word.doc2bow(text) for text in train_]    # Term Document Frequency\n    lda = ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=num_topics)\n    return lda\n\ndef get_lda_topics(model, num_topics, topn=5):\n    word_dict = {};\n    for i in range(num_topics):\n        words = model.show_topic(i, topn = topn);\n        word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words];\n    return pd.DataFrame(word_dict)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results\n\nThe topics include virus, cov, rna, protein, antibody, cells, infection which are research-based words in our data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# lda = train_lda(data.title.values.tolist())\n# lda_all_titles = get_lda_topics(lda, num_topics)\n# lda_all_titles","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_clickable(link):\n    # target _blank to open new window\n    return f'<a target=\"_blank\" href=\"{link}\">{link}</a>'\n# df.style.format({'url': make_clickable})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def search(q, out_prefix=\"result\"):\n    res = calc_dist(q, data.title)\n    res.to_csv(f'result_{out_prefix}.csv', index=False)\n\n    # second iteration using word distance\n#     res2 = calc_dist_wm(q, res.title)\n#     res2.to_csv(f'result_{out_prefix}_wmd.csv', index=False)\n\n#     lda = train_lda(res.title.values.tolist())\n#     lda_res = get_lda_topics(lda, num_topics)\n#     print(lda_res)\n    \n    topn = 20\n    wc = WordCloud(background_color='white', min_font_size=6, stopwords=stop_words).generate(' '.join(res.title.values.tolist()[:topn]).lower())\n    plt.imshow(wc)\n    plt.axis('off')\n\n    return res\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Communicating with high-risk populations"},{"metadata":{"trusted":true},"cell_type":"code","source":"q='communicating with high-risk populations'\nres = search(q, 'high_risk')\nres.head(20)[['title','url']].style.format({'url': make_clickable})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Information sharing and inter-sectoral collaboration"},{"metadata":{"trusted":true},"cell_type":"code","source":"q='information sharing and inter-sectoral collaboration'\nres = search(q, 'info_sharing')\nres.head(20)[['title','url']].style.format({'url': make_clickable})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"q='data standards and nomenclature'\nres = search(q, 'data_standards')\nres.head(20)[['title','url']].style.format({'url': make_clickable})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Governmental public health"},{"metadata":{"trusted":true},"cell_type":"code","source":"q='governmental public health'\nres = search(q, 'govt_public_health')\nres.head(20)[['title','url']].style.format({'url': make_clickable})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Community measures"},{"metadata":{"trusted":true},"cell_type":"code","source":"q='clarify community measures'\nres = search(q, 'comm_measure')\nres.head(20)[['title','url']].style.format({'url': make_clickable})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Equity considerations and problems of inequity"},{"metadata":{"trusted":true},"cell_type":"code","source":"q='equity considerations and problems of inequity'\nres = search(q, 'inequity')\nres.head(20)[['title','url']].style.format({'url': make_clickable})","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}