{"cells":[{"metadata":{},"cell_type":"markdown","source":"Okaaay!!\nLet's do this.. \n\nSo, I prepared this kernel to try to model a Convolutional Neural Network and updated it via backpropagation only using numpy. The idea behind all of this is to further understand backpropagation and the problems with vanishing gradients.\n\nBefore I beggin I'll like to disclaim the following statements:\n1. I do not intend to built the most accurate model at this moment since the ideia is to unravel backpropagation\n2.  Running all this will consume a lot of you memory and it will take a long time, mainly because i wanted to be more didactical with my coding and use A LOTTTTTT of for loops. So I apologise for not being to efficient, but that was not really my goal in this kernel.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# As always, lets beggin with uploading the libraries\nimport matplotlib.pyplot as plt \nimport numpy as np\nfrom PIL import Image \nimport pandas as pd\nimport cv2\nimport os\n\n# Loading the training data\ndata =  pd.read_csv(\"../input/mnist-in-csv/mnist_train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets vizualise the data\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Now let's see the distribution of the labels within the training data\n%matplotlib inline\nn_labels = np.zeros(10)\nfor i in data['label'].values:\n    n_labels[i] +=1\nn_labels\nlabels = np.linspace(0, 9, 10)\n \nplt.figure(figsize=(10,10))\nplt.bar(labels,n_labels)\nplt.xticks(labels, rotation=30)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# So the data is fairly distributed\n\n# Lets load now the test data\ntestdata =  pd.read_csv(\"../input/mnist-in-csv/mnist_test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating training data\ntraining_features = data.drop(['label'], axis=1).values\nX_train = np.zeros((len(training_features),28,28)) #28, 28 is the image file with 784 pixels\nfor i in range(0,len(training_features)):\n    X_train[i][:,:] = training_features[i].reshape(28,28)/255 # Normalize the data between 0 and 1\n\nY_train = data['label'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating test data\ntest_features = testdata.drop(['label'], axis=1).values\nX_test = np.zeros((len(test_features),28,28))\nfor i in range(0,len(test_features)):\n    X_test[i][:,:] = test_features[i].reshape(28,28)/255\n\nY_test = testdata['label'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X_train),len(X_test) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that the data has been loaded and transformed it's time to model our Neural Network\nFor this excercise I'm going to use the following architecture:\n\n Architecture                                  Ouputs\n Input   -> Image                         - ( , 28, 28)     - Parameters to learn (0)\n Layer 1 -> CNN_nº kernels(2) kernel(3x3) - ( , 26, 26, 2)  - Parameters to learn (20)\n Layer 2 -> Maxpooling Kernel(2x2) s=2    - ( , 13, 13, 2)  - Parameters to learn (0)   \n Layer 4 -> Flatten                       - ( , 338)        - Parameters to learn (0)\n Output  -> Dense(10)                     - (, 10)         - Parameters to learn (3380)\n"},{"metadata":{},"cell_type":"markdown","source":"## Defining functions for calculating Convolutional foward passes"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Relu and Sigmoid are activation functions, later I'll decide which one is performing better\n\ndef sigmoid(x): \n    return(1/(1+np.exp(-x)))\n\ndef relu(x):\n    x = (x > 0) * x\n    return(x)\n\n# Defining the function to perform the convolutional forwardpass\n# If you have any doubts in how CNN works i suggest you watch Andrew Ng youtube channel\ndef conv_forward_pass(inputs, kernel, bias):\n    '''Calculate the convolutional foward pass using an activation function \n       Inputs:\n       layer_input -> A group of images\n       kernel -> a matrix containing the filters and number of filters (kernel_row, kernel_col, kernel_ch, n_filters)\n       bias -> Matrix with biases for the convolutional calculation\n       \n       Output:\n       Array containing the output of the images passing through the convolutional layer\n       Each output corresponds to a function(x)= activation(z + b)\n    '''\n    batches = len(inputs)\n    conv_outputs =[]\n    for batch in range(0,batches):\n        layer_input = inputs[batch] #passing a single image through the convolutional layer\n        if len(layer_input.shape) == 2: # in the case there input layer has only one channel, e.g black&white picture\n            z_row = layer_input.shape[0] - kernel.shape[0] + 1\n            z_col = layer_input.shape[1] - kernel.shape[1] + 1\n            if len(kernel.shape) == 2: # in the case there is only one filter in the convolutional layer\n                z = np.empty((z_row,z_col))\n                for i in range(0,z_row):\n                    for j in range(0,z_col):\n                        x= 0\n                        for ik in range(0,kernel.shape[0]):\n                            for ij in range(0,kernel.shape[1]):\n                                x += layer_input[i+ik,j+ij]*kernel[ik,ij]\n                        z[i,j]= x + bias\n            else:\n                nfilter = kernel.shape[-1]\n                z = np.empty((z_row,z_col,nfilter))\n                for f in range(0, nfilter):\n                    for i in range(0,z_row):\n                        for j in range(0,z_col):\n                            x= 0\n                            for ik in range(0,kernel.shape[0]):\n                                for ij in range(0,kernel.shape[1]):\n                                    x += layer_input[i+ik,j+ij]*kernel[ik,ij,f]\n                            z[i,j,f]= x + bias[f]\n        else: # in the case there input layer has more than one channel\n            layer_input_ch = layer_input.shape[2]     \n            z_row = layer_input.shape[0] - kernel.shape[0] + 1\n            z_col = layer_input.shape[1] - kernel.shape[1] + 1\n            if len(kernel.shape) == 3: # in the case there is only one filter in the convolutional layer\n                z = np.empty((z_row,z_col))\n                for i in range(0,z_row):\n                    for j in range(0,z_col):\n                        x= 0\n                        for ik in range(0,kernel.shape[0]):\n                            for ij in range(0,kernel.shape[1]):\n                                for ch in range(0,layer_input_ch):\n                                    x += layer_input[i+ik,j+ij,ch]*kernel[ik,ij,ch]\n                        z[i,j]= x + bias\n            else:\n                layer_input_ch = layer_input.shape[2]\n                nfilter = kernel.shape[-1]\n                z = np.empty((z_row,z_col,nfilter))\n                a = np.empty(z.shape)\n                for f in range(0, nfilter):\n                    for i in range(0,z_row):\n                        for j in range(0,z_col):\n                            for ik in range(0,kernel.shape[0]):\n                                for ij in range(0,kernel.shape[1]):\n                                    x= 0\n                                    for ch in range(0,layer_input_ch):\n                                        x += layer_input[i+ik,j+ij,ch]*kernel[ik,ij,ch,f]\n                            z[i,j,f]= x + bias[f]\n        a = sigmoid(z)\n        #a = relu(z)\n        conv_outputs.append(a)\n    conv_outputs = np.array(conv_outputs)\n    return(conv_outputs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Defining functions for calculating Maxpool foward passes"},{"metadata":{"trusted":true},"cell_type":"code","source":"def maxpool_forward_pass(inputs, pool_filter_size, stride):\n    '''Calculate the maxpool foward pass\n       Inputs:\n       pool_input -> Input from previous layer\n       pool_filter_size -> integer of the row and col dimension of the pool filter\n       stride -> number of places the filter slides with in the pool_input dimensions\n       \n       Output:\n       Array containing the output of the previous layer passing through the maxpool layer\n    '''\n    batches = len(inputs)\n    output = []\n    for batch in range(0,batches):\n        pool_input = inputs[batch]\n        f_row = pool_filter_size\n        f_col = pool_filter_size\n        if len(pool_input.shape) == 2: # in the case there is no Channel dimension\n            pool_input_row = pool_input.shape[0]\n            pool_input_col = pool_input.shape[1]\n            pool_output_row = int(((pool_input_row - f_row)/stride)+1)\n            pool_output_col = int(((pool_input_col - f_col)/stride)+1)\n            pool_output = np.empty((pool_output_row, pool_output_col))\n            for i in range(0,pool_output_row):\n                for j in range(0,pool_output_col):\n                    x= []\n                    for ik in range(0,f_row):\n                        for ij in range(0,f_col):\n                            x.append(pool_input[(((i-1)*stride)+f_row)+ik,(((j-1)*stride)+f_col)+ij])\n                    pool_output[i,j] = max(x)\n        else:    \n            pool_input_row = pool_input.shape[0]\n            pool_input_col = pool_input.shape[1]\n            pool_input_ch = pool_input.shape[2]\n            pool_output_row = int(((pool_input_row - f_row)/stride)+1)\n            pool_output_col = int(((pool_input_col - f_col)/stride)+1)\n            pool_output_ch = pool_input_ch\n            pool_output = np.empty((pool_output_row, pool_output_col, pool_output_ch))\n            for f in range(0, pool_output_ch):\n                        for i in range(0,pool_output_row):\n                            for j in range(0,pool_output_col):\n                                x= []\n                                for ik in range(0,f_row):\n                                    for ij in range(0,f_col):\n                                        x.append(pool_input[(((i-1)*stride)+f_row)+ik,(((j-1)*stride)+f_col)+ij,f])\n                                pool_output[i,j,f] = max(x)\n        output.append(pool_output)                        \n    output = np.array(output)    \n    return(output)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Defining functions for calculating flatten foward passes"},{"metadata":{"trusted":true},"cell_type":"code","source":"def flatten_foward_pass(input_flatten):\n    '''Transform input in a flatten array of one column'''\n    batches = len(input_flatten)\n    flatten_outputs=[]\n    for batch in range(0,batches):\n        flatten_outputs.append(np.reshape(input_flatten[batch],-1))    \n    flatten_outputs = np.array(flatten_outputs)\n    return(flatten_outputs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Defining functions for calculating Dense foward passes"},{"metadata":{"trusted":true},"cell_type":"code","source":"def dense_foward_pass(dense_input, n_output_nodes, weights, bias):\n    '''Calculate the outputs of a dense layer \n       Inputs:\n       dense_input -> result from previous layer\n       n_output_nodes -> number of output nodes\n       weights -> matrix of weights \n       bias - > Matrix with biases\n\n       Outputs:\n       Group of array of nodes considering a activation function of previous nodes * weights + biases \n    '''\n    batches = len(dense_input)\n    dense_outputs=[]\n    for batch in range(0,batches):\n        z = np.empty((n_output_nodes))\n        for i in range(0,n_output_nodes):\n            z[i] = (dense_input[batch] @ weights[:,i]) + bias[i]\n        #z = z/len(dense_input[batch])\n        a = sigmoid(z)\n        #a = relu(z)\n        dense_outputs.append(a)    \n    return(np.array(dense_outputs))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Defining functions for calculating final outputs foward passes"},{"metadata":{"trusted":true},"cell_type":"code","source":"def stable_softmax(x):\n    exps = np.exp(x - np.max(x))\n    return(exps / np.sum(exps))\n\ndef output_foward_pass(dense_input, n_output_nodes, weights, bias):\n    '''Calculate the outputs of a dense layer\n       Inputs:\n       dense_input -> single result from previous layer\n       n_output_nodes -> number of output nodes\n       weights -> matrix of weights \n       bias - > Matrix with biases\n\n       Outputs:\n       Flatten array of nodes considering a softmax function of previous nodes * weights + biases \n    '''\n    batches = len(dense_input)\n    dense_outputs=[]\n    for batch in range(0,batches):\n        z = np.empty((n_output_nodes))\n        for i in range(0,z.shape[0]):\n            z[i] = dense_input[batch] @ weights[:,i] + bias[i]\n        a = stable_softmax(z)\n        dense_outputs.append(a)\n    dense_outputs = np.array(dense_outputs)\n    return(dense_outputs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Defining Calculation for Loss Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def reshape_labels(prediction,labels):\n    label_reshape = np.zeros(prediction.shape)\n    for i in range(0,len(prediction)):\n        label_reshape[i][labels[i]] = 1\n    return(label_reshape)\n\ndef loss_SSE(prediction, labels):\n    loss_SSE = []\n    labels_reshape = reshape_labels(prediction, labels)\n    for i in range(0,len(prediction)):\n        loss_SSE.append(np.sum((prediction[i] - labels_reshape[i])**2))\n    return(np.mean(loss_SSE))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okkayy!\nNow, for those who are still with me here comes the tricky part.\nWe now have defined all the functions we are going to need to do a single foward pass in our CNN model.\nIn order now for our model to learn, we need to define the backpropagation functions.\n\nAs you may know, backpropagation uses partial derivatives and gradient descent to update the parameters (weights and biases) in order to minimize the loss and therefore learn.\n\nIf you have doubts about the subject, i encourage you to view the materials of 3blue1brown, Siraj Raval and Andrew Ng on youtube."},{"metadata":{},"cell_type":"markdown","source":"## Output Layer Backpropagation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def grad_loss(prediction, labels):\n    label_reshape = reshape_labels(prediction, labels)\n    grad_loss_out = []\n    for i in range(0,len(prediction)):\n        grad_loss_out.append(2*(prediction[i] - label_reshape[i]))\n    return(np.array(grad_loss_out))\n\ndef grad_softmax(x):\n    '''Compute the gradient of the output layer over each z, Output = softmax(z)\n    Input -> Output layer you waht to do the gradient of softmax over\n    Outpu -> Matrix of the gradient of each outpu w.r.t each z\n    '''\n    grad_softmax =[]\n    for k in range(0,len(x)):\n        jacobian =np.empty((x.shape[-1],x.shape[-1]))\n        for i in range(0, jacobian.shape[0]):\n            for j in range(0, jacobian.shape[1]):\n                if i == j:\n                    jacobian[i][j] =  x[k][i] * (1 - x[k][j])\n                else:\n                    jacobian[i][j] =  x[k][i] * (0 - x[k][j])\n        grad_softmax.append(jacobian)            \n    return(np.array(grad_softmax))     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def output_backpropagation(prediction, labels, previous_layer_weights, previous_layer_bias, previous_layer_outputs):\n    '''Return the gradients of the loss function w.r.t the previous weights, biases, and inputs.\n    Inputs:\n    Prediction -> Group prediction for the batch\n    Labels -> Correct answers for the predictions\n    Previous_layer_weights -> Matrix of weights from the previous layer\n    Previous_layer_bias -> Array of biases from the previous layer\n    Previous_layer_ouput -> Array of the output of previous layer, .e.g, this layer input\n    \n    Output:\n    Gradients for Weights, Biases and previous activation nodes\n    '''\n    \n    # Calculating the Weights Gradients\n    \n    #Step 1 - Calculate the Gradient of the loss w.r.t the outputs\n    grad_loss_outputs = grad_loss(prediction, labels)\n    \n    #Step 2 - Calculate the Gradient of each output w.r.t each z, where output[i] = softmax(z[i])\n    grad_outputs_z = grad_softmax(prediction)\n    \n    #Step 3 - Calculate the Gradient of each z w.r.t each weight, where z[i] = W[i,j]X[j] + W[i,j+1]X[j+1] ... + W[i,j+n]X[j+n] + b[i]  \n    grad_z_weights = previous_layer_outputs\n    \n    #Step 4 - Calculate the Gradient the loss w.r.t each weight using the chain rule\n    w_row = previous_layer_weights.shape[0]\n    w_col = previous_layer_weights.shape[1]\n    w_ch = len(prediction)\n    grad_w = np.empty((w_ch, w_row, w_col))\n    for ch in range(0,w_ch):\n        for i in range(0, w_row):\n            for j in range(0,w_col):\n                grad_w[ch,i,j] = (grad_loss_outputs[ch] @ grad_outputs_z[ch][:][j])* grad_z_weights[ch][i]\n    \n    #Step 5 - Calculate the average of the gradients\n    grad_loss_weight = np.empty((w_row,w_col))\n    for i in range(0,w_row):\n            for j in range(0,w_col):\n                grad_loss_weight[i,j] = np.mean(grad_w[:,i,j])\n    \n    #return(grad_loss_weight)\n    \n    # Calculating the Bias Gradients\n    \n    #Step 6 - Calculate the Gradient of each z w.r.t each bias, where z[i] = W[i,j]X[j] + W[i,j+1]X[j+1] ... + W[i,j+n]X[j+n] + b[i]  \n    grad_z_bias = 1\n    \n    #Step 7 - Calculate the Gradient the loss w.r.t each bias using the chain rule\n    b_col = len(previous_layer_bias)\n    b_ch = len(prediction)\n    grad_b = np.empty((b_ch, b_col))\n    for ch in range(0,b_ch):\n        for j in range(0,b_col):\n            grad_b[ch,j] = (grad_loss_outputs[ch] @ grad_outputs_z[ch][j])* grad_z_bias\n    \n    #Step 8 - Calculate the average of the gradients\n    grad_loss_bias = np.empty((b_col))\n    for j in range(0,b_col):\n        grad_loss_bias[j] = np.mean(grad_b[:,j])\n    \n    #return(grad_loss_bias)\n    \n    # Calculating the last layer Activation Node Gradient\n    \n    #Step 9 - Calculate the Gradient of each z w.r.t each A, where z[i] = W[i,j]A[j] + W[i,j+1]A[j+1] ... + W[i,j+n]A[j+n] + b[i] \n    grad_z_previous_activation = previous_layer_weights\n    \n    #Step 10 - Calculate the Gradient the loss w.r.t each previous activation layer, using the chain rule\n    a_row = previous_layer_outputs.shape[-1]\n    a_ch = len(prediction)\n    grad_a = np.zeros((a_ch, a_row))\n    for ch in range(0,a_ch):\n        for i in range(0,a_row):\n            for j in range(0, prediction.shape[-1]):\n                x = 0\n                for sj in range(0, grad_outputs_z.shape[-1]):\n                    x += (grad_loss_outputs[ch][j] * grad_outputs_z[ch][j][sj] * grad_z_previous_activation[i][j])\n                grad_a[ch,i] = x\n       \n    return(grad_loss_weight, grad_loss_bias, grad_a)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Flatten Layer Backpropagation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def inverse_flatten(x, row ,col ,ch):\n    '''Return array into matrix of batch,(i,j,ch) dimensions\n    Inputs:\n    x -> Array of (batch, y) dimension\n    (row, col ,ch) -> number of rows, columns and channels the output matrix must have\n    \n    Output -> Array of dimension (batch, (row,col,ch))\n    '''\n    inverse_flatten = []\n    for batch in range(0,x.shape[0]):\n        inverse_flatten.append(x[batch].reshape((row,col,ch)))\n    \n    return(np.array(inverse_flatten))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Maxpool Layer Backpropagation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def inverse_maxpool(gradients, output_matrix, input_matrix, pool_filter_size, pool_stride):\n    inverse_maxpool = np.zeros(input_matrix.shape)\n    for batch in range(0, output_matrix.shape[0]):\n        for ch in range(0, output_matrix.shape[-1]):\n            for i in range(0, output_matrix.shape[1]):\n                for j in range(0, output_matrix.shape[2]):\n                    for i_slide in  range(0,pool_filter_size):\n                        for j_slide in  range(0,pool_filter_size):\n                            if output_matrix[batch][i][j][ch] == input_matrix[batch][((i-1)*pool_stride)+i_slide][((j-1)*pool_stride)+j_slide][ch]:\n                                inverse_maxpool[batch][((i-1)*pool_stride)+i_slide][((j-1)*pool_stride)+j_slide][ch] = gradients[batch][i][j][ch]\n    return(inverse_maxpool)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Convolutional Layer Backpropagation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def convolutional_input_backpropagation(actual_layer_outputs, previous_layer_kernel_weights, previous_layer_bias, previous_layer_outputs, grad_loss_outputs):\n    '''Return the gradients of the loss function w.r.t the previous weights, biases, and inputs.\n    Inputs:\n    actual_layer_outputs -> Group of the outputs for this layer\n    Previous_layer__kernel_weights -> Matrix of weights from the convolutional kernel\n    Previous_layer_bias -> Array of biases from the previous layer\n    Previous_layer_ouput -> Array of the output of previous layer, .e.g, this layer input\n    grad_loss_outputs -> Array of the gradient of the loss function w.r.t. each node of this layer\n    \n    Output:\n    Gradients for Weights, Biases and previous activation nodes\n    '''\n    # Calculating the Weights Gradients\n    \n    #Step 1 - Calculate the Gradient of the loss w.r.t the layers node outputs\n    '''This step was previously calculated in the backpropagation of the output layer'''\n    #grad_loss_outputs = grad_loss_outputs\n    \n    #Step 2 - Calculate the Gradient of each node w.r.t each z, where output[i] = sigmoid(z[i])\n    # Deriviative of sigmoid function\n    grad_outputs_z = actual_layer_outputs * (1 - actual_layer_outputs)\n    #grad_outputs_z = grad_relu(actual_layer_outputs)\n    \n    #Step 3 - Calculate the Gradient the loss w.r.t each kernel weight using the chain rule  \n    batches = len(actual_layer_outputs)\n    n_kernels = previous_layer_kernel_weights.shape[-1]\n    k_row = previous_layer_kernel_weights.shape[0]\n    k_col = previous_layer_kernel_weights.shape[1]\n    \n    outputs_row = actual_layer_outputs.shape[1]\n    outputs_col = actual_layer_outputs.shape[2]\n       \n    grad_k = np.empty((batches, k_row, k_col, n_kernels))\n    for batch in range(0,batches):\n        for nk in range(0, n_kernels):\n            for i in range(0, k_row):\n                for j in range(0,k_col):\n                    grad_k[batch, i, j] = np.sum((grad_loss_outputs[batch][:,:,nk] * grad_outputs_z[batch][:,:,nk]) *  previous_layer_outputs[batch][i:i+outputs_row,j:j+outputs_col])\n    \n    #Step 4 - Calculate the average of the gradients\n    grad_loss_kernel = np.empty((k_row,k_col, n_kernels))\n    for nk in range(0, n_kernels):\n        for i in range(0, k_row):\n            for j in range(0,k_col):\n                grad_loss_kernel[i,j,nk] = np.mean(grad_k[:,i,j,nk])\n    \n    #return(grad_loss_kernel)\n    \n    #Step 5 - Calculate the Gradient of each z w.r.t each bias\n    grad_z_bias = 1\n    \n    #Step 6 - Calculate the Gradient the loss w.r.t each bias using the chain rule\n    grad_b = np.empty((batches, n_kernels))\n    for batch in range(0,batches):\n        for nk in range(0, n_kernels):\n            grad_b[batch,nk] = np.sum(grad_loss_outputs[batch][:,:,nk] * grad_outputs_z[batch][:,:,nk])\n    \n    #Step 7 - Calculate the average of the gradients\n    grad_loss_bias = np.empty((n_kernels))\n    for nk in range(0, n_kernels):\n        grad_loss_bias[nk] = np.mean(grad_b[:,nk])\n    \n    #return(grad_loss_bias)\n\n    return(grad_loss_kernel, grad_loss_bias)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\"Is this the real life... is this just fantasy..\"\n\nWell, the worst part (or best if you like this stuff) is over. Now that all the functions have been declared we can beggin training our data. \n\nTo do that we must define a new function.. the mother of all functions.. \n\n\"One Funtion to rule them all, One Funtion to find them,\nOne Function to bring them all and in the darkness bind them\"\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nimport sys\n\ndef fit_CNN_small(training_features, training_labels, test_features, test_labels, epochs, batch_size, lr):\n    '''Fit the CNN architecture to the training data'''\n    #================================================================================================================\n    # Step 1: initialize weights, kernels, and biases\n    #================================================================================================================\n    #Layer 1\n    l1_kernel_size = 3\n    l1_kernel_numbers = 2\n    l1_kernel = np.random.randn(l1_kernel_size,\n                          l1_kernel_size,\n                          l1_kernel_numbers)\n    \n    l1_bias = np.random.randn(l1_kernel_numbers)\n    \n    #Layer 2\n    #N/A\n    \n    #Layer 3\n    #N/A\n    \n    #Layer 4 \n    output_nodes = 10\n    l4_weights =np.random.randn(338,output_nodes) #338 is the input shape from the flatten layer\n    l4_bias = np.random.randn(output_nodes)\n    \n    #================================================================================================================\n    # Step 2: divide the number of batches\n    #================================================================================================================\n    \n    number_of_batches = len(training_features)/batch_size\n    number_of_batches = math.ceil(number_of_batches)\n\n    #================================================================================================================\n    # Step 3: train for each epoch and batch\n    #================================================================================================================\n    metric=pd.DataFrame(columns=['Epoch', 'Train Loss', 'Train Accuracy', 'Test Loss', 'Test Accuracy'])\n    counter = 0\n    last_test_acc = 0\n    for epoch in range(0, epochs):\n        for i in range(0,number_of_batches-1):\n            # Forward Pass\n            l1_inputs = training_features[i*batch_size:i*batch_size + batch_size]\n            l2_inputs = conv_forward_pass(l1_inputs, l1_kernel, l1_bias)\n            l3_inputs = maxpool_forward_pass(l2_inputs, 2, 2)\n            l4_inputs = flatten_foward_pass(l3_inputs)\n            outputs = output_foward_pass(l4_inputs, output_nodes, l4_weights, l4_bias)\n\n            # Calculate Loss\n            batch_label = training_labels[i*batch_size:i*batch_size + batch_size]\n            batch_loss = loss_SSE(outputs, batch_label)\n            sys.stdout.write('\\rEpoch: %d ,Batch: %d ,loss: %.4f '%(epoch,i,batch_loss))\n            sys.stdout.flush()\n\n\n            # Backpropagation\n            grad_l4_weights, grad_l4_bias, grad_l4_inputs = output_backpropagation(outputs, batch_label, l4_weights , l4_bias, l4_inputs)\n            grad_l3_inputs = inverse_flatten(grad_l4_inputs, 13, 13, 2)\n            grad_l2_inputs = inverse_maxpool(grad_l3_inputs, l3_inputs, l2_inputs, 2, 2)\n            grad_l1_kernel, grad_l1_bias = convolutional_input_backpropagation(l2_inputs, l1_kernel, l1_bias, l1_inputs, grad_l2_inputs)\n\n            #Updating Kernel, weights and biases\n            l1_kernel = l1_kernel - (lr*grad_l1_kernel)\n            l1_bias = l1_bias - (lr*grad_l1_bias)\n\n            l4_weights = l4_weights - (lr*grad_l4_weights)\n            l4_bias = l4_bias - (lr*grad_l4_bias)\n\n\n        # Calculate for last batch\n        # Forward Pass\n        l1_inputs = training_features[(number_of_batches-1)*batch_size:]\n        l2_inputs = conv_forward_pass(l1_inputs, l1_kernel, l1_bias)\n        l3_inputs = maxpool_forward_pass(l2_inputs, 2, 2)\n        l4_inputs = flatten_foward_pass(l3_inputs)\n        outputs = output_foward_pass(l4_inputs, output_nodes, l4_weights, l4_bias)\n\n        # Calculate Loss\n        batch_label = training_labels[(number_of_batches-1)*batch_size:]\n        batch_loss = loss_SSE(outputs, batch_label)\n        sys.stdout.write('\\rEpoch: %d ,Batch: %d ,loss: %.4f'%(epoch,number_of_batches,batch_loss))\n        sys.stdout.flush()\n\n        \n        # Backpropagation\n        grad_l4_weights, grad_l4_bias, grad_l4_inputs = output_backpropagation(outputs, batch_label, l4_weights , l4_bias, l4_inputs)\n        grad_l3_inputs = inverse_flatten(grad_l4_inputs, 13, 13, 2)\n        grad_l2_inputs = inverse_maxpool(grad_l3_inputs, l3_inputs, l2_inputs, 2, 2)\n        grad_l1_kernel, grad_l1_bias = convolutional_input_backpropagation(l2_inputs, l1_kernel, l1_bias, l1_inputs, grad_l2_inputs)\n        \n        #Updating Kernel, weights and biases\n        l1_kernel = l1_kernel - (lr*grad_l1_kernel)\n        l1_bias = l1_bias - (lr*grad_l1_bias)\n\n        l4_weights = l4_weights - (lr*grad_l4_weights)\n        l4_bias = l4_bias - (lr*grad_l4_bias)\n    \n    #================================================================================================================\n    # Step 4: Calculate Epoch train Loss\n    #================================================================================================================\n        # Calculate for Foward Pass for epoch\n        l1_inputs = training_features\n        l2_inputs = conv_forward_pass(l1_inputs, l1_kernel, l1_bias)\n        l3_inputs = maxpool_forward_pass(l2_inputs, 2, 2)\n        l4_inputs = flatten_foward_pass(l3_inputs)\n        outputs = output_foward_pass(l4_inputs, output_nodes, l4_weights, l4_bias)\n\n        # Calculate Epoch train Loss\n        batch_label = training_labels\n        batch_loss = loss_SSE(outputs, batch_label)\n    \n    #================================================================================================================\n    # Step 5: Calculate Epoch train Accuracy\n    #================================================================================================================\n    \n        outputs_acc = np.zeros(outputs.shape)\n        for i in range(0,len(outputs)):\n            outputs_acc[i][np.argmax(outputs[i])] = 1.\n\n        labels_reshape = reshape_labels(outputs, batch_label)\n        train_acc = np.sum(outputs_acc * labels_reshape)/len(outputs_acc)\n               \n     #================================================================================================================\n    # Step 6: Calculate Epoch test Loss\n    #================================================================================================================\n        # Calculate for Foward Pass for epoch\n        l1_inputs = test_features\n        l2_inputs = conv_forward_pass(l1_inputs, l1_kernel, l1_bias)\n        l3_inputs = maxpool_forward_pass(l2_inputs, 2, 2)\n        l4_inputs = flatten_foward_pass(l3_inputs)\n        outputs_test = output_foward_pass(l4_inputs, output_nodes, l4_weights, l4_bias)\n\n        # Calculate Epoch train Loss\n        batch_test_label = test_labels\n        batch_test_loss = loss_SSE(outputs_test, batch_label)   \n\n    #================================================================================================================\n    # Step 7: Calculate Epoch test Accuracy\n    #================================================================================================================\n        outputs_test_acc = np.zeros(outputs_test.shape)\n        for i in range(0,len(outputs_test)):\n            outputs_test_acc[i][np.argmax(outputs_test[i])] = 1.\n\n        labels_reshape = reshape_labels(outputs_test, batch_test_label)\n        test_acc = np.sum(outputs_test_acc * labels_reshape)/len(outputs_test_acc)\n        \n        print('\\nEpoch:',epoch,'\\tTrain_loss:',batch_loss, '\\tTrain_acc:',train_acc,'\\tTest_loss:',batch_test_loss, '\\tTest_acc:',test_acc)\n        metric.loc[counter] = [epoch, batch_loss, train_acc, batch_test_loss, test_acc]\n        counter +=1\n        \n        \n    return(l1_kernel, l1_bias, l4_weights, l4_bias, metric)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay.. Now everything is ready.\nLet's beging the training"},{"metadata":{"trusted":true},"cell_type":"code","source":"l1_kernel, l1_bias, l4_weights, l4_bias, metric = fit_CNN_small(X_train, Y_train, X_test, Y_test, 2, 64, 1e-02)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I ran this in my computer and after a loooooooooong time training the accuracy for the test model improved from 14.8% up to 37.7%. I’ve stopped because the rate of learning was very slow and improvement will take more time. Although the accuracy is not that great the coding proved that the model was learning (which was the goal afterall)."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}