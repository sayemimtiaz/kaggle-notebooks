{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Customer Churn in a Bank\n\n### Attempting to ascertain if a customer would stay with the bank or leave\n\n#### Data Description:\n- RowNumber: Row Serial number\n- CustomerID: Customer Unique Identifier\n- Surname: Customer Last name\n- CreditScore: Customer Credit Score\n- Geography: Conuntry where the Customer lives\n- Gender: Customer Gender\n- Age: Customer Age\n- Tenure: Tenure with Bank\n- Balance: Account Balance\n- NumOfProducts: Number of bank products customer is using\n- HasCrCard: Has a credit card (0 = No, 1 = Yes)\n- IsActiveMember: Is an active member (0 = No, 1 = Yes)\n- EstimatedSalary: Estimated Salary\n- Exited: Exited Bank (0 = No, 1 = Yes\n\n### Since our Target variable has a binary classification (0 and 1), this problem will need to be solved using a Classification Algorithm.\n\n#### We will use below algorithms:\n- Logistic Regression\n- Gaussian naïve Bayes\n- k-nearest neighbors\n- Decision tree\n- Random forest\n- XGBoost"},{"metadata":{},"cell_type":"markdown","source":"### **Importing required Python Libraries**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nimport xgboost as xgb\n\nimport joblib  #Joblib is a set of tools to provide lightweight pipelining in Python (Avoid computing twice the same thing)\n\n\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import GridSearchCV\n\n\nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score,roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Importing data into a DataFrame & checking the import data**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"filename = \"/kaggle/input/analysis-of-banking-data-model/Bank_churn_modelling.csv\"\ndf = pd.read_csv(filename)\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This data set does not seem to have any missing values. We will not neeed to do any data imputation.\n#### Dropping the 'RowNumber' column\n\nSince the 'RowNumber' column is a number, there is a possibility that the model will consider it relevant in the prediction of the output and this can possibly skew the data. Hence, we will drop this column."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['RowNumber'], axis = 1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will now try and find the correlation among the variables in the dataframe using the ``DataFrame.corr()`` function.\n<br>\nThis function uses the **Pearson** method.\nFor the purpose of this matrix, we will remove the **'CustomerId'** for the same reason as we why dropped **'RowNumber'** column"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['CustomerId'], axis = 1).corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Visualising the above correlation matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(10,8))\nsns.heatmap(df.drop(['CustomerId'], axis = 1).corr(), annot = True, cmap = 'YlGnBu')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.countplot(x = 'Geography', data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"France has the highest number of customers. Remaining are equally split between Spain and Germany.\n<br>\nTrying to check if there is any relationship between **'Geography'** and **'CreditScore'**."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x = 'Geography', y = 'CreditScore', data = df, palette = 'hls')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Country does not seem to have any bearing on the Credit Score of the customers.\n<br>\nWe will now check the relationship between **'Age'**, **'Gender'** and whether that has any relationship with customers **'Exiting'**."},{"metadata":{"trusted":true},"cell_type":"code","source":"exited = 'Exited'\nnot_exited = 'Not Exited'\n\nfig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (16,6))\n\nmale = df[df['Gender']=='Male']\nfemale = df[df['Gender']=='Female']\n\nax = sns.distplot(female[female['Exited']==1].Age, bins = 18, label = exited, ax = axes[0], kde = False)\nax = sns.distplot(female[female['Exited']==0].Age, bins = 40, label = not_exited, ax = axes[0], kde = False)\nax.legend()\nax.set_title('Female')\n\nax = sns.distplot(male[male['Exited']==1].Age, bins = 18, label = exited, ax = axes[1], kde = False)\nax = sns.distplot(male[male['Exited']==0].Age, bins = 40, label = not_exited, ax = axes[1], kde = False)\nax.legend()\nax.set_title('Male')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.distplot(df[df['Exited']==1].Age, bins = 18, label = exited, kde = False)\nax = sns.distplot(df[df['Exited']==0].Age, bins = 40, label = not_exited, kde = False)\nax.legend()\nax.set_title('Overall')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While the **'Gender'** of the customer seems to be a factor, **'Age'** also seems to be a contributing factor.\n<br>\nOlder customer are more prone to exit than younger customers. More older Female customers are at risk of leaving the bank compared to older Male customers. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='Exited',y='CreditScore',data=df,palette='hls')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above box plot indicates that the credit score of a customer is not a significant factor in their Exit."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='Exited',y='Age',data=df,palette='hls')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Age of a customer is again proving to be significant. The older a customer is, there is a higher change that they would exit the bank. Let's explore this further."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.scatterplot(data=df, x=\"Age\", y=\"Balance\", hue=\"Exited\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(16, 6))\n\nexiting = df[df['Exited']==1]\nstaying = df[df['Exited']==0]\n\nax = sns.regplot(x=\"Age\", y=\"Balance\", data=staying, ax = axes[0])\nax.legend()\nax.set_title('Not Exited')\n\nax = sns.regplot(x=\"Age\", y=\"Balance\", data=exiting, ax = axes[1])\nax.legend()\nax.set_title('Exited')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='Exited',y='Balance',data=df,palette='hls')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based of a visual assessment of the above three plots, it seems that if a customer is more senior in age and their mean balance is higher, they would have a higher chance of exiting the bank. The customers staying are comparatively younger."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='Geography',y='Balance',data=df,palette='hls')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Customers in Germany have a better avergage balance maintained than those in France and Spain."},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.catplot(x='HasCrCard', hue='Exited',data=df, kind='count',height=4, aspect=.7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.catplot(x='Geography', hue='Gender', col='Exited',data=df, kind='count',height=4, aspect=.7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see fewer males have exited, compared to females. Gender seems to a factor in the churn. This seems to be an observation across the three geographies."},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.catplot(x='Geography', hue='Exited',data=df, kind='count',height=4, aspect=.7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While Germany and Spain have equal number of customers, higher percentage of German customers seem to be leaving the bank compared to Spain and France. Geography seems to be a factor in customer churn."},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.countplot(x=\"NumOfProducts\", data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,5))\nsns.violinplot(x='NumOfProducts',y='Age',hue='Exited',data=df,palette='Blues', split=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.catplot(x='NumOfProducts', hue='Exited', data=df, kind='count',height=4, aspect=.7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When checking number of products purchased by customer who have stayed, it shows the customers who have been sold/are using more than two products are more at risk of leaving. So upselling/cross-selling beyond two products might not be a good idea. While the number of such customers is comparatively lower, they are still at risk of leaving the bank.\n\nBank should target selling only 2 products to the customers. Any more or less can lead to customer attrition."},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.catplot(x='Exited', hue='IsActiveMember',data=df, kind='count',height=4, aspect=.7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, people who are not active members are more prone to exit."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figsize=(10, 6)\nsns.catplot(x='Exited', hue='Tenure',data=df, kind='count', palette=\"Blues\", height=4, aspect=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"Exited\", data=df, palette=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When comparing the distribution of Customers Exited by Tenure, we can see that tenure of customers is not a significant factor in customer exiting."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,5))\nsns.distplot(df.Age)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" sns.boxplot(x='Age',data=df,palette='Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since Age is a factor in exit of customers, there seem to be a few outliers in the data as seen the above box plot. In this scenario since this is a banking dataset, banks usually have senior customers. These customers are important to the bank as you have seen that older customers tend to have a higher bank balance. We will leave this in for this scenario.\n\nOutliers can adversely impact the performance of the model. If performance of the model is not satisfactory, a potential remedy can to be remove the outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='Exited',y='EstimatedSalary',data=df,palette='hls')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see from the above plot, the distribution of salary of customers that have exited and those that have not, seems to quite similar. Salary does not seem to be an important factor in the exit of customers.\n\n### So based on our Exploratory Data Anaysis, we have identfied below variables as features for our model:\n- RowNumber: Serial Number\n- CustomerId: Customer Unique identifier\n- Surname: Customer Last Name\n- CreditScore - Not Relevant\n- Geography - Relevant\n- Gender - Relevant\n- Age - Relevant\n- Tenure - Not Relevant\n- Balance - Relevant\n- NumOfProducts - Relevant\n- HasCrCard - Not Relevant\n- IsActiveMember - Relevant\n- EstimatedSalary - Not Relevant\n- Exited - Dependant variable\n\n\n# Building the Model\n\n## Step 1: One Hot Encoding\n\nSince algorithms understand only numbers, we will convert the categorical variable into binary using get_dummies method."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cat = pd.get_dummies(df[['Geography','Gender']])\ndf_cat.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Selecting the remaining relevant features into another column."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_num = df[['Age','Balance','NumOfProducts','IsActiveMember']]\ndf_num.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Merging the numeric and categorical data back again and forming a DataFrame of only Feature or Independent Variables and storing this data in variable **X**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_cat.join(df_num)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating an array containing Target or Dependent variable and storing this in variable **y**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df['Exited']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking the Shape or Dimensions of **X**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking the Shape or Dimensions of **y**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting our X and y data into Train and Test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test= train_test_split(X,y, test_size=0.3, stratify = y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking the Shape or Dimensions of our newly created **X_train**, **X_test**, **y_train** and **y_test**\n<br>\nUsing ``stratify`` ensure that the percentage of customer Exiting and Not Exiting are split evenly between Train and Test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 2: Fitting the Algorithms and Evaluating Model Performance\n\nWe will now pass our **X_train** and **y_train** data through the different Classification algorithms. Each algorithm tries to **fit** the train and test data and understand the relationship between all the independent variables in the train dataset (**X**) how it affects the target variable (**y**).\n\nIt then uses this ``fit`` to ``predict`` the target outcome based on independent variables supplied to it.\n\nModel fitting is a measure of how well a machine learning model generalizes to similar data to that on which it was trained. A model that is well-fitted produces more accurate outcomes. A model that is overfitted matches the data too closely and might fail to generalise when exposed to unknown data. A model that is underfitted doesn’t match closely enough.\n\nWe then capture various evaulation metrics for each algorithm for comparison later\n\nWe will also visualise in a **Confusion Matrix** also known as an error matrix. This allows visualization of the performance of an algorithm.\n\n### More information on Confusion Matrix\nhttps://en.wikipedia.org/wiki/Confusion_matrix\n<br>\nhttps://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62\n<br>\nhttps://www.analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning/\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(class_weight='balanced')\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_train)\n\nacc_log = round(lr.score(X_train, y_train) * 100, 2)\nprec_log = round(precision_score(y_train,y_pred) * 100,2)\nrecall_log = round(recall_score(y_train,y_pred) * 100,2)\nf1_log = round(f1_score(y_train,y_pred) * 100,2)\nroc_auc_log = round(roc_auc_score(y_train,y_pred) * 100,2)\n\ny_pred_test = lr.predict(X_test)\n\nacc_log_test = round(lr.score(X_test, y_test) * 100, 2)\nprec_log_test = round(precision_score(y_test,y_pred_test) * 100,2)\nrecall_log_test = round(recall_score(y_test,y_pred_test) * 100,2)\nf1_log_test = round(f1_score(y_test,y_pred_test) * 100,2)\nroc_auc_log_test = round(roc_auc_score(y_test,y_pred_test) * 100,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(acc_log)\nprint(prec_log)\nprint(recall_log)\nprint(f1_log)\nprint(roc_auc_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names = ['True Neg','False Pos','False Neg','True Pos']\ncounts = [\"{0:0.0f}\".format(value) for value in confusion_matrix(y_train, y_pred).flatten()]\npercentages = [\"{0:.2%}\".format(value) for value in confusion_matrix(y_train, y_pred).flatten()/np.sum(confusion_matrix(y_train, y_pred))]\n\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(names,counts,percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(confusion_matrix(y_train, y_pred), annot=labels, fmt='', cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gaussian naïve Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"gnb = GaussianNB()\ngnb.fit(X_train,y_train)\ny_pred = gnb.predict(X_train)\n\nacc_gnb = round(gnb.score(X_train, y_train) * 100, 2)\nprec_gnb = round(precision_score(y_train,y_pred) * 100,2)\nrecall_gnb = round(recall_score(y_train,y_pred) * 100,2)\nf1_gnb = round(f1_score(y_train,y_pred) * 100,2)\nroc_auc_gnb = round(roc_auc_score(y_train,y_pred) * 100,2)\n\ny_pred_test = gnb.predict(X_test)\n\nacc_gnb_test = round(gnb.score(X_test, y_test) * 100, 2)\nprec_gnb_test = round(precision_score(y_test,y_pred_test) * 100,2)\nrecall_gnb_test = round(recall_score(y_test,y_pred_test) * 100,2)\nf1_gnb_test = round(f1_score(y_test,y_pred_test) * 100,2)\nroc_auc_gnb_test = round(roc_auc_score(y_test,y_pred_test) * 100,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(acc_gnb)\nprint(prec_gnb)\nprint(recall_gnb)\nprint(f1_gnb)\nprint(roc_auc_gnb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names = ['True Neg','False Pos','False Neg','True Pos']\ncounts = [\"{0:0.0f}\".format(value) for value in confusion_matrix(y_train, y_pred).flatten()]\npercentages = [\"{0:.2%}\".format(value) for value in confusion_matrix(y_train, y_pred).flatten()/np.sum(confusion_matrix(y_train, y_pred))]\n\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(names,counts,percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(confusion_matrix(y_train, y_pred), annot=labels, fmt='', cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# k-nearest neighbors (KNN)"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_train)\n\nacc_knn = round(knn.score(X_train, y_train) * 100, 2)\nprec_knn = round(precision_score(y_train,y_pred) * 100,2)\nrecall_knn = round(recall_score(y_train,y_pred) * 100,2)\nf1_knn = round(f1_score(y_train,y_pred) * 100,2)\nroc_auc_knn = round(roc_auc_score(y_train,y_pred) * 100,2)\n\ny_pred_test = knn.predict(X_test)\n\nacc_knn_test = round(knn.score(X_test, y_test) * 100, 2)\nprec_knn_test = round(precision_score(y_test,y_pred_test) * 100,2)\nrecall_knn_test = round(recall_score(y_test,y_pred_test) * 100,2)\nf1_knn_test = round(f1_score(y_test,y_pred_test) * 100,2)\nroc_auc_gnb_test = round(roc_auc_score(y_test,y_pred_test) * 100,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(acc_knn)\nprint(prec_knn)\nprint(recall_knn)\nprint(f1_knn)\nprint(roc_auc_knn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names = ['True Neg','False Pos','False Neg','True Pos']\ncounts = [\"{0:0.0f}\".format(value) for value in confusion_matrix(y_train, y_pred).flatten()]\npercentages = [\"{0:.2%}\".format(value) for value in confusion_matrix(y_train, y_pred).flatten()/np.sum(confusion_matrix(y_train, y_pred))]\n\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(names,counts,percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(confusion_matrix(y_train, y_pred), annot=labels, fmt='', cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, y_train)\ny_pred = decision_tree.predict(X_train)\n\nacc_decision_tree = round(decision_tree.score(X_train, y_train) * 100, 2)\nprec_decision_tree = round(precision_score(y_train,y_pred) * 100,2)\nrecall_decision_tree = round(recall_score(y_train,y_pred) * 100,2)\nf1_decision_tree = round(f1_score(y_train,y_pred) * 100,2)\nroc_auc_decision_tree = round(roc_auc_score(y_train,y_pred) * 100,2)\n\ny_pred_test = decision_tree.predict(X_test)\n\nacc_decision_tree_test = round(decision_tree.score(X_test, y_test) * 100, 2)\nprec_decision_tree_test = round(precision_score(y_test,y_pred_test) * 100,2)\nrecall_decision_tree_test = round(recall_score(y_test,y_pred_test) * 100,2)\nf1_decision_tree_test = round(f1_score(y_test,y_pred_test) * 100,2)\nroc_auc_decision_tree_test = round(roc_auc_score(y_test,y_pred_test) * 100,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(acc_decision_tree)\nprint(prec_decision_tree)\nprint(recall_decision_tree)\nprint(f1_decision_tree)\nprint(roc_auc_decision_tree)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names = ['True Neg','False Pos','False Neg','True Pos']\ncounts = [\"{0:0.0f}\".format(value) for value in confusion_matrix(y_train, y_pred).flatten()]\npercentages = [\"{0:.2%}\".format(value) for value in confusion_matrix(y_train, y_pred).flatten()/np.sum(confusion_matrix(y_train, y_pred))]\n\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(names,counts,percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(confusion_matrix(y_train, y_pred), annot=labels, fmt='', cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, y_train)\ny_pred = random_forest.predict(X_train)\n\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\nprec_random_forest = round(precision_score(y_train,y_pred) * 100,2)\nrecall_random_forest = round(recall_score(y_train,y_pred) * 100,2)\nf1_random_forest = round(f1_score(y_train,y_pred) * 100,2)\nroc_auc_random_forest = round(roc_auc_score(y_train,y_pred) * 100,2)\n\ny_pred_test = random_forest.predict(X_test)\n\nacc_random_forest_test = round(random_forest.score(X_test, y_test) * 100, 2)\nprec_random_forest_test = round(precision_score(y_test,y_pred_test) * 100,2)\nrecall_random_forest_test = round(recall_score(y_test,y_pred_test) * 100,2)\nf1_random_forest_test = round(f1_score(y_test,y_pred_test) * 100,2)\nroc_auc_random_forest_test = round(roc_auc_score(y_test,y_pred_test) * 100,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(acc_random_forest)\nprint(prec_random_forest)\nprint(recall_random_forest)\nprint(f1_random_forest)\nprint(roc_auc_random_forest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names = ['True Neg','False Pos','False Neg','True Pos']\ncounts = [\"{0:0.0f}\".format(value) for value in confusion_matrix(y_train, y_pred).flatten()]\npercentages = [\"{0:.2%}\".format(value) for value in confusion_matrix(y_train, y_pred).flatten()/np.sum(confusion_matrix(y_train, y_pred))]\n\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(names,counts,percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(confusion_matrix(y_train, y_pred), annot=labels, fmt='', cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost\nWhy and What is XBGoost?\n<br>\nMore information: https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define Model parameters to tune\nmodel_parameters = { \n        'n_estimators':[10, 50, 100, 200, 500, 750, 100],\n        'max_depth': [3, 5, 10],\n        'min_samples_leaf': [np.random.randint(1,10)],\n        'max_features': [None, 'sqrt', 'log2']\n                  }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gridsearch the parameters to find the best parameters. Using L2 penalty\nmodel = XGBClassifier()\ngscv = GridSearchCV(estimator=model, \n                    param_grid=model_parameters, \n                    cv=5, \n                    verbose=1, \n                    n_jobs=-1,\n                    scoring='roc_auc')\n\ngscv.fit(X_train, y_train)  ## Model building ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The best parameter are -', gscv.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Re-fit the model with the best parameters\nfinal_mod = XGBClassifier(**gscv.best_params_)\nfinal_mod.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction\ntrain_pred = final_mod.predict(X_train)\ntest_pred = final_mod.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_xgb = round(accuracy_score(y_train, train_pred) * 100, 2)\nprec_xgb = round(precision_score(y_train, train_pred, average='macro') * 100, 2)\nrecall_xgb = round(recall_score(y_train, train_pred, average='macro') * 100, 2)\nf1_xgb = round(f1_score(y_train, train_pred, average='macro') * 100, 2)\nroc_auc_xgb = round(roc_auc_score(y_train, train_pred, average='macro') * 100, 2)\n\nacc_xgb_test = round(accuracy_score(y_test, test_pred) * 100, 2)\nprec_xgb_test = round(precision_score(y_test, test_pred, average='macro') * 100, 2)\nrecall_xgb_test = round(recall_score(y_test, test_pred, average='macro') * 100, 2)\nf1_xgb_test = round(f1_score(y_test, test_pred, average='macro') * 100, 2)\nroc_auc_xgb_test = round(roc_auc_score(y_test, test_pred, average='macro') * 100, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names = ['True Neg','False Pos','False Neg','True Pos']\ncounts = [\"{0:0.0f}\".format(value) for value in confusion_matrix(y_train, train_pred).flatten()]\npercentages = [\"{0:.2%}\".format(value) for value in confusion_matrix(y_train, train_pred).flatten()/np.sum(confusion_matrix(y_train, train_pred))]\n\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(names,counts,percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(confusion_matrix(y_train, train_pred), annot=labels, fmt='', cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 3: Model Comparison\n\n# Results - Using Training Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"results = pd.DataFrame({\n    'Model': ['Logistic Regression','Gaussian NB','k-Nearest Neighbor','Decision Tree','Random Forest', 'XGBoost'],\n    'Accuracy': [ acc_log, acc_gnb,acc_knn, acc_decision_tree, acc_random_forest, acc_xgb],\n    'Precision': [prec_log, prec_gnb, prec_knn, prec_decision_tree, prec_random_forest, prec_xgb],\n    'Recall': [recall_log, recall_gnb, recall_knn, recall_decision_tree, recall_random_forest, recall_xgb],\n    'F1 Score': [f1_log, f1_gnb, f1_knn, f1_decision_tree, f1_random_forest, f1_xgb],\n    'ROC-AUC': [roc_auc_log, roc_auc_gnb, roc_auc_knn, roc_auc_decision_tree, roc_auc_random_forest, roc_auc_xgb],\n})\nresult_df = results.sort_values(by=['Accuracy','Recall','Precision','F1 Score'], ascending=[False,False,False,False])\nresult_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\n\n# set width of bar\nbarWidth = 0.15\n \n# Set position of bar on X axis\nr1 = np.arange(len(result_df))\nr2 = [x + barWidth for x in r1]\nr3 = [x + barWidth for x in r2]\nr4 = [x + barWidth for x in r3]\nr5 = [x + barWidth for x in r4]\n \n# Make the plot\nplt.bar(r1, result_df['Accuracy'], color= 'seagreen', width=barWidth, edgecolor='white', label='Accuracy')\nplt.bar(r2, result_df['Precision'], color= 'darkslateblue', width=barWidth, edgecolor='white', label='Precision')\nplt.bar(r3, result_df['Recall'], color= 'dodgerblue', width=barWidth, edgecolor='white', label='Recall')\nplt.bar(r4, result_df['F1 Score'], color= 'maroon', width=barWidth, edgecolor='white', label='F1 Score')\nplt.bar(r5, result_df['ROC-AUC'], color= 'darkgoldenrod', width=barWidth, edgecolor='white', label='ROC-AUC')\n\n \n# Add xticks on the middle of the group bars\nplt.xlabel('group', fontweight='bold')\nplt.xticks([r + barWidth for r in range(len(result_df))], result_df.Model)\n\n# displaying the title \nplt.title(label='Train Dataset Results', \n          fontsize=25, \n          color=\"black\") \n# Create legend & Show graphic\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results - Using Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"results_test = pd.DataFrame({\n    'Model': ['Logistic Regression','Gaussian NB','k-Nearest Neighbor', 'Decision Tree', 'Random Forest', 'XGBoost'],\n    'Accuracy': [acc_log_test ,acc_gnb_test ,acc_knn_test ,acc_decision_tree_test ,acc_random_forest_test, acc_xgb_test],\n    'Precision': [prec_log_test ,prec_gnb_test ,prec_knn_test ,prec_decision_tree_test ,prec_random_forest_test, prec_xgb_test],\n    'Recall': [recall_log_test ,recall_gnb_test ,recall_knn_test ,recall_decision_tree_test ,recall_random_forest_test, recall_xgb_test],\n    'F1 Score': [f1_log_test ,f1_gnb_test ,f1_knn_test ,f1_decision_tree_test ,f1_random_forest_test, f1_xgb_test],\n    'ROC-AUC': [roc_auc_log_test ,roc_auc_gnb_test ,roc_auc_gnb_test ,roc_auc_decision_tree_test ,roc_auc_random_forest_test, roc_auc_xgb_test],\n})\nresult_df_test = results_test.sort_values(by=['Accuracy','Recall','Precision','F1 Score'], ascending=[False,False,False,False])\nresult_df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\n\n# set width of bar\nbarWidth = 0.15\n \n# Set position of bar on X axis\nr1 = np.arange(len(result_df_test))\nr2 = [x + barWidth for x in r1]\nr3 = [x + barWidth for x in r2]\nr4 = [x + barWidth for x in r3]\nr5 = [x + barWidth for x in r4]\n \n# Make the plot\nplt.bar(r1, result_df_test['Accuracy'], color= 'seagreen', width=barWidth, edgecolor='white', label='Accuracy')\nplt.bar(r2, result_df_test['Precision'], color= 'darkslateblue', width=barWidth, edgecolor='white', label='Precision')\nplt.bar(r3, result_df_test['Recall'], color= 'dodgerblue', width=barWidth, edgecolor='white', label='Recall')\nplt.bar(r4, result_df_test['F1 Score'], color= 'maroon', width=barWidth, edgecolor='white', label='F1 Score')\nplt.bar(r5, result_df_test['ROC-AUC'], color= 'darkgoldenrod', width=barWidth, edgecolor='white', label='ROC-AUC')\n\n \n# Add xticks on the middle of the group bars\nplt.xlabel('group', fontweight='bold')\nplt.xticks([r + barWidth for r in range(len(result_df_test))], result_df_test.Model)\n\n# displaying the title \nplt.title(label='Test Dataset Results', \n          fontsize=25, \n          color=\"black\") \n# Create legend & Show graphic\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## What do these scores mean?\n### Lets look at the meaning and calculations of all these evaluation parameters.\n#### Accuracy:\nAccuracy calculates the total number of instances where prediction was correctly made as positive or negative compared to total number of observations.\n-Most useful when the data is balanced\nWhat to do if in this case model classifies all as **Not Exited**? Since almost 80% of our data is that of customers **Not Exited**, if all customers are labeled as **Not Exited**, the model will fail its purpose. To mitigate this issue, we look at other parameters.\n\n#### Overall Accuracy = (TN+TP)/N\n\n#### Precision:\nPrecision calculates the total instances where out of those predicted as positive, how many of them are actual positive. The question that this metric answer is of all customers that labeled as Exited, how many actually Exited?\n\n####Recall (a.k.a. Sensitivity or True Positive Rate):\nRecall calculates the total number of instances where prediction was correctly made as positive compared to total number of actual positive observations in the data. The question recall answers is: Of all the customers that actually Exited, how many did we label correctly?\n\nThis is even more important in cases like Spam filteration where user can miss an email if it is mistakenly classified as spam.\n\n#### Sensitivity = TP/(TP+FN)\n\n#### F1 Score or F-Beta Score:\nF1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. F1 Score is needed when you want to seek a balance between Precision and Recall.\n\n#### ROC - AUC:\nThe Receiver Operator Characteristic (ROC) curve is an evaluation metric for binary classification problems. The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve.\n\n#### More Information:\nhttps://towardsdatascience.com/whats-the-deal-with-accuracy-precision-recall-and-f1-f5d8b4db1021\n<br>\nhttps://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9\n<br>\nhttps://www.analyticsvidhya.com/blog/2020/06/auc-roc-curve-machine-learning/\n\n# Step 4: Further Tuning Hyperparameters\n## Logistic Regression - Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"logopt1 = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = [    \n    {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n    'C' : np.logspace(-4, 4, 20),\n    'solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],\n    'max_iter' : [100, 1000,2500, 5000]\n    }\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = GridSearchCV(logopt1, param_grid = param_grid, cv = 3, verbose=True, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_clf = clf.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_clf.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (f'Accuracy - : {best_clf.score(X,y):.3f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logopt2 = LogisticRegression(C=0.03359818286283781, max_iter=2500, penalty='l1',solver='liblinear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logopt2.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_lr_pred = logopt2.predict(X_train)\ntest_lr_pred = logopt2.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_lropt2 = round(accuracy_score(y_train, train_lr_pred) * 100, 2)\nprec_lropt2 = round(precision_score(y_train, train_lr_pred, average='macro') * 100, 2)\nrecall_lropt2 = round(recall_score(y_train, train_lr_pred, average='macro') * 100, 2)\nf1_lropt2 = round(f1_score(y_train, train_lr_pred, average='macro') * 100, 2)\nroc_auc_lropt2 = round(roc_auc_score(y_train, train_lr_pred, average='macro') * 100, 2)\n\nacc_lropt2_test = round(accuracy_score(y_test, test_lr_pred) * 100, 2)\nprec_lropt2_test = round(precision_score(y_test, test_lr_pred, average='macro') * 100, 2)\nrecall_lropt2_test = round(recall_score(y_test, test_lr_pred, average='macro') * 100, 2)\nf1_lropt2_test = round(f1_score(y_test, test_lr_pred, average='macro') * 100, 2)\nroc_auc_lropt2_test = round(roc_auc_score(y_test, test_lr_pred, average='macro') * 100, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optres = pd.DataFrame({\n    'Parameter': ['Accuracy','Precision','Recall', 'F1 Score', 'ROC-AUC'],\n    'Optimised on Train': [acc_lropt2 ,prec_lropt2 ,recall_lropt2 ,f1_lropt2 ,roc_auc_lropt2],\n    'Optimised on Test': [acc_lropt2_test ,prec_lropt2_test ,recall_lropt2_test ,f1_lropt2_test ,roc_auc_lropt2_test],\n    'Original on Train': [acc_log ,prec_log ,recall_log ,f1_log ,roc_auc_log],\n    'Original on Test': [acc_log_test ,prec_log_test ,recall_log_test ,f1_log_test ,roc_auc_log_test]\n})\n# result_df_test = results_test.sort_values(by=['Accuracy','Recall','Precision','F1 Score'], ascending=[False,False,False,False])\noptres","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logopt3 = LogisticRegression(C=0.03359818286283781, max_iter=2500, penalty='l2',solver='newton-cg')\nlogopt3.fit(X_train,y_train)\ntrain_lr_pred = logopt3.predict(X_train)\ntest_lr_pred = logopt3.predict(X_test)\n\nacc_lropt3 = round(accuracy_score(y_train, train_lr_pred) * 100, 2)\nprec_lropt3 = round(precision_score(y_train, train_lr_pred, average='macro') * 100, 2)\nrecall_lropt3 = round(recall_score(y_train, train_lr_pred, average='macro') * 100, 2)\nf1_lropt3 = round(f1_score(y_train, train_lr_pred, average='macro') * 100, 2)\nroc_auc_lropt3 = round(roc_auc_score(y_train, train_lr_pred, average='macro') * 100, 2)\n\nacc_lropt3_test = round(accuracy_score(y_test, test_lr_pred) * 100, 2)\nprec_lropt3_test = round(precision_score(y_test, test_lr_pred, average='macro') * 100, 2)\nrecall_lropt3_test = round(recall_score(y_test, test_lr_pred, average='macro') * 100, 2)\nf1_lropt3_test = round(f1_score(y_test, test_lr_pred, average='macro') * 100, 2)\nroc_auc_lropt3_test = round(roc_auc_score(y_test, test_lr_pred, average='macro') * 100, 2)\n\noptres3 = pd.DataFrame({\n    'Parameter': ['Accuracy','Precision','Recall', 'F1 Score', 'ROC-AUC'],\n    'Optimised Newton CG Solver L2 on Train': [acc_lropt3 ,prec_lropt3 ,recall_lropt3 ,f1_lropt3 ,roc_auc_lropt3],\n    'Optimised Newton CG Solver L2 on Test': [acc_lropt3_test ,prec_lropt3_test ,recall_lropt3_test ,f1_lropt3_test ,roc_auc_lropt3_test],\n    'Optimised liblinear Solver L1 on Train': [acc_lropt2 ,prec_lropt2 ,recall_lropt2 ,f1_lropt2 ,roc_auc_lropt2],\n    'Optimised liblinear Solver L1 on Test': [acc_lropt2_test ,prec_lropt2_test ,recall_lropt2_test ,f1_lropt2_test ,roc_auc_lropt2_test],\n    'Original on Train': [acc_log ,prec_log ,recall_log ,f1_log ,roc_auc_log],\n    'Original on Test': [acc_log_test ,prec_log_test ,recall_log_test ,f1_log_test ,roc_auc_log_test]\n})\n# result_df_test = results_test.sort_values(by=['Accuracy','Recall','Precision','F1 Score'], ascending=[False,False,False,False])\noptres3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we can see the Logistic Regression model Optimised using the best estimators provided by GridSearchCV has performed the best. We can tweak futher but we can only improve certain parameters but that might cause reduction in performance in other parameters**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}