{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nimport itertools\n\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\n\nfrom sklearn.linear_model import Lasso, LogisticRegression # For LASSO & Logistic \nfrom sklearn.metrics import mean_squared_error # For evaluation\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn import linear_model\nfrom sklearn.metrics import f1_score, accuracy_score, confusion_matrix,roc_curve, roc_auc_score, precision_score, recall_score, precision_recall_curve\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn import neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeRegressor   #Decision Tree Regressor\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nimport warnings # Suppress warnings because they are annoying\nwarnings.filterwarnings('ignore') \n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-08T23:18:53.057144Z","iopub.execute_input":"2021-06-08T23:18:53.057592Z","iopub.status.idle":"2021-06-08T23:18:55.325785Z","shell.execute_reply.started":"2021-06-08T23:18:53.057493Z","shell.execute_reply":"2021-06-08T23:18:55.32404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/california-housing-prices/housing.csv')\ndata = data.dropna(axis = 0) #remove missing values \nprint('Final data has ',data.shape[0],'rows and ',data.shape[1],' columns') \ndata = pd.get_dummies(data, columns = ['ocean_proximity'], drop_first = True) # create dummy variables for categorical variable\n\n# #Define X & Y variables for model building\nX = data.copy().drop(['median_house_value'], axis =1 )\ny_cont = data['median_house_value'] \n\nprint('######################### LINEAR REGRESSION #########################')\nX_int = sm.add_constant(X)\nlinreg = sm.OLS(y_cont, X_int).fit()\nprint(linreg.summary())","metadata":{"execution":{"iopub.status.busy":"2021-06-08T23:28:42.278566Z","iopub.execute_input":"2021-06-08T23:28:42.278959Z","iopub.status.idle":"2021-06-08T23:28:42.452565Z","shell.execute_reply.started":"2021-06-08T23:28:42.278923Z","shell.execute_reply":"2021-06-08T23:28:42.451399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Linear Regression Model Analysis\n# Has Rsq of 0.64. Ok model\n# All non Categorical variables are significant. Only categorical variable\n# ocean_proximity_NEAR BAY has a igh p value 0.039 implying that this\n# variable is not significant when compared to ocean_proximity base case (<1H OCEAN)\n# housing_median_age has the biggest impact on median_house_values, followed by\n# total_bedrooms, households and population in decreasing order","metadata":{"execution":{"iopub.status.busy":"2021-06-03T20:10:30.55774Z","iopub.status.idle":"2021-06-03T20:10:30.558193Z"}}},{"cell_type":"code","source":"# Model Assessment\nprint(data.corr()) #check correlation between variables \n\nplt.scatter(linreg.fittedvalues, linreg.resid) #Residual vs fitted value plot\nplt.xlabel('Fitted Values')\nplt.ylabel('Residuals')\nplt.title('Residuals vs Fitted Values')\nplt.show()\n\n# # QQ Plot\nqqplot = sm.qqplot(linreg.resid)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T20:10:30.559616Z","iopub.status.idle":"2021-06-03T20:10:30.560215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#There's a high correlation between longitude and latitude and longitude and ocean_proximity_NEAR BAY[]\nClearly the residual plot has a non random pattern, which shows the data is not linear.\nThis can be further confirmed by looking at it's QQ plot, which is not a straight line. \nSince the residual plot is concentrated to the top right, it shows non linear relationship between the variables. According to Tukey's transformation of variables, the residual graph pattern suggests \nX (preferable), increasing the power of variable to nth degree might result in a better looking residual graph. value of n is decided by trying various values. \nLet's check which variables do not have a linear relationship and need transformation with plots below \n","metadata":{}},{"cell_type":"code","source":"data['median_house_cat'] = np.where(data['median_house_value']< np.median(data['median_house_value']),0,1)\ny_cat = data['median_house_cat']\nX = data.copy().drop(['median_house_value', 'median_house_cat'], axis =1 )\n\nprint('######################### LOGISTIC REGRESSION #########################')\n\nlogit = LogisticRegression()\nlogit.fit(X,y_cat)\nprint(logit.intercept_)\nprint(pd.DataFrame(zip(logit.coef_, X.columns)))\n","metadata":{"execution":{"iopub.status.busy":"2021-06-03T20:10:30.561495Z","iopub.status.idle":"2021-06-03T20:10:30.562078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interpreting coeficients: \n\n longitude: A unit change in longitude is associated with decrease in the odds of getting median house value greater than meadian value, decrease by -2.681e+04 times\n\nhousing median age: A unit change in housing median age is associated with increase in the odds of getting median house value greater than median value 1072.5200 times\n\npopulation: A unit change in housing median age is associated with decrease in the odds of getting median house value greater than median value -37.9691 times\n","metadata":{}},{"cell_type":"code","source":"print('######################### PREDICTIONS #########################')\n# Splitting data into train_valid, test data set with 20% test set\nX_train_valid, X_test, y_train_valid, y_test = train_test_split(X, y_cont, test_size=0.2, random_state = 283)\n\n# # #Further splitting the data into train and valid set\nX_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size = 0.37, random_state = 283) #further splitting into train and validation\n\nprint('######################### LASSO MODEL #########################')\n# Use StandardScalar to scale values to perform lasso regularisation\nss = StandardScaler().fit(X_train)\nX_train_lasso = pd.DataFrame(ss.transform(X_train))\nX_valid_lasso = pd.DataFrame(ss.transform(X_valid))\n\n# # Set up lambda/alpha candidate values \nalphas = np.logspace(-10, 10, 21) # We will use lambda on powers of 10 scale\nValidation_Scores = []\nfor a in alphas:\n    lm = linear_model.Lasso(alpha=a)\n    lm.fit(X_train_lasso, y_train) # Fit model on training set\n    Validation_Scores.append(metrics.mean_squared_error(lm.predict(X_valid_lasso), y_valid)) # Evaluate model on validation set\n# Find the minimum validation error, and it's minimizer\nmin_alpha = alphas[np.argmin(Validation_Scores)]\nprint('Best Alpha value is ', min_alpha)\n\nss = StandardScaler().fit(X_train_valid)\nX_train_valid_lasso = pd.DataFrame(ss.transform(X_train_valid))\nX_test_lasso = pd.DataFrame(ss.transform(X_test))\n# # # Refit Lasso model with selected alpha value\nlm = linear_model.Lasso(alpha = min_alpha)\nlm.fit(X_train_valid_lasso, y_train_valid)\nprint(pd.DataFrame(zip(lm.coef_,X.columns)))\nprint(\"The MSE on the test set is\", metrics.mean_squared_error(lm.predict(X_test_lasso), y_test))\n","metadata":{"execution":{"iopub.status.busy":"2021-06-08T23:29:19.456057Z","iopub.execute_input":"2021-06-08T23:29:19.456561Z","iopub.status.idle":"2021-06-08T23:29:22.617548Z","shell.execute_reply.started":"2021-06-08T23:29:19.456522Z","shell.execute_reply":"2021-06-08T23:29:22.615676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('######################### RANDOM FOREST #########################')\n#Tuning depth and number of trees using gridsearch\nn_estimators = [50,100,150,200,250,300]\nmax_depth= range(10,20)\n# features = range(1, 14)\nhyperparameter_triplets = list(itertools.product(n_estimators, max_depth)) #create unique triplet pairs using itertools.product\nvalidation_scores = [] #initialise list to store MSE validation scores\nfor index, triplets in enumerate(hyperparameter_triplets): #iterate on each hyperparameter triplet \n    rf = RandomForestRegressor(n_estimators = triplets[0], max_depth = triplets[1]) #Build RandomForest model for the triplets\n    rf.fit(X_train, y_train) # Fit model on training set\n    mse = metrics.mean_squared_error(rf.predict(X_valid), y_valid) #Calculate MSE on Validation set\n    validation_scores.append(mse)\n\nbest_triplet = hyperparameter_triplets[np.argmin(validation_scores)]\nprint('Final Tunes Parameters are : n_estimators: ', best_triplet[0], 'max_depth: ', best_triplet[1])\nrf = RandomForestRegressor(n_estimators = best_triplet[0], max_depth = best_triplet[1])\nrf.fit(X_train_valid, y_train_valid)\nprint('MSE of Random Forest Model on Test set is ',mean_squared_error(rf.predict(X_test), y_test))\nprint(sorted(zip(rf.feature_importances_,X.columns.values), reverse = True))","metadata":{"execution":{"iopub.status.busy":"2021-06-03T20:10:30.565938Z","iopub.status.idle":"2021-06-03T20:10:30.566562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"print('######################### COMPARING LASSO & RANDOM FOREST #########################')\nprint('Final Tunes Parameters are : n_estimators: ', best_triplet[0], 'max_depth: ', best_triplet[1],'best features: ',best_triplet[2])\n\n\nprint('MSE of Random Forest Model on Test set is ',mean_squared_error(rf.predict(X_test), y_test))\n\nBest Alpha for Lasso is 100 & MSE Lasso is 4689741469 \n\nBest n_estimators: 150 & max_depth: 19 & MSE Random Forest is 2545876926.0074525 which is almost half of Lasso's\nAccording to Random forest Median Income is the most important variable(0.48), In Lasso it has the highest coeficient value of 74127\n\nSecond is ocean_proximity_INLAND, in lasso it has a coef of -19856.644608\n\nFollowed by longitude and latitude, with lasso's regression coef values of -47738.5 and -48245 respectively. These are the most sentisite coefs","metadata":{}},{"cell_type":"code","source":"\nprint('######################### KNN CLASSIFIER MODEL #########################')\n# Splitting data into train_valid, test data set with 20% test set\ndata['median_house_cat'] = np.where(data['median_house_value']< np.median(data['median_house_value']),0,1)\ny_cat = data['median_house_cat']\nX_train_valid, X_test, y_train_valid, y_test = train_test_split(X, y_cat, test_size=0.2, random_state = 283)\n\nn_neighbors = np.arange(1,21,3)\nleaf_size = list(range(1,50))\nkfold = KFold(5, False) \nhyperparameter_couple = list(itertools.product(n_neighbors, leaf_size))\nvalid_precision = []\nfor index, couple in enumerate(hyperparameter_couple): #iterate on each hyperparameter triplet \n    valid_precision_tmp = []\n    for train_index, valid_index in kfold.split(X_train_valid): #splitting into train, valid set\n        X_train, Y_train = X_train_valid.iloc[train_index], y_train_valid.iloc[train_index] # Training set\n        X_valid, Y_valid = X_train_valid.iloc[valid_index], y_train_valid.iloc[valid_index] # Validation set\n        ss = StandardScaler().fit(X_train)\n        X_train = pd.DataFrame(ss.transform(X_train))\n        X_valid = pd.DataFrame(ss.transform(X_valid))\n        \n        knn = neighbors.KNeighborsClassifier(n_neighbors = couple[0], leaf_size = couple[1])\n        knn.fit(X_train, Y_train)\n        y_hat = knn.predict(X_valid)\n        score = metrics.precision_score(Y_valid, y_hat)\n        valid_precision_tmp.append(metrics.precision_score(Y_valid, y_hat))\n    valid_precision.append(np.mean(valid_precision_tmp))\n\nbest_couple = hyperparameter_couple[np.argmax(valid_precision)+1]\nbestK = best_couple[0]\nbest_leaf_size = best_couple[1]\n\n# # Calculating final precision on the Tesing Set\n# # KNN Scale Data\nss = StandardScaler().fit(X_train_valid)\nX_train_valid_knn = pd.DataFrame(ss.transform(X_train_valid))\nX_test_knn = pd.DataFrame(ss.transform(X_test))\n\nknn = neighbors.KNeighborsClassifier(n_neighbors = bestK, leaf_size =  best_leaf_size)\nknn.fit(X_train_valid_knn, y_train_valid)\ny_hat = knn.predict(X_test_knn)\nscore = metrics.precision_score(y_test, y_hat)\nprint('Precision Score of KNN Classifier is ',score) # ~ 0.90\n\nprint('######################### DECISION TREE MODEL #########################')\nmax_depth = list(range(2,50))\nleaf_nodes = list(range(2,50))\nkfold = KFold(5, False) \nhyperparameter_couple = list(itertools.product(max_depth, leaf_nodes))\nvalid_precision = []\nfor index, couple in enumerate(hyperparameter_couple): #iterate on each hyperparameter triplet \n    valid_precision_tmp = []\n    for train_index, valid_index in kfold.split(X_train_valid): #splitting into train, valid set\n        X_train, Y_train = X_train_valid.iloc[train_index], y_train_valid.iloc[train_index] # Training set\n        X_valid, Y_valid = X_train_valid.iloc[valid_index], y_train_valid.iloc[valid_index] # Validation set\n        tree_clf = DecisionTreeClassifier(max_depth = couple[0], max_leaf_nodes = couple[1])\n        tree_clf.fit(X_train, Y_train)\n        y_hat = tree_clf.predict(X_valid)\n        score = metrics.precision_score(Y_valid, y_hat)\n        valid_precision_tmp.append(metrics.precision_score(Y_valid, y_hat))\n    valid_precision.append(np.mean(valid_precision_tmp))\n\nbest_couple = hyperparameter_couple[np.argmax(valid_precision)+1]\nbest_max_depth = best_couple[0]\nbest_leaf_node = best_couple[1]\n\n# # # Calculating final precision on the Tesing Set\ntree_clf = DecisionTreeClassifier(max_depth= best_max_depth, max_leaf_nodes= best_leaf_node)\ntree_clf.fit(X_train_valid, y_train_valid)\ny_hat = tree_clf.predict(X_test)\nscore = metrics.precision_score(y_test, y_hat)\nprint('Precision Score of KNN Classifier is ',score) # ~ 0.91\n\nprint('######################### BOOSTED TREE MODEL #########################')\nmax_depth = list(range(2,6))\nn_estimators =  np.linspace(100, 500, 10, dtype = int)\nkfold = KFold(5, False) \nhyperparameter_couple = list(itertools.product(max_depth, n_estimators))\nprint(\"Size is \", len(hyperparameter_couple))\nvalid_precision = []\niter = 0\nfor index, couple in enumerate(hyperparameter_couple): #iterate on each hyperparameter triplet \n    valid_precision_tmp = []\n    for train_index, valid_index in kfold.split(X_train_valid): #splitting into train, valid set\n        X_train, Y_train = X_train_valid.iloc[train_index], y_train_valid.iloc[train_index] # Training set\n        X_valid, Y_valid = X_train_valid.iloc[valid_index], y_train_valid.iloc[valid_index] # Validation set\n        gbc_clf = GradientBoostingClassifier(max_depth= couple[0], n_estimators = couple[1])\n        gbc_clf.fit(X_train, Y_train)\n        y_hat = gbc_clf.predict(X_valid)\n        score = metrics.precision_score(Y_valid, y_hat)\n        valid_precision_tmp.append(metrics.precision_score(Y_valid, y_hat))\n    valid_precision.append(np.mean(valid_precision_tmp))\n    iter += 1\n    print(\"Done \", iter)\n\nbest_couple = hyperparameter_couple[np.argmax(valid_precision)]\nbest_max_depth = best_couple[0]\nbest_n_estimator = best_couple[1]\n\n#Tesing Set\ntree_clf = DecisionTreeClassifier(max_depth= best_max_depth, n_estimators = best_n_estimator )\ntree_clf.fit(X_train_valid, y_train_valid)\ny_hat = tree_clf.predict(X_test)\nscore = metrics.precision_score(y_test, y_hat)\nprint('Precision Score of BOOSTED TREE MODEL is ',score) # ~ 0.\n\n# print('######################### PCA #########################')\n\ndata = data_orig.dropna(axis = 0)\ndata = pd.get_dummies(data, columns = ['ocean_proximity']) \n\nX = data.copy().drop(['median_house_value'], axis =1 )\ny_cont = data['median_house_value']\n\nX_scale = StandardScaler().fit_transform(X)\npca = PCA(0.9) # Use enough PC to capture 90% of the variability\npca.fit(X_scale) \nX_trans = pca.transform(X_scale)\nprint(X_trans.shape[1], ' principal components are needed to cover 90% variability for this data') \n# 7 principal components are needed to cover 90% variability for this data\n\n#Scree Plot \nplt.plot(range(1, 8), np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Proportion of Variance Explained')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-08T23:31:55.514787Z","iopub.execute_input":"2021-06-08T23:31:55.51523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}