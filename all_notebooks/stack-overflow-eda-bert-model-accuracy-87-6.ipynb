{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import re\nimport string\n\n# Reset the output dimensions\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import decomposition\nfrom sklearn.svm import LinearSVC, NuSVC, SVC\nfrom sklearn.metrics import f1_score, accuracy_score, hamming_loss\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_extraction import text\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n\nfrom skmultilearn.problem_transform import BinaryRelevance, LabelPowerset\n\nfrom scipy import linalg\nimport networkx as nx\n\nfrom collections import Counter, defaultdict\nimport pickle\n\nimport nltk\nnltk.download('wordnet')\nfrom nltk import stem\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer\n\nfrom gensim import matutils, models\nfrom gensim.models import Word2Vec\nfrom wordcloud import WordCloud\n\n\nimport scipy.sparse\n\nfrom wordcloud import WordCloud, STOPWORDS\nimport seaborn as sns\n\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n\nimport squarify\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n! pip install apyori\nfrom apyori import apriori\n\nfrom mlxtend.preprocessing import TransactionEncoder\nfrom multiprocessing import  Pool\n\nimport transformers\n\nimport tensorflow as tf\nfrom tqdm import tqdm\n\nplt.rcParams['figure.figsize'] = [24, 12]\nplt.style.use('seaborn-darkgrid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout\nfrom tensorflow.keras.layers import SpatialDropout1D\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.optimizers import Adam\nfrom keras.initializers import Constant","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\n\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS available: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/60k-stack-overflow-questions-with-quality-rate/data.csv')\ntrain.columns = train.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n\nlang = pd.read_csv('/kaggle/input/programming-languages/languages.csv')\nlang = lang[['name']]\n\nprint('Train Data shape: ', train.shape)\n\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['tags'] = train['tags'].str.replace('>',',')\ntrain['tags'] = train['tags'].str.replace('<','')\nlang['name'] = lang['name'].apply(lambda x: x.lower())\nlang.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['y'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(ncols = 2, figsize = (17, 4), dpi = 100)\nplt.tight_layout()\n\ntrain.groupby('y').count()['id'].plot(kind = 'pie', ax = axes[0], labels = ['High Quality (92.9%)', 'Low Quality (Closed) (7.1%)', 'Low Quality (Multiple Edits)'])\nsns.countplot(x = train['y'], hue = train['y'], ax = axes[1])\n\naxes[0].set_ylabel('')\naxes[1].set_ylabel('')\naxes[1].set_xticklabels(['High Quality', 'Low Quality (Closed)', 'Low Quality (Multiple Edits)'])\naxes[0].tick_params(axis = 'x', labelsize = 15)\naxes[0].tick_params(axis = 'y', labelsize = 15)\naxes[1].tick_params(axis = 'x', labelsize = 15)\naxes[1].tick_params(axis = 'y', labelsize = 15)\n\naxes[0].set_title('Label Distribution in Training Set', fontsize = 13)\naxes[1].set_title('Label Count in Training Set', fontsize = 13)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Finding top categories and sub categories which are mostly askede by the users"},{"metadata":{"trusted":true},"cell_type":"code","source":"tags = train[['tags']]\ntags = pd.concat([tags[['tags']], tags['tags'].str.split(',', expand = True)], axis = 1)\ntags = tags.drop('tags', axis = 1)\ntags.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Most popular technologies"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (6, 6)\nwordcloud = WordCloud(background_color = 'white', width = 1200,  height = 1200, max_words = 121).generate(str(tags[0]))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.title('Most Popular technologies',fontsize = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Frequency plot of most popular languages"},{"metadata":{"trusted":true},"cell_type":"code","source":"# looking at the frequency of most popular items \n\nplt.rcParams['figure.figsize'] = (18, 7)\ncolor = plt.cm.copper(np.linspace(0, 1, 40))\ntags[0].value_counts().head(40).plot.bar(color = color)\nplt.title('Frequency of most popular languages', fontsize = 20)\nplt.xticks(rotation = 90 )\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tree map for most pouplar technologies"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = tags[0].value_counts().head(50).to_frame()\ny.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting a tree map\n\nplt.rcParams['figure.figsize'] = (17, 17)\ncolor = plt.cm.cool(np.linspace(0, 1, 50))\nsquarify.plot(sizes = y.values, label = y.index, alpha = .8, color = color)\nplt.title('Tree Map for Popular Items')\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Top 15 technologies with most asked questions and top 15 sub categories' plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"tags['technology'] = 'Technology'\ntechnology = tags.truncate(before = -1, after = 15)\n\ntechnology = nx.from_pandas_edgelist(technology, source = 'technology', target = 0, edge_attr = True)\n\nplt.rcParams['figure.figsize'] = (20, 20)\npos = nx.spring_layout(technology)\ncolor = plt.cm.Wistia(np.linspace(0, 15, 1))\nnx.draw_networkx_nodes(technology, pos, node_size = 15000, node_color = color)\nnx.draw_networkx_edges(technology, pos, width = 3, alpha = 0.6, edge_color = 'black')\nnx.draw_networkx_labels(technology, pos, font_size = 20, font_family = 'sans-serif')\nplt.axis('off')\nplt.grid()\nplt.title('Top 15 technologies', fontsize = 40)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tags['secondchoice'] = 'Second Choice'\nsecondchoice = tags.truncate(before = -1, after = 15)\nsecondchoice = nx.from_pandas_edgelist(secondchoice, source = 'technology', target = 1, edge_attr = True)\n\nplt.rcParams['figure.figsize'] = (20, 20)\npos = nx.spring_layout(secondchoice)\ncolor = plt.cm.Blues(np.linspace(0, 15, 1))\nnx.draw_networkx_nodes(secondchoice, pos, node_size = 15000, node_color = color)\nnx.draw_networkx_edges(secondchoice, pos, width = 3, alpha = 0.6, edge_color = 'brown')\nnx.draw_networkx_labels(secondchoice, pos, font_size = 20, font_family = 'sans-serif')\nplt.axis('off')\nplt.grid()\nplt.title('Top 15 Second Choices', fontsize = 40)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Average word length in a tweet"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1, ax2) = plt.subplots(1, 2, figsize = (16, 5))\nword  =  train[train['y'] == 'HQ']['body'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax  =  ax1, color = 'red')\nax1.set_title('High Quality Questions')\nword  =  train[(train['y'] == 'LQ_EDIT') | (train['y'] == 'LQ_CLOSE')]['body'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax  =  ax2, color = 'green')\nax2.set_title('Low  Quality Questions')\nfig.suptitle('Average word length in each question')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Analyze questions content among different categories**\n\n#### 1. Category: High Quality"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_corpus(target):\n    corpus = []\n    \n    for x in train[train['y'] == target]['body'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = create_corpus('HQ')\n\nplt.figure(figsize = (12, 5))\n\ndic = defaultdict(int)\nfor word in corpus:\n    if word in set(stopwords.words('english')):\n        dic[word] += 1\n        \ntop = sorted(dic.items(), key = lambda x:x[1], reverse = True)[:10]\n\nx, y = zip(*top)\nplt.bar(x, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2. Category: Open Low Quality questions"},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = create_corpus('LQ_EDIT')\n\nplt.figure(figsize = (12, 5))\n\ndic = defaultdict(int)\nfor word in corpus:\n    if word in set(stopwords.words('english')):\n        dic[word] += 1\n        \ntop = sorted(dic.items(), key = lambda x:x[1], reverse = True)[:10]\n\nx, y = zip(*top)\nplt.bar(x, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3. Category: Closed Low Quality questions"},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = create_corpus('LQ_CLOSE')\n\nplt.figure(figsize = (12, 5))\n\ndic = defaultdict(int)\nfor word in corpus:\n    if word in set(stopwords.words('english')):\n        dic[word] += 1\n        \ntop = sorted(dic.items(), key = lambda x:x[1], reverse = True)[:10]\n\nx, y = zip(*top)\nplt.bar(x, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Common Words"},{"metadata":{"trusted":true},"cell_type":"code","source":"counter = Counter(corpus)\nmost = counter.most_common()\nx = []\ny = []\nfor word,count in most[:40]:\n    if (word not in set(stopwords.words('english'))) :\n        x.append(word)\n        y.append(count)\n\nplt.figure(figsize = (12, 8))\n\nsns.barplot(x = y,y = x)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_ques_bigrams(corpus, n = None):\n    vec = CountVectorizer(ngram_range = (2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis = 0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n    return words_freq[:n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12, 5))\ntop_ques_bigrams = get_top_ques_bigrams(train['body'])[:10]\nx, y = map(list,zip(*top_ques_bigrams))\nsns.barplot(x = y, y = x)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing and Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopwords(string):\n    word_list = [word.lower() for word in string.split()]\n    stopwords_list = list(stopwords.words(\"english\"))\n    for word in word_list:\n        if word in stopwords_list:\n            word_list.remove(word)\n    return ' '.join(word_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in ['body', 'title']:\n\n    train[column] = train[column].map(lambda x: re.sub('\\\\n',' ',str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r'\\W',' ',str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r'https\\s+|www.\\s+',r'', str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r'http\\s+|www.\\s+',r'', str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r'\\s+[a-zA-Z]\\s+',' ',str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r'\\^[a-zA-Z]\\s+',' ',str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r'\\s+',' ',str(x)))\n    train[column] = train[column].str.lower()\n\n    train[column] = train[column].map(lambda x: re.sub(r\"\\’\", \"\\'\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"won\\'t\", \"will not\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"can\\'t\", \"can not\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"don\\'t\", \"do not\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"dont\", \"do not\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"n\\’t\", \" not\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"n\\'t\", \" not\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"\\'re\", \" are\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"\\'s\", \" is\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"\\’d\", \" would\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"\\d\", \" would\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"\\'ll\", \" will\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"\\'t\", \" not\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"\\'ve\", \" have\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"\\'m\", \" am\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"\\n\", \"\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"\\r\", \"\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"[0-9]\", \"digit\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"\\'\", \"\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r\"\\\"\", \"\", str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r'[?|!|\\'|\"|#]',r'', str(x)))\n    train[column] = train[column].map(lambda x: re.sub(r'[.|,|)|(|\\|/]',r' ', str(x)))\n    train[column] = train[column].apply(lambda x: remove_stopwords(x))\n\ntrain['description'] = train['title'] + \" \" + train['body']\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = {'HQ': 0, 'LQ_EDIT': 1, 'LQ_CLOSE': 2}\ntrain['y'] = train['y'].map(targets)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BERT Model"},{"metadata":{},"cell_type":"markdown","source":"## Bert large uncased\n\n- 24-layer, 1024-hidden, 16-heads, 340M parameters\n- Trained on lower-cased English text"},{"metadata":{},"cell_type":"markdown","source":"- max_len: Maximum sequence size for BERT is 512\n- batch_encode_plus: It will generate a dictionary which contains the input_ids, token_type_ids and the attention_mask as list for each input sentence"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Maximum sequence size for BERT is 512\n\ndef regular_encode(texts, tokenizer, maxlen = 512):\n    enc_di = tokenizer.batch_encode_plus(texts, return_attention_masks = False, return_token_type_ids = False, pad_to_max_length = True, max_length = maxlen)\n    return np.array(enc_di['input_ids'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#bert large uncased pretrained tokenizer\n\ntokenizer = transformers.BertTokenizer.from_pretrained('bert-large-uncased')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting the data into Train an Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train['description'], train['y'], random_state = 22, test_size = 0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tokenizing the questions descriptions and converting the categories into one hot vectors using tf.keras.utils.to_categorical\n\nXtrain_encoded = regular_encode(X_train.astype('str'), tokenizer, maxlen = 128)\nytrain_encoded = tf.keras.utils.to_categorical(y_train, num_classes = 3, dtype = 'int32')\nXtest_encoded = regular_encode(X_test.astype('str'), tokenizer, maxlen = 128)\nytest_encoded = tf.keras.utils.to_categorical(y_test, num_classes = 3, dtype = 'int32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(transformer, loss = 'categorical_crossentropy', max_len = 512):\n    input_word_ids = tf.keras.layers.Input(shape = (max_len,), dtype = tf.int32, name = \"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n\n    #adding dropout layer\n    x = tf.keras.layers.Dropout(0.40)(cls_token)\n\n    #using a dense layer of 3 neurons as the number of unique categories is 3. \n    out = tf.keras.layers.Dense(3, activation = 'sigmoid')(x)\n\n    model = tf.keras.Model(inputs = input_word_ids, outputs = out)\n    model.compile(tf.keras.optimizers.Adam(lr = 3e-5), loss = loss, metrics = ['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#building the model on tpu\n\nwith strategy.scope():\n    transformer_layer = transformers.TFAutoModel.from_pretrained('bert-large-uncased')\n    model = build_model(transformer_layer, max_len = 128)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating the training and testing dataset.\n\nBATCH_SIZE = 32*strategy.num_replicas_in_sync\nAUTO = tf.data.experimental.AUTOTUNE \ntrain_dataset = (tf.data.Dataset.from_tensor_slices((Xtrain_encoded, ytrain_encoded)).repeat().shuffle(2048).batch(BATCH_SIZE).prefetch(AUTO))\ntest_dataset = (tf.data.Dataset.from_tensor_slices(Xtest_encoded).batch(BATCH_SIZE))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#training for 20 epochs\n\nn_steps = Xtrain_encoded.shape[0] // BATCH_SIZE\ntrain_history = model.fit(train_dataset, steps_per_epoch = n_steps, epochs = 20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction Accuracy on Validation dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#making predictions \n\npreds = model.predict(test_dataset, verbose = 1)\n\n#converting the one hot vector output to a linear numpy array.\npred_classes = np.argmax(preds, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Prediction Accuracy on Validation dataset: ', np.round(100*accuracy_score(pred_classes, y_test), 2), '%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Classification Accuracy over validation dataset: 87.6%\n\nNext Steps:\n\n- Keep 20% dataset aside for testing\n- Use 80% dataset for training/validation purpose\n- Train model on 70-30 split of 80% of above data\n- Run model, fetch predictions on 20% left aside earlier"},{"metadata":{},"cell_type":"markdown","source":"### To be updated......"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}