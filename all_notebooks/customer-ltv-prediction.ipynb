{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nimport statsmodels.api as sm\nfrom statsmodels.tools.eval_measures import mse, rmse\nimport seaborn as sns\nimport scipy.stats as stats\nfrom scipy.stats.mstats import winsorize\nfrom datetime import datetime\nimport json\nfrom wordcloud import WordCloud\n\n%matplotlib inline\npd.options.display.float_format = '{:.2f}'.format\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Goal : Predict Customer Life-time Value (CLV )for an Auto Insurance Company.\nCustomer lifetime value is the net profit acquired from a customer throughout a company’s relationship with them.\n\nKnowing each customer’s customer lifetime value helps you know how much you should be spending on customer acquisition. A customer’s acquisition cost could be more than what they spend on their purchase, but if you nurture that relationship, their CLV may grow to an amount that’s well worth the investment. That’s just one of the many reasons why success in the customer-centered economy means understanding the importance of customer lifetime value.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df = pd.read_csv('Marketing-Customer-Value-Analysis.csv')\ndf.sort_values('Customer Lifetime Value')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#lets edit date format\ndf['Effective To Date']= df['Effective To Date'].astype('datetime64[ns]')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 9134 Observations of 24  Different Variable. (mix of categorical and continous DataType)\n\nDependent Variable is Customer Life Time Value as we have to predict the CLV.\n\nIndependent Variables are: Customer, StateCustomerLifetimeValue, Response, Coverage, Education, EffectiveToDate, EmploymentStatus, Gender, Income, LocationCode, MaritalStatus, MonthlyPremiumAuto, MonthsSinceLastClaim, MonthsSincePolicyInception, NumberofOpenComplaints, NumberofPoliciesPolicyType, Policy, RenewOfferType, SalesChannel, TotalClaimAmountVehicleClass, VehicleSize\n\nContinues Independed Variables are : CustomerLifetimeValue, Income,MonthlyPremiumAuto, MonthsSinceLastClaim, MonthsSincePolicyInception, NumberofOpenComplaints, NumberofPolicies, TotalClaimAmount\n\n","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df.describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.isnull().sum()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Looking at outliers of continuos variables\n\nsignificant_cont = ['Income','Monthly Premium Auto','Total Claim Amount']\n\nsns.set(color_codes=True)\nplt.figure(figsize=(15,20))\nplt.subplots_adjust(hspace=0.5)\n\nfor i in range(len(significant_cont)):\n    plt.subplot(3,2,i+1)\n    plt.boxplot(df[significant_cont[i]])\n    plt.title(significant_cont[i])\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As it can be seen there are outliers in the total claim amount and also in monthly premium auto , usually we remove the outliers for a better model. Since our dataset is related to insurance industry, outliers can be our potential customer. So, we will check the alternative models that includes outliers and do not include outliers.\n\nThere are no outliers in the income.\n\n\n","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"#checking all categorical variables to determine significant ones.\n\ncat_df = df.select_dtypes(include='object')\ncat_df = cat_df.drop(['Customer'], axis = 1)\ncols = cat_df.columns\ncols","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.set(color_codes=True)\nplt.subplots_adjust(hspace=0.5)\nplt.figure(figsize=(20,40))\n\nfor i in range(len(cols)):\n    plt.subplot(7,2,i+1)\n    sns.barplot(x = cols[i],y='Customer Lifetime Value',data = df)\n    plt.title(cols[i])\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interpretations from graphs:\n\nCustomers who have taken only 1 policy have lower customer lifetime value and customers who have taken 3 or greater show a similar trend. So, we can combine all of them into one bin and we can also see that the customers who have taken 2 policies have very high customer lifetime value comparitively.\n\nCustomer Lifetime Value is different for different types of coverage.\n","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(15,6))\nax = sns.violinplot(x=\"Number of Policies\", y=\"Customer Lifetime Value\", data=df)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Statistical Analysis\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Interpretation of graphs gives us some insights but we need to do statistical analysis for statistically significant variables and more clear results.\n\nConsidering CLV (Customer Lifetime Value) as the target variable, we will try to understand how each of the independent variables are contributing towards the target variable.\n\nBecause our target variable  CLV is a continuous variable, we will have to perform f test/ ANOVA to understand how significant are the independent variables towards target variable.\n\n","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# Test whether Gender differences are significant or not.\ngender = df[['Customer Lifetime Value','Gender']].groupby('Gender')\nfemale = gender['Customer Lifetime Value'].get_group('F')\nmale = gender['Customer Lifetime Value'].get_group('M')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"stats.ttest_ind(female,male)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MEANS ARE SAME FOR GENDER\n\npvalue > 0.05 implies that there is no significant difference in the mean of target variable for 'Gender' which means 'Gender' feature is not significant for predicting 'Customer Lifetime Value'","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# Test whether Covarage differences are significant or not.\nCoverage = df[['Customer Lifetime Value','Coverage']].groupby('Coverage')\nBasic = Coverage['Customer Lifetime Value'].get_group('Basic')\nExtended = Coverage['Customer Lifetime Value'].get_group('Extended')\nPremium =Coverage['Customer Lifetime Value'].get_group('Premium')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"stats.f_oneway(Basic,Extended,Premium)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"pvalue > 0.05 implies that there is no significant difference in the mean of target variable for 'Coverage' which means 'Gender' feature is not significant for predicting 'Customer Lifetime Value'","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# Test whether Marital Status differences are significant or not.\n\nMarital = df[['Customer Lifetime Value','Marital Status']].groupby('Marital Status')\nmarried = Marital['Customer Lifetime Value'].get_group('Married')\nsingle = Marital['Customer Lifetime Value'].get_group('Single')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"stats.ttest_ind(married,single)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"pvalue < 0.05 shows that there is significant difference in the mean of target variable for at least one group of 'Marital Status' that means 'Marital Status' could be a significant feature for predicting 'Customer Lifetime Value'","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# Test whether Vehicle Class differences are significant or not.\n\nVehicleclass = df[['Customer Lifetime Value','Vehicle Class']].groupby('Vehicle Class')\nfourdoor = Vehicleclass['Customer Lifetime Value'].get_group('Four-Door Car')\ntwodoor = Vehicleclass['Customer Lifetime Value'].get_group('Two-Door Car')\nsuv = Vehicleclass['Customer Lifetime Value'].get_group('SUV')\nluxurysuv =Vehicleclass['Customer Lifetime Value'].get_group('Luxury SUV')\nluxurycar =Vehicleclass['Customer Lifetime Value'].get_group('Luxury Car')\nsportscar =Vehicleclass['Customer Lifetime Value'].get_group('Sports Car')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"stats.f_oneway(fourdoor,twodoor,suv,luxurysuv,luxurycar,sportscar)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Test whether Renew Offer Type differences are significant or not.\n\nRenewoffer = df[['Customer Lifetime Value','Renew Offer Type']].groupby('Renew Offer Type')\noffer1 = Renewoffer['Customer Lifetime Value'].get_group('Offer1')\noffer2 = Renewoffer['Customer Lifetime Value'].get_group('Offer2')\noffer3 = Renewoffer['Customer Lifetime Value'].get_group('Offer3')\noffer4 =Renewoffer['Customer Lifetime Value'].get_group('Offer4')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"stats.f_oneway(offer1,offer2,offer3,offer4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Test whether EmploymentStatus differences are significant or not.\n\n\nEmploymentStatus = df[['Customer Lifetime Value','EmploymentStatus']].groupby('EmploymentStatus')\nemployed = EmploymentStatus['Customer Lifetime Value'].get_group('Employed')\nunemployed = EmploymentStatus['Customer Lifetime Value'].get_group('Unemployed')\nmedleave = EmploymentStatus['Customer Lifetime Value'].get_group('Medical Leave')\ndisabled = EmploymentStatus['Customer Lifetime Value'].get_group('Disabled')\nretired = EmploymentStatus['Customer Lifetime Value'].get_group('Retired')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"stats.f_oneway(employed,unemployed,medleave,disabled,retired)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"pvalue < 0.05 implies that there is significant difference in the mean of target variable for atleast one group of 'EmploymentStatus' which means 'EmploymentStatus' feature can be a significant for predicting 'Customer Lifetime Value'","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"\n# Test whether Education differences are significant or not.\n\nEducation = df[['Customer Lifetime Value','Education']].groupby('Education')\nbachelor = Education['Customer Lifetime Value'].get_group('Bachelor')\ncollege = Education['Customer Lifetime Value'].get_group('College')\nhighschool = Education['Customer Lifetime Value'].get_group('High School or Below')\nmaster = Education['Customer Lifetime Value'].get_group('Master')\ndoctor = Education['Customer Lifetime Value'].get_group('Doctor')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"stats.f_oneway(bachelor,college,highschool,master,doctor)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"pvalue < 0.05 implies that there is significant difference in the mean of target variable for atleast one group of 'Education' which means 'Education' feature can be a significant for predicting 'Customer Lifetime Value'","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" ### Furthur Modelling:\n\n#### So we did the EDA and also the Statistical Analysis, so now we can just disregard the features which that are not significant  for our model.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df2 =df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df2.drop(['State','Coverage','Renew Offer Type','Vehicle Class','Customer','Response','Gender','Location Code','Vehicle Size','Policy','Policy Type','Sales Channel','Effective To Date'],axis=1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although months since policy inception, months since last claim, number of open complaints and number of policies are all numerical we will consider them as categorical features while preparing the model because numerical values are not high.\n\nFirstly, according to our EDA, we saw that the number of policies >= 3 have similar trend so we will group all of them as 3","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df2['Number of Policies'] = np.where(df2['Number of Policies']>2,3,df2['Number of Policies'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets get dummies of chosen categorical variables","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"new = pd.get_dummies(df2,columns=['Marital Status','Number of Policies','Education','EmploymentStatus'],drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"new","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Continuous Variables","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Check continious variables and relations of them with categorical variables to see if there is any possibility to create new categorical variables from continuous ones.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"\nax = sns.scatterplot(x=\"Income\", y=\"Customer Lifetime Value\", hue=\"State\",\n                     data=df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\nmaritalstts = sns.scatterplot(x=\"Income\", y=\"Customer Lifetime Value\", hue=\"EmploymentStatus\",\n                     data=df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\nax = sns.scatterplot(x=\"Total Claim Amount\", y=\"Customer Lifetime Value\", hue=\"Marital Status\",\n                     data=df)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model 1","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"There is no obvious pattern to create new categorical variable from continious variables. So far, I have explored the dataset in detail and got familiar with it. Now it is time to create the model and see if I can predict Customer Life Time Value.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"import statsmodels.api as sm\n\ny = new['Customer Lifetime Value']\nx = new.drop('Customer Lifetime Value',axis=1)\n\n\nx = sm.add_constant(x)\nresults = sm.OLS(y, x).fit()\nresults.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then I will split my dataset into training and testing data which means I will select 25% of the data randomly and separate it from the training data. (test_size shows the percentage of the test data – 25%) (If you don’t specify the random_state in your code, then every time you run (execute) your code, a new random value is generated and training and test datasets would have different values each time.)","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 450)\n\nprint('Train Data Count: {}'.format(x_train.shape[0]))\nprint('Test Data Count: {}'.format(x_test.shape[0]))\n\nx_train = sm.add_constant(x_train)\nresults = sm.OLS(y_train, x_train).fit()\nresults.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Model graph to see predictions\n\n\nx_test = sm.add_constant(x_test)\n\ny_preds = results.predict(x_test)\nsns.set(color_codes=True)\nplt.scatter(y_test, y_preds)\nplt.plot(y_test, y_test, color=\"red\")\nplt.xlabel(\"Actual ltv\")\nplt.ylabel(\"Estimated ltv\", )\nplt.title(\"Actual vs Estimated Customer LTV\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#lets see their errors\n\nprint(\"Mean Absolute Error (MAE)        : {}\".format(mean_absolute_error(y_test, y_preds)))\nprint(\"Mean Sq. Error (MSE)          : {}\".format(mse(y_test, y_preds)))\nprint(\"Root Mean Sq. Error (RMSE)     : {}\".format(rmse(y_test, y_preds)))\nprint(\"Mean Abs. Perc. Error (MAPE) : {}\".format(np.mean(np.abs((y_test - y_preds) / y_test)) * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"all_score = []\n\nall_score.append((results.rsquared,\n                  mean_absolute_error(y_test, y_preds),\n                 mse(y_test, y_preds),rmse(y_test, y_preds),\n                 np.mean(np.abs((y_test - y_preds) / y_test)) * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## not a good prediction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Model 2","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"\n#duplicate the original data and get the log version of it to be able to reach higher R2(with outliers)\ndf3 = new.copy()\n\ndf3['Monthly Premium Auto'] = np.log(df2['Monthly Premium Auto'])\ndf3['Total Claim Amount'] = np.log(df2['Total Claim Amount'])\ny = np.log(df3['Customer Lifetime Value'])\n\nimport statsmodels.api as sm\n\n\nx1 =  df3.drop('Customer Lifetime Value',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"x1_train, x1_test, y_train, y_test = train_test_split(x1, y, test_size = 0.25, random_state = 450)\n\nprint('Train Data Count: {}'.format(x1_train.shape[0]))\nprint('Test Data Count: {}'.format(x1_test.shape[0]))\n\nx1_train = sm.add_constant(x1_train)\nresults_log = sm.OLS(y_train, x1_train).fit()\nresults_log.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Model graph to see predictions\n\n\nx1_test = sm.add_constant(x1_test)\n\ny_preds = results_log.predict(x1_test)\nsns.set(color_codes=True)\nplt.scatter(y_test, y_preds)\nplt.plot(y_test, y_test, color=\"red\")\nplt.xlabel(\"Actual ltv\")\nplt.ylabel(\"Estimated ltv\", )\nplt.title(\"Actual vs Estimated Customer LTV-Log Transformation with outliers\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"Mean Absolute Error (MAE)        : {}\".format(mean_absolute_error(y_test, y_preds)))\nprint(\"Mean Sq. Error (MSE)          : {}\".format(mse(y_test, y_preds)))\nprint(\"Root Mean Sq. Error (RMSE)     : {}\".format(rmse(y_test, y_preds)))\nprint(\"Mean Abs. Perc. Error (MAPE) : {}\".format(np.mean(np.abs((y_test - y_preds) / y_test)) * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"exp_ypreds = np.exp(y_preds)\nexp_ytest = np.exp(y_test)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"Mean Absolute Error (MAE)        : {}\".format(mean_absolute_error(exp_ytest, exp_ypreds)))\nprint(\"Mean Sq. Error (MSE)          : {}\".format(mse(exp_ytest, exp_ypreds)))\nprint(\"Root Mean Sq. Error (RMSE)     : {}\".format(rmse(exp_ytest, exp_ypreds)))\nprint(\"Mean Abs. Perc. Error (MAPE) : {}\".format(np.mean(np.abs((exp_ytest - exp_ypreds) / exp_ytest)) * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"all_score.append((results.rsquared,\n                  mean_absolute_error(exp_ytest, exp_ypreds),\n                 mse(exp_ytest, exp_ypreds),rmse(exp_ytest, exp_ypreds),\n                 np.mean(np.abs((exp_ytest - exp_ypreds) / exp_ytest)) * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model 3","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"#duplicate the original data and winsorize the data at %5\ndf4 = new.copy()\n\ndf4['Monthly Premium Auto'] = winsorize(df4['Monthly Premium Auto'],(0, 0.05))\ndf4['Total Claim Amount'] = winsorize(df4['Total Claim Amount'],(0, 0.05))\n\n\ny = df4['Customer Lifetime Value']\nx3 =  df4.drop('Customer Lifetime Value',axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"x3_train, x3_test, y_train, y_test = train_test_split(x3, y, test_size = 0.25, random_state = 450)\n\nprint('Train Data Count: {}'.format(x3_train.shape[0]))\nprint('Test Data Count: {}'.format(x3_test.shape[0]))\n\n\nx3_train = sm.add_constant(x3_train)\nresults_wins = sm.OLS(y_train, x3_train).fit()\nresults_wins.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Model graph to see predictions\n\n\nx3_test = sm.add_constant(x3_test)\n\ny_preds = results_wins.predict(x3_test)\nsns.set(color_codes=True)\nplt.scatter(y_test, y_preds)\nplt.plot(y_test, y_test, color=\"red\")\nplt.xlabel(\"Actual ltv\")\nplt.ylabel(\"Estimated ltv\", )\nplt.title(\"Actual vs Estimated Customer LTV-5% Winsorize\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"Mean Absolute Error (MAE)        : {}\".format(mean_absolute_error(y_test, y_preds)))\nprint(\"Mean Sq. Error (MSE)          : {}\".format(mse(y_test, y_preds)))\nprint(\"Root Mean Sq. Error (RMSE)     : {}\".format(rmse(y_test, y_preds)))\nprint(\"Mean Abs. Perc. Error (MAPE) : {}\".format(np.mean(np.abs((y_test - y_preds) / y_test)) * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"all_score.append((results_wins.rsquared,\n                  mean_absolute_error(y_test, y_preds),\n                 mse(y_test, y_preds),rmse(y_test, y_preds),\n                 np.mean(np.abs((y_test - y_preds) / y_test)) * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model 4","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"#duplicate the original data and take log of the data without outlier\n\ndf5 = df4.copy()\n\n\ndf5['Monthly Premium Auto'] = np.log(df5['Monthly Premium Auto'])\ndf5['Total Claim Amount'] = np.log(df5['Total Claim Amount'])\n\n\ny = np.log(df5['Customer Lifetime Value'])\nx7 =df5.drop('Customer Lifetime Value',axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"x7_train, x7_test, y_train, y_test = train_test_split(x7, y, test_size = 0.25, random_state = 450)\n\nprint('Train Data Count: {}'.format(x7_train.shape[0]))\nprint('Test Data Count: {}'.format(x7_test.shape[0]))\n\n\nx7_train = sm.add_constant(x7_train)\nresults_logwins = sm.OLS(y_train, x7_train).fit()\nresults_logwins.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Model graph to see predictions\n\n\nx7_test = sm.add_constant(x7_test)\n\ny_preds = results_logwins.predict(x7_test)\nsns.set(color_codes=True)\nplt.scatter(y_test, y_preds)\nplt.plot(y_test, y_test, color=\"red\")\nplt.xlabel(\"Actual ltv\")\nplt.ylabel(\"Estimated ltv\", )\nplt.title(\"Actual vs Estimated Customer LTV- Both Log Transformation & 5% Winsorize\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"Mean Absolute Error (MAE)        : {}\".format(mean_absolute_error(y_test, y_preds)))\nprint(\"Mean Sq. Error (MSE)          : {}\".format(mse(y_test, y_preds)))\nprint(\"Root Mean Sq. Error (RMSE)     : {}\".format(rmse(y_test, y_preds)))\nprint(\"Mean Abs. Perc. Error (MAPE) : {}\".format(np.mean(np.abs((y_test - y_preds) / y_test)) * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"exp_ypreds = np.exp(y_preds)\nexp_ytest = np.exp(y_test)\n\nall_score.append((results_logwins.rsquared,\n                  mean_absolute_error(exp_ytest, exp_ypreds),\n                 mse(exp_ytest, exp_ypreds),rmse(exp_ytest, exp_ypreds),\n                 np.mean(np.abs((exp_ytest - exp_ypreds) / exp_ytest)) * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model 5","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"#the best model is the one with log transformation and outliers included\n\n#Let's use polynomial features to see if we can do better\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\n\ny = np.log(df3['Customer Lifetime Value'])\nx5 =df3.drop('Customer Lifetime Value',axis=1)\n\n\npol = PolynomialFeatures()\n\n\narray = pol.fit_transform(x5)\n\ndf_pol = pd.DataFrame(array)\ndf_pol.columns = pol.get_feature_names(x5.columns)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_pol_train, df_pol_test, y_train, y_test = train_test_split(df_pol, y, test_size = 0.25, random_state = 450)\n\nprint('Train Data Count: {}'.format(df_pol_train.shape[0]))\nprint('Test Data Count: {}'.format(df_pol_test.shape[0]))\n\ndf_pol_train = sm.add_constant(df_pol_train)\nresults_pol = sm.OLS(y_train, df_pol_train).fit()\nresults_pol.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Model graph to see predictions\n\n\ndf_pol_test = sm.add_constant(df_pol_test)\n\ny_preds = results_pol.predict(df_pol_test)\nsns.set(color_codes=True)\nplt.scatter(y_test, y_preds)\nplt.plot(y_test, y_test, color=\"red\")\nplt.xlabel(\"Actual ltv\")\nplt.ylabel(\"Estimated ltv\", )\nplt.title(\"Actual vs Estimated Customer LTV-Polynomial Features\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Model graph to see predictions\n\n\ndf_pol_test = sm.add_constant(df_pol_test)\n\ny_preds = results_pol.predict(df_pol_test)\nsns.set(color_codes=True)\nplt.scatter(y_test, y_preds)\nplt.plot(y_test, y_test, color=\"red\")\nplt.xlabel(\"Actual ltv\")\nplt.ylabel(\"Estimated ltv\", )\nplt.title(\"Actual vs Estimated Customer LTV-Polynomial Features\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"Mean Absolute Error (MAE)     : {}\".format(mean_absolute_error(y_test, y_preds)))\nprint(\"Mean Sq. Error (MSE)          : {}\".format(mse(y_test, y_preds)))\nprint(\"Root Mean Sq. Error (RMSE)    : {}\".format(rmse(y_test, y_preds)))\nprint(\"Mean Abs. Perc. Error (MAPE)  : {}\".format(np.mean(np.abs((y_test - y_preds) / y_test)) * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"exp_ypreds = np.exp(y_preds)\nexp_ytest = np.exp(y_test)\n\nall_score.append((results_pol.rsquared,\n                  mean_absolute_error(exp_ytest, exp_ypreds),\n                 mse(exp_ytest, exp_ypreds),rmse(exp_ytest, exp_ypreds),\n                 np.mean(np.abs((exp_ytest - exp_ypreds) / exp_ytest)) * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Model graph to see exponential version of predictions\n\n\ndf_pol_test = sm.add_constant(df_pol_test)\n\ny_preds = np.exp(results_pol.predict(df_pol_test))\nsns.set(color_codes=True)\nplt.scatter(exp_ytest, y_preds)\nplt.plot(exp_ytest, exp_ytest, color=\"red\")\nplt.xlabel(\"Actual ltv\")\nplt.ylabel(\"Estimated ltv\", )\nplt.title(\"Actual vs Estimated Customer LTV-Polynomial Features-Exp\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Actual scores and predicted scores have good linearity but after some point we see that linearity is not good enough. In the graph, it is seen that customer life time value prediction is better with the values lower than 10.000.Lets check if there is any improvement on mean sq error term when we predict customer LTV lower than 10.000.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"mse( y_test[y_test<10],y_preds[y_test<10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that Mean Sq. Error decreased from 0.04 to 0.02 which is almost half of the initial error.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Model 6","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We see some improvements when we get polynomial feautures into the scene. However, there are some insignificant features in the model that p-values are more than 0.05. Thats why we will build a new model by removing insignificant features towards target variable.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"significant_features = list(results_pol.pvalues[results_pol.pvalues <= 0.05].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\n\ndf_sig_train, df_sig_test, y_train, y_test = train_test_split(df_pol[significant_features], y, test_size = 0.25, random_state = 450)\n\nprint('Train Data Count: {}'.format(df_sig_train.shape[0]))\nprint('Test Data Count: {}'.format(df_sig_test.shape[0]))\n\ndf_sig_train = sm.add_constant(df_sig_train)\nresults_sig = sm.OLS(y_train, df_sig_train).fit()\nresults_sig.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Model graph to see predictions\n\n\ndf_sig_test = sm.add_constant(df_sig_test)\n\ny_preds = results_sig.predict(df_sig_test)\nsns.set(color_codes=True)\nplt.scatter(y_test, y_preds)\nplt.plot(y_test, y_test, color=\"red\")\nplt.xlabel(\"Actual ltv\")\nplt.ylabel(\"Estimated ltv\" )\nplt.title(\"Actual vs Estimated Customer LTV-Polynomial Features with significant variables\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the graph, we see that model predicts lower values betten than higher ones.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"Mean Absolute Error (MAE)        : {}\".format(mean_absolute_error(y_test, y_preds)))\nprint(\"Mean Sq. Error (MSE)          : {}\".format(mse(y_test, y_preds)))\nprint(\"Root Mean Sq. Error (RMSE)     : {}\".format(rmse(y_test, y_preds)))\nprint(\"Mean Abs. Perc. Error (MAPE) : {}\".format(np.mean(np.abs((y_test - y_preds) / y_test)) * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"exp_ypreds = np.exp(y_preds)\nexp_ytest = np.exp(y_test)\n\nall_score.append((results_sig.rsquared,\n                  mean_absolute_error(exp_ytest, exp_ypreds),\n                 mse(exp_ytest, exp_ypreds),rmse(exp_ytest, exp_ypreds),\n                 np.mean(np.abs((exp_ytest - exp_ypreds) / exp_ytest)) * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_allscore = pd.DataFrame(all_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_allscore.index = ['Standard','Log with outliers','Without Outliers','Log without outliers',\n                       'Polynomial Features',\n                       'Polynomial with significant features']\n\ndf_allscore.columns = ['R2', 'MAE', 'MSE','RMSE','MAPE']\n\n\ndf_allscore","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's check the test /train data prediction if there is underfitting/overfitting problem","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"lrm = LinearRegression()\nlrm.fit(df_pol_train, y_train)\n\ny_train_predict = lrm.predict(df_pol_train)\ny_test_predict = lrm.predict(df_pol_test)\n\nprint(\"Train observation number  : {}\".format(df_pol_train.shape[0]))\nprint(\"Test observation number   : {}\".format(df_pol_test.shape[0]), \"\\n\")\n\nprint(\"Train R-Square  : {}\".format(lrm.score(df_pol_train, y_train)))\nprint(\"-----Test Scores---\")\nprint(\"Test R-Square   : {}\".format(lrm.score(df_pol_test, y_test)))\nprint(\"Mean_absolute_error (MAE)             : {}\".format(mean_absolute_error(y_test, y_test_predict)))\nprint(\"Mean squared error (MSE)              : {}\".format(mse(y_test, y_test_predict)))\nprint(\"Root mean squared error(RMSE)         : {}\".format(rmse(y_test, y_test_predict)))\nprint(\"Mean absolute percentage error (MAPE) : {}\".format(np.mean(np.abs((y_test - y_test_predict) / y_test)) * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Lasso\nfrom yellowbrick.datasets import load_concrete\nfrom yellowbrick.regressor import PredictionError\n\n\n# Create the train and test data\ndf_pol_train, df_pol_test, y_train, y_test = train_test_split(df_pol, y, test_size = 0.25, random_state = 450)\n\n# Instantiate the linear model and visualizer\nmodel = Lasso()\nvisualizer = PredictionError(model)\n\nvisualizer.fit(df_pol_train, y_train)  # Fit the training data to the visualizer\nvisualizer.score(df_pol_test, y_test)  # Evaluate the model on the test data\nvisualizer.show()                 # Finalize and render the figure","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom yellowbrick.datasets import load_concrete\nfrom yellowbrick.regressor import ResidualsPlot\n\n\n# Instantiate the linear model and visualizer\nModel = Ridge()\nvisualizer_residual = ResidualsPlot(Model)\n\nvisualizer_residual.fit(df_pol_train, y_train)  # Fit the training data to the visualizer\nvisualizer_residual.score(df_pol_test, y_test)  # Evaluate the model on the test data\nvisualizer_residual.show()                 # Finaliz","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We have created six different models to reach the best model with highest R-square and lower error terms.\n\nIn the light of comparison table, we could choose to go for the 5th model which have both log transformation and polynomial features. We see that R square is 0.91 means that 91% of the variance can be explained, which is really high. \n\nIt seems like I predict values really good! Actual scores and predicted scores have good linearity but after some point we see that linearity is not good enough. In the graph, it is seen that customer life time value prediction is better with the values lower than 10.000. If we predict customer LTV lower than 10.000, we see that Mean Sq. Error decreased from 0.04 to 0.02 which is almost half of the initial error.\n\nWe do not see overfitting problem with the model but still I have checked Lasso and Ridge models to see if there is any change on the model.\n\nFrom marketing perspective, we have a better opinion which customer have higher predicted life time value. With that information it is easier to lead marketing activities into more profitable scale.\n\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}