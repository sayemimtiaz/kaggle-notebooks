{"cells":[{"metadata":{},"cell_type":"markdown","source":"# EURUSD 15 Minutes Prediction Using Several Currency Pairs Feed Simultaneously\n\nThis notebook is base on this article \n[medium.com/daveyungookim](https://medium.com/@daveyungookim/forex-usdcad-predict-price-every-15-minutes-116554a424b5)\n"},{"metadata":{},"cell_type":"markdown","source":"   # Define Constants"},{"metadata":{"trusted":true},"cell_type":"code","source":"PAIRS_LIST        = [\"EURUSD\", \"USDJPY\", \"EURJPY\"] #, \"USDCHF\", \"EURCHF\"] #, \"AUDUSD\" ]\nPREDICTING_PAIR   = \"EURUSD\" \nPREDICTING_COLUMN = \"close\"\nLOOK_BACK         = 30 # 20 * 15 six hour\nSPLIT             = 0.95 # data split ration for training and testing","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Installing and loading Dependencies"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# install talib\n%cd /kaggle/working\n%rm -rf temp\n!mkdir temp\n%cd ./temp\n!wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\n!tar -xzvf ta-lib-0.4.0-src.tar.gz\n%cd ./ta-lib\n!./configure --prefix=/usr\n!make\n!make install\n!pip install Ta-Lib\n%cd /kaggle/working\n!rm -rf temp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import libraries\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Input, LSTM, Dense, Flatten\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.callbacks import TensorBoard\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.decomposition import PCA, KernelPCA\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Used TA-Lib for creating additional features. More on this later.\nfrom talib.abstract import *\nfrom talib import MA_Type\n\nimport datetime","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# loading data\ndata = {}\n\nfor pair_name in PAIRS_LIST:\n    data[pair_name] = pd.read_csv(\"/kaggle/input/forex-top-currency-pairs-20002020/\"+pair_name+\"-2000-2020-15m.csv\")\n\n# normalize data shape and format\ndef norm_data_shape_format(df):\n    orig_cols = [\"DATE_TIME\", \"OPEN\", \"HIGH\", \"LOW\", \"CLOSE\"]\n    cols_name = [\"timestamp\", \"open\", \"high\", \"low\", \"close\"]\n    df.rename(columns=dict(zip(orig_cols, cols_name)), inplace=True)\n    df['timestamp'] = pd.to_datetime(df['timestamp'], infer_datetime_format=True)\n    df.set_index(\"timestamp\", inplace=True)\n    df = df.reindex(columns=cols_name[1:])\n    return df.astype(float)\n    \nfor key in data:\n    data[key] = norm_data_shape_format(data[key])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"EURUSD\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_features(df):\n    df['hour'] = df.index.hour\n    df['day']  = df.index.weekday\n    df['week'] = df.index.week\n    # df['volume'] = pd.to_numeric(df['volume'])\n    df['close']  = pd.to_numeric(df['close'])\n    df['open']   = pd.to_numeric(df['open'])\n    # df['momentum']   = df['volume'] * (df['open'] - df['close'])\n    df['avg_price']  = (df['low'] + df['high'])/2\n    df['range']      = df['high'] - df['low']\n    df['ohlc_price'] = (df['low'] + df['high'] + df['open'] + df['close'])/4\n    df['oc_diff']      = df['open'] - df['close']\n    # df['spread_open']  = df['ask_open'] - df['bid_open']\n    # df['spread_close'] = df['ask_close'] - df['bid_close']\n    inputs = {\n        'open'   : df['open'].values,\n        'high'   : df['high'].values,\n        'low'    : df['low'].values,\n        'close'  : df['close'].values,\n        'volume' : np.zeros(df['close'].shape[0]) # for sake of using TA lib\n    }\n    df['ema'] = MA(inputs, timeperiod=15, matype=MA_Type.T3)\n    df['bear_power'] = df['low'] - df['ema']\n    df['bull_power'] = df['high'] - df['ema']\n    # Since computing EMA leave some of the rows empty, we want to remove them. (EMA is a lagging indicator)\n    df.dropna(inplace=True)\n    # Add 1D PCA vector as a feature as well. This helped increasing the accuracy by adding more variance to the feature set\n    pca_input = df.drop('close', axis=1).copy()\n    pca_features = pca_input.columns.tolist()\n    pca = PCA(n_components=1)\n    df['pca'] = pca.fit_transform(pca_input.values.astype('float32'))\n\ncolumns_order = [\"open\", \"high\", \"low\", \"close\", \"hour\", \"day\", \"week\", \"avg_price\", \"range\", \"ohlc_price\", \"oc_diff\", \"ema\", \"bear_power\", \"bull_power\", \"pca\"]    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for key in data:\n    extract_features(data[key])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"EURUSD\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.plot(data['EURUSD'][2900:5150][\"close\"])\n# plt.plot(data['EURUSD'][2900:5150][\"ema\"])\n# plt.plot(data['EURUSD'][1900:2150][\"bull_power\"])\n# plt.plot(data['EURUSD'][1900:2150][\"bear_power\"])\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Merge All Pairs"},{"metadata":{"trusted":true},"cell_type":"code","source":"# sort and rename column names\ndef sort_and_rename(df, suffix):\n    cols_name = [c + suffix for c in columns_order]    \n    df.rename(columns=dict(zip(columns_order,cols_name)), inplace=True)\n    df = df.reindex(columns=cols_name, copy=False)\n    return df\n    \nall_columns = [] # to save the correct order of data\n\nfor key in PAIRS_LIST:\n    data[key] = sort_and_rename(data[key], \"_\" + key)\n    all_columns += list(data[key].columns) # to save the correct order of data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge \nmerged_data = pd.DataFrame(data[PAIRS_LIST[0]])\nfor key in PAIRS_LIST[1:]:\n    merged_data = merged_data.merge(data[key], how=\"inner\", left_index=True, right_index=True)\n    \nmerged_data = merged_data.reindex(columns=all_columns)\n\n# drop duplicate columns\nfor key in PAIRS_LIST[1:]:\n    merged_data.drop(columns=[\n        \"hour_\"+key,\n        \"day_\" +key,\n        \"week_\"+key,\n    ],inplace=True)\n    \nmerged_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A Little Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"# seeing correlation between columns\ncorr = merged_data.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nf, ax = plt.subplots(figsize=(15, 15))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Shaping Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_dataset(df, look_back=10):\n    dataX, dataY = [], []\n    for i in range(len(df)-look_back-1):\n        a = df[i:(i+look_back)]\n        dataX.append(a)\n        dataY.append(df[i + look_back])\n    return np.array(dataX), np.array(dataY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scale reshape and group the data\n\ntarget_column_name = PREDICTING_COLUMN + \"_\" + PREDICTING_PAIR\n\n# Create scalers\nscaler = MinMaxScaler()\nscaled = pd.DataFrame(scaler.fit_transform(merged_data), columns=merged_data.columns)\n\nx_scaler = MinMaxScaler(feature_range=(0, 1))\nx_scaler = x_scaler.fit(merged_data.values.astype('float32'))\ny_scaler = MinMaxScaler(feature_range=(0, 1))\ny_scaler = y_scaler.fit(merged_data[target_column_name].values.astype('float32').reshape(-1,1))\n\n# Create dataset\ntarget_index = scaled.columns.tolist().index(target_column_name)\ndataset = scaled.values.astype('float32')\n\nX, y = create_dataset(dataset, look_back=LOOK_BACK)\ny = y[:,target_index]\n\ntrain_size = int(len(X) * SPLIT)\ntrainX = X[:train_size]\ntrainY = y[:train_size]\ntestX = X[train_size:]\ntestY = y[train_size:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"all data shape:\", X.shape)\nprint(\"train data shape:\", trainX.shape)\nprint(\"test data shape:\", testX.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    model = Sequential()\n    model.add(LSTM(20, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n    model.add(LSTM(20, return_sequences=True))\n    model.add(LSTM(10, return_sequences=True))\n    model.add(Dropout(0.2))\n    model.add(LSTM(4, return_sequences=False))\n    model.add(Dense(4, kernel_initializer='uniform', activation='relu'))\n    model.add(Dense(1, kernel_initializer='uniform', activation='relu'))\n    \n    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae', 'mse'])\n    print(model.summary())\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the best weight during training.\nfrom keras.callbacks import ModelCheckpoint\ncheckpoint = ModelCheckpoint(\"weights.best.hdf5\", monitor='val_mse', verbose=1, save_best_only=True, mode='min')\n\n# Monitor the trianing progress via TensorBoard\n# log_dir = \"logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n# tensorboard = TensorBoard(log_dir=log_dir)\n\n# Callbacks\ncallbacks_list = [checkpoint] # , tensorboard]\n\n# Fit\nhistory = model.fit(trainX, trainY, epochs=200, batch_size=512, verbose=1, callbacks=callbacks_list, validation_split=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualize_history():\n    epoch = len(history.history['loss'])\n    for k in list(history.history.keys()):\n        if 'val' not in k:\n            plt.figure(figsize=(40,10))\n            plt.plot(history.history[k])\n            plt.plot(history.history['val_' + k])\n            plt.title(k)\n            plt.ylabel(k)\n            plt.xlabel('epoch')\n            plt.legend(['train', 'test'], loc='upper left')\n            plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize_history()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To improve the weights towards the global optimal, I retrained the model with LearningRateScheduler added\n\nfrom keras.callbacks import LearningRateScheduler\nimport keras.backend as K\ndef scheduler(epoch):\n    if epoch%10==0 and epoch!=0:\n        lr = K.get_value(model.optimizer.lr)\n        K.set_value(model.optimizer.lr, lr*.9)\n        print(\"lr changed to {}\".format(lr*.9))\n    return K.get_value(model.optimizer.lr)\nlr_decay = LearningRateScheduler(scheduler)\ncallbacks_list = [checkpoint, lr_decay] # , tensorboard]\nhistory = model.fit(trainX, trainY, epochs=3, batch_size=1024, callbacks=callbacks_list, validation_split=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize_history()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing\nOnce the training was complete, Iâ€™ve loaded the best weights discovered to my model and checked if the prediction worked as intended."},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights(\"weights.best.hdf5\") # load best validation ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(testX)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot Scaled Predictions vs Scaled Actual Price"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error \n\npredictions = pd.DataFrame()\npredictions['predicted'] = pd.Series(np.reshape(pred, (pred.shape[0])))\npredictions['actual'] = testY\npredictions = predictions.astype(float)\n\npredictions.plot(figsize=(20,10))\nplt.show()\n\npredictions['diff'] = predictions['predicted'] - predictions['actual']\nplt.figure(figsize=(10,10))\nsns.distplot(predictions['diff']);\nplt.title('Distribution of differences between actual and prediction')\nplt.show()\n\nprint(\"MSE : \", mean_squared_error(predictions['predicted'].values, predictions['actual'].values))\nprint(\"MAE : \", mean_absolute_error(predictions['predicted'].values, predictions['actual'].values))\npredictions['diff'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compare the unscaled values and see if the prediction falls within the Low and High"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(testX)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = y_scaler.inverse_transform(pred)\nclose = y_scaler.inverse_transform(np.reshape(testY, (testY.shape[0], 1)))\n\npredictions = pd.DataFrame()\npredictions['predicted'] = pd.Series(np.reshape(pred, (pred.shape[0])))\npredictions['close'] = pd.Series(np.reshape(close, (close.shape[0])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p = merged_data[-pred.shape[0]:].copy()\npredictions.index = p.index\npredictions = predictions.astype(float)\npredictions = predictions.merge(p[['low_'+PREDICTING_PAIR, 'high_'+PREDICTING_PAIR]], right_index=True, left_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"zoom = 200\n\nax = predictions[:zoom].plot(y='close', c='red', figsize=(40,10))\nax = predictions[:zoom].plot(y='predicted', c='blue', figsize=(40,10), ax=ax)\nindex = [str(item) for item in predictions[:zoom].index]\nplt.fill_between(x=index, y1='low_'+PREDICTING_PAIR, y2='high_'+PREDICTING_PAIR, data=p[:zoom], alpha=0.4)\nplt.title('Prediction vs Actual (low and high as blue region)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions['diff'] = predictions['predicted'] - predictions['close']\nplt.figure(figsize=(10,10))\nsns.distplot(predictions['diff']);\nplt.title('Distribution of differences between actual and prediction ')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.jointplot(\"diff\", \"predicted\", data=predictions, kind=\"kde\", space=0)\nplt.title('Distributtion of error and price')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"MSE : \", mean_squared_error(predictions['predicted'].values, predictions['close'].values))\nprint(\"MAE : \", mean_absolute_error(predictions['predicted'].values, predictions['close'].values))\npredictions['diff'].describe()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}