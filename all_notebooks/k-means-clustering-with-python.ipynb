{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0\"></a>\n# **K-Means Clustering with Python**\n\nHello friends,\n\n**K-Means clustering** is the most popular unsupervised machine learning algorithm. K-Means clustering is used to find intrinsic groups within the unlabelled dataset and draw inferences from them. In this kernel, I implement K-Means clustering to find intrinsic groups within the dataset that display the same `status_type` behaviour. The `status_type` behaviour variable consists of posts of a different nature (video, photos, statuses and links).\n\n\nSo, let's get started."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"**As always, I hope you find this kernel useful and some <font color=\"red\"><b>UPVOTES</b></font> would be highly appreciated**."},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0.1\"></a>\n# **Table of Contents**\n\n\n1.\t[Introduction to K-Means Clustering](#1)\n1.  [Applications of clustering](#2)\n1.\t[K-Means Clustering intuition](#3)\n1.\t[Choosing the value of K](#4)\n1.\t[The elbow method](#5)\n1.  [Import libraries](#6)\n1.\t[Import dataset](#7)\n1.\t[Exploratory data analysis](#8)\n1.\t[Declare feature vector and target variable](#9)\n1.\t[Convert categorical variable into integers](#10)\n1.\t[Feature scaling](#11)\n1.\t[K-Means model with two clusters](#12)\n1.\t[K-Means model parameters study](#13)\n1.\t[Check quality of weak classification by the model](#14)\n1.\t[Use elbow method to find optimal number of clusters](#15)\n1.\t[K-Means model with different clusters](#16)\n1.\t[Results and conclusion](#17)\n1.  [References](#18)\n"},{"metadata":{},"cell_type":"markdown","source":"# **1. Introduction to K-Means Clustering** <a class=\"anchor\" id=\"1\"></a>\n\n[Table of Contents](#0.1)\n\n\nMachine learning algorithms can be broadly classified into two categories - supervised and unsupervised learning. There are other categories also like semi-supervised learning and reinforcement learning. But, most of the algorithms are classified as supervised or unsupervised learning. The difference between them happens because of presence of target variable. In unsupervised learning, there is no target variable. The dataset only has input variables which describe the data. This is called unsupervised learning.\n\n**K-Means clustering** is the most popular unsupervised learning algorithm. It is used when we have unlabelled data which is data without defined categories or groups. The algorithm follows an easy or simple way to classify a given data set through a certain number of clusters, fixed apriori. K-Means algorithm works iteratively to assign each data point to one of K groups based on the features that are provided. Data points are clustered based on feature similarity.\n\nK-Means clustering can be represented diagrammatically as follows:-\n"},{"metadata":{},"cell_type":"markdown","source":"## K-Means\n\n![K-Means](https://miro.medium.com/max/2160/1*tWaaZX75oumVwBMcKN-eHA.png)"},{"metadata":{},"cell_type":"markdown","source":"# **2. Applications of clustering** <a class=\"anchor\" id=\"2\"></a>\n\n[Table of Contents](#0.1)\n\n\n- K-Means clustering is the most common unsupervised machine learning algorithm. It is widely used for many applications which include-\n\n  1. Image segmentation\n\n  2. Customer segmentation\n\n  3. Species clustering\n\n  4. Anomaly detection\n\n  5. Clustering languages"},{"metadata":{},"cell_type":"markdown","source":"# **3. K-Means Clustering intuition** <a class=\"anchor\" id=\"3\"></a>\n\n[Table of Contents](#0.1)\n\n\nK-Means clustering is used to find intrinsic groups within the unlabelled dataset and draw inferences from them. It is based on centroid-based clustering.\n\n\n**Centroid** - A centroid is a data point at the centre of a cluster. In centroid-based clustering, clusters are represented by a centroid. It is an iterative algorithm in which the notion of similarity is derived by how close a data point is to the centroid of the cluster.\nK-Means clustering works as follows:-\nThe K-Means clustering algorithm uses an iterative procedure to deliver a final result. The algorithm requires number of clusters K and the data set as input. The data set is a collection of features for each data point. The algorithm starts with initial estimates for the K centroids. The algorithm then iterates between two steps:-\n\n\n## **3.1 Data assignment step**\n\n\nEach centroid defines one of the clusters. In this step, each data point is assigned to its nearest centroid, which is based on the squared Euclidean distance. So, if ci is the collection of centroids in set C, then each data point is assigned to a cluster based on minimum Euclidean distance. \n\n\n\n## **3.2 Centroid update step**\n\n\nIn this step, the centroids are recomputed and updated. This is done by taking the mean of all data points assigned to that centroid’s cluster. \n\n\nThe algorithm then iterates between step 1 and step 2 until a stopping criteria is met. Stopping criteria means no data points change the clusters, the sum of the distances is minimized or some maximum number of iterations is reached.\nThis algorithm is guaranteed to converge to a result. The result may be a local optimum meaning that assessing more than one run of the algorithm with randomized starting centroids may give a better outcome.\n\nThe K-Means intuition can be represented with the help of following diagram:-\n"},{"metadata":{},"cell_type":"markdown","source":"## K-Means intuition\n![K-Means intuition](https://i.ytimg.com/vi/_aWzGGNrcic/hqdefault.jpg)"},{"metadata":{},"cell_type":"markdown","source":"# **4. Choosing the value of K** <a class=\"anchor\" id=\"4\"></a>\n\n[Table of Contents](#0.1)\n\n\nThe K-Means algorithm depends upon finding the number of clusters and data labels for a pre-defined value of K. To find the number of clusters in the data, we need to run the K-Means clustering algorithm for different values of K and compare the results. So, the performance of K-Means algorithm depends upon the value of K. We should choose the optimal value of K that gives us best performance. There are different techniques available to find the optimal value of K. The most common technique is the **elbow method** which is described below.\n"},{"metadata":{},"cell_type":"markdown","source":"# **5. The elbow method** <a class=\"anchor\" id=\"5\"></a>\n\n[Table of Contents](#0.1)\n\n\nThe elbow method is used to determine the optimal number of clusters in K-means clustering. The elbow method plots the value of the cost function produced by different values of K. The below diagram shows how the elbow method works:-"},{"metadata":{},"cell_type":"markdown","source":"## The elbow method\n\n![Elbow method in K-Means](https://www.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/995b8b58-06f1-4884-a2a1-f3648428e947.png)"},{"metadata":{},"cell_type":"markdown","source":"We can see that if K increases, average distortion will decrease.  Then each cluster will have fewer constituent instances, and the instances will be closer to their respective centroids. However, the improvements in average distortion will decline as K increases. The value of K at which improvement in distortion declines the most is called the elbow, at which we should stop dividing the data into further clusters.\n"},{"metadata":{},"cell_type":"markdown","source":"# **6. Import libraries** <a class=\"anchor\" id=\"6\"></a>\n\n[Table of Contents](#0.1)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for data visualization\nimport seaborn as sns # for statistical data visualization\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ignore warnings\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **7. Import dataset** <a class=\"anchor\" id=\"7\"></a>\n\n[Table of Contents](#0.1)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = '/kaggle/input/facebook-live-sellers-in-thailand-uci-ml-repo/Live.csv'\n\ndf = pd.read_csv(data)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **8. Exploratory data analysis** <a class=\"anchor\" id=\"8\"></a>\n\n[Table of Contents](#0.1)\n"},{"metadata":{},"cell_type":"markdown","source":"### Check shape of the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are 7050 instances and 16 attributes in the dataset. In the dataset description, it is given that there are 7051 instances and 12 attributes in the dataset.\n\nSo, we can infer that the first instance is the row header and there are 4 extra attributes in the dataset. Next, we should take a look at the dataset to gain more insight about it."},{"metadata":{},"cell_type":"markdown","source":"### Preview the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### View summary of dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check for missing values in dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are 4 redundant columns in the dataset. We should drop them before proceeding further."},{"metadata":{},"cell_type":"markdown","source":"### Drop redundant columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['Column1', 'Column2', 'Column3', 'Column4'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Again view summary of dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we can see that redundant columns have been removed from the dataset. \n\nWe can see that, there are 3 character variables (data type = object) and remaining 9 numerical variables (data type = int64).\n"},{"metadata":{},"cell_type":"markdown","source":"### View the statistical summary of numerical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 3 categorical variables in the dataset. I will explore them one by one."},{"metadata":{},"cell_type":"markdown","source":"### Explore `status_id` variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"# view the labels in the variable\n\ndf['status_id'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# view how many different types of variables are there\n\nlen(df['status_id'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are 6997 unique labels in the `status_id` variable. The total number of instances in the dataset is 7050. So, it is approximately a unique identifier for each of the instances. Thus this is not a variable that we can use. Hence, I will drop it."},{"metadata":{},"cell_type":"markdown","source":"### Explore `status_published` variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"# view the labels in the variable\n\ndf['status_published'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# view how many different types of variables are there\n\nlen(df['status_published'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, we can see that there are 6913 unique labels in the `status_published` variable. The total number of instances in the dataset is 7050. So, it is also a approximately a unique identifier for each of the instances. Thus this is not a variable that we can use. Hence, I will drop it also."},{"metadata":{},"cell_type":"markdown","source":"### Explore `status_type` variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"# view the labels in the variable\n\ndf['status_type'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# view how many different types of variables are there\n\nlen(df['status_type'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are 4 categories of labels in the `status_type` variable."},{"metadata":{},"cell_type":"markdown","source":"### Drop `status_id` and `status_published` variable from the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['status_id', 'status_published'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### View the summary of dataset again"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preview the dataset again"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there is 1 non-numeric column `status_type` in the dataset. I will convert it into integer equivalents."},{"metadata":{},"cell_type":"markdown","source":"# **9. Declare feature vector and target variable** <a class=\"anchor\" id=\"9\"></a>\n\n[Table of Contents](#0.1)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df\n\ny = df['status_type']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **10. Convert categorical variable into integers** <a class=\"anchor\" id=\"10\"></a>\n\n[Table of Contents](#0.1)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\nX['status_type'] = le.fit_transform(X['status_type'])\n\ny = le.transform(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### View the summary of X"},{"metadata":{"trusted":true},"cell_type":"code","source":"X.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preview the dataset X"},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **11. Feature Scaling** <a class=\"anchor\" id=\"11\"></a>\n\n[Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = X.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nms = MinMaxScaler()\n\nX = ms.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.DataFrame(X, columns=[cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **12. K-Means model with two clusters** <a class=\"anchor\" id=\"12\"></a>\n\n[Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=2, random_state=0) \n\nkmeans.fit(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **13. K-Means model parameters study** <a class=\"anchor\" id=\"13\"></a>\n\n[Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The KMeans algorithm clusters data by trying to separate samples in n groups of equal variances, minimizing a criterion known as **inertia**, or within-cluster sum-of-squares Inertia, or the within-cluster sum of squares criterion, can be recognized as a measure of how internally coherent clusters are.\n\n\n- The k-means algorithm divides a set of N samples X into K disjoint clusters C, each described by the mean j of the samples in the cluster. The means are commonly called the cluster **centroids**.\n\n\n- The K-means algorithm aims to choose centroids that minimize the inertia, or within-cluster sum of squared criterion."},{"metadata":{},"cell_type":"markdown","source":"### Inertia\n\n\n- **Inertia** is not a normalized metric. \n\n- The lower values of inertia are better and zero is optimal. \n\n- But in very high-dimensional spaces, euclidean distances tend to become inflated (this is an instance of `curse of dimensionality`). \n\n- Running a dimensionality reduction algorithm such as PCA prior to k-means clustering can alleviate this problem and speed up the computations.\n\n- We can calculate model inertia as follows:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans.inertia_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The lesser the model inertia, the better the model fit.\n\n- We can see that the model has very high inertia. So, this is not a good model fit to the data."},{"metadata":{},"cell_type":"markdown","source":" # **14. Check quality of weak classification by the model** <a class=\"anchor\" id=\"14\"></a>\n\n[Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = kmeans.labels_\n\n# check how many of the samples were correctly labeled\ncorrect_labels = sum(y == labels)\n\nprint(\"Result: %d out of %d samples were correctly labeled.\" % (correct_labels, y.size))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have achieved a weak classification accuracy of 1% by our unsupervised model."},{"metadata":{},"cell_type":"markdown","source":"# **15. Use elbow method to find optimal number of clusters** <a class=\"anchor\" id=\"15\"></a>\n\n[Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\ncs = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n    kmeans.fit(X)\n    cs.append(kmeans.inertia_)\nplt.plot(range(1, 11), cs)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('CS')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- By the above plot, we can see that there is a kink at k=2. \n\n- Hence k=2 can be considered a good number of the cluster to cluster this data.\n\n- But, we have seen that I have achieved a weak classification accuracy of 1% with k=2.\n\n- I will write the required code with k=2 again for convinience."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=2,random_state=0)\n\nkmeans.fit(X)\n\nlabels = kmeans.labels_\n\n# check how many of the samples were correctly labeled\n\ncorrect_labels = sum(y == labels)\n\nprint(\"Result: %d out of %d samples were correctly labeled.\" % (correct_labels, y.size))\n\nprint('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, our weak unsupervised classification model achieved a very weak classification accuracy of 1%."},{"metadata":{},"cell_type":"markdown","source":"I will check the model accuracy with different number of clusters."},{"metadata":{},"cell_type":"markdown","source":"# **16. K-Means model with different clusters** <a class=\"anchor\" id=\"16\"></a>\n\n[Table of Contents](#0.1)"},{"metadata":{},"cell_type":"markdown","source":"### K-Means model with 3 clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters=3, random_state=0)\n\nkmeans.fit(X)\n\n# check how many of the samples were correctly labeled\nlabels = kmeans.labels_\n\ncorrect_labels = sum(y == labels)\nprint(\"Result: %d out of %d samples were correctly labeled.\" % (correct_labels, y.size))\nprint('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K-Means model with 4 clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters=4, random_state=0)\n\nkmeans.fit(X)\n\n# check how many of the samples were correctly labeled\nlabels = kmeans.labels_\n\ncorrect_labels = sum(y == labels)\nprint(\"Result: %d out of %d samples were correctly labeled.\" % (correct_labels, y.size))\nprint('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have achieved a relatively high accuracy of 62% with k=4."},{"metadata":{},"cell_type":"markdown","source":"# **17. Results and conclusion** <a class=\"anchor\" id=\"17\"></a>\n\n[Table of Contents](#0.1)\n\n\n1.\tIn this project, I have implemented the most popular unsupervised clustering technique called **K-Means Clustering**.\n\n2.\tI have applied the elbow method and find that k=2 (k is number of clusters) can be considered a good number of cluster to cluster this data.\n\n3.\tI have find that the model has very high inertia of 237.7572. So, this is not a good model fit to the data.\n\n4.\tI have achieved a weak classification accuracy of 1% with k=2 by our unsupervised model.\n\n5.\tSo, I have changed the value of k and find relatively higher classification accuracy of 62% with k=4.\n\n6.\tHence, we can conclude that k=4 being the optimal number of clusters.\n"},{"metadata":{},"cell_type":"markdown","source":"# **18. References** <a class=\"anchor\" id=\"18\"></a>\n\n[Table of Contents](#0.1)\n\nThe work done in this project is inspired from following books and websites:-\n\n  1. Udemy course – Machine Learning – A Z by Kirill Eremenko and Hadelin de Ponteves\n\n  2. https://en.wikipedia.org/wiki/K-means_clustering\n\n  3. https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html\n\n  4. https://www.datacamp.com/community/tutorials/k-means-clustering-python\n\n  5. https://www.datascience.com/blog/k-means-clustering\n\n  6. https://acadgild.com/blog/k-means-clustering-algorithm\n\n"},{"metadata":{},"cell_type":"markdown","source":"So, now we will come to the end of this kernel.\n\nI hope you find this kernel useful and enjoyable.\n\nYour comments and feedback are most welcome.\n\nThank you\n"},{"metadata":{},"cell_type":"markdown","source":"[Go to Top](#0)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}