{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Fun with Conditional Variational Autoencoders\n\nThis is a starter kernel to understand the main principles of conditional variational autoencoders. PyTorch will be used for modelling.\n\n### **Please, give this kernel an upvote if it's helpful for you.**\n\nYou can find explanation of the vanilla and variational autoencoders in the following notebook:\n\n* [Fun with Variational Autoencoders](https://www.kaggle.com/averkij/variational-autoencoder-and-faces-generation/edit/run/28076587)\n\nIn this kernel we will discuss the conditional variation of the autoencoder and build a model, train it and analyse the latent space with t-SNE technique.\n\n![architecture](https://www.mdpi.com/sensors/sensors-19-02528/article_deploy/html/images/sensors-19-02528-g002.png)\n"},{"metadata":{},"cell_type":"markdown","source":"# The dataset"},{"metadata":{},"cell_type":"markdown","source":"We will use the MNIST dataset because it's simple and small. You can fork this notebook and experiment with the dataset you like. Fashion MNIST, etc."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox, TextArea\nimport matplotlib\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torchvision\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torch.utils import data\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\nfrom torchvision.utils import save_image\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm, tqdm_notebook\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint('Training on',DEVICE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 128\nlearning_rate = 0.005\ninput_size = 28*28\nhidden_size = 12\nlabels_length = 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = transforms.Compose(\n    [transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = MNIST('./data', transform=transform, download=True)\ntrain_data, test_data = data.random_split(dataset, (50000,10000))\n\ntrain_dataset = DataLoader(train_data, batch_size=batch_size, shuffle=True)\nval_dataset = DataLoader(test_data, batch_size=batch_size, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#helper functions\ndef one_hot(x, max_x):\n    return torch.eye(max_x + 1)[x]\n\ndef plot_gallery(images, h, w, n_row=3, n_col=6):\n    plt.figure(figsize=(2 * n_col, 2 * n_row))\n    for i in range(n_row * n_col):\n        plt.subplot(n_row, n_col, i + 1)\n        plt.axis(\"off\")\n        plt.imshow(images[i].reshape(h, w), cmap = matplotlib.cm.binary)\n    plt.show()\n    \ndef plot_loss(history):\n    loss, val_loss = zip(*history)\n    plt.figure(figsize=(15, 9))\n    plt.plot(loss, label=\"train_loss\")\n    plt.plot(val_loss, label=\"val_loss\")\n    plt.legend(loc='best')\n    plt.xlabel(\"epochs\")\n    plt.ylabel(\"loss\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class autoencoder(nn.Module):\n    def __init__(self):\n        super(autoencoder, self).__init__()\n\n        self.encoder = nn.Sequential(\n            nn.Linear(input_size + labels_length, 64),\n            #nn.ReLU(True),\n            #nn.Linear(128, 64),\n            nn.ReLU(True),\n            nn.Linear(64, hidden_size))\n             \n        self.decoder = nn.Sequential(\n            nn.Linear(hidden_size + labels_length, 64),\n            nn.ReLU(True),\n            #nn.Linear(64, 128),\n            #nn.ReLU(True),\n            nn.Linear(64, input_size),\n            nn.Tanh())\n\n    def encode(self, x, labels):\n        x = x.view(-1, 1*28*28)\n        x = torch.cat((x,labels),dim=1)\n        return self.encoder(x)\n    \n    def decode(self, x, labels):\n        x = torch.cat((x,labels),dim=1)\n        return self.decoder(x)\n\n    def forward(self, x, labels):\n        x = self.encode(x,labels)\n        x = self.decode(x,labels)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = autoencoder().to(DEVICE)\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_epoch(model, train_loader, criterion, optimizer, is_cvae):\n    running_loss = 0.0\n    running_corrects = 0\n    processed_data = 0\n  \n    for inputs, labels in train_loader:\n        inputs = inputs.to(DEVICE)\n        labels = one_hot(labels,9).to(DEVICE)\n        optimizer.zero_grad()\n        if is_cvae:\n            outputs, mu, logvar = model(inputs,labels)\n            loss = vae_loss_fn(inputs.view(-1,28*28), outputs, mu, logvar)\n            loss.backward()\n        else:\n            outputs = model(inputs,labels)\n            loss = criterion(outputs, inputs.view(-1,28*28))\n            loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n        processed_data += inputs.size(0)\n              \n    train_loss = running_loss / processed_data\n    return train_loss\n  \ndef eval_epoch(model, val_loader, criterion, is_cvae):\n    model.eval()\n    running_loss = 0.0\n    processed_size = 0\n    inp,out = [],[]\n    for inputs, labels in val_loader:\n        inputs = inputs.to(DEVICE)\n        labels = one_hot(labels,9).to(DEVICE)\n\n        with torch.set_grad_enabled(False):\n            if is_cvae:\n                outputs, mu, logvar = model(inputs,labels)\n                loss = vae_loss_fn(inputs.view(-1,28*28), outputs, mu, logvar)\n                loss.backward()\n            else:\n                outputs = model(inputs,labels)\n                loss = criterion(outputs, inputs.view(-1,28*28))\n                inp,out = inputs, outputs\n\n        running_loss += loss.item() * inputs.size(0)\n        processed_size += inputs.size(0)\n        \n    with torch.set_grad_enabled(False):\n        plot_gallery([inp[0].cpu(),out[0].cpu()],28,28,1,2)\n\n    val_loss = running_loss / processed_size\n    return val_loss\n  \ndef train(train_loader, val_loader, model, epochs, batch_size, is_cvae=False):\n    history = []\n    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} val_loss {v_loss:0.4f}\"\n    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:        \n        opt = torch.optim.Adam(model.parameters())\n        criterion = nn.MSELoss()\n        for epoch in range(epochs):\n            train_loss = fit_epoch(model, train_loader, criterion, opt, is_cvae)\n            print(\"loss\", train_loss)            \n            val_loss = eval_epoch(model, val_loader, criterion, is_cvae)\n            history.append((train_loss, val_loss))            \n            pbar_outer.update(1)\n            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss, v_loss=val_loss))            \n    return history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = train(train_dataset, val_dataset, model=model, epochs=200, batch_size=128, is_cvae=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_loss(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sampling"},{"metadata":{},"cell_type":"markdown","source":"Now we can try to get not the random pictures, but the numbers we want to generate. Let's say numbers from 0 to 9."},{"metadata":{"trusted":true},"cell_type":"code","source":"with torch.set_grad_enabled(False):\n    number_for_generation=[0,1,2,3,4,5,6,7,8,9]\n    inputs = torch.FloatTensor(np.random.randn(10*28*28).reshape(-1,28,28))\n    inputs = inputs.to(DEVICE)\n    label = one_hot(number_for_generation,9).to(DEVICE)\n    outputs = model(inputs,label)\n    plot_gallery(outputs.cpu(),28,28,1,10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring the latent space using t-SNE"},{"metadata":{},"cell_type":"markdown","source":"t-Distributed Stochastic Neighbor Embedding (t-SNE) is a technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. Let's use it and plot the results."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_latent_data(net, count=1000, is_cvae=False):\n    latent_vectors = []\n    latent_labels = []\n    img_inputs = []\n    rounds = count/100\n    i=0\n    with torch.set_grad_enabled(False):\n        dataset_loader = DataLoader(dataset, batch_size=100, shuffle=True)\n        for inputs,labels in dataset_loader:\n            inputs = inputs.to(DEVICE)\n            labels_one_hot = one_hot(labels,9).to(DEVICE)\n            if is_cvae:\n                outputs, mu, logvar = net(inputs,labels_one_hot)\n            else:\n                outputs = net(inputs,labels_one_hot)\n            outputs = outputs.cpu()\n            if i==0:\n              latent_vectors = outputs\n              latent_labels = labels\n              img_inputs = inputs\n            else:\n              latent_vectors = torch.cat((latent_vectors,outputs),0)\n              latent_labels = torch.cat((latent_labels,labels),0)\n              img_inputs = torch.cat((img_inputs,inputs),0)\n            if i>rounds:\n              break\n            i+=1\n    return img_inputs, latent_vectors, latent_labels\n\ndef plot_tsne(net, mode, count, is_cvae=False):\n    img_inputs,latent_vectors,latent_labels = get_latent_data(net=net, count=count, is_cvae=is_cvae)\n    fig, ax = plt.subplots(figsize=(10, 7))\n    ax.set_title('t-SNE')\n    coords = TSNE(n_components=2,random_state=42).fit_transform(latent_vectors)\n    if mode == 'imgs':\n        for image, (x, y) in zip(img_inputs.cpu(), coords):\n            im = OffsetImage(image.reshape(28, 28), zoom=1, cmap='gray')\n            ab = AnnotationBbox(im, (x, y), xycoords='data', frameon=False)\n            ax.add_artist(ab)\n        ax.update_datalim(coords)\n        ax.autoscale()\n    elif mode == 'dots':\n        classes = latent_labels\n        plt.scatter(coords[:, 0], coords[:, 1], c=classes)\n        plt.colorbar()\n        for i in range(10):\n            class_center = np.mean(coords[classes == i], axis=0)\n            text = TextArea('{}'.format(i))\n            ab = AnnotationBbox(text, class_center, xycoords='data', frameon=True)\n            ax.add_artist(ab)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_tsne(net=model, mode='dots', count=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_tsne(net=model, mode='imgs', count=300)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conditional Variational Autoencoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"class CVAE(nn.Module):\n    def __init__(self, input_size, hidden_size=20):\n        super(CVAE, self).__init__()\n        input_size_with_label = input_size + labels_length\n        hidden_size += labels_length\n        \n        self.fc1 = nn.Linear(input_size_with_label,512)\n        self.fc21 = nn.Linear(512, hidden_size)\n        self.fc22 = nn.Linear(512, hidden_size)\n        \n        self.relu = nn.ReLU()\n        \n        self.fc3 = nn.Linear(hidden_size, 512)\n        self.fc4 = nn.Linear(512, input_size)\n    \n    def encode(self, x, labels):\n        x = x.view(-1, 1*28*28)\n        x = torch.cat((x, labels), 1)\n        x = self.relu(self.fc1(x))\n        return self.fc21(x), self.fc22(x)\n        \n    def decode(self, z, labels):\n        torch.cat((z, labels), 1)\n        z = self.relu(self.fc3(z))\n        return torch.sigmoid(self.fc4(z))\n        \n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5 *logvar)\n        eps = torch.randn_like(std)\n        return eps.mul(std).add_(mu)\n        \n    def forward(self,x, labels):\n        #targets = one_hot(targets,labels_length-1).float().to(DEVICE)\n        mu, logvar = self.encode(x, labels)\n        z = self.reparameterize(mu, logvar)\n        x = self.decode(z, labels)\n        return x, mu, logvar\n\ndef train_cvae(net, dataloader, test_dataloader, flatten=True, epochs=20):\n    validation_losses = []\n    optim = torch.optim.Adam(net.parameters())\n\n    log_template = \"\\nEpoch {ep:03d} val_loss {v_loss:0.4f}\"\n    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:  \n        for i in range(epochs):\n            for batch, labels in dataloader:\n                batch = batch.to(DEVICE)\n                labels = one_hot(labels,9).to(DEVICE)\n\n                if flatten:\n                    batch = batch.view(batch.size(0), 28*28)\n\n                optim.zero_grad()\n                x,mu,logvar = net(batch, labels)\n                loss = vae_loss_fn(batch, x[:, :784], mu, logvar)\n                loss.backward()\n                optim.step()\n            evaluate(validation_losses, net, test_dataloader, flatten=True)\n            pbar_outer.update(1)\n            tqdm.write(log_template.format(ep=i+1, v_loss=validation_losses[i]))\n    plt.show()\n    return validation_losses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cvae = CVAE(28*28).to(DEVICE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def vae_loss_fn(x, recon_x, mu, logvar):\n    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    return BCE + KLD\n\ndef evaluate(losses, autoencoder, dataloader, flatten=True):\n    model = lambda x, y: autoencoder(x, y)[0]    \n    loss_sum = []\n    inp, out = [],[]\n    loss_fn = nn.MSELoss()\n    for inputs, labels in dataloader:\n        inputs = inputs.to(DEVICE)\n        labels = one_hot(labels,9).to(DEVICE)\n\n        if flatten:\n            inputs = inputs.view(inputs.size(0), 28*28)\n\n        outputs = model(inputs, labels)\n        loss = loss_fn(inputs, outputs)            \n        loss_sum.append(loss)\n        inp = inputs\n        out = outputs\n\n    with torch.set_grad_enabled(False):\n        plot_gallery([inp[0].detach().cpu(),out[0].detach().cpu()],28,28,1,2)    \n\n    losses.append((sum(loss_sum)/len(loss_sum)).item())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_cvae(net, dataloader, test_dataloader, flatten=True, epochs=50):\n    validation_losses = []\n    optim = torch.optim.Adam(net.parameters())\n\n    log_template = \"\\nEpoch {ep:03d} val_loss {v_loss:0.4f}\"\n    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:  \n        for i in range(epochs):\n            for batch, labels in dataloader:\n                batch = batch.to(DEVICE)\n                labels = one_hot(labels,9).to(DEVICE)\n\n                if flatten:\n                    batch = batch.view(batch.size(0), 28*28)\n\n                optim.zero_grad()\n                x,mu,logvar = net(batch, labels)\n                loss = vae_loss_fn(batch, x[:, :784], mu, logvar)\n                loss.backward()\n                optim.step()\n            evaluate(validation_losses, net, test_dataloader, flatten=True)\n            pbar_outer.update(1)\n            tqdm.write(log_template.format(ep=i+1, v_loss=validation_losses[i]))\n    plt.show()\n    return validation_losses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = train_cvae(cvae, train_dataset, val_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_loss = history\nplt.figure(figsize=(15, 9))\nplt.plot(val_loss, label=\"val_loss\")\nplt.legend(loc='best')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting CVAE latent space using t-SNE"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_tsne(net=cvae, mode='dots', count=2000, is_cvae=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_tsne(net=cvae, mode='imgs', count=300, is_cvae=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"Optimizing both **reconstruction** loss and **KL divergence** loss together results in the generation of a latent space which maintains the similarity of nearby encodings on the local scale via clustering, yet globally, is very densely packed near the latent space origin (compare the axes with the original).\n\nCool pic:\n\n![https://media.giphy.com/media/lqq0em9cuivVNWFwSX/giphy.gif](https://media.giphy.com/media/lqq0em9cuivVNWFwSX/giphy.gif)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}