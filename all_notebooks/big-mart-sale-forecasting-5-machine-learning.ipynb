{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Basic Library \nimport numpy as np \nimport pandas as pd \nimport math\nimport scipy \n\n#Visualization\nimport missingno as msno\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style(\"whitegrid\")\n%matplotlib inline\nimport matplotlib.gridspec as gridspec\nimport cufflinks as cf\ncf.go_offline()\nfrom IPython.display import display\nfrom PIL import Image\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"pip install feature-engine","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Greetting and Description\nA Quick Summary .. this is the 3rd out of 6 begginer Notebooks\n* I am unsing Machine Learning Algorithms such as ***Linear Regression, Decision Tree, Random Forest , Xgboost*** , I also  used Cross_Score to figure out the best algorithms , and finally tunning Parameter using ***GridSearchCv and  RandomizedSearchCV***\n* Another Important Library is feature-engine Which ***named the columns*** After applying Onehotencoder\n* Following Data Science Standad I am /using OSEMN Methodology to have a proper workflow\n* Please Feel Free to leave Any Feedback"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"path=\"../input/imagenes/osemn.jpeg\"\ndisplay(Image.open(path))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Obtain Data\n\n1.1 General View"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/big-mart-sales/train_v9rqX0R.csv')\nprint(\"---------Technocal Information-------------\")\nprint(\"Data Set shape = {}\".format(df.shape))\nprint('Data set Memory Usage = {:.2f} MB'.format(df.memory_usage().sum()/1024**2))\nprint(\"Data Colums type\\n{}\".format(df.dtypes.value_counts()))\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Scrub Data\n\n2.1 General View"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#General view\nprint('Target columns is = {}'.format(df.columns[-1]))\nprint(\"---------------\")\nprint(\"Variable Target = 'Showing 10 only out of 1559' \\n{}\".format(df['Item_Identifier'].value_counts().index.to_list()[0:10]))\nprint('-------------')\nprint('Columns in df are ={}'.format(df.columns.to_list()))\nprint(\"\\n----- Null Values ------\\n\")\n\n# Missing Values\nprint(df.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"Data Set\")\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Notes:**\n\n1. This is Regression challenge in which we have to predict the ***Total Sales***\n2. Almost 4,000 ***Missing/null/nan*** values\n\n#### ***Approach*** \n1. ***Identify any relation between Variables*** to fill those mising values ."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"msno.matrix(df, sparkline=True, figsize=(10,5), sort='ascending', fontsize=12, labels=True, color=(0.25, 0.45, 0.6))\nprint(\"Based on this Visualization we can assume that NAN values are in the same row, now we will show those missing values using pandas and find any relation\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":" df[df.isna().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def corr (df):\n    \n    correlation = df.corr()\n    f,ax= plt.subplots(figsize = (15,10))\n    mask = np.triu(correlation)\n    sns.heatmap(correlation, annot=True, mask=mask,ax=ax,cmap='viridis')\n    bottom,top = ax.get_ylim()\n    ax.set_ylim(bottom+ 0.5, top - 0.5)\n    \ncorr(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2.2 Missing Values"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for i in df.columns:\n    value = df[i].isnull().sum()\n    if value >0:\n        print(i , \"Missing Values\",value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2.3 Filling ***Weight***\n* I logic is the Following group the set by Item which has unique,Size and bring the Mean weight then use this value to replace the Nan values "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Finding Item_Identifier, item Weight and Oulet Size (no null)\n\nndf = df[(df['Item_Weight'].notnull())&(df['Outlet_Size'].notnull())] #no null in the whole Data Set\ndict_ndf = ndf.groupby(['Item_Identifier','Outlet_Size'])['Item_Weight'].mean().to_dict()\n\n\ndef finding_weights (col):\n    model = col[0]\n    measure = col[1]\n    weight = col[2]\n    \n    for k, v in dict_ndf.items():\n        if (k[0]==model) & (k[1]== measure):\n            weight = v\n        else :\n            weight = weight \n            \n    return weight\n\n# Replacing Values \n\ndf['Item_Weight']= df[['Item_Identifier','Outlet_Size','Item_Weight']].apply(finding_weights,axis = 1)\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"markdown","source":"2.4 Filling ***Outlet_size***\n\n* In my understanding Each Item has unique Code, so I Create a list of unique Item-Size and the comparing them with the Item which has Nan values in Outlet-size"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Finding Index and Replacing Grcoery \ngrocery_index = df[(df['Outlet_Size'].isnull())&(df['Outlet_Type'] == 'Grocery Store')].index\ndf.loc[grocery_index,'Outlet_Size']=\"Small\"\n\n#Creating a List to Filter item \nOut_no_NAN = df.loc[(df['Outlet_Size'].notna())&(df['Outlet_Size'] =='Small')][['Item_Identifier','Outlet_Size']].value_counts().to_frame().reset_index()\nvalues = list(zip(Out_no_NAN.Item_Identifier, Out_no_NAN.Outlet_Size))\n\n\ndef finding_size (col):\n    item = col[0]\n    outsize = col[1]\n    \n    if pd.isnull(outsize) :\n        \n        for i in values:\n            if i[0] == item:\n                outsize = i[1]\n        return outsize \n\n    else:\n        return outsize\n    \n# Replacing Values\ndf['Outlet_Size']= df[['Item_Identifier','Outlet_Size']].apply(finding_size,axis = 1)\n\n\nprint(\"Replacing Values Under 2 Condition- 1.Item name, 2. Outlet_size\\nReplacing..............Done..!!!\\n\")\nprint(\"After Checking Out_size and Grouping by Outlet_Type I can noticed that All outlet_size in Grocery Store are small\\nReplacing Nan for Small in outlet_size for Grocery Store......Done..!!\")\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"for i in df.columns:\n    value = df[i].isnull().sum()\n    if value >0:\n        print(i , \"Missing Values\",value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Index\nindex_weight = df.loc[df['Item_Weight'].isna()].index\nindex_size = df.loc[df['Outlet_Size'].isna()].index\n\n#Dropping \n\ndf.drop(index_weight, inplace = True)\ndf.reset_index(drop=True)\n\ndf.drop(index_size, inplace = True)\ndf.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2.5 LF to Low Fat & reg to Regular"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def changing(x):\n    \n    if x == 'LF':\n        x = 'Low Fat'\n    elif x == 'low fat':\n        x = 'Low Fat'\n    elif x == 'reg':\n        x = 'Regular'\n    else:\n        x=x\n    return x\n\ndf['Item_Fat_Content'] = df['Item_Fat_Content'].apply(lambda x: changing(x))\n\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Explore Data"},{"metadata":{},"cell_type":"markdown","source":"3.1 Exploration, Segmentation, Total Sales, Total Weight, Best Performance (Location) and more "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = plt.figure(constrained_layout=True, figsize=(20,10))\ngrid = gridspec.GridSpec(ncols=6, nrows=2, figure=fig)\n\n\n\n#bar plot Horizontal\nax1 = fig.add_subplot(grid[0, :2])\nax1.set_title('Type of Product per Store')\nsns.countplot(y='Outlet_Type',hue = 'Item_Fat_Content',data = df, ax=ax1,) #Outlet Type\n\n#bar plot Vertical\nax2 = fig.add_subplot(grid[1, :2])\nax2.set_title('Type of Products')\nbar = sns.countplot(x='Item_Type', data = df, ax = ax2)\nbar.set_xticklabels(bar.get_xticklabels(),  rotation=90, horizontalalignment='right') #Type of Products\n\n#box plot Sales\nax3 = fig.add_subplot(grid[:, 2])\nax3.set_title('Total Sales')\nsns.boxplot(df.loc[:,'Item_Outlet_Sales'], orient='v', ax = ax3) #Total Sales\n\n#box plot Monthly payment\nax4 = fig.add_subplot(grid[:,3])\nax4.set_title(\"Weight\")\nsns.boxplot(df['Item_Weight'], orient='v' ,ax=ax4)\n\n\n#-----------> Displot Distribution\nax5 = fig.add_subplot(grid[0, 4:6])\nax5.set_title(\"Sales by Store\")\nsns.barplot(x='Outlet_Type', y='Item_Outlet_Sales',data=df,ax=ax5) \n\n\n#-----------> Displot Distribution\nax6 = fig.add_subplot(grid[1, 4:6])\nax6.set_title(\"Sale Distribution, ST1='Orange',ST2 ='Green',ST3='Blue'\")\nSuperT1 = df[df['Outlet_Type']=='Supermarket Type1']\nSuperT2 = df[df['Outlet_Type']=='Supermarket Type2']\nSuperT3 = df[df['Outlet_Type']=='Supermarket Type3']\nGrocery = df[df['Outlet_Type']=='Grocery Store']\n\nsns.distplot(SuperT1['Item_Outlet_Sales'], color = 'orange', ax=ax6)\nsns.distplot(SuperT2['Item_Outlet_Sales'], color = 'Green', ax=ax6)\nsns.distplot(SuperT3['Item_Outlet_Sales'], color = 'Blue', ax=ax6)\n\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plt.figure(constrained_layout=True, figsize=(20,12))\ngrid = gridspec.GridSpec(ncols=2, nrows=2, figure=fig)\n\nax1 = fig.add_subplot(grid[0:, :1])\nax1.set_title('Product distribution per store')\ndistr=sns.swarmplot(x='Outlet_Type', y='Item_Outlet_Sales', hue='Item_Type' ,data=df,ax=ax1)\ndistr.set_xticklabels(distr.get_xticklabels(),  rotation=90, horizontalalignment='right')\n\nax2 = fig.add_subplot(grid[0, 1:])\nax2.set_title('Relation Sales~ Visibility per Product')\nsns.scatterplot(y='Item_Outlet_Sales' ,x= 'Item_Visibility',  hue = 'Item_Type', data =df ,cmap='Spectral')\n\nax3 = fig.add_subplot(grid[1, 1:])\nax3.set_title('Relation Sales~ MRP per Product')\nsns.scatterplot(y ='Item_Outlet_Sales', x = 'Item_MRP',data= df, hue = 'Outlet_Type', ax=ax3)\nplt.show()\n\n\nprint(\"Low Fat is the category that performance better in sales , no matter the Store\")\nprint(\"Top 5 items : Fruit and vegetables, Snack Foods, Baking food, Frozen foods , Dairy\")\nprint(\"Average Sale in this data ser was almost 2000\")\nprint(\"Average Weight 12.5\")\nprint(\"Supermarket Type 3 perfomance better in sales \")\nprint(\"In ST1(Green) seems to sell many product which the price range is 0~2500\\nIn ST3(Blue) seems to sell many product which the price range is 2500~7500\")\nprint(\"The most expensive items are in ST1, however the one who has more products in ST1\")\nprint(\"There is no relation between Sales and Visibility but there is between MRP and Sales\")\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2.2 Polloting After Normalization"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"# Transforming 'Sale Price' using log1p\n'''\ndf[\"Item_Outlet_Sales\"] = np.log1p(df[\"Item_Outlet_Sales\"])\n\nfig = plt.figure(constrained_layout=True, figsize=(20,10))\ngrid = gridspec.GridSpec(ncols=6, nrows=2, figure=fig)\n\n\n\n#bar plot Horizontal\nax1 = fig.add_subplot(grid[0, :2])\nax1.set_title('Type of Product per Store')\nsns.countplot(y='Outlet_Type',hue = 'Item_Fat_Content',data = df, ax=ax1,) #Outlet Type\n\n#bar plot Vertical\nax2 = fig.add_subplot(grid[1, :2])\nax2.set_title('Type of Products')\nbar = sns.countplot(x='Item_Type', data = df, ax = ax2)\nbar.set_xticklabels(bar.get_xticklabels(),  rotation=90, horizontalalignment='right') #Type of Products\n\n#box plot Sales\nax3 = fig.add_subplot(grid[:, 2])\nax3.set_title('Total Sales')\nsns.boxplot(df.loc[:,'Item_Outlet_Sales'], orient='v', ax = ax3) #Total Sales\n\n#box plot Monthly payment\nax4 = fig.add_subplot(grid[:,3])\nax4.set_title(\"Weight\")\nsns.boxplot(df['Item_Weight'], orient='v' ,ax=ax4)\n\n\n#-----------> Displot Distribution\nax5 = fig.add_subplot(grid[0, 4:6])\nax5.set_title(\"Sales by Store\")\nsns.barplot(x='Outlet_Type', y='Item_Outlet_Sales',data=df,ax=ax5) \n\n\n#-----------> Displot Distribution\nax6 = fig.add_subplot(grid[1, 4:6])\nax6.set_title(\"Sale Distribution, ST1='Orange',ST2 ='Green',ST3='Blue'\")\nSuperT1 = df[df['Outlet_Type']=='Supermarket Type1']\nSuperT2 = df[df['Outlet_Type']=='Supermarket Type2']\nSuperT3 = df[df['Outlet_Type']=='Supermarket Type3']\nGrocery = df[df['Outlet_Type']=='Grocery Store']\n\nsns.distplot(SuperT1['Item_Outlet_Sales'], color = 'orange', ax=ax6)\nsns.distplot(SuperT2['Item_Outlet_Sales'], color = 'Green', ax=ax6)\nsns.distplot(SuperT3['Item_Outlet_Sales'], color = 'Blue', ax=ax6)\n\nplt.show()\n\nprint(\"After Standarization\\n\") '''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n\n# 4. Model Data"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"#Applying Machine Learning \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV,cross_val_score\nfrom sklearn.preprocessing import MinMaxScaler,LabelEncoder,OneHotEncoder\nfrom feature_engine.categorical_encoders import OneHotCategoricalEncoder\nfrom sklearn.metrics import mean_squared_error\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4.1 Outliers\n"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"#Fuction to find Outliers\ndef Outliers (data,column):\n    mean_ = data[column].mean()\n    Sdev_= data[column].std()\n    Upper_limit= mean_+ (3*Sdev_)\n    lower_limit= mean_- (3*Sdev_) #error \n    out= data[(data[column]>Upper_limit)|(data[column]<lower_limit)].index\n    \n       \n    data.drop(out ,inplace=True)\n    data.reset_index(drop= True)\n    print(out)\n\n\n#Dropping Colums \n\ndf.drop(['Item_Identifier','Outlet_Identifier'], axis = 1 , inplace= True)\nOutliers(df,\"Item_Outlet_Sales\")\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4.2. Label Encoding and OneHot Encoding(Using, feature-engine)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Label Encoding for Outlet_Size\nLE = LabelEncoder()\ndf['Outlet_Size']=LE.fit_transform(df['Outlet_Size'])\n\n#Encoding item_Fat Content\nEn = pd.get_dummies(df['Item_Fat_Content'], drop_first = True, prefix = 'LF')\n\n#Encoding Other Variables \nOHC_ENG = OneHotCategoricalEncoder(top_categories = None,\n                                  variables=['Item_Type','Outlet_Type','Outlet_Location_Type'],drop_last= False)\n                                  \n\nOHC_ENG = OHC_ENG.fit_transform(df[['Item_Type','Outlet_Type','Outlet_Location_Type']]) \n\n#Dropping\ndf.drop(['Item_Type','Outlet_Location_Type','Outlet_Type','Item_Fat_Content'],axis = 1 , inplace=True) \n\n# Concat all Variables\ndf = pd.concat([df,En,OHC_ENG],axis = 1 )\ndf.reset_index(drop=True)\n\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4.2. Comparing ML Algorithms Performance-using cross_val_score\n* cross_var_score\n* MinMax Scaler"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Evaluating Best Model \n\nX = df.drop('Item_Outlet_Sales',axis= 1)\ny = np.log1p(df[\"Item_Outlet_Sales\"])\n\n#Scaling \nMN = MinMaxScaler()\nX = MN.fit_transform(X)\n\n# Model initilization\n\nLnr = LinearRegression()\nSVR_rbf = SVR(kernel = 'linear')\nDT = DecisionTreeRegressor()\nRDF = RandomForestRegressor()\nXR = XGBRegressor()\n\n# Model \n\nmodel_list = [Lnr,DT,RDF,XR,SVR_rbf]\n\ndef best_ml (model,X,y,cv=10): #Finding the Best model \n    cross = -cross_val_score(model,X,y,cv=cv,scoring='neg_mean_squared_error').mean()\n    final = np.sqrt(cross)\n    final_score.append(final)\n    \nfinal_score = []\n\n\n\nfor i in model_list:\n    best_ml(i,X,y)\n\npd.DataFrame(data = final_score,\n             columns = ['mean_Squared_error'],\n             index = ['LinearRegression','SupVecMach','DecisionTree','RandomForest','XGBRegressor'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4.3. Feature Importance"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.inspection import permutation_importance\nimport shap\n\nname = df.drop('Item_Outlet_Sales',axis = 1)\npd.DataFrame(data = X, columns =name.columns)\nX = name\ny = np.log1p(df[\"Item_Outlet_Sales\"])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nSVR_rbf = SVR(kernel = 'linear')\n\nXGB = XGBRegressor()\nXGB = XGB.fit(X_test,y_test)\n\nperm_importance = permutation_importance(XGB, X_test, y_test)\nexplainer = shap.TreeExplainer(XGB)\nshap_values = explainer.shap_values(X_test)\nshap.summary_plot(shap_values, X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4.4. Tunnig XGBRegressor and SVR"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#splitting \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nSVR_rbf = SVR(kernel = 'linear')\nXR = XGBRegressor()\n\n#--------------------->   GridSearchCV   <-----------------------------------\nparam_grid = {'C' :[0.1,1,10,100,1000], 'gamma' : [1,0.1,0.01,0.001,0.0001]}\ngrid = GridSearchCV(SVR_rbf , param_grid , verbose = 3 , n_jobs = -1)\ngrid.fit(X_train,y_train)  # if you download the code please remove the # to find the best Parameter but it will take arround 20 min to finish the computation\nSVR_y_pred = grid.predict(X_test)\n\n\n#---------------------> Hyper Parameter Optimization <-----------------------\nn_estimators = [100,500,900,1100,1500]\nmax_depth = [2,3,5,10,15]\nbooster = ['gbtree', 'gblinear']\nlearning_rate = [0.05,0.1,0.15,0.20]\nmin_child_weight = [1,2,3,4]\nbase_score = [0.25,0.5,0.75,1]\n\n#-------------> Define the grid of Hyperparameters to search\nhyperparameter_grid = { 'n_estimators': n_estimators,'max_depth': max_depth,'booster': booster,\n                       'learning_rate': learning_rate,'min_child_weight': min_child_weight,\n                       'base_score' : base_score}\nrandom_cv = RandomizedSearchCV(estimator = XR,\n                              param_distributions=hyperparameter_grid,\n                              cv=5,n_iter = 50,\n                              scoring = 'neg_mean_absolute_error',n_jobs = -1,\n                              verbose = 5,\n                              return_train_score=True,\n                              random_state=42)\n\nrandom_cv.fit(X_train,y_train)  # if you download the code please remove the # to find the best Parameter but it will take arround 20 min to finish the computation\nXR_y_pred = random_cv.predict(X_test)\n\n# -----------------> Best Parameters <---------------------\n#grid..best_parameter_\n#random_cv.best_parameter_\n\n\n# -----------------> Final Results <------------------------\n\nprint(\"Mean Squared Error in SVM\",np.sqrt(mean_squared_error(y_test,SVR_y_pred)))\nprint(\"Mean Squared Error in XGBoost\",np.sqrt(mean_squared_error(y_test,XR_y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Improvemen After Tunning only 0.3 reduction in Mean Square Error"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"path=\"../input/loanimages/upvote.jpg\"\ndisplay(Image.open(path))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}