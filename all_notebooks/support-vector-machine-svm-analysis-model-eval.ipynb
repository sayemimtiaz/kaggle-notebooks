{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hello, this is an SVM analysis based on the knowledge I could gather from the internet (:p) and from the notebook by [Niraj Verma](https://www.kaggle.com/nirajvermafcb/support-vector-machine-detail-analysis). Please do go through this notebook and let me know if it makes sense. Do critically evaluate and let me know where I can do better. Thanks\n\nI have given few links below if anyone wants to understand the math behind it.\n\nReferences:\n\nhttps://towardsdatascience.com/understanding-support-vector-machine-part-1-lagrange-multipliers-5c24a52ffc5e\n\nhttps://www.youtube.com/watch?v=ax8LxRZCORU\n\nhttps://www.youtube.com/watch?v=_PwhiWxHK8o","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The contents of the notebook are given below:<br>\n- [About this Dataset](#About-this-Dataset)\n- [Check the data](#Step-1:-Check-the-data)\n- [EDA](#Step-2:-EDA)\n- [Preprocessing and Model builiding](#Step-3:-Preprocessing-and-Model-builiding)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# About this Dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Voice Gender\n#### Gender Recognition by Voice and Speech Analysis\n\nThis database was created to identify a voice as male or female, based upon acoustic properties of the voice and speech. The dataset consists of 3,168 recorded voice samples, collected from male and female speakers. The voice samples are pre-processed by acoustic analysis in R using the seewave and tuneR packages, with an analyzed frequency range of 0hz-280hz (human vocal range).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The following acoustic properties of each voice are measured and included within the CSV:\n\n- meanfreq: mean frequency (in kHz)\n- sd: standard deviation of frequency\n- median: median frequency (in kHz)\n- Q25: first quantile (in kHz)\n- Q75: third quantile (in kHz)\n- IQR: interquantile range (in kHz)\n- skew: skewness (see note in specprop description)\n- kurt: kurtosis (see note in specprop description)\n- sp.ent: spectral entropy\n- sfm: spectral flatness\n- mode: mode frequency\n- centroid: frequency centroid (see specprop)\n- peakf: peak frequency (frequency with highest energy)\n- meanfun: average of fundamental frequency measured across acoustic signal\n- minfun: minimum fundamental frequency measured across acoustic signal\n- maxfun: maximum fundamental frequency measured across acoustic signal\n- meandom: average of dominant frequency measured across acoustic signal\n- mindom: minimum of dominant frequency measured across acoustic signal\n- maxdom: maximum of dominant frequency measured across acoustic signal\n- dfrange: range of dominant frequency measured across acoustic signal\n- modindx: modulation index. Calculated as the accumulated absolute difference between adjacent measurements of -fundamental frequencies divided by the frequency range\n- label: male or female","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Questions\n- What other features differ between male and female voices?\n- Can we find a difference in resonance between male and female voices?\n- Can we identify falsetto from regular voices? (separate data-set likely needed for this)\n- Are there other interesting features in the data?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Step 1: Check the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/voicegender/voice.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe([.25,.50,.75,.80,.90])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().sum() #no missing data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Step 2: EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#univariate\ndef dist_male(x):\n    if x == 'label':\n        pass\n    else:\n        data[x][data['label']=='male'].plot.kde()\n        plt.xlabel(x)\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dist_female(y):\n    if y == 'label':\n        pass\n    else:\n        data[y][data['label']=='female'].plot.kde(color='maroon')\n        plt.xlabel(y)\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"cols = data.columns.drop('label')\n\n\nfor j, i in enumerate(cols):\n#     print(j)\n    dist_male(i)\n    dist_female(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_male = data[data['label']=='male'].drop('label', axis=1)\ndata_female = data[data['label']=='female'].drop('label', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def box_plt_m(x,i):\n    sns.boxplot(x=x, data=data_male)\n    plt.show()\n    \n\ndef box_plt_f(x,i):\n    sns.boxplot(x=x, data=data_female)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"for j,i in enumerate(cols):\n    plt.figure(figsize=(20,50))\n    plt.subplot(21,2,j+1)\n    box_plt_m(i,j)\n    plt.figure(figsize=(20,50))\n    plt.subplot(21,2,j+2)\n    box_plt_f(i,j)\n    print(j)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Inference:</b>\n\n#### What other features differ between male and female voices?\n\n    - The meanfreq has a different mean, there are more outliers in the female as compared to male data\n    - The standard deviation is more for male as compared to female\n    - The median is slightly different for male and female with more outliers in female \n    - The Q25 has a lot of outliers to the left for female while for the male it is both sides but more on the left\n    - The Q75 has also slightly different median\n    - The IQR is significantly different for male and female with male IQR having outliers on both sides (low and high outliers)\n    - The skew has a lot of outliers for both female and male\n    - The sp.ent and sfm is almost similar for both male and female\n    - The mode is also similar but has outliers for female\n    - The centroid is similar but has outliers for both male and female\n    - The meanfun and minfun is also similar, the distribution are different\n    - The maxfun, meandom is almost same\n    - The mindom is varies in distribution and male data points have a lot of outliers\n    - The maxdom and dfrange are also similar\n    - The modindx is same","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Can we find a difference in resonance between male and female voices? <br>\n\nThere are a number of factors which determine the resonance characteristics of a resonator. Included among them are the following: size, shape, type of opening, composition and thickness of the walls, surface, and combined resonators. The quality of a sound can be appreciably changed by rather small variations in these conditioning factors\n\nSource: Wikipedia","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Can we identify falsetto from regular voices? (separate data-set likely needed for this)\n\nYes, a separate dataset will be needed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#check the correlation between features\n#bivariate\ndata.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,15))\nsns.heatmap(data.corr(), annot=True, fmt='.2g')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Inference: </b>\n\n#### Are there other interesting features in the data?\n\n- Multi-collinearity is really high as coefficient constant is high for feature variables\n- These can effect our models if we use Logistic, Linear Classifiers\n    ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking for more than .50 and -.50 correlation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,15))\nsns.heatmap(data.corr(), annot=True, fmt='.2g', mask=~(((data.corr()) <=-.50) | ((data.corr())>=.50)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Since SVM are not that affected by multicollinearity, we will go to model building and cross validation process.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Before that we will check for class imbalance as well","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The number of male in our output is: ',data[data['label']=='male'].shape[0])\nprint('The number of female in our output is: ',data[data['label']=='female'].shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Step 3: Preprocessing and Model builiding","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First let us convert the object 'label' column type to category type","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data.iloc[:, -1]\n\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\ny","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data standardization: Since SVM deals with distance to classify we need to standardize the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nX = data.iloc[:,:-2]\nstd_scaler = StandardScaler()\nstd_scaler.fit(X)\n\nX = std_scaler.transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting into train test\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.3, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First we shall try on default parameters","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Default SVM (RBF)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn import metrics\n\nsvc = SVC() #default parameters\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\n\nprint(f'The score for this model {svc.__class__.__name__} is {metrics.accuracy_score(y_test, y_pred)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Default SVM  (Linear)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC(kernel='linear') #default parameters\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\n\nprint(f'The score for this model {svc.__class__.__name__} is {metrics.accuracy_score(y_test, y_pred)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Default SVM (Polynomial)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC(kernel='poly') #default parameters\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\n\nprint(f'The score for this model {svc.__class__.__name__} is {metrics.accuracy_score(y_test, y_pred)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The polynomial kernel did not do so well as compared to other kernels, but all have a high accuracy score\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We need to check with <b>K fold cross validation </b> that if the results are same when we split the training and testing more times.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Default SVM (RBF)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nsvc = SVC()\nscore = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\nprint(score)\nprint('The mean accuracy for the model on 10 K fold cross validation is: {%.3f}'%score.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Default SVM (Linear)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC(kernel='linear')\nscore = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\nprint(score)\nprint('The mean accuracy for the model on 10 K fold cross validation is: {%.3f}'%score.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Default SVM (Polynomial)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC(kernel='poly')\nscore = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\nprint(score)\nprint('The mean accuracy for the model on 10 K fold cross validation is: {%.2f}'%score.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>After K fold cross validation we get accuracy score for RBF and Linear as .97 and polynomial as .94</b>\n\nThe cross validation splits the data into train and test a number of times (here cv is 10) and gives us an accuracy score. Since scores are dependent on the data and the how the split occurred. Using cross validation we can reduce that error.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Taking different values of C and checking which is performing better","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The C parameter trades off correct classification of training examples against maximization of the decision functionâ€™s margin. For larger values of C, a smaller margin will be accepted if the decision function is better at classifying all training points correctly. A lower C will encourage a larger margin, therefore a simpler decision function, at the cost of training accuracy.\n\nReference: https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For large value of C, I am basically looking for every point to be correctly classified. I am not concerned with the width of the margin.\n\nFor small value of C, I am basically looking for the widest width between clusters of data points and do not mind misclassification","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Putting different C and checking the result for linear model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"C_range = list(range(1,26))\n\n\nacc_score = []\n\nfor i in C_range:\n    svc = SVC(kernel='linear', C=i)\n    score = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\n    acc_score.append(score.mean())\nprint('The best mean accuracy for the model on 10 K fold cross validation with a range of C value (0-25) is: {} and index {}'.format(max(acc_score), acc_score.index(max(acc_score))))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting a graph\n\nplt.plot(C_range, acc_score)\nplt.xticks(np.arange(0,27,2))\nplt.xlabel('C values')\nplt.ylabel('Cross-Validated Accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fine tuning to see which c is the best","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"C_range = list(np.arange(7,13,.1))\n\nacc_score = []\n\nfor i in C_range:\n    svc = SVC(kernel='linear', C=i)\n    score = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\n    acc_score.append(score.mean())\nprint('The best mean accuracy for the model on 10 K fold cross validation with a range of C value (0-25) is: {} and index {}'.format(max(acc_score), acc_score.index(max(acc_score))))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.plot(C_range, acc_score)\nplt.xticks(np.arange(7,14,1))\nplt.xlabel('C values')\nplt.ylabel('Cross-Validated Accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Inference:</b>\nWe have a range of C's (7,12) that have the same accuracy. C is the number that when we increase, we tell the classifer that we want all points to be correctly classified, hence the width will be small. \n\nWe also are testing with linear kernel, which is not much affected by C and Gamma as you see below. (for this dataset)\n\nAlso the model evaluation is done on accuracy, which is (true positive + true negative) / (true positive + true negative + false positive + false negative)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking Gamma for kernel=linear","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gamma_range = [.00001,.0001,.001,.01,.1,1,10,100]\n\n\nacc_score = []\n\nfor i in gamma_range:\n    svc = SVC(kernel='linear', gamma=i)\n    score = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\n    acc_score.append(score.mean())\n# print('The best mean accuracy for the model on 10 K fold cross validation with a range of C value (0-25) is: {} and index {}'.format(max(acc_score), acc_score.index(max(acc_score))))\nacc_score  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Gamma has no effect on the model with kernel as linear","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking c and Gamma for kernel=rbf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"C_range = list(range(1,25))\n\n\nacc_score = []\n\nfor i in tqdm(C_range):\n    svc = SVC(kernel='rbf', C=i)\n    score = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\n    acc_score.append(score.mean())\n# print('The best mean accuracy for the model on 10 K fold cross validation with a range of C value (0-25) is: {} and index {}'.format(max(acc_score), acc_score.index(max(acc_score))))\n\nacc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(C_range, acc_score)\nplt.xticks(np.arange(1,26,1))\nplt.xlabel('C values')\nplt.ylabel('Cross-Validated Accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gamma_range = [.00001,.0001,.001,.01,.1,1,10,100]\n\n\nacc_score = []\n\nfor i in gamma_range:\n    svc = SVC(kernel='rbf', gamma=i)\n    score = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\n    acc_score.append(score.mean())\nprint('The best mean accuracy for the model on 10 K fold cross validation with a range of gamma is: {} and index {}'.format(max(acc_score), acc_score.index(max(acc_score))))\nacc_score  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(gamma_range, acc_score)\n# plt.xticks(np.arange(0,9))\nplt.xlabel('C values')\nplt.ylabel('Cross-Validated Accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"C and gamma change on every variation, with C between 1-2 giving us the highest accuracy. Gamma = .01 gives us the highest model accuracy. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<b>Taking both gamma and C value together</b>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gamma_range = [.00001,.0001,.001,.01,.1,1,10,100]\nC_range = list(range(1,25))\n\n\nacc_score = []\n\n\nfor j in tqdm(gamma_range):\n    for i in C_range:\n        svc = SVC(kernel='rbf', C=i, gamma=j)\n        score = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\n        acc_score.append(score.mean())\n# print('The best mean accuracy for the model on 10 K fold cross validation with a range of C value (0-25) is: {} and index {}'.format(max(acc_score), acc_score.index(max(acc_score))))\n\ntemp = pd.DataFrame(acc_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp['gamma_C'] = [(x,y) for x in gamma_range for y in C_range]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(temp[0])\n# plt.xticks(np.arange(0,27,2))\nplt.xlabel('C values')\nplt.ylabel('Cross-Validated Accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp.sort_values(by=0,ascending=False)\n\ntemp.iloc[75,:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The highest accuracy of .969 with kernel as rbf is with gamma = 0.01 and C = 4","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Using the default parameters we did not get a high accuracy for the model with poly kernel. But we can see if there is any change when we have different degrees.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"degrees = [2,3,4,5,6]\n\nacc_score = []\n\nfor i in degrees:\n    svc = SVC(kernel='poly', degree=i)\n    score = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\n    acc_score.append(score.mean())\n\nprint('The mean accuracy for the model on 10 K fold cross validation is: {}'.format(acc_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot( degrees, acc_score)\nplt.xlabel('Power')\nplt.ylabel('Cross-Validated Accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accuracy is highest for degree = 3 and goes down. As you increase the degree the complexity of the model increases and may cause overfitting.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### The best accuracy was with rbf kernel model using gamma with gamma = 0.01 and C = 4","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Checking on f1 and roc_auc score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC(kernel='rbf', C=4, gamma=.01)\nscore = cross_val_score(svc, X, y, cv=10, scoring='f1')\nscore.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC(kernel='rbf', C=4, gamma=.01)\nscore = cross_val_score(svc, X, y, cv=10, scoring='roc_auc')\nscore.mean()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}