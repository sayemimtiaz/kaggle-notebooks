{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from IPython.display import HTML\ndisplay(HTML('<marquee direction=\"down\" width=100% height=300 behavior=\"alternate\" >\\\n             <marquee height=300 style = \"color: red; font-size : 100px;\" behavior=\"alternate\" >\\\n    <b>Trump vs. Biden!</b>\\\n             </marquee>\\\n             </marquee>'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analysis of First and Second Presidential Debates in 2020\n\nIn this notebook we're trying to analyze a hot dataset that's filled with political topics and mystries! To me as someone who got both confused and intrigued watching the debates, nothing was more interesting than figuring out the character and personality of each debater by analyzing their speech!\n\nSo I picked up my laptop and searched through the internet and could find the script of both first and second debate, but hopefully a great soul had already put it in a neat csv file on [Kaggle](https://www.kaggle.com/headsortails/us-election-2020-presidential-debates) so a bit of the hassle was already saved! \n\nIn this notebook I want to present what I found interesting as I went through their talk. I used different packages and methods to analyze the data as best as I could, so I'll talk about libraries like TextBlob, Transformers, nltk and so many others and I'll give you an example of how each can be used and what advantages or disadvantages each has.\n\nFirstl, we need to import and install some packages and libraries."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%HTML\n<a id=\"Analysis\"></a>\n<center>\n<iframe width=\"700\" height=\"315\" src=\"https://www.youtube.com/embed/kyuDlnYGGQI\"\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" style=\"position: relative;top: 0;left: 0;\" allowfullscreen ng-show=\"showvideo\"></iframe>\n</center>","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# For math and analyzing dataframes\nimport numpy as np\nimport pandas as pd\n\n\n# For analyzing text\nimport spacy\nfrom spacy import displacy\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\nimport nltk\nimport string\nimport regex as re\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\nfrom collections import Counter \n\n# TextBlob and its classifier\nfrom textblob import TextBlob \nfrom textblob.classifiers import NaiveBayesClassifier \n\n# For vis\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport warnings\nimport os\n%matplotlib inline\n\n# For time column\nimport datetime\n\n# For word cloud\nfrom PIL import Image\nfrom wordcloud import WordCloud ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"pip install -U textblob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!python -m textblob.download_corpora","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!python -m pip install -U pip\n!pip install wheel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"pip install plotly==4.12.0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Take a look at the datasets:"},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/us-election-2020-presidential-debates'\n\nfirst = pd.read_csv(path + '/us_election_2020_1st_presidential_debate.csv')\nsecond = pd.read_csv(path + '/us_election_2020_2nd_presidential_debate.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first[100:105]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"second[50:55]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analysis Begins!\n\n### **Null Values**\nFrom the code below, we can see how many null values exist in each column and dataset. As you can see there is only one null value and it is in the first dataset, in the minute column. This column displays the starting point of each person's utterance. After searching through the first dataset we understand that it belongs to the starting point of the second part. So we can simply substitute it with 00:00.\n\nAlso for more coherence and simplicity we change the names of debaters to '**Donald Trump**' and '**Joe Biden**' and the mediators to '**mediator_1**' and '**mediator_2**'."},{"metadata":{"trusted":true},"cell_type":"code","source":"null_df = pd.DataFrame(pd.concat([first.isnull().sum(), second.isnull().sum()], axis = 1))\nnull_df.columns = ['first', 'second']\nnull_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first.iloc[178:181,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first.loc[first.minute.isnull(), 'minute'] = '00:00'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('names in the first dataset:', (first.speaker.unique()))\nprint('names in the second dataset:', (second.speaker.unique()))\n\nfirst.loc[first.speaker.str.contains('Chris Wallace:'), 'speaker'] = 'Chris Wallace' # correcting the typo in the name\n\n# changing their names for more simplicity and coherence in two datasets\nfirst.loc[first.speaker.str.contains('Vice President Joe Biden'), 'speaker'] = 'Joe Biden'\nfirst.loc[first.speaker.str.contains('President Donald J. Trump'), 'speaker'] = 'Donald Trump'\n\nfirst.loc[first.speaker.str.contains('Chris Wallace'), 'speaker'] = 'mediator_1'\nsecond.loc[second.speaker.str.contains('Kristen Welker'), 'speaker'] = 'mediator_2'\n\nprint('Modified names in the first dataset:', (first.speaker.unique()))\nprint('Modified names in the second dataset:', (second.speaker.unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **The Minute Column**\n\nIn this part we're trying to have a consistent timeframe instead of having two parts we'll have one that covers all their speaches. So what we do is that we parse the minute column into hour, minute and second and then give a specific format to all.\n\nFor each cell(paragraph) we calculate the time difference between this paragraph and the previous one. And then add this difference to the previous time to calculate the current time. But because we have two parts and the second part starts at 00:00, we again start calculating from zero in the beggining of the second part, but we keep the last value of the first part(the overal time of the first debate in seconds) and also the index of this last paragraph so we can find from what row we should start adding the time of the first part to the values of the second part.\n\nThis was my way of finding the consecutive time frame, but if you had any better ideas please let me know I'd love to hear your comment and suggestions!"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# making the time consecutive\n\n# First Debate\n\nfirst['seconds'] = 0 # we assume we start from 0\n                  # and then add the values accordingly\n\n\nfor i, tm in enumerate(first.minute[1:],1):\n    timeParts = [int(s) for s in str(tm).split(':')]\n    \n    # when we have hour like 01:10:50\n    if (len(timeParts)>2) and (i<len(first)):\n        \n        current = (timeParts[0] * 60 + timeParts[1]) * 60 + timeParts[2]\n        difference = current - first.loc[i-1, 'seconds']\n        first.loc[i, 'seconds'] = first.loc[i-1, 'seconds'] + difference\n    # when we get to the second half of the debate\n    elif str(tm) == '00:00' :\n        first.loc[i, 'seconds'] = 0\n        second_round_idx = i\n        second_round_final_time = first.loc[i-1, 'seconds']\n\n    # when there's only minute and seconds like 10:50\n    elif (i<len(first)):\n        current = timeParts[0] * 60 + timeParts[1]\n        difference = current - first.loc[i-1, 'seconds']\n        first.loc[i, 'seconds'] = first.loc[i-1, 'seconds'] + difference\n\nfirst.loc[second_round_idx:, 'seconds'] += second_round_final_time\n\n\n# Second Debate\n\nsecond['seconds'] = 0 \n\nfor i, tm in enumerate(second.minute[1:],1):\n    timeParts = [int(s) for s in str(tm).split(':')]\n    \n    # when we have hour like 01:10:50\n    if (len(timeParts)>2) and (i<len(second)):\n        \n        current = (timeParts[0] * 60 + timeParts[1]) * 60 + timeParts[2]\n        difference = current - second.loc[i-1, 'seconds']\n        second.loc[i, 'seconds'] = second.loc[i-1, 'seconds'] + difference\n\n    # when we get to the second half of the debate\n    elif str(tm) == '00:00' :\n        first.loc[i, 'seconds'] = 0\n        second_round_idx = i\n        second_round_final_time = second.loc[i-1, 'seconds']\n    # when there's only minute and seconds like 10:50\n    elif (i<len(second)):\n        current = timeParts[0] * 60 + timeParts[1]\n        difference = current - second.loc[i-1, 'seconds']\n        second.loc[i, 'seconds'] = second.loc[i-1, 'seconds'] + difference\n\nsecond.loc[second_round_idx:, 'seconds'] += second_round_final_time\n\n\n\nfirst['minutes'] = first.seconds.apply(lambda x:x//60)\nsecond['minutes'] = second.seconds.apply(lambda x:x//60)\n\n# We use this format of %h:%m:%s by using the following command\nfirst['time'] = first.seconds.apply(lambda x:str(datetime.timedelta(seconds=x)))\nsecond['time'] = second.seconds.apply(lambda x:str(datetime.timedelta(seconds=x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first[55:60]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Heat of The Discussion with <span style =\"color :red\">Heat Map</span>!\n\nThis part came into my mind when I saw how many times the candidates were interupting in eachothers speach. So for seeing which parts were the candidates most anxious to talk or to interupt each other, I plotted a heatmap for each debate. I found another cool representation of heatmap in [this article](https://towardsdatascience.com/1st-presidential-debate-by-the-numbers-dee50b35f4ac), although the author uses multiple resources (and not just python) for creating the plot.\n\nIn the following plot the darker the color, the more times each one started talking(or even interupted each other). As you can see there are times that both speakers start talking more than what is usuall and normal. I consider a discussion normal when you see no color(silence or Nan) for one speaker when the other is colored(is speaking).\n\nGenerally speaking, in both debates it looks like there are **three parts** where the candidates start firing at each other, one after the introductions and warming up, another in the middle and one around 15 minutes before the end where they may try to prove their points by talking faster and persumably finish off strong!\n\nAlso after each heated discussion, we can see that they cool down and talk normally for about 20 minutes."},{"metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"heat = first.groupby(['minutes', 'speaker']).count().reset_index()\nfig = go.Figure(data=go.Heatmap(\n                z=heat.minute,\n                x=heat.minutes,\n                y=heat.speaker,\n                colorscale='Viridis_r',\n                colorbar=dict(\n                title=\"Heat of the discussion\",\n                titleside=\"top\",\n                tickmode=\"array\",\n                tickvals=[1, 4, 10],\n                ticktext=[\"very cool\", \"normal\", \"Hot!\"],\n                ticks=\"outside\"\n    )\n        ))\n\nfig.update_layout(title='First Debate: # of times each one talks in each minute',\n                 xaxis_nticks=36)\n\n\nfig.show()\n\n# Create and show figure\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"heat = second.groupby(['minutes', 'speaker']).count().reset_index()\nfig = go.Figure(data=go.Heatmap(\n        z=heat.minute,\n        x=heat.minutes,\n        y=heat.speaker,\n        colorscale='Viridis_r',\n        colorbar=dict(\n        title=\"Heat of the discussion\",\n        titleside=\"top\",\n        tickmode=\"array\",\n        tickvals=[2, 5, 10],\n        ticktext=[\"very cool\", \"normal\", \"Hot!\"],\n        ticks=\"outside\"\n    )\n        ))\n\nfig.update_layout(title='Second Debate: # of times each one talks in each minute',\n                 xaxis_nticks=36)\n\nfig.show()\n\n# Create and show figure\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Go Sentence Level\n\nNow let's use a sentence tokenizer and analyze the sentences used in each debate. Below I used NLTK sent_detector for this task and also added one another field called number_of_sents for the length of each sentence used by each every time they start speaking."},{"metadata":{"trusted":true},"cell_type":"code","source":"# we want to analyze the debate based on the sentences\n# so we use a sentence level tokenizer from nltk\nsent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n\n# number of sentences used by each person, each time their allowed to talk\nfirst['number_of_sents'] = first.text.apply(lambda x:len(sent_detector.tokenize(x)))\nsecond['number_of_sents'] = second.text.apply(lambda x:len(sent_detector.tokenize(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summing up the number of sentences\nnumber_of_sentences_1 = first.groupby(['speaker']).sum()[['number_of_sents']].reset_index()\nnumber_of_sentences_2 = second.groupby(['speaker']).sum()[['number_of_sents']].reset_index()\n# total number of sentences in each debate\ntotal_num_sents_1 = number_of_sentences_1.sum().number_of_sents \ntotal_num_sents_2 = number_of_sentences_2.sum().number_of_sents \n# percentage of conversation dominated by each candidate\n# based on the number of sentences they used\nnumber_of_sentences_1.loc[:, 'percentage'] = number_of_sentences_1.number_of_sents.apply(lambda x:round(x/total_num_sents_1, 2))\nnumber_of_sentences_2.loc[:, 'percentage'] = number_of_sentences_2.number_of_sents.apply(lambda x:round(x/total_num_sents_2, 2))\n\nnumber_of_sentences_1","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=2, cols=2,\n                    specs=[[{\"rowspan\": 2}, {}]\n                            ,[None,         {}]],\n                    subplot_titles=(\"# of sentences in total\",\"First Debate\", \"Second Debate\"))\n\nfig.add_trace(go.Bar(x=['First Debate', 'Second Debate'], \n                     y=[total_num_sents_1, total_num_sents_2],\n                     text =[total_num_sents_1, total_num_sents_2]),\n                     row=1, col=1)\n\nfig.add_trace(go.Bar(x=number_of_sentences_1.speaker,\n                     y=number_of_sentences_1.number_of_sents,\n                     text =number_of_sentences_1.percentage),\n                     row=1, col=2)\nfig.add_trace(go.Bar(x=number_of_sentences_2.speaker,\n                     y=number_of_sentences_2.number_of_sents,\n                     text =number_of_sentences_2.percentage),\n                     row=2, col=2)\n\nfig.update_traces(textposition='outside', textfont_size=14)\nfig.update_layout(showlegend=False, title_text=\"Number of sentences in Both debates\")\nfig.update_yaxes(title_text='count')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"while analyzing this plot, I though that the number of sentences is not showing all the characteristics of the debaters because you don't know how many words they used in each sentence. Like when you say \"what?!\" or \"Oh, really?!\" you are more likely someone who interupts more and uses less words for each sentence. So this time we simply use a word tokenizer and see how each character presents himself!\nFor a better understanding of the debaters you can click on the mediator_1 and mediator_2 to make them disappear and look at what is left."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=2, cols=2,\n                    specs=[[{\"colspan\": 2}, None]\n                            ,[{\"colspan\": 2}, None]],\n                    subplot_titles=(\"First Debate\", \"Second Debate\"))\n#-------------------------------------------------- First Debate -----------------------------------------------------#\nfig.add_trace(go.Histogram(\n    x=first[first.speaker == 'Donald Trump'].number_of_sents,\n    name='Trump_1',  xbins=dict(start=-1, end=24, size=1),\n    marker_color='red', opacity=0.75),\n    row = 1, col = 1)\n\nfig.add_trace(go.Histogram(\n    x=first[first.speaker == 'Joe Biden'].number_of_sents,\n     name='Biden_1', xbins=dict(start=-1, end=24, size=1),\n    marker_color='#3498DB', opacity=0.75),\n    row = 1, col = 1)\n\nfig.add_trace(go.Histogram( \n    x=first[first.speaker == 'mediator_1'].number_of_sents,\n    name='mediator_1', xbins=dict(start=-1, end=24, size=1),\n    marker_color='#5D6D7E', opacity=0.75),\n    row = 1, col = 1)\n#-------------------------------------------------- Second Debate -----------------------------------------------------#\n\nfig.add_trace(go.Histogram(\n    x=second[second.speaker == 'Donald Trump'].number_of_sents,\n    name='Trump_2',  xbins=dict(start=-1, end=24, size=1),\n    marker_color='red', opacity=0.75),\n    row = 2, col = 1)\n\nfig.add_trace(go.Histogram(\n    x=second[second.speaker == 'Joe Biden'].number_of_sents,\n     name='Biden_2', xbins=dict(start=-1, end=24, size=1),\n    marker_color='#3498DB', opacity=0.75),\n    row = 2, col = 1)\n\nfig.add_trace(go.Histogram( \n    x=second[second.speaker == 'mediator_2'].number_of_sents,\n    name='mediator_2', xbins=dict(start=-1, end=24, size=1),\n    marker_color='#5D6D7E', opacity=0.75),\n    row = 2, col = 1)\n\n\n\n\nfig.update_layout(\n    title_text='First Debate: Histogram of # of Sentences Each One Used Each Time',\n    yaxis_title_text='Count', \n    bargap=0.1, bargroupgap=0.1)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well as we can see the Trump is the one who uses the smallest and also the biggest number of sentences each time he starts talking. In comparison to Biden we can see that he uses one sentence, 20 to 30 times more which shows how he tends to interupt his apponenet or simply answer shortly with one ore two sentences(which I think is not the case because he uses long paragraphs more than Biden ,too)."},{"metadata":{},"cell_type":"markdown","source":"# Go Word Level\n\nIn order to explore the lexicon used by candidates we simply take all the text and try to clean the unnecessary words and punctuations like 'is', 'in', 'at', etc. as much as possible to get to the gist of what they're trying to say. Below you'll see a function called clean() and you may notice arguments like 'http' although we don't have any links in this dataset.\n\nThis is because this function is a general custom function that I built for myself and I usually use it with a few tweeks here and there to adapt it to the dataset. I wanted to share the whole thing here so you can use it for cleaning other datasets ,as well. So don't get confused and just set each one to be **True**, **False**, or whatever you wish."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Spacy packages\nsp = spacy.load('en_core_web_sm') \nspacy_st = list(nlp.Defaults.stop_words) # 362 stop words \n# NLTK packages\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk_st = stopwords.words('english') # 179 stop words\n\n# some additional pucntuations observed in the dataset\npunc = '‚Äô‚Äù‚Äú‚Ä¶'.join(string.punctuation)\n\n\n# a general cleaning function\ndef clean(t, lower = False, http = False, punct = False,\n          lem = False, stop_w = False, num = False,\n          custom_st = ['a','the', 'and', 'there', 'that', 'this', 'am', 'on',\n                       'if', 'it', 'to', 'at' 'a', 'of', 'in', 'out', 'were',\n                       'was', 'do', 'did', \"don't\",\"didn't\", 'be', 'are', 'is',\n                       'being', \"it's\", 'have', 'had', 's', 'j', 't', 're',\n                       'at', 'with', 'just', 'now', \"can't\", 'can', 'up',\n                       'as', 'from', 'thing', 'by', 'so', 'here', 'will', 'for']):\n\n    if lower:\n        t = t.lower()\n    \n    if http:\n        t = re.sub(\"https?:\\/\\/t.co\\/[A-Za-z0-9]*\", '', t)\n\n    # lemmitizing\n    if lem:\n        # spacy replaces pronouns with '-PRON-' and we don't want that to happen\n        # so we lemmatize everything exept words that are recognized as pronouns\n        lemmatized = [word.lemma_ if word.lemma_ !='-PRON-' else word.text for word in sp(t)]\n        t = ' '.join(lemmatized)\n\n    # stop words\n    if stop_w == 'nltk':\n        t = [word for word in word_tokenize(t) if not word.lower() in nltk_st]\n        t = ' '.join(t)\n\n    elif stop_w == 'spacy':\n        t = [word for word in word_tokenize(t) if not word.lower() in spacy_st]\n        t = ' '.join(t)\n        \n    elif stop_w == 'custom':\n        t = [word for word in word_tokenize(t) if not word.lower() in custom_st]\n        t = ' '.join(t)\n\n    # punctuation removal\n    if punct:\n        t = t.translate(str.maketrans('', '', punc))\n    if num:\n        t = re.sub(\"[0-9]\",\"\", t)\n    # removing extra spaces and letters\n    t = re.sub(\"\\s+\", ' ', t)\n    t = re.sub(\"\\b\\w\\b\", '', t)\n    return t\n\nfirst['cleaned_text'] = first.text.apply(lambda x: clean(x, lower = True, http = False,\n                                                         punct = True, lem = True, \n                                                         stop_w = 'custom', num = True))\nsecond['cleaned_text'] = second.text.apply(lambda x: clean(x, lower = True, http = False, \n                                                           punct = True, lem = True, \n                                                           stop_w = 'custom', num = True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because we want to findout about the most common words, we simply split all the words in the cleaned_text column and then with the most_common() function, we count and sort all the most frequently used words by each candidate in both debates. I applied the most_common function on concatinated version of first and second dataframes, but you can do the same for each separately. ***(I did so at first but there wasn't much difference. Turns out people have the propensity to use the same lexicon and structure, given different topics and situations.)***"},{"metadata":{"trusted":true},"cell_type":"code","source":"first['words'] = first['cleaned_text'].apply(lambda x:x.split())\nsecond['words'] = second['cleaned_text'].apply(lambda x:x.split())\n\ndef most_common(df, name, top_n):\n    list_ = []\n    for w in df[df.speaker == name].words:\n        list_.extend(w)\n    Counter_1 = Counter(list_) \n    most_occured = Counter_1.most_common(top_n) \n    return most_occured\n\nTrump_w = most_common(pd.concat([first, second], axis = 0), 'Donald Trump', 60)\nBiden_w = most_common(pd.concat([first, second], axis = 0), 'Joe Biden', 60)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you look closer to the treemap above, you'll notice that each person has a different way of referring to the other. Trump is more likely to address his opponent directly, while Biden tends to talk more to the mediator and use the pronoun 'he' to address Trump. From what I see, they mostly are using the same words, but in different ways. For example, of all the top 60 words Trump uses, around 10 percent is the pronoun 'I' (nearly twice as much as Biden does), another 10 percent is the pronoun 'you', while Biden tends to use words more broadly than Trump by not having any word that dominates more than 7 percent of his commonly used words."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = make_subplots(\n    cols = 2, rows = 1,\n    column_widths = [0.5, 0.5],\n    subplot_titles = ('most common words used by: <b>Trump', 'most common words used by: <b>Biden'),\n    specs = [[{'type': 'treemap', 'rowspan': 1}, {'type': 'treemap'}]]\n)\n\nfig.add_trace(go.Treemap(\n    labels = [k for k,v in Trump_w],\n    parents = ['Trump']*100,\n    values = [v for k,v in Trump_w],\n    textinfo = \"label+value+percent parent\",\n    ),\n              row = 1, col = 1)\n\nfig.add_trace(go.Treemap(\n\n    labels = [k for k,v in Biden_w],\n    parents = ['Biden']*100,\n    values = [v for k,v in Biden_w],\n    textinfo = \"label+value+percent parent\",\n    outsidetextfont = {\"size\": 20, \"color\": \"darkblue\"},\n    marker = {\"line\": {\"width\": 2}}),\n              row = 1, col = 2)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word Cloud\n\nHaving all these words, we can use the most common words used by each candidate to make a word cloud for each. For a word cloud in a picture(or a mask) you have to have two things:\n 1. Provide the words \n 2. Provide the mask(the picture)\n\nYou can use the images named 'Trump.png' and 'Biden.png' from my repository on github or make your own image. Just remember that they have to have transparent backgrounds."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"tweet_mask = np.array(Image.open(\"../input/analysis-of-trump-biden-debates/Biden.png\"))\nwc = WordCloud(collocations=False,\n               background_color=\"black\",\n               max_words=200,\n               mask = tweet_mask,\n               contour_color='yellow',\n               contour_width=20,)\n\n# Generate a wordcloud\nwc.generate(' '.join([k for k,v in Biden_w]))\n\n\n# show\nplt.figure(figsize=[20,10])\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"\ntweet_mask = np.array(Image.open(\"../input/analysis-of-trump-biden-debates/Trump.png\"))\nwc = WordCloud(collocations=False,\n               background_color=\"black\",\n               max_words=200,\n               mask = tweet_mask,\n               contour_color='yellow',\n               contour_width=10,)\n\n# Generate a wordcloud\nwc.generate(' '.join([k for k,v in Trump_w]))\n\n\n# show\nplt.figure(figsize=[20,10])\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now we get to the Sentiment Analysis\n\n We want to analyze each sentence so once again we use the sent_tokenizer and tokenize paragraphs into sentences but this time we put each sentence in one row to have a better understanding of our data. In this part we use packages like TextBlob, ü§ó Transformers Pipelines and models.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of sentences in each cell\nlens_1 = first.number_of_sents\nlens_2 = second.number_of_sents\n\n# making a long list of all sentences\nlist_1 =[]\nlist_2 = []\nfor x in first.text.apply(lambda x:sent_detector.tokenize(x)):\n    list_1.extend(x)\nfor x in second.text.apply(lambda x:sent_detector.tokenize(x)):\n    list_2.extend(x)\n\n# create new dataframes, repeating as appropriate\nfirst_sent = pd.DataFrame({'speaker': np.repeat(first.speaker, lens_1),\n                            'time': np.repeat(first.time, lens_1),\n                            'sent': list_1})\nsecond_sent = pd.DataFrame({'speaker': np.repeat(second.speaker, lens_2),\n                            'time': np.repeat(second.time, lens_2),\n                            'sent': list_2})\nfirst_sent.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"display(HTML('<dev direction=\"down\" width=100% height=\"40\" behavior=\"still\" >\\\n<dev  style = \"  font-size : 50px;\" behavior=\"still\" >\\\n  <b>üëà  Polarity and Subjectivity üëâ </b>\\\n  </dev>\\\n</dev>'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **TextBlob**\nTextBlob is a library specified for textual data and also common tasks related to natural language processing. In this notebook, we only want to use the sentiment property to see how each candidate expresses their feelings and also we'll see whether this is the best package for doing so.\n\\\n**Polarity** is a value between -1 and 1 that shows how positive or negative the sentiment of a sentence is. What TextBlob does is that it first identifies the words that are in its [lexicon](https://github.com/sloria/TextBlob/blob/eb08c120d364e908646731d60b4e4c6c1712ff63/textblob/en/en-sentiment.xml) and averages the polarity of all different meanings of one word and then multiplies these average with each other and gives us that number as the polarity of the whole sentence. [This is a great blog post](https://planspace.org/20150607-textblob_sentiment/) where you can read more on how it is calculated.\n\n\nBut because polarity is a value and not a concrete label, I decided to divide the spectrum into 5 different groups and give a lable to each:\n\n0 - negative\n\n 1 - somewhat negative \n \n 2 - neutral \n \n 3 - somewhat positive \n \n 4 - positive\n \n**Subjectivity** is a factor between 0 and 1, showing how subjective a word or phrase is.\n\n(Pretty obvious! I know.)\n\nAdvantages of TextBlob: TextBlob provides you with fast and easy to use tools to analyze the overal sentiment and subjectivity of sentences and phrases in your data. But as we said it has a pretty naive approach compared to deep learning models. Let's see an example of how it may disappoint us. Take a look at the following sentence:\n\n\n***Fewer people are dying every day***\n\nAs human being we can understand that this sentence is evidently conveying a positive messege. But remember that we said TextBlob only takes into account words and phrases that are in its lexicon and then multiplies their polarity. Well it turns out that its vocabulary does not include words like \"fewer\" or \"dying\" and even after lemmatization and getting the following sentence:\n\n***Few people are die every day***\n\nyou won't be able to recognize this sentence as a positive one because again, \"die\" is not included in its vocabulary.\nSo for sentences that don't have a definite negative or positive tone and can be a little like facts, polarity doesn't do a great job, but subjectivity if the sentence is short and the tone clear, it can show good results as you can see in the table bellow. Go ahead and experiment yourself with different words and sentences to see how it performs.\n\n### more info:\nYou may say that we can change this and it is true we can do something rather different. Hopefully TextBlob provides us with a NaiveBayesClassifier that you can train on your own and then ask it to classify your text based on that labeled data.\n\nBut still even this classifier isn't our best bet as it won't help us with polarity and we have to provide a lot of data to a model that is not the best model for text classification.\nThough you can find some datasets online([like these two from IBM datasets](https://www.research.ibm.com/cgi-bin/haifa/vst/debating_ds19.pl)) that have a large lexicon with specified polarity and maybe train your naive bayes on them to improve your classification. But as they are so much better models out there for sentiment classification, personally I won't get into its details.\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"TextBlob('Fewer people are dying everyday').sentiment.polarity","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# first df\nfirst_sent['polarity'] = first_sent.sent.apply(lambda x: TextBlob(x).polarity)\nfirst_sent['subjectivity'] = first_sent.sent.apply(lambda x: TextBlob(x).subjectivity)\nfirst_sent['sentiment'] = first_sent.polarity.apply(lambda x: 4 if x>0.6 else 3 if x>0.2 else 2 if x>-0.2 else 1 if x>-0.6  else 0)\n\n# second df\nsecond_sent['polarity'] = second_sent.sent.apply(lambda x: TextBlob(x).polarity)\nsecond_sent['subjectivity'] = second_sent.sent.apply(lambda x: TextBlob(x).subjectivity)\nsecond_sent['sentiment'] = second_sent.polarity.apply(lambda x: 4 if x>0.6 else 3 if x>0.2 else 2 if x>-0.2 else 1 if x>-0.6  else 0)\n\nfirst_sent.reset_index(drop = True, inplace = True)\nsecond_sent.reset_index(drop = True, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"both = pd.concat([first_sent, second_sent], axis = 0)\nfig = go.Figure()\nfig.add_trace(go.Histogram(\n    x=both[both.speaker == 'Donald Trump'].subjectivity,\n    name='Trump',  xbins=dict(start=-1, end=2, size=0.1),\n    marker_color='red', opacity=0.75))\n\nfig.add_trace(go.Histogram(\n    x=both[both.speaker == 'Joe Biden'].subjectivity,\n     name='Biden', xbins=dict(start=-1, end=2, size=0.1),\n    marker_color='#3498DB', opacity=0.75))\n\nfig.update_layout(\n    title_text=\"Number of Sentences used by Debaters with different Subjectivities\",\n    yaxis_title_text='Number of Sentences', \n    xaxis_title_text='Subjectivity',\n    bargap=0.1, bargroupgap=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we see here is that Trump has more sentences with 0 subjectivity(uses more neutral statements), but we cannot conclude that he talks less subjectivly because as we saw earlier he dominates around 40 percent of the conversations and therefore it is normal to see him have more neutral words. But interestingly, there is only one time when Biden wins over Trump by using twise as much sentences with 0.8 subjectivity on the right hand side of the graph."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"cmap = cmap=sns.diverging_palette(5, 250, as_cmap=True, )\n\nfirst_sent.loc[(first_sent['polarity']>0.6) | (first_sent['polarity']<-0.6),['speaker', 'sent', 'polarity']].head(15).style.background_gradient(cmap, subset=['polarity'])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=2, cols=2,\n                    specs=[[{\"rowspan\": 2}, {}]\n                            ,[None,         {}]],\n                    subplot_titles=(\"Both Debates and all sentences\",\n                                    \"First Debate individual candidates\", \n                                    \"Second Debate Debate individual candidates\"))\n\n#-------------------------------------------------- Both Debates -----------------------------------------------------#\nfig.add_trace(go.Histogram(\n    x=first_sent['sentiment'],\n    name='First Debate',  xbins=dict(start=-1, end=5, size=1),\n    marker_color='purple', opacity=0.75),\n    row = 1, col = 1)\n\nfig.add_trace(go.Histogram(\n    x=second_sent['sentiment'],\n     name='Second Debate', xbins=dict(start=-1, end=5, size=1),\n    marker_color='#ba8cd7', opacity=0.75),\n    row = 1, col = 1)\n#-------------------------------------------------- First Debate -----------------------------------------------------#\n\n\nfig.add_trace(go.Histogram(\n    x=first_sent[first_sent.speaker == 'Donald Trump']['sentiment'],\n    name='Trump_1',  xbins=dict(start=-1, end=5, size=1),\n    marker_color='red', opacity=0.75),\n    row = 1, col = 2)\n\n\nfig.add_trace(go.Histogram(\n    x=first_sent[first_sent.speaker == 'Joe Biden']['sentiment'],\n    name='Biden_1', xbins=dict(start=-1, end=5, size=1),\n    marker_color='#3498DB', opacity=0.75),\n    row = 1, col = 2)\n\nfig.add_trace(go.Histogram( \n    x=first_sent[first_sent.speaker == 'mediator_1']['sentiment'],\n    name='mediator_1', xbins=dict(start=-1, end=5, size=1),\n    marker_color='#5D6D7E', opacity=0.75),\n    row = 1, col = 2)\n#-------------------------------------------------- Second Debate -----------------------------------------------------#\n\nfig.add_trace(go.Histogram(\n    x=second_sent[second_sent.speaker == 'Donald Trump']['sentiment'],\n    name='Trump_2',  xbins=dict(start=-1, end=5, size=1),\n    marker_color='red', opacity=0.75),\n    row = 2, col = 2)\n\nfig.add_trace(go.Histogram(\n    x=second_sent[second_sent.speaker == 'Joe Biden']['sentiment'],\n     name='Biden_2', xbins=dict(start=-1, end=5, size=1),\n    marker_color='#3498DB', opacity=0.75),\n    row = 2, col = 2)\n\nfig.add_trace(go.Histogram( \n    x=second_sent[second_sent.speaker == 'mediator_2']['sentiment'],\n    name='mediator_2', xbins=dict(start=-1, end=5, size=1),\n    marker_color='#5D6D7E', opacity=0.75),\n    row = 2, col = 2)\n\n\n\n\nfig.update_layout(\n    title_text='First Debate: Histogram of # of Sentences Each One Used Each Time',\n    yaxis_title_text='Count', \n    bargap=0.1, bargroupgap=0.1)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is a visualization of polarity resulted from TextBlob, but I wasn't satisfied with this version of sentiment analysis after seeing all the flaws this package has and the naive approach it uses to classify and analyze the data. So I took another step and went over what could really classify this text as best as possible with all the tools that I have!"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"display(HTML('<dev width=100% height=\"40\" behavior=\"still\" >\\\n             <dev  style = \"  font-size : 50px;\" >\\\n             <b>ü§ó Transformers </b></dev></dev>'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we want to start using some ligit tools that are specified for text classification and sentiment analysis! I'll introduce pipeline and also a trained model to again find out the best way of classifying this debate.\n1. Pipelines:\n\n**Advantages:** Pipelines are tools provided by the transformers library to help us do a wide range of tasks with our textual data with only one or two lines of code! For example by using the pipeline function and just inputing our data and the name of the task we want to perform we can simply get the result we want without worrying about constructing a model or training it. Take a look at the code below to get the intuition.\n\nYou may wonder this function performs. well, the default model that is used by the transformers pipeline for sentiment analysis is a pretrained \"distilbert-base-uncased\" model that has been trained on sst2 (movie reviews dataset). So you are actually using transfer learning and predict new sets of data based on the weights that were trained by another person or organization. \n\nYou can see a live demo of sentiment classification [here](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english). The models are very veried and you can both use them here or see the live demo of every and each one in the [Huggingface website](https://huggingface.co/models?filter=text-classification&search=roberta-large).\n\n**Disadvantages:** All these models are pretty useful and good but remember all models are not as accurate or good as their other fellows! For example if you go with the default(distill bert model which is not the most accurate or best among transformers models) you can see that it will identify some sentences like\n\n **\"The audience here in the hall has promised to remain silent\"**\n\n as Negative with 0.92 percent certainty!\n\n Also, because these models were trained on a certain dataset with specified labels, you can't get anything different from what they can offer you. Like in this example if we want to have 5 labels, we won't be able to get it because the sst2 dataset was based on a dataset with 3 different labels: Negative, Neutral and Positive.\n\n Although there are a few flaws when you classify your text with distill bert, it is still a great way (I think better than polarity) for sentiment analysis as it is not simply averaging and takes into account the real meaning behind each word."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install transformers\nfrom transformers import pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentimentAnalysis = pipeline(\"sentiment-analysis\")\nprint(sentimentAnalysis(\"I have to say this is the coolest kernel ever\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# first dataset\nfirst_sent['pipeline_sentiment'] = first_sent.sent.apply(lambda x: sentimentAnalysis(x))\nfirst_sent['pipeline_score'] = first_sent.pipeline_sentiment.apply(lambda x:x[0]['score'])\nfirst_sent['pipeline_sentiment'] = first_sent.pipeline_sentiment.apply(lambda x:x[0]['label'])\n# second dataset\nsecond_sent['pipeline_sentiment'] = second_sent.sent.apply(lambda x: sentimentAnalysis(x))\nsecond_sent['pipeline_score'] = second_sent.pipeline_sentiment.apply(lambda x:x[0]['score'])\nsecond_sent['pipeline_sentiment'] = second_sent.pipeline_sentiment.apply(lambda x:x[0]['label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"second_sent.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(HTML('<dev direction=\"down\" width=100% height=\"40\" behavior=\"still\" >\\\n<dev  style = \"font-size : 50px;\" behavior=\"alternate\" >\\\n<b>Optional but Cool and Accurate results with RoBERTa ;) </b></dev></dev>'))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So as I said I really wanted to get the best and most accurate results and havev 5 labeled outputs to identify different sentiments as best as I could. So after searching a bit, I found[ this dataset on Kaggle](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data?select=sampleSubmission.csv). Although it contains classified text based on movie reviews, it's training set includes all different words and phrases separately so we can identify and learn the sentiment of each word, phrase and sentence using a partly customized model!\n\nThe best model that we can get our hands on, is a Large Roberta model that outperforms Large Bert, XLNet and DistillBert. (I actually experimented with all these models to see if it really is the best model and it turns out that it can give us great results with even a few number of epochs)\n\nBecause this notebook is dedicated to analysis and visualization I won't write the RoBERTa model code here but instead I'll represent you with the results. [Here is the Colab notebook](https://colab.research.google.com/drive/16F3RoWOGY1knLRdqMw2yHw68TdxpaImf?usp=sharing) with all you need to use any Tensorflow transformers model. You can see how I trained these models and use it to train your own models too!"},{"metadata":{"trusted":true},"cell_type":"code","source":"both = pd.read_csv('../input/analysis-of-trump-biden-debates/Trump_Biden_debates_sentiments.csv')\nfirst_sent = both.loc[:first.shape[0], :]\nsecond_sent = both.loc[first.shape[0]:, :]\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=2, cols=2,\n                    specs=[[{\"rowspan\": 2}, {}]\n                            ,[None,         {}]],\n                    subplot_titles=(\"Both Debates and all sentences\",\n                                    \"First Debate individual candidates\", \n                                    \"Second Debate Debate individual candidates\"))\n\n#-------------------------------------------------- Both Debates -----------------------------------------------------#\nfig.add_trace(go.Histogram(\n    x=first_sent['sentiment'],\n    name='First Debate',  xbins=dict(start=-1, end=5, size=1),\n    marker_color='purple', opacity=0.75),\n    row = 1, col = 1)\n\nfig.add_trace(go.Histogram(\n    x=second_sent['sentiment'],\n     name='Second Debate', xbins=dict(start=-1, end=5, size=1),\n    marker_color='#ba8cd7', opacity=0.75),\n    row = 1, col = 1)\n#-------------------------------------------------- First Debate -----------------------------------------------------#\n\n\nfig.add_trace(go.Histogram(\n    x=first_sent[first_sent.speaker == 'Donald Trump']['sentiment'],\n    name='Trump_1',  xbins=dict(start=-1, end=5, size=1),\n    marker_color='red', opacity=0.75),\n    row = 1, col = 2)\n\n\nfig.add_trace(go.Histogram(\n    x=first_sent[first_sent.speaker == 'Joe Biden']['sentiment'],\n    name='Biden_1', xbins=dict(start=-1, end=5, size=1),\n    marker_color='#3498DB', opacity=0.75),\n    row = 1, col = 2)\n\nfig.add_trace(go.Histogram( \n    x=first_sent[first_sent.speaker == 'mediator_1']['sentiment'],\n    name='mediator_1', xbins=dict(start=-1, end=5, size=1),\n    marker_color='#5D6D7E', opacity=0.75),\n    row = 1, col = 2)\n#-------------------------------------------------- Second Debate -----------------------------------------------------#\n\nfig.add_trace(go.Histogram(\n    x=second_sent[second_sent.speaker == 'Donald Trump']['sentiment'],\n    name='Trump_2',  xbins=dict(start=-1, end=5, size=1),\n    marker_color='red', opacity=0.75),\n    row = 2, col = 2)\n\nfig.add_trace(go.Histogram(\n    x=second_sent[second_sent.speaker == 'Joe Biden']['sentiment'],\n     name='Biden_2', xbins=dict(start=-1, end=5, size=1),\n    marker_color='#3498DB', opacity=0.75),\n    row = 2, col = 2)\n\nfig.add_trace(go.Histogram( \n    x=second_sent[second_sent.speaker == 'mediator_2']['sentiment'],\n    name='mediator_2', xbins=dict(start=-1, end=5, size=1),\n    marker_color='#5D6D7E', opacity=0.75),\n    row = 2, col = 2)\n\n\n\n\nfig.update_layout(\n    title_text='First Debate: Histogram of # of Sentences Each One Used Each Time',\n    yaxis_title_text='Count', \n    bargap=0.1, bargroupgap=0.1)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well now this is something! We can see how Trump outperforms Biden when it comes to positive comments in the second debate although in the first debate they were somehow equal. Overall,I think Biden tends to use more general and neutral words in his speach whereas trump sometimes tries to show his emotions with expressive and too positive or too negative commands!\n\nAll that you read in this notebook was what I could understand from what I see from the data. I do not oppose or confirm any candidate or political group. Also I'd love to hear from you guys and see what you think from all the plots above! So leave your comments here or on YouTube or Medium!"},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"border:2px solid blue; text-align: center\">Thanks for reading! Don't forget to upvote!</h1>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}