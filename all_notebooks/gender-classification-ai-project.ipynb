{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Classification Between Males And Females\n<b>  In this dataset, I have chosen to deal with the gender classification of human beings based on facial structure such as nose width, Forehead length, etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nsns.set()\nimport matplotlib.pyplot as plt \nfrom numpy import cov","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"person=pd.read_csv('../input/gender-classification-dataset/gender_classification_v7.csv')\nperson\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Explanation Of Features"},{"metadata":{},"cell_type":"markdown","source":"<b>long hair:</b> Boolean features. If the hair is short=0, if it's long=1.\\\n<b>forehead_width_cm:</b> The width of the forehead in cm.\\\n<b>forehead_height_cm:</b> The hught of the forehead in cm.\\\n<b>nose_wide:</b> Boolean features. If the nose is wide=1, if it's thin=0.\\\n<b>nose_long:</b> Boolean features. If the nose is short=0, if it's long=1.\\\n<b>lips_thin:</b> Boolean features. If the lips are thin =1, if they width=0.\\\n<b>distance_nose_to_lip_long:</b> boolean features. If the distance between the nose and the lip is short=0, if it's long=1.\\\n<b>gender:</b> Male or female\n\n<B>Please note </b>- I did not find according to what the ratio is defined as long and short, and therefore please assume that it was defined after a general average calculation of the relevant parameters in the population. \n\n    \n    "},{"metadata":{},"cell_type":"markdown","source":"## Information"},{"metadata":{"trusted":true},"cell_type":"code","source":"person.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> There is 5000 raws and 8 columns. \nThere is no null values in this dataFrame<b>\n    \n"},{"metadata":{},"cell_type":"markdown","source":"## Gender\nGender is the target that I want to predict. Let's see the relationship between gender and other parameters\n\n* Note that \"gender\" is categorical feature, so we  change it to numerical (after we show the data):\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"person.gender.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<B>We can see that there is 2501 sumples of males and 2500 sumples of females.\n\n"},{"metadata":{},"cell_type":"markdown","source":"### The Average Parameters Of Each Gender:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"gender1=person.groupby(['gender']).mean()\ngender1","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"from matplotlib.ticker import FuncFormatter\n\nplt.figure(figsize=(20,20))\ngender2=gender1.plot(kind='bar',figsize=(10,6))\n\nfor item in gender2.get_xticklabels():\n    item.set_rotation(0)\n    \nplt.ylim(0.0, 14.0)\nplt.legend(loc='center')\nplt.title('avarage parameters per gender')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<B>We can see that the average of all the parameters (accept of hair) in males is higher than the females. lets see it more closely:"},{"metadata":{},"cell_type":"markdown","source":"### Lip Thickness By Gender"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"person.groupby(['lips_thin','gender']).size().unstack().plot(kind='bar',figsize=(10,6))\nplt.title('lips thin per gender')\nperson.groupby(['distance_nose_to_lip_long','gender']).size().unstack().plot(kind='bar',figsize=(10,6))\nplt.title('distance from nose to lip  per gender')\nplt.show()\n# print the size:\nperson.groupby(['lips_thin','gender']).size()\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"person.groupby(['distance_nose_to_lip_long','gender']).size()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* <b>We can see that  most female have wider lips and shorter distance between the nose and lips than the males."},{"metadata":{},"cell_type":"markdown","source":"###  Foreheade Wide And Height By Gender:"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"sns.displot(data=person, x=\"forehead_width_cm\", col=\"gender\",kde=True, color='b')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"sns.displot(data=person, x=\"forehead_height_cm\", col=\"gender\",kde=True, color='r' )\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* <B>we can see that males has more wide and high forehead than females."},{"metadata":{},"cell_type":"markdown","source":"### Hair Length By Gender :"},{"metadata":{"trusted":true},"cell_type":"code","source":"person.groupby(['long_hair','gender']).size().unstack().plot(kind='bar',figsize=(10,6))\n\nplt.title('long hair per gender')\nplt.show()\nperson.groupby(['long_hair','gender']).size()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* <B> We can see that there is very small diferences in hair length between males and females"},{"metadata":{},"cell_type":"markdown","source":"## Length And Width Nose By Gender"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"person.groupby(['nose_wide','gender']).size().unstack().plot(kind='bar',figsize=(10,6))\nplt.title('nose wide per gender')\nperson.groupby(['nose_long','gender']).size().unstack().plot(kind='bar',figsize=(10,6))\nplt.title('nose long per gender')\nplt.show()\n##printing the size\nperson.groupby(['nose_wide','gender']).size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"person.groupby(['nose_long','gender']).size()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* <B> We can see that females have thiner and shorter noses than males."},{"metadata":{},"cell_type":"markdown","source":"## Preparing The Data:\n#### Changing Gender To Numeric\nmales=0 females=1:"},{"metadata":{"trusted":true},"cell_type":"code","source":" person['gender_code']=pd.factorize(person.gender)[0]\nperson.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Erase The Original Column Of Gender"},{"metadata":{"trusted":true},"cell_type":"code","source":"person.drop(['gender'],axis=1 ,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation:"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"mask = np.zeros_like(person.corr())\nmask[np.tril_indices_from(mask)] = True\nf, ax = plt.subplots(figsize=(10, 8))\ncorr = person.corr()\nsns.heatmap(corr, vmax=1,annot=True,cmap='twilight', mask=mask.T,square=True)\n\nplt.title('Correlation between facial features')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We can see that there is a negative correlation between gender and nose width, nose length, lips width and  distance  from nose to lips.\n* There is a low positive correlation between the parameters of the nose and lips. "},{"metadata":{},"cell_type":"markdown","source":"## Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ny = person[\"gender_code\"]\nx = person.drop([\"gender_code\"], axis=1)\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.4, random_state = 42)\n\nprint(\"x_train : \",x_train)\nprint ('x train size is:' ,x_train.shape)\nprint ()\n\nprint(\"x_test : \",x_test)\nprint ('x test size is:' ,x_test.shape)\nprint ()\n\nprint(\"y_train : \",y_train)\nprint ('y train size is:' ,y_train.shape)\nprint ()\n\nprint(\"y_test : \",y_test)\nprint ('y test size is:' ,y_test.shape)\nprint ()\n\nprint(f'Size of Training set: {len(x_train), len(y_train)}')\nprint(f'Size of Test set {len(x_test), len(y_test)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling\n### KNN:"},{"metadata":{},"cell_type":"markdown","source":"<B> We will check in range of 100  which accuracy we get with specific k, and get just the accuracy above 0.97"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nk_range = list(range(1, 100))\nscores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(x_train, y_train)\n    y_pred = knn.predict(x_test)\n    # calculate accuracy\n    mas=metrics.accuracy_score(y_test, y_pred)\n    if mas>=0.97:\n        print('for k=',k,'the accuracy is:', mas)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* <B> It can be seen that the maximum accuracy is about 0.97 so I checked wich k gives that:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"k_range = list(range(1, 100))\nscores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(x_train, y_train)\n    y_pred_knn = knn.predict(x_test)\n    mas=metrics.accuracy_score(y_test, y_pred_knn)\n    if mas>=0.972:\n        print('for k=',k,'the accuracy is:', mas)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* <B>We can see that all this k gives the same accuracy, **so the final accuraccy is 0.9720139930034982, for k=43/49/95/97**"},{"metadata":{},"cell_type":"markdown","source":"### Confution Metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion = metrics.confusion_matrix(y_test, y_pred)\nTP = confusion[1, 1]\nTN = confusion[0, 0]\nFP = confusion[0, 1]\nFN = confusion[1, 0]\n\nf, ax = plt.subplots(figsize = (5,5))\n\nsns.heatmap(confusion,annot = True, linewidth = 0.5, fmt = \".0f\", ax = ax,cmap=\"crest\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print('Out of 2001 samples the model was right in',TP+TN,'of the samples and wrong in', FP+FN, 'sampels')\n# how often is the classifier correct?\nprint('Classification Accuracy:',(TP + TN) / float(TP + TN + FP + FN))\n#how often is the classifier incorrect?\nprint('Classification Error:',(FP + FN) / float(TP + TN + FP + FN))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* <B> we can see that the model 95% accurate. "},{"metadata":{},"cell_type":"markdown","source":"### Roc Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import  roc_curve\nfrom sklearn.metrics import roc_auc_score\nfalse_positive_rate, true_positiv_rate, _= roc_curve(y_test, y_pred)\nauc= roc_auc_score(y_test, y_pred)\nplt.plot(false_positive_rate, true_positiv_rate, label=\"auc=\"+str(auc))\nplt.box(False)\nplt.title('ROC CURVE')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.grid(True)\nplt.show()\n \nprint(f\"The score ROC Curve is: {round(auc,3)*100}%\")\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cross Validation"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n# 10-fold cross-validation\nknn = KNeighborsClassifier(n_neighbors=43)\nprint(cross_val_score(knn, x, y, cv=10, scoring='accuracy'))\nprint(cross_val_score(knn, x, y, cv=10, scoring='accuracy').mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* <B> we can see that the final accuracy is 97%\n"},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regassion, Random Forest and Decision Tree"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\nmodels = [LogisticRegression(),RandomForestClassifier(), DecisionTreeClassifier()]\nmodel_names=['LogisticRegression','RandomForestClassifier','DecisionTree']\nacc=[]\ncolors1=[\"mako\",\"rocket\",\"magma\"]\ncolors2=[\"y\",\"b\",\"r\"]\n\nfor model in range(3):\n    clf=models[model]\n    clf.fit(x_train,y_train)\n    pred=clf.predict(x_test)\n    \n    #accuracy of each model\n    acc.append(accuracy_score(pred,y_test))\n    \n    # confusion matrix\n    cm = confusion_matrix(y_test, pred)\n    f ,ax = plt.subplots(figsize = (5,5))\n    sns.heatmap(cm, annot = True, linewidth = 0.5, linecolor = \"red\", fmt = \".0f\", ax = ax, cmap=colors1[model])\n    titels=['Logistic Regression','Random Forest','Decision Tree']\n    plt.title(\"Confusion Metrix of \"+titels[model])\n    plt.show()\n    TP = cm[1, 1]\n    TN = cm[0, 0]\n    FP = cm[0, 1]\n    FN = cm[1, 0]\n    print('Out of',TP + TN + FP + FN,'samples the model was right in',TP+TN,'of the samples and wrong in', FP+FN, 'sampels')\n    print('Classification Accuracy:',(TP + TN) / float(TP + TN + FP + FN))\n    print('Classification Error:',(FP + FN) / float(TP + TN + FP + FN))\n   \n    #ROC\n    false_positive_rate, true_positiv_rate, _= roc_curve(y_test, pred)\n    auc= roc_auc_score(y_test, pred)\n    plt.plot(false_positive_rate, true_positiv_rate, label=\"auc=\"+str(auc), color=colors2[model])\n    plt.box(False)\n    plt.title('ROC Curve of '+titels[model])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.grid(True)\n    plt.show()\n    print(f\"The score ROC Curve is: {round(auc,3)*100}%\")\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Accuracy:\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"d={'Modelling':model_names,'Accuracy':acc}    \nacc_frame=pd.DataFrame(d)\nacc_frame\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='Modelling',y='Accuracy',data=acc_frame,kind='point',height=4,aspect=3.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We can see that the heigest accuracy is in the model  Random Forest Classifier with 96%."},{"metadata":{},"cell_type":"markdown","source":"## Dummy Model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.dummy import DummyClassifier\n  \ndummy_clf = DummyClassifier(strategy=\"uniform\")\ndummy_clf.fit(x_train, y_train)\n\ny_pred_dummy=dummy_clf.predict(x_test)\ndummy_clf.score(y_test, y_pred_dummy)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confution Metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred_dummy)\nf ,ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm, annot = True, linewidth = 0.5, linecolor = \"red\", fmt = \".0f\", ax = ax, cmap=colors1[model])\nplt.title(\"Confusion Metrix of Dummy Model \")\nplt.show()\nTP = cm[1, 1]\nTN = cm[0, 0]\nFP = cm[0, 1]\nFN = cm[1, 0]\nprint('Out of',TP + TN + FP + FN,'samples the model was right in',TP+TN,'of the samples and wrong in', FP+FN, 'sampels')\nprint('Classification Accuracy:',(TP + TN) / float(TP + TN + FP + FN))\nprint('Classification Error:',(FP + FN) / float(TP + TN + FP + FN))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### Roc Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"false_positive_rate, true_positiv_rate, _= roc_curve(y_test, y_pred_dummy)\nauc_dum= roc_auc_score(y_test, y_pred_dummy)\nplt.plot(false_positive_rate, true_positiv_rate, label=\"auc=\"+str(auc_dum))\nplt.box(False)\nplt.title('ROC CURVE For Dummy')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.grid(True)\nplt.show()\n \nprint(f\"The score ROC Curve is: {round(auc,3)*100}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* <B> We can see that the best model is KNN  of all the modeles that I checked, with 97%"},{"metadata":{},"cell_type":"markdown","source":"## Conclusion:\nIn this project I worked on a dataset of facial features and tried to predict gender according to those characteristics.\nDuring the presentation of the data it could be seen that there is a correlation between the forehead and nose fillers and the gender type. No correlation was found between hair length and gender.\nIn the model training it can be seen that the percentage of accuracy in all of them is relatively high - over 95% for all models, and out of them the model KNN showed the highest percentage of accuracy-97%.\n"},{"metadata":{},"cell_type":"markdown","source":"# Thank you for watching! ðŸ™ƒ"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}