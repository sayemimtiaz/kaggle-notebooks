{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Health Insurance Lead Prediction\n\nProblem Statement:\nYour Client FinMan is a financial services company that provides various financial services like loan, investment funds, insurance etc. to its customers. FinMan wishes to cross-sell health insurance to the existing customers who may or may not hold insurance policies with the company. The company recommend health insurance to it's customers based on their profile once these customers land on the website. Customers might browse the recommended health insurance policy and consequently fill up a form to apply. When these customers fill-up the form, their Response towards the policy is considered positive and they are classified as a lead.\n\nOnce these leads are acquired, the sales advisors approach them to convert and thus the company can sell proposed health insurance to these leads in a more efficient manner.\n\nNow the company needs your help in building a model to predict whether the person will be interested in their proposed Health plan/policy."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Printing the filepath of data\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scientific and Data Manipulation Libraries :\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Data Viz & Regular Expression Libraries :\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nget_ipython().run_line_magic('matplotlib', 'inline')\n\n# Scikit-Learn Pre-Processing Libraries :\n\nfrom sklearn.preprocessing import *\n\n# Garbage Collection Libraries :\n\nimport gc\n\n# Boosting Algorithm Libraries :\n\nfrom xgboost                          import XGBClassifier\n# from catboost                         import CatBoostClassifier, Pool\n# from lightgbm                         import LGBMClassifier\n# from sklearn.ensemble                 import RandomForestClassifier, VotingClassifier\n\n# Model Evaluation Metric & Cross Validation Libraries :\nfrom sklearn.metrics                  import roc_auc_score\nfrom sklearn.model_selection          import StratifiedKFold,KFold, RepeatedStratifiedKFold, train_test_split\n\n# Setting SEED to Reproduce Same Results even with \"GPU\" :\nseed_value = 1994\n\nimport os\nos.environ['PYTHONHASHSEED'] = str(seed_value)\nimport random\nrandom.seed(seed_value)\nimport numpy as np\nnp.random.seed(seed_value)\nSEED=seed_value\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading files\n\ntrain = pd.read_csv('../input/jobathon-analytics-vidhya/train.csv')\ntest = pd.read_csv('../input/jobathon-analytics-vidhya/test.csv')\nsub = pd.read_csv('../input/jobathon-analytics-vidhya/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f'Train set has {train.shape[0]} rows and {train.shape[1]} columns.')\nprint(f'Test set has {test.shape[0]} rows and {test.shape[1]} columns.')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Percentage of missing values in each column')\ntrain.isnull().sum()/train.shape[0]*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There are no duplicate values.') if train.duplicated().sum()==0 else print('Duplicates found!!')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print('Number of unique values in categorical column.')\ncat_col = ['City_Code', 'Region_Code', 'Accomodation_Type',\n       'Reco_Insurance_Type', 'Is_Spouse','Health Indicator',\n        'Holding_Policy_Type','Reco_Policy_Cat']\ntrain[cat_col].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There are missing values in only 3 columns namely Health Indicator, Holding_Policy_Duration and Holding_Policy_Type. Since these are categorical columns and no domain knowledge can be gathered about their classes, I will impute missing values using mode. Alternatively, filling them with some constant say 0 also gives same result.\n* There are no duplicate values in train data.\n* Region_Code feature has very large number of distinct classes. I tried frequency encoding for it but no major improvement was achieved in score.\n* Features Accomodation_Type, Reco_Insurance_Type and Is_Spouse have only two distinct classes so I will do binary encoding for them.\n* For other features I tried to reduce number of distinct classes using their frequency and response rate but score reached only 0.62. I also tried using FeatureHasher function from sklearn.feature_extraction but score reached only 0.59.\n* So,finally I simply one hot encoded these features using get_dummies function from pandas."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,5))\nsns.barplot(data=train,x='City_Code',y='Region_Code');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot(data=train,x='Response',y='Region_Code',palette='summer');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no information about Region_Code and no pattern with respect to the target variable. So I cannot transform it much."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.set_style('darkgrid')\nsns.countplot(data=train,x='Accomodation_Type',hue='Response',palette='summer')\nplt.xlabel('Customer Owns or Rents the house',fontdict={'fontsize': 15,'color':'Brown'},labelpad=3);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Customers wh own a house are more likely to give positive response."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.set_style('darkgrid')\nsns.countplot(data=train,x='Reco_Insurance_Type',hue='Response',palette='summer')\nplt.xlabel('Type for the recommended insurance',fontdict={'fontsize': 15,'color':'Green'},labelpad=3);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Customers have mostly showed interest in Individual insurance. Though response rate seems similar for both."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"sns.countplot(data=train[train['Reco_Insurance_Type']=='Joint'],x='Is_Spouse',hue='Response',palette='summer')\nplt.xlabel('If the customers are married to each other')\nplt.title('Distribution of those who were Recommended Joint type of Insurance',fontsize=15);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Customers showing interest in joint insurance are mostly couples."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Distribution of Age of primary customer.')\nfig,axes=plt.subplots(2,1,figsize=(8,4))\nsns.distplot(train[train['Response']==0]['Upper_Age'],bins=30,color='red',ax=axes[0])\naxes[0].set_title('Response=0',fontsize=18)\nsns.distplot(train[train['Response']==1]['Upper_Age'],bins=30,color='blue',ax=axes[1])\naxes[1].set_title('Response=1',fontsize=18)\nplt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Age does not seem to affect target variable."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,4))\nsns.countplot(data=train,x='Health Indicator',hue='Response',palette='summer')\nplt.xlabel('Encoded values for health of the customer',fontdict={'fontsize': 15,'color':'Green'},labelpad=3);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As Health Indicator goes from X1 to X9 customer is less likely to be a lead."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,4))\nsns.countplot(data=train,x='Holding_Policy_Duration',hue='Response',palette='summer',\n              order=['1.0', '2.0', '3.0', '4.0', '5.0', '6.0', '7.0',\n       '8.0', '9.0', '10.0', '11.0', '12.0', '13.0', '14.0','14+'])\nplt.xlabel('Duration (in years) of holding policy',fontdict={'fontsize': 15,'color':'Green'},labelpad=3);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Majority of customers hold policies of less than 5 years. As number of years increases, probability of response 1 decreases. Class 14+ seems heavier may be because it include many classes. To consider it as a numerical column I will replace 14+ with 15."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,4))\nsns.countplot(data=train,x='Holding_Policy_Type',hue='Response',palette='summer')\nplt.xlabel('Type of holding policy',fontdict={'fontsize': 15,'color':'Green'},labelpad=3);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Policy corresponding to 3.0 seems most popular."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,4))\nsns.countplot(data=train,x='Reco_Policy_Cat',hue='Response',palette='summer')\nplt.xlabel('Encoded value for recommended health insurance',fontdict={'fontsize': 15,'color':'Green'},labelpad=3);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Insurance policies corresponding to 16-22 are most likely."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Annual Premium (INR) for the recommended health insurance')\nfig,axes=plt.subplots(2,1,figsize=(10,5))\nsns.distplot(train[train['Response']==0]['Reco_Policy_Premium'],color='red',ax=axes[0])\naxes[0].set_title('Response=0',fontsize=18)\nsns.distplot(train[train['Response']==1]['Reco_Policy_Premium'],color='blue',ax=axes[1])\naxes[1].set_title('Response=1',fontsize=18)\nplt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Annual Premium distribution is similar fot both the classes."},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Joining training and test data for preprocessing\nfull_df=pd.concat([train,test])\n\n# Creating a new feature by combining 'Upper_Age' and 'Lower_Age'\nfull_df['Age']=full_df.Upper_Age-full_df.Lower_Age\n\n# Label encoding categorical features with two classes\nfull_df.Is_Spouse = full_df.Is_Spouse.map({'No':0,'Yes':1})\nfull_df.Reco_Insurance_Type = full_df.Reco_Insurance_Type.map({'Individual':0,'Joint':1})\nfull_df.Accomodation_Type = full_df.Accomodation_Type.map({'Owned':0,'Rented':1})\n\n# Filling missing values with mode\nfor i in ['Health Indicator','Holding_Policy_Duration','Holding_Policy_Type']:\n    full_df[i].fillna(0,inplace=True)\nfor i in ['Reco_Policy_Cat','Holding_Policy_Type']:\n    full_df[i] = full_df[i].astype(object)\n\n# Holding_policy_duration\nfull_df['Holding_Policy_Duration'].replace('14+',15.0,inplace=True)\nfull_df['Holding_Policy_Duration']=full_df['Holding_Policy_Duration'].astype(float)\n\n    \n# One hot encoding categorical features with multiple classes\ndummies = pd.get_dummies(full_df[['City_Code','Health Indicator','Holding_Policy_Type','Reco_Policy_Cat']],drop_first=True)\nfinal_data = pd.concat([full_df,dummies],axis=1)\nfinal_data.drop(['ID','City_Code','Health Indicator','Lower_Age','Holding_Policy_Type','Reco_Policy_Cat'],axis=1,inplace=True)\nfinal_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting combined data into preprocessed train and test data\ntrain_data=final_data.dropna()\ntest_data = final_data.iloc[50882:]\ntest_data.drop('Response',axis=1,inplace=True)\ntrain_data.shape,test_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling the data - this didn't make much difference in score so I dropped it\n\n# X=train_data.drop('Response',axis=1)\n# y=train_data.Response\n# X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30)\n\n# to_be_scaled_feat = [ 'Age','Reco_Policy_Premium','Upper_Age', 'Region_Code']\n# scaler=StandardScaler()\n# scaler.fit(train_data[to_be_scaled_feat])\n# X_train[to_be_scaled_feat] = scaler.transform(X_train[to_be_scaled_feat])\n# X_test[to_be_scaled_feat] = scaler.transform(X_test[to_be_scaled_feat])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost model cross-validated with 10 folds"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictor_train_scale = train_data.drop('Response',axis=1)\npredictor_test_scale = test_data\ntarget_train = train_data.Response","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# making folds\nkf=KFold(n_splits=5,shuffle=True)\n\npreds_3   = list()\ny_pred_3  = []\nrocauc_score = []\n\n# Applying model on each fold and calculating mean of score\nfor i,(train_idx,val_idx) in enumerate(kf.split(predictor_train_scale)):    \n    \n    X_train, y_train = predictor_train_scale.iloc[train_idx,:], target_train.iloc[train_idx]    \n    X_val, y_val = predictor_train_scale.iloc[val_idx, :], target_train.iloc[val_idx]\n   \n    print('\\nFold: {}\\n'.format(i+1))\n\n    xg=XGBClassifier(eval_metric='auc',\n                     random_state=294,\n                     learning_rate=0.15, \n                     max_depth=4,\n                     n_estimators=494, \n                     objective='binary:logistic'\n                    )\n\n    xg.fit(X_train, y_train\n           ,eval_set=[(X_train, y_train),(X_val, y_val)]\n           ,early_stopping_rounds=100\n           ,verbose=100\n           )\n\n    roc_auc = roc_auc_score(y_val,xg.predict_proba(X_val)[:, 1])\n    rocauc_score.append(roc_auc)\n#     preds_3.append(xg.predict_proba(predictor_test_scale[predictor_test_scale.columns])[:, 1])\n    \n# y_pred_final_3         = np.mean(preds_3,axis=0)    \n# sub['Response']=y_pred_final_3\n\nprint('ROC_AUC - CV Score: {}'.format((sum(rocauc_score)/5)),'\\n')\nprint(\"Score : \",rocauc_score)\n\n# Download and Show Submission File :\n\n# display(\"sample_submmission\",sub)\n# sub_file_name_3 = \"S3. XGB_GPU_1994SEED_LGBM_NoScaler_MyStyle.csv\"\n# sub.to_csv(sub_file_name_3,index=False)\n# Blend_model_3 = sub.copy()\n# sub.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# An ensemble model of LightGBM, XGBoost and CatBoost also gave me 0.68 score in AV.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**PLEASE UPVOTE IF YOU LIKED MY ANALYSIS AND GIVE FEEDBACK.**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}