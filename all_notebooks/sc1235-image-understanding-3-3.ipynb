{"cells":[{"metadata":{"slideshow":{"slide_type":"slide"},"_uuid":"1959de6aeabc634c682f121fcc30a1aed6b2616f"},"cell_type":"markdown","source":"# Generative Adversarial Networks\n* Idea: Optimize one network by letting it compete against another: the adversarial network.\n* The two networks are usually named\n    * The Generator network -- aims to generate an output that is indistinguishable from the desired output\n    * The Discriminator network -- tries to tell the fake (Generator-produced) output from the true output\n* Training: In each iteration, ...\n    * A number of fake examples is generated;\n    * A number of real examples is drawn from the train images;\n    * The discriminator is trained for some iterations;\n    * The adversarial model is trained (i.e. the generator is free to adjust weights while the discriminator weights are is fixed in some implementations, and the loss is the discrimination loss)","outputs":[],"execution_count":null},{"metadata":{"slideshow":{"slide_type":"slide"},"trusted":false,"collapsed":true,"_uuid":"881d807c336a11e8a631d1471e6638130a118c9a"},"cell_type":"code","source":"'''\nDCGAN on MNIST using Keras\nCode basis: Rowel Atienza; https://github.com/roatienza/Deep-Learning-Experiments\nAdapted by Markus Wenzel, Fraunhofer MEVIS\n'''\n\nimport numpy as np\nimport time\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten, Reshape\nfrom keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\nfrom keras.layers import LeakyReLU, Dropout\nfrom keras.layers import BatchNormalization\nfrom keras.optimizers import Adam, RMSprop\n\nimport matplotlib.pyplot as plt\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"0d1eb89fd8a001b38334f362525e3cbeba16f234"},"cell_type":"code","source":"class ElapsedTimer(object):\n    def __init__(self):\n        self.start_time = time.time()\n    def elapsed(self,sec):\n        if sec < 60:\n            return str(sec) + \" sec\"\n        elif sec < (60 * 60):\n            return str(sec / 60) + \" min\"\n        else:\n            return str(sec / (60 * 60)) + \" hr\"\n    def elapsed_time(self):\n        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time) )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"4aced2582fdeb9663f11884c241ab5a2d4864186"},"cell_type":"code","source":"class DCGAN(object):\n    def __init__(self, img_rows=28, img_cols=28, channel=1):\n\n        self.img_rows = img_rows\n        self.img_cols = img_cols\n        self.channel = channel\n        self.D = None   # discriminator\n        self.G = None   # generator\n        self.AM = None  # adversarial model\n        self.DM = None  # discriminator model\n\n    # (W−F+2P)/S+1\n    def discriminator(self):\n        if self.D:\n            return self.D\n        self.D = Sequential()\n        depth = 64\n        dropout = 0.4\n        # In: 28 x 28 x 1, depth = 1\n        # Out: 14 x 14 x 1, depth=64\n        input_shape = (self.img_rows, self.img_cols, self.channel)\n        self.D.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,\\\n            padding='same'))\n        self.D.add(LeakyReLU(alpha=0.2))\n        self.D.add(Dropout(dropout))\n\n        self.D.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n        self.D.add(LeakyReLU(alpha=0.2))\n        self.D.add(Dropout(dropout))\n\n        self.D.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n        self.D.add(LeakyReLU(alpha=0.2))\n        self.D.add(Dropout(dropout))\n\n        self.D.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n        self.D.add(LeakyReLU(alpha=0.2))\n        self.D.add(Dropout(dropout))\n\n        # Out: 1-dim probability\n        self.D.add(Flatten())\n        self.D.add(Dense(1))\n        self.D.add(Activation('sigmoid'))\n        self.D.summary()\n        return self.D\n\n    def generator(self):\n        if self.G:\n            return self.G\n        self.G = Sequential()\n        dropout = 0.4\n        depth = 64+64+64+64\n        dim = 7\n        # In: 100\n        # Out: dim x dim x depth\n        self.G.add(Dense(dim*dim*depth, input_dim=100))\n        self.G.add(BatchNormalization(momentum=0.9))\n        self.G.add(Activation('relu'))\n        self.G.add(Reshape((dim, dim, depth)))\n        self.G.add(Dropout(dropout))\n\n        # In: dim x dim x depth\n        # Out: 2*dim x 2*dim x depth/2\n        self.G.add(UpSampling2D())\n        self.G.add(Conv2DTranspose(int(depth/2), 5, padding='same'))\n        self.G.add(BatchNormalization(momentum=0.9))\n        self.G.add(Activation('relu'))\n\n        self.G.add(UpSampling2D())\n        self.G.add(Conv2DTranspose(int(depth/4), 5, padding='same'))\n        self.G.add(BatchNormalization(momentum=0.9))\n        self.G.add(Activation('relu'))\n\n        self.G.add(Conv2DTranspose(int(depth/8), 5, padding='same'))\n        self.G.add(BatchNormalization(momentum=0.9))\n        self.G.add(Activation('relu'))\n\n        # Out: 28 x 28 x 1 grayscale image [0.0,1.0] per pix\n        self.G.add(Conv2DTranspose(1, 5, padding='same'))\n        self.G.add(Activation('sigmoid'))\n        self.G.summary()\n        return self.G\n\n    def discriminator_model(self):\n        if self.DM:\n            return self.DM\n        optimizer = RMSprop(lr=0.0002, decay=6e-8)\n        self.DM = Sequential()\n        self.DM.add(self.discriminator())\n        self.DM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n            metrics=['accuracy'])\n        return self.DM\n\n    def adversarial_model(self):\n        if self.AM:\n            return self.AM\n        optimizer = RMSprop(lr=0.0001, decay=3e-8)\n        self.AM = Sequential()\n        self.AM.add(self.generator())\n        self.AM.add(self.discriminator())\n        self.AM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n            metrics=['accuracy'])\n        return self.AM\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"61ff7e26865ad4bad63b37790c87555cc9e0aa12"},"cell_type":"code","source":"class MNIST_DCGAN(object):\n    def __init__(self):\n        self.img_rows = 28\n        self.img_cols = 28\n        self.channel = 1\n\n        # On local (faster to read):\n        # self.x_train = self.read_mnist_tf()\n        # On Kaggle (also works locally, but slower):\n        self.x_train = self.read_mnist()\n\n        self.DCGAN = DCGAN()\n        self.discriminator =  self.DCGAN.discriminator_model()\n        self.adversarial = self.DCGAN.adversarial_model()\n        self.generator = self.DCGAN.generator()\n        \n    def read_mnist(self):\n        import pandas as pd\n        mnist_train_df = pd.read_csv('../input/mnist-original/mnist_train.csv', sep=',', nrows=60000, header=None)\n        train = mnist_train_df.values.astype(np.float32)\n        x_train = train[5000:,1:].reshape(55000,28,28,1)\n        return x_train/255. # Make it similar as TF data.\n    \n    def read_mnist_tf(self):\n        self.x_train = input_data.read_data_sets(\"mnist\", one_hot=True).train.images\n        return self.x_train.reshape(-1, self.img_rows, self.img_cols, 1).astype(np.float32)\n\n\n    def train(self, train_steps=2000, batch_size=256, save_interval=0):\n        noise_input = None\n        if save_interval>0:\n            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n        for i in range(train_steps):\n            images_train = self.x_train[np.random.randint(0,\n                self.x_train.shape[0], size=batch_size), :, :, :]\n            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n            images_fake = self.generator.predict(noise)\n            x = np.concatenate((images_train, images_fake))\n            y = np.ones([2*batch_size, 1])\n            y[batch_size:, :] = 0\n            d_loss = self.discriminator.train_on_batch(x, y)\n\n            # [mwenzel] note that the discriminator loss is not fixed -- both the generator and the discriminator are optimized!\n            y = np.ones([batch_size, 1])\n            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n            a_loss = self.adversarial.train_on_batch(noise, y)\n            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n            print(log_mesg)\n            if save_interval>0:\n                if (i+1)%save_interval==0:\n                    self.plot_images(save2file=True, samples=noise_input.shape[0],\\\n                        noise=noise_input, step=(i+1))\n\n    def plot_images(self, save2file=False, fake=True, samples=16, noise=None, step=0):\n        filename = 'mnist.png'\n        if fake:\n            if noise is None:\n                noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n            else:\n                filename = \"mnist_%d.png\" % step\n            images = self.generator.predict(noise)\n        else:\n            i = np.random.randint(0, self.x_train.shape[0], samples)\n            images = self.x_train[i, :, :, :]\n\n        plt.figure(figsize=(10,10))\n        for i in range(images.shape[0]):\n            plt.subplot(4, 4, i+1)\n            image = images[i, :, :, :]\n            image = np.reshape(image, [self.img_rows, self.img_cols])\n            plt.imshow(image, cmap='gray')\n            plt.axis('off')\n        plt.tight_layout()\n        if save2file:\n            plt.savefig(filename)\n            plt.close('all')\n        else:\n            plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"slideshow":{"slide_type":"slide"},"trusted":false,"collapsed":true,"_uuid":"ec0af4626290e389a63792494cd36e398a3018b4"},"cell_type":"code","source":"if __name__ == '__main__':\n    mnist_dcgan = MNIST_DCGAN()\n    timer = ElapsedTimer()\n    mnist_dcgan.train(train_steps=1000, batch_size=256, save_interval=100)\n    timer.elapsed_time()\n    mnist_dcgan.plot_images(fake=True)\n    mnist_dcgan.plot_images(fake=False, save2file=True)","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"slide"},"_uuid":"bec0070f0d1a2a2be9ef2a448ef37ce9cc7bc70d"},"cell_type":"markdown","source":"# Assignment 1: Adapt to larger images.\nExtend the DCGAN class so that the generator can generate images of arbitrary size. \n* For this, calculate the required `dims`.\n* Add the parameter to the class definition.\n* Create a new class liver_DCGAN to instantiate the new DCGAN appropriately, and to load the image data.","outputs":[],"execution_count":null},{"metadata":{"slideshow":{"slide_type":"notes"},"trusted":false,"collapsed":true,"_uuid":"6785f053ae62299861635ec48a04dd1fe0e67ab3"},"cell_type":"code","source":"# Solution proposal\n\nclass DCGAN_new(object):\n    def __init__(self, img_rows=28, img_cols=28, channel=1):\n\n        self.img_rows = img_rows\n        self.img_cols = img_cols\n        assert self.img_rows == self.img_cols, \"Input image has to be square, at least for now...\"\n        self.dims     = int(img_rows / 4)\n        assert self.dims == img_rows / 4, \"Input image extents have to be divisible by 4.\"\n        self.channel = channel\n        self.D = None   # discriminator\n        self.G = None   # generator\n        self.AM = None  # adversarial model\n        self.DM = None  # discriminator model\n\n    # (W−F+2P)/S+1\n    def discriminator(self):\n        if self.D:\n            return self.D\n        self.D = Sequential()\n        depth = 64\n        dropout = 0.2\n        # In: 28 x 28 x 1, depth = 1\n        # Out: 14 x 14 x 1, depth=64\n        input_shape = (self.img_rows, self.img_cols, self.channel)\n        self.D.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,\\\n            padding='same'))\n        self.D.add(LeakyReLU(alpha=0.2))\n        self.D.add(Dropout(dropout))\n\n        self.D.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n        self.D.add(LeakyReLU(alpha=0.2))\n        self.D.add(Dropout(dropout))\n\n        self.D.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n        self.D.add(LeakyReLU(alpha=0.2))\n        self.D.add(Dropout(dropout))\n\n        self.D.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n        self.D.add(LeakyReLU(alpha=0.2))\n        self.D.add(Dropout(dropout))\n\n        # Out: 1-dim probability\n        self.D.add(Flatten())\n        self.D.add(Dense(1))\n        self.D.add(Activation('sigmoid'))\n        self.D.summary()\n        return self.D\n\n    def generator(self):\n        if self.G:\n            return self.G\n        self.G = Sequential()\n        dropout = 0.2\n        depth = 64+64+64+64\n        # In: 100\n        # Out: dims x dims x depth\n        self.G.add(Dense(self.dims*self.dims*depth, input_dim=100))\n        self.G.add(BatchNormalization(momentum=0.9))\n        self.G.add(Activation('relu'))\n        self.G.add(Reshape((self.dims, self.dims, depth)))\n        self.G.add(Dropout(dropout))\n\n        # In: dim x dim x depth\n        # Out: 2*dim x 2*dim x depth/2\n        self.G.add(UpSampling2D())\n        self.G.add(Conv2DTranspose(int(depth/2), 5, padding='same'))\n        self.G.add(BatchNormalization(momentum=0.9))\n        self.G.add(Activation('relu'))\n\n        self.G.add(UpSampling2D())\n        self.G.add(Conv2DTranspose(int(depth/4), 5, padding='same'))\n        self.G.add(BatchNormalization(momentum=0.9))\n        self.G.add(Activation('relu'))\n\n        self.G.add(Conv2DTranspose(int(depth/8), 5, padding='same'))\n        self.G.add(BatchNormalization(momentum=0.9))\n        self.G.add(Activation('relu'))\n\n        # Out: 28 x 28 x 1 grayscale image [0.0,1.0] per pix\n        self.G.add(Conv2DTranspose(1, 5, padding='same'))\n        self.G.add(Activation('sigmoid'))\n        self.G.summary()\n        return self.G\n\n    def discriminator_model(self):\n        if self.DM:\n            return self.DM\n        optimizer = RMSprop(lr=0.0001, decay=6e-8)\n        self.DM = Sequential()\n        self.DM.add(self.discriminator())\n        self.DM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n            metrics=['accuracy'])\n        return self.DM\n\n    def adversarial_model(self):\n        if self.AM:\n            return self.AM\n        optimizer = RMSprop(lr=0.00005, decay=3e-8)\n        self.AM = Sequential()\n        self.AM.add(self.generator())\n        self.AM.add(self.discriminator())\n        self.AM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n            metrics=['accuracy'])\n        return self.AM\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"d837d6b214bbfde9cd5ba6424733aa25c3f1836a"},"cell_type":"code","source":"# Solution proposal\n\nfrom os import listdir\nfrom os.path import isfile, join, abspath\nimport numpy as np\n\n\nclass liver_DCGAN(object):\n    def __init__(self):\n        self.img_rows = 76\n        self.img_cols = 76\n        self.channel = 1\n        \n        ORIGINAL_PATH = './singleSlicePNGs/originals/'\n        MASK_PATH = './singleSlicePNGs/masks/'\n\n        original_files = sorted([join(ORIGINAL_PATH,f) for f in listdir(ORIGINAL_PATH) if (isfile(join(ORIGINAL_PATH, f)) and f.split('.')[1]=='png')])\n        mask_files     = sorted([join(MASK_PATH,f)     for f in listdir(MASK_PATH)     if (isfile(join(MASK_PATH, f))     and f.split('.')[1]=='png')])\n\n        permuted_file_indexer = np.random.permutation(np.arange(len(original_files)))\n        crop_size = 76\n        self.x_train = np.array([np.array(plt.imread(original_files[permuted_file_indexer[index]])[:crop_size,:crop_size,]) for index in permuted_file_indexer])\n        self.y_train = np.array([np.array(plt.imread(mask_files[permuted_file_indexer[index]])[:crop_size,:crop_size,]) for index in permuted_file_indexer])\n\n        self.DCGAN = DCGAN_new(self.img_rows, self.img_cols)\n        self.discriminator =  self.DCGAN.discriminator_model()\n        self.adversarial = self.DCGAN.adversarial_model()\n        self.generator = self.DCGAN.generator()\n\n    def train(self, train_steps=2000, batch_size=256, save_interval=0):\n        noise_input = None\n        if save_interval>0:\n            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n        for i in range(train_steps):\n            images_train = self.x_train[np.random.randint(0,\n                self.x_train.shape[0], size=batch_size), :, :]\n            images_train = np.reshape(images_train, images_train.shape + (1,))\n            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n            images_fake = self.generator.predict(noise)\n            x = np.concatenate((images_train, images_fake))\n            y = np.ones([2*batch_size, 1])\n            y[batch_size:, :] = 0\n            d_loss = self.discriminator.train_on_batch(x, y)\n\n            y = np.ones([batch_size, 1])\n            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n            a_loss = self.adversarial.train_on_batch(noise, y)\n            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n            print(log_mesg)\n            if save_interval>0:\n                if (i+1)%save_interval==0:\n                    self.plot_images(save2file=True, samples=noise_input.shape[0],\\\n                        noise=noise_input, step=(i+1))\n\n    def plot_images(self, save2file=False, fake=True, samples=16, noise=None, step=0):\n        filename = 'liver.png'\n        if fake:\n            if noise is None:\n                noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n            else:\n                filename = \"liver_%d.png\" % step\n            images = self.generator.predict(noise)\n        else:\n            i = np.random.randint(0, self.x_train.shape[0], samples)\n            images = self.x_train[i, :, :, :]\n\n        plt.figure(figsize=(10,10))\n        for i in range(images.shape[0]):\n            plt.subplot(4, 4, i+1)\n            image = images[i, :, :, :]\n            image = np.reshape(image, [self.img_rows, self.img_cols])\n            plt.imshow(image, cmap='gray')\n            plt.axis('off')\n        plt.tight_layout()\n        if save2file:\n            plt.savefig(filename)\n            plt.close('all')\n        else:\n            plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"slideshow":{"slide_type":"slide"},"trusted":false,"collapsed":true,"_uuid":"181dd5974ed83465fbe623f40f7925782e2eb7d3"},"cell_type":"code","source":"if __name__ == '__main__':\n    liver_dcgan = liver_DCGAN()\n    timer = ElapsedTimer()\n    liver_dcgan.train(train_steps=500, batch_size=256, save_interval=50)\n    timer.elapsed_time()\n    liver_dcgan.plot_images(fake=True)\n    liver_dcgan.plot_images(fake=False, save2file=True)","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"slide"},"_uuid":"ddf7e2ec646feabdbe3752ca1eec7bcc6e4b246b"},"cell_type":"markdown","source":"# Problems","outputs":[],"execution_count":null},{"metadata":{"slideshow":{"slide_type":"fragment"},"_uuid":"082b27c68f059b845b1b537600acb5bd997db048"},"cell_type":"markdown","source":"* If you observe that the discriminator loss drops rapidly but the adversarial loss doesn't: why is this a problem? What is a solution?\n\n[` `] Relatively lower the learning rate of the discriminator.\n\n[` `] Relatively lower the learning rate of the generator.\n\n[` `] Increase the learning rates of both generator and discriminator.","outputs":[],"execution_count":null},{"metadata":{"slideshow":{"slide_type":"fragment"},"_uuid":"d2b14f91a63bdc6d7daa14e5f7082933c9592bf1"},"cell_type":"markdown","source":"* What is the effect of (not) using dropout, of using it only in generator or discriminator, or using too high (> 0.6) values? Run experiments. (Attention: GPU required to run at least 500-1000 iterations before effects become apparent.)","outputs":[],"execution_count":null},{"metadata":{"slideshow":{"slide_type":"slide"},"_uuid":"62355a4d2ec2bb0ea22b27b44a93e10e7492aa6b"},"cell_type":"markdown","source":"# Assignment 2: Semantic Segmentation using Adversarial Training\nChange the DCGAN so that it produces segmentations instead of fake originals.\n* For this, replace the generator with a segmentation network.\n* The adversarial net receives the original and generated segmentations.\n\nRead \"Semantic Segmentation using Adversarial Networks\" (Luc et al. 2016). ","outputs":[],"execution_count":null},{"metadata":{"slideshow":{"slide_type":"notes"},"trusted":false,"collapsed":true,"_uuid":"bfbcb94119c8ceb869b5cf9b6ad4d781f2b8348b"},"cell_type":"code","source":"# Solution proposal\nfrom keras.layers import UpSampling2D, MaxPool2D\nfrom keras.layers import BatchNormalization\nfrom keras.layers.advanced_activations import LeakyReLU, PReLU\nfrom keras.layers import Input, concatenate\nfrom keras.models import Model, Sequential\n\nclass DCGAN_segmentation(object):\n    def __init__(self, img_rows=28, img_cols=28, channel=1):\n\n        self.img_rows = img_rows\n        self.img_cols = img_cols\n        assert self.img_rows == self.img_cols, \"Input image has to be square, at least for now...\"\n        self.dims     = int(img_rows / 4)\n        assert self.dims == img_rows / 4, \"Input image extents have to be divisible by 4.\"\n        self.channel = channel\n        self.D = None   # discriminator\n        self.G = None   # generator\n        self.AM = None  # adversarial model\n        self.DM = None  # discriminator model\n        print(\"DCGAN_segmentation initialised.\")\n\n    def addConvBNSequential(self, model, filters=32, kernel_size=(3,3), batch_norm=True, activation='prelu', padding='same', kernel_regularizer=None):\n        if batch_norm:\n            model = BatchNormalization()(model)\n        if activation == 'prelu':\n            model = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation='linear', kernel_regularizer=kernel_regularizer)(model)\n            model = PReLU()(model)\n        elif activation == 'lrelu':\n            model = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation='linear', kernel_regularizer=kernel_regularizer)(model)\n            model = LeakyReLU()(model)\n        else:\n            model = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation=activation, kernel_regularizer=kernel_regularizer)(model)\n        return model\n\n    def get_batchnorm_unet(self, _filters=32, _filters_add=0, _kernel_size=(3,3), _padding='same', _activation='prelu', _kernel_regularizer=None, _final_layer_nonlinearity='sigmoid', _batch_norm=True):\n        if self.G:\n            return self.G\n\n        input_layer = Input(shape=(self.img_rows,self.img_cols,1))\n\n        x0 = self.addConvBNSequential(input_layer, filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n        x0 = self.addConvBNSequential(x0,          filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n        x1 = MaxPool2D()(x0)\n\n        x1 = self.addConvBNSequential(x1,          filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n        x1 = self.addConvBNSequential(x1,          filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n        x2 = MaxPool2D()(x1)\n\n        x2 = self.addConvBNSequential(x2,          filters=_filters+2*_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n        x2 = self.addConvBNSequential(x2,          filters=_filters+2*_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n        x3 = UpSampling2D()(x2)\n\n        x3 = concatenate([x1,x3])\n        x3 = self.addConvBNSequential(x3,          filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n        x3 = self.addConvBNSequential(x3,          filters=_filters+_filters_add, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n        x4 = UpSampling2D()(x3)\n\n        x4 = concatenate([x0,x4])\n        x4 = self.addConvBNSequential(x4,          filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n        x4 = self.addConvBNSequential(x4,          filters=_filters, kernel_size=_kernel_size, padding=_padding, activation=_activation, kernel_regularizer=_kernel_regularizer, batch_norm=_batch_norm)\n\n        output_layer = Conv2D(1, kernel_size=(1,1), activation=_final_layer_nonlinearity)(x4)\n\n        self.G = Model(input_layer, output_layer)\n        self.G.summary()\n        return self.G    \n    \n    def discriminator(self):\n        if self.D:\n            return self.D\n        self.D = Sequential()\n        depth = 64\n        dropout = 0.2\n        input_shape = (self.img_rows, self.img_cols, self.channel)\n        self.D.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,\\\n            padding='same'))\n        self.D.add(LeakyReLU(alpha=0.2))\n        self.D.add(Dropout(dropout))\n\n        self.D.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n        self.D.add(LeakyReLU(alpha=0.2))\n        self.D.add(Dropout(dropout))\n\n        self.D.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n        self.D.add(LeakyReLU(alpha=0.2))\n        self.D.add(Dropout(dropout))\n\n        self.D.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n        self.D.add(LeakyReLU(alpha=0.2))\n        self.D.add(Dropout(dropout))\n\n        # Out: 1-dim probability\n        self.D.add(Flatten())\n        self.D.add(Dense(1))\n        self.D.add(Activation('sigmoid'))\n        self.D.summary()\n        return self.D\n\n    # The learning rate of discriminator and adversarial model need to be adjusted carefully.\n    def discriminator_model(self):\n        if self.DM:\n            return self.DM\n        optimizer = RMSprop(lr=0.00005, decay=6e-8)\n        self.DM = Sequential()\n        self.DM.add(self.discriminator())\n        self.DM.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n        return self.DM\n    \n    def generator_model(self):\n        if self.GM:\n            return self.GM\n        optimizer = \"adam\"\n        self.GM = self.get_batchnorm_unet(_activation='relu', _batch_norm=True)\n        self.GM.compile(loss=self.dice_loss(), optimizer=optimizer)\n        return self.GM\n\n    def adversarial_model(self):\n        if self.AM:\n            return self.AM\n        optimizer = RMSprop(lr=0.0001, decay=3e-8)\n        self.AM = Sequential()\n        self.AM.add(self.get_batchnorm_unet(_activation='relu', _batch_norm=True))\n        self.AM.add(self.discriminator())\n        self.AM.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n        return self.AM\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"def1b06d16a3501b986c07c78a2e3aeee5e1315d"},"cell_type":"code","source":"# Solution proposal\n\nfrom os import listdir\nfrom os.path import isfile, join, abspath\nimport numpy as np\n\n\nclass segmentation_DCGAN(object):\n    def __init__(self):\n        self.img_rows = 76\n        self.img_cols = 76\n        self.channel = 1\n        \n        ORIGINAL_PATH = './singleSlicePNGs/originals/'\n        MASK_PATH = './singleSlicePNGs/masks/'\n        original_files = [join(ORIGINAL_PATH,f) for f in listdir(ORIGINAL_PATH) if (isfile(join(ORIGINAL_PATH, f)) and f.split('.')[1]=='png')]\n        mask_files     = [join(MASK_PATH,f)     for f in listdir(MASK_PATH)     if (isfile(join(MASK_PATH, f))     and f.split('.')[1]=='png')]\n        original_files = sorted(original_files)\n        mask_files     = sorted(mask_files)\n\n        permuted_file_indexer = np.random.permutation(np.arange(len(original_files)))\n        crop_size = 76\n        self.x_train = np.array([np.array(plt.imread(original_files[permuted_file_indexer[index]])[:crop_size,:crop_size,]) for index in permuted_file_indexer])\n        self.y_train = np.array([np.array(plt.imread(mask_files[permuted_file_indexer[index]])[:crop_size,:crop_size,]) for index in permuted_file_indexer])\n\n        self.DCGAN = DCGAN_segmentation(self.img_rows, self.img_cols)\n        self.discriminator =  self.DCGAN.discriminator_model()\n        self.adversarial = self.DCGAN.adversarial_model()\n        self.generator = self.DCGAN.get_batchnorm_unet(_activation='relu', _batch_norm=True)\n\n    def train(self, train_steps=2000, batch_size=256, save_interval=0):\n        noise_input = None\n        if save_interval>0:\n            # Select a number of test images to predict\n            rand_selection = np.random.randint(0, self.y_train.shape[0], size=16)\n            originals_input = self.x_train[rand_selection, :, :]\n            originals_input = np.reshape(originals_input, originals_input.shape + (1,))\n        for i in range(train_steps):\n            # images_train becomes the masks.\n            # noise (the generator input) becomes the originals.\n            # images_fake are the generator results -- the predicted masks.\n            \n            # (I) Train discriminator\n            rand_selection = np.random.randint(0, self.x_train.shape[0], size=batch_size)\n            originals_train = self.x_train[rand_selection, :, :]\n            originals_train = np.reshape(originals_train, originals_train.shape + (1,))\n            masks_train = self.y_train[rand_selection, :, :]\n            masks_train = np.reshape(masks_train, masks_train.shape + (1,))\n            masks_fake = self.generator.predict(originals_train)\n            x = np.concatenate((masks_train, masks_fake))\n            y = np.ones([2*batch_size, 1])\n            y[batch_size:, :] = 0\n            d_loss = self.discriminator.train_on_batch(x, y)\n            \n            # (II) Train generator\n            \n\n            # (III) Train adversarial\n            rand_selection = np.random.randint(0, self.y_train.shape[0], size=batch_size)\n            y = np.ones([batch_size, 1])\n            originals_train = self.x_train[rand_selection, :, :]\n            originals_train = np.reshape(originals_train, originals_train.shape + (1,))\n            a_loss = self.adversarial.train_on_batch(originals_train, y)\n            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n            print(log_mesg)\n            if save_interval>0:\n                if (i+1)%save_interval==0:\n                    self.plot_images(save2file=True, samples=originals_input.shape[0],\\\n                        noise=originals_input, step=(i+1))\n\n    def plot_images(self, save2file=False, fake=True, samples=16, noise=None, step=0):\n        filename = 'segmentation.png'\n        if fake:\n            if noise is None:\n                # Replace noise with random input images\n                rand_selection = np.random.randint(0, self.x_train.shape[0], size=samples)\n                originals_train = self.x_train[rand_selection, :, :]\n                noise = np.reshape(originals_train, originals_train.shape + (1,))\n            else:\n                filename = \"segmentation_%d.png\" % step\n            images = self.generator.predict(noise)\n        else:\n            i = np.random.randint(0, self.x_train.shape[0], samples)\n            images = self.x_train[i, :, :, :]\n            images = np.reshape(images, images.shape + (1,))\n\n        plt.figure(figsize=(10,10))\n        for i in range(images.shape[0]):\n            plt.subplot(4, 4, i+1)\n            image = images[i, :, :, :]\n            image = np.reshape(image, [self.img_rows, self.img_cols])\n            plt.imshow(image, cmap='gray')\n            plt.axis('off')\n        plt.tight_layout()\n        if save2file:\n            plt.savefig(filename)\n            plt.close('all')\n        else:\n            plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"slideshow":{"slide_type":"slide"},"trusted":false,"collapsed":true,"_uuid":"e3958c6d619e541fcbf3d2fd7030eef2bdacc0f2"},"cell_type":"code","source":"if __name__ == '__main__':\n    liver_dcgan = segmentation_DCGAN()\n    timer = ElapsedTimer()\n    liver_dcgan.train(train_steps=10000, batch_size=512, save_interval=50)\n    timer.elapsed_time()\n    liver_dcgan.plot_images(fake=True)\n    liver_dcgan.plot_images(fake=False, save2file=True)","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"slide"},"_uuid":"944c8f4c53cd1c778d1807834c4bfee807eee354"},"cell_type":"markdown","source":"## Discussion\n* The segmenter is now only implicitly trained towards the discriminator metric, binary crossentropy. There is no direct measure of segmentation mask quality anymore. Is this useful? Can you implement a mixed metric judging both?\n","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"97cc5abe4b57b8c4f50d2bc91dd42fa678cdf310"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"celltoolbar":"Slideshow","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}