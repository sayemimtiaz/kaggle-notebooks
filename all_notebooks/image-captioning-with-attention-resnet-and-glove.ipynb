{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import string\nimport numpy as np\nimport pandas as pd\nfrom numpy import array\nfrom pickle import load\n\nfrom PIL import Image\nimport pickle\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\nimport sys, time, os, warnings\nwarnings.filterwarnings(\"ignore\")\nimport re\n\nimport keras\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense, BatchNormalization\nfrom tensorflow.compat.v1.keras.layers import CuDNNLSTM\nfrom keras.layers import Embedding\nfrom keras.layers import Dropout\nfrom keras.layers import GRU\nfrom keras.layers.merge import add\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.applications.resnet_v2 import ResNet152V2, preprocess_input\n\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-31T19:06:16.679617Z","iopub.execute_input":"2021-05-31T19:06:16.679998Z","iopub.status.idle":"2021-05-31T19:06:16.68937Z","shell.execute_reply.started":"2021-05-31T19:06:16.679965Z","shell.execute_reply":"2021-05-31T19:06:16.688129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_path = \"../input/flickr-8k-images-with-captions/Images/\"\ndir_Flickr_text = \"../input/flickr8k-text/flickr8k.token.txt\"\njpgs = os.listdir(image_path)\n\nprint(\"Total Images in Dataset = {}\".format(len(jpgs)))","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:06:16.707488Z","iopub.execute_input":"2021-05-31T19:06:16.707799Z","iopub.status.idle":"2021-05-31T19:06:16.720305Z","shell.execute_reply.started":"2021-05-31T19:06:16.707771Z","shell.execute_reply":"2021-05-31T19:06:16.719088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file = open(dir_Flickr_text,'r')\ntext = file.read()\nfile.close()\n\ndatatxt = []\nfor line in text.split('\\n'):\n   pre_col = line.split()\n   if len(pre_col) == 0:\n       continue\n   col = []\n   col.append(pre_col[0])\n   col.append(' '.join(pre_col[1:]))\n   w = col[0].split(\"#\")\n   datatxt.append(w + [col[1].lower()])\n\ndata = pd.DataFrame(datatxt,columns=[\"filename\",\"index\",\"caption\"])\ndata = data.reindex(columns =['index','filename','caption'])\ndata = data[data.filename != '2258277193_586949ec62.jpg.1']\nuni_filenames = np.unique(data.filename.values)\n\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:06:16.736235Z","iopub.execute_input":"2021-05-31T19:06:16.736495Z","iopub.status.idle":"2021-05-31T19:06:17.170454Z","shell.execute_reply.started":"2021-05-31T19:06:16.736469Z","shell.execute_reply":"2021-05-31T19:06:17.169495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocabulary = []\nfor txt in data.caption.values:\n   vocabulary.extend(txt.split())\nprint('Vocabulary Size: %d' % len(set(vocabulary)))","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:06:17.172214Z","iopub.execute_input":"2021-05-31T19:06:17.172749Z","iopub.status.idle":"2021-05-31T19:06:17.264295Z","shell.execute_reply.started":"2021-05-31T19:06:17.172692Z","shell.execute_reply":"2021-05-31T19:06:17.263341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_punctuation(text_original):\n   text_no_punctuation = text_original.translate(string.punctuation)\n   return(text_no_punctuation)\n\ndef remove_single_character(text):\n   text_len_more_than1 = \"\"\n   for word in text.split():\n       if len(word) > 1:\n           text_len_more_than1 += \" \" + word\n   return(text_len_more_than1)\n\ndef remove_numeric(text): \n   text_no_numeric = \"\"\n   for word in text.split():\n       isalpha = word.isalpha()\n       if isalpha:\n           text_no_numeric += \" \" + word\n   return(text_no_numeric)\n\ndef text_clean(text_original):\n   text = remove_punctuation(text_original)\n   text = remove_single_character(text)\n   text = remove_numeric(text)\n   return(text)\n\nfor i, caption in enumerate(data.caption.values):\n   newcaption = text_clean(caption)\n   data[\"caption\"].iloc[i] = newcaption","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:06:17.265606Z","iopub.execute_input":"2021-05-31T19:06:17.266141Z","iopub.status.idle":"2021-05-31T19:06:30.831498Z","shell.execute_reply.started":"2021-05-31T19:06:17.2661Z","shell.execute_reply":"2021-05-31T19:06:30.830606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_vocabulary = []\nfor txt in data.caption.values:\n   clean_vocabulary.extend(txt.split())\nprint('Clean Vocabulary Size: %d' % len(set(clean_vocabulary)))","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:06:30.833805Z","iopub.execute_input":"2021-05-31T19:06:30.834796Z","iopub.status.idle":"2021-05-31T19:06:30.928009Z","shell.execute_reply.started":"2021-05-31T19:06:30.834742Z","shell.execute_reply":"2021-05-31T19:06:30.927095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_captions = []\nfor caption  in data[\"caption\"].astype(str):\n   caption = '<start> ' + caption+ ' <end>'\n   all_captions.append(caption)\n\nall_captions[:10]","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:06:30.931351Z","iopub.execute_input":"2021-05-31T19:06:30.931862Z","iopub.status.idle":"2021-05-31T19:06:30.988185Z","shell.execute_reply.started":"2021-05-31T19:06:30.931821Z","shell.execute_reply":"2021-05-31T19:06:30.98726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = \"../input/flickr-8k-images-with-captions/Images/\"\nall_img_name_vector = []\nfor annot in data[\"filename\"]:\n   full_image_path = PATH + annot\n   all_img_name_vector.append(full_image_path)\n\nall_img_name_vector[:10]","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:06:30.991678Z","iopub.execute_input":"2021-05-31T19:06:30.992068Z","iopub.status.idle":"2021-05-31T19:06:31.027671Z","shell.execute_reply.started":"2021-05-31T19:06:30.992029Z","shell.execute_reply":"2021-05-31T19:06:31.026781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"npic = 5\nnpix = 224\ntarget_size = (npix,npix,3)\ncount = 1\n\nfig = plt.figure(figsize=(10,20))\nfor jpgfnm in uni_filenames[10:14]:\n   filename = image_path + '/' + jpgfnm\n   captions = list(data[\"caption\"].loc[data[\"filename\"]==jpgfnm].values)\n   image_load = load_img(filename, target_size=target_size)\n   ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n   ax.imshow(image_load)\n   count += 1\n\n   ax = fig.add_subplot(npic,2,count)\n   plt.axis('off')\n   ax.plot()\n   ax.set_xlim(0,1)\n   ax.set_ylim(0,len(captions))\n   for i, caption in enumerate(captions):\n       ax.text(0,i,caption,fontsize=20)\n   count += 1\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:06:31.029068Z","iopub.execute_input":"2021-05-31T19:06:31.02963Z","iopub.status.idle":"2021-05-31T19:06:31.626777Z","shell.execute_reply.started":"2021-05-31T19:06:31.029589Z","shell.execute_reply":"2021-05-31T19:06:31.625939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"len(all_img_name_vector) : {len(all_img_name_vector)}\")\nprint(f\"len(all_captions) : {len(all_captions)}\")","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:06:31.628206Z","iopub.execute_input":"2021-05-31T19:06:31.62889Z","iopub.status.idle":"2021-05-31T19:06:31.642834Z","shell.execute_reply.started":"2021-05-31T19:06:31.628849Z","shell.execute_reply":"2021-05-31T19:06:31.641777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_limiter(num,total_captions,all_img_name_vector):\n train_captions, img_name_vector = shuffle(total_captions,all_img_name_vector,random_state=1)\n train_captions = train_captions[:num]\n img_name_vector = img_name_vector[:num]\n return train_captions,img_name_vector\n\ntrain_captions,img_name_vector = data_limiter(40000,all_captions,all_img_name_vector)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:06:31.644787Z","iopub.execute_input":"2021-05-31T19:06:31.651905Z","iopub.status.idle":"2021-05-31T19:06:31.755258Z","shell.execute_reply.started":"2021-05-31T19:06:31.651857Z","shell.execute_reply":"2021-05-31T19:06:31.754161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_image(image_path):\n   img = tf.io.read_file(image_path)\n   img = tf.image.decode_jpeg(img, channels=3)\n   img = tf.image.resize(img, (224, 224))\n   img = preprocess_input(img)\n   return img, image_path\n\nimage_model = ResNet152V2(include_top=False, weights='imagenet')\nnew_input = image_model.input\nhidden_layer = image_model.layers[-1].output\nimage_features_extract_model = Model(new_input, hidden_layer)\n\nimage_features_extract_model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:06:31.756624Z","iopub.execute_input":"2021-05-31T19:06:31.761114Z","iopub.status.idle":"2021-05-31T19:06:36.19833Z","shell.execute_reply.started":"2021-05-31T19:06:31.76107Z","shell.execute_reply":"2021-05-31T19:06:36.19752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encode_train = sorted(set(img_name_vector))\nimage_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\nimage_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(64)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:06:36.199461Z","iopub.execute_input":"2021-05-31T19:06:36.199733Z","iopub.status.idle":"2021-05-31T19:06:36.335171Z","shell.execute_reply.started":"2021-05-31T19:06:36.199691Z","shell.execute_reply":"2021-05-31T19:06:36.334377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nos.makedirs('/kaggle/flickr_features')\nfor img, path in tqdm(image_dataset):\n batch_features = image_features_extract_model(img)\n batch_features = tf.reshape(batch_features,\n                             (batch_features.shape[0], -1, batch_features.shape[3]))\n\n for bf, p in zip(batch_features, path):\n   path_of_feature = p.numpy().decode(\"utf-8\")\n   path_of_feature = \"/kaggle/flickr_features/\"+path_of_feature.split('/')[-1]\n   np.save(path_of_feature, bf.numpy())","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:06:36.337775Z","iopub.execute_input":"2021-05-31T19:06:36.338098Z","iopub.status.idle":"2021-05-31T19:07:37.380635Z","shell.execute_reply.started":"2021-05-31T19:06:36.338065Z","shell.execute_reply":"2021-05-31T19:07:37.379582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_k = len(clean_vocabulary)\ntokenizer = Tokenizer(num_words=top_k,\n                      oov_token=\"<unk>\",\n                      filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n\ntokenizer.fit_on_texts(train_captions)\ntrain_seqs = tokenizer.texts_to_sequences(train_captions)\ntokenizer.word_index['<pad>'] = 0\ntokenizer.index_word[0] = '<pad>'\n\ntrain_seqs = tokenizer.texts_to_sequences(train_captions)\ncap_vector = pad_sequences(train_seqs, padding='post')","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:07:37.382561Z","iopub.execute_input":"2021-05-31T19:07:37.383393Z","iopub.status.idle":"2021-05-31T19:07:40.734Z","shell.execute_reply.started":"2021-05-31T19:07:37.383348Z","shell.execute_reply":"2021-05-31T19:07:40.732871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_max_length(tensor):\n   return max(len(t) for t in tensor)\nmax_length = calc_max_length(train_seqs)\n\ndef calc_min_length(tensor):\n   return min(len(t) for t in tensor)\nmin_length = calc_min_length(train_seqs)\n\nprint('Max Length of any caption : Min Length of any caption = '+ str(max_length) +\" : \"+str(min_length))","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:07:42.076851Z","iopub.execute_input":"2021-05-31T19:07:42.077483Z","iopub.status.idle":"2021-05-31T19:07:42.104788Z","shell.execute_reply.started":"2021-05-31T19:07:42.077443Z","shell.execute_reply":"2021-05-31T19:07:42.103796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index = 0\nfor path in img_name_vector:\n    img_name_vector[index] = \"/kaggle/flickr_features/\" + path.split('/')[-1]\n    index = index + 1\nimg_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,cap_vector, test_size=0.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:07:42.106546Z","iopub.execute_input":"2021-05-31T19:07:42.106938Z","iopub.status.idle":"2021-05-31T19:07:42.175363Z","shell.execute_reply.started":"2021-05-31T19:07:42.1069Z","shell.execute_reply":"2021-05-31T19:07:42.174505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 64\nBUFFER_SIZE = 1000\nembedding_dim = 256\nunits = 512\nvocab_size = len(tokenizer.word_index) + 1\nnum_steps = len(img_name_train) // BATCH_SIZE\nfeatures_shape = 512\nattention_features_shape = 49","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:07:42.176875Z","iopub.execute_input":"2021-05-31T19:07:42.177228Z","iopub.status.idle":"2021-05-31T19:07:42.183277Z","shell.execute_reply.started":"2021-05-31T19:07:42.177192Z","shell.execute_reply":"2021-05-31T19:07:42.182201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def map_func(img_name, cap):\n img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n return img_tensor, cap\ndataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n\n# Use map to load the numpy files in parallel\ndataset = dataset.map(lambda item1, item2: tf.numpy_function(\n        map_func, [item1, item2], [tf.float32, tf.int32]),\n         num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\ndataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:07:42.185482Z","iopub.execute_input":"2021-05-31T19:07:42.185918Z","iopub.status.idle":"2021-05-31T19:07:42.345942Z","shell.execute_reply.started":"2021-05-31T19:07:42.185878Z","shell.execute_reply":"2021-05-31T19:07:42.345188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The advantage of using Glove over Word2Vec is that \n#GloVe does not just rely on the local context of words but it incorporates global word co-occurrence to obtain word vectors\nembeddings_index = {} \nf = open('../input/glove840b300dtxt/glove.840B.300d.txt', encoding=\"utf-8\")\nfor line in f:\n    values = line.split(' ')\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\n","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:07:42.347695Z","iopub.execute_input":"2021-05-31T19:07:42.34816Z","iopub.status.idle":"2021-05-31T19:11:29.409375Z","shell.execute_reply.started":"2021-05-31T19:07:42.348115Z","shell.execute_reply":"2021-05-31T19:11:29.408545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_dim = len(list(embeddings_index.values())[0])\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:11:29.411052Z","iopub.execute_input":"2021-05-31T19:11:29.411629Z","iopub.status.idle":"2021-05-31T19:11:29.518258Z","shell.execute_reply.started":"2021-05-31T19:11:29.411572Z","shell.execute_reply":"2021-05-31T19:11:29.51738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ResNet_Encoder(Model):\n   # This encoder passes the features through a Fully connected layer\n   def __init__(self, embedding_dim):\n       super(ResNet_Encoder, self).__init__()\n       # shape after fc == (batch_size, 49, embedding_dim)\n       self.fc = Dense(embedding_dim)\n       self.dropout = Dropout(0.5, noise_shape=None, seed=None)\n   def call(self, x):\n       #x= self.dropout(x)\n       x = self.fc(x)\n       x = tf.nn.relu(x)\n       return x   ","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:11:29.519622Z","iopub.execute_input":"2021-05-31T19:11:29.520182Z","iopub.status.idle":"2021-05-31T19:11:29.527512Z","shell.execute_reply.started":"2021-05-31T19:11:29.520141Z","shell.execute_reply":"2021-05-31T19:11:29.526449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rnn_type(units):\n   if tf.test.is_gpu_available():\n       return CuDNNLSTM(units,\n                        return_sequences=True,\n                        return_state=True,\n                        recurrent_initializer='glorot_uniform')\n   else:\n       return GRU(units,\n                  return_sequences=True,\n                  return_state=True,\n                  recurrent_activation='sigmoid',\n                  recurrent_initializer='glorot_uniform')","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:11:29.529275Z","iopub.execute_input":"2021-05-31T19:11:29.529793Z","iopub.status.idle":"2021-05-31T19:11:29.54143Z","shell.execute_reply.started":"2021-05-31T19:11:29.529754Z","shell.execute_reply":"2021-05-31T19:11:29.540357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''The encoder output(i.e. 'features'), hidden state(initialized to 0)(i.e. 'hidden') and\nthe decoder input (which is the start token)(i.e. 'x') is passed to the decoder.'''\n\nclass Rnn_Local_Decoder(Model):\n def __init__(self, embedding_dim, units, vocab_size):\n   super(Rnn_Local_Decoder, self).__init__()\n   self.units = units\n   self.embedding = Embedding(vocab_size, embedding_dim, trainable=False, weights=[embedding_matrix])\n   #self.embedding = Embedding(vocab_size, embedding_dim)\n   self.gru = GRU(self.units,\n                  return_sequences=True,\n                  return_state=True,\n                  recurrent_initializer='glorot_uniform')\n  \n   self.fc1 = Dense(self.units)\n\n   self.dropout = Dropout(0.5, noise_shape=None, seed=None)\n   self.batchnormalization = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n\n   self.fc2 = Dense(vocab_size)\n\n   # Implementing Attention Mechanism\n   self.Uattn = Dense(units)\n   self.Wattn = Dense(units)\n   self.Vattn = Dense(1)\n def call(self, x, features, hidden):\n   # features shape ==> (64,49,256) ==> Output from ENCODER\n   # hidden shape == (batch_size, hidden_size) ==>(64,512)\n   # hidden_with_time_axis shape == (batch_size, 1, hidden_size) ==> (64,1,512)\n\n   hidden_with_time_axis = tf.expand_dims(hidden, 1)\n\n   # score shape == (64, 49, 1)\n   # Attention Function\n   '''e(ij) = f(s(t-1),h(j))'''\n   ''' e(ij) = Vattn(T)*tanh(Uattn * h(j) + Wattn * s(t))'''\n\n   score = self.Vattn(tf.nn.tanh(self.Uattn(features) + self.Wattn(hidden_with_time_axis)))\n\n   # self.Uattn(features) : (64,49,512)\n   # self.Wattn(hidden_with_time_axis) : (64,1,512)\n   # tf.nn.tanh(self.Uattn(features) + self.Wattn(hidden_with_time_axis)) : (64,49,512)\n   # self.Vattn(tf.nn.tanh(self.Uattn(features) + self.Wattn(hidden_with_time_axis))) : (64,49,1) ==> score\n\n   # you get 1 at the last axis because you are applying score to self.Vattn\n   # Then find Probability using Softmax\n   '''attention_weights(alpha(ij)) = softmax(e(ij))'''\n\n   attention_weights = tf.nn.softmax(score, axis=1)\n\n   # attention_weights shape == (64, 49, 1)\n   # Give weights to the different pixels in the image\n   ''' C(t) = Summation(j=1 to T) (attention_weights * VGG-16 features) '''\n\n   context_vector = attention_weights * features\n   context_vector = tf.reduce_sum(context_vector, axis=1)\n\n   # Context Vector(64,256) = AttentionWeights(64,49,1) * features(64,49,256)\n   # context_vector shape after sum == (64, 256)\n   # x shape after passing through embedding == (64, 1, 256)\n\n   x = self.embedding(x)\n   # x shape after concatenation == (64, 1,  512)\n\n   x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n   # passing the concatenated vector to the GRU\n\n   output, state = self.gru(x)\n   # shape == (batch_size, max_length, hidden_size)\n\n   x = self.fc1(output)\n   # x shape == (batch_size * max_length, hidden_size)\n\n   x = tf.reshape(x, (-1, x.shape[2]))\n\n   # Adding Dropout and BatchNorm Layers\n   x= self.dropout(x)\n   x= self.batchnormalization(x)\n\n   # output shape == (64 * 512)\n   x = self.fc2(x)\n\n   # shape : (64 * 8329(vocab))\n   return x, state, attention_weights\n\n def reset_state(self, batch_size):\n   return tf.zeros((batch_size, self.units))\n\n\nencoder = ResNet_Encoder(embedding_dim)\ndecoder = Rnn_Local_Decoder(embedding_dim, units, vocab_size)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:11:29.543156Z","iopub.execute_input":"2021-05-31T19:11:29.543548Z","iopub.status.idle":"2021-05-31T19:11:29.601581Z","shell.execute_reply.started":"2021-05-31T19:11:29.543511Z","shell.execute_reply":"2021-05-31T19:11:29.600842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n   from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n mask = tf.math.logical_not(tf.math.equal(real, 0))\n loss_ = loss_object(real, pred)\n mask = tf.cast(mask, dtype=loss_.dtype)\n loss_ *= mask\n\n return tf.reduce_mean(loss_)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:11:29.60329Z","iopub.execute_input":"2021-05-31T19:11:29.603647Z","iopub.status.idle":"2021-05-31T19:11:29.611213Z","shell.execute_reply.started":"2021-05-31T19:11:29.603612Z","shell.execute_reply":"2021-05-31T19:11:29.610051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_plot = []\n\n@tf.function\ndef train_step(img_tensor, target):\n loss = 0\n # initializing the hidden state for each batch\n # because the captions are not related from image to image\n\n hidden = decoder.reset_state(batch_size=target.shape[0])\n dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n\n with tf.GradientTape() as tape:\n     features = encoder(img_tensor)\n     for i in range(1, target.shape[1]):\n         # passing the features through the decoder\n         predictions, hidden, _ = decoder(dec_input, features, hidden)\n         loss += loss_function(target[:, i], predictions)\n\n         # using teacher forcing\n         dec_input = tf.expand_dims(target[:, i], 1)\n\n total_loss = (loss / int(target.shape[1]))\n trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n gradients = tape.gradient(loss, trainable_variables)\n optimizer.apply_gradients(zip(gradients, trainable_variables))\n\n return loss, total_loss","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:11:29.613259Z","iopub.execute_input":"2021-05-31T19:11:29.613761Z","iopub.status.idle":"2021-05-31T19:11:29.627535Z","shell.execute_reply.started":"2021-05-31T19:11:29.613725Z","shell.execute_reply":"2021-05-31T19:11:29.626279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 20\nelapsed_start = time.time()\nfor epoch in range(0, EPOCHS):\n   start = time.time()\n   total_loss = 0\n\n   for (batch, (img_tensor, target)) in enumerate(dataset):\n       batch_loss, t_loss = train_step(img_tensor, target)\n       total_loss += t_loss\n\n       if batch % 100 == 0:\n           print ('Epoch {} Batch {} Loss {:.4f}'.format(\n             epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n   # storing the epoch end loss value to plot later\n   loss_plot.append(total_loss / num_steps)\n\n   print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n                                        total_loss/num_steps))\n\n   print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\nprint(f'Time taken to train model: {time.time() - elapsed_start} seconds')","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:11:29.62958Z","iopub.execute_input":"2021-05-31T19:11:29.630214Z","iopub.status.idle":"2021-05-31T19:30:37.084251Z","shell.execute_reply.started":"2021-05-31T19:11:29.630173Z","shell.execute_reply":"2021-05-31T19:30:37.083285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(loss_plot)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss Plot')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:30:37.152727Z","iopub.status.idle":"2021-05-31T19:30:37.153523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(image):\n   attention_plot = np.zeros((max_length, attention_features_shape))\n\n   hidden = decoder.reset_state(batch_size=1)\n   temp_input = tf.expand_dims(load_image(image)[0], 0)\n   img_tensor_val = image_features_extract_model(temp_input)\n   img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n\n   features = encoder(img_tensor_val)\n   dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n   result = []\n\n   for i in range(max_length):\n       predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n       attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n       predicted_id = tf.argmax(predictions[0]).numpy()\n       result.append(tokenizer.index_word[predicted_id])\n\n       if tokenizer.index_word[predicted_id] == '<end>':\n           return result, attention_plot\n\n       dec_input = tf.expand_dims([predicted_id], 0)\n   attention_plot = attention_plot[:len(result), :]\n\n   return result, attention_plot","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:30:37.154612Z","iopub.status.idle":"2021-05-31T19:30:37.155416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_attention(image, result, attention_plot):\n   temp_image = np.array(Image.open(image))\n   fig = plt.figure(figsize=(10, 10))\n   len_result = len(result)\n   for l in range(len_result):\n       temp_att = np.resize(attention_plot[l], (8, 8))\n       ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n       ax.set_title(result[l])\n       img = ax.imshow(temp_image)\n       ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n\n   plt.tight_layout()\n   plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:30:37.156553Z","iopub.status.idle":"2021-05-31T19:30:37.157308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index = 0\nfor path in img_name_val:\n    img_name_val[index] = '../input/flickr-8k-images-with-captions/Images/' + path.split('/')[-1]\n    index = index + 1","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:30:37.160316Z","iopub.status.idle":"2021-05-31T19:30:37.160977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rid = np.random.randint(0, len(img_name_val))\nimage = img_name_val[rid]\nstart = time.time()\nreal_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\nresult, attention_plot = evaluate(image)\n\nfirst = real_caption.split(' ', 1)[1]\nreal_caption = first.rsplit(' ', 1)[0]\n\n#remove \"<unk>\" in result\nfor i in result:\n   if i==\"<unk>\":\n       result.remove(i)\n\n#remove <end> from result        \nresult_join = ' '.join(result)\nresult_final = result_join.rsplit(' ', 1)[0]\n\nreal_appn = []\nreal_appn.append(real_caption.split())\nreference = real_appn\ncandidate = result_final\n\nprint ('Real Caption:', real_caption)\nprint ('Prediction Caption:', result_final)\n\n#plot_attention(image, result, attention_plot)\nprint(f\"time took to Predict: {round(time.time()-start)} sec\")\n\nImage.open(img_name_val[rid])","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:30:37.162104Z","iopub.status.idle":"2021-05-31T19:30:37.162828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"actual = []\npredicted = []\n\nwith open('../input/flickr8k-text/flickr_8k.testImages.txt', 'r') as image_names:\n    for img in image_names:\n        captions = [cap.split() for cap in list(data[data.filename==img]['caption'])]\n        result, attention_plot = evaluate('../input/flickr-8k-images-with-captions/Images/'+img.strip())\n        \n        #remove \"<unk>\" in result\n        for i in result:\n           if i==\"<unk>\":\n               result.remove(i)\n\n        #remove <end> from result        \n        result_join = ' '.join(result)\n        result_final = result_join.rsplit(' ', 1)[0]\n\n        actual.append(captions)\n        predicted.append(result_final.split()) ","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:30:37.165643Z","iopub.status.idle":"2021-05-31T19:30:37.166364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'BLEU-1: {corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))}')\nprint(f'BLEU-2: {corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))}')\nprint(f'BLEU-3: {corpus_bleu(actual, predicted, weights=(0.33, 0.33, 0.33, 0))}')\nprint(f'BLEU-4: {corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))}')","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:30:37.167516Z","iopub.status.idle":"2021-05-31T19:30:37.168178Z"},"trusted":true},"execution_count":null,"outputs":[]}]}