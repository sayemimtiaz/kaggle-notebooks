{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Preamble\n\nThis notebook aims to illustrate some feature importance methods using the [Heart Failure Prediction dataset](https://www.kaggle.com/andrewmvd/heart-failure-clinical-data). **Feature importance is defined as how much of an impact a feature has on a model's predictions.** The dataset we'll use has 12 numerical features for each example and the target for each feature is whether or not death occurred (binary classification). The topics covered and coded are as follows:\n\n- Feature value histograms\n- Permutation importance\n- SHAP summary plots\n- SHAP dependence contribution plots\n\n*Note: this notebook is a follow-up to one of my other notebooks [Feature selection for heart disease prediction](https://www.kaggle.com/valbauman/feature-selection-for-heart-disease-prediction). Big thanks to the Kaggle course Machine Learning Explainability where I learned about some of the topics used here.*\n\n# Create + Train SVM model\nIn my previous notebook, we trained an SVM model that used 5 of the 12 features to predict whether or not death occurred. We will use this same model to explain the importance of each feature used in this model.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import balanced_accuracy_score\n\n#load data and create 90/10 train/validation splits\ndata= pd.read_csv('../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv', delimiter= ',')\ntop5_feats= ['age', 'ejection_fraction', 'serum_creatinine', 'serum_sodium', 'time']\nfeats= data.loc[:,top5_feats] #only keep 5 of the 12 feature columns\nlabels= data.iloc[:,-1]\nx_train, x_devel, y_train, y_devel= train_test_split(feats, labels, test_size= 0.1, random_state= 20)\n\n#scale features (0 mean, unit variance for each)\nscaler= StandardScaler()\nx_train_scale= scaler.fit_transform(x_train)\nx_devel_scale= scaler.transform(x_devel)\n\n#train SVM\nsvm= SVC(probability= True)\nsvm.fit(x_train_scale, y_train)\n\n#for interest's sake, we'll display the average recall of each class for this model\npredicts= svm.predict(x_devel_scale)\nprint(balanced_accuracy_score(y_devel, predicts))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Obviously this score is not great but the focus of this notebook is **feature importance**, not trying to get the best prediction accuracies.\n\n# Feature Value Histograms\nFor each class, we can visualize the values for each feature using histograms.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_hist(feature_vals, labels):\n    \"\"\"\n    FUNCTION TO DISPLAY HISTOGRAMS OF THE FEATURE VALUES FOR THE TWO CLASSES\n    \n    INPUTS:\n    feature_vals : pd series of the values of the single feature of interest\n    labels : pd series of the corresponding labels for the feature values\n    \n    OUTPUTS:\n    None (displays histograms only)\n    \"\"\"\n    feat_0= feature_vals.loc[y_train == 0] #feature values that belong to class 0\n    feat_1= feature_vals.loc[y_train == 1] #features values that belong to class 1\n    \n    #for histogram bins, get the min and max feature values. we'll use a total of 10 bins using the min/max\n    feat_min= np.min(feature_vals)\n    feat_max= np.max(feature_vals)\n    bins= np.linspace(feat_min, feat_max, num= 10)\n    \n    #display histograms. illustrates the distribution of values for the feature for each class\n    plt.figure()\n    plt.hist(feat_0, bins, alpha= 0.5, label= 'Target= 0')\n    plt.hist(feat_1, bins, alpha= 0.5, label= 'Target= 1')\n    plt.legend()\n    plt.xlabel('Feature value'); plt.ylabel('Frequency')\n    plt.title(('Feature value histograms - '+ feature_vals.name))\n    \n    return\n\nfor i in top5_feats:\n    create_hist(x_train[i], y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From these histograms, we can see that both classes take on the same or similar values for most of the features.** The fact that there's a lot of overlap may explain our model's mediocre performance - since there's a lot of overlap in feature values across the two classes, the model may have difficulty distinguishing between classes by using only these features. If we did some feature engineering and added some interaction features, for example, that did not have much or any overlap, our model's average recall may have been better. Although we haven't quantified the importance of each feature using these histograms, it's interesting to visualize the values of each feature for the two classes being considered.\n\n# Permutation Importance\nPermutation importance is one approach to quantifying feature importance. It involves shuffling the values for a single feature so they no longer correspond to their original observation and then making a prediction using these shuffled feature values. We repeat this for each feature for a set number of times so that we can get a mean and standard deviation importance for each feature. The worse a prediction is with the feature values shuffled, the higher the feature's permutation importance. We'll use our trained SVM model and the five features to illustrate how to get permutation importance.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.inspection import permutation_importance\n\n#express scaled development set as a dataframe before getting importances\nx_devel_scaledf= pd.DataFrame(x_devel_scale, columns= x_devel.columns, index= x_devel.index)\n\npermut_importance= permutation_importance(svm, x_devel_scaledf, y_devel, n_repeats= 5, random_state= 20)\nprint(x_devel_scaledf.columns)\nprint(permut_importance['importances_mean'].round(3))\nprint(permut_importance['importances_std'].round(3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Time* (follow-up period in days) has the highest importance of the five features followed by *serum_sodium* and *age*. The mean importance of *time*, *serum_sodium*, and *age* are 0.087, 0.013, and 0.02 respectively.** The negative signs on the importances for ejection_fraction and serum_creatinine  signify that those features are entirely unimportant - random chance in the shuffling made the predictions on the shuffled data more accurate than without shuffling.\n\n# SHAP Summary Plots\nAlthough permutation importance values are great for quantitively comparing features, they don't tell us *how* each feature matters - this is where SHAP (Shapley Additive Explanations) summary plots are helpful. The goal of SHAP is to explain a prediction from any \"black box\" machine learning model as a sum of contributions from it's individual feature values. The individual feature values can be thought of as players in a cooperative game where the payout is the model's prediction.\n\nWithout delving too heavily into the theory behind calculating SHAP values,we can train a number of interpretable weighted linear regression models using the predictions of our black box model (SVM) and the regression coefficients of these surrogate linear regression models give us the approximate SHAP value for each feature. This is all accomplished using KernelExplainer from the shap library.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import shap\n\n#explain the model and then get SHAP values\nexplain= shap.KernelExplainer(svm.predict_proba, x_train_scale, link= 'logit')\nshap_vals= explain.shap_values(x_devel_scale, nsamples= len(x_devel_scale), l1_reg= 'num_features(5)')\n\n#get shap summary plot\nshap.summary_plot(shap_vals, x_devel_scaledf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From this plot, the feature *time* had the greatest influence on the predictions for both classes followed by *ejection_fraction* and then *serum_creatinine*.** This is indicated by the size of the bars for each class for each feature (for both classes, the bars were largest for the feature *time*).\n\nFor each of the two target classes, we can create a density scatter of SHAP values. The plot below has the values for class 0.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.summary_plot(shap_vals[0],x_devel_scaledf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From this plot, we can see that high values for *time* and low values for *serum_creatinine* increases the probability that an example is classified as 0 (no death event).** High values for *time* (indicated by the red colouring) and low values for *serum_creatinine* (indicated by the blue colouring) have high SHAP values (x-axis of plot).\n\n# SHAP Dependence Plots\nUsing the same SHAP values calculated previously, SHAP dependence plots reveal interaction effects between features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.dependence_plot('time', shap_vals[0], x_devel_scaledf, interaction_index= 'ejection_fraction')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the shape of this plot, we can see that the larger the feature value *time*, the higher that model's probability is for predicting the label. Based on the spread of this plot, other features must interact with this feature to predict the label. We can also see that there is no linear correlation between *time* and *ejection_fraction* based on the combination of the colouring and positioning of the points.\n\n# Conclusions\nIn this notebook, we visualized the feature values from a binary classification dataset using histograms to qualitatively decide if there are similar values for some features across classes. We also used a model trained on a 5-feature feature set to quantify the importance of each feature on making a prediction. The SHAP summary plots used in this notebook illustrated how each feature matters in making a prediction.\n\nThe 5 features used were *time*, *ejection_fraction*, *serum_creatinine*, *age*, and *serum_sodium*. *Time* had the highest permutation importance and was therefore the most important feature. The SHAP summary plots indicated that high values for *time* and low values for *serum_creatinine* increase the probability that an example is classified as 0 (non-death event).","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}