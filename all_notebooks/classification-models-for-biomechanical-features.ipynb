{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0.\"></a>\nWe will aplly classification algoritms on our data\n\n# **Contents**\n\n* [1. Import Libraries](#1.)\n* [2. Analyze Data](#2.)\n* [3. Data Preprocessing](#3.)\n* [4. Classification Models](#4.)\n* * [4.1. Logistic Regression](#4.1.)\n* * [4.2. K-Nearest Neighbhour(KNN)](#4.2.)\n* * [4.3. Suport Vector Machine(SVC)](#4.3.)\n* * [4.4. Naive Bayes](#4.4.)\n* * [4.5. Decision Tree](#4.5.)\n* [5. Comparison of Models](#5.)\n* [6. Conclusion](#6.)"},{"metadata":{},"cell_type":"markdown","source":"[Go to the Head](#0.)\n\n# <a class=\"anchor\" id=\"1.\"></a>**1. Import Libraries**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.figure_factory as ff\nfrom plotly.offline import iplot\nimport plotly.graph_objs as go\n\n\n\n# Close warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Import Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_2c = pd.read_csv('/kaggle/input/biomechanical-features-of-orthopedic-patients/column_2C_weka.csv')\ndata_3c = pd.read_csv('/kaggle/input/biomechanical-features-of-orthopedic-patients/column_3C_weka.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[Go to the Head](#0.)\n\n# <a class=\"anchor\" id=\"2.\"></a>**2. Analyze Data**"},{"metadata":{},"cell_type":"markdown","source":"Information about data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_2c.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_3c.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see there's not any null data. So we don't need modify data. Then let's analyze data."},{"metadata":{"trusted":true},"cell_type":"code","source":"def look_univariate(dataset, discrete_feature):\n    fig, plots=plt.subplots(nrows=1,ncols=2, figsize=(8,5))\n    \n    labels = dataset[discrete_feature].value_counts().index\n    sizes = dataset[discrete_feature].value_counts().values\n    \n    dataset[discrete_feature].value_counts().plot(kind=\"bar\",ax=plots[0])\n    plt.pie(sizes,labels=labels,autopct=\"%1.1f%%\") \n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"look_univariate(data_3c,discrete_feature=\"class\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"look_univariate(data_2c,discrete_feature=\"class\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_norm = data_2c[data_2c[\"class\"]==\"Normal\"]\ndata_abnorm = data_2c[data_2c[\"class\"]==\"Abnormal\"]\ndata_2n = data_2c.drop(['class'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can lok correlation map"},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation map\nf,ax = plt.subplots(figsize=(10,10))\nsns.heatmap(data_2n.corr(),annot=True,linewidths=.5,fmt='.1f',ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def class_wth_scatter(dataset):\n    fig = ff.create_scatterplotmatrix(dataset,diag=\"box\",colormap_type=\"cat\",\n                                      height=1000,width=1000)\n    iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_wth_scatter(data_2n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data_3c, hue=\"class\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[Go to the Head](#0.)\n\n# <a class=\"anchor\" id=\"3.\"></a>**3. Data Preprocessing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_2c['class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# change class data tpye\ndata_2c['class'] = [ 0 if each == \"Abnormal\" else 1 for each in data_2c['class']]\n# x_data and y_data\nx_data = data_2c.drop([\"class\"],axis=1)\ny_data = data_2c[\"class\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Firstly we need normalize data.\n\n* Normalization Formula = (x - min(x))/(max(x) - min(x))|"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before create model, we need separate data which is test and train."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split # import library for this\nx_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size = 0.3,random_state = 2)\n\nprint(\"x_train :\", x_train.shape)\nprint(\"x_test :\" , x_test.shape)\nprint(\"y_train :\" , y_train.shape)\nprint(\"y_test :\" , y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[Go to the Head](#0.)\n\n# <a class=\"anchor\" id=\"4.\"></a>**4. Classification Models**"},{"metadata":{},"cell_type":"markdown","source":"In this kaggle we will do: \n* Logistic Regression Classification\n* K-Nearest Neighbhour(KNN) Classification\n* Support Vector Machine(SVM) Classification\n* Naive Bayes Classification\n* Decision Tree Classification\n* Random Forest Classification\n* Evaluate Classification all of these models\n* Comparison of Models"},{"metadata":{},"cell_type":"markdown","source":"[Go to the Head](#0.)\n\n# <a class=\"anchor\" id=\"4.1.\"></a>**4.1. Logistic Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import library\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\n# fit model\nlr.fit(x_data,y_data)\n\nlr_test_accuracy = lr.score(x_test,y_test)\nlr_train_accuracy = lr.score(x_train,y_train)\nlr_predict = lr.predict(x_test)\n\n#Print Test and Train Accuracy\nprint(\"Test Accuracy(LR): \",lr.score(x_test,y_test))\nprint(\"Train Accuracy(LR): \",lr.score(x_train,y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix for Logistic Regression\nfrom sklearn.metrics import confusion_matrix\n\ny_true = y_test\ny_predict = lr_predict\n\ncm_lr = confusion_matrix(y_true,y_predict)\n\nf,ax = plt.subplots(figsize = (8,6))\nsns.heatmap(cm_lr,annot = True, linewidths=0.5,fmt = \".0f\", cmap = \"RdPu\" , ax= ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[Go to the Head](#0.)\n\n# <a class=\"anchor\" id=\"4.2.\"></a>**4.2. K-Nearest Neighbhour(KNN)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Implement KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3) # k value\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\n\n#Print Test and Train Accuracy\nprint(\"KNN (K=3) test score is {}\".format(knn.score(x_test,y_test)))\nprint(\"KNN (K=3) train score is {}\".format(knn.score(x_train,y_train)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, it's correct k value? I don't know. We should try find optimal k value."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Optimal K value\nneighbors = range(1,40)\nknn_train = []\nknn_test = []\nfor i,k in enumerate(neighbors):\n    # k from 1 to 40 (40 exclude)\n    knn_model = KNeighborsClassifier(n_neighbors=k)\n    # fit knn\n    knn_model.fit(x_train,y_train)\n    knn_train.append(knn_model.score(x_train,y_train))    # train accuracy\n    knn_test.append(knn_model.score(x_test,y_test))       # test accuracy\n    \n\n# vizualization results\n\n# create trace1\ntrace1 = go.Scatter(\n                    x = np.array(neighbors),\n                    y = knn_train,\n                    mode = \"lines\",\n                    name=\"train accuracy\",\n                    marker = dict(color = 'rgba(160, 112, 2, 0.8)'),\n                    text=\"train_accuracy\")\n\n# create trace2\ntrace2 = go.Scatter(\n                    x = np.array(neighbors),\n                    y = knn_test,\n                    mode = \"lines+markers\",\n                    name=\"test accuracy\",\n                    marker = dict(color = 'rgba(80, 26, 80, 0.8)'),\n                    text=\"train_accuracy\")\n\ndata = [trace1,trace2]\n\nlayout = dict(title = \"K Value vs Accuracy\",\n             xaxis = dict(title = \"Number of Neighbors\", ticklen = 10,zeroline = True))\n\nfig = dict(data = data, layout = layout)\niplot(fig)\n\nknn_test_accuracy  = np.max(knn_test)\nprint(\"Best accuracy is {} with K = {}\".format(np.max(knn_test),1 + knn_test.index(np.max(knn_test))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix for KNN\nknn = KNeighborsClassifier(n_neighbors=10) # k value, we found K best value is 10\nknn.fit(x_train,y_train)\ny_predict = knn.predict(x_test)\ny_true = y_test\n\nfrom sklearn.metrics import confusion_matrix\ncm_knn = confusion_matrix(y_true,y_predict)\n\nf,ax = plt.subplots(figsize = (8,6))\nsns.heatmap(cm_knn,annot=True,linewidths=0.5,fmt = \".0f\", cmap = \"RdPu\" , ax= ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[Go to the Head](#0.)\n\n# <a class=\"anchor\" id=\"4.3.\"></a>**4.3. Support Vector Machine(SVM)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import library\nfrom sklearn.svm import SVC\n\nsvm = SVC(random_state=20)\nsvm.fit(x_train,y_train)\n\nsvm_test_accuray = svm.score(x_test,y_test)\n\nprint(\"SVM Test accucary: \",svm.score(x_test,y_test))\nprint(\"SVM Train accucary: \",svm.score(x_train,y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix for SVM\nfrom sklearn.metrics import confusion_matrix\ny_true = y_test\ny_predict = svm.predict(x_test)\n\ncm_svc = confusion_matrix(y_true,y_predict)\n\nf,ax = plt.subplots(figsize = (8,6))\nsns.heatmap(cm_svc,annot=True,linewidths=0.5,fmt=\".0f\",cmap=\"RdPu\",ax = ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[Go to the Head](#0.)\n\n# <a class=\"anchor\" id=\"4.4.\"></a>**4.4. Naive Bayes**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import library\nfrom sklearn.naive_bayes import GaussianNB\n\nnaive_bayes = GaussianNB()\nnaive_bayes.fit(x_train,y_train)\n\nnaive_bayes_score = naive_bayes.score(x_test,y_test)\nnb_test_accuracy = naive_bayes.score(x_test,y_test)\n\nprint(\"naive bayes test score: \",naive_bayes.score(x_test,y_test))\nprint(\"naive bayes train score: \",naive_bayes.score(x_train,y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix for Naive Bayes\nfrom sklearn.metrics import confusion_matrix\n\ny_true = y_test\ny_predict = naive_bayes.predict(x_test)\n\ncm_nb = confusion_matrix(y_true,y_predict)\n\nf,ax = plt.subplots(figsize = (8,6))\nsns.heatmap(cm_nb,annot=True,linewidths=0.5,fmt=\".0f\",cmap=\"RdPu\",ax = ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[Go to the Head](#0.)\n\n# <a class=\"anchor\" id=\"4.5.\"></a>**4.5. Decision Tree**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import library\nfrom sklearn.tree import DecisionTreeClassifier\n\ndtree = DecisionTreeClassifier()\ndtree.fit(x_train,y_train)\n\ndecision_tree_score = dtree.score(x_test,y_test)\ndt_test_accuracy = dtree.score(x_test,y_test)\n\nprint(\"decision tree test score: \",dtree.score(x_test,y_test))\nprint(\"decision tree train score: \",dtree.score(x_train,y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix for Decision Tree \nfrom sklearn.metrics import confusion_matrix\n\ny_true = y_test\ny_predict = dtree.predict(x_test)\n\ncm_dt = confusion_matrix(y_true,y_predict)\n\nf,ax = plt.subplots(figsize = (8,6))\nsns.heatmap(cm_dt, annot=True,linewidths=0.5,fmt=\".0f\",cmap=\"RdPu\",ax = ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[Go to the Head](#0.)\n\n# <a class=\"anchor\" id=\"4.5.\"></a>**4.6. Random Forest**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import library\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=50,random_state=20)\nrf.fit(x_train,y_train)\n\nrandom_forest_score = rf.score(x_test,y_test)\nrf_test_accuracy = rf.score(x_test,y_test)\n\nprint(\"random forest test score: \",rf.score(x_test,y_test))\nprint(\"random forest train score: \",rf.score(x_train,y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix for Random Forest\nfrom sklearn.metrics import confusion_matrix\n\ny_true = y_test\ny_predict = rf.predict(x_test)\n\ncm_rf = confusion_matrix(y_true,y_predict)\n\nf,ax = plt.subplots(figsize = (8,6))\nsns.heatmap(cm_rf,annot=True,linewidths=0.5,fmt=\".0f\",cmap=\"RdPu\",ax = ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[Go to the Head](#0.)\n\n# <a class=\"anchor\" id=\"5.\"></a>**5. Comparison of Models**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.suptitle(\"Confusion Matrices of Classification Models\",fontsize=30)\n\nplt.subplot(2,3,1)\nplt.title(\"Logistic Regression Classification\")\nsns.heatmap(cm_lr,annot=True,cmap='YlGnBu',fmt=\".0f\",cbar=False)\n\nplt.subplot(2,3,2)\nplt.title(\"K Nearest Neighbors(KNN) Classification\")\nsns.heatmap(cm_knn,annot=True,cmap='YlGnBu',fmt=\".0f\",cbar=False)\n\nplt.subplot(2,3,3)\nplt.title(\"Support Vector Machine(SVM) Classification\")\nsns.heatmap(cm_svc,annot=True,cmap='YlGnBu',fmt=\".0f\",cbar=False)\n\nplt.subplot(2,3,4)\nplt.title(\"Naive Bayes Classification\")\nsns.heatmap(cm_nb,annot=True,cmap='YlGnBu',fmt=\".0f\",cbar=False)\n\nplt.subplot(2,3,5)\nplt.title(\"Decision Tree Classification\")\nsns.heatmap(cm_dt,annot=True,cmap='YlGnBu',fmt=\".0f\",cbar=False)\n\nplt.subplot(2,3,6)\nplt.title(\"Random Forest Classification\")\nsns.heatmap(cm_rf,annot=True,cmap='YlGnBu',fmt=\".0f\",cbar=False)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_values = [lr_test_accuracy,knn_test_accuracy,svm_test_accuray,nb_test_accuracy,dt_test_accuracy,rf_test_accuracy]\nmodel_names = [\"logistic regression\",\"KNN\",\"SVM\",\"naive bayes\",\"decision tree\",\"random forest\"]\ncolors = [\"green\",\"red\",\"blue\",\"orange\",\"yellow\",\"brown\"]\n\nfig = go.Figure([go.Bar(x=model_names, y = acc_values,marker = dict(color = acc_values))])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[Go to the Head](#0.)\n# <a class=\"anchor\" id=\"6.\"></a>**6. Conclusion**"},{"metadata":{},"cell_type":"markdown","source":"We did analyze data and implemented Classification models. If you like it, Please upvote my kernel. If you have any question, I will happy to hear it"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}