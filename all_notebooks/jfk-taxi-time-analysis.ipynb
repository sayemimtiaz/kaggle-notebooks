{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Order of Contents of the Notebook:\n1. Preprocessing\n2. Scaling, Encoding, and Splitting\n3. Exploratory Visualizations\n4. Defining Model Comparision Function\n5. Model Comparisions","metadata":{}},{"cell_type":"markdown","source":"Summary of Findings:\n\nIt seems that the one-hot encoded data tends to perform slightly better than the label encoded data, although this is not always quite the case.  Furthermore, it seems that the Random Forest model performs the best on either data set, with the labled data slightly outperforming the one-hot encoded data.  ","metadata":{}},{"cell_type":"code","source":"#preprocessing\n\nimport numpy as np\nimport pandas as pd\nimport os\n\ndata = pd.read_csv(\"/kaggle/input/flight-take-off-data-jfk-airport/M1_final.csv\")\n\n#change column names\ndata.columns = ['Month', 'Day_Month', 'Day_Week', 'Airline', 'Tail_Num',\n       'Dest', 'Dep_Delay', 'Flight_Time', 'Distance', 'Sch_Push_Time',\n       'Actual_Push_Time', 'Sch_Arr_Time', 'Temperature', 'Dew_Point', 'Humidity',\n       'Wind', 'Wind_Speed', 'Wind_Gust', 'Pressure', 'Condition', 'Dep_Traffic',\n       'Arr_Traffic', 'Taxi_Out']\n\n#change wind into degrees\ndic = {'NNW': 340, 'CALM': 0, 'NNE':20, 'NE':45, 'VAR':0, 'WSW':230, 'S':180, 'SSW':200, 'WNW':290, 'ESE':115, 'N': 360, 'SW':225, 'E':90, 'W':270, 'SSE':155, 'ENE':70, 'NW':315, 'SE':135}\n\nfor item in dic:\n    data.loc[data['Wind'] == item, \"Wind\"] = dic[item]\n    \ndata.loc[data['Wind'].isnull() == True, \"Wind\"] = int(data.loc[data['Wind'].isnull() == False, \"Wind\"].mean())\n\n# print(data.describe(include=\"all\"))\n# print(data.DISTANCE.isnull().any())\n# print(set(data.Wind.values))\n# print(data.dtypes)\n\ndata['Dew_Point'] = data['Dew_Point'].astype('int')\ndata['Wind'] = data['Wind'].astype('int')\n# for name in data.columns:\n#     print(name, pd.unique(data[name]))\n\n# data.describe(include='all')\ndata.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-07-29T20:21:46.421325Z","iopub.execute_input":"2021-07-29T20:21:46.42206Z","iopub.status.idle":"2021-07-29T20:21:46.826169Z","shell.execute_reply.started":"2021-07-29T20:21:46.421931Z","shell.execute_reply":"2021-07-29T20:21:46.825277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#scaling, encoding, and splitting\n\n#need to scale since we are using gradient descent and distance based models\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nscaler = StandardScaler()\ndatascaled = pd.DataFrame(data =scaler.fit_transform(data.select_dtypes(include=['int64', 'float64'])), columns=data.select_dtypes(include=['int64','float64']).columns )\ndatascaled = data.select_dtypes(exclude=['int64','float64']).join(datascaled)\n\n\n#label encode data\ndftrain_lab = datascaled.copy()\n\nfor column in dftrain_lab.select_dtypes(include=['object']):\n    dftrain_lab[column] = dftrain_lab[column].astype('category')\n    dftrain_lab[column] = dftrain_lab[column].cat.codes\n\ndftrain_lab\n\n#one-hot encode data\n\n#need to figure out why this is not working\ndftrain_o_h = datascaled.copy()\ndftrain_o_h = dftrain_o_h.drop(\"Tail_Num\",axis='columns')\n#drop Tail Num since it adds little value and messes up linear regression\ndftrain_o_h = pd.get_dummies(data = dftrain_o_h, columns=dftrain_o_h.select_dtypes(include=['object']).columns)\n\n#organizing and splitting data\nX_l, y_l = dftrain_lab.drop(['Taxi_Out'], axis='columns'), dftrain_lab['Taxi_Out']\nX_o, y_o = dftrain_o_h.drop(['Taxi_Out'], axis='columns'), dftrain_o_h['Taxi_Out']\n\nX_l_train, X_l_test, y_l_train, y_l_test = train_test_split(X_l, y_l, test_size=0.1)\nX_o_train, X_o_test, y_o_train, y_o_test = train_test_split(X_o, y_o, test_size=0.1)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-29T20:21:46.827787Z","iopub.execute_input":"2021-07-29T20:21:46.828089Z","iopub.status.idle":"2021-07-29T20:21:47.856438Z","shell.execute_reply.started":"2021-07-29T20:21:46.828061Z","shell.execute_reply":"2021-07-29T20:21:47.855267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#exploratory visualizations\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(20,20))\n\nsns.heatmap(dftrain_lab.corr('pearson'), annot=True)\n\ndatascaled.select_dtypes(include=['int64','float64']).plot(kind='box', figsize=(30,30))\ndftrain_o_h.plot(kind='box', figsize=(30,30))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T20:21:47.858591Z","iopub.execute_input":"2021-07-29T20:21:47.859061Z","iopub.status.idle":"2021-07-29T20:21:56.003551Z","shell.execute_reply.started":"2021-07-29T20:21:47.859016Z","shell.execute_reply":"2021-07-29T20:21:56.002538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#defining model comparison function\n\n#inputs below as in order:\n#list of models, list of model names, list of [trainfeat,testfeat], list of [trainlab,testlab], list of dataset names, error function, error function name, root of error\ndef compare_models(models, model_names, lst_datafeatures, lst_datalables, dataset_names, error_func, error_name, root=1):\n    df = pd.DataFrame(data=np.zeros([len(dataset_names), len(model_names) + 1]),columns=(['Dataset_Name']+model_names))\n    print(df.columns)\n    for i in range(len(models)):\n        for j in range(len(dataset_names)):\n            df.iloc[j,0] = dataset_names[j]\n            X_train, y_train = lst_datafeatures[j][0], lst_datalables[j][0]\n            X_test, y_test = lst_datafeatures[j][1], lst_datalables[j][1]\n            models[i].fit(X_train, y_train)\n            predictions = models[i].predict(X_test)\n            error = (error_func(predictions, y_test))**(1/root)\n            df.iloc[j, i+1] = error\n    \n    plt.figure(figsize=(20,20))\n    for j in range(len(dataset_names)):\n        print(df.iloc[j,1:])\n        plt.plot(model_names, df.iloc[j,1:], marker='*')\n\n    plt.legend(dataset_names)\n    plt.xlabel(\"Model_Names\")\n    plt.ylabel(error_name)\n\n    plt.show()\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-07-29T20:21:56.005251Z","iopub.execute_input":"2021-07-29T20:21:56.005822Z","iopub.status.idle":"2021-07-29T20:21:56.017593Z","shell.execute_reply.started":"2021-07-29T20:21:56.005775Z","shell.execute_reply":"2021-07-29T20:21:56.016631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model comparisions\n\n#Linear Models\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\n\n#Nonlinear Models\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom lightgbm import LGBMRegressor\n\n#Error Metrics\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\n\nlinreg = LinearRegression(normalize=True)\nlassoreg = Lasso(alpha=0.05, normalize=True)\nridgereg = Ridge(alpha=0.05, normalize=True)\nkNNreg = KNeighborsRegressor(n_neighbors=50)\nSVreg = SVR()\nbayridgereg = BayesianRidge()\nrfreg = RandomForestRegressor()\nLGBMreg = LGBMRegressor()\n\n\nmodels=[linreg,lassoreg, ridgereg, kNNreg, SVreg, bayridgereg, rfreg, LGBMreg]\nmodel_names=[\"Linear\", \"Lasso\", \"Ridge\", \"kNN\", \"SV\", \"Bayesian Ridge\", \"Random Forest\", \"LGBM\"]\ndatasets_X = [[X_o_train,X_o_test],[X_l_train,X_l_test]]\ndatasets_y = [[y_o_train,y_o_test],[y_l_train,y_l_test]]\ndataset_names=[\"One Hot\", \"Label Encoding\"]\n\n#compare_models(models, model_names, datasets_X, datasets_y, dataset_names, mean_absolute_error, \"MAE\")\n#compare_models(models, model_names, datasets_X, datasets_y, dataset_names, mean_squared_error, \"MSE\", root=1)\ncompare_models(models, model_names, datasets_X, datasets_y, dataset_names, mean_squared_error, \"RMSE\", root=2)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T20:21:56.018681Z","iopub.execute_input":"2021-07-29T20:21:56.018954Z","iopub.status.idle":"2021-07-29T20:28:48.882785Z","shell.execute_reply.started":"2021-07-29T20:21:56.018928Z","shell.execute_reply":"2021-07-29T20:28:48.881713Z"},"trusted":true},"execution_count":null,"outputs":[]}]}