{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.ticker\nfrom matplotlib import rc\nfrom matplotlib.ticker import StrMethodFormatter\n%matplotlib inline\nfrom scipy import stats\n\n# Regular expressions\nimport re\n\n# seaborn : advanced visualization\nimport seaborn as sns\nprint('seaborn version\\t:',sns.__version__)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Load data from the file path of csv as a DataFrame object**\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/tvs-loan-default/TVS.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Rename columns to identify data in it**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_bkp = df.rename(columns={'V1': 'Customer ID',\n'V2': 'Customer has bounced in first EMI', \n'V3': 'No of times bounced 12 months',\n'V4': 'Maximum MOB',\n'V5': 'No of times bounced while repaying the loan',\n'V6': 'EMI',\n'V7': 'Loan Amount',\n'V8': 'Tenure',\n'V9': 'Dealer codes from where customer has purchased the Two wheeler',\n'V10': 'Product code of Two wheeler', \n'V11': 'No of advance EMI paid',\n'V12': 'Rate of interest',\n'V13': 'Gender',\n'V14': 'Employment type',\n'V15': 'Resident type of customer',\n'V16': 'Date of birth',\n'V17': 'Customer age when loanwas taken',\n'V18': 'No of loans',\n'V19': 'No of secured loans',\n'V20': 'No of unsecured loans',\n'V21': 'Max amount sanctioned in the Live loans',\n'V22': 'No of new loans in last 3 months',\n'V23': 'Total sanctioned amount in the secured Loans which are Live',\n'V24': 'Total sanctioned amount in the unsecured Loans which are Live',\n'V25': 'Maximum amount sanctioned for any Two wheeler loan',\n'V26': 'Time since last Personal loan taken (in months)',\n'V27': 'Time since first consumer durables loan taken (in months)',\n'V28': 'No of times 30 days past due in last 6 months',\n'V29': 'No of times 60 days past due in last 6 months',\n'V30': 'No of times 90 days past due in last 3 months',\n'V31': 'Tier',\n'V32': 'Target variable'})\n\ntrain_data_bkp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom io import StringIO\nfrom datetime import datetime\n\ntrain_data_bkp[\"Date of birth\"] = train_data_bkp[\"Date of birth\"].fillna(datetime.today().strftime('%Y-%m-%d'))\n\ntrain_data_bkp[\"Date of birth\"] = pd.to_datetime(train_data_bkp[\"Date of birth\"])\n\nfrom dateutil.relativedelta import relativedelta\n\ndef f(end):\n    r = relativedelta(pd.to_datetime('now'), end) \n    return '{}'.format(r.years)\n\ntrain_data_bkp['Age'] = train_data_bkp[\"Date of birth\"].apply(f).astype(int)\ntrain_data_bkp['Age'] = train_data_bkp['Age'].replace(0, np.nan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_bkp = train_data_bkp.drop(['Customer ID','Date of birth','Time since last Personal loan taken (in months)','Time since first consumer durables loan taken (in months)','Total sanctioned amount in the secured Loans which are Live','Total sanctioned amount in the unsecured Loans which are Live','Max amount sanctioned in the Live loans','No of new loans in last 3 months'],axis=1)\ntrain_data_bkp['Dealer codes from where customer has purchased the Two wheeler'] = train_data_bkp['Dealer codes from where customer has purchased the Two wheeler'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = df.rename(columns={'V1': 'Customer ID',\n'V2': 'Customer has bounced in first EMI', \n'V3': 'No of times bounced 12 months',\n'V4': 'Maximum MOB',\n'V5': 'No of times bounced while repaying the loan',\n'V6': 'EMI',\n'V7': 'Loan Amount',\n'V8': 'Tenure',\n'V9': 'Dealer codes from where customer has purchased the Two wheeler',\n'V10': 'Product code of Two wheeler', \n'V11': 'No of advance EMI paid',\n'V12': 'Rate of interest',\n'V13': 'Gender',\n'V14': 'Employment type',\n'V15': 'Resident type of customer',\n'V16': 'Date of birth',\n'V17': 'Customer age when loanwas taken',\n'V18': 'No of loans',\n'V19': 'No of secured loans',\n'V20': 'No of unsecured loans',\n'V21': 'Max amount sanctioned in the Live loans',\n'V22': 'No of new loans in last 3 months',\n'V23': 'Total sanctioned amount in the secured Loans which are Live',\n'V24': 'Total sanctioned amount in the unsecured Loans which are Live',\n'V25': 'Maximum amount sanctioned for any Two wheeler loan',\n'V26': 'Time since last Personal loan taken (in months)',\n'V27': 'Time since first consumer durables loan taken (in months)',\n'V28': 'No of times 30 days past due in last 6 months',\n'V29': 'No of times 60 days past due in last 6 months',\n'V30': 'No of times 90 days past due in last 3 months',\n'V31': 'Tier',\n'V32': 'Target variable'})\n\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Target Variable Distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"target_nm = 'Target variable'\nc = train_data[target_nm].value_counts(dropna=False)\np = train_data[target_nm].value_counts(dropna=False, normalize=True)\npd.concat([c,p], axis=1, keys=['counts', '%']).to_excel(\"Target_Variable_Distribution.xlsx\", header=True)\nprint(pd.concat([c,p], axis=1, keys=['counts', '%']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[target_nm].value_counts().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot('Tier','Rate of interest',data=train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.distplot(train_data['Rate of interest'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot(target_nm,'Rate of interest',data=train_data,bw='scott')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.hist(figsize=(15,20))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"New Derived field 'Age', (by taking difference of DOB and As_of_today's date)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom io import StringIO\nfrom datetime import datetime\n\ntrain_data[\"Date of birth\"] = train_data[\"Date of birth\"].fillna(datetime.today().strftime('%Y-%m-%d'))\n\ntrain_data[\"Date of birth\"] = pd.to_datetime(train_data[\"Date of birth\"])\n\nfrom dateutil.relativedelta import relativedelta\n\ndef f(end):\n    r = relativedelta(pd.to_datetime('now'), end) \n    return '{}'.format(r.years)\n\ntrain_data['Age'] = train_data[\"Date of birth\"].apply(f).astype(int)\ntrain_data['Age'] = train_data['Age'].replace(0, np.nan)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Descriptive_Stats_for All Columns**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data= train_data.drop(['Customer ID','Date of birth','Time since last Personal loan taken (in months)','Time since first consumer durables loan taken (in months)','Total sanctioned amount in the secured Loans which are Live','Total sanctioned amount in the unsecured Loans which are Live','Max amount sanctioned in the Live loans','No of new loans in last 3 months'],axis=1)\ntrain_data['Dealer codes from where customer has purchased the Two wheeler'] = train_data['Dealer codes from where customer has purchased the Two wheeler'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## All Feature Descriptive Statistics","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.describe(include='all').to_csv(\"Descriptive_Stats_Continuous.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(train_data.corr())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = train_data.corr()\nprint(corr_matrix['Target variable'].sort_values(ascending=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split Continuous and Categorical features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\ndf_cont = train_data.select_dtypes(include=[np.number])\ndf_cat = train_data.select_dtypes(exclude=[np.number]) \ndf_cat['Target variable'] = train_data['Target variable']\nprint(df_cont.info())\nprint(df_cat.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorical Feature Binning,WOE,IV","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"################# Categorical field Weight of Evidence and IV Calculation ####################\n'''This User Defined Function performs Binning of Categorical Variables and generate Weight of Evidence and Information Value of the same'''\n\n'''\nInput  :Dataframe having only Categorical Features and Target\n        Target column name\n        Output Excel filename\n        \nOutput :Excel File With Binning information along with WOE and IV\n\nReturns:Modified DataFrame with New Categorical columns with suffix \"WOE\" \n        having corresponding WOE Value\n'''\n\nimport xlsxwriter\nimport os\nimport math\nimport pandas as pd\nimport numpy as np\n\ndef calc_iv(var_name):\n    '''Calculates the Wieght of Evidence (WOE) and Information Value(IV) for Catgeorical fields'''\n    global categorical_target_nx\n    global data_fnl\n    global IV_lst\n    lst=[]\n    \n    categorical_target_nx.Event=categorical_target_nx.Event.fillna(0)\n    categorical_target_nx['Non-Event']=categorical_target_nx['Non-Event'].fillna(0)\n    row_cnt_without_total=len(categorical_target_nx)-int(1)\n    \n    for i in range(len(categorical_target_nx)):\n        data_bin = categorical_target_nx.Levels[i]\n        data_All = int(categorical_target_nx.Total[i])\n        data_Target_1 = int(categorical_target_nx.Event[i])\n        data_Target_0 = int(categorical_target_nx['Non-Event'][i])\n        data_Target_1_Rate = categorical_target_nx.Event[i] / categorical_target_nx.Total[i]\n        data_Target_0_Rate = categorical_target_nx['Non-Event'][i] / categorical_target_nx.Total[i]\n        data_Distribution_Target_1 = int(categorical_target_nx['Event'][i])/ categorical_target_nx['Event'].head(row_cnt_without_total).sum().sum()\n        data_Distribution_Target_0 = int(categorical_target_nx['Non-Event'][i])/categorical_target_nx['Non-Event'].head(row_cnt_without_total).sum()\n        #'WOE' value by ln(Distribution Good/Distribution Bad)\n        data_WoE = np.log(data_Distribution_Target_1 / data_Distribution_Target_0)\n        \n        data_vol_p   = categorical_target_nx[\"Volume(%)\"][i]\n        data_event_p = categorical_target_nx[\"Event(%)\"][i]\n        \n        if (data_WoE == np.inf) or (data_WoE == -np.inf):\n            data_WoE = 0\n            \n        data_IV = data_WoE * (data_Distribution_Target_1 - data_Distribution_Target_0)\n        data=[data_bin,data_Target_1,data_Target_0,data_All,data_vol_p,data_event_p,\n              data_Target_1_Rate,data_Target_0_Rate,data_Distribution_Target_1,data_Distribution_Target_0,\n              data_WoE,data_IV]\n        lst.append(data)\n    \n    data_fnl = pd.DataFrame(lst,columns=['Levels', 'Event', 'Non-Event', 'Total','Volume(%)','Event(%)',                                         'Event_Rate','Non_Event_Rate','Distribution_Event','Distribution_Non_Event',\n                        'WOE','IV'])\n    iv_val=[var_name,data_fnl['IV'].head(row_cnt_without_total).sum()]\n    IV_lst.append(iv_val)\n\n    \ndef cat_bin_trend(df_cat_fnl,df_categorical_column,i,target_col,filename):\n    '''This User defined function creates the bins / groups on the 'Levels' of the Categorical Columns\n    1. Event -> Target = 1\n    2. Non-Event -> Target = 0 \n    3. ALong with the Levels of the categorical Columns, A summary record is also created with header \"Total\" '''\n\n    global categorical_target_nx\n\n    categorical_target_nx = pd.DataFrame([df_cat_fnl[(df_cat_fnl[target_col] == 1)].groupby('Levels')[df_categorical_column[i]].count(),\n                                         df_cat_fnl[(df_cat_fnl[target_col] == 0)].groupby('Levels')[df_categorical_column[i]].count(),\n                                         df_cat_fnl.groupby('Levels')[df_categorical_column[i]].count()]).T\n\n    categorical_target_nx.columns = [\"Event\",\"Non-Event\",\"Total\"]    \n    categorical_target_nx['Event'] = categorical_target_nx['Event'].fillna(0)\n    categorical_target_nx['Non-Event'] = categorical_target_nx['Non-Event'].fillna(0)\n    categorical_target_nx['Total'] = categorical_target_nx['Total'].fillna(0)\n\n    categorical_target_nx=categorical_target_nx.reset_index()\n    categorical_target_nx = categorical_target_nx.rename(columns={categorical_target_nx.columns[0]: \"Levels\"})    \n        \n    list_vol_pct=[]\n    list_event_pct=[]\n\n    for j in range(len(categorical_target_nx.Event)):\n\n        list_vol_pct.append(categorical_target_nx['Total'][j]/categorical_target_nx['Total'].sum())\n        list_event_pct.append(categorical_target_nx['Event'][j]/categorical_target_nx['Total'][j])\n    \n    categorical_target_nx = pd.concat([categorical_target_nx,pd.Series(list_vol_pct),pd.Series(list_event_pct)],axis=1)\n    \n    \n    categorical_target_nx = categorical_target_nx[[\"Levels\",\"Event\",\"Non-Event\",\"Total\",0,1]]        \n    categorical_target_nx = categorical_target_nx.rename(columns={categorical_target_nx.columns[len(categorical_target_nx.keys())-2]: \"Volume(%)\"})\n    categorical_target_nx = categorical_target_nx.rename(columns={categorical_target_nx.columns[len(categorical_target_nx.keys())-1]: \"Event(%)\"})\n    categorical_target_nx = categorical_target_nx.sort_values(by=['Total'], ascending=False)\n    \n    categorical_target_nx = categorical_target_nx.append(\n        {\"Levels\":\"Total\",\n         \"Event\":categorical_target_nx['Event'].sum(),\n         \"Non-Event\":categorical_target_nx['Non-Event'].sum(),\n         \"Total\":categorical_target_nx['Total'].sum(),\n         \"Volume(%)\":categorical_target_nx['Volume(%)'].sum(),\n         \"Event(%)\":categorical_target_nx['Event'].sum()/categorical_target_nx['Total'].sum()\n        },ignore_index=True)\n                 \n\ndef cat_bin(df,df_categorical,target_col,filename):\n    ''' This User Defined Function performs the following :\n    1. Replace the NAN value with \"Missing\" value \n    2. Binning is done on the unique Labels of the Categorical columns\n    3. For Missing Values, it has been treated as seperate Label - \"Missing\"\n    4. Calculates Weight of Evidence (WOE) of each bin/label of Categorical Variables\n    5. Calculates Information Vale (IV) for each Categorical Variables  '''\n\n    global categorical_target_nx,data_fnl,IV_lst\n    \n    df_categorical_column = list(df_categorical.columns)    \n    \n    '''Initialization of list and excel workbook'''\n    IV_lst=[]\n    writer1 = pd.ExcelWriter(filename,engine='xlsxwriter')\n    workbook=writer1.book\n    worksheet=workbook.add_worksheet('WOE')\n    writer1.sheets['WOE'] = worksheet\n    n = 0\n    m = -1\n    \n    for i in range(len(df_categorical_column)):\n\n        if (df_categorical_column[i] != target_col):       \n            '''Repplacing the NAN Value with \"Missing\" Value for treating the Missing Value as seperate bin/group'''\n            nparray_cat=df_categorical[df_categorical_column[i]].fillna('Missing').unique()\n            nparray_sort=np.sort(nparray_cat)        \n            df_cat = pd.concat([pd.Series(nparray_sort),pd.Series(nparray_sort)],axis=1, keys=[df_categorical_column[i],'Levels'])       \n            df_tst = df.loc[:, [df_categorical_column[i],target_col]].sort_values(by=[df_categorical_column[i]]).fillna('Missing')            \n            df_cat_fnl = pd.merge(df_tst, df_cat, how='left', on=[df_categorical_column[i]]) \n            \n            ''' Creates Groups for each of the unique values of categorical variables '''\n            cat_bin_trend(df_cat_fnl,df_categorical_column,i,target_col,filename)                                   \n\n            ''' Calculates WOE and IV '''\n            calc_iv(df_categorical_column[i])\n     \n            ''' Writing the WOE in seperate worksheet \"WOE\" of Final Excel '''\n            worksheet.write_string(n, 0, df_categorical_column[i])\n            data_fnl.to_excel(writer1,sheet_name='WOE',startrow=n+1 , startcol=0,index = False)\n            n += len(categorical_target_nx.index) + 4\n    \n    ''' Writing the IV in seperate worksheet \"IV\" of Final Excel '''\n    data_IV = pd.DataFrame(IV_lst,columns=['Variable','IV_value'])\n    data_IV = data_IV.sort_values(by=['Variable','IV_value'],ascending=[True,False])\n    data_IV.to_excel(writer1,sheet_name='IV',startrow=m+1 , startcol=0,index = False)       \n    writer1.save()\n\n        \ndef automate_woe_population(df,df_categorical_list,filename):\n    '''This User Defined Function creates a new field with suffix \"_WOE\" and gets populated with Weight of Evidence\n    as obtained for each 'Levels' of the Categorical Variables'''\n    import pandas as pd\n    woe_df=pd.read_excel(filename,sheet_name='WOE',header=None)\n    woe_col_nbr = ''    \n\n    for cat_i in df_categorical_list:\n        match_fnd=\"\"\n        new_col=str(cat_i)+ \"_WOE\"\n\n        for j in range(len(woe_df)):        \n            if str(cat_i) == str(woe_df.iloc[j][0]) and match_fnd ==\"\":            \n                match_fnd='y'\n\n            if str(woe_df.iloc[j][0]) == \"Total\":\n                match_fnd = \"\"\n\n            ''' Get the Column Number of WOE '''\n            if (str(woe_df.iloc[j][0]) == \"Levels\") and woe_col_nbr == '' :\n                for k in range(len(woe_df.columns)):\n                    if str(woe_df.iloc[j][k]) == \"WOE\":\n                        woe_col_nbr = int(k)                    \n                                        \n            if match_fnd == 'y':            \n                if (str(woe_df.iloc[j][0]) != \"Levels\") :\n                    \n                    if (str(cat_i) == str(woe_df.iloc[j][0])):\n                        woe_ln=str(\"df['\") + str(cat_i) + str(\"_WOE']=0\")\n                        df[new_col]=0\n                    else:\n\n                        if  str(woe_df.iloc[j][0]) == \"Missing\":\n                            woe_ln = str(\"df.loc[df['\") + str(cat_i) + str(\"'].isna()\")  + str(\",'\") + str(cat_i) + str(\"_WOE']=\") + str(woe_df.iloc[j][woe_col_nbr])\n\n                            df.loc[df[cat_i].isna(),new_col]=woe_df.iloc[j][woe_col_nbr]\n\n                        else:\n                            woe_ln = str(\"df.loc[df['\") + str(cat_i) + str(\"']==\") + str('\"') + str(woe_df.iloc[j][0]) + str('\"') + str(\",'\") + str(cat_i) + str(\"_WOE']=\") + str(woe_df.iloc[j][woe_col_nbr])\n                            print(woe_df.iloc[j][woe_col_nbr])\n                            df.loc[df[cat_i]==str(woe_df.iloc[j][0]),new_col]=woe_df.iloc[j][woe_col_nbr]\n\n\n\n                    print(woe_ln)\n\n    return df    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_bin(train_data,df_cat,target_col=target_nm,filename=\"Categorical_WOE.xlsx\")\ntrain_data=automate_woe_population(train_data,df_cat,filename=\"Categorical_WOE.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cat_list = df_cat.columns.to_list()\ndf_cat_list.remove(target_nm)\nprint(df_cat_list)\ntrain_data=automate_woe_population(train_data,df_cat_list[:1],filename='../input/coarse-classing/Categorical_WOE_coarse_classing.xlsx')\ntrain_data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorical Feature Rank & Plot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#################### Categorical field Descriptive statistics with PLOT  ####################\n'''This User Defined Function performs Binning of Categorical Variables and generate Rank and Plot of the same'''\n\n'''\nInput  :Dataframe having only Categorical Features and Target\n        Number of Bins to be created\n        Target column name\n        Output Excel filename\n        \nOutput :Excel File With Binning information along with Line and Scatter Plot\n\nReturns: None\n'''\n\nimport xlsxwriter\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mimg\nimport numpy as np\nimport os\nimport math\n\n''' Creating two new directory in the working directory to store the Plots of variables ''' \nwork_dir=os.getcwd()\nprint(work_dir)\nnew_dir=work_dir+\"/line_plot/\"\ncheck_dir_present=os.path.isdir(new_dir)\nif check_dir_present == False:\n    os.mkdir(new_dir)\n    print(\"New Directory created : \" + str(new_dir))\nelse:\n    print(\"Existing Directory used : \" + str(new_dir))\n    \nnew_dir=work_dir+\"/scatter_plot/\"\ncheck_dir_present=os.path.isdir(new_dir)\nif check_dir_present == False:\n    os.mkdir(new_dir)\n    print(\"New Directory created : \" + str(new_dir))\nelse:\n    print(\"Existing Directory used : \" + str(new_dir))\n\n    \ndef plot_stat(df_categorical,title_name,target_col):\n    '''This User Defined Functions performs following\n    1. Line Plot showing Volume% and Event% against 'Levels' of the Categorical Variables\n    2. Scatter Plot showing Target against 'Levels' of the Categorical Variables'''\n    \n    global df_plot,file_i,img_name,work_dir,scatter_img_name\n    \n    ''' Line Plot '''\n    width=0.35\n    df_plot['Volume(%)']=df_plot['Volume(%)']*100\n    df_plot['Event(%)']=df_plot['Event(%)']*100\n\n    df_plot.plot(x='Levels',y='Volume(%)',kind='bar',width=width,label=('Volume(%)'),color='b')\n    y_pos=range(len(df_plot['Levels']))\n    plt.ylabel('Volume(%)',color='b')\n    plt.ylim((0,100))\n\n    df_plot['Event(%)'].plot(secondary_y=True,label=('Event(%)'),color='r',rot=90)\n    plt.ylabel('Event(%)',color='r')\n    plt.ylim((0,100))\n    plt.legend(bbox_to_anchor=(.965,.88),loc='upper right',ncol=2,borderaxespad=0)\n    \n    axis_1=plt.gca()\n\n    for i,j in zip(df_plot['Volume(%)'].index,df_plot['Volume(%)']):\n        i=round(i,2)\n        j=round(j,2)\n        axis_1.annotate('%s' %j,xy=(i,j),color='b')\n        \n        \n    for i,j in zip(df_plot['Event(%)'].index,df_plot['Event(%)']):\n        i=round(i,2)\n        j=round(j,2)\n        axis_1.annotate('%s' %j,xy=(i,j),color='r')\n        \n\n    plt.xlim([-width,len(df_plot['Volume(%)'])-width])\n    plt.title(title_name)\n    plt.xlabel('Levels')\n    plt.grid()\n    img_name=str(work_dir)+str(\"/line_plot/\")+str(title_name)+ str(\".png\")\n    plt.savefig(img_name,dpi=300,bbox_inches='tight')\n    plt.clf()\n    \n    ''' Scatter Plot '''\n    fig=plt.figure(figsize=(6.4,4.8))\n    df_categorical[title_name]=df_categorical[title_name].fillna(\"Missing\")\n    df_cat_mod =  df_categorical.loc[df_categorical[title_name].isin(list(df_plot['Levels']))]\n    plt.scatter(df_cat_mod[title_name],df_cat_mod[target_col],c='DarkBlue')                       \n    plt.ylabel(target_col,color='b')\n    plt.xlabel(title_name,color='b')\n    plt.xticks(rotation=90)\n    plt.title(title_name)\n    plt.grid()\n    scatter_img_name=str(work_dir)+str(\"/scatter_plot/\")+str(title_name)+ str(\".png\")\n    plt.show()\n    fig.savefig(scatter_img_name,dpi=300,bbox_inches='tight')\n    plt.clf()\n\ndef add_table_plot(df_categorical,in_file,sheet_nm,target_col,n_levels,out_file):\n    ''' This User Defined Function adds Line Plot and Scatter Plot in an excel \n    just on the beside of the binning data of the categorical Fields.\n    This gives better readability in analysing the data.\n    '''\n    \n    global df_plot,img_name,work_dir,scatter_img_name\n    \n    work_dir=os.getcwd()\n    df_base=pd.read_excel(in_file,header=None,sheet_name=sheet_nm)\n\n    df_base.columns=['Levels', 'Event', 'Non-Event', 'Total','Volume(%)','Event(%)',\n                     'Event_Rate','Non_Event_Rate','Distribution_Event','Distribution_Non_Event',\n                     'WOE','IV']   \n\n    df_base=df_base[['Levels', 'Event', 'Non-Event', 'Total','Volume(%)','Event(%)']]\n    \n    df_base=df_base.fillna('')\n\n    wb=xlsxwriter.Workbook(out_file)\n    ws=wb.add_worksheet('Rank_Plot')\n    wrt_pos_i=0\n    img_pos_i=0\n    \n    for i in range(len(df_base)):\n        if wrt_pos_i == 0:\n            wrt_pos_i = i\n        else:\n            wrt_pos_i = wrt_pos_i + 1        \n        \n        for j in range(len(df_base.columns)):\n            col_pos = chr(ord('A') + j)\n            wrt_pos=str(col_pos)+str(wrt_pos_i)\n            ws.write(wrt_pos,df_base.iloc[i,j])\n       \n        if df_base.iloc[:,0][i] == \"Levels\":\n            img_pos_i = wrt_pos_i\n            pos_min_loc=i+int(1)\n            if pos_min_loc==1:\n                title_name=df_base.columns[0]\n            else:\n                title_loc=int(pos_min_loc)-int(2)\n                title_name=df_base.iloc[:,0][title_loc]\n        \n        if df_base.iloc[:,0][i] == \"Total\":\n\n            pos_max_loc=i\n\n            if n_levels > 0 :  \n                df_plot=df_base[pos_min_loc:pos_max_loc].head(n_levels)\n            else:\n                df_plot=df_base[pos_min_loc:pos_max_loc]\n\n            df_plot.columns=['Levels','Event','Non-Event','TOTAL','Volume(%)','Event(%)']\n            df_plot=df_plot.reset_index()\n\n            ''' Calls plot_stat() to create line plot and scatter plot '''\n            plot_stat(df_categorical,title_name,target_col)\n            img_pos=str('H') + str(int(img_pos_i))   \n            img2=mimg.imread(img_name)\n            imgplot2=plt.imshow(img2)\n\n            ''' Inset line plot in the excel '''\n            ws.insert_image(img_pos,img_name,{'x_scale' : 0.6, 'y_scale' : 0.6})\n\n            ''' Insert Scatter plot in the excel '''\n            scatter_img_pos=str('N') + str(int(img_pos_i))   \n            ws.insert_image(scatter_img_pos,scatter_img_name,{'x_scale' : 0.6, 'y_scale' : 0.6})\n            \n            ''' Provides spacing for image - 14 vertical cells requried for the image '''\n            if len(df_plot) < 14 :\n                wrt_pos_i = wrt_pos_i + 14 - len(df_plot)\n    wb.close()\n    \ndef cat_bin_trend(df_cat_fnl,df_categorical_column,i,target_col,filename):\n    '''This User defined function creates the bins / groups on the 'Levels' of the Categorical Columns\n    1. Event -> Target = 1\n    2. Non-Event -> Target = 0 \n    3. ALong with the Levels of the categorical Columns, A summary record is also created with header \"Total\" '''\n\n    global categorical_target_nx\n\n    categorical_target_nx = pd.DataFrame([df_cat_fnl[(df_cat_fnl[target_col] == 1)].groupby('Levels')[df_categorical_column[i]].count(),\n                                         df_cat_fnl[(df_cat_fnl[target_col] == 0)].groupby('Levels')[df_categorical_column[i]].count(),\n                                         df_cat_fnl.groupby('Levels')[df_categorical_column[i]].count()]).T\n\n    categorical_target_nx.columns = [\"Event\",\"Non-Event\",\"Total\"]    \n    categorical_target_nx['Event'] = categorical_target_nx['Event'].fillna(0)\n    categorical_target_nx['Non-Event'] = categorical_target_nx['Non-Event'].fillna(0)\n    categorical_target_nx['Total'] = categorical_target_nx['Total'].fillna(0)\n\n    categorical_target_nx=categorical_target_nx.reset_index()\n    categorical_target_nx = categorical_target_nx.rename(columns={categorical_target_nx.columns[0]: \"Levels\"})    \n        \n    list_vol_pct=[]\n    list_event_pct=[]\n\n    for j in range(len(categorical_target_nx.Event)):\n\n        list_vol_pct.append(categorical_target_nx['Total'][j]/categorical_target_nx['Total'].sum())\n        list_event_pct.append(categorical_target_nx['Event'][j]/categorical_target_nx['Total'][j])\n    \n    categorical_target_nx = pd.concat([categorical_target_nx,pd.Series(list_vol_pct),pd.Series(list_event_pct)],axis=1)\n    \n    \n    categorical_target_nx = categorical_target_nx[[\"Levels\",\"Event\",\"Non-Event\",\"Total\",0,1]]        \n    categorical_target_nx = categorical_target_nx.rename(columns={categorical_target_nx.columns[len(categorical_target_nx.keys())-2]: \"Volume(%)\"})\n    categorical_target_nx = categorical_target_nx.rename(columns={categorical_target_nx.columns[len(categorical_target_nx.keys())-1]: \"Event(%)\"})\n    categorical_target_nx = categorical_target_nx.sort_values(by=['Total'], ascending=False)\n    \n    categorical_target_nx = categorical_target_nx.append(\n        {\"Levels\":\"Total\",\n         \"Event\":categorical_target_nx['Event'].sum(),\n         \"Non-Event\":categorical_target_nx['Non-Event'].sum(),\n         \"Total\":categorical_target_nx['Total'].sum(),\n         \"Volume(%)\":categorical_target_nx['Volume(%)'].sum(),\n         \"Event(%)\":categorical_target_nx['Event'].sum()/categorical_target_nx['Total'].sum()\n        },ignore_index=True)\n    print(categorical_target_nx)\n                 \n\ndef cat_bin(df,df_categorical,target_col,filename):\n    ''' This User Defined Function performs the following :\n    1. Replace the NAN value with \"Missing\" value \n    2. Binning is done on the unique Labels of the Categorical columns\n    3. For Missing Values, it has been treated as seperate Label - \"Missing\"\n    4. Calculates Weight of Evidence (WOE) of each bin/label of Categorical Variables\n    5. Calculates Information Vale (IV) for each Categorical Variables  '''\n\n    global categorical_target_nx,data_fnl,IV_lst\n    \n    df_categorical_column = list(df_categorical.columns)    \n    \n    '''Initialization of list and excel workbook'''\n    IV_lst=[]\n    writer1 = pd.ExcelWriter(filename,engine='xlsxwriter')\n    workbook=writer1.book\n    worksheet=workbook.add_worksheet('WOE')\n    writer1.sheets['WOE'] = worksheet\n    n = 0\n    m = -1\n    \n    for i in range(len(df_categorical_column)):\n\n        if (df_categorical_column[i] != target_col):       \n            '''Repplacing the NAN Value with \"Missing\" Value for treating the Missing Value as seperate bin/group'''\n            nparray_cat=df_categorical[df_categorical_column[i]].fillna('Missing').unique()\n            nparray_sort=np.sort(nparray_cat)        \n            df_cat = pd.concat([pd.Series(nparray_sort),pd.Series(nparray_sort)],axis=1, keys=[df_categorical_column[i],'Levels'])       \n            df_tst = df.loc[:, [df_categorical_column[i],target_col]].sort_values(by=[df_categorical_column[i]]).fillna('Missing')            \n            df_cat_fnl = pd.merge(df_tst, df_cat, how='left', on=[df_categorical_column[i]]) \n            \n            ''' Creates Groups for each of the unique values of categorical variables '''\n            cat_bin_trend(df_cat_fnl,df_categorical_column,i,target_col,filename)                                   \n\n            ''' Calculates WOE and IV '''\n            calc_iv(df_categorical_column[i])\n     \n            ''' Writing the WOE in seperate worksheet \"WOE\" of Final Excel '''\n            worksheet.write_string(n, 0, df_categorical_column[i])\n            data_fnl.to_excel(writer1,sheet_name='WOE',startrow=n+1 , startcol=0,index = False)\n            n += len(categorical_target_nx.index) + 4\n    \n    ''' Writing the IV in seperate worksheet \"IV\" of Final Excel '''\n    data_IV = pd.DataFrame(IV_lst,columns=['Variable','IV_value'])\n    data_IV = data_IV.sort_values(by=['Variable','IV_value'],ascending=[True,False])\n    data_IV.to_excel(writer1,sheet_name='IV',startrow=m+1 , startcol=0,index = False)       \n    writer1.save()    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_bin(train_data,df_cat,target_col=target_nm,filename=\"Categorical_Base.xlsx\")       \nadd_table_plot(df_cat,in_file= \"Categorical_Base.xlsx\",sheet_nm='WOE',target_col=target_nm,n_levels=5,out_file=\"Categorical_Rank_Plot.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Continuous Feature Binning,WOE,IV","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"################ Continuous Field  : Bin Based WOE and IV calculation ##########\n'''This User Defined Function performs Binning of Continuous Variables and generate Weight of Evidence and Information Value of the same'''\n\n'''\nInput  :Dataframe having only Continuous Features\n        Number of Bins to be created\n        Target column name\n        Output Excel filename\n        \nOutput :Excel File With Binning information along with WOE and IV\n\nReturns: None\n'''\n\nimport xlsxwriter\nimport pandas as pd\nimport numpy as np\nimport os\nimport math\n\n\n\ndef create_volume_group(df_continuous,curr_var,target_col,n_bin):\n    '''This User defined function creates bins on the basis of parameter n_bin (number of bins) provided\n    This algorithm creates almost eqi_volume groups with unique values in groups : \n    1. It calculates the Average Bin Volume by dividing the total volume of data by number of bins\n    2. It sorts the data based on the value of the continous variables\n    3. It directly moves to index (I1) having value Average Bin Volume (ABV) \n    4. It checks the value of continous variable at the index position decided in previosu step\n    5. It finds the index(I2) of last position of the value identified at previous step 4\n    6. It concludes the data of the First Bin within the range (0 to I2)\n    7. The Index I1 is again calculated as I1 = I2 + ABV and step 4-6 is repeated\n    8. This Process is continued till the desired number of bins are created\n    9. Seperate Bin is created if the continuous variable is having missing value\n    \n    Note : qcut() does provide equi-volume groups but does not provide unique values in the groups.\n    hence,qcut() is not used in binning the data.\n    '''\n    \n    global df_fnl\n    df_continuous_column=[]\n    df_continuous_column.append(curr_var)\n    ttl_vol=len(df_continuous)\n    avg_bin_vol = round(ttl_vol/n_bin)\n    lst=[]\n    df_fnl=pd.DataFrame(lst)\n    df_mod=pd.DataFrame(lst)\n    df_mod_null=pd.DataFrame(lst)\n        \n    for i in range(len(df_continuous_column)):\n        \n        if (df_continuous_column[i] != target_col):\n            curr_var=df_continuous_column[i]\n\n            df_mod1=pd.DataFrame(df_continuous[[curr_var,target_col]])\n\n            #### Sort the Data ####\n            df_mod1=df_mod1.sort_values(by=[curr_var],ascending=True)\n            df_mod1=df_mod1.reset_index()\n        \n            df_mod_null=df_mod1[pd.isnull(df_mod1[curr_var])]\n            df_mod1=df_mod1.dropna(subset=[curr_var]) \n        \n            seq=list(range(1,len(df_mod1)+int(1)))\n            df_seq=pd.DataFrame(seq,columns=['sequence'])\n\n            df_mod1=pd.concat([df_mod1,df_seq],axis=1)\n        \n            '''Creating the Missing BIN'''\n            if len(df_mod_null) > int(0):\n                ttl_vol=len(df_mod1)\n                avg_bin_vol = round(ttl_vol/n_bin)\n                group_num='missing'\n\n        \n            pos_indx_max = 0\n            val_list=df_mod1[curr_var].unique()\n            \n            '''Checks if the Unique Values of the continuos variable is 2,then simply create 2 bins'''       \n            if len(val_list) == 2:\n                for i in range(len(val_list)):\n                    val_of_indx=val_list[i]\n                    df_mod3=df_mod1[df_mod1[curr_var] == val_of_indx]\n                    group_num=i+int(1)\n                    df_mod3['Decile']=group_num\n                    df_fnl=pd.concat([df_fnl,df_mod3])\n\n            '''Checks if the Unique Values of the continuos variable is more than 2'''\n            if len(val_list) != 2:        \n                for bin_num in range(n_bin):\n                    if  pos_indx_max < ttl_vol-1 :\n                        '''For the First Group, index is assigned to length of average bin volume'''\n                        if bin_num == 0:\n                            indx = (bin_num+int(1)) * avg_bin_vol\n                        else:\n                            '''Next Groups:index = length of average bin volume \n                            plus the index of last group'''\n                            indx = int(pos_indx_max) + int(avg_bin_vol)\n                \n                        if indx > ttl_vol:\n                            '''Setting the Index for last Group'''\n                            indx = ttl_vol-int(1)\n\n                        val_of_indx = df_mod1[curr_var].iat[indx]\n                                        \n                        if math.isnan(val_of_indx) == True:\n                            pos_indx_min=pos_indx_max+int(1)\n                            pos_indx_max=ttl_vol-int(1)\n                        else:\n                            df_mod3=df_mod1[df_mod1[curr_var] == val_of_indx]\n                            pos_indx_min = pos_indx_max\n                            pos_indx_max = df_mod3['sequence'].iat[-1]\n                       \n                        df_mod3=df_mod1[pos_indx_min:pos_indx_max]\n                        group_num=bin_num+int(1)\n                        df_mod3['Decile']=group_num\n                    \n                        df_fnl=pd.concat([df_fnl,df_mod3])\n                       \n            df_fnl=pd.concat([df_fnl,df_mod_null])\n            \ndef cont_bin_Miss(df_dcl_fnl,df_continuous_column,i,target_col,filename):\n    '''This User Defined Function creates BINS for continous variables having some Missing values'''\n    global continuous_target_nx\n\n    '''The Missing Values have been grouped together as a seperate bin - \"Missing\" '''\n    df_dcl_fnl[df_continuous_column[i]] = df_dcl_fnl[df_continuous_column[i]].astype(object)\n    df_dcl_fnl[df_continuous_column[i]] = df_dcl_fnl[df_continuous_column[i]].fillna('Missing')\n    df_dcl_fnl['Decile'] = df_dcl_fnl['Decile'].fillna('Missing')\n    \n\n    continuous_target_nx = pd.DataFrame([df_dcl_fnl.groupby('Decile')[df_continuous_column[i]].min(),\n                                                 df_dcl_fnl.groupby('Decile')[df_continuous_column[i]].max(),\n                                                 df_dcl_fnl[(df_dcl_fnl[target_col] == 1)].groupby('Decile')[df_continuous_column[i]].count(),\n                                                 df_dcl_fnl[(df_dcl_fnl[target_col] == 0)].groupby('Decile')[df_continuous_column[i]].count(),\n                                                 df_dcl_fnl.groupby('Decile')[df_continuous_column[i]].count()]).T\n    continuous_target_nx.columns = [\"MIN\",\"MAX\",\"Event\",\"Non-Event\",\"TOTAL\"]\n\n    continuous_target_nx.Event=continuous_target_nx.Event.fillna(0)\n    continuous_target_nx['Non-Event']=continuous_target_nx['Non-Event'].fillna(0)\n    \n    continuous_target_nx=continuous_target_nx.reset_index()\n    list1=[]\n    list_vol_pct=[]\n    list_event_pct=[]\n    for i in range(len(continuous_target_nx.MIN)):\n        list1.append(str(continuous_target_nx['MIN'][i])+'-'+str(continuous_target_nx['MAX'][i]))\n        list_vol_pct.append(continuous_target_nx['TOTAL'][i]/continuous_target_nx['TOTAL'].sum())\n        list_event_pct.append(continuous_target_nx['Event'][i]/continuous_target_nx['TOTAL'][i])\n    continuous_target_nx = pd.concat([pd.Series(list1),continuous_target_nx,pd.Series(list_vol_pct),pd.Series(list_event_pct)],axis=1)\n    continuous_target_nx=continuous_target_nx.reindex(columns=[\"Decile\",\"MIN\",\"MAX\",0,\"Event\",\"Non-Event\",\"TOTAL\",1,2])\n    continuous_target_nx = continuous_target_nx[[\"Decile\",\"MIN\",\"MAX\",0,\"Event\",\"Non-Event\",\"TOTAL\",1,2]]\n\n    #print(continuous_target_nx.head(10))\n    continuous_target_nx = continuous_target_nx.rename(columns={continuous_target_nx.columns[len(continuous_target_nx.keys())-6]: \"Range\"})\n    continuous_target_nx = continuous_target_nx.rename(columns={continuous_target_nx.columns[len(continuous_target_nx.keys())-2]: \"Volume(%)\"})\n    continuous_target_nx = continuous_target_nx.rename(columns={continuous_target_nx.columns[len(continuous_target_nx.keys())-1]: \"Event(%)\"})\n    continuous_target_nx = continuous_target_nx.rename(columns={continuous_target_nx.columns[len(continuous_target_nx.keys())-9]: \"BINS\"})       \n    continuous_target_nx = continuous_target_nx.append({\"BINS\":\"Total\",\n                                 \"MIN\":\" \",\n                                 \"MAX\":\" \",\n                                 \"Range\":\" \",\n                                 \"Event\":continuous_target_nx['Event'].sum(),\n                                 \"Non-Event\":continuous_target_nx['Non-Event'].sum(),\n                                 \"TOTAL\":continuous_target_nx['TOTAL'].sum(),\n                                 \"Volume(%)\":continuous_target_nx['Volume(%)'].sum(),\n                                 \"Event(%)\":continuous_target_nx['Event'].sum()/continuous_target_nx['TOTAL'].sum()\n                                                       },ignore_index=True)\n    \ndef cont_bin_NO_Miss(df_dcl_fnl,df_continuous_column,i,target_col,filename):\n    '''This User defined function creates BINS for continuous variables having no missing values'''\n    global continuous_target_nx\n\n    continuous_target_nx = pd.DataFrame([df_dcl_fnl.groupby('Decile')[df_continuous_column[i]].min(),\n                                         df_dcl_fnl.groupby('Decile')[df_continuous_column[i]].max(),\n                                         df_dcl_fnl[(df_dcl_fnl[target_col] == 1)].groupby('Decile')[df_continuous_column[i]].count(),\n                                         df_dcl_fnl[(df_dcl_fnl[target_col] == 0)].groupby('Decile')[df_continuous_column[i]].count(),\n                                         df_dcl_fnl.groupby('Decile')[df_continuous_column[i]].count()]).T\n    continuous_target_nx.columns = [\"MIN\",\"MAX\",\"Event\",\"Non-Event\",\"TOTAL\"]\n\n    continuous_target_nx=continuous_target_nx.reset_index()\n    list1=[]\n    list_vol_pct=[]\n    list_event_pct=[]\n    for i in range(len(continuous_target_nx.MIN)):\n        list1.append(str(continuous_target_nx['MIN'][i])+'-'+str(continuous_target_nx['MAX'][i]))\n        list_vol_pct.append(continuous_target_nx['TOTAL'][i]/continuous_target_nx['TOTAL'].sum())\n        list_event_pct.append(continuous_target_nx['Event'][i]/continuous_target_nx['TOTAL'][i])\n    continuous_target_nx = pd.concat([pd.Series(list1),continuous_target_nx,pd.Series(list_vol_pct),pd.Series(list_event_pct)],axis=1)\n    continuous_target_nx = continuous_target_nx[[\"Decile\",\"MIN\",\"MAX\",0,\"Event\",\"Non-Event\",\"TOTAL\",1,2]]\n    continuous_target_nx = continuous_target_nx.rename(columns={continuous_target_nx.columns[len(continuous_target_nx.keys())-6]: \"Range\"})\n    continuous_target_nx = continuous_target_nx.rename(columns={continuous_target_nx.columns[len(continuous_target_nx.keys())-2]: \"Volume(%)\"})\n    continuous_target_nx = continuous_target_nx.rename(columns={continuous_target_nx.columns[len(continuous_target_nx.keys())-1]: \"Event(%)\"})\n    continuous_target_nx = continuous_target_nx.rename(columns={continuous_target_nx.columns[len(continuous_target_nx.keys())-9]: \"BINS\"})       \n    continuous_target_nx = continuous_target_nx.append({\"BINS\":\"Total\",\n                                 \"MIN\":\" \",\n                                 \"MAX\":\" \",\n                                 \"Range\":\" \",\n                                 \"Event\":continuous_target_nx['Event'].sum(),\n                                 \"Non-Event\":continuous_target_nx['Non-Event'].sum(),\n                                 \"TOTAL\":continuous_target_nx['TOTAL'].sum(),\n                                 \"Volume(%)\":continuous_target_nx['Volume(%)'].sum(),\n                                 \"Event(%)\":continuous_target_nx['Event'].sum()/continuous_target_nx['TOTAL'].sum()\n                                                       },ignore_index=True)\n    \n\ndef calc_iv(var_name):\n    '''Calculates the Wieght of Evidence (WOE) and Information Value(IV) for Continuous fields'''\n    global continuous_target_nx\n    global data_fnl\n    global IV_lst\n    lst=[]\n    \n    continuous_target_nx.Event=continuous_target_nx.Event.fillna(0)\n    continuous_target_nx['Non-Event']=continuous_target_nx['Non-Event'].fillna(0)\n    row_cnt_without_total=len(continuous_target_nx)-int(1)\n    \n    for i in range(len(continuous_target_nx)):\n        data_bin = continuous_target_nx.BINS[i]\n        data_min = continuous_target_nx.MIN[i]\n        data_max = continuous_target_nx.MAX[i]\n        data_Value = continuous_target_nx.Range[i]\n        data_Total = int(continuous_target_nx.TOTAL[i])\n        data_Target_1 = int(continuous_target_nx.Event[i])\n        data_Target_0 = int(continuous_target_nx['Non-Event'][i])\n        data_Target_1_Rate = continuous_target_nx.Event[i] / continuous_target_nx.TOTAL[i]\n        data_Target_0_Rate = continuous_target_nx['Non-Event'][i] / continuous_target_nx.TOTAL[i]\n        data_Distribution_Target_1 = int(continuous_target_nx['Event'][i])/ continuous_target_nx['Event'].head(row_cnt_without_total).sum().sum()\n        data_Distribution_Target_0 = int(continuous_target_nx['Non-Event'][i])/continuous_target_nx['Non-Event'].head(row_cnt_without_total).sum()\n        \n        data_vol_p = continuous_target_nx[\"Volume(%)\"][i]\n        data_event_p = continuous_target_nx[\"Event(%)\"][i]\n        \n        #'WOE' value by ln(Distribution Good/Distribution Bad)\n        data_WoE = np.log(data_Distribution_Target_1 / data_Distribution_Target_0)\n        \n        if (data_WoE == np.inf) or (data_WoE == -np.inf):\n            data_WoE = 0\n            \n        data_IV = data_WoE * (data_Distribution_Target_1 - data_Distribution_Target_0)\n        data=[data_bin,data_min,data_max,data_Value,data_Target_1,data_Target_0,data_Total,data_vol_p,data_event_p,\n              data_Target_1_Rate,data_Target_0_Rate,data_Distribution_Target_1,data_Distribution_Target_0,\n              data_WoE,data_IV]\n        lst.append(data)\n    \n    \n    data_fnl = pd.DataFrame(lst,columns=\n                            ['Bins', 'Min' , 'Max' , 'Range', 'Event', 'Non-Event','Total','Volume(%)','Event(%)',\n                             'Event_Rate','Non_Event_Rate','Distribution_Event','Distribution_Non_Event',\n                             'WOE','IV'])\n    iv_val=[var_name,data_fnl['IV'].head(row_cnt_without_total).sum()]\n    IV_lst.append(iv_val)\n    \ndef cont_bin(df_continuous,n_bin,target_col,filename):\n    ''' This User Defined Function performs the following \n    1. Intiialize Excel Workbook\n    2. Calls cont_bin_Miss() or cont_bin_NO_Miss() to perform binning based on the n_bin provided \n    3. Write to excel with the binning information'''\n    global continuous_target_nx\n    global data_fnl\n    global IV_lst\n    global df_fnl\n    \n    df_continuous_column = list(df_continuous.columns)\n    \n    '''Initialize Excel Work Book for writing'''\n    IV_lst=[]\n    writer1 = pd.ExcelWriter(filename,engine='xlsxwriter')\n    workbook=writer1.book\n    worksheet=workbook.add_worksheet('WOE')\n    writer1.sheets['WOE'] = worksheet\n    n = 1\n    m = -1\n    \n    for i in range(len(df_continuous_column)):\n        if (df_continuous_column[i] != target_col):  \n            '''calls create_volume_group() to create bins - equal volume of bins with unique values '''\n            '''n_bin - > it indicates the number of bins to be created'''\n            \n            create_volume_group(df_continuous,df_continuous_column[i],target_col,n_bin)\n            \n            df_fnl=df_fnl[[df_continuous_column[i],target_col,'Decile']]            \n\n            ''' Checking the Continous Variable is having Missing Values '''\n            if df_fnl[df_continuous_column[i]].isnull().sum() > 0:\n                '''calls cont_bin_Miss() for the conitnous variable having missing value'''                                \n                cont_bin_Miss(df_fnl,df_continuous_column,i,target_col,filename)                \n            else:                \n                '''calls cont_bin_NO_Miss() for the conitnous variable having NO missing value'''             \n                cont_bin_NO_Miss(df_fnl,df_continuous_column,i,target_col,filename)\n            \n            calc_iv(df_continuous_column[i])\n\n            '''Write to excel with the binning information'''\n            worksheet.write_string(n, 0, df_continuous_column[i])\n            data_fnl.to_excel(writer1,sheet_name='WOE',startrow=n+1 , startcol=0,index = False)\n            n += len(continuous_target_nx.index) + 4\n    \n    data_IV=pd.DataFrame(IV_lst,columns=['Variable','IV_value'])\n    data_IV=data_IV.sort_values(by=['Variable','IV_value'],ascending=[True,False])\n    print(data_IV)\n    data_IV.to_excel(writer1,sheet_name='IV',startrow=m+1 , startcol=0,index = False)       \n    writer1.save() \n       \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_bin(df_cont,n_bin=5,target_col=target_nm,filename=\"Continuous_WOE.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Continuous Feature Binning, Rank & Plot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"################ Continuous Field  : Bin Based WOE and IV calculation ##########\n'''This User Defined Function performs Binning of Continuous Variables and generate Weight of Evidence and Information Value of the same'''\n\n'''\nInput  :Dataframe having only Continuous Features\n        Number of Bins to be created\n        Target column name\n        Output Excel filename\n        \nOutput :Excel File With Binning information along with WOE and IV\n\nReturns: None\n'''\n\nimport xlsxwriter\nimport pandas as pd\nimport numpy as np\nimport os\nimport math\n\n\n\ndef create_volume_group(df_continuous,curr_var,target_col,n_bin):\n    '''This User defined function creates bins on the basis of parameter n_bin (number of bins) provided\n    This algorithm creates almost eqi_volume groups with unique values in groups : \n    1. It calculates the Average Bin Volume by dividing the total volume of data by number of bins\n    2. It sorts the data based on the value of the continous variables\n    3. It directly moves to index (I1) having value Average Bin Volume (ABV) \n    4. It checks the value of continous variable at the index position decided in previosu step\n    5. It finds the index(I2) of last position of the value identified at previous step 4\n    6. It concludes the data of the First Bin within the range (0 to I2)\n    7. The Index I1 is again calculated as I1 = I2 + ABV and step 4-6 is repeated\n    8. This Process is continued till the desired number of bins are created\n    9. Seperate Bin is created if the continuous variable is having missing value\n    \n    Note : qcut() does provide equi-volume groups but does not provide unique values in the groups.\n    hence,qcut() is not used in binning the data.\n    '''\n    \n    global df_fnl\n    df_continuous_column=[]\n    df_continuous_column.append(curr_var)\n    ttl_vol=len(df_continuous)\n    avg_bin_vol = round(ttl_vol/n_bin)\n    lst=[]\n    df_fnl=pd.DataFrame(lst)\n    df_mod=pd.DataFrame(lst)\n    df_mod_null=pd.DataFrame(lst)\n\n    for i in range(len(df_continuous_column)):\n        if (df_continuous_column[i] != target_col):\n            curr_var=df_continuous_column[i]\n\n            df_mod1=pd.DataFrame(df_continuous[[curr_var,target_col]])\n\n            #### Sort the Data ####\n            df_mod1=df_mod1.sort_values(by=[curr_var],ascending=True)\n            df_mod1=df_mod1.reset_index()\n        \n            df_mod_null=df_mod1[pd.isnull(df_mod1[curr_var])]\n            df_mod1=df_mod1.dropna(subset=[curr_var]) \n        \n            seq=list(range(1,len(df_mod1)+int(1)))\n            df_seq=pd.DataFrame(seq,columns=['sequence'])\n\n            df_mod1=pd.concat([df_mod1,df_seq],axis=1)\n        \n            '''Creating the Missing BIN'''\n            if len(df_mod_null) > int(0):\n                ttl_vol=len(df_mod1)\n                avg_bin_vol = round(ttl_vol/n_bin)\n                group_num='missing'\n\n        \n            pos_indx_max = 0\n            val_list=df_mod1[curr_var].unique()\n            \n            '''Checks if the Unique Values of the continuos variable is 2,then simply create 2 bins'''       \n            if len(val_list) == 2:\n                for i in range(len(val_list)):\n                    val_of_indx=val_list[i]\n                    df_mod3=df_mod1[df_mod1[curr_var] == val_of_indx]\n                    group_num=i+int(1)\n                    df_mod3['Decile']=group_num\n                    df_fnl=pd.concat([df_fnl,df_mod3])\n\n            '''Checks if the Unique Values of the continuos variable is more than 2'''\n            if len(val_list) != 2:        \n                for bin_num in range(n_bin):\n                    if  pos_indx_max < ttl_vol-1 :\n                        '''For the First Group, index is assigned to length of average bin volume'''\n                        if bin_num == 0:\n                            indx = (bin_num+int(1)) * avg_bin_vol\n                        else:\n                            '''Next Groups:index = length of average bin volume \n                            plus the index of last group'''\n                            indx = int(pos_indx_max) + int(avg_bin_vol)\n                \n                        if indx > ttl_vol:\n                            '''Setting the Index for last Group'''\n                            indx = ttl_vol-int(1)\n\n                        val_of_indx = df_mod1[curr_var].iat[indx]\n                                        \n                        if math.isnan(val_of_indx) == True:\n                            pos_indx_min=pos_indx_max+int(1)\n                            pos_indx_max=ttl_vol-int(1)\n                        else:\n                            df_mod3=df_mod1[df_mod1[curr_var] == val_of_indx]\n                            pos_indx_min = pos_indx_max\n                            pos_indx_max = df_mod3['sequence'].iat[-1]\n                       \n                        df_mod3=df_mod1[pos_indx_min:pos_indx_max]\n                        group_num=bin_num+int(1)\n                        df_mod3['Decile']=group_num\n                    \n                        df_fnl=pd.concat([df_fnl,df_mod3])\n            \n            df_fnl=pd.concat([df_fnl,df_mod_null])\n            \ndef cont_bin_Miss(df_dcl_fnl,df_continuous_column,i,target_col,filename):\n    '''This User Defined Function creates BINS for continous variables having some Missing values'''\n    global continuous_target_nx\n\n    '''The Missing Values have been grouped together as a seperate bin - \"Missing\" '''\n    df_dcl_fnl[df_continuous_column[i]] = df_dcl_fnl[df_continuous_column[i]].astype(object)\n    df_dcl_fnl[df_continuous_column[i]] = df_dcl_fnl[df_continuous_column[i]].fillna('Missing')\n    df_dcl_fnl['Decile'] = df_dcl_fnl['Decile'].fillna('Missing')\n    \n\n    continuous_target_nx = pd.DataFrame([df_dcl_fnl.groupby('Decile')[df_continuous_column[i]].min(),\n                                                 df_dcl_fnl.groupby('Decile')[df_continuous_column[i]].max(),\n                                                 df_dcl_fnl[(df_dcl_fnl[target_col] == 1)].groupby('Decile')[df_continuous_column[i]].count(),\n                                                 df_dcl_fnl[(df_dcl_fnl[target_col] == 0)].groupby('Decile')[df_continuous_column[i]].count(),\n                                                 df_dcl_fnl.groupby('Decile')[df_continuous_column[i]].count()]).T\n    continuous_target_nx.columns = [\"MIN\",\"MAX\",\"Event\",\"Non-Event\",\"TOTAL\"]\n\n    continuous_target_nx.Event=continuous_target_nx.Event.fillna(0)\n    continuous_target_nx['Non-Event']=continuous_target_nx['Non-Event'].fillna(0)\n    \n    continuous_target_nx=continuous_target_nx.reset_index()\n    list1=[]\n    list_vol_pct=[]\n    list_event_pct=[]\n    for i in range(len(continuous_target_nx.MIN)):\n        list1.append(str(continuous_target_nx['MIN'][i])+'-'+str(continuous_target_nx['MAX'][i]))\n        list_vol_pct.append(continuous_target_nx['TOTAL'][i]/continuous_target_nx['TOTAL'].sum())\n        list_event_pct.append(continuous_target_nx['Event'][i]/continuous_target_nx['TOTAL'][i])\n    continuous_target_nx = pd.concat([pd.Series(list1),continuous_target_nx,pd.Series(list_vol_pct),pd.Series(list_event_pct)],axis=1)\n    continuous_target_nx=continuous_target_nx.reindex(columns=[\"Decile\",\"MIN\",\"MAX\",0,\"Event\",\"Non-Event\",\"TOTAL\",1,2])\n    continuous_target_nx = continuous_target_nx[[\"Decile\",\"MIN\",\"MAX\",0,\"Event\",\"Non-Event\",\"TOTAL\",1,2]]\n\n    #print(continuous_target_nx.head(10))\n    continuous_target_nx = continuous_target_nx.rename(columns={continuous_target_nx.columns[len(continuous_target_nx.keys())-6]: \"Range\"})\n    continuous_target_nx = continuous_target_nx.rename(columns={continuous_target_nx.columns[len(continuous_target_nx.keys())-2]: \"Volume(%)\"})\n    continuous_target_nx = continuous_target_nx.rename(columns={continuous_target_nx.columns[len(continuous_target_nx.keys())-1]: \"Event(%)\"})\n    continuous_target_nx = continuous_target_nx.rename(columns={continuous_target_nx.columns[len(continuous_target_nx.keys())-9]: \"BINS\"})       \n    continuous_target_nx = continuous_target_nx.append({\"BINS\":\"Total\",\n                                 \"MIN\":\" \",\n                                 \"MAX\":\" \",\n                                 \"Range\":\" \",\n                                 \"Event\":continuous_target_nx['Event'].sum(),\n                                 \"Non-Event\":continuous_target_nx['Non-Event'].sum(),\n                                 \"TOTAL\":continuous_target_nx['TOTAL'].sum(),\n                                 \"Volume(%)\":continuous_target_nx['Volume(%)'].sum(),\n                                 \"Event(%)\":continuous_target_nx['Event'].sum()/continuous_target_nx['TOTAL'].sum()\n                                                       },ignore_index=True)\n    \ndef cont_bin_NO_Miss(df_dcl_fnl,df_continuous_column,i,target_col,filename):\n    '''This User defined function creates BINS for continuous variables having no missing values'''\n    global continuous_target_nx\n\n    continuous_target_nx = pd.DataFrame([df_dcl_fnl.groupby('Decile')[df_continuous_column[i]].min(),\n                                         df_dcl_fnl.groupby('Decile')[df_continuous_column[i]].max(),\n                                         df_dcl_fnl[(df_dcl_fnl[target_col] == 1)].groupby('Decile')[df_continuous_column[i]].count(),\n                                         df_dcl_fnl[(df_dcl_fnl[target_col] == 0)].groupby('Decile')[df_continuous_column[i]].count(),\n                                         df_dcl_fnl.groupby('Decile')[df_continuous_column[i]].count()]).T\n    continuous_target_nx.columns = [\"MIN\",\"MAX\",\"Event\",\"Non-Event\",\"TOTAL\"]\n\n    continuous_target_nx=continuous_target_nx.reset_index()\n    list1=[]\n    list_vol_pct=[]\n    list_event_pct=[]\n    for i in range(len(continuous_target_nx.MIN)):\n        list1.append(str(continuous_target_nx['MIN'][i])+'-'+str(continuous_target_nx['MAX'][i]))\n        list_vol_pct.append(continuous_target_nx['TOTAL'][i]/continuous_target_nx['TOTAL'].sum())\n        list_event_pct.append(continuous_target_nx['Event'][i]/continuous_target_nx['TOTAL'][i])\n    continuous_target_nx = pd.concat([pd.Series(list1),continuous_target_nx,pd.Series(list_vol_pct),pd.Series(list_event_pct)],axis=1)\n    continuous_target_nx = continuous_target_nx[[\"Decile\",\"MIN\",\"MAX\",0,\"Event\",\"Non-Event\",\"TOTAL\",1,2]]\n    continuous_target_nx = continuous_target_nx.rename(columns={continuous_target_nx.columns[len(continuous_target_nx.keys())-6]: \"Range\"})\n    continuous_target_nx = continuous_target_nx.rename(columns={continuous_target_nx.columns[len(continuous_target_nx.keys())-2]: \"Volume(%)\"})\n    continuous_target_nx = continuous_target_nx.rename(columns={continuous_target_nx.columns[len(continuous_target_nx.keys())-1]: \"Event(%)\"})\n    continuous_target_nx = continuous_target_nx.rename(columns={continuous_target_nx.columns[len(continuous_target_nx.keys())-9]: \"BINS\"})       \n    continuous_target_nx = continuous_target_nx.append({\"BINS\":\"Total\",\n                                 \"MIN\":\" \",\n                                 \"MAX\":\" \",\n                                 \"Range\":\" \",\n                                 \"Event\":continuous_target_nx['Event'].sum(),\n                                 \"Non-Event\":continuous_target_nx['Non-Event'].sum(),\n                                 \"TOTAL\":continuous_target_nx['TOTAL'].sum(),\n                                 \"Volume(%)\":continuous_target_nx['Volume(%)'].sum(),\n                                 \"Event(%)\":continuous_target_nx['Event'].sum()/continuous_target_nx['TOTAL'].sum()\n                                                       },ignore_index=True)\n    \n\ndef calc_iv(var_name):\n    '''Calculates the Wieght of Evidence (WOE) and Information Value(IV) for Continuous fields'''\n    global continuous_target_nx\n    global data_fnl\n    global IV_lst\n    lst=[]\n    \n    continuous_target_nx.Event=continuous_target_nx.Event.fillna(0)\n    continuous_target_nx['Non-Event']=continuous_target_nx['Non-Event'].fillna(0)\n    row_cnt_without_total=len(continuous_target_nx)-int(1)\n    \n    for i in range(len(continuous_target_nx)):\n        data_bin = continuous_target_nx.BINS[i]\n        data_min = continuous_target_nx.MIN[i]\n        data_max = continuous_target_nx.MAX[i]\n        data_Value = continuous_target_nx.Range[i]\n        data_Total = int(continuous_target_nx.TOTAL[i])\n        data_Target_1 = int(continuous_target_nx.Event[i])\n        data_Target_0 = int(continuous_target_nx['Non-Event'][i])\n        data_Target_1_Rate = continuous_target_nx.Event[i] / continuous_target_nx.TOTAL[i]\n        data_Target_0_Rate = continuous_target_nx['Non-Event'][i] / continuous_target_nx.TOTAL[i]\n        data_Distribution_Target_1 = int(continuous_target_nx['Event'][i])/ continuous_target_nx['Event'].head(row_cnt_without_total).sum().sum()\n        data_Distribution_Target_0 = int(continuous_target_nx['Non-Event'][i])/continuous_target_nx['Non-Event'].head(row_cnt_without_total).sum()\n        \n        data_vol_p = continuous_target_nx[\"Volume(%)\"][i]\n        data_event_p = continuous_target_nx[\"Event(%)\"][i]\n        \n        #'WOE' value by ln(Distribution Good/Distribution Bad)\n        data_WoE = np.log(data_Distribution_Target_1 / data_Distribution_Target_0)\n        \n        if (data_WoE == np.inf) or (data_WoE == -np.inf):\n            data_WoE = 0\n            \n        data_IV = data_WoE * (data_Distribution_Target_1 - data_Distribution_Target_0)\n        data=[data_bin,data_min,data_max,data_Value,data_Target_1,data_Target_0,data_Total,data_vol_p,data_event_p,\n              data_Target_1_Rate,data_Target_0_Rate,data_Distribution_Target_1,data_Distribution_Target_0,\n              data_WoE,data_IV]\n        lst.append(data)\n    \n    \n    data_fnl = pd.DataFrame(lst,columns=\n                            ['Bins', 'Min' , 'Max' , 'Range', 'Event', 'Non-Event','Total','Volume(%)','Event(%)',\n                             'Event_Rate','Non_Event_Rate','Distribution_Event','Distribution_Non_Event',\n                             'WOE','IV'])\n    iv_val=[var_name,data_fnl['IV'].head(row_cnt_without_total).sum()]\n    IV_lst.append(iv_val)\n    \ndef cont_bin(df_continuous,n_bin,target_col,filename):\n    ''' This User Defined Function performs the following \n    1. Intiialize Excel Workbook\n    2. Calls cont_bin_Miss() or cont_bin_NO_Miss() to perform binning based on the n_bin provided \n    3. Write to excel with the binning information'''\n    global continuous_target_nx\n    global data_fnl\n    global IV_lst\n    global df_fnl\n    \n    df_continuous_column = list(df_continuous.columns)\n    \n    '''Initialize Excel Work Book for writing'''\n    IV_lst=[]\n    writer1 = pd.ExcelWriter(filename,engine='xlsxwriter')\n    workbook=writer1.book\n    worksheet=workbook.add_worksheet('Continous')\n    writer1.sheets['Continous'] = worksheet\n    n = 1\n    m = -1\n    \n    for i in range(len(df_continuous_column)):\n        if (df_continuous_column[i] != target_col):  \n            '''calls create_volume_group() to create bins - equal volume of bins with unique values '''\n            '''n_bin - > it indicates the number of bins to be created'''\n            create_volume_group(df_continuous,df_continuous_column[i],target_col,n_bin)\n            df_fnl=df_fnl[[df_continuous_column[i],target_col,'Decile']]\n\n            ''' Checking the Continous Variable is having Missing Values '''\n            if df_fnl[df_continuous_column[i]].isnull().sum() > 0:\n                '''calls cont_bin_Miss() for the conitnous variable having missing value'''                \n                cont_bin_Miss(df_fnl,df_continuous_column,i,target_col,filename)                \n            else:\n                '''calls cont_bin_NO_Miss() for the conitnous variable having NO missing value'''             \n                cont_bin_NO_Miss(df_fnl,df_continuous_column,i,target_col,filename)\n            \n            calc_iv(df_continuous_column[i])\n\n            '''Write to excel with the binning information'''\n            worksheet.write_string(n, 0, df_continuous_column[i])\n            data_fnl.to_excel(writer1,sheet_name='Continous',startrow=n+1 , startcol=0,index = False)\n            n += len(continuous_target_nx.index) + 4\n    \n    data_IV=pd.DataFrame(IV_lst,columns=['Variable','IV_value'])\n    data_IV=data_IV.sort_values(by=['Variable','IV_value'],ascending=[True,False])\n    print(data_IV)\n    data_IV.to_excel(writer1,sheet_name='IV',startrow=m+1 , startcol=0,index = False)       \n    writer1.save()      \n\n    '''This User Defined Function performs Rank and Plot of Continuous Variables'''\n\n'''\nInput  :Excel File With Binning information\n        \nOutput :Excel File With Binning information along with Line and Scatter Plot\n\nReturns: None\n'''\n\nimport xlsxwriter\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mimg\nimport numpy as np\nimport os\nimport math\nimport pandas as pd\nimport numpy as np\n\n''' Creating two new directory in the working directory to store the Plots of variables ''' \nwork_dir=os.getcwd()\nprint(work_dir)\nnew_dir=work_dir+\"/line_plot/\"\ncheck_dir_present=os.path.isdir(new_dir)\nif check_dir_present == False:\n    os.mkdir(new_dir)\n    print(\"New Directory created : \" + str(new_dir))\nelse:\n    print(\"Existing Directory used : \" + str(new_dir))\n    \nnew_dir=work_dir+\"/scatter_plot/\"\ncheck_dir_present=os.path.isdir(new_dir)\nif check_dir_present == False:\n    os.mkdir(new_dir)\n    print(\"New Directory created : \" + str(new_dir))\nelse:\n    print(\"Existing Directory used : \" + str(new_dir))\n\ndef plot_stat(df_continuous,title_name,target_col):\n    '''This User Defined Functions performs following\n    1. Line Plot showing Volume% and Event% against 'BINS' of the Continuous Variables\n    2. Scatter Plot showing Target against 'BINS' of the Continuous Variables'''\n    \n    global df_plot,file_i,img_name,work_dir,scatter_img_name\n    \n    ''' Line Plot '''\n    \n    width=0.35\n    df_plot['Volume(%)']=df_plot['Volume(%)']*100\n    df_plot['Event(%)']=df_plot['Event(%)']*100\n\n    df_plot.plot(x='Bins',y='Volume(%)',kind='bar',width=width,label=('Volume(%)'),color='b')\n    plt.ylabel('Volume(%)',color='b')\n    plt.ylim((0,100))\n    \n    df_plot['Event(%)'].plot(secondary_y=True,label=('Event(%)'),color='r')\n    plt.ylabel('Event(%)',color='r')\n    plt.ylim((0,100))\n    plt.legend(bbox_to_anchor=(.965,.88),loc='upper right',ncol=2,borderaxespad=0)\n    \n    \n    axis_1=plt.gca()\n\n    for i,j in zip(df_plot['Volume(%)'].index,df_plot['Volume(%)']):\n        i=round(i,1)\n        j=round(j,1)\n        axis_1.annotate('%s' %j,xy=(i,j),color='b')\n        \n    for i,j in zip(df_plot['Event(%)'].index,df_plot['Event(%)']):\n        i=round(i,1)\n        j=round(j,1)\n        axis_1.annotate('%s' %j,xy=(i,j),color='r')\n\n    plt.xlim([-width,len(df_plot['Volume(%)'])-width])\n    plt.title(title_name)\n    plt.xlabel(\"BINS\")\n    plt.grid()\n    img_name=str(work_dir)+str(\"/line_plot/\")+str(title_name)+ str(\".png\")\n    plt.savefig(img_name,dpi=300,bbox_inches='tight')\n    plt.clf()\n    \n    ''' Scatter Plot '''\n    fig=plt.figure(figsize=(6.4,4.8))\n    plt.scatter(df_continuous[title_name],df_continuous[target_col],c='DarkBlue')\n    plt.ylabel(target_col,color='b')\n    plt.xlabel(title_name,color='b')\n    plt.title(title_name)\n    plt.grid()\n    scatter_img_name=str(work_dir)+str(\"/scatter_plot/\")+str(title_name)+ str(\".png\")\n    #plt.show()\n    fig.savefig(scatter_img_name,dpi=300,bbox_inches='tight')\n    plt.clf()\n\ndef add_table_plot(df_continuous,in_file,sheet_nm,out_file,target_col):\n    ''' This User Defined Function adds Line Plot and Scatter Plot in an excel \n    just on the beside of the binning data of the categorical Fields.\n    This gives better readability in analysing the data.\n    '''\n    \n    global df_plot,img_name,work_dir,scatter_img_name\n    \n    work_dir=os.getcwd()\n    df_cont=pd.read_excel(in_file,sheet_name=sheet_nm,header=None)\n    \n    df_cont.columns=['Bins', 'Min' , 'Max' , 'Range', 'Event', 'Non-Event','Total','Volume(%)','Event(%)',\n                             'Event_Rate','Non_Event_Rate','Distribution_Event','Distribution_Non_Event',\n                             'WOE','IV' ]   \n\n    df_cont=df_cont[['Bins','Min','Max','Range','Event','Non-Event','Total','Volume(%)','Event(%)']]\n    \n    df_cont=df_cont.fillna('')\n    wb=xlsxwriter.Workbook(out_file)\n    ws=wb.add_worksheet('Rank_Plot')\n    wrt_pos_i=0\n    img_pos_i=0    \n    \n    for i in range(len(df_cont)):\n        if wrt_pos_i == 0:\n            wrt_pos_i = i\n        else:\n            wrt_pos_i = wrt_pos_i + 1        \n        \n        for j in range(len(df_cont.columns)):\n            col_pos = chr(ord('A') + j)\n            wrt_pos=str(col_pos)+str(wrt_pos_i)\n            ws.write(wrt_pos,df_cont.iloc[i,j])\n\n        if df_cont.iloc[:,0][i] == \"Bins\":\n            img_pos_i = wrt_pos_i\n            pos_min_loc=i+int(1)\n            if pos_min_loc==1:\n                title_name=df_cont.columns[0]\n                \n            else:\n                title_loc=int(pos_min_loc)-int(2)\n                title_name=df_cont.iloc[:,0][title_loc]                \n        \n        if df_cont.iloc[:,0][i] == \"Total\":\n            pos_max_loc=i\n            df_plot=df_cont[pos_min_loc:pos_max_loc]\n            df_plot.columns=['Bins','Min','Max','Range','Event','Non-Event','Total','Volume(%)','Event(%)']\n            df_plot=df_plot.reset_index()\n            \n            ''' Calls plot_stat() to create line plot and scatter plot '''\n            \n            plot_stat(df_continuous,title_name,target_col)\n            img_pos=str('K')+ str(int(img_pos_i)) \n            img2=mimg.imread(img_name)\n            imgplot2=plt.imshow(img2)\n\n            ''' Insert line plot in the excel '''\n            ws.insert_image(img_pos,img_name,{'x_scale' : 0.6, 'y_scale' : 0.6})\n            \n            ''' Insert scatter plot in the excel '''\n            scatter_img_pos=str('Q')+ str(int(img_pos_i))\n            ws.insert_image(scatter_img_pos,scatter_img_name,{'x_scale' : 0.6, 'y_scale' : 0.6})\n            \n            ''' Provides spacing for image - 14 vertical cells requried for the image '''\n            if len(df_plot) < 14 :\n                wrt_pos_i = wrt_pos_i + 14 - len(df_plot)\n           \n            \n    wb.close()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_bin(df_cont,n_bin=5,target_col=target_nm,filename=\"Continuous_Base.xlsx\") \nadd_table_plot(df_cont,in_file=\"Continuous_Base.xlsx\",sheet_nm='Continous',target_col=target_nm,out_file=\"Continuous_Rank_Plot.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def automate_woe_population(df,df_continuous_list,filename):\n    '''This User Defined Function creates a new field with suffix \"_WOE\" and gets populated with Weight of Evidence\n    as obtained for each 'Levels' of the Categorical Variables'''\n    import pandas as pd\n    woe_df=pd.read_excel(filename,header=0,sheet_name='WOE')\n    woe_col_nbr = ''    \n\n    for cat_i in df_continuous_list:\n        match_fnd=\"\"\n        new_col=str(cat_i)+ \"_WOE\"\n        print(new_col)        \n\n        for j in range(len(woe_df)):        \n            if str(cat_i) == str(woe_df.iloc[j][0]) and match_fnd ==\"\":            \n                match_fnd='y'\n\n            if str(woe_df.iloc[j][0]) == \"Total\":\n                match_fnd = \"\"\n\n            ''' Get the Column Number of WOE '''\n            if (str(woe_df.iloc[j][0]) == \"Bins\") and woe_col_nbr == '' :\n                for k in range(len(woe_df.columns)):\n                    if str(woe_df.iloc[j][k]) == \"WOE\":\n                        woe_col_nbr = int(k)                    \n                                        \n            if match_fnd == 'y':            \n                if (str(woe_df.iloc[j][0]) != \"Levels\") :\n                    \n                    if (str(cat_i) == str(woe_df.iloc[j][0])):\n                        woe_ln=str(\"df['\") + str(cat_i) + str(\"_WOE']=0\")\n                        df[new_col]=0\n                    else:\n\n                        if  str(woe_df.iloc[j][0]) == \"Missing\":\n                            woe_ln = str(\"df.loc[df['\") + str(cat_i) + str(\"'].isna()\")  + str(\",'\") + str(cat_i) + str(\"_WOE']=\") + str(woe_df.iloc[j][woe_col_nbr])\n\n                            df.loc[df[cat_i].isna(),new_col]=woe_df.iloc[j][woe_col_nbr]\n                            df[new_col] = df[new_col].astype(float)                        \n                        \n                        elif str(woe_df.iloc[j][0]) == \"Bins\":\n                            continue\n                            \n                        else:\n                            woe_ln = str(\"df.loc[df['\") + str(cat_i) + str(\"']>=\") + str(woe_df.iloc[j][1]) + str(' & ') + str(\"df.loc[df['\") + str(cat_i) + str(\"']<=\") + str(woe_df.iloc[j][2]) + str(\",'\") + str(cat_i) + str(\"_WOE']=\") + str(woe_df.iloc[j][woe_col_nbr])                           \n                            df.loc[(df[cat_i] >= float(woe_df.iloc[j][1])) & (df[cat_i] <= float(woe_df.iloc[j][2])),new_col]=woe_df.iloc[j][woe_col_nbr]\n                            df[new_col] = df[new_col].astype(float)\n\n\n\n                    print(woe_ln)\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_data=automate_woe_population(train_data,df_cont,filename=\"Continuous_Base.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cont_list = df_cont.columns.to_list()\ndf_cont_list.remove(target_nm)\nprint(df_cont_list)\ntrain_data=automate_woe_population(train_data,df_cont_list,filename='../input/coarse-classing/Continuous_WOE_coarse_classing.xlsx')\ntrain_data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dummy Variable Creation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"slct_cont_cat_woe = ['Dealer codes from where customer has purchased the Two wheeler_WOE','Product code of Two wheeler_WOE','Gender_WOE','Employment type_WOE','Resident type of customer_WOE','Tier_WOE','Customer has bounced in first EMI_WOE','No of times bounced 12 months_WOE','Maximum MOB_WOE','No of times bounced while repaying the loan_WOE','EMI_WOE','Loan Amount_WOE','Tenure_WOE','No of advance EMI paid_WOE','Rate of interest_WOE','Customer age when loanwas taken_WOE','No of loans_WOE','No of secured loans_WOE','No of unsecured loans_WOE','Maximum amount sanctioned for any Two wheeler loan_WOE','No of times 30 days past due in last 6 months_WOE','No of times 60 days past due in last 6 months_WOE','No of times 90 days past due in last 3 months_WOE','Age_WOE','Target variable']\ntrain_data_dummy = train_data[slct_cont_cat_woe]\ntrain_data_dummy['Dealer codes from where customer has purchased the Two wheeler_WOE'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Dealer_code_dummies = pd.get_dummies(train_data_dummy['Dealer codes from where customer has purchased the Two wheeler_WOE'], drop_first=True, prefix='Dealer_code')\nProduct_code_dummies = pd.get_dummies(train_data_dummy['Product code of Two wheeler_WOE'], drop_first=True, prefix='Product_code')\nGender_dummies = pd.get_dummies(train_data_dummy['Gender_WOE'], drop_first=True, prefix='Gender')\nEmployment_type_dummies = pd.get_dummies(train_data_dummy['Employment type_WOE'], drop_first=True, prefix='Employment_type')\nRes_type_cust_dummies = pd.get_dummies(train_data_dummy['Resident type of customer_WOE'], drop_first=True, prefix='Res_type_cust')\nTier_dummies = pd.get_dummies(train_data_dummy['Tier_WOE'], drop_first=True, prefix='Tier')\nCust_bounced_first_EMI_dummies = pd.get_dummies(train_data_dummy['Customer has bounced in first EMI_WOE'], drop_first=True, prefix='Cust_bounced_first')\nN_times_bounced_12m_dummies = pd.get_dummies(train_data_dummy['No of times bounced 12 months_WOE'], drop_first=True, prefix='N_times_bounced_12m')\nMaximum_MOB_dummies = pd.get_dummies(train_data_dummy['Maximum MOB_WOE'], drop_first=True, prefix='Maximum_MOB')\nN_time_bounced_dummies = pd.get_dummies(train_data_dummy['No of times bounced while repaying the loan_WOE'], drop_first=True, prefix='N_time_bounced')\nEMI_dummies = pd.get_dummies(train_data_dummy['EMI_WOE'], drop_first=True, prefix='EMI')\nLoan_Amount_dummies = pd.get_dummies(train_data_dummy['Loan Amount_WOE'], drop_first=True, prefix='Loan Amount')\nTenure_dummies = pd.get_dummies(train_data_dummy['Tenure_WOE'], drop_first=True, prefix='Tenure')\nNo_of_adv_EMI_dummies = pd.get_dummies(train_data_dummy['No of advance EMI paid_WOE'], drop_first=True, prefix='No_of_adv_EMI')\nRate_of_interest_dummies = pd.get_dummies(train_data_dummy['Rate of interest_WOE'], drop_first=True, prefix='Rate_of_interest')\nCustomer_age_ln_taken_dummies = pd.get_dummies(train_data_dummy['Customer age when loanwas taken_WOE'], drop_first=True, prefix='Customer_age_ln_taken')\nNo_of_loans_dummies = pd.get_dummies(train_data_dummy['No of loans_WOE'], drop_first=True, prefix='No_of_loans')\nNo_of_secured_loans_dummies = pd.get_dummies(train_data_dummy['No of secured loans_WOE'], drop_first=True, prefix='No of secured loans')\nNo_of_unsecured_loans_dummies = pd.get_dummies(train_data_dummy['No of unsecured loans_WOE'], drop_first=True, prefix='No_of_unsecured_loans')\nMin_amt_sanc_dummies = pd.get_dummies(train_data_dummy['Maximum amount sanctioned for any Two wheeler loan_WOE'], drop_first=True, prefix='Min_amt_sanc')\nN_time_30dpd_last6m_dummies = pd.get_dummies(train_data_dummy['No of times 30 days past due in last 6 months_WOE'], drop_first=True, prefix='N_time_30dpd_last6m')\nN_time_60dpd_last6m_dummies = pd.get_dummies(train_data_dummy['No of times 60 days past due in last 6 months_WOE'], drop_first=True, prefix='N_time_60dpd_last6m')\nN_time_90dpd_last3m_dummies = pd.get_dummies(train_data_dummy['No of times 90 days past due in last 3 months_WOE'], drop_first=True, prefix='N_time_90dpd_last3m')\nAge_dummies = pd.get_dummies(train_data_dummy['Age_WOE'], drop_first=True, prefix='Age')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_dummy_final = pd.concat([Dealer_code_dummies ,Product_code_dummies ,Gender_dummies ,Employment_type_dummies ,Res_type_cust_dummies ,Tier_dummies ,Cust_bounced_first_EMI_dummies ,N_times_bounced_12m_dummies ,Maximum_MOB_dummies ,N_time_bounced_dummies ,EMI_dummies ,Loan_Amount_dummies ,Tenure_dummies ,No_of_adv_EMI_dummies ,Rate_of_interest_dummies ,Customer_age_ln_taken_dummies ,No_of_loans_dummies ,No_of_secured_loans_dummies ,No_of_unsecured_loans_dummies ,Min_amt_sanc_dummies ,N_time_30dpd_last6m_dummies ,N_time_60dpd_last6m_dummies ,N_time_90dpd_last3m_dummies ,Age_dummies ,\n        train_data_dummy[['Target variable']]], axis=1)\ntrain_data_dummy_final.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.concat([train_data,train_data_dummy_final], axis=1)\nprint(train_data.shape)\nprint(train_data.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlations between Dummy Variables [Heatmap]","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,10))\nsns.set(font_scale = 1.25)\nsns.heatmap(train_data_dummy_final[train_data_dummy_final.columns[1:]].corr(),annot = True, fmt = \".1f\", \n           cmap = (sns.cubehelix_palette(20, start = 0.5, rot = -0.75)))\nplt.show()\nplt.savefig(\"Correlation_features_Heatmap.png\",dpi=300,bbox_inches='tight')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Pie chart of loans taken by *Gender***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"darkgrid\")\n\nfig = plt.figure(figsize=(13,6))\nplt.subplot(121)\n\ntrain_data[\"Gender\"].value_counts().plot.pie(autopct = \"%1.0f%%\",colors = [\"gold\",\"b\"],startangle = 60,\n                                                                        wedgeprops={\"linewidth\":2,\"edgecolor\":\"k\"},explode=[.05,0],shadow =True)\nplt.title(\"distribution of client owning a car\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1. Loan Amount**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"darkgrid\")\n\nfig, ax=plt.subplots(nrows =1,ncols=3,figsize=(25,8))\nax[0].set_title(\"Loan Amount (Distribution Plot)\")\nsns.distplot(train_data['Loan Amount'],ax=ax[0])\nax[1].set_title(\"Loan Amount (Violin Plot)\")\nsns.violinplot(data =train_data, x='Loan Amount',ax=ax[1], inner=\"quartile\")\nax[2].set_title(\"Loan Amount (Box Plot)\")\nsns.boxplot(data =train_data, x='Loan Amount',ax=ax[2],orient='v')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2. Interest Rate**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"darkgrid\")\n\nfig, ax=plt.subplots(nrows =1,ncols=3,figsize=(25,8))\nax[0].set_title(\"Loan Amount (Distribution Plot)\")\nsns.distplot(train_data['Rate of interest'],ax=ax[0])\nax[1].set_title(\"Loan Amount (Violin Plot)\")\nsns.violinplot(data =train_data, x='Rate of interest',ax=ax[1], inner=\"quartile\")\nax[2].set_title(\"Loan Amount (Box Plot)\")\nsns.boxplot(data =train_data, x='Rate of interest',ax=ax[2],orient='v')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"**3. Age at which customer has taken the loan**  ***(Client's Age)***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,6))\n\n#bar plot\ntrain_data['Customer age when loanwas taken'].value_counts().plot(kind='bar',color='b',alpha=0.7, edgecolor='black')\nplt.xlabel(\"Age\", labelpad=14)\nplt.ylabel(\"Count of People\", labelpad=14)\nplt.title(\" Age of Customer when the loan was approved\")\nplt.legend(loc=\"best\",prop={\"size\":12})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,6))\n\n#histogarm\nsns.distplot(train_data[\"Customer age when loanwas taken\"],color=\"b\")\nplt.xlabel('Age')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3B. Customer Age as of today **  ***(Client's Age)***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,6))\n\n#bar plot\ntrain_data['Age'].value_counts().plot(kind='bar',color='b',alpha=0.7, edgecolor='black')\nplt.xlabel(\"Age\", labelpad=14)\nplt.ylabel(\"Count of People\", labelpad=14)\nplt.title(\"Customer Age as of today\")\nplt.legend(loc=\"best\",prop={\"size\":12})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,6))\n\n#histogarm\nsns.distplot(train_data[\"Age\"],color=\"b\")\nplt.xlabel('Age')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**4. Ratio of loan taken by Two-Wheeler type**\n#### MC : Motorcycle , MO : Moped, SC : Scooter","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\ntrain_data[\"Product code of Two wheeler\"].value_counts().plot.pie(autopct = \"%1.1f%%\",fontsize=8,\n                                                             colors = sns.color_palette(\"prism\",5),\n                                              wedgeprops={\"linewidth\":2,\"edgecolor\":\"white\"},\n                                                               shadow =True)\nplt.title(\"Two Wheeler Type\")\n#MC : Motorcycle , MO : Moped, SC : Scooter\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**5. Number of times bounced while repaying the loan**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(24,8))\nsns.countplot(x=\"No of times bounced while repaying the loan\", hue=\"Employment type\", data=train_data, edgecolor='k')\nplt.xlim(0, 6)\nplt.ylim(0, 16000)\nplt.legend(loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dealing with Missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_missing=0\ndef missing_values_table(df_phase3):\n    mis_val = df_phase3.isnull().sum()\n    mis_val_percent = 100*df_phase3.isnull().sum()/len(df_phase3)\n    mis_val_table = pd.concat([mis_val, mis_val_percent], axis = 'columns')\n    mis_val_table_ren_columns = mis_val_table.rename(columns ={0: 'Count of Missing Values', 1: '% of Total Values'} )\n\n    #Sort the table by percent of missing values descending\n    mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:, 1] != 0 ].sort_values(by='% of Total Values', ascending = False).round(1)\n    mis_val_table_ren_columns = mis_val_table_ren_columns.reset_index()\n    #print summary\n    #print(\"The dataset has \" + str(df_phase3.shape[0]) + \" rows and \" + str(df_phase3.shape[1]) + \" columns.\" )\n    #print(str(mis_val_table_ren_columns.shape[0]) )\n\n    return mis_val_table_ren_columns;\n    \ndf_missing=missing_values_table(train_data)\nprint(df_missing)\ndf_dummy_missing=missing_values_table(train_data_dummy_final)\nprint(df_dummy_missing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(missing_values_table(train_data_bkp)).to_excel(\"Missing_statistics.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Continuous Missing Value Imputation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Impute with most similar bin value(referring rank & plot excel)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_bkp['No of times bounced while repaying the loan'] = train_data_bkp['No of times bounced while repaying the loan'].replace(np.nan,1)\ntrain_data_bkp['Customer age when loanwas taken'] = train_data_bkp['Customer age when loanwas taken'].replace(np.nan,32)\ntrain_data_bkp['Age'] = train_data_bkp['Age'].replace(np.nan,36)\ntrain_data_bkp['Maximum MOB'] = train_data_bkp['Maximum MOB'].replace(np.nan,14)\ntrain_data_bkp['Rate of interest'] = train_data_bkp['Rate of interest'].replace(np.nan,13.01)\ntrain_data_bkp['No of advance EMI paid'] = train_data_bkp['No of advance EMI paid'].replace(np.nan, 2)\ntrain_data_bkp['Tenure'] = train_data_bkp['Tenure'].replace(np.nan,25)\ntrain_data_bkp['Loan Amount'] = train_data_bkp['Loan Amount'].replace(np.nan, 25012)\ntrain_data_bkp['EMI'] = train_data_bkp['EMI'].replace(np.nan, 2158)\ntrain_data_bkp['Maximum amount sanctioned for any Two wheeler loan'] = train_data_bkp['Maximum amount sanctioned for any Two wheeler loan'].replace(np.nan, 33204)\n\ntrain_data['No of times bounced while repaying the loan_MSB'] = train_data_bkp['No of times bounced while repaying the loan'] \ntrain_data['Customer age when loanwas taken_MSB'] = train_data_bkp['Customer age when loanwas taken'] \ntrain_data['Age_MSB'] = train_data_bkp['Age'] \ntrain_data['Maximum MOB_MSB'] = train_data_bkp['Maximum MOB'] \ntrain_data['Rate of interest_MSB'] = train_data_bkp['Rate of interest'] \ntrain_data['No of advance EMI paid_MSB'] = train_data_bkp['No of advance EMI paid'] \ntrain_data['Tenure_MSB'] = train_data_bkp['Tenure'] \ntrain_data['Loan Amount_MSB'] = train_data_bkp['Loan Amount'] \ntrain_data['EMI_MSB'] = train_data_bkp['EMI'] \ntrain_data['Maximum amount sanctioned for any Two wheeler loan_MSB'] = train_data_bkp['Maximum amount sanctioned for any Two wheeler loan'] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Impute with Mean value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Replace Null Values (np.nan) with mean\ntrain_data['EMI'] = train_data['EMI'].replace(np.nan, train_data['EMI'].mean())\ntrain_data['Loan Amount'] = train_data['Loan Amount'].replace(np.nan, train_data['Loan Amount'].mean())\ntrain_data['Maximum amount sanctioned for any Two wheeler loan'] = train_data['Maximum amount sanctioned for any Two wheeler loan'].replace(np.nan, train_data['Maximum amount sanctioned for any Two wheeler loan'].mean())\n\n#Checking for null values in Age column\nprint(train_data['EMI'].isnull().sum())\nprint(train_data['Loan Amount'].isnull().sum())\nprint(train_data['Maximum amount sanctioned for any Two wheeler loan'].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Impute with Median value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#In the same way we can impute using median and mode\ntrain_data['Maximum MOB'] = train_data['Maximum MOB'].replace(np.nan, train_data['Maximum MOB'].median())\ntrain_data['No of times bounced while repaying the loan'] = train_data['No of times bounced while repaying the loan'].replace(np.nan, train_data['No of times bounced while repaying the loan'].median())\ntrain_data['Tenure'] = train_data['Tenure'].replace(np.nan, train_data['Tenure'].median())\ntrain_data['No of advance EMI paid'] = train_data['No of advance EMI paid'].replace(np.nan, train_data['No of advance EMI paid'].median())\ntrain_data['Rate of interest'] = train_data['Rate of interest'].replace(np.nan, train_data['Rate of interest'].median())\ntrain_data['Customer age when loanwas taken'] = train_data['Customer age when loanwas taken'].replace(np.nan, train_data['Customer age when loanwas taken'].median())\ntrain_data['Age'] = train_data['Age'].fillna(train_data['Age'].mode())\n#Checking for null values in Age column\nprint(train_data['Maximum MOB'].isnull().sum())\nprint(train_data['No of times bounced while repaying the loan'].isnull().sum())\nprint(train_data['Tenure'].isnull().sum())\nprint(train_data['No of advance EMI paid'].isnull().sum())\nprint(train_data['Rate of interest'].isnull().sum())\nprint(train_data['Customer age when loanwas taken'].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Separating dependent and independent variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"slct_cat_woe = ['Customer has bounced in first EMI','No of times bounced 12 months','Maximum MOB','No of times bounced while repaying the loan','EMI','Loan Amount','Tenure','No of advance EMI paid','Rate of interest','Customer age when loanwas taken','No of loans','No of secured loans','No of unsecured loans','Maximum amount sanctioned for any Two wheeler loan','No of times 30 days past due in last 6 months','No of times 60 days past due in last 6 months','No of times 90 days past due in last 3 months','Age','Dealer codes from where customer has purchased the Two wheeler_WOE','Product code of Two wheeler_WOE','Gender_WOE','Employment type_WOE','Resident type of customer_WOE','Tier_WOE']\nslct_cat_cont_woe = ['Dealer codes from where customer has purchased the Two wheeler_WOE','Product code of Two wheeler_WOE','Gender_WOE','Employment type_WOE','Resident type of customer_WOE','Tier_WOE','Customer has bounced in first EMI_WOE','No of times bounced 12 months_WOE','Maximum MOB_WOE','No of times bounced while repaying the loan_WOE','EMI_WOE','Loan Amount_WOE','Tenure_WOE','No of advance EMI paid_WOE','Rate of interest_WOE','Customer age when loanwas taken_WOE','No of loans_WOE','No of secured loans_WOE','No of unsecured loans_WOE','Maximum amount sanctioned for any Two wheeler loan_WOE','No of times 30 days past due in last 6 months_WOE','No of times 60 days past due in last 6 months_WOE','No of times 90 days past due in last 3 months_WOE','Age_WOE']\nslct_cont_msb_cat_woe = ['Age_MSB','Customer age when loanwas taken_MSB','Customer has bounced in first EMI','Dealer codes from where customer has purchased the Two wheeler_WOE','EMI_MSB','Employment type_WOE','Gender_WOE','Loan Amount_MSB','Maximum amount sanctioned for any Two wheeler loan_MSB','Maximum MOB_MSB','No of loans','No of times 60 days past due in last 6 months','No of times 90 days past due in last 3 months','No of times bounced 12 months','No of times bounced while repaying the loan_MSB','No of unsecured loans','No of secured loans','No of times 30 days past due in last 6 months','No of advance EMI paid_MSB','Product code of Two wheeler_WOE','Rate of interest_MSB','Resident type of customer_WOE','Tenure_MSB','Tier_WOE']\nslct_woe_dummy = ['Dealer_code_0.0','Dealer_code_0.9559735155414971','Dealer_code_1.5370520479052074','Dealer_code_2.688962596900705','Product_code_-0.1421213491381836','Product_code_-0.05826982136746174','Product_code_0.0','Product_code_0.2804872039936926','Gender_0.03696311356576965','Employment_type_-0.7765767471804874','Employment_type_-0.5012532942327598','Employment_type_-0.1998256728216269','Employment_type_0.08504806076174212','Res_type_cust_0.02044958160637848','Res_type_cust_0.162970846857935','Tier_-0.1321380510081557','Tier_-0.04634936386258279','Tier_0.5574790752604555','Cust_bounced_first_0.4048291007267008','N_times_bounced_12m_0.3834542680840288','N_times_bounced_12m_0.9716050326130725','Maximum_MOB_-0.007371935931145213','Maximum_MOB_0.006369669776595649','Maximum_MOB_0.02113918192071991','Maximum_MOB_0.1901809851536456','N_time_bounced_0.03673379522856265','N_time_bounced_0.6774751994813883','EMI_-0.04318109198821392','EMI_-0.02880476453071618','EMI_-0.0007890225289003329','EMI_0.09630845948690948','Loan Amount_-0.1395277728465895','Loan Amount_0.06770667473690144','Loan Amount_0.07420114820122564','Loan Amount_0.1275794007605065','Tenure_-0.04662229017942052','Tenure_0.135602511064546','No_of_adv_EMI_-0.009504717159535709','No_of_adv_EMI_0.09060713127320336','Rate_of_interest_-0.07496472192459401','Rate_of_interest_-0.05066882693021101','Rate_of_interest_0.08283613016745002','Rate_of_interest_0.08362052533588261','Customer_age_ln_taken_-0.2321446559895609','Customer_age_ln_taken_-0.1199394823627936','Customer_age_ln_taken_0.150577032084122','Customer_age_ln_taken_0.3537724451758127','No_of_loans_-0.06929027459199408','No_of_loans_0.01329016868114212','No_of_loans_0.06903902702373141','No of secured loans_0.02712016444232146','No of secured loans_0.03335534453738747','No_of_unsecured_loans_-0.001442499592240792','No_of_unsecured_loans_0.1528126652237951','Min_amt_sanc_-0.1132248423693573','Min_amt_sanc_0.04771310010720148','Min_amt_sanc_0.08600392248638013','Min_amt_sanc_0.1195470256915129','N_time_30dpd_last6m_1.288083810799486','N_time_60dpd_last6m_1.328746050835782','N_time_90dpd_last3m_1.322251040827673','Age_-0.2299749873451578','Age_-0.1059038923019072','Age_0.1542023151831859','Age_0.3474719468108172']\n\nprint(len(slct_cat_woe))\nprint(len(slct_cat_cont_woe))\nprint(len(slct_cont_msb_cat_woe))\nprint(len(slct_woe_dummy))\n\ny = train_data['Target variable']\ny = y.loc[:,~y.columns.duplicated()]\nX = train_data[slct_woe_dummy]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Variance Inflation Factor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif[\"features\"] = X.columns\nvif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif.to_csv(\"VIF_dummy_variable.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Test Split (70% - Train,30% - Test)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nmax_depth = [10]\nmax_feature = ['auto']\ncriterion=[\"entropy\", \"gini\"]\n\nparam = {'max_depth':max_depth, \n         'max_features':max_feature, \n         'criterion': criterion}\ngrid = GridSearchCV(DecisionTreeClassifier(), \n                                param_grid = param, \n                                verbose=False, \n                                cv=StratifiedKFold(n_splits=20, random_state=15, shuffle=True),\n                                n_jobs = -1)\ngrid.fit(X_train,y_train)\nprint( grid.best_params_)\nprint (grid.best_score_)\nprint (grid.best_estimator_)\ndectree_grid = rf_grid = grid.best_estimator_\ndectree_grid.score(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's look at the feature importance from decision tree grid","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## feature importance\nfeature_importances = pd.DataFrame(dectree_grid.feature_importances_,\n                                   index = X.columns,\n                                    columns=['importance'])\npd.DataFrame(feature_importances.sort_values(by='importance', ascending=False)).to_excel(\"Decision_tree_feature_importance.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nclf = DecisionTreeClassifier(max_depth=10,min_samples_split=50,min_samples_leaf=10,max_features=10,max_leaf_nodes=30,random_state=100)\n\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_train)\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, precision_score\n# printing confision matrix\nprint('Confusion Matrix:',pd.DataFrame(confusion_matrix(y_train,y_pred),\\\n            columns=[\"Predicted Not-Delinquent\", \"Predicted Delinquent\"],\\\n            index=[\"Not-Delinquent\",\"Delinquent\"] ))\nprint('Accuracy Score:', accuracy_score(y_train, y_pred))\nprint('Recall Score:', recall_score(y_train, y_pred))\nprint('Precission Score:', precision_score(y_train, y_pred))\nprint('Classofication Report:',classification_report(y_train, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classifier(RF)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.ensemble import RandomForestClassifier\nn_estimators = [200];\nmax_depth = [10];\ncriterions = ['gini', 'entropy'];\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\n\nparameters = {'n_estimators':n_estimators,\n              'max_depth':max_depth,\n              'criterion': criterions\n              \n        }\ngrid = GridSearchCV(estimator=RandomForestClassifier(max_features='auto'),\n                                 param_grid=parameters,\n                                 cv=cv,\n                                 n_jobs = -1)\ngrid.fit(X_train,y_train)\nprint (grid.best_score_)\nprint (grid.best_params_)\nprint (grid.best_estimator_)\nrf_grid = grid.best_estimator_\nrf_grid.score(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## feature importance\nfeature_importances = pd.DataFrame(rf_grid.feature_importances_,\n                                   index = X.columns,\n                                    columns=['importance'])\npd.DataFrame(feature_importances.sort_values(by='importance', ascending=False)).to_excel(\"Random_forest_feature_importance.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(n_estimators=200,max_depth=10,min_samples_split=50,min_samples_leaf=10,max_features=10,max_leaf_nodes=30, bootstrap=True\n                             ,n_jobs=-1,random_state=100)\n\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_train)\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, precision_score\n# printing confusion matrix\nprint('Confusion Matrix:',pd.DataFrame(confusion_matrix(y_train,y_pred),\\\n            columns=[\"Predicted Not-Delinquent\", \"Predicted Delinquent\"],\\\n            index=[\"Not-Delinquent\",\"Delinquent\"] ))\nprint('Accuracy Score:', accuracy_score(y_train, y_pred))\nprint('Recall Score:', recall_score(y_train, y_pred))\nprint('Precission Score:', precision_score(y_train, y_pred))\nprint('Classofication Report:',classification_report(y_train, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = clf.predict(X_test)\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, precision_score\n# printing confision matrix\nprint('Confusion Matrix:',pd.DataFrame(confusion_matrix(y_test,y_pred),\n            columns=[\"Predicted Not-Delinquent\", \"Predicted Delinquent\"],\n            index=[\"Not-Delinquent\",\"Delinquent\"] ))\nprint('Accuracy Score:', accuracy_score(y_test, y_pred))\nprint('Recall Score:', recall_score(y_test, y_pred))\nprint('Precission Score:', precision_score(y_test, y_pred))\nprint('Classofication Report:',classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gain Chart and Lift charts(RF)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\ny_pred = clf.predict(X_train)\ny_train_score = clf.predict_proba(X_train)\nrandom_forest_model_accuracy = metrics.accuracy_score(y_train,y_pred)\n                \nprint(\"====== Classification Metrics - Development ======\")\nprint(\" Accuracy : \"  + str(metrics.accuracy_score(y_train,y_pred)))\nprint(\" Recall : \"  + str(metrics.recall_score(y_train,y_pred)))\nprint(\" Precision : \"  + str(metrics.precision_score(y_train,y_pred)))\nprint(\" F1_Score : \"  + str(metrics.f1_score(y_train,y_pred)))\nprint(\" Confusion_metrics : \"  + str(metrics.confusion_matrix(y_train,y_pred)))\nprint(\" \")\n\ny_train_score_df = pd.DataFrame(y_train_score, index=range(y_train_score.shape[0]),columns=range(y_train_score.shape[1]))\ny_train_score_df['Actual'] = pd.Series(y_train.values[:,0], index=y_train_score_df.index)\ny_train_score_df['Predicted'] = pd.Series(y_pred, index=y_train_score_df.index)\ny_train_score_df['Decile'] = pd.qcut(y_train_score_df[1],10,duplicates='drop')\n\nlift_tbl = pd.DataFrame([y_train_score_df.groupby('Decile')[1].min(),\n                                                 y_train_score_df.groupby('Decile')[1].max(),\n                                                 y_train_score_df[(y_train_score_df['Actual'] == 1)].groupby('Decile')[1].count(),\n                                                 y_train_score_df[(y_train_score_df['Actual'] == 0)].groupby('Decile')[1].count(),\n                                                 y_train_score_df.groupby('Decile')[1].count()]).T\nlift_tbl.columns = [\"MIN\",\"MAX\",\"Event\",\"Non-Event\",\"TOTAL\"]\nlift_tbl = lift_tbl.sort_values(\"MIN\", ascending=False)\nlift_tbl = lift_tbl.reset_index()\n\nlist_vol_pct=[]\nlist_event_pct=[]\n\nfor i in range(len(lift_tbl.Event)):\n    list_vol_pct.append(lift_tbl['TOTAL'][i]/lift_tbl['TOTAL'].sum())\n    list_event_pct.append(lift_tbl['Event'][i]/lift_tbl['TOTAL'][i])\n\nlift_tbl = pd.concat([lift_tbl,pd.Series(list_vol_pct),pd.Series(list_event_pct)],axis=1)\n\n\nlift_tbl = lift_tbl[[\"Decile\",\"MIN\",\"MAX\",\"Event\",\"Non-Event\",\"TOTAL\",0,1]]        \nlift_tbl = lift_tbl.rename(columns={lift_tbl.columns[len(lift_tbl.keys())-2]: \"Volume(%)\"})\nlift_tbl = lift_tbl.rename(columns={lift_tbl.columns[len(lift_tbl.keys())-1]: \"Event(%)\"})\n\nlift_tbl[\"Cumm_Event\"] = lift_tbl[\"Event\"].cumsum()\nlift_tbl[\"Cumm_Event_Pct\"] = lift_tbl[\"Cumm_Event\"] / lift_tbl[\"Event\"].sum()\n#lift_tbl\nlift_tbl.to_excel(\"TVS_Loan_data_Lift_Chart_optimized_rf_20August2020.xlsx\", index = None, header=True)\nlift_tbl","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# logistic regression ------\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(penalty='l1', C=0.9, solver='saga', n_jobs=-1)\nlr.fit(X_train, y_train)\nprint(lr.coef_)\nprint(lr.intercept_)\n\n# predicted proability\ntrain_pred = lr.predict_proba(X_train)[:,1]\ntest_pred = lr.predict_proba(X_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install git+git://github.com/shichenxie/scorecardpy.git","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Traditional Credit Scoring Using Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import scorecardpy as sc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Experimental purpose(using train_data_bkp data)\n### breaking input data into train and test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = sc.split_df(train_data_bkp,target_nm).values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Calculating IV and WoE\n## WOE binning (Fine & Coarse Classing)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = sc.woebin(train_data_bkp, y= target_nm)\nsc.woebin_plot(bins)\n\n#Writing BINS statitics on csv file\ntst_df = pd.DataFrame()\nfor x in bins:\n    tmp_df = pd.DataFrame()\n    for y in bins[x]:\n        tmp_df = pd.concat([tmp_df,pd.DataFrame(bins[x][y])], axis=1)\n    #print(tmp_df)            \n    tst_df = tst_df.append(tmp_df)\n    tst_df.reindex(tst_df.index.tolist()).reset_index()\n    \ntst_df.to_csv(\"WOE_ALL_by_package_scoremodel.csv\", header=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Converting train and test into woe values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_woe = sc.woebin_ply(train, bins)\ntest_woe = sc.woebin_ply(test, bins)\ny_train = train_woe.loc[:,target_nm]\nX_train = train_woe.loc[:,train_woe.columns != target_nm]\ny_test = test_woe.loc[:,target_nm]\nX_test = test_woe.loc[:,train_woe.columns != target_nm]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## (Modelling) Logistic regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# logistic regression ------\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(penalty='l1', C=0.9, solver='saga', n_jobs=-1)\nlr.fit(X_train, y_train)\nprint(lr.coef_)\nprint(lr.intercept_)\n\n# predicted proability of LR\ntrain_pred = lr.predict_proba(X_train)[:,1]\ntest_pred = lr.predict_proba(X_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Performance \n* 1. (KolmogorovSmirnov(KS) Statistic \n* 2. Receiver Operating Characteristic Curve(ROC)/Area Under Receiver Operating Characteristic (AUC) Curve \n* 3. Gini Coefficient or Accuracy Ratio (AR)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# performance ks & roc ------\ntrain_perf = sc.perf_eva(y_train, train_pred, title = \"train\")\ntest_perf = sc.perf_eva(y_test, test_pred, title = \"test\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Measures of Model Discrimination for Train data: ',train_perf)\nprint('Measures of Model Discrimination for Test data: ',test_perf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Credit Risk Scoring for train and test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"card = sc.scorecard(bins, lr, X_train.columns)\n\n# credit score\ntrain_score = sc.scorecard_ply(train, card, print_step=0)\ntest_score = sc.scorecard_ply(test, card, print_step=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluating Model stability (Population Stability Index (PSI))","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mimg\n\n# credit score\ntrain_score = sc.scorecard_ply(train, card, print_step=0)\ntest_score = sc.scorecard_ply(test, card, print_step=0)\n\n# psi\npsi = sc.perf_psi(\n  score = {'train':train_score, 'test':test_score},\n  label = {'train':y_train, 'test':y_test}\n)\nprint(psi)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_absolute_error, accuracy_score\nlog =LogisticRegression()\nlog.fit(X_train,y_train)\n\n#model on train using all the independent values in df\ny_pred = log.predict(X_train)\nlog_score= accuracy_score(y_train,y_pred)\nprint('Accuracy score on train set using Logistic Regression :',log_score)\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n# printing confision matrix\nprint('Confusion Matrix:',pd.DataFrame(confusion_matrix(y_train,y_pred),\n            columns=[\"Predicted Not-Delinquent\", \"Predicted Delinquent\"],\n            index=[\"Not-Delinquent\",\"Delinquent\"] ))\nprint('Recall Score:', recall_score(y_train, y_pred))\nprint('Precission Score:', precision_score(y_train, y_pred))\nprint('F1_sccore on train set :',f1_score(y_train, y_pred))\nprint('Classofication Report:',classification_report(y_train, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## call on the model object\nlogreg = LogisticRegression(solver='liblinear',\n                            penalty= 'l1',random_state = 100                           \n                            )\n## fit the model with \"train_x\" and \"train_y\"\nlogreg.fit(X_train,y_train)\n## Once the model is trained we want to find out how well the model is performing, so we test the model. \n## we use \"X_test\" portion of the data(this data was not used to fit the model) to predict model outcome. ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = logreg.predict(X_train)\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n# printing confision matrix\nprint('Confusion Matrix:',pd.DataFrame(confusion_matrix(y_train,y_pred),\n            columns=[\"Predicted Not-Delinquent\", \"Predicted Delinquent\"],\n            index=[\"Not-Delinquent\",\"Delinquent\"] ))\nprint('Accuracy Score:', accuracy_score(y_train, y_pred))\nprint('Recall Score:', recall_score(y_train, y_pred))\nprint('Precission Score:', precision_score(y_train, y_pred))\nprint('F1_sccore on train set :',f1_score(y_train, y_pred))\nprint('Classofication Report:',classification_report(y_train, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = logreg.predict(X_test)\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, precision_score\n# printing confision matrix\nprint('Confusion Matrix:',pd.DataFrame(confusion_matrix(y_test,y_pred),\n            columns=[\"Predicted Not-Delinquent\", \"Predicted Delinquent\"],\n            index=[\"Not-Delinquent\",\"Delinquent\"] ))\nprint('Accuracy Score:', accuracy_score(y_test, y_pred))\nprint('Recall Score:', recall_score(y_test, y_pred))\nprint('Precission Score:', precision_score(y_test, y_pred))\nprint('Classofication Report:',classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## AUC & ROC Curve","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n#plt.style.use('seaborn-pastel')\ny_score = logreg.decision_function(X_test)\n\nFPR, TPR, _ = roc_curve(y_test, y_score)\nROC_AUC = auc(FPR, TPR)\nprint (ROC_AUC)\n\nplt.figure(figsize =[11,9])\nplt.plot(FPR, TPR, label= 'ROC curve(area = %0.2f)'%ROC_AUC, linewidth= 4)\nplt.plot([0,1],[0,1], 'k--', linewidth = 4)\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.05])\nplt.xlabel('False Positive Rate', fontsize = 18)\nplt.ylabel('True Positive Rate', fontsize = 18)\nplt.title('ROC for Delinquent Customer', fontsize= 18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Precision Recall_curve","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\n\ny_score = logreg.decision_function(X_test)\n\nprecision, recall, _ = precision_recall_curve(y_test, y_score)\nPR_AUC = auc(recall, precision)\n\nplt.figure(figsize=[11,9])\nplt.plot(recall, precision, label='PR curve (area = %0.2f)' % PR_AUC, linewidth=4)\nplt.xlabel('Recall', fontsize=18)\nplt.ylabel('Precision', fontsize=18)\nplt.title('Precision Recall Curve for Titanic survivors', fontsize=18)\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Grid Search on Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, StratifiedKFold\n## C_vals is the alpla value of lasso and ridge regression(as alpha increases the model complexity decreases,)\n## remember effective alpha scores are 0<alpha<infinity \nC_vals = [0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16,16.5,17,17.5,18]\n## Choosing penalties(Lasso(l1) or Ridge(l2))\npenalties = ['l1','l2']\n## Choose a cross validation strategy. \ncv = StratifiedShuffleSplit(n_splits = 10, test_size = .25)\n\n## setting param for param_grid in GridSearchCV. \nparam = {'penalty': penalties, 'C': C_vals}\n\nlogreg = LogisticRegression(solver='liblinear')\n## Calling on GridSearchCV object. \ngrid = GridSearchCV(estimator=LogisticRegression(), \n                           param_grid = param,\n                           scoring = 'accuracy',\n                            n_jobs =-1,\n                           cv = cv\n                          )\n## Fitting the model\ngrid.fit(X_train,y_train)\n## Getting the best of everything. \nprint (grid.best_score_)\nprint (grid.best_params_)\nprint(grid.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using the best parameters from the grid-search.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### Using the best parameters from the grid-search.\nlogreg_grid = grid.best_estimator_\nlogreg_grid.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge,RidgeCV,ElasticNet,Lasso,LassoCV,LassoLarsCV,LassoLarsIC\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport pickle\nimport pandas as pd\n# LassoLinear\nclf=Lasso(alpha=0.00001,normalize=True)\nclf.fit(X_train,y_train)\nclf.score(X_train,y_train)\n#clf.coef_\ncoef=pd.Series(clf.coef_,index=X_train.columns)\nprint(\"Lasso linear\")\nprint(coef)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hyper Tuning : Randomized Search CV (Random Forest)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\n#Number of tress in Random Forest\nn_estimators = [int(x) for x in np.linspace(start = 200,stop = 1000, num = 5)]\n\n#Number of features to consider while splitting\nmax_features = ['auto', 'sqrt']\n\n#Maximum number of levels in the tree\nmax_depth = [int(x) for x in np.linspace(start = 7, stop = 10, num = 4)]\n\n#Minimum # of samples required to split the node\nmin_samples_split = [10,15]\n\n#Minimum # of samples required at each leaf node\nmin_samples_leaf = [3,5]\n\n#Method of selecting samples from training each tree\nbootstrap = [True, False]\n\nfrom sklearn.metrics import accuracy_score,make_scorer,precision_score,recall_score,roc_auc_score,f1_score\nscoring={'AUC' : make_scorer(roc_auc_score) , \n         'Accuracy':make_scorer(accuracy_score), \n         'Recall':make_scorer(recall_score), \n         'Precision':make_scorer(precision_score),\n         'F1 Score':make_scorer(f1_score)}\n\n#Create the random grid:\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf ,\n               'bootstrap': bootstrap                \n              }\nfrom sklearn.ensemble import RandomForestClassifier\nRFcl = RandomForestClassifier(random_state = 0, n_jobs = -1) \n\nfrom sklearn.model_selection import RandomizedSearchCV\nCV_rfc = RandomizedSearchCV(estimator=RFcl, param_distributions =random_grid, n_jobs = -1, cv= 10,scoring=scoring,refit='Recall',return_train_score=True,n_iter=10)\nCV_rfc.fit(X_train, y_train)\nprint(CV_rfc.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hyper Tuning : Grid Search CV (Random Forest)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_grid_search_result(res):\n    global df_gs_result\n    gs_results=res\n    \n    gs_model=gs_results['params']\n    \n    # Grid Search : AUC Metrics\n    gs_mean_test_AUC=pd.Series(gs_results['mean_test_AUC'])\n    gs_std_test_AUC=pd.Series(gs_results['std_test_AUC'])\n    gs_rank_test_AUC=pd.Series(gs_results['rank_test_AUC'])\n    \n    # Grid Search : Accuracy Metrics\n    gs_mean_test_Accuracy=pd.Series(gs_results['mean_test_Accuracy'])\n    gs_std_test_Accuracy=pd.Series(gs_results['std_test_Accuracy'])\n    gs_rank_test_Accuracy=pd.Series(gs_results['rank_test_Accuracy'])\n    \n    # Grid Search : Recall Metrics\n    gs_mean_test_Recall=pd.Series(gs_results['mean_test_Recall'])\n    gs_std_test_Recall=pd.Series(gs_results['std_test_Recall'])\n    gs_rank_test_Recall=pd.Series(gs_results['rank_test_Recall'])\n\n    # Grid Search : Precision Metrics\n    gs_mean_test_Precision=pd.Series(gs_results['mean_test_Precision'])\n    gs_std_test_Precision=pd.Series(gs_results['std_test_Precision'])\n    gs_rank_test_Precision=pd.Series(gs_results['rank_test_Precision'])\n    \n    # Grid Search : F1-Score Metrics\n    gs_mean_test_F1_Score=pd.Series(gs_results['mean_test_F1 Score'])\n    gs_std_test_F1_Score=pd.Series(gs_results['std_test_F1 Score'])\n    gs_rank_test_F1_Score=pd.Series(gs_results['rank_test_F1 Score'])   \n\n    \n    gs_model_split=str(gs_model).replace(\"[{\",\"\").replace(\"}]\",\"\").split('}, {')\n    df_gs_result=pd.DataFrame(gs_model_split,index=None,columns=['Model_attributes'])\n    df_gs_result=pd.concat([df_gs_result,gs_mean_test_AUC,gs_std_test_AUC,gs_rank_test_AUC,gs_mean_test_Accuracy,gs_std_test_Accuracy,gs_rank_test_Accuracy,gs_mean_test_Recall,gs_std_test_Recall,gs_rank_test_Recall,gs_mean_test_Precision,gs_std_test_Precision,gs_rank_test_Precision,gs_mean_test_F1_Score,gs_std_test_F1_Score,gs_rank_test_F1_Score],axis=1)\n    \n    df_gs_result.columns=['Model_attributes','mean_test_AUC','std_test_AUC','rank_test_AUC','mean_test_Accuracy','std_test_Accuracy','rank_test_Accuracy','mean_test_Recall','std_test_Recall','rank_test_Recall','mean_test_Precision','std_test_Precision','rank_test_Precision','mean_test_F1_Score','std_test_F1_Score','rank_test_F1_Score']  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\n#Number of tress in Random Forest\nn_estimators = [int(x) for x in np.linspace(start = 200,stop = 1000, num = 5)]\n\n#Number of features to consider while splitting\nmax_features = ['auto', 'sqrt']\n\n#Maximum number of levels in the tree\nmax_depth = [int(x) for x in np.linspace(start = 7, stop = 10, num = 4)]\n\n#Minimum # of samples required to split the node\nmin_samples_split = [10,15]\n\n#Minimum # of samples required at each leaf node\nmin_samples_leaf = [3,5]\n\n#Method of selecting samples from training each tree\nbootstrap = [True, False]\n\nfrom sklearn.metrics import accuracy_score,make_scorer,precision_score,recall_score,roc_auc_score,f1_score\nscoring={'AUC' : make_scorer(roc_auc_score) , \n         'Accuracy':make_scorer(accuracy_score), \n         'Recall':make_scorer(recall_score), \n         'Precision':make_scorer(precision_score),\n         'F1 Score':make_scorer(f1_score)}\n\n#Create the grid:\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf ,\n               'bootstrap': bootstrap                \n              }\n   \nfrom sklearn.ensemble import RandomForestClassifier\nRFcl = RandomForestClassifier(random_state = 0, n_jobs = -1) \n\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nGS_rfc = GridSearchCV(estimator=RFcl, param_grid=random_grid, cv= 10, n_jobs = -1,scoring=scoring,refit='Recall',return_train_score=True)\nGS_rfc.fit(X_train, y_train)\nprint(GS_rfc.best_score_)\n#print(GS_rfc.cv_results_)\n    \nformat_grid_search_result(GS_rfc.cv_results_)\ndf_gs_result.to_excel('Random_forest_Grid_Search_19August.xlsx')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBOOST Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn import metrics\n\nxgb_model=XGBClassifier(learning_rate=0.1,            \n                        n_estimators=200,\n                        max_depth=10,\n                        objective='binary:logistic',\n                        nthread=4,scale_pos_weight=9.8,seed=100).fit(X_train,y_train)\n\ny_pred    = xgb_model.predict(X_train)  \naccuracy  = str(metrics.accuracy_score(y_train,y_pred))\nrecall    = str(metrics.recall_score(y_train,y_pred))\nprecision = str(metrics.precision_score(y_train,y_pred))\nf1_score  = str(metrics.f1_score(y_train,y_pred))\nconf_mat  = str(metrics.confusion_matrix(y_train,y_pred))\n\nprint(\"====== XGBOOST Development Metrics ======\")\nprint(\" Accuracy : \"  + str(accuracy))\nprint(\" Recall : \"  + str(recall))\nprint(\" Precision : \"  + str(precision))\nprint(\" F1_Score : \"  + str(f1_score))\nprint(\" Confusion_metrics : \"  + str(conf_mat))\nprint(\" \")\n\ny_pred = xgb_model.predict(X_test)\ny_test_score = xgb_model.predict_proba(X_test)\n\nprint(\"####### XGBOOST Validation Metrics ########\")\nprint(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\nprint(\"Recall:\", metrics.recall_score(y_test, y_pred))\nprint(\"Precision:\", metrics.precision_score(y_test, y_pred))\nprint(\"f1 score:\", metrics.f1_score(y_test, y_pred))\nprint(\"Confusion Matrix:\", metrics.confusion_matrix(y_test, y_pred))\n\n\n''' Variance Inflation Factor (VIF) Calculation '''\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nX=X_train\nvif = pd.DataFrame()\nvif[\"features\"] = X.columns\nvif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nprint(vif)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gain Chart and Lift Chart (xgboost)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\ny_pred = xgb_model.predict(X_train)\ny_train_score = xgb_model.predict_proba(X_train)\nxgboost_model_accuracy = metrics.accuracy_score(y_train,y_pred)\n                \nprint(\"====== Classification Metrics - Development ======\")\nprint(\" Accuracy : \"  + str(metrics.accuracy_score(y_train,y_pred)))\nprint(\" Recall : \"  + str(metrics.recall_score(y_train,y_pred)))\nprint(\" Precision : \"  + str(metrics.precision_score(y_train,y_pred)))\nprint(\" F1_Score : \"  + str(metrics.f1_score(y_train,y_pred)))\nprint(\" Confusion_metrics : \"  + str(metrics.confusion_matrix(y_train,y_pred)))\nprint(\" \")\n\ny_train_score_df = pd.DataFrame(y_train_score, index=range(y_train_score.shape[0]),columns=range(y_train_score.shape[1]))\ny_train_score_df['Actual'] = pd.Series(y_train.values, index=y_train_score_df.index)\ny_train_score_df['Predicted'] = pd.Series(y_pred, index=y_train_score_df.index)\ny_train_score_df['Decile'] = pd.qcut(y_train_score_df[1],10,duplicates='drop')\n\nlift_tbl = pd.DataFrame([y_train_score_df.groupby('Decile')[1].min(),\n                                                 y_train_score_df.groupby('Decile')[1].max(),\n                                                 y_train_score_df[(y_train_score_df['Actual'] == 1)].groupby('Decile')[1].count(),\n                                                 y_train_score_df[(y_train_score_df['Actual'] == 0)].groupby('Decile')[1].count(),\n                                                 y_train_score_df.groupby('Decile')[1].count()]).T\nlift_tbl.columns = [\"MIN\",\"MAX\",\"Event\",\"Non-Event\",\"TOTAL\"]\nlift_tbl = lift_tbl.sort_values(\"MIN\", ascending=False)\nlift_tbl = lift_tbl.reset_index()\n\nlist_vol_pct=[]\nlist_event_pct=[]\n\nfor i in range(len(lift_tbl.Event)):\n    list_vol_pct.append(lift_tbl['TOTAL'][i]/lift_tbl['TOTAL'].sum())\n    list_event_pct.append(lift_tbl['Event'][i]/lift_tbl['TOTAL'][i])\n\nlift_tbl = pd.concat([lift_tbl,pd.Series(list_vol_pct),pd.Series(list_event_pct)],axis=1)\n\n\nlift_tbl = lift_tbl[[\"Decile\",\"MIN\",\"MAX\",\"Event\",\"Non-Event\",\"TOTAL\",0,1]]        \nlift_tbl = lift_tbl.rename(columns={lift_tbl.columns[len(lift_tbl.keys())-2]: \"Volume(%)\"})\nlift_tbl = lift_tbl.rename(columns={lift_tbl.columns[len(lift_tbl.keys())-1]: \"Event(%)\"})\n\nlift_tbl[\"Cumm_Event\"] = lift_tbl[\"Event\"].cumsum()\nlift_tbl[\"Cumm_Event_Pct\"] = lift_tbl[\"Cumm_Event\"] / lift_tbl[\"Event\"].sum()\n#lift_tbl\nlift_tbl.to_excel(\"TVS_Loan_data_Validation_Lift_Chart_optimized_xgboost_20August2020.xlsx\", index = None, header=True)\nlift_tbl","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Long Short-Term Memory(LSTM) Model for Imbalanced Classification","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# standard neural network on an imbalanced classification dataset\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import roc_auc_score\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\n\n\n \n# define the neural network model\ndef define_model(n_input):\n    # define model\n    model = Sequential()\n    # define LSTM model\n    model.add(LSTM(5, input_shape=(2,1)))\n    # define first hidden layer and visible layer\n    model.add(Dense(50, input_dim=n_input, activation='relu', kernel_initializer='he_uniform'))\n    # define output layer\n    model.add(Dense(1, activation='sigmoid'))\n    # define loss and optimizer\n    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n    return model\n \n# define the model\nn_input = X_train.shape[1]\nmodel = define_model(n_input)\n# fit model\nmodel.fit(X_train, y_train, batch_size=10, epochs=100, verbose=0)\n# make predictions on the test dataset\nyhat = model.predict(X_test)\n# evaluate the ROC AUC of the predictions\nscore = roc_auc_score(y_test, yhat)\nprint('ROC AUC: %.3f' % score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Perfomance Evaluation(Neural Network)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.datasets import make_circles\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\n\nn_input = X_train.shape[1]\nmodel = define_model(n_input)\n# fit model\nmodel.fit(X_train, y_train, epochs=100, verbose=0)\n\n# predict probabilities for train set\nyhat_probs = model.predict(X_train, verbose=0)\n# predict crisp classes for train set\nyhat_classes = model.predict_classes(X_train, verbose=0)\n# reduce to 1d array\nyhat_probs = yhat_probs[:, 0]\nyhat_classes = yhat_classes[:, 0]\n \nprint(\"####### LSTM Developement Evaluation Metrics ########\")\n# accuracy: (tp + tn) / (p + n)\naccuracy = accuracy_score(y_train, yhat_classes)\nprint('Accuracy: %f' % accuracy)\n# precision tp / (tp + fp)\nprecision = precision_score(y_train, yhat_classes)\nprint('Precision: %f' % precision)\n# recall: tp / (tp + fn)\nrecall = recall_score(y_train, yhat_classes)\nprint('Recall: %f' % recall)\n# f1: 2 tp / (2 tp + fp + fn)\nf1 = f1_score(y_train, yhat_classes)\nprint('F1 score: %f' % f1)\n \n# kappa\nkappa = cohen_kappa_score(y_train, yhat_classes)\nprint('Cohens kappa: %f' % kappa)\n# ROC AUC\nauc = roc_auc_score(y_train, yhat_probs)\nprint('ROC AUC: %f' % auc)\n# confusion matrix\nmatrix = confusion_matrix(y_train, yhat_classes)\nprint(matrix)\n\n# predict probabilities for test set\nyhat_probs = model.predict(X_test, verbose=0)\n# predict crisp classes for test set\nyhat_classes = model.predict_classes(X_test, verbose=0)\n# reduce to 1d array\nyhat_probs = yhat_probs[:, 0]\nyhat_classes = yhat_classes[:, 0]\n \nprint(\"####### LSTM Validation Evaluation Metrics ########\")\n# accuracy: (tp + tn) / (p + n)\naccuracy = accuracy_score(y_test, yhat_classes)\nprint('Accuracy: %f' % accuracy)\n# precision tp / (tp + fp)\nprecision = precision_score(y_test, yhat_classes)\nprint('Precision: %f' % precision)\n# recall: tp / (tp + fn)\nrecall = recall_score(y_test, yhat_classes)\nprint('Recall: %f' % recall)\n# f1: 2 tp / (2 tp + fp + fn)\nf1 = f1_score(y_test, yhat_classes)\nprint('F1 score: %f' % f1)\n \n# kappa\nkappa = cohen_kappa_score(y_test, yhat_classes)\nprint('Cohens kappa: %f' % kappa)\n# ROC AUC\nauc = roc_auc_score(y_test, yhat_probs)\nprint('ROC AUC: %f' % auc)\n# confusion matrix\nmatrix = confusion_matrix(y_test, yhat_classes)\nprint(matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gain Chart and Lift Chart (LSTM [Neural Net])","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\nyhat_classes = model.predict_classes(X_train, verbose=0)\n# predict probabilities for train set\ny_train_score = model.predict(X_train, verbose=0)\n\n# reduce to 1d array\nyhat_probs = y_train_score[:, 0]\ny_pred = yhat_classes[:, 0]\n\ny_train_score_df = pd.DataFrame(y_train_score, index=range(y_train_score.shape[0]),columns=range(y_train_score.shape[1]))\ny_train_score_df['Actual'] = pd.Series(y_train.values, index=y_train_score_df.index)\ny_train_score_df['Predicted'] = pd.Series(y_pred, index=y_train_score_df.index)\ny_train_score_df['Decile'] = pd.qcut(y_train_score_df[[0]].values[:,0],10,duplicates='drop')\n\nlift_tbl = pd.DataFrame([y_train_score_df.groupby('Decile')[0].min(),\n                                                 y_train_score_df.groupby('Decile')[0].max(),\n                                                 y_train_score_df[(y_train_score_df['Actual'] == 1)].groupby('Decile')[0].count(),\n                                                 y_train_score_df[(y_train_score_df['Actual'] == 0)].groupby('Decile')[0].count(),\n                                                 y_train_score_df.groupby('Decile')[0].count()]).T\nlift_tbl.columns = [\"MIN\",\"MAX\",\"Event\",\"Non-Event\",\"TOTAL\"]\nlift_tbl = lift_tbl.sort_values(\"MIN\", ascending=False)\nlift_tbl = lift_tbl.reset_index()\n\nlist_vol_pct=[]\nlist_event_pct=[]\n\nfor i in range(len(lift_tbl.Event)):\n    list_vol_pct.append(lift_tbl['TOTAL'][i]/lift_tbl['TOTAL'].sum())\n    list_event_pct.append(lift_tbl['Event'][i]/lift_tbl['TOTAL'][i])\n\nlift_tbl = pd.concat([lift_tbl,pd.Series(list_vol_pct),pd.Series(list_event_pct)],axis=1)\n\n\nlift_tbl = lift_tbl[[\"Decile\",\"MIN\",\"MAX\",\"Event\",\"Non-Event\",\"TOTAL\",0,1]]        \nlift_tbl = lift_tbl.rename(columns={lift_tbl.columns[len(lift_tbl.keys())-2]: \"Volume(%)\"})\nlift_tbl = lift_tbl.rename(columns={lift_tbl.columns[len(lift_tbl.keys())-1]: \"Event(%)\"})\n\nlift_tbl[\"Cumm_Event\"] = lift_tbl[\"Event\"].cumsum()\nlift_tbl[\"Cumm_Event_Pct\"] = lift_tbl[\"Cumm_Event\"] / lift_tbl[\"Event\"].sum()\n#lift_tbl\nlift_tbl.to_excel(\"TVS_Loan_data_Validation_Lift_Chart_optimized_lstm_24August2020.xlsx\", index = None, header=True)\nlift_tbl","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# decision tree  on imbalanced dataset with SMOTE oversampling and random undersampling\nfrom numpy import mean\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n\n#model = define_model(n_input)\nmodel = DecisionTreeClassifier(max_depth=10,min_samples_split=50,min_samples_leaf=10,max_features=10,max_leaf_nodes=30,random_state=100)\nover = SMOTE()\n#under = RandomUnderSampler(sampling_strategy=0.5)\nsteps = [('over', over), ('model', model)]\npipeline = Pipeline(steps=steps)\n# evaluate pipeline\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nscores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\nprint('Mean ROC AUC: %.3f' % mean(scores))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}