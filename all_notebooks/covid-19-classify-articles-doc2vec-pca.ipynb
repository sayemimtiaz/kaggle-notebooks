{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Objective\nNumber corona virus related research articles are increaseing rapially.Objective of this article is to help researcher for revelant articles.\n\nIn this notebook, I will attempt to present as way to categorize articles using word2vec and neural network. \n* create word2vec using\n\nUsed helper function from : https://www.kaggle.com/xhlulu/cord-19-eda-parse-json-and-generate-clean-csv and https://www.kaggle.com/luisblanche/cord-19-match-articles-to-tasks-w-doc2vec\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm.notebook import tqdm\nimport os\nimport json\nfrom pprint import pprint\nfrom copy import deepcopy\n\n# define global variable to print logs\nloglevel ='INFO'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"Helper Functions\nUnhide the cell below to find the definition of the following functions:\n\n+ format_name(author)\n+ format_affiliation(affiliation)\n+ format_authors(authors, with_affiliation=False)\n+ format_body(body_text)\n+ format_bib(bibs)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_name(author):\n    middle_name = \" \".join(author['middle'])\n    \n    if author['middle']:\n        return \" \".join([author['first'], middle_name, author['last']])\n    else:\n        return \" \".join([author['first'], author['last']])\n\n\ndef format_affiliation(affiliation):\n    text = []\n    location = affiliation.get('location')\n    if location:\n        text.extend(list(affiliation['location'].values()))\n    \n    institution = affiliation.get('institution')\n    if institution:\n        text = [institution] + text\n    return \", \".join(text)\n\ndef format_authors(authors, with_affiliation=False):\n    name_ls = []\n    \n    for author in authors:\n        name = format_name(author)\n        if with_affiliation:\n            affiliation = format_affiliation(author['affiliation'])\n            if affiliation:\n                name_ls.append(f\"{name} ({affiliation})\")\n            else:\n                name_ls.append(name)\n        else:\n            name_ls.append(name)\n    \n    return \", \".join(name_ls)\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    \n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    \n    return body\n\ndef format_bib(bibs):\n    if type(bibs) == dict:\n        bibs = list(bibs.values())\n    bibs = deepcopy(bibs)\n    formatted = []\n    \n    for bib in bibs:\n        bib['authors'] = format_authors(\n            bib['authors'], \n            with_affiliation=False\n        )\n        formatted_ls = [str(bib[k]) for k in ['title', 'authors', 'venue', 'year']]\n        formatted.append(\", \".join(formatted_ls))\n\n    return \"; \".join(formatted)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"+ get_filenames(dirname)\n+ generate_clean_df(all_files)"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"def get_filenames(dirname):\n    filenames = os.listdir(dirname)\n    raw_files = []\n\n    for filename in tqdm(filenames):\n        filename = dirname + filename\n        file = json.load(open(filename, 'rb'))\n        raw_files.append(file)\n    \n    return raw_files\n\ndef create_df(all_files,article_type):\n    cleaned_files = []\n    \n    for file in tqdm(all_files):\n        features = [\n            file['paper_id'],\n            file['metadata']['title'],\n            format_authors(file['metadata']['authors']),\n            format_authors(file['metadata']['authors'], \n                           with_affiliation=True),\n            format_body(file['abstract']),\n            format_body(file['body_text']),\n            format_bib(file['bib_entries']),\n            file['metadata']['authors'],\n            file['bib_entries'],\n            article_type\n        ]\n\n        cleaned_files.append(features)\n\n    col_names = ['paper_id', 'title', 'authors',\n                 'affiliations', 'abstract', 'text', \n                 'bibliography','raw_authors','raw_bibliography','article_type']\n\n    clean_df = pd.DataFrame(cleaned_files, columns=col_names)\n    clean_df.head()\n    \n    return clean_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dirnames = [{'dir':'/kaggle/input/CORD-19-research-challenge/2020-03-13/biorxiv_medrxiv/biorxiv_medrxiv/','type':'biorxiv'},{'dir':'/kaggle/input/CORD-19-research-challenge/2020-03-13/pmc_custom_license/pmc_custom_license/','type':'PMC'},\n            {'dir':'/kaggle/input/CORD-19-research-challenge/2020-03-13/comm_use_subset/comm_use_subset/','type':'Comm'},{'dir':'/kaggle/input/CORD-19-research-challenge/2020-03-13/noncomm_use_subset/noncomm_use_subset/','type':'NonComm'}]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe = pd.DataFrame()\nfor item in dirnames:\n    print(item['dir'],item['type'])\n    dirname = item['dir']\n    types = item['type']\n    filenames = get_filenames(dirname)\n    # extract article type and article body text\n    df = create_df(filenames,types)[['article_type','text']]\n    dataframe= pd.concat((dataframe,df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using Doc2Vec and PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"# library to clean data \nimport re  \n  \n# Natural Language Tool Kit \nimport nltk  \n\n# to remove stopword \nfrom nltk.corpus import stopwords \n  \n# for Stemming propose  \nfrom nltk.stem.porter import PorterStemmer ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample https://www.geeksforgeeks.org/python-nlp-analysis-of-restaurant-reviews/\nloglevel ='INFO'\ndef clean_article_text(texts):\n    \"\"\"\n    clean review columns from row and reconstruct and return\n    \"\"\"\n    if loglevel == 'DEBUG':\n        print(f\"intital quote\\n {reviews}\")\n    # column : \"Review\" with only a-zA-Z char\n    text = re.sub('[^a-zA-Z]', ' ', texts[0])  \n    if loglevel == 'DEBUG':\n        print(f\"step1 quote\\n {text}\")\n      \n    # convert all cases to lower cases \n    text = text.lower()\n    if loglevel == 'DEBUG':\n        print(f\"step3 quote\\n {text}\")\n      \n    # split to array(default delimiter is \" \") \n    text = text.split()  \n    if loglevel == 'DEBUG':\n        print(f\"step4 quote\\n {review}\")\n      \n    # creating PorterStemmer object to \n    # take main stem of each word \n    ps = PorterStemmer()  \n      \n    # loop for stemming each word \n    # in string array at ith row   \n    # remove words thats are in stopwords file\n    text = [ps.stem(word) for word in text \n                if not word in set(stopwords.words('english'))]  \n    \n    if loglevel == 'DEBUG':\n        print(f\"step5 quote\\n {text}\")\n    # rejoin all string array elements \n    # to create back into a string \n    text = ' '.join(text)   \n    if loglevel == 'DEBUG':\n        print(f\"step6 quote\\n {text}\")\n\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndataframe.loc[:,'clean_text'] = np.apply_along_axis(clean_article_text,1,dataframe[['text']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim \nfrom gensim.models import Doc2Vec\nfrom sklearn.decomposition import PCA\n\n# create contains that can be used in Dc2Vec\n\nLabeledSentence1 = gensim.models.doc2vec.TaggedDocument\nall_content_train = []\nj=0\nfor em in dataframe['clean_text'].values:\n    all_content_train.append(LabeledSentence1(em,[j]))\n    j+=1\nprint(\"Number of texts processed: \", j)\n\n# create document vectors\nd2v_model = Doc2Vec(all_content_train, vector_size = 100, window = 10, min_count = 500, workers=7, dm = 1,alpha=0.025, min_alpha=0.001)\nd2v_model.train(all_content_train, total_examples=d2v_model.corpus_count, epochs=10, start_alpha=0.002, end_alpha=-0.016)\n\n# Now lets use  Kmean clustering\nkmeans_model = KMeans(n_clusters=10, init='k-means++', max_iter=200) \nX = kmeans_model.fit(d2v_model.docvecs.vectors_docs)\n# get reviews labels\nlabels=kmeans_model.labels_.tolist()\n#l = kmeans_model.fit_predict(d2v_model.docvecs.vectors_docs)\n# using PCA to project vectors to 2D for plotting\npca = PCA(n_components=2).fit(d2v_model.docvecs.vectors_docs)\ndatapoint = pca.transform(d2v_model.docvecs.vectors_docs)\n\n\n                      \nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.figure(figsize=(20,10))\n#label1 = ['#FFFF00', '#008000', '#0000FF', '#800080']\n#color = [label1[i] for i in labels]\nplt.scatter(datapoint[:, 0], datapoint[:, 1], c=labels)\n# show cluser centers on plot\ncentroids = kmeans_model.cluster_centers_\ncentroidpoint = pca.transform(centroids)\nplt.scatter(centroidpoint[:, 0], centroidpoint[:, 1], marker='^', s=150, c='#000000')\nplt.show()\n\n\n# add clusters labels to review so that we can sample reviews in a clusters\n\ndf_review.loc[:,'cluster'] = labels","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}