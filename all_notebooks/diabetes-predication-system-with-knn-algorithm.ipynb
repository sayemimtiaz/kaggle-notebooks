{"cells":[{"metadata":{},"cell_type":"markdown","source":"\nFirst I would like to approach this mini project as if I have a problem to solve, the project will be solved with adopting  K-nearest neighbour (KNN) predictive model , so obviously it is a classification model challenge. In this case I’ve chosen a dataset related to healthcare which is associated to a diabetes database from the National Institute of Diabetes and Digestive , I will try to test the data if a person is a diabetes or not .\nThe dataset was obtained from Kaggle which is a website offers a variety list of different datasets based on real live data and occurrences. Therefore, without further ado let’s start deciphering the dataset and create a predictive model using KNN with cross validation model. \n"},{"metadata":{},"cell_type":"markdown","source":"**Description of dataset : **\n\nI’ve obtained the subjected dataset from Kaggle , however the dataset was initially presented by the National Institute of Diabetes and Digestive and Kidney Diseases  .The dataset is consist of predictive variables and Outcome in which it describes if a person is a diabetes of not. The dataset represents a list of study from different patients that leads to classification of either diabetic or not. \nFor this coursework I will use these presented data and adopt a Knn algorithm to test some given data of patients and see if they are under either category diabetes or non-diabetic. Total number of studied list in this dataset related to diabetic and non-diabetic patient is 768 , which we will manipulate ,scrap and clean these data to use them in our KNN predictive model.  \nBefore we start working on our predictive model using  Knn algorithm , we need to know a bit about what is KNN algorithm .\n\n**KNN algorithm** is a supervised machine learning algorithm that deals with similarity .\nKNN stands for K-Nearest Neighbors. It’s basically a classification algorithm that will make a prediction of a class of a target variable based on a defined number of nearest neighbors. It will calculate distance from the instance you want to classify to every instance of the training dataset, and then classify your instance based on the majority classes of k nearest instances.\n"},{"metadata":{},"cell_type":"markdown","source":"**Distance between data points in Knn algorithm**\n\nFor this project the library by default will consider the Euclidean distance to measure the distance between two data points or vectors from the dataset.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfrom IPython.display import Image\nprint(\"**Euclidean Distance Formula**\")\nImage(filename=\"../input/euclidean-distance/euclidean distance.JPG\", width= 500, height=200)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reading and exploring the dataset**\n\nBelow we start by opening the subjected dataset using pandas syntax **csv_read()** which read the dataset and transform it to a structured tabular data for us to read. \n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# First let's start with calling all the dependencies for this project \nimport numpy as np \nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n%matplotlib inline\n\n\nlocation = '../input/diabetes/diabetes.csv'\nf = pd.read_csv(location)\ndata = pd.DataFrame(f)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Manipulating and Cleaning our dataset**\n\nIn this section , we will attempt to clean our dataset from al zeros and missing values such as NaN , and replace them with the mean of the designated columns. \nI ‘ve decided to use a specific number of columns to do the cleaning as these subjected columns which are mentioned as following **['Glucose','BloodPressure','SkinThickness','Insulin','BMI','Pedigree'] **, because they are the most important data with a visible impact which determine if a patient is diabetic or not . \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#cleaning the dataset  from missing values or zeros\n#zeros or missing values will be replaced by the mean of that particular column\n# this practice is the best practie to have a readable and consistent data values\ncols_clean = ['Glucose','BloodPressure','SkinThickness','Insulin','BMI','Pedigree']\n\n# with this function , i dealt with missing values and NaN values \nfor i in cols_clean:\n    data[i] = data[i].replace(0,np.NaN)\n    cols_mean = int(data[i].mean(skipna=True))\n    data[i] = data[i].replace(np.NaN, cols_mean)\ndata1 = data\ndata1.head().style.highlight_max(color=\"lightblue\").highlight_min(color=\"red\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's take a quick statistcal view of the data provided\nprint(data1.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Plotting the  dataset**\n\nThe diabetes updated dataset is ready for a basic plotting, in order to see how would our data looks like, also plotting at this stage will help me decide which column I will choose to run a K-nearest neighbour (KNN) experiment. \nFor plotting I’ve used **pairplot()** function with the help of Seaborn library , that will give me a range of graph plotting for each group of data presented in the dataset . \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"graph = ['Glucose','Insulin','BMI','Age','Outcome']\nsns.set()\nprint(sns.pairplot(data1[graph],hue='Outcome', diag_kind='kde'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's obvious we are dealing with a rich multideminsional dataset with many data points belong to the presented variables.\nTo make our life easier and for simplicity , we will select only a few variables to test our model. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for the purpose of simplicity and analysing the most relevent  data , we will select three features of the dataset\n# Glucose , Insulin and BMI\nq_cols = ['Glucose','Insulin','BMI','Outcome']\n\n# defining variables and features for the dataset for splitting \ndf = data1[q_cols]\nprint(df.head(2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Splitting the dataset into training and testing dataset**\n\nA particularly important part of machine learning modelling or preparing data for machine learning algorithms is splitting our dataset into training and testing datasets.\n\nMainly , datasets undergo a splitting process for the purpose of testing the model , the testing process will determine how accurate your machine learning algorithm in predicting every testing sets against training sets, and how that will take shape in the real world.\nThen , we take the presented data and compute the accuracy rate of the Machine learning algorithm.  Ideally , the higher the accuracy rate of your machine learning algorithm the better is your model in predicting presented sample data.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's split the data into training and testing datasets\nsplit = 0.75 # 75% train and 25% test dataset\ntotal_len = len(df)\nsplit_df = int(total_len*split)\ntrain, test = df.iloc[:split_df,0:4],df.iloc[split_df:,0:4] \ntrain_x = train[['Glucose','Insulin','BMI']]\ntrain_y = train['Outcome']\ntest_x = test[['Glucose','Insulin','BMI']]\ntest_y = test['Outcome']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to run a quick syntax to see if these data are split correctly "},{"metadata":{"trusted":true},"cell_type":"code","source":"a = len(train_x) \nb = len(test_x)\nprint(' Training data =',a,'\\n','Testing data =',b,'\\n','Total data length = ',a+b)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Knn algorithm dealing with similarity between the sample test data and training data. This similarity is determined by K values , These values are defined by the closest data to the sample data points in this case , we will use two distance measurement to get the closest distances between our test data and the training dataset . \nThe chosen distance measurement in this exercise is the Euclidean distance, However , I used a build-in library to run these operations on the model , the library I used was scikit-learn library. \n"},{"metadata":{},"cell_type":"markdown","source":"**KNN function**\n\nI wrote a function to populate the result of adopting KNN algorithm against the split data. This function will run the KNN algorithm K times and populate the result in a form of Lines plot . \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's test it using KNN  classifier with a loop to cover as much n-neightbors as possible \ndef knn(x_train, y_train, x_test, y_test,n):\n    n_range = range(1, n)\n    results = []\n    for n in n_range:\n        knn = KNeighborsClassifier(n_neighbors=n)\n        knn.fit(x_train, y_train)\n        #Predict the response for test dataset\n        predict_y = knn.predict(x_test)\n        accuracy = metrics.accuracy_score(y_test, predict_y)\n        #matrix = confusion_matrix(y_test,predict_y)\n        #seaborn_matrix = sns.heatmap(matrix, annot = True, cmap=\"Blues\",cbar=True)\n        results.append(accuracy)\n    return results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For this exercise , i will test and plot the model with K values from 1 up to 500 and see where are we with the best overall k values "},{"metadata":{"trusted":true},"cell_type":"code","source":"n= 500\noutput = knn(train_x,train_y,test_x,test_y,n)\nn_range = range(1, n)\nplt.plot(n_range, output) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Having experimenting with different K from **n=1 to n=500** , From the figure I can conclude that the best k that could optimise this model is between **100 to 200** offering a 77% accuracy .\n\nThe ideal k value for this dataset should be **120** give or take. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}