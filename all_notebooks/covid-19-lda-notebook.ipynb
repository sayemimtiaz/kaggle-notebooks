{"cells":[{"metadata":{"id":"T_USq_DTBgho","colab_type":"text"},"cell_type":"markdown","source":"# Importing Key Libraries"},{"metadata":{"id":"ptFn63qqBghr","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"import os\nimport glob\nimport json\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{"id":"01QjldBsBghx","colab_type":"text"},"cell_type":"markdown","source":"# Reading in the data"},{"metadata":{"id":"sGipnonFBghy","colab_type":"code","outputId":"3dc53757-3b25-4e30-ae05-017fb5e854b5","colab":{},"trusted":true},"cell_type":"code","source":"all_json_paths = glob.glob(f'/kaggle/input/CORD-19-research-challenge/comm_use_subset/comm_use_subset/*.json', recursive=True)\nlen(all_json_paths)","execution_count":null,"outputs":[]},{"metadata":{"id":"kRrcR3p6Bgh5","colab_type":"code","outputId":"21210341-e78f-4701-a45f-7ece34070b93","colab":{},"trusted":true},"cell_type":"code","source":"#source:https://www.kaggle.com/amogh05/cord-19-eda-question-topic-modeling-starter\n#add more vars as required\n\nclass FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.title = content['metadata']['title']\n            self.abstract = []\n            self.body_text = []\n            self.biblio = []\n            self.biblio_doi = []\n            self.img_tables = []\n            self.back_matter = []\n            \n            \n            # Abstract\n            for entry in content['abstract']:\n                self.abstract.append(entry['text'])\n            self.abstract = '\\n'.join(self.abstract)\n            \n            # Body text\n            for entry in content['body_text']:\n                self.body_text.append(entry['text'])          \n            self.body_text = '\\n'.join(self.body_text)\n            \n            # bibliography\n            for bib_id, details in content['bib_entries'].items():\n                self.biblio.append(details['title'])\n                self.biblio_doi.append(details['other_ids'])\n            self.biblio = '\\n'.join(self.biblio)\n            #self.biblio_doi = '\\n'.join(self.biblio_doi)\n            \n            #img and table references\n            for ref_id,details in content['ref_entries'].items():\n                self.img_tables.append(details['text'])\n            self.img_tables = '\\n'.join(self.img_tables)\n            \n            #back_matter\n            for entry in content['back_matter']:\n                self.back_matter.append(entry['text'])\n            self.back_matter = '\\n'.join(self.back_matter)\n            \n    def __repr__(self):\n        return f'{self.paper_id}:{self.title}-{self.abstract}... {self.body_text}...{self.biblio}...{self.img_tables}...{self.back_matter}'\n        \n    \ndict_ = {'paper_id': [],'title':[], 'abstract': [], 'body_text': [],'biblio':[],'bidoi':[],'img_tables':[]}\nfor idx, entry in enumerate(all_json_paths):\n    if idx % (len(all_json_paths) // 10) == 0:\n        print(f'Processing index: {idx} of {len(all_json_paths)}')\n    #print(entry)\n    content = FileReader(entry)\n    dict_['paper_id'].append(content.paper_id)\n    dict_['title'].append(content.title)\n    dict_['abstract'].append(content.abstract)\n    dict_['body_text'].append(content.body_text)\n    dict_['biblio'].append(content.biblio)   \n    dict_['img_tables'].append(content.img_tables)  \ndf_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text','biblio','img_tables'])\ndf_covid.head()\n\n#identify dups\ndf_covid.describe(include='all')\n\ndf_covid.drop_duplicates(['abstract'], inplace=True)\ndf_covid.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"id":"R4g3odpnBgh8","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"df_covid['all_text'] = df_covid['abstract'] + '' + df_covid['body_text'] ","execution_count":null,"outputs":[]},{"metadata":{"id":"DzSi43YiBgh_","colab_type":"text"},"cell_type":"markdown","source":"# Find synonyms"},{"metadata":{"id":"rP4LiZK6Bgh_","colab_type":"code","outputId":"354c606d-1375-4ee6-b9ab-d50c77026cb1","colab":{},"trusted":true},"cell_type":"code","source":"#This approach does not work well:  defining a list is better\nimport nltk \nfrom nltk.corpus import wordnet \nsynonyms = [] \n\n  \nfor syn in wordnet.synsets('exposure'): \n    for l in syn.lemmas(): \n        synonyms.append(l.name()) \n        if l.antonyms(): \n            antonyms.append(l.antonyms()[0].name())\nprint(set(synonyms))","execution_count":null,"outputs":[]},{"metadata":{"id":"bgcCgM0dBgiC","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#defining a list better\nstage_syn_list = ['exposure','vulnerability','vulnerable'] ","execution_count":null,"outputs":[]},{"metadata":{"id":"5HeiIWjBBgiF","colab_type":"text"},"cell_type":"markdown","source":"# Filter By Stage"},{"metadata":{"id":"psF8crebBgiG","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"disease_stage_list = ['exposure' ,'acquisition' ,'progression', 'development' ,'complications' ,'fatality', 'disability']","execution_count":null,"outputs":[]},{"metadata":{"id":"799CUu-QBgiJ","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def filterByStage(text,stage_syn_list):\n    paper_list =[]\n    \n    for idx_num,row in text.iterrows():\n        for stage in stage_syn_list:\n            stage_found = False\n            if stage in row.all_text.split():\n                stage_found = True\n            else:\n                pass \n        if stage_found==True:\n            paper_list.append(row.all_text)\n    return paper_list","execution_count":null,"outputs":[]},{"metadata":{"id":"f--Oywl2BgiM","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"stage_dict = {}\n\nstage = disease_stage_list[0]\n\nstage_dict[stage] = filterByStage(df_covid,stage_syn_list)","execution_count":null,"outputs":[]},{"metadata":{"id":"KqBwy-cBBgiO","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#for later ease while searching for relevant papers\nexposure = pd.DataFrame(stage_dict[stage])","execution_count":null,"outputs":[]},{"metadata":{"id":"kVZ7kb-lBgiR","colab_type":"text"},"cell_type":"markdown","source":"# NLP Starts"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install scispacy scipy\n!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz\n!pip install tqdm -U\n!pip install spacy-langdetect","execution_count":null,"outputs":[]},{"metadata":{"id":"0SW7BY4EBgiR","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"import spacy\nimport en_core_sci_lg\nnlp = en_core_sci_lg.load()\n\n# We also need to detect language, or else we'll be parsing non-english text \n# as if it were English. \nfrom spacy_langdetect import LanguageDetector\nnlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n\nnlp.max_length=2000000\n\n# New stop words list \ncustomize_stop_words = [\n    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'fig', 'fig.', 'al.',\n    'di', 'la', 'il', 'del', 'le', 'della', 'dei', 'delle', 'una', 'da',  'dell',  'non', 'si'\n]\n\n# Mark them as stop words\nfor w in customize_stop_words:\n    nlp.vocab[w].is_stop = True","execution_count":null,"outputs":[]},{"metadata":{"id":"zxBNAcnfBgiV","colab_type":"text"},"cell_type":"markdown","source":"# LDA : Kaggle Notebook Approach\nhttps://www.kaggle.com/danielwolffram/topic-modeling-finding-related-articles"},{"metadata":{"id":"l_9eTyZUBgiV","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def spacy_tokenizer(sentence):\n    return [word.lemma_ for word in nlp(sentence) if not (word.like_num or word.is_stop or word.is_punct or word.is_space or len(word)==1)] \n    # remove numbers (e.g. from references [1], etc.)","execution_count":null,"outputs":[]},{"metadata":{"id":"CHowpLdvBgiY","colab_type":"text"},"cell_type":"markdown","source":"### Create vector representation of data"},{"metadata":{"id":"67rnpT5yBgiY","colab_type":"code","outputId":"e82fb073-be2d-43e4-dec6-bbb4c05f7f2b","colab":{"referenced_widgets":["669f984b48d5431cb5bbb3a1260c9dff"]},"trusted":true},"cell_type":"code","source":"tf_vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, max_features=800000) \ntf = tf_vectorizer.fit_transform(tqdm(stage_dict[stage]))\n\nprint(tf.shape)\n\nimport joblib\njoblib.dump(tf_vectorizer, '/kaggle/working/tf_vectorizer.csv')\njoblib.dump(tf, '/kaggle/working/tf.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"jWEc66wABgib","colab_type":"text"},"cell_type":"markdown","source":"### LDA"},{"metadata":{"id":"JE6W07IBBgib","colab_type":"code","outputId":"8daeaa9c-8636-4f04-c173-2c1092779388","colab":{},"trusted":true},"cell_type":"code","source":"lda_tf = LatentDirichletAllocation(n_components=50, random_state=0)\nlda_tf.fit(tf)\njoblib.dump(lda_tf, '/kaggle/working/lda.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"FSNlxmPABgii","colab_type":"text"},"cell_type":"markdown","source":"### Discover Topics"},{"metadata":{"id":"Fh6roGudBgij","colab_type":"code","outputId":"b7d437f1-4186-4ad9-d7b5-0fc901e6f26c","colab":{},"trusted":true},"cell_type":"code","source":"tfidf_feature_names = tf_vectorizer.get_feature_names()\n\ndef print_top_words(model, feature_names, n_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        message = \"\\nTopic #%d: \" % topic_idx\n        message += \" \".join([feature_names[i]\n                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n        print(message)\n    print()\n    \nprint_top_words(lda_tf, tfidf_feature_names, 25)","execution_count":null,"outputs":[]},{"metadata":{"id":"6iyid7cJBgip","colab_type":"text"},"cell_type":"markdown","source":"### Create Topic Distance Matrix"},{"metadata":{"id":"MrLbqxiyBgiq","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"topic_dist = pd.DataFrame(lda_tf.transform(tf))\ntopic_dist.to_csv('/kaggle/working/topic_dist.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"LaTZa7l4Bgit","colab_type":"code","outputId":"4548c53f-90c7-442a-f1be-b26443fc9fc5","colab":{},"trusted":true},"cell_type":"code","source":"topic_dist.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"k3yT53kaBgiv","colab_type":"text"},"cell_type":"markdown","source":"### Get Paper Related to Stage of Disease"},{"metadata":{"id":"_Iw5VbE1Bgiy","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#get most similar paper\nfrom scipy.spatial import distance\ndef get_k_nearest_docs(doc_dist, k=5, lower=1950, upper=2020, only_covid19=False, get_dist=False):\n    '''\n    doc_dist: topic distribution (sums to 1) of one article\n    \n    Returns the index of the k nearest articles (as by Jensenâ€“Shannon divergence in topic space). \n    '''\n    \n    #relevant_time = df.publish_year.between(lower, upper)\n    \n   # if only_covid19:\n   #     is_covid19_article = df.body_text.str.contains('COVID-19|SARS-CoV-2|2019-nCov|SARS Coronavirus 2|2019 Novel Coronavirus') #TODO: move outside\n   #     topic_dist_temp = topic_dist[relevant_time & is_covid19_article]\n   #     \n   # else:\n    #    topic_dist_temp = topic_dist[relevant_time]\n    \n    distances = topic_dist.apply(lambda x: distance.jensenshannon(x, doc_dist), axis=1)\n    k_nearest = distances[distances != 0].nsmallest(n=k).index\n    \n    if get_dist:\n        k_distances = distances[distances != 0].nsmallest(n=k)\n        return k_nearest, k_distances\n    else:\n        return k_nearest\n    \n#d = get_k_nearest_docs(topic_dist[1].iloc[0],k=10)","execution_count":null,"outputs":[]},{"metadata":{"id":"58uvnzDPBgiz","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def relevant_articles(df,tasks, k=3, lower=1950, upper=2020, only_covid19=False):\n    tasks = [tasks] if type(tasks) is str else tasks \n    \n    tasks_tf = tf_vectorizer.transform(tasks)\n    tasks_topic_dist = pd.DataFrame(lda_tf.transform(tasks_tf))\n\n    for index, bullet in enumerate(tasks):\n        print(bullet)\n        recommended = get_k_nearest_docs(tasks_topic_dist.iloc[index], k, lower, upper, only_covid19)\n        print(list(recommended))\n        recommended = df.iloc[recommended] #stage_dict[stage][','.join(list(recommended))]#\n    return recommended","execution_count":null,"outputs":[]},{"metadata":{"id":"OtIX0_IEBgi2","colab_type":"code","outputId":"feb00cff-a1c4-4111-c506-f159f83ca091","colab":{},"trusted":true},"cell_type":"code","source":"task = ['exposure']\nrelevant_articles(exposure,task,k=10) #k is the number of relevant articles","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Exposure- json files .ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":4}