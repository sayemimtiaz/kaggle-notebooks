{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Explanation of data for future reference:\n**1. flag: Whether the customer has bought the target product or not\n2. gender: Gender of the customer\n3. education: Education background of customer\n4. house_val: Value of the residence the customer lives in\n5. age: Age of the customer by group\n6. online: Whether the customer had online shopping experience or not\n7. customer_psy: Variable describing consumer psychology based on the area of residence\n8. marriage: Marriage status of the customer\n9. children: Whether the customer has children or not\n10. occupation: Career information of the customer\n11. mortgage: Housing Loan Information of customers\n12. house_own: Whether the customer owns a house or not\n13. region: Information on the area in which the customer are located\n14. car_prob: The probability that the customer will buy a new car(1 means the maximum possibleï¼‰\n15. fam_income: Family income Information of the customer(A means the lowest, and L means the highest)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport numpy as np\nimport missingno as mno\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nprint(\"Setup Complete\")\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Load data\nsales_data_filepath = \"../input/individual-company-sales-data/sales_data.csv\"\nsales_data = pd.read_csv(sales_data_filepath, index_col=0, encoding=\"latin-1\")\nsales_data.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualize null data in array\nmno.matrix(sales_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#delete marriage column, since it has so many missing data values\nsales_data = sales_data.drop('marriage', axis=1)\nsales_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#what are the unique values in education\nsales_data['education'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I notice the following issues with the data:\n* The house owner column has missing values, and is a binary (they either own a house, or they don't). So, I'm tempted to drop the column entirely. If the data were numerical, I could fill all null values with the mode or median.\n* There are also missing values in the educational column, but few enough that we lose less data by removing every entry (row) that has a null value, rather than remove the column entirely\n* The education column is using both numbers and phrases to describe the category, which is redundant. I would like to rename these.\n* The age column is doing the same thing, and also doesn't provide tidy categories. Does under 45 also mean over 25, because the next youngest category cutoff is 25? I need to rename these.\n* I don't know what the customer psychology or family income categories map to which characteristics. I posed this question on the discussion forum, but will remove the column for now. \n* There are some 'U's in the child category, which I take to mean unknown. Since this comprises almost one-fourth of the data entries, I'm going to delete the entire column.\n\nI'm tempted to just remove anything I don't understand, but first I'll look at a correlation heatmap to see if I can't afford to lose any one category. Except I can't look at correlations between non-continuous, numerical variables. I would love to come back to this later, but for now I will just eliminate all the data that doesn't play nice."},{"metadata":{"trusted":true},"cell_type":"code","source":"#column deletions\nsales_data = sales_data.drop(['customer_psy','child', 'house_owner', 'fam_income'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And it turns out I have no idea how to do the equivalent of Find & Replace All in a dataframe. I'm going to use iterrows, which is strongly warned against on stack overflow:\nhttps://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas\n\nWith iterrows, I want to be able to use iloc(). However, right now my index is not the number of the row, but the flag (Y/N). I want to be able to reference the index value of any row to instantly know which nth row that one is in the dataframe. So, I'm going to create a new column for the flag data and fill the index with ascending integers."},{"metadata":{"trusted":true},"cell_type":"code","source":"#create a new column for the flag\nsales_data['flag'] = sales_data.index\n#replace the old values in the index with ascending integers\nsales_data.index = np.arange(len(sales_data))\n#rename index using the rename_axis method\nsales_data = sales_data.rename_axis('customerID')\nsales_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sales_data.iloc[[2],[0]] = \"M\"\n#print(sales_data.iloc[[2],[0]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sales_data['flag'] = sales_data.index\n#print(sales_data['flag'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#clean strings while iterating through each row\n'''\nfor index, row in sales_data.iterrows():\n    if (row['education'] == '4. Grad'):\n        row['education'] = '4'\n    elif (row['education'] == '3. Bach'):\n        row['education'] = '3'\n    elif (row['education'] == '2. Some College'):\n        row['education'] = '2'\n       #print(row)\n'''\n#The above code appears to work when I print out the rows, but when I look at sales_data.head() again\n# nothing seems to have changed. Credit to Rish Patel for showing me replace.\n\n#On education\nsales_data.replace('0. <HS', 'dropout', inplace=True)\nsales_data.replace('1. HS', 'hs', inplace=True)\nsales_data.replace('2. Some College', 'associates', inplace=True)\nsales_data.replace('3. Bach', 'bachelors', inplace=True)\nsales_data.replace('4. Grad', 'masters', inplace=True)\n\n#On age\nsales_data.replace('1_Unk', '1', inplace=True)\nsales_data.replace('2_<=25', '2', inplace=True)\nsales_data.replace('3_<=35', '3', inplace=True)\nsales_data.replace('4_<=45', '4', inplace=True)\nsales_data.replace('5_<=55', '5', inplace=True)\nsales_data.replace('6_<=65', '6', inplace=True)\nsales_data.replace('7_>65', '7', inplace=True)\n\n#On mortgage\nsales_data.replace('1Low', 'low', inplace=True)\nsales_data.replace('2Med', 'medium', inplace=True)\nsales_data.replace('3High', 'high', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#There are two ways to address the null values in the education column. You can either fill them or drop the entire row\n# Uncomment the one you prefer (I'm opting to drop rows)\n\n#1.Replace null values\n#sales_data['education'].fillna('none', inplace=True)\n#2.drop all columns that have a null value\nsales_data = sales_data.dropna()\n\n#Two different ways to check if data contains empty values\nprint('Columns with null values:\\n', sales_data.isnull().sum())\nprint(\"-\"*10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, I need to work on **Feature Engineering**, which is described well here:\n(credit to https://www.kaggle.com/mjbahmani/a-comprehensive-ml-workflow-with-python)\n\n**6-4-2 Feature Encoding**\nIn machine learning projects, one important part is feature engineering. It is very common to see categorical features in a dataset. However, our machine learning algorithm can only read numerical values. It is essential to encoding categorical features into numerical values[28]\n\nEncode labels with value between 0 and n_classes-1\nLabelEncoder can be used to normalize labels.\nIt can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split data into testing and training sets\n#This divides the data by absolute number of entries. However we want to divide by percent.\n'''\ntrain_data = sales_data[:200]\ntest_data = sales_data[200:]\n'''\n#train_test_split function is imported from sklearn. \n#test_size set so that 80% of data is used for training\ntrain_data, test_data = train_test_split(sales_data,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_features(df_train, df_test):\n    features = ['gender', 'education','house_val','age','online','occupation','mortgage','region','car_prob']\n    df_combined = pd.concat([df_train[features], df_test[features]])\n    \n    for feature in features:\n        le = preprocessing.LabelEncoder()\n        le = le.fit(df_combined[feature])\n        df_train[feature] = le.transform(df_train[feature])\n        df_test[feature] = le.transform(df_test[feature])\n    return df_train, df_test\n\ntrain_data, test_data = encode_features(train_data, test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#I want to see if all the data was successfully converted to numerical:\ntest_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Separate data into features(x) and target(y)\nx_all = train_data.drop(['flag'], axis=1)\ny_all = train_data['flag']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We're using train_test_split for the second time, but this time with additional parameters\nnum_test = 0.3\nX_train, X_test, y_train, y_test = train_test_split(x_all, y_all, test_size=num_test, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n# Choose the type of classifier. \nrfc = RandomForestClassifier()\n\n# Choose some parameter combinations to try\nparameters = {'n_estimators': [4, 6, 9], \n              'max_features': ['log2', 'sqrt','auto'], \n              'criterion': ['entropy', 'gini'],\n              'max_depth': [2, 3, 5, 10], \n              'min_samples_split': [2, 3, 5],\n              'min_samples_leaf': [1,5,8]\n             }\n\n# Type of scoring used to compare parameter combinations\nacc_scorer = make_scorer(accuracy_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(rfc, parameters, scoring=acc_scorer)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nrfc = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nrfc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_prediction = rfc.predict(X_test)\nrfc_score=accuracy_score(y_test, rfc_prediction)\nprint(rfc_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It worked! The output was:\n**# 0.689164809508649**\nIt took about two minutes to compute"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}