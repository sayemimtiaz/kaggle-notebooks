{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n---\n# Machine Learning Model for Breast Cancer Survival Prediction using Gene Expression Profiles \n\n\n<img src=\"https://biox.stanford.edu/sites/g/files/sbiybj7941/f/rna_polymerase_highlight_banner.png\" style=\"height: 250px; width: 1000px\">\n\n---"},{"metadata":{},"cell_type":"markdown","source":"## Problem Statment\n\nMost of us know someone who struggled with breast cancer, or at least heard about the struggles facing patients who are fighting against breast cancer. Breast cancer is the most frequent cancer among women, impacting 2.1 million women each year. Breast cancer causes the greatest number of cancer-related deaths among women. In 2018 alone, it is estimated that 627,000 women died from breast cancer.  \n\nThe most important part of a process of clinical decision-making in patients with cancers in general is the accurate estimation of prognosis and survival duration. Breast cancer patients with the same stage of disease and the same clinical characteristics can have different treatment responses and overall survival, but why?\n\nCancers are associated with genetic abnormalities. Gene expression measures the level of gene activity in a tissue and gives information about its complex activities. Comparing the genes expressed in normal and diseased tissue can bring better insights about the cancer prognosis and outcomes. Using machine learning techniques on genetic data has the potentials of giving the correct estimation of survival time and can prevent unnecessary surgical and treatment procedures.\n\n##### The aim of this project is to predict breast cancer survival using  machine learning models with clinical data and gene expression profiles. \n"},{"metadata":{},"cell_type":"markdown","source":"## Executive Summary\n\nThe aim of this project is to predict breast cancer survival using machine learning models with clinical data and gene expression profiles. Using machine learning models on genetic data has the potential to improve our understanding of cancers and survival prediction.\n\nThe dataset used in this project is the Molecular Taxonomy of Breast Cancer International Consortium (METABRIC) database, which is a Canada-UK Project which contains targeted sequencing data of 1,980 primary breast cancer samples. Clinical and genomic data was downloaded from [cBioPortal](https://www.cbioportal.org/).\n\n\nThe following metrics were used to evaluate the outputs of the model:\n1. The Confusion Matrix, which includes the four possible outcomes of binary classification:\n    \n    • True Positive (TP): The number of patients who survived and were classified as survived.\n    \n    • True Negative (TN): The number of patients who died and were classified as died.\n    \n    • False Negative (FN): The number of patients who survived and were classified as died.\n    \n    • False Positive (FP): The number of patients who died and were classified as died.\n\n2. The AUC is the Area Under the Receiver Operating Characteristic (ROC) Curve. It can be interpreted as the extent of how well the model is able to distinguish between the two different classes.\n\n3. Accuracy: Number of correct assessments (True positives + true negatives) / Total number of instances\n\n\nThe model with the best preformace was XGBoost with max_depth=5 and min_child_weight=1 that was trained with the full dataframe with the addition of all of the combination of all genetic data values. The accuacy score was 0.779 and the AUC was 0.76. To enhance this project, increase the number of samples, include mutations and raw genetic data into the modeling part, and maybe try some deep learning models."},{"metadata":{},"cell_type":"markdown","source":"### Contents:\n- [Datasets Description](#Datasets_Description)\n- [Data Import & Cleaning](#Data_Import_and_Cleaning)\n- [Exploratory Data Analysis and Data Visualization](#Exploratory_Data_Analysis)\n    - [Relationship between clinical attributes and outcomes](#clinical)\n    - [Relationship between genetic attributes and outcomes](#genetic)\n    - [Relationship between genetic mutation attributes and outcomes](#mutation)\n- [Preprocessing and Modeling](#Preprocessing_and_Modeling)\n- [Conclusions](#Conclusions_and_Recommendations)\n- [References](#References)\n---"},{"metadata":{},"cell_type":"markdown","source":"<a name=\"Datasets_Description\"></a>\n## Datasets Description\n\nThe Molecular Taxonomy of Breast Cancer International Consortium (METABRIC) database is a Canada-UK Project which contains targeted sequencing data of 1,980 primary breast cancer samples. Clinical and genomic data was downloaded from [cBioPortal](https://www.cbioportal.org/).\n\nThe dataset was collected by Professor Carlos Caldas from Cambridge Research Institute and Professor Sam Aparicio from the British Columbia Cancer Centre in Canada and published on Nature Communications [(Pereira et al., 2016)](https://www.nature.com/articles/ncomms11479). It was also featured in multiple papers including Nature and others:\n\n- [Associations between genomic stratification of breast cancer and centrally reviewed tumour pathology in the METABRIC cohort](https://www.nature.com/articles/s41523-018-0056-8)\n- [Predicting Outcomes of Hormone and Chemotherapy in the Molecular Taxonomy of Breast Cancer International Consortium (METABRIC) Study by Biochemically-inspired Machine Learning](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5461908/)\n\n#### Clinical attributes in the dataset:\n\n| Name                           | Type   | Description |\n|--------------------------------|--------|-------------|\n| patient_id                     | object | Patient ID  |\n| age_at_diagnosis               | float  |    Age of the patient at diagnosis time         |\n| type_of_breast_surgery         | object | Breast cancer surgery type: 1-  MASTECTOMY, which refers to a surgery to remove all breast tissue from a breast as a way to treat or prevent breast cancer.  2- BREAST CONSERVING, which refers to a urgery where only the part of the breast that has cancer is removed     |\n| cancer_type                    | object | Breast cancer types: 1- Breast Cancer or  2- Breast Sarcoma           |\n| cancer_type_detailed           | object | Detailed Breast cancer types: 1- Breast Invasive Ductal Carcinoma 2- Breast Mixed Ductal and Lobular Carcinoma 3- Breast Invasive Lobular Carcinoma  4- Breast Invasive Mixed Mucinous Carcinoma 5- Metaplastic Breast Cancer   |\n| cellularity                    | object | Cancer cellularity post chemotherapy, which refers to the amount of tumor cells in the specimen and their arrangement into clusters         |\n| chemotherapy                   | int    |  Whether or not the patient had chemotherapy as a treatment (yes/no)    |\n| pam50_+_claudin-low_subtype    | object |  Pam 50: is a tumor profiling test that helps show whether some estrogen receptor-positive (ER-positive), HER2-negative breast cancers are likely to metastasize (when breast cancer spreads to other organs). The claudin-low breast cancer subtype is defined by gene expression characteristics, most prominently: Low expression of cell–cell adhesion genes, high expression of epithelial–mesenchymal transition (EMT) genes, and stem cell-like/less differentiated gene expression patterns       |\n| cohort                         | float  |  Cohort is a group of subjects who share a defining characteristic (It takes a value from 1 to 5)        |\n| er_status_measured_by_ihc      | float  |  To assess if estrogen receptors are expressed on cancer cells by using immune-histochemistry (a dye used in pathology that targets specific antigen, if it is there, it will give a color, it is not there, the tissue on the slide will be colored)  (positive/negative)         |\n| er_status                      | object |   Cancer cells are positive or negative for estrogen receptors          |\n| neoplasm_histologic_grade      | int  |  Determined by pathology by looking the nature of the cells, do they look aggressive or not  (It takes a value from 1 to 3)         |\n| her2_status_measured_by_snp6   | object | To assess if the cancer positive for HER2 or not by using advance molecular techniques (Type of next generation sequencing)       |\n| her2_status                    | object |   Whether the cancer is positive or negative for HER2          |\n| tumor_other_histologic_subtype | object |  Type of the cancer based on microscopic examination of the cancer tissue (It takes a value of  'Ductal/NST', 'Mixed', 'Lobular', 'Tubular/ cribriform', 'Mucinous', 'Medullary', 'Other', 'Metaplastic'  )      |\n| hormone_therapy                | int |   Whether or not the patient had hormonal as a treatment (yes/no)           |\n| inferred_menopausal_state      | object |  Whether the patient is  is post menopausal or not   (post/pre)        |\n| integrative_cluster            | object | Molecular subtype of the cancer based on some gene expression (It takes a value from '4ER+', '3', '9', '7', '4ER-', '5', '8', '10', '1', '2', '6')            |\n| primary_tumor_laterality       | object |   Whether it is involving the right breast or the left breast           |\n| lymph_nodes_examined_positive  | float  |  To take samples of the lymph node during the surgery and see if there were involved by the cancer            |\n| mutation_count                 | float  |  Number of gene that has relevant mutations            |\n| nottingham_prognostic_index    | float  |   It is used to determine prognosis following surgery for breast cancer. Its value is calculated using three pathological criteria: the size of the tumour; the number of involved lymph nodes; and the grade of the tumour.          |\n| oncotree_code                  | object |  The OncoTree is an open-source ontology that was developed at Memorial Sloan Kettering Cancer Center (MSK) for standardizing cancer type diagnosis from a clinical perspective by assigning each diagnosis a unique OncoTree code.           |\n| overall_survival_months        | float  |  Duration from the time of the intervention to death        |\n| overall_survival               | object |   Target variable wether the patient is alive of dead.          |\n| pr_status                      | object |    Cancer cells are positive or negative for progesterone  receptors          |\n| radio_therapy                  | int    | Whether or not the patient had radio as a treatment (yes/no)             |\n| 3-gene_classifier_subtype      | object | Three Gene classifier subtype It takes a value from 'ER-/HER2-', 'ER+/HER2- High Prolif', nan, 'ER+/HER2- Low Prolif','HER2+'           |\n| tumor_size                     | float  | Tumor size measured by imaging techniques            |\n| tumor_stage                    | float  | Stage of the cancer based on the involvement of surrounding structures, lymph nodes and distant spread          |\n| death_from_cancer              | int  |  Wether the patient's death was due to cancer or not (yes/no)           |\n\n\n#### Genetic attributes in the dataset:\n\nThe genetics part of the dataset contains m-RNA levels z-score for 331 genes, and mutation for 175 genes. \n\nFrom CBioPortal:\n> ##### What are mRNA? \nThe DNA molecules attached to each slide act as probes to detect gene expression, which is also known as the transcriptome or the set of messenger RNA (mRNA) transcripts expressed by a group of genes. To perform a microarray analysis, mRNA molecules are typically collected from both an experimental sample and a reference sample.\n\n>##### What are mRNA Z-Scores? \nFor mRNA expression data, The calculations of the relative expression of an individual gene and tumor to the gene's expression distribution in a reference population is done. That reference population is all samples in the study . The returned value indicates the number of standard deviations away from the mean of expression in the reference population (Z-score). This measure is useful to determine whether a gene is up- or down-regulated relative to the normal samples or all other tumor samples.\n\nThe formula is : \n\n`z = (expression in tumor sample - mean expression in reference sample) / standard deviation of expression in reference sample`\n\n\n---"},{"metadata":{},"cell_type":"markdown","source":"<a name=\"Data_Import_and_Cleaning\"></a>\n## Data Import & Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Basic libraries\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\n# Visualization libraries\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\nimport yellowbrick as yb\nfrom matplotlib.colors import ListedColormap\nfrom yellowbrick.classifier import ROCAUC\nfrom matplotlib_venn import venn3\nimport matplotlib.patches as mpatches\n\n# Statistics, EDA, metrics libraries\nfrom scipy.stats import normaltest, skew\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error, accuracy_score, f1_score\nfrom sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom scipy.special import boxcox, inv_boxcox\n\n# Modeling libraries\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold, cross_val_predict,  KFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.decomposition import PCA\nfrom scipy.stats import zscore\nfrom itertools import combinations\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.ensemble import IsolationForest\nimport kmapper as km\nfrom sklearn.cluster import KMeans\n\n\nfrom IPython.display import set_matplotlib_formats \nplt.style.use('ggplot')\nsns.set_style('whitegrid')\nsns.set(font_scale=1.5)\n%config InlineBackend.figure_format = 'retina'\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# Pallets used for visualizations\ncolor= \"Spectral\"\ncolor_plt = ListedColormap(sns.color_palette(color).as_hex())\ncolor_hist = 'teal'\ntwo_colors = [ sns.color_palette(color)[0], sns.color_palette(color)[5]]\nthree_colors = [ sns.color_palette(color)[5],sns.color_palette(color)[2], sns.color_palette(color)[0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/breast-cancer-gene-expression-profiles-metabric/METABRIC_RNA_Mutation.csv', delimiter=',')\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding missing data and the percentage of it in each column\ntotal = df.isnull().sum().sort_values(ascending = False)\npercent = (df.isnull().sum() / df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis = 1, keys = ['Total_NaN', 'Percent_Nan'])\nmissing_data.head(14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualization of missing data\nfig, ax = plt.subplots( figsize = (15, 8))\nsns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='viridis')\nax.set_title('Main Data Frame')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The genetic data in the dataframe is complete with no missing data. However, the clinical data has some categorical clinical features that had up to 26% missing data (tumor_stage column has 26% missing data, while 3-gene_classifier_subtype has 10.7% missing data, and primary_tumor_laterality contains 3.7% missing data). There are 9 features that contains around 3 -0.001% missing data, and the rest of the features are complete. "},{"metadata":{},"cell_type":"markdown","source":"<a name=\"Exploratory_Data_Analysis\"></a>\n## Exploratory Data Analysis  and Data Visualization\n\n\n<a name=\"clinical\"></a>\n### a) Relationship between clinical attributes and outcomes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a new dataframe for clinical attributes only\nclinical_features_to_drop = df.columns[31:] # non clinical attributes\nclinical_df = df.drop(clinical_features_to_drop, axis=1)\nclinical_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clinical_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- #### The Distribution of Clinical Columns in the Dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"# a function that takes a dataframe and transforms it into a standard form after dropping nun_numirical columns\ndef to_standard (df):\n    \n    num_df = df[df.select_dtypes(include = np.number).columns.tolist()]\n    \n    ss = StandardScaler()\n    std = ss.fit_transform(num_df)\n    \n    std_df = pd.DataFrame(std, index = num_df.index, columns = num_df.columns)\n    return std_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax, fig = plt.subplots(1, 1, figsize = (15, 5))\nplt.title('The Distribution of Clinical Attributes in the Dataframe', fontsize = 20) #Change please\n\nsns.boxplot(y = \"variable\", x = \"value\", data = pd.melt(to_standard(clinical_df)), palette = 'Spectral')\nplt.xlabel('Range after Standarization', size = 16)\nplt.ylabel('Attribue', size = 16)\n\n# fix for mpl bug that cuts off top/bottom of seaborn viz\nb, t = plt.ylim() # discover the values for bottom and top\nb += 0.5 # Add 0.5 to the bottom\nt -= 0.5 # Subtract 0.5 from the top\nplt.ylim(b, t) # update the ylim(bottom, top) values\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the distribution of all numerical data, some of them are normally distributed (like tumor_stage, and age_at_diagnosis), but  most of the features are right skewed with a lot of outliers (lymph_nodes_examined_positive, mutation_count, and tumor_size).   We decided to keep the outliers, as they are very important in healthcare data. "},{"metadata":{},"cell_type":"markdown","source":"- #### The Distribution of the Two Target Classes in Numerical Clinical Columns in the Dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (20, 25))\nj = 0\nnum_clinical_columns= ['age_at_diagnosis', 'lymph_nodes_examined_positive','mutation_count','nottingham_prognostic_index', 'overall_survival_months', 'tumor_size' ]\nfor i in clinical_df[num_clinical_columns].columns:\n    plt.subplot(6, 4, j+1)\n    j += 1\n    sns.distplot(clinical_df[i][clinical_df['overall_survival']==1], color='g', label = 'survived')\n    sns.distplot(clinical_df[i][clinical_df['overall_survival']==0], color='r', label = 'died')\n    plt.legend(loc='best')\nfig.suptitle('Clinical Data Analysis')\nfig.tight_layout()\nfig.subplots_adjust(top=0.95)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- #### Visualizations Clinical Columns in the Dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"died = clinical_df[clinical_df['overall_survival']==0]\nsurvived = clinical_df[clinical_df['overall_survival']==1]\n\nalive = clinical_df[clinical_df['death_from_cancer']=='Living']\ndied_cancer = clinical_df[clinical_df['death_from_cancer']=='Died of Disease']\ndied_not_cancer = clinical_df[clinical_df['death_from_cancer']=='Died of Other Causes']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(ncols=2, figsize=(15,3), sharey=True)\n\nsns.boxplot(x='overall_survival_months', y='overall_survival', orient='h', data=clinical_df, ax=ax[0], palette = two_colors, saturation=0.90)\nsns.boxplot(x='age_at_diagnosis', y='overall_survival', orient='h', data=clinical_df, ax=ax[1], palette = two_colors, saturation=0.90)\n\nfig.suptitle('The Distribution of Survival time in months and age with Target Attribute', fontsize = 18)\n\nax[0].set_xlabel('Total survival time in Months')\nax[0].set_ylabel('survival')\nax[1].set_xlabel('Age at diagnosis')\nax[1].set_ylabel('')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To compare between the two classes of patients who survived and patients who did not, we can see the difference between the two distributions in age_at_diagnosis column, as patients who were younger when diagnosed with breast cancer were more likely to survive. Also, the duration from the time of the intervention to death or to current time is longer in the patients who survive. That means that pateints are either dying early from breast cancer or surviving."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(ncols=2, figsize=(15,3), sharey=True)\n\nsns.boxplot(x='overall_survival_months', y='death_from_cancer', orient='h', data=clinical_df, ax=ax[0], palette = three_colors, saturation=0.90)\nsns.boxplot(x='age_at_diagnosis', y='death_from_cancer', orient='h', data=clinical_df, ax=ax[1], palette = three_colors, saturation=0.90)\n\nfig.suptitle('The Distribution of Survival and Recurrence with Target Attribute', fontsize = 18)\n\nax[0].set_xlabel('Total survival time in Months')\nax[0].set_ylabel('survival')\nax[1].set_xlabel('Age at diagnosis')\nax[1].set_ylabel('')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The variable 'death_from_cancer' shows us if the patient is alive or died from cancer or its complications or died of other causes. From the distribution of the three classes, we can see that the median of the survival time in months of patients who died from breast cancer is low compared to the other two classes, and its distribution os right-skewed with a lot of outliers. Also, patients who died from other causes than cancer tend to be older than the other two classes. The distribution of it is left-skewed with some younger outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"ig, ax = plt.subplots( figsize=(12, 6))\nax = sns.boxplot(x ='tumor_size', y ='tumor_stage',  data = clinical_df, orient='h', hue='overall_survival', palette=two_colors)\n\nax.set_ylabel('Tumor stage')\nax.set_xlabel('Tumor size')\nfig.suptitle('Tumor stage vs. Tumor size and overall_survival', fontsize=20)\n\n#legend = ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the Tumer stage increases the tumor size increases as well. Also, if lower tumor stages the probability of survival is higher than when the patient reaches the fourth stage "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(ncols=2, figsize=(15,3), sharey=True)\n\nfig.suptitle('The Distribution of Survival and Recurrence with Target Attribute', fontsize = 18)\n\nax[0].hist(died['overall_survival_months'], alpha=0.9, color=sns.color_palette(color)[0], label='Died')\nax[0].hist(survived['overall_survival_months'], alpha=0.9, color=sns.color_palette(color)[5], label='Survived')\nax[0].legend()\n\nax[1].hist(alive['overall_survival_months'], alpha=0.9, color=sns.color_palette(color)[5], label='Survived')\nax[1].hist(died_cancer['overall_survival_months'], alpha=0.9, color=sns.color_palette(color)[0], label='Died from cancer')\nax[1].hist(died_not_cancer['overall_survival_months'], alpha=0.9, color=sns.color_palette(color)[1], label='Died not from cancer')\nax[1].legend()\n\nax[0].set_xlabel('Total survival time in Months')\nax[0].set_ylabel('Number of patients')\nax[1].set_xlabel('Total survival time in Months')\nax[1].set_ylabel('')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When the total survival time in months increases, the probability of survival increases as well, and the probability of dying from reasons other than cancer decrease with time slightly."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(ncols=3, figsize=(15,3), sharey=True)\n\nsns.boxplot(x='age_at_diagnosis', y='overall_survival', orient='h', data=clinical_df, ax=ax[0], palette = two_colors, saturation=0.90)\nsns.boxplot(x='tumor_size', y='overall_survival', orient='h', data=clinical_df, ax=ax[1], palette = two_colors, saturation=0.90)\nsns.boxplot(x='lymph_nodes_examined_positive', y='overall_survival', orient='h', data=clinical_df, ax=ax[2], palette = two_colors, saturation=0.90)\n\nfig.suptitle('The Distribution of Continuous Clinical Attributes', fontsize = 18)\nplt.yticks([-0.5, 0, 1, 1.5], ['','Died', 'Survived',''])\nax[0].set_xlabel('Age in Year')\nax[0].set_ylabel('survive (yes/no)')\n\nax[1].set_xlabel('Tumour diameter in (mm)')\nax[1].set_ylabel('')\n\nax[2].set_xlabel('Number of positive lymph nodes')\nax[2].set_ylabel('')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(ncols=3, figsize=(15,3))\nfig.suptitle('The Distribution of Continuous Clinical Attributes', fontsize = 18)\n\nax[0].hist(survived['age_at_diagnosis'], alpha=0.9, color=sns.color_palette(color)[5], label='Survived')\nax[0].hist(died['age_at_diagnosis'], alpha=0.9, color=sns.color_palette(color)[0], label='Died')\n#ax[0].legend()\n\nax[1].hist(survived['tumor_size'], alpha=0.9, color=sns.color_palette(color)[5], label='Survived')\nax[1].hist(died['tumor_size'], alpha=0.9, color=sns.color_palette(color)[0], label='Died')\n#ax[1].legend()\n\nax[2].hist(survived['lymph_nodes_examined_positive'], alpha=0.9, color=sns.color_palette(color)[5], label='Survived')\nax[2].hist(died['lymph_nodes_examined_positive'], alpha=0.9, color=sns.color_palette(color)[0], label='Died')\nax[2].legend()\n\nax[0].set_xlabel('Age in Year')\nax[0].set_ylabel('Number of patients')\nax[1].set_xlabel('Tumour diameter in (mm)')\nax[1].set_ylabel('')\nax[2].set_xlabel('Number of positive nodes')\nax[2].set_ylabel('')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The median of tumor size and the number of positive lymph nodes is lower in the survived class than the died class."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(ncols=3, figsize=(15,3))\nfig.suptitle('The Distribution of Continuous Clinical Attributes', fontsize = 18)\n\nax[0].hist(alive['age_at_diagnosis'], alpha=0.8, color=sns.color_palette(color)[5], label='Survived')\nax[0].hist(died_cancer['age_at_diagnosis'], alpha=0.8, color=sns.color_palette(color)[0], label='Died from cancer')\nax[0].hist(died_not_cancer['age_at_diagnosis'], alpha=0.8, color=sns.color_palette(color)[1], label='Died not from cancer')\n#ax[0].legend()\n\nax[1].hist(alive['tumor_size'], alpha=0.8, color=sns.color_palette(color)[5], label='Survived')\nax[1].hist(died_cancer['tumor_size'], alpha=0.8, color=sns.color_palette(color)[0], label='Died from cancer')\nax[1].hist(died_not_cancer['tumor_size'], alpha=0.8, color=sns.color_palette(color)[1], label='Died not from cancer')\n#ax[1].legend()\n\nax[2].hist(survived['lymph_nodes_examined_positive'], alpha=0.8, color=sns.color_palette(color)[5], label='Survived')\nax[2].hist(died_cancer['lymph_nodes_examined_positive'], alpha=0.8, color=sns.color_palette(color)[0], label='Died from cancer')\nax[2].hist(died_not_cancer['lymph_nodes_examined_positive'], alpha=0.8, color=sns.color_palette(color)[1], label='Died not from cancer')\nax[2].legend()\n\nax[0].set_xlabel('Age in Year')\nax[0].set_ylabel('Number of patients')\nax[1].set_xlabel('Tumour diameter in (mm)')\nax[1].set_ylabel('')\nax[2].set_xlabel('Number of positive nodes')\nax[2].set_ylabel('')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(ncols=3, figsize=(15,3))\nfig.suptitle('The Distribution of treatment and survival', fontsize = 18)\n\nsns.countplot(died['chemotherapy'], color=sns.color_palette(color)[0], label='Died', ax=ax[0], saturation=0.90)\nsns.countplot(x= survived['chemotherapy'] , color=sns.color_palette(color)[5], label='Survived', ax=ax[0], saturation=0.90)\n\n#ax[0].legend()\nax[0].set(xticklabels=['No','Yes'])\n\nsns.countplot(died['hormone_therapy'], color=sns.color_palette(color)[0], label='Died', ax=ax[1], saturation=0.90)\nsns.countplot(x=  survived['hormone_therapy'], color=sns.color_palette(color)[5], label='Survived', ax=ax[1], saturation=0.90)\n\nax[1].legend()\nax[1].set(xticklabels=['No','Yes'])\n\nsns.countplot(died['radio_therapy'], color=sns.color_palette(color)[0], label='Died', ax=ax[2], saturation=0.90)\nsns.countplot(x=  survived['radio_therapy'], color=sns.color_palette(color)[5], label='Survived', ax=ax[2], saturation=0.90)\n\n#ax[2].legend()\nax[2].set(xticklabels=['No','Yes'])\n\nax[0].set_xlabel('Chemotherapy')\nax[0].set_ylabel('Number of patients')\nax[1].set_xlabel('Hormonal therapy')\nax[1].set_ylabel('')\nax[2].set_xlabel('Radio therapy')\nax[2].set_ylabel('')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualise the treatments and proportion death for other groups using venn diagram\n\n#first create subsets for different combinations of treatments\nchemo = clinical_df[(clinical_df[\"chemotherapy\"]==True) & (clinical_df[\"radio_therapy\"]==False) & (clinical_df[\"hormone_therapy\"]==False)]\nradio = clinical_df[(clinical_df[\"chemotherapy\"]==False) & (clinical_df[\"radio_therapy\"]==True) & (clinical_df[\"hormone_therapy\"]==False)]\nhormonal = clinical_df[(clinical_df[\"chemotherapy\"]==False) & (clinical_df[\"radio_therapy\"]==False) & (clinical_df[\"hormone_therapy\"]==True)]\nchemo_radio = clinical_df[(clinical_df[\"chemotherapy\"]==True) & (clinical_df[\"radio_therapy\"]==True) & (clinical_df[\"hormone_therapy\"]==False)]\nradio_hormonal = clinical_df[(clinical_df[\"chemotherapy\"]==False) & (clinical_df[\"radio_therapy\"]==True) & (clinical_df[\"hormone_therapy\"]==True)]\nhormonal_chemo = clinical_df[(clinical_df[\"chemotherapy\"]==True) & (clinical_df[\"radio_therapy\"]==False) & (clinical_df[\"hormone_therapy\"]==True)]\nall_3 = clinical_df[(clinical_df[\"chemotherapy\"]==True) & (clinical_df[\"radio_therapy\"]==True) & (clinical_df[\"hormone_therapy\"]==True)]\n\n#calculate number of people for each combination and proportion death\ndf_subsets = [chemo, radio, hormonal, chemo_radio, radio_hormonal, hormonal_chemo, all_3]\nsizes=[]\nproportiondeath=[]\nfor dataframe in df_subsets:\n    sizes.append(np.shape(dataframe)[0])\n    proportiondeath.append(np.mean(dataframe[\"overall_survival\"]))\n\n#set size of circles relative to size of each subset (where possible)\n#set gradient of blue according to proportion of death in subset calculated above\nfig, ax = plt.subplots(figsize=(8,6))\nv = venn3(subsets=sizes, set_labels=(\"Chemo\", \"Radio \", \"Hormonal\"), ax=ax, alpha=0.6, set_colors= sns.color_palette(color))\n\nfor text in v.set_labels:\n    text.set_fontsize(14)\n    \nax.set_title(\"Patients by treatment group\", size=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Venn diagram for the three different treatments for breast cancer and the distribution of patients amongst them. we can see that most patients either have chemo and hormonal therapy or chemo and radio therapy. there is a group that is not shown here in the diagram, which are the patients that did not receive any of the three treatments. they were 289 patients and their survival rate was slightly lower than the rest of patients. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots( figsize=(10,5))\nfig.suptitle('The Distribution histopathological class and survival', fontsize = 18)\n\nsns.countplot(x='neoplasm_histologic_grade', hue='overall_survival' ,data = clinical_df, palette=two_colors , ax=ax, saturation=0.90)\nax.legend([ 'Died', 'Survived'])\n\nax.set_xlabel('histopathological class')\nax.set_ylabel('Number of patients')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- #### Visualize Correlation of between the Clinical Attributes"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(figsize = (13, 10)) \ncategorical_columns = clinical_df.select_dtypes(include=['object']).columns.tolist()\nunwanted_columns = ['patient_id','death_from_cancer' ]\ncategorical_columns = [ele for ele in categorical_columns if ele not in unwanted_columns] \nno_id_clinical_df = pd.get_dummies(clinical_df.drop('patient_id',axis=1 ), columns= categorical_columns)\n#no_id_clinical_df= clinical_df.drop('ID',axis=1 )\nmask = np.triu(np.ones_like(no_id_clinical_df.corr(), dtype = np.bool))\nsns.heatmap(no_id_clinical_df.corr(), ax = axs, mask = mask, cmap = sns.diverging_palette(180, 10, as_cmap = True))\nplt.title('Correlation between the Clinical Attributese')\n\n# fix for mpl bug that cuts off top/bottom of seaborn viz\nb, t = plt.ylim() # discover the values for bottom and top\nb += 0.5 # Add 0.5 to the bottom\nt -= 0.5 # Subtract 0.5 from the top\nplt.ylim(b, t) # update the ylim(bottom, top) values\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there is high correlation between some of the columns. "},{"metadata":{},"cell_type":"markdown","source":"- #### Correlation between the Clinical Attributes and survival"},{"metadata":{"trusted":true},"cell_type":"code","source":"Corr_survival = no_id_clinical_df.corr()['overall_survival'].sort_values(ascending = False)\nCorr_df = pd.DataFrame({'Correlation':Corr_survival})\nCorr_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Corr_df.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a positive correlation between survival and overall survival in months, conserving surgery type, pre menopaus status, but a negative correlation between survival and lymph nodes examined positive, mastectomy surgery type, tumor stage, and age at diagnosis."},{"metadata":{},"cell_type":"markdown","source":"- ####  Statistical Summaries of Clinical Columns in the Dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_clinical_columns= ['age_at_diagnosis', 'lymph_nodes_examined_positive','mutation_count','nottingham_prognostic_index', 'overall_survival_months', 'tumor_size' ]\ncat_clinical_columns = ['chemotherapy', 'cohort', 'neoplasm_histologic_grade','hormone_therapy', 'overall_survival', 'radio_therapy', 'tumor_stage' ]\n# Statistical summary for numerical clinical attributes \nclinical_df[num_clinical_columns].describe(). T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Statistical summary for categorical clinical attributes \ncat_clinical_columns.extend(clinical_df.select_dtypes(include=['object']).columns.tolist())\nclinical_df[cat_clinical_columns].astype('category').describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#statistics for the no treatment group and comparison with the baseline\nno_treatment = clinical_df[(clinical_df['chemotherapy']==0) & (clinical_df['hormone_therapy']==0) & (clinical_df['radio_therapy']==0)]\nprint(\"Number of patients who had no treatment: \" , no_treatment.shape[0])\nprint(\"Proportion of survival in this group: \" , (\"%.3f\" %np.mean(no_treatment[\"overall_survival\"])))\nprint(\"Baseline Proportion of survival in all groups: \", (\"%.3f\" %np.mean(clinical_df[\"overall_survival\"])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- #### What are the characteristics of the average member of the population?"},{"metadata":{"trusted":true},"cell_type":"code","source":"#what the average patient looks like\nprint(\"Mean age: \" + \"%.3f\" %np.mean(clinical_df['age_at_diagnosis']))\nprint(\"Most occurring tumour stage: \" , stats.mode(clinical_df['tumor_stage'])[0][0].astype(int))\nprint(\"Most occurring histopathological type: \" , stats.mode(clinical_df['neoplasm_histologic_grade'])[0][0].astype(int))\nprint(\"Mean tumour diameter: \" + \"%.3f\" %np.mean(clinical_df['tumor_size']))\nprint(\"Probability of survival: \"+ \"%.3f\" %(clinical_df[\"overall_survival\"].value_counts()/clinical_df[\"overall_survival\"].count()).iloc[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The average breast cancer patient in the dataset is a 61-year-old women with a stage 2 tumor with 2 lymph nodes examined positive, with a mean tumor size of 26 mm. The patient has a probability of 76% of not having chemotherapy as a treatment, but only hormonal and radiotherapy with surgery."},{"metadata":{},"cell_type":"markdown","source":"- #### Number of outliers in each clinical feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding number of outliers in each column\nQ1 = clinical_df.quantile(0.25)\nQ3 = clinical_df.quantile(0.75)\nIQR = Q3 - Q1\n((clinical_df < (Q1 - 1.5 * IQR)) | (clinical_df > (Q3 + 1.5 * IQR))).sum().sort_values(ascending = False).head(7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a name=\"genetic\"></a>\n### b) Relationship between genetic attributes and outcomes"},{"metadata":{"jupyter":{"outputs_hidden":false},"trusted":true},"cell_type":"code","source":"# dropping mutations\ngenetic_features_to_drop = df.columns[520:]\ngenetic_df = df.drop(genetic_features_to_drop, axis=1)\n# droping clinical data\ngenetic_features_to_drop = genetic_df.columns[4:35]\ngenetic_df = genetic_df.drop(genetic_features_to_drop, axis=1)\ngenetic_df = genetic_df.drop(['age_at_diagnosis','type_of_breast_surgery', 'cancer_type'], axis=1)\ngenetic_df = genetic_df.iloc [:,:-174]\ngenetic_df['overall_survival']= df['overall_survival']\n\ngenetic_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding Maximum values and std in each column, std is always 1 because the datapoints are z-scores\nmax_values = genetic_df.max()\nstd = genetic_df.std(axis = 0, skipna = True)\nmax_data = pd.concat([max_values, std], axis = 1, keys = ['max_values', 'std'])\nmax_data.sort_values(by='max_values', ascending = False).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding minimum values and std in each column, std is always 1 because the datapoints are z-scores\nmin_values = genetic_df.min()\nstd = genetic_df.std(axis = 0, skipna = True)\nmin_data = pd.concat([min_values, std], axis = 1, keys = ['min_values', 'std'])\nmin_data.sort_values(by='min_values', ascending = True).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizing the mRNA values in a heatmap.\nfig, axs = plt.subplots(figsize = (17, 10)) \nsns.heatmap(genetic_df.drop(['patient_id','overall_survival'], axis=1), ax = axs, cmap = sns.diverging_palette(180, 10, as_cmap = True))\nplt.title('Gene Expression Heatmap')\n\n# fix for mpl bug that cuts off top/bottom of seaborn viz\nb, t = plt.ylim() # discover the values for bottom and top\nb += 0.5 # Add 0.5 to the bottom\nt -= 0.5 # Subtract 0.5 from the top\nplt.ylim(b, t) # update the ylim(bottom, top) values\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'axes.titlesize':'10',\n          'xtick.labelsize':'9',\n          'ytick.labelsize':'9'}\nmatplotlib.rcParams.update(params)\n#plt.subplots_adjust(hspace=0.5) \ngenetic_df.drop(['patient_id','overall_survival'], axis=1).iloc[:,:9].hist(figsize=(15,8), color=color_hist)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (20, 25))\nj = 0\n\ngene_list = ['rab25', 'eif5a2', 'pik3ca', 'kit', 'fgf1', 'myc', 'egfr', 'notch3', 'kras', 'akt1', 'erbb2', 'pik3r1', 'ccne1', 'akt2', 'aurka']\nfor i in genetic_df.drop(['patient_id'], axis=1).loc[:,gene_list].columns:\n    plt.subplot(6, 4, j+1)\n    j += 1\n    sns.distplot(genetic_df[i][genetic_df['overall_survival']==0], color='g', label = 'survived')\n    sns.distplot(genetic_df[i][genetic_df['overall_survival']==1], color='r', label = 'died')\n    plt.legend(loc='best')\nfig.suptitle('Clinical Data Analysis')\nfig.tight_layout()\nfig.subplots_adjust(top=0.95)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution of data in the two classes of survival are very similar with few outliers in some genes. "},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Maximum value possible in genetic data:', genetic_df.drop(['patient_id','overall_survival'], axis = 1).max().max())\nprint('Minimum value possible in genetic data:', genetic_df.drop(['patient_id','overall_survival'], axis = 1).min().min())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- #### Number of outliers in the top 10 genetic features"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding number of outliers in each column\nQ1 = genetic_df.quantile(0.25)\nQ3 = genetic_df.quantile(0.75)\nIQR = Q3 - Q1\n((genetic_df < (Q1 - 1.5 * IQR)) | (genetic_df > (Q3 + 1.5 * IQR))).sum().sort_values(ascending = False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- #### Visualize Correlation of between the genetic Attributes and outcome"},{"metadata":{"trusted":true},"cell_type":"code","source":"#how varied are genes and how well do they correlate with eventdeath?\nfig, ax = plt.subplots(figsize=(10,4))\n\n#plot histogram of variation using standard deviation as a measure\ncorrs=[]\nfor col in genetic_df.drop(['patient_id'], axis = 1).columns:\n    corr = genetic_df[[col,'overall_survival']].corr()['overall_survival'][col]\n    corrs.append(corr)\n\ncorrs.pop(-1)\nax.hist(corrs,  bins=25, color = color_hist)\nax.set_xlabel(\"Correlation\")\nax.set_ylabel(\"Number of genes\")\nax.set_title(\"Histogram of Correlation of genes with the survival\", size=16)\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Maximum Correlation: \" + \"%.3f\" %max(corrs))\nprint(\"Minimum Correlation: \" + \"%.3f\" %min(corrs))\nprint(\"Mean Correlation: \" + \"%.3f\" %np.mean(corrs))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The correlation between our target and the genetic features shows that most features do not actually correlate."},{"metadata":{},"cell_type":"markdown","source":"<a name=\"mutation\"></a>\n### c) Relationship between genetic mutation attributes and outcomes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# droping clinical and genetic data\nmutation_features_to_drop = df.columns[4:520]\nmutation_df = df.drop(mutation_features_to_drop, axis=1)\nmutation_df = mutation_df.drop(['age_at_diagnosis','type_of_breast_surgery', 'cancer_type'], axis=1)\n\n# if there is a mutation=1, no-mutation=0\nfor column in mutation_df.columns[1:]:\n    mutation_df[column]=pd.to_numeric(mutation_df[column], errors='coerce').fillna(1).astype(int)\n\nmutation_df.insert(loc=1 , column='overall_survival', value=df['overall_survival'])\n\nmutation_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some genes had much more mutations than other genes. For example: PIK3CA (coding mutations in 40.1% of the samples) and TP53 (35.4%) dominated the mutation landscape. Only five other genes harboured coding mutations in at least 10% of the samples: MUC16 (16.8%); AHNAK2 (16.2%); SYNE1 (12.0%); KMT2C (also known as MLL3; 11.4%) and GATA3 (11.1%)."},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot histogram of variation using standard deviation as a measure\nfig, ax = plt.subplots(figsize=(10,4))\ncorrs=[]\nfor col in mutation_df.drop(['patient_id'], axis = 1).columns:\n    corr = mutation_df[[col,'overall_survival']].corr()['overall_survival'][col]\n    corrs.append(corr)\n    \ncorrs.pop(0)\nax.hist(corrs,  bins=25, color = color_hist)\nax.set_xlabel(\"Correlation\")\nax.set_ylabel(\"Number of genes\")\nax.set_title(\"Histogram of Correlation of genes with the survival\", size=16)\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Maximum Correlation: \" + \"%.3f\" %max(corrs))\nprint(\"Minimum Correlation: \" + \"%.3f\" %min(corrs))\nprint(\"Mean Correlation: \" + \"%.3f\" %np.mean(corrs))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No correlation at all between survival and mutations, as we changed the mutation to 0s and 1s instead of 0s if there is no mutations and the kind of mutation if there is a mutation. We decided to exclude the mutations from the mdeoling part for now, and maybe include it later when we analyse them in more detail. "},{"metadata":{},"cell_type":"markdown","source":"<a name=\"Preprocessing_and_Modeling\"></a>\n## Preprocessing and Modeling\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"BOLD = '\\033[1m'\nEND = '\\033[0m'\n# using a stratfied k fold because we need the distribution of the to classes in all of the folds to be the same.\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Baseline accuracy:' )\nprint(df[\"overall_survival\"].value_counts()/df[\"overall_survival\"].count())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### a) Classification with only clinical attributes"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_columns = clinical_df.select_dtypes(include=['object']).columns.tolist()\nunwanted_columns = ['patient_id','death_from_cancer' ]\ncategorical_columns = [ele for ele in categorical_columns if ele not in unwanted_columns] \n# Getting dummies for all categorical columns\ndummies_clinical_df = pd.get_dummies(clinical_df.drop('patient_id',axis=1 ), columns= categorical_columns, dummy_na=True)\ndummies_clinical_df.dropna(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data splitting\nX = dummies_clinical_df.drop(['death_from_cancer', 'overall_survival'], axis=1)\ny = dummies_clinical_df['overall_survival']\n# using stratify for y because we need the distribution of the two classes to be equal in train and test sets.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify = y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Some helpful functions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_metrics(model, kfold, X_train, X_test, y_train, y_test):\n    \n    model.fit(X_train, y_train)\n\n    #metrics\n    results = cross_val_score(model, X_train, y_train, cv = kfold)\n    print(\"CV scores: \", results); print(\"CV Standard Deviation: \", results.std()); print();\n    print('CV Mean score: ', results.mean()); \n    print('Train score:   ', model.score(X_train, y_train))\n    print('Test score:    ', model.score(X_test, y_test))\n    \n    pred = model.predict(X_test)\n    # CODE HERE PLEASE\n    print()\n    print('Confusion Matrix: ')\n    print(confusion_matrix(y_test, pred))\n    print('Classification Report:  ')\n    print(classification_report(y_test, pred))\n    train_score =  model.score(X_train, y_train)\n    test_score = model.score(X_test, y_test)\n    test_pred = model.predict(X_test)\n    return test_pred, test_score, results.mean()\n\ndef basic_classifiers (X_train, X_test, y_train, y_test, kfold):\n    BOLD = '\\033[1m'\n    END = '\\033[0m'\n    \n    # Scaling \n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n    \n    ######################################################################################################  K Neighbors Classifier model\n    \n    params = {\n    \"n_neighbors\" : [5,15,25,30,35,40, 100],\n    \"weights\" : [\"uniform\" , \"distance\"]\n    }\n    print(); print(BOLD + 'K Neighbors Classifier Model:' + END)\n    knn= GridSearchCV(KNeighborsClassifier(), params, n_jobs=-1, cv=4)\n    knn_pred, knn_test, knn_train = model_metrics(knn, kfold, X_train, X_test, y_train, y_test)\n    \n    ###################################################################################################### Logistic Regression\n    params = {\n    \"penalty\": [\"l1\", \"l2\"],\n    \"C\": np.logspace(-2,4,100)\n    }\n    print(); print(BOLD + 'Logistic Regression Model:' + END)\n    logistic_regression = GridSearchCV(LogisticRegression(random_state=42), params, n_jobs=-1, cv=4)\n    lg_pred, lg_test, lg_train = model_metrics(logistic_regression, kfold, X_train, X_test, y_train, y_test)\n    \n    ###################################################################################################### Decision Tree\n    \n    print(); print(BOLD + 'Decision Tree Classifier Model:' + END)\n    decision_tree = DecisionTreeClassifier(random_state=42)\n    dt_pred, dt_test, dt_train = model_metrics(decision_tree, kfold, X_train, X_test, y_train, y_test)\n    \n    ###################################################################################################### Random Forest Classifier\n    \n    print(); print(BOLD + 'Random Forest Classifier Model:' + END)\n    random_forest = RandomForestClassifier(random_state=42)\n    rf_pred, rf_test, rf_train = model_metrics(random_forest, kfold, X_train, X_test, y_train, y_test)\n    \n    ###################################################################################################### Extra Trees Classifier\n   \n    print(); print(BOLD + 'Extra Trees Classifier Model:' + END)\n    extra_trees = ExtraTreesClassifier(random_state=42)\n    et_pred, et_test, et_train = model_metrics(extra_trees, kfold, X_train, X_test, y_train, y_test)\n    \n    ###################################################################################################### AdaBoost Classifier\n    \n    print(); print(BOLD + 'AdaBoost Classifier Model:' + END)\n    ada_boost = AdaBoostClassifier(random_state=42)\n    ab_pred, ab_test, ab_train = model_metrics(ada_boost, kfold, X_train, X_test, y_train, y_test)\n    \n    ###################################################################################################### SVC Classifier\n    \n    print(); print(BOLD + 'SVC Classifier Model:' + END)\n    svc = SVC(random_state=42)\n    svc_pred, svc_test, svc_train = model_metrics(svc, kfold, X_train, X_test, y_train, y_test)\n\n    fig, (ax1, ax2) = plt.subplots(ncols=2, nrows=1, figsize=(15,6))\n\n    \n    #bar chart of accuracy scores\n    inds = range(1,8)\n    labels = [\"KNN\", \"Logistic Regression\", \"Decision Tree\", \"Random Forest\",'Extra Trees', 'AdaBoost', 'SVC' ]\n    scores_all = [knn_train, lg_train, dt_train, rf_train, et_train, ab_train, svc_train]\n    scores_predictive = [knn_test, lg_test, dt_test, rf_test, et_test, ab_test, svc_test]\n    \n    ax1.bar(inds, scores_all, color=sns.color_palette(color)[5], alpha=0.3, hatch=\"x\", edgecolor=\"none\",label=\"CrossValidation Set\")\n    ax1.bar(inds, scores_predictive, color=sns.color_palette(color)[0], label=\"Testing set\")\n    ax1.set_ylim(0.4, 1)\n    ax1.set_ylabel(\"Accuracy score\")\n    ax1.axhline(0.5793, color=\"black\", linestyle=\"--\")\n    ax1.set_title(\"Accuracy scores for basic models\", fontsize=17)\n    ax1.set_xticks(range(1,8))\n    ax1.set_xticklabels(labels, size=12, rotation=40, ha=\"right\")\n    ax1.legend()\n\n    labels = [\"KNN\", \"Logistic Regression\", \"Decision Tree\", \"Random Forest\",'Extra Trees', 'AdaBoost', 'SVC' ]\n    for label, pred in zip(labels, [knn_pred, lg_pred, dt_pred, rf_pred, et_pred, ab_pred, svc_pred]):\n        fpr, tpr, threshold = roc_curve(y_test.values, pred)\n        roc_auc = auc(fpr, tpr)\n        ax2.plot(fpr, tpr, label=label+' (area = %0.2f)' % roc_auc, linewidth=2)\n    ax2.plot([0, 1], [0, 1], 'k--', linewidth=2)\n    ax2.set_xlim([-0.05, 1.0])\n    ax2.set_ylim([-0.05, 1.05])\n    ax2.set_xlabel('False Positive Rate')\n    ax2.set_ylabel('True Positive Rate')\n    ax2.legend(loc=\"lower right\", prop={'size': 12})\n    ax2.set_title(\"Roc curve for for basic models\", fontsize=17)\n\n    plt.show()\n    \n    \n# a function that takes a dataframe and plots histograms for all columns \ndef subplot_histograms(dataframe, list_of_columns, list_of_titles, list_of_xlabels, big_title_name):\n    \n    nrows = int(np.ceil(len(list_of_columns)/3)) # Makes sure you have enough rows\n    fig, ax = plt.subplots(ncols=3,nrows=nrows, figsize=(15, 10)) # You'll want to specify your figsize\n    fig.suptitle(big_title_name, fontsize=15)\n    ax = ax.ravel() # Ravel turns a matrix into a vector, which is easier to iterate\n    for i, column in enumerate(list_of_columns): # Gives us an index value to get into all our lists\n        ax[i].hist(dataframe[column].dropna(), color= color_hist ) # feel free to add more settings\n        #ax[i].set_xlabel(list_of_xlabels[i])\n        ax[i].set_ylabel('Frequency')\n        ax[i].set_title(list_of_titles[i]) # Set titles, labels, etc here for each subplot    \n    plt.show()\n    \n    \n# a function that takes a dataframe and plots barplot for all columns \ndef subplot_bargraph(dataframe, list_of_columns, list_of_titles, list_of_xlabels, big_title_name):\n    \n    nrows = int(np.ceil(len(list_of_columns)/3)) # Makes sure you have enough rows\n    fig, ax = plt.subplots(ncols=3,nrows=nrows, figsize=(15, 10)) # You'll want to specify your figsize\n    fig.suptitle(big_title_name, fontsize=20)\n    ax = ax.ravel() # Ravel turns a matrix into a vector, which is easier to iterate\n    for i, column in enumerate(list_of_columns): # Gives us an index value to get into all our lists\n        sns.countplot(dataframe[column].dropna(), color= color_hist, ax=ax[i], hue=dataframe['eventdeath']) # feel free to add more settings\n        #ax[i].set_xlabel(list_of_xlabels[i])\n        ax[i].set_xlabel('')\n        ax[i].set_ylabel('Frequency')\n        ax[i].set_title(list_of_titles[i]) # Set titles, labels, etc here for each subplot    \n    plt.show()        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"basic_classifiers( X_train, X_test, y_train, y_test, kfold)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic regression model preformed the best with accuracy of 0.777 and AUC of 0.777, KNN having the lowest accuracy of 0.64, and AUC of 0.62"},{"metadata":{"trusted":true},"cell_type":"code","source":"def RandomForest_GridSearch(X_train, X_test, y_train, y_test, kfold):\n    BOLD = '\\033[1m'\n    END = '\\033[0m'\n    print(); print(BOLD + 'Grid Search with Random Forest Classifier Model:' + END)\n    #kfold=5\n    rf_params = {\n        #'n_estimators': [10, 50, 100, 150, 200, 250],\n        'max_features':[2, 3, 5, 7, 8],\n        #'max_depth': [1, 2, 3, 4, 5, 8],\n        #'criterion':['gini', 'entropy'],\n    }\n\n    random_forest = RandomForestClassifier(n_estimators=100)\n    gs = GridSearchCV(random_forest, param_grid=rf_params, cv=5, verbose = 1)\n    gs_pred, gs_test, gs_train = model_metrics(gs, kfold, X_train, X_test, y_train, y_test)\n    \n    return gs.best_estimator_, gs_pred, gs_test, gs_train\n\n\ndef ExtraTrees_GridSearch(X_train, X_test, y_train, y_test, kfold):\n    BOLD = '\\033[1m'\n    END = '\\033[0m'\n    print(); print(BOLD + 'Grid Search with Extra Trees Model:' + END)\n    # Scaling \n      \n    rf_params = {\n        #'n_estimators': [10, 100, 400, 800, 1100, 1850],\n        #'max_features':['auto'],\n        'max_depth': [1, 2, 3, 4, 5, 8],\n        #'criterion':['gini'],\n    }\n\n    extra_trees = ExtraTreesClassifier(n_estimators=100)    \n    gs = GridSearchCV(extra_trees, param_grid=rf_params, cv=5, verbose = 1)\n    gs_pred, gs_test, gs_train = model_metrics(gs, kfold, X_train, X_test, y_train, y_test)\n    \n    return gs.best_estimator_, gs_pred, gs_test, gs_train\n\ndef RF_ET_GridSearch (X_train, X_test, y_train, y_test, kfold):\n    rf_gs_best_estimator, rf_pred, rf_test, rf_train = RandomForest_GridSearch(X_train, X_test, y_train, y_test, kfold)\n    et_gs_best_estimator, et_pred, et_test, et_train = ExtraTrees_GridSearch(X_train, X_test, y_train, y_test, kfold)\n    \n    fig, (ax1, ax2) = plt.subplots(ncols=2, nrows=1, figsize=(13,6))\n\n    fig.suptitle(\"Random Forest and Extra Trees with Grid Search\", fontsize=16)\n    #bar chart of accuracy scores\n    inds = range(1,3)\n    labels = [\"Random Forest\", \"Extra Trees\" ]\n    scores_all = [rf_train, et_train]\n    scores_predictive = [rf_test, et_test]\n    \n    ax1.bar(inds, scores_all, color=sns.color_palette(color)[5], alpha=0.3, hatch=\"x\", edgecolor=\"none\",label=\"CrossValidation Set\") #\n    ax1.bar(inds, scores_predictive, color=sns.color_palette(color)[0], label=\"Testing set\")\n    ax1.set_ylim(0.4, 1)\n    ax1.set_ylabel(\"Accuracy score\")\n    ax1.axhline(0.5793, color=\"black\", linestyle=\"--\")\n    ax1.set_title(\"Accuracy scores\", fontsize=17)\n    ax1.set_xticks(range(1,3))\n    ax1.set_xticklabels(labels, size=14)\n    ax1.legend()\n\n    labels = [\"Random Forest\", \"Extra Trees\" ]\n    for label, pred in zip(labels, [rf_pred, et_pred]):\n        fpr, tpr, threshold = roc_curve(y_test.values, pred)\n        roc_auc = auc(fpr, tpr)\n        ax2.plot(fpr, tpr, label=label+' (area = %0.2f)' % roc_auc, linewidth=2)\n    ax2.plot([0, 1], [0, 1], 'k--', linewidth=2)\n    ax2.set_xlim([-0.05, 1.0])\n    ax2.set_ylim([-0.05, 1.05])\n    ax2.set_xlabel('False Positive Rate')\n    ax2.set_ylabel('True Positive Rate')\n    ax2.legend(loc=\"lower right\", prop={'size': 14})\n    ax2.set_title(\"Roc curve\", fontsize=17)\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#RF_ET_GridSearch (X_train, X_test, y_train, y_test, kfold)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will not be running the part for Random Forest and Extra Trees with greid search, as it takes a long time, but the results were as follows:\n\n\n* Grid Search with Random Forest Classifier Model Accuracy = 0.7546296296296297\n\n* Grid Search with Extra Trees Classifier Model Accuracy = 0.6875"},{"metadata":{},"cell_type":"markdown","source":"- #### Predicting without the time related column (overall_survival_months)"},{"metadata":{},"cell_type":"markdown","source":"For a first time diagnosed patient, there will be no information about survival time, so we want to check if we can predited survival without survival time duration"},{"metadata":{"trusted":true},"cell_type":"code","source":"# data splitting\nX_no_time = dummies_clinical_df.drop(['death_from_cancer', 'overall_survival','overall_survival_months' ], axis=1)\ny_no_time = dummies_clinical_df['overall_survival']\n\nX_train_no_time, X_test_no_time, y_train_no_time, y_test_no_time = train_test_split(X_no_time, y_no_time, test_size=0.33, random_state=42, stratify = y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"basic_classifiers( X_train_no_time, X_test_no_time, y_train_no_time, y_test_no_time, kfold)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Without survival time duration, the prediction scores decreases segneficantly compared to prediction with survival time duration"},{"metadata":{"trusted":false},"cell_type":"code","source":"#RF_ET_GridSearch (X_train_no_time, X_test_no_time, y_train_no_time, y_test_no_time, kfold)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Grid Search with Random Forest Classifier Model Accuracy = 0.6597222222222222\n\n* Grid Search with Extra Trees Classifier Model Accuracy = 0.6550925925925926"},{"metadata":{},"cell_type":"markdown","source":" ### b) Classification with only genetic attributes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# data splitting\nX = genetic_df.drop(['patient_id', 'overall_survival'], axis=1)\ny = genetic_df['overall_survival']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify = y)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"basic_classifiers( X_train, X_test, y_train, y_test, kfold)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When we train the models only with genetic data, we can see that the performance is bad in most of the models, with decision tree falling under the base line of 57%."},{"metadata":{"trusted":false},"cell_type":"code","source":"#RF_ET_GridSearch (X_train, X_test, y_train, y_test, kfold)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Grid Search with Random Forest Classifier Model Accuracy = 0.6518282988871225\n\n* Grid Search with Extra Trees Classifier Model Accuracy = 0.6518282988871225"},{"metadata":{},"cell_type":"markdown","source":" ### c) Classification with all attributes"},{"metadata":{"trusted":true},"cell_type":"code","source":"features_to_drop = df.columns[520:]\ndf = df.drop(features_to_drop, axis=1)\nall_categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\nunwanted_columns = ['patient_id','death_from_cancer' ]\nall_categorical_columns = [ele for ele in all_categorical_columns if ele not in unwanted_columns] \ndummies_df = pd.get_dummies(df.drop('patient_id',axis=1 ), columns= all_categorical_columns, dummy_na=True)\ndummies_df.dropna(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data splitting\nX = dummies_df.drop( ['death_from_cancer','overall_survival'], axis=1)\ny = dummies_df['overall_survival']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"basic_classifiers( X_train, X_test, y_train, y_test, kfold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#RF_ET_GridSearch (X_train, X_test, y_train, y_test, kfold)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Grid Search with Random Forest Classifier Model Accuracy = 0.6755725190839694\n\n* Grid Search with Extra Trees Classifier Model Accuracy = 0.6946564885496184"},{"metadata":{},"cell_type":"markdown","source":"- ### XGBoost: Final try to increase the predictive score\n\nAfter tuning the hyperparamenters, these are the final parameters:\n- max_depth: 5\n- min_child_weight: 3, 1\n- gamma: 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_visualization (labels, scores_all, scores_predictive, pred, title, inds, y_test_sets):\n    fig, (ax1, ax2) = plt.subplots(ncols=2, nrows=1, figsize=(13,6))\n\n    fig.suptitle(title, fontsize=16)\n    #bar chart of accuracy scores\n\n    ax1.bar(inds, scores_all, color=sns.color_palette(color)[5], alpha=0.3, hatch=\"x\", edgecolor=\"none\",label=\"CrossValidation Set\") #\n    ax1.bar(inds, scores_predictive, color=sns.color_palette(color)[0], label=\"Testing set\")\n    ax1.set_ylim(0.4, 1)\n    ax1.set_ylabel(\"Accuracy score\")\n    ax1.axhline(0.5793, color=\"black\", linestyle=\"--\")\n    ax1.axhline(0.7758346581875993, color=\"red\", linestyle=\"--\")\n    ax1.set_title(\"Accuracy scores\", fontsize=17)\n    ax1.set_xticks(inds)\n    ax1.set_xticklabels(labels, size=14)\n    ax1.legend()\n    \n\n\n    for label, pred, y_test in zip(labels, pred, y_test_sets):\n        fpr, tpr, threshold = roc_curve(y_test.values, pred)\n        roc_auc = auc(fpr, tpr)\n        ax2.plot(fpr, tpr, label=label+' (area = %0.2f)' % roc_auc, linewidth=2)\n    ax2.plot([0, 1], [0, 1], 'k--', linewidth=2)\n    ax2.set_xlim([-0.05, 1.0])\n    ax2.set_ylim([-0.05, 1.05])\n    ax2.set_xlabel('False Positive Rate')\n    ax2.set_ylabel('True Positive Rate')\n    ax2.legend(loc=\"lower right\", prop={'size': 14})\n    ax2.set_title(\"Roc curve\", fontsize=17)\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- ### XGBoost for clinical features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_columns = clinical_df.select_dtypes(include=['object']).columns.tolist()\nunwanted_columns = ['patient_id','death_from_cancer' ]\ncategorical_columns = [ele for ele in categorical_columns if ele not in unwanted_columns] \ndummies_clinical_df = pd.get_dummies(clinical_df.drop('patient_id',axis=1 ), columns= categorical_columns, dummy_na=True)\n\n# data splitting\nX = dummies_clinical_df.drop(['death_from_cancer', 'overall_survival'], axis=1)\ny = dummies_clinical_df['overall_survival']\n\nX_train, X_test, y_train, y_test_c = train_test_split(X, y, test_size=0.33, random_state=42, stratify = y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb1 = XGBClassifier(\n learning_rate =0.1,\n n_estimators=1000,\n max_depth=5,\n min_child_weight=1,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\n\nclinical_xgb1_pred_, clinical_xgb1_test_score, clinical_xgb1_cv_score = model_metrics(xgb1, kfold, X_train, X_test, y_train, y_test_c)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- ### XGBoost for genatic features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"genetic_features_to_drop = df.columns[4:35]\ngenetic_df = df.drop(genetic_features_to_drop, axis=1)\ngenetic_df = genetic_df.drop(['age_at_diagnosis','type_of_breast_surgery', 'cancer_type'], axis=1)\ngenetic_df = genetic_df.iloc [:,:-174]\ngenetic_df['overall_survival']= df['overall_survival']\n# data splitting\nX = genetic_df.drop(['patient_id', 'overall_survival'], axis=1)\ny = genetic_df['overall_survival']\n\nX_train, X_test, y_train, y_test_g = train_test_split(X, y, test_size=0.33, random_state=42, stratify = y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb1 = XGBClassifier(\n learning_rate =0.1,\n n_estimators=1000,\n max_depth=5,\n min_child_weight=1,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\n\ngene_xgb1_pred_, gene_xgb1_test_score, gene_xgb1_cv_score = model_metrics(xgb1, kfold, X_train, X_test, y_train, y_test_g)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- ### XGBoost for all Features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\nunwanted_columns = ['patient_id','death_from_cancer' ]\nall_categorical_columns = [ele for ele in all_categorical_columns if ele not in unwanted_columns] \ndumm_df = pd.get_dummies(df.drop('patient_id',axis=1 ), columns= all_categorical_columns, dummy_na=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data splitting\nX = dumm_df.drop(['death_from_cancer','overall_survival'], axis=1)\ny = dumm_df['overall_survival']\n\nX_train, X_test, y_train, y_test_all = train_test_split(X, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb1 = XGBClassifier(\n learning_rate =0.1,\n n_estimators=1000,\n max_depth=5,\n min_child_weight=1,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\n\ndf_xgb1_pred_, df_xgb1_test_score, df_xgb1_cv_score = model_metrics(xgb1, kfold, X_train, X_test, y_train, y_test_all)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- ### XGBoost for all features with combination of genatic data"},{"metadata":{"trusted":true},"cell_type":"code","source":"combin_geneatic_df = pd.read_csv(\"../input/threshold-005csv/threshold_0.05.csv\", sep=\",\", index_col=\"Unnamed: 0\")\nclinical_df_new = pd.merge(clinical_df, combin_geneatic_df, left_index=True, right_index=True, sort='patient_id', how='outer')\n\ncategorical_columns = clinical_df_new.select_dtypes(include=['object']).columns.tolist()\nunwanted_columns = ['patient_id','death_from_cancer' ]\ncategorical_columns = [ele for ele in categorical_columns if ele not in unwanted_columns] \ndummies_clinical_df = pd.get_dummies(clinical_df_new.drop('patient_id',axis=1 ), columns= categorical_columns, dummy_na=True)\n\n# data splitting\nX_combin = dummies_clinical_df.drop(['death_from_cancer', 'overall_survival'], axis=1)\ny = dummies_clinical_df['overall_survival']\n\nX_train, X_test, y_train, y_test_comp = train_test_split(X_combin, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb1 = XGBClassifier(\n learning_rate =0.1,\n n_estimators=1000,\n max_depth=5,\n min_child_weight=1,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\n\nxgb1_pred, xgb1_test_score, xgb1_cv_score = model_metrics(xgb1, kfold, X_train, X_test, y_train, y_test_comp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title = \"Performance of XGBoost with different subsets of the dataset\"\ninds = range(1,5)\nlabels = [\"Clinical\", \"Genatic\", \"All\" , \"All Combinated\"]\nscores_all = [clinical_xgb1_cv_score, gene_xgb1_cv_score, df_xgb1_cv_score, xgb1_cv_score ]\nscores_predictive = [clinical_xgb1_test_score, gene_xgb1_test_score, df_xgb1_test_score, xgb1_test_score ]\npred = [clinical_xgb1_pred_, gene_xgb1_pred_, df_xgb1_pred_, xgb1_pred]\ny_test_sets = [y_test_c, y_test_g, y_test_all, y_test_comp]\nmodel_visualization (labels, scores_all, scores_predictive, pred, title, inds, y_test_sets)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGBoost preformed very well combared to traditional basic models, and the best model was the one that was trained with all of the features combined with accuracy of 0.779 and AUC of 0.76"},{"metadata":{},"cell_type":"markdown","source":"<a name=\"Conclusions_and_Recommendations\"></a>\n## Conclusions and Recommendations\n\nUsing machine learning models on genetic data has the potential to improve our understanding of cancers and survival prediction. Huge open-source  datasets are available for public to analyze and hopefully get some insights. The model with the best preformace was XGBoost with max_depth=5 and min_child_weight=1 that was trained with the full dataframe with the addition of all of the combination of all genetic data values. The accuacy score was 0.779 and the AUC was 0.76. To enhance this project, increase the number of samples, include mutations and raw genetic data into the modeling part, and maybe try some deep learning models.  \n\n\n"},{"metadata":{},"cell_type":"markdown","source":"<a name=\"References\"></a>\n## References\n\n- [Pereira, B., Chin, S. F., Rueda, O. M., Vollan, H. K. M., Provenzano, E., Bardwell, H. A., ... & Tsui, D. W. (2016). The somatic mutation profiles of 2,433 breast cancers refine their genomic and transcriptomic landscapes. Nature communications, 7(1), 1-16.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4866047/pdf/ncomms11479.pdf)\n- [Bashiri, A., Ghazisaeedi, M., Safdari, R., Shahmoradi, L., & Ehtesham, H. (2017). Improving the prediction of survival in cancer patients by using machine learning techniques: experience of gene expression data: a narrative review. Iranian journal of publ](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5402773/)\n- [Breast Cancer dataset (METABRIC, Nature 2012 & Nat Commun 2016) in CBioPortal](https://www.cbioportal.org/study/summary?id=brca_metabric)\n- [Increasing the resolution on breast cancer – the METABRIC study](https://scienceblog.cancerresearchuk.org/2012/04/18/increasing-the-resolution-on-breast-cancer-the-metabric-study/)\n- [Cerami et al. The cBio Cancer Genomics Portal: An Open Platform for Exploring Multidimensional Cancer Genomics Data. Cancer Discovery. May 2012 2; 401.](https://www.ncbi.nlm.nih.gov/pubmed/22588877)\n- [Gao et al. Integrative analysis of complex cancer genomics and clinical profiles using the cBioPortal. Sci. Signal. 6, pl1 (2013).](https://www.ncbi.nlm.nih.gov/pubmed/23550210)\n- [World Health Organization - Breast cancer](https://www.who.int/cancer/prevention/diagnosis-screening/breast-cancer/en/)\n- [Van't Veer, L. J., Dai, H., Van De Vijver, M. J., He, Y. D., Hart, A. A., Mao, M., ... & Schreiber, G. J. (2002). Gene expression profiling predicts clinical outcome of breast cancer. nature, 415(6871), 530-536.](https://www.ncbi.nlm.nih.gov/pubmed/11823860)\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}