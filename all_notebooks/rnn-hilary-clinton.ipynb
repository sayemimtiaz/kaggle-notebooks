{"cells":[{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cbb499e61593a47780d85df6c715a3ba705d2d3c"},"cell_type":"code","source":"# encoding: UTF-8\n# Copyright 2017 Google.com\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport glob\nimport sys\n\n# size of the alphabet that we work with\nALPHASIZE = 98\n\n\n# Specification of the supported alphabet (subset of ASCII-7)\n# 10 line feed LF\n# 32-64 numbers and punctuation\n# 65-90 upper-case letters\n# 91-97 more punctuation\n# 97-122 lower-case letters\n# 123-126 more punctuation\ndef convert_from_alphabet(a):\n    \"\"\"Encode a character\n    :param a: one character\n    :return: the encoded value\n    \"\"\"\n    if a == 9:\n        return 1\n    if a == 10:\n        return 127 - 30  # LF\n    elif 32 <= a <= 126:\n        return a - 30\n    else:\n        return 0  # unknown\n\n\n# encoded values:\n# unknown = 0\n# tab = 1\n# space = 2\n# all chars from 32 to 126 = c-30\n# LF mapped to 127-30\ndef convert_to_alphabet(c, avoid_tab_and_lf=False):\n    \"\"\"Decode a code point\n    :param c: code point\n    :param avoid_tab_and_lf: if True, tab and line feed characters are replaced by '\\'\n    :return: decoded character\n    \"\"\"\n    if c == 1:\n        return 32 if avoid_tab_and_lf else 9  # space instead of TAB\n    if c == 127 - 30:\n        return 92 if avoid_tab_and_lf else 10  # \\ instead of LF\n    if 32 <= c + 30 <= 126:\n        return c + 30\n    else:\n        return 0  # unknown\n\n\ndef encode_text(s):\n    \"\"\"Encode a string.\n    :param s: a text string\n    :return: encoded list of code points\n    \"\"\"\n    return list(map(lambda a: convert_from_alphabet(ord(a)), s))\n\n\ndef decode_to_text(c, avoid_tab_and_lf=False):\n    \"\"\"Decode an encoded string.\n    :param c: encoded list of code points\n    :param avoid_tab_and_lf: if True, tab and line feed characters are replaced by '\\'\n    :return:\n    \"\"\"\n    return \"\".join(map(lambda a: chr(convert_to_alphabet(a, avoid_tab_and_lf)), c))\n\n\ndef sample_from_probabilities(probabilities, topn=ALPHASIZE):\n    \"\"\"Roll the dice to produce a random integer in the [0..ALPHASIZE] range,\n    according to the provided probabilities. If topn is specified, only the\n    topn highest probabilities are taken into account.\n    :param probabilities: a list of size ALPHASIZE with individual probabilities\n    :param topn: the number of highest probabilities to consider. Defaults to all of them.\n    :return: a random integer\n    \"\"\"\n    p = np.squeeze(probabilities)\n    p[np.argsort(p)[:-topn]] = 0\n    p = p / np.sum(p)\n    return np.random.choice(ALPHASIZE, 1, p=p)[0]\n\n\ndef rnn_minibatch_sequencer(raw_data, batch_size, sequence_size, nb_epochs):\n    \"\"\"\n    Divides the data into batches of sequences so that all the sequences in one batch\n    continue in the next batch. This is a generator that will keep returning batches\n    until the input data has been seen nb_epochs times. Sequences are continued even\n    between epochs, apart from one, the one corresponding to the end of raw_data.\n    The remainder at the end of raw_data that does not fit in an full batch is ignored.\n    :param raw_data: the training text\n    :param batch_size: the size of a training minibatch\n    :param sequence_size: the unroll size of the RNN\n    :param nb_epochs: number of epochs to train on\n    :return:\n        x: one batch of training sequences\n        y: on batch of target sequences, i.e. training sequences shifted by 1\n        epoch: the current epoch number (starting at 0)\n    \"\"\"\n    data = np.array(raw_data)\n    data_len = data.shape[0]\n    # using (data_len-1) because we must provide for the sequence shifted by 1 too\n    nb_batches = (data_len - 1) // (batch_size * sequence_size)\n    assert nb_batches > 0, \"Not enough data, even for a single batch. Try using a smaller batch_size.\"\n    rounded_data_len = nb_batches * batch_size * sequence_size\n    xdata = np.reshape(data[0:rounded_data_len], [batch_size, nb_batches * sequence_size])\n    ydata = np.reshape(data[1:rounded_data_len + 1], [batch_size, nb_batches * sequence_size])\n\n    for epoch in range(nb_epochs):\n        for batch in range(nb_batches):\n            x = xdata[:, batch * sequence_size:(batch + 1) * sequence_size]\n            y = ydata[:, batch * sequence_size:(batch + 1) * sequence_size]\n            x = np.roll(x, -epoch, axis=0)  # to continue the text from epoch to epoch (do not reset rnn state!)\n            y = np.roll(y, -epoch, axis=0)\n            yield x, y, epoch\n\n\ndef find_book(index, bookranges):\n    return next(\n        book[\"name\"] for book in bookranges if (book[\"start\"] <= index < book[\"end\"]))\n\n\ndef find_book_index(index, bookranges):\n    return next(\n        i for i, book in enumerate(bookranges) if (book[\"start\"] <= index < book[\"end\"]))\n\n\ndef print_learning_learned_comparison(X, Y, losses, bookranges, batch_loss, batch_accuracy, epoch_size, index, epoch):\n    \"\"\"Display utility for printing learning statistics\"\"\"\n    print()\n    # epoch_size in number of batches\n    batch_size = X.shape[0]  # batch_size in number of sequences\n    sequence_len = X.shape[1]  # sequence_len in number of characters\n    start_index_in_epoch = index % (epoch_size * batch_size * sequence_len)\n    for k in range(batch_size):\n        index_in_epoch = index % (epoch_size * batch_size * sequence_len)\n        decx = decode_to_text(X[k], avoid_tab_and_lf=True)\n        decy = decode_to_text(Y[k], avoid_tab_and_lf=True)\n        bookname = find_book(index_in_epoch, bookranges)\n        formatted_bookname = \"{: <10.40}\".format(bookname)  # min 10 and max 40 chars\n        epoch_string = \"{:4d}\".format(index) + \" (epoch {}) \".format(epoch)\n        loss_string = \"loss: {:.5f}\".format(losses[k])\n        print_string = epoch_string + formatted_bookname + \" │ {} │ {} │ {}\"\n        print(print_string.format(decx, decy, loss_string))\n        index += sequence_len\n    # box formatting characters:\n    # │ \\u2502\n    # ─ \\u2500\n    # └ \\u2514\n    # ┘ \\u2518\n    # ┴ \\u2534\n    # ┌ \\u250C\n    # ┐ \\u2510\n    format_string = \"└{:─^\" + str(len(epoch_string)) + \"}\"\n    format_string += \"{:─^\" + str(len(formatted_bookname)) + \"}\"\n    format_string += \"┴{:─^\" + str(len(decx) + 2) + \"}\"\n    format_string += \"┴{:─^\" + str(len(decy) + 2) + \"}\"\n    format_string += \"┴{:─^\" + str(len(loss_string)) + \"}┘\"\n    footer = format_string.format('INDEX', 'BOOK NAME', 'TRAINING SEQUENCE', 'PREDICTED SEQUENCE', 'LOSS')\n    print(footer)\n    # print statistics\n    batch_index = start_index_in_epoch // (batch_size * sequence_len)\n    batch_string = \"batch {}/{} in epoch {},\".format(batch_index, epoch_size, epoch)\n    stats = \"{: <28} batch loss: {:.5f}, batch accuracy: {:.5f}\".format(batch_string, batch_loss, batch_accuracy)\n    print()\n    print(\"TRAINING STATS: {}\".format(stats))\n\n\nclass Progress:\n    \"\"\"Text mode progress bar.\n    Usage:\n            p = Progress(30)\n            p.step()\n            p.step()\n            p.step(start=True) # to restart form 0%\n    The progress bar displays a new header at each restart.\"\"\"\n    def __init__(self, maxi, size=100, msg=\"\"):\n        \"\"\"\n        :param maxi: the number of steps required to reach 100%\n        :param size: the number of characters taken on the screen by the progress bar\n        :param msg: the message displayed in the header of the progress bat\n        \"\"\"\n        self.maxi = maxi\n        self.p = self.__start_progress(maxi)()  # () to get the iterator from the generator\n        self.header_printed = False\n        self.msg = msg\n        self.size = size\n\n    def step(self, reset=False):\n        if reset:\n            self.__init__(self.maxi, self.size, self.msg)\n        if not self.header_printed:\n            self.__print_header()\n        next(self.p)\n\n    def __print_header(self):\n        print()\n        format_string = \"0%{: ^\" + str(self.size - 6) + \"}100%\"\n        print(format_string.format(self.msg))\n        self.header_printed = True\n\n    def __start_progress(self, maxi):\n        def print_progress():\n            # Bresenham's algorithm. Yields the number of dots printed.\n            # This will always print 100 dots in max invocations.\n            dx = maxi\n            dy = self.size\n            d = dy - dx\n            for x in range(maxi):\n                k = 0\n                while d >= 0:\n                    print('=', end=\"\", flush=True)\n                    k += 1\n                    d -= dx\n                d += dy\n                yield k\n\n        return print_progress\n\n\ndef read_data_files(directory, validation=True):\n    \"\"\"Read data files according to the specified glob pattern\n    Optionnaly set aside the last file as validation data.\n    No validation data is returned if there are 5 files or less.\n    :param directory: for example \"data/*.txt\"\n    :param validation: if True (default), sets the last file aside as validation data\n    :return: training data, validation data, list of loaded file names with ranges\n     If validation is\n    \"\"\"\n    codetext = []\n    bookranges = []\n    shakelist = glob.glob(directory, recursive=True)\n    for shakefile in shakelist:\n        shaketext = open(shakefile, \"r\")\n        print(\"Loading file \" + shakefile)\n        start = len(codetext)\n        codetext.extend(encode_text(shaketext.read()))\n        end = len(codetext)\n        bookranges.append({\"start\": start, \"end\": end, \"name\": shakefile.rsplit(\"/\", 1)[-1]})\n        shaketext.close()\n\n    if len(bookranges) == 0:\n        sys.exit(\"No training data has been found. Aborting.\")\n\n    # For validation, use roughly 90K of text,\n    # but no more than 10% of the entire text\n    # and no more than 1 book in 5 => no validation at all for 5 files or fewer.\n\n    # 10% of the text is how many files ?\n    total_len = len(codetext)\n    validation_len = 0\n    nb_books1 = 0\n    for book in reversed(bookranges):\n        validation_len += book[\"end\"]-book[\"start\"]\n        nb_books1 += 1\n        if validation_len > total_len // 10:\n            break\n\n    # 90K of text is how many books ?\n    validation_len = 0\n    nb_books2 = 0\n    for book in reversed(bookranges):\n        validation_len += book[\"end\"]-book[\"start\"]\n        nb_books2 += 1\n        if validation_len > 90*1024:\n            break\n\n    # 20% of the books is how many books ?\n    nb_books3 = len(bookranges) // 5\n\n    # pick the smallest\n    nb_books = min(nb_books1, nb_books2, nb_books3)\n\n    if nb_books == 0 or not validation:\n        cutoff = len(codetext)\n    else:\n        cutoff = bookranges[-nb_books][\"start\"]\n    valitext = codetext[cutoff:]\n    codetext = codetext[:cutoff]\n    return codetext, valitext, bookranges\n\n\ndef print_data_stats(datalen, valilen, epoch_size):\n    datalen_mb = datalen/1024.0/1024.0\n    valilen_kb = valilen/1024.0\n    print(\"Training text size is {:.2f}MB with {:.2f}KB set aside for validation.\".format(datalen_mb, valilen_kb)\n          + \" There will be {} batches per epoch\".format(epoch_size))\n\n\ndef print_validation_header(validation_start, bookranges):\n    bookindex = find_book_index(validation_start, bookranges)\n    books = ''\n    for i in range(bookindex, len(bookranges)):\n        books += bookranges[i][\"name\"]\n        if i < len(bookranges)-1:\n            books += \", \"\n    print(\"{: <60}\".format(\"Validating on \" + books), flush=True)\n\n\ndef print_validation_stats(loss, accuracy):\n    print(\"VALIDATION STATS:                                  loss: {:.5f},       accuracy: {:.5f}\".format(loss,\n                                                                                                           accuracy))\n\n\ndef print_text_generation_header():\n    print()\n    print(\"┌{:─^111}┐\".format('Generating random text from learned state'))\n\n\ndef print_text_generation_footer():\n    print()\n    print(\"└{:─^111}┘\".format('End of generation'))\n\n\ndef frequency_limiter(n, multiple=1, modulo=0):\n    def limit(i):\n        return i % (multiple * n) == modulo*multiple\n    return limit","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"5c57fa3fb4b2b389cc548dee9bce2d745b4a4e65","trusted":true,"collapsed":true},"cell_type":"code","source":"# Generating speech using Hilary's email as Corpus\nimport pandas as pd\nimport tensorflow as tf\nimport numpy as np\n#Import email body text and convert to corpus\nemaildf = pd.read_csv(\"../input/Emails.csv\")\ncorpus = []\nfor row in emaildf[\"ExtractedBodyText\"]:\n    corpus.extend(encode_text(str(row)))","execution_count":2,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"61e7e17e7417390aefd9c824d08a98fce0bc7244"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"391e075a977cd3935d363c4e0f5732657251e403","collapsed":true},"cell_type":"code","source":"len(corpus)","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"8a39c2a1062e8fce4b20cc40bbedf6ca1c11d3b9","trusted":true,"scrolled":true,"collapsed":true},"cell_type":"code","source":"tf.reset_default_graph()\nSEQLEN = 30\nBATCHSIZE = 200\nALPHASIZE = 98\nINTERNALSIZE = 512\nNLAYERS = 3\nlearning_rate = 0.001  # fixed learning rate\ndropout_pkeep = 0.8    # some dropout\n\ncodetext = corpus[int((len(corpus)*.1)):]\nvalitext = corpus[:int((len(corpus)*.1))]#valitext, bookranges = txt.read_data_files(shakedir, validation=True)\n\n# display some stats on the data\nepoch_size = len(codetext) // (BATCHSIZE * SEQLEN)\nprint_data_stats(len(codetext), len(valitext), epoch_size)\n\n#\n# the model (see FAQ in README.md)\n#\nlr = tf.placeholder(tf.float32, name='lr')  # learning rate\npkeep = tf.placeholder(tf.float32, name='pkeep')  # dropout parameter\nbatchsize = tf.placeholder(tf.int32, name='batchsize')\n\n# inputs\nX = tf.placeholder(tf.uint8, [None, None], name='X')    # [ BATCHSIZE, SEQLEN ]\nXo = tf.one_hot(X, ALPHASIZE, 1.0, 0.0)                 # [ BATCHSIZE, SEQLEN, ALPHASIZE ]\n# expected outputs = same sequence shifted by 1 since we are trying to predict the next character\nY_ = tf.placeholder(tf.uint8, [None, None], name='Y_')  # [ BATCHSIZE, SEQLEN ]\nYo_ = tf.one_hot(Y_, ALPHASIZE, 1.0, 0.0)               # [ BATCHSIZE, SEQLEN, ALPHASIZE ]\n# input state\nHin = tf.placeholder(tf.float32, [None, INTERNALSIZE*NLAYERS], name='Hin')  # [ BATCHSIZE, INTERNALSIZE * NLAYERS]\n\n# using a NLAYERS=3 layers of GRU cells, unrolled SEQLEN=30 times\n# dynamic_rnn infers SEQLEN from the size of the inputs Xo\n\n# How to properly apply dropout in RNNs: see README.md\ncells = [tf.nn.rnn_cell.GRUCell(INTERNALSIZE) for _ in range(NLAYERS)]\n# \"naive dropout\" implementation\ndropcells = [tf.nn.rnn_cell.DropoutWrapper(cell,input_keep_prob=pkeep) for cell in cells]\nmulticell = tf.nn.rnn_cell.MultiRNNCell(dropcells, state_is_tuple=False)\nmulticell = tf.nn.rnn_cell.DropoutWrapper(multicell, output_keep_prob=pkeep)  # dropout for the softmax layer\n\nYr, H = tf.nn.dynamic_rnn(multicell, Xo, dtype=tf.float32, initial_state=Hin)\n# Yr: [ BATCHSIZE, SEQLEN, INTERNALSIZE ]\n# H:  [ BATCHSIZE, INTERNALSIZE*NLAYERS ] # this is the last state in the sequence\n\nH = tf.identity(H, name='H')  # just to give it a name\n\n# Softmax layer implementation:\n# Flatten the first two dimension of the output [ BATCHSIZE, SEQLEN, ALPHASIZE ] => [ BATCHSIZE x SEQLEN, ALPHASIZE ]\n# then apply softmax readout layer. This way, the weights and biases are shared across unrolled time steps.\n# From the readout point of view, a value coming from a sequence time step or a minibatch item is the same thing.\n\nYflat = tf.reshape(Yr, [-1, INTERNALSIZE])    # [ BATCHSIZE x SEQLEN, INTERNALSIZE ]\nYlogits = tf.contrib.layers.linear(Yflat, ALPHASIZE)     # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\nYflat_ = tf.reshape(Yo_, [-1, ALPHASIZE])     # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\nloss = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Yflat_)  # [ BATCHSIZE x SEQLEN ]\nloss = tf.reshape(loss, [batchsize, -1])      # [ BATCHSIZE, SEQLEN ]\nYo = tf.nn.softmax(Ylogits, name='Yo')        # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\nY = tf.argmax(Yo, 1)                          # [ BATCHSIZE x SEQLEN ]\nY = tf.reshape(Y, [batchsize, -1], name=\"Y\")  # [ BATCHSIZE, SEQLEN ]\ntrain_step = tf.train.AdamOptimizer(lr).minimize(loss)\n\n# stats for display\nseqloss = tf.reduce_mean(loss, 1)\nbatchloss = tf.reduce_mean(seqloss)\naccuracy = tf.reduce_mean(tf.cast(tf.equal(Y_, tf.cast(Y, tf.uint8)), tf.float32))\nloss_summary = tf.summary.scalar(\"batch_loss\", batchloss)\nacc_summary = tf.summary.scalar(\"batch_accuracy\", accuracy)\nsummaries = tf.summary.merge([loss_summary, acc_summary])\n\n# Init Tensorboard stuff. This will save Tensorboard information into a different\n# folder at each run named 'log/<timestamp>/'. Two sets of data are saved so that\n# you can compare training and validation curves visually in Tensorboard.\nimport math, time\ntimestamp = str(math.trunc(time.time()))\nsummary_writer = tf.summary.FileWriter(\"log/\" + timestamp + \"-training\")\nvalidation_writer = tf.summary.FileWriter(\"log/\" + timestamp + \"-validation\")","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"89c75c36c820bead4d1dcfbb2a2d88b1956a57e7","trusted":true,"collapsed":true},"cell_type":"code","source":"import os\n# Init for saving models. They will be saved into a directory named 'checkpoints'.\n# Only the last checkpoint is kept.\nif not os.path.exists(\"checkpoints\"):\n    os.mkdir(\"checkpoints\")\nsaver = tf.train.Saver(max_to_keep=1000)\n\n# for display: init the progress bar\nDISPLAY_FREQ = 50\n_50_BATCHES = DISPLAY_FREQ * BATCHSIZE * SEQLEN\nprogress = Progress(DISPLAY_FREQ, size=111+2, msg=\"Training on next \"+str(DISPLAY_FREQ)+\" batches\")\n\n# init\nistate = np.zeros([BATCHSIZE, INTERNALSIZE*NLAYERS])  # initial zero input state\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\nstep = 0\n\n# training loop\nfor x, y_, epoch in rnn_minibatch_sequencer(codetext, BATCHSIZE, SEQLEN, nb_epochs=10):\n\n    # train on one minibatch\n    feed_dict = {X: x, Y_: y_, Hin: istate, lr: learning_rate, pkeep: dropout_pkeep, batchsize: BATCHSIZE}\n    _, y, ostate = sess.run([train_step, Y, H], feed_dict=feed_dict)\n\n    # log training data for Tensorboard display a mini-batch of sequences (every 50 batches)\n    if step % _50_BATCHES == 0:\n        feed_dict = {X: x, Y_: y_, Hin: istate, pkeep: 1.0, batchsize: BATCHSIZE}  # no dropout for validation\n        y, l, bl, acc, smm = sess.run([Y, seqloss, batchloss, accuracy, summaries], feed_dict=feed_dict)\n        #print_learning_learned_comparison(x, y, l, bookranges, bl, acc, epoch_size, step, epoch)\n        summary_writer.add_summary(smm, step)\n\n    # run a validation step every 50 batches\n    # The validation text should be a single sequence but that's too slow (1s per 1024 chars!),\n    # so we cut it up and batch the pieces (slightly inaccurate)\n    # tested: validating with 5K sequences instead of 1K is only slightly more accurate, but a lot slower.\n    if step % _50_BATCHES == 0 and len(valitext) > 0:\n        VALI_SEQLEN = 1*1024  # Sequence length for validation. State will be wrong at the start of each sequence.\n        bsize = len(valitext) // VALI_SEQLEN\n        #print_validation_header(len(codetext), bookranges)\n        vali_x, vali_y, _ = next(rnn_minibatch_sequencer(valitext, bsize, VALI_SEQLEN, 1))  # all data in 1 batch\n        vali_nullstate = np.zeros([bsize, INTERNALSIZE*NLAYERS])\n        feed_dict = {X: vali_x, Y_: vali_y, Hin: vali_nullstate, pkeep: 1.0,  # no dropout for validation\n                     batchsize: bsize}\n        ls, acc, smm = sess.run([batchloss, accuracy, summaries], feed_dict=feed_dict)\n        print_validation_stats(ls, acc)\n        # save validation data for Tensorboard\n        validation_writer.add_summary(smm, step)\n\n    # display a short text generated with the current weights and biases (every 150 batches)\n    if step // 3 % _50_BATCHES == 0:\n        print_text_generation_header()\n        ry = np.array([[convert_from_alphabet(ord(\"K\"))]])\n        rh = np.zeros([1, INTERNALSIZE * NLAYERS])\n        for k in range(1000):\n            ryo, rh = sess.run([Yo, H], feed_dict={X: ry, pkeep: 1.0, Hin: rh, batchsize: 1})\n            rc = sample_from_probabilities(ryo, topn=10 if epoch <= 1 else 2)\n            print(chr(convert_to_alphabet(rc)), end=\"\")\n            ry = np.array([[rc]])\n        print_text_generation_footer()\n\n    # save a checkpoint (every 500 batches)\n    if step // 10 % _50_BATCHES == 0:\n        saved_file = saver.save(sess, 'checkpoints/rnn_train_' + timestamp, global_step=step)\n        print(\"Saved file: \" + saved_file)\n\n    # display progress bar\n    progress.step(reset=step % _50_BATCHES == 0)\n\n    # loop state around\n    istate = ostate\n    step += BATCHSIZE * SEQLEN","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"fcbfdd675c64cc28d1d687fefd29c6633c3d4c31","trusted":false,"collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}