{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import the libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy import stats\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport PIL as pil\n\n\n#reading in the file that we cleaned\ndf = pd.read_csv('../input/retail-data/cleaned_data.csv', parse_dates=True, index_col=['InvoiceDate'], low_memory=False)\n\n# Work around because pandas keeps adding an index column even if i specify i don't want one\ndf.drop(df.filter(regex=\"Unname\"),axis=1, inplace=True)\n\n# Making a Revenue Column\ndf['Revenue'] = df['Price'] * df['Quantity']\n\n# UA of the Quantity Column\n# sns.displot(df['Quantity'])\n\n# Questions to answer: #1. How many invoices were there and how many of each?\n\n# grouping data by the invoice count\ninvoiceCount = df.groupby(['Invoice']).size().sort_values(ascending=False);\n\n# #2. Who bought the most most stuff from the comapany?\ncustomerCount = df.groupby(['Customer ID']).size().sort_values(ascending=False);\n\n#2a. Okay, do we have more large customers or a bunch of small ones\n\nsns.displot(customerCount, bins=50);\nplt.clf()\n\n# Response: it appears that we have more large customers than small ones in terms of order counts\n# How do we know?\n# Test to find that out: look at the median compared to the mean\nprint(np.median(customerCount)) # 53.0\nprint(np.mean(customerCount)) # 137\n\n# Can you give me a count of everything? \n\ncolNames = list(df.columns)\ncountArray = []\n\nfor name in colNames:\n    Count =   df.groupby([name]).size().sort_values(ascending=False);\n    countArray.append(Count)\n\n# print(countArray)\n\"\"\"\ncountArray is an array that holds the result of a size sort by columns, \nits purpose is to hold the counts of all variables sorted by largest first\n\n\"\"\"\n\n# Boss ask: I wanna know which country gave us the most revenue\n\nrevenueByCountry = df.groupby(['Country']).sum().sort_values('Revenue', ascending=False)\n\n# Resampling data by month (specificallu for revenue and Quantity)\nmonthResample = df[['Quantity', 'Revenue']].resample('M').sum()\n\n\n# Plotting the Quantity by months\nplt.plot(monthResample['Quantity'])\nplt.clf()\n\n# Plotting Revenue by Month\nplt.plot(monthResample['Revenue'])\nplt.clf()\n\n#turning the description column into a set\ndescriptionSet = list(set(df['Description']))\n\n# Getting an ddea of the types of product colors most bought by our customers\nlowerDescriptionSet = ''.join([word.lower() for word in descriptionSet])\ndescriptionSetCleaned = lowerDescriptionSet.replace(' ', '')\n\n# With the data prepared, the stop words needs to be prepared\n\nstopWords = ''.join(list(STOPWORDS)).replace(' ', '')\n\n# Generating the wordCloud by setting the stopwords agrument to our cleaned list and explicitly typing the description list\nwordCloud = WordCloud(stopwords=stopWords,max_words=200).generate(str(descriptionSetCleaned))\n\n\n# Having Python generate the image\nplt.imshow(wordCloud, interpolation='bilinear')\n\n\n# reseting the index -- This is important for the other analysis we intented to run\ndf = df.reset_index()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-20T15:47:38.482598Z","iopub.execute_input":"2021-07-20T15:47:38.483141Z","iopub.status.idle":"2021-07-20T15:47:43.793058Z","shell.execute_reply.started":"2021-07-20T15:47:38.483088Z","shell.execute_reply":"2021-07-20T15:47:43.7918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Boss is asking for a RFM Score for customers to know if he can treat his customeres differently\n\n\"\"\"\n\nThe point of figuring out the RFM scores for customers is that it lets you get an idea of the type of customer that you have,\nyou score them on three seperate metrics\n\n(R) - Recentcy - A measure of when they made the last purchase of a product or item, basically the sort by most recent orders\n\n(F) - Fequency - A measure of how much/how often they purchase our products\n\n(M) - Monetary - A measure of the overall value of the customer (total investment in our products)\n\n\n\"\"\"\n\n# Step 1. Format data\n\n# Cleaning the columns\ndf = df.dropna()\n\n# Converting the columns into strings\ndf[['Customer ID']] = df[[\"Customer ID\"]].astype(int).astype(str)\n\n# Converting the InvoiceDate into dataTime objects\ndf[\"InvoiceDate\"] = pd.to_datetime(df['InvoiceDate'])\n\n\n# Step 2. Gathering the R score!\n\n# R - needs a benchmark of recentcy (or a reference date for all the measurements)\n# Using the pandas method of to_datetime the string above was converted into a datetime object\n\nrecentcyDate = pd.to_datetime('20120101', format='%Y%m%d')\n\n# Creating a column the locks at the dates between the date of the purchase and the reference date\n# done by converting the difference(which returns a timedelta64[ns]) to day format and taking the abs value of that\n\ndf['DaysSincePurchase'] = abs((df[[\"InvoiceDate\"]] - recentcyDate).astype(\"timedelta64[D]\"))\n\n# Grouping by CustomerID using the minimum date as the returned Value\n\nRecent = df.groupby([\"Customer ID\"], as_index=False)['DaysSincePurchase'].min()\n\n# Renaming the columns headers then setting the Customer ID as the index\nRecent.columns = ['Customer ID', 'Recency']\n\n\n# This final step provides us with a formated dataframe\nRecent_df = Recent.set_index('Customer ID')\n\nprint(Recent_df)\n\n# Step 3 -- Gathering the F\n\n\"\"\"\n\nThere is a couple of ways of doing this, and I will show a groupby first then an easier approach\n----\nThe group by methodology is as follows, \n1. Group by the customerID and the Invoice columns and summing up the revenue\n2. With that group, further group this by customerID and and then getting the values of revenue by size\n\n----\nThe easier methodology is:\n1. Drop all duplicates from the df based on the Inovie column subset\n2. take the values of the customerID column and value count them.\n3. Then plug it back into a df format\n\n\"\"\"\n\n# Groupby method\n# This group by lets me see the invoices by the customerID\ncId_invoice = df.groupby(['Customer ID', 'Invoice'], as_index=False)['Revenue'].sum()\n\n# Further grouping this by ID but then taking the \nFrequencyGroupBy = cId_invoice.groupby(['Customer ID'], as_index=False)['Revenue'].size()\nFrequencyGroupBy.columns = ['Customer ID','Frequency']\nFrequencyGroupBy.set_index('Customer ID', inplace = True)\n\n# Renaming the size column as Frequency\n\nprint(FrequencyGroupBy)\n\n\n# Easier Method\n\n# Dropping nas from the invoice column\nFrequency_df = df\nFrequency_df = Frequency_df.drop_duplicates(subset='Invoice')\n# After Droping duplicates from the data you can grab your counts\nCounts = Frequency_df['Customer ID'].value_counts()\n# Turning the counts back into a dataFrame\nCounts = pd.DataFrame(Counts)\n\n# Step 4 - The M\n\n# Calculating the Montery Engagement from all of the customers\n\n\"\"\"\nThis is a fairly simple one as we are just going to group by the customer id and sum up the revenue column\n \n\"\"\"\n\n# groupby\nMonetary_df = df.groupby([\"Customer ID\"],as_index=False)[\"Revenue\"].sum()\nprint()\nMonetary_df.columns = [\"Customer ID\", 'Monetary']\nMonetary_df.set_index(\"Customer ID\", inplace=True)\n\nprint(Monetary_df)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-20T17:27:08.946192Z","iopub.execute_input":"2021-07-20T17:27:08.946649Z","iopub.status.idle":"2021-07-20T17:27:11.2944Z","shell.execute_reply.started":"2021-07-20T17:27:08.946612Z","shell.execute_reply":"2021-07-20T17:27:11.293058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combining all of these steps together!\n\n# You need to run all above steps in order to proceed with this.\n# The purpose of leaving the list in this command is so that you can try to make this df howevery you name you df or whatever ones you decide to use\nrfm_dataFrame_list = []\nrfm_dataFrame_list.append(Recent_df)\nrfm_dataFrame_list.append(FrequencyGroupBy)\nrfm_dataFrame_list.append(Monetary_df)\n\n# combine all the dataframes along the same axis\nrfm_data = pd.concat(rfm_dataFrame_list, axis=1)\n\n# This will be used in the last part of this article (Not Mandatory as I keep all the work in a single work book)\nrfm_data.to_csv('RFM_cluster.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-20T15:47:46.538951Z","iopub.execute_input":"2021-07-20T15:47:46.539289Z","iopub.status.idle":"2021-07-20T15:47:46.580369Z","shell.execute_reply.started":"2021-07-20T15:47:46.539258Z","shell.execute_reply":"2021-07-20T15:47:46.57915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This next section I will be using the **.qcut** function that is provided by pandas. \nThis is done for a couple of reasons that are explained below.\n\n#1 It simplifies binning by handling the math of figureing them out.\n#2 It allows you to label the bins in whatever fashion you want as we will see\n\n","metadata":{}},{"cell_type":"code","source":"# creating a seperate container for the rfm value\nrfmScoreTable = rfm_data\n\n# creating labels for R tab \n# the labels are backwards do to the fact that the bigger the number the less recent the purchase was\nR_labels = ['5', '4', '3', '2', '1']\n\n# Creating the Recency Score\nrfmScoreTable['Recency Score'] = pd.qcut(rfmScoreTable['Recency'], q=5, labels=R_labels)\n\n# --- REPEATING FOR THE NEXT TWO MEASUREMENTS --- #\n\n# F-Labels \nF_labels = ['1', '2', '3', '4', '5']\n\n# ranking the values so that they all have a unqiue value\nfre_ranked = rfmScoreTable['Frequency'].rank(method='first')\n\n# This would throw an error because of the way the bin edges are defined, so ranking them is easier to do\n# You could do the ranking for each measure when you automating it\n# alt_rfmScoreTable['Frequency Score'] = pd.qcut(rfmScoreTable['Frequency'], q=R_bins, labels=R_labels, duplicates='drop')\n\n# Creating the Frequency Score\nrfmScoreTable['Frequency Score'] = pd.qcut(fre_ranked, q=5,labels=F_labels)\n\n# M-Labels\nM_labels = ['1', '2', '3', '4', '5']\nrfmScoreTable['Monetary Score'] = pd.qcut(rfmScoreTable['Monetary'], q=5, labels=M_labels)\n\n# Creating a final score column\nrfmScoreTable['RFM Total'] = rfmScoreTable['Recency Score'].astype(str) + rfmScoreTable['Frequency Score'].astype(str) + rfmScoreTable['Monetary Score'].astype(str)\n\n# Finished RFM Table\nprint(rfmScoreTable) \n\nrfm_classification = []\n\n\n# setting up tuples for comparision\nHibernating = ('11', '12', '21', '22')\nAt_risk = ('13','14','23','24')\nCant_lose = ('15', '25')\nAbout_to_sleep = ('31', '32')\nNeed_Attention = ('33')\nLoyal_customers = ('34','35','44','45')\nPromising = ('41')\nNew_customers = ('51')\nPotential_loyalist = ('42','43','52','53')\nChampions = ('54','55')\n\n\n# Assessing the values and labeling them based on the tuple\nfor values in rfmScoreTable['RFM Total']:\n    if values.startswith(Hibernating):\n        rfm_classification.append('Hibernating')\n    elif values.startswith(At_risk):\n        rfm_classification.append('At Risk')\n    elif values.startswith(Cant_lose):\n        rfm_classification.append('Cant Lose')\n    elif values.startswith(About_to_sleep):\n        rfm_classification.append('About to sleep')\n    elif values.startswith(Need_Attention):\n        rfm_classification.append('Need Attention')\n    elif values.startswith(Loyal_customers):\n        rfm_classification.append('Loyal customers')\n    elif values.startswith(Promising):\n        rfm_classification.append('Promising')\n    elif values.startswith(New_customers):\n        rfm_classification.append('New customers')\n    elif values.startswith(Potential_loyalist):\n        rfm_classification.append('Potential loyalist')\n    elif values.startswith(Champions):\n        rfm_classification.append('Champions')\n    else:\n        pass\n    \nrfmScoreTable['RFM Segment'] = rfm_classification\nprint(rfmScoreTable)\n\n\n# --- WORK ON THIS ---- #\nplt.pie(rfmScoreTable['RFM Segment'].value_counts())\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T17:27:26.384778Z","iopub.execute_input":"2021-07-20T17:27:26.385153Z","iopub.status.idle":"2021-07-20T17:27:26.555826Z","shell.execute_reply.started":"2021-07-20T17:27:26.385122Z","shell.execute_reply":"2021-07-20T17:27:26.555132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This section will focus on using clustering to classify instead of what we did with the known clusters above\n# We will need to import a couple of things\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits import mplot3d\n\n# Step 1 --- Import the dateframe with the RFM values already established\ncluster_df = pd.read_csv('./RFM_cluster.csv')\ncluster_df.set_index(\"Customer ID\", inplace=True)\n\n# Step 2 -- Define the a scalar object and fit it to our data\nscaler = StandardScaler()\nscaled_array = scaler.fit_transform(cluster_df)\n\n# Step 3 -- Transform the array back into a df\nscaled_df = pd.DataFrame(scaled_array)\nscaled_df.columns = ['RecencyScaled', 'FrequencyScaled', 'MonetaryScaled']\n\n# Step 4 -- Visuallising the transformed data (ADD COLORS)\nfig = plt.figure(figsize=(8,10))\nax = fig.add_subplot(projection='3d')\n\n# Step 5 -- Start the Clustering\n# Declare the model\nmodel = KMeans(n_clusters = 10)\n# Fit the model to the data\nmodel.fit(scaled_df)\n# Predict labels\nlabels = model.predict(scaled_df)\n\n# Adding label to the df\nscaled_df['Label'] = labels\n\n# Plotting with colors auto generated by the label column \nax.scatter(xs = scaled_df['RecencyScaled'], ys = scaled_df['FrequencyScaled'], zs = scaled_df['MonetaryScaled'], c=scaled_df['Label'])\nax.set_xlabel(\"Recency\")\nax.set_ylabel(\"Frequency\")\nax.set_zlabel('Monetary')\nax.set_title('Customer Distrubition')\nax.view_init(30, 60)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-20T16:23:22.682186Z","iopub.execute_input":"2021-07-20T16:23:22.682606Z","iopub.status.idle":"2021-07-20T16:23:27.210316Z","shell.execute_reply.started":"2021-07-20T16:23:22.682546Z","shell.execute_reply":"2021-07-20T16:23:27.209435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 6 -- using the model data\n\n# Getting the cluster centers!\nKC = model.cluster_centers_\nClusterCenter = pd.DataFrame(KC)\n\nprint(ClusterCenter)\n\n# Step 7 -- Plotting the cluster centers and giving the 3d plot some more data \nfig = plt.figure(figsize=(8,10))\nax = fig.add_subplot(projection='3d')\nxs,ys,zs = [ClusterCenter[0], ClusterCenter[1], ClusterCenter[2]]\nlabels = scaled_df['Label'].unique()\nax.scatter(xs, ys, zs, s=100, c=labels)\n\n# Enhancing the visual appeal of the graph\n\n# adding '--' to each label in the labels array\npointLabels = []\nfor label in labels:\n    pointLabels.append('#'+label.astype(str))\n    \n    \nfor x,y,z,i in zip(xs+(abs(.1*xs)),ys+(abs(.1*ys)),zs,pointLabels):\n    ax.text(x,y,z,i)\nax.set_xlabel(\"Recency\")\nax.set_ylabel(\"Frequency\")\nax.set_zlabel('Monetary')\nax.set_title('Cluster Centers')\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-20T16:18:36.579594Z","iopub.execute_input":"2021-07-20T16:18:36.580222Z","iopub.status.idle":"2021-07-20T16:18:36.881633Z","shell.execute_reply.started":"2021-07-20T16:18:36.580186Z","shell.execute_reply":"2021-07-20T16:18:36.88051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- This section is to optimize the K cluster through a WCSS and reduce the sum to a reasonable value --- #\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits import mplot3d\n\n\ndef wcssScoreGenerator(df, maxClusters = 10):\n    # @arg MaxClusters -- The amount of clusters you want the function to recurcisily check\n    # @arg df -- dataframe you want the to fit \n    # @return -- array of the individual sums at each cluster level\n    \n    # Error control\n    if df.empty:\n        print('No dataframe was provided')\n        return\n    \n    # Setting up an empty array to hold the WCSS scores (will be returned)\n    wcssScores = []\n    \n    # Establish loop\n    for x in range (1, maxClusters):\n        # Declare Model\n        model = KMeans(n_clusters = x)\n        model.fit(df)\n        wcssScores.append(model.inertia_)\n        \n    return(wcssScores)\n\n# importing csv and scaling it\ncluster_df = pd.read_csv('./RFM_cluster.csv')\ncluster_df.set_index(\"Customer ID\", inplace=True)\n\n# Step 2 -- Define the a scalar object and fit it to our data\nscaler = StandardScaler()\nscaled_array = scaler.fit_transform(cluster_df)\n\n# Step 3 -- Transform the array back into a df\nscaled_df = pd.DataFrame(scaled_array)\nscaled_df.columns = ['RecencyScaled', 'FrequencyScaled', 'MonetaryScaled']\n\n\nScores = wcssScoreGenerator(scaled_df, 15)\nplt.plot(Scores, 'x')\nplt.title('The Elbow Point Graph')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()\n    \n#\n","metadata":{"execution":{"iopub.status.busy":"2021-07-20T17:25:02.288124Z","iopub.execute_input":"2021-07-20T17:25:02.288526Z","iopub.status.idle":"2021-07-20T17:25:39.721364Z","shell.execute_reply.started":"2021-07-20T17:25:02.288485Z","shell.execute_reply":"2021-07-20T17:25:39.720475Z"},"trusted":true},"execution_count":null,"outputs":[]}]}