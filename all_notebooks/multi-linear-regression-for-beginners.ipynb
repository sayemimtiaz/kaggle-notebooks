{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Market Mix Analysis\n\n# Preface: \n- ***Im a Data Enthusiast, still learning. Please show your support by upvoting and comment your suggestion and feel free to correct me whereever Im wrong.***\n\n## Predicting New Sales using Linear Regression Model\n\nI wrote this notebook to help the **#machineLearning** and **#linearregresion** novice to understand better how these things work. In this Notebook one can learn or get an overview of **how to perform EDA on both numerical and categorical feature, handling missing-valuesbasic and build a Linear regression model** from **#statsmodels** and aslo to check the model performance considering the **#mean_absolute_error** and **#MAPE scores**. \n\nI encourage you to fork this kernel, play with the code and can ask for any kind of help to understand these in comments . Good luck!\n\n## Quick ride through Linear Regression\n\n***For getting a gist of Linear Regression click on the link below.***\n\nClick here: **[Introduction_to_Linear_Regression](http://github.com/veer064/Linear-Regression/blob/master/README.md)** (This will take you to github.com)\n---","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Dataset\n\n**We will be using *mktmix.csv* a Marketing Mix dataset in csv format.**\n\nYou can download the dataset from the following github link: [mktmix.csv](https://github.com/veer064/Linear-Regression/tree/master/Market_Mix_Analysis-Sales_Prediction)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Objective:\n- Each row in the data describes the different characteristics of different marketing methods of a product with Total no. of. sales for a week \n- (1 week - 1 row).\n- Our goal is to analys this data and build a predictive model that can predict the No. of. Sales for a week, given these features.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Now that we have some context, let's get started","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Importing Packages and Libraries\n\n#Essential\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n#Ploting/Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Ignore useless warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\npd.options.display.max_seq_items = 8000\npd.options.display.max_rows = 8000\n\n#Model\nimport statsmodels.formula.api as smf\n\n#Model Evaluation\nimport sklearn.metrics as metrics\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n\n#Dataset Path, which I uploaded to Data folder.\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        print('Copy the above path and paste this in your read_csv(), to load the dataset as pandas dataframe.')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Reading the dataset as a DataFrame\n#Lets give our dataframe name -> marketing\n\nmarketing = pd.read_csv('/kaggle/input/mktmix.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top 5 records of the marketing DataFrame\n\n- This gives you a quick view and idea of the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Top 5 records of the marketing DataFrame\n\nmarketing.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Shape of the data that we are deaing with\n- It is always important to understand and good to know that quantitative information of data\n- So that you can decide on the tools and techniques to use to deal with is.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Shape of the data that we are deaing with\n\n#marketing.shape\n\nprint(\"NO. Of. Rows = %s\" % marketing.shape[0])\nprint(\"NO. Of. Columns = %s\" % marketing.shape[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary Statistics\n- Lets look at the  Summary Statistics of the marketing data,* **this gives you the information like mean, std, Quartile1, Quartile2, Quartile3, min and max values of a each and evry numeric features in our marketing DataFrame. ***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Summary Statistics\n\n#Using pandas options to set float_format to 2 decimals after the point.\npd.options.display.float_format = '{:.2f}'.format   #This makes the table clear and easy to understand\n\nmarketing.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Observations:\n1. Looking at the counts we can see that we have 4 missing values under radio marketing method feature.\n2. Range of the Radio data points is comparitively huge.\n3. InStore have high std, expecting outliers.\n4. The median of the Discount feature is 0, interesting need to look into it.\n5. TV datapoints seem to be distributed normally, lets check on it.\n6. Mean of the Stout feature is 2.55, it seems like the stock out rate on an average for week is 2.5 times.  \n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Useful, detailed information of the marketing dataset\n\nmarketing.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Observations:\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. The dtypes of the available features in the Data are as follows:\n   - float64 -> 6 \n   - int64   -> 1 \n   - object  -> 2\n    \n2. Total no. of records/entries in the data are 104, ranging from 0 to 103.\n\n3. We can see that we have null - values in  features:\n   - Radio, NewspaperInserts and Website_Campaign ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Now, Lets begin with the crusial part of any Machine Learning or Data Analysis task.\n\n# Epolatory Data Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Lets start with ***NewVolSales***, the variable which we are going to predict.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n\n#Check the new distribution \nsns.distplot(marketing['NewVolSales'], color=\"darkgreen\");\nax.xaxis.grid(True)\nax.yaxis.grid(True)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"NewVolSales\")\nax.set(title=\"NewVolSales Distribution\")\nsns.despine(trim=True, left=True)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Observations:\n1. From the above we can see that the target variable, ***NewVolSales*** follows **Bimodal Distribution** *(Double-Peaked Distribution)*.\n2. Most of the data is around the mean, 20,171.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Skewness and Kurtosis of the target variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Skew and kurt\nprint(\"Skewness: %f\" % marketing['NewVolSales'].skew())\nprint(\"Kurtosis: %f\" % marketing['NewVolSales'].kurt())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### For Starter to ML & Statisics:\n\n## Skewness: \n- Skewness is a measure of symmetry, or more precisely, the lack of symmetry. A distribution, or data set, is symmetric if it looks the same to the left and right of the center point.\n- The skewness for **a normal distribution is zero**, and **any symmetric data should have a skewness near zero**.**Negative values for the skewness indicate data that are skewed left** and **positive values for the skewness indicate data that are skewed right.** \n\n## Kurtosis: \n- **Kurtosis is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution.** \n- Data sets with *high kurtosis tend to have heavy tails, or outliers.* Data sets with *low kurtosis tend to have light tails, or lack of outliers.*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Observation:\n\nFrom Skewness and Kurtosis we can say that the NewVolSales is not symetrically distributed and is heavy tailed.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Lets Deep dive into independent features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding numeric features\n\nmarketing.select_dtypes(include = ['float64', 'int64']).columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Assigning a variable name to the list of numeric cols in df\n\nnum_features = marketing.select_dtypes(include = ['float64', 'int64']).columns.tolist()\n\nnum_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualising num_cols data from marketing dataframe\n\nmarketing[num_features].hist(figsize = (12,8), bins = 10, xlabelsize = 8, ylabelsize = 8, color= 'purple');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In my previous Kernal : [House For Sale: At what price?](https://www.kaggle.com/veer06b/house-for-sale-at-what-price) ***I have explained how to read useful insights from histograms, please feel free to go through it.***","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Observations:\n- Almost all the features have Outliers.\n- Features like ***Discount* and *Radio*** have Mode value '0' for some reason, which effects the mean of these feature. While *imputing missing values for these we should use median.*\n- ***Stout*** is a Bimodal distribution, that means its not symetrically distributed.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Lets move to ahead and do some data cleaning, which highly impacts the efficiency of the model learning.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Before doing that we need to work on column names which are not properly framed(They have space at some of thier tails, lts cut them and make them neat.)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"marketing.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that, only **'Radio '** and **'Website_Campaign '** are the two column names with such error.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"marketing = marketing.rename(columns = { 'NewVolSales' : 'NewVolSales', \n                  'Base_Price' : 'Base_Price', \n                  'Radio ':  'Radio', \n                  'InStore' : 'InStore', \n                  'NewspaperInserts' : 'NewspaperInserts', \n                  'Discount' : 'Discount' , \n                  'TV' : 'TV', \n                  'Stout' : 'Stout', \n                  'Website_Campaign ':  'Website_Campaign'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"marketing.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Identifying and Handling Missing Values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"marketing.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Observation:\n- As already discussed we can see that the data has 4 NaN values under Radio feature, and 98 NaN values in each under NewspaperInserts and Website_Campaign.\n\n### Note:\n- 98 NaNs out of 104 records is a very huge amount. Generally in such cases the whole columns is droped from the data, but here comes the domain knowledge advantage. Here dropping the whole column is not a good idea, we are seeing this high rate of NaN values because these two might not be the major methods of marketing for this company, and they dont use these methods much often.\n- In our analysis studying the impact of these two methods is important because, if thse are impacting on the sales for a week when ever these are used then we can suggest the company to increase the usage of these methods.\n\n### So, we are not going to drop any columns or rows from the marketing data we have.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Imputing Missing Values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Imputing Median for NaN in Radio\n\nmarketing.Radio = marketing.Radio.fillna(marketing['Radio'].median())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### For Handling missing values under categorical feature like \"NewspaperInserts\" and \"Website_Campaign\"\nwe will be using get_dummies() one hot encoding.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#One Hot Encoding on NewspaperInserts\n\nmarketing = pd.get_dummies(marketing, columns = [\"NewspaperInserts\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#One Hot Encoding on Website_Campaign\n\n\nmarketing = pd.get_dummies(marketing, columns = [\"Website_Campaign\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Top 5 records with new columns after encoding\n\nmarketing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Total no. of. columns now, after encoding: \n\nmarketing.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Column name after encoding:\n\nmarketing.columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets now check for NaN values, if there are any left untreated.\n\nmarketing.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we are successful in treating NaNs.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Outlier Analysis and Treatment","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Base_Price","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Boxplot for Base_Price Outliers\n\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(5, 4))\n\nsns.boxplot('Base_Price', data = marketing, orient = 'v', color = 'darkgreen')\nax.set(title=\"Base_Price Boxplot\")\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*We can see there are few outliers under \"Base_Price\"* below the minimum value, i, e; lower outliers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Minimum Value\n\nprint(\"Minimum Value of 'Base_Price' is %s\" % marketing['Base_Price'].quantile(0.01))\n\n#The records in which Base_Price has outliers\n\nmarketing[marketing['Base_Price'] < marketing['Base_Price'].quantile(0.01)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Replacing these outliers with quantile(0.1) value, the min value.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"marketing.loc[(marketing['Base_Price'] < marketing['Base_Price'].quantile(0.01)), \"Base_Price\"]= marketing['Base_Price'].quantile(0.01)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Radio","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Boxplot for Radio Outliers\n\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(5, 4))\n\nsns.boxplot('Radio', data = marketing, orient = 'v', color = 'yellow')\nax.set(title=\"Radio Boxplot\")\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are not going treate these outliers at 0 (zero), my idea of reasoning this outlier is, its possible that the company didn't promote the product on the Radio, so its obvious that this will be 0 at such times.\n\n- One thing we can do is to understand for how many week in two year this happend.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"marketing['Radio'].quantile(np.arange(0,1,0.05))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Minimum Value\n\nprint(\"Minimum Value of 'Radio' is %s\" % marketing['Radio'].quantile(0.10))\n\n#The records in which Base_Price has outliers\n\nmarketing[marketing['Radio'] < marketing['Radio'].quantile(0.10)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Count of such weeks\n\nmarketing[marketing['Radio'] < marketing['Radio'].quantile(0.10)].shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### As we already created new columns with the help of One hot encoding. Lets what other feature we can create that helps us in our analysis.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"marketing.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can classify all the promotion methods into Online and Offline promotion.**\n  - Where 'Website_Campaign_Facebook', 'Website_Campaign_Twitter' and 'Website_Campaign_Website Campaign all together can considered as Online.\n  - And 'Radio', 'InStore' and 'TV' can be considered as Offline.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"marketing['Online'] = marketing['Website_Campaign_Facebook']+ marketing['Website_Campaign_Twitter']+ marketing['Website_Campaign_Website Campaign ']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"marketing[\"Offline\"] = marketing['TV'] + marketing['InStore'] + marketing['Radio']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"marketing.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation between the target variable and all other independent variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = marketing.corr()\n\n#Only the reltion coefficients between all other features to NewVolSales.\ncorr = corr.NewVolSales \n\ncorr = corr.drop('NewVolSales')# Because we dont need the correlation NewVolSales - NewVolSales.\n\ncorr[abs(corr).sort_values(ascending = False).index] #ascenfing order irrespective of their sign","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Obervations:\n\n1. Its obvious that Base_Price and Sales have high negitive Correlation.\n2. InStore have comparitively good correlation becuase the sales majorly happens at the store and the customer influenced by promotion, has good chances of thinking to buy the product at the store itself.\n3. Its obvious that Discount and Sales have positive correlation Correlation.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Visualisation of the correlation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = marketing.corr()\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.index.values);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets Check\n# Feature to Feture correlation for checking multicollinearity\n\nThis will help us in feture reduction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#Pair Plot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data = marketing)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Heat Map","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = marketing.drop('NewVolSales', axis=1).corr() # We already examined SalePrice correlations\nplt.figure(figsize=(15, 20))\n\n\nax = sns.heatmap(corr[(corr >= 0.6) | (corr <= -0.8)], \n            cmap='viridis', vmax=1.0, vmin=-1.0, linewidths=0.1,\n            annot=True, annot_kws={\"size\": 8}, square=True)\n\nax.xaxis.grid(True)\nax.yaxis.grid(True)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only the feature we created have good coorelation with its component features, which is obvious.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# relation to the target\nfig = plt.figure(figsize = (12,7))\nfor i in np.arange(11):\n    ax = fig.add_subplot(5,5,i+1)\n    sns.regplot(x = marketing.iloc[:,i], y = marketing.NewVolSales, color = 'orange')\n\nplt.tight_layout()\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Observations:\n- Features such as 'Base_Price', 'Instore', 'Tv' and 'Stout' are showing linear relationship with 'NewVolSales', though they have few data point farer from the regression line.\n- Lets now fit a model with these features and then lets see the performance of the model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Linear Regression Model with statsmodel","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_model = smf.ols(\"NewVolSales~Base_Price+InStore+TV+Discount+Stout\",data=marketing) \n#ols - Ordinary Least Square model\n#OLS fits the linear regression model with Ordinary Least Squares\n\n#Fitting the model\n\nresults = reg_model.fit()\n\nprint(results.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Observations:\n1. We can observe that the model we got fit has the capacity of 79% of expalinable variance. Can be further improved.\n2. The intercept of the regresion function is 5.454e+04.\n3. The follwed by Intercept coeff values are the respect coeffiencient of those X's.\n4. P-Values for all the features except 'Discount' are less than 0.05.  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Now, let get the predictions from the model and compare the prediction with the actual values, from the we can caluclate MAE and MAPE score.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Model Quality Evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prediction on marketing data\n\npred = results.predict(marketing)\n\n#Actaul values of marketing.NewVolSales\n\nactual = marketing.NewVolSales","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Actual vs Predicted plot\nplt.plot(actual,\"blue\")\nplt.plot(pred,\"green\")\nplt.figure(figsize=(70,50));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Residuals vs. predicting variables (actual) plots","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"residuals = results.resid\n\nresiduals.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(actual, residuals);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Note:\n### We can plot the residuals versus each of the predicting variables to look for ***independence assumption***. \n     - If the residuals are distributed uniformly randomly around the zero x-axes and do not form specific clusters, then the assumption holds true. \n     \n### In this particular problem, we observe ***more or less uniformly random.***","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Fitted vs. residuals plot to check homoscedasticity","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(pred, residuals);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Variance is increasing\n- We clearly observe that the variance of the residuals increases with response variable magnitude. Therefore, the problem does not respect homoscedasticity and some kind of variable transformation may be needed to improve model quality.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Variance influence factors","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1, len(marketing.columns[:])):\n    v = vif(np.matrix(marketing[:]), i)\n    print(\"Variance inflation factor for {}: {}\".format(marketing.columns[i], round(v, 2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mean Absolute Error","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"MAE = metrics.mean_absolute_error(actual,pred)\n\nMAE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**MAE measures the average magnitude of the errors in a set of predictions, without considering their direction. Itâ€™s the average over the test sample of the absolute differences between prediction and actual observation where all individual differences have equal weight.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Mean Absolute Percentage Error","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(abs((pred-actual)/actual))*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The mean absolute percent error (MAPE) expresses accuracy as a percentage of the error. Because the MAPE is a percentage, it can be easier to understand than the other accuracy measure statistics.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Summary:\nSo we have built a good model with few more iterations we can get more accurate prediction. In this way you can do you EDA and Visualisation to your data and build a Linear Regression model with the statsmodels libraries. ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}