{"cells":[{"metadata":{},"cell_type":"markdown","source":"# The COVID pandmic: socioeconomic and health disparities\n\n#### Javaheri, B. The COVID-19 Pandemic: Socioeconomic and Health Disparities. Preprints 2020, 2020120599 (doi: 10.20944/preprints202012.0599.v1\n\n### This notebook contains all the steps taken to process and analyse the COVID-19 data. These are:\n\n#### 1. Loading required libraries\n#### 2. Importing the dataset\n#### 3. Exploratory data analysis\n#### 4. Data imputation to process missing values\n#### 5. Data visualisation\n#### 6. Data distribution, transformation and correlation matrix\n#### 7. Data imputation to process missing values\n#### 8. Ridge regression\n#### 9. XGBoost \n\n"},{"metadata":{},"cell_type":"markdown","source":"\n\n### 1. Loading the required libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom pandas.plotting import scatter_matrix\nfrom pandas import to_datetime\nimport numpy as np\nfrom numpy import mean\nfrom numpy import std\nfrom numpy import absolute\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nimport plotly.express as px\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nimport seaborn as sns\nfrom scipy.stats import norm, skew\nfrom scipy import stats\nfrom sklearn import metrics\nfrom sklearn.linear_model import LinearRegression, Ridge, RidgeCV\nfrom sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, StratifiedKFold, RepeatedKFold, KFold, GridSearchCV\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom xgboost import XGBRegressor, plot_importance \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom cycler import cycler\nimport matplotlib as mpl\nfrom yellowbrick.regressor import PredictionError, ResidualsPlot\nfrom yellowbrick.model_selection import learning_curve, ValidationCurve, FeatureImportances, CVScores\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x))\nimport datetime\nimport operator\nimport random\nimport math\nimport time\n# to improve matplotlib graphs\nfrom IPython.display import set_matplotlib_formats\nset_matplotlib_formats('retina')\npio.renderers.default='notebook'\nimport warnings\nwarnings.simplefilter(\"ignore\")\nwarnings.filterwarnings('ignore')\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn\nmpl.rcParams['figure.dpi'] = 300","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Importing the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing the csv file\ndf = pd.read_csv(\"../input/covid19-socioeconomic-and-health-disparities/data.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Exploratory Data Analysis (EDA)\nIn this step basic information about the data structure is obtained. "},{"metadata":{},"cell_type":"markdown","source":"#### 3.1 Data dimension and head to obtain some information on the structure"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the shape of dataframe\nprint(\"Dimension of this datasets (rows, columns) is: \", df.shape)\nprint()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The first few rows: \")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n#### 3.2 Concise summary of dataframe\nHere, df.info() method is used to print summary and data types."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.info(verbose=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nVariable \"date\" has \"object\" as data type. Whilst, time aspect of this dataset only used for visualisation and not part of analysis and model building,\ntype needs to be changed appropriately. This is achieved using to_datetime"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y-%m-%d\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n#### 3.3 Descriptive statistics"},{"metadata":{},"cell_type":"markdown","source":"This is generated by describe() function to summarise the central tendency and distribution of the dataframe columns excluding missing values.\nCount provides information on missing values.\nMean provides mean of variable.\nStd provides standard deviation of that variables. etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Brief statistical description of the data\ndf.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Data imputation to process missing values"},{"metadata":{},"cell_type":"markdown","source":"#### 4.1 Count the missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://towardsdatascience.com/data-cleaning-with-python-and-pandas-detecting-missing-values-3e9c6ebcf78b\n# Count the missing values.\nmiss_values = df.columns[df.isnull().any()]\nprint(f\"Missing values:\\n{df[miss_values].isnull().sum()}\")\nnull_values = df.columns[df.isna().any()]\nprint(f\"Null values:\\n{df[null_values].isna().sum()}\")\ndf_missing = df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.2 Dropping empty columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping columns from Yougov source as they are mostly empty\ndf.drop([col for col in df.columns if \"weekly\" in col], axis=1, inplace=True)\ndf.drop([col for col in df.columns if \"yougov\" in col], axis=1, inplace=True)\ndf.drop([col for col in df.columns if \"ox_m1_wildcard\" in col], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pandas drop columns using list of column names\ndf.drop([\"iso_code\", \"jhu_confirmed\", \"jhu_deaths\", \"owid_new_tests_smoothed\", \"owid_new_tests_per_thousand\", \"owid_new_tests_smoothed_per_thousand\", \n        \"owid_tests_per_case\", \"owid_positive_rate\", \"owid_tests_units\", \"ox_confirmed_cases\", \"owid_total_tests\", \"owid_new_tests\", \"owid_total_tests_per_thousand\",\n        \"ox_confirmed_deaths\", \"marioli_ci_65_u\", \"marioli_ci_65_l\", \"marioli_ci_95_u\", \"marioli_ci_95_l\", \"sdsn_effective_reproduction_rate_smoothed\",\n        \"sdsn_positive_test_rate_smoothed\", \"sdsn_new_cases_per_million_smoothed\", \"sdsn_new_deaths_per_million_smoothed\", \"owid_total_cases_per_million\",\n        \"owid_total_deaths_per_million\", \"ox_c1_flag\", \"ox_c2_flag\", \"ox_c3_flag\", \"ox_c4_flag\", \"ox_c5_flag\", \"ox_c6_flag\", \"ox_c7_flag\", \"ox_e1_flag\", \"ox_h1_flag\",\n        \"owid_handwashing_facilities\", \"sdsn_overall_transmission\", \"google_mobility_change_grocery_and_pharmacy\", \"google_mobility_change_parks\",\n        \"google_mobility_change_transit_stations\", \"google_mobility_change_retail_and_recreation\", \"google_mobility_change_residential\",\n        \"google_mobility_change_workplaces\", \"marioli_effective_reproduction_rate\",\"ox_stringency_index_for_display\", \"ox_stringency_legacy_index_for_display\",\n        \"ox_government_response_index_for_display\", \"ox_containment_health_index_for_display\", \"ox_economic_support_index_for_display\", \"ox_stringency_legacy_index\",\n        \"owid_stringency_index\",\"owid_aged_70_older\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.3 Selecting countires with population more than 1,000,000"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.loc[df[\"owid_population\"] > 1000000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n#### 4.4 Limiting analysis to top 5 most affected countries"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_n_country_names = df.groupby(\"country\").max()[\"owid_total_deaths\"].nlargest(5).keys()\ndf = df.loc[df['country'].isin(top_n_country_names)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.5 Confining period of analysis to between 01/April and 30/October/2020"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Find Start and finish Date\nstart_date = df.groupby('country').min()['date'].min()\nend_date = df.groupby('country').max()['date'].max()\ndate_range = pd.date_range(start_date, end_date, freq='D')\nprint (\"Start Date : \", start_date)\nprint (\"End Date : \", end_date)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[~(df['date'] < '2020-04-01')]\nstart_date = df.groupby('country').min()['date'].min()\ndate_range = pd.date_range(start_date, end_date, freq='D')\nprint (\"Start Date : \", start_date)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.6 Engineering a new feature: COVID-19 daily recovery per million"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Recovered cases = daily - deaths - recovered\ndf['owid_new_recovered_per_million'] = df['owid_new_cases_per_million'] - df['owid_new_deaths_per_million']\n# Recovered cases = daily - deaths - recovered\ndf['owid_new_recovered'] = df['owid_new_cases'] - df['owid_new_deaths']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.7 Forward and backward fill"},{"metadata":{"trusted":true},"cell_type":"code","source":"## 2. Forward Fill --------------------------\ndf = df.fillna(method='ffill').fillna(method='bfill')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.8 Visualisation of data before and after imputation to remove missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(2, 1, figsize=(22, 15));\nfig.suptitle('Visualisation of dataset for missing values (in yellow) before and after data imputation', fontsize=32, weight=\"bold\");\nplt.subplot(2,1,1);\nsns.heatmap(df_missing.isnull(),xticklabels=False,cbar=False,cmap='summer');\nplt.title('Before imputation: missing values (yellow)', size=30, weight=\"bold\");\nplt.subplot(2,1,2);\nsns.heatmap(df.isnull(),xticklabels=True,cbar=False,cmap='summer');\nplt.title('After imputation: no missing values', size=30, weight=\"bold\");\nplt.subplots_adjust(top=0.92);\nplt.subplots_adjust(wspace=0, hspace=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Data visualisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('white');\nfig, axes = plt.subplots(2, 4, figsize=(24, 10));\nfig.suptitle('COVID-19 mortality, survival, recovery, health indices and governments restrictions', fontsize=28, weight=\"bold\");\nplt.subplot(2,4,1);\nplt.gca().set_title('COVID-19 mortality/million', fontsize=22, weight=\"bold\");\nst = sns.stripplot(x = 'country', y = 'owid_new_deaths_per_million', data = df, jitter=0.25, split=True, linewidth=0.5, palette = \"husl\");\nbox = sns.boxplot(palette=['#BBBBBB','#DDDDDD'], linewidth=1, x = 'country', y = 'owid_new_deaths_per_million', data = df,showfliers=False);\nbox.set(xlabel=None);\nbox.set(ylabel=\"Daily mortality/million\");\nplt.subplot(2,4,2);\nplt.gca().set_title('COVID-19 recovery/million', fontsize=22, weight=\"bold\");\nst = sns.stripplot(x = 'country', y = 'owid_new_recovered_per_million', data = df, jitter=0.25, split=True, linewidth=0.5, palette = \"husl\");\nbox = sns.boxplot(palette=['#BBBBBB','#DDDDDD'], linewidth=1, x = 'country', y = 'owid_new_recovered_per_million', data = df,showfliers=False);\nbox.set(xlabel=None);\nbox.set(ylabel=\"Daily recovery/million\");\nplt.subplot(2,4,3);\nplt.gca().set_title('Health containment policy', fontsize=22, weight=\"bold\");\nst = sns.stripplot(x = 'country', y = 'ox_containment_health_index', data = df, jitter=0.25, split=True, linewidth=0.5, palette = \"husl\");\nbox = sns.boxplot(palette=['#BBBBBB','#DDDDDD'], linewidth=1, x = 'country', y = 'ox_containment_health_index', data = df,showfliers=False);\nbox.set(xlabel=None);\nbox.set(ylabel=\"Containment health index\");\nplt.subplot(2,4,4);\nplt.gca().set_title('Governments stringency policy', fontsize=22, weight=\"bold\");\nst = sns.stripplot(x = 'country', y = 'ox_stringency_index', data = df, jitter=0.25, split=True, linewidth=0.5, palette = \"husl\");\nbox = sns.boxplot(palette=['#BBBBBB','#DDDDDD'], linewidth=1, x = 'country', y = 'ox_stringency_index', data = df,showfliers=False);\nbox.set(xlabel=None);\nbox.set(ylabel=\"Government policy stringency\");\nplt.subplot(2,4,5);\nplt.gca().set_title('Extreme poverty', fontsize=22, weight=\"bold\");\nbar = sns.barplot(x = 'country', y = 'owid_extreme_poverty', data = df, palette = \"husl\");\nbar.set(xlabel=None);\nbar.set(ylabel=\"Extreme povery\");\nplt.subplot(2,4,6);\nplt.gca().set_title('Life expectancy', fontsize=22, weight=\"bold\");\nbar = sns.barplot(x = 'country', y = 'owid_life_expectancy', data = df, linewidth=0.5, palette = \"husl\");\nbar.set(xlabel=None);\nbar.set(ylabel=\"Average life expectancy\");\nplt.subplot(2,4,7);\nplt.gca().set_title('Age 65 or over/million', fontsize=22, weight=\"bold\");\nbar = sns.barplot(x = 'country', y = 'owid_aged_65_older', data = df, linewidth=0.5, palette = \"husl\");\nbar.set(xlabel=None);\nbar.set(ylabel=\"Age >= 65 per million\");\nplt.subplot(2,4,8);\nplt.gca().set_title('Hospital beds/thousand', fontsize=22, weight=\"bold\");\nbar = sns.barplot(x = 'country', y = 'owid_hospital_beds_per_thousand', data = df, linewidth=0.5, palette = \"husl\");\nbar.set(xlabel=None);\nbar.set(ylabel=\"Number of hospital beds/thousand\");\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/therealcyberlord/coronavirus-covid-19-visualization-prediction/notebook\ndf1 = df[(df.iloc[:,2:-1] >= 0).all(1)]\nfig, axes = plt.subplots(2, 1, figsize=(24, 10))\nfig.suptitle('Daily COVID-19 survival and mortality (per million) in top 5 affected countries', fontsize=30, weight=\"bold\");\nplt.subplot(2,1,1)\nplt.bar(df1.date, df1.owid_new_recovered_per_million, label=\"Recovered\", color='teal')\nplt.ylabel('Recovered (per million)', size=22, weight=\"bold\");\nplt.xticks(visible=False)\nplt.yticks(size=20)\nplt.legend(loc='upper right', shadow=True, fontsize='xx-large')\nplt.subplot(2,1,2)\nplt.bar(df1.date, df1.owid_new_deaths_per_million, label=\"Mortality\", color='lightcoral')\nplt.xlabel('Days Since 1/04/2020', size=30, weight=\"bold\");\nplt.ylabel('Mortality (per million)', size=22, weight=\"bold\");\nplt.legend(loc='upper right', shadow=True, fontsize='xx-large')\nplt.xticks(size=24)\nplt.yticks(size=24)\nplt.subplots_adjust(wspace=0, hspace=0.02)\nplt.subplots_adjust(top=0.93);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping columns not needed for analysis\ndf.drop([\"jhu_recovered\", \"owid_total_cases\", \"owid_new_cases\", \"owid_total_deaths\", \"owid_new_deaths\", \"owid_population\", \"owid_new_recovered\", \"owid_new_cases_per_million\",\n        \"owid_new_recovered_per_million\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6. Data distribution, transformation and correlation matrix"},{"metadata":{},"cell_type":"markdown","source":"#### 6.1 Distribution of data"},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":false},"cell_type":"code","source":"#https://simply-python.com/2019/08/21/useful-seaborn-plots-for-data-exploration/\n\n#numeric_features= df.select_dtypes(exclude=[\"object\",\"datetime\"])\n#numeric_features = numeric_features.stack().reset_index().rename(columns = {\"level_1\":\"Variable\",0:\"Value\"})\n#g = sns.FacetGrid(data =numeric_features, col=\"Variable\",  col_wrap=6, sharex=False, sharey=False)\n#g = g.map(sns.distplot, \"Value\", color ='blue')\n#plt.subplots_adjust(top=0.93)\n#plt.suptitle(\"Histograms of various variables to test distribution of target (first) and predictors (remainder)\", fontsize=28, weight=\"bold\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plot above suggests that the target variable (first histogram) suffers from assymetry and therefore skewed. "},{"metadata":{},"cell_type":"markdown","source":"#### 6.2 Transformation of skewed target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/duonghoanvu1/momo-secret-finding\n#### Some variables are skewed and as linear models like normally distributed data , we will transform SalePrice and make it more normally distributed.\npal = sns.color_palette('Paired')\nfig, axes = plt.subplots(3, 2, figsize=(20, 15))\nfig.suptitle('COVID-19 daily mortality before and after log transformation', fontsize=32, weight=\"bold\");\nplt.subplots_adjust(top=0.93)\nax = plt.subplot(2,2,1)\nsns.distplot(df['owid_new_deaths_per_million'] , fit=norm, color = \"dodgerblue\");\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df['owid_new_deaths_per_million']);\n# Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best');\nplt.ylabel('Frequency')\nplt.title('Mortality/million before transformation', fontsize=18, weight=\"bold\");\n# Get also the QQ-plot\nax = plt.subplot(2,2,2)\nres = stats.probplot(df['owid_new_deaths_per_million'], plot=plt);\nplt.title('Probability plot mortality/million before transformation', fontsize=18, weight=\"bold\");\n\nax = plt.subplot(2,2,3)\ndf['owid_new_deaths_per_million_transf'] = np.log1p(df['owid_new_deaths_per_million']);\nsns.distplot(df['owid_new_deaths_per_million_transf'], fit=norm, color = \"dodgerblue\");\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df['owid_new_deaths_per_million_transf'])\n# Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],  loc='best')\nplt.ylabel('Frequency')\nplt.title('Mortality/million after transformation', fontsize=18, weight=\"bold\");\nax = plt.subplot(2,2,4)\n# Get also the QQ-plot\nres = stats.probplot(df['owid_new_deaths_per_million_transf'], plot=plt);\nplt.title('Probability plot mortality/million after transformation', fontsize=18, weight=\"bold\");\nplt.subplots_adjust(wspace=0.08, hspace=0.14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping the skewed target variable as this is no longer needed. For analysis transformed variable is used.\ndf = df.drop(['owid_new_deaths_per_million'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 6.3 Correlation matrix to test correlation of predictors to target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Computing correlation matrix to describe correlation of variables\ncorrmat = df.corr() \nk = 50 \ncols = corrmat.nlargest(k, 'owid_new_deaths_per_million_transf')['owid_new_deaths_per_million_transf'].index \ncm = np.corrcoef(df[cols].values.T)\nf, ax = plt.subplots(figsize =(16, 12)) \nsns.heatmap(cm, ax = ax, cmap = \"coolwarm\", \n            linewidths = 0.1, yticklabels = cols.values,  \n                              xticklabels = cols.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation with output target variable: graph\ncor_target = corrmat[\"owid_new_deaths_per_million_transf\"].sort_values(ascending=False)\n#Selecting highly correlated features\nplt.figure(figsize=(20,8))\ncor_target.drop(\"owid_new_deaths_per_million_transf\").plot.bar(color=\"darkcyan\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation with output variable: numbers\ncor_target = corrmat[\"owid_new_deaths_per_million_transf\"].sort_values(ascending=False)\n#Selecting highly correlated features\ncor_target","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8. Ridge regression"},{"metadata":{},"cell_type":"markdown","source":"#### 8.1 Defining predictors (X) and target outcome (y)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['country','date','owid_new_deaths_per_million_transf'], axis = 1)\ny = df.owid_new_deaths_per_million_transf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 8.2 Splitting dataset into train and test; 80 and 20% respectively"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 8.3 Testing for multicollinearity"},{"metadata":{"trusted":true},"cell_type":"code","source":"vif = pd.DataFrame()\nvif[\"variables\"] = X.columns\nvif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 8.4 Testing linearity"},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://boostedml.com/2018/08/testing-linear-regression-assumptions-the-kaggle-housing-price-dataset.html\nfrom scipy import stats\nfrom statsmodels.regression.linear_model import OLS \nimport statsmodels as sm\ndef abline(slope, intercept):\n     #Plot a line from slope and intercept, borrowed from https://stackoverflow.com/questions/7941226/how-to-add-line-based-on-slope-and-intercept-in-matplotlib\"\"\"\n     axes = plt.gca()\n     x_vals = np.array(axes.get_xlim())\n     y_vals = intercept + slope * x_vals\n     plt.plot(x_vals, y_vals, '--')\n #fit an OLS model to data\n\nX_train_np = np.array(X_train)\ny_np = np.array(y_train)\n\nmodel = OLS(y_np,sm.tools.add_constant(X_train_np))\nresults = model.fit()\n#predict y values for training data\ny_hat = model.predict(results.params)\n#plot predicted vs actual\nplt.plot(y_hat,y_np,'o')\nplt.xlabel('Predicted')#,color='white')\nplt.ylabel('Actual')#,color='white')\nplt.title('Predicted vs. Actual: Visual Linearity Test')#,color='white')\nplt.tick_params(axis='x', colors='white')\nplt.tick_params(axis='y', colors='white')\nabline(1,0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n#### 8.5 Cross-validation for ridge regression\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create an array of alpha values\n#https://harvard-iacs.github.io/2018-CS109A/labs/lab-5/solutions/\n\nalphas = np.logspace(-4, 0, 50)\nsplitter = KFold(10, random_state=42, shuffle=True)\n\n# select the best alpha with RidgeCV\nfrom sklearn.linear_model import RidgeCV\nridge_CV = RidgeCV(alphas=alphas, normalize=True, scoring='neg_mean_squared_error', cv=splitter)\nridge_CV.fit(X_train, y_train)\n\nbest_alpha = ridge_CV.alpha_;\nprint(\"Best model searched:\\nalpha = {}\\nintercept = {}\\nbetas = {}, \".format(best_alpha, ridge_CV.intercept_, ridge_CV.coef_))\nprint()\n\ntuned_ridge = Ridge(alpha=best_alpha, normalize=True,)\ntuned_ridge.fit(X_train, y_train)\npred_y = tuned_ridge.predict(X_train)\nridge_score = metrics.r2_score(y_train, pred_y)\nridge_EV=metrics.explained_variance_score(y_train, pred_y) #Explained variance\nridge_MAE=metrics.mean_absolute_error(y_train, pred_y) #Mean absolute error\nridge_mse = metrics.mean_squared_error(y_train, pred_y)\nridge_RMSE= np.sqrt(metrics.mean_squared_error(y_train, pred_y))\n\n\n#ypredict_ridge_best = est.predict(test_set)\ntuned_ridge.coef_\n\n# calculate R^2 value, MAE, MSE, RMSE\nprint(\"Performance of tuned ridge regression on entire training dataset: \\n R2:{:.3f}, EV: {:.3f}, MAE: {:.3f}, MSE:{:.3f}, RMSE:{:.3f}\"\\\n      .format(ridge_score, ridge_EV, ridge_MAE, ridge_score, ridge_mse, ridge_RMSE))\nprint()\n\ny_pred = tuned_ridge.predict(X_test)\nridge_score = metrics.r2_score(y_test, y_pred)\nridge_EV=metrics.explained_variance_score(y_test, y_pred) #Explained variance\nridge_MAE=metrics.mean_absolute_error(y_test, y_pred) #Mean absolute error\nridge_mse = metrics.mean_squared_error(y_test, y_pred)\nridge_RMSE= np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n\nprint(\"Performance of ridge regression on testing dataset: \\n R2:{:.3f}, EV: {:.3f}, MAE: {:.3f}, MSE:{:.3f}, RMSE:{:.3f}\"\\\n      .format(ridge_score, ridge_EV, ridge_MAE, ridge_mse, ridge_RMSE))\nprint()\nprint()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 8.6 Visualisation of cross-validation scores, validation curve, residual plot and prediction error"},{"metadata":{"trusted":true},"cell_type":"code","source":"from yellowbrick.style import set_palette\nset_palette('yellowbrick')\nf, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2,figsize=(25, 12))\nviz = CVScores(tuned_ridge, cv=10, scoring='r2', ax=ax1)\nviz.fit(X_train, y_train)\nviz.finalize()\nviz = ValidationCurve(Ridge(), param_name=\"alpha\", param_range=alphas, cv=10, scoring=\"r2\", ax=ax2);\nviz.fit(X_train, y_train)\nviz.finalize()\nviz = ResidualsPlot(tuned_ridge, hist=False, qqplot=True, ax=ax3)\nviz.fit(X_train, y_train)\nviz.score(X_test, y_test);\nviz.finalize()\nviz = PredictionError(tuned_ridge, ax=ax4)\nviz.fit(X_train, y_train) \nviz.score(X_test, y_test) \nviz.finalize()\nf.suptitle('Performance of ridge regression on train and test datasets', fontsize=32, weight=\"bold\");\nplt.subplots_adjust(top=0.91)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 8.7 Performance of ridge regression on train and test datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(22, 6))\nax = plt.subplot(1,2,1)\nx_ax = range(len(X_train))\nplt.scatter(x_ax, y_train, s=15, color=\"dodgerblue\", label=\"Train original\")\nplt.scatter(x_ax, pred_y, s=15, color=\"m\", label=\"Train predicted\")\n#plt.plot(pred_y, 'm--', label=r\"$\\lambda =  {{{0:1.6f}}}$\".format(best_alpha),alpha=0.4)\nplt.ylabel('Log daily COVID-19 mortality', size=20)\nplt.xlabel('Days since 01/April/2020', size=20)\nplt.title('Ridge regression prediction on training dataset', size=20)\nplt.xticks(size=20)\nplt.yticks(size=20)\nplt.legend()\nplt.text(3, 3, 'Train R2 = 0.676',\n         {'color': 'black', 'fontsize': 20, \n          'bbox': dict(boxstyle=\"round\", fc=\"white\", ec=\"black\", pad=0.2)});\n\nax = plt.subplot(1,2,2)\nx_ax = range(len(X_test))\nplt.scatter(x_ax, y_test, s=15, color=\"dodgerblue\", label=\"Test original\")\nplt.scatter(x_ax, y_pred, s=15, color=\"m\", label=\"Test predicted\")\n#plt.plot(y_pred, 'm--', label=r\"$\\lambda =  {{{0:1.6f}}}$\".format(best_alpha),alpha=0.4)\n\nplt.ylabel('Log daily COVID-19 mortality', size=20)\nplt.xlabel('Days since 01/April/2020', size=20)\nax.set_title('Ridge regression prediction on testing dataset', size=20)\nplt.xticks(size=20)\nplt.yticks(size=20)\nplt.legend()\nfig.suptitle('Comparison of ridge regression COVID-19 mortality prediction to original data', fontsize=28, weight=\"bold\");\nplt.subplots_adjust(top=0.84)\nplt.text(2.8, 2.7, 'Test R2 = 0.700',\n         {'color': 'black', 'fontsize': 20, \n          'bbox': dict(boxstyle=\"round\", fc=\"white\", ec=\"black\", pad=0.2)});","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 8.8 Computation of predictor importance by ridge regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature importance by Ridge Regression \nfeatures = X.keys();\n#mpl.rcParams['axes.prop_cycle'] = cycler('color', ['dodgerblue']);\nfig = plt.gcf();\nfig.set_size_inches(20,13);\nax = plt.subplot(211);\nlabels = features;\nviz = FeatureImportances(tuned_ridge, ax=ax, labels=labels, relative=False);\nax.spines['right'].set_visible(False);\nax.spines['top'].set_visible(False);\nax.grid(False);\nfig.suptitle('Predictor importance computed by the ridge regression', fontsize=28, weight=\"bold\");\nplt.subplots_adjust(top=0.84);\n# Fit and display\nviz.fit(X, y);\nviz.poof();\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### 9. XGBoost"},{"metadata":{},"cell_type":"markdown","source":"#### 9.1 Grid-search for optimal parameters for XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.mikulskibartosz.name/xgboost-hyperparameter-tuning-in-python-using-grid-search/\n\ngbm_param_grid = {\n    'colsample_bytree': [0.3, 0.7],\n    'n_estimators': range(60, 220, 40),\n    'max_depth': range (2, 10, 1),\n    'learning_rate': [0.1, 0.01, 0.05] };\n\n# Instantiate the regressor: gbm\ngbm = XGBRegressor();\n\n# Perform grid search: grid_mse\ngrid_mse = GridSearchCV(estimator=gbm, param_grid=gbm_param_grid,\n                        scoring='neg_mean_squared_error', cv=10, verbose=False);\n\ngrid_mse.fit(X_train, y_train);\n\n# Print the best parameters and lowest RMSE\nprint(\"Best parameters found: \", grid_mse.best_params_)\nprint(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))\n\ngrid_mse.best_estimator_ # The best_estimator_ field contains the best model trained by GridSearch.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model1 = grid_mse.best_estimator_\nxgb_model1.fit(X_train, y_train, verbose=False)\ny_train_pred1 = xgb_model1.predict(X_train)\ny_test_pred1 = xgb_model1.predict(X_test)\n\nprint('Train r2 score: ', r2_score(y_train, y_train_pred1))\nprint('Test r2 score: ', r2_score(y_test, y_test_pred1))\ntrain_mse1 = mean_squared_error(y_train, y_train_pred1)\ntest_mse1 = mean_squared_error(y_test, y_test_pred1)\ntrain_rmse1 = np.sqrt(train_mse1)\ntest_rmse1 = np.sqrt(test_mse1)\nprint('Train RMSE: %.4f' % train_rmse1)\nprint('Test RMSE: %.4f' % test_rmse1)\nprint()\nprint()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 9.2 Visualisation of cross-validation scores, validation curve, residual plot and prediction error"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2,figsize=(25, 12))\nviz = CVScores(xgb_model1, cv=10, scoring='r2', ax=ax1)\nviz.fit(X_train, y_train)\nviz.finalize()\nviz = ValidationCurve(xgb_model1, param_name=\"max_depth\", param_range=np.arange(1, 11), cv=10, scoring=\"r2\", ax=ax2);\nviz.fit(X_train, y_train)\nviz.finalize()\nviz = ResidualsPlot(xgb_model1, hist=False, qqplot=True, ax=ax3)\nviz.fit(X_train, y_train)\nviz.score(X_test, y_test);\nviz.finalize()\nviz = PredictionError(xgb_model1, ax=ax4)\nviz.fit(X_train, y_train) \nviz.score(X_test, y_test) \nviz.finalize()\nf.suptitle('Performance of ridge regression on train and test datasets', fontsize=28, weight=\"bold\");\nplt.subplots_adjust(top=0.91)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 9.3 Performance of XGBoost on train and test datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(22, 6))\nax = plt.subplot(1,2,1)\nx_ax = range(len(X_train))\nplt.scatter(x_ax, y_train, s=15, color=\"dodgerblue\", label=\"Train original\")\nplt.scatter(x_ax, y_train_pred1, s=15, color=\"m\", label=\"Train predicted\")\n#plt.plot(pred_y, 'm--', label=r\"$\\lambda =  {{{0:1.6f}}}$\".format(best_alpha),alpha=0.4)\nplt.ylabel('Log daily COVID-19 mortality', size=20)\nplt.xlabel('Days since 01/April/2020', size=20)\nplt.title('XGBoost prediction on training dataset', size=20)\nplt.xticks(size=20)\nplt.yticks(size=20)\nplt.legend()\n#plt.text(3.1, 3.1, 'Train R2 = 0.846')\nplt.text(3, 3, 'Train R2 = 0.846',\n         {'color': 'black', 'fontsize': 20, \n          'bbox': dict(boxstyle=\"round\", fc=\"white\", ec=\"black\", pad=0.2)});\n\nax = plt.subplot(1,2,2)\nx_ax = range(len(X_test))\nplt.scatter(x_ax, y_test, s=15, color=\"dodgerblue\", label=\"Test original\")\nplt.scatter(x_ax, y_test_pred1, s=15, color=\"m\", label=\"Test predicted\")\n#plt.plot(y_pred, 'm--', label=r\"$\\lambda =  {{{0:1.6f}}}$\".format(best_alpha),alpha=0.4)\n\nplt.ylabel('Log daily COVID-19 mortality', size=20)\nplt.xlabel('Days since 01/April/2020', size=20)\nax.set_title('XGBoost prediction on testing dataset', size=20)\nplt.xticks(size=20)\nplt.yticks(size=20)\nplt.legend()\nfig.suptitle('Comparison of XGBoost COVID-19 mortality prediction to original data', fontsize=28, weight=\"bold\");\nplt.subplots_adjust(top=0.84)\nplt.text(2.8, 2.7, 'Test R2 = 0.825',\n         {'color': 'black', 'fontsize': 20, \n          'bbox': dict(boxstyle=\"round\", fc=\"white\", ec=\"black\", pad=0.2)});","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 9.4 Computation of predictor importance by ridge regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature importance by XGBoost \nfeatures = X.keys()\n#mpl.rcParams['axes.prop_cycle'] = cycler('color', ['purple'])\nfig = plt.gcf()\nfig.set_size_inches(20,13)\nax = plt.subplot(211)\nlabels = features\nviz = FeatureImportances(xgb_model1, ax=ax, labels=labels, relative=True)\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\nax.grid(False)\nfig.suptitle('Predictor importance computed by the XGBoost', fontsize=28, weight=\"bold\");\nplt.subplots_adjust(top=0.90);\n# Fit and display\nviz.fit(X, y)\nviz.poof()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}