{"cells":[{"metadata":{},"cell_type":"markdown","source":"![Neural Networks (Image by <a href=\"https://pixabay.com/users/geralt-9301/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=4430786\">Gerd Altmann</a> from <a href=\"https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=4430786\">Pixabay</a>)](https://cdn.pixabay.com/photo/2019/08/26/05/21/network-4430786_1280.jpg)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Objective :: Using weighted loss function to tackle class imbalance\nWe all have dealt with class imbalance problems in binary classification scenarios where the minority class is often less than 5% or even 1% of the whole dataset. In practice there exist multiple ways to address class imbalance:\n* Oversample the minority class (SMOTE etc.)\n* Undersample the majority class\n* Use class_weight kind of parameters provided within several ML libraries\n\nIn the following example, I have tried to address this class imbalance problem by modifying the loss function itself to give heavy weightage to the minority class. This incentivizes the model to give weightage to positive (minority) class as that minimizes the loss function. Choice of metric is F1 score.\n\n### A note on using classes\nI have found that usage of classes in inherent methods cleans up the code overall. Once Keras class object is declared its methods can be easily used.\n\n### Specifically using TensorFlow backend\nIn my earlier attempts while looping through Keras models I encountered an issue where the model graph was often not cleaned and one model was impacting the other in the loop. I could only solve this problem by using the TensorFlow backend (instead of Theano) and then using set session and clear session functionalities. You may find a different way to separate-out the models being looped.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nos.environ['KERAS_BACKEND'] = 'tensorflow'\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import f1_score, make_scorer, precision_recall_curve, precision_score, recall_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedShuffleSplit\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nimport warnings\nfrom sklearn.exceptions import DataConversionWarning\nfrom sklearn.model_selection import StratifiedKFold\nfrom datetime import datetime\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport random\nimport math\nfrom scipy import stats\nimport xgboost as xgb\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nimport time\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import LabelEncoder\nimport keras as K\nfrom keras.layers import Dropout, BatchNormalization, Activation\nfrom sklearn.utils import class_weight\nimport keras.backend as K1\nfrom sklearn.model_selection import StratifiedKFold\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nimport gc\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.metrics import confusion_matrix\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning)\nimport tensorflow as tf\n\nseed = 2145\ntf.random.set_seed(seed)\nnp.random.seed(seed)\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndata = pd.read_csv('../input/financial-distress/Financial Distress.csv')\n\n# Dropping variable x80 for now\ndata = data.drop(['x80'], axis=1)\n\n# Coverting target variable from continuous to binary form \n# as per the problem description  \ndef label_conv(x):\n    if x > -0.5:\n        return 0\n    else:\n        return 1\n\nlabels = data.iloc[:,2].apply(label_conv).values\n            \ndf = data.iloc[:,3:].values\n\nX_train0, X_test, y_train0, y_test = train_test_split(df, labels, test_size=0.25, stratify=labels, random_state=33897)\n\nX_train0, X_val, y_train0, y_val = train_test_split(X_train0, y_train0, test_size=0.10, stratify=y_train0, random_state=33897)\n\nsc = StandardScaler()\nsc.fit(X_train0)\n\nX_train0 = sc.transform(X_train0)\nX_val = sc.transform(X_val)\nX_test = sc.transform(X_test)\n\nprint(\"Shape of X_train:\",X_train0.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def f1_metric(y_true, y_pred):\n    true_positives = K1.sum(K1.round(K1.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K1.sum(K1.round(K1.clip(y_true, 0, 1)))\n    predicted_positives = K1.sum(K1.round(K1.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K1.epsilon())\n    recall = true_positives / (possible_positives + K1.epsilon())\n    f1_val = 2*(precision*recall)/(precision+recall+K1.epsilon())\n    return f1_val\n\nclass build_keras_model(object):\n    \n    def __init__(self, layers, activation, opt, init, input_dim, weight, patience, \n                 is_batchnorm, use_weighted_loss, print_model_summary, verbose):\n        \n        self.model = K.models.Sequential()\n        self.layers = layers\n        self.activation = activation\n        self.opt = opt\n        self.init = init\n        self.input_dim = input_dim\n        self.weight = weight\n        self.patience = patience\n        self.is_batchnorm = is_batchnorm\n        self.prmodsum = print_model_summary\n        self.use_weighted_loss = use_weighted_loss\n        self.verbose = verbose\n\n    def create_model(self):\n        \n        now = datetime.now()\n\n        for i, nodes in enumerate(self.layers):\n            if i==0:\n                self.model.add(K.layers.Dense(nodes,input_dim=self.input_dim,kernel_initializer=self.init))\n                self.model.add(Activation(self.activation))\n                if self.is_batchnorm == 1:\n                    self.model.add(BatchNormalization())\n            else:\n                self.model.add(K.layers.Dense(nodes,kernel_initializer=self.init))\n                self.model.add(Activation(self.activation))\n                if self.is_batchnorm == 1:\n                    self.model.add(BatchNormalization())\n\n        self.model.add(K.layers.Dense(1))\n        self.model.add(Activation('sigmoid')) # Note: no activation beyond this point\n        \n        if self.prmodsum == 1:\n            print(self.model.summary())\n        \n        def weighted_loss(y_true, y_pred):\n            weights = (y_true * self.weight) + 1.\n            cross_entop = K1.binary_crossentropy(y_true, y_pred)\n            weighted_loss = K1.mean(cross_entop * weights)\n            return weighted_loss\n        \n        if self.use_weighted_loss == 1:\n            loss_func = weighted_loss\n        else:\n            loss_func = 'binary_crossentropy'\n\n        self.model.compile(optimizer=self.opt, loss=loss_func,metrics=[f1_metric])\n        return\n    \n    def fit_model(self, X, y, X_validation, y_validation, batch_size, epochs, random_state):\n\n        pt = self.patience\n        vb = self.verbose\n        \n        earlystopping = EarlyStopping(monitor='val_loss', mode='min', verbose=vb, patience=pt, restore_best_weights = True)\n        callbacks_list = [earlystopping]\n\n        np.random.seed(random_state)\n        self.model.fit(X, y, validation_data = (X_validation,y_validation), \n                                                batch_size=batch_size,\n                                                epochs=epochs, \n                                                verbose=vb, \n                                                callbacks=callbacks_list)\n        \n        return\n    \n    def predict_from_model(self,test_df):\n\n        return self.model.predict(test_df)\n    \n    def __del__(self): \n        del self.model\n        gc.collect()\n        if self.prmodsum == 1:\n            print(\"Keras Destructor called\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train0\ny_train = y_train0\n\nXinput_dimension = X_train.shape[1]\n\n# Define Keras parameters\nXpt = 25\nXep = 5000\nXbnorm = 1\nXverbose = 0\nXpms = 0\nXactivation = 'relu'\nXlayer = (64, 256, 512, 256, 64)\nXbsz = 32\n\n# We will iterate on using simple binary loss vs weighted loss with different weights\nNUM = 7\nuse_weighted_loss_flags = np.append(0,np.repeat(1,NUM))\ncross_entropy_weights = np.append('Not Applicable',range(111,110+NUM,1))\n\n# We can notice that there is some improvement on F1 score when we assign weights to cross entropy\nfor X_use_weighted_loss,Xwt in zip(use_weighted_loss_flags,cross_entropy_weights):\n    \n    opt_name = 'Adagrad'\n    init_name = 'glorot_uniform'\n    Xopt = K.optimizers.Adagrad(learning_rate=0.01)\n    Xinit = K.initializers.glorot_uniform(seed=1)\n\n    np.random.seed(2018)\n    tf.random.set_seed(2018)\n    K1.set_session\n\n    km1 = build_keras_model(layers= Xlayer, \n                           activation = Xactivation, \n                           opt = Xopt, \n                           init = Xinit, \n                           input_dim = Xinput_dimension,\n                           weight = Xwt,\n                           patience = Xpt,\n                           is_batchnorm = Xbnorm,\n                           print_model_summary = Xpms,\n                           use_weighted_loss = X_use_weighted_loss,\n                           verbose = Xverbose)\n\n    km1.create_model()\n\n    km1.fit_model(\n                 X = X_train, \n                 y = y_train, \n                 X_validation = X_val, \n                 y_validation = y_val, \n                 batch_size = Xbsz, \n                 epochs = Xep,\n                 random_state = 3397)\n\n    preds = km1.predict_from_model(test_df = X_test)\n\n    del km1\n    K1.clear_session()\n    gc.collect()\n\n    best_f1 = 0\n    best_predval = []\n    best_thresh = 0.5\n\n    for thresh in np.arange(0.001,1,0.001):\n        thresh = round(thresh,3)\n        predval = (preds > thresh).astype(int)\n        f1s = f1_score(y_test,predval)\n        if f1s > best_f1:\n            best_f1 = f1s\n            best_thresh = thresh\n            best_predval = predval\n    \n    def wloss(i):\n        switcher={\n                0:'No',\n                1:'Yes',\n             }\n        return switcher.get(i,\"Invalid Choice\")\n        \n    print(\"*********************\")\n    print(\"For Use Weighted Loss = \",wloss(X_use_weighted_loss),\", Crossentropy Weight = \",Xwt)\n    print(\"*********************\")\n    print(\"\")\n    print(\"Best Threshold = \",best_thresh)\n\n    print(\"\")\n    if(np.sum(best_predval)==0):\n        print(\"All zeros predicted, so no confusion matrix\")\n    else:\n        print(confusion_matrix(y_test,best_predval))\n        print(\"\")\n        print(\"Precision = \",round(precision_score(y_test,best_predval),4))\n        print(\"Recall = \",round(recall_score(y_test,best_predval),4))\n\n    print(\"\")\n    print(\"Test F1_SCORE = \",round(best_f1,4))\n    print(\"\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}