{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **1. Load In Libraries and Data**","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sb\nfrom sklearn import preprocessing\nfrom math import log10, floor\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\n\n\nplt.rcParams[\"figure.figsize\"] = [20,10]\ndf_Data = pd.read_csv('/kaggle/input/unsupervised-learning-on-country-data/Country-data.csv')\n\n# convert exports, health and imports into montery values\ndf_Data['exports'] = df_Data['exports']/100 * df_Data['gdpp']\ndf_Data['health'] = df_Data['health']/100 * df_Data['gdpp']\ndf_Data['imports'] = df_Data['imports']/100 * df_Data['gdpp']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **2. Exploratory Analysis**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**2.1 Check For Nulls**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The findings here show there are no nulls in the data set","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**2.2 Correlation heat map**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nsb.heatmap(df_Data.corr(),annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Strong positive linear correlation between:\n1. Imports and exports\n2. Health and gdpp\n3. Income and gdpp\n4. Exports and gdpp\n5. Imports and gdpp\n6. Total fertility and Child mortality\n\nStrong negative linear correlation between:\n1. life expectancy and child mortality\n2. life expectancy and total fertility\n\n\nThis is not the full list, just most obvious ones","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**2.3 Univariate Analysis - boxplot**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"####  9 features so 9 subplots (3 columns and 3 rows)\nnum_rows=3\nnum_cols=3\nfig = plt.figure()\ncount=0\n\nfor criteria in df_Data.columns.to_list()[1:]:# don't use first column\n    count=count+1\n    ax = fig.add_subplot(num_rows,num_cols,count)\n    ax = sb.boxplot(y=df_Data[criteria])\n    ax.set_title(criteria)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All features have outliers ==> should not remove since only 167 countries (removing data will ahve large effect)\n\nAll data is skewed","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**2.4 Bivariate Analysis - pairplot**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sb.pairplot(df_Data,diag_kind='kde',corner=True) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **3. PCA**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**3.1 Scale Data**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here we isolate the numerical data and scale it (vital step for PCA)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Isolate numerical data from the dataframe\n\n# Drop the countries\ndf_X = df_Data.drop(['country'],axis=1) # ==> so that data is numerical and can scale\nX = df_X.values # get only the values from it ==> array\n\n# Scale the data (mean = 0 and sd = 1)\nscaler = preprocessing.StandardScaler().fit(X)\nX_scaled = preprocessing.scale(X)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3.2 Explained Variance vs Number of Components**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Define function to found to a certain number of significant figures\ndef round_sig(x, sig):\n    return round(x, sig-int(floor(log10(abs(x))))-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Set up PCA and get explained variances\npca = PCA()\nprincipalComponents = pca.fit_transform(X_scaled) # principle components for each country (row), for each feature\nExplainedVariance = pca.explained_variance_ratio_ # explained variance of each principle component","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Plot the explained variances against number of components\n\n# create dataframe from the explained variance data\ndf_ExplainedVariance = pd.DataFrame(np.array([list(range(1,len(ExplainedVariance)+1)),ExplainedVariance]).transpose(),\n                                    columns=['No. Components','ExplainedVariance'])\n\n# calculate cumulative sum of the explained variances\ndf_ExplainedVariance['EV Cum Sum'] = df_ExplainedVariance['ExplainedVariance'].cumsum()\n\n\n# plot the explained variances agianst number of components\nfig = plt.figure()\n\nax = fig.add_subplot(2,1,1) # first subplot is plot of the cumulative sum\nax.plot(df_ExplainedVariance['No. Components'].values,\n        df_ExplainedVariance['EV Cum Sum'].values,\n        linestyle='-', marker='x')\nax.set_xlabel('Total Number of Components, N')\nax.set_xticks(df_ExplainedVariance['No. Components'].values)\nax.set_xlim(0,10)\nax.set_ylabel('Cumulative Sum of Explained Variance')\nax.set_ylim(0,1.05)\nplt.grid()\nfor i, txt in enumerate(df_ExplainedVariance['EV Cum Sum'].values): # annotate\n    ax.annotate(round_sig(txt,3), (df_ExplainedVariance['No. Components'].values[i]+0.1, df_ExplainedVariance['EV Cum Sum'].values[i]-0.04))\n\n\n\nax = fig.add_subplot(2,1,2) #second suplot is plot of the bar chart\nax.bar(df_ExplainedVariance['No. Components'].values,\n        df_ExplainedVariance['ExplainedVariance'].values)\nax.set_xlabel('Component, n')\nax.set_xticks(df_ExplainedVariance['No. Components'].values)\nax.set_xlim(0,10)\nax.set_ylim(0,0.65)\nax.set_ylabel('Explained Variance')\nplt.grid(axis='y')\nfor i, txt in enumerate(df_ExplainedVariance['ExplainedVariance'].values): # annotate\n    ax.annotate(round_sig(txt,3), (df_ExplainedVariance['No. Components'].values[i]-0.1, df_ExplainedVariance['ExplainedVariance'].values[i]+0.01))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"top plot shows the cumulative explained variance plotted agianst the number of components.\nThe bototm plot shows the individual expained variances for different components.\n\nIf we take 4 principle components then we have over 0.90 of the explained variance ==> do this","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**3.3 K-Means Clustering**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Get Training data for 4 principle components\n\nn_PC = 4 # this is the number of principle components used for the ML model, set based on previous graphs\n\n# redo PCA with the number of components = n_PC  and fit to scaled data to get the data used for clustering\npca = PCA(n_components = n_PC)\nprincipalComponents = pca.fit_transform(X_scaled) # this is the data used for clustering","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Find elbow point to get most appropriate nubmer of clusters (plot inertia vs number of clusters)\n\n# loop through different numbers of clusters and plot effects of number of clusters on intertias\n\nks = range(1, 10)\ninertias = []\nfor k in ks:\n    # Create a KMeans instance with k clusters: model\n    model = KMeans(n_clusters=k)\n    # Fit model to samples\n    model.fit(principalComponents)\n    # Append the inertia to the list of inertias\n    inertias.append(model.inertia_)\n\n    \n    \nplt.figure()\nplt.plot(ks, inertias, '-o')\nplt.xlabel('number of clusters, k')\nplt.ylabel('inertia')\nplt.title('Inertia vs Clusters for ' +str(n_PC)+' Principle Components')\nplt.xticks(ks)\nplt.grid()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This shows that the elbow point is when there are 3 clusters. Use this for to train clustering model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train model, using prescribed number of clusters\nn_k_means_Clusters = 3 #from elbow point identification\nMYSEED = 5 # set random seed so that the clusters are always the same\n\n# set up and trian model\nmodel = KMeans(n_clusters=n_k_means_Clusters, random_state=MYSEED)\nmodel.fit(principalComponents)\n\n# Combine the predicted clusters with the orginal data and make new dataframe\nGroups = model.labels_ # Groups relates o the clusters found by the K-means model\ndf_Groups = pd.DataFrame(Groups, columns = ['Groups'])\ndf_KMeans = df_Data.join(df_Groups)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The df_KMeans can now be used for visualisation of the clusters","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **4. Visualising the clusters**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**4.1 Scatter Plots**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To visualise the clusters, I use plotly.\n\nI loop through all combinations of features and within each feature I plot the different clusters as points on a scatter graph. This therefore plots every cluster for every unique combination of features as a different layer. I then use plotly to create buttons in a dropdown menu to isolate each unique ocmbination of features, one at a time.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objs as go # use plotly\nfrom plotly.offline import download_plotlyjs, init_notebook_mode,  iplot\ninit_notebook_mode()\n\n#rename df_KMeans as df_Data\ndf_Data = df_KMeans\n\nGroups = np.sort(df_Data['Groups'].unique()) # this is a list of the unique gorups in ascending order\n\n\n\n#### loop through different combinations of columns (for cluster plot) and within each different colouring for each group\n\n# set up empty lists\ndata = [] # This stores the different layers\nbuttons_list=[] # this stores the buttons\n\n   \n\n# get column names (names of the features)\ncol_names = df_Data.columns.to_list()\ncol_names = col_names[1:len(col_names)-1]# drop first and last columns (this is the country and the group)\n\n# Create own names for the buttons and the point labels in plotly\nbutton_names = ['Child Mortality','Exports','Health Spending','Imports','Income','Inflation','Life Expectency','Fertility Rate','GDP per Capita']\nLabel_Names = ['Child mortality [deaths children under 5 per 1000 live births]','Exports [% gdp per capita]','Health spending [% gdp per capita]','Imports [% gdp per capita]','Net income per person [$]','Inflation [% growth rate of GDP]','Life expectency','Fertility rate [Children per woman]','GDP per capita [$/person]']\n\n\n# find pairs of unique combinations of these names\nimport itertools\ncomb_cols = list(itertools.combinations(col_names, 2)) # pairs of unique combinations for the dataframe columns\ncomb_button_names = list(itertools.combinations(button_names, 2)) #unique names for the buttons\ncomb_Label_Names = list(itertools.combinations(Label_Names, 2)) # unique names for the labels\n\ncount = -1 # This is used to count what layer we are on (in the loops)\ncount_button = -1 # this is a counter for what button/label we are on\n\nfor comb in comb_cols: # loop through the different combinations of column names\n    \n    feature_1 = comb[0] # first feature in the unique combination\n    feature_2 = comb[1] # second feature in the unique combination\n    \n    count_button = count_button+1\n    button_name_1 = comb_button_names[count_button][0] # first feature in the unique button name\n    button_name_2 = comb_button_names[count_button][1] # second feature in the unique button name\n    \n    label_name_1 = comb_Label_Names[count_button][0] # first feature in the unique label\n    label_name_2 = comb_Label_Names[count_button][1] # second feature in the unique label\n    \n    df_comb = df_Data[['country','Groups',feature_1,feature_2]] # dataframe for that combination of columns + country name + group\n    Bool_Button = [False] * len(Groups) * len(comb_cols) # Initialise a list which will be used by the correpsonding button to turn layers on and off\n    # ==> length of it is equal to number of layers\n    \n    # within each unique column combination, loop through the different groups and create scatter plot for each\n    for group in Groups:\n        count = count+1\n        df_comb_group = df_comb[df_comb['Groups']==group]\n        \n        data.append(go.Scatter(\n            x=df_comb_group[feature_1],\n            y=df_comb_group[feature_2],\n            mode='markers',\n            marker=go.scatter.Marker(\n                size=10,\n                opacity=1),\n            text='<br><b>'+ df_comb_group['country'] + '</b>' +\n            '<br>'+\n            '<br>' + label_name_2 + ': ' + df_comb_group[feature_2].astype(str)+\n            '<br>' + label_name_1 + ': ' + df_comb_group[feature_1].astype(str),\n            hoverinfo='text',\n            name = str(group),\n            visible=False))\n        \n        Bool_Button[count] = True  # This links the button to the layers plotted in this for loop. e.g. the first button will turn on the groups for the first unique combination of columns\n    \n    # Create the different buttons (one for each unique combination of columns)\n    buttons_list.append(dict(args=['visible', Bool_Button],\n                label= button_name_2 + ' vs ' + button_name_1,\n                method='restyle'))\n\n\n# turn on the layers for the first button\nfor  i in range(len(Groups)):\n    data[i]['visible']=True\n\n\nlayout = go.Layout(width = 900, height=550)\n\nlayout.update(updatemenus=list([\n        dict(x=-0.05,\n            y=1,\n            yanchor='top',\n            buttons=buttons_list)]),\n    legend=dict(x=-0.25, y=0.5))\n\n\nfig=go.Figure(data=data, layout = layout)\niplot(fig, filename = 'clusters') \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You may need to zoom out in your browser to view this visualisation properly!\n\nI could not figure out how to do the axes for each layer so the axes labels are within the hover text (hover over each  point for that information)\n\nThe dropdown menu on the left shows all the unique combinations of columns (36 of them).\n\nBy going through them we can see that in most of them the clusters don't overlap (or show minimal overlap). There is however alot of overlap  when plotting against inflation data (e.g. Inflation vs income)\n\nFor each option in the drop down menu we can isolate a cluster in the graph by double clicking on the correpsonding legend on the left of the figure.\n\nFor this model and the seed i have set, cluster 1 represents the top priority countries for aid. The full list of countries is below\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**4.2 Countries by groups**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"4.2.1 Below is a choropleth map showing countries by group","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pycountry\nimport plotly.graph_objs as go # use plotly\n\ncountry_geo = 'https://raw.githubusercontent.com/python-visualization/folium/master/examples/data/world-countries.json' # country information\n\n\n#### Change several country names so pycountry can find the isocode\ndf_Data['country'] = df_Data['country'].str.replace('Cape Verde', 'Cabo Verde')\ndf_Data['country'] = df_Data['country'].str.replace('Congo, Dem. Rep.', 'Congo, The Democratic Republic of the')\ndf_Data['country'] = df_Data['country'].str.replace('Congo, Rep.', 'Republic of the Congo')\ndf_Data['country'] = df_Data['country'].str.replace('Macedonia, FYR', 'North Macedonia')\ndf_Data['country'] = df_Data['country'].str.replace('Micronesia, Fed. Sts.', 'Micronesia, Federated States of')\ndf_Data['country'] = df_Data['country'].str.replace('South Korea', 'Korea, Republic of')\ndf_Data['country'] = df_Data['country'].str.replace('St. Vincent and the Grenadines', 'Saint Vincent and the Grenadines')\n\n\n#### Turn country names into isocodes and put the codes into dataframe\nlist_countries = df_Data['country'].unique().tolist() # unique list of countries\n\ndict_country_code = {}  # To hold the country names and their ISO\nfor country in list_countries:\n    try:\n        country_data = pycountry.countries.search_fuzzy(country) # try searching for country ==> fuzzy matching\n        country_code = country_data[0].alpha_3\n        dict_country_code.update({country: country_code})\n    except:\n        print('could not add ISO 3 code for ->', country)\n        # If could not find country, make ISO code ' '\n        dict_country_code.update({country: ' '})\n\n# Output:\n#could not add ISO 3 code for -> Cape Verde\n#could not add ISO 3 code for -> Congo, Dem. Rep.\n#could not add ISO 3 code for -> Congo, Rep.\n#could not add ISO 3 code for -> Macedonia, FYR\n#could not add ISO 3 code for -> Micronesia, Fed. Sts.\n#could not add ISO 3 code for -> South Korea\n#could not add ISO 3 code for -> St. Vincent and the Grenadines\n# ==> used above to change names of these so same as in the isocode\n\n#### create a new column iso_alpha in the data and fill it with appropriate iso 3 code\nfor key in dict_country_code:\n    df_Data.loc[(df_Data.country == key), 'iso_alpha_3'] = dict_country_code[key]\n\n    \n#### Check for duplicate codes\nduplicateRows = df_Data[df_Data['iso_alpha_3'].duplicated(keep=False)]\n\n# Niger's iso code is wrong ==> change to NER\ndf_Data['iso_alpha_3'] = df_Data['iso_alpha_3'].str.replace('NGA', 'NER')\n\n\n\nlayout_Ch = go.Layout(width = 1100, height=700)\n\ndata_Ch = go.Choropleth(geojson=country_geo, \n              locations=df_Data['iso_alpha_3'].to_list(), \n              z=df_Data['Groups'].to_numpy(),\n              text =df_Data['country'],\n              #autocolorscale=True,\n              colorscale  = [[0, 'rgb(0,0,255)'], [1, 'rgb(255,0,0)']],\n              hoverinfo  = 'text+z',\n              colorbar = dict(thickness=20, ticklen=3, title = 'Groups'),\n              zmin = df_Data['Groups'].to_numpy().min(),\n              zmax = df_Data['Groups'].to_numpy().max(),\n              marker_line_width=0,\n              marker_opacity=0.7,\n              visible=True)\n\nfig2=go.Figure(data=data_Ch, layout = layout_Ch)\niplot(fig2) \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"May need to zoom out a bit to see this properly. there is a color bar on the right to help deintify the cluster number","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"4.2.2 Below is list of priority countries for aid","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_AID = df_Data[df_Data['Groups']==1]['country'].reset_index()\ndf_AID = df_AID.drop(['index'],axis=1)\nAid_Countries = df_AID.values.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Aid_Countries)\nprint('\\n')\nprint(len(Aid_Countries))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the list of top priority countries in alphabetical order found by:\n1. turning imports, exports and health into monetary values (rather than percentages)\n2. scaling the data\n3. using 4 principle components (0.934 explained variance)\n4. Using 3 clusters (elbow point)\n\n\nCan use this list in addition to the visual in 4.2.1 to select certain countries for aid ==> might be most beneficaial to give aid to countries that are geographically close together (i.e. give aid to regions, not just individual countries). Can use the visual in 4.2.1 to achieve this.\n\n\n\nImprovements to what have been done here could be:\n1. removing the inflation data (due to the observations at end of section 4.1) and reclustering\n2. Investigating effect of using different number of clusters and principle components\n==> right now 48 potential countries is definitely too many for $10 million of aid. Should find ways to narrow this down a bit. one way was, as i suggested, selecting countries that are geographically close together. Another could be by using more clusters.\n3. Trying out other clustering models\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}