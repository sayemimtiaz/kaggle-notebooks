{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading data (feature hp gets dropped since is 85 for each row)\ndata   = pd.read_csv(\"../input/car-price-prediction/car_price.csv\")\ntarget = data[\"price\"].values\ndata.drop(columns=[\"hp\"], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data preprocessing\nThe dataset has almost exclusively categorical data, (except for ```km``` attribute). Being the numerosity of unique elements (for each categorical feature) not too high, we could think of One-Hot encode each one of them. ```Extras``` is composed by a string of extra accessories for each row. Since they are separated by a comma, they can be easily splitted by using ```DataFrame.str.get_dummies(sep=\",\")```. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Saving attributes categories for later analysis\nfeatures = [\"Body Color\", \"Gearing Type\", \"body_type\", \"make_model\"]\nfeatures_info = {feature: {\"uniques\": data[feature].unique(),\n                           \"mean\"   : [data.loc[data[feature] == val, \"price\"].mean() for val in data[feature].unique()],\n                           \"std\"    : [data.loc[data[feature] == val, \"price\"].std() for val in data[feature].unique()]\n                          }\n                for feature in features}\n\n# Data preprocessing\ndata = pd.get_dummies(data, columns=[\"make_model\", \"body_type\", \"Body Color\", \"Gearing Type\"])\ncolumns = data.columns.values\ndata.rename({column: column.split(\"_\")[-1] for column in columns}, axis=\"columns\", inplace=True)\n\nextras = data[\"Extras\"].str.get_dummies(\",\")\ndata.drop(columns=[\"Extras\"], inplace=True)\ndata = pd.concat([data, extras], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is what we get after the preprocessing steps. Note that some column have been renamed, just for convenience."},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Some EDA\nThe ```km``` column probably refers to used cars. If we look at the scatter plot ```price```-```km```, there is a negative correlation between the two attributes. We can also plot the average price for a car, given a certain body type, color, gearing and model. Looks like orange cars are cheaper! "},{"metadata":{"trusted":true},"cell_type":"code","source":"from math import floor\n\n# Kilometers - Price \nfig, km_to_price = plt.subplots(figsize=(15,5))\nkms = data[\"km\"].values\nkm_to_price.scatter(kms, target, s=3)\nkm_to_price.set_xlabel(\"Kilometers\")\nkm_to_price.set_xlabel(\"Price\")\nkm_to_price.set_title(\"Kilometers -> Price (Used cars?)\")\nplt.show()\n\n# Features - Price\nfig, axes = plt.subplots(2, len(features) // 2, figsize=(15,15))\nfor i, feature in enumerate(features_info):\n    axes[floor(i // 2), i % 2].set_title(feature)\n    axes[floor(i // 2), i % 2].set_xlabel(\"Price\")\n    axes[floor(i // 2), i % 2].barh(y=features_info[feature][\"uniques\"],\n                 width=features_info[feature][\"mean\"],\n                 xerr=features_info[feature][\"std\"]\n                )\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building a model\nTrying to predict the car price."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Target mean: {target.mean()} target std: {target.std()}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The target standard deviation give us the error of the trivial predictor (target mean). "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train / Test splitting\nfrom sklearn.model_selection import train_test_split\n\nnumpy_data = data.drop(columns=[\"price\"]).values\n\nx_train, x_test, y_train, y_test = train_test_split(numpy_data, target, test_size=0.3, random_state=24)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset is quite small, so we can directly use sklearn with grid search fpr hyperparameters optimization. I tried an Histogram-based Gradient Boosting Regression Tree. No particular reason for that, it is possible to try out different models and see how they perform. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Models testing \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics         import r2_score, mean_squared_error, make_scorer\n\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble     import HistGradientBoostingRegressor                           \n\nestimators = {'HistGradientBoostingRegressor': {'func'  : HistGradientBoostingRegressor(random_state=42),\n                                                'params': {'learning_rate'    : [0.05, 0.1, 0.15],\n                                                           'min_samples_leaf' : [10, 20, 30],\n                                                           'max_leaf_nodes'   : [31, 41, 51]}},\n             }\n\nmodels_to_test = estimators.keys()\nfor name in models_to_test:\n    model = GridSearchCV(estimator=estimators[name][\"func\"],\n                         param_grid=estimators[name][\"params\"],\n                         scoring=make_scorer(r2_score),\n                         n_jobs=-1)\n    model.fit(x_train, y_train)\n    preds   = model.predict(x_test)\n    print(\"{}: \\n R2: {:.3f} \\n RMSE {} \\n BP: {} \\n\".format(name, \n                                                          r2_score(y_test, preds),\n                                                          np.sqrt(r2_score(y_test, preds)),\n                                                          model.best_params_))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}