{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1>EDA of Netflix Data</h1>\n\n<p>\nHello everyone, this is my first notebook on Kaggle and here I attempt to answer the following questions and visualize the relevant data. <br><br>\n    1. What content is available in various countries and what are the popular categories in Netflix's top 20 countries? <br>\n    2. Has Netflix of late been focusing on movies instead of TV shows? <br>\n\nIn addition, the notebook contains a recommender based on the text description of the shows and network analysis of top actors and directors.</p>\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/netflix-shows/netflix_titles.csv',parse_dates=['date_added'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Country-wise content availability and popular categories</h2>"},{"metadata":{},"cell_type":"markdown","source":"In order to know which categories are available in various countries, we create a dataframe with countries as indices and categories as columns. But before we can do that, we need unique lists of categories and countries."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing nulls from country column\ndf['country'].fillna('',inplace=True)\n\n#Creating country and category lists\ncountries_list = set()\nfor country in df['country'].unique():\n    for substr in country.strip().split(','):\n        countries_list.add(substr.strip())\nif '' in countries_list:\n    countries_list.remove('')\n\ncategories_list = set()\nfor category in df['listed_in'].unique():\n    for substr in category.strip().split(','):\n        categories_list.add(substr.strip())\nif '' in categories_list:\n    categories_list.remove('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we create the dataframe and populate it as below,"},{"metadata":{"trusted":true},"cell_type":"code","source":"country_category_df = pd.DataFrame(index=sorted(countries_list),columns=sorted(categories_list))\nfor country in countries_list:\n    for category in categories_list:\n        country_category_df.loc[country, category] = \\\n        int(len(df[df['country'].str.contains(country) & df['listed_in'].str.contains(category)]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the dataframe populated, we can see which categories and how many of them are available in each of the countries."},{"metadata":{"trusted":true},"cell_type":"code","source":"country_category_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"country_category_df.loc['Denmark'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's add a 'Total' column to this dataframe, this will allow us to get the top 20 countries. (Note: Total does not represent total shows in the country but is just the total occurances of categories in the 'listed_in' column. It only serves the purpose of getting the top countries)"},{"metadata":{"trusted":true},"cell_type":"code","source":"country_category_df['Total'] = country_category_df.sum(axis=1)\ntop_20 = country_category_df['Total'].sort_values(ascending=False).head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a dataframe for top 20 countries to show the 3 most popular categories\npopular_categories = pd.DataFrame(index=top_20.index,columns=['Most popular','2nd Most popular','3rd Most popular'])\nfor country in popular_categories.index:\n    popular_categories.loc[country,'Most popular'] = country_category_df.loc[country].sort_values(ascending=False).index[1]\n    popular_categories.loc[country,'2nd Most popular'] = country_category_df.loc[country].sort_values(ascending=False).index[2]\n    popular_categories.loc[country,'3rd Most popular'] = country_category_df.loc[country].sort_values(ascending=False).index[3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"popular_categories","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's visualize the above data using nested pie charts,"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = sorted(list(set(popular_categories['Most popular'].values)))\nsizes = [list(popular_categories['Most popular'].values).count(label) for label in labels]\ngroup_names = labels\ngroup_size = sizes\nsubgroup_names = popular_categories.sort_values(by='Most popular').index\nsubgroup_size = np.ones(20)\ntxt = 'In most of the countries, Movies are the most popular category. Only Japan, South Korea and Taiwan are exceptions with TV Shows taking the top spot' \n#Set colors\na, b, c = [plt.cm.Blues, plt.cm.Reds, plt.cm.Greens]\n\n#Outer ring \nfig, ax = plt.subplots(figsize=(6,6))\nfig.suptitle('Most popular category',fontsize=20)\nfig.text(.5, .05, txt, ha='center')\nax.axis('equal')\nmypie, _ = ax.pie(group_size, radius=1.3, labels=group_names, colors=[a(0.6), b(0.6), c(0.6)] )\nplt.setp(mypie, width=0.3, edgecolor='white')\n\nsub_colors = [tuple(a(0.4))]+[tuple(b(0.4)) for i in range(16)]+[tuple(c(0.4)) for i in range(3)]\n\n#Inner ring\nmypie2, _ = ax.pie(subgroup_size, radius=1.3-0.3,rotatelabels=True, labels=subgroup_names, labeldistance=0.7, textprops = dict(rotation_mode = 'anchor', va='center', ha='center'), colors=sub_colors)\nplt.setp(mypie2, width=0.4, edgecolor='white')\nplt.margins(0,0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = sorted(list(set(popular_categories['2nd Most popular'].values)))\nsizes = [list(popular_categories['2nd Most popular'].values).count(label) for label in labels]\ngroup_names = labels\ngroup_size = sizes\nsubgroup_names = popular_categories.sort_values(by='2nd Most popular').index\nsubgroup_size = np.ones(20)\ntxt = 'The field for the 2nd most popular category is more diversified with the inclusion of Dramas and Comedies. 2nd spot is also taken by TV Shows in Japan, South Korea and Taiwan'\n#Set colors\na, b, c, d, e, f = [plt.cm.Blues, plt.cm.Reds, plt.cm.Greens, plt.cm.Purples, plt.cm.Oranges, plt.cm.Greys]\n\n#Outer ring \nfig, ax = plt.subplots(figsize=(6,6))\nfig.suptitle('2nd Most popular category',fontsize=20)\nfig.text(.5, .05, txt, ha='center')\nax.axis('equal')\nmypie, _ = ax.pie(group_size, radius=1.3, labels=group_names, colors=[a(0.6), b(0.6), c(0.6), d(0.6), e(0.6), f(0.6)] )\nplt.setp(mypie, width=0.3, edgecolor='white')\n\nsub_colors = [tuple(a(0.4))]+[tuple(b(0.4)) for i in range(3)]+[tuple(c(0.4)) for i in range(11)]+ \\\n            [tuple(d(0.4)) for i in range(3)]+[tuple(e(0.4))]+[tuple(f(0.4))]\n\n#Inner ring\nmypie2, _ = ax.pie(subgroup_size, radius=1.3-0.3,rotatelabels=True, labels=subgroup_names, labeldistance=0.7, textprops = dict(rotation_mode = 'anchor', va='center', ha='center'), colors=sub_colors)\nplt.setp(mypie2, width=0.4, edgecolor='white')\nplt.margins(0,0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = sorted(list(set(popular_categories['3rd Most popular'].values)))\nsizes = [list(popular_categories['3rd Most popular'].values).count(label) for label in labels]\ngroup_names = labels\ngroup_size = sizes\nsubgroup_names = popular_categories.sort_values(by='3rd Most popular').index\nsubgroup_size = np.ones(20)\ntxt = 'More diversification can be seen with Action & Adventure, Romantic TV Shows, Anime Series, British and Korean TV Shows'\n#Set colors\na, b, c, d, e, f, g, h, i = [plt.cm.Blues, plt.cm.Reds, plt.cm.Greens, plt.cm.Purples, plt.cm.Oranges, plt.cm.Greys, plt.cm.copper, plt.cm.cool, plt.cm.Wistia]\n\n#Outer ring \nfig, ax = plt.subplots(figsize=(6,6))\nfig.suptitle('3rd Most popular category',fontsize=20)\nfig.text(.5, .05, txt, ha='center')\nax.axis('equal')\nmypie, _ = ax.pie(group_size, radius=1.3, labels=group_names, colors=[a(0.6), b(0.6), c(0.6), d(0.6), e(0.6), f(0.6),g(0.6), h(0.6), i(0.6),] )\nplt.setp(mypie, width=0.3, edgecolor='white')\n\nsub_colors = [tuple(a(0.4)) for i in range(2)]+[tuple(b(0.4))]+[tuple(c(0.4))]+ \\\n            [tuple(d(0.4)) for i in range(3)]+[tuple(e(0.4)) for i in range(9)]+[tuple(f(0.4))]+ \\\n            [tuple(g(0.8))]+[tuple(h(0.5))]+[tuple(i(0.4))]\n\n#Inner ring\nmypie2, _ = ax.pie(subgroup_size, radius=1.3-0.3,rotatelabels=True, labels=subgroup_names, labeldistance=0.7, textprops = dict(rotation_mode = 'anchor', va='center', ha='center'), colors=sub_colors)\nplt.setp(mypie2, width=0.4, edgecolor='white')\nplt.margins(0,0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Movies vs TV Shows over the years</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"years = list(sorted(set(df['date_added'].dropna().dt.year)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yearly_movie_count = [len(df[(df['date_added'].dt.year==year) & (df['type']=='Movie')]) for year in years]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yearly_tvshow_count = [len(df[(df['date_added'].dt.year==year) & (df['type']=='TV Show')]) for year in years]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's plot a line graph to compare the number of movies and TV shows added since 2008,"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8,6))\nfig.suptitle('Movies vs TV Shows', fontsize=20)\nline1, = ax.plot(years, yearly_movie_count)\nline2, = ax.plot(years, yearly_tvshow_count)\nax.set_xlabel('Year', fontsize=15)\nax.set_ylabel('No. of Movies/Shows', fontsize=15)\nax.legend((line1, line2), ('Movies', 'TV Shows'), loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above graph clearly shows that Netflix has been focusing heavily on movies in the last few years."},{"metadata":{},"cell_type":"markdown","source":"<h2>Recommender based on text features using NLTK and KMeans clustering</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import names\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use the text features in description column to create the recommender"},{"metadata":{"trusted":true},"cell_type":"code","source":"docs = df['description']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First of all, we need to clean the data by eliminating numbers, names, inflectional and derivational forms of words. Then, we use the Tfidf Vectorizer before feeding the data to the KMeans clustering algorithm. We do this below,"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_names = set(names.words())\nlemmatizer = WordNetLemmatizer()\ndata_cleaned = []\nfor doc in docs:\n    doc = doc.lower()\n    doc_cleaned = ' '.join(lemmatizer.lemmatize(word) for word in doc.split() if word.isalpha() and word not in all_names)\n    data_cleaned.append(doc_cleaned)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_vector = TfidfVectorizer(stop_words='english',max_features=None,max_df=0.5,min_df=2)\ndata_tfidf = tfidf_vector.fit_transform(data_cleaned)\nterms = tfidf_vector.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k=100 #No. of clusters\nkmeans = KMeans(n_clusters=k, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans.fit(data_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clusters = kmeans.labels_\ncluster_label = {i: df['description'].iloc[np.where(clusters==i)].index for i in range(k)}\ncentroids = kmeans.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The below function accepts a show_id and prints keywords and the list of similar content."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_similar_content(show_id):\n    idx = df[df['show_id']==show_id].index\n    for key, value in cluster_label.items():\n        if idx in list(value):\n            print('Keywords: ')\n            for ind in centroids[key].argsort()[-10:]:\n                print(' %s' % terms[ind].title(), end=\"\")\n            print()\n            print(df[['title','type','release_year','rating']].iloc[value])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_similar_content('s741') # Here, it appears to be doing a decent job of clustering kids programs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_similar_content('s16') # The keywords suggest content could be for mature audiences and this can be verified\n                           # in the ratings columns where most are TV-MA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_similar_content('s6412') # These appear to be documentaries","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Overall, the recommender seems to be doing ok but with a lot of room for improvement."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<h2>Network Analysis of top actors and directors</h2>"},{"metadata":{},"cell_type":"markdown","source":"I will be doing network analysis at the country level because doing this on the whole data will make the graph messy and not reveal any useful patterns. I will create a function that accepts the country name and plots the network graph of top 50 actors/directors. It will also print the actor/director with most connections."},{"metadata":{"trusted":true},"cell_type":"code","source":"import networkx as nx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing nulls in the relevant columns\ndf['director'].fillna(' ', inplace=True)\ndf['cast'].fillna(' ', inplace=True)\ndf['country'].fillna(' ', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will select top actors and directors based on their appearances in the data. For top actors/directors, I will be considering names which have atleast 2 words as single word names likely have inaccurate counts (e.g. An actor called 'Ram' will get counted when the names are 'Ramlal Singh' and 'Ram Charan Tej'). \n\nI will be using the networkx module to add the nodes for actors and directors. Then, whenever an actor and a director work together, we add an edge."},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_network_graph(country_name):\n    # Creating actor and director lists\n    actors_list = set()\n    for actor, country in zip(df.cast,df.country):\n        if country_name in country:\n            for substr in actor.strip().split(','):\n                actors_list.add(substr.strip())\n    if '' in actors_list:\n        actors_list.remove('')\n\n    directors_list = set()\n    for director, country in zip(df.director,df.country):\n        if country_name in country:\n            for substr in director.strip().split(','):\n                directors_list.add(substr.strip())\n    if '' in directors_list:\n        directors_list.remove('')\n    \n    # Creating dataframes for actor count and director count and populating them \n    actor_count = pd.DataFrame(columns=['Name','Count'])\n    for actor in actors_list:\n        new_row = {'Name':actor,'Count':len(df[df['cast'].str.contains(actor) & df['country'].str.contains(country_name)])}\n        actor_count = actor_count.append(new_row,ignore_index=True)\n    actor_count.sort_values(by='Count',inplace=True,ascending=False)\n\n    director_count = pd.DataFrame(columns=['Name','Count'])\n    for director in directors_list:\n        new_row = {'Name':director,'Count':len(df[df['director'].str.contains(director) & df['country'].str.contains(country_name)])}\n        director_count = director_count.append(new_row,ignore_index=True)\n    director_count.sort_values(by='Count',inplace=True,ascending=False)\n\n    top_50_actors = actor_count[actor_count['Name'].str.contains(' ')].head(50)\n    top_50_directors = director_count[director_count['Name'].str.contains(' ')].head(50)\n    \n    G = nx.DiGraph()\n    \n    for actor in top_50_actors['Name']:\n        G.add_node(actor)\n    for director in top_50_directors['Name']:\n        G.add_node(director)\n    for actor in top_50_actors['Name']:\n        for director in top_50_directors['Name']:\n            if len(df[df['director'].str.contains(director) & df['cast'].str.contains(actor) & df['country'].str.contains(country_name)]) > 0:\n                G.add_edge(actor, director)\n    \n    #Blue nodes for actors and red for directors\n    color_map = []\n    for node in G:\n        if node in top_50_actors['Name'].values:\n            color_map.append('blue')\n        else:\n            color_map.append('red')\n    \n    plt.figure(1,figsize=(30,30))\n    nx.draw(G,node_color=color_map, with_labels=True,font_color='green',font_size=15)\n    print('Max connections: '+ str(max(dict(G.degree()).items(), key = lambda x : x[1])))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_network_graph('India')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I hope the notebook was useful."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}