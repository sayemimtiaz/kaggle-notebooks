{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# ML\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler, PowerTransformer, QuantileTransformer\nfrom xgboost import XGBClassifier\n# Imbalanced libraries\nfrom imblearn.under_sampling import TomekLinks, NeighbourhoodCleaningRule, NearMiss, RepeatedEditedNearestNeighbours, RandomUnderSampler\nfrom imblearn.combine import SMOTETomek, SMOTEENN\nfrom imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\nfrom sklearn.metrics import classification_report,roc_curve, auc, roc_auc_score\nfrom imblearn.pipeline import Pipeline\n# Explanations and graphs\nimport shap\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\n!pip install shap -U\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"text-align: center; font-size:50px;\"> Imbalanced assurance data</h1>\n\n<hr>"},{"metadata":{},"cell_type":"markdown","source":"## Table of contents\n\n- [Exploration](#a)\n- [Class Assurance](#b)\n- [Explanations](#c)"},{"metadata":{},"cell_type":"markdown","source":"## First, let explore the data: <a id=a></a>\n<hr>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/imbalanced-data-practice/aug_train.csv')\ndata_test = pd.read_csv('/kaggle/input/imbalanced-data-practice/aug_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation plots\n\nThe following figures give us the correlation between variables with kendall and spearman method:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (15,15)\nfig = plt.figure()\n\nplt.gcf().subplots_adjust(wspace = 0, hspace = 0.3)\n\nax = fig.add_subplot(211)\nsns.heatmap(data.corr(method='kendall'), annot=True, ax=ax)\nplt.title('Correlation between variables with kendall method')\nax = fig.add_subplot(212)\nsns.heatmap(data.corr(method='spearman'), annot=True, ax=ax)\nplt.title('Correlation between variables spearman method')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Parallel categories diagram\n\nThis categories parallel plot is very interesting to quickly understand the data. We can clearly observ the proportion of each class involved on the output response 1 (Positive response).\n\nto understand the diagram:\n - $P(color (blue) \\cap something (Male))$ means the proportion of Male with response 0 in total of people\n - $P(something (Male) | color (blue))$ means the proportion of Male in response 0\n - $P(color (blue) | something (Male))$ means the proportion of response 0 in Male"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import plotly.express as px\nimport plotly.graph_objects as go\nfrom ipywidgets import interact, interactive, fixed, interact_manual\n\nlim1 = np.quantile(data.Annual_Premium,0.33)\nlim2 = np.quantile(data.Annual_Premium,0.66)\n\ndata['Annual_Premium_cat'] = ['<'+str(lim1) if val<=lim1 else str(lim1)+\"-\"+str(lim2) if lim1<val<=lim2 else '>'+str(lim2) for val in data['Annual_Premium']]\ndata['Age_cat'] = ['<25' if val<=25 else '25-50' if 26<=val<=50 else '>50' for val in data.Age]\ndata['Region_cat'] = ['<15' if val<=15 else '15-30' if 16<=val<=30 else '>30' for val in data.Region_Code]\n\ncategorical_dimensions = ['Gender', 'Age_cat', \n                          'Driving_License','Vehicle_Age', 'Region_cat',\n                          'Vehicle_Damage', 'Annual_Premium_cat', 'Response']\n\ndimensions = [dict(values=data.loc[:,label], label=label) for label in categorical_dimensions]\n\nresponse = data.Response\ncolorscale = [[0, 'lightsteelblue'], [1, 'mediumseagreen']]\n\n\nfig = go.Figure(data = [go.Parcats(dimensions=dimensions,\n        line={'color': response, 'colorscale': colorscale},\n        hoveron='color', hoverinfo='count+probability',\n        labelfont={'size': 18, 'family': 'Times'},\n        tickfont={'size': 16, 'family': 'Times'},\n        arrangement='freeform')])\n\nfig.update_layout(\n        title=\"Parallel categories diagram\",\n        height=600, width=1200,\n        dragmode='lasso', hovermode='closest')\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Violon plots"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (15,10)\n\nfor idx,plot in enumerate(['Annual_Premium', 'Policy_Sales_Channel', \"Age\", \"Region_Code\"]):\n    plt.subplot(2,2,idx+1)\n    sns.violinplot(data=data, x=\"Response\", y=plot, hue='Gender',\n                   split=True, palette=\"Set3\", bw=.2, cut=1, linewidth=1)\n    sns.despine(left=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distributions in 2D"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (15,20)\n\ncolumns = [('Annual_Premium', 'Policy_Sales_Channel'),\n           ('Annual_Premium', \"Age\"),\n           ('Annual_Premium', \"Region_Code\"),\n           ('Annual_Premium', 'Policy_Sales_Channel'),\n           ('Policy_Sales_Channel', \"Age\"),\n           ('Policy_Sales_Channel',\"Region_Code\"),\n           (\"Age\", \"Region_Code\")]\n\nfor idx, col in enumerate(columns):\n    plt.subplot(3,3,idx+1)\n    cmap = sns.cubehelix_palette(start=2, light=1, as_cmap=True, rot=-.3)\n\n    sns.kdeplot(\n        data=data[:10000].query('Response==0')[[col[0],col[1]]],cmap=cmap, label='Response: No',\n        cut=10,thresh=0, levels=15,      \n    )\n    sns.kdeplot(\n        data=data[:10000].query('Response==1')[[col[0],col[1]]], color ='b', alpha=0.5, label='Response: Yes'  \n    )\n    if col[0] == 'Annual_Premium':\n        plt.xlim(0,100000)\n\n    plt.legend()\n    plt.xlabel(col[0])\n    plt.ylabel(col[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion:\n\nSame distributions for male and female. We can see that people around 40 years old are more likely to accept the assurance, while young people refuse it (violon plots). Annual premiums and Region codes have closed distributions for positive and negative responses, with a maximum for region codes around 30 and 9 and annual premium between 20000 and 60000 (dollars I supposed?). The proportion of positive response is higher with a policy sale channel around 125.\n\nFinally, from previous plots, some patterns are defining a positive response: A man or woman aged 40 years old living in zone 30 or 10. Owner of a vehicle aged 2 years with an annual insurance between 20000 euros and 60000 euros who has already had an accident. Let's confirm that with a ML algorithm."},{"metadata":{},"cell_type":"markdown","source":"## Assurance class <a id=b></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.initjs()\n\nclass Assurance:\n    \"\"\"\n    Class defining the methods usefull for the binary classification problem\n    \"\"\"\n    \n    def __init__(self, X, X_test, y, col_to_drop, xgb_kws={'learning_rate':0.01,'n_estimators':500, 'max_depth':8, 'scale_pos_weight':6}):\n        \"\"\"\n        self.data, self.y: the inputs and outputs\n        self.data_test, self.y_test: the test inputs and outputs\n        self.cat_values and self.num_values: columns nouns for categories and numericals values respectively\n        self.encoder: OrdianlEncoder\n        self.model: XGBClassifier\n        \"\"\"\n        self.data = X.drop([y]+col_to_drop, axis=1)\n        self.data_test = X_test.drop([y]+col_to_drop, axis=1)\n        self.y = X[y]\n        self.y_test = X_test[y]\n        \n        self.cat_values = self.data.dtypes.loc[self.data.dtypes=='object'].index\n        self.num_values = self.data.dtypes.loc[self.data.dtypes.isin(['int64','float64'])].index\n\n        if len(self.num_values) == 0:\n            self.cat_values = self.data.dtypes.loc[self.data.dtypes=='object'].index\n            self.num_values = self.data.dtypes.loc[self.data.dtypes!='object'].index\n        \n        # Data augmentation\n        for col in self.cat_values:\n            for other_col in self.cat_values.drop([col]):\n                self.data[col+'_'+other_col] = self.data[col]+'_'+self.data[other_col]\n                self.data_test[col+'_'+other_col] = self.data_test[col]+'_'+self.data_test[other_col]\n            self.cat_values = self.cat_values.drop([col])\n        self.cat_values = self.data.dtypes.loc[self.data.dtypes=='object'].index\n        \n        self.encoder = OrdinalEncoder(\n            categories = [pd.concat([self.data[col], self.data_test[col]],axis=0).unique() for col in self.cat_values])\n        self.data[self.cat_values] = self.encoder.fit_transform(self.data[self.cat_values])\n        self.data_test[self.cat_values] = self.encoder.fit_transform(self.data_test[self.cat_values])\n        \n        self.model = self.pipe(XGBClassifier(**xgb_kws, tree_method='gpu_hist', gpu_id=0, random_state=0, eval_metric='auc'))\n        \n    def outliers(self, col_out, n_neighbors=2, cls=0):\n        \"\"\" Detecte the outliers among the majority class and remove them.\n        \n        parameters:\n         - col_out: list[string].\n           The detection of outliers will be focused on the selected columns.\n         - n_neighbors: int.\n           Define the number of neighbors to take into account for the calculation of the density factor\n        \"\"\"\n        \n        if cls == 0: \n            cls_ = 1\n        else: \n            cls_ = 0\n            \n        outliers = LocalOutlierFactor(n_neighbors=n_neighbors)\n        data_out = outliers.fit_predict(self.data[self.num_values].loc[self.y==cls])\n        self.data = pd.concat([self.data.loc[self.y==cls].loc[data_out==1], self.data.loc[self.y==cls_]], axis=0)\n        self.y = pd.concat([self.y.loc[self.y==cls].loc[data_out==1], self.y.loc[self.y==cls_]], axis=0)\n    \n    def imb_prepro(self):\n        for estimator in [SMOTE(k_neighbors=10), TomekLinks()]:\n            self.data, self.y = estimator.fit_resample(self.data, self.y)\n        \n    def pipe(self, estimator):\n        \"\"\" Return pipeline\n        \n        parameter:\n         - Estimator: estimator object\n           Define the estimator in the pipeline\n        \"\"\"\n        \n        pipe = Pipeline([\n                         ('model', estimator)])\n        return pipe\n             \n    def best_params(self, **kwargs):\n        \"\"\" Find the best parameters of the XGBClassifier.\n        \n        parameters:\n         - params: dict.\n           Collection of XGBClassifier paramaters.\n        \"\"\"\n        \n        params_grid = {'model__n_estimators':[150,300,500],\n                       'model__max_depth': [5,6,8],\n                       'model__lambda': [1,2,3],\n                       'model__scale_pos_weight':[3,6,9],\n                       'model__learning_rate':[0.005,0.01,0.1]}\n        \n        params = kwargs.get('params',params_grid)\n        \n        self.model = GridSearchCV(self.model, param_grid=params_grid, cv=5, scoring='roc_auc')\n        self.model.fit(self.data, self.y, model__eval_set=[(self.data_test, self.y_test)], model__early_stopping_rounds=50)\n        self.model = self.model.best_estimator_\n        print('Best estimator: ',self.model.best_estimator_)\n        print('Best params: ',self.model.best_params_)\n        print('Best score: ',self.model.best_score_)\n\n    def train(self):\n        \"\"\"Training the model\"\"\"\n        \n        self.model.fit(self.data, self.y)\n        \n    def score(self, tresh=0.5):\n        \"\"\"print the classification_report for the training and test sets\"\"\"\n        \n        print('Test set')\n        out = self.model.predict_proba(self.data_test)\n        print(classification_report(self.y_test, output(out, tresh)))\n        print('Training set')\n        out = self.model.predict_proba(self.data)\n        print(classification_report(self.y, output(out, tresh)))        \n        \n    def graphs_score(self, cls=0):\n        \"\"\"Plot the ROC curve for a given classe and confusion matrix\n        \n        parameter:\n         - cls: int(0 or 1)\n           classes\n        \"\"\"\n        \n        tn, fp, fn, tp = confusion_matrix(self.model.predict(self.data_test), self.y_test).ravel()\n        total = tn + fp + fn + tp\n        conf_matrix = pd.DataFrame([[tn/total,fp/total],[fn/total,tp/total]], index=[0,1], columns=[0,1])\n        \n        sns.heatmap(conf_matrix, annot=True)\n        plt.xlabel('Predicted')\n        plt.ylabel('True')\n        plt.title('Confusion matrix in percentage')\n        plt.show()\n        \n        y_score = self.model.predict_proba(self.data_test)\n        fpr = dict()\n        tpr = dict()\n        roc_auc = dict()\n        for i in [0,1]:\n            fpr[i], tpr[i], _ = roc_curve(self.y_test, y_score[:,i], pos_label=i)\n            roc_auc[i] = auc(fpr[i], tpr[i])\n            \n        plt.figure()\n        lw = 2\n        plt.plot(fpr[cls], tpr[cls], color='darkorange',\n                 lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[cls])\n        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('Assurance ROC curve')\n        plt.legend(loc=\"lower right\")\n        plt.show()\n        \ndef output(y, treshold):\n    y = [1 if el>treshold else 0 for el in y[:,1]]\n    return y\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's apply the previous methods"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/imbalanced-data-practice/aug_train.csv')\n\n# Defining our test and training set \nclass1 = data.loc[data.Response==1].sample(frac = 1, random_state=0)\nclass2 = data.loc[data.Response==0].sample(frac = 1, random_state=0)\ndata_train = pd.concat([class1.iloc[:int(len(class1)*0.8)], class2.iloc[:int(len(class2)*0.8)]], axis=0)\ndata_test = pd.concat([class1.iloc[int(len(class1)*0.8):], class2.iloc[int(len(class2)*0.8):]], axis=0)\n\nob = Assurance(data_train, data_test, 'Response', ['id','Previously_Insured'])\nob.outliers(ob.num_values) # Remove outliers in classe 0\nob.outliers(ob.num_values, cls=1) # Remove outliers in classe 1\nob.imb_prepro()\n#ob.best_params() # Calcul the best parameters\nob.train()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following graphs are very usefull to evaluate our scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"ob.score()\nob.graphs_score(cls=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Recall metric is really important for our task because we need to classify the maximum of positive response. However we need to keep a good rate between false and positive classification, that's why we maximize the ROC AUC metric for our best params search"},{"metadata":{},"cell_type":"markdown","source":"### Translation"},{"metadata":{},"cell_type":"markdown","source":"Before any explanation we need to inverse translate categories values "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"pd.DataFrame(pd.DataFrame(ob.encoder.get_params()['categories'],\n                         index=ob.cat_values).transpose().fillna(method='ffill'))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"ob.encoder.transform(pd.DataFrame(pd.DataFrame(ob.encoder.get_params()['categories']).transpose().fillna(method='ffill')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Explanations <a id=c></a>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(ob.model, scoring='roc_auc', random_state=1).fit(ob.data_test, ob.y_test)\neli5.show_weights(perm, feature_names = ob.cat_values.tolist()+ob.num_values.tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vehicle_age_vehicle_damage which is a created column is the most important variable, it has the higher impact on the score metric"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def pdp_plt(col):\n    pdp_assurance = pdp.pdp_isolate(model=ob.model, dataset=ob.data_test[:10000], model_features=ob.data_test.columns, feature=col)\n\n    pdp.pdp_plot(pdp_assurance, col, plot_lines=True)\n    plt.show()\n    \npdp_plt('Age')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thank to that function we can this the partial dependence plot for each column"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"explainer = shap.Explainer(ob.model['model'], ob.data_test)\nshap_values = explainer(ob.data_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Force plot"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"shap.force_plot(explainer.expected_value, shap_values[:5000,:].values, ob.data_test.iloc[:5000].values, feature_names= ob.data.columns, link='logit')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Beeswarm"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"shap.plots.beeswarm(shap_values, max_display=14)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Bar plot"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"clustering = shap.utils.hclust(ob.data_test[:100000], ob.y_test[:100000])\nshap.plots.bar(shap_values, clustering=clustering)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Heatmap"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"shap.plots.heatmap(shap_values[:1000])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion:\n    \nRemainder: A man or woman aged 40 years old living in zone 30 or 10. Owner of a vehicle aged 2 years with an annual insurance between 20000 euros and 60000 euros who has already had an accident. Let's confirm that with a ML algorithm.\n\nActually, we can find same patterns with beeswarm plot, bar plots or pdp plot. For exemple, age has a bigger impact between 30 and 50 as we can see on pdp plot."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}