{"cells":[{"metadata":{},"cell_type":"markdown","source":"![Riga](https://europa.eu/youth/sites/default/files/article/riga-panorama-latvia-travel_0.jpg)\n\n# Introduction\n[Riga](https://en.wikipedia.org/wiki/Riga) is a lovely city near the Baltic Sea, the capital of Latvia. \n\n\nThis kernel is written for the Riga Data Science Club - an international community of data scientists based in Riga and Slack ðŸ˜ƒ\nWe will be happy to accept people from all over the world to join our friendly chat. It is totally free. Please sign up here: [http://rigadsclub.com/join-us/](http://rigadsclub.com/join-us/)\n\nYours,\nRiga DS Club"},{"metadata":{},"cell_type":"markdown","source":"# Previous work\n\nIn the previous [kernel](https://www.kaggle.com/dmitryyemelyanov/riga-real-estate-data-cleaning-riga-ds-club) we have explored [original Riga Real Estate dataset](https://www.kaggle.com/trolukovich/riga-real-estate-dataset) and applied some data cleaning techniques. This kernel aims to proceed on the [cleaned dataset](https://www.kaggle.com/dmitryyemelyanov/riga-real-estate-dataset-cleaned) and is focused on choosing most efficient ML model to predict real estate price.\n"},{"metadata":{},"cell_type":"markdown","source":"# Data exploration"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\ndf = pd.read_csv('/kaggle/input/riga-real-estate-dataset-cleaned/riga.csv')\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check value counts for **op_type** column that describes real estate operation type:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.op_type.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As both operations have quite a large number of samples, we have two design options:\n*  Build general model that predicts price for both operations and takes op_type as one of the inputs\n*  Split dataset into distinct \"Sale\" and \"Rent\" datasets to build separate \"Sale\" and \"Rent\" models\n\nNext, let's review how balanced our dataset is in terms of real estate property location:"},{"metadata":{"trusted":true},"cell_type":"code","source":"viz = df.plot(kind='scatter', x='lon', y='lat', alpha=0.4, figsize=(10,10))\nviz.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems we have more samples coming from the city center, while having other districts represented with fewer number of samples. Let's check that:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.district.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to get consistent model performance across all Riga districts, we will define a resample function that downsamples central district and upsamples other districts."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import resample\n\n# Helper method to upsample dataset to have equal sample count per district\ndef resample_by_district(df):\n    df_result = pd.DataFrame(columns=df.columns)\n    n_samples = np.minimum(df.district.value_counts()['centrs'], 500)\n    for district in df.district.unique():\n        df_district = df[df['district'] == district]\n        df_resampled = resample(df_district, \n                             replace=True,\n                             n_samples=n_samples)\n        df_result = pd.concat([df_result, df_resampled])\n    return df_result\n    \n# Verifying that it works\nresample_by_district(df)['district'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Please note we will not use this function immediately - resampling will be applied for training set only upon dataset split."},{"metadata":{},"cell_type":"markdown","source":"# Splitting the data\nDataset will be split into train, validation, and test sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_df_info(df_train, df_test, df_val):\n    print(len(df_train), 'train examples')\n    print(len(df_test), 'validation examples')\n    print(len(df_val), 'test examples')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sale = df[df['op_type'] == 'For sale']\n# Final cleanup\ndf_sale_clean = df_sale[(df_sale.price < 300000) & (df_sale.area <160)  \n                  & (~((df_sale.price < 50000) &(df_sale.area > 80))) \n                 & (~((df_sale.price < 100000)&(df_sale.area > 130)))\n                 ].copy()\n\ndf_sale_train, df_sale_test = train_test_split(df_sale_clean, test_size=0.1)\ndf_sale_train, df_sale_val = train_test_split(df_sale_train, test_size=0.1)\n#df_sale_train = resample_by_district(df_sale_train)\ndfs_sale = [df_sale_train, df_sale_val, df_sale_test]\nprint(\"For sale:\")\nprint_df_info(*dfs_sale)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_rent = df[df['op_type'] == 'For rent']\n# Final cleanup\ndf_rent_clean = df_rent[(df_rent.price < 1390) & (df_rent.area <125) & (df_rent.price > 60) \n                  & (~((df_rent.price < 110) &(df_rent.area > 40))) \n                 & (~((df_rent.price < 400)&(df_rent.area > 100)))\n                  & (~((df_rent.price > 1000)&(df_rent.area < 70)))\n                 ].copy()\n\ndf_rent_train, df_rent_test = train_test_split(df_rent_clean, test_size=0.1)\ndf_rent_train, df_rent_val = train_test_split(df_rent_train, test_size=0.1)\n#df_rent_train = resample_by_district(df_rent_train)\ndfs_rent = [df_rent_train, df_rent_val, df_rent_test]\nprint(\"For rent:\")\nprint_df_info(*dfs_rent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all_clean = pd.concat([df_sale_clean, df_rent_clean])\ndf_all_train, df_all_test = train_test_split(df_all_clean, test_size=0.1)\ndf_all_train, df_all_val = train_test_split(df_all_train, test_size=0.1)\n#df_all_train = resample_by_district(df_all_train)\ndfs_all = [df_all_train, df_all_test, df_all_val]\nprint(\"All:\")\nprint_df_info(*dfs_all)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we will wrap the dataframes with tf.data. This will enable us to use feature columns as a bridge to map from the columns in the Pandas dataframe to features used to train the model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# A utility method to create a tf.data dataset from a Pandas Dataframe\ndef input_fn(dataframe, shuffle=True, batch_size=32):\n    dataframe = dataframe.copy()\n    labels = dataframe.pop('price')\n    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(dataframe))\n    return ds.batch(batch_size)\n\n\ndef get_input_data_sets(batch_size, df_train, df_val, df_test):\n    train_ds = input_fn(df_train, batch_size=batch_size)\n    val_ds = input_fn(df_val, shuffle=False, batch_size=batch_size)\n    test_ds = input_fn(df_test, shuffle=False, batch_size=batch_size)\n    return [train_ds, val_ds, test_ds]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering\nIn this notebook we are using TensoFlow feature columns, that enable transformation of raw column values into TensorFlow model input in a friendly way for further model training. You can find more on this topic in this great [TensorFlow tutorial](https://www.tensorflow.org/tutorials/structured_data/feature_columns)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing dependencies\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nfrom tensorflow import feature_column\nfrom tensorflow.keras import layers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initializing array to store feature columns\nfeature_columns = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will start by adding most simple columns representing numeric values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create numeric columns\nfor header in ['rooms', 'floor', 'total_floors']:\n    feature_columns.append(feature_column.numeric_column(header))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's bucketize area, as we are mostly interested in some levels of property area instead of exact values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create bucketized columns\narea = feature_column.numeric_column(\"area\")\n#area_boundaries = list(np.arange(start=25, stop=df['area'].max(), step = 10))\narea_boundaries = [25.0, 35.0, 45.0, 55.0, 65.0, 75.0, 85.0, 95.0, 105.0, 115.0, 125.0, 155.0, 175.0, 225.0, 275.0, 325.0, 375.0, 425.0]\narea_buckets = feature_column.bucketized_column(area, boundaries=area_boundaries)\nfeature_columns.append(area_buckets)\nprint(\"Area boundaries\", area_boundaries)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Categorical columns will be added using one-hot encoding:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create categorical one-hot encoded columns\ncategorical_headers = ['op_type', 'house_seria', 'house_type', 'condition']\nfor header in categorical_headers:\n    categorical = feature_column.categorical_column_with_vocabulary_list(header, df[header].unique())\n    feature_columns.append(feature_column.indicator_column(categorical))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"District might be treated separately, as it has quite large count of unique values. Let's check it out:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate district vocabulary\nlen(df.district.unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In case vocabulary of any categorical value is large, it might be problematic to train a neural network using one-hot encodings due to large dimensions of the one-hot matrix. TensorFlow provides some extra feature columns to lower the dimensions of the input:\n* embedding_column\n* categorical_column_with_hash_bucket\n\nModel evaluation has shown that we are fine with simple one-hot encoding for districts, so other options are commented out and left for your information:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create categorical representation for district\ndistrict = feature_column.categorical_column_with_vocabulary_list('district', df.district.unique())\n\n# Option 1: Use indicator_column to create one-hot encoding:\nfeature_columns.append(feature_column.indicator_column(district))\n\n# Option 2: Use embedding_column to reduce dimensions of encoding:\n# district_embedding = feature_column.embedding_column(district, dimension=8)\n# feature_columns.append(district_embedding)\n\n# Option 3: Use categorical_column_with_hash_bucket to reduce dimensions of encoding:\n# This feature column calculates a hash value of the input, then selects one of the hash_bucket_size buckets to encode a string. \n# district_hashed = feature_column.categorical_column_with_hash_bucket('district', hash_bucket_size=1000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What about geolocation? In our use case treating latitude and longitude independently is highly inefficient, as the real location is a concatenation of them both. Combining several features into a single one is known as **feature crosses**. This enables a model to learn separate weights for each combination of features. Let's build a feature cross for latitude and longitude."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create numeric columns first\nfeature_lat = feature_column.numeric_column(\"lat\")\nfeature_lon = feature_column.numeric_column(\"lon\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Crossed column can't be created directly from the numeric one, so we will bucketize numeric columns first. This is extremely useful approach for geolocation scenario - you may think of it as splitting the map into tiles and make model learn weights for each tile independently.\n\nNumber of tiles to use depends on the \"resolution\" we will fine-tune to get best model performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Helper method to calculate equidistant coordinates on a single coordinate axis\n# \"axis_key\" is either \"lat\" or \"lon\"\n# \"n_coords\" is representing \"resolution\" that will affect tile size and density\ndef get_equidistant_coords(axis_key, n_coords):\n    geo_min = df[axis_key].min()\n    geo_max = df[axis_key].max()\n    geo_range = geo_max - geo_min\n    geo_step = geo_range / n_coords\n    return np.arange(geo_min, geo_max, geo_step).tolist()\n\n\n# Constructs a grid of equidistant points of required \"resolution\"\ndef get_equidistant_points(n_coords):\n    latitudes = get_equidistant_coords('lat', n_coords)\n    longitudes = get_equidistant_coords('lon', n_coords)\n    xv, yv = np.meshgrid(latitudes, longitudes, sparse=False, indexing='ij')\n    points = []\n    for i in range(n_coords):\n        for j in range(n_coords):\n            points.append([xv[i,j], yv[i,j]])\n            \n    return points\n\n# Checking how it works\nprint(\"Equidistant longitudes:\", get_equidistant_coords(\"lon\", 2))\nprint(\"Equidistant latitudes:\", get_equidistant_coords(\"lat\", 2))\nprint(\"Equidistant point grid:\", get_equidistant_points(2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This allows us to cover Riga with tiles of arbitrary density and use these coordinates to feature engineer best possible lat-lon crossed feature column. Let's define some more helper functions to proceed:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import folium\n\n# Constructs helper method to convert point to rect\ndef get_point_to_rect(n_tiles):\n    dx = (df['lat'].max() - df['lat'].min()) / n_tiles\n    dy = (df['lon'].max() - df['lon'].min()) / n_tiles\n    def point_to_rect(x,y):\n        return [\n            [x,y],\n            [x+dx, y],\n            [x+dx, y+dy],\n            [x, y+dy]\n        ]\n    return point_to_rect\n    \n# Helper method to get tile grid\ndef get_tiles(n_tiles):\n    points = get_equidistant_points(n_tiles)\n    point_to_rect = get_point_to_rect(n_tiles)\n    return list(map(lambda point: point_to_rect(*point), points))\n\n# Helper method to visualize coordinate tiles over Riga map\ndef plot_tiles(tiles): \n    tiles_map = folium.Map(\n        location=[56.946285, 24.105078],\n        tiles='cartodbpositron',\n        zoom_start=11,\n    )\n    for points in tiles:\n        folium.Rectangle(bounds=points, color='#ff7800', fill=True, fill_color='#ffff00', fill_opacity=0.2).add_to(tiles_map)\n        \n    return tiles_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize tiles to get the intuition how well they fit Riga coordinates\nn_tiles = 2\ntiles = get_tiles(n_tiles)\nplot_tiles(tiles)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking out higher tile density\nn_tiles = 20\ntiles = get_tiles(n_tiles)\nplot_tiles(tiles)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualization helps to find out some weak sides of this feature engineering approach:\n*  some of the tiles are placed over the sea or outside the city borders\n*  difference in population and real estate density is not taken into account\n\nMore optimal approach would be to have different-sized tile grid depending on real estate density and price variance accross coordinate axis. We should also take into account real shape of Riga borders. Let's fix this!"},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\n\n# Load Riga boundaries \nwith open(\"/kaggle/input/riga-city-boundaries-geojson/riga_boundaries_polygon.json\") as f:\n    geometry_collection = json.load(f)\n    riga_boundaries = geometry_collection['geometries'][0]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from shapely.geometry import shape, Point\n\n# Helper function to check whether point is fitting Riga borders\ndef is_within_riga_boundaries(y, x):\n    for polygon in shape(riga_boundaries):\n        if Point(x,y).within(polygon):\n            return True\n    return False\n\n# Verify that it's working by checking Riga center point\nis_within_riga_boundaries(56.946285, 24.105078)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from shapely.geometry import Point\n\n# Helper method to get tiles fitting Riga boundaries\ndef get_tiles_within_riga_boundaries(n_tiles):\n    points = get_equidistant_points(n_tiles)\n    points_filtered = list(filter(lambda point: is_within_riga_boundaries(*point), points))\n    point_to_rect = get_point_to_rect(n_tiles)\n    return list(map(lambda point: point_to_rect(*point), points_filtered))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Comparing results between original and improved approach\noriginal_tiles = get_tiles(30)\nfiltered_tiles = get_tiles_within_riga_boundaries(30)\nprint(\"Original tile count:\", len(original_tiles))\nprint(\"Filtered tile count:\", len(filtered_tiles))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Quite impressive improvement! We now have 3 times less features for bucket-encoded geo coordinates if we use tile density of 30 per-axis. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking result on the map\nplot_tiles(filtered_tiles)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are ready to create buckets out of tiled latitude and longitude values. This time we will not plot them, as such increased tile density is producing quite a heavy load on CPU."},{"metadata":{},"cell_type":"markdown","source":"In order to create a crossed column, we have to follow a pipeline:\n1. Create numeric columns for lat and lon\n2. Create bucketized columns out of created numeric columns\n3. Create a crossed column out of bucketized columns\n\nOut of three steps we will add to our final **feature_columns** array only the crossed column."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Optimal tile density\nn_tiles = 50\n# Grid of points\npoints = get_equidistant_points(n_tiles)\n# Points fitting Riga boundaries\npoints_filtered = list(filter(lambda point: is_within_riga_boundaries(*point), points))\n# Getting back to separate lists\nlatitudes, longitudes = zip(*points_filtered)\n# Getting back to unique sorted values\nlatitude_boundaries = list(set(latitudes))\nlongitude_boundaries = list(set(longitudes))\nlatitude_boundaries.sort()\nlongitude_boundaries.sort()\n\n# Numeric columns required to initialize bucketized columns later\nfeature_lat = feature_column.numeric_column('lat')\nfeature_lon = feature_column.numeric_column('lon')\n\n#latitude_boundaries = get_equidistant_coords('lat', n_tiles)\n#longitude_boundaries = get_equidistant_coords('lon', n_tiles)\n# Bucketize longitude and latitude using numeric columns and calculated boundaries\nbucketized_lat = feature_column.bucketized_column(feature_lat, boundaries=latitude_boundaries)\nbucketized_lon = feature_column.bucketized_column(feature_lon, boundaries=longitude_boundaries)\n\n\n# Create a crossed column from the bucketized values\ncrossed_geo = feature_column.crossed_column([bucketized_lat, bucketized_lon], hash_bucket_size=n_tiles*n_tiles)\ncrossed_geo = feature_column.indicator_column(crossed_geo)\nfeature_columns.append(crossed_geo)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that crossed_column does not build the full table of all possible combinations (which could be very large). Instead, it is backed by a hashed_column, so you can choose how large the table is by tuning **hash_bucket_size**."},{"metadata":{},"cell_type":"markdown","source":"# Neural network model\n\nWe will evaluate two neural network models: shallow one and the deep one. In order to reduce overfitting we will add dropout and early stopping."},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import DenseFeatures, Dense, Dropout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kernel_initializer='he_uniform'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"TensorFlow provides DenseFeatures layer that produces a dense Tensor based on given **feature_columns**. This way we can create input layer for our model very easily."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initializing array to store feature columns\nfeature_columns = []\n# Create numeric columns\nfor header in ['rooms', 'floor', 'total_floors']:\n    feature_columns.append(feature_column.numeric_column(header))\n    \n# Create bucketized columns\narea = feature_column.numeric_column(\"area\")\n#area_boundaries = list(np.arange(start=25, stop=df['area'].max(), step = 10))\narea_boundaries = [25.0, 35.0, 45.0, 55.0, 65.0, 75.0, 85.0, 95.0, 105.0, 115.0, 125.0, 155.0, 175.0, 225.0, 275.0, 325.0, 375.0, 425.0]\narea_buckets = feature_column.bucketized_column(area, boundaries=area_boundaries)\nfeature_columns.append(area_buckets)\nprint(\"Area boundaries\", area_boundaries)\n\n# Create categorical one-hot encoded columns\ncategorical_headers = ['op_type', 'house_seria', 'house_type', 'condition', 'district']\nfor header in categorical_headers:\n    categorical = feature_column.categorical_column_with_vocabulary_list(header, df[header].unique())\n    #feature_columns.append(feature_column.indicator_column(categorical))\n    \n# Create numeric latitude columns first\nfeature_lat = feature_column.numeric_column(\"lat\")\nfeature_lon = feature_column.numeric_column(\"lon\")\nlatitude_boundaries = get_equidistant_coords('lat', n_tiles)\nlongitude_boundaries = get_equidistant_coords('lon', n_tiles)\nbucketized_lat = feature_column.bucketized_column(feature_lat, boundaries=latitude_boundaries)\nbucketized_lon = feature_column.bucketized_column(feature_lon, boundaries=longitude_boundaries)\n\n# Create a crossed column from the bucketized values\ncrossed_geo = feature_column.crossed_column([bucketized_lat, bucketized_lon], hash_bucket_size=n_tiles*n_tiles)\ncrossed_geo = feature_column.indicator_column(crossed_geo)\nfeature_columns.append(crossed_geo)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_shallow_model():\n    model = Sequential()\n\n    #Input Layer\n    model.add(DenseFeatures(feature_columns))\n\n    #Hidden Layers\n    model.add(Dense(128, kernel_initializer=kernel_initializer, activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(Dense(64, kernel_initializer=kernel_initializer, activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(Dense(16, kernel_initializer=kernel_initializer, activation='relu'))\n    #Output Layer\n    model.add(Dense(1, kernel_initializer=kernel_initializer, activation='relu'))\n\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_deep_model():\n    model = Sequential()\n\n    #Input Layer\n    model.add(DenseFeatures(feature_columns))\n    \n    #Hidden Layers\n    model.add(Dense(512, kernel_initializer=kernel_initializer, activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(Dense(256, kernel_initializer=kernel_initializer, activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(Dense(64, kernel_initializer=kernel_initializer, activation='relu'))\n    model.add(Dense(32, kernel_initializer=kernel_initializer, activation='relu'))\n    model.add(Dense(16, kernel_initializer=kernel_initializer, activation='relu'))\n    model.add(Dense(8, kernel_initializer=kernel_initializer, activation='relu'))\n    \n    #Output Layer\n    model.add(Dense(1, kernel_initializer=kernel_initializer, activation='relu'))\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Helper method to evaluate model performance on different dataset combinations (all rows / sale-only rows / rent-only rows)\ndef compile_train_evaluate(model, min_delta, train_ds, val_ds, test_ds):\n    model.compile(\n      loss=tf.keras.losses.MeanAbsoluteError(), \n      optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9), \n      metrics=[tf.keras.metrics.MeanAbsoluteError()]\n    )\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(monitor='val_mean_absolute_error', min_delta=min_delta, patience=20,verbose=1)\n    ]\n    history = model.fit(train_ds,\n              validation_data=val_ds,\n              callbacks=callbacks,\n              epochs=150,\n              verbose=0)\n    loss, mae = model.evaluate(test_ds) \n    return [history, mae]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_sizes = [64]\nmodel_names = ['Deep']\nmodel_creators = [lambda: create_deep_model()]\ndataframe_names = ['All']\ndataframes = [dfs_all]\n\nmodels = {}\nhistories = {}\nresults = {}\n\nfor i, dfs in enumerate(dataframes, start=0):\n    df_name = dataframe_names[i]\n    for j, create_model in enumerate(model_creators, start=0):\n        model_name = model_names[j]\n        for batch_size in batch_sizes:\n            key = \"[model=\" + model_name + \"][dataframe=\" +  df_name + \"][batch_size=\" + str(batch_size) + \"]\"\n            models[key] = create_model()\n            datasets = get_input_data_sets(batch_size, *dfs)\n            # Different early stopping parameters for \"Rent-only\" dataset\n            min_delta = 5 if \"Rent\" in key else 100\n            print('Evaluating configuration: ' + key)\n            history, mae = compile_train_evaluate(models[key], min_delta, *datasets)\n            results[key] = mae\n            histories[key] = history\n            models[key].save('models')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check all results\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"figure, axes = plt.subplots(nrows=0, ncols=len(histories))\ndef plot_mae(history, name):\n    plt.title(name)\n    plt.ylabel('MAE')\n    plt.xlabel('epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    plt.plot(history.history['mean_absolute_error'])\n    plt.plot(history.history['val_mean_absolute_error'])\n    plt.show()\n\nfor name, history in histories.items():\n    plot_mae(history, name) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Verification on the real samples\n\nIt is always enjoyable to see machine learning performing on the real-world data, so I have kindly asked my friend to disclose information on his flat located in Riga for the sake of data science:\n* Discrict: Teika\n* Number of rooms: 3\n* Area: 75.3 m2\n* Floor: 3\n* Total floors: 3\n* House seria: StaÄ¼ina\n* House type: Brick\n* Location: 56.967406Â° N 24.186564Â° E\n* Actual purchase price: 72 000 EUR\n\nLet's compare it with the price predicted by the model!\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_test_property(op_type):\n    return {\n        'op_type': [op_type],\n        'district': ['Teika'],\n        'street': [''],\n        'rooms': [3],\n        'area': [75.3],\n        'floor': [3],\n        'total_floors': [3],\n        'house_seria': ['StaÄ¼ina'],\n        'house_type': ['Brick'],\n        'condition': ['All amenities'],\n        'lat': [56.967406], \n        'lon': [24.186564],\n        'price': [0]\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Helper method to prepare single real estate object to be fed into model\n# As model was trained using batches, in order to make a single prediction we have feed it with the data of the same batch size\n# Please let me know if there is more elegant way to do this\ndef create_test_ds(batch_size, real_estate_object):\n    test_df = pd.DataFrame.from_dict(real_estate_object)\n    # Creating input of the same batch size as model was trained on\n    test_df.append([real_estate_object]*(batch_size-1),ignore_index=True)\n    return input_fn(test_df, shuffle=False, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rent_test_ds = create_test_ds(64, get_test_property('For rent'))\n\nfor key in models.keys():\n    if \"Rent\" in key or \"All\" in key:\n        print(\"Rent prediction:\", models[key].predict(rent_test_ds)[0][0], \"EUR by\", key)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sale_test_ds = create_test_ds(64, get_test_property('For sale'))\n\nfor key in models.keys():\n    if \"Sale\" in key or \"All\" in key:\n        print(\"Sale prediction:\", models[key].predict(sale_test_ds)[0][0], \"EUR by\", key)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('TensorFlow Version: {}'.format(tf.__version__))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = models['[model=Deep][dataframe=All][batch_size=64]']\ntf.keras.models.save_model(model, 'models')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights('weights.index')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}