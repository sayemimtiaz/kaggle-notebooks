{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/company-bankruptcy-prediction/data.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"df['Bankrupt?'].value_counts().plot(kind='bar')\nsns.set_style('whitegrid')\nsns.set_palette('bwr')\nplt.title('Survive(0) vs Bankrupt(1)')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\nsns.set_style('whitegrid')\nsns.set_palette('bwr')\nsns.boxplot(data = df.drop(['Bankrupt?'], axis=1), orient='h', color='blue')\nplt.title('Data range')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = df['Bankrupt?']\nX = df.drop(['Bankrupt?'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1.SVM","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import f1_score, recall_score, accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify = y, random_state=15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train score of SVM model\nfor weight in [1, 3, 5, 6, 10, 50, 100]:\n    #Build SVC model\n    pipe_svc = Pipeline(steps=[('scale', StandardScaler()), ('SVC', SVC(class_weight={0:1, 1:weight}))])\n    score = cross_val_score(pipe_svc, X_train, y_train, n_jobs=4, scoring = 'f1', cv=5).mean()\n    print('Mean F1 cross-val-score for model with weight %i is %.2f' % (weight, score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the results above we pick the weight of for SVM is 0:1 - 1:6","metadata":{}},{"cell_type":"code","source":"#Train and evaluate on test set\npipe_svc = Pipeline(steps=[('scale', StandardScaler()), ('SVC', SVC(class_weight={0:1, 1:6}, random_state=10))])\npipe_svc.fit(X_train, y_train)\ny_pred = pipe_svc.predict(X_test)\ntest_f1 = f1_score(y_test, y_pred)\nprint('Test f1_score is', test_f1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_recall =  recall_score(y_test, y_pred)\nprint('Test recall score is ', test_recall)\ntest_accuracy = accuracy_score(y_test, y_pred)\nprint('Test accuarcy score is', test_accuracy) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.heatmap(y_pred.reshape(2046,1), cmap='bwr')\nplt.title('Visualize predicted bankrupt company')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. PCA","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Scale X_train**","metadata":{}},{"cell_type":"code","source":"Scaled_X_train = StandardScaler().fit_transform(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Choosing the number of components**","metadata":{}},{"cell_type":"code","source":"##Choosing the best number of components\nTEV = []\nn_components = np.arange(3,50)\nfor n in n_components:\n    pca = PCA(n_components = n)\n    pca.fit_transform(Scaled_X_train)\n    total_variance = pca.explained_variance_ratio_.sum()\n    TEV.append(total_variance)\n\n##Plotting total explained variance to the number of components\nsns.set_style('whitegrid')\nsns.set_palette('bwr')\nplt.figure(figsize = (10,8))\nplt.plot(n_components, TEV)\nplt.xlabel('Number of components')\nplt.ylabel('Total explained variance')\nplt.title('Total explained variance per components')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see from above that we need roughly 50 components to explain 95% of the data variance. However, the first 3 components already explain nearly 25% of the variances. \n\nOff the back of this notebook, I've tried visualise the dataset with 8 components and 3 components. I noticed that if only using 3 components, I can intereprete the data quite well using my business knowledge already, though the total variance is not as high as 8. The 3 components extracted do not share any dominant features, while it also have strong internal consistency and is closely related to existing business/ finance concepts. \n\nI'll demonstrate how the data can be visualised with 3 components and write some brief business interpretation of each components below. ","metadata":{}},{"cell_type":"code","source":"#Fit and visualised the PCA with 8 components:\npca = PCA(n_components = 3, random_state = 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t_X_train = pca.fit_transform(Scaled_X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t_X_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca.components_[0][:20]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#PCs = ['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8']\nPCs = ['PC1', 'PC2', 'PC3']\ncomponents_df = pd.DataFrame(pca.components_)\ncomponents_df = components_df.T\ncomponents_df.columns = PCs\ncomponents_df.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(2, 1, figsize=(14,8), gridspec_kw={'height_ratios':[4,1]})\nsns.heatmap(components_df,ax = ax[0], cmap='bwr')\nax[0].set_xlabel('Components')\nax[0].set_ylabel('Feature weights')\nax[0].set_title('Feature weights heatmap of PCA')\n\nax[1].bar(PCs, pca.explained_variance_ratio_)\nax[1].set_xlabel('Components')\nax[1].set_ylabel('Explained variance ratio of each components')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_val = pca.explained_variance_ratio_.sum()\nprint('Total variance explained by these components is %.2f' %total_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"** Analysing the components' structure**","metadata":{}},{"cell_type":"markdown","source":"Fromt the heat map, we can see that each component is constructed by 4 - 5 dominant features (those with lightest or darkest color). It's reasonable to consider them to be the more meaningful metrics to look at when monitoring company's likelihood to when bankrupt. \n\nI'll map the features index with their name to have a closer look at what they are. ","metadata":{}},{"cell_type":"code","source":"##Get names and weights of 5 most dominant featyres for each component:\nPC_feature_names = []\nPC_feature_weights = []\nfor PC in PCs:\n    dominant_features = np.abs(components_df[PC]).sort_values(ascending = False)[:5]\n    for i in dominant_features.index:\n        PC_feature_weights.append(components_df[PC][i])\n    for i in dominant_features.index:\n        PC_feature_names.append(X.columns[i]) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Create 8 dataframes to store names and weights of 8 components:\n#PC_df = ['PC1_df', 'PC2_df', 'PC3_df', 'PC4_df', 'PC5_df', 'PC6_df', 'PC7_df', 'PC8_df']\nPC_df = ['PC1_df', 'PC2_df', 'PC3_df']\nfor i in np.arange(0, 15, 5):\n     PC_df[int(i/5)]= pd.DataFrame(zip(PC_feature_names[i:i+5], PC_feature_weights[i:i+5]), columns = ['Feature names',  PC_df[int(i/5)]])\n\n## Noted that some components have the same dominant feature, therefore I Outter joined 8 dataframes to visualise components with unique features:\nPC_DF = PC_df[0]\nfor i in np.arange(1, 3):\n    PC_DF = PC_DF.merge(PC_df[i], on='Feature names', how='outer')\nPC_DF = PC_DF.fillna(0).set_index('Feature names')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Create a heatmapt to visualise the construction of each components\nplt.figure(figsize=(12,12))\nsns.heatmap(PC_DF, cmap = 'bwr')\nplt.title('Visualize Components Structure')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"I like how each components do not share any features. There're some internal correlation between the feature of one components as well. In details: \n\nPC1: Profitability: All features making up this componets are related to return, earning, profit. The main differences in each feature are how tax, interest, depreciatation, capital and seasonality are taken into account. \n\nPC2: Liability: 4/5 features are related to how much debt (long/short term) the company bares. \n\nPC3: Liquidity: 2/5 features are clear indicator of the company liquidity (Current Assets/ Total Assets, Quick Assets/ Total Assets), while the other 3 indicate the demand for liquidifiable assets to finance company operations (Total Asset Turnover) or debt repayment (Current Liability to Assets).","metadata":{}},{"cell_type":"markdown","source":"# **Visualise data using PCA components**","metadata":{}},{"cell_type":"code","source":"#Create PCA train data:\nPC_train_df = pd.DataFrame(t_X_train)\nPC_train_df.columns = ['Profitability', 'Liability', 'Liquidity']\nPC_train_df = pd.concat([y_train.reset_index().drop(columns='index'), PC_train_df], axis=1)\nPC_train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Visualise data on a 3D map:\nfig = plt.figure(figsize = (16, 16))\nax = fig.add_subplot(111,projection='3d')\nax.scatter(PC_train_df['Profitability'], PC_train_df['Liability'], PC_train_df['Liquidity'], c=PC_train_df['Bankrupt?'], cmap = 'viridis')\nax.set_xlabel('PC1 - Profitability')\nax.set_ylabel('PC2 - Liability')\nax.set_zlabel('PC3 - Liquidity')\nax.set_title('Bankruptcy visualisation with PCA')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Visualise on 2D relational plot:\nplt.figure(figsize = (16, 6))\nsns.relplot(\n    data=PC_train_df, x='Liability', y=\"Profitability\",\n    col=\"Bankrupt?\", hue=\"Liquidity\",kind=\"scatter\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The visualising effort does not seem to be as rewarding as I thought. However, it's quite clear that PC1 - Profitability does have some predictive power as most bankrupt cases have PC1 value larger than 0.","metadata":{}},{"cell_type":"markdown","source":"# **Taking a step further: Using liability to predict profitability**","metadata":{}},{"cell_type":"markdown","source":"I noted there are opposite linear trends in the chart above. \n- For surviving companies, their profitability and liability components follow a negative linear relationship. \n- For bankrupt companies, their profitability and liability components follow a positive linear relationship. \n\nI'll take some statistic tests to see if this relationship is meaningful.","metadata":{}},{"cell_type":"code","source":"## Plot the regression line\nplt.figure(figsize = (16, 16))\nsns.lmplot(x=\"Liability\", y=\"Profitability\", col=\"Bankrupt?\",\n               data=PC_train_df, scatter=True, fit_reg=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Check the residual plot\nfig, ax =  plt.subplots(1, 2, figsize = (16, 6))\nsns.residplot(\n    data=PC_train_df[PC_train_df['Bankrupt?']==0], x='Liability', y=\"Profitability\", ax=ax[0])\nsns.residplot(\n    data=PC_train_df[PC_train_df['Bankrupt?']==1], x='Liability', y=\"Profitability\", ax=ax[1])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import statsmodels.api as sm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Check regression model for surviving company: \ndata = PC_train_df[PC_train_df['Bankrupt?']==0]\nX = data['Liability']\ny = data['Profitability']\n\nresults = sm.OLS(y,X).fit()\nprint(results.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Check regression model for surviving company: \ndata = PC_train_df[PC_train_df['Bankrupt?']==1]\nX = data['Liability']\ny = data['Profitability']\n\nresults = sm.OLS(y,X).fit()\nprint(results.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the relationship in both models are statistically significant (Prob (F-statisitc)) < 0.005). It looks like for bankrupt companies, liability can predict profitability better than for surviving companies (R-squared is 35.1% versus 6.9%). \n\nThis raises several interesting hypothesis regarding the nature of bankrupt companies. My first thought is how company's interest baring debt may play a role in the company state's of survival. (i.e: Bankrupt companies may have liability with higher interest than surviving companies, which hurts their profitability). Another hypothesis is where company spend their revenue (i.e: Bankrupt companies may spend all their revenue paying back debt instead of investments to increase profitability). These hypothesis need to be validated using business knowledge, not just statistical models, so I'll end the notebook here and leave the question open for anyone interested. ","metadata":{}}]}