{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom itertools import combinations\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/supermarket/GroceryStoreDataSet.csv',sep=',',header=None,index_col=False)\ndata['I1'],data['I2'],data['I3'],data['I4'] = np.nan,np.nan,np.nan,np.nan\nfor r in range(data.shape[0]):\n    l = data.iloc[r,0].split(',')\n    n = len(l)\n    for i in range(1,n+1):\n        data.iloc[r,i] = l[i-1]\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Initializing minimum support score, records lookup table and itemlist\n## Here, let minimum support score = 2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"min_sup,records = 2,[]\nfor i in range(0,data.shape[0]):\n    records.append([str(data.values[i,j]) for j in range(1,len(data.columns)) if str(data.values[i,j]) != 'nan'])\nitemlist = sorted([item for sublist in records for item in sublist if item != np.nan])\nrecords","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# STAGE 1 - First frequent itemset (k=1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def stage_1(itemlist,min_sup):\n    c1 = {i: itemlist.count(i) for i in itemlist}\n    l1 = {}\n    for key,val in c1.items():\n        if val >= min_sup:\n            l1[key] = val\n    return c1,l1\n\n# Test run\nc1,l1 = stage_1(itemlist,min_sup)\nprint(c1)\nprint(l1)\n\ndf_stage1 = pd.DataFrame(l1,index=['sup_count']).T\ndf_stage1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion : All items pass the minimum support threshold\n# STAGE 2 - Rule of pairing (k=2)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Function to check if for each subset of the current itemlist(k), whether the combination of k-1 items(previous grouping/pairing),\n  belongs to the previous itemlist, so that it qualifies to be a frequent itemlist. \n  Arguments : current itemlist, previous itemlist, n(= k-1)'''\ndef check_freq(curr,prev,n):\n    if n > 1:\n        subsets = list(combinations(curr,n))\n    else:\n        subsets = curr\n    for item in subsets:\n        if not item in prev:\n            return False\n        else:\n            return True\n\n'''Function to check if i1 is a sublist/subset of i2'''\ndef sublist(i1,i2):\n    return set(i1) <= set(i2)\n\ndef stage_2(l1,records,min_sup):\n    l1 = sorted(list(l1.keys()))\n    L1 = list(combinations(l1,2))\n    c2,l2 = {},{}\n    for it1 in L1:\n        count = 0\n        for it2 in records:\n            if sublist(it1,it2):\n                count += 1\n        c2[it1] = count\n    for key,val in c2.items():\n        if val >= min_sup:\n            if check_freq(key,l1,1):\n                l2[key] = val\n    return c2,l2\n\n# Test run\nc2,l2 = stage_2(l1,records,min_sup)\nl2 = {key: value for key,value in l2.items() if value != 0}\nprint(c2)\nprint(\"\\n\",l2)\nprint(\"\\nNo. of itemsets = {}, No. of frequent itemsets = {}\".format(len(list(c2)),len(list(l2))))\ndf_stage2 = pd.DataFrame(l2,index=['sup_count']).T\ndf_stage2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# STAGE 3 - Rule of Self-Join (k=3)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def stage_3(l2,records,min_sup):\n    l2 = list(l2.keys())\n    L2 = sorted(list(set([item for temp in l2 for item in temp])))\n    L2 = list(combinations(L2,3))\n    c3,l3 = {},{}\n    for it1 in L2:\n        count = 0\n        for it2 in records:\n            if sublist(it1,it2):\n                count += 1\n        c3[it1] = count\n    for key,val in c3.items():\n        if val >= min_sup:\n            if check_freq(key,l2,2):\n                l3[key] = val\n    return c3,l3\n\n# Test run\nc3,l3 = stage_3(l2,records,min_sup)\nl3 = {key: value for key,value in l3.items() if value != 0}\nprint(\"CURRENT ITEMSETS : \\n\\n\",c3)\nprint(\"\\nCURRENT FREQUENT ITEMSETS : \\n\\n\",l3)\nprint(\"\\nNo. of itemsets = {}, No. of frequent itemsets = {}\".format(len(list(c3)),len(list(l3))))\ndf_stage3 = pd.DataFrame(l3,index=['sup_count']).T\ndf_stage3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# STAGE 4 (LAST STAGE) - Rule of quadruplets (k=4)\n### **How can we say confidently that this is the last stage ?** --> Since the maximum no. of items a person has bought in any transaction is 4, so we can't possibly form frequent itemlists of size 5. Hence, we stop at 4. Now we check if quadruplet is acceptable or we've to settle for triplets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def stage_4(l3,records,min_sup):\n    l3 = list(l3.keys())\n    L3 = sorted(list(set([item for temp in l3 for item in temp])))\n    L3 = list(combinations(L3,4))\n    c4,l4 = {},{}\n    for it1 in L3:\n        count = 0\n        for it2 in records:\n            if sublist(it1,it2):\n                count += 1\n        c4[it1] = count\n        for key,val in c4.items():\n            if val >= min_sup:\n                if check_freq(key,l3,3):\n                    l4[key] = val\n    return c4,l4\n\n# Test run\nc4,l4 = stage_4(l3,records,min_sup)\nl4 = {key: value for key,value in l4.items() if value != 0}\nprint(\"CURRENT ITEMSETS : \\n\\n\",c4)\nprint(\"\\nCURRENT FREQUENT ITEMSETS : \\n\\n\",l4)\nprint(\"\\nNo. of itemsets = {}, No. of frequent itemsets = {}\".format(len(list(c4)),len(list(l4))))\ndf_stage4 = pd.DataFrame(l4,index=['sup_count']).T\ndf_stage4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CONCLUSIONS (1) : \n1. People tend to buy biscuit, cock, coffee, cornflakes together.\n2. For the 2nd part of the conclusion, we create association rules on l3.\n\n# BUILDING THE **ASSOCIATION RULES for MINING (ARM)** :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"items = {**l1,**l2,**l3,**l4}\nitems","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Working on l3 to break the triplets to form dual pair + individual item comnbination sets for forming the association rules (like, {A,B,C} => {A,B} --> {C} and more)'''\nassc_sets = []\nfor it1 in list(l3.keys()):\n    assc_subset = list(combinations(it1,2))\n    assc_sets.append(assc_subset)\n\n'''Implementing the association rule.\n   An association rule is formed iff the confidence of that rule exceeds the minimum confidence threshold.\n   Assuming minimum confidence = 50%\n'''\nmin_conf = 50\n# Function to calculate support score\ndef sup_calc(it,items):\n    return items[it]\n# Calculating confidence\nl3_assc = list(l3.keys())\nselected_assc = []\nfor i in range(len(l3_assc)):\n    for it1 in assc_sets[i]:\n        denom = it1\n        d = list(denom)\n        num = set(l3_assc[i]) - set(it1)\n        n = list(num)\n        confidence = ((sup_calc(l3_assc[i],items))/(sup_calc(it1,items)))*100\n        if confidence > min_conf:\n            print(\"Confidence of the association rule {} --> {} = {:.2f}%\".format(denom,num,confidence))\n            print(\"STATUS : SELECTED RULE\\n* People who buy {} and {} also tend to buy : {} *\\n\".format(d[0],d[1],n[0]))\n        else:\n            print(\"Confidence of the association rule {} --> {} = {:.2f}%\".format(denom,num,confidence))\n            print(\"STATUS : REJECTED RULE\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CONCLUSION (2) :\n### The association rules with 'STATUS : SELECTED' are the appropriate rules.\n# HOW TO READ THE ASSOCIATION RULES ?\n### \"{A,B} --> {C}\" : Customers who buy items A and B also tend to buy item C","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}