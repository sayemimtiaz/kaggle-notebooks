{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Churn Prediction with XGBoost and hyperopt\n\nExtreme Gradient Boosting is used to predict customer churn. To select the best hyperparameters a 5-Fold Cross Validation with bayesian search is used. \nSince the dataset is unbalanced with respect to Churn the area under the ROC curve (roc_auc) was used as criterium for the selection."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing the data for machine learning\n\n1. Set empty TotalCharges to 0 and convert it to numeric\n2. Encode Churn as 0 for 'No' and 1 for 'Yes'\n3. One-Hot encoding of categorical data\n4. Drop customerID"},{"metadata":{"trusted":true},"cell_type":"code","source":"telco_original = pd.read_csv('../input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n\ntelco_original['TotalCharges'] = telco_original.TotalCharges.replace({' ': 0})\ntelco_original['TotalCharges'] = pd.to_numeric(telco_original.TotalCharges, errors='coerce')\n# remove the 9 rows with missing values\nprint(telco_original.info())\n\ntelco_original = telco_original.drop('customerID', axis=1)\n\ntelco_original['Churn'] = telco_original.Churn.replace({'No': 0, 'Yes':1})\n\nX, y = telco_original.drop('Churn', axis=1), telco_original.Churn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the object columns contain only few categories (<5) and can be OneHot encoded (this step could have been done before splitting too, but for consistency with the standardization procedure it will be done after)."},{"metadata":{"trusted":true},"cell_type":"code","source":"telco_original.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\n# generate the list of categorical and numerical variables\ncategorical_variables = X.nunique()[X.nunique() < 5].keys().to_list()\n\nnumerical_variables=list(set(X.columns) - set(categorical_variables))\n\nohe = OneHotEncoder(drop='first', sparse=False)\n\nX_ohe = ohe.fit_transform(X[categorical_variables])\nX_ohe_df = pd.DataFrame(X_ohe, columns=ohe.get_feature_names(categorical_variables))\n\n# Merging the transformed dataframe togheter\nX = pd.merge(X[numerical_variables], X_ohe_df, left_index=True, right_index=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, \n                                                    test_size=0.2, random_state=123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Models fitting\n"},{"metadata":{},"cell_type":"markdown","source":"The hyperparameters tuning of the XGBoost model will be accomplished with a bayesian optimization using Tree Parzen Estimator implemented in the hyperopt package."},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom hyperopt import STATUS_OK\n\ntrain_dmatrix = xgb.DMatrix(data=X_train, label=y_train)\n\nN_FOLDS = 5\n\n# define objective to minimize\ndef objective(params, n_folds = N_FOLDS):\n    params['objective'] = 'binary:logistic'\n    # Perform cross-validation: cv_results\n    cv_results = xgb.cv(dtrain=train_dmatrix, params=params,\n                  nfold=n_folds, num_boost_round=10000, early_stopping_rounds=100, \n                  metrics=\"auc\", as_pandas=True, seed=123)\n\n    # Print the accuracy\n    loss = 1 - cv_results[\"test-auc-mean\"].iloc[-1]\n    n_estimators = cv_results[\"test-auc-mean\"].idxmax() + 1\n    return {'loss': loss, 'params': params, 'n_estimators': n_estimators, 'status': STATUS_OK}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from hyperopt import hp\nfrom hyperopt.pyll.stochastic import sample\n\nhyperparameter_space = {\n    'n_jobs': -1,\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.6, 1.0),\n    'subsample': hp.uniform('subsample', 0.6, 1.0),\n    'min_child_weight': hp.quniform('min_child_weight', 1, 7, 2),\n    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n    'max_depth': hp.randint('max_depth', 1,16),\n    'gamma': hp.uniform('gamma', 0.1,0.4),\n    'max_delta_step': hp.randint('max_delta_step',0,10),\n    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2))\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from hyperopt import Trials\n\nbayes_trials = Trials()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from hyperopt import fmin\nfrom hyperopt import tpe\n\nMAX_EVALS = 50\n\nbest = fmin(fn = objective, space = hyperparameter_space, algo = tpe.suggest, max_evals = MAX_EVALS,\n           trials = bayes_trials, rstate = np.random.RandomState(50))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best['num_boost_round']=10000\nbest['early_stopping_rounds']=100\n\nxgb_best = xgb.XGBClassifier(**best)\n\nxgb_best.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import roc_curve\nfrom sklearn.metrics import roc_curve\nimport matplotlib.pyplot as plt\n\ndef add_roc_plot(model, test_x, test_y, legend_text):\n    y_pred_prob = model.predict_proba(test_x)[:, 1]\n    # Calculate the roc metrics\n    fpr, tpr, thresholds = roc_curve(test_y, y_pred_prob)\n    # Plot the ROC curve\n    plt.plot(fpr, tpr, label=legend_text)\n    plt.legend()\n\n    \nmodels_list = [xgb_best]\nmodel_names = ['Extreme Gradient Boosting']\n\nplt.figure(figsize=(6, 6))\n[add_roc_plot(model, X_test, y_test, legend_text) for model, legend_text in zip(models_list, model_names)]\n\n# Add labels and diagonal line\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlim((0,1))\nplt.ylim((0,1))\nplt.plot([0, 1], [0, 1], \"k-\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_score\n\nlist_scores = [roc_auc_score, recall_score, precision_score, accuracy_score]\ncalc_scores = []\ndef compute_scores(model, x_test, y_test, scores):\n    return [round(score(y_test, model.predict(x_test)), 2) for score in scores]\n    \n[calc_scores.append(compute_scores(model, X_test, y_test, list_scores)) for model in models_list] \n\nscore_names = ['roc_auc', 'recall', 'precision', 'accuracy']\nscores_df = pd.DataFrame(calc_scores, columns=score_names, index=model_names)\n\nscores_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1,1,figsize=(12,10))\nxgb.plot_importance(xgb_best, ax = ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}