{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Linear Discriminant Analysis <br>\nTill now, we have mostly dealt and seen PCA for dimensionality reduction. It's an unsupervised algorithm which  is used for clustering and doesn't take into account target labels . What if, we need a method which take features as well as labels for clustering. Such a requirement is handled by LDA."},{"metadata":{},"cell_type":"markdown","source":"In this tutorial, we will see another dimesionality reduction technique called LDA and compare it with PCA. Later on, we will see how we can use LDA to do topic modelling and how we can cluster documents based on this topic modelling. Tutorial goes in three steps.\n* **LDA**\n* **LDA vs PCA**\n* **Topic Modelling**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import nltk\nimport numpy as np\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import *\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import  CountVectorizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.decomposition import PCA,LatentDirichletAllocation\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### PCA revisit\n\nI will try to be as less technical as possible here.\n\nIf we have say 1000 features in our dataset. It is impossible to visualise them and too many features may lead to [curse of dimensionaliy](https://towardsdatascience.com/curse-of-dimensionality-2092410f3d27). It is primarily due to these issues we change our dataset to lower dimension.\nLet's say we transform our dataset to 2 dimension data using PCA.These two new dimensions are called principal components. Now, our data originally will have variation in many directions. first component will be along the the direction of maximum variance and second component will be along the direction of second maximum variance. Each component will explain certain amount of variance of data.\n "},{"metadata":{},"cell_type":"markdown","source":"### LDA(Linear Discriminant Analysis)  and LDA vs PCA\n\n**LDA** is a dimesionality reduction tecnique.It transforms  data from  say 'n' dimensions to 'k'dimensions.   It is pretty similar to PCA in terms of in output but with one **major** difference. LDA is a supervised algorithm whereas PCA is not, PCA ignores **class labels**.\n\n As we know, PCA tries to find directions of maximum variance. PCA projects  data onto new axis in such a way they explain the maximum variance without taking class labels into consideration. **LDA** on the other hand, creates new axis in such a way that when we project data on this axis, there is a maximum separation bewtween two class categories. LDA tries to separate classes as much as feasible on the new axis. \n \nBelow is the demonstration of same with Iris dataset.  "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data=load_iris().data\ntarget=load_iris().target\ntarget_names=load_iris().target_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe=pd.DataFrame(data=np.concatenate((data,target.reshape(150,1)),axis=1),columns=['col_1','col_2','col_3','col_4','target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe.drop(columns=['target'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA (n_components=2)\nX_feature_reduced = pca.fit(dataframe).transform(dataframe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('First component explain {} variance of data and second component explain {} variance of data'.format(pca.explained_variance_ratio_[0],pca.explained_variance_ratio_[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(X_feature_reduced[:,0],X_feature_reduced[:,1],c=target)\nplt.title(\"PCA\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's analyse how dimesnionality reduction is done in LDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"lda = LatentDirichletAllocation(n_components=2)\nX_feature_reduced = lda.fit(dataframe).transform(dataframe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(X_feature_reduced[:,0],X_feature_reduced[:,1],c=target)\nplt.title('LDA')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation\nAs we can see from above that LDA projected data on new axis in such a way that class are separated as much as possible on the new axis."},{"metadata":{},"cell_type":"markdown","source":"#### How does LDA achieves this?\n\nLDA creates new axis based on two criteria:\n* Distance between means of classes\n* Variation within each category\n\nIt projects data on new axis and finds mean for each class and variance for each class. It tries to maximise the distance between class means and tries to minimise the variation with each class. Using these into consideration we get a new axis."},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://doc-00-1s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/bungn2im34a1bg89ogch4ck0itg96ddv/1555668000000/17997770589742191698/*/1nCniO6xSTL6XXMluJo45FdTiDSbhwpyu\" width=\"500px\">\n\nAbove is the data for two Genes, we want to project them on new axis with one dimension."},{"metadata":{},"cell_type":"markdown","source":"\n<img src=\"https://doc-14-1s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/jq93dbg311mj8g829oauf672o0ou0fne/1555668000000/17997770589742191698/*/1I6DsctCgnYhwEIeV0n1-BF2i_4e_bviA\" width=\"500px\">\n\nNow, while projecting the data on new axis, we want to maximise the mean distance and minimise the variation.\n"},{"metadata":{},"cell_type":"markdown","source":"Criterion which we choose above to solve this is\n\n**(µ1-µ2)^2\n/(s1+s2)^2**\n\n,where µ1 and µ2 are mean of each class and 's1 and s2' are variation/scatter within a class while making new axis\n\nWe try to **maximise** this criteria while making new axis."},{"metadata":{},"cell_type":"markdown","source":"### Topic modelling \n\n**Topic modelling**  is a method of assigning topic to each document. Each topic is made up of certain words.\n\nConsider for example:\n\nWe have two topics, topic 1 and topic 2. **'Topic1'** is represented by 'apple, banana, mango' and **topic2** is represented by 'tennis, cricket, hockey'. We can infer that topic1 is talking about fruits and topic2 is talking about sports. We can assign new incoming document into one of these topics and that can be used for **clustering** purpose too. It is used in recommendation systems and many more. \n\nAnother  example: \nConsider we have 6 documents\n* apple banana\n* apple orange\n* banana orange\n* tiger cat\n* tiger dog\n* cat dog\n\nWhat topic modelling would do is if want to extract say two topic out of these  documents, it will give two distributions, topic-word distribution and doc-topic distribution.  In topic-word representation it should give word wise distribution for each topic and in doc-topic it would give for each document, it's topic representation or distribution of document for each topic.\n\nIt's ideal topic-word distribution should be:\n\n|  Topic | Apple | Banana | Orange | Tiger | Cat | Dog | \n| --- | --- | --- | --- | --- | --- | --- | \n| Topic 1 |   .33 | .33 | .33 | 0 | 0 | 0 |\n| Topic 2 |   0 | 0 | 0 | 0.33 | 0.33 | 0.33 |\n\nand it's ideal document-topic distrubution should be:\n\n|  Topic | doc1 | doc2 | doc3 | doc4 | doc5 | doc6 | \n| --- | --- | --- | --- | --- | --- | --- | \n| Topic 1 |   1 | 1 | 1 | 0 | 0 | 0 |\n| Topic 2 |   0 | 0 | 0 | 1 | 1 | 1 |\n\nand now suppose we have a new document say, ' cat dog apple', its topic wise representation should be\n\n\n**Topic1:  0.33**\n\n**Topic2: 0.63**\n\n\nLDA is highly used for this purpose.It's usage for topic  modelling has been demonstrated below. We give to it the number of topics we want to find out of the corpus. Remember it follow bow approach therefore, relationship between words are lost in this manner.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatizer=WordNetLemmatizer() #For words Lemmatization\nstop_words=set(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def TokenizeText(text):\n    ''' \n     Tokenizes text by removing various stopwords and lemmatizing them\n    '''\n    text=re.sub('[^A-Za-z0-9\\s]+', '', text)\n    word_list=word_tokenize(text)\n    word_list_final=[]\n    for word in word_list:\n        if word not in stop_words:\n            word_list_final.append(lemmatizer.lemmatize(word))\n    return word_list_final","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gettopicwords(topics,cv,n_words=10):\n    '''\n        Print top n_words for each topic.\n        cv=Countvectorizer\n    '''\n    for i,topic in enumerate(topics):\n        top_words_array=np.array(cv.get_feature_names())[np.argsort(topic)[::-1][:n_words]]\n        print (\"For  topic {} it's top {} words are \".format(str(i),str(n_words)))\n        combined_sentence=\"\"\n        for word in top_words_array:\n            combined_sentence+=word+\" \"\n        print (combined_sentence)\n        print (\" \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.listdir(\"../input/million-headlines/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/million-headlines/abcnews-date-text.csv',usecols=[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \nnum_features=100000\ncv=CountVectorizer(tokenizer=TokenizeText,max_features=num_features,ngram_range=(1,2))\ntransformed_data=cv.fit_transform(df['headline_text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nno_topics=10  ## We can change this, hyperparameter\nlda = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(transformed_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lda.components_** is a topic_word table, it shows representation of each word in the topic. components_[i, j] can be viewed as pseudocount that represents the number of times word j was assigned to topic i. It can also be viewed as distribution over the words for each topic after normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"gettopicwords(lda.components_,cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Assigning new topic \nWe can see that each document is a combination of each topic. Let's see topic representation of first ten documents.\nFirst ten documents and their topicwise representation is shown below."},{"metadata":{"trusted":true},"cell_type":"code","source":"docs=df['headline_text'][:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=[]\nfor doc in docs:\n    data.append(lda.transform(cv.transform([doc])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols=['topic'+str(i) for i in range(1,11)]\ndoc_topic_df=pd.DataFrame(columns=cols,data=np.array(data).reshape((10,10)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc_topic_df['major_topic']=doc_topic_df.idxmax(axis=1)\ndoc_topic_df['raw_doc']=docs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc_topic_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"Complete\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We saw how LDA can be used for topic modelling. This can be used for document custering based on the doc topic  representation."},{"metadata":{},"cell_type":"markdown","source":"### References\n[Statquest LDA](https://www.youtube.com/watch?v=azXCzI57Yfc)\n\n[https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/](https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/)\n\n[https://sebastianraschka.com/faq/docs/lda-vs-pca.html](https://sebastianraschka.com/faq/docs/lda-vs-pca.html)\n\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}