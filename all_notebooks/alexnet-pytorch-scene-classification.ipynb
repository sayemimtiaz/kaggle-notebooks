{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Imports"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport torchvision\nfrom torch.utils.data import DataLoader, Dataset\nfrom skimage import io, color\nimport pandas as pd\nimport os\nfrom tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Implementing Local Response Normalization"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Takes inputs with dims = (N, C, *)\n# Gives outputs with dimes = (N, C, *)\nclass LocalResponseNormalization(nn.Module):\n    def __init__(self, neighbourhood_length, normalisation_const_alpha, contrast_const_beta, noise_k):\n        super(LocalResponseNormalization, self).__init__()\n        self.nbd_len = neighbourhood_length\n        self.alpha = normalisation_const_alpha\n        self.beta = contrast_const_beta\n        self.k = noise_k\n    \n    # The following is exactly what pytorch does under the hood as well. I only replicated it for my understanding :)\n    def forward(self, x):\n        # Lets validate if x is atleast 3 dimensional\n        dim = x.dim()\n        if dim < 3:\n            raise ValueError(\"Expected tensor of atleast 3 dimensions, found only {}\".format(dim))\n        denom = x.pow(2).unsqueeze(1)\n        if dim == 3:\n            denom = F.pad(denom, (0, 0, self.nbd_len // 2, (self.nbd_len - 1) // 2))\n            denom = F.avg_pool2d(denom, (self.nbd_len, 1), stride=1)\n            denom = denom.squeeze(1)\n        else:\n            sizes = x.size()\n            # The last two dimensions make up a single channel. The third dimension decides the number of channels\n            # across which we will apply local response normalization.\n            denom = denom.view(sizes[0], 1, sizes[1], sizes[2], -1)\n            # The point is to pad in front and back of the channels across which we'll apply normalization\n            denom = F.pad(denom, (0, 0, 0, 0, self.nbd_len // 2, (self.nbd_len - 1) // 2))\n            denom = F.avg_pool3d(denom, (self.nbd_len, 1, 1), stride=1)\n            denom = denom.squeeze(1).view(sizes)\n        denom = denom.mul(self.alpha).add(self.k).pow(self.beta)\n        return x.div(denom)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build Alexnet with PyTorch nn Modules and LRN Module"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Expects input tensor to be of dimensions (batch_size, 3, 224, 224)\nclass Alexnet(nn.Module):\n    def __init__(self):\n        super(Alexnet, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=2)\n        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding=2)\n        self.conv3 = nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1)\n        self.conv4 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1)\n        self.conv5 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1)\n        self.fc1 = nn.Linear(in_features=256 * 6 * 6, out_features=4096)\n        self.fc2 = nn.Linear(in_features=4096, out_features=4096)\n        self.fc3 = nn.Linear(in_features=4096, out_features=10)\n        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2)\n        # This layer helps us avoid calculating output map size when feeding into a linear layer in PyTorch.\n        self.adaptive_pool = nn.AdaptiveAvgPool2d(output_size=(6, 6))\n        self.norm = LocalResponseNormalization(neighbourhood_length=5, normalisation_const_alpha=1e-4, contrast_const_beta=0.75, noise_k=1.0)\n        self.dropout = nn.Dropout()\n    \n    def forward(self, x):\n        x = self.max_pool(self.norm(F.relu(self.conv1(x))))\n        x = self.max_pool(self.norm(F.relu(self.conv2(x))))\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.conv4(x))\n        x = self.adaptive_pool(self.norm(F.relu(self.conv5(x))))\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.dropout(x)\n        x = self.fc3(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's set up a custom dataset for our Scene Classification Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SceneDataset(Dataset):\n    def __init__(self, annotations_csv, root_dir, transform=None):\n        self.annotations = pd.read_csv(annotations_csv)\n        self.root_dir = root_dir\n        self.transform = transform\n                    \n    def __len__(self):\n        return len(self.annotations)\n    \n    def __getitem__(self, index):\n        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n        image = io.imread(img_path)\n        label = torch.tensor(int(self.annotations.iloc[index, 1]))\n        if self.transform:\n            image = self.transform(image)\n        return [image, label]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Utility to check train / test accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_accuracy(loader, model):\n    num_correct = 0\n    num_samples = 0\n    # Don't forget to toggle to eval mode!\n    model.eval()\n    \n    with torch.no_grad():\n        for data, targets in tqdm(loader):\n            data = data.to(device)\n            targets = targets.to(device)\n            scores = model(data)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == targets).sum()\n            num_samples += predictions.size(0)\n        print(\"Correct: {}, Total: {}, Accuracy: {}\".format(num_correct, num_samples, int(num_correct) / int(num_samples)))\n    # Don't forget to toggle back to model.train() since you're done with evaluation\n    model.train()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if __name__ == '__main__':\n    \n    LEARNING_RATE = 0.0001\n    # You could try playing around with the batch size(say 16) and learning rate(say 0.001) for faster convergence.\n    BATCH_SIZE = 8\n    EPOCHS = 10\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    \n    transform_img = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize((224, 224)),\n        transforms.ToTensor()\n    ])\n    \n    data = SceneDataset(annotations_csv=\"../input/scene-classification/train-scene classification/train.csv\",\n                          root_dir=\"../input/scene-classification/train-scene classification/train\",\n                          transform = transform_img)\n    train_data, test_data = torch.utils.data.random_split(data, [12034, 5000])\n    train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n    test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=True)\n    \n    alexnet = Alexnet()\n    alexnet.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(alexnet.parameters(), lr=LEARNING_RATE) \n    \n    data, targets = next(iter(train_loader))\n    for epoch in tqdm(range(EPOCHS)):\n        losses = []\n        with tqdm(total=len(train_loader)) as pbar:\n            for batch_idx, (data, targets) in enumerate(train_loader):\n                data = data.to(device=device)\n                targets = targets.to(device=device)\n\n                scores = alexnet(data)\n                loss = criterion(scores, targets)\n                losses.append(loss)\n\n                # backprop\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n#                 print(loss.item())\n                pbar.update(1)\n        print(\"Cost at epoch {} is {}\".format(epoch, sum(losses) / len(losses)))\n        check_accuracy(train_loader, alexnet)\n        check_accuracy(test_loader, alexnet)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's check our network's performance on a couple of images"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef show_image_and_prediction(img_path):\n    idx2label = {0: \"Buildings\", 1: \"Forests\", 2: \"Mountains\", 3: \"Glacier\", 4: \"Sea\", 5: \"Street\"}\n    img = io.imread(img_path)\n    transformed_img = transform_img(img)\n    out = alexnet(transformed_img.unsqueeze(0).to(device=device))\n    _, pred = out.max(1)\n    plt.imshow(img)\n    plt.show()\n    print(\"Alexnet thinks this scence contains: {}\".format(idx2label[pred.item()]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_image_and_prediction(\"../input/scene-classification/train-scene classification/train/10010.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_image_and_prediction(\"../input/scene-classification/train-scene classification/train/10.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_image_and_prediction(\"../input/scene-classification/train-scene classification/train/37.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_image_and_prediction(\"../input/scene-classification/train-scene classification/train/45.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_image_and_prediction(\"../input/scene-classification/train-scene classification/train/85.jpg\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}