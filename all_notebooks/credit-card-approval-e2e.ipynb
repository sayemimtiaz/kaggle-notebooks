{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade pip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install sweetviz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pandas_profiling","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\n\n# Data Viz\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n#%config InlineBackend.figure_format = 'svg'\nimport plotly.express as px\nimport plotly.offline as pyo\nimport plotly.graph_objs as go\n\n# Stats & ML\nfrom scipy import stats\nimport category_encoders as ce\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.metrics import balanced_accuracy_score\nfrom category_encoders import TargetEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder, LabelBinarizer\nfrom sklearn.preprocessing import RobustScaler\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\n# Auto ML\nimport sweetviz as sv\nfrom pandas_profiling import ProfileReport\n#import scorecardpy as sc\n\n# Supress Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# read/load dataset\napplication = pd.read_csv('../input/credit-card-approval-prediction/application_record.csv')\ncredit = pd.read_csv('../input/credit-card-approval-prediction/credit_record.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"application.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"credit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Helper Function - Missing data check\ndef missing_data(data):\n    missing = data.isnull().sum()\n    available = data.count()\n    total = (missing + available)\n    percent = (data.isnull().sum()/data.isnull().count()*100).round(4)\n    return pd.concat([missing, available, total, percent], axis=1, keys=['Missing', 'Available', 'Total', 'Percent']).sort_values(['Missing'], ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data(application)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data(credit)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation:-**\n\n* Duplicate entries are observed with same client number but different gender and other details. This seems unlikely and possible re-entry with corrected details\n* Hence deleting first entries and keeping the latest ones"},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop duplicate entries\napplication_clean = application.drop_duplicates(subset=['ID'], keep='last')\nprint(\"Original Data= \",application.shape)\nprint(\"Cleaned Data = \",application_clean.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"credit['STATUS'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def percent(column,decimals=2):\n    assert decimals >= 0\n    return (round(column*100,decimals).astype(str) + \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find the tenure for each entry\ncredit['Tenure'] = credit['MONTHS_BALANCE'].apply(lambda x : x*(-1))\ncredit.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"credit = credit.sort_values(by=['ID','Tenure'],ascending=True)\ncredit.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"credit_new = credit.groupby('ID').agg(max).reset_index()\ncredit_new = credit_new[['ID', 'MONTHS_BALANCE', 'Tenure', 'STATUS']]\ncredit_new.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge datasets application details with credit approval record\ndf = pd.merge(application_clean, credit_new, how='inner', on=['ID'])\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Dictionary:-**\nID: Unique Id of the row in application record.\nMONTHS_BALANCE: The number of months from record time.\nSTATUS: Credit status for this month.\nX: No loan for the month\nC: paid off that month\n0: 1-29 days past due\n1: 30-59 days past due\n2: 60-89 days overdue\n3: 90-119 days overdue\n4: 120-149 days overdue\n5: Overdue or bad debts, write-offs for more than 150 days"},{"metadata":{"trusted":true},"cell_type":"code","source":"# replacing X & C by 0 in credit status\ndf['STATUS'].replace('X', 0, inplace=True)\ndf['STATUS'].replace('C', 0, inplace=True)\ndf['STATUS'] = df['STATUS'].astype('int')\n\npercent(df['STATUS'].value_counts(normalize=True, sort=False),decimals=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reasons for Credit Card Rejection/Disapproval¶**\nThe reasons for a credit card rejection in India have been provided based on previous rejection experiences that applicant have faced and these reasons have found to be the most common across all major banks in the country.\n\n* Low credit score\n* Unstable employment\n* Insufficient income\n* Working in a delisted private company\n* Not falling into the age limit\n* Living in an address which is present in the defaulter list\n\n**Reference:-**\n[creditmantri](https://www.creditmantri.com/credit-card-rejection/)\n[Balance](https://www.thebalance.com/denied-credit-card-application-960247)\n[Late Payment](https://www.thebalance.com/when-does-a-late-payment-go-on-my-credit-report-960434)\n\nBased on above reference information, applicants who have past record for delayed credit payment greater than 59 days will have their applications rejected"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create target variable from credit status\ndf['STATUS'] = df['STATUS'].apply(lambda x: 1 if x >= 2 else 0)\npercent(df['STATUS'].value_counts(normalize=True, sort=False),decimals=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop months_balance column as it is redundant\ndf = df.drop(['MONTHS_BALANCE'], axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#renaming the column headers for easy understanding\n\ndf.rename(columns={'ID': 'User_id', 'CODE_GENDER':'Gender',\n                   'FLAG_OWN_CAR':'Car', 'FLAG_OWN_REALTY':'Realty_owned', 'CNT_CHILDREN':'Children_count',\n                   'AMT_INCOME_TOTAL':'Income_amount', 'NAME_INCOME_TYPE':'Income_type', 'NAME_EDUCATION_TYPE':'Education',\n                   'NAME_FAMILY_STATUS':'Family_status', 'NAME_HOUSING_TYPE':'Housing_type', 'DAYS_BIRTH':'Days_birth',\n                   'DAYS_EMPLOYED':'Days_employed', 'FLAG_MOBIL':'Mobile', 'FLAG_WORK_PHONE':'Work_phone',\n                   'FLAG_PHONE':'Phone', 'FLAG_EMAIL':'Email', 'OCCUPATION_TYPE':'Occupation_type',\n                   'CNT_FAM_MEMBERS':'Family_members','Tenure':'Tenure', 'STATUS':'Reject_Status'}, inplace=True)\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping feature \"mobile\" as all applicants own a mobile phone and hence the column \"Mobile\" seem to be of no importance. Hence it can be dropped\ndf = df.drop(['Mobile'], axis=1)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill missing values of feature occupation type with unknown\ndf[\"Occupation_type\"].fillna(\"Unknown\", inplace = True)\ndf[\"Occupation_type\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_category(df):\n    cols = df.select_dtypes(include='object').columns\n    for col in cols:\n        ratio = len(df[col].value_counts()) / len(df)\n        if ratio < 0.05:\n            df[col] = df[col].astype('category')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df = (df.pipe(to_category))\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyzing the dataset using pandas profiling library\napplication_profile = ProfileReport(df, \n                        title='Pandas Profiling Report for Application Data', \n                        html={'style':{'full_width':True}}) \n\napplication_profile.to_widgets()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyzing the dataset using sweetviz library\nadvert_report = sv.analyze(df)\n\n# Display the report\nadvert_report.show_html('CreditCard.html')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation¶**\n* All applicants in merged dataset own a mobile phone and hence the column \"Mobile\" seem to be of no importance. Hence it can be dropped\n* Majority of tyhe applicants are female\n* Occupation type column has 30% missing values and hence can either dropped or imputed (missing values as unknown)\n* Most of the categorical columns/features seem to be binary in nature\n* Major class imbalance observed. Only 0.26% of applicants belong to rejected category\n* Income and number of family members seem to have extreme values/data points (possibly outliers)\n* Outliers in income are found in approved class than rejected. Hence removing extreme values might not affect the model performance in detecting the reject cases"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Outlier treatment - IQR Method\ndef remove_outlier(col):\n    q25 = col.quantile(0.25)\n    q75 = col.quantile(0.75)\n    iqr = q75 - q25\n    cutoff = iqr*x\n    lower = q25 - cutoff\n    upper = q75 + cutoff\n    return lower, upper\n\n#Remove outliers for Income for 1.5 * IQR\nx=1.5\nlower_1, upper_1 = remove_outlier(df['Income_amount'])\ndf_IQR1 = df.loc[(df['Income_amount'] > lower_1) & (df['Income_amount'] < upper_1)]\nplt.figure(figsize=(15,8))\nax1 = sns.boxplot(x=\"Gender\", y=\"Income_amount\", hue = \"Reject_Status\",data=df_IQR1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remove outliers for children count for 1.5 * IQR\nx=1.5\nlower_2, upper_2 = remove_outlier(df_IQR1['Children_count'])\ndf_IQR1N = df_IQR1.loc[(df_IQR1['Children_count'] > lower_2) & (df_IQR1['Children_count'] < upper_2)]\nplt.figure(figsize=(15,8))\nax4 = sns.boxplot(x=\"Family_members\", y=\"Income_amount\", hue = \"Reject_Status\", showfliers = True,data=df_IQR1N)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = df.shape[0] - df_IQR1N.shape[0]\nprint(\"Outlier Count = \",a,\"\\nOutlier % = \",round(((a/df.shape[0])*100),2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_IQR1N.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_lab = df_IQR1N.copy()\ndf_woe = df_IQR1N.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we will convert all the non-numeric values into numeric ones because it results in faster computation and also many machine learning models\n# Label Encoding is used here\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\nfor col in df_lab:\n    if df_lab[col].dtypes == \"object\":\n        df_lab[col] = le.fit_transform(df_lab[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_lab.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split into input and output elements\nx_1  = df_lab.iloc[:, 1:-1]\ny_1 = df_lab.iloc[:, -1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_1.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test Train Split for category encoding\nx_1_train, x_1_test, y_1_train, y_1_test = train_test_split(x_1, y_1, test_size=0.3, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature scaling using robust scalar\n# Robust Scaler shrinks data to medians and quantiles, thus not influenced by outliers. Hence robust scalar will be used here\nx_1_train_rs = pd.DataFrame(RobustScaler().fit_transform(x_1_train), columns=x_1_train.columns)\nx_1_test_rs = pd.DataFrame(RobustScaler().fit_transform(x_1_test), columns=x_1_test.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Handle Class Imabalance using SMOTE Oversampling - Synthetic Minority Oversampling Technique\noversample = SMOTE()\n\na_target_0 = df_lab[df_lab['Reject_Status'] == 0].Reject_Status.count() / df_lab['Reject_Status'].count()\na_target_1 = df_lab[df_lab['Reject_Status'] == 1].Reject_Status.count() / df_lab['Reject_Status'].count()\n\nprint(round(a_target_0,4))\nprint(round(a_target_1,4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Apply to train data\nx_1_train_rs_over, y_1_train_over = oversample.fit_resample(x_1_train_rs, y_1_train)\nprint(Counter(y_1_train_over))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Apply to test data\nx_1_test_rs_over, y_1_test_over = oversample.fit_resample(x_1_test_rs, y_1_test)\nprint(Counter(y_1_test_over))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_1_train_rs_over.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Building\nclassifiers = {\n    \"LogisticRegression\" : LogisticRegression(),\n    \"KNeighbors\" : KNeighborsClassifier(),\n    \"SVC\" : SVC(),\n    \"DecisionTree\" : DecisionTreeClassifier(),\n    \"RandomForest\" : RandomForestClassifier(),\n    \"XGBoost\" : XGBClassifier()\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifiers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_scores = []\ntest_scores = []\n\nfor key, classifier in classifiers.items():\n    classifier.fit(x_1_train_rs_over, y_1_train_over)\n    train_score = round(classifier.score(x_1_train_rs_over, y_1_train_over),2)\n    train_scores.append(train_score)\n    test_score = round(classifier.score(x_1_test_rs_over, y_1_test_over),2)\n    test_scores.append(test_score)\n\nprint(train_scores)\nprint(test_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb.fit(x_1_train_rs_over, y_1_train_over)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The XBBoost classifier gives better results compared with other models as seen from above train/test scores\n* Hyperparameter tuning can be attempted by executing below cross validation code for XGBoost classifier\n* WOE and target encoding didnot yield any better results compared with one hot encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparams = {\n    'learning_rate': [0.05,0.1,0.15,0.2,0.25,0.3],\n    'max_depth':[5,10,15],\n    'min_child_weight':[6,8,10,12],\n    'subsample': [0.6,0.7,0.8,0.9], \n    'colsample_bytree':[0.6,0.7,0.8],\n    'gamma':[i/10.0 for i in range(0,5)]\n \n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''# Grid Search\ngsearch = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=354), \n param_grid = params, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n\n#Fit\ngsearch.fit(x_1_train_rs_over, y_1_train_over)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#gsearch.best_params_, gsearch.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}