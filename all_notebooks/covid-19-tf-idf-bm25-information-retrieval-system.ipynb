{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COVID-19 TF-IDF and BM25 Based Information Retrieval System","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**If you find this helpful leave a like!**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this notebook we build a simple information retrieval system using **TF-IDF** to generate an array of **inverted indices**, and then using **BM25** (Best Match the 25th iteration) to search and index the articles. [More information about BM25](https://en.wikipedia.org/wiki/Okapi_BM25)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"***","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Description","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We break this task down into a few steps:\n \n    1. Extract: here we extract the data from our dataset\n    \n    2. Organize: our organize step allows us to create a dictionary where we store our information of interest, then we generate a new dictionary where we store our inverted indices based off of keywords that we find for each document\n    \n    3. Retrieve: this step is where we run BM25 and generate our list of sorted results","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"***","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Libraries that we use ","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"#libraries for getting data and extracting\nimport os\nimport urllib.request\nimport tarfile\nimport json\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\n\n#libraries for text preprocessing\nimport re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nnltk.download('wordnet') \nfrom nltk.stem.wordnet import WordNetLemmatizer\n\n#libraries for keyword extraction with tf-idf\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom scipy.sparse import coo_matrix\n\n#libraries for reading and writing files\nimport pickle\n\n#libraries for BM25\n!pip install rank_bm25\nfrom rank_bm25 import BM25Okapi","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# EXTRACT","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### getting data\n> input: None\n \n> output: None\n \nDownloads the jsonfiles from the collection and puts them all in a folder, called data. Within data, creates subfolders based on where each article is from.\nWe will calling on these jsonfiles stored in data in the extract step to create our dataset.\n","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def getData():\n    urls = ['https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-03-27/comm_use_subset.tar.gz', 'https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-03-27/noncomm_use_subset.tar.gz', 'https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-03-27/custom_license.tar.gz', 'https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-03-27/biorxiv_medrxiv.tar.gz']\n\n    # Create data directory\n    try:\n        os.mkdir('./data')\n        print('Directory created')\n    except FileExistsError:\n        print('Directory already exists')\n\n    #Download all files\n    for i in range(len(urls)):\n        urllib.request.urlretrieve(urls[i], './data/file'+str(i)+'.tar.gz')\n        print('Downloaded file '+str(i+1)+'/'+str(len(urls)))\n        tar = tarfile.open('./data/file'+str(i)+'.tar.gz')\n        tar.extractall('./data')\n        tar.close()\n        print('Extracted file '+str(i+1)+'/'+str(len(urls)))\n        os.remove('./data/file'+str(i)+'.tar.gz')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### preprocessing\n>input: text \n \n>output: preprocessed text - take out stop words, punctuation, change all to lowercase, remove digits/special characters, stem, lemmatize\n\nNecessary for extract step because we will be preprocessing all extracted text (title and abstract)","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def preprocess(text):\n    #define stopwords\n    stop_words = set(stopwords.words(\"english\"))\n    #Remove punctuations\n    text = re.sub('[^a-zA-Z]', ' ', text)\n    #Convert to lowercase\n    text = text.lower()\n    #remove tags\n    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n    # remove special characters and digits\n    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n    ##Convert to list from string\n    text = text.split()\n    ##Stemming\n    ps=PorterStemmer()\n    text = [ps.stem(word) for word in text if not word in stop_words]\n    #Lemmatisation\n    lem = WordNetLemmatizer()\n    text = [lem.lemmatize(word) for word in text if not word in  stop_words] \n    text = \" \".join(text) \n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### extraction step\n>input: None\n \n>output: a dataframe with all the extracted information \n \nFor every article, we are collecting its: \n1. paper_id \n2. title and \n3. abstract \n\nWe store these values in pandas dataframe, which we write to a pickle file.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def extract():\n    #create our collection locally in the data folder\n    \n    #creating our initial datastructure\n    x = {'paper_id':[], 'title':[], 'abstract': []}\n    \n    #Iterate through all files in the data directory\n    for subdir, dirs, files in os.walk('./data'):\n        for file in tqdm(files):\n            with open(os.path.join(subdir, file)) as f:\n                data = json.load(f)\n                \n               #Append paper ID to list\n                x['paper_id'].append(data['paper_id'])\n               #Append article title to list & preprocess the text\n                x['title'].append((data['metadata']['title']))\n                \n                #Append abstract text content values only to abstract list & preprocess the text\n                abstract = \"\"\n                for paragraph in data['abstract']:\n                    abstract += paragraph['text']\n                    abstract += '\\n'\n                #if json file no abstract in file, set the body text as the abstract (happens rarely, but often enough that this edge case matters)\n                if abstract == \"\": \n                    for paragraph in data['body_text']:\n                        abstract += paragraph['text']\n                        abstract += '\\n'\n                x['abstract'].append(preprocess(abstract))\n                \n    #Create Pandas dataframe & write to pickle file\n    df = pd.DataFrame.from_dict(x, orient='index')\n    df = df.transpose()\n    pickle.dump( df, open( \"full_data_processed_FINAL.p\", \"wb\" ) )\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# ORGANIZE","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### sort coordinate matrix\n>input: tf_idf coordinate representation\n \n>output: tf_idf items sorted in descending order -- so things with highest scores at the top!\n\nFunction for sorting tf_idf in descending order\n","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def sort_coo(coo_matrix):\n    tuples = zip(coo_matrix.col, coo_matrix.data)\n    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### get top n words with highest tf-idf scores\n>input: \n \n        1. feature_names = vocabulary\n        2. sorted_items = tf-idf vectors sorted in descending order\n        3. topN = # of keywords you would like extract from text\n       \n>output: dictionary of topN # words with highest tf-idf scores in the text (key) and their corresponding tf-idf scores (value)\n\nGets keyword names and their tf-idf scores of topN items","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def extract_topn_from_vector(feature_names, sorted_items, topN):\n    #use only topn items from vector\n    sorted_items = sorted_items[:topN]\n \n    score_vals = []\n    feature_vals = []\n    # word index and corresponding tf-idf score\n    for idx, score in sorted_items:\n        #keep track of feature name and its corresponding score\n        score_vals.append(round(score, 3))\n        feature_vals.append(feature_names[idx])\n \n    #create a tuples of feature,score\n    results= {}\n    for idx in range(len(feature_vals)):\n        results[feature_vals[idx]]=score_vals[idx]\n    return results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### getting abstract keywords\n> input: \n \n        1. entry = row in the article dataframe, which represents one article\n        2. cv = CountVectorizer, from sklearn.feature_extraction.text\n        3. X = vector that represents the CountVectorizer fit to the corpus\n        4. tfidf_transformer = object that holds our tf_idf data -- again, fit to our corpus\n        5. feature_names = vocabulary\n        6. topN = # of keywords we'd like to extract from the abstract\n        \n> output: the topN keywords from the abstract\n\nExtracts the topN keywords from the abstract\n","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def getAbstractKeywords(entry, cv, X, tfidf_transformer, feature_names, topN):\n    abstract = entry['abstract']\n    \n    #first check that abstract is full\n    if type(abstract) == float:\n        return []\n \n    #generate tf-idf for the given document\n    tf_idf_vector=tfidf_transformer.transform(cv.transform([abstract])) \n    #sort the tf-idf vectors by descending order of scores\n    sorted_items=sort_coo(tf_idf_vector.tocoo())\n    #extract only the topN # items\n    keywords_dict=extract_topn_from_vector(feature_names,sorted_items,topN)\n    #just want words themselves, so only need keys of the dictionary\n    keywords = list(keywords_dict.keys()) \n     \n    return keywords","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### getting title keywords\n>input: entry = row in the article dataframe, which represents one article\n\n>output: list of all the words in the title\n\nAssumed that if a word is in the title of the article, it must be important to the article and treated like a keyword. \nThus, this method just extracts all the words from the (already processed) title.\n","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def getTitleKeywords(entry):\n    title = entry['title']  \n    title = preprocess(title)\n    #first check that the title of that entry is full\n    if type(title) == float:\n        return []\n    \n    keywords_title = title.split(' ')\n    return keywords_title","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### getting final keywords\n>input: \n \n        1. entry = row in the article dataframe, which represents one article\n        2. cv = CountVectorizer, from sklearn.feature_extraction.text\n        3. X = vector that represents the CountVectorizer fit to the corpus\n        4. tfidf_transformer = object that holds our tf_idf data -- again, fit to our corpus\n        5. feature_names = vocabulary\n        6. topN = # of keywords we'd like to extract from the abstract\n> output: list of *all* keywords for an article -- extracted from both title and abstract!\n\nCalls getTitleKeywords() and getAbstractKeywords() and concatenates the two lists, resulting in a final list of keywords","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def getFinalKeywords(entry, cv, X, tfidf_trans, feature_names, topN):\n    #get keywords from abstract and title\n    fromAbstract = getAbstractKeywords(entry, cv, X, tfidf_trans, feature_names, topN)\n    fromTitle = getTitleKeywords(entry)\n    #concatenate two lists\n    finalKeywords = fromAbstract + fromTitle\n    #convert to set and then back to list to ensure there are no duplicates in list\n    final_no_duplicates = list(set(finalKeywords))\n    return final_no_duplicates","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### getting corpus\n>input: dataframe that contains the 1) paper_id 2) abstract and 3) article for every article. All text is processed.\n\n>output: a list that contains every abstracts in the in the article dataframe\n\nCreating a corpus, which is a necessary input to our tf_idf step.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def getCorpus(articlesDf):\n    #creating a new dataframe, abstractDf, of just the abstracts, so that we don't modify the original dataframe, articlesDf\n    abstractDf = pd.DataFrame(columns = ['abstract'])\n    #filling abstractDf with the abstract column from articlesDf\n    abstractDf['abstract'] = articlesDf['abstract']\n    #converting column of dataframe to a list\n    corpus = abstractDf['abstract'].to_list()\n    return corpus","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### adding keywords\n\n>input: \n\n        1. df = dataframe that contains the 1) paper_id 2) title and 3) abstract for every article. All text is processed \n        2. topN = # of keywords we'd like to extract from the abstract\n        3. makeFile = boolean, whether you'd like this method to make a pickle file of the output dataframe\n        4. fileName = what the user would like to name the pickle file\n        \n> output: pandas dataframe that contains the \n\n        1. paper_id \n        2. title \n        3. abstract and \n        4. keywords associated with every article","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def addKeywords(df, topN, makeFile, fileName):\n    #defining stopwords\n    stop_words = set(stopwords.words(\"english\"))\n\n    #creating following variables that are needed for keyword extract from abstract, using tf-idf methodology,\n    #all input in getFinalKewords method\n    corpus = getCorpus(df)\n    cv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=1000, ngram_range=(1,1))    \n    X=cv.fit_transform(corpus)\n    tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n    tfidf_transformer.fit(X)\n    feature_names=cv.get_feature_names()\n    \n    #adding keywords article to dataframe\n    df = df.reindex(columns = ['paper_id', 'title', 'abstract','keywords'])                \n    #getting keywords for each entry in article dataframe -- using apply to be more efficient\n    df['keywords'] = df.apply(lambda row: getFinalKeywords(row, cv, X, tfidf_transformer, feature_names, topN), axis=1)\n\n    #make pickle file depending on user input\n    if makeFile == True:\n        pickle.dump( df, open( fileName, \"wb\" ) )\n    return df  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### creating inverted indices\n\n>input: pandas dataframe that contains the 1) paper_id 2) title 3)abstract and 4) keywords associated with every article\n\n>output: dictionary of inverted indices -- key = word that is a keyword; value = all articles' paper_id's that have word as a keyword\n\nCreating an inverted indices dictionary. Will use this when deciding which subset of articles to run our ranking/retrieving algorithm on.\nIt is important that our output is a dictionary because it has constant look up time.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def createInvertedIndices(df):\n    numEntries = df.shape[0]\n    invertInd = {}\n    \n    for i in range (numEntries):\n        entry = df.iloc[i]\n        paper_id = entry['paper_id']    \n        keywords = entry['keywords']\n        for k in keywords:\n            if k not in invertInd:\n                invertInd[k] = []\n                invertInd[k].append(paper_id)\n            else:\n                invertInd[k].append(paper_id)\n    return invertInd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### organize step","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def organize():\n    df_without_keywords = pickle.load(open(\"full_data_processed_FINAL.p\", \"rb\"))\n    df_with_keywords = addKeywords(df_without_keywords, 10, False, \"full_data_withKeywords_FINAL.p\")\n    invertedIndices = createInvertedIndices(df_with_keywords)\n    pickle.dump( invertedIndices, open( \"invertedIndices_FINAL.p\", \"wb\" ) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Lets take a look at the processed data!","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"getData()\nextract()\n\ndf_without_keywords = pickle.load(open(\"full_data_processed_FINAL.p\", \"rb\"))\ndf_with_keywords = addKeywords(df_without_keywords, 10, False, \"full_data_withKeywords_FINAL.p\")\ndisplay(df_without_keywords)\ndisplay(df_with_keywords)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# RETRIEVE","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### getting our subset of articles\n>input: query\n\n>output: a list of the potential articles that may be of interest, as they have some of the query terms as their keyword(s)\n\nDoing this step so we don't have to run our ranking algorithm, BM25, over all ~30,000 articles. \nWant to identify this subset in order to make our ranking (and therefore retrieving) more efficient!","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def getPotentialArticleSubset(query):\n    #load in inverted indices\n    invertedIndices = pickle.load(open(\"invertedIndices_FINAL.p\", \"rb\"))\n    \n    #preprocess query and split into individual terms\n    query = preprocess(query)\n    queryTerms = query.split(' ')\n    \n    potentialArticles = []\n    #concatenate list of potential articles by looping through potential articles for each word in query\n    for word in queryTerms:\n        if word in invertedIndices: #so if someone types in nonsensical query term that's not in invertedIndices, still won't break!\n            someArticles = invertedIndices[word]\n            potentialArticles = potentialArticles + someArticles\n            \n    #convert to set then back to list so there are no repeat articles\n    potentialArticles = list(set(potentialArticles))\n    return potentialArticles","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### bm25 method\n\n>input: list of articles, dictionary with all of the documents, weight of the title, weight of the abstract, and the query\n\n>output: list of ranked articles\n\nThis is the main information retrieval method implementing Okapi BM25","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def bm25(articles, df_dic, title_w, abstract_w, query):\n    corpus_title = []\n    corpus_abstract = []\n    \n    for article in articles:\n        arr = df_dic.get(article)\n        #title\n        if type(arr[0]) != float:\n            preprocessedTitle = preprocess(arr[0])\n            corpus_title.append(preprocessedTitle)\n        else:\n            corpus_title.append(\" \")\n        \n        #abstract\n        if type(arr[1]) != float:\n            preprocessedAbst = preprocess(arr[1])\n            corpus_abstract.append(preprocessedAbst)\n        else:\n            corpus_abstract.append(\" \")\n            \n    query = preprocess(query)\n    \n    tokenized_query = query.split(\" \")\n    \n    tokenized_corpus_title = [doc.split(\" \") for doc in corpus_title]\n    tokenized_corpus_abstract = [doc.split(\" \") for doc in corpus_abstract]\n    \n    #running bm25 on titles\n    bm25_title = BM25Okapi(tokenized_corpus_title)\n    doc_scores_titles = bm25_title.get_scores(tokenized_query)\n    #weighting array\n    doc_scores_titles = np.array(doc_scores_titles)\n    doc_scores_titles = doc_scores_titles**title_w\n    \n    #running bm25 on abstracts\n    bm25_abstract = BM25Okapi(tokenized_corpus_abstract)\n    doc_scores_abstracts = bm25_abstract.get_scores(tokenized_query)\n    #weighting\n    doc_scores_abstracts = np.array(doc_scores_abstracts)\n    doc_scores_abstracts = doc_scores_abstracts ** abstract_w\n    \n    #summing up the two different scores\n    doc_scores = np.add(doc_scores_abstracts,doc_scores_titles)\n    \n    #creating a dictionary with the scores\n    score_dict = dict(zip(articles, doc_scores))\n    \n    #creating list of ranked documents high to low\n    doc_ranking = sorted(score_dict, key=score_dict.get, reverse = True)\n    \n    #get top 100\n    doc_ranking = doc_ranking[0:100]\n    \n    for i in range(len(doc_ranking)):\n        dic_entry = df_dic.get(doc_ranking[i])\n        doc_ranking[i] = dic_entry[0]\n    \n    return doc_ranking","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### retrieval step","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def retrieve(queries):\n    #performing information retrieval\n    df_without_keywords = pickle.load(open(\"full_data_processed_FINAL.p\", \"rb\"))\n    df_dic = df_without_keywords.set_index('paper_id').T.to_dict('list')\n    results = []\n    for q in queries:\n        articles = getPotentialArticleSubset(q)\n        result = bm25(articles,df_dic,1,2,q)\n        results.append(result)\n\n    #Output results\n    for query in range(len(results)):\n        for rank in range(len(results[query])):\n            print(str(query+1)+'\\t'+str(rank+1)+'\\t'+str(results[query][rank]))\n            ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Putting it all together","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"this will give us the top 100 articles for each query sorted by BM25 rank","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"getData()\nextract()\norganize()\nq = ['coronavirus origin',\n'coronavirus response to weather changes',\n'coronavirus immunity']\nretrieve(q)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# References","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"[bm25 documentation](https://pypi.org/project/rank-bm25/)\n\n[sklearn TFidfTransformer documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}