{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a35ba617-6e93-42c1-f1c1-2ef28aa6dddd"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nplt.rc('figure', figsize=(15, 12))\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1662ad16-3876-1a4a-5cce-8b5de32df413"},"outputs":[],"source":"# read the data\ndmds = pd.read_csv('../input/diamonds.csv')\ndmds.drop('Unnamed: 0', axis=1, inplace=True)\ndmds.head(3)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"73423bea-7c81-6797-482c-315b2ebe21ad"},"outputs":[],"source":"# encode cut, color and clarity\ncategorical_cols = ['cut', 'color', 'clarity']\nfor c in categorical_cols:\n    dmds[c] = pd.factorize(dmds[c])[0]\ndmds.head(3)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"128c4e73-1b34-1892-2cc2-168ed5e5c14e"},"outputs":[],"source":"# graphs\n# use a reduced set without categorical columns\ndmds_reduced = dmds[dmds.columns.difference(['cut', 'color', 'clarity'])]\n\n# correlations\nplt.matshow(dmds_reduced.corr())\nplt.colorbar()\ntick_marks = dmds_reduced.columns.values\nplt.xticks(np.arange(tick_marks.size), tick_marks)\nplt.yticks(np.arange(tick_marks.size), tick_marks)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"91f75b7e-244c-bed5-3efc-fbd4077782d0"},"outputs":[],"source":"# let's compare the correlation visually\nfrom pandas.tools.plotting import scatter_matrix\n\nscatter_matrix(dmds_reduced, diagonal='kde')\nplt.grid()\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"53ec0640-1454-df4c-1fa3-93f402716ab7"},"outputs":[],"source":"# split\nfrom sklearn.model_selection import train_test_split\n\nX, y = dmds.iloc[:, dmds.columns != 'price'].values, dmds.iloc[:, dmds.columns == 'price'].values.ravel()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"},{"cell_type":"markdown","metadata":{"_cell_guid":"789e949a-cc17-688a-5cae-f348b1159f0f"},"source":"## Regression ##"},{"cell_type":"markdown","metadata":{"_cell_guid":"de7c6e28-2cea-1e5d-e339-686792ba83a8"},"source":"Let's compare the basic linear regression and random forest regression. SVR's were lagging on my PC, hence these are commented out"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"433ed608-4ac1-0f3c-c1b7-e92993f65a27"},"outputs":[],"source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\n\nmodels = [('LR', LinearRegression(n_jobs=-1)),\n          ('RF', RandomForestRegressor(n_estimators=100, criterion='mse', random_state=1, n_jobs=-1)),\n#           ('SVR-lin', SVR(kernel='linear', C=1e3))\n#           ('SVR-rbf', SVR(kernel='rbf', C=1e3)),\n#           ('SVR-poly', SVR(kernel='poly', C=1e3, degree=2))\n         ]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0c93f893-8d34-2c3c-422d-e3bcd2d71ec5"},"outputs":[],"source":"from sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import cross_val_score, KFold\n\n# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=5, random_state=123)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, n_jobs=-1)    \n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7ba9ac4a-75c0-9a79-f8ef-e239c5e2562a"},"outputs":[],"source":"# compute ms-error and R^2\nfor name, model in models:\n    model.fit(X_train, y_train)\n\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n\n    print('%s: MSE train: %.4f, test: %.4f' % (name, mean_squared_error(y_train, y_train_pred),\n                                           mean_squared_error(y_test, y_test_pred)))\n    print('%s: R^2 train: %.4f, test: %.4f' % (name, r2_score(y_train, y_train_pred),\n                                           r2_score(y_test, y_test_pred)))"},{"cell_type":"markdown","metadata":{"_cell_guid":"e69dcd42-f1a6-5902-f921-4ff01ce4a8fb"},"source":"Random forests estimator looks promising, let's compare algorithms visually"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1906809a-0ac8-0772-8af5-761cc1388add"},"outputs":[],"source":"# Compare Algorithms\nfig = plt.figure(figsize=(16, 8))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results, vert=False)\nax.set_yticklabels(names)\nplt.grid()\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"299d4b3a-ae4e-1dbc-0fe6-c22d970c7983"},"source":"## Classification by price ranges ##\nLet's split the price by price bands and try to train different classifiers on it"},{"cell_type":"markdown","metadata":{"_cell_guid":"1aa6e0e9-5160-865b-07b0-7a352ae8d433"},"source":"### 10 classes ###"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fd45c15a-ef99-45a3-49f7-15992c3cf70a"},"outputs":[],"source":"# convert price data to classes\nn_classes = 10\n\ny_classes = np.linspace(0, y.max(), n_classes)\ny_train_cl = np.digitize(y_train, bins=y_classes)\ny_test_cl = np.digitize(y_test, bins=y_classes)\n\nprint('Price classes: %s' % (y_classes))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"65aa1394-24c8-6e4d-4f5b-89197438dc5b"},"outputs":[],"source":"# test different models on the data\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nmodels = [('LR', LogisticRegression()),\n          ('KNN', KNeighborsClassifier()),\n          ('CART', DecisionTreeClassifier()),\n          ('NB', GaussianNB()),\n          ('SVM-lin', SVC(kernel='linear')),\n          ('SVM-rbf', SVC(kernel='rbf')),\n          ('RF', RandomForestClassifier()),\n          ('MLP', MLPClassifier(alpha=1)),\n          ('ADA', AdaBoostClassifier())]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eb439ced-703c-922b-6ee8-68cf2411cfd0"},"outputs":[],"source":"# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=5, random_state=42)\n    cv_results = cross_val_score(model, X_train, y_train_cl, cv=kfold, n_jobs=-1)    \n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c8d753a0-2be2-c6b4-200f-a9adffeda367"},"outputs":[],"source":"# Compare Algorithms\nfig = plt.figure(figsize=(16, 8))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results, vert=False)\nax.set_yticklabels(names)\nplt.grid()\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"38391c46-d89f-b813-7ed1-36ef60eb8471"},"source":"### 5 classes ###"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"de7c3bc8-8fa1-7096-d9ef-1b3d37ef829a"},"outputs":[],"source":"# convert price data to classes\nn_classes = 5\n\ny_classes = np.linspace(0, y.max(), n_classes)\ny_train_cl = np.digitize(y_train, bins=y_classes)\ny_test_cl = np.digitize(y_test, bins=y_classes)\n\nprint('Price classes: %s' % (y_classes))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ed13e5a9-42be-3602-ceb9-c70f42cd06f3"},"outputs":[],"source":"# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=5, random_state=42)\n    cv_results = cross_val_score(model, X_train, y_train_cl, cv=kfold, n_jobs=-1)    \n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"08eeb16f-7e78-3dc8-1462-0ba391026f4a"},"outputs":[],"source":"# Compare Algorithms\nfig = plt.figure(figsize=(16, 8))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results, vert=False)\nax.set_yticklabels(names)\nplt.grid()\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"41596501-ef1f-8cb3-dda3-3f48fe023caa"},"source":"The random forest classifier has won in both scenarios with 5 & 10 classes. In general, classification accuracy is better with 5 classes"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}