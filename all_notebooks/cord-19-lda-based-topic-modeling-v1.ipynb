{"cells":[{"metadata":{},"cell_type":"markdown","source":"## ** LDA-based Topic Modeling**\n\n***\n\nThis notebook applied Natural Language Processing (NLP) and other AI techniques to generate insights in the support of the ongoing fight against this infectious disease. There is a growing urgency for these approaches because of the rapid acceleration in new coronavirus literature, making it difficult for the medical research community to keep up. \n<br>\n<br>Language is unstructured data that has been produced by people to be understood by other people. Text data is not random, it is governed by linguistic properties that make it very understandable to other people and also processable by computers !!\n\n***\n\n**Methodology:** This notebook retrieve insights from a corpus composed of 2020 COVID-19 full-text research papers. First, the authors proposed text-mining approaches to explore the corpus, including 1) wordcloud, 2) Word2Vec model to retrieve the most similar words to a specific word (e.g., retrieve the most similar words to *\"origin\"*, *\"symptom\"*), and 3) t-SNE visualization of semantic clusters from the corpus. Then, the proposed framework implemented an unsupervised Latent Dirichlet Allocation-based modeling of the strategic topics present in the corpus.\n***\n**Highlights:** \n1.  Text Data Loading and Preparation\n2.  Wordcloud of COVID-19 Abstracts\n3.  Word2Vec Model and Textual Similarities\n4.  TSNE-Visualization of Semantic Clusters\n5.  **Latent Dirichlet Allocation-based Topic Modeling**\n***\n\n**Pros:**\n* Focus on new coronavirus literature\n* Application of diverse text mining techniques\n* **Automated LDA-based Topic Modeling**\n* Insightful Data Visualization Tools\n\n**Cons:**\n* Reduced Scope: Analysis of 1,994 full-text research papers published in 2020.\n\n***"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"from tqdm import tqdm\nimport json\nimport re\nimport fnmatch\nimport numpy as np\nimport pandas as pd\nfrom pprint import pprint\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nnltk.download('stopwords')\nimport os\nfrom time import time  # to time our operations\nimport operator\nimport sys\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.manifold import TSNE\n\n# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Part 1: Data Extraction and Preparation**\n\n### ** <font color=green> Funtion to Extract Text Data from Json Files **"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fonction to extract text data from json files\n\ndef extract_values(obj, key):\n    \"\"\"Pull all values of specified key from nested JSON.\"\"\"\n    arr = []\n\n    def extract(obj, arr, key):\n        \"\"\"Recursively search for values of key in JSON tree.\"\"\"\n        if isinstance(obj, dict):\n            for k, v in obj.items():\n                if isinstance(v, (dict, list)):\n                    extract(v, arr, key)\n                elif k == key:\n                    arr.append(v)\n        elif isinstance(obj, list):\n            for item in obj:\n                extract(item, arr, key)\n        return arr\n\n    results = extract(obj, arr, key)\n    return results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ** <font color=green> Data Exploration **"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/CORD-19-research-challenge/metadata.csv\")\ndf=df[[\"title\", \"authors\", \"publish_time\", \"abstract\",\"sha\"]]\ntry:\n    df[\"publish_time\"] = pd.DatetimeIndex(df[\"publish_time\"]).year\nexcept:\n    pass\nplt.subplots(figsize = (10,6))\nplt.hist(df[\"publish_time\"],bins = 30, edgecolor =\"black\")\nplt.title(\"Coronavirus-Related Academic Publications \\n\", fontsize = 24, fontweight = \"bold\")\nplt.xlabel(\"Published Year\")\nplt.ylabel(\"Publications\")\nplt.savefig(\"COVID-19_Publications_Histogram.png\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ** <font color=green> Retrieval of Relevant Research Papers **"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df[df[\"publish_time\"]==2020]\ndf=df.dropna(subset = [\"sha\"])\ndf=df.dropna(subset = [\"abstract\"])\ndf =df[df[\"abstract\"].str.contains(\"COVID|covid|Covid|coronavirus|Coronavirus|2019-nCov|SARS-CoV-2\", regex=True)]\nprint(\"Number of Retrieved Full-Text Papers\", len(df))\ndisplay(df.head(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ** <font color=green> Text Data Preprocessing (Tokenization, Stopword Removal, Bigrams/Trigrams) **"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Text data extraction and preprocessing(tokenization, stopword removal, and bigrams/trigrams)\n\nt =time()\n\nstopwords = nltk.corpus.stopwords.words('english')\nstopwords.extend([\"could\", \"medrxiv\", \"http\", \"license\", \"preprint\"])\ncorpus1 = []\ntokenizer = RegexpTokenizer(r'\\w+')\n\n\n    \nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if fnmatch.fnmatch(filename, '*.json'):\n                \n            path = os.path.join(dirname, filename)\n            data = json.load(open(path))\n            paper_id = data[\"paper_id\"]\n            \n            if paper_id in df[\"sha\"].unique().tolist():\n                data = extract_values(data, 'text')\n                \n                for sentence in data:\n                    shortword = re.compile(r'\\W*\\b\\w{1,4}\\b')\n                    sentence = shortword.sub('', sentence).lower()\n                    word_list = tokenizer.tokenize(sentence.lower())\n                    word_list1 = [word for word in word_list if word.isalpha()]\n                    word_list2 = [word for word in word_list1 if word not in stopwords]\n                    corpus1.append(word_list2)\n\n\n# Build the bigram and trigram models\nbigram = gensim.models.Phrases(corpus1, min_count=10, threshold=100)  # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[corpus1], threshold=100)\n\n# Faster way to get a sentence clubbed as a trigram/bigram\nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)\n\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\n\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n\ncorpus2 = make_bigrams(corpus1)\ncorpus2 = make_trigrams(corpus2)\ncorpus2 =  list(filter(lambda x: x != [], corpus2))\ncovid_corpus =  list(filter(lambda x: len(x) > 4, corpus2))\nprint('Time to process data: {} mins'.format(round((time() - t) / 60, 2)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(covid_corpus[:3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Part 2: Wordcloud of 2020 Covid-19 Research Papers**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def return_sum(my_dict):\n    sum = 0\n    for i in my_dict:\n        sum = sum + my_dict[i]\n    return sum\n\ndef dict_for_wordcloud(corpus):\n    words_dict = {}\n\n    filtered_words =[]\n\n    for i in range(len(corpus)):\n        for j in range(len(corpus[i])):\n            filtered_words.append(corpus[i][j])\n\n    filtered_words1 = [w for w in filtered_words if w.isalpha()]\n\n    lemmatizer = WordNetLemmatizer()\n\n    for w in range(len(filtered_words1)):\n        filtered_words1[w] = lemmatizer.lemmatize(filtered_words1[w])\n\n    for word in filtered_words1:\n        words_dict[word] = words_dict.get(word, 0) + 1\n\n    print('Total Number of Words:', return_sum(words_dict))\n\n    sorted_d = sorted(words_dict.items(), key=operator.itemgetter(1), reverse=True)\n    \n    print('Distinct words', len(sorted_d))\n    \n    return words_dict\n\n\ndef plot_wordcloud(corpus):\n\n    wordcloud = WordCloud(width=800, height=400, max_words=150, max_font_size=50, relative_scaling=0.5,\n                          background_color=\"white\").generate_from_frequencies(dict_for_wordcloud(corpus)) #color_func=lambda *args, **kwargs: (0, 50, 1)).generate_from_frequencies(words_dict)\n    \n    # Display the generated image:\n    plt.figure(figsize=(10,8))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.title('WordCloud of Covid-19 Research Papers \\n', fontsize = 24, fontweight = \"bold\")\n    plt.savefig('WordCloud of Covid-19 Research Papers.png')\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = time()\n\nplot_wordcloud(covid_corpus)    \n\nprint('Time to process data: {} mins'.format(round((time() - t) / 60, 2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In a wordcloud, the importance of each word is shown with font size. In this section, a wordcloud of the most frequent words appearing in corpus of COVID-19 abstracts is built. A number of preprocessing steps (e.g., tokenization, lemmatization) are required to build a word cloud. As expected, words such as *\"covid\"*, *\"\"patient*, and *\"infection\"* are particularly prominent. Other words such as *\"wuhan\"* and *\"proteine\"* have also been extensively discussed in literature.  "},{"metadata":{},"cell_type":"markdown","source":"# **Part 3: Word2Vec Model and Textual Cosine Similarities**\n\nA **Word2Vec** model (Word to Vector) was built using Gensim Python library to produce word embeddings. Using a large corpus of text as an input, a Word2vec model returns a vector (here, 100 dimensions) for each unique word in the corpus. The similarity between vectors is measured through the cosine similarity metric. Similar vectors represent words that are semantically related in the original corpus."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from gensim.models import word2vec, KeyedVectors\nfilename = 'testword2vec_Covid_10min_count'\n\n#### Word2Vec Model ####\n\nmodel = word2vec.Word2Vec(covid_corpus, size=100, window=8, min_count=10, workers=10)\nmodel.train(covid_corpus, total_examples=len(covid_corpus), epochs=15)\nmodel.wv.save(filename)\nword_vectors = KeyedVectors.load(filename)\n\n#### Example of Word Embedding ####\n\nmodel.wv['coronavirus']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word = \"origin\"\nprint(\"Similar words to {}:\".format(word))\nprint(word_vectors.most_similar(positive=word, topn=10))\nprint(\"\\n\")\nword = \"symptom\"\nprint(\"Similar words to {}:\".format(word))\nprint(word_vectors.most_similar(positive=word, topn=10))\nprint(\"\\n\")\nword = \"diagnostic\"\nprint(\"Similar words to {}:\".format(word))\nprint(word_vectors.most_similar(positive=word, topn=10))\nprint(\"\\n\")\nword = \"transmission\"\nprint(\"Similar words to {}:\".format(word))\nprint(word_vectors.most_similar(positive=word, topn=10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For instance, it is interesting to understand at a glance that the origin of coronaviruses is linked to bigrams such as *\"natural_reservoir\"*, seafood_markets\", *\"animal_reservoir\"*, and *\"zoonotic_origin\"* (FYI, A zoonosis is an infectious disease caused by a pathogen that has jumped from non-human animals to humans) <br><br>\nSimilarly, regarding what is known about COVID-19 *symptoms*, it is worth having a look at \"symptom\"'s most similar words, including words such as *\"fever\"* and *\"cough\"*, and *\"fatigue\"*."},{"metadata":{},"cell_type":"markdown","source":"# **Part 4: T-SNE Visualization of Semantic Clusters**"},{"metadata":{},"cell_type":"markdown","source":"The **T-distributed Stochastic Neighbor Embedding (t-SNE)** dimensionality reduction technique was ultimately applied to project the 2D position of each word with its label. A machine learning **Kmean** algorithm was also implemented using *Scikit-learn* Python Library to partition n words into semantic clusters. To determine the optimal number of clusters K, the **elbow method** was used with below the plot of sum of squared distances for K in the range [1, 30]. If the plot looks like an arm, then the elbow on the arm is the optimal K. Here, **K =7**."},{"metadata":{"trusted":true},"cell_type":"code","source":"def Word2Vec_Sorted(model):\n    ''' \n    Function to extract the word2vec embeddings \n    of the most frequent terms in the corpus\n    '''\n    stopwords.extend([\"also\", \"however\", \"could\", \"rights_reserved\", \"reviewed_https_biorxiv\", \"author_funder\", \"copyright_holder\", \\\n                      \"without_permission\", \"reviewed\", \" author_funder_granted\"])\n    w2c = dict()  \n    \n    for item in model.wv.vocab:\n        w2c[item]=model.wv.vocab[item].count\n    w2cSorted=dict(sorted(w2c.items(), key=lambda x: x[1],reverse=True))\n    w2cSortedList = list(w2cSorted.keys())\n    w2cSortedList = [word for word in w2cSortedList if word not in stopwords]\n    \n    return w2cSortedList\n\n\n#### Implementation of the elbow method to find the optimal number of clusters K ####\n\nSum_of_squared_distances = []\ntokens = []\n\nfor word in Word2Vec_Sorted(model):\n    tokens.append(model[word])\n\ntsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=500)\nnew_values1 = tsne_model.fit_transform(tokens)\n\nK = range(1,30)\n\nfor k in tqdm(K):\n    km = KMeans(n_clusters=k)\n    km = km.fit(new_values1)\n    Sum_of_squared_distances.append(km.inertia_)\n    \n#### Plot the \"elbow\" curve ####\nplt.subplots(figsize = (10,6))\nplt.plot(K, Sum_of_squared_distances, 'bx-')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal Number of Clusters K', fontsize = 24, fontweight = \"bold\")\nplt.savefig(\"Elbow_Method_Optimal_K.png\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tsne_plot(model, key_words):\n    \"Creates a TSNE model and plots it\"\n    \n    labels = []\n    tokens = []\n    \n    for word in Word2Vec_Sorted(model)[:300]:\n        tokens.append(model[word])\n        labels.append(word)\n\n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=500)\n    new_values = tsne_model.fit_transform(tokens)\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n\n    clusters = KMeans(n_clusters=7)\n    clusters.fit(new_values)\n    y_kmeans = clusters.predict(new_values)\n    \n    colmap = {0: 'red', 1: 'green', 2: 'blue', 3 :'black', 4:'fuchsia', 5:'orange', 6:'grey', 7:'grey'}\n\n    dict={}\n    for i in range(len(colmap)):\n        dict[colmap[i]]=list(y_kmeans).count(i)/len(y_kmeans)*100\n    \n    plt.figure(figsize=(20,15))\n\n    plt.title('Word2Vec Model Vizualization')\n    plt.xlabel('Dimension 1')\n    plt.ylabel('Dimension 2')\n\n    for i in range(len(new_values)):\n        plt.scatter(x[i], y[i], color=colmap[y_kmeans[i]], s=12)\n        if labels[i] in key_words:\n            plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom', fontsize = 12, weight=\"bold\", color= 'red')\n        else:\n            plt.annotate(labels[i],\n                         xy=(x[i], y[i]),\n                         xytext=(5, 2),\n                         textcoords='offset points',\n                         ha='right',\n                         va='bottom', fontsize=12)\n    \n \n    plt.savefig(\"tsne_visualization.png\")\n    plt.show()\n    plt.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = time()\n\n#### TSNE Data Visualization ####\n\nkey_words = ['pangolin', 'origin', 'transmission', 'vaccine','symptom', 'environment', 'patient', 'outbreak']\n\ntsne_plot(model, key_words)\n\nprint('Time to process data: {} mins'.format(round((time() - t) / 60, 2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above TSNE visualization of Word2Vec embeddings, we can distinguish several clusters among which we can recognize semantic similarities including for instance, *medical treatment, government policies and measures, vaccine research, epidemiological research, covid-19 detection, transmission, causes and consequences of the disease*... Inter-word distance in the 2D plane is an indication of inter-word similarity.\n***"},{"metadata":{},"cell_type":"markdown","source":"# **Part 5: Latent Dirichlet Based Topic Modeling**"},{"metadata":{},"cell_type":"markdown","source":"### ** <font color=green> Literature Review **"},{"metadata":{},"cell_type":"markdown","source":"In recent years, the Latent Dirichlet Allocation (LDA) method for topic modeling has gained gradual popularity in project management and engineering research. LDA is an unsupervised machine learning technique that can extrapolate the core topics from a set of unlabeled documents. In LDA, each document d is viewed as a probabilistic distribution θ_d over a set of K topics and each topic k∈{1,…,K} is, in turn, represented as a probabilistic distribution φ_k over keywords in the vocabulary (Blei, Ng, & Jordan, 2003). Each word has a certain contribution to each topic. The mathematical annotations are clearly indicated in the figures below; for example, θ denotes a matrix with rows defined by documents and columns defined by topics and θ_(d,k) represents the probability of topic k occurring in document d. Similarly, φ is a matrix with rows defined by topics and columns defined by words. A simplistic representation of the LDA process is shown below."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from IPython.display import HTML, Image\nImage(filename='/kaggle/input/figure/LDA_Process_1.png', width=500,height=500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from IPython.display import HTML, Image\nImage(filename='/kaggle/input/figure/LDA_Process_2.png', width=500,height=500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ** <font color=green> Grid-based Determination of the optimal number of topics  **"},{"metadata":{},"cell_type":"markdown","source":"In this notebook, the grid-search optimization technique was implemented to find the optimal number of topics K that produces the most coherent model. To elaborate, to determine the optimal number of topics K for the corpus of abstracts, the C_v coherence metric have been computed after training baseline models over the range [10;30] of K. The coherence score C_UMass of the topic model averages the topic coherence scores for all topics in the model. Due to the log, C_UMass returns negative values, with values closer to 0 referring to more coherent topics in terms of human interpretability."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from IPython.display import HTML, Image\nImage(filename='/kaggle/input/figure/C_Umass_Formula.png', width=500,height=500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.notebook import tqdm\nfrom gensim.models.coherencemodel import CoherenceModel\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#### Fonction to estimate the optimal number of topics (i.e., the one maximizing C_Umass) ####\n#### Time-consuming function ####\n\ndef optimal_topic_number(start, end, text):\n    \n    Lda = gensim.models.ldamodel.LdaModel\n    coherenceList_cv = []\n    coherenceList_umass = []\n        \n    dictionary = corpora.Dictionary(text)\n    doc_term_matrix = [dictionary.doc2bow(doc) for doc in text]\n\n    num_topics_list = np.arange(start,end)\n\n    for num_topics in tqdm(num_topics_list):\n        lda= Lda(doc_term_matrix, num_topics=num_topics,id2word = dictionary, \n                 passes=20,chunksize=10000,random_state=43)\n        cm = CoherenceModel(model=lda, corpus=doc_term_matrix, \n                            dictionary=dictionary, coherence='u_mass')\n        coherenceList_umass.append(cm.get_coherence())\n\n        #cm_cv = CoherenceModel(model=lda, corpus=doc_term_matrix,\n         #                      texts=text, dictionary=dictionary, coherence='c_v')\n        #coherenceList_cv.append(cm_cv.get_coherence())\n\n\n    plotData = pd.DataFrame({'Number of topics':num_topics_list,\n                             'CoherenceScore':coherenceList_umass})\n    f,ax = plt.subplots(figsize=(10,6))\n    sns.set_style(\"darkgrid\")\n    sns.pointplot(x='Number of topics',y= 'CoherenceScore',data=plotData)\n    plt.title('Topic coherence \\n', fontsize = 24, fontweight = \"bold\")\n    plt.savefig('Topic_Coherence.png')\n    plt.show()\n    index = coherenceList_umass.index(max(coherenceList_umass))\n    return index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = time()\n\n#### Estimate of the optimal number of topics based on the existing corpus ####\n\nstart =10\nend = 25\n\nnum_optimal_topics = start + optimal_topic_number( start, end, covid_corpus)\nprint(\"Optimal Number of Topics\", num_optimal_topics)\n\nprint('Time to process data: {} mins'.format(round((time() - t) / 60, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = time()\n\ndata_lemmatized = covid_corpus\nid2word = corpora.Dictionary(data_lemmatized)\nid2word.save('dictionary.gensim')\ntexts = data_lemmatized\ncorpus = [id2word.doc2bow(text) for text in texts]\n\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                                id2word=id2word,\n                                                num_topics=num_optimal_topics,\n                                                random_state=100,\n                                                update_every=1,\n                                                chunksize=100,\n                                                passes=10,\n                                                alpha='auto',\n                                                per_word_topics=True)\n\nlda_model.save('model.gensim')\n\n# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('Coherence Score: ', coherence_lda)\n\n\nprint('Time to process data: {} mins'.format(round((time() - t) / 60, 2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"pyLDAvis is an interactive web-based LDA visualization Python package"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport os \n%matplotlib inline\nimport pyLDAvis.gensim\nimport gensim\npyLDAvis.enable_notebook()\n\nd = gensim.corpora.Dictionary.load('dictionary.gensim')\nc = [id2word.doc2bow(text) for text in texts]\nlda = gensim.models.LdaModel.load('model.gensim')\n\ndata = pyLDAvis.gensim.prepare(lda, c, d, mds='tsne')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"pyLDAvis.save_html(data, 'lda_{}topics.v0.html'.format(num_optimal_topics))\n#from IPython.core.display import display, HTML\n#display(HTML('lda_{}topics.v0.html'.format(num_optimal_topics)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Lda = gensim.models.LdaModel\nlda_final = Lda.load('model.gensim')\na = lda_final.show_topics(num_optimal_topics, formatted = False, num_words = 10)\nb = lda_final.top_topics(c,dictionary=d,topn=10) # This orders the topics in the decreasing order of coherence score\n\ntopic2skillb = {}\ntopic2csb = {}\ntopic2skilla = {}\ntopic2csa = {}\nnum_topics =lda_final.num_topics\ncnt =1\n\nfor ws in b:\n    wset = set(w[1] for w in ws[0])\n    topic2skillb[cnt] = wset\n    topic2csb[cnt] = ws[1]\n    cnt +=1\n\nfor ws in a:\n    wset = set(w[0]for w in ws[1])\n    topic2skilla[ws[0]+1] = wset\n    \nfor i in range(1,num_topics+1):\n    for j in range(1,num_topics+1):  \n        if topic2skilla[i].intersection(topic2skillb[j])==topic2skilla[i]:\n            topic2csa[i] = topic2csb[j]\n\nfinalData = pd.DataFrame([],columns=['Topic','words'])\nfinalData['Topic']=topic2skilla.keys()\nfinalData['Topic'] = finalData['Topic'].apply(lambda x: 'Topic'+str(x))\nfinalData['words']=topic2skilla.values()\nfinalData['cs'] = topic2csa.values()\nfinalData.sort_values(by='cs',ascending=False,inplace=True)\nfinalData.to_csv('CoherenceScore.csv')\nfinalData","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token_percent = data.topic_coordinates.sort_values(by ='topics').loc[:, [\"topics\", \"Freq\"]]\ndf =token_percent.iloc[:, [1]]\ndf[\"Topic\"]=1\nfor i in range(len(df)):\n    df[\"Topic\"][i]=\"Topic\"+str(i+1)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To aid in the task of topic interpretation, pyLDAvis enables users to adjust the relevance measure proposed by Sievert et al. (2015) to rank the words within topics."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_relevant_words(vis,lam=0.3,topn=10):\n    a = data.topic_info\n    a['finalscore'] = a['logprob']*lam+(1-lam)*a['loglift']\n    a = a.loc[:,['Category','Term','finalscore']].groupby(['Category'])\\\n    .apply(lambda x: x.sort_values(by='finalscore',ascending=False).head(topn))\n    a = a.loc[:,'Term'].reset_index().loc[:,['Category','Term']]\n    a = a[a['Category']!='Default']\n    a = a.to_dict('split')['data']\n    d ={}\n    for k,v in a: \n        if k not in d.keys():\n            d[k] =set()\n            d[k].add(v)\n        else:\n            d[k].add(v)\n    finalData = pd.DataFrame([],columns=['Topic','words with Relevance'])\n    finalData['Topic']=d.keys()\n    finalData['words with Relevance']=d.values()\n    return finalData","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('max_colwidth', 170)\n\na = get_relevant_words(data,0.4,15).merge(finalData,how='left',on ='Topic').merge(df, how='left', on = 'Topic').sort_values(by='Freq',ascending=False)\na.rename(columns={'cs':'Coherence','Freq':'Frequency', 'words with Relevance':'Relevant Words'}, inplace=True)\nb = a[['Topic', 'Frequency','Relevant Words', 'Coherence']]\nb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After adjusting the relevance metrics, the top-10 relevant words of each topics modeled aid topic interpretation. Related topic frequencies and CUMass are also indicated. Then, the identified topics are easily interpretable and refers to topics such as, for instance, *medical treatment, government policies and measures, vaccine research, epidemiological research, covid-19 detection, transmission, causes and consequences of the disease*...."},{"metadata":{},"cell_type":"markdown","source":"**Many thanks for your time and consideration.**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}