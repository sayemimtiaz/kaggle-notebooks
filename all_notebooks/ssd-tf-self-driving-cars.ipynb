{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Single Shot MultiBox Detector (SSD) Self-Driving Cars"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/keras-ssd/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN, CSVLogger\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import load_model\nfrom math import ceil\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\nfrom models.keras_ssd7 import build_model\nfrom keras_loss_function.keras_ssd_loss import SSDLoss\nfrom keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\nfrom keras_layers.keras_layer_DecodeDetections import DecodeDetections\nfrom keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n\nfrom ssd_encoder_decoder.ssd_input_encoder import SSDInputEncoder\nfrom ssd_encoder_decoder.ssd_output_decoder import decode_detections, decode_detections_fast\n\nfrom data_generator.object_detection_2d_data_generator import DataGenerator\nfrom data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\nfrom data_generator.data_augmentation_chain_variable_input_size import DataAugmentationVariableInputSize\nfrom data_generator.data_augmentation_chain_constant_input_size import DataAugmentationConstantInputSize\nfrom data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_height = 300\nimg_width = 480\nimg_channels = 3\n\nintensity_mean = 127.5\nintensity_range = 127.5\n\nn_classes =5\nscales = [0.08, 0.16, 0.32, 0.64, 0.96]\naspect_ratios = [0.5, 1.0, 2.0]\ntwo_boxes_for_ar1 = True\nsteps = None\noffsets = None\nclip_boxes = False\nvariances = [1.0, 1.0, 1.0, 1.0]\nnormalize_coords = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"K.clear_session()\n\nmodel = build_model(image_size=(img_height, img_width, img_channels),\n                    n_classes=n_classes,\n                    mode='training',\n                    l2_regularization=0.0005,\n                    scales=scales,\n                    aspect_ratios_global=aspect_ratios,\n                    aspect_ratios_per_layer=None,\n                    two_boxes_for_ar1=two_boxes_for_ar1,\n                    steps=steps,\n                    offsets=offsets,\n                    clip_boxes=clip_boxes,\n                    variances=variances,\n                    normalize_coords=normalize_coords,\n                    subtract_mean=intensity_mean,\n                    divide_by_stddev=intensity_range)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights('../input/keras-ssd/ssd7_weights.h5')\n\nadam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n\nssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n\nmodel.compile(optimizer=adam, loss=ssd_loss.compute_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\nval_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#images_dir = '../input/deleted-datas/del_hatjpgbasler/hatjpgbasler/'\nimages_dir = '../input/self-driving-cars/images/'\ntrain_labels_filename = '../input/self-driving-cars/labels_train.csv'\nval_labels_filename   = '../input/self-driving-cars//labels_val.csv'\n#train_df = pd.read_csv('../input/deleted-datas/new_augmnts_datas4')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.rename(columns={'x':'xmin'}, inplace=True)\ntrain_df.rename(columns={'y':'ymin'}, inplace=True)\ntrain_df.rename(columns={'filename':'image_name'}, inplace=True)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df[['image_name', 'region_count', 'height', 'width','xmin','ymin']]\ntrain_df[\"xmax\"] = train_df.apply(lambda col: col.width + col.xmin, axis=1)\ntrain_df[\"ymax\"] = train_df.apply(lambda col: col.height + col.ymin, axis = 1)\ntrain_df=train_df.drop(['height', 'width'], axis=1)\ntrain_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_df['class_id'] = train_df['region_count'].map(lambda c_row: 1 if c_row>0 else 0)\ntrain_df.drop(['region_count'], axis=1, inplace=True)\ntrain_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['xmin'] = train_df['xmin'].astype(int)\ntrain_df['ymin'] = train_df['ymin'].astype(int)\ntrain_df['xmax'] = train_df['xmax'].astype(int)\ntrain_df['ymax'] = train_df['ymax'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[0:13400].to_csv('train_labels' , index=False)\ntrain_df[13401:16827].to_csv('val_labels', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels_filename = './train_labels'\nval_labels_filename   = './val_labels'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = pd.read_csv( './train_labels')\na","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset.parse_csv(images_dir=images_dir,\n                        labels_filename=train_labels_filename,\n                        input_format=['image_name', 'xmin', 'xmax', 'ymin', 'ymax', 'class_id'],\n                        include_classes='all')\n\nval_dataset.parse_csv(images_dir=images_dir,\n                      labels_filename=val_labels_filename,\n                      input_format=['image_name', 'xmin', 'xmax', 'ymin', 'ymax', 'class_id'],\n                      include_classes='all')","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"train_dataset_size = train_dataset.get_dataset_size()\nval_dataset_size   = val_dataset.get_dataset_size()\n\nprint(\"Number of images in the training dataset:\\t{:>6}\".format(train_dataset_size))\nprint(\"Number of images in the validation dataset:\\t{:>6}\".format(val_dataset_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 16\n\ndata_augmentation_chain = DataAugmentationConstantInputSize(random_brightness=(-48, 48, 0.5),\n                                                            random_contrast=(0.5, 1.8, 0.5),\n                                                            random_saturation=(0.5, 1.8, 0.5),\n                                                            random_hue=(18, 0.5),\n                                                            random_flip=0.5,\n                                                            random_translate=((0.03,0.5), (0.03,0.5), 0.5),\n                                                            random_scale=(0.5, 2.0, 0.5),\n                                                            n_trials_max=3,\n                                                            clip_boxes=True,\n                                                            overlap_criterion='area',\n                                                            bounds_box_filter=(0.3, 1.0),\n                                                            bounds_validator=(0.5, 1.0),\n                                                            n_boxes_min=1,\n                                                            background=(0,0,0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictor_sizes = [model.get_layer('classes4').output_shape[1:3],\n                   model.get_layer('classes5').output_shape[1:3],\n                   model.get_layer('classes6').output_shape[1:3],\n                   model.get_layer('classes7').output_shape[1:3]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n                                    img_width=img_width,\n                                    n_classes=n_classes,\n                                    predictor_sizes=predictor_sizes,\n                                    scales=scales,\n                                    aspect_ratios_global=aspect_ratios,\n                                    two_boxes_for_ar1=two_boxes_for_ar1,\n                                    steps=steps,\n                                    offsets=offsets,\n                                    clip_boxes=clip_boxes,\n                                    variances=variances,\n                                    matching_type='multi',\n                                    pos_iou_threshold=0.5,\n                                    neg_iou_limit=0.3,\n                                    normalize_coords=normalize_coords)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_generator = train_dataset.generate(batch_size=batch_size,\n                                         shuffle=True,\n                                         transformations=[data_augmentation_chain],\n                                         label_encoder=ssd_input_encoder,\n                                         returns={'processed_images',\n                                                  'encoded_labels'},\n                                         keep_images_without_gt=False)\n\nval_generator = val_dataset.generate(batch_size=batch_size,\n                                     shuffle=False,\n                                     transformations=[],\n                                     label_encoder=ssd_input_encoder,\n                                     returns={'processed_images',\n                                              'encoded_labels'},\n                                     keep_images_without_gt=False)","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"model_checkpoint = ModelCheckpoint(filepath='ssd7_weights.h5',\n                                   monitor='val_loss',\n                                   verbose=1,\n                                   save_best_only=True,\n                                   save_weights_only=False,\n                                   mode='auto',\n                                   period=1)\n\nearly_stopping = EarlyStopping(monitor='val_loss',\n                               min_delta=0.0,\n                               patience=10,\n                               verbose=1)\n\nreduce_learning_rate = ReduceLROnPlateau(monitor='val_loss',\n                                         factor=0.2,\n                                         patience=8,\n                                         verbose=1,\n                                         min_delta=0.001,\n                                         cooldown=0,\n                                         min_lr=0.00001)\n\ncallbacks = [model_checkpoint,\n             early_stopping,\n             reduce_learning_rate]","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"def train(model):\n    initial_epoch   = 0\n    final_epoch     = 20\n    steps_per_epoch = 1000\n\n    history = model.fit_generator(generator=train_generator,\n                                steps_per_epoch=steps_per_epoch,\n                                epochs=final_epoch,\n                                callbacks=callbacks,\n                                validation_data=val_generator,\n                                validation_steps=ceil(val_dataset_size/batch_size),\n                                initial_epoch=initial_epoch)\n\n    plt.figure(figsize=(20,12))\n    plt.plot(history.history['loss'], label='loss')\n    plt.plot(history.history['val_loss'], label='val_loss')\n    plt.legend(loc='upper right', prop={'size': 24})\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_generator = val_dataset.generate(batch_size=1,\n                                         shuffle=True,\n                                         transformations=[],\n                                         label_encoder=None,\n                                         returns={'processed_images',\n                                                  'processed_labels',\n                                                  'filenames'},\n                                         keep_images_without_gt=False)","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"batch_images, batch_labels, batch_filenames = next(predict_generator)\n\ni =0\n\nprint(\"Image:\", batch_filenames[i])\nprint()\nprint(\"Ground truth boxes:\\n\")\nprint(batch_labels[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(batch_images)","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"# 4: Decode the raw prediction `y_pred`\n\ny_pred_decoded = decode_detections(y_pred * 266.,\n                                   confidence_thresh=0.5,\n                                   iou_threshold=0.45,\n                                   top_k=200,\n                                   normalize_coords=normalize_coords,\n                                   img_height=img_height,\n                                   img_width=img_width)\n\nnp.set_printoptions(precision=2, suppress=True, linewidth=90)\nprint(\"Predicted boxes:\\n\")\nprint('   class   conf xmin   ymin   xmax   ymax')\nprint(y_pred_decoded[i])","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,12))\nplt.imshow(batch_images[i])\n\ncurrent_axis = plt.gca()\n\ncolors = plt.cm.hsv(np.linspace(0, 1, n_classes+1)).tolist()\nclasses = ['background', 'car', 'truck', 'pedestrian', 'bicyclist', 'light']\n\nfor box in batch_labels[i]:\n    xmin = box[1]\n    ymin = box[2]\n    xmax = box[3]\n    ymax = box[4]\n    label = '{}'.format(classes[int(box[0])])\n    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color='green', fill=False, linewidth=2))  \n    current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':'green', 'alpha':1.0})\n\nfor box in y_pred_decoded[i]:\n    xmin = box[-4]\n    ymin = box[-3]\n    xmax = box[-2]\n    ymax = box[-1]\n    color = colors[int(box[0])]\n    label = '{}: {:.2f}'.format(classes[int(box[0])], box[1])\n    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color=color, fill=False, linewidth=2))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,12))\nplt.imshow(batch_images[i])\n\ncurrent_axis = plt.gca()\n\ncolors = plt.cm.hsv(np.linspace(0, 1, n_classes+1)).tolist()\nclasses = ['background', 'car', 'truck', 'pedestrian', 'bicyclist', 'light']\n\nfor box in batch_labels[i]:\n    xmin = box[1]\n    ymin = box[2]\n    xmax = box[3]\n    ymax = box[4]\n    label = '{}'.format(classes[int(box[0])])\n    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color='green', fill=False, linewidth=2))  \n    current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':'green', 'alpha':1.0})\n\nfor box in y_pred_decoded[i]:\n    xmin = box[-4]\n    ymin = box[-3]\n    xmax = box[-2]\n    ymax = box[-1]\n    color = colors[int(box[0])]\n    label = '{}: {:.2f}'.format(classes[int(box[0])], box[1])\n    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color=color, fill=False, linewidth=2))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,12))\nplt.imshow(batch_images[i])\n\ncurrent_axis = plt.gca()\n\ncolors = plt.cm.hsv(np.linspace(0, 1, n_classes+1)).tolist()\nclasses = ['background', 'car', 'truck', 'pedestrian', 'bicyclist', 'light']\n\nfor box in batch_labels[i]:\n    xmin = box[1]\n    ymin = box[2]\n    xmax = box[3]\n    ymax = box[4]\n    label = '{}'.format(classes[int(box[0])])\n    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color='green', fill=False, linewidth=2))  \n    current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':'green', 'alpha':1.0})\n\nfor box in y_pred_decoded[i]:\n    xmin = box[-4]\n    ymin = box[-3]\n    xmax = box[-2]\n    ymax = box[-1]\n    color = colors[int(box[0])]\n    label = '{}: {:.2f}'.format(classes[int(box[0])], box[1])\n    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color=color, fill=False, linewidth=2))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}