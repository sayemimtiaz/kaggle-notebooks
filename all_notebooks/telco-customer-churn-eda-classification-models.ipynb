{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading Data and Checking Basic Information","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(df.head(5))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# #Converting \"Total Charges\" to Numeric Column ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#df['TotalCharges'] = df['TotalCharges'].astype(float)\n\n\ndf['TotalCharges'] = pd.to_numeric(df['TotalCharges'],errors='coerce')\n\n#Checking Data Type after making changes\nprint(df.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  **Checking distribution & skewness of continous variables and trying to improve skewness**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(df[df['TotalCharges'].isnull()])\n\nsns.distplot(df['TotalCharges'])\nplt.show()\nprint(df['TotalCharges'].skew())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Above figure shows Actual distribution of column \"Total Charges\" **","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Lets Try to improve distribution of this column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#sns.pairplot(df)\n#plt.show()\nfrom scipy.stats import boxcox\n\n#sns.distplot(df['TotalCharges'])\n#df['TotalCharges']=df['TotalCharges'].transform('sqrt')\ndf['TotalCharges'] = boxcox(df['TotalCharges'],0.5)\nprint(df['TotalCharges'].skew())\n\nsns.distplot(df['TotalCharges'])\nplt.show()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**we can see from above figure and skewness score that distribution is little better if we take square root of the column, instead of actual values!!**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['MonthlyCharges'])\nplt.show()\nprint(\"Checking Skewness :- \",df['MonthlyCharges'].skew())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Since Skewness is already very low we will not take any action on this column!!**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Analyzing all Categorical Varibles with respect to Churn Variable**","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"# Creating a function to take DF and identify Categorical Varibles and create a crosstab \n#and plot the same.\ndef category_rel_y(df):\n    X=df.columns\n    #print(df[X[1]])\n \n    #print(len(X))\n \n    for i in range(1,len(X)-2):\n        \n    \n        if (df[X[i]].dtype=='object'):\n                fig, axs = plt.subplots(1, 2,figsize=(13,3))\n                tab_values=pd.crosstab(df[X[i]],df.iloc[:,20])\n                #Creating  % of total values cross tab by using 'normalize=True' in normal tab\n                tab_percentage=pd.crosstab(df[X[i]],df.iloc[:,20],normalize=True)\n                #print(tab_percentage)\n                tab_values.plot(kind='bar', stacked=False ,ax=axs[0] )\n                sns.heatmap(tab_percentage,annot=True,cmap='YlGnBu',ax=axs[1])\n                plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=1, hspace=None)\n                #sns.barplot(tab)\n                axs[0].set_title('Actual counts for CHURN output versus '+X[i].upper())\n              \n                plt.title(\"% of total distribution for CHURN and \"+(X[i]).upper()) \n                plt.show()\n                 \n            \n        \n        \n #plt.show()     \ncategory_rel_y(df)   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**After Observing above charts we can see some variables like** \n* \"Contract\"\n* \"OnlineSecurity\"\n* \"InternetService\" \n* \"Payment Method\"\n* \"Paperless Billing\"\n\n\n\n**Can strongly differentiate between Churn and No Churn**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Checking **Tenure** relationship with **Churn** by creating bins","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['tenure'])\ndf['tenure'].skew()\n\ndef bins(x):\n    if x>=0 and x<=15:\n        return '0-15'\n    elif x>=16 and x<=30:\n        return '16-30'\n    elif x>=31 and x<=45:\n        return '31-45'\n    elif x>=46 and x<=60:\n        return '46-60'\n    elif x>=61 and x<=75:\n        return '61-75'\n    else:\n        return 'Above 80'\ndf['tenure_bins']=df['tenure'].map(bins)\n#print(df[['tenure_bins','tenure']])\n\nx=(pd.crosstab(df['tenure_bins'],df['Churn']))\n\nx.plot(kind='bar', stacked=False)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is visible here that people with less tenure are likely to quit more!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **We will now check Correlation among numeric variables.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#sns.heatmap(df.corr(),annot=True,cmap='viridis')\n#plt.show()\n\n\n#Convert Churn into numerical values of 0 and 1 to get correlation with other numerical variables\nchurn=pd.get_dummies(df[\"Churn\"],drop_first=True)\nchurn.rename(columns = {'Yes':'Churn_continuous'}, inplace = True) \ndf=pd.concat([df,churn],axis=1)\n\n\n\ndf_corr=df[['MonthlyCharges','TotalCharges','tenure','Churn_continuous']].corr()\n#create a mask to avoid duplicates in correlation matrix\nmask = np.zeros_like(df_corr, dtype=np.bool)\n#print(mask.shape)\nmask[np.triu_indices_from(mask)] = True\nfig=plt.subplots(figsize=(8,4))\nsns.heatmap(df_corr,annot=True,cmap='viridis',mask=mask)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. **We can see churn has high negative relation with \"Total Charges\" and \"Tenure\", which makes sense because if you are a long tenure customer or Charges are high then you are less like to quit the membership!!**\n \n1. **Also, We should remove \"tenure\" Or \"Total Charges\" from our train model since they seem to have very high Correlation among themselves.**\n1. **But we observe that \"Total Charges\" also have high correaltion with \"Monthly Charges\", So we will remove \"Total Charges\" from our Dataset**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Lets check Relation of other important categorical variables with Monthly charges, since its seems to have strong negative relation with Churn","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Both Boxplot and violin chart show following Information:-\n*   median (a white dot on the violin plot)\n*   interquartile range (the black bar in the center of violin)\n*   Outliers\n\n![](https://miro.medium.com/max/780/1*TTMOaNG1o4PgQd-e8LurMg.png)\nSource for image and info:-https://towardsdatascience.com/violin-plots-explained-fb1d115e023d\n  \n\n\n**We will use Violin Chart instead of box plot** because of following reasons:-\nAdvantage of the violin plot over the box plot is that aside from showing the \nstatistics like it also shows the entire distribution of the data.\nThis is of interest, especially when dealing with multimodal data, i.e.,\na distribution with more than one peak. \n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"list=[\"Contract\",\n\"OnlineSecurity\",\n\"InternetService\",\n\"PaymentMethod\",\n\"PaperlessBilling\",\"TechSupport\",\"StreamingMovies\"]\n\nfor i in range(len(list)):\n    fig, ax = plt.subplots(figsize =(8, 4)) \n    sns.violinplot(ax = ax,data=df, x = list[i],  \n                  y = \"MonthlyCharges\", hue=\"Churn\", kind='violin',split=True) \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# * *We can clearly See from above Charts that if Monthly charges are high, there is tendancy for High Churn*\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Check the target variable for Class Imbalance**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df['Churn'].value_counts()\nprint(X)\ny=X/len(df)\nsns.barplot(x=X,y=(X/len(df))*100)\n\n\n\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there is class imbalance!! and we need to fix this.\nMost machine learning algorithms work best when the number of samples in each class are about equal.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We have 5 different methods for dealing with imbalanced datasets:\n1. Change the performance metric\n1. Change the algorithm\n1. Oversample minority class\n1. Undersample majority class\n1. Generate synthetic samples (SMOTE)\n\nWe will use SMOTE for now\n\n1. Randomly pick a point from the minority class.\n1. Compute the k-nearest neighbors (for some pre-specified k) for this point.\n1. Add k new points somewhere between the chosen point and each of its neighbors","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(df.columns)\n# drop 13 rows with null in \"Total charges\"\ndf = df.dropna()\n\n#one-hot encoding\n\nx=df[['gender', 'SeniorCitizen', 'Partner', 'Dependents','tenure',\n        'PhoneService', 'MultipleLines', 'InternetService',\n       'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',\n       'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',\n       'PaymentMethod', 'MonthlyCharges']]\nX=pd.get_dummies(x, drop_first=True )\n#print(X.columns)\nc=X.columns\n\n\nc=X.columns        \n#for i in range(0,len(c)):\n#    if (X[c[i]].dtype=='object'):\n#        X.drop(c[i],axis=1,inplace=True)\n    \n\ny=pd.get_dummies(df['Churn'], drop_first=True)\n#print(y)\n\n\n#print(\"After \",X)\n#print(\"After \",y)\n\n#Applyting SMOTE after doing one-hot encoding and removing null values\n\nimport sklearn.model_selection as model_selection\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=0.70,test_size=0.30, random_state=101)\n\n        \n\nfrom imblearn.over_sampling import SMOTE\nsmt = SMOTE(random_state = 2)\nX_train, y_train = smt.fit_sample(X_train, y_train)\n\n\nsmote_class=y_train['Yes'].value_counts()\nprint(smote_class)\n#print(X_train.info())\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can see above that SMOTE algo has provided eqaul samples for Churn and non Churn! in Training Data**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Applying models and evaluting performance**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"****1. Random Forest ********","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix  \nfrom sklearn.metrics import classification_report  \n\n#from sklearn.preprocessing import StandardScaler\n#SC = StandardScaler()\n#X_train_MC = SC.fit_transform(X_train['MonthlyCharges'])\n#X_train.drop['MonthyCharges']\n#X_test = SC.transform(X_test)\n\n\nclf=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\nclf.fit(X_train,y_train)\n\ny_pred=clf.predict(X_test)\n\n# we can use predict_proba() as well\n#y_pred=clf.predict_proba(X_test)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Checking how Good is our model performance!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"True Positives (TP) - These are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted class is also yes. \n\nTrue Negatives (TN) - These are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no. \nFalse positives and false negatives, these values occur when your actual class contradicts with the predicted class.\n\nFalse Positives (FP) â€“ When actual class is no and predicted class is yes. \nFalse Negatives (FN) â€“ When actual class is yes but predicted class in no. \n\nOnce you understand these four parameters then we can calculate Accuracy, Precision, Recall and F1 score.\n\nAccuracy - Accuracy is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations. One may think that, if we have high accuracy then our model is best. Yes, accuracy is a great measure but only when you have symmetric datasets where values of false positive and false negatives are almost same. Therefore, you have to look at other parameters to evaluate the performance of your model. \nAccuracy = TP+TN/TP+FP+FN+TN\n\nPrecision - Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. The question that this metric answer is of all passengers that labeled as survived, how many actually survived? High precision relates to the low false positive rate. \n\nPrecision = TP/TP+FP\n\nRecall (Sensitivity) - Recall is the ratio of correctly predicted positive observations to the all observations in actual class - yes. \nRecall = TP/TP+FN\n\nF1 score - F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, itâ€™s better to look at both Precision and Recall.\n\nF1 Score = 2*(Recall * Precision) / (Recall + Precision)\n\n\noriginal source Link:-\nStudy for more details with example\n\nhttps://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/#:~:text=F1%20score%20%2D%20F1%20Score%20is,have%20an%20uneven%20class%20distribution.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\n\nprint(\"Accuracy:******************\",metrics.accuracy_score(y_test, y_pred))\n\n\n#visualizing confusion matrix\nfrom sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(clf, X_test, y_test,cmap='viridis')  # doctest: +SKIP\nplt.title(\"*Confusion Matrix*\")\nplt.show()\n\nprint(\"***************Classification report*************\")\nprint(classification_report(y_test, y_pred))\n\n#sns.heatmap(classification_report(y_test, y_pred))\n#plt.show()\n\nfeature_imp = pd.Series(clf.feature_importances_,index=X_train.columns).sort_values(ascending=False)\n#sns.barplot(x=feature_imp, y=feature_imp.index)\n# Add labels to your graph\n#print(feature_imp)\nvisual=feature_imp.reset_index()\n   \n   \nvisual.rename(columns = {'index':'features', 0:'Importance_score'}, inplace = True) \nprint(visual)\nfig=plt.subplots(figsize=(10,15))\nsns.barplot(x = 'Importance_score', y = 'features', data = visual)\nplt.xlabel('Importance_score')\nplt.ylabel('Features')\n#plt.title(\"Visualizing Important Features\")\n\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****2.Light Gradient Boosting ****","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\nfrom lightgbm import LGBMClassifier\nmodel = LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n               importance_type='split', learning_rate=0.1, max_depth=-1,\n               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n               n_estimators=500, n_jobs=-1, num_leaves=31, objective=None,\n               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\nmodel.fit(X_train, y_train)\ny_pred=model.predict(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" **3. Gradient Boosting**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# define dataset\n\nmodel = XGBClassifier(silent=False, \n                      scale_pos_weight=1,\n                      learning_rate=0.1,  \n                      colsample_bytree = 0.4,\n                      subsample = 0.8,\n                      objective='binary:logistic', \n                      n_estimators=1000, \n                      reg_alpha = 0.3,\n                      max_depth=4, \n                      gamma=10)\nmodel.fit(X_train, y_train)\ny_pred=model.predict(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using **Artificial neural network (ANN)** to check if there is **any improvement in accuracy**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dropout,Dense,Flatten\n\nmodel=Sequential()\n\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dropout(.4))\nmodel.add(Dense(64,activation='relu'))\n\n\n\nmodel.add(Dense(1,activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n\n\nfrom tensorflow.keras.callbacks import EarlyStopping\nearly_stop=EarlyStopping(monitor='val_loss',patience=2)\nresults=model.fit(X_train,y_train,epochs=50,validation_data=(X_test,y_test),callbacks=[early_stop])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We can from above results that there is no significant improvement even after using ANN.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}