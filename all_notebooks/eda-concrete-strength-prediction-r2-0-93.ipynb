{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import the necessary libraries.","metadata":{}},{"cell_type":"code","source":"# Built-in packages\nimport json\nimport warnings\nimport re\nwarnings.filterwarnings(\"ignore\")\n\n# Third party packages\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\n\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn import model_selection\n\nfrom sklearn import linear_model\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\n\n\nsns.set(context= \"notebook\", color_codes=True)\nplt.style.use('bmh')\n\n%matplotlib inline\npd.set_option('display.max_columns', None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read the data as a data frame","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/regression-with-neural-networking/concrete_data.csv\")\ndf.columns = ['cement', 'slag', 'flyash', 'water', 'superplasticizer', 'coarse_agg', 'fine_agg', 'age', 'strength']\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"The shape of the DataFrame is: {df.shape}, which means there are {df.shape[0]} rows and {df.shape[1]} columns.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**cement**$~~~~~~~~~~~~~~~~~~~~~~~$- measured in kg in a m3 mixture\n____\n**slag**$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$- measured in kg in a m3 mixture\n____\n**flyash**$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$- measured in kg in a m3 mixture\n____\n**water**$~~~~~~~~~~~~~~~~~~~~~~~~~~$- measured in kg in a m3 mixture\n____\n**superplasticizer**$~~~~~~~~~~~~~~~$- measured in kg in a m3 mixture\n____\n**coarse_agg**$~~~~~~~~~~~~~~~~~~$- measured in kg in a m3 mixture\n____\n**fine_agg**$~~~~~~~~~~~~~~~~~~~~~~~$- measured in kg in a m3 mixture\n____\n**age**$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$- day (1~365)\n____\n**strength**$~~~~~~~~~~~~~~~~~~~~~$- measured in kg in a m3 mixture\n____","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 8 columns with float datatype and one column with int. There seem to be no null values in any of the columns","metadata":{}},{"cell_type":"code","source":"df_summary = df.describe()\ndf_summary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The columns **slag**, **ash** and **superplasticizer** have zero values which could be thought of as missing values and fill them using regression.","metadata":{}},{"cell_type":"markdown","source":"## Check for outliers","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(3, 3, figsize = (20,20));\ncol_cnt = 0\ncol_names = df.columns.tolist()\nfor r in range(3):\n    for c in range(3):\n        try:\n            sns.boxplot(df[col_names[col_cnt]], ax=axes[r][c], orient=\"h\");\n            col_cnt += 1\n        except:\n            pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are a few outliers in **slag**, **water**, **superplasticizer**, **fine_agg** and **age**. \n\nRemoving the rows with outliers removes around 10% of the data so we will substitute them with the mean values.","metadata":{}},{"cell_type":"code","source":"outlier_cols = [\"slag\", \"water\", \"superplasticizer\", \"fine_agg\", \"age\"]\ndef cap_outliers(df, col):\n    q1 = df_summary[col].loc[\"25%\"]\n    q3 = df_summary[col].loc[\"75%\"]\n    iqr = q3 - q1\n    lower_bound = q1 - (1.5 * iqr)\n    upper_bound = q3 + (1.5 * iqr)\n    df.loc[df[col] < lower_bound, col] = df_summary[col].loc[\"mean\"]\n    df.loc[df[col] > upper_bound, col] = df_summary[col].loc[\"mean\"]\n    return df\n    \n    \nfor col in outlier_cols:\n    df = cap_outliers(df, col)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Multivariate analysis","metadata":{}},{"cell_type":"code","source":"sns.pairplot(df, diag_kind=\"kde\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 3, figsize = (20,10));\n\nsns.scatterplot(y=\"slag\", x=\"cement\", data=df, ax=axes[0], alpha=0.5);\nsns.scatterplot(y=\"flyash\", x=\"cement\", data=df, ax=axes[1], alpha=0.5);\nsns.scatterplot(y=\"superplasticizer\", x=\"cement\", data=df, ax=axes[2], alpha=0.5);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the charts above, we see some sort of pattern in the data and a lot of columns with zero values. We shall impute them using Linear Regression by considering other features to predict them.\n\n**Observations:**\n\n* As the amount of cement increases, the amount of slag does too. A pattern is visible in the first chart.\n* If we look at the cement vs flyash closely, we can see pattern with a negative slope, the amount of cement is inversely proportional to the amount of flyash.\n* The cement vs superplasticizer trend is similar to that of cement vs slag.","metadata":{}},{"cell_type":"markdown","source":"### Perform necessary imputation","metadata":{}},{"cell_type":"code","source":"def impute_values(alg, df, a, cols):\n    df[a] = df[a].replace({0: np.nan})\n    df_notnull = df[df[a].notna()]\n    \n    X = df_notnull[cols]\n    y = df_notnull[a]\n    \n    df_null = df[df[a].isna()]\n    test_X = df_null[cols]\n\n    regr = alg\n    regr.fit(X, y)\n\n    pred = regr.predict(test_X)\n    df_null = df[df[a].isna()]\n    df_null[a] = pred\n    \n    xx = df[df[a].notna()]\n    xx[\"null\"] = 0\n    df_null[\"null\"] = 1\n    df = pd.concat([xx, df_null], axis=0)\n    cols.append(a)\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col, params in {\"flyash\": [\"cement\", \"water\", \"fine_agg\"], \"slag\": [\"cement\", \"coarse_agg\", \"fine_agg\"], \"superplasticizer\" : [\"cement\", \"water\", \"coarse_agg\"]}.items():\n    alg = LinearRegression()\n    df = impute_values(alg, df, col, params)\n    \nfig, axes = plt.subplots(1, 3, figsize = (20,10));\n\nsns.scatterplot(y=\"slag\", x=\"cement\", data=df, hue=\"null\", ax=axes[0], alpha=0.5, palette=[\"steelblue\", \"green\"]);\nsns.scatterplot(y=\"flyash\", x=\"cement\", data=df, hue=\"null\", ax=axes[1], alpha=0.5, palette=[\"steelblue\", \"green\"]);\nsns.scatterplot(y=\"superplasticizer\", x=\"cement\", data=df, hue=\"null\", ax=axes[2], alpha=0.5, palette=[\"steelblue\", \"green\"]);\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above plot shows the zero values imputed in green and the rest in blue. Looks like our imputed values fit the patterns.","metadata":{}},{"cell_type":"code","source":"df.drop(columns=[\"null\"], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.jointplot(x=\"flyash\", y=\"slag\", data=df[[\"flyash\", \"slag\"]], kind=\"reg\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For some of the data points in the above plot, there seems to be a direct correlation between the amount of slag and flyash with a slope of almost equal to 0.8 which is quite high. Amount of slag increases with increase in flyash.","metadata":{}},{"cell_type":"markdown","source":"# Feature engineering","metadata":{}},{"cell_type":"markdown","source":"### Creating composite features\n\nWe can create 2 composite features using **cement** - **water** and **coarse_agg** - **fine_agg**","metadata":{}},{"cell_type":"code","source":"df_copy = df.copy()\ndf_copy[\"cement_water_ratio\"] = df_copy[\"cement\"]/df_copy[\"water\"]\ndf_copy[\"average_agg\"] = (df_copy[\"coarse_agg\"] + df_copy[\"fine_agg\"])/2\ndf_copy.drop(columns=[\"cement\", \"water\", \"coarse_agg\", \"fine_agg\"], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_col = [\"strength\"]\ncol_names = df_copy.columns.to_list()\ncol_names.remove(target_col[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,7))\nsns.heatmap(df_copy[col_names + target_col].corr(), fmt=\".2g\", annot=True);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* It looks like our composite feature **cement_water_ratio** and **flyash** are negatively correlated. \n* The target column **strength** has good amount of correlation with **age** and **cement_water_ratio**.","metadata":{}},{"cell_type":"markdown","source":"### Explore for Gaussians","metadata":{}},{"cell_type":"code","source":"g = sns.PairGrid(df_copy, diag_sharey=False)\ng.map_upper(sns.scatterplot, s=15)\ng.map_lower(sns.kdeplot)\ng.map_diag(sns.kdeplot, lw=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Looking at the kdeplots of strength vs other independent columns, we can see that the data is infact a mixture of Gaussians.\n\n* Strength vs flyash shows us two distinct Gaussians and 2 clusters but looking at the position of the two clusters, it might not be very helpful in determining/predicting the strength and the same goes for Strength vs superplasticizer.","metadata":{}},{"cell_type":"code","source":"sns.jointplot(x=\"strength\", y=\"age\", data=df_copy[[\"age\", \"strength\"]], kind=\"kde\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Strength vs age is very interesting to look at. We can clearly see four clusters, 2 big ones and 2 tiny. We can also see that as the age increases, the strength increases too so this might be a helpful feature in predicting the Strength. There is one cluster that is far away from the rest of the clusters. It could mean that the concrete that has an age range of 80-100 has average strength.\n\n* We will check if it really works by creating a KNeighbors Regressor model.","metadata":{}},{"cell_type":"markdown","source":"## Create the models","metadata":{}},{"cell_type":"code","source":"# Prepare data for training\n\nX = df_copy[col_names]    # Contains the independent columns \ny = df_copy[target_col]     # Our target column\n\nseed = 16\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.3, random_state = seed)\ntrain_X_poly, train_y_poly, test_X_poly, test_y_poly = (None,) * 4\ntrain_y = train_y[target_col[0]]\ntest_y = test_y[target_col[0]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluation = {\"Metrics\" : ['Root Mean Squared Error (RMSE)', 'Mean Absolute Error (MAE)', 'Mean Squared Error (MSE)', 'R2 score', 'CV-mean']}\ndef evaluate_model(name, train_X, train_y, test_X, test_y, types, alg, plot=True):\n    global evaluation, pred\n    \n    alg.fit(train_X, train_y)\n    print(f\"Score: {alg.score(test_X, test_y)}\")\n    if plot:\n        fig, axes = plt.subplots(2, 1, figsize=(20, 10))\n        try:\n            if types == \"Coefs\":\n                print(f\"Intercept: {alg.intercept_}\")\n                try:\n                    coefs = pd.DataFrame({\"coefs\" : alg.coef_, \"col\" : col_names})\n                except:\n                    coefs = pd.DataFrame({\"coefs\" : alg.coef_[0], \"col\" : col_names})\n                sns.barplot(x=\"col\", y=\"coefs\", data=coefs, ax=axes[1]);\n            else:\n                features = pd.DataFrame({\"features\" : alg.feature_importances_, \"col\" : col_names})\n                sns.barplot(x=\"col\", y=\"features\", data=features, ax=axes[1]);\n        except:\n            pass\n    else:\n        plt.figure(figsize=(20,5));\n        axes = [None]\n\n    pred = alg.predict(test_X)\n    rmsecm = np.sqrt(metrics.mean_squared_error(test_y,pred))\n    mae = mean_absolute_error(test_y, pred)\n    mse = mean_squared_error(test_y, pred)\n    r2 = r2_score(test_y, pred)\n\n    p = pd.DataFrame(pred, columns=[0])\n    p[\"Type\"] = \"Predictions\"\n    p[\"n\"] = list(range(p.shape[0]))\n    t = test_y.copy()\n    t = t.reset_index().set_index(\"index\")\n    t.columns = [0]\n    t[\"Type\"] = \"Actual\"\n    t = t[t[0] != \"Actual\"]\n    t[\"n\"] = list(range(p.shape[0]))\n    x = pd.concat([p,t], axis=0).reset_index()\n    sns.lineplot(x=\"n\", y=0, hue=\"Type\", data=x, markers=[\"o\", \"o\"], style=\"Type\", ax=axes[0]);\n    \n    cv = cross_val_score(alg, X, y, cv=10)\n    cv_mean = cv.ravel().mean()\n    \n    evaluation[name] = [rmsecm, mae, mse, r2, cv_mean]\n    df_ev = pd.DataFrame(evaluation)\n    plt.show()\n    return df_ev, cv","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Multiple linear regression","metadata":{}},{"cell_type":"code","source":"lr = linear_model.LinearRegression()\nevaluation, cv_scores = evaluate_model(\"Multiple Regression\", train_X, train_y, test_X, test_y, \"Coefs\", lr)\nevaluation.set_index(\"Metrics\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The column **cement_water_ratio** has the highest coefficient value than the rest of the features which means it has the highest weight in the prediction of the **strength** column.\n\n___________","metadata":{}},{"cell_type":"code","source":"print(cv_scores)\nsns.distplot(cv_scores);\nprint(f\"Average score: {round(cv_scores.mean(),2)*100}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ridge regressor","metadata":{}},{"cell_type":"code","source":"rg = linear_model.Ridge()\nevaluation, cv_scores = evaluate_model(\"Ridge\", train_X, train_y, test_X, test_y, \"Coefs\", rg)\nevaluation.set_index(\"Metrics\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The column **cement_water_ratio** has the highest coefficient value than the rest of the features which means it has the highest weight in the prediction of the **strength** column.\n\n___________","metadata":{}},{"cell_type":"markdown","source":"## Decide on complexity of the model\n\n### Polynomial regression","metadata":{}},{"cell_type":"code","source":"# Fitting Polynomial Regression to the dataset\n\npoly_reg = PolynomialFeatures(degree=3)\nX_poly = poly_reg.fit_transform(X)\n\ntrain_X_poly, test_X_poly, train_y_poly, test_y_poly = train_test_split(X_poly, y, test_size = 0.3, random_state = seed)\ntrain_y_poly = train_y_poly[target_col[0]]\ntest_y_poly = test_y_poly[target_col[0]]\n\nplr = LinearRegression()\nevaluation, cv_scores = evaluate_model(\"Polynomial Regression\", train_X_poly, train_y_poly, test_X_poly, test_y_poly,  \"Coefs\", plr, plot=False)\nevaluation.set_index(\"Metrics\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cv_scores)\nsns.distplot(cv_scores);\nprint(f\"Average score: {round(cv_scores.mean(),2)*100}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Looking at the cross validation scores of both Multiple linear regressor and Polynomial regressor with parameter of higher degrees, they are exactly the same so we don't really need a complex model. Also, looking at the coefficients, it looks like the strength can be calculated with a simple linear equation using these coefficients (meaning it is made up simple proportions of the other features). \n##### On a separate not, it doesn't give us any extra accuracy, instead increases complexity and time required to give the same results.\n\n___________","metadata":{}},{"cell_type":"markdown","source":"## Support Vector Regressor","metadata":{}},{"cell_type":"code","source":"svr = SVR(C=10, kernel=\"linear\")\nevaluation, cv_scores = evaluate_model(\"Support Vector Regression\", train_X, train_y, test_X, test_y, \"Coefs\", svr)\nevaluation.set_index(\"Metrics\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn = KNeighborsRegressor(n_neighbors=4, metric=\"manhattan\", weights=\"distance\")\nevaluation, cv_scores = evaluate_model(\"KNN\", train_X, train_y, test_X, test_y, \"Coefs\", knn, plot=False)\nevaluation.set_index(\"Metrics\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtr = DecisionTreeRegressor()\nevaluation, cv_scores = evaluate_model(\"Decision Tree Regression\", train_X, train_y, test_X, test_y, \"Features\", dtr)\nevaluation.set_index(\"Metrics\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is clearly visible that the feature that have the most importance are the **cement_water_ratio** and **age** in predicting the **strength**.\n\n___________","metadata":{}},{"cell_type":"code","source":"rfr = RandomForestRegressor()\nevaluation, cv_scores = evaluate_model(\"Random Forest Regression\", train_X, train_y, test_X, test_y, \"Features\", rfr)\nevaluation.set_index(\"Metrics\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is clearly visible that the feature that have the most importance are the **cement_water_ratio** and **age** in predicting the **strength**.\n\n___________","metadata":{}},{"cell_type":"code","source":"xgc = XGBRegressor()\nevaluation, cv_scores = evaluate_model(\"XGBoost\", train_X, train_y, test_X, test_y, \"Features\", xgc)\nevaluation.set_index(\"Metrics\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is clearly visible that the feature that have the most importance are the **cement_water_ratio** and **age** in predicting the **strength**. Though **age** has almost half the importance as **cement_water_ratio**.\n\n___________","metadata":{}},{"cell_type":"markdown","source":"#### Let's make use of GridSearchCV to find the right set of parameters for our model","metadata":{}},{"cell_type":"code","source":"model = XGBRegressor(n_jobs=4)\n\nparameters = {\n    'n_estimators': [50, 100, 500],\n    'max_depth': [2, 4, 6, 8, 10],\n    'gamma': [0.001, 0.01],\n    'learning_rate': [0.01, 0.1],\n    'booster': ['gbtree']\n}\n\ngrid_obj = GridSearchCV(\n    estimator=xgc,\n    param_grid=parameters\n)\n\ngrid_obj.fit(X, y)\n\nbest_model = grid_obj.best_estimator_\nprint(best_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgc = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0.001, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.1, max_delta_step=0, max_depth=5,\n             min_child_weight=1, monotone_constraints='()',\n             n_estimators=500, n_jobs=0, num_parallel_tree=1, random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n             tree_method='exact', validate_parameters=1, verbosity=None)\nevaluation, cv_scores =  evaluate_model(\"XGBoost Tuned\", train_X, train_y, test_X, test_y, \"Features\", xgc)\nevaluation.set_index(\"Metrics\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Please consider upvoting if you found this useful and informative. Thank you.","metadata":{}}]}