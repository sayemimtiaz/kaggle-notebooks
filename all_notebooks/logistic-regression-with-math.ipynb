{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n<font color='black'>\n# Content:\n*  [Introduction](#1)\n    \n*  [Data Set](#2)\n\n    * [Loading the Dataset](#2)\n    * [Exploratory Data Analysis](#3)\n    * [Missing Values Treatment](#4)\n    * [Visualization](#5)\n    \n    \n*  [Logistic Regression](#6)\n\n    * [Preprocessing: Label encoder and Normalization](#7)\n    * [Train Test Split](#8)\n    * [Parameter Initialize and Sigmoid Function](#9)\n    * [Forward and Backward Propogarion Combined](#10)\n    * [Gradient Decent for Logistic Regression](#11)\n    * [Prediction](#12)\n    * [Logistic Regression with Math](#13)\n    * [Logistic Regression with Sklearn](#14)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"---\n\n<a id = '1'></a>\n## Introduction\nDetermining a personâ€™s gender as male or female, based upon a sample of their voice seems to initially be an easy task. Often, the human ear can easily detect the difference between a male or female voice within the first few spoken words. However, designing a computer program to do this turns out to be a bit trickier.\n\nThe model is constructed using 3,168 recorded samples of male and female voices, speech, and utterances. The samples are processed using acoustic analysis and then applied to an artificial intelligence/machine learning algorithm to learn gender-specific traits.\n\n---\n\n#### A short description as on 'Data' tab on kaggle is :\n\nmeanfreq: mean frequency (in kHz)\n\nsd: standard deviation of frequency\n\nmedian: median frequency (in kHz)\n\nQ25: first quantile (in kHz)\n\nQ75: third quantile (in kHz)\n\nIQR: interquantile range (in kHz)\n\nskew: skewness (see note in specprop description)\n\nkurt: kurtosis (see note in specprop description)\n\nsp.ent: spectral entropy\n\nsfm: spectral flatness\n\nmode: mode frequency\n\ncentroid: frequency centroid (see specprop)\n\npeakf: peak frequency (frequency with highest energy)\n\nmeanfun: average of fundamental frequency measured across acoustic signal\n\nminfun: minimum fundamental frequency measured across acoustic signal\n\nmaxfun: maximum fundamental frequency measured across acoustic signal\n\nmeandom: average of dominant frequency measured across acoustic signal\n\nmindom: minimum of dominant frequency measured across acoustic signal\n\nmaxdom: maximum of dominant frequency measured across acoustic signal\n\ndfrange: range of dominant frequency measured across acoustic signal\n\nmodindx: modulation index. Calculated as the accumulated absolute difference between adjacent measurements of fundamental frequencies divided by the frequency range\n\nlabel: male or female\n\nNote that we have 3168 voice samples and for each of sample 20 different acoustic properties are recorded. Finally the 'label' column is the target variable which we have to predict which is the gender of the person****\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"---\n\n<h2><center>Importing Various Modules</center></h2>\n\n---","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n<a id = '2'></a>\n<h2><center>Loading the Dataset</center></h2>\n\n\n---","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_voice = pd.read_csv(\"/kaggle/input/voicegender/voice.csv\")\n\ndata = data_voice.copy()\n\ndata.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n<a id = '3'></a>\n<h2><center>Exploratory Data Analysis</center></h2>\n\n---","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n<a id = '4'></a>\n<h2><center>Missing Values Treatment</center></h2>\n\n---","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2><center>Missing data visualization</center></h2>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.matrix(data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n<a id = '5'></a>\n<h2><center>Visualization</center></h2>\n\n---","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(4,5,figsize=(15,15))\nfor i in range(1,21):\n    plt.subplot(4,5,i)\n    plt.title(data.columns[i-1])\n    sns.kdeplot(data.loc[data['label'] == \"female\", data.columns[i-1]], color= 'red', label='Female')\n    sns.kdeplot(data.loc[data['label'] == \"male\", data.columns[i-1]], color= 'blue', label='Male')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n* Most significant features are Q25, IQR and meanfun. We will build models by using the 20 features and the 3 distinct features\n* We will plot female and male classes according to their meanfun(X), IQR(Y), and Q25(Z).\n---","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# import data again to avoid confusion\n\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\ndata_voice = pd.read_csv(\"/kaggle/input/voicegender/voice.csv\")\ndata = data_voice.copy()\n\nmale = data[data.label == \"male\"]\n\nfemale = data[data.label == \"female\"]\n\n# trace1\ntrace1 = go.Scatter3d(\n    x=male.meanfun,\n    y=male.IQR,\n    z=male.Q25,\n    mode='markers',\n    name = \"MALE\",\n    marker=dict(\n        color='rgb(54, 170, 127)',\n        size=12,\n        line=dict(\n            color='rgb(204, 204, 204)',\n            width=0.1\n        )\n    )\n)\n\ntrace2 = go.Scatter3d(\n    x=female.meanfun,\n    y=female.IQR,\n    z=female.Q25,\n    mode='markers',\n    name = \"FEMALE\",\n    marker=dict(\n        color='rgb(217, 100, 100)',\n        size=12,\n        line=dict(\n            color='rgb(255, 255, 255)',\n            width=0.1\n        )\n    )\n)\n\ndata1 = [trace1, trace2]\nlayout = go.Layout(\n    title = ' 3D VOICE DATA ',\n    margin=dict(\n        l=0,\n        r=0,\n        b=0,\n        t=0\n    )\n)\nfig = go.Figure(data=data1, layout=layout)\n\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n<a id = '6'></a>\n<h2><center>Logistic Regression</center></h2>\n\nLogistic regression is a classification algorithm used to assign observations to a discrete set of classes. Unlike linear regression which outputs continuous number values, logistic regression transforms its output using the logistic sigmoid function to return a probability value which can then be mapped to two or more discrete classes.\n\n---","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"---\n\n<h2><center>Computation Graph of Logistic Regression</center></h2>\n\n![](https://machinethink.net/images/tensorflow-on-ios/LogisticRegression@2x.png)\n\n*      Parameters are weight and bias.\n*      Weights: coefficients of each accoustic properties\n*      Bias: intercept\n*      z = (w.t)x + b  => z equals to (transpose of weights times input x) + bias \n*      In an other saying => z = b + x1*w1 + x2*w2 + ... + x18*w18 + x19*w19\n\n\n---","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"---\n\n<a id = '7'></a>\n<h2><center>Preprocessing: label encoder and normalization</center></h2>\n\nNormalization is a systematic approach of decomposing tables to eliminate data redundancy(repetition) and undesirable characteristics like Insertion, Update and Deletion Anomalies. It is a multi-step process that puts data into tabular form, removing duplicated data from the \nrelation tables.\n\n---","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normalization\n\ndata.label = [1 if each == \"male\" else 0 for each in data_voice.label]\ny = data.label.values\nx_data = data.drop([\"label\"],axis=1)\nx = (x_data - np.min(x_data))/(np.max(x_data)-np.min(x_data)).values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n<a id = '8'></a>\n<h2><center>Train Test Split</center></h2>\n\n\n<center>80% of the data will be used for the training, rest of the data will be used for the test</center>\n\n---","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=42)\n\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n<a id = '9'></a>\n<h2><center>Parameter Initialize and Sigmoid Function</center></h2>\n\n\nSigmoid activation\nIn order to map predicted values to probabilities, we use the sigmoid function. The function maps any real value into another value between 0 and 1. In machine learning, we use sigmoid to map predictions to probabilities.\n#  $$S(z) = \\frac{1} {1 + e^{-z}}$$\n\n![](https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg)\n\n\n*Figure1 : https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg*\n\n---","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# w,b = Initialize Weights and Bias\n\ndef initialize_weights_and_bias(dimension):\n    \n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w,b\n\n\n# Sigmoid Function\n\ndef sigmoid(z):\n    \n    y_head = 1/(1+ np.exp(-z))\n    return y_head","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n<a id = '10'></a>\n<h2><center>Forward and Backward Propogarion Combined</center></h2>\n\n---\n\n## $$Loss Function   =>  L(\\hat{y},y) =  -[ylog\\hat{y}+(1-y)log(1-\\hat{y})]$$\n\n* Cost function is summation of all loss(error).\n\n## $$Cost Function = \\frac{1}{m} \\sum_{i=1}^{m} L(\\hat{y},y)$$","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head) # Loss Function\n    cost = (np.sum(loss))/x_train.shape[1]                      # Cost Function\n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost,gradients","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n<a id = '11'></a>\n<h2><center>Gradient Decent for Logistic Regression</center></h2>\n\n* Unlike linear regression, which has a closed-form solution, gradient decent is applied in logistic regression. The general idea of gradient descent is to tweak parameters w and b iteratively to minimize a cost function. There are three typical gradient decent, including Batch Gradient Decent, Mini-batch Gradient Decent and Stochastic Gradient Decent. In this blog, Batch Gradient Decent is used.\n\n---\n\n\n![](https://miro.medium.com/max/1200/1*iNPHcCxIvcm7RwkRaMTx1g.jpeg)\nFigure Source: https://saugatbhattarai.com.np/what-is-gradient-descent-in-machine-learning/\n\n---\n\n* An initial value is assigned to w; then iteratively update w by Learning Rate * Gradient of cost function. The algorithm will not stop until it converges.\n\n![](https://miro.medium.com/max/966/1*kUmtH0lRS-euZriSNaqgHQ.gif)\n\n---","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 1000 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n            \n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.grid(True)\n    plt.title(\"We Update(learn) Parameters Weights and Bias\")\n    plt.show()\n    return parameters, gradients, cost_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n<a id = '12'></a>\n<h2><center>Prediction</center></h2>\n\n---","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n<a id = '13'></a>\n<h2><center>Logistic Regression with Math</center></h2>\n\n---\n\n<h2><center>Hyperparameters</center></h2>\n<h4><center>* Learning Rate = 0.2 </center></h4>\n<h4><center>* Iteration = 15000</center></h4>\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  \n    w,b = initialize_weights_and_bias(dimension)\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n\n    # Print test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.2, num_iterations = 15000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n<a id = '14'></a>\n<h2><center>Logistic Regression with Sklearn</center></h2>\n\n---","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train.T,y_train.T)\nxx = print(\"train accuracy {} %\".format(lr.score(x_train.T,y_train.T)*100))\nyy = print(\"test accuracy {} %\".format(lr.score(x_test.T,y_test.T)*100))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n|   | <center><h2>With Math </h2></center>|<center><h2> With Sklearn </h2></center>|\n|------------|-------------|--------------|\n|<h3>Train Accuracy</h3>|<center><h3>96.882399368 %</h3></center>|<center><h3>96.8034727703 %</h3></center>|\n|<h3>Test Accuracy</h3>|<h2><font color='red'>98.107255520 %</font></h2>|<h2><font color='red'>98.264984227 %</font></h2>|\n\n\n---","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## References\n\nhttps://towardsdatascience.com/an-introduction-to-logistic-regression-8136ad65da2e<br>\nhttps://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners<br>\nhttps://machinethink.net/blog/tensorflow-on-ios/","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}