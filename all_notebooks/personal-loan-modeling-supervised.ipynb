{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as pi\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nprint('Listing files in the folder')\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nbank_cust_data = pd.read_csv('/kaggle/input/personal-loan-modeling/Bank_Personal_Loan_Modelling.csv')\n\nbank_cust_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(\"Shape of the data :{0}\".format(bank_cust_data.shape))\nprint(\"Size of the data :{0}\".format(bank_cust_data.size))\nprint(\"nDim of the data :{0}\".format(bank_cust_data.ndim))\n\nbank_cust_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Identify the types and the missing values of the data\n\nprint(\"\\nList of Null values in each column\")\nprint(bank_cust_data.isna().sum())\n\nprint(\"\\nData types of each column\")\nprint(bank_cust_data.dtypes)\n\n#All fields are numeric and no missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Unique Values\")\nprint(bank_cust_data['Personal Loan'].unique())\n\nprint(\"\\nSeacrching for 'scott' in all column\")\nprint(bank_cust_data.isin(['scott']).any())\n\nsns.boxplot('Personal Loan',data=bank_cust_data);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop un-wanted column\nbank_extract_data = bank_cust_data.drop(['ID'],axis=1).copy()\n\nprint('Shape of Data: ', bank_extract_data.shape)\n\nprint('\\nMedian for the data')\nprint(bank_extract_data.median())\n\nprint('\\nMode for the data')\nprint(bank_extract_data.mode())\n\nprint('\\n Five Point Summary for the data')\nbank_extract_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Unique values for the categorical\n\nprint('Number of Unique values of ZipCodes: ',bank_extract_data['ZIP Code'].nunique())\nprint('Unique values of Family: ',bank_extract_data['Family'].unique())\nprint('Unique values of Education: ',bank_extract_data['Education'].unique())\nprint('Unique values of Personal Loan: ',bank_extract_data['Personal Loan'].unique())\nprint('Unique values of Securities Account: ',bank_extract_data['Securities Account'].unique())\nprint('Unique values of CD Account ',bank_extract_data['CD Account'].unique())\nprint('Unique values of Online ',bank_extract_data['Online'].unique())\nprint('Unique values of CreditCard ',bank_extract_data['CreditCard'].unique())\n\n\nprint('\\n*** Observation: Education attribute needs \"One hot Encoding\"')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution for the non-categorical attributes\n\nfig, axis = plt.subplots(2, 3, figsize=(25, 15), sharex=False)\n\nsns.distplot(bank_extract_data['Age'],bins=10,ax=axis[0,0]);\n\nsns.distplot(bank_extract_data['Experience'],ax=axis[0,1]);\n\nsns.distplot(bank_extract_data['Income'],ax=axis[0,2]);\n\nsns.distplot(bank_extract_data['CCAvg'],ax=axis[1,0],color='orange');\n\nsns.distplot(bank_extract_data['Mortgage'],ax=axis[1,1],color='orange');\n\nsns.distplot(bank_extract_data['Family'],ax=axis[1,2],color='orange');\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary\n\nThe parameter Age looks Normalise not much skew,\nThe parameter Experience is also Normalised not much skew\n\nThe parameter Income right skewed,\nThe parameter CCAvg right skewed,\nThe parameter Mortgage "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Age Parameter Right Skewed: \", bank_extract_data['Age'].mean() > bank_extract_data['Age'].median() )\nprint(\"CCAvg Parameter Right Skewed: \", bank_extract_data['CCAvg'].mean() > bank_extract_data['CCAvg'].median() )\nprint(\"Mortgage Parameter Right Skewed: \", bank_extract_data['CCAvg'].mean() > bank_extract_data['CCAvg'].median() )\n\nprint(\"\\n*** The skwed data does not impact the algorithm we are going to use\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axis1 = plt.subplots(1, 2, figsize=(20, 6), sharex=False)\n\nsns.boxplot(bank_extract_data['ZIP Code'],ax=axis1[0],orient='h');\nsns.boxplot(bank_extract_data['Mortgage'],ax=axis1[1],color='red',orient='v');\n\nplt.show()\n\n#The outlier shown below does not impact the algorithm hence we are leaving out the outliers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Target variable - Analysis. \n\n#### Target variable is identified as \"Personal Load\" doing some analysis with on other variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axis = plt.subplots(1, 2, figsize=(20, 10), sharex=False)\n\nsns.violinplot(bank_extract_data['Personal Loan'],bank_extract_data['Income'],ax=axis[0]);\nsns.violinplot(bank_extract_data['Personal Loan'],bank_extract_data['Age'],ax=axis[1]);\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.clf()\nfig, axis = plt.subplots(1, 2, figsize=(20, 10), sharex=False)\n\nsns.violinplot(bank_extract_data['Personal Loan'],bank_extract_data['Experience'],ax=axis[0]);\nsns.violinplot(bank_extract_data['Personal Loan'],bank_extract_data['Mortgage'],ax=axis[1]);\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Summary \n\nInteresting obeservations on the Personl Loan with Income, Age, Experience & Mortgage\n\nThe PL is positive for the higher income range\nThe PL is equally positive and negative with the Age, I would say Age does not matter on the PL\nThe PL has the effect for the Experience as well no difference\nThe PL has limited effect on Mortgage, non-mortgage people are mostly not interested on PL\n(The not accepted loan seems higher for people without mortgage )\n\n\n#### The target column(Personal Loan) looked to be un-balance the data, more observarion see below later\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#bank_pairplot_data=bank_extract_data.drop(['Personal Loan',])\n\n#bank_pairplot_data.head();\n\nplot_vars=['Age','Experience','Income','ZIP Code','CCAvg','Mortgage','Online']\n\nsns.pairplot(bank_extract_data,kind='scatter',x_vars=plot_vars,y_vars=plot_vars,hue='Personal Loan');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary of pairplot\n\nSome of the observations from the pairplots, as expexted the age vs experience is very linear. More the age and the experience is. \n\nZipcode is a strange field does not have any corelation to other field, I expected it would make difference in the loan or the online, but it did nit\n\nThe CCAvg and the Income has a good corelation. More the income more the CC Avg, there are cases of less CCAvg with more Income group\n\nThe Mortage also looks similar and more linear, more the income and more the mortgage value"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='Income', y='Mortgage', kind='swarm', data=bank_extract_data,aspect=2);\n\n# Deeper look into income/Mortgage out of curiosity to see the linear reference","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axis = plt.subplots(2, 1, figsize=(25, 30), sharex=False)\nthreshold=0.25\n\nsns.heatmap(bank_extract_data.corr(),annot=True,fmt='f',ax=axis[0])\n\nfiltered_data =pd.DataFrame(bank_extract_data.corr()>threshold)\n\nprint('The correlated data based on threshold: ',threshold)\nsns.heatmap(filtered_data,annot=True,fmt='d',ax=axis[1]);\nplt.show()\n\n\n# As per corelation factor only 3 feilds are applicable, but considering all attributes as for now.\n# so as the attributes might have other positive impact","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Spliting the training and test data\n\n### Creating dummy columns for the education field \n### Reducing the income to monthly\n### Reducing the mortgage value by 10 for consistency"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying encoding on the Education parameter to generate the even 0/1 parameter\nbank_modified = pd.get_dummies(bank_extract_data['Education'],prefix='Edu_') \nbank_modified=pd.concat([bank_extract_data.copy(), bank_modified], axis=1)\n\nparameter_drop=['Education'] # dropping the duplicate param\n\nbank_modified.drop(parameter_drop,axis=1,inplace=True)\n\nbank_modified['Income']=bank_extract_data['Income']/12 # Converting to the monthly income\nbank_modified['Mortgage']=bank_extract_data['Mortgage']/10 # reducing the mortgage to normalise data\n\nbank_modified.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Spliting the data for the model training\n\naxis_x = bank_modified.drop(['Personal Loan'],axis=1)\naxis_y = bank_modified['Personal Loan']\n\n#spliting the data into 70/30\nx_train,x_test,y_train,y_test = train_test_split(axis_x,axis_y,test_size=0.3,random_state=100)\n\nx_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Identidy the true and false for Personal Loan\nloan_true = len(bank_modified.loc[bank_modified['Personal Loan'] == 1])\nloan_false = len(bank_modified.loc[bank_modified['Personal Loan'] == 0])\n\nprint (f\"{len(x_train)/len(bank_modified)*100} % data in the Training\")\nprint (f\"{len(x_test)/len(bank_modified)*100} % data in the Testing\")\n\nprint(\"\\nPercent of the Loan offer accepted\")\nprint (f\"Accepted: {loan_true} in total {len(bank_modified)} {loan_true/len(bank_modified)*100}%\")\nprint (f\"Not Accepted: {loan_false} in total {len(bank_modified)} {loan_false/len(bank_modified)*100}%\")\n\nprint(\"\\nPercent of the Loan offer accepted in Training data\")\nprint (f\"Accepted: {len(y_train.loc[y_train[:]==1])} in total {len(y_train)} {len(y_train.loc[y_train[:]==1])/len(y_train)*100}%\")\nprint (f\"Not Accepted: {len(y_train.loc[y_train[:]==0])} in total {len(y_train)} {len(y_train.loc[y_train[:]==0])/len(y_train)*100}%\")\n\nprint(\"\\nPercent of the Loan offer accepted in Test data\")\nprint (f\"Accepted: {len(y_test.loc[y_test[:]==1])} in total {len(y_test)} {len(y_test.loc[y_test[:]==1])/len(y_test)*100}%\")\nprint (f\"Not Accepted: {len(y_test.loc[y_test[:]==0])} in total {len(y_test)} {len(y_test.loc[y_test[:]==0])/len(y_test)*100}%\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary on Target Variable\n\nThe PL has low percentage of Accepted/Not Accepted (9.6%/90.4), which seems to be a very unbalance data. The model needs to consider the senstivity, recall, precission score along with the accuracy so as to determine the best model. Also the posibility of the prediction might tend towards the 'Not Accepted' more."},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression - Classification Model "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n#model train is set\nmodel = LogisticRegression(solver='liblinear')\nmodel.fit(x_train,y_train)\n\nthres_predict_train=model.predict_proba(x_train)[:,1] > 0.8 # Threshold changes the confusion matrix\nthres_predict_test=model.predict_proba(x_test)[:,1] > 0.8 \n\nprint(f\"Training Accuracy Score: \",metrics.accuracy_score(y_train,thres_predict_train))\nprint(f\"Test Accuracy Score: \",metrics.accuracy_score(y_test,thres_predict_test))\n\nprint(\"\\nClassification Report\")\nreport_logistic=classification_report(y_test,thres_predict_test,labels=[1,0])\nprint(report_logistic)\n\nconfusin_matix_logistic = metrics.confusion_matrix(y_test,thres_predict_test,labels=[1,0])\nconfusin_matix_logistic = pd.DataFrame(confusin_matix_logistic,index=['Accepted','NotAccepted'],\n                              columns=['Pred_Accepted', 'Pred_NotAccepted'])\n\nplt.figure(figsize=(10,5))\nsns.heatmap(confusin_matix_logistic,annot=True,fmt='g');\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Naive Bayes - Classification Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB \n\ndata_model = GaussianNB()\n\ndata_model.fit(x_train,y_train.ravel()) #ravel convert to an one dimensional array\n\nnaive_train_predict = data_model.predict(x_train)\nnaive_test_predict = data_model.predict(x_test)\n\nprint(f\"Training Accuracy Score: \",metrics.accuracy_score(y_train,naive_train_predict))\nprint(f\"Test Accuracy Score: \",metrics.accuracy_score(y_test,naive_test_predict))\n\nprint(\"\\nClassification Report\")\nreport_naiveBayes=classification_report(y_test,naive_test_predict,labels=[1,0])\nprint(report_naiveBayes)\n\nconfusin_matix_Naive = metrics.confusion_matrix(y_test,naive_test_predict,labels=[1,0])\nconfusin_matix_Naive = pd.DataFrame(confusin_matix_Naive,index=['Accepted','NotAccepted'],\n                              columns=['Pred_Accepted', 'Pred_NotAccepted'])\n\nplt.figure(figsize=(10,5))\nsns.heatmap(confusin_matix_Naive,annot=True,fmt='g');\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\n\nKNN_model = KNeighborsClassifier(n_neighbors=5,weights='distance',metric='euclidean') # \n\nKNN_model.fit(x_train,y_train)\n\nKNN_predict_train = KNN_model.predict(x_train)\nKNN_predict_test = KNN_model.predict(x_test)\n\nprint(f\"Training Accuracy Score: \",metrics.accuracy_score(y_train,KNN_predict_train))\nprint(f\"Test Accuracy Score: \",metrics.accuracy_score(y_test,KNN_predict_test))\n\nprint(\"\\nClassification Report\")\nreport_KNN = classification_report(y_test,KNN_predict_test,labels=[1,0])\nprint(report_KNN)\n\nconfusin_matix_KNN = metrics.confusion_matrix(y_test,KNN_predict_test,labels=[1,0])\nconfusin_matix_KNN = pd.DataFrame(confusin_matix_KNN,index=['Accepted','NotAccepted'],\n                              columns=['Pred_Accepted', 'Pred_NotAccepted'])\n\nplt.figure(figsize=(10,5))\nsns.heatmap(confusin_matix_KNN,annot=True,fmt='g');\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Support Vector Machine"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\n\nsvm_model = svm.SVC(gamma=100,C=3)\nsvm_model.fit(x_train,y_train)\n\n\nsvm_predic_train = svm_model.predict(x_train)\nsvm_predic_test = svm_model.predict(x_test)\n\nprint(f\"Training Accuracy Score: \",metrics.accuracy_score(y_train,svm_predic_train))\nprint(f\"Test Accuracy Score: \",metrics.accuracy_score(y_test,svm_predic_test))\n\nprint(\"\\nClassification Report\")\nreport_svm=classification_report(y_test,svm_predic_test,labels=[1,0])\nprint(report_svm)\n\nconfusin_matix_svm = metrics.confusion_matrix(y_test,svm_predic_test,labels=[1,0])\nconfusin_matix_svm = pd.DataFrame(confusin_matix_svm,index=['Accepted','NotAccepted'],\n                              columns=['Pred_Accepted', 'Pred_NotAccepted'])\n\nplt.figure(figsize=(10,5))\nsns.heatmap(confusin_matix_svm,annot=True,fmt='g');\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data is imbalanced hence model accuracy alone does not alone, the below parater to be verifed for selecting a model\n\nprint(\"\\nLogisitic Regression\\n\",report_logistic)\nprint(\"\\nNaive Bayes\\n\",report_naiveBayes)\nprint(\"\\nKNN Classifier\\n\",report_KNN)\nprint(\"\\nSVM\\n\",report_svm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary\n\nBased on the percision, recall score of the various models shown above. The KNN & Naive Bayes perform better than other models\n\nThe final model preference is Naive Bayes as the False Postivie in this model is lower, that would be priority for this requirement. Native Bayes also has the closer accuracy range for the test and training data"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}