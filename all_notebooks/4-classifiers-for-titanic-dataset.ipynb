{"nbformat_minor":1,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"file_extension":".py","nbconvert_exporter":"python","name":"python","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","version":"3.6.1"}},"nbformat":4,"cells":[{"metadata":{"_uuid":"ef2b3e761ed82fd4ad37af3423a7209cd149e235","_cell_guid":"a3b0d6e2-06a2-4509-99ca-b0795cdc1afb"},"source":"The goal of this notebook is to try a 4 simple classifiers (svm, mlp, decision tree and random forest) for the [Kaggle's Titanic: Machine Learning from Disaster dataset](https://www.kaggle.com/c/titanic). I use Python for this project. First, let's start by reading the dataset to see what we have:","cell_type":"markdown"},{"metadata":{"collapsed":true,"_uuid":"99489fc99c8b60b2726f5da8e13f9334c1ec0cb5","_cell_guid":"a1da24ac-280f-4b4b-b5b1-c69f10948d08"},"outputs":[],"source":"import pandas as pd\ndirectory = '../input/'\ntitanic_train = pd.read_csv(directory + 'train.csv')\ntitanic_test = pd.read_csv(directory + 'test.csv')","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"b64f896438e111adcd656c30ab90e778c718ad82","_cell_guid":"df8eb60d-66b3-4401-8603-7197105317ee"},"outputs":[],"source":"titanic_train.info()","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"4fcdd4403c68ab58446fb74f55cc7472b371656c","_cell_guid":"56aaf39b-1555-4375-8caa-32d00d8e3596"},"source":"**Missing Data**\n\nThere are some missing data in Age, Cabin, and Fare as you can see below. ","cell_type":"markdown"},{"metadata":{"_uuid":"c424c5561dde68181bde8bf4702b0e4ebae342af","_cell_guid":"4417de17-8da7-4c8d-8fed-d8d29e7e7ed7"},"outputs":[],"source":"titanic_train.isnull().sum()","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"7b9cbc58ad14178335a4e560782692345ae7f351","_cell_guid":"f0da346f-e8a9-4c05-b0e6-f7f30e09828d"},"outputs":[],"source":"titanic_test.isnull().sum()","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"6ee7705a7fb4b62e252c24d01ceedc198588ad89","_cell_guid":"677b0043-a791-481b-956a-cc3f033a8227"},"source":"Let's combine the training and test data and fill the missing data. For now, I'll use simple methods. We can improve them later. ","cell_type":"markdown"},{"metadata":{"collapsed":true,"_uuid":"c09332e0dadf322a4cbd5ce706c8c5d5c32f5190","_cell_guid":"0b484865-f4ae-46c7-8e9d-1bba58f3b0d0"},"outputs":[],"source":"titanic_train_test = [titanic_train, titanic_test]","execution_count":null,"cell_type":"code"},{"metadata":{"collapsed":true,"_uuid":"3582b0683bc35cdcbe499b1f5fff72a9aa0d25c5","_cell_guid":"034d314a-5385-470c-84a3-933a4c00c193"},"outputs":[],"source":"for dataset in titanic_train_test:\n    dataset['Age'].fillna(dataset.Age.median(), inplace=True)\n    dataset['Cabin'].fillna('U', inplace=True)\n    dataset['Embarked'].fillna('S', inplace=True)\n    dataset['Fare'].fillna(dataset.Fare.mean(), inplace=True)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"d97be236782fa1b937e41c3afe5577f4616de80b","_cell_guid":"e651ada1-a8e3-44fb-8d56-f263712d0067"},"outputs":[],"source":"titanic_train.info()\ntitanic_test.info()","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"39bf41de9d212cc97de3bb3535d0c2fbe3684f61","_cell_guid":"b4bb4554-692e-4c40-90d9-e9ee07b3ba6f"},"source":"**Adding new features**\n\nI'm going to define a few more columns for now. I define FamilySize (based on SibSp:# of siblings / spouses aboard the Titanic and Parch (of parents / children aboard the Titanic). ","cell_type":"markdown"},{"metadata":{"collapsed":true,"_uuid":"dc537338754e9e8b075cecaebff8c8f39aaafaa4","_cell_guid":"2d2fded3-20bf-4c42-8389-969cba170888"},"outputs":[],"source":"for dataset in titanic_train_test:\n    dataset['FamilySize'] = dataset.SibSp + dataset.Parch","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"5d1dd2de8aca6ef2dd6ae4481ab447e1e7ba37d2","_cell_guid":"ec79c70f-b3ad-4d19-8d58-783efd20efe0"},"outputs":[],"source":"import matplotlib.pyplot as plt\n%matplotlib inline  \ntitanic_train.groupby('Age').count().PassengerId.plot()","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"7d341f5a890257371adf3df359e58611bb4bdae3","_cell_guid":"83746c82-402f-40e9-9942-e104d9aca892"},"source":"It's better to group the ages so we have a better understanding of the groups. ","cell_type":"markdown"},{"metadata":{"collapsed":true,"_uuid":"73f09e2908733229f901847f411a847b2760848e","_cell_guid":"d3ba2007-6b2e-46fb-86f5-dc06cdba8604"},"outputs":[],"source":"for dataset in titanic_train_test:\n    dataset['AgeRange'], AgeBins = pd.cut(dataset['Age'], 10, retbins=True)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"3118266b5ed5746cab92796e8e543e4c864771cd","_cell_guid":"757adf01-30ca-43ec-8502-70c5e08cb1cb"},"outputs":[],"source":"titanic_train.groupby('AgeRange').count().PassengerId.plot.barh()","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"a3f2115698c7f132c2821b476f98ec5fa5bd32c1","_cell_guid":"b3efdde6-a9bd-4568-8564-7e6b6a6d3846"},"outputs":[],"source":"titanic_train.Name.head(10)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"5749cfe37e89f53183a81834815bf25e82b9a6a1","_cell_guid":"c2732e34-bc71-4f71-9076-e7afd7db339e"},"source":"We can extract titles and then use it instead of Sex. Actually, there is a clear correlation between Sex and Title (Male cannot use Mrs for title). So, it doesn't make sense to keep both. As the same time, I think Title shows more information than a binary variable Sex.","cell_type":"markdown"},{"metadata":{"collapsed":true,"_uuid":"ddc127bf1578b22b2b6e0187dc1f933b1207e768","_cell_guid":"e5b63cc5-9d3a-407e-98f0-70f0c02bba29"},"outputs":[],"source":"for dataset in titanic_train_test:\n    dataset['Title'] = dataset['Name'].map(lambda name: name.split(',')[1].split('.')[0].strip())","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"bca93a0e2a84f75595cfc63a3767fc873f2c8f85","_cell_guid":"c003814f-a144-4c3d-a860-1f2f1e28e734"},"outputs":[],"source":"titanic_train.Title.head(10)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"dc0a9418abef833ca96cac26706da43b1c4cca90","_cell_guid":"5d80c657-ea31-448a-b404-0749a1555e7c"},"outputs":[],"source":"titanic_train.groupby('Title').count().PassengerId","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"fa6dabb94ec84ba5df8459153abb033522c52e73","_cell_guid":"f1caf27d-da25-4bd2-b167-67f9a349139f"},"outputs":[],"source":"titanic_test.groupby('Title').count().PassengerId","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"f8eb46cd269de76e3af09b2cba6e8e8784c2f845","_cell_guid":"1fbafa16-adf0-4b14-a46a-05adc321a1d9"},"source":"It make sense to just keep a few titles and get rid of the rare cases. ","cell_type":"markdown"},{"metadata":{"collapsed":true,"_uuid":"b45e4fd66a94d58fa5fc25d967f6999970ea6175","_cell_guid":"7c2075a9-d5e2-40b1-ae02-d2d75fe4a9c5"},"outputs":[],"source":"for dataset in titanic_train_test:\n   dataset['Title'] = dataset['Title'].map(\n    lambda x: 'unusual' if x in [\"Capt\", \"Col\", \"Major\", \"Jonkheer\", \"Don\", \"Sir\", \"Dr\", \"Rev\", \"the Countess\", \"Dona\",\n                              \"Lady\"] else ('Mrs' if x in [\"Mme\", \"Ms\"] else ('Miss' if x in [\"Mlle\"] else x)))","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"6e70a43706ca85b158dce4f15e91b2c4a25bc05f","_cell_guid":"1b8f276e-b9f5-4125-ab5e-71722a0ed7d7"},"outputs":[],"source":"plt.subplot(1,2,1)\ntitanic_train.groupby('Title').count().PassengerId.plot.bar()\nplt.subplot(1,2,2)\ntitanic_test.groupby('Title').count().PassengerId.plot.bar()","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"aec443491e2ce51032bb2666d83728f35b53d14a","_cell_guid":"d5d261fd-dfe5-46ff-a6e2-b4615b1db267"},"source":"Let's see what we can get from last name. Basically, if we can use the same last name to distinguish people from the same family, it can be useful. The problem is that some last names are common and we should not use them (see max value is 9 in the training data).","cell_type":"markdown"},{"metadata":{"collapsed":true,"_uuid":"6d4fedbe0a36a7eeb53bf394824177fd8fb6dda7","_cell_guid":"6a46ad66-37b4-4891-83bb-5a5b89efdaed"},"outputs":[],"source":"for dataset in titanic_train_test:\n    dataset['LastName'] = dataset['Name'].map(lambda name: name.split(',')[0].strip())","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"6f5aa7e6d336464ad7b84e999228cb06e8ac0014","_cell_guid":"b7aac1b9-7eb8-4671-a2f1-e55464bd24ab"},"outputs":[],"source":"titanic_train.groupby('LastName').count().PassengerId.describe()\n","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"9fc421234251f81c6219f2eb845df6714ce4515a","_cell_guid":"01e4cd33-2c69-41d2-890c-6b0ccc58dc91"},"outputs":[],"source":"titanic_test.groupby('LastName').count().PassengerId.describe()","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"12cc20b92062870f8516296c1e2acefb8fbb48c3","_cell_guid":"a7e44218-de14-4421-9ce3-542f70dfe536"},"source":"Let's investigate what we have from tickets, cabins, and fares. Remeber, 'U' in Cabin means unknown. The 687 value in the training data is for that one. For ticket, things are better. The max number of items with the same ticket is 7. Maybe we can use it to find groups. Obviously, same fare does not mean anything is most cases. But, it we have very few persons with the same fare, maybe they got their tickets together.","cell_type":"markdown"},{"metadata":{"_uuid":"8eddce8b1e3bf8ceb1545af5682e7213faffef3c","_cell_guid":"9f0d1e99-2007-41c9-8e19-658ec71aee6c"},"outputs":[],"source":"titanic_train.groupby('Cabin').count().PassengerId.describe()","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"848d896fb55fa3bc5d3df566216bde0ed24342db","_cell_guid":"194a4913-307b-4199-a749-33ff040aafa4"},"outputs":[],"source":"titanic_train.groupby('Ticket').count().PassengerId.describe()","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"65ce73535a53638cd4f0eb8494d7daa13a977e7f","_cell_guid":"5bb34315-5b0c-4ef9-b31c-05090514a096"},"outputs":[],"source":"titanic_train.groupby('Fare').count().PassengerId.describe()","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"8fc4535ea37f6e7682e2a2da532652944adb2c72","_cell_guid":"3cc90813-5e97-4df5-b055-ffa25af0bc75"},"source":"I think we can find the frequencies of things now to see what are the distributions. ","cell_type":"markdown"},{"metadata":{"collapsed":true,"_uuid":"e2e57143e64088644ce5c9a26eb3d01cf0986547","_cell_guid":"b6fccb67-a846-4e4a-b362-bb3c89044348"},"outputs":[],"source":"for dataset in titanic_train_test:\n    for col in ['Ticket', 'Cabin', 'Fare', 'LastName']:\n        freq_col = f'Freq{col}'\n\n        freq = dataset[col].value_counts().to_frame()\n        freq.columns = [freq_col]\n\n        dataset[freq_col] = dataset.merge(freq, how='left', left_on=col, right_index=True)[freq_col]","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"d99ab4d39367024a90d36513244095110d126d4e","_cell_guid":"1c0412e6-b368-4368-92e0-958a0a26befd"},"outputs":[],"source":"titanic_train.info()","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"23867cd32124f00396b961c9ea75e0feb3ac0d6e","_cell_guid":"38e70b08-a397-4142-a500-4e9360e215ef"},"outputs":[],"source":"titanic_train.groupby('FreqTicket').count().PassengerId.plot.bar()","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"60c64f09641e6d287654e4f2eda9c503833afa7e","_cell_guid":"f3385761-9952-4137-b094-5e1d7a8e71cf"},"outputs":[],"source":"titanic_train.groupby('FreqCabin').count().PassengerId.plot.bar()","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"ac6624288102b3447982c4c2ec19e9de2a6fff50","_cell_guid":"994b5e5b-e367-4cab-82f2-3a7ddb19e542"},"outputs":[],"source":"titanic_train.groupby('FreqLastName').count().PassengerId.plot.bar()","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"dea6da7a4e662639bb265958fb66a2efea60553f","_cell_guid":"3d9db45c-dae3-4044-8b33-5e60ce6824ab"},"outputs":[],"source":"titanic_train.groupby('FreqFare').count().PassengerId.plot.barh()","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"4db8efd6ca3f0c929ac0f76428c44a71e123c0fe","_cell_guid":"eb295fb4-67e4-4805-bcac-948d0a3e2b85"},"source":"Now, we can group things together. I'm going to use FamilySize first, then FreqTicket, then FreqCabin and then FreqLastName. If there is nothing, then the passanger was alone.","cell_type":"markdown"},{"metadata":{"collapsed":true,"_uuid":"d45f22f83eb1e2a15d12cf5f96f1f16a6bc06c97","_cell_guid":"2f1c9f89-d08b-4a0e-af43-5f63b4388948"},"outputs":[],"source":"def groupify(x):\n    max_group = 5\n    if x['FamilySize'] > 0:\n        return x['FamilySize']\n    elif x['FreqTicket'] > 1:\n        return x['FreqTicket']\n    elif x['FreqCabin'] > 1 and x['Cabin'] != 'U':\n        return x['FreqCabin']\n    elif 1 < x['FreqLastName'] < max_group:\n        return x['FreqLastName']\n    elif 1 < x['FreqFare'] < max_group:\n        return x['FreqFare']\n    else:\n        return 0","execution_count":null,"cell_type":"code"},{"metadata":{"collapsed":true,"_uuid":"c63ff383c687dbff51e4f02b697ef797d8254a04","_cell_guid":"6e08e8c2-cc17-4774-8586-1f27b12f7e7e"},"outputs":[],"source":"for dataset in titanic_train_test:\n    dataset['GroupSize'] = dataset.apply(groupify, axis=1)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"a0649026e6a125d03ee8681bce7f31bf2ce01ae5","_cell_guid":"0f192a82-2c7d-412c-b687-9a14936a57c9"},"outputs":[],"source":"titanic_train.groupby('GroupSize').count().PassengerId.plot.bar()","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"d98b0179bbb5939e1151f61c5578b0d2ecb578db","_cell_guid":"ca197c60-fa4c-41aa-9cac-185df7179b05"},"source":"**Final set of features**\n\nLet's see what we got finally:","cell_type":"markdown"},{"metadata":{"_uuid":"851511afa234be6ed832174a4dc46997c59362b4","_cell_guid":"9d439ac2-116f-40b6-9fb7-a8e3e351dc01"},"outputs":[],"source":"print(titanic_train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean())\nprint()\nprint(titanic_train[['GroupSize', 'Survived']].groupby(['GroupSize'], as_index=False).mean())\nprint()\nprint(titanic_train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean())\nprint()\nprint(titanic_train[['AgeRange', 'Survived']].groupby(['AgeRange'], as_index=False).mean())\nprint()\nprint(titanic_train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean())\nprint()\n","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"1e277165fd0c910f4f2fbd58b8003186ca0fa16c","_cell_guid":"ee4b5909-9302-4600-883b-106ac9c835ea"},"source":"Let's remove the ununsed columns and clean the final dataset.","cell_type":"markdown"},{"metadata":{"collapsed":true,"_uuid":"2573e2870f88d0867614c6de13e3dcc1a28aaf32","_cell_guid":"1349d92d-51fa-4753-a079-1c70122db6b4"},"outputs":[],"source":"y = titanic_train['Survived']\ntitanic_train.drop(['Survived'], axis=1, inplace=True)","execution_count":null,"cell_type":"code"},{"metadata":{"collapsed":true,"_uuid":"44752a2a2a5c97d9a7e94ef1cccc1f20b1ae23cd","_cell_guid":"98125d4a-e377-4406-9a46-b03adaa7fd3a"},"outputs":[],"source":"for dataset in titanic_train_test:\n    dataset['Title'] = dataset['Title'].map({\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"unusual\": 5}).astype(int)\n\n    dataset['Embarked'] = dataset['Embarked'].map({'S': 0, 'C': 1, 'Q': 2}).astype(int)\n\n    for AgeGroup in range(0, len(AgeBins)):\n        if AgeGroup == len(AgeBins) - 1:\n            dataset.loc[dataset['Age'] > AgeBins[AgeGroup], 'Age'] = AgeGroup\n        else:\n            dataset.loc[\n                (dataset['Age'] > AgeBins[AgeGroup]) & (dataset['Age'] <= AgeBins[AgeGroup + 1]), 'Age'] = AgeGroup\n\n    dataset[\"Pclass\"] = dataset[\"Pclass\"].astype('int')\n\n    # Sex & Title have correclation. We keep Title.\n    for col in dataset.columns:\n        if col not in ['Pclass', 'Age', 'Embarked', 'Title', 'GroupSize']:\n            dataset.drop([col], inplace=True, axis=1)\n    for col in dataset.columns:\n        dataset[col] = dataset[col].astype(\"category\")","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"642a8dcd9161ef86ddd46cf017f5a0e1a3909660","_cell_guid":"6daaeaf4-8b70-4ddd-be03-a4653bba1115"},"outputs":[],"source":"titanic_train.columns","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"490245673324bcfaacc2fa1dd6ee7269ca859033","_cell_guid":"8be34e28-bdea-46c9-a363-9a9ce669383b"},"source":"**Converting to 0/1 Features**\n\nAs you can see, I almost remove all columns and kept the very few. I beleive that other columns are very related to the above ones and as such there will be correlation between them.\n\nI'm going to make binary variables from these columns.","cell_type":"markdown"},{"metadata":{"collapsed":true,"_uuid":"228aa29ac3384e53a55d0fbbc256c570adb8c941","_cell_guid":"5f529aa9-10fb-4729-89c7-613d421bcfb9"},"outputs":[],"source":"titanic_train = pd.get_dummies(titanic_train, columns=None)\ntitanic_test = pd.get_dummies(titanic_test, columns=None)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"612efe8a0206b2a043d5b099ee234374a8ba2829","_cell_guid":"f51abeb2-996d-4122-8ccb-8b5b00036392"},"outputs":[],"source":"titanic_train.info()","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"d0dfb9d800a7f01e70077322a9987dd9a8a059a8","_cell_guid":"efdb681e-24af-4d13-b3c1-4c0c6561d0b9"},"source":"To make sure we are using the same feature sets for both train and test, I need to clean the dataset a little more.","cell_type":"markdown"},{"metadata":{"collapsed":true,"_uuid":"3afa8f0d91465c62a1e4875f322bd83bc65cbd10","_cell_guid":"ab4d685f-410f-40ac-bde7-82cf7722c4e6"},"outputs":[],"source":"missing_cols = set(titanic_train.columns) - set(titanic_test.columns)\nfor c in missing_cols:\n    titanic_test[c] = 0\nmissing_cols = set(titanic_test.columns) - set(titanic_train.columns)\nfor c in missing_cols:\n    titanic_test[c] = 0","execution_count":null,"cell_type":"code"},{"metadata":{"collapsed":true,"_uuid":"acb55720c2cd91ba5710b3e1ccd50a03d41c92cf","_cell_guid":"8321cbf8-72b0-4ebb-bade-fcfdcbb6a04d"},"outputs":[],"source":"X_train, y_train = titanic_train, y\nX_test = titanic_test","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"ff9b7a36162b3788bece55a45c367a08de6aee43","_cell_guid":"339f993e-282a-41fa-8893-a64aef640324"},"source":"**Cross Validation with different classifiers**\n\nI'm going to try a few classifiers with different paramters (cross validation)","cell_type":"markdown"},{"metadata":{"collapsed":true,"_uuid":"4b3b01cc9c06c3391f98a1c6814f258d1b525391","_cell_guid":"18a0e957-07dc-419c-87b6-c7cb46dd1312"},"outputs":[],"source":"from sklearn.model_selection import StratifiedShuffleSplit\n\n# Set the parameters by cross-validation\ncv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"064f64f5508a27fed2dfb45e436e5e11834c39da","_cell_guid":"668b0a9e-0730-49e9-9117-853837d301fc"},"outputs":[],"source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn import svm\nimport numpy as np\n\n# run svm\nparam_grid = {\"gamma\": np.logspace(-3, 3, 7),\n              \"C\": np.logspace(-3, 3, 7)              }\nsvm_model = GridSearchCV(svm.SVC(), param_grid=param_grid, cv=cv)\nsvm_model.fit(X_train, y_train)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"3c9d87be7ff3e4fc989ed5d45dbd58b6323856e7","_cell_guid":"2b4a9127-4829-4725-b8c5-6e25e0fb2407"},"outputs":[],"source":"print(\"[SVM] The best parameters are %s with a score of %0.2f\"\n      % (svm_model.best_params_, svm_model.best_score_))","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"145f9225f001afb6200fcebf893d00bf651f27fa","_cell_guid":"ace76db6-92a5-4c1c-b4ce-41dee6af5d0e"},"source":"Now, let's try Multi-layer Perceptron and Decision Trees.","cell_type":"markdown"},{"metadata":{"_uuid":"8239092afc61b44b4de36a1ec9c73243961b580f","_cell_guid":"1195fbcb-fc4b-4a37-a91d-3e6bc74b6eb5"},"outputs":[],"source":"from sklearn.neural_network import MLPClassifier\n\n# MLP\nparam_grid = {\"hidden_layer_sizes\": [(50,), (50, 50)],\n              \"alpha\": np.logspace(-3, 3, 7)\n              }\nmlp = GridSearchCV(MLPClassifier('lbfgs', max_iter=600), param_grid=param_grid, cv=cv, n_jobs=-1, verbose=2)\nmlp.fit(X_train, y_train)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"05f2ec8ac206bac132eac35969393d03325a8ea0","_cell_guid":"bc3116a8-a37a-4119-b184-8d35a8fe1671"},"outputs":[],"source":"print(\"[MLP] The best parameters are %s with a score of %0.2f\"\n      % (mlp.best_params_, mlp.best_score_))","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"94068bd181b9543a99538fdd62bc118f1c6fb1cb","_cell_guid":"d9351363-489d-4355-84c5-eda65e475d8c"},"outputs":[],"source":"from sklearn.tree import DecisionTreeClassifier\n\n# Tree\nparam_grid = {\"max_depth\": np.linspace(10, 15, 6).astype(int),\n              \"min_samples_split\": np.linspace(2, 5, 4).astype(int)\n              }\nclf = GridSearchCV(DecisionTreeClassifier(), param_grid=param_grid, cv=cv)\nclf.fit(X_train, y_train)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"cb06f99187e86f2b5bb8ea341cfd80f64056b3c8","_cell_guid":"10906f81-e394-45c7-8df4-aa1a5c9bba8c"},"outputs":[],"source":"print(\"[TREE] The best parameters are %s with a score of %0.2f\"\n      % (clf.best_params_, clf.best_score_))","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"f9ca32743966f26571d2467611e05702156c1708","_cell_guid":"966fa74a-c0af-4ddc-853c-2a84a716d6d0"},"outputs":[],"source":"importances = clf.best_estimator_.feature_importances_\nindices = np.argsort(importances)\nplt.figure(figsize=[16, 8])\nplt.title('Feature Importances for DecisionTreeClassifier')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), X_train.columns[indices])\nplt.xlabel('Relative Importance')","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"e155fb9c21490d96218727935576674b248b7c5f","_cell_guid":"57e2028d-b0f9-4e0c-8ae1-853ff60b362b"},"outputs":[],"source":"from sklearn.ensemble import RandomForestClassifier\n\n# Random Forest\nparam_grid = {\"n_estimators\": [250, 300],\n              \"criterion\": [\"gini\", \"entropy\"],\n              \"max_depth\": [10, 15, 20],\n              \"min_samples_split\": [2, 3, 4]}\nforest = GridSearchCV(RandomForestClassifier(), param_grid=param_grid, cv=cv, verbose=1)\nforest.fit(X_train, y_train)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"260ce8019410b495eb02020e029ae0c372b59f2a","_cell_guid":"2717e9e0-ca56-42ac-90c7-5836dc2f59f3"},"outputs":[],"source":"print(\"[FOREST] The best parameters are %s with a score of %0.2f\"\n      % (forest.best_params_, forest.best_score_))","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"722f2e5bb3bc235f6676e23db09cc03a351ca3e8","_cell_guid":"c904fc10-d10c-4b00-8228-990c64ea315c"},"outputs":[],"source":"importances = forest.best_estimator_.feature_importances_\nindices = np.argsort(importances)\nplt.figure(figsize=[16, 8])\nplt.title('Feature Importances for RandomForestClassifier')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), X_train.columns[indices])\nplt.xlabel('Relative Importance')","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"88aa36f06413f0e55fac5a0f10f4ad3dff617e9f","_cell_guid":"4627988b-1076-4f8b-be9a-1249ef818d27"},"source":"So, it seems the random forest is the best one.  Now, we can stack the models discussed above to potentially improve the prediction. I use the simple paramters based on the experiments we did above. We can run GridSearchCV in combination with the stacked model as well. ","cell_type":"markdown"},{"metadata":{"_uuid":"78d368ffdb5c0719080cf77eb611758f53be4c66","_cell_guid":"6076413f-0264-4181-be5c-500a987cae8a"},"outputs":[],"source":"from sklearn.linear_model import LogisticRegression\nfrom mlxtend.classifier import StackingCVClassifier\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\n\n\nclf1 = svm.SVC(C=1, gamma=0.1)\nclf2 = MLPClassifier(hidden_layer_sizes=(50,), max_iter=600, alpha=1)\nclf3 = DecisionTreeClassifier(max_depth=10, min_samples_split=4)\nclf4 = RandomForestClassifier(n_estimators=250, max_depth=10, min_samples_split=4, criterion='gini')\nlr = LogisticRegression()\n\nsclf = StackingCVClassifier(classifiers=[clf1, clf2, clf3, clf4],\n                            meta_classifier=lr)\n\nX_train2, X_cv, y_train2, y_cv = train_test_split(X_train, y_train, test_size=0.33, random_state=0)\n\nsclf.fit(X_train2.values, y_train2.values)\nprint(\"[Stacking] score on training data is %0.2f\", sclf.score(X_train2.values, y_train2.values))\nprint(\"[Stacking] score on the crossvalidation data is %0.2f\", sclf.score(X_cv.values, y_cv.values))","execution_count":null,"cell_type":"code"}]}