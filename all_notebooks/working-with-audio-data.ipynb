{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Working with audio data and building a recommender."},{"metadata":{},"cell_type":"markdown","source":"### Why?\nBecause, I am a music producer and a mastering engineer who is learning how machines learn and it will be a shame if I do not work on atleast one music related to ML problem and plus, it's fun so why not? ヽ(‘ ∇‘ )ノ\n\nWant to hear my music?\nClick on the link here:- https://www.youtube.com/c/FusionAssam"},{"metadata":{},"cell_type":"markdown","source":"### First Step: Importing the basic libraries.\n\nFirst, we will import the basic libraries that we mostly use. They are like siblings to you. You may like them or hate them, love them or fight with them but, at the end of the day you end up needing them. Yes, I am also a self made philosopher.\n\nWe will import machine learning libraries later on."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Second Step: Importing librosa.\n\nLibrosa is the fuel to the atom bomb we are going to work on today.\n\nKnow more about it here: https://librosa.org/doc/latest/index.html"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import librosa\nimport librosa.display\n\n# Importing other libraries just in case\n\nimport IPython.display as ipd\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfile_path = \"../input/gtzan-dataset-music-genre-classification/Data\"\nprint(os.listdir(f'{file_path}/genres_original/'))  # This will show us the 10 genre in the file.\n\n# I don't make music on any of the genres displayed below. That's a little heartbreaking.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Third Step: Exploring audio files\n\nHere we will check the sequence of the vibrations which will be an array because, it will be in numbers. A sound is analog but, inside a computer it is just a sequence of numbers. We will also check the sample rate of the sound.\n\n**What is a sample rate?**\n\nWe will talk about it a little later."},{"metadata":{"trusted":true},"cell_type":"code","source":"sound, sample_rate = librosa.load(f'{file_path}/genres_original/classical/classical.00003.wav')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Vibration sequence:', sound)  # Audio time series\narray_len = sound.shape\nprint('\\nSound shape:', array_len)\nprint('Sample Rate (Hz):', sample_rate)\n\n# Length of the sound\nprint('Check Len of Audio:', array_len[0] / sample_rate)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see above, the sample rate of the file is 22050Khz and length of the array is 661794.\n\n**A sample rate of an audio file is defined as the number of times a sample is obtained or recorded or exported by a device in one second.** \n\nIn simple terms, when we listen to a music or sound around us that does not come from any electronic device like the natural sound of human speaking, your friend playing a guitar in front of you without a mic or the sound of wind and water, we listen to the full range of sounds available out there out of which the sound in frequency of 20hz to 20,000hz is audible to human ears. In truth most humans cannot listen to sound of frequency below 50hz and above 17000hz but, people can feel the absence of the remaining frequencies. But, in a whole it is fixed that the audio range from 20hz to 20,000hz is audible to human ears.\n\nBut, now comes the electrical instruments which record the sounds. These instruments record the analog sound of the source i.e the musical instruments or human and convert it into digital signal/sound. These electronic devices records the sounds as small samples per second and add them together to create the music. Like in integral calculus, where we find area under a curve or an analog signal by stacking up small slices to cover the area below the curve and add them to get a rough picture close to the actual area under the curve. *The finer the slices, the more is the number of slices hence, we get more precise area under the curve.*\n\nSimilarly, in sound and music those slices are the samples. More the number of samples per second, the better will be the quality of the sound we listen from an electronic device since, more precise information of the real sound could be reproduced digitally.\n\nAnd this logic of sampling rate is perfectly stated under the Nyquist–Shannon sampling Theorem.\n\nThis theorem states that:\n> If a system uniformly samples an analog signal at a rate that exceeds the signal’s highest frequency by at least a factor of two, the original analog signal can be perfectly recovered from the discrete values produced by sampling.\n\nThat means, if you want to record a song or sound that needs to be close to sound that comes from the source, you will have to sample at least twice as fast as the bandwidth of the signal from the source.\nOtherwise, the high-frequency content creates an alias/distortion in the waveform i.e from 20hz to 20,000hz. What is an Alias? [Click here](http://zone.ni.com/reference/en-XX/help/370524V-01/siggenhelp/fund_aliased_images/)\n\nIn short, humans listen to sound upto 20,000hz so, sampling rate should be 40,000hz or in more digital terms atleast 44,100hz. Why 44,100hz? [Click here](http://en.wikipedia.org/wiki/44,100_Hz)\n\nThe sound samples in this dataset is 22050hz that means means we are loosing a lot of informations in these audio files which is not so cool in todays world but, very cool for machine learning. Because, lower the sample rate, lower the resolution of the sound hence, less data for processing but enough to make a machine think efficiently. Or maybe because Kaggle is not a music streaming service there it downsamples the sound. I don't know, my own song got downsampled while uploading here. For that you will have to scroll down a little. \n\nThe length of the array of the sounds in this dataset is 661794 that means, the audio sample consists of a collection of 661794 samples or vibration value of sound which were collected at a speed of 22050 samples per second which results in 661794/22050 = 30 seconds(approx).\n\nTime to code again."},{"metadata":{},"cell_type":"markdown","source":"All the sounds in this dataset are hard cut instead of fade in and fade out hence, there are clicks in the beginning and end of the samples instead of silence or in more computer terms, there is information in the both ends. So, we do not need to trim them using librosa.effects.trim(). I don't know I may be wrong here but, this is how I feel."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's look at the waveform of the sound.\n\nplt.figure(figsize=(16, 6))\nlibrosa.display.waveplot(y=sound, sr=sample_rate, color=\"#2f7d92ff\")\nplt.title(\"Waveform of classical.00003.wav\", fontsize=12)  # Classical music are highly dynamic.\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Just for fun let's see one of my own sound file. \n\nmy_sample, my_sample_rate = librosa.load('../input/fusiona-mandelbrot/FusionA - Mandelbrot Mixed File (22-7-2020) 1.mp3')\nprint('Vibration sequence:', my_sample)  # Audio time series\narray_len = my_sample.shape\nprint('\\nSound shape:', array_len)\nprint('Sample Rate (Hz):', my_sample_rate)\n\n# Length of the sound\nprint('Check Len of Audio:', array_len[0] / my_sample_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's look at the waveform of my sound. (ﾟ▽^*)\n\nplt.figure(figsize=(60, 15))\nlibrosa.display.waveplot(y=my_sample, sr=my_sample_rate, color=\"#d3a5a7ff\")\nplt.title(\"Waveform of FusionA - Mandelbrot\", fontsize=60)\nplt.xticks(fontsize=60)\nplt.xlabel('Time', fontsize=60)\nplt.yticks(fontsize=60)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Wow ヽ(゜∇゜)ノ\n\nNow, thats a wonderful waveform. Listen to my song \"Mandelbrot\" guys. (¬‿¬) [Click here](http://www.youtube.com/watch?v=pSDtn8PT-Mg) and enjoy, my fellow comrades."},{"metadata":{},"cell_type":"markdown","source":"### Time for some Fourier Transform\n\nIn mathematics, a Fourier transform (FT) is a mathematical transform that decomposes a function (often a function of time, or a signal) into its constituent frequencies, such as the expression of a musical chord in terms of the volumes and frequencies of its constituent notes. The term Fourier transform refers to both the frequency domain representation and the mathematical operation that associates the frequency domain representation to a function of time.\n\nDirectly copied from [wikipedia](http://**en.wikipedia.org/wiki/Fourier_transform)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# For this we will go back to using the original sound samples that were on the dataset.\n# In out case it was stored in a variable name 'sound'.\n\n# Default FFT window size\nn_fft = 2048 # FFT window size\nhop_length = 512 # number audio of frames between STFT columns (looks like a good default)\n\n# Short-time Fourier transform (STFT)\nD = np.abs(librosa.stft(sound, n_fft=n_fft, hop_length=hop_length))\n\nprint('Shape of D object:', np.shape(D))\nprint('\\nD:-\\n', D)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16, 6))\nplt.plot(D)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Time to look at the spectrogram\n\n**What is a spectrogram?**\n\nA spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. When applied to an audio signal, spectrograms are sometimes called sonographs, voiceprints, or voicegrams.\n\nDirectly copied from [you guessed it](http://en.wikipedia.org/wiki/Spectrogram).\n\n    In musical term, a spectrogram is a detailed view of audio, able to represent time, frequency, and amplitude all on one graph. A spectrogram can visually reveal broadband, electrical, or intermittent noise in audio."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert an amplitude spectrogram to Decibels-scaled spectrogram.\ndb = librosa.amplitude_to_db(D, ref=np.max)\n\n# Creating the Spectogram\nplt.figure(figsize = (16, 6))\nlibrosa.display.specshow(db, sr=sample_rate, hop_length=hop_length, x_axis='time', y_axis='mel',\n                        cmap='gist_heat')\nplt.colorbar();\n# We are using 'gist_heat' colour map because this colour is similar to Izotope Rx Spectogram\n# And it is mostly common in the world of music.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To know about the spectogram in Izotope Rx [click here](http://help.izotope.com/docs/rx/pages/userguide_spectrogramwaveformdisplay.htm)."},{"metadata":{},"cell_type":"markdown","source":"### Time to look at the Zero-Crossing Rate:\n\nThe zero-crossing rate is the rate of sign-changes along a signal, i.e., the rate at which the signal changes from positive to zero to negative or from negative to zero to positive. This feature has been used heavily in both speech recognition and music information retrieval, being a key feature to classify percussive sounds.\n\nA voice signal oscillates slowly - for example, a 100 Hz signal will cross zero 100 per second - whereas an unvoiced fricative can have 3000 zero crossing per second."},{"metadata":{"trusted":true},"cell_type":"code","source":"zc = librosa.zero_crossings(sound, pad=False)  # The zero crossing rate of the sound sample\nsum(zc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"zc2 = librosa.zero_crossings(my_sample, pad=False)  # The zero crossing rate of my song\nsum(zc2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analyze Harmonics, Perceptual, Tempo and Pitch:\n\nHarmonics: When a musical instrument is playing a note, what we are actually hearing is the fundamental pitch, which is the pitch being played by the instrument, accompanied by a series of frequencies that are usually heard as a single composite tone. Those frequencies that are integer multiples of the fundamental pitch's frequency are called harmonics. To know more [click here](http://study.com/academy/lesson/what-are-harmonics-definition-types-quiz.html).\n\nPerceptual: Music involves the manipulation of sound. Our perception of music is thus influenced by how the auditory system encodes and retains acoustic information. To know more [click here](http://www.researchgate.net/publication/220723259_Perceptual_and_Cognitive_Applications_in_Music_Information_Retrieval) or [here](http://serious-science.org/perception-of-music-9396).\n\nTempo (Beats per minute): “Beats per minute” (or BPM) is self-explanatory: it indicates the number of beats in one minute. For instance, a tempo notated as 60 BPM would mean that a beat sounds exactly once per second. To know more [click here](http://www.masterclass.com/articles/music-101-what-is-tempo-how-is-tempo-used-in-music#what-is-beats-per-minute-bpm).\n\nPitch: You know what is pitch."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decompose an audio time series into harmonic and percussive components.\n\ny_harm, y_perc = librosa.effects.hpss(sound)\nplt.figure(figsize = (16, 6))\nlibrosa.display.waveplot(y_harm, sr=sample_rate, color=\"#6885a7ff\", alpha=0.25);\nlibrosa.display.waveplot(y_perc, sr=sample_rate, color='#cf27a7ff', alpha=0.5);\nax = plt.axes()\nax.set(title='Harmonic + Percussive');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detecting the tempo of the track\n\ntempo = librosa.beat.tempo(y=sound, sr=sample_rate)\nprint(tempo)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tempo = librosa.beat.tempo(y=my_sample, sr=my_sample_rate)\nprint(tempo)  # This value is wrong. This is not the tempo of my song.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Chromogram\nchromagram = librosa.feature.chroma_stft(sound, sr=sample_rate, hop_length=10000)\nplt.figure(figsize=(16, 6))\nlibrosa.display.specshow(chromagram, x_axis='time', y_axis='chroma', hop_length=10000, cmap='coolwarm');\n\n# Low hop_legth = finer cell blocks.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Time to speak some big big words like Explanatory Data Analysis.\n\n╰(◡‿◡✿╰)"},{"metadata":{},"cell_type":"markdown","source":"### Forth Step: Running Pandas for EDA and other stuffs.\n\nFinally, this library became useful for this problem."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing 30 secs csv file.\npd.set_option('max_columns', None)\ndata = pd.read_csv(f'{file_path}/features_30_sec.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here columns mfcc are nothing but [Mel-Frequency Cepstral Coefficients](http://medium.com/prathena/the-dummys-guide-to-mfcc-aceab2450fd)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a box-plot to check the distribution of the genres\n\nx = data[[\"label\", \"tempo\"]]\n\nf, ax = plt.subplots(figsize=(16, 9));\nsns.boxplot(x = \"label\", y = \"tempo\", data = x, palette = 'PuBuGn');\n\nplt.title('BPM Boxplot for Genres', fontsize = 15)\nplt.xticks(fontsize = 10)\nplt.yticks(fontsize = 10);\nplt.xlabel(\"Genre\", fontsize = 10)\nplt.ylabel(\"BPM\", fontsize = 10)\nplt.savefig(\"BPM Boxplot.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"point = data.iloc[:, 2:]\npoint","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\n\ndata = data.iloc[0:, 1:]\ny = data['label']\nX = data.drop('label', axis=1)\n\ncols = X.columns\nmin_max_scaler = preprocessing.MinMaxScaler()\nnp_scaled = min_max_scaler.fit_transform(X)\nX = pd.DataFrame(np_scaled, columns = cols)\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(X)\nprincipalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\n\nfinalDf = pd.concat([principalDf, y], axis = 1)\n\npca.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16, 9))\nsns.scatterplot(x = \"principal component 1\", y = \"principal component 2\", data = finalDf, hue = \"label\", alpha = 0.7,\n               s = 100);\n\nplt.title('PCA on Genres', fontsize = 15)\nplt.xticks(fontsize = 12)\nplt.yticks(fontsize = 12);\nplt.xlabel(\"Principal Component 1\", fontsize = 15)\nplt.ylabel(\"Principal Component 2\", fontsize = 15)\nplt.savefig(\"PCA Scattert.jpg\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fifth Step: Machine Learning\n\nFinally, time to see what we can do with the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nHere we will do the following task:-\n    * We will first import the libraries we want for machine learning.\n    * We will use the feature_3_sec.csv for building and testing.\n    * We will use Random Forest, KNN, XGBoost and support vector machine.\n'''\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split as tts\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve, precision_score, recall_score, f1_score\n# Feature ranking with recursive feature elimination\nfrom sklearn.feature_selection import RFE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/gtzan-dataset-music-genre-classification/Data/features_3_sec.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['length'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting data\n# We will remove column 'filename' since all the data in this column is unique.\n# We will remove 'length' column since all the data in this column is same.\n\ndf = data.iloc[0:, 2:]\n\ny = df['label'].values\nX = df.drop('label', axis=1)\n\nscale = MinMaxScaler()\nscaled_data = scale.fit_transform(X)\nX = pd.DataFrame(scaled_data, columns = X.columns).values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since, human beings are lazy it is better to build a function to do net repeated tasks.\n\nFirstly, we will split the dataset using KFold to get a better understanding of our models while evaluating.\n\nSecondly, we will wait for the kernel to perform it's magic."},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_build(model, kf, title = \"Default\"):\n    accuracy_scores = []\n    precision_scores = []\n    recall_scores = []\n    f1_scores = []\n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        accuracy_scores.append(accuracy_score(y_test, y_pred))\n    print(\"Accuracy score of\", title, \"is:\", round(np.mean(accuracy_scores), 2))\n    # Let's see the confusion matrix of the last split for a little insight\n    con_mat = confusion_matrix(y_test, y_pred)\n    plt.figure(figsize = (16, 9))\n    sns.heatmap(con_mat, cmap=\"Blues\", annot=True, \n                xticklabels = [\"blues\", \"classical\", \"country\", \"disco\", \n                               \"hiphop\", \"jazz\", \"metal\", \"pop\", \"reggae\", \"rock\"], \n                yticklabels=[\"blues\", \"classical\", \"country\", \"disco\", \"hiphop\", \n                             \"jazz\", \"metal\", \"pop\", \"reggae\", \"rock\"])\n    plt.show()\n    \n\n# Leave 2 blank spaces after a function definition","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"split = KFold(n_splits=5, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### First let us have a look at the Shenanigans performed by the Random Forest Classifier followed by KNN, XG Boost and later on with SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest Classifer\n\nrfc = RandomForestClassifier(n_estimators=1000, max_depth=10, random_state=10)\nmodel_build(rfc, split, 'Random Forest Classifier')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# K Nearest Neighbor\n\nknn = KNeighborsClassifier(n_neighbors=10)\nmodel_build(knn, split, 'K Nearest Neighbor')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# XG Boost\n\nxgb = XGBClassifier()\nmodel_build(xgb, split, 'XG Boost')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Support Vector Machine\n\nsvm = SVC(decision_function_shape=\"ovo\")\nmodel_build(svm, split, 'Support Vector Machine')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above data we can see that XG Boost has greater chance of success in this problem. Let's fine tune it a little more. We will use Grid Search CV for this operation."},{"metadata":{},"cell_type":"markdown","source":"### Sixth Step: Tuning the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = tts(X, y, test_size=0.25, random_state=1)\nmodel = XGBClassifier(n_estimators=1000, learning_rate=0.3)\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test)\nprint('Accuracy', ':', round(accuracy_score(y_test, y_pred), 5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Seventh Step: Feature importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(estimator=model, random_state=1)\nperm.fit(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = df.drop('label', axis=1).columns.tolist()\neli5.show_weights(estimator=perm, feature_names = columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Eighth Step: Building recommender system"},{"metadata":{"trusted":true},"cell_type":"code","source":"# First we will scale the data\n\nimport IPython.display as ipd\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn import preprocessing\n\n# Read data\ndata = pd.read_csv(f'{file_path}/features_30_sec.csv', index_col='filename')\n\n# Extract labels\nlabels = data[['label']]\n\n# Drop labels from original dataframe\ndata = data.drop(columns=['length','label'])\ndata.head()\n\n# Scale the data\ndata_scaled=preprocessing.scale(data)\nprint('Scaled data type:', type(data_scaled))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And then we will find the [Cosine Similarity](http://www.sciencedirect.com/topics/computer-science/cosine-similarity#:~:text=Cosine%20similarity%20measures%20the%20similarity,document%20similarity%20in%20text%20analysis.). This will be the tool for our recommender system."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cosine similarity\nsimilarity = cosine_similarity(data_scaled)\nprint(\"Similarity shape:\", similarity.shape)\n\n# Convert into a dataframe and then set the row index and column names as labels\nsim_df_labels = pd.DataFrame(similarity)\nsim_df_names = sim_df_labels.set_index(labels.index)\nsim_df_names.columns = labels.index\n\nsim_df_names.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now, we will define the recommender function."},{"metadata":{"trusted":true},"cell_type":"code","source":"def recommender(name):\n    series = sim_df_names[name].sort_values(ascending = False)\n    \n    # Remove cosine similarity == 1 (songs will always have the best match with themselves)\n    series = series.drop(name)\n    topfive = series.head(5)\n    songnames = topfive.index.tolist()\n    address_list = ['../input/gtzan-dataset-music-genre-classification/Data/genres_original/blues',\n                   '../input/gtzan-dataset-music-genre-classification/Data/genres_original/classical',\n                   '../input/gtzan-dataset-music-genre-classification/Data/genres_original/country',\n                   '../input/gtzan-dataset-music-genre-classification/Data/genres_original/disco',\n                   '../input/gtzan-dataset-music-genre-classification/Data/genres_original/hiphop',\n                   '../input/gtzan-dataset-music-genre-classification/Data/genres_original/jazz',\n                   '../input/gtzan-dataset-music-genre-classification/Data/genres_original/metal',\n                   '../input/gtzan-dataset-music-genre-classification/Data/genres_original/pop',\n                   '../input/gtzan-dataset-music-genre-classification/Data/genres_original/reggae',\n                   '../input/gtzan-dataset-music-genre-classification/Data/genres_original/rock']\n    genre_list = [\"blues\", \"classical\", \"country\", \"disco\", \n                  \"hiphop\", \"jazz\", \"metal\", \"pop\", \"reggae\", \"rock\"]\n    songlist = []\n    songnames = []\n    for songname in topfive.index:\n        songgenre = songname.split('.')[0]\n        address = genre_list.index(songgenre)\n        fileaddress = address_list[address] + ('/') + songname\n        songlist.append(fileaddress)\n        songnames.append(songname)\n    return songlist, songnames","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Time for some experiments:"},{"metadata":{},"cell_type":"markdown","source":"### Experiment no. 1:\n\nFirst, we will check on a hip hop song. I liked the beat of the song hiophop.00010.wav so, I am using it."},{"metadata":{"trusted":true},"cell_type":"code","source":"now_playing = 'hiphop.00010.wav'\nplaylist, songname = recommender(now_playing)\nprint('Now playing:', now_playing)\nipd.Audio('../input/gtzan-dataset-music-genre-classification/Data/genres_original/hiphop/hiphop.00010.wav')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below, is the recommended song based on the song playing now."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Recommended songs:\\n\\n', pd.Series(songname))\nipd.Audio(playlist[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both, the songs sound quite similar. So, we can see that our recommender system is working good."},{"metadata":{},"cell_type":"markdown","source":"### Experiment no.: 2\n\nNow, let's check on a classical music."},{"metadata":{"trusted":true},"cell_type":"code","source":"now_playing = 'classical.00003.wav'\nplaylist, songname = recommender(now_playing)\nprint('Now playing:', now_playing)\nipd.Audio('../input/gtzan-dataset-music-genre-classification/Data/genres_original/classical/classical.00003.wav')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Recommended songs:\\n\\n', pd.Series(songname))\nipd.Audio(playlist[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above information we can see that our recommender system is working quite good."},{"metadata":{},"cell_type":"markdown","source":"Hence, we can conclude in a way that the recommender system we built is working great. I am so happy that I learned it.\n\nI want to thank [Miss Andrada Olteanu](http://www.kaggle.com/andradaolteanu) for sharing her notebook on her Kaggle account from where I learned this concept. And, yes I am very happy with what I learned and want to share it with you all. Have fun, and I hope I may have shown you a few styles of my coding from which you can learn.\n\nThank you for your patience. "},{"metadata":{},"cell_type":"markdown","source":"### And, here is my song. I hope you will like it ヽ(^◇^*)/\n\nFor better quality [click here](http://www.youtube.com/watch?v=pSDtn8PT-Mg).\n\nThank you all. Have fun learning."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"FusionA - Mandelbrot\")\nipd.Audio('../input/fusiona-mandelbrot/FusionA - Mandelbrot Mixed File (22-7-2020) 1.mp3')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}