{"cells":[{"metadata":{},"cell_type":"markdown","source":"***\n# CoNetz\n***\n## HIGHLIGHTS\n\nThis submission used the power of network based analysis for rapid identification of current and potential new non-pharmaceutical interventions (NPIs). Text and network mining techniques are employed to build a comprehensive network of relations between a vast variety of biological entities. The tools allow easy exploration and visualization of the network for generation of leads. The submission utilizes both the CORD-19 corpus as well as relevant MEDLINE abstracts.\n\nSome of the immediate insights derived in the context of COVID 19:\n\n* While school closures was seen as an important non-phermaceutical intervention (NPI), many articles did point out different social difficulties that would need to be factored in when planning such NPIs.\n* The fact that airport screening would not be sufficient to detect COVID-19 positive persons is also brought out.\n\nSome use cases to explore leads are illustrated. We encourage the community to leverage the power of this network and its easy to use Python interface, for this task and beyond.\n\n## Introduction\nSince the outbreak of the COVID-19 pandemic, there has been a massive pursuit by the research community to find drugs to treat this disease as well as discover vaccines against the disease. A large number of research papers have been published to this end, peer-reviewed as well as those posted in preprint repositories such as bioRxiv (www.bioRxiv.org) and medRxiv (www.medRxiv.org). In addition, a large number of peer-reviewed papers on earlier coronavirus-related diseases such as SARS and MERS are also available.\n\nThe COVID-19 Open Research Dataset (CORD-19 corpus) consists of abstracts and full-text articles on COVID-19, SARS-CoV-2, and related coronaviruses. This freely available dataset is provided to the global research community via this Kaggle challenge to apply recent advances in natural language processing (NLP) and other related techniques to generate insights in support of the ongoing fight against this infectious disease.\n\nAt a specific level, it means we have to help uncover **\"*unknown known*\"** entities such as drugs and vaccines that are maybe unknown to the larger set of researchers but mentioned in specific scientific article(s) part of the CORD-19 dataset. Our goal is to help the medical research community uncover these \"unknown known\" entities through a combination of text-mining and network analyses.\n\n## Approach\n### Association Network Creation\nWe had earlier built a framework for NLP called TPX, a web-based text-mining tool that supports real-time entity assisted search and navigation of the MEDLINE repository whilst continuing to use PubMed as the underlying search engine (1). TPX is a modular and versatile biomedical text-mining framework. For instance, we recently built PRIORI-T (2), a pipeline for phenotype-driven rare disease gene prioritization, by re-purposing specific modules of TPX. The modules include:\n1. Dictionary Curation module\n2. Annotator for entity annotations\n3. MEDLINE Processor\n4. Network Creation module, to build a network of the correlations extracted by the Correlation Extraction module\n\nWe re-purposed TPX for the COVID-19 Open Research Dataset Challenge (CORD-19) as follows:\n1.  We took the provided CORD19 dataset corpus (Corpus Date: 2020-04-10 consisting of 51k full-text articles). We used the Full-text article Processor module of TPX to process this corpus.\n2.  The Annotator module of TPX performed the annotation based on the following dictionaries: HUMAN_GENE, GENE_SARS, GENE_MERS, GENE_COVID, PHENOTYPE, CHEMICALS, DRUGS, DISEASE, SYMPTOM, GOPROC, GOFUNC, GOLOC, CELLTYPE, TISSUE, ANATOMY, ORGANISM, COUNTRIES, ETHICS TERMS (general terms related to human ethics), NON-PHARMA INTERVENTION (terms related to non-pharmaceutical intervention), SURVEILLANCE TERMS (general terms related to disease surveillance), VACCINE TERMS, VIROLOGY TERMS (general terms used in virology studies) and EARTH SCIENCE TERMS.\n3.  We then used the Correlation Extraction module to extract out correlations amongst these entity types and\n4.  These correlations extracted by the Correlation Extraction module are then used by the Network Creation module to build a network called TCS\\_COVID\\_NETWORK (cord19\\_pc\\_assocs\\_v1.tsv). This network can is queried to obtain information and pointers from the corpus.\n\nThus, the TCS\\_COVID\\_NETWORK serves as a knowledge base that could help the COVID-19 research community obtain pointers in this regard through a combination of text-mining and network analyses. It is available from Kaggle for use by anyone to possibly try and solve some of text mining related questions that are posed in this Kaggle challenge\n\n### PyVis Visualization\nWe then used PyVis, a Python-based library for constructing and visualizing an intuitive and interactive exploration of TCS\\_COVID\\_NETWORK. The Jupyter notebook provides details on this. For instructive purpose, we have included a set of use cases for exploring the network using PyVis and Networkx library. These cases are by no means exhaustive. However, the PyVis and Networkx functionalities can be easily used to provide and/or build richer exploration features."},{"metadata":{},"cell_type":"markdown","source":"## Non-Pharmaceutical Interventions (NPIs)\nWe now describe how this network can be used to answer some of the questions posed in the **non-pharmaceutical interventions (NPIs) task**. The task focuses on *\"What do we know about the effectiveness of non-pharmaceutical interventions (NPIs)? Specifically, we want to know what the literature reports about rapid assessment of the likely efficacy of school closures, travel bans, bans on mass gatherings of various sizes, and other social distancing approaches.\"*."},{"metadata":{},"cell_type":"markdown","source":"### Our initial analysis showed that provided corpus lacked a few articles that talk about NPIs. Hence for this task, we augment the provided corpus with MEDLINE articles. \n\nTowards this, we included 30.7 mn articles from the MEDLINE corpus till April 05, 2020 and CORD-19 the articles (w/titles and abstracts) that were missing in MEDLINE corpus. We ran NER on these articles and filtered those articles that had atleast one of the entities corresponding to \n> 1. Diseases - \"Coronavirus infections\", \"SARS\", \"MERS\", \"COVID-19\" or \n> 2. Taxonomies - (\"Severe acute respiratory syndrome-related coronavirus\", \"Middle East respiratory syndrome-related coronavirus\", \"Severe acute respiratory syndrome coronavirus 2\"). \n\nWe computed pair-wise associations for various biomedical entities from this filtered corpus of 14,139 articles. The article corpus (cord19_medline_arts.tsv) and associations network (cord19_medline_pc_assocs_v1.tsv) were included as part of the dataset. We will be updating the association network after running on the full-text articles in the subsequent versions. "},{"metadata":{},"cell_type":"markdown","source":"Install needed packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyvis","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Import all the needed python libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyvis.network import Network\nimport networkx as nx\nimport pandas as pd\nfrom IPython.display import IFrame","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define global variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_DIR='/kaggle/input/cord19/submissions/'\nNETWORK_FILE = DATA_DIR+'cord19_medline_assocs_v2.3.tsv'\nENTITY_NAME_FILE = DATA_DIR+'dicts/entity_name.tsv'\nENT_METADATA_FILE = DATA_DIR+'dicts/entity_metadata.csv'\nent_name=None\nent_id_name=None\nent_name_id={}\nenttype_map=None\nent_cmap=None\nent_srcmap=None\nconnected_nodes=False\nnotebook_mode=True\nnet_options = {\n  \"nodes\": {\n    \"scaling\": {\n      \"min\": 46\n    }\n  },\n  \"edges\": {\n    \"color\": {\n      \"inherit\": True\n    },\n    \"shadow\": {\n      \"enabled\": True\n    },\n    \"smooth\": True\n  },\n  \"interaction\": {\n    \"hover\": True,\n    \"navigationButtons\": True\n  },\n  \"physics\": {\n    \"enabled\": True,\n    \"forceAtlas2Based\": {\n      \"gravitationalConstant\": -150,\n      \"springLength\": 100\n    },\n    \"minVelocity\": 0.05,\n    \"timestep\":0.1,\n    \"solver\": \"forceAtlas2Based\"\n  }\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Helper Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def getEntityMaps():\n    ent_meta_map = pd.read_csv(ENT_METADATA_FILE, sep=',')\n    enttype_map = ent_meta_map[['entid','enttype','entsource']]\n    \n    ent_name_df = pd.read_csv(ENTITY_NAME_FILE, sep='\\t', converters={'TypeId':str})\n    ent_name_df.TypeId=ent_name_df.TypeId.str.upper()\n    ent_name_df.Synonym=ent_name_df.Synonym.str.upper()\n    ent_name_df.DictId = ent_name_df.DictId.map(enttype_map.set_index('entid')['enttype'])\n    \n    ent_name = ent_name_df.set_index('Synonym').to_dict()\n    ent_id_name = ent_name_df.set_index(['TypeId','DictId']).to_dict()\n    \n    ent_color = ent_meta_map[['enttype','entcolor']]\n    ent_cmap = ent_color.set_index('enttype').to_dict(orient='index')\n    \n    ent_source = ent_meta_map[['enttype','entsource']]\n    ent_srcmap = ent_source.set_index('enttype').to_dict(orient='index')\n    return enttype_map, ent_name, ent_id_name, ent_cmap, ent_srcmap\n\ndef getNetwork():\n    nw = pd.read_csv(NETWORK_FILE,sep='\\t',converters={'src_ent':str, 'target_ent':str})\n    nw.src_type = nw.src_type.map(enttype_map.set_index('entid')['enttype'])\n    nw.target_type = nw.target_type.map(enttype_map.set_index('entid')['enttype'])\n    nw.src_ent=nw.src_ent.str.upper()\n    nw.target_ent=nw.target_ent.str.upper()\n    return nw\n\ndef buildQueryCriteria(src_ents, source_ent_types=None, target_ents=None, target_ent_types=None, \n                       queryByEntityName=True, topk=50, topkByType=None, connected_nodes=False, indirect_links=None):\n    # Normalize it upper-case\n    src_ents = [i.upper() for i in src_ents]\n    \n    criteria = {'src_ents' : src_ents,\n                'src_ent_types': source_ent_types,\n                'target_ents': target_ents,\n                'target_ent_types': target_ent_types,\n                'query_entname' : queryByEntityName,\n                'topk' : topk,\n                'topkByType' : topkByType,\n                'connected_nodes' : connected_nodes,\n                'indirect_links' : indirect_links\n                }\n    return criteria\n\ndef queryByEntityTypes(nw, src_ent_types, target_ent_types):\n    #Fetch the network\n    qnw=None\n    if(src_ent_types is not None):\n        qnw = nw[nw.src_type.isin(src_ent_types)]\n    if(target_ent_types is not None):\n        qnw = nw[nw.target_type.isin(target_ent_types)]\n    if(qnw is None):\n        qnw=nw\n    return qnw\n\ndef queryByEntityID(nw, src_ents, target_ents=None):\n    #Fetch the network\n    qnw = nw[nw.src_ent.isin(src_ents)]\n    if(target_ents is not None):\n        target_ents = [i.upper() for i in target_ents]\n        qnw = nw[nw.target_ent.isin(target_ents)]\n    return qnw\n\ndef getEntityIds(ents):\n    print(' Querying by Entity Name ..')\n    #Get the entity triplet for the entities\n    typeids = [ent_name['TypeId'][i] for i in ents]\n    dictids = [ent_name['DictId'][i] for i in ents]\n    #Update Entity Name to Entity/Node ID for reference\n    for i in range(len(ents)):\n        ent_name_id[ents[i]]=getNodeID(typeids[i], dictids[i])\n    return typeids, dictids\n\n\ndef queryByEntityName(nw, src_ents, target_ents=None):\n    typeids, dictids = getEntityIds(src_ents)\n    qnw = nw[nw.src_ent.isin(typeids) & (nw.src_type.isin(dictids))]\n    if(target_ents is not None):\n        target_ents = [i.upper() for i in target_ents]\n        typeids, dictids = getEntityIds(target_ents)\n        qnw = nw[nw.src_ent.isin(typeids) & (nw.src_type.isin(dictids))]\n    return qnw\n\ndef queryTopk(nw, topk, topkByType):\n    qnw=None\n    if(topkByType!=None):\n        qnw = nw.groupby(['src_ent','target_type']).head(topkByType)\n    else:\n        qnw = nw.groupby(['src_ent','target_type']).head(topk)\n    return qnw\n\ndef queryNetwork(nw, criteria):\n    # Use the criteria to query the network by entity name\n    qnw=None\n    \n    if(criteria['query_entname']==True):\n        qnw = queryByEntityName(nw, criteria['src_ents'], target_ents=criteria['target_ents'])\n    else:\n        qnw = queryByEntityID(nw, criteria['src_ents'], target_ents=criteria['target_ents'])\n    \n    # Query by entity types\n    qnw = queryByEntityTypes(qnw, criteria['src_ent_types'], criteria['target_ent_types'])\n    \n    # Display only Top-k entites\n    qnw = queryTopk(qnw, criteria['topk'], criteria['topkByType'])\n    \n    return qnw\n\ndef getEntityNames(src, target):\n    src_name = ent_id_name['Synonym'][src]\n    target_name = ent_id_name['Synonym'][target]\n    return src_name, target_name\n\ndef getNodeID(typeid, dictid):\n    return typeid+'-'+dictid[:2]\n\ndef buildNodeAttributes(e):\n    # Build Node attributes - node_id, node_label, node_title, node_color \n    src_label, target_label = getEntityNames((e[0],e[1]), (e[2],e[3]))\n    \n    # Build src node\n    src_id = getNodeID(e[0], e[1])\n    src_title=\"<b>\"+src_label+\"</b><br><i>\"+e[1]+\"<br>\"+e[0]+\"</i><br>\"+ent_srcmap[e[1]]['entsource']\n    src_color=ent_cmap[e[1]]['entcolor']\n    \n    # Build target node\n    target_id = getNodeID(e[2], e[3])\n    target_title=\"<b>\"+target_label+\"</b><br><i>\"+e[3]+\"<br>\"+e[2]+\"</i><br>\"+ent_srcmap[e[3]]['entsource']\n    target_color=ent_cmap[e[3]]['entcolor']\n    \n    return (src_id, src_label, src_title, src_color), (target_id, target_label, target_title, target_color)\n\ndef edgeAttributes(ent1, ent2, edge_props):\n    #Build edge attributes\n    edge_prop_arr = edge_props.split(sep=',')\n    num_arts = int(edge_prop_arr[0])-3\n    edge_title = '<b>'+ent1+' --- '+ent2+'</b><br>Article Evidence(s) :<br>'\n    art_type=''\n    for i in range(3, len(edge_prop_arr)):\n        art=edge_prop_arr[i].replace(\"[\",\"\")\n        art=art.replace(\"]\",\"\")\n        if(\"FT_\" in art):\n            art=art.replace(\"FT_\",\"\")\n            art_type='CORD_UID :'\n        else:\n            art_type='PUBMED_ID :'\n        edge_title+=art_type+'<i>'+art+'</i><br>'\n    if(num_arts>5):\n        edge_title+='and <i><b>'+str(num_arts)+'</b> more articles ...</i>'\n    return edge_title\n\ndef buildGraph(G, filters=False):\n    #Define Network layout\n    net = Network(height=\"750px\", width=\"100%\", bgcolor=\"white\", font_color=\"black\", notebook=notebook_mode)\n    net.options=net_options\n    \n    #Convert networkx G to pyvis network\n    edges = G.edges(data=True)\n    nodes = G.nodes(data=True)\n    if len(edges) > 0:\n        for e in edges:\n            snode_attr=nodes[e[0]]\n            tnode_attr=nodes[e[1]]            \n            net.add_node(e[0], snode_attr['label'], title=snode_attr['title'], color=snode_attr['color'])\n            net.add_node(e[1], tnode_attr['label'], title=tnode_attr['title'], color=tnode_attr['color'])\n            net.add_edge(e[0], e[1], value=e[2]['value'], title=e[2]['title'])\n    return net    \n\ndef applyGraphFilters(G, criteria):\n    \n    fnodes={}\n    # Filter1 - Connected nodes\n    if(criteria['connected_nodes']):    \n        bic = nx.biconnected_components(G)\n        for i in bic:\n            if(len(i)>2):\n                fnodes=i.union(fnodes)\n    \n        # Get the sub-graph after applying the filter(s)\n        G=G.subgraph(fnodes)\n    \n    # Filter2 - 'indirect_links'\n    il_dicts = criteria['indirect_links']\n    if(il_dicts is not None):    \n        snode = il_dicts['source_node'] if ('source_node' in il_dicts) else criteria['src_ents'][0]\n        snode = ent_name_id[snode.upper()]\n        #Depth=Hops+1\n        depth=(il_dicts['hops']+1) if('hops' in il_dicts) else 2\n        \n        if('target_nodes' in il_dicts):\n            tnodes = il_dicts['target_nodes']\n        elif(criteria['target_ents'] is not None):\n            tnodes = criteria['target_ents']\n        else:\n            tnodes=criteria['src_ents']        \n        tnodes = [ ent_name_id[i.upper()] for i in  tnodes]\n    \n        # Traverse k-hops from source to target nodes.            \n        paths_between_generator = nx.all_simple_paths(G, source=snode, target=tnodes, cutoff=depth)\n        #indirect_paths = [tuple(e) for e in paths_between_generator]\n        \n        indirect_paths=[]\n        i=0\n        for k, path in enumerate(paths_between_generator):\n            #if(len(path)==depth+1):\n            ce=[]\n            #print(path)\n            for j, e in enumerate(path):\n                if j+1 <= len(path)-1:\n                    ce.append((path[j], path[j+1]))\n            indirect_paths.extend(ce)\n        G=G.edge_subgraph(indirect_paths)\n    return G\n\ndef run(criteria):\n    # Load the entire network\n    nw_df = getNetwork()\n\n    # Query the network with the defined search criteria\n    qnw = queryNetwork(nw_df, criteria)\n    print(' Number of Associations in the Final Network -->'+str(len(qnw)))\n\n    # Build association network using the query result\n    sources = qnw['src_ent']\n    source_types=qnw['src_type']\n    targets = qnw['target_ent']\n    target_types=qnw['target_type']\n    weights = qnw['score']\n    stats = qnw['debug']\n    edge_data = zip(sources, source_types, targets, target_types, weights, stats)\n\n    G=nx.Graph()\n    for e in edge_data:\n        snode, tnode = buildNodeAttributes(e)\n        G.add_node(snode[0], label=snode[1], title=snode[2], color=snode[3])\n        G.add_node(tnode[0], label=tnode[1], title=tnode[2], color=tnode[3])\n        G.add_edge(snode[0], tnode[0], value=e[4], title=edgeAttributes(snode[1],tnode[1], e[5]))\n\n    applyFilter = (criteria['connected_nodes'] or criteria['indirect_links'])\n    if(applyFilter):\n        G=applyGraphFilters(G, criteria)\n\n    net = buildGraph(G, applyFilter)\n    return net","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define your Queries - Query by Keyword (text term)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare Entity Maps\nenttype_map, ent_name, ent_id_name, ent_cmap, ent_srcmap = getEntityMaps()\n#Display Entity Types\nenttype_map","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Top-15 Non-pharma interventions for Covid19"},{"metadata":{"trusted":true},"cell_type":"code","source":"QueryTerms=['covid-19']\ncriteria = buildQueryCriteria(QueryTerms, topk=15, target_ent_types=['NON-PHARMA INTERVENTION'])\ncriteria\nnet = run(criteria)\nnet.show(\"cord19_npi.html\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Non-pharma interventions by countries for Covid19"},{"metadata":{"trusted":true},"cell_type":"code","source":"QueryTerms=['covid-19']\ncriteria = buildQueryCriteria(QueryTerms,target_ent_types=['NON-PHARMA INTERVENTION','COUNTRIES'], topkByType=15)\ncriteria\nnet = run(criteria)\nnet.show(\"cord19_npi_countries.html\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Connecting Non-pharma interventions"},{"metadata":{"trusted":true},"cell_type":"code","source":"QueryTerms=['Entry screening', 'Exit screening', 'school closure', 'contact tracing', 'isolation', 'movement restriction', 'personal protective measures']\ncriteria = buildQueryCriteria(QueryTerms, topkByType=15, connected_nodes=True, target_ent_types=['NON-PHARMA INTERVENTION'])\ncriteria\nnet = run(criteria)\nnet.show(\"cord19_connected_npis.html\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Connecting Non-pharma interventions by countries"},{"metadata":{"trusted":true},"cell_type":"code","source":"QueryTerms=['Entry screening', 'Exit screening', 'school closure', 'contact tracing', 'isolation', 'movement restriction', 'personal protective measures']\ncriteria = buildQueryCriteria(QueryTerms, topkByType=15, connected_nodes=True, target_ent_types=['NON-PHARMA INTERVENTION', 'COUNTRIES'])\ncriteria\nnet = run(criteria)\nnet.show(\"cord19_connected_npis_countries.html\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Covid-19 and school closure"},{"metadata":{"trusted":true},"cell_type":"code","source":"QueryTerms=['Entry screening', 'Exit screening', 'school closure', 'contact tracing', 'isolation', 'covid-19']\ncriteria = buildQueryCriteria(QueryTerms, topkByType=15, target_ent_types=['NON-PHARMA INTERVENTION', 'COUNTRIES'],\n                              indirect_links={'source_node':'covid-19', 'target_nodes':['school closure'], 'hops':2})\ncriteria\nnet = run(criteria)\nnet.show(\"cord19_school_closure.html\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Key Articles and Sentences\n### School closure\nMany of the articles that talk about school closures seem against this NPI because of the social impact. For instance, \n* PMID:32213332 says *\"However, quarantine and workplace distancing should be prioritised over school closure because at this early stage, symptomatic children have higher withdrawal rates from school than do symptomatic adults from work.\"*\n* PMID:32222161 full-text says *\"School closures during the 2014â€“16 Ebola epidemic increased dropouts, child labour, violence against children, teen pregnancies, and persisting socioeconomic and gender disparities. Access to distance learning through digital technologies is highly unequal, and subsidised meal programmes, vaccination clinics, and school nurses are essential to child health care, especially for marginalised communities\"*\n* A contrarian article is PMID:32242349 that says *\"The simulation results show that the government could reduce at least 200 cases\"*, making a case for school closure in South Korea.\n\n### Entry/Travel Restrictions vs Airport screening NPIs\n* PMID:32093043 says *\"We found that in countries with low connectivity to China but with relatively high R loc, the most beneficial control measure to reduce the risk of outbreaks is a further reduction in their importation number either by entry screening or travel restrictions\"*\n* Moreover, PMID:32046816 suggests *\"Airport screening is unlikely to detect a sufficient proportion of 2019-nCoV infected travellers to avoid entry of infected travellers.\"*\n\n### Isolation of child patients\n* PMID:32243729 says *\"The continuous positive real-time reverse transcription- polymerase chain reaction assay for SARS-CoV-2 in the child's throat swab sample indicated the isolation period for suspected child cases should be longer than 14 days.\"*\n\n### Countries included the cruise ship Diamond Princess\n* PMID:32190785 looked at the *\"Transmission potential of the novel coronavirus (COVID-19) onboard the diamond Princess Cruises Ship, 2020.\"* The authors state *\"Our findings suggest that Rt decreased substantially compared to values during the early phase after the Japanese government emented an enhanced quarantine control.\"\n* PMID:32183930 looks at \"Estimating the asymptomatic proportion of coronavirus disease 2019 (COVID-19) cases on board the Diamond Princess cruise ship, Yokohama, Japan, 2020.\" They mention that lack of \"Most infections occurred before the quarantine start.\"\n\nOf course these are very limited data points but based on these it seems:\n1. **Entry/travel restrictions should be prioritized over airport screening**\n2. **School closures may not be so effective, and also come with adverse social costs**\n3. **Isolation period for suspected child cases should be > 14 days**\n4. **Enforced quarantine of all passengers onboard the Diamond Princess might have led to increased cases**\n\nLike with many of the other NPIs, it is difficult to give a one NPI fits all scenarios.\n\n## References\n1. Joseph T, Saipradeep VG, Raghavan GS, Srinivasan R, Rao A, Kotte S, Sivadasan N. TPX: Biomedical literature search made easy. Bioinformation 8(12): 578-80 (2012).\n2. Rao A, Joseph T, Saipradeep VG, Kotte S, Sivadasan N, Srinivasan R. PRIORI-T: a tool for rare disease gene prioritization using MEDLINE. PLOS One (In Press)."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}