{"cells":[{"metadata":{},"cell_type":"markdown","source":"#Table of content\n\n1. [Import Packages](#1)\n2. [EDA](#2)\n3. [Data processing](#3)\n    - 3.1. [One-hot encoding and scaling](#3.1.)\n    - 3.2. [Oversampling using SMOTE](#3.2.)\n    - 3.3. [Feature selection](#3.3.)\n4. [Fitting models](#4)\n    - 4.1. [Model evaluation - Accuracy, Confusion matrix, ROC-AUC score](#4.1)\n    - 4.2. [Adding weights to Logistic Regression](#4.2)\n    - 4.3. [Using different Classifiers](#4.3)\n    - 4.4. [Hyperparameter tuning for KNN an RandomForest](#4.4)\n5. [Summary](#5)\n"},{"metadata":{},"cell_type":"markdown","source":"# Import packages <a id=\"1\"></a>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-apython\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\nimport statsmodels.api as sm\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA <a id=\"2\"></a>"},{"metadata":{},"cell_type":"markdown","source":"First let's get familiar with the data structure, data types and check if there are any missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is some mix up in data tracking in a sense that sometimes we use boolean values and sometimes Yes/No. This is not a problem at all, as I will later one hot encode all categorical features, but it's something to keep an eye on, because it can cause problems with pipeline/automatization, when we assume that all Yes/No features would be in certain format (Yes/No or 0/1).\n\nAlso TotalCharges are object, which is suspicious. Let's convert them to float, as that's what I would expect them to be, since they are only aggregate of MonthlyCharges."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.TotalCharges = data.TotalCharges.replace(' ',None)\ndata.TotalCharges = data.TotalCharges.apply(float)\ndata.TotalCharges","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are no missing values in the dataset. Next I will check some basic statistics for numeric columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.Churn.value_counts().plot(kind='bar',figsize=(10,5))\nplt.title('Churn comparison')\nplt.ylabel('Number of customers')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's very important to understand distributions of your data and visualization is very easy way to check for outliers in data.\n\nNow let's see if there is any feature (column), that could be indicative of churn. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows = 1,ncols = 2,figsize = (20,5))\nsns.distplot(data.tenure.tolist(),ax=axes[1])\nplt.xlabel('Number of months with company')\nplt.ylabel('Numbers of customers')\nplt.title('Distribution of tenure lenght of customers')\nsns.boxplot(data.tenure,ax=axes[0]) #boxplot is useful, because it has built-in visualization for outliers (black points)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows = 1,ncols = 2,figsize = (20,5))\nsns.distplot(data.MonthlyCharges.tolist(),ax=axes[1])\nplt.xlabel('Monthly charge')\nplt.ylabel('Numbers of customers')\nplt.title('Distribution of Monthly charges of customers')\nsns.boxplot(data.MonthlyCharges,ax=axes[0])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows = 1,ncols = 2,figsize = (20,5))\nsns.distplot(data.TotalCharges.tolist(),ax=axes[1])\nplt.xlabel('Total Charges')\nplt.ylabel('Numbers of customers')\nplt.title('Distribution of Total charges of customers')\nsns.boxplot(data.TotalCharges,ax=axes[0])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table=pd.crosstab(data.gender,data.Churn)\ntable.div(table.sum(1).astype(float), axis=0).plot(kind='bar',figsize=(10,5),title='Churn distribution based on gender',stacked=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby('Churn').mean() #quick way to see if there are any significant indicators, from numerical columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Big difference in tenure, is obvious, because retained users, simply spent more time with the company. Similar logic can be applied to Total charges.\n\nWhat is more interesting is that the customers, with higher monthly chargers seem to incline to churn more. We can leverage this information, by contacting Product team, and ask them to review the higher priced products. \n\nNow let's look at some distributions of some parameteres, split by churn indicator."},{"metadata":{"trusted":true},"cell_type":"code","source":"table=pd.crosstab(data.SeniorCitizen,data.Churn)\ntable.div(table.sum(1).astype(float), axis=0).plot(kind='bar',figsize=(10,5),title='Churn distribution based on senior citizenship',stacked=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Senior citizens seem to be more inclined to churning. This info can be leveraged by Marketing team, that can use this to better adjust their advertisements to less incentisive senior audience and rather focus on non-seniors."},{"metadata":{"trusted":true},"cell_type":"code","source":"table=pd.crosstab(data.tenure,data.Churn)\ntable.div(table.sum(1).astype(float), axis=0).plot(kind='bar',figsize=(20,5),title='Churn distribution based on tenure',stacked=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"This is the closest we can come to retention from this dataset. \n\nIt's not very accurate, because we don't know the whole starting cohorts, but nonetheless it's very important to understand retention of your customers, mainly to be able to estimate lifetime value of your custome. The biggest advantage of this dataset is that the setting between customer and company is contractual, so there are no silent churns, which greatly improves labeling and also it provides opportunities to gather feedback from customers on why they've decided to churn. \n\nThis would be very useful data for future modelling."},{"metadata":{"trusted":true},"cell_type":"code","source":"table=pd.crosstab(data.Contract,data.Churn)\ntable.div(table.sum(1).astype(float), axis=0).plot(kind='bar',figsize=(10,5),title='Churn distribution based on contract type',stacked=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This graph confirms intuition that the customer is likely to churn with longer term contract. I will work of an assumption that you can cancel only after your contract expires. \n\nIt's an obvious information, but having it reinforced with data and graphs we can communicate to Product/Sales to really push for long-term contracts. And even differencee between Onee year and Two year options looks really significant !"},{"metadata":{"trusted":true},"cell_type":"code","source":"table=pd.crosstab(data.PaymentMethod,data.Churn)\ntable.div(table.sum(1).astype(float), axis=0).plot(kind='bar',figsize=(10,5),title='Churn distribution based on payment method',stacked=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The payment method that really stands out here is using Electronic check. \n\nAnd it's so significant that it's worth looking at more closely, to understand who are these customers."},{"metadata":{"trusted":true},"cell_type":"code","source":"ec_df=data.loc[data.PaymentMethod == 'Electronic check',]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ec_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.countplot(x='InternetService',data=ec_df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.countplot(x='MultipleLines',data=ec_df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.countplot(x='Contract',data=ec_df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By summarizing the 3 above graphs and the table, we can make a hypothesis that customers, who prefer paying by non-automated Electronic check, tend to do it on really short term contracts and prefer more expensive (my assumption) services, therefore driving up the monthly charge.\n\nHere it's very important to understand the product positioning on the market in a sense how does this particular company's Fiber+MultipleLines compare to competitors. Is the pricing competitive? Since the mean MonthlyCharges are significantly higher in this group than in rest of dataset? What are their cancellation policies? "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.scatterplot(x=\"MonthlyCharges\", y=\"TotalCharges\", hue=\"Churn\",data=data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that increased monthly chargers bring in more revenue, but they also tend to increase churn. Similar action points can be drawn here as with the previous analysis on Electronic check data."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.scatterplot(x=\"tenure\", y=\"MonthlyCharges\", hue=\"Churn\",\n                     data=data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The main info from this graph, can be that there seems to be kind of a safe Monthly Charge rate, that doesn't indicate churn across all tenures. \n\nTo better see this line let's look at the Mothly Charges a bit differently."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nax = sns.kdeplot(data.MonthlyCharges[(data[\"Churn\"] == 'No') ],\n                color=\"Blue\", shade = True)\nax = sns.kdeplot(data.MonthlyCharges[(data[\"Churn\"] == 'Yes') ],\n                ax =ax, color=\"Orange\", shade= True)\nax.legend([\"Retained\",\"Churned\"],loc='upper right')\nax.set_ylabel('Density')\nax.set_xlabel('Monthly Charges')\nax.set_title('Distribution of monthly charges by churn')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that a lot of users, who churned, were paying above 60 USD in Monthly Charges and on the other hand the most prevalent Monthly Charges group in retained customers is spending around 20 USD monthly."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nax = sns.kdeplot(data.TotalCharges[(data[\"Churn\"] == 'No') ],\n                color=\"Blue\", shade = True)\nax = sns.kdeplot(data.TotalCharges[(data[\"Churn\"] == 'Yes') ],\n                ax =ax, color=\"Orange\", shade= True)\nax.legend([\"Retained\",\"Churned\"],loc='upper right')\nax.set_ylabel('Density')\nax.set_xlabel('Total Charges')\nax.set_title('Distribution of total charges by churn')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is look at another really important metric, that I've already mentioned - Customer lifetime value (CLV).\n\nSame as with retention, this number is not as precise as I would like, but it gives us a good idea. \n\nThe company is not getting much revenue, from customers, as it seems that they are canceling their contracts early. Again Product team can have a look at starting packages for customers and how they stack-up, if there are any."},{"metadata":{},"cell_type":"markdown","source":"# Data processing <a id=\"3\"></a>"},{"metadata":{},"cell_type":"markdown","source":"After exploring data, it's time to start building models. \n\nFirst in order is to prepare data, so they are better suited for models, I will later user.\n\nFirst I will remove the user_ID column, since the data set is one row per one customer, so there is no need to aggregate data here.\n\nI assume here, that the user_ID is assigned only once to each user and if they cancel and rejoin, they go back to their respective ID."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_mod=data.iloc[:,1::]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## One-hot encoding and scaling of data <a id=\"3.1\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Since there are a lot of categorical data, I have opted for one-hot encoding the features, as the numerical input is desirable for most common models. \n\nAlso there was an option to encode the categorical data in random order, but I'm not really a big fan of this approach as assigning a certain value to certain feature  (i.e. saying Electric Check = 1 vs Electric check = 4) value can have an impact on final coefficients and adds need for more validation of a model.\n\nBiggest problem with one-hot encoding is that it creates additional columns, which in turn increases linearity in a model, which can lead to increased overfitting. That's why I have decided to reduce the number of new columns where possible.\n\nI'm looking for binary columns, because those columns can be easily transformed to 1 column, by dropping one value and whole information will still be contained in the kept column."},{"metadata":{"trusted":true},"cell_type":"code","source":"bin_cols = data.columns[data.nunique() == 2] #columns with binary values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dumm_bin=pd.get_dummies(data_mod,columns=bin_cols,drop_first=True) #dropping one column, because for binary one-hot encoding it's not necessary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dumm=pd.get_dummies(data_dumm_bin) #one hot encoding the rest of the dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dumm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dumm.groupby('Churn_Yes').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data_dumm['Churn_Yes'].values\nX = data_dumm.drop(columns = ['Churn_Yes'])\n\n\nfeatures = X.columns.values\nscaler = MinMaxScaler(feature_range = (0,1)) #It's important to scale your values, because the models used are not robust in relation to scales used, meaning that \n                                             #features with bigger scales, would skew the ceofficient weights to them.\nscaler.fit(X)\nX = pd.DataFrame(scaler.transform(X))\nX.columns = features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data oversampling - SMOTE <a id=\"3.2\"></a>"},{"metadata":{},"cell_type":"markdown","source":"As we could see in the first graph of this notebook this dataset is imbalanec - there is roughly 3:1 ratio of retained to churned users. \n\nHaving a balanced dataset always reduces the difficulty of modelling tasks. \n\nI have decided to use SMOTE oversampling, because it creates new users in manner that there might be new observations, that wouldn't be there if I just went with regular over-/undersampling.\n\nOne potentional pitfall with SMOTE and this particular dataset is that I'm using a lot of binary values. The way SMOTE works is that it connects two random points from dataset and then randomly selects a point in between them, so this will create for example 0.7 values in a binary feature. \n\nThe effect of this is then showcased in really good training set metrics, but the will perform poorly on validation (test) set. \n\nBut in my experience it was always a good starting point."},{"metadata":{"trusted":true},"cell_type":"code","source":"os = SMOTE(random_state=10)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10) #splitting data into training and testing, reduces overfitting and better \n                                                                                          #indicates performance of a model on live (unseen) data.\ncolumns = X_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os_data_X,os_data_y=os.fit_sample(X_train, y_train)\nos_data_X = pd.DataFrame(data=os_data_X,columns=columns )\nos_data_y= pd.DataFrame(data=os_data_y,columns=['Churn_Yes'])\nprint(\"length of oversampled data is \",len(os_data_X))\nprint(\"Number of churned users in oversampled data\",len(os_data_y[os_data_y['Churn_Yes']==0]))\nprint(\"Number of retained\",len(os_data_y[os_data_y['Churn_Yes']==1]))\nprint(\"Proportion of churned users in oversampled data is \",len(os_data_y[os_data_y['Churn_Yes']==0])/len(os_data_X))\nprint(\"Proportion of retained users in oversampled data is \",len(os_data_y[os_data_y['Churn_Yes']==1])/len(os_data_X))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature selection <a id=\"3.3\"></a>"},{"metadata":{},"cell_type":"markdown","source":"As mentioneed before, biggest issue with introducing more features is increasing linearity in model, which in turn negatively impacts interpretability (which is really important in this usecase) of the model. \n\nRule of thumb I've learned is that for every 1 feature you need 10 data points, which is more than sufficiently satisfied here, but number of features also impacts the speed at which we train model and that can be problematic if we want to deploy this model on real-time data."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dumm_vars=data_dumm.columns.values.tolist()\ny=['Churn_Yes']\nX=[i for i in data_dumm_vars if i not in y]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cor_selector(X, y,num_feats):\n    cor_list = []\n    feature_name = X.columns.tolist()\n    # calculate the correlation with y for each feature\n    for i in X.columns.tolist():\n        cor = np.corrcoef(X[i], y.iloc[:,0])[0, 1]\n        cor_list.append(cor)\n    # replace NaN with 0\n    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n    # feature name\n    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist()\n    # feature selection? 0 for not select, 1 for select\n    cor_support = [True if i in cor_feature else False for i in feature_name]\n    return cor_support, cor_feature\ncor_support, cor_feature = cor_selector(os_data_X, os_data_y,10) #here I'm just reducing the number of features to 25% of starting values. This is one of the parameters,\n                                                                 #that can be tuned later.\nprint(str(len(cor_feature)), 'selected features')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols=cor_feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=os_data_X[cols]\ny=os_data_y['Churn_Yes']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fitting model <a id=\"4\"></a>"},{"metadata":{},"cell_type":"markdown","source":"For a starting model I have originally decided to go with Logistic regression, but there was a problem in a sense that it would converge, but the R^2 was inf, function value was inf etc. and solving this problem would require significant efforts, that are not warranted in my opinion, for this problem.\n\nAlso you can notice that I'm not using sklearn library for this part of modelling. The reason is simple - statsmodels has implemented report with coefficients, that's very similar to R's reports, that are very good for infering about features' impact on predicted variable.\n\nIt's **VERY IMPORTANT** to note that in this dataset churned customer is labeled as 1. That means when we are looking at coefficients we are looking for negative valueed coefficients as those positively impact retention. "},{"metadata":{"trusted":true},"cell_type":"code","source":"glm_model=sm.GLM(y,X) \nresult=glm_model.fit()\nprint(result.summary2())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As I've mentioned this report is really clean way of seeing, how each feature influences the target label. \n\nWe can see that all except the tenure are positive, which means they are generating churn and are something that should be looked at.\n\nAlso it's important to  note that there is 1 feature where p>0.05, so I will remove that one and refit model. \n\nAlso tenure is not something, that the company can directly influence and is byproduct of customers hapiness, so I will be removing that as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_red=['TechSupport_No internet service', 'DeviceProtection_No',\n       'OnlineBackup_No', 'InternetService_Fiber optic',\n       'PaymentMethod_Electronic check', 'TechSupport_No',\n       'OnlineSecurity_No', 'Contract_Month-to-month']\n\nX=os_data_X[cols_red]\ny=os_data_y['Churn_Yes']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glm_model=sm.GLM(y,X)\nresult=glm_model.fit()\nprint(result.summary2())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we have only columns with p<0.05 so we can say that the features are important. \n\nWhat can be said about those features ? \n* **One is related to contract length**. The strongest predictor of churn is Month-to-month contract.\n* **Fiber optic** product is also a strong indicator of churn. We might need to look performance of this product (What do customers expect when they buy this?) and how does it stack up against competitors.\n* **4 missing products** \n* **One payment method**\n* **No internet service**\n\nThis info can be forwarded to Product/Sales team in order to improve current offerings to include those products for new customers and for existing customers, that are missing these products and later models identify those customers as 'at risk of churning', we can offer them these products in new deals.\n\nNow let's see how good is the Logistic regression at predicting possible Churn !"},{"metadata":{},"cell_type":"markdown","source":"## Model evaluation - Accuracy, Confusion matrix, ROC-AUC score <a id=\"4.1\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nlogreg = LogisticRegression(max_iter=4000)\nlogreg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What metrics should we use for evaluating the models?\n\nI'm demostrating the evaluation on 3 metrics:\n* Accuracy\n* Confusion matrix and related metrics\n* Area under curve for receiver operating characteristic \n\nWhy am I not using only accuracy? Well the biggest problem with standalone accuracy is that with imbalanced dataset, if the model labels everything as the major group, then it will have high accuracy, but we will miss the most valuable targets. \n\nConfusion matrix, helps exactly with this as we will see how many false negatives/positives did the model produce.\n\nAUC ROC is a metric that tells us how good is our model at distinguishing between labels, based on probability outputs from the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = logreg.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Considering the simplicity of this model, 76% accuracy on testing set isn't that bad. Problem is that it's roughly the ratio of retained to all users, so simply assuming we retain everyone would give the same result and that model is useless."},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix = confusion_matrix(y_test, y_pred)\nprint(confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that model was able to classify 592 churned users and definitly didn't label everyone as retained !"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logit_roc_auc = roc_auc_score(y_test, logreg.predict_proba(X_test)[:,1])\nfpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\nplt.figure(figsize=(10,5))\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also AUC ROC of 0.84 (max is 1) is really prosiming!"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Adding weights to logistic regression <a id=\"4.2\"></a>"},{"metadata":{},"cell_type":"markdown","source":"One of the ways that I think I can improve this model is by making it more vary about churned class. \n\nFrom Economics theory it's known that retaining a customer is way chaper than signing a new one. By how much? Well I don't have the exact number, but it's something that can be provided by internal Sales team. \n\nSo I will randomly generate cost of signing new customer to be 5-20 times more than keeping and existing customer."},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = []\nks=[]\nfor i in range(1,40):\n    k=np.random.uniform(0.05,0.2)\n    logreg = LogisticRegression(class_weight={0:k,1:1-k},max_iter=4000)\n    logreg.fit(X_train, y_train)\n    score = roc_auc_score(y_test, logreg.predict_proba(X_test)[:,1])\n    scores.append(score)\n    ks.append(k)\n\nplt.figure(figsize=(10,5))\nplt.plot(range(1, 40), scores,marker='o', markerfacecolor='red', markersize=5)\nplt.xlabel('Iteration')\nplt.ylabel('ROC-AUC')\nplt.title('Model response to using weights of neighbors')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see from the graph that the model is really indifferent to using weights, which could be cause by balancing the dataset beforehand."},{"metadata":{},"cell_type":"markdown","source":"## Using different classifiers <a id=\"4.3\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Since it's really easy to use different models, without need of implementation, let's try different classifier and see how they stack against the Logistic Regression.\n\nThe way I imagine this could work is that, we gather information about feature importance from more interpretable models (Logistic Regression, Bayesian models) and then we use more sophisticated, but less interpretable (although there are methods like LIME, SHAP) models to identify potential customers at risk of churn. \n\nIt's important to note here, that I have various degree of experience with these classifier (ranging from theoretical to using on multiple occasions)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.dummy import DummyClassifier\n\nclassifiers = {\n    \"Dummy\"        : DummyClassifier(strategy='uniform', random_state=2),\n    \"KNN(3)\"       : KNeighborsClassifier(3), \n    \"RBF SVM\"      : SVC(gamma=2, C=1,probability=True), \n    \"Decision Tree\": DecisionTreeClassifier(max_depth=7), \n    \"Random Forest\": RandomForestClassifier(max_depth=7, n_estimators=10, max_features=8), \n    \"Neural Net\"   : MLPClassifier(alpha=1), \n    \"AdaBoost\"     : AdaBoostClassifier(),\n    \"Naive Bayes\"  : GaussianNB(), \n    \"QDA\"          : QuadraticDiscriminantAnalysis(),\n    \"Gaussian Proc\": GaussianProcessClassifier(1.0 * RBF(1.0)),\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from time import time\nnfast = 9      # Not running the Gaussian Process, because it's very very slow method\nhead = list(classifiers.items())[:nfast]\n\nfor name, classifier in head:\n    start = time()\n    classifier.fit(X, y)\n    train_time = time() - start\n    start = time()\n    score = roc_auc_score(y_test, classifier.predict_proba(X_test)[:,1])\n    score_time = time()-start\n    print(\"{:<15}| ROC-AUC score = {:.3f} | time = {:,.3f}s/{:,.3f}s\".format(name, score, train_time, score_time))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there is a significant improvement when using Random Forest Classifier, so let's see if we can get more out of that one. \n\nAlso when I was running this against the accuracy prediction, the KNN Clasifier was performing really well, so let's look at that one as well."},{"metadata":{},"cell_type":"markdown","source":"## Hyperparameter tuning for KNN an RandomForest  <a id=\"4.4\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = []\nfor k in range(1, 40):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X, y)\n    score = roc_auc_score(y_test, knn.predict_proba(X_test)[:,1])\n    scores.append(score)\n    \nscores = pd.Series(scores, index=range(1,40), name=\"Score\")\nscores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's hard to make an inference about parameter, just from seeing it printed, so let's look at it through graph."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.plot(range(1, 40), scores,marker='o', markerfacecolor='red', markersize=5)\nplt.xlabel('n_neighbors')\nplt.ylabel('ROC-AUC')\nplt.title('Model response to number of neighbors')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the models is indiferent to number of neighbors used for classification and it only slightly improves, peaking at 11.\n\nNow let's look at the number of trees used for predictions in Random Forest. \n\nNote : If someone reading this isn't familiar with RF - it's an ensemble method, that has multiple Decision Trees and result is a weighted vote on the outputs of those Trees."},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = []\nfor k in range(1, 300, 10):\n    RFC = RandomForestClassifier(max_depth=7, n_estimators=k, max_features=8)\n    RFC.fit(X, y)\n    score = roc_auc_score(y_test, RFC.predict_proba(X_test)[:,1])\n    scores.append(score)\n    \nscores = pd.Series(scores, range(1, 300, 10), name=\"Score\")\nscores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.plot(range(1, 300, 10), scores,marker='o', markerfacecolor='red', markersize=5)\nplt.xlabel('n_neighbors')\nplt.ylabel('ROC-AUC')\nplt.title('Model response to number of trees')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Same as with neighbors KNN, this parameter doesn't seem to influence the model quality. \n\nThese were just 2 examples of parameters we can tune. Also I'm tuning them here in isolation. There are methods i.e. CVGridSearch, that look at multiple values of parameters at the same time and tune them according to specified metric, but implementing those methods is out of scope of this assignement in my opinion."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Summary <a id=\"5\"></a>"},{"metadata":{},"cell_type":"markdown","source":"After applying data science techniques I was able to identify possible causes of churns in products/services provided. \n\nAlso the team would be provided by list of customers, that are most likely to churn and they can cross-reference the list of their services with aforementioned list, to mitigate churn.\n\nSince this is a contractual setting, it would be very helpful to try and gather more info at the time of churn.\n\nThe way I see the models' improvement would be through better hyperparameter tuning and through better feature engineering, which I haven't performed and would like to as part of the next iteration. \nFor example there would be a feature that would tell if TotalCharges is tenure * MonthlyCharges, because if the TotalCharges were higher, it might indicate purchasing some items over limit, which in my experience is very negative experience.\nNext would be interactions between combinations of services, meaning there would be a variable that would indicate if user has a Fibre+Multiple lines at the same time. "},{"metadata":{},"cell_type":"markdown","source":"# References "},{"metadata":{},"cell_type":"markdown","source":"During the completion of this notebook I have used parts of code from these kaggle notebooks :\n\nhttps://www.kaggle.com/bandiatindra/telecom-churn-prediction\n\nhttps://www.kaggle.com/nicholasgah/churn-prediction-model-and-cap-curve"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}