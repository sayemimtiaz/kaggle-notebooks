{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. Introduction: Business Goal & Problem Definition\n\nIF YOU LIKE IT OR IF IT HELPS YOU SOMEHOW, COULD YOU PLEASE UPVOTE? THANK YOU VERY MUCH!!!\n\nThe goal of this project is to identify, study and analyze movies clustering, so the movie industry can have a better understanding of the customers segmentations according to their movies preferences and adapt different marketing strategies to each of them, bringing more revenue to the business. For that we´ll use the Movie Industry dataset available in Kaggle, containing 6820 movies (220 movies per year, 1986-2016). Each movie has the following attributes:\n\n* budget: the budget of a movie. Some movies don't have this, so it appears as 0\n* company: the production company\n* country: country of origin\n* director: the director\n* genre: main genre of the movie.\n* gross: revenue of the movie\n* name: name of the movie\n* rating: rating of the movie (R, PG, etc.)\n* released: release date (YYYY-MM-DD)\n* runtime: duration of the movie\n* score: IMDb user rating\n* votes: number of user votes\n* star: main actor/actress\n* writer: writer of the movie\n* year: year of release"},{"metadata":{},"cell_type":"markdown","source":"# 2. Importing Basic Libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import io\nimport openpyxl\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Data Collection"},{"metadata":{"trusted":true},"cell_type":"code","source":"movies_ds = pd.read_csv('../input/movies/movies.csv', encoding='latin1', sep=\",\")\n\nmovies_ds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Data Preliminary Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking a dataset sample\n\npd.set_option(\"display.max_rows\", 100)\npd.set_option(\"display.max_columns\", 100)\npd.options.display.float_format=\"{:,.2f}\".format\nmovies_ds.sample(n=10, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking dataset info by feature\n\nmovies_ds.info(verbose=True, null_counts=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the existence of zeros in rows\n\n(movies_ds==0).sum(axis=0).to_excel(\"zeros_per_feature.xlsx\")\n(movies_ds==0).sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the existence of duplicated rows\n\nmovies_ds.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking basic statistical data by feature\n\nmovies_ds.describe(include=\"all\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Data Cleaning\n\n    We´ll perform the following:\n    \n    \n    1. I noticed budget is zero in 2182 observations, so we´ll treat it, making them proportional to \"gross\" since they are correlated\n\n\n    2. I noticed there are 309 ratings as NOT RATED, UNRATED or Not Specified; since it´s not a significant amount those rows will be deleted\n\n\n    3. Create a feature (gross_to_budget_ratio) to analyze the revenue to budget ratio relevance in the model\n\n\n    4. Keep only the most relevant features for our clustering purpose (budget, country, genre, gross, rating, runtime, score, year), so we make the model easier to interpret, we reduce the training time, avoid curse of dimensionality and reduce overfitting (OCCAM´S RAZOR)\n    \n    \n    5. Convert categorical variables (country, genre, rating) to dummies\n    \n    \n    * No duplications found\n    * No outliers found"},{"metadata":{"trusted":true},"cell_type":"code","source":"#1\n\nmovies_ds[\"budget\"].replace(0, np.nan, inplace=True)\nmovies_ds[\"budget\"].fillna(movies_ds[\"budget\"].sum() / movies_ds[\"gross\"].sum() * movies_ds[\"gross\"], inplace=True)\n\n#2\n\nmovies_ds = movies_ds[~movies_ds[\"rating\"].isin([\"NOT RATED\", \"UNRATED\", \"Not specified\"])]\n\n#3\n\nmovies_ds[\"gross_to_budget_ratio\"] = movies_ds[\"gross\"] / movies_ds[\"budget\"] #feature engineering\n\n#4\n\nmovies_ds = movies_ds[[\"budget\", \"country\", \"genre\", \"gross\", \"rating\", \"runtime\", \"score\", \"year\", \"gross_to_budget_ratio\"]] #keeping only the most relevant features\n\n#5\n\n# movies_ds = pd.concat([movies_ds, pd.get_dummies(movies_ds[\"country\"])], axis=1) #country dummy coding (we´re skipping this line since it woulc generate a 87 columns dataset and we don´t want to make complex the problem explanation to the business in this example)\nmovies_ds = pd.concat([movies_ds, pd.get_dummies(movies_ds[\"genre\"])], axis=1) #genre dummy coding\nmovies_ds = pd.concat([movies_ds, pd.get_dummies(movies_ds[\"rating\"])], axis=1) #rating dummy coding\n\nmovies_ds.to_excel(\"movies_ds_clean.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting Categorical Variables\n\nfig, ax = plt.subplots(1, 2)\nmovies_ds[\"country\"].value_counts().plot.bar(color=\"purple\", ax=ax[0])\nmovies_ds[\"country\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[1])\nfig.suptitle(\"Country Frequency\", fontsize=15)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\nfig, ax = plt.subplots(1, 2)\nmovies_ds[\"genre\"].value_counts().plot.bar(color=\"purple\", ax=ax[0])\nmovies_ds[\"genre\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[1])\nfig.suptitle(\"Genre Frequency\", fontsize=15)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\nfig, ax = plt.subplots(1, 2)\nmovies_ds[\"rating\"].value_counts().plot.bar(color=\"purple\", ax=ax[0])\nmovies_ds[\"rating\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[1])\nfig.suptitle(\"Rating Frequency\", fontsize=15)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\n\n#Plotting Numerical Variables\n\nfig, ax = plt.subplots(1, 3)\nfig.suptitle(\"Budget Distribution\", fontsize=15)\nsns.distplot(movies_ds[\"budget\"], ax=ax[0])\nsns.boxplot(movies_ds[\"budget\"], ax=ax[1])\nsns.violinplot(movies_ds[\"budget\"], ax=ax[2])\n\nfig, ax = plt.subplots(1, 3)\nfig.suptitle(\"Gross Revenue Distribution\", fontsize=15)\nsns.distplot(movies_ds[\"gross\"], ax=ax[0])\nsns.boxplot(movies_ds[\"gross\"], ax=ax[1])\nsns.violinplot(movies_ds[\"gross\"], ax=ax[2])\n\nfig, ax = plt.subplots(1, 3)\nfig.suptitle(\"Runtime Distribution\", fontsize=15)\nsns.distplot(movies_ds[\"runtime\"], ax=ax[0])\nsns.boxplot(movies_ds[\"runtime\"], ax=ax[1])\nsns.violinplot(movies_ds[\"runtime\"], ax=ax[2])\n\nfig, ax = plt.subplots(1, 3)\nfig.suptitle(\"Score Distribution\", fontsize=15)\nsns.distplot(movies_ds[\"score\"], ax=ax[0])\nsns.boxplot(movies_ds[\"score\"], ax=ax[1])\nsns.violinplot(movies_ds[\"score\"], ax=ax[2])\n\nfig, ax = plt.subplots(1, 3)\nfig.suptitle(\"Year Distribution\", fontsize=15)\nsns.distplot(movies_ds[\"year\"], ax=ax[0])\nsns.boxplot(movies_ds[\"year\"], ax=ax[1])\nsns.violinplot(movies_ds[\"year\"], ax=ax[2])\n\nfig, ax = plt.subplots(1, 3)\nfig.suptitle(\"Gross to Budget Distribution\", fontsize=15)\nsns.distplot(movies_ds[\"gross_to_budget_ratio\"], ax=ax[0])\nsns.boxplot(movies_ds[\"gross_to_budget_ratio\"], ax=ax[1])\nsns.violinplot(movies_ds[\"gross_to_budget_ratio\"], ax=ax[2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Correlations Analysis & Features Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Deleting original categorical columns\n\nmovies_ds2 = movies_ds.drop([\"country\", \"genre\", \"rating\"], axis=1)\n\n# #Plotting a Heatmap\n\n# fig, ax = plt.subplots(1, figsize=(25,25))\n# sns.heatmap(movies_ds2.corr(), annot=True, fmt=\",.2f\")\n# plt.title(\"Heatmap Correlation\", fontsize=20)\n# plt.tick_params(labelsize=12)\n# plt.xticks(rotation=90)\n# plt.yticks(rotation=45)\n\n# #Plotting a Pairplot\n\n# sns.pairplot(movies_ds2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8. Data Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Defining Xs\n\nX_orig = movies_ds\nX = movies_ds2\n\n#Scaling all features\n\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_scaled = sc_X.fit_transform(X)\nX_scaled = pd.DataFrame(X_scaled)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 9. Dimensionality Reduction"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying PCA\n\nfrom sklearn.decomposition import PCA\n\n#Creating a model\npca = PCA(n_components=X_scaled.shape[1], random_state=0) #there are 18 features at the dataset\n\n#Fitting to the model\npca.fit(X_scaled)\n\n#Generating all components in an array\nX_pca = pca.transform(X_scaled)\n# X_pca_output = pd.DataFrame(X_pca)\n# X_pca_output.to_excel(\"X_pca_file.xlsx\",index=False)\n\n#Displaying the explained variance by number of components\nfor n in range(0, X_scaled.shape[1]):\n    print(f\"Variance explained by the first {n+1} principal components = {np.cumsum(pca.explained_variance_ratio_ *100)[n]:.1f}%\")\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel(\"Number of components\")\nplt.ylabel(\"Explained variance\")\n\n#Creating a model with the chosen number of components (#75% explainability = 20 components)\npca_selected = PCA(n_components=20, random_state=0)\npca_selected.fit(X_scaled)\nX_pca_selected = pca_selected.transform(X_scaled)\n# X_pca_selected_output = pd.DataFrame(X_pca_selected)\n# X_pca_selected_output.to_excel(\"X_pca_selected_file.xlsx\",index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 10. Machine Learning Algorithms Implementation & Assessment"},{"metadata":{},"cell_type":"markdown","source":"# 10.1 K-means"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a K-means model and checking its Metrics\n\nfrom sklearn.cluster import KMeans\n\n#Applying the Elbow Method to calculate distortion for a range of number of cluster\n\ndistortions = []\nfor i in range(1, 21):\n    km = KMeans(n_clusters=i, init=\"random\", n_init=10, max_iter=300, tol=1e-04, random_state=0)\n    km.fit(X_pca_selected)\n    distortions.append(km.inertia_)\n\n#Plotting\n\nplt.plot(range(1, 21), distortions, marker=\"o\")\nplt.xlabel(\"Number of clusters\")\nplt.ylabel(\"Distortion\")\nplt.show()\n\n#Applying the Silhouette Method to interpret and validate of consistency within clusters of data\n\nfrom sklearn.metrics import silhouette_score\nsilhouette_coefficients = []\nfor j in range(2, 21):\n    km = KMeans(n_clusters=j, init=\"random\", n_init=10, max_iter=300, tol=1e-04, random_state=0)\n    km.fit(X_pca_selected)\n    score = silhouette_score(X_pca_selected, km.labels_)\n    silhouette_coefficients.append(score)\n\n#Plotting\n\nplt.style.use(\"fivethirtyeight\")\nplt.plot(range(2, 21), silhouette_coefficients)\nplt.xticks(range(2, 21))\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"Silhouette Coefficient\")\nplt.show()\n\n#Choosing number of clusters\n\nn_clusters = 17\nprint('Estimated number of clusters: %d' % n_clusters)\nkm = KMeans(n_clusters=n_clusters)\nkm.fit(X_pca_selected)\nprint(\"Silhouette Coefficient: %0.3f\" % silhouette_score(X_pca_selected, km.fit(X_pca_selected).labels_))\n\n#Plotting chosen number of clusters\n\nfrom yellowbrick.cluster import silhouette_visualizer\nsilhouette_visualizer(KMeans(n_clusters=n_clusters, random_state=0), X_pca_selected)\n\n#Visualizing clusters in the dataset\nX_orig = pd.DataFrame(X_orig)\nX_orig[\"cluster\"] = km.labels_\nX_orig.to_excel(\"model_km.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #Plotting scatter graph per pair features\n\n# #Mapping every individual cluster to a color\n\n# colors = ['royalblue', 'mediumorchid', 'tan', 'deeppink', 'olive', 'goldenrod', 'lightcyan', 'navy']\n\n# vectorizer = np.vectorize(lambda x: colors[x % len(colors)])\n\n# #Plotting\n\n# for i in range(1, X_pca_selected.shape[1]-1):\n#     plt.scatter(X_pca_selected.iloc[:,0], X_pca_selected.iloc[:,i], c=vectorizer(clusters))\n#     plt.xlabel(X_pca_selected.columns[0])\n#     plt.ylabel(X_pca_selected.columns[i])\n#     plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 10.2 DBSCAN"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a DBSCAN model and checking its Metrics\n#OBS: we´re exploring DBSCAN only as a study exercise in this project - we´ll adopt K-Means\n\nfrom sklearn.neighbors import NearestNeighbors\n\n#We can calculate the distance from each point to its closest neighbour using the NearestNeighbors. The point itself is included in n_neighbors. The kneighbors method returns two arrays, one which contains the distance to the closest n_neighbors points and the other which contains the index for each of those points\n\nneigh = NearestNeighbors(n_neighbors=2)\nnbrs = neigh.fit(X_pca_selected)\ndistances, indices = nbrs.kneighbors(X_pca_selected)\n\n#Soring and plotting results\n\ndistances = np.sort(distances, axis=0)\ndistances = distances[:,1]\nplt.plot(distances)\nplt.xlabel(\"Distances to the closest n_neighbors\")\nplt.ylabel(\"eps\")\nplt.show()\n\nfrom sklearn.cluster import DBSCAN\n\n#Selecting the best eps (the optimal value for epsilon will be found at the point of maximum curvature)\n\ndbs = DBSCAN(eps=10)\ndbs.fit(X_pca_selected)\n\n#The labels_ property contains the list of clusters and their respective points\n\nclusters = dbs.labels_\n\nfrom sklearn import metrics\n\n#Number of clusters in labels, ignoring noise (outlier) (-1) if present\n\nn_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)\nn_noise_ = list(clusters).count(-1)\nprint('Estimated number of clusters: %d' % n_clusters)\nprint('Estimated number of noise points: %d' % n_noise_)\nprint(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(X_pca_selected, clusters))\n\n#Visualizing clusters in the dataset\nX_orig = pd.DataFrame(X_orig)\nX_orig[\"cluster\"] = dbs.labels_\nX_orig.to_excel(\"model_dbs.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 11. Conclusions\n\nIF YOU LIKE IT OR IF IT HELPS YOU SOMEHOW, COULD YOU PLEASE UPVOTE? THANK YOU VERY MUCH!!!\n\nIn this exercise we went through all the process from collecting data, exploring features and distributions, treating data, understanding correlations, selecting relevant features, data modelling and presenting a clustering model, indicating groups of movies with similarities to be further developed and explored, so the movie industry can have a better understanding of the customers segmentations according to their movies preferences and adapt different marketing strategies to each of them, bringing more revenue to the business."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}