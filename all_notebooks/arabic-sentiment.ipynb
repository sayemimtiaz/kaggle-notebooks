{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Arabic Sentiment using Tensorflow","metadata":{}},{"cell_type":"markdown","source":"Text -> Numaric representation -> Model\n# First Approach using Vector Space Model VSM\nin two steps:\n* document vector and remove stopword\n* convert document vector to numbers by:\n1. Term Frequency TF\n2. Invers Document Frequency IDF\n\nIDF=Log(N/DF)+1 Number of documents\n","metadata":{}},{"cell_type":"markdown","source":"first approach has some limitations:\n1. dictionary size (dimensions) can became huge\n2. context not preserved","metadata":{}},{"cell_type":"markdown","source":"the solution: using Universal Sentence Encoder (USE). it will be the second approach ","metadata":{}},{"cell_type":"markdown","source":"    Generating features from Text by: Vector space model (VSM)\n    Including Inverse Document frequency (IDF)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import regularizers\nimport operator\nimport math\nfrom functools import reduce\nfrom sklearn.model_selection import train_test_split\nimport os\nimport json\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-15T10:19:19.010114Z","iopub.execute_input":"2021-07-15T10:19:19.010404Z","iopub.status.idle":"2021-07-15T10:19:24.691079Z","shell.execute_reply.started":"2021-07-15T10:19:19.010338Z","shell.execute_reply":"2021-07-15T10:19:24.690208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/reviews/5556-ar-reviews.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-15T10:19:24.692828Z","iopub.execute_input":"2021-07-15T10:19:24.693438Z","iopub.status.idle":"2021-07-15T10:19:24.915321Z","shell.execute_reply.started":"2021-07-15T10:19:24.693389Z","shell.execute_reply":"2021-07-15T10:19:24.914383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['label'].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T10:19:24.919454Z","iopub.execute_input":"2021-07-15T10:19:24.919794Z","iopub.status.idle":"2021-07-15T10:19:24.947774Z","shell.execute_reply.started":"2021-07-15T10:19:24.919759Z","shell.execute_reply":"2021-07-15T10:19:24.946802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.sample(frac = 1) ","metadata":{"execution":{"iopub.status.busy":"2021-07-15T10:19:24.951597Z","iopub.execute_input":"2021-07-15T10:19:24.953654Z","iopub.status.idle":"2021-07-15T10:19:24.96374Z","shell.execute_reply.started":"2021-07-15T10:19:24.953601Z","shell.execute_reply":"2021-07-15T10:19:24.96185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Process Data","metadata":{}},{"cell_type":"code","source":"stopwords = {'فإذا', 'أنى', 'بمن', 'حتى', 'لم', 'أنتما', 'هناك', 'تينك', 'بل', 'إي', 'عن', 'ولكن', 'وإذا', 'دون', 'إنا', 'إذن', 'بكم', 'حين', 'عند', 'هل', 'إلا', 'هاته', 'ذينك', 'اللواتي', 'كذا', 'لستما', 'هي', 'اللتان', 'أكثر', 'كلتا', 'لكن', 'ليستا', 'هكذا', 'عسى', 'إذ', 'إن', 'اللاتي', 'إذا', 'بهم', 'نحن', 'فيما', 'ذاك', 'بكن', 'بيد', 'لهن', 'هذي', 'كأي', 'ذوا', 'أي', 'كلاهما', 'هذين', 'أينما', 'كي', 'إليكن', 'ماذا', 'هيا', 'هنالك', 'بي', 'بما', 'تلكما', 'بعض', 'بهن', 'تين', 'ريث', 'على', 'غير', 'حيثما', 'كأن', 'بخ', 'هاتان', 'هاهنا', 'ما', 'هيهات', 'لدى', 'شتان', 'لسنا', 'كيفما', 'مع', 'ممن', 'كما', 'إنما', 'يا', 'عليه', 'لك', 'ذه', 'ذان', 'لهما', 'ليست', 'لنا', 'مه', 'أنتن', 'في', 'لولا', 'بس', 'لها', 'أقل', 'عليك', 'فلا', 'مهما', 'ليسا', 'ذين', 'ذات', 'كلما', 'ذا', 'ذو', 'فيه', 'تي', 'هنا', 'هاتين', 'ها', 'هم', 'ألا', 'لا', 'سوى', 'وإذ', 'كم', 'لست', 'حيث', 'إليكما', 'لوما', 'الذين', 'كلا', 'التي', 'كأين', 'ذواتي', 'لستم', 'هذا', 'فمن', 'ذلكم', 'وما', 'كيف', 'لكم', 'حاشا', 'بك', 'والذي', 'أن', 'لهم', 'لسن', 'ثمة', 'ذي', 'وإن', 'ومن', 'أيها', 'له', 'متى', 'بلى', 'اللتين', 'لستن', 'بكما', 'قد', 'كليكما', 'لكما', 'هلا', 'آي', 'لكنما', 'اللذين', 'اللائي', 'ذلكن', 'لاسيما', 'ذلك', 'مذ', 'اللتيا', 'هما', 'إليك', 'سوف', 'منها', 'والذين', 'أنتم', 'هاتي', 'لكي', 'اللذان', 'ذواتا', 'عما', 'فيها', 'إلى', 'تلك', 'كل', 'لي', 'هو', 'فيم', 'إليكم', 'بها', 'ذانك', 'إنه', 'هؤلاء', 'أولئك', 'إذما', 'بنا', 'من', 'خلا', 'ليسوا', 'ثم', 'لعل', 'وهو', 'نحو', 'أين', 'لئن', 'عدا', 'آه', 'كأنما', 'كليهما', 'الذي', 'لن', 'نعم', 'هذه', 'بهما', 'ليت', 'تلكم', 'أما', 'منذ', 'أو', 'هاك', 'بماذا', 'كذلك', 'أنا', 'آها', 'فإن', 'عل', 'منه', 'هيت', 'أف', 'أم', 'إيه', 'كيت', 'ته', 'لكيلا', 'ليس', 'مما', 'هذان', 'أنت', 'حبذا', 'ولو', 'أوه', 'إما', 'لو', 'بين', 'به', 'ولا', 'لما', 'بعد', 'هن', 'ذلكما', 'أولاء','و'}\n\nmaxDictionaryLength = 8000\n\ndef tokenize(sentence, isCreateDict=False):\n    tmpTokens = sentence.lower().split()\n    tokens = [token for token in tmpTokens if ((token not in stopwords) and (len(token)> 0)) ]\n     \n    if isCreateDict:\n        for token in tokens:\n            if token in dictionary_dict:\n                dictionary_dict[token] += 1\n            else:\n                dictionary_dict[token] = 1\n    documentTokens.append(tokens)\n    return tokens\n\n\ndef getInverseDocumentFrequency(documentTokens, dictionary):\n    return list(map(lambda word : 1 + math.log(len(documentTokens) / reduce(lambda acc,curr: (1 if (word in curr) else 0) + acc, documentTokens,0)),dictionary))\n\n\n  \ndef encoder(sentence, dictionary, idfs):\n    tokens = tokenize(sentence)\n    tfs = getTermFrequency(tokens, dictionary)\n    tfidfs = getTfIdf(tfs,idfs)\n    return tfidfs\n\n\ndef getTermFrequency(tokens, dictionary):\n    return  list(map(lambda token: reduce(lambda acc,curr : (acc + 1 if (curr == token) else acc), tokens,0), dictionary))\n\n\n\ndef getTfIdf(tfs, idfs):\n    return [tf * idf for (tf,idf) in zip(tfs,idfs)]\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-15T10:19:24.96544Z","iopub.execute_input":"2021-07-15T10:19:24.966007Z","iopub.status.idle":"2021-07-15T10:19:24.985831Z","shell.execute_reply.started":"2021-07-15T10:19:24.965958Z","shell.execute_reply":"2021-07-15T10:19:24.985042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Sample Test** Code used in the slides ( Module : preparing data for machine learning model )","metadata":{}},{"cell_type":"code","source":"dictionary_dict = {}\ndocumentTokens = []\ntestComments = ['للراحة عنوان . كل شي. لا شي', 'شيء جميل']\n\nfor comment in testComments:\n    documentTokens.append(tokenize(comment,True))\n\n\ndictionary = sorted(dictionary_dict, key=dictionary_dict.get, reverse=True)\nidfs = getInverseDocumentFrequency(documentTokens, dictionary);\n\ntfidfs = []\n\nfor comment in testComments:\n    tfidfs.append(encoder(comment, dictionary, idfs))\n\nprint(dictionary_dict)\nprint(dictionary)\nprint(idfs)\nprint(tfidfs)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T10:19:24.987094Z","iopub.execute_input":"2021-07-15T10:19:24.987457Z","iopub.status.idle":"2021-07-15T10:19:25.001085Z","shell.execute_reply.started":"2021-07-15T10:19:24.987422Z","shell.execute_reply":"2021-07-15T10:19:24.999887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dictionary_dict = {}\ndocumentTokens = []\ndf['tokens'] = df['text'].apply(lambda x : tokenize(x, True))","metadata":{"execution":{"iopub.status.busy":"2021-07-15T10:19:25.002637Z","iopub.execute_input":"2021-07-15T10:19:25.003048Z","iopub.status.idle":"2021-07-15T10:19:25.188005Z","shell.execute_reply.started":"2021-07-15T10:19:25.003011Z","shell.execute_reply":"2021-07-15T10:19:25.187154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-15T10:19:25.190555Z","iopub.execute_input":"2021-07-15T10:19:25.190914Z","iopub.status.idle":"2021-07-15T10:19:25.204695Z","shell.execute_reply.started":"2021-07-15T10:19:25.190877Z","shell.execute_reply":"2021-07-15T10:19:25.203933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dictionary = sorted(dictionary_dict, key=dictionary_dict.get, reverse=True)\ndictionary = dictionary[:maxDictionaryLength]\nprint('Length of dictionary : {0}'.format(len(dictionary)))\nprint(dictionary[:10])","metadata":{"execution":{"iopub.status.busy":"2021-07-15T10:19:25.206603Z","iopub.execute_input":"2021-07-15T10:19:25.207156Z","iopub.status.idle":"2021-07-15T10:19:25.230511Z","shell.execute_reply.started":"2021-07-15T10:19:25.207096Z","shell.execute_reply":"2021-07-15T10:19:25.229682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idfs = getInverseDocumentFrequency(documentTokens, dictionary)\nlen(idfs)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T10:19:25.233556Z","iopub.execute_input":"2021-07-15T10:19:25.23384Z","iopub.status.idle":"2021-07-15T10:20:02.361084Z","shell.execute_reply.started":"2021-07-15T10:19:25.233806Z","shell.execute_reply":"2021-07-15T10:20:02.359603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['features'] = df['text'].apply(lambda x : encoder(x,dictionary, idfs))\ndf['features'].head()","metadata":{"execution":{"iopub.status.busy":"2021-07-15T10:20:02.362416Z","iopub.execute_input":"2021-07-15T10:20:02.362835Z","iopub.status.idle":"2021-07-15T10:23:44.10329Z","shell.execute_reply.started":"2021-07-15T10:20:02.362793Z","shell.execute_reply":"2021-07-15T10:23:44.102438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_new = df['features'].apply(lambda x : pd.Series(x))\ndf_new['label'] = df['label']","metadata":{"execution":{"iopub.status.busy":"2021-07-15T10:23:44.104562Z","iopub.execute_input":"2021-07-15T10:23:44.105093Z","iopub.status.idle":"2021-07-15T10:23:55.74844Z","shell.execute_reply.started":"2021-07-15T10:23:44.105039Z","shell.execute_reply":"2021-07-15T10:23:55.747457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Test Split","metadata":{}},{"cell_type":"code","source":"train, test = train_test_split(df_new, test_size=0.2)\ntrain, val = train_test_split(train, test_size=0.1)\nprint(len(train), 'train examples')\nprint(len(val), 'validation examples')\nprint(len(test), 'test examples')","metadata":{"execution":{"iopub.status.busy":"2021-07-15T10:23:55.750793Z","iopub.execute_input":"2021-07-15T10:23:55.751095Z","iopub.status.idle":"2021-07-15T10:23:56.05572Z","shell.execute_reply.started":"2021-07-15T10:23:55.751053Z","shell.execute_reply":"2021-07-15T10:23:56.054808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape, test.shape, val.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T10:23:56.057371Z","iopub.execute_input":"2021-07-15T10:23:56.058187Z","iopub.status.idle":"2021-07-15T10:23:56.064558Z","shell.execute_reply.started":"2021-07-15T10:23:56.058142Z","shell.execute_reply":"2021-07-15T10:23:56.063456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def df_to_dataset(dataframe, shuffle=True, batch_size=16):\n    dataframe = dataframe.copy()\n    labels = dataframe.pop('label')\n    ds = tf.data.Dataset.from_tensor_slices((dataframe.values, labels))\n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(dataframe))\n    ds = ds.batch(batch_size)\n    return ds","metadata":{"execution":{"iopub.status.busy":"2021-07-15T10:23:56.066114Z","iopub.execute_input":"2021-07-15T10:23:56.066649Z","iopub.status.idle":"2021-07-15T10:23:56.076303Z","shell.execute_reply.started":"2021-07-15T10:23:56.066607Z","shell.execute_reply":"2021-07-15T10:23:56.075038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 100\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T10:23:56.078107Z","iopub.execute_input":"2021-07-15T10:23:56.078514Z","iopub.status.idle":"2021-07-15T10:23:58.986294Z","shell.execute_reply.started":"2021-07-15T10:23:56.07847Z","shell.execute_reply":"2021-07-15T10:23:58.985177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numOfFeatures = len(dictionary)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T10:23:58.987758Z","iopub.execute_input":"2021-07-15T10:23:58.988193Z","iopub.status.idle":"2021-07-15T10:23:58.992758Z","shell.execute_reply.started":"2021-07-15T10:23:58.988147Z","shell.execute_reply":"2021-07-15T10:23:58.991532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build Model","metadata":{}},{"cell_type":"code","source":"def get_build_model():\n    model = tf.keras.Sequential([\n    tf.keras.layers.Dense(15, activation='relu', input_shape=(numOfFeatures,)),\n    tf.keras.layers.Dropout(0.3),    \n    tf.keras.layers.Dense(15, activation='relu'),  \n    tf.keras.layers.Dropout(0.3),   \n    tf.keras.layers.Dense(1,activation='sigmoid')\n  ])\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n                loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n                metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-07-15T10:23:58.994683Z","iopub.execute_input":"2021-07-15T10:23:58.995463Z","iopub.status.idle":"2021-07-15T10:23:59.005907Z","shell.execute_reply.started":"2021-07-15T10:23:58.995415Z","shell.execute_reply":"2021-07-15T10:23:59.004987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_build_model()\nmodel.summary()\nmodel.fit(train_ds,epochs=20 ,validation_data=val_ds)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T10:23:59.007223Z","iopub.execute_input":"2021-07-15T10:23:59.007755Z","iopub.status.idle":"2021-07-15T10:24:12.605001Z","shell.execute_reply.started":"2021-07-15T10:23:59.007713Z","shell.execute_reply":"2021-07-15T10:24:12.603963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate Model","metadata":{}},{"cell_type":"code","source":"model.evaluate(test_ds)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T10:24:12.60683Z","iopub.execute_input":"2021-07-15T10:24:12.607207Z","iopub.status.idle":"2021-07-15T10:24:12.800176Z","shell.execute_reply.started":"2021-07-15T10:24:12.607168Z","shell.execute_reply":"2021-07-15T10:24:12.79898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make Predictions","metadata":{}},{"cell_type":"code","source":"## make predictions\ntestComments = ['صباح الخيرات', 'للراحة عنوان . كل شي. لا شي']\ntfidfs = []\nfor comment in testComments:\n    tfidfs.append(encoder(comment, dictionary, idfs))\nprint(f'predicted probabliities : {model.predict(tfidfs)}')\nprint(f'predicted classes : {tf.round(model.predict(tfidfs))}')","metadata":{"execution":{"iopub.status.busy":"2021-07-15T10:24:12.801965Z","iopub.execute_input":"2021-07-15T10:24:12.802414Z","iopub.status.idle":"2021-07-15T10:24:13.104793Z","shell.execute_reply.started":"2021-07-15T10:24:12.802375Z","shell.execute_reply":"2021-07-15T10:24:13.103813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Export Model","metadata":{}},{"cell_type":"code","source":"model.save('ar_reviews.h5')","metadata":{"execution":{"iopub.status.busy":"2021-07-15T10:24:13.106303Z","iopub.execute_input":"2021-07-15T10:24:13.106864Z","iopub.status.idle":"2021-07-15T10:24:13.142694Z","shell.execute_reply.started":"2021-07-15T10:24:13.106807Z","shell.execute_reply":"2021-07-15T10:24:13.141927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# write dictionary and IDFs\n\nwith open('dictionary.json', 'w', encoding='utf-8') as outfile:\n    json.dump(dictionary, outfile,  ensure_ascii=False, indent=4)\n\nwith open('idfs.json', 'w', encoding='utf-8') as outfile:\n    json.dump(idfs, outfile, ensure_ascii=False, indent=4)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T10:24:13.147107Z","iopub.execute_input":"2021-07-15T10:24:13.147454Z","iopub.status.idle":"2021-07-15T10:24:13.183242Z","shell.execute_reply.started":"2021-07-15T10:24:13.147417Z","shell.execute_reply":"2021-07-15T10:24:13.182467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# sklearn.ensemble import RandomForestClassifier","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","metadata":{"execution":{"iopub.status.busy":"2021-07-15T10:24:13.204534Z","iopub.execute_input":"2021-07-15T10:24:13.205008Z","iopub.status.idle":"2021-07-15T10:24:13.423896Z","shell.execute_reply.started":"2021-07-15T10:24:13.204971Z","shell.execute_reply":"2021-07-15T10:24:13.422906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_model = RandomForestClassifier(n_jobs=-1,n_estimators=180)\nrf_model.fit(train.loc[:, train.columns != 'label'], train['label'])","metadata":{"execution":{"iopub.status.busy":"2021-07-15T10:24:13.425131Z","iopub.execute_input":"2021-07-15T10:24:13.425489Z","iopub.status.idle":"2021-07-15T10:24:28.865936Z","shell.execute_reply.started":"2021-07-15T10:24:13.425452Z","shell.execute_reply":"2021-07-15T10:24:28.865024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_model.score(train.loc[:, train.columns != 'label'], train['label'])","metadata":{"execution":{"iopub.status.busy":"2021-07-15T10:24:28.867644Z","iopub.execute_input":"2021-07-15T10:24:28.868398Z","iopub.status.idle":"2021-07-15T10:24:29.488969Z","shell.execute_reply.started":"2021-07-15T10:24:28.868357Z","shell.execute_reply":"2021-07-15T10:24:29.488089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"rf_model.score(test.loc[:, test.columns != 'label'],test['label'])","metadata":{"execution":{"iopub.status.busy":"2021-07-15T10:24:29.49052Z","iopub.execute_input":"2021-07-15T10:24:29.491203Z","iopub.status.idle":"2021-07-15T10:24:29.803935Z","shell.execute_reply.started":"2021-07-15T10:24:29.491157Z","shell.execute_reply":"2021-07-15T10:24:29.80308Z"},"trusted":true},"execution_count":null,"outputs":[]}]}