{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\nimport numpy as np # linear algebra\nimport pandas as pd # data procmount essing, CSV file I/O (e.g. pd.read_csv)\nimport cv2\nimport time\nfrom timeit import default_timer as timer\nimport matplotlib.pyplot as plt\nimport pickle\n\nfrom keras.models import load_model\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('../input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\nprint(os.listdir('../input'))\n\n# Any results we write to the current directory are saved as output\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-25T16:41:28.972174Z","iopub.execute_input":"2021-08-25T16:41:28.972567Z","iopub.status.idle":"2021-08-25T16:41:33.413317Z","shell.execute_reply.started":"2021-08-25T16:41:28.972486Z","shell.execute_reply":"2021-08-25T16:41:33.411764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading csv file with labels' names\n# Loading two columns [0, 1] into Pandas dataFrame\nlabels = pd.read_csv('../input/traffic-signs-preprocessed/label_names.csv')\n\n# Check point\n# Showing first 5 rows from the dataFrame\nprint(labels.head())\nprint()\n\n# To locate by class number use one of the following\n# ***.iloc[0][1] - returns element on the 0 column and 1 row\nprint(labels.iloc[0][1])  # Speed limit (20km/h)\n# ***['SignName'][1] - returns element on the column with name 'SignName' and 1 row\nprint(labels['SignName'][1]) # Speed limit (30km/h)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:41:33.414776Z","iopub.execute_input":"2021-08-25T16:41:33.415117Z","iopub.status.idle":"2021-08-25T16:41:33.446938Z","shell.execute_reply.started":"2021-08-25T16:41:33.415078Z","shell.execute_reply":"2021-08-25T16:41:33.445714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìç Loading trained Keras CNN model for Classification","metadata":{}},{"cell_type":"code","source":"model = load_model('../input/modelgts/traffic.h5')\n\n# Loading mean image to use for preprocessing further\n# Opening file for reading in binary mode\nwith open('../input/traffic-signs-preprocessed/mean_image_rgb.pickle', 'rb') as f:\n    mean = pickle.load(f, encoding='latin1')  # dictionary type\n    \nprint(mean['mean_image_rgb'].shape)  # (3, 32, 32)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:41:33.448656Z","iopub.execute_input":"2021-08-25T16:41:33.448985Z","iopub.status.idle":"2021-08-25T16:41:37.108636Z","shell.execute_reply.started":"2021-08-25T16:41:33.44895Z","shell.execute_reply":"2021-08-25T16:41:37.107796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:41:37.112675Z","iopub.execute_input":"2021-08-25T16:41:37.114688Z","iopub.status.idle":"2021-08-25T16:41:37.130915Z","shell.execute_reply.started":"2021-08-25T16:41:37.114647Z","shell.execute_reply":"2021-08-25T16:41:37.130179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Parsing markings dataset","metadata":{}},{"cell_type":"code","source":"import math\nimport random\nfrom PIL import Image\n\npath_prefix = \"/root/darknet/rmarkings\"\nvalidate_percentage = 0.2\n\ndef RepresentsInt(s):\n    try:\n        int(s)\n        return True\n    except ValueError:\n        return False\n\ndef parse():\n    train = {}\n    validate = {}\n    cls_labels = {}\n    \n    # Read data\n    with open('dataset_annotations.txt') as file:\n        classes = {}\n        for line in file.readlines():\n            lines = line.strip().split(\",\")\n            file_name, clas_spec = lines[-1], lines[-2]\n            file_name = file_name.replace(\".png\", \".jpg\")\n\n            xs = [int(float(lines[0])), int(float(lines[2])), int(float(lines[4])), int(float(lines[6]))]\n            ys = [int(float(lines[1])), int(float(lines[3])), int(float(lines[5])), int(float(lines[7]))]\n\n            x_min, x_max = min(xs), max(xs)\n            y_min, y_max = min(ys), max(ys)\n\n            width = x_max - x_min\n            height = y_max - y_min\n\n            im = Image.open(file_name)\n            im_width, im_height = im.size\n\n            center_x, center_y = (width / 2) + x_min, (height /2) + y_min\n\n            if classes.get(clas_spec) is None:\n                classes[clas_spec] = []\n            data = {\"name\": file_name, \"x\": center_x / im_width, \"y\": center_y / im_height, \"width\": width / im_width, \"height\": height / im_height}\n            classes[clas_spec].append(data)\n            print(data)\n        \n        it = 0\n        for key, values in classes.items():\n            if len(values) > 20 and not RepresentsInt(key):\n                cls_labels[it] = key\n                random.shuffle(values)\n                test_len = math.floor(len(values) * validate_percentage)\n                train[key] = values[-(len(values) - test_len):]\n                validate[key] = values[:test_len]\n\n\n                print(f\"validate: {key}: {len(validate[key])}\")\n                print(f\"train: {key}: {len(train[key])}\")\n\n                for value in values:\n                    f_name = value[\"name\"].replace(\".jpg\", \".txt\")\n                    with open(f_name, \"w+\") as w_file:\n                        w_file.write(f\"{it} {value['x']} {value['y']} {value['width']} {value['height']}\\n\")\n                it += 1\n\n    with open(\"classes.names\", \"w+\") as cls_file:\n        for cls_name in cls_labels.values():\n            cls_file.write(f\"{cls_name}\\n\")\n\n    with open(\"train.txt\", \"w+\") as data_file:\n        for vls in train.values():\n            for val in vls:\n                data_file.write(f\"{path_prefix}/{val['name']}\\n\")\n\n    with open(\"test.txt\", \"w+\") as data_file:\n        for vls in validate.values():\n            for val in vls:\n                data_file.write(f\"{path_prefix}/{val['name']}\\n\")\n    \n    with open(\"data.data\", \"w+\") as data_file:\n        data_file.write(f\"\"\"classes = {len(cls_labels.keys())}\ntrain = {path_prefix}/train.txt\nvalid = {path_prefix}/test.txt\nnames = {path_prefix}/classes.names\nbackup = backup1\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:41:37.134555Z","iopub.execute_input":"2021-08-25T16:41:37.136551Z","iopub.status.idle":"2021-08-25T16:41:37.163122Z","shell.execute_reply.started":"2021-08-25T16:41:37.136513Z","shell.execute_reply":"2021-08-25T16:41:37.16229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading *trained weights* and *cfg file* into the Network","metadata":{}},{"cell_type":"code","source":"# Trained weights can be found in the course mentioned above\n\npath_to_weights = '../input/weights/Yolo_Custom_final.weights'\npath_to_weights_markings = '../input/car-data/poziome_rtx_final.weights'\npath_to_cfg = '../input/weights/Yolo_Custom.cfg'\npath_to_cfg_markings = '../input/car-data/markings_test.cfg'\n\n# Loading trained YOLO v3 weights and cfg configuration file by 'dnn' library from OpenCV\nnetwork = cv2.dnn.readNetFromDarknet(path_to_cfg, path_to_weights)\nnetwork_markings = cv2.dnn.readNetFromDarknet(path_to_cfg_markings, path_to_weights_markings)\n\n# To use with GPU\nnetwork.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\nnetwork.setPreferableTarget(cv2.dnn.DNN_TARGET_OPENCL_FP16)\n\nnetwork_markings.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\nnetwork_markings.setPreferableTarget(cv2.dnn.DNN_TARGET_OPENCL_FP16)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:41:37.167782Z","iopub.execute_input":"2021-08-25T16:41:37.169884Z","iopub.status.idle":"2021-08-25T16:41:43.151821Z","shell.execute_reply.started":"2021-08-25T16:41:37.169848Z","shell.execute_reply":"2021-08-25T16:41:43.150846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Getting *output layers* where detections are made","metadata":{}},{"cell_type":"code","source":"# Getting names of all YOLO v3 layers\nlayers_all = network.getLayerNames()\nlayers_names_output = [layers_all[i[0] - 1] for i in network.getUnconnectedOutLayers()]\nprint(layers_names_output)\n\nprint(\"<===========>\")\n\n# Getting names of all YOLO v4 layers\nlayers_all_markings = network_markings.getLayerNames()\nlayers_names_output_markings = [layers_all_markings[i[0] - 1] for i in network_markings.getUnconnectedOutLayers()]\nprint(layers_names_output_markings)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:41:43.15325Z","iopub.execute_input":"2021-08-25T16:41:43.153598Z","iopub.status.idle":"2021-08-25T16:41:43.162642Z","shell.execute_reply.started":"2021-08-25T16:41:43.153558Z","shell.execute_reply":"2021-08-25T16:41:43.161475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setting *probability*, *threshold* and *colour* for bounding boxes","metadata":{}},{"cell_type":"code","source":"# Minimum probability to eliminate weak detections\nprobability_minimum = 0.1\n\n# Setting threshold to filtering weak bounding boxes by non-maximum suppression\nthreshold = 0.1\n\n# Generating colours for bounding boxes\n# randint(low, high=None, size=None, dtype='l')\ncolours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')\ncolours_markings = np.random.randint(0, 255, size=(1, 3), dtype='uint8')\n\n# Check point\nprint(type(colours))  # <class 'numpy.ndarray'>\nprint(colours.shape)  # (43, 3)\nprint(colours[0])  # [25  65 200]\n","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:41:43.16538Z","iopub.execute_input":"2021-08-25T16:41:43.165953Z","iopub.status.idle":"2021-08-25T16:41:43.177535Z","shell.execute_reply.started":"2021-08-25T16:41:43.165915Z","shell.execute_reply":"2021-08-25T16:41:43.176526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üé¨ Reading input video","metadata":{}},{"cell_type":"code","source":"# Reading video from a file by VideoCapture object\n# ../input/testyolo/1.mp4\nvideo = cv2.VideoCapture('../input/carladataset/carla.mp4')\n\n# video = cv2.VideoCapture('../input/car-data/70maiMiniDashCam-Dzien.mp4')\n# video = cv2.VideoCapture('../input/car-data/DODRX8W(lusterko)-roadtestwsonecznydzien_podsonce1080p30.mp4')\n# video = cv2.VideoCapture('../input/traffic-signs-dataset-in-yolo-format/traffic-sign-to-test.mp4')\n\n# Writer that will be used to write processed frames\nwriter = None\n\n# Variables for spatial dimensions of the frames\nh, w = None, None\n","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:41:43.179423Z","iopub.execute_input":"2021-08-25T16:41:43.180008Z","iopub.status.idle":"2021-08-25T16:41:43.32279Z","shell.execute_reply.started":"2021-08-25T16:41:43.179966Z","shell.execute_reply":"2021-08-25T16:41:43.321924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ‚ûø Processing frames in the loop","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image, clear_output\n\n%matplotlib inline\n\n# Setting default size of plots\nplt.rcParams['figure.figsize'] = (3, 3)\n\n# Variable for counting total amount of frames\nf = 0\n\n# Variable for counting total processing time\nt = 0\n\n# Catching frames in the loop\nwhile True:\n    # Capturing frames one-by-one\n    ret, frame = video.read()\n\n    # If the frame was not retrieved\n    if not ret:\n        break\n       \n    # Getting spatial dimensions of the frame for the first time\n    if w is None or h is None:\n        # Slicing two elements from tuple\n        h, w = frame.shape[:2]\n\n    # Blob from current frame\n    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n\n    # Forward pass with blob through output layers\n    network.setInput(blob)\n    network_markings.setInput(blob)\n    start = time.time()\n    output_from_network = network.forward(layers_names_output)\n    output_from_network_markings = network_markings.forward(layers_names_output_markings)\n    end = time.time()\n\n    # Increasing counters\n    f += 1\n    t += end - start\n\n    # Spent time for current frame\n    print('Frame number {0} took {1:.5f} seconds'.format(f, end - start))\n\n    # Lists for detected bounding boxes, confidences and class's number\n    bounding_boxes = []\n    bounding_boxes_markings = []\n    confidences = []\n    confidences_markings = []\n    class_numbers = []\n    class_numbers_markings = []\n\n    # Going through all output layers after feed forward pass\n    for result in output_from_network:\n        # Going through all detections from current output layer\n        for detected_objects in result:\n            # Getting 80 classes' probabilities for current detected object\n            scores = detected_objects[5:]\n            # Getting index of the class with the maximum value of probability\n            class_current = np.argmax(scores)\n            # Getting value of probability for defined class\n            confidence_current = scores[class_current]\n            # Eliminating weak predictions by minimum probability\n            if confidence_current > probability_minimum:\n                try:\n                    # Scaling bounding box coordinates to the initial frame size\n                    box_current = detected_objects[0:4] * np.array([w, h, w, h])\n\n                    # Getting top left corner coordinates\n                    x_center, y_center, box_width, box_height = box_current\n                    x_min = int(x_center - (box_width / 2))\n                    y_min = int(y_center - (box_height / 2))\n\n                    # Adding results into prepared lists\n                    bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n                    confidences.append(float(confidence_current))\n                    class_numbers.append(class_current)\n                except Exception as e:\n                    print(e)\n                \n\n    # Implementing non-maximum suppression of given bounding boxes\n    results = cv2.dnn.NMSBoxes(bounding_boxes, confidences, probability_minimum, threshold)\n    results_markings = cv2.dnn.NMSBoxes(bounding_boxes_markings, bounding_boxes_markings, probability_minimum, threshold)\n\n    # Checking if there is any detected object been left\n    if len(results) > 0:\n        # Going through indexes of results\n        for i in results.flatten():\n            # Bounding box coordinates, its width and height\n            x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n            box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n            \n            \n            # Cut fragment with Traffic Sign\n            c_ts = frame[y_min:y_min+int(box_height), x_min:x_min+int(box_width), :]\n            \n            if c_ts.shape[:1] == (0,) or c_ts.shape[1:2] == (0,):\n                pass\n            else:\n                # Getting preprocessed blob with Traffic Sign of needed shape\n                blob_ts = cv2.dnn.blobFromImage(c_ts, 1 / 255.0, size=(32, 32), swapRB=True, crop=False)\n                blob_ts[0] = blob_ts[0, :, :, :] - mean['mean_image_rgb']\n                blob_ts = blob_ts.transpose(0, 2, 3, 1)\n\n                # Feeding to the Keras CNN model to get predicted label among 43 classes\n                scores = model.predict(blob_ts)\n\n                # Scores is given for image with 43 numbers of predictions for each class\n                # Getting only one class with maximum value\n                prediction = np.argmax(scores)\n\n\n                # Colour for current bounding box\n                colour_box_current = colours[class_numbers[i]].tolist()\n\n                # Drawing bounding box on the original current frame\n                cv2.rectangle(frame, (x_min, y_min),\n                              (x_min + box_width, y_min + box_height),\n                              colour_box_current, 2)\n\n                # Preparing text with label and confidence for current bounding box\n                text_box_current = '{}: {:.4f}'.format(labels['SignName'][prediction],\n                                                       confidences[i])\n\n                # Putting text with label and confidence on the original image\n                cv2.putText(frame, text_box_current, (x_min, y_min - 5),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, colour_box_current, 2)\n\n    # For markings\n    for result in output_from_network_markings:\n        for detected_objects in result:\n            scores = detected_objects[5:]\n            class_current = np.argmax(scores)\n            confidence_current = scores[class_current]\n            if confidence_current > probability_minimum:\n                try:\n                    box_current = detected_objects[0:4] * np.array([w, h, w, h])\n\n                    x_center, y_center, box_width, box_height = box_current\n                    x_min = int(x_center - (box_width / 2))\n                    y_min = int(y_center - (box_height / 2))\n\n                    bounding_boxes_markings.append([x_min, y_min, int(box_width), int(box_height)])\n                    confidences_markings.append(float(confidence_current))\n                    class_numbers_markings.append(class_current)\n                except Exception as e:\n                    print(e)\n\n    if len(results_markings) > 0:\n        for i in results_markings.flatten():\n            x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n            box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n\n            cv2.rectangle(frame, (x_min, y_min),\n                          (x_min + box_width, y_min + box_height),\n                            colours[0].toList(), 2)\n\n\n    # Initializing writer only once\n    if writer is None:\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n\n        # Writing current processed frame into the video file\n        writer = cv2.VideoWriter('resultcarla.mp4', fourcc, 25,\n                                 (frame.shape[1], frame.shape[0]), True)\n\n    # Write processed current frame to the file\n    writer.write(frame)\n\n\n# Releasing video reader and writer\nvideo.release()\nwriter.release()\n\nclear_output()\n","metadata":{"execution":{"iopub.status.busy":"2021-08-25T16:41:43.324093Z","iopub.execute_input":"2021-08-25T16:41:43.324468Z","iopub.status.idle":"2021-08-25T17:58:27.104613Z","shell.execute_reply.started":"2021-08-25T16:41:43.324428Z","shell.execute_reply":"2021-08-25T17:58:27.103741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## üèÅ FPS results","metadata":{}},{"cell_type":"code","source":"print('Total number of frames', f)\nprint('Total amount of time {:.5f} seconds'.format(t))\nprint('FPS:', round((f / t), 1))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-25T17:58:27.106131Z","iopub.execute_input":"2021-08-25T17:58:27.106537Z","iopub.status.idle":"2021-08-25T17:58:27.112672Z","shell.execute_reply.started":"2021-08-25T17:58:27.106495Z","shell.execute_reply":"2021-08-25T17:58:27.111701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving locally without committing\nfrom IPython.display import FileLink\nimport os\n\n#os.chdir(r'kaggle/working')\nFileLink('resultcarla.mp4')\n","metadata":{"execution":{"iopub.status.busy":"2021-08-25T17:58:27.114157Z","iopub.execute_input":"2021-08-25T17:58:27.114598Z","iopub.status.idle":"2021-08-25T17:58:27.129267Z","shell.execute_reply.started":"2021-08-25T17:58:27.114559Z","shell.execute_reply":"2021-08-25T17:58:27.128354Z"},"trusted":true},"execution_count":null,"outputs":[]}]}