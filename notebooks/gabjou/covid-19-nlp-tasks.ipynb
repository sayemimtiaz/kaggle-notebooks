{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Clustering and topic performed with simple BERT respresentation\n\nThe goal of this notebook is to bring a powerful application of BERT representation of body text vocabulary article associated to usual NLP method.\nTwo task will be completed in this notebook:\n* A task of article clustering to help to identify relationship around the data base.\n\n* A task of matching sentences with topic request. The idea is to use BERT representation to identify relevant part of the article macthing with the Topic.\n\nUsual NLP tools to construct article keywords set will be used to filter the database when topic request will be done."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Installation of package including BERT method \n!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data preparation as data frame**\n\nTo achieve this specific task, the following code was inspired by [covid-eda-initial-exploration-tool](https://www.kaggle.com/ivanegapratama/covid-eda-initial-exploration-tool).\n"},{"metadata":{},"cell_type":"markdown","source":"The following cell construct the df_covid data frame frome the CORD-19-research-challenge file."},{"metadata":{"trusted":true},"cell_type":"code","source":"## DO NOT RUN HERE\n# This code has been use to prepare the df_covid data frame representing the 27 March 2020 version of the COVID-19 Open Research Dataset Challenge (CORD-19) data base.\n\n# Process all json information and update df_covid data frame from the input path in the working path\nimport os\nimport json\nimport glob as gl\nimport sys\nimport numpy as np\nimport pandas as pd\nsys.path.insert(0, \"../\")\n\nclass FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.abstract = []\n            self.body_text = []\n            # Abstract\n            try:\n                for entry in content['abstract']:\n                    self.abstract.append(entry['text'])\n                self.abstract = '\\n'.join(self.abstract)\n            except:\n                self.abstract.append(['Not provided.'])\n            # Body text\n            try:\n                for entry in content['body_text']:\n                    self.body_text.append(entry['text'])\n                self.body_text = '\\n'.join(self.body_text)\n            except:\n                self.body_text.append(['Not provided.'])    \n    def __repr__(self):\n        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\n    \ndef get_breaks(content, length):\n    data = \"\"\n    words = content.split(' ')\n    total_chars = 0\n\n    # add break every length characters\n    for i in range(len(words)):\n        total_chars += len(words[i])\n        if total_chars > length:\n            data = data + \"<br>\" + words[i]\n            total_chars = 0\n        else:\n            data = data + \" \" + words[i]\n    return data\n\nroot_path = '/kaggle/input/CORD-19-research-challenge/'\n# Just set up a quick blank dataframe to hold all these medical papersb. \n\njson_filenames = gl.glob(f'{root_path}/***/**/*.json', recursive=True)\n\nmetadata_path = f'{root_path}/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\n\nall_json = json_filenames\n\ndict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': [],'publish_time':[]}\nfor idx, entry in enumerate(all_json):\n    if idx % (len(all_json) // 10) == 0:\n        print(f'Processing index: {idx} of {len(all_json)}')\n    content = FileReader(entry)\n    \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    # no metadata, skip this paper\n    if len(meta_data) == 0:\n        continue\n    \n    dict_['paper_id'].append(content.paper_id)\n    dict_['abstract'].append(content.abstract)\n    dict_['body_text'].append(content.body_text)\n    \n    # also create a column for the summary of abstract to be used in a plot\n    if len(content.abstract) == 0: \n        # no abstract provided\n        dict_['abstract_summary'].append(\"Not provided.\")\n    elif len(content.abstract.split(' ')) > 100:\n        # abstract provided is too long for plot, take first 300 words append with ...\n        info = content.abstract.split(' ')[:100]\n        summary = get_breaks(' '.join(info), 40)\n        dict_['abstract_summary'].append(summary + \"...\")\n    else:\n        # abstract is short enough\n        summary = get_breaks(content.abstract, 40)\n        dict_['abstract_summary'].append(summary)\n        \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    \n    try:\n        # if more than one author\n        authors = meta_data['authors'].values[0].split(';')\n        if len(authors) > 2:\n            # more than 2 authors, may be problem when plotting, so take first 2 append with ...\n            dict_['authors'].append(\". \".join(authors[:2]) + \"...\")\n        else:\n            # authors will fit in plot\n            dict_['authors'].append(\". \".join(authors))\n    except Exception as e:\n        # if only one author - or Null valie\n        dict_['authors'].append(meta_data['authors'].values[0])\n    \n    # add the title information, add breaks when needed\n    try:\n        title = get_breaks(meta_data['title'].values[0], 40)\n        dict_['title'].append(title)\n    # if title was not provided\n    except Exception as e:\n        dict_['title'].append(meta_data['title'].values[0])\n    \n    # add the publish time information, add breaks when needed\n    try:\n        publish_time = meta_data['publish_time'].values[0]\n        dict_['publish_time'].append(publish_time)\n    # if title was not provided\n    except Exception as e:\n        dict_['publish_time'].append(meta_data['publish_time'].values[0])\n        \n    # add the journal information\n    dict_['journal'].append(meta_data['journal'].values[0])\n    \ndf_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text', 'authors', 'title', 'journal', 'abstract_summary','publish_time'])\ndf_covid.to_csv('df_covid.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data importation**\nThe following cell load the df_covid data frame constructed frome the CORD-19-research-challenge file version of 27 March 2020 and corresponding to the ArticleBertEmb data base."},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport json\nimport glob as gl\nimport sys\nimport numpy as np\nimport pandas as pd\nsys.path.insert(0, \"../\")\n\nsys.path.insert(0, \"../\")\nroot_path = '/kaggle/input/'\n\n## /kaggle/input/covid-19-nlp-tasks/df_covid.csv --> Problème de localisation du fichier dans ma version\n\n# Import the 27 March 2020 version of the COVID-19 Open Research Dataset Challenge (CORD-19) data base Version 27 March 2020=> 27 678 articles.\ndf_covid = pd.read_csv(f'{root_path}df-covid/df_covid.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Article embedding signature by BERT\n\nThe goal is to assign a vector to each article present in the data base. The vector is representative of the current vocabulary used to in the body part of the article. \nHaving a such signature permit us to easly represent all articles in a vector space and achieve a clustering to understand relationships between articles.  \n\nTo learn more about [BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding](https://arxiv.org/pdf/1810.04805.pdf) from the [transformers](https://github.com/huggingface/transformers) package.\n\nThe use of BERT methods to reprsent each article in a vector space was inspired by [covid-19-search-engine-with-bert](https://www.kaggle.com/zoupet/covid-19-search-engine-with-bert)."},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom transformers import *\n\n# Transformers has a unified API\n# for 10 transformer architectures and 30 pretrained weights.\n#          Model          | Tokenizer          | Pretrained weights shortcut\nMODELS = 'bert-base-uncased'\n\n# To use TensorFlow 2.0 versions of the models, simply prefix the class names with 'TF', e.g. `TFRobertaModel` is the TF 2.0 counterpart of the PyTorch model `RobertaModel`\n\n# Load pretrained model/tokenizer\ntokenizer = BertTokenizer.from_pretrained(MODELS)\nmodel = BertModel.from_pretrained(MODELS)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To avoid computation times and notebook breaks, the following cell has been comptued externally to produce 27678 article's signature stored in the ArticleParaEmb.csv "},{"metadata":{"trusted":true},"cell_type":"code","source":"## DO NOT RUN HERE, PROCESS EXTREMELY SLOW\n# USE INSTEAD the ArticleParaEmb.csv file to have the signature of 27678 articles.\n\n# # BERT PARAMETERS\n# block_size = 512\n# Vector_size = 768 \n\n# for a in range(df_covid.shape[0]):\n#     print(a,'/',len(df_covid['body_text'])) # Processing of the text\n        \n#     # Article body text\n#     text = df_covid['body_text'].iloc[a]\n    \n#     # Encode text\n#     input_ids_1 = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])  # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n\n#     # Result table\n#     result = torch.tensor(np.empty(list(input_ids_1.size())+[Vector_size]))   \n\n#     # In case oh the number of words in the sentence is superior of block_size of BERT: \n#     if input_ids_1.shape[1] > block_size:\n#         nbatch = round(input_ids_1.shape[1]/block_size)\n#         if nbatch < input_ids_1.shape[1]/block_size:\n#             last_size = input_ids_1.shape[1]-block_size*nbatch\n#         else:\n#             nbatch = nbatch-1\n#             last_size = input_ids_1.shape[1]-block_size*nbatch\n#         iter_=0\n#         with torch.no_grad():\n#             while iter_ != nbatch:\n#                 result[:,(iter_*block_size):((iter_+1)*block_size),:] = model(input_ids_1[:,(iter_*block_size):((iter_+1)*block_size)])[0]\n#                 iter_ += 1\n#             result[:,(iter_*block_size):((iter_*block_size)+last_size),:] = model(input_ids_1[:,(iter_*block_size):((iter_*block_size)+last_size)])[0]\n#     else:\n#         with torch.no_grad():\n#             result = model(input_ids_1)[0]\n#     result = result.numpy()[0,:,:]\n\n#     # Compute the mean of all vocabulary BERT representation of the body text\n#     result = result.mean(dim=1)\n\n#     # Save all article BERT representation\n#     if( a == 0):\n#         ArticleParaEmb = result[np.newaxis]\n#     else:\n#         ArticleParaEmb = np.concatenate((ArticleParaEmb,result[np.newaxis]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Article clustering and keywords searching\n\n**Key ideas of this part**:\n1. Catch n keywords by article to facilitate plot filtering in the vector space reduce with T-SNE process. After a filtering of any stopwords in the body part of the current analysed article, the keywords retrival task is accomplished by catching the \"n most used words\". \n2. Construct n cluster using [MiniBatchKMeans](https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf) algorithm ([sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html) package) on the article embedding. \n3. Catch n keywords by cluster constructed. This task is accomplished by concatenating all abstract summary of article present in the current cluster analysed. The text obtained by the precedent concatenating is filtered and the \"n most used words\" are captured.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"###### Hyperparameters #######\nnArticleKeywords = 100\nnArticleClusters = 60\nnTopArticleCluster = 100\nnClusterKeywords = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nimport re\nfrom scipy.spatial.distance import cdist\nfrom nltk.corpus import stopwords\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.feature_extraction.text import CountVectorizer\nrandom.seed(12)\n\n# Article embedding load from the 27 March 2020 version of the COVID-19 Open Research Dataset Challenge (CORD-19) data base Version 27 March 2020=> 27 678 articles embedding corresponding to the df_covid data base.\nArticleParaEmb_np =  pd.read_csv(f'{root_path}articlebertembedding768d/ArticleParaEmb.csv')\n\n\n###### Data Processing ##########\n# Function to supress non-pertinent article\ndef TitleToDel(x):\n    if isinstance(x, str):\n        x = x.split(' ')\n        k = 0\n        flag=False\n        while (k < len(x))&(flag==False):\n            flag = x[k] in ['Index','Subject','Cumulative']\n            k=k+1\n        return(flag)\n    else:\n        return(True)\n\n\n# Mask to delete non informative article of the data base\nsub_df_covid = df_covid.copy()\nind_to_del = sub_df_covid['title'].apply(lambda x: TitleToDel(x))\nmask = np.ones(len(sub_df_covid['title']), dtype=bool)\nmask[ind_to_del] = False\n\nArticleParaEmb_np = ArticleParaEmb_np.to_numpy()[mask,1:]\nsub_df_covid = sub_df_covid.iloc[mask,:]\n\n\n\n###### Top Keywords Article ##########\n\n### Delete punctuation and lowercase the text\ndf = sub_df_covid.copy()\ndf['body_text'] = df['body_text'].map(lambda x: re.sub(\"[,\\.!?]\", \"\", x))\ndf['body_text'] = df['body_text'].map(lambda x: x.lower())\n\n# Word tokenizer\ncount_vectorizer = CountVectorizer(stop_words='english')\ndf['nKeywords'] = \"\"\ndf['nCount'] = \"\"\n\n### Remove stopwords and add 10 most used words for each paper\nfor index, row in df.iterrows():\n    row['body_text'] = [row['body_text']]\n    count_data = count_vectorizer.fit_transform(row['body_text'])\n    words = count_vectorizer.get_feature_names()\n    total_counts = np.zeros(len(words))\n    for t in count_data:\n        total_counts += t.toarray()[0]\n\n    count_dict = (zip(words, total_counts))\n    count_dict = sorted(count_dict, key=lambda x: x[1], reverse=True)[:nArticleKeywords]\n    words = [w[0] for w in count_dict]\n    counts = [w[1] for w in count_dict]\n    df.at[index, 'nKeywords'] = words\n    df.at[index, 'nCount'] = counts\n\nsub_df_covid['nKeywords'] = df['nKeywords']\nsub_df_covid['nCount'] = df['nCount']\n###### Article Clustering ##########\nclust = MiniBatchKMeans(n_clusters=nArticleClusters,random_state=0,batch_size=round(ArticleParaEmb_np.shape[0]*0.4)).fit(ArticleParaEmb_np)\n\n\n\n###### Top Keywords Cluster  ##########\n\ncatch = re.compile(r'/*[.-;\\*()\\[§\\]]')\ncatch1 = re.compile(r'/*[.]') #-> punctuation to separate each sentence \ncatch2 = re.compile(r'\\n') #-> separator of section \ncatch3 = re.compile(r'/*[(]') #-> open parenthesis/citation\ncatch4 = re.compile(r'/*[)]') #-> close parenthesis/citation\ncatch5 = re.compile(r'\\[') #-> open parenthesis/citation\ncatch6 = re.compile(r'\\]') #-> close parenthesis/citation\ncatch7 = re.compile(r'\\s\\[\\s') #-> punctuation to separate each sentence \ncatch8 = re.compile(r'\\s\\]\\s') #-> punctuation to separate each sentence \ncatch9 = re.compile(r'\\s\\(\\s') #-> punctuation to separate each sentence \ncatch10 = re.compile(r'\\s\\)\\s') #-> punctuation to separate each sentence \nmatch1 = re.compile(r'([0-9]\\.[0-9])') #-> punctuation to separate each sentence \nmatch2 = re.compile(r'Fig.') #-> punctuation to separate each sentence \nmatch3 = re.compile(r'<br>') #-> punctuation to separate each sentence \nmatch4 = re.compile(r'##') #-> punctuation to separate each sentence\nmatch5 = re.compile(r',') #-> punctuation to separate each sentence \nmatch6 = re.compile(r'/*[-;\\'%\"+=£«^»]') #-> punctuation to separate each sentence\nmatch7 = re.compile(r'\\[\\d\\]') #-> punctuation to separate each sentence \nmatch8 = re.compile(r'\\s\\w\\s') #-> punctuation to separate each sentence \n\n\nstop_words = set(stopwords.words('english'))\nClusterKeyWord = list()\nfor a in set(clust.labels_):\n    if(((a+1) % 10) == 0):\n        print((a+1),'/',len(set(clust.labels_)))\n    clust_dist = ArticleParaEmb_np[clust.labels_==a,:]\n    clust_abs = sub_df_covid.iloc[clust.labels_==a,:]\n    all_dist = np.array([cdist(clust.cluster_centers_[a][np.newaxis],clust_dist[i][np.newaxis])[0][0].tolist() for i in range(clust_dist.shape[0])])\n    index_sort = np.argsort(all_dist)\n    if len(index_sort)>nTopArticleCluster:\n        abstract = clust_abs.iloc[index_sort[:nTopArticleCluster],:]\n    else:\n        abstract = clust_abs.iloc[:,:]\n        \n    text = abstract['abstract_summary'].copy()\n\n    study = abstract.loc[abstract['abstract_summary'] == \"Not provided.\",:]\n    replace_text = study.copy()\n    for k in range(study.shape[0]):\n        if isinstance(study['title'].iloc[k], str):\n            replace_text['abstract_summary'].iloc[k] = '. '.join([study['title'].iloc[k],catch2.sub(\" \\n \",study['body_text'].iloc[k]).split(' \\n ')[0]])\n        else:\n            replace_text['abstract_summary'].iloc[k] = catch2.sub(\" \\n \",study['body_text'].iloc[k]).split(' \\n ')[0]\n        \n    text.loc[abstract['abstract_summary'] == \"Not provided.\"]=replace_text['abstract_summary']\n    text = text.map(lambda x: x.lower())\n    text = \". \".join(text.iloc[:])\n    text = catch.sub(\"\", text)\n    text = match3.sub(\" \", text)\n    text = match5.sub(\"\", text)\n    text = match6.sub(\"\", text)\n    text = match7.sub(\"\", text)\n    text = catch2.sub(\" \", text)\n    \n    text = text.split(\" \")\n    text = [w for w in text if not w in stop_words]\n    text = \" \".join(text) \n    text = match8.sub(\"\", text)\n    \n    count_data = count_vectorizer.fit_transform([text])\n    words = count_vectorizer.get_feature_names()\n    total_counts = np.zeros(len(words))\n    for t in count_data:\n        total_counts += t.toarray()[0]\n\n    count_dict = (zip(words, total_counts))\n    count_dict = sorted(count_dict, key=lambda x: x[1], reverse=True)[:nClusterKeywords]\n    words = [w[0] for w in count_dict]\n    ClusterKeyWord.append(words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PCA & t-SNE processing\nKey ideas of this part:\n* Use common tools to decompose and reduce high-dimensionnal data [PCA sklearn](https://scikit-learn.org/stable/modules/decomposition.html#pca),[t-SNE](https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf),[t-SNE sklearn](https://scikit-learn.org/stable/modules/manifold.html#t-sne).\n* The PCA reduction applied on the article embeddings permit to not overload the t-SNE method by reduce the high dimension of the covariates (768 in our case) in D principals components resuming 0.9 of the cumulative variance explained.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"########### Preprocessing for plotting results ###########\n\nimport colorcet\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE  \n\ncolors = [colorcet.glasbey[i] for i in range(0,len(colorcet.glasbey) ,round(len(colorcet.glasbey)/len(set(clust.labels_))))]\n\nmatch10 = re.compile(r\"(?:ed$)\")\ni=0\nListKey = []\nfor i in range(len(ClusterKeyWord)):\n    tmp = [w for w in ClusterKeyWord[i] if not w in stop_words]\n    tmp = [match10.sub(\"\", w) for w in tmp]\n    ListKey.append(\" ; \".join(tmp))\n\n\npca = PCA(n_components=100)\npca_result = pca.fit_transform(ArticleParaEmb_np)\n\n  \nX_pca = pca_result[:,:(np.where(np.cumsum(pca.explained_variance_ratio_)>= 0.9)[0][0])]\nX_embedded = TSNE(n_components=2,verbose=1, perplexity=100).fit_transform(X_pca)\n\n\nX_par = sub_df_covid.copy()\nX_par['X'] = X_embedded[:,0]\nX_par['Y'] = X_embedded[:,1]\nX_par['Cluster'] = clust.labels_\nX_par['Colors'] = [colors[i] for i in clust.labels_]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results plots with Bokeh\n\nKey ideas of this part:\n* Plotting clustering results in a 2D scatter plot showing the title and published time of each article: [bokeh package](https://docs.bokeh.org/en/latest/docs/user_guide/jupyter.html#userguide-jupyter-notebook).\n* Add 2 plot filtering function \"ArticleKeywordSearch\" and \"ClusterKeywordSearch\" based on article and cluster keywords.\n* Construction of a Topics research function with BERT sentence representation:\n> 1. Catch n top article based with the matching of the topic words and the article keywords.\n> > You can reorder by date to catch the most current information.\n> 2. Compute a BERT representation of the Topic sentence and of the body text of each of the n article selected.\n> 3. Compute the mean of all word's BERT representation contained on each sentence to form a sentence BERT representation.\n> 4. Catch the n best sentences by computing the [cosinus distance](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html) between the Topic BERT representation and each sentences BERT representation."},{"metadata":{"trusted":true},"cell_type":"code","source":"########### Plotting results ###########\n\n# BERT PARAMETERS\nblock_size = 512\nVector_size = 768\n\nfrom IPython.display import display\nfrom bokeh.io import push_notebook, show, output_notebook\nfrom bokeh.plotting import figure,Figure\nfrom bokeh.layouts import column, row\nfrom bokeh.models import ColumnDataSource,Panel, Tabs,TextInput,CustomJS, Slider\nfrom ipywidgets import interact\nfrom scipy.spatial.distance import cosine\nmatch10 = re.compile(r\"/*[0-9-&\\/\\\\#,+()$~%.'\\\":*?<>{}]\")\n\n###################### CLUSTERING BY BERT REPRESENTATION AND PCA&t-SNE ######################\noutput_notebook()\nsource1 = ColumnDataSource(data=X_par[[\"X\",\"Y\",\"Colors\",\"Cluster\",\"title\",\"publish_time\",\"nKeywords\"]])\nsourceUpdate = ColumnDataSource(data=X_par[[\"X\",\"Y\",\"Colors\",\"Cluster\",\"title\",\"publish_time\",\"nKeywords\"]])\nsourceKeywords = ColumnDataSource(data=pd.DataFrame(ListKey,columns=['Keywords']))\n# sourceParameters = ColumnDataSource(data=pd.DataFrame([nArticlePlot]))\n\n# Java script code for update filtering text box\nClusterKeywordSearch = CustomJS(args=dict(source1= source1,sourceUpdate=sourceUpdate,sourceKeywords=sourceKeywords), code=\"\"\"\n    const reducer = (accumulator, currentValue) => accumulator + currentValue;\n    var data = sourceUpdate.data;\n    const data2 = source1.data\n    const returnedTarget1 = Object.assign(data, data2)\n    var text = cb_obj.value\n    if (text == \"\") {\n        const newdata = source1.data\n        const returnedTarget1 = Object.assign(data, newdata)\n        sourceUpdate.change.emit();\n    } else {\n        var textlist = text.split(';')\n        const newdata = {\"index\":[],\"X\":[],\"Y\":[],\"Colors\":[],\"title\":[],\"publish_time\":[]}\n        const keys1 = Object.keys(newdata)\n        var cluster = []\n        for (var i = 0; i < sourceKeywords.data['Keywords'].length; i++) {\n            var keywords = sourceKeywords.data['Keywords'][i].split(\" ; \")\n            for (var t = 0; t < textlist.length; t++) {\n                text = textlist[t]\n                text = text.toLowerCase()\n                text = text.replace(/[0-9-&\\/\\\\#,+()$~%.'\":*?<>{}]/g,'')\n                if(t==0){\n                    var tab = [keywords.includes(text)]\n                }\n                if(t>0){\n                    tab.push(keywords.includes(text))\n                }\n            }\n            if(tab.reduce(reducer)==(textlist.length)){\n                cluster.push(i)\n            }\n        }\n        var indices = []\n        for (var i = 0; i < data2[\"Cluster\"].length; i++) {\n            if (cluster.includes(data2[\"Cluster\"][i])) \n                indices.push(i)\n        }\n        if (indices.length > 0) {\n            for (var i = 0; i < keys1.length; i++) {\n                for (var k = 0; k < indices.length; k++) {\n                    newdata[keys1[i]].push(data2[keys1[i]][indices[k]])\n                }\n            }\n            const returnedTarget2 = Object.assign(data, newdata)\n        }\n        if (indices.length == 0) {\n            const newdata = source1.data\n            const returnedTarget2 = Object.assign(data, newdata)\n        }\n        sourceUpdate.change.emit();\n    }\"\"\")\n\nArticleKeywordSearch = CustomJS(args=dict(source1= source1,sourceUpdate=sourceUpdate), code=\"\"\"\n    const reducer = (accumulator, currentValue) => accumulator + currentValue;\n    var data = sourceUpdate.data;\n    const data2 = source1.data\n    const returnedTarget1 = Object.assign(data, data2)\n    var text = cb_obj.value\n    if (text == \"\") {\n        const newdata = source1.data\n        const returnedTarget1 = Object.assign(data, newdata)\n        sourceUpdate.change.emit();\n    } else {\n        var textlist = text.split(';')\n        const newdata = {\"index\":[],\"X\":[],\"Y\":[],\"Colors\":[],\"title\":[],\"publish_time\":[]}\n        const keys1 = Object.keys(newdata)\n        var indices = []\n        for (var i = 0; i < data2[\"nKeywords\"].length; i++) {\n            for (var t = 0; t < textlist.length; t++) {\n                text = textlist[t]\n                text = text.toLowerCase()\n                text = text.replace(/[0-9-&\\/\\\\#,+()$~%.'\":*?<>{}]/g,'')\n                if(t==0){\n                    var tab = [data2[\"nKeywords\"][i].includes(text)]\n                }\n                if(t>0){\n                    tab.push(data2[\"nKeywords\"][i].includes(text))\n                }\n            }\n            if(tab.reduce(reducer)==(textlist.length)){\n                indices.push(i)\n            }\n        }\n        if (indices.length > 0) {\n            for (var i = 0; i < keys1.length; i++) {\n                for (var k = 0; k < indices.length; k++) {\n                    newdata[keys1[i]].push(data2[keys1[i]][indices[k]])\n                }\n            }\n            const returnedTarget2 = Object.assign(data, newdata)\n        }\n        if (indices.length == 0) {\n            const newdata = source1.data\n            const returnedTarget2 = Object.assign(data, newdata)\n        }\n        sourceUpdate.change.emit();\n    }\"\"\")\n\n\nTOOLTIPS=[\n    (\"Title\", \"@title\"),\n    (\"Publishdate\", \"@publish_time\")\n]\n\np = figure(title='t-SNE representation of the article BERT-768d-embedding reduced by PCA',tooltips=TOOLTIPS)\n\np.circle(x='X',y='Y',size=5,color='Colors', source=sourceUpdate)\n\ntext_clusterkeyword = TextInput(value=\"\", title=\"Cluster Keywords filtering\\n\\n(TIPS: ';' to separate multiple words, entry with nothing to reset):\")\ntext_clusterkeyword.js_on_change('value', ClusterKeywordSearch)\n\ntext_articlekeyword = TextInput(value=\"\", title=\"Article Keywords filtering \\n (TIPS: ';' to separate multiple words, entry with nothing to reset):\")\ntext_articlekeyword.js_on_change('value', ArticleKeywordSearch)\n\n###################### TOPIC RESEARCH SENTENCES BY BERT REPRESENTATION ######################\ndef TopicSearch(Topics,nTopArticle,nTopSentences,dateOrder):\n    if(Topics != ''):\n        if(len(Topics.split(' '))>6):\n            nTopArticle = int(nTopArticle)\n            nTopSentences = int(nTopSentences)\n            print('In Process of ',Topics)\n            \n            # Topic BERT encoding and representation\n            input_ids_1 = torch.tensor([tokenizer.encode(Topics, add_special_tokens=True)])  # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n            with torch.no_grad():\n                result = model(input_ids_1)[0]\n            result = result.numpy()[0,:,:]\n            if(result.shape[1]>1):\n                result = result.mean(axis=0)\n            \n            # Topic filtering and processing for best article keyword matching\n            x = Topics.lower()\n            x = x.split(\" \")\n            x = [match10.sub(\"\", i) for i in x]\n            x = [w for w in x if not w in stop_words]\n            x = [x[i] for i in range(len(x)) if x[i] != '']\n            \n            #Matching\n            list_indices = []\n            for i in range(X_par.shape[0]):\n                list_indices.append(sum([k in X_par['nKeywords'].iloc[i] for k in x]))\n            \n            # Take nTopArticle best articles on the n words from the topic matched on each keyword articles\n            First = len(list_indices)-1\n            sort_indices = np.sort(list_indices)\n            index_sort = np.argsort(list_indices)\n            while sort_indices[First] == max(list_indices):\n                First=First-1\n            First = First+1\n            if(len(range(First,X_par.shape[0])) > nTopArticle):\n                newindices = index_sort[First:]\n            else:\n                newindices = index_sort[nTopArticle:]  \n            \n            # Rordering on most current Dates\n            if(dateOrder):\n                date_ = []\n                for i in newindices:\n                    tmp = sub_df_covid['publish_time'].iloc[i].split(' ')\n                    if(len(tmp)>1):\n                        date_.append(int(tmp[0]))\n                    else:\n                        date_.append(int(sub_df_covid['publish_time'].iloc[i].split('-')[0]))\n                index_sort = np.argsort(date_)\n                newindices = np.flip(newindices[index_sort[(len(index_sort)-nTopArticle):]])\n                \n            # Article BERT sentence representation research loop\n            for a in newindices:\n                ## Body text preocessing \n                # Special character spliting and elimination of all \".\" not used as sentence separator\n                text = match2.sub(\"Fig\", sub_df_covid['body_text'].iloc[a])\n                text = catch1.sub(\" . \", text)\n                text = catch2.sub(\" \\n \", text)\n                text = catch3.sub(\" ( \", text)\n                text = catch4.sub(\" ) \", text)\n                text = catch5.sub(\" [ \", text)\n                text = catch6.sub(\" ] \", text)\n                text = text.split(\" . \")\n                \n                # Loop to collapse all part separated part of a parenthesis\n                h=0\n                n=len(text)\n                i=0\n                while i < n:\n                    if(h==1):\n                        i = i-1\n                    tmp = text[i].split(' ')\n                    h=0\n                    for k in range(len(tmp)):\n                        if tmp[k] in ['(','[']:\n                            h=1\n                        if ((tmp[k] in [')',']'])&(h==1)):\n                            h=0\n                        if((k==(len(tmp)-1))&(h==1)):\n                            if(i != (len(text)-1)):\n                                text[i] = text[i]+text[i+1]\n                                del text[i+1]\n                    i=i+1\n                    n= len(text)\n                text = \" . \".join(text)  \n                text = catch7.sub(\"(\", text)\n                text = catch8.sub(\")\", text)\n                text = catch9.sub(\"[\", text)\n                text = catch10.sub(\"]\", text)\n                text = text.split(\" . \")\n                text = [text[i] for i in range(len(text)) if text[i] != '']\n                \n                # Encode vocabulary body text\n                input_ids_1 = torch.tensor([tokenizer.encode(\" . \".join(text), add_special_tokens=True)])  # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n\n                # vocabulary BERT tokenized\n                old_text_token = tokenizer.tokenize(\" . \".join(text),add_special_tokens=True)\n                old_text_token.insert(0,\"<CLS>\")\n                old_text_token.insert(len(old_text_token),\"<CLS>\")\n                \n                # loop to collapse word separated in the BERT tokenized representation\n                text_token = [match4.sub(' ',old_text_token[k]) for k in range(len(old_text_token)) ]\n                new_text_token = []\n                k=0\n                h=0\n                first=0\n                list_index = np.empty(len(text_token))\n                while k < len(text_token):\n                    if(len(text_token[k].split(' '))>1):\n                        if first == 0:\n                            list_index[h-1] = 1\n                            first = 1\n                        new_text_token.append(''.join([new_text_token[k-1],text_token[k].split(' ')[1]]))\n                        del text_token[k]\n                        del new_text_token[k-1]\n                        list_index[h] = 2\n                    else:\n                        first=0\n                        new_text_token.append(text_token[k])\n                        list_index[h] = 0\n                        k=k+1\n                    h=h+1\n                \n                # Sentence reconstruction with the body text tokenized reconstructed\n                concatnex = []\n                old_i = 0\n                for i in range(len(new_text_token)):\n                    if ((new_text_token[i]=='.')|(i==(len(new_text_token)-1))):\n                        if old_i == 0:\n                            concatnex = [' '.join(new_text_token[old_i:i])]\n                        else:\n                            concatnex.append(' '.join(new_text_token[old_i:i]))\n                        old_i=i+1\n\n                \n                result_bis = torch.tensor(np.empty(list(input_ids_1.size())+[Vector_size]))   \n                \n                # In case oh the number of words in the sentence is superior of block_size of BERT: \n                if input_ids_1.shape[1] > block_size:\n                    nbatch = round(input_ids_1.shape[1]/block_size)\n                    if nbatch < input_ids_1.shape[1]/block_size:\n                        last_size = input_ids_1.shape[1]-block_size*nbatch\n                    else:\n                        nbatch = nbatch-1\n                        last_size = input_ids_1.shape[1]-block_size*nbatch\n                    iter_=0\n                    \n                # Vocabulary BERT representation\n                    with torch.no_grad():\n                        while iter_ != nbatch:\n                            result_bis[:,(iter_*block_size):((iter_+1)*block_size),:] = model(input_ids_1[:,(iter_*block_size):((iter_+1)*block_size)])[0]\n                            iter_ += 1\n                        result_bis[:,(iter_*block_size):((iter_*block_size)+last_size),:] = model(input_ids_1[:,(iter_*block_size):((iter_*block_size)+last_size)])[0]\n                else:\n                    with torch.no_grad():\n                        result_bis = model(input_ids_1)[0]\n                \n                result_bis = result_bis.numpy()[0,:,:]\n                \n                # Sentence BERT representation\n                old_i = 0\n                for i in range(len(old_text_token)):\n                    if ((old_text_token[i]=='.')|(i==(len(old_text_token)-1))):\n                        if old_i == 0:\n                            result_bis_bis = result_bis[old_i:i,:].mean(axis=0)[np.newaxis]\n                        else:\n                            result_bis_bis = np.concatenate((result_bis_bis,result_bis[old_i:i,:].mean(axis=0)[np.newaxis]))\n                        old_i=i\n                # Cosinus distance between Topic and Sentences\n                all_dist = np.array([cosine(result[np.newaxis],result_bis_bis[i,:]) for i in range(result_bis_bis.shape[0])])\n                index_sort = np.argsort(all_dist)\n                concatnex = np.array(concatnex)[index_sort[:nTopSentences]]\n                print('\\n\\n Title: ',sub_df_covid['title'].iloc[a],'\\n Sentences: ','. '.join(concatnex.tolist()))\n            display(sub_df_covid[['title',\"publish_time\", 'paper_id']].iloc[newindices,:])\n\n\n# Set up layouts and add to document\ninputs = row(column(text_clusterkeyword,text_articlekeyword),p)\nshow(inputs, notebook_handle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TOPIC RESEARCH TOOLS"},{"metadata":{"trusted":true},"cell_type":"code","source":"interact(TopicSearch, Topics='What do we know about COVID-19 risk factors ?',nTopArticle='10',nTopSentences='5',dateOrder=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interact(TopicSearch, Topics='What do we know about COVID-19 risk factors ?',nTopArticle='10',nTopSentences='5',dateOrder=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# General remarks\n* Our clustering methodology permits to have a representation of body text of each article in a 2D space with a filtering function based on mutiple keywords computed by word set counting. Our two filtering functions can be used to have a gloabl overview of the neighborhood of the articles choose in the topics research tools.\n> * This approach is simple to write but require a huge amount  of computation time depending of each body text vocabulary size to produce each article 'signature'.\n> * The keyword filtering could be easly improve by more powerfull NLP score as [Tf-idf](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction) to produce a more robust keyword set to each article and cluster and so improve the filtering.\n* The topic tool allow us to approach the answering question/topics tasks in the COVID-19 Open Research Dataset Challenge (CORD-19).\n> * Apart the inefficient filtering based on the keywords extracted on word counting, this tools extracts/highlights n closest sentences to the topics according the BERT representation. The highlighted sentences bring a quick view of the article agree to the topics words. \n> * The topics tools could be used to analyse sentences of some given articles around a specific topic or set of topics.\n> * It could be really interesting to compare a native [WordToVec](https://arxiv.org/pdf/1301.3781.pdf) ([gensim](https://radimrehurek.com/gensim/models/word2vec.html) package) approach to evaluate BERT accuracy."},{"metadata":{},"cell_type":"markdown","source":"# Credits:\n![Scalian](http://www.trocaderocp.com/wp-content/uploads/2017/01/Eurogiciel-Scalian.jpg)\nDevelopped by sing JOUAN Gabriel (<gabriel.jouan@scalian.com>),CIFRE PhD Student and MAITRE Elliot CIFRE PhD Student at [Scalian](https://www.scalian.com/accueil/).\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}