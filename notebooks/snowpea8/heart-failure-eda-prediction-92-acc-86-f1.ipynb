{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a id=\"overview\"></a>\n# Overview ğŸ§\n<img src=\"https://i.imgur.com/wC22WgL.jpeg\" width=\"600\"><br>\nWe are going to predict mortality by heart failure based on the 12 features included in the data set. This can be used to help hospitals in assessing the severity of patients with cardiovascular diseases (CVDs).<br>\n<font color=\"RoyalBlue\">ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å«ã¾ã‚Œã‚‹12ã®ç‰¹å¾´é‡ã«åŸºã¥ã„ã¦å¿ƒä¸å…¨ã«ã‚ˆã‚‹æ­»äº¡ç‡ã‚’äºˆæ¸¬ã—ã¦ã„ãã¾ã™ã€‚ã“ã®äºˆæ¸¬ã¯ã€ç—…é™¢ã§å¿ƒè¡€ç®¡ç–¾æ‚£ï¼ˆCVDsï¼‰æ‚£è€…ã®é‡ç—‡åº¦ã‚’è©•ä¾¡ã™ã‚‹éš›ã«å½¹ç«‹ã¦ã‚‰ã‚Œã‚‹ã§ã—ã‚‡ã†ã€‚</font><br>\n\nI have also run a similar analysis in R ([Heart Failureâ£ï¸ EDA & Prediction with R (91.5%acc)](https://www.kaggle.com/snowpea8/heart-failure-eda-prediction-with-r-91-5-acc)), \nif you would like to take a look at it.<br>\n<font color=\"RoyalBlue\">åŒæ§˜ã®åˆ†æã‚’ R ã§ã‚‚å®Ÿè¡Œã—ã¦ã„ã¾ã™ã®ã§ã€ãã¡ã‚‰ã‚‚å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚</font><br>\n\nIn this notebook, we will first discover and visualize the data to gain insights. Then we split the data into a training and a test set and use the training set to train various machine learning models. At the same time, we evaluate the performance of the models with cross-validation. Finally, we will ensemble each model to improve its accuracy.<br>\n<font color=\"RoyalBlue\">ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€ã¾ãšæ´å¯Ÿã‚’å¾—ã‚‹ãŸã‚ã«ãƒ‡ãƒ¼ã‚¿ã‚’ç ”ç©¶ã€å¯è¦–åŒ–ã—ã¾ã™ã€‚ãã‚Œã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ã«åˆ†å‰²ã—ã€è¨“ç·´ã‚»ãƒƒãƒˆã‚’ä½¿ã£ã¦æ§˜ã€…ãªæ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¾ã™ã€‚åŒæ™‚ã«ã€äº¤å·®æ¤œè¨¼ã§ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡ã—ã¾ã™ã€‚æœ€å¾Œã«ãã‚Œãã‚Œã®ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã—ã€ç²¾åº¦ã®å‘ä¸Šã‚’ç›®æŒ‡ã—ã¦ã„ãã¾ã™ã€‚</font>\n\n# Table of contents ğŸ“–\n* [Overview ğŸ§](#overview)\n* [Setup ğŸ’»](#setup)\n* [Load CSV data ğŸ“ƒ](#load)\n* [Explore CSV data ğŸ“Š](#explore)\n    * [Distribution of the binary features](#binary)\n    * [Distribution of the numeric features](#numeric)\n* [Data preprocessing ğŸ§¹](#preprocessing)\n* [Train models and make predictions ğŸ’­](#models)\n    * [LightGBM ğŸŒ³](#gbm)\n    * [XGBoost ğŸŒ³](#xgb)\n    * [CatBoost ğŸŒ³](#cat)\n    * [Random forest ğŸŒ³](#rf)\n    * [Extremely randomized trees ğŸŒ³](#ert)\n    * [Linear model ğŸ“ˆ](#lm)\n    * [Deep learning ğŸ§ ](#dl)\n* [Simple ensemble ğŸ¤](#ensemble)\n\n<a id=\"setup\"></a>\n# Setup ğŸ’»\nAll seed values are fixed at zero.<br>\n<font color=\"RoyalBlue\">ã‚·ãƒ¼ãƒ‰å€¤ã¯å…¨ã¦0ã§å›ºå®šã—ã¦ã„ã¾ã™ã€‚</font><br>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.graph_objects as go\nfrom mlxtend.plotting import plot_confusion_matrix\n\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nimport tensorflow as tf\n\nimport optuna\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\n\ndef seed_everything(seed=2020):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\nseed_everything(0)\n\nsns.set_style(\"whitegrid\")\npalette_ro = [\"#ee2f35\", \"#fa7211\", \"#fbd600\", \"#75c731\", \"#1fb86e\", \"#0488cf\", \"#7b44ab\"]\n\nROOT = \"../input/heart-failure-clinical-data\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"load\"></a>\n# Load CSV data ğŸ“ƒ"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(ROOT + \"/heart_failure_clinical_records_dataset.csv\")\n\nprint(\"Data shape: \", df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataset from: [Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone](https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5)\n\n* `age` - Age\n* `anaemia` - Decrease of red blood cells or hemoglobin (boolean) (0:`False`, 1:`True`)\n<br>ã€€<font color=\"RoyalBlue\">è²§è¡€ - èµ¤è¡€çƒã¾ãŸã¯ãƒ˜ãƒ¢ã‚°ãƒ­ãƒ“ãƒ³ã®æ¸›å°‘ãŒèµ·ã“ã£ã¦ã„ã‚‹ã‹</font>\n* `creatinine_phosphokinase` - Level of the CPK enzyme in the blood (mcg/L)\n<br>ã€€<font color=\"RoyalBlue\">ã‚¯ãƒ¬ã‚¢ãƒãƒ³ãƒ•ã‚©ã‚¹ãƒ•ã‚©ã‚­ãƒŠãƒ¼ã‚¼ - è¡€ä¸­ CPK é…µç´ ï¼ˆç­‹è‚‰ç´°èƒã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ä»£è¬ã«é‡è¦ãªå½¹å‰²ã‚’æœãŸã™é…µç´ ï¼‰ã®ãƒ¬ãƒ™ãƒ«ï¼ˆÎ¼g/Lï¼‰</font>\n* `diabetes` - If the patient has diabetes (boolean) (0:`False`, 1:`True`)\n<br>ã€€<font color=\"RoyalBlue\">ç³–å°¿ç—… - æ‚£è€…ãŒç³–å°¿ç—…ã‹ã©ã†ã‹</font>\n* `ejection_fraction` - Percentage of blood leaving the heart at each contraction (percentage)\n<br>ã€€<font color=\"RoyalBlue\">é§†å‡ºç‡ - å¿ƒæ‹ã”ã¨ã«å¿ƒè‡“ãŒé€ã‚Šå‡ºã™è¡€æ¶²é‡ï¼ˆé§†å‡ºé‡ï¼‰ï¼å¿ƒè‡“ãŒæ‹¡å¼µã—ãŸã¨ãã®å·¦å¿ƒå®¤å®¹é‡ï¼ˆï¼…ï¼‰\n<br>ã€€ï¼ˆâ€»ã€€å…ƒè«–æ–‡ã§é‡è¦è¦–ï¼‰</font>\n* `high_blood_pressure` - If the patient has hypertension (boolean) (0:`False`, 1:`True`)\n<br>ã€€<font color=\"RoyalBlue\">é«˜è¡€åœ§ - æ‚£è€…ãŒé«˜è¡€åœ§ã‹ã©ã†ã‹</font>\n* `platelets` - Platelets in the blood (kiloplatelets/mL)\n<br>ã€€<font color=\"RoyalBlue\">è¡€å°æ¿æ•° - è¡€ä¸­ã®è¡€å°æ¿æ•°ï¼ˆåƒï¼mLï¼‰</font>\n* `serum_creatinine` - Level of serum creatinine in the blood (mg/dL)\n<br>ã€€<font color=\"RoyalBlue\">è¡€æ¸…ã‚¯ãƒ¬ã‚¢ãƒãƒ‹ãƒ³å€¤ - è¡€ä¸­ã®è¡€æ¸…ã‚¯ãƒ¬ã‚¢ãƒãƒ‹ãƒ³ï¼ˆè…è‡“ã®ç³¸çƒä½“ã‹ã‚‰æ’æ³„ã•ã‚Œã‚‹ï¼‰ã®ãƒ¬ãƒ™ãƒ«ï¼ˆmg/dLï¼‰\n<br>ã€€ï¼ˆâ€»ã€€å…ƒè«–æ–‡ã§é‡è¦è¦–ï¼‰</font>\n* `serum_sodium` - Level of serum sodium in the blood (mEq/L)\n<br>ã€€<font color=\"RoyalBlue\">è¡€æ¸…ãƒŠãƒˆãƒªã‚¦ãƒ å€¤ - è¡€ä¸­ã®è¡€æ¸…ãƒŠãƒˆãƒªã‚¦ãƒ å€¤ã®ãƒ¬ãƒ™ãƒ«ï¼ˆmEq/Lï¼‰</font>\n* `sex` - Woman or man (binary) (0: Woman, 1: Man)\n* `smoking` - If the patient smokes or not (boolean) (0:`False`, 1:`True`)\n* `time` - Follow-up period (days)\n<br>ã€€<font color=\"RoyalBlue\">æ™‚é–“ - æ‚£è€…ã®çµŒéè¦³å¯Ÿæ™‚é–“ï¼ˆæ—¥ï¼‰</font>\n* `DEATH_EVENT` - If the patient deceased during the follow-up period (boolean)\n<br>ã€€<font color=\"RoyalBlue\">æ­»äº¡ - çµŒéè¦³å¯ŸæœŸé–“ä¸­ã«æ‚£è€…ãŒæ­»äº¡ã—ãŸã‹ã©ã†ã‹</font>\n\n<a id=\"explore\"></a>\n# Explore CSV data ğŸ“Š"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"binary\"></a>\n## Distribution of the binary features"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(16, 12))\n\nsns.countplot(x=\"anaemia\", ax=ax1, data=df,\n              palette=palette_ro[3::-3], alpha=0.9)\nsns.countplot(x=\"diabetes\", ax=ax2, data=df,\n              palette=palette_ro[3::-3], alpha=0.9)\nsns.countplot(x=\"high_blood_pressure\", ax=ax3, data=df,\n              palette=palette_ro[3::-3], alpha=0.9)\nsns.countplot(x=\"sex\", ax=ax4, data=df,\n              palette=palette_ro[2::3], alpha=0.9)\nsns.countplot(x=\"smoking\", ax=ax5, data=df,\n              palette=palette_ro[3::-3], alpha=0.9)\nsns.countplot(x=\"DEATH_EVENT\", ax=ax6, data=df,\n              palette=palette_ro[1::5], alpha=0.9)\nfig.suptitle(\"Distribution of the binary features and DEATH_EVENT\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Insights:\n\n* The distribution of the objective variable is not 1:1, but is biased.\n<br>ã€€<font color=\"RoyalBlue\">ç›®çš„å¤‰æ•°ã®åˆ†å¸ƒã¯ï¼‘ï¼šï¼‘ã§ã¯ãªãã€åã‚ŠãŒã‚ã‚‹ã€‚</font>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"bin_features = [\"anaemia\", \"diabetes\", \"high_blood_pressure\", \"sex\", \"smoking\"]\n\ndf_d = pd.DataFrame(columns=[0, 1, \"value\"])\nfor col in bin_features:\n    for u in df[col].unique():\n        df_d.loc[col+\"_\"+str(u)] = 0\n        for i in df[\"DEATH_EVENT\"].unique():\n            if u == 0:\n                df_d[\"value\"][col+\"_\"+str(u)] = \"0 (False)\"\n            else:\n                df_d[\"value\"][col+\"_\"+str(u)] = \"1 (True)\"\n            df_d[i][col+\"_\"+str(u)] = df[df[col]==u][\"DEATH_EVENT\"].value_counts(normalize=True)[i] * 100\n\ndf_d = df_d.reindex(index=[\"anaemia_0\", \"anaemia_1\", \"diabetes_0\", \"diabetes_1\", \"high_blood_pressure_0\", \"high_blood_pressure_1\",\n                           \"sex_0\", \"sex_1\", \"smoking_0\", \"smoking_1\"])\ndf_d.at[\"sex_0\", \"value\"] = \"0 (Female)\"\ndf_d.at[\"sex_1\", \"value\"] = \"1 (Male)\"\n\nfig = go.Figure(data=[\n    go.Bar(y=[[\"anaemia\", \"anaemia\",\"diabetes\",\"diabetes\",\"high_blood_pressure\",\"high_blood_pressure\",\"sex\",\"sex\",\"smoking\",\"smoking\"], list(df_d[\"value\"])],\n           x=df_d[0], name=\"DEATH_EVENT = 0<br>(survived)\", orientation='h', marker=dict(color=palette_ro[1])),\n    go.Bar(y=[[\"anaemia\", \"anaemia\",\"diabetes\",\"diabetes\",\"high_blood_pressure\",\"high_blood_pressure\",\"sex\",\"sex\",\"smoking\",\"smoking\"], list(df_d[\"value\"])],\n           x=df_d[1], name=\"DEATH_EVENT = 1<br>(dead)\", orientation='h', marker=dict(color=palette_ro[6]))\n])\nfig.update_layout(barmode=\"stack\",\n                  title=\"Percentage of DEATH_EVENT per binary features\")\nfig.update_yaxes(autorange=\"reversed\")\nfig.show(config={\"displayModeBar\": False})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Insights:\n\n* For diabetes, sex, and smoking, there was little difference in the distribution of the objective variable.\n<br>ã€€<font color=\"RoyalBlue\">diabetes, sex, smoking ã«ãŠã„ã¦ã¯ã€ç›®çš„å¤‰æ•°ã®åˆ†å¸ƒã«ã»ã¨ã‚“ã©å·®ã¯è¦‹ã‚‰ã‚Œãªã„ã€‚</font>\n* For anaemia and high_blood_pressure, there are some differences in the distributions of the objective variables, but we do not know if we can say that the differences are significant.\n<br>ã€€<font color=\"RoyalBlue\">anaemia, high_blood_pressure ã«ãŠã„ã¦ã¯ã€ç›®çš„å¤‰æ•°ã®åˆ†å¸ƒã«å¤šå°‘ã®å·®ãŒã‚ã‚‹ãŒã€æœ‰æ„ãªå·®ãŒã‚ã‚‹ã¨è¨€ãˆã‚‹ã‹ã©ã†ã‹ã¯åˆ†ã‹ã‚‰ãªã„ã€‚</font>\n\n<a id=\"numeric\"></a>\n## Distribution of the numeric features"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n\nrange_bin_width = np.arange(df[\"age\"].min(), df[\"age\"].max()+5, 5)\n\nsns.distplot(df[\"age\"], ax=ax1, bins=range_bin_width, color=palette_ro[5])\nsns.distplot(df[df[\"DEATH_EVENT\"]==0].age, label=\"DEATH_EVENT=0\", ax=ax2, bins=range_bin_width, color=palette_ro[1])\nsns.distplot(df[df[\"DEATH_EVENT\"]==1].age, label=\"DEATH_EVENT=1\", ax=ax2, bins=range_bin_width, color=palette_ro[6])\nax1.set_title(\"age distribution\", fontsize=16);\nax2.set_title(\"Relationship between age and DEATH_EVENT\", fontsize=16)\n\nax1.axvline(x=df[\"age\"].median(), color=palette_ro[5], linestyle=\"--\", alpha=0.5)\nax2.axvline(x=df[df[\"DEATH_EVENT\"]==0].age.median(), color=palette_ro[1], linestyle=\"--\", alpha=0.5)\nax2.axvline(x=df[df[\"DEATH_EVENT\"]==1].age.median(), color=palette_ro[6], linestyle=\"--\", alpha=0.5)\n\nax1.annotate(\"Min: {:.0f}\".format(df[\"age\"].min()), xy=(df[\"age\"].min(), 0.010), \n             xytext=(df[\"age\"].min()-7, 0.015),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.2\"))\nax1.annotate(\"Max: {:.0f}\".format(df[\"age\"].max()), xy=(df[\"age\"].max(), 0.005), \n             xytext=(df[\"age\"].max(), 0.008),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=-0.2\"))\nax1.annotate(\"Med: {:.0f}\".format(df[\"age\"].median()), xy=(df[\"age\"].median(), 0.032), \n             xytext=(df[\"age\"].median()-8, 0.035),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.25\"))\n\nax2.annotate(\"Survived\\nMed: {:.0f}\".format(df[df[\"DEATH_EVENT\"]==0].age.median()), xy=(df[df[\"DEATH_EVENT\"]==0].age.median(), 0.033), \n             xytext=(df[df[\"DEATH_EVENT\"]==0].age.median()-18, 0.035),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=-0.25\"))\nax2.annotate(\"Dead\\nMed: {:.0f}\".format(df[df[\"DEATH_EVENT\"]==1].age.median()), xy=(df[df[\"DEATH_EVENT\"]==1].age.median(), 0.026), \n             xytext=(df[df[\"DEATH_EVENT\"]==1].age.median()+7, 0.029),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.25\"))\nax2.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Insights:\n\n* The age of patients was highest around 60 years old, and the number of patients decreased in a bell-shaped pattern around that age.\n<br>ã€€<font color=\"RoyalBlue\">æ‚£è€…ã®å¹´é½¢ã¯60æ­³ä»˜è¿‘ãŒæœ€ã‚‚å¤šãã€ãã“ã‚’ä¸­å¿ƒã«é‡£é˜çŠ¶ã«æ¸›å°‘ã—ã¦ã„ã‚‹ã€‚</font>\n* There is a difference in the distribution of each objective variable, with the younger the age, the more difficult it is to die; the probability density reverses after the age of just under 70.\n<br>ã€€<font color=\"RoyalBlue\">ç›®çš„å¤‰æ•°åˆ¥ã«è¦‹ã‚‹ã¨åˆ†å¸ƒã«å·®ãŒã‚ã‚Šã€å¹´é½¢ãŒè‹¥ã„ã»ã©æ­»äº¡ã—ã¥ã‚‰ã„å‚¾å‘ã«ã‚ã‚‹ã€‚70æ­³å¼±ã‚’å¢ƒã«ç¢ºç‡å¯†åº¦ãŒé€†è»¢ã™ã‚‹ã€‚</font>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n\nsns.distplot(df[\"creatinine_phosphokinase\"], ax=ax1, color=palette_ro[5])\nsns.distplot(df[df[\"DEATH_EVENT\"]==0].creatinine_phosphokinase, label=\"DEATH_EVENT=0\", ax=ax2, hist=None, color=palette_ro[1])\nsns.distplot(df[df[\"DEATH_EVENT\"]==1].creatinine_phosphokinase, label=\"DEATH_EVENT=1\", ax=ax2, hist=None, color=palette_ro[6])\nax1.set_title(\"creatinine_phosphokinase distribution\", fontsize=16);\nax2.set_title(\"Relationship between creatinine_phosphokinase and DEATH_EVENT\", fontsize=16)\n\nax1.axvline(x=df[\"creatinine_phosphokinase\"].median(), color=palette_ro[5], linestyle=\"--\", alpha=0.5)\nax2.axvline(x=df[df[\"DEATH_EVENT\"]==0].creatinine_phosphokinase.median(), color=palette_ro[1], linestyle=\"--\", alpha=0.5)\nax2.axvline(x=df[df[\"DEATH_EVENT\"]==1].creatinine_phosphokinase.median(), color=palette_ro[6], linestyle=\"--\", alpha=0.5)\n\nax1.annotate(\"Min: {:,}\".format(df[\"creatinine_phosphokinase\"].min()), xy=(df[\"creatinine_phosphokinase\"].min(), 0.00085), \n             xytext=(df[\"creatinine_phosphokinase\"].min()-700, 0.0010),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.2\"))\nax1.annotate(\"Max: {:,}\".format(df[\"creatinine_phosphokinase\"].max()), xy=(df[\"creatinine_phosphokinase\"].max(), 0.00005), \n             xytext=(df[\"creatinine_phosphokinase\"].max()-500, 0.0002),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.2\"))\nax1.annotate(\"Med: {:.0f}\".format(df[\"creatinine_phosphokinase\"].median()), xy=(df[\"creatinine_phosphokinase\"].median(), 0.0014), \n             xytext=(df[\"creatinine_phosphokinase\"].median()+500, 0.0015),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.25\"))\n\nax2.annotate(\"Survived\\nMed: {:.0f}\".format(df[df[\"DEATH_EVENT\"]==0].creatinine_phosphokinase.median()), xy=(df[df[\"DEATH_EVENT\"]==0].creatinine_phosphokinase.median(), 0.00145), \n             xytext=(df[df[\"DEATH_EVENT\"]==0].creatinine_phosphokinase.median()+600, 0.00145),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.25\"))\nax2.annotate(\"Dead\\nMed: {:.0f}\".format(df[df[\"DEATH_EVENT\"]==1].creatinine_phosphokinase.median()), xy=(df[df[\"DEATH_EVENT\"]==1].creatinine_phosphokinase.median(), 0.00135), \n             xytext=(df[df[\"DEATH_EVENT\"]==1].creatinine_phosphokinase.median()+700, 0.00125),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=-0.25\"))\nax2.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Insights:\n\n* The distribution is heavily skewed to one side, with the highest value more than 30 times the median.\n<br>ã€€<font color=\"RoyalBlue\">ç‰‡å´ã«è£¾ã®é‡ã„åˆ†å¸ƒã¨ãªã£ã¦ãŠã‚Šã€æœ€é«˜ã§ä¸­å¤®å€¤ã®30å€ä»¥ä¸Šã®å€¤ã‚’æŒã¤ã‚±ãƒ¼ã‚¹ãŒã‚ã‚‹ã€‚</font>\n* By objective variable, there is little difference in the median, although there are some differences in the distribution.\n<br>ã€€<font color=\"RoyalBlue\">ç›®çš„å¤‰æ•°åˆ¥ã«è¦‹ã‚‹ã¨ã€åˆ†å¸ƒã«å¤šå°‘ã®é•ã„ã¯ã‚ã‚Œã©ã€ä¸­å¤®å€¤ã«ã¯ã»ã¨ã‚“ã©å·®ãŒç„¡ã„ã€‚</font>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n\nrange_bin_width = np.arange(df[\"ejection_fraction\"].min(), df[\"ejection_fraction\"].max()+1, 1)\n\nsns.distplot(df[\"ejection_fraction\"], ax=ax1, bins=range_bin_width, color=palette_ro[5])\nsns.distplot(df[df[\"DEATH_EVENT\"]==0].ejection_fraction, label=\"DEATH_EVENT=0\", ax=ax2, bins=range_bin_width, color=palette_ro[1])\nsns.distplot(df[df[\"DEATH_EVENT\"]==1].ejection_fraction, label=\"DEATH_EVENT=1\", ax=ax2, bins=range_bin_width, color=palette_ro[6])\nax1.set_title(\"ejection_fraction distribution\", fontsize=16);\nax2.set_title(\"Relationship between ejection_fraction and DEATH_EVENT\", fontsize=16)\n\nax1.axvline(x=df[\"ejection_fraction\"].median(), color=palette_ro[5], linestyle=\"--\", alpha=0.5)\nax2.axvline(x=df[df[\"DEATH_EVENT\"]==0].ejection_fraction.median(), color=palette_ro[1], linestyle=\"--\", alpha=0.5)\nax2.axvline(x=df[df[\"DEATH_EVENT\"]==1].ejection_fraction.median(), color=palette_ro[6], linestyle=\"--\", alpha=0.5)\n\nax1.annotate(\"Min: {:,}\".format(df[\"ejection_fraction\"].min()), xy=(df[\"ejection_fraction\"].min(), 0.005), \n             xytext=(df[\"ejection_fraction\"].min()-5, 0.022),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.2\"))\nax1.annotate(\"Max: {:,}\".format(df[\"ejection_fraction\"].max()), xy=(df[\"ejection_fraction\"].max(), 0.001), \n             xytext=(df[\"ejection_fraction\"].max(), 0.022),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=-0.2\"))\nax1.annotate(\"Med: {:.0f}\".format(df[\"ejection_fraction\"].median()), xy=(df[\"ejection_fraction\"].median(), 0.041), \n             xytext=(df[\"ejection_fraction\"].median()+5, 0.074),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.25\"))\n\nax2.annotate(\"Survived\\nMed: {:.0f}\".format(df[df[\"DEATH_EVENT\"]==0].ejection_fraction.median()), xy=(df[df[\"DEATH_EVENT\"]==0].ejection_fraction.median(), 0.051), \n             xytext=(df[df[\"DEATH_EVENT\"]==0].ejection_fraction.median()+5, 0.091),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.25\"))\nax2.annotate(\"Dead\\nMed: {:.0f}\".format(df[df[\"DEATH_EVENT\"]==1].ejection_fraction.median()), xy=(df[df[\"DEATH_EVENT\"]==1].ejection_fraction.median(), 0.03), \n             xytext=(df[df[\"DEATH_EVENT\"]==1].ejection_fraction.median()-18, 0.04),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=-0.25\"))\nax2.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Insights:\n\n* The distribution is discrete, not continuous, with the first peak near 38 and the second peak near 60.\n<br>ã€€<font color=\"RoyalBlue\">é€£ç¶šçš„ã§ã¯ãªãé›¢æ•£çš„ãªåˆ†å¸ƒã‚’ã¨ã£ã¦ã„ã‚‹ã€‚38ä»˜è¿‘ã«ç¬¬ä¸€ã®å±±ãŒã€60ä»˜è¿‘ã«ç¬¬äºŒã®å±±ãŒã‚ã‚‹ã€‚</font>\n* By objective variable, there are considerable differences in the shape of the distribution and in the median. Survivors are mostly located near the first and second mountains. The values of the dead are mostly around 30 and decrease slowly from there.\n<br>ã€€<font color=\"RoyalBlue\">ç›®çš„å¤‰æ•°åˆ¥ã«è¦‹ã‚‹ã¨ã€åˆ†å¸ƒã®å½¢ã«ã‚‚ä¸­å¤®å€¤ã«ã‚‚ã‹ãªã‚Šã®å·®ãŒã‚ã‚‹ã€‚ç”Ÿå­˜è€…ã¯ç¬¬ä¸€ã®å±±ã¨ç¬¬äºŒã®å±±ä»˜è¿‘ã«å¤šã„ã€‚æ­»äº¡è€…ã®å€¤ã¯30ä»˜è¿‘ãŒå¤šãã€ãã“ã‹ã‚‰ç·©ã‚„ã‹ã«æ¸›å°‘ã—ã¦ã„ãã€‚</font>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n\nsns.distplot(df[\"platelets\"], ax=ax1, color=palette_ro[5])\nsns.distplot(df[df[\"DEATH_EVENT\"]==0].platelets, label=\"DEATH_EVENT=0\", ax=ax2, hist=None, color=palette_ro[1])\nsns.distplot(df[df[\"DEATH_EVENT\"]==1].platelets, label=\"DEATH_EVENT=1\", ax=ax2, hist=None, color=palette_ro[6])\nax1.set_title(\"platelets distribution\", fontsize=16);\nax2.set_title(\"Relationship between platelets and DEATH_EVENT\", fontsize=16)\n\nax1.axvline(x=df[\"platelets\"].median(), color=palette_ro[5], linestyle=\"--\", alpha=0.5)\nax2.axvline(x=df[df[\"DEATH_EVENT\"]==0].platelets.median(), color=palette_ro[1], linestyle=\"--\", alpha=0.5)\nax2.axvline(x=df[df[\"DEATH_EVENT\"]==1].platelets.median(), color=palette_ro[6], linestyle=\"--\", alpha=0.5)\n\nax1.annotate(\"Min: {:,.0f}\".format(df[\"platelets\"].min()), xy=(df[\"platelets\"].min(), 2e-7), \n             xytext=(df[\"platelets\"].min()-50000, 7e-7),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.2\"))\nax1.annotate(\"Max: {:,.0f}\".format(df[\"platelets\"].max()), xy=(df[\"platelets\"].max(), 1e-7), \n             xytext=(df[\"platelets\"].max()-30000, 7e-7),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=-0.2\"))\nax1.annotate(\"Med: {:,.0f}\".format(df[\"platelets\"].median()), xy=(df[\"platelets\"].median(), 5.9e-6), \n             xytext=(df[\"platelets\"].median()+25000, 5.5e-6),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.25\"))\n\nax2.annotate(\"Survived\\nMed: {:,.0f}\".format(df[df[\"DEATH_EVENT\"]==0].platelets.median()), xy=(df[df[\"DEATH_EVENT\"]==0].platelets.median(), 6.2e-6), \n             xytext=(df[df[\"DEATH_EVENT\"]==0].platelets.median()+50000, 5.5e-6),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.25\"))\nax2.annotate(\"Dead\\nMed: {:,.0f}\".format(df[df[\"DEATH_EVENT\"]==1].platelets.median()), xy=(df[df[\"DEATH_EVENT\"]==1].platelets.median(), 4.5e-6), \n             xytext=(df[df[\"DEATH_EVENT\"]==1].platelets.median()-200000, 5.2e-6),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=-0.25\"))\nax2.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Insights:\n\n* The distribution is roughly symmetrical and almost bell-shaped.\n<br>ã€€<font color=\"RoyalBlue\">å·¦å³å¯¾ç§°ã®é‡£é˜çŠ¶ã«è¿‘ã„åˆ†å¸ƒã‚’ã¨ã£ã¦ã„ã‚‹ã€‚</font>\n* By objective variable, there is little difference in the median. Survivors have slightly higher platelet counts, and the values are clustered around the median.\n<br>ã€€<font color=\"RoyalBlue\">ç›®çš„å¤‰æ•°åˆ¥ã«è¦‹ã‚‹ã¨ã€ä¸­å¤®å€¤ã«ã»ã¨ã‚“ã©å·®ã¯ç„¡ã„ã€‚ç”Ÿå­˜è€…ã®æ–¹ãŒè‹¥å¹²è¡€å°æ¿æ•°ãŒå¤šãã€å€¤ãŒä¸­å¤®å€¤ä»˜è¿‘ã«é›†ã¾ã£ã¦ã„ã‚‹ã€‚</font>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n\nrange_bin_width = np.arange(df[\"serum_creatinine\"].min(), df[\"serum_creatinine\"].max()+0.25, 0.25)\n\nsns.distplot(df[\"serum_creatinine\"], ax=ax1, bins=range_bin_width, color=palette_ro[5])\nsns.distplot(df[df[\"DEATH_EVENT\"]==0].serum_creatinine, label=\"DEATH_EVENT=0\", ax=ax2, bins=range_bin_width, color=palette_ro[1])\nsns.distplot(df[df[\"DEATH_EVENT\"]==1].serum_creatinine, label=\"DEATH_EVENT=1\", ax=ax2, bins=range_bin_width, color=palette_ro[6])\nax1.set_title(\"serum_creatinine distribution\", fontsize=16);\nax2.set_title(\"Relationship serum_creatinine age and DEATH_EVENT\", fontsize=16)\n\nax1.axvline(x=df[\"serum_creatinine\"].median(), color=palette_ro[5], linestyle=\"--\", alpha=0.5)\nax2.axvline(x=df[df[\"DEATH_EVENT\"]==0].serum_creatinine.median(), color=palette_ro[1], linestyle=\"--\", alpha=0.5)\nax2.axvline(x=df[df[\"DEATH_EVENT\"]==1].serum_creatinine.median(), color=palette_ro[6], linestyle=\"--\", alpha=0.5)\n\nax1.annotate(\"Min: {:.1f}\".format(df[\"serum_creatinine\"].min()), xy=(df[\"serum_creatinine\"].min(), 0.31), \n             xytext=(df[\"serum_creatinine\"].min()-0.7, 0.5),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.2\"))\nax1.annotate(\"Max: {:.1f}\".format(df[\"serum_creatinine\"].max()), xy=(df[\"serum_creatinine\"].max(), 0.05), \n             xytext=(df[\"serum_creatinine\"].max()-0.2, 0.25),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=-0.2\"))\nax1.annotate(\"Med: {:.1f}\".format(df[\"serum_creatinine\"].median()), xy=(df[\"serum_creatinine\"].median(), 1.22), \n             xytext=(df[\"serum_creatinine\"].median()+0.5, 1.3),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.25\"))\n\nax2.annotate(\"Survived\\nMed: {:.1f}\".format(df[df[\"DEATH_EVENT\"]==0].serum_creatinine.median()), xy=(df[df[\"DEATH_EVENT\"]==0].serum_creatinine.median(), 1.47), \n             xytext=(df[df[\"DEATH_EVENT\"]==0].serum_creatinine.median()-1.3, 1.5),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=-0.25\"))\nax2.annotate(\"Dead\\nMed: {:.1f}\".format(df[df[\"DEATH_EVENT\"]==1].serum_creatinine.median()), xy=(df[df[\"DEATH_EVENT\"]==1].serum_creatinine.median(), 0.62), \n             xytext=(df[df[\"DEATH_EVENT\"]==1].serum_creatinine.median()+0.4, 0.7),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.25\"))\nax2.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Insights:\n\n* The distribution is heavily skewed to one side, with rare cases having values more than four times the median.\n<br>ã€€<font color=\"RoyalBlue\">ç‰‡å´ã«è£¾ã®é‡ã„åˆ†å¸ƒã¨ãªã£ã¦ãŠã‚Šã€ç¨€ã«ä¸­å¤®å€¤ã®ï¼”å€ä»¥ä¸Šã®å€¤ã‚’æŒã¤ã‚±ãƒ¼ã‚¹ãŒã‚ã‚‹ã€‚</font>\n* By objective variable, there are considerable differences in the shape of the distribution. For survivors, the values are clustered around the median, but for the dead, there are often cases where the values exceed 1.5.\n<br>ã€€<font color=\"RoyalBlue\">ç›®çš„å¤‰æ•°åˆ¥ã«è¦‹ã‚‹ã¨ã€åˆ†å¸ƒã®å½¢ã«ã‹ãªã‚Šã®å·®ãŒã‚ã‚‹ã€‚ç”Ÿå­˜è€…ã¯å€¤ãŒã»ã¼ä¸­å¤®å€¤ä»˜è¿‘ã«é›†ã¾ã£ã¦ã„ã‚‹ãŒã€æ­»äº¡è€…ã¯1.5ã‚’è¶…ãˆã‚‹ã‚ˆã†ãªã‚±ãƒ¼ã‚¹ãŒã—ã°ã—ã°ã‚ã‚‹ã€‚</font>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n\nrange_bin_width = np.arange(df[\"serum_sodium\"].min(), df[\"serum_sodium\"].max()+1, 1)\n\nsns.distplot(df[\"serum_sodium\"], ax=ax1, bins=range_bin_width, color=palette_ro[5])\nsns.distplot(df[df[\"DEATH_EVENT\"]==0].serum_sodium, label=\"DEATH_EVENT=0\", ax=ax2, bins=range_bin_width, color=palette_ro[1])\nsns.distplot(df[df[\"DEATH_EVENT\"]==1].serum_sodium, label=\"DEATH_EVENT=1\", ax=ax2, bins=range_bin_width, color=palette_ro[6])\nax1.set_title(\"serum_sodium distribution\", fontsize=16);\nax2.set_title(\"Relationship between serum_sodium and DEATH_EVENT\", fontsize=16)\n\nax1.axvline(x=df[\"serum_sodium\"].median(), color=palette_ro[5], linestyle=\"--\", alpha=0.5)\nax2.axvline(x=df[df[\"DEATH_EVENT\"]==0].serum_sodium.median(), color=palette_ro[1], linestyle=\"--\", alpha=0.5)\nax2.axvline(x=df[df[\"DEATH_EVENT\"]==1].serum_sodium.median(), color=palette_ro[6], linestyle=\"--\", alpha=0.5)\n\nax1.annotate(\"Min: {:.0f}\".format(df[\"serum_sodium\"].min()), xy=(df[\"serum_sodium\"].min(), 0.005), \n             xytext=(df[\"serum_sodium\"].min()-3, 0.015),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.2\"))\nax1.annotate(\"Max: {:.0f}\".format(df[\"serum_sodium\"].max()), xy=(df[\"serum_sodium\"].max(), 0.005), \n             xytext=(df[\"serum_sodium\"].max(), 0.015),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=-0.2\"))\nax1.annotate(\"Med: {:.0f}\".format(df[\"serum_sodium\"].median()), xy=(df[\"serum_sodium\"].median(), 0.103), \n             xytext=(df[\"serum_sodium\"].median()-6, 0.115),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=-0.25\"))\n\nax2.annotate(\"Survived\\nMed: {:.0f}\".format(df[df[\"DEATH_EVENT\"]==0].serum_sodium.median()), xy=(df[df[\"DEATH_EVENT\"]==0].serum_sodium.median(), 0.117), \n             xytext=(df[df[\"DEATH_EVENT\"]==0].serum_sodium.median()+5, 0.135),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.25\"))\nax2.annotate(\"Dead\\nMed: {:.0f}\".format(df[df[\"DEATH_EVENT\"]==1].serum_sodium.median()), xy=(df[df[\"DEATH_EVENT\"]==1].serum_sodium.median(), 0.09), \n             xytext=(df[df[\"DEATH_EVENT\"]==1].serum_sodium.median()-5.5, 0.11),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=-0.25\"))\nax2.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Insights:\n\n* The distribution is roughly symmetrical and almost bell-shaped, with no value exceeding 148, but there are rare cases below 125.\n<br>ã€€<font color=\"RoyalBlue\">ã»ã¨ã‚“ã©å·¦å³å¯¾ç§°ã®é‡£é˜å‹ã«è¿‘ã„åˆ†å¸ƒã§ã€148ã‚’è¶…ãˆã‚‹å€¤ã¯ç„¡ã„ãŒã€125æœªæº€ã®ã‚±ãƒ¼ã‚¹ã¯ç¨€ã«å­˜åœ¨ã™ã‚‹ã€‚</font>\n* By objective variable, there is some difference in the median and in the distribution. The values of survivors are clustered around the median, while the values of deaths are lower and tend to be more dispersed.\n<br>ã€€<font color=\"RoyalBlue\">ç›®çš„å¤‰æ•°åˆ¥ã«è¦‹ã‚‹ã¨ã€ä¸­å¤®å€¤ã«ã‚‚åˆ†å¸ƒã«ã‚‚å¤šå°‘ã®å·®ãŒã‚ã‚‹ã€‚ç”Ÿå­˜è€…ã®å€¤ã¯ä¸­å¤®å€¤ä»˜è¿‘ã«é›†ã¾ã£ã¦ã„ã‚‹ãŒã€æ­»äº¡è€…ã®å€¤ã¯ã‚ˆã‚Šä½ãã€åˆ†æ•£å‚¾å‘ã«ã‚ã‚‹ã€‚</font>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n\nrange_bin_width = np.arange(df[\"time\"].min(), df[\"time\"].max()+10, 10)\n\nsns.distplot(df[\"time\"], ax=ax1, bins=range_bin_width, color=palette_ro[5])\nsns.distplot(df[df[\"DEATH_EVENT\"]==0].time, label=\"DEATH_EVENT=0\", ax=ax2, bins=range_bin_width, color=palette_ro[1])\nsns.distplot(df[df[\"DEATH_EVENT\"]==1].time, label=\"DEATH_EVENT=1\", ax=ax2, bins=range_bin_width, color=palette_ro[6])\nax1.set_title(\"time distribution\", fontsize=16);\nax2.set_title(\"Relationship between time and DEATH_EVENT\", fontsize=16)\n\nax1.axvline(x=df[\"time\"].median(), color=palette_ro[5], linestyle=\"--\", alpha=0.5)\nax2.axvline(x=df[df[\"DEATH_EVENT\"]==0].time.median(), color=palette_ro[1], linestyle=\"--\", alpha=0.5)\nax2.axvline(x=df[df[\"DEATH_EVENT\"]==1].time.median(), color=palette_ro[6], linestyle=\"--\", alpha=0.5)\n\nax1.annotate(\"Min: {:.0f}\".format(df[\"time\"].min()), xy=(df[\"time\"].min(), 0.0021), \n             xytext=(df[\"time\"].min()-30, 0.0032),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.2\"))\nax1.annotate(\"Max: {:.0f}\".format(df[\"time\"].max()), xy=(df[\"time\"].max(), 0.001), \n             xytext=(df[\"time\"].max(), 0.0017),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.2\"))\nax1.annotate(\"Med: {:.0f}\".format(df[\"time\"].median()), xy=(df[\"time\"].median(), 0.0041), \n             xytext=(df[\"time\"].median()+8, 0.0052),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=-0.25\"))\n\nax2.annotate(\"Survived\\nMed: {:.0f}\".format(df[df[\"DEATH_EVENT\"]==0].time.median()), xy=(df[df[\"DEATH_EVENT\"]==0].time.median(), 0.0035), \n             xytext=(df[df[\"DEATH_EVENT\"]==0].time.median()-40, 0.007),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=0.25\"))\nax2.annotate(\"Dead\\nMed: {:.0f}\".format(df[df[\"DEATH_EVENT\"]==1].time.median()), xy=(df[df[\"DEATH_EVENT\"]==1].time.median(), 0.0082), \n             xytext=(df[df[\"DEATH_EVENT\"]==1].time.median()+7, 0.0105),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\", facecolor='slategray', edgecolor='slategray',\n                             connectionstyle=\"arc3, rad=-0.25\"))\nax2.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Insights:\n\n* The distribution of the follow-up period is spread out with no large peaks, and there are small peaks around 90 and 200.\n<br>ã€€<font color=\"RoyalBlue\">çµŒéè¦³å¯ŸæœŸé–“ã®åˆ†å¸ƒã«ã¯å¤§ããªå±±ã¯ç„¡ãã°ã‚‰ã‘ã¦ã„ã¦ã€90ä»˜è¿‘ã¨200ä»˜è¿‘ã«å°ã•ãªå±±ãŒã‚ã‚‹ã€‚</font>\n* By objective variable, there are clear differences in the medians and distributions. Survivors have a long follow-up period and two peaks in the distribution, while the dead tend to have a short follow-up period, with a gradual decrease from a large peak around 30 days.\n<br>ã€€<font color=\"RoyalBlue\">ç›®çš„å¤‰æ•°åˆ¥ã«è¦‹ã‚‹ã¨ã€ä¸­å¤®å€¤ã‚„åˆ†å¸ƒã«æ˜ç¢ºãªå·®ãŒã‚ã‚‹ã€‚ç”Ÿå­˜è€…ã¯çµŒéè¦³å¯ŸæœŸé–“ãŒé•·ãåˆ†å¸ƒã«ï¼’ã¤ã®å±±ãŒã‚ã‚‹ãŒã€æ­»äº¡è€…ã¯çµŒéè¦³å¯ŸæœŸé–“ãŒçŸ­ã„å‚¾å‘ã«ã‚ã‚Šã€30æ—¥ä»˜è¿‘ã®å¤§ããªå±±ã‹ã‚‰ç·©ã‚„ã‹ã«æ¸›å°‘ã—ã¦ã„ãã€‚</font>"},{"metadata":{},"cell_type":"markdown","source":"The figure below is based on a [scatterplot from the paper](https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5/figures/3)\n."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n\nsns.scatterplot(x=df[\"serum_creatinine\"], y=df[\"ejection_fraction\"], ax=ax,\n                palette=[palette_ro[1], palette_ro[6]], hue=df[\"DEATH_EVENT\"])\nax.plot([0.9, 5.3], [13, 80.0], color=\"gray\", ls=\"--\")\n\nfig.suptitle(\"Relationship between serum_creatinine and ejection_fraction against DEATH_EVENT\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> This plot shows a clear distinction between alive patients and dead patients, that we highlighted by manually inserting a black straight line.\n\n<font color=\"RoyalBlue\">ã“ã®å›³ã¯ã€ç”Ÿå­˜ã—ãŸæ‚£è€…ã¨æ­»äº¡ã—ãŸæ‚£è€…ã®æ˜ç¢ºãªé•ã„ã‚’ç¤ºã—ã¦ãŠã‚Šã€æ‰‹å‹•ã§é»’ã„ç›´ç·šã‚’æŒ¿å…¥ã—ã¦å¼·èª¿ã—ã¦ã„ã¾ã™ã€‚</font>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n\nsns.heatmap(df.corr(), ax=ax, vmax=1, vmin=-1, center=0,\n            annot=True, fmt=\".2f\",\n            cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            mask=np.triu(np.ones_like(df.corr(), dtype=np.bool)))\n\nfig.suptitle(\"Diagonal correlation matrix\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Insights:\n\n* The explanatory variables that can be said to be significantly correlated with the objective variable are, in order of increasing correlation, `time`, `serum_creatinine`, `ejection_fraction`, `age`, and `serum_creatinine`.\n<br>ã€€<font color=\"RoyalBlue\">ç›®çš„å¤‰æ•°ã«å¯¾ã—ã¦æœ‰æ„ã«ç›¸é–¢ãŒã‚ã‚‹ã¨è¨€ãˆã‚‹èª¬æ˜å¤‰æ•°ã¯ã€ç›¸é–¢ãŒé«˜ã„é †ã« time, serum_creatinine, ejection_fraction, age, serum_creatinine ã®ï¼•ã¤ã§ã‚ã‚‹ã€‚</font>\n* The correlation between explanatory variables is not very high.\n<br>ã€€<font color=\"RoyalBlue\">èª¬æ˜å¤‰æ•°åŒå£«ã®ç›¸é–¢ã¯ãã‚Œã»ã©é«˜ããªã„ã€‚</font>\n\n<a id=\"preprocessing\"></a>\n# Data preprocessing ğŸ§¹\nExcept for models based on decision trees, we need to do feature scaling. In this case, let's do standardization (converting the mean of each feature to 0 and the standard deviation to 1). There are no categorical variables in this case, so there is no need for one-hot encoding or anything else.<br>\n<font color=\"RoyalBlue\">æ±ºå®šæœ¨ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ãŸãƒ¢ãƒ‡ãƒ«ä»¥å¤–ã§ã¯ã€ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’è¡Œã†å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ä»Šå›ã¯ã€æ¨™æº–åŒ–ï¼ˆå„ç‰¹å¾´é‡ã®å¹³å‡ã‚’ï¼ã€æ¨™æº–åå·®ã‚’ï¼‘ã«å¤‰æ›ï¼‰ã‚’è¡Œã£ã¦ãŠãã¾ã—ã‚‡ã†ã€‚ä»Šå›ã¯ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ãŒç„¡ã„ã®ã§ã€ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãªã©ã¯å¿…è¦ã‚ã‚Šã¾ã›ã‚“ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_features = [\"age\", \"creatinine_phosphokinase\", \"ejection_fraction\", \"platelets\", \"serum_creatinine\", \"serum_sodium\", \"time\"]\nnum_features_s = []\n\nfor i in range(len(num_features)):\n    num_features_s.append(num_features[i] + \"_s\")\n\nsc = StandardScaler()\ndf[num_features_s] = sc.fit_transform(df[num_features])\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The `train_test_split()` function is used to split the train set and the test set. You can split the dataset while keeping the ratio of `y` by specifying `y` as the argument `stratify`.<br>\n<font color=\"RoyalBlue\">train_test_split() é–¢æ•°ã§è¨“ç·´ç”¨ã‚»ãƒƒãƒˆã¨ãƒ†ã‚¹ãƒˆç”¨ã‚»ãƒƒãƒˆã‚’åˆ†å‰²ã—ã¾ã™ã€‚ã“ã®ã¨ãã€å¼•æ•° stratify ã« y ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€y ã®å‰²åˆã‚’ä¿ã£ãŸã¾ã¾ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ†å‰²ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.copy()\ny = X[\"DEATH_EVENT\"]\nX = X.drop([\"DEATH_EVENT\"], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, stratify=y, random_state=0)\nX_train.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"models\"></a>\n# Train models and make predictions ğŸ’­\nNow, let's create some models and check the performance measures. We will also optimize the models using optuna.<br>\nThe performance measure for classifiers are as follows.<br>\n<font color=\"RoyalBlue\">ã§ã¯ã€ã„ãã¤ã‹ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã€æ€§èƒ½æŒ‡æ¨™ã‚’ç¢ºèªã—ã¦ã„ãã¾ã—ã‚‡ã†ã€‚ã¾ãŸã€optuna ã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–ã‚‚è¡Œã„ã¾ã™ã€‚<br>\nåˆ†é¡å™¨ã®æ€§èƒ½æŒ‡æ¨™ã«ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªã‚‚ã®ãŒã‚ã‚Šã¾ã™ã€‚</font>\n\n> Referenced from Hands-On Machine Learning with Scikit-Learn and TensorFlow (Aurelien Geron, 2017).\n* accuracy - the ratio of correct predictions\n<br>ã€€<font color=\"RoyalBlue\">æ­£è§£ç‡ - æ­£ã—ã„äºˆæ¸¬ã®å‰²åˆ</font>\n* confusion matrix - counting the number of times instances of class A are classified as class B\n<br>ã€€<font color=\"RoyalBlue\">æ··åŒè¡Œåˆ— - ã‚¯ãƒ©ã‚¹ï¼¡ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒã‚¯ãƒ©ã‚¹ï¼¢ã«åˆ†é¡ã•ã‚ŒãŸå›æ•°ã‚’æ•°ãˆã‚‹</font>\n* precision - the accuracy of the positive predictions\n<br>ã€€<font color=\"RoyalBlue\">é©åˆç‡ - é™½æ€§ã®äºˆæ¸¬ã®æ­£è§£ç‡ï¼ˆé™½æ€§ã§ã‚ã‚‹ã¨äºˆæ¸¬ã—ãŸã†ã¡ã€å½“ãŸã£ã¦ã„ãŸç‡ï¼‰</font>\n* recall (sensitivity, true positive rate: TPR) - the ratio of positive instances that are correctly detected by the classifier\n<br>ã€€<font color=\"RoyalBlue\">å†ç¾ç‡ï¼ˆæ„Ÿåº¦ã€çœŸé™½æ€§ç‡ï¼‰- åˆ†é¡å™¨ãŒæ­£ã—ãåˆ†é¡ã—ãŸé™½æ€§ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®å‰²åˆï¼ˆæœ¬å½“ã«é™½æ€§ã§ã‚ã‚‹ã‚±ãƒ¼ã‚¹ã®ã†ã¡ã€é™½æ€§ã ã¨åˆ¤å®šã§ããŸç‡ï¼‰</font>\n* F1 score - the harmonic mean of precision and recall\n<br>ã€€<font color=\"RoyalBlue\">F1 ã‚¹ã‚³ã‚¢ï¼ˆF å€¤ï¼‰ - é©åˆç‡ã¨å†ç¾ç‡ã®èª¿å’Œå¹³å‡ï¼ˆç®—è¡“å¹³å‡ã«æ¯”ã¹ã€èª¿å’Œå¹³å‡ã¯ä½ã„å€¤ã«ãã†ã§ãªã„å€¤ã‚ˆã‚Šã‚‚ãšã£ã¨å¤§ããªé‡ã¿ã‚’ç½®ãï¼‰</font>\n* AUC - the area under the ROC curve (plotting the true positive rate (another name for recall) against the false positive rate)\n<br>ã€€<font color=\"RoyalBlue\">AUC - ROC æ›²ç·šï¼ˆå½é™½æ€§ç‡ã«å¯¾ã™ã‚‹çœŸé™½æ€§ç‡ï¼ˆå†ç¾ç‡ï¼‰ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ãŸæ›²ç·šï¼‰ã®ä¸‹ã®é¢ç©</font><br>\n\nIn this notebook, we will look at their accuracy, F1 score, and confusion matrix.<br>\n<font color=\"RoyalBlue\">ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€æ­£è§£ç‡ã€F1 ã‚¹ã‚³ã‚¢ã€ãã—ã¦æ··åŒè¡Œåˆ—ã‚’è¦‹ã¦ã„ãã¾ã™ã€‚</font>\n\n<a id=\"gbm\"></a>\n## LightGBM ğŸŒ³"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [\"age\", \"anaemia\", \"creatinine_phosphokinase\", \"diabetes\", \"ejection_fraction\", \"high_blood_pressure\", \"platelets\",\n            \"serum_creatinine\", \"serum_sodium\", \"sex\", \"smoking\", \"time\"]\nNFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(features)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = lgb.LGBMClassifier(objective=\"binary\",\n                             metric=\"binary_logloss\")\n    clf.fit(X_tr, y_tr, eval_set = [(X_va, y_va)],\n            early_stopping_rounds=10,\n            verbose=-1)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.feature_importances_\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test[features], num_iteration=clf.best_iteration_)\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\n\nprint(f\"\\nOut-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), features), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of default LightGBM\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of default LightGBM\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1-score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [False, True], fontsize=16)\nplt.yticks(np.arange(2), [False, True], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"features = [\"ejection_fraction\", \"serum_creatinine\", \"time\"]\n\ndef objective(trial):\n    skf = StratifiedKFold(n_splits=NFOLD)\n    oof = np.zeros((len(X_train), ))\n\n    for fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n        print(f\"FOLD {fold_id+1}\")\n        X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n        y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n\n        clf = lgb.LGBMClassifier(objective=\"binary\",\n                                 metric=\"binary_logloss\",\n                                 colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.4, 1.0),\n                                 learning_rate = trial.suggest_uniform(\"learning_rate\", 1e-8, 1.0),\n                                 max_depth = trial.suggest_int(\"max_depth\", 2, 32),\n                                 min_child_samples = trial.suggest_int(\"min_child_samples\", 3, 500),\n                                 min_child_weight = trial.suggest_loguniform(\"min_child_weight\", 1e-4, 1e+1),\n                                 n_estimators = trial.suggest_int(\"n_estimators\", 20, 200),\n                                 num_leaves = trial.suggest_int(\"num_leaves\", 2, 512),\n                                 reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 10.0),\n                                 reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 10.0),\n                                 subsample = trial.suggest_uniform(\"subsample\", 0.4, 1.0),\n                                 subsample_freq = trial.suggest_int(\"subsample_freq\", 0, 7),\n                                )\n        clf.fit(X_tr, y_tr, eval_set = [(X_va, y_va)],\n                early_stopping_rounds=20,\n                verbose=-1)\n        oof[va_idx] = clf.predict(X_va)\n        \n    score = accuracy_score(y_train, oof)\n    return score\n\nstudy = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=0))\nstudy.optimize(objective, n_trials=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study.best_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optuna.importance.get_param_importances(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = optuna.visualization.plot_param_importances(study)\nfig.show(config={\"displayModeBar\": False})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(features)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = lgb.LGBMClassifier(objective=\"binary\",\n                             metric=\"binary_logloss\",\n                             **study.best_params)\n    clf.fit(X_tr, y_tr, eval_set = [(X_va, y_va)],\n            early_stopping_rounds=10,\n            verbose=-1)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.feature_importances_\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test[features], num_iteration=clf.best_iteration_)\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\ny_pred_gbm = np.mean(y_preds, axis=1)\n\nprint(f\"\\nOut-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), features), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of optimized LightGBM\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of optimized LightGBM\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [False, True], fontsize=16)\nplt.yticks(np.arange(2), [False, True], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"xgb\"></a>\n## XGBoost ğŸŒ³"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [\"age\", \"anaemia\", \"creatinine_phosphokinase\", \"diabetes\", \"ejection_fraction\", \"high_blood_pressure\", \"platelets\",\n            \"serum_creatinine\", \"serum_sodium\", \"sex\", \"smoking\", \"time\"]\nNFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(features)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = xgb.XGBClassifier()\n    clf.fit(X_tr, y_tr)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.feature_importances_\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test[features])\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), features), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of default XGBoost\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of default XGBoost\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1-score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [False, True], fontsize=16)\nplt.yticks(np.arange(2), [False, True], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"features = [\"ejection_fraction\", \"serum_creatinine\", \"time\"]\n\ndef objective(trial):\n    skf = StratifiedKFold(n_splits=NFOLD)\n    models = []\n    imp = np.zeros((NFOLD, len(features)))\n    oof = np.zeros((len(X_train), ))\n    y_preds = np.zeros((len(X_test), NFOLD))\n\n    for fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n        # print(f\"FOLD {fold_id+1}\")\n        X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n        y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n\n        clf = xgb.XGBClassifier(n_estimators = trial.suggest_int(\"n_estimators\", 20, 200),\n                                max_depth = trial.suggest_int(\"max_depth\", 2, 32),\n                                learning_rate = trial.suggest_uniform(\"learning_rate\", 1e-8, 1.0),\n                                min_child_weight = trial.suggest_loguniform(\"min_child_weight\", 1e-4, 1e+1),\n                                subsample = trial.suggest_uniform(\"subsample\", 0.4, 1.0),\n                                colsample_bytree = trial.suggest_uniform(\"colsample_bytree\", 0.4, 1.0),\n                                reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 10.0),\n                                reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 10.0),\n                                scale_pos_weight = trial.suggest_int(\"scale_pos_weight\", 1, 100)\n                                )\n        clf.fit(X_tr, y_tr)\n        oof[va_idx] = clf.predict(X_va)\n        \n    score = accuracy_score(y_train, oof)\n    return score\n\nstudy = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=0))\nstudy.optimize(objective, n_trials=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study.best_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optuna.importance.get_param_importances(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(features)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = xgb.XGBClassifier(**study.best_params)\n    clf.fit(X_tr, y_tr)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.feature_importances_\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test[features])\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\ny_pred_xgb = np.mean(y_preds, axis=1)\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), features), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of optimized XGBoost\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of optimized XGBoost\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [False, True], fontsize=16)\nplt.yticks(np.arange(2), [False, True], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"cat\"></a>\n## CatBoost ğŸŒ³"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [\"age\", \"anaemia\", \"creatinine_phosphokinase\", \"diabetes\", \"ejection_fraction\", \"high_blood_pressure\", \"platelets\",\n            \"serum_creatinine\", \"serum_sodium\", \"sex\", \"smoking\", \"time\"]\nNFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(features)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = CatBoostClassifier(loss_function=\"Logloss\")\n    clf.fit(X_tr, y_tr, eval_set = [(X_va, y_va)],\n            early_stopping_rounds=10,\n            verbose=False)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.get_feature_importance()\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test[features])\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), features), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of default CatBoost\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of default CatBoost\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1-score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [False, True], fontsize=16)\nplt.yticks(np.arange(2), [False, True], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"features = [\"ejection_fraction\", \"serum_creatinine\", \"time\"]\n\ndef objective(trial):\n    skf = StratifiedKFold(n_splits=NFOLD)\n    models = []\n    imp = np.zeros((NFOLD, len(features)))\n    oof = np.zeros((len(X_train), ))\n    y_preds = np.zeros((len(X_test), NFOLD))\n\n    for fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n        # print(f\"FOLD {fold_id+1}\")\n        X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n        y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n\n        clf = CatBoostClassifier(loss_function=\"Logloss\",\n                                 iterations = trial.suggest_int(\"iterations\", 1000, 6000),\n                                 learning_rate = trial.suggest_uniform(\"learning_rate\", 1e-4, 1e-1),\n                                 l2_leaf_reg = trial.suggest_loguniform(\"l2_leaf_reg\", 1e-8, 10.0),\n                                 # bagging_temperature = trial.suggest_loguniform(\"bagging_temperature\", 1e-8, 100.0),\n                                 subsample = trial.suggest_uniform(\"subsample\", 0.4, 1.0),\n                                 # random_strength = trial.suggest_loguniform(\"random_strength\", 1e-8, 100.0),\n                                 depth = trial.suggest_int(\"depth\", 2, 16),\n                                 min_data_in_leaf = trial.suggest_int(\"min_data_in_leaf\", 1, 200),\n                                 # od_type = trial.suggest_categorical(\"od_type\", [\"IncToDec\", \"Iter\"]),\n                                 # od_wait = trial.suggest_int(\"od_wait\", 10, 50)\n                                )\n        clf.fit(X_tr, y_tr, eval_set = [(X_va, y_va)],\n                early_stopping_rounds=10,\n                verbose=False)\n        oof[va_idx] = clf.predict(X_va)\n        \n    score = accuracy_score(y_train, oof)\n    return score\n\nstudy = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=0))\nstudy.optimize(objective, n_trials=80)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study.best_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optuna.importance.get_param_importances(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(features)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = CatBoostClassifier(loss_function=\"Logloss\",\n                             **study.best_params)\n    clf.fit(X_tr, y_tr, eval_set = [(X_va, y_va)],\n            early_stopping_rounds=10,\n            verbose=False)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.get_feature_importance()\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test[features])\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\ny_pred_cat = np.mean(y_preds, axis=1)\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), features), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of optimized CatBoost\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of optimized CatBoost\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [False, True], fontsize=16)\nplt.yticks(np.arange(2), [False, True], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"rf\"></a>\n## Random forest ğŸŒ³"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [\"age\", \"anaemia\", \"creatinine_phosphokinase\", \"diabetes\", \"ejection_fraction\", \"high_blood_pressure\", \"platelets\",\n            \"serum_creatinine\", \"serum_sodium\", \"sex\", \"smoking\", \"time\"]\nNFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(features)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = RandomForestClassifier(random_state=0)\n    clf.fit(X_tr, y_tr)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.feature_importances_\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test[features])\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), features), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of default Random forest\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of default Random forest\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [False, True], fontsize=16)\nplt.yticks(np.arange(2), [False, True], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"features = [\"ejection_fraction\", \"serum_creatinine\", \"time\"]\n\ndef objective(trial):\n    skf = StratifiedKFold(n_splits=NFOLD)\n    models = []\n    imp = np.zeros((NFOLD, len(features)))\n    oof = np.zeros((len(X_train), ))\n    y_preds = np.zeros((len(X_test), NFOLD))\n\n    for fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n        # print(f\"FOLD {fold_id+1}\")\n        X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n        y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n\n        clf = RandomForestClassifier(random_state=0,\n                                     n_estimators = trial.suggest_int(\"n_estimators\", 20, 200),\n                                     max_depth = trial.suggest_int(\"max_depth\", 2, 32),\n                                     min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 16),\n                                     min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 16))\n        clf.fit(X_tr, y_tr)\n        oof[va_idx] = clf.predict(X_va)\n        \n    score = accuracy_score(y_train, oof)\n    return score\n\nstudy = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=0))\nstudy.optimize(objective, n_trials=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study.best_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optuna.importance.get_param_importances(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(features)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = RandomForestClassifier(random_state=0,\n                                 **study.best_params)\n    clf.fit(X_tr, y_tr)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.feature_importances_\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test[features])\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\ny_pred_rf = np.mean(y_preds, axis=1)\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), features), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of optimized Random forest\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of optimized Random forest\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [False, True], fontsize=16)\nplt.yticks(np.arange(2), [False, True], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ert\"></a>\n## Extremely randomized trees ğŸŒ³"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [\"age\", \"anaemia\", \"creatinine_phosphokinase\", \"diabetes\", \"ejection_fraction\", \"high_blood_pressure\", \"platelets\",\n            \"serum_creatinine\", \"serum_sodium\", \"sex\", \"smoking\", \"time\"]\nNFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(features)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = ExtraTreesClassifier(random_state=0)\n    clf.fit(X_tr, y_tr)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.feature_importances_\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test[features])\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), features), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of default Extremely randomized trees\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of default Extremely randomized trees\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [False, True], fontsize=16)\nplt.yticks(np.arange(2), [False, True], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"features = [\"age\", \"ejection_fraction\", \"serum_creatinine\", \"time\"]\n\ndef objective(trial):\n    skf = StratifiedKFold(n_splits=NFOLD)\n    models = []\n    imp = np.zeros((NFOLD, len(features)))\n    oof = np.zeros((len(X_train), ))\n    y_preds = np.zeros((len(X_test), NFOLD))\n\n    for fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n        # print(f\"FOLD {fold_id+1}\")\n        X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n        y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n\n        clf = RandomForestClassifier(random_state=0,\n                                     n_estimators = trial.suggest_int(\"n_estimators\", 20, 200),\n                                     max_depth = trial.suggest_int(\"max_depth\", 2, 32),\n                                     min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 16),\n                                     min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 16))\n        clf.fit(X_tr, y_tr)\n        oof[va_idx] = clf.predict(X_va)\n        \n    score = accuracy_score(y_train, oof)\n    return score\n\nstudy = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=0))\nstudy.optimize(objective, n_trials=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study.best_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optuna.importance.get_param_importances(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\nimp = np.zeros((NFOLD, len(features)))\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = ExtraTreesClassifier(random_state=0,\n                               **study.best_params)\n    clf.fit(X_tr, y_tr)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n    imp[fold_id] = clf.feature_importances_\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test[features])\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\ny_pred_ert = np.mean(y_preds, axis=1)\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")\n\nfeature_imp = pd.DataFrame(sorted(zip(np.mean(imp, axis=0), features), reverse=True), columns=[\"values\", \"features\"])\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.barplot(x=\"values\", y=\"features\", data=feature_imp, palette=\"Blues_r\")\nplt.title(\"Feature importance of optimized Extremely randomized trees\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of optimized Extremely randomized trees\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [False, True], fontsize=16)\nplt.yticks(np.arange(2), [False, True], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"lm\"></a>\n## Linear model ğŸ“ˆ"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [\"age_s\", \"anaemia\", \"creatinine_phosphokinase_s\", \"diabetes\", \"ejection_fraction_s\", \"high_blood_pressure\", \"platelets_s\",\n            \"serum_creatinine_s\", \"serum_sodium_s\", \"sex\", \"smoking\", \"time_s\"]\nNFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = LogisticRegression(random_state=0)\n    clf.fit(X_tr, y_tr)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test[features])\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of default linear model\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [False, True], fontsize=16)\nplt.yticks(np.arange(2), [False, True], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"features = [\"age_s\", \"creatinine_phosphokinase_s\", \"ejection_fraction_s\", \"serum_creatinine_s\", \"serum_sodium_s\", \"time_s\"]\nNFOLD = 10\n\ndef objective(trial):\n    skf = StratifiedKFold(n_splits=NFOLD)\n    models = []\n    imp = np.zeros((NFOLD, len(features)))\n    oof = np.zeros((len(X_train), ))\n    y_preds = np.zeros((len(X_test), NFOLD))\n\n    for fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n        # print(f\"FOLD {fold_id+1}\")\n        X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n        y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n\n        clf = LogisticRegression(random_state=0,\n                                 C = trial.suggest_uniform(\"C\", 0.1, 10.0),\n                                 intercept_scaling = trial.suggest_uniform(\"intercept_scaling\", 0.1, 2.0),\n                                 max_iter = trial.suggest_int(\"max_iter\", 100, 1000)\n                                 )\n        clf.fit(X_tr, y_tr)\n        oof[va_idx] = clf.predict(X_va)\n        \n    score = accuracy_score(y_train, oof)\n    return score\n\nstudy = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=0))\nstudy.optimize(objective, n_trials=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study.best_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optuna.importance.get_param_importances(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NFOLD = 10\n\nskf = StratifiedKFold(n_splits=NFOLD)\nmodels = []\noof = np.zeros((len(X_train), ))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    clf = LogisticRegression(random_state=0,\n                             **study.best_params)\n    clf.fit(X_tr, y_tr)\n    oof[va_idx] = clf.predict(X_va)\n    models.append(clf)\n\nfor fold_id, clf in enumerate(models):\n    pred_ = clf.predict(X_test[features])\n    y_preds[:, fold_id] = pred_\ny_pred = np.rint(np.mean(y_preds, axis=1))\ny_pred_lm = np.mean(y_preds, axis=1)\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of optimized linear model\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [False, True], fontsize=16)\nplt.yticks(np.arange(2), [False, True], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"dl\"></a>\n## Deep learning ğŸ§ \nBecause of the long learning time, I manually adjusted the hyperparameters instead of optuna.<br>\n<font color=\"RoyalBlue\">å­¦ç¿’ã«æ™‚é–“ãŒã‹ã‹ã£ãŸãŸã‚ã€optuna ã§ã¯ãªãæ‰‹å‹•ã§ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ã—ã¦ã„ã¾ã™ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [\"age_s\", \"anaemia\", \"creatinine_phosphokinase_s\", \"diabetes\", \"ejection_fraction_s\", \"high_blood_pressure\", \"platelets_s\",\n            \"serum_creatinine_s\", \"serum_sodium_s\", \"sex\", \"smoking\", \"time_s\"]\nNFOLD = 10\nseed_everything(0)\n\nBATCH_SIZE = 32\n\nskf = StratifiedKFold(n_splits=NFOLD)\noof = np.zeros((len(X_train), 1))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(128, activation=\"relu\", input_shape=(len(features), )),\n        tf.keras.layers.Dropout(0.1),\n        tf.keras.layers.Dense(128, activation=\"relu\"),\n        tf.keras.layers.Dense(1)\n    ])\n    \n    model.compile(loss=\"binary_crossentropy\",\n                  optimizer=tf.keras.optimizers.Adam(lr=0.001,\n                                                     decay=0.0),\n                  metrics=[\"accuracy\"])\n    \n    model.fit(X_tr, y_tr,\n              validation_data=(X_va, y_va),\n              epochs=100, batch_size=BATCH_SIZE,\n              verbose=0)\n    \n    oof[va_idx] = model.predict(X_va, batch_size=BATCH_SIZE, verbose=0)\n    y_preds += model.predict(X_test[features], batch_size=BATCH_SIZE, verbose=0) / NFOLD\n\noof = (np.mean(oof, axis=1) > 0.5).astype(int)\ny_pred = (np.mean(y_preds, axis=1) > 0.5).astype(int)\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of deep learning\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [False, True], fontsize=16)\nplt.yticks(np.arange(2), [False, True], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [\"ejection_fraction_s\", \"serum_creatinine_s\", \"serum_sodium_s\", \"time_s\"]\nNFOLD = 10\nseed_everything(0)\n\nBATCH_SIZE = 32\n\nskf = StratifiedKFold(n_splits=NFOLD)\noof = np.zeros((len(X_train), 1))\ny_preds = np.zeros((len(X_test), NFOLD))\n\nfor fold_id, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train)):\n    # print(f\"FOLD {fold_id+1}\")\n    X_tr, X_va = X_train[features].iloc[tr_idx], X_train[features].iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n    \n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(64, activation=\"elu\", input_shape=(len(features), )),\n        tf.keras.layers.Dropout(0.1),\n        tf.keras.layers.Dense(64, activation=\"elu\"),\n        tf.keras.layers.Dropout(0.1),\n        tf.keras.layers.Dense(1)\n    ])\n    \n    model.compile(loss=\"binary_crossentropy\",\n                  optimizer=tf.keras.optimizers.Adam(lr=0.001,\n                                                     decay=0.0),\n                  metrics=[\"accuracy\"])\n    \n    model.fit(X_tr, y_tr,\n              validation_data=(X_va, y_va),\n              epochs=102, batch_size=BATCH_SIZE,\n              verbose=0)\n    \n    oof[va_idx] = model.predict(X_va, batch_size=BATCH_SIZE, verbose=0)\n    y_preds += model.predict(X_test[features], batch_size=BATCH_SIZE, verbose=0) / NFOLD\n\noof = (np.mean(oof, axis=1) > 0.5).astype(int)\ny_pred = (np.mean(y_preds, axis=1) > 0.5).astype(int)\ny_pred_dl = y_pred\n\nprint(f\"Out-of-fold accuracy: {accuracy_score(y_train, oof)}\")\nprint(f\"Out-of-fold F1 score: {f1_score(y_train, oof)}\")\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of optimized deep learning\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1 score={:0.4f}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)), fontsize=14)\nplt.xticks(np.arange(2), [False, True], fontsize=16)\nplt.yticks(np.arange(2), [False, True], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ensemble\"></a>\n# Simple ensemble ğŸ¤"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_em = y_pred_gbm + y_pred_xgb*2 + y_pred_cat + y_pred_rf + y_pred_ert*2 + y_pred_lm + y_pred_dl\ny_pred_em = (y_pred_em > 3.0).astype(int)\n\nprint(f\"Test accuracy:        {accuracy_score(y_test, y_pred_em)}\")\nprint(f\"Test F1 score:        {f1_score(y_test, y_pred_em)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plot_confusion_matrix(confusion_matrix(y_test, y_pred_em), figsize=(12,8), hide_ticks=True, colorbar=True, class_names=[\"true\", \"false\"])\n\nplt.title(\"Confusion Matrix of the ensembled model\", fontsize=18)\nplt.ylabel(\"True label\", fontsize=14)\nplt.xlabel(\"Predicted label\\naccuracy={:0.4f}, F1-score={:0.4f}\".format(accuracy_score(y_test, y_pred_em), f1_score(y_test, y_pred_em)), fontsize=14)\nplt.xticks(np.arange(2), [False, True], fontsize=16)\nplt.yticks(np.arange(2), [False, True], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We were able to get better accuracy by using the ensemble model. Thanks so much for reading!<br>\n<font color=\"RoyalBlue\">è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã™ã‚‹ã“ã¨ã§ã‚ˆã‚Šè‰¯ã„ç²¾åº¦ã‚’å‡ºã™ã“ã¨ãŒã§ãã¾ã—ãŸã€‚ã“ã“ã¾ã§èª­ã‚“ã§ãã ã•ã‚Šã©ã†ã‚‚ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸï¼</font>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}