{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"61384bb0-de67-7c30-51be-a79339cfac31"},"source":"In this notebook, we'll try to use Tensorflow to classify the state of the subject according to raw EEG data. I use Nick Merril's Kernel to help guide my data cleaning. We try two tasks, here\n\n 1. Classify task label by EEG power for 1 subject, only.\n 2. Classify task label by EEG power for all subjects. This is harder, as it tries to use EEG Power data (which is unique to each person) to create a general classification rule for everyone.\n\nhttps://www.kaggle.com/elsehow/d/berkeley-biosense/synchronized-brainwave-dataset/classifying-relaxation-versus-doing-math"},{"cell_type":"markdown","metadata":{"_cell_guid":"756b8617-ac51-efb6-80d9-0f381cb26536"},"source":"## Step 1: Classify tasks for just one subject. ##"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dfb6a238-7b67-0d51-d3c9-faedb3c1e5b8"},"outputs":[],"source":"import json\nimport pandas as pd\nimport tensorflow as tf\nimport numpy as np\nfrom sklearn.cross_validation import train_test_split\n\ndf = pd.read_csv(\"../input/eeg-data.csv\")\n# convert to arrays from strings\ndf['eeg_power'] = df.eeg_power.map(json.loads)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9e75e42e-6c59-72cc-f282-8a3c449e1313"},"outputs":[],"source":"df = df.drop('Unnamed: 0', 1)\ndf = df.drop('indra_time', 1)\ndf = df.drop('browser_latency', 1)\ndf = df.drop('reading_time', 1)\ndf = df.drop('attention_esense', 1)\ndf = df.drop('meditation_esense', 1)\ndf = df.drop('raw_values', 1)\ndf = df.drop('signal_quality', 1)\ndf = df.drop('createdAt', 1)\ndf = df.drop('updatedAt', 1)\n\nprint(df.columns.values)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"df72a89a-2d3a-94af-f8fc-4ed06a9a7894"},"outputs":[],"source":"# separate eeg power to multiple columns\nto_series = pd.Series(df['eeg_power']) # df to series\neeg_features=pd.DataFrame(to_series.tolist()) #series to list and then back to df\ndf = pd.concat([df,eeg_features], axis=1) # concatenate the create columns"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d6fbe747-6123-0292-f61b-489ea8505b6f"},"outputs":[],"source":"# just look at first subject\ndf=df.loc[df['id'] == 1]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1040721b-ff18-355b-5a20-d9f51885809a"},"outputs":[],"source":"\ndf = df.drop('eeg_power', 1) # drop comma separated cell\ndf = df.drop('id', 1) # drop comma separated cell"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5d9287f6-6686-ece9-799c-02719ab4bc61"},"outputs":[],"source":"# prepare for training\nlabel=df.pop(\"label\") # pop off labels to new group\nprint(df.shape)\nprint(df.head())\n# convert to np array. df has our featuers\ndf=df.values\n\n\n\n# convert labels to onehots \ntrain_labels = pd.get_dummies(label)\n# make np array\ntrain_labels = train_labels.values\nprint(train_labels.shape)\n\nx_train,x_test,y_train,y_test = train_test_split(df,train_labels,test_size=0.2)\n# so now we have predictors and y values, separated into test and train\n\nx_train,x_test,y_train,y_test = np.array(x_train,dtype='float32'), np.array(x_test,dtype='float32'),np.array(y_train,dtype='float32'),np.array(y_test,dtype='float32')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6db35222-1c2e-edb4-28ad-72c385accd7a"},"outputs":[],"source":"# place holder for inputs. feed in later\nx = tf.placeholder(tf.float32, [None, 8])\n# # # take 20 features  to 10 nodes in hidden layer\nw1 = tf.Variable(tf.random_normal([8, 1000],stddev=.5,name='w1'))\n# # # add biases for each node\nb1 = tf.Variable(tf.zeros([1000]))\n# # calculate activations \nhidden_output = tf.nn.softmax(tf.matmul(x, w1) + b1)\nw2 = tf.Variable(tf.random_normal([1000, 65],stddev=.5,name='w2'))\nb2 = tf.Variable(tf.zeros([65]))\n\n# # placeholder for correct values \ny_ = tf.placeholder(\"float\", [None,65])\n# # #implement model. these are predicted ys\ny = tf.nn.softmax(tf.matmul(hidden_output, w2) + b2)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3db5c116-f2a7-19a5-4d20-04cb0dfc94d7"},"outputs":[],"source":"loss = tf.reduce_mean(tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(y, y_, name='xentropy')))\nopt = tf.train.AdamOptimizer(learning_rate=.002)\ntrain_step = opt.minimize(loss, var_list=[w1,b1,w2,b2])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bd5bd436-1e7f-0b09-a2a2-0464ad5a46c1"},"outputs":[],"source":"def get_mini_batch(x,y):\n\trows=np.random.choice(x.shape[0], 50)\n\treturn x[rows], y[rows]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c6a624db-d380-4865-625c-42b296b93f1d"},"outputs":[],"source":"# start session\nsess = tf.Session()\n# init all vars\ninit = tf.initialize_all_variables()\nsess.run(init)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8afc8598-9068-7c64-d1c1-e702a452ab49"},"outputs":[],"source":"ntrials = 10000\nfor i in range(ntrials):\n    # get mini batch\n    a,b=get_mini_batch(x_train,y_train)\n    # run train step, feeding arrays of 100 rows each time\n    _, cost =sess.run([train_step,loss], feed_dict={x: a, y_: b})\n    if i%500 ==0:\n    \tprint(\"epoch is {0} and cost is {1}\".format(i,cost))\n\ncorrect_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\nprint(\"test accuracy is {}\".format(sess.run(accuracy, feed_dict={x: x_test, y_: y_test})))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"708787df-0eae-47ea-2518-bdca36e2ebbd"},"outputs":[],"source":"sess.close"},{"cell_type":"markdown","metadata":{"_cell_guid":"cf465aff-359a-1afb-a1c4-678565247c5a"},"source":"## Step 2: Classify tasks for all subjects. ##"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"889acb97-6940-b332-4106-fd736488f3e4"},"outputs":[],"source":"import json\nimport pandas as pd\nimport tensorflow as tf\nimport numpy as np\nfrom sklearn.cross_validation import train_test_split\n\ndf = pd.read_csv(\"../input/eeg-data.csv\")\n# convert to arrays from strings\ndf['eeg_power'] = df.eeg_power.map(json.loads)"},{"cell_type":"markdown","metadata":{"_cell_guid":"54307894-b64d-8531-a59e-b18a37f41b3b"},"source":"Now we clean the dataset, and drop most of the columns that we won't use in this analysis. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"47388d69-0d6d-cfef-ea0d-d10416f4a12f"},"outputs":[],"source":"df = df.drop('Unnamed: 0', 1)\ndf = df.drop('id', 1)\ndf = df.drop('indra_time', 1)\ndf = df.drop('browser_latency', 1)\ndf = df.drop('reading_time', 1)\ndf = df.drop('attention_esense', 1)\ndf = df.drop('meditation_esense', 1)\ndf = df.drop('raw_values', 1)\ndf = df.drop('signal_quality', 1)\ndf = df.drop('createdAt', 1)\ndf = df.drop('updatedAt', 1)\n\nprint(df.columns.values)"},{"cell_type":"markdown","metadata":{"_cell_guid":"749aa8c7-e622-5fa7-12db-69801ae752e5"},"source":"eeg_power contains data on  8 commonly-recognized types of EEG frequency bands...these are comma separated, but we need to create new columns for each band (delta (0.5 - 2.75Hz), theta (3.5 - 6.75Hz), low-alpha (7.5 - 9.25Hz), high-alpha (10 - 11.75Hz), low-beta (13 - 16.75Hz), high-beta (18 - 29.75Hz), low-gamma (31 - 39.75Hz), and mid-gamma (41 - 49.75Hz))."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"16f37c88-5e2a-e79d-f401-d53645438c74"},"outputs":[],"source":"to_series = pd.Series(df['eeg_power']) # df to series\neeg_cols=pd.DataFrame(to_series.tolist()) #series to list and then back to df\nprint(eeg_cols.head())"},{"cell_type":"markdown","metadata":{"_cell_guid":"7a4f9e31-fedf-8e81-43c5-4f63d5e1926c"},"source":"This works. We have the 8 variables split into 8 distinct columns. Nice. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eca31719-3116-d233-9bda-ec8fe44596d7"},"outputs":[],"source":"df = pd.concat([df,eeg_cols], axis=1, join='outer') # concatenate the create columns\ndf = df.drop('eeg_power', 1) # drop comma separated cell\nprint(df.head())"},{"cell_type":"markdown","metadata":{"_cell_guid":"a60b4a18-b175-14fd-1c93-7a0c6cb7f8b4"},"source":"We have a dataframe that we can now split into test and train sets and do train a NN on. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f52f5ba0-106d-d77f-fb95-a939693dca3b"},"outputs":[],"source":"# prepare for training\nlabel=df.pop(\"label\") # pop off labels to new group\nprint(\"the df of features now as shape{0} and the label set has shape {1}\".format(df.shape,label.shape))"},{"cell_type":"markdown","metadata":{"_cell_guid":"a65224b2-c4c6-29e8-d638-10234b004aa5"},"source":"We convert these two sets to np arrays to TF training."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"664f3387-f761-1ccf-7c0e-46808d2c47ae"},"outputs":[],"source":"# convert to np array. df has our featuers\ndf=df.values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"808f3cc8-d774-87a3-135f-64787cc2264d"},"outputs":[],"source":"# convert labels to onehots \ntrain_labels = pd.get_dummies(label)\nprint(train_labels.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6cdb4e70-d27f-61fc-dcb5-b0fa88c40ddc"},"source":"There are 69 different tasks classified by the researchers. There is redundancy here (blink 1 is distinct from blink 2...this will complicate our ability to correctly classify the various task labels, but let's just stick with it for now.)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aa4e52d5-b9ab-2bab-f42f-90561b7c9920"},"outputs":[],"source":"# convert train_labels to np array, too\ntrain_labels = train_labels.values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ec48930a-4412-cf90-1a8b-4eb1e602f821"},"outputs":[],"source":"# use sklearn to split for training\nx_train,x_test,y_train,y_test = train_test_split(df,train_labels,test_size=0.2)\nx_train,x_test,y_train,y_test = np.array(x_train,dtype='float32'), np.array(x_test,dtype='float32'),np.array(y_train,dtype='float32'),np.array(y_test,dtype='float32')"},{"cell_type":"markdown","metadata":{"_cell_guid":"7e291008-16c0-7cca-4de4-3fc9ebfd3715"},"source":"Now, let's do some tensorflow and build a simple model with 1 hidden layer with 1000 nodes in this layer. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79d080d7-e265-6540-c1e1-e20ce3f28a39"},"outputs":[],"source":"x = tf.placeholder(tf.float32, [None, 8])\nw1 = tf.Variable(tf.random_normal([8, 1000],stddev=.5,name='w1'))\nb1 = tf.Variable(tf.zeros([1000]))\n# # calculate hidden output\nhidden_output = tf.nn.softmax(tf.matmul(x, w1) + b1)\n# bring from 1000 nodes to one of 69 possible labels\nw2 = tf.Variable(tf.random_normal([1000, 68],stddev=.5,name='w2'))\nb2 = tf.Variable(tf.zeros([68]))\n# # placeholder for correct values \ny_ = tf.placeholder(\"float\", [None,68])\n# # #implement model. these are predicted ys\ny = tf.nn.softmax(tf.matmul(hidden_output, w2) + b2)"},{"cell_type":"markdown","metadata":{"_cell_guid":"c4ab3def-31bf-c761-8742-164b4c089c35"},"source":"Prepare the training. Use ADAM optimizer to adjust learning rate over time."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2bcb024b-28b9-70d2-40a0-de4a976ee59c"},"outputs":[],"source":"loss = tf.reduce_mean(tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(y, y_, name='xentropy')))\nopt = tf.train.AdamOptimizer(learning_rate=.005)\ntrain_step = opt.minimize(loss, var_list=[w1,b1,w2,b2])"},{"cell_type":"markdown","metadata":{"_cell_guid":"967c7624-76c8-cb5e-67be-de4fd0c4eb1e"},"source":"Create a function to get mini_batch, so that we aren't feeding data in every training epoch."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ebda287b-b6c1-fcb8-321e-897a9cdd939b"},"outputs":[],"source":"def get_mini_batch(x,y):\n\trows=np.random.choice(x.shape[0], 100)\n\treturn x[rows], y[rows]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8029bf1c-069b-0803-6367-50dd7877fd6b"},"outputs":[],"source":"sess = tf.Session()\n# init all vars in graph\ninit = tf.initialize_all_variables()\nsess.run(init)"},{"cell_type":"markdown","metadata":{"_cell_guid":"2e168321-f072-8d72-b5d5-e578fb0b79e0"},"source":"Train!"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d27eb3c1-e826-0262-5033-78d5ee98ee26"},"outputs":[],"source":"ntrials = 10000\nfor i in range(ntrials):\n    # get mini batch\n    a,b=get_mini_batch(x_train,y_train)\n    # run train step, feeding arrays of 100 rows each time\n    _, cost =sess.run([train_step,loss], feed_dict={x: a, y_: b})\n    if i%500 ==0:\n    \tprint(\"epoch is {0} and cost is {1}\".format(i,cost))\n\ncorrect_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\nprint(\"test accuracy is {}\".format(sess.run(accuracy, feed_dict={x: x_test, y_: y_test})))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"72e26dd9-faca-60e9-5cee-c58a0f7dad7b"},"outputs":[],"source":"sess.close"},{"cell_type":"markdown","metadata":{"_cell_guid":"b287a73c-76e2-e3e3-ab21-2d80d56cc529"},"source":"Given that we are trying to predict one of 69 possible labels, and that there are redundant labels, this accuracy is decent. Random guessing would get this right 1/69 = 1.5%.\n\nUpon further consideration, we see that most of the data is unlabeled. For instance, for subject 1, there are 943 rows...of these 943, the label \"everyone paired\"  appears 361 times and \"unlabeled\" appears 189 times. So if we can just get these two right, we get 58% accuracy. So this is actually not nearly as good as we would think -- the non-uniformity of the distribution makes this classification problem easier. "},{"cell_type":"markdown","metadata":{"_cell_guid":"97910ccd-c733-c636-2434-acf834278a66"},"source":"## Step 3: Consolidate labels and run for individual subject. Compare only MATH and RELAX, similar to Nick's kernel.##"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c2047866-7e4f-e671-d6fb-b725c10987cf"},"outputs":[],"source":"import pandas as pd \nimport json\nfrom sklearn.cross_validation import train_test_split\nimport tensorflow as tf\nimport numpy as np\nimport random\n\ndef prepare_individual_data(df,individual):\n\t# drop unused features. just leave eeg_power and the label\n\tdf = df.drop('Unnamed: 0', 1)\n\t# df = df.drop('id', 1)\n\tdf = df.drop('indra_time', 1)\n\tdf = df.drop('browser_latency', 1)\n\tdf = df.drop('reading_time', 1)\n\tdf = df.drop('attention_esense', 1)\n\tdf = df.drop('meditation_esense', 1)\n\tdf = df.drop('raw_values', 1)\n\tdf = df.drop('signal_quality', 1)\n\tdf = df.drop('createdAt', 1)\n\tdf = df.drop('updatedAt', 1)\n\t# separate eeg power to multiple columns\n\tto_series = pd.Series(df['eeg_power']) # df to series\n\teeg_features=pd.DataFrame(to_series.tolist()) #series to list and then back to df\n\tdf = pd.concat([df,eeg_features], axis=1) # concatenate the create columns\n\t# df = pd.concat([df,eeg_features], axis=1, join='outer') # concatenate the create columns\n\t# just look at first subject\n\tdf=df.loc[df['id'] == individual]\n\tdf = df.drop('eeg_power', 1) # drop comma separated cell\n\t# df = df.drop('id', 1) # drop comma separated cell\n\treturn df\n\ndf = pd.read_csv(\"../input/eeg-data.csv\")\n\nrelax = df[df.label == 'relax']\n# df['label'] = df[\"label\"].astype('category')\ndf['label'].value_counts()\ndf['eeg_power'] = df.eeg_power.map(json.loads)\n\nindividual_data=prepare_individual_data(df,1)\n\ndef clean_labels(dd):\n\t# clean labels\n\tdd.loc[dd.label == 'math1', 'label'] = \"math\"\n\tdd.loc[dd.label == 'math2', 'label'] = \"math\"\n\tdd.loc[dd.label == 'math3', 'label'] = \"math\"\n\tdd.loc[dd.label == 'math4', 'label'] = \"math\"\n\tdd.loc[dd.label == 'math5', 'label'] = \"math\"\n\tdd.loc[dd.label == 'math6', 'label'] = \"math\"\n\tdd.loc[dd.label == 'math7', 'label'] = \"math\"\n\tdd.loc[dd.label == 'math8', 'label'] = \"math\"\n\tdd.loc[dd.label == 'math9', 'label'] = \"math\"\n\tdd.loc[dd.label == 'math10', 'label'] = \"math\"\n\tdd.loc[dd.label == 'math11', 'label'] = \"math\"\n\tdd.loc[dd.label == 'math12', 'label'] = \"math\"\n\tdd.loc[dd.label == 'colorRound1-1', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound1-2', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound1-3', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound1-4', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound1-5', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound1-6', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound2-1', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound2-2', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound2-3', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound2-4', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound2-5', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound2-6', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound3-1', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound3-2', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound3-3', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound3-4', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound3-5', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound3-6', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound4-1', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound4-2', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound4-3', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound4-4', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound4-5', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound4-6', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound5-1', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound5-2', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound5-3', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound5-4', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound5-5', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound5-6', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'readyRound1', 'label'] = \"ready\"\n\tdd.loc[dd.label == 'readyRound2', 'label'] = \"ready\"\n\tdd.loc[dd.label == 'readyRound3', 'label'] = \"ready\"\n\tdd.loc[dd.label == 'readyRound4', 'label'] = \"ready\"\n\tdd.loc[dd.label == 'readyRound5', 'label'] = \"ready\"\n\tdd.loc[dd.label == 'blink1', 'label'] = \"blink\"\n\tdd.loc[dd.label == 'blink2', 'label'] = \"blink\"\n\tdd.loc[dd.label == 'blink3', 'label'] = \"blink\"\n\tdd.loc[dd.label == 'blink4', 'label'] = \"blink\"\n\tdd.loc[dd.label == 'blink5', 'label'] = \"blink\"\n\tdd.loc[dd.label == 'thinkOfItemsInstruction-ver1', 'label'] = \"instruction\"\n\tdd.loc[dd.label == 'colorInstruction2', 'label'] = \"instruction\"\n\tdd.loc[dd.label == 'colorInstruction1', 'label'] = \"instruction\"\n\tdd.loc[dd.label == 'colorInstruction2', 'label'] = \"instruction\"\n\tdd.loc[dd.label == 'musicInstruction', 'label'] = \"instruction\"\n\tdd.loc[dd.label == 'videoInstruction', 'label'] = \"instruction\"\n\tdd.loc[dd.label == 'mathInstruction', 'label'] = \"instruction\"\n\tdd.loc[dd.label == 'relaxInstruction', 'label'] = \"instruction\"\n\tdd.loc[dd.label == 'blinkInstruction', 'label'] = \"instruction\"\n\treturn dd\n\ncleaned_individual_data = clean_labels(individual_data)\n\ndef drop_useless_labels(df):\n\t# drop unlabeled and everyone paired and others. leave only relax and math. \n\tdf = df[df.label != 'unlabeled']\n\tdf = df[df.label != 'everyone paired']\n\tdf = df[df.label != 'instruction']\n\tdf = df[df.label != 'blink']\n\tdf = df[df.label != 'ready']\n\tdf = df[df.label != 'colors']\n\tdf = df[df.label != 'thinkOfItems-ver1']\n\tdf = df[df.label != 'music']\n\tdf = df[df.label != 'video-ver1']\n\treturn df\n\nfinal_individual_full_data= drop_useless_labels(cleaned_individual_data)\n\nprint(final_individual_full_data['label'].value_counts())\n\nprint(final_individual_full_data.head())\n\nfor i in range(9):\n\tcopy = final_individual_full_data\n\tcopy[0]=copy[0]+random.gauss(1,.1) # add noice to mean freq var\n\tfinal_individual_full_data=final_individual_full_data.append(copy,ignore_index=True) # make voice df 2x as big\n\tprint(\"shape of df after {0}th intertion of this loop is {1}\".format(i,final_individual_full_data.shape))\n\n\ndef get_traintest_data(individualdata):\n\tlabel=individualdata.pop(\"label\") # pop off labels to new group\n\ttrain_labels = pd.get_dummies(label)\n\ttrain_labels = train_labels.values\n\tdf=individualdata.values\n\tx_train,x_test,y_train,y_test = train_test_split(df,train_labels,test_size=0.2)\n\t#so now we have predictors and y values, separated into test and train\n\tx_train,x_test,y_train,y_test = np.array(x_train,dtype='float32'), np.array(x_test,dtype='float32'),np.array(y_train,dtype='float32'),np.array(y_test,dtype='float32')\n\treturn x_train, x_test, y_train, y_test\n\n\nx_train, x_test, y_train, y_test = get_traintest_data(final_individual_full_data)\n\n\n\n\n\n\ndef get_mini_batch(x,y):\n\trows=np.random.choice(x.shape[0], 100)\n\treturn x[rows], y[rows]\n\n\ndef trainNN(x_train, y_train,x_test,y_test,number_trials):\n\t# there are 8 features\n\t# place holder for inputs. feed in later\n\tx = tf.placeholder(tf.float32, [None, x_train.shape[1]])\n\t# # # take 20 features  to 10 nodes in hidden layer\n\tw1 = tf.Variable(tf.random_normal([x_train.shape[1], 1000],stddev=.5,name='w1'))\n\t# # # add biases for each node\n\tb1 = tf.Variable(tf.zeros([1000]))\n\t# # calculate activations \n\thidden_output = tf.nn.softmax(tf.matmul(x, w1) + b1)\n\tw2 = tf.Variable(tf.random_normal([1000, y_train.shape[1]],stddev=.5,name='w2'))\n\tb2 = tf.Variable(tf.zeros([y_train.shape[1]]))\n\t# # placeholder for correct values \n\ty_ = tf.placeholder(\"float\", [None,y_train.shape[1]])\n\t# # #implement model. these are predicted ys\n\ty = tf.nn.softmax(tf.matmul(hidden_output, w2) + b2)\n\t# loss and optimization \n\tloss = tf.reduce_mean(tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(y, y_, name='xentropy')))\n\topt = tf.train.AdamOptimizer(learning_rate=.0005)\n\ttrain_step = opt.minimize(loss, var_list=[w1,b1,w2,b2])\n\t# start session\n\tsess = tf.Session()\n\t# init all vars\n\tinit = tf.initialize_all_variables()\n\tsess.run(init)\n\tntrials = number_trials\n\tfor i in range(ntrials):\n\t    # get mini batch\n\t    a,b=get_mini_batch(x_train,y_train)\n\t    # run train step, feeding arrays of 100 rows each time\n\t    _, cost =sess.run([train_step,loss], feed_dict={x: a, y_: b})\n\t    if i%500 ==0:\n\t    \tprint(\"epoch is {0} and cost is {1}\".format(i,cost))\n\tcorrect_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n\taccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\tprint(\"test accuracy is {}\".format(sess.run(accuracy, feed_dict={x: x_test, y_: y_test})))\n\tans = sess.run(y, feed_dict={x: x_test})\n\tprint(y_test[0:3])\n\tprint(\"Correct prediction\\n\",ans[0:3])\n\ntrainNN(x_train,y_train,x_test,y_test,10000)\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"17f994b3-d6a0-118a-f478-b2b26a30ca5f"},"outputs":[],"source":"sess.close()"},{"cell_type":"markdown","metadata":{"_cell_guid":"7b6f7326-8595-a187-4bba-b22a61189d3a"},"source":"This works about as well as Nick Merril's classification (a bit better)."},{"cell_type":"markdown","metadata":{"_cell_guid":"9e2b448b-9abb-df63-dc47-09d3122f5337"},"source":"## Step 4: Consolidate labels and run for individual subject. Try to classify more than 2 activities. ##"},{"cell_type":"markdown","metadata":{"_cell_guid":"fb63b4bf-cada-ebfc-f370-fb0e0b232f42"},"source":"Let's try to classify an individual subject's mode of thinking, with more possible categories. This is harder.  "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2eb2a596-e3a1-5681-e2dd-899a971eb721"},"outputs":[],"source":"import pandas as pd \nimport json\nfrom sklearn.cross_validation import train_test_split\nimport tensorflow as tf\nimport numpy as np\nimport random\n\n\ndef prepare_individual_data(df,individual):\n\t# drop unused features. just leave eeg_power and the label\n\tdf = df.drop('Unnamed: 0', 1)\n\t# df = df.drop('id', 1)\n\tdf = df.drop('indra_time', 1)\n\tdf = df.drop('browser_latency', 1)\n\tdf = df.drop('reading_time', 1)\n\tdf = df.drop('attention_esense', 1)\n\tdf = df.drop('meditation_esense', 1)\n\tdf = df.drop('raw_values', 1)\n\tdf = df.drop('signal_quality', 1)\n\tdf = df.drop('createdAt', 1)\n\tdf = df.drop('updatedAt', 1)\n\t# separate eeg power to multiple columns\n\tto_series = pd.Series(df['eeg_power']) # df to series\n\teeg_features=pd.DataFrame(to_series.tolist()) #series to list and then back to df\n\tdf = pd.concat([df,eeg_features], axis=1) # concatenate the create columns\n\t# df = pd.concat([df,eeg_features], axis=1, join='outer') # concatenate the create columns\n\t# just look at first subject\n\tdf=df.loc[df['id'] == individual]\n\tdf = df.drop('eeg_power', 1) # drop comma separated cell\n\t# df = df.drop('id', 1) # drop comma separated cell\n\treturn df\n\ndf = pd.read_csv(\"../input/eeg-data.csv\")\n\nrelax = df[df.label == 'relax']\n# df['label'] = df[\"label\"].astype('category')\ndf['label'].value_counts()\ndf['eeg_power'] = df.eeg_power.map(json.loads)\n\nindividual_data=prepare_individual_data(df,1)\n\ndef clean_labels(dd):\n\t# clean labels\n\tdd.loc[dd.label == 'math1', 'label'] = \"math\"\n\tdd.loc[dd.label == 'math2', 'label'] = \"math\"\n\tdd.loc[dd.label == 'math3', 'label'] = \"math\"\n\tdd.loc[dd.label == 'math4', 'label'] = \"math\"\n\tdd.loc[dd.label == 'math5', 'label'] = \"math\"\n\tdd.loc[dd.label == 'math6', 'label'] = \"math\"\n\tdd.loc[dd.label == 'math7', 'label'] = \"math\"\n\tdd.loc[dd.label == 'math8', 'label'] = \"math\"\n\tdd.loc[dd.label == 'math9', 'label'] = \"math\"\n\tdd.loc[dd.label == 'math10', 'label'] = \"math\"\n\tdd.loc[dd.label == 'math11', 'label'] = \"math\"\n\tdd.loc[dd.label == 'math12', 'label'] = \"math\"\n\n\tdd.loc[dd.label == 'colorRound1-1', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound1-2', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound1-3', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound1-4', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound1-5', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound1-6', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound2-1', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound2-2', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound2-3', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound2-4', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound2-5', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound2-6', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound3-1', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound3-2', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound3-3', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound3-4', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound3-5', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound3-6', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound4-1', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound4-2', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound4-3', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound4-4', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound4-5', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound4-6', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound5-1', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound5-2', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound5-3', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound5-4', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound5-5', 'label'] = \"colors\"\n\tdd.loc[dd.label == 'colorRound5-6', 'label'] = \"colors\"\n\n\tdd.loc[dd.label == 'readyRound1', 'label'] = \"ready\"\n\tdd.loc[dd.label == 'readyRound2', 'label'] = \"ready\"\n\tdd.loc[dd.label == 'readyRound3', 'label'] = \"ready\"\n\tdd.loc[dd.label == 'readyRound4', 'label'] = \"ready\"\n\tdd.loc[dd.label == 'readyRound5', 'label'] = \"ready\"\n\n\tdd.loc[dd.label == 'blink1', 'label'] = \"blink\"\n\tdd.loc[dd.label == 'blink2', 'label'] = \"blink\"\n\tdd.loc[dd.label == 'blink3', 'label'] = \"blink\"\n\tdd.loc[dd.label == 'blink4', 'label'] = \"blink\"\n\tdd.loc[dd.label == 'blink5', 'label'] = \"blink\"\n\n\tdd.loc[dd.label == 'thinkOfItemsInstruction-ver1', 'label'] = \"instruction\"\n\tdd.loc[dd.label == 'colorInstruction2', 'label'] = \"instruction\"\n\tdd.loc[dd.label == 'colorInstruction1', 'label'] = \"instruction\"\n\tdd.loc[dd.label == 'colorInstruction2', 'label'] = \"instruction\"\n\tdd.loc[dd.label == 'musicInstruction', 'label'] = \"instruction\"\n\tdd.loc[dd.label == 'videoInstruction', 'label'] = \"instruction\"\n\tdd.loc[dd.label == 'mathInstruction', 'label'] = \"instruction\"\n\tdd.loc[dd.label == 'relaxInstruction', 'label'] = \"instruction\"\n\tdd.loc[dd.label == 'blinkInstruction', 'label'] = \"instruction\"\n\n\treturn dd\n\ncleaned_individual_data = clean_labels(individual_data)\n\ndef drop_useless_labels(df):\n\t# drop unlabeled and everyone paired.\n\tdf = df[df.label != 'unlabeled']\n\tdf = df[df.label != 'everyone paired']\n\treturn df\n\nfinal_individual_full_data= drop_useless_labels(cleaned_individual_data)\n\nprint(final_individual_full_data['label'].value_counts())\n\nfor i in range(9):\n\tcopy = final_individual_full_data\n\tcopy[0]=copy[0]+random.gauss(1,.1) # add noice to mean freq var\n\tfinal_individual_full_data=final_individual_full_data.append(copy,ignore_index=True) # make voice df 2x as big\n\tprint(\"shape of df after {0}th intertion of this loop is {1}\".format(i,final_individual_full_data.shape))\n\n\ndef get_traintest_data(individualdata):\n\tlabel=individualdata.pop(\"label\") # pop off labels to new group\n\ttrain_labels = pd.get_dummies(label)\n\ttrain_labels = train_labels.values\n\tdf=individualdata.values\n\tx_train,x_test,y_train,y_test = train_test_split(df,train_labels,test_size=0.2)\n\t#so now we have predictors and y values, separated into test and train\n\tx_train,x_test,y_train,y_test = np.array(x_train,dtype='float32'), np.array(x_test,dtype='float32'),np.array(y_train,dtype='float32'),np.array(y_test,dtype='float32')\n\treturn x_train, x_test, y_train, y_test\n\n\nx_train, x_test, y_train, y_test = get_traintest_data(final_individual_full_data)\n\n\ndef get_mini_batch(x,y):\n\trows=np.random.choice(x.shape[0], 50)\n\treturn x[rows], y[rows]\n\n\ndef trainNN(x_train, y_train,x_test,y_test,number_trials):\n\t# there are 8 features\n\t# place holder for inputs. feed in later\n\tx = tf.placeholder(tf.float32, [None, x_train.shape[1]])\n\t# # # take 20 features  to 10 nodes in hidden layer\n\tw1 = tf.Variable(tf.random_normal([x_train.shape[1], 1000],stddev=.5,name='w1'))\n\t# # # add biases for each node\n\tb1 = tf.Variable(tf.zeros([1000]))\n\t# # calculate activations \n\thidden_output = tf.nn.softmax(tf.matmul(x, w1) + b1)\n\tw2 = tf.Variable(tf.random_normal([1000, y_train.shape[1]],stddev=.5,name='w2'))\n\tb2 = tf.Variable(tf.zeros([y_train.shape[1]]))\n\t# # placeholder for correct values \n\ty_ = tf.placeholder(\"float\", [None,y_train.shape[1]])\n\t# # #implement model. these are predicted ys\n\ty = tf.nn.softmax(tf.matmul(hidden_output, w2) + b2)\n\t# loss and optimization \n\tloss = tf.reduce_mean(tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(y, y_, name='xentropy')))\n\topt = tf.train.AdamOptimizer(learning_rate=.002)\n\ttrain_step = opt.minimize(loss, var_list=[w1,b1,w2,b2])\n\t# start session\n\tsess = tf.Session()\n\t# init all vars\n\tinit = tf.initialize_all_variables()\n\tsess.run(init)\n\tntrials = number_trials\n\tfor i in range(ntrials):\n\t    # get mini batch\n\t    a,b=get_mini_batch(x_train,y_train)\n\t    # run train step, feeding arrays of 100 rows each time\n\t    _, cost =sess.run([train_step,loss], feed_dict={x: a, y_: b})\n\t    if i%500 ==0:\n\t    \tprint(\"epoch is {0} and cost is {1}\".format(i,cost))\n\tcorrect_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n\taccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\tprint(\"test accuracy is {}\".format(sess.run(accuracy, feed_dict={x: x_test, y_: y_test})))\n\n\n\ntrainNN(x_train,y_train,x_test,y_test,10000)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a4792f2e-ad24-f60c-1d9a-207d715a90a1"},"outputs":[],"source":"sess.close()"},{"cell_type":"markdown","metadata":{"_cell_guid":"61fcab3b-6b7f-5eb0-ece5-bfb34b27fa2a"},"source":"The results from this are less satisfactory. Even when I created more fake data, the results didn't improve much. Hmmmm."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}