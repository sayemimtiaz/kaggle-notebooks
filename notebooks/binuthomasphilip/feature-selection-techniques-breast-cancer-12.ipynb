{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Recently I published a self help book titled Inspiration: Thoughts on Spirituality, Technology, Wealth, Leadership and Motivation. The preview of the book can be read from the Amazon link https://lnkd.in/gj7bMQA\n\n### You can refer to my other notebooks from https://www.kaggle.com/binuthomasphilip/code"},{"metadata":{},"cell_type":"markdown","source":"In an machine learning problem one of the most import steps is feature selection.Feature selection helps in simplification of model,improves accuracy,reduces training time,reduces overfitting and helps in avoiding curse of dimentionality.Here we will cover different techniques like\n\n1.Univariate feature Selection using p value and f Distribution\n\n2.Types of Univarate Feature Selection \n\na)Select K Best\n\nb)Select Percentile\n\nc)Generic Univariate Select\n\n3.Tree Based using Random Forest\n\n4.Regularization using Lasso\n\n5.Conclusion "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importing Data Set "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/breast-cancer-wisconsin-data/data.csv\")\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dropping Columns"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df = df.drop(['id','diagnosis','Unnamed: 32'],axis=1) # Dropping Multiple Columns\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have dropped the columns which will not be useful for us during demonstration of feature selection techniques."},{"metadata":{},"cell_type":"markdown","source":"### Splitting data into Independent and Dependent Features"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"X = df.iloc[:,:-1]\ny = df.iloc[:,-1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we are taking all the feature except fractal_dimension_worst as independent variables and fractal_dimension_worst is considered as dependent variables."},{"metadata":{},"cell_type":"markdown","source":"### Test Train Split"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1,random_state= 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train,y_train)\ny_predict = lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Mean Square Error"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nimport math\nrsme = math.sqrt(mean_squared_error(y_test,y_predict))\nrsme","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.Univariate Feature Selection using f Distribution"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import f_regression as fr\nresult = fr(X,y)\nf_score = result[0]\np_values = result[1]\n\n# Getting column names \ncolumns = list(X.columns)\nprint(\" \")\nprint(\" \")\nprint(\" \")\n\nprint(\"     Features                     \",\"F-Score  \",\"P-Values\")\nprint(\"     ------------                   --------  ---------\")\n\nfor i in range(0,len(columns)):\n    f1 = \"%4.2f\" % f_score[i]\n    p1 = \"%2.6f\" % p_values[i]\n    print(\"    \",columns[i].ljust(25),f1.rjust(12),\"\",p1.rjust(8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So now we have the F_score and p_values for all the features.For a feature to be statistically significant the p values should be less than 5% ie 0.05.If for any feature the p values is more than 0.05 we can discard the feature as it wont be statistically significant for predicting the dependent feature."},{"metadata":{},"cell_type":"markdown","source":"## Model Performance With Selected Features"},{"metadata":{},"cell_type":"markdown","source":"### Updated Matrix of features"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"X_train_n = X_train[['texture_mean','smoothness_mean','compactness_mean','concavity_mean','concave points_mean','symmetry_mean','fractal_dimension_mean','perimeter_se','smoothness_se','compactness_se','concavity_se','concave points_se','fractal_dimension_se','radius_worst','texture_worst','perimeter_worst','smoothness_worst','compactness_worst','concavity_worst','concave points_worst','symmetry_worst']]\nX_test_n = X_test[['texture_mean','smoothness_mean','compactness_mean','concavity_mean','concave points_mean','symmetry_mean','fractal_dimension_mean','perimeter_se','smoothness_se','compactness_se','concavity_se','concave points_se','fractal_dimension_se','radius_worst','texture_worst','perimeter_worst','smoothness_worst','compactness_worst','concavity_worst','concave points_worst','symmetry_worst']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Updated Linear Model"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr1 = LinearRegression()\nlr1.fit(X_train_n,y_train)\ny_predict1 = lr1.predict(X_test_n)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nimport math\nrsme1 = math.sqrt(mean_squared_error(y_test,y_predict1))\nrsme1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Doing a feature selection based on p value of 0.05 has increased rsme value.So it tells us that some features having p value more than 0.05 have statistical significance on the outcome of the prediction.So we can either change the p value or include all the features while predicting the outcome."},{"metadata":{},"cell_type":"markdown","source":"### You can refer to my other notebooks from https://www.kaggle.com/binuthomasphilip/code"},{"metadata":{},"cell_type":"markdown","source":"# 2.Types of Univariate Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"### a)Select K Best\n\nIn this case we will be specifying the value of K which represents the number of best features we want to select."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import f_regression as fr\nfrom sklearn.feature_selection import SelectKBest\nselectorK = SelectKBest(score_func=fr,k=5)\nX_k = selectorK.fit_transform(X,y)\n#X_k","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Getting f_score and p values for the selected features\n#f_score = X_k[0]\n#p_values = X_k[1]\n\nf_score = selectorK.scores_\np_values = selectorK.pvalues_\n\n# Getting column names \ncolumns = list(X.columns)\nprint(\" \")\nprint(\" \")\nprint(\" \")\n\nprint(\"     Features                     \",\"F-Score  \",\"P-Values\")\nprint(\"     ------------                   --------  ---------\")\n\nfor i in range(0,len(columns)):\n    f1 = \"%4.2f\" % f_score[i]\n    p1 = \"%2.6f\" % p_values[i]\n    print(\"    \",columns[i].ljust(25),f1.rjust(12),\"\",p1.rjust(8))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# get the column names \ncols = selectorK.get_support(indices = True)\nselectedCols = X.columns[cols].tolist()\nprint(selectedCols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So if we were to selected 5 best features for our prediction then they will be the five features desplayed above.They are based on five lowest p values which is less than 0.05"},{"metadata":{},"cell_type":"markdown","source":"### b)Select Percentile"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import f_regression as fr\nfrom sklearn.feature_selection import SelectPercentile\nselectorP = SelectPercentile(score_func=fr,percentile=20)\nX_p = selectorP.fit_transform(X,y)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# get the column names \ncols_p = selectorP.get_support(indices = True)\nselectedCols_p = X.columns[cols_p].tolist()\nprint(selectedCols_p)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we have selected 20 % of the important features in our dataset.So we get a list of 6 most important features."},{"metadata":{},"cell_type":"markdown","source":"### c)Generic Univariate Select"},{"metadata":{},"cell_type":"markdown","source":"#### Based on K Best"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import f_regression as fr\nfrom sklearn.feature_selection import GenericUnivariateSelect\nselectorG1 = GenericUnivariateSelect(score_func=fr,mode='k_best',param=3)\nX_g1 = selectorG1.fit_transform(X,y)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# get the column names \ncols_g1 = selectorG1.get_support(indices = True)\nselectedCols_g1 = X.columns[cols_g1].tolist()\nprint(selectedCols_g1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Based on Percentile"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import f_regression as fr\nfrom sklearn.feature_selection import GenericUnivariateSelect\nselectorG2 = GenericUnivariateSelect(score_func=fr,mode='percentile',param=20)\nX_g2 = selectorG2.fit_transform(X,y)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# get the column names \ncols_g2 = selectorG2.get_support(indices = True)\nselectedCols_g2 = X.columns[cols_g2].tolist()\nprint(selectedCols_g2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we have covered different Univariate Feature selection techiniques in the above sections."},{"metadata":{},"cell_type":"markdown","source":"# 3.Tree Based"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nnp.random.seed()\nforest = RandomForestRegressor(n_estimators=1000)\nfit = forest.fit(X_train,y_train)\naccuracy = fit.score(X_test,y_test)\npredict = fit.predict(X_test)\n#cmatrix = confusion_matrix(y_test,predict)\n\n#-------------------------------------------------------------------------------------------------#\n# Perform k Fold cross- validation \n\nprint('Accuracy of Random Forest: %s'% \"{0:.2%}\".format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Importance "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Feature importance \nimportances = forest.feature_importances_\nindices = np.argsort(importances)[::-1]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nprint(\"Feature ranking\")\nfor f in range(X.shape[1]):\n    print(\"Feature %s (%f)\" % (list(X)[f],importances[indices[f]]))\n\nfeat_imp = pd.DataFrame({'Feature':list(X),\n                        'Gini importance':importances[indices]})\nplt.rcParams['figure.figsize']=(12,12)\nsns.set_style('whitegrid')\nax = sns.barplot(x='Gini importance',y='Feature',data=feat_imp)\nax.set(xlabel='Gini Importance')\npass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the Random Forest Regressor we have done feature selection based on their importance."},{"metadata":{},"cell_type":"markdown","source":"# 4.Lasso Regression"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso \nfrom sklearn.feature_selection import SelectFromModel\nsel_ = SelectFromModel(Lasso(alpha=0.005,random_state=0))\nsel_.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sel_.get_support()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"selected_feat=X_train.columns[(sel_.get_support())]\n\nprint('total features: {}'.format((X_train.shape[1])))\nprint('Selected features: {}'.format(len(selected_feat)))\nprint('Features with coefficients shrank to zero: {}'.format(np.sum(sel_.estimator_.coef_==0)))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"selected_feat","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"selected_feat = X_train.columns[(sel_.estimator_.coef_!=0).ravel().tolist()]\nselected_feat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have see how we are able to do a feature slection using Lasso Regression.By changing the value of alpha we will be ablw to get desired number of featured for our ML Model."},{"metadata":{},"cell_type":"markdown","source":"# 5.Conclusion \n\nIn the Kernel we have covered Univariate,Tree based and Lasso feature selection techniques for Regression Problems,I hope this kernel will be useful for you in future to carry out activity of feature selection on your dataset."},{"metadata":{},"cell_type":"markdown","source":"### You can refer to my other notebooks from https://www.kaggle.com/binuthomasphilip/code"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# TO BE CONTINUED"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}