{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score\n\nsns.set_theme()\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset\n\nThis data approach student achievement in secondary education of two Portuguese schools. The data attributes include student grades, demographic, social and school-related features) and it was collected by using school reports and questionnaires. Two datasets are provided regarding the performance in two distinct subjects: Mathematics (mat) and Portuguese language (por). In [Cortez and Silva, 2008], the two datasets were modeled under binary/five-level classification and regression tasks.\n\n### Attribute Information:\n\n- sex - student's sex (binary: 'F' - female or 'M' - male)\n- age - student's age (numeric: from 15 to 22)\n- famsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)\n- Pstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart)\n- Medu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 â€“ 5th to 9th grade, 3 â€“ secondary education or 4 â€“ higher education)\n- Fedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 â€“ 5th to 9th grade, 3 â€“ secondary education or 4 â€“ higher education)\n- Mjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\n- Fjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\n- reason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')\n- traveltime - home to school travel time (numeric: 1 - 1 hour)\n- studytime - weekly study time (numeric: 1 - 10 hours)\n- failures - number of past class failures (numeric: n if 1<=n<3, else 4)\n- schoolsup - extra educational support (binary: yes or no)\n- famsup - family educational support (binary: yes or no)\n- paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n- activities - extra-curricular activities (binary: yes or no)\n- internet - Internet access at home (binary: yes or no)\n- romantic - with a romantic relationship (binary: yes or no)\n- goout - going out with friends (numeric: from 1 - very low to 5 - very high)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/student-grade-prediction/student-mat.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Categorical Encoding\n\nLinear Regression required that the attribute values be numerical. Therefore, columns with categorical data need to be encoded to a suitable numeric format. Attributes with 2 categories are encoded using `binary encoding` which converts the values to either `1 or 0`. Attributes with more than 2 categories are encoded using `one-hot encoding`."},{"metadata":{"trusted":true},"cell_type":"code","source":"binary = [\"sex\", \"famsize\", \"Pstatus\", \"schoolsup\", \"famsup\", \"paid\", \"activities\", \"internet\", \"romantic\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multiple = [\"Medu\", \"Fedu\", \"Fjob\", \"Mjob\", \"reason\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def binary_encoder(dataset, col):\n    dataset[col] = dataset[col].astype('category')\n    dataset[col] = dataset[col].cat.codes\n    dataset[col] = dataset[col].astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.get_dummies(df, columns=multiple, prefix=multiple)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in binary:\n    binary_encoder(df, col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Drop all unnecessary columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = df.drop([\"guardian\", \"nursery\", \"higher\", \"address\", \"school\", \"famrel\", \"freetime\", \"Dalc\", \"Walc\", \"health\", \"absences\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation between Attributes\n\nThe heatmap shows the correlation between different attributes. We can use it to find which attributes are highle correlated with the target label and select them whereas we can also drop the features that are highly correlated to other features"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1,ncols=1,figsize=(15,12))\n\nax = sns.heatmap(data=df.corr(), ax=ax, cmap=\"Blues\")\nax.set_xlabel('Features',fontdict={\"fontsize\":16})\nax.set_ylabel('Features',fontdict={\"fontsize\":16})\nax.set_title('Correlation between different Features', loc=\"center\", fontdict={\"fontsize\": 16, \"fontweight\":\"bold\"})\n\nplt.savefig(\"heatmap.png\", bbox_inches=\"tight\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above heatmap, columns `G1`, `G2` and `G3` are highly correlated to each other. The below plots show this correlation."},{"metadata":{"trusted":true},"cell_type":"code","source":"pairplot = sns.pairplot(dataset[[\"G1\", \"G2\", \"G3\"]], palette=\"viridis\")\n\nplt.savefig(\"pairplot.png\", bbox_inches=\"tight\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,8))\n\nax[0] = sns.lineplot(x=\"G1\", y=\"G3\", data=dataset, palette=\"viridis\", ax=ax[0])\nax[0].set_xlabel('G1',fontdict={\"fontsize\":16})\nax[0].set_ylabel('G3',fontdict={\"fontsize\":16})\nax[0].set_title('G3 vs G1', loc=\"center\", fontdict={\"fontsize\": 16, \"fontweight\":\"bold\"})\n\nax[1] = sns.lineplot(x=\"G2\", y=\"G3\", data=dataset, palette=\"viridis\", ax=ax[1])\nax[1].set_xlabel('G2',fontdict={\"fontsize\":16})\nax[1].set_ylabel('G3',fontdict={\"fontsize\":16})\nax[1].set_title('G3 vs G2', loc=\"center\", fontdict={\"fontsize\": 16, \"fontweight\":\"bold\"})\n\nplt.savefig(\"lineplot.png\", bbox_inches=\"tight\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare Data for Training\n\n- Separate the target column from the features\n- Scale the features for faster training\n- Split data into training and test sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_cols = dataset.drop(\"G3\", axis=1).columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = dataset[x_cols]\ny = dataset[\"G3\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = StandardScaler(with_mean=True, with_std=True)\nX = pd.DataFrame(sc.fit_transform(X), columns=x_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=69)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Linear Regression\n\nLinear regression is a technique where a straight line is used to model the relationship between input and output values. In more than two dimensions, this straight line may be thought of as a plane or hyperplane.\n\nPredictions are made as a combination of the input values to predict the output value.\n\n# Gradient Descent\n\nGradient Descent is the process of minimizing the cost function by following the gradients of the cost function. On every iteration, the derivative of the cost function is computed and minimized by changing the values of the parameters. This is done until the algorithm converges to the best-fit straight line.\n\n### Notations\n\n- $\\theta$ - Weights Vector\n- $h$ - Hypothesis\n- $X$ - Feature Vector\n- $m$ - Number of training examples\n- $\\alpha$ - Learning Rate\n\n### Hypothesis Representation\n\n1. Univariate Hypothesis\n\n![alt](https://i.imgur.com/jp5OpXK.png)\n\n2. Multivariate Hypothesis\n\n![alt](https://i.imgur.com/j0FXWJV.png)\n\n### Cost Function\n\nThe function used is the mean squared error function.\n\n![alt](https://i.imgur.com/JUzmYVm.png)\n\n### Gradient Descent Update\n\nThe updates to $\\theta$ need to be made simultaneously\n\n![alt](https://i.imgur.com/1PlWQoc.png)\n\n![alt](https://i.imgur.com/1Q3IQdY.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Vectorised Gradient Descent\ndef gradient_descent(X, y, m, theta, alpha, iterations):\n    J = [] # List to store cost of every iteration\n    for _ in range(iterations):\n        h_theta = np.dot(X,theta)\n        cost = np.sum((h_theta - y) ** 2) / (2*m)\n        J.append(cost)\n        theta = theta - alpha * np.dot(X.T, (h_theta - y)) / m\n    return J, theta","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def linear_regression(X, y, alpha, iterations):\n    # Add theta_0 column\n    X = pd.concat([X, pd.Series(1, index=X.index, name=\"x_0\")], axis=1)\n    m, n = X.shape\n    theta = np.zeros(n)\n    return gradient_descent(X, y, m, theta, alpha, iterations)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy(X, y, theta):\n    X = pd.concat([X, pd.Series(1, index=X.index, name=\"x_0\")], axis=1)\n    y_preds = np.dot(X, theta)\n    return r2_score(y, y_preds) * 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Univariate Linear Regression\n\nIn univariate linear regression, only one feature is used to predict the target value. Below, gradient descent is run for 100 iterations.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"J, theta = linear_regression(X_train[\"G1\"] + X_train[\"G2\"], y_train, alpha=0.3, iterations=100)\nscore = accuracy(X_test[\"G1\"] + X_test[\"G2\"], y_test, theta)\nprint(f\"Accuracy - {score}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Multivariate Linear Regression\n\nIn multivariate linear regression, more than 1 feature is used to predict the target value. Below all features have been used and algorithm is run for 100 iterations. There is an improvement in accuracy after taking all the features."},{"metadata":{"trusted":true},"cell_type":"code","source":"J, theta = linear_regression(X_train, y_train, alpha=0.3, iterations=100)\nscore = accuracy(X_test, y_test, theta)\nprint(f\"Accuracy - {score}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cost vs Number of Iterations\n\nBelow is the plot for the Cost vs Number of iterations. From the plot, after every iteration the cost function output decreases untill it becomes a constant at which point gradient descent has converged on the best-fit straight line."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15,8))\nax = sns.lineplot(x=np.arange(1, 21), y=J[:20], palette=\"viridis\", ax=ax)\nax.set_xlabel(\"Number of Iterations\", fontdict={\"fontsize\":16})\nax.set_ylabel(\"Cost\", fontdict={\"fontsize\":16})\nax.set_title(\"Cost vs Number of Iterations\", fontdict={\"fontsize\":16})\n\nplt.savefig(\"cost_vs_iter.png\", bbox_inches=\"tight\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Learning Rate\n\nThe learning rate controls how big of a step gradient descent takes in the direction of the minima.\n\n- If $\\alpha$ is too small, gradient descent takes small steps and a long time to converge to the minima\n- If $\\alpha$ is too large, gradient descent overshoots and start diverging\n\nBelow plot shows the effect of different values of $\\alpha$ on gradient descent cost"},{"metadata":{"trusted":true},"cell_type":"code","source":"J_1, theta = linear_regression(X_train, y_train, alpha=0.01, iterations=20)\nscore = accuracy(X_test, y_test, theta)\n\nJ_2, theta = linear_regression(X_train, y_train, alpha=0.03, iterations=20)\nscore = accuracy(X_test, y_test, theta)\n\nJ_3, theta = linear_regression(X_train, y_train, alpha=0.1, iterations=20)\nscore = accuracy(X_test, y_test, theta)\n\nJ_4, theta = linear_regression(X_train, y_train, alpha=0.3, iterations=20)\nscore = accuracy(X_test, y_test, theta)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"costs = pd.DataFrame({\n    \"Iterations\": np.arange(0,20),\n    \"J_1\": J_1,\n    \"J_2\": J_2,\n    \"J_3\": J_3,\n    \"J_4\": J_4,\n})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,8))\nax = sns.lineplot(x='Iterations', y='value', hue='variable', data=pd.melt(costs, ['Iterations']),  ax=ax)\nax.set_xlabel(\"Number of Iterations\", fontdict={\"fontsize\":16})\nax.set_ylabel(\"Cost\", fontdict={\"fontsize\":16})\nax.set_title(\"Cost vs Number of Iterations\", fontdict={\"fontsize\":16})\n\nplt.savefig(\"alpha.png\", bbox_inches=\"tight\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Polynomial Linear Regression\n\nIn polynomial regression, the hypothesis function is a polynomial function of the features. This allows fitting more complex functions to the data.\n\nHypothesis Representation\n\n$h = \\theta _0 + \\theta _1  X _1^2 + \\theta _2  X _2^3$"},{"metadata":{"trusted":true},"cell_type":"code","source":"X[\"G1\"] = X[\"G1\"] ** 2\nX[\"G2\"] = X[\"G2\"] ** 3\n\nsc = StandardScaler(with_mean=True, with_std=True)\nX = pd.DataFrame(sc.fit_transform(X), columns=x_cols)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=69)\nJ, theta = linear_regression(X_train, y_train, alpha=0.3, iterations=100)\nscore = accuracy(X_test, y_test, theta)\nprint(f\"Accuracy - {score}\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}