{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1>Predicting Job Change of Data Scientists using HR Analytics</h1>\n<p>Task : Using the given dataset relating to the details of candidates in the training program, predict whether a specific candidate will work for the company or not.</p>","metadata":{}},{"cell_type":"markdown","source":"![](https://payslip.com/wp-content/uploads/2019/11/shutterstock_788621173.jpg)","metadata":{}},{"cell_type":"markdown","source":"<h1>Getting Started - Importing Libraries</h1>","metadata":{"execution":{"iopub.status.busy":"2021-07-31T07:05:28.570848Z","iopub.execute_input":"2021-07-31T07:05:28.571388Z","iopub.status.idle":"2021-07-31T07:05:28.584218Z","shell.execute_reply.started":"2021-07-31T07:05:28.571284Z","shell.execute_reply":"2021-07-31T07:05:28.582745Z"}}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score","metadata":{"execution":{"iopub.status.busy":"2021-08-14T07:16:34.826086Z","iopub.execute_input":"2021-08-14T07:16:34.826509Z","iopub.status.idle":"2021-08-14T07:16:40.044321Z","shell.execute_reply.started":"2021-08-14T07:16:34.826404Z","shell.execute_reply":"2021-08-14T07:16:40.043135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Preparing Data</h1>\n<p>The data is present in test and train files. Let us combine both to get a complete set for analysis. Drop the ID columns since it does not make a meaningful feature for analysis</p>","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"../input/hr-analytics-job-change-of-data-scientists/aug_train.csv\") \ntest_data = pd.read_csv(\"../input/hr-analytics-job-change-of-data-scientists/aug_test.csv\") \ndata = pd.concat([train_data,test_data])\ndata.drop([\"enrollee_id\"],axis=1,inplace=True)\ndata.head()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-08-14T07:16:40.0479Z","iopub.execute_input":"2021-08-14T07:16:40.048226Z","iopub.status.idle":"2021-08-14T07:16:40.18993Z","shell.execute_reply.started":"2021-08-14T07:16:40.04819Z","shell.execute_reply":"2021-08-14T07:16:40.189003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>Let us go through dataset documentation to understand what each columns stands for</p>","metadata":{}},{"cell_type":"markdown","source":"<ul>\n    <li>enrollee_id : Unique ID for candidate</li>\n    <li>city: City code</li>\n    <li>city_ development _index : Developement index of the city (scaled)</li>\n    <li>gender: Gender of candidate</li>\n    <li>relevent_experience: Relevant experience of candidate</li>\n    <li>enrolled_university: Type of University course enrolled if any</li>\n    <li>education_level: Education level of candidate</li>\n    <li>major_discipline :Education major discipline of candidate</li>\n    <li>experience: Candidate total experience in years</li>\n    <li>company_size: No of employees in current employer's company</li>\n    <li>company_type : Type of current employer</li>\n    <li>lastnewjob: Difference in years between previous job and current job</li>\n    <li>training_hours: training hours completed</li>\n    <li>target: 0 – Not looking for job change, 1 – Looking for a job change</li>\n</ul>","metadata":{}},{"cell_type":"markdown","source":"<h2>Looking at column data types</h2>\n<p>In this work, we are dealing with a large number of categorical values and less of numerical values</p>","metadata":{}},{"cell_type":"code","source":"ColumnsDescription = {\"Name\" : [],\"Type\" : []}\nfor cols in data.columns:\n    ColumnsDescription[\"Name\"].append(cols)\n    ColumnsDescription[\"Type\"].append(data[cols].dtypes)\npd.DataFrame(ColumnsDescription,index=None)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-08-14T07:16:40.191859Z","iopub.execute_input":"2021-08-14T07:16:40.192212Z","iopub.status.idle":"2021-08-14T07:16:40.210125Z","shell.execute_reply.started":"2021-08-14T07:16:40.192155Z","shell.execute_reply":"2021-08-14T07:16:40.208601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>There are a lot of missing values in the dataset. For Categorical values, we will replace them with the Mode and for numerical values we will replace them with the mean of the values. Firstly, let us look at the data type of each column and the number of NA values in them.</p>","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-08-14T07:16:40.212809Z","iopub.execute_input":"2021-08-14T07:16:40.213375Z","iopub.status.idle":"2021-08-14T07:16:40.251938Z","shell.execute_reply.started":"2021-08-14T07:16:40.213331Z","shell.execute_reply":"2021-08-14T07:16:40.250806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Data Engineering</h1>\n<ul>\n    <li>Replace all categorical values with mode of that column and label encode them.</li>\n    <li>Replace all numerical values with the mean of that column.</li>\n</ul>","metadata":{}},{"cell_type":"code","source":"data['city'] = data['city'].fillna(data['city'].value_counts().index[0])\nCityLabelEncoder = LabelEncoder().fit(data['city'])\nCityList = CityLabelEncoder.classes_\ndata['city'] = CityLabelEncoder.transform(data['city'])\n\ndata['company_type'] = data['company_type'].fillna(data['company_type'].value_counts().index[0])\nCTypeLabelEncoder = LabelEncoder().fit(data['company_type'])\nCTypeList = CTypeLabelEncoder.classes_\ndata['company_type'] = CTypeLabelEncoder.transform(data['company_type'])\n\ndata['company_size'].replace(['<10','10/49', '50-99', '100-500', '500-999', '1000-4999', '5000-9999', '10000+',],\n                             ['Startup','Small','Small','Medium','Medium','Large','Large','Large'],inplace=True)\ndata['company_size'] = data['company_size'].fillna(data['company_size'].value_counts().index[0])\nCSizeLabelEncoder = LabelEncoder().fit(data['company_size'])\nCSizeList = CSizeLabelEncoder.classes_\ndata['company_size'] = CSizeLabelEncoder.transform(data['company_size'])\n\ndata['education_level'] = data['education_level'].fillna(data['education_level'].value_counts().index[0])\nEduLabelEncoder = LabelEncoder().fit(data['education_level'])\nEduList = EduLabelEncoder.classes_\ndata['education_level'] = EduLabelEncoder.transform(data['education_level'])\n\ndata['enrolled_university'] = data['enrolled_university'].fillna(data['enrolled_university'].value_counts().index[0])\nUniLabelEncoder = LabelEncoder().fit(data['enrolled_university'])\nUniList = UniLabelEncoder.classes_\ndata['enrolled_university'] = UniLabelEncoder.transform(data['enrolled_university'])\n\ndata['gender'] = data['gender'].fillna(data['gender'].value_counts().index[0])\nGenderLabelEncoder = LabelEncoder().fit(data['gender'])\nGenderList = GenderLabelEncoder.classes_\ndata['gender'] = GenderLabelEncoder.transform(data['gender'])\n\ndata['target'] = data['target'].fillna(data['target'].value_counts().index[0])\n\ndata['major_discipline'] = data['major_discipline'].fillna(data['major_discipline'].value_counts().index[0])\nMajorLabelEncoder = LabelEncoder().fit(data['major_discipline'])\nMajorList = MajorLabelEncoder.classes_\ndata['major_discipline'] = MajorLabelEncoder.transform(data['major_discipline'])\n\ndata['relevent_experience'] = data['relevent_experience'].fillna(data['relevent_experience'].value_counts().index[0])\nExpLabelEncoder = LabelEncoder().fit(data['relevent_experience'])\nExpList = ExpLabelEncoder.classes_\ndata['relevent_experience'] = ExpLabelEncoder.transform(data['relevent_experience'])","metadata":{"execution":{"iopub.status.busy":"2021-08-14T07:16:40.253421Z","iopub.execute_input":"2021-08-14T07:16:40.25376Z","iopub.status.idle":"2021-08-14T07:16:40.39184Z","shell.execute_reply.started":"2021-08-14T07:16:40.253725Z","shell.execute_reply":"2021-08-14T07:16:40.391004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>Few categorical columns require replacement of values with a fixed string or a number. If a number is encountered, convert that column values to Floating point values.</p>","metadata":{}},{"cell_type":"code","source":"data['last_new_job'].replace(['>4','never'],['4','0'],inplace=True)\ndata['last_new_job'].fillna(data['last_new_job'].value_counts().index[0],inplace=True)\ndata['last_new_job'] = [float(i) for i in data['last_new_job']]\n\ndata['experience'].replace(['>20','<1'],['20','1'],inplace=True)\ndata['experience'].fillna(data['experience'].value_counts().index[0],inplace=True)\ndata['experience'] = [float(i) for i in data['experience']]","metadata":{"execution":{"iopub.status.busy":"2021-08-14T07:16:40.393086Z","iopub.execute_input":"2021-08-14T07:16:40.393443Z","iopub.status.idle":"2021-08-14T07:16:40.44711Z","shell.execute_reply.started":"2021-08-14T07:16:40.393409Z","shell.execute_reply":"2021-08-14T07:16:40.446234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>Replace all missing values in numerical columns with their mean</p>","metadata":{}},{"cell_type":"code","source":"data['training_hours'] = data['training_hours'].fillna(data['training_hours'].mean())\ndata['training_hours'] = [float(i) for i in data['training_hours']]\ndata['city_development_index'] = data['city_development_index'].fillna(data['city_development_index'].mean())","metadata":{"execution":{"iopub.status.busy":"2021-08-14T07:16:40.448456Z","iopub.execute_input":"2021-08-14T07:16:40.448858Z","iopub.status.idle":"2021-08-14T07:16:40.468302Z","shell.execute_reply.started":"2021-08-14T07:16:40.448818Z","shell.execute_reply":"2021-08-14T07:16:40.467518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-08-14T07:16:40.470581Z","iopub.execute_input":"2021-08-14T07:16:40.470934Z","iopub.status.idle":"2021-08-14T07:16:40.48846Z","shell.execute_reply.started":"2021-08-14T07:16:40.470898Z","shell.execute_reply":"2021-08-14T07:16:40.487258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-08-14T07:16:40.49016Z","iopub.execute_input":"2021-08-14T07:16:40.490611Z","iopub.status.idle":"2021-08-14T07:16:40.508833Z","shell.execute_reply.started":"2021-08-14T07:16:40.490568Z","shell.execute_reply":"2021-08-14T07:16:40.507838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>Now that our data is prepared and ready, let us move onto EDA to understand our data better.</p>","metadata":{}},{"cell_type":"markdown","source":"<h1>EDA of the transformed dataset</h1>\n<p>Generate the dashboard of the dataset to better understand the distribution of the data</p>","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,25))\nplt.subplot(5,2,1)\ngenders,count = np.unique(data['gender'],return_counts=True)\nplt.title(\"Gender Count\")\nplt.xlabel(\"Gender\")\nplt.ylabel(\"Count\")\nplt.bar([GenderList[i] for i in genders],count)\n\nplt.subplot(5,2,2)\nexp,count = np.unique( data['relevent_experience'],return_counts=True)\nplt.title(\"Experience\")\nplt.xlabel(\"Experience Level\")\nplt.ylabel(\"Count\")\nplt.bar([ExpList[i] for i in exp],count)\n\nplt.subplot(5,2,3)\nuni,count = np.unique( data['enrolled_university'],return_counts=True)\nplt.title(\"University Enrolled\")\nplt.xlabel(\"University\")\nplt.ylabel(\"Count\")\nplt.bar([UniList[i] for i in uni],count)\n\nplt.subplot(5,2,4)\nedu,count = np.unique( data['education_level'],return_counts=True)\nplt.title(\"Education Level\")\nplt.xlabel(\"Level\")\nplt.ylabel(\"Count\")\nplt.bar([EduList[i] for i in edu],count)\n\nplt.subplot(5,2,5)\nmajor,count = np.unique( data['major_discipline'],return_counts=True)\nplt.title(\"Major Discipline\")\nplt.xlabel(\"Major\")\nplt.ylabel(\"Count\")\nplt.bar([MajorList[i] for i in major],count)\n\nplt.subplot(5,2,6)\nct,count = np.unique( data['company_type'],return_counts=True)\nplt.title(\"Company Type\")\nplt.xlabel(\"Type\")\nplt.ylabel(\"Count\")\nplt.bar([CTypeList[i] for i in ct],count)\n\nplt.subplot(5,2,7)\ncs,count = np.unique( data['company_size'],return_counts=True)\nplt.title(\"Company Size\")\nplt.xlabel(\"Size\")\nplt.ylabel(\"Count\")\nplt.bar([CSizeList[i] for i in cs],count)\n\nplt.subplot(5,2,8)\ntg,count = np.unique( data['target'],return_counts=True)\nplt.title(\"Target\")\nplt.xlabel(\"Target label\")\nplt.ylabel(\"Count\")\nplt.bar([[\"Not Looking\",\"Looking\"][int(i)] for i in tg],count)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Total cities : \",len(CityList))\nprint(\"Min City development index : {:.2f}\".format(min(data['city_development_index'])))\nprint(\"Max City development index : \",max(data['city_development_index']))\nprint(\"Min difference between last and current job : \",min(data['last_new_job']))\nprint(\"Max difference between last and current job : \",max(data['last_new_job']))\nprint(\"Min training hours : \",min(data['training_hours']))\nprint(\"Max training hours : \",max(data['training_hours']))","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-08-14T07:16:40.510571Z","iopub.execute_input":"2021-08-14T07:16:40.511094Z","iopub.status.idle":"2021-08-14T07:16:41.928895Z","shell.execute_reply.started":"2021-08-14T07:16:40.511056Z","shell.execute_reply":"2021-08-14T07:16:41.927912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Modeling</h1>\n<p>The dataset is highly imbalanced. Use SMOTE to upsample the dataset to generate more data.</p>","metadata":{"execution":{"iopub.status.busy":"2021-08-07T09:09:32.93279Z","iopub.execute_input":"2021-08-07T09:09:32.933139Z","iopub.status.idle":"2021-08-07T09:09:32.938542Z","shell.execute_reply.started":"2021-08-07T09:09:32.933102Z","shell.execute_reply":"2021-08-07T09:09:32.93731Z"}}},{"cell_type":"code","source":"X_org = data[data.columns[:len(data.columns)-1]].to_numpy()\nY_org = data[data.columns[len(data.columns)-1]].to_numpy()\nover = SMOTE(random_state=42)\nX, Y = over.fit_resample(X_org,Y_org)\n\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,shuffle=True,random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-14T07:16:41.930058Z","iopub.execute_input":"2021-08-14T07:16:41.930406Z","iopub.status.idle":"2021-08-14T07:16:42.007712Z","shell.execute_reply.started":"2021-08-14T07:16:41.930369Z","shell.execute_reply":"2021-08-14T07:16:42.006698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>RandomForest is a good choice since the dataset was imbalanced and SMOTE was used to generate samples. Use RandomSearchCV() to find the best parameters.</p>","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import plot_confusion_matrix \nfrom sklearn.model_selection import RandomizedSearchCV\n\nmodel=RandomForestClassifier()\ndistributions = dict(n_estimators = list(range(100,1001,100)),\n                     max_depth = list(range(10,200,10))),\nbest_params = RandomizedSearchCV(model,distributions,random_state=0)\nparams = best_params.fit(X_train,Y_train)\nprint(params.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-08-14T09:24:06.583995Z","iopub.execute_input":"2021-08-14T09:24:06.58437Z","iopub.status.idle":"2021-08-14T09:34:26.713534Z","shell.execute_reply.started":"2021-08-14T09:24:06.584339Z","shell.execute_reply":"2021-08-14T09:34:26.712652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=RandomForestClassifier(n_estimators=params.best_params_['n_estimators'],max_depth=params.best_params_['max_depth'])\nmodel.fit(X_train,Y_train)\nYPred = model.predict(X_test)\nprint(\"Accuracy score : {:.2f}\".format(accuracy_score(YPred,Y_test)))\nprint(\"Recall score : {:.2f}\".format(recall_score(YPred,Y_test,average='macro',zero_division=True)))\nprint(\"Precision score : {:.2f}\".format(precision_score(YPred,Y_test,zero_division = True)))\nprint(\"F1 score : {:.2f}\".format(f1_score(YPred,Y_test,zero_division=True)))\n\ndisp = plot_confusion_matrix(model,\n                                     X_test,\n                                     Y_test,\n                                     display_labels=[\"Not Leaving Job\",\"Leaving Job\"])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-14T09:36:26.36238Z","iopub.execute_input":"2021-08-14T09:36:26.362705Z","iopub.status.idle":"2021-08-14T09:36:42.993273Z","shell.execute_reply.started":"2021-08-14T09:36:26.362677Z","shell.execute_reply":"2021-08-14T09:36:42.99247Z"},"trusted":true},"execution_count":null,"outputs":[]}]}