{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hello! In this notebook, I am going to be analysing the Happiness Report dataset, see the correlations of the other factors with the Score. I will also analyse if game sales correlate to Happiness Scores as well."},{"metadata":{},"cell_type":"markdown","source":"This notebook is divided into several sections:\n1. Preprocessing Happiness Report\n2. Data Analysis (and modelling) on Happiness Report\n3. Preprocessing Video Game Sales and Happiness Report for analysis with the Video Game Sales\n4. Data Analysis on Video Game Sales and Happiness Report\n5. Conclusions\n6. Appendix"},{"metadata":{},"cell_type":"markdown","source":"First, import necessary libraries and also load the files"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plotting\nimport seaborn as sns # plotting\nimport re\n\n# read input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Preprocessing Happiness Report"},{"metadata":{},"cell_type":"markdown","source":"I am preprocessing the Happiness data by cleaning them from empty values, and combining them into a single table for easier use later on"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load happiness dataset and preprocess it\ndata2015 = pd.read_csv(\"../input/world-happiness/2015.csv\")\ndata2016 = pd.read_csv(\"../input/world-happiness/2016.csv\")\ndata2017 = pd.read_csv(\"../input/world-happiness/2017.csv\")\ndata2018 = pd.read_csv(\"../input/world-happiness/2018.csv\")\ndata2019 = pd.read_csv(\"../input/world-happiness/2019.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# simplify column names\ndata2015.columns = ['Country', 'Region', 'Rank', 'Score',\n                    'SE', 'GDP', 'Family', 'LifeExpectancy', 'Freedom', 'Corruption',\n                    'Generosity', 'DystopiaResidual']\n\ndata2016.columns = ['Country', 'Region', 'Rank', 'Score',\n                    'CILower', 'CIUpper', 'GDP', 'Family', \n                    'LifeExpectancy', 'Freedom', 'Corruption', 'Generosity', \n                    'DystopiaResidual']\n\ndata2017.columns = ['Country', 'Rank', 'Score','WhiskerHigh',\n                    'WhiskerLow','GDP','Family', 'LifeExpectancy',\n                    'Freedom','Generosity','Corruption','DystopiaResidual']\n\ndata2018.columns = ['Rank', 'Country', 'Score','GDP', \n                    'Social','LifeExpectancy','Freedom', \n                    'Generosity', 'Corruption']\n\ndata2019.columns = ['Rank', 'Country', 'Score', 'GDP',\n                    'Social', 'LifeExpectancy', 'Freedom', \n                    'Generosity', 'Corruption']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# and mean the values from each tables\nd2015 = data2015[['Country', 'Score']]\nd2016 = data2016[['Country', 'Score']]\nd2017 = data2017[['Country', 'Score']]\nd2018 = data2018[['Country', 'Score']]\nd2019 = data2019[['Country', 'Score']]\nscore_df = d2015.merge(d2016, left_on='Country', right_on='Country').merge(\n    d2017, left_on='Country', right_on='Country').merge(\n    d2018, left_on='Country', right_on='Country').merge(\n    d2019, left_on='Country', right_on='Country')\nscore_avg = score_df.iloc[:, 1:6].apply(np.mean, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d2015 = data2015[['Country', 'GDP']]\nd2016 = data2016[['Country', 'GDP']]\nd2017 = data2017[['Country', 'GDP']]\nd2018 = data2018[['Country', 'GDP']]\nd2019 = data2019[['Country', 'GDP']]\ngdp_df = d2015.merge(d2016, left_on='Country', right_on='Country').merge(\n    d2017, left_on='Country', right_on='Country').merge(\n    d2018, left_on='Country', right_on='Country').merge(\n    d2019, left_on='Country', right_on='Country')\ngdp_avg = gdp_df.iloc[:, 1:6].apply(np.mean, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d2015 = data2015[['Country', 'GDP']]\nd2016 = data2016[['Country', 'GDP']]\nd2017 = data2017[['Country', 'GDP']]\nfam_df = d2015.merge(d2016, left_on='Country', right_on='Country').merge(\n    d2017, left_on='Country', right_on='Country')\nfam_avg = fam_df.iloc[:, 1:4].apply(np.mean, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d2015 = data2015[['Country', 'LifeExpectancy']]\nd2016 = data2016[['Country', 'LifeExpectancy']]\nd2017 = data2017[['Country', 'LifeExpectancy']]\nd2018 = data2018[['Country', 'LifeExpectancy']]\nd2019 = data2019[['Country', 'LifeExpectancy']]\nlfe_df = d2015.merge(d2016, left_on='Country', right_on='Country').merge(\n    d2017, left_on='Country', right_on='Country').merge(\n    d2018, left_on='Country', right_on='Country').merge(\n    d2019, left_on='Country', right_on='Country')\nlfe_avg = lfe_df.iloc[:, 1:6].apply(np.mean, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d2015 = data2015[['Country', 'Corruption']]\nd2016 = data2016[['Country', 'Corruption']]\nd2017 = data2017[['Country', 'Corruption']]\nd2018 = data2018[['Country', 'Corruption']]\nd2019 = data2019[['Country', 'Corruption']]\ncrp_df = d2015.merge(d2016, left_on='Country', right_on='Country').merge(\n    d2017, left_on='Country', right_on='Country').merge(\n    d2018, left_on='Country', right_on='Country').merge(\n    d2019, left_on='Country', right_on='Country')\ncrp_avg = crp_df.iloc[:, 1:6].apply(np.mean, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d2015 = data2015[['Country', 'Generosity']]\nd2016 = data2016[['Country', 'Generosity']]\nd2017 = data2017[['Country', 'Generosity']]\nd2018 = data2018[['Country', 'Generosity']]\nd2019 = data2019[['Country', 'Generosity']]\ngen_df = d2015.merge(d2016, left_on='Country', right_on='Country').merge(\n    d2017, left_on='Country', right_on='Country').merge(\n    d2018, left_on='Country', right_on='Country').merge(\n    d2019, left_on='Country', right_on='Country')\ngen_avg = gen_df.iloc[:, 1:6].apply(np.mean, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d2015 = data2015[['Country', 'DystopiaResidual']]\nd2016 = data2016[['Country', 'DystopiaResidual']]\nd2017 = data2017[['Country', 'DystopiaResidual']]\ndysr_df = d2015.merge(d2016, left_on='Country', right_on='Country').merge(\n    d2017, left_on='Country', right_on='Country')\ndysr_avg = dysr_df.iloc[:, 1:4].apply(np.mean, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d2016 = data2016[['Country', 'CILower']]\nd2017 = data2017[['Country', 'WhiskerLow']]\ncilow_df = d2016.merge(d2017, left_on='Country', right_on='Country')\ncilow_avg = cilow_df.iloc[:, 1:5].apply(np.mean, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d2016 = data2016[['Country', 'CIUpper']]\nd2017 = data2017[['Country', 'WhiskerHigh']]\ncihigh_df = d2016.merge(d2017, left_on='Country', right_on='Country')\ncihigh_avg = cihigh_df.iloc[:, 1:3].apply(np.mean, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make new dataframe, and use the mean values of each column for the new dataframe\ndata_all = pd.DataFrame({'Country':score_df['Country'],\n                            'Score':score_avg,\n                            'GDP':gdp_avg,\n                            'Family':fam_avg,\n                            'LifeExpectancy':lfe_avg,\n                            'Corruption':crp_avg,\n                            'Generosity':gen_avg,\n                            'DystopiaResidual':dysr_avg,\n                            'CILower':cilow_avg,\n                            'CIUpper':cihigh_avg})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop rows with empty values, and save dataframe to a csv file\ndata_all = data_all.dropna()\ndata_all.shape\ndata_all.to_csv(\"all.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load resulting CSV file\nfor dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am also going to use the 2016 Happiness Report data for comparison with the Video Game Sales data. I select 2016 because it is the latest data available for the Video Game Sales dataset."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load the cleaned Happiness report dataset\ncleaned_df = pd.read_csv(\"../working/all.csv\")\n# use the 2016 data for comparison against cleaned data, because its most recent\nhappiness = data2016\n# look at the head of the 2016 data\nhappiness.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# describe the dataframe\nhappiness_summary = happiness.describe()\nhappiness_summary = happiness_summary.transpose()\nhappiness_summary.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look at the head of the cleaned data\ncleaned_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# describe the dataframe\ncleaned_summary = cleaned_df.describe()\ncleaned_summary = cleaned_summary.transpose()\ncleaned_summary.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Analysis (and modelling) on Happiness Report"},{"metadata":{},"cell_type":"markdown","source":"Doing data analysis on the cleaned World Happiness Report dataset by comparing the different features with the Happiness scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"# divide countries into categories for density\nhappycat1 = cleaned_df[cleaned_df['Score'].between(2, 4)]\nhappycat2 = cleaned_df[cleaned_df['Score'].between(4, 5)]\nhappycat3 = cleaned_df[cleaned_df['Score'].between(5, 6)]\nhappycat4 = cleaned_df[cleaned_df['Score'].between(6, 7)]\nhappycat5 = cleaned_df[cleaned_df['Score'].between(7, 10)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot line and density together\nplt.style.use('ggplot')\n\nfig, (ax1,ax2) = plt.subplots(1,2)\nfig.set_size_inches(20,10)\n\n# plot line graph of GDP vs Score\nx = cleaned_df['GDP']\ny = cleaned_df['Score']\nx = x.sort_values(ascending = False)\n\nax1.plot(x,y,color='black',linewidth=1)\nax1.title.set_text(\"Happiness Score vs GDP\")\nax1.set_xlabel(\"GDP\")\nax1.set_ylabel(\"Happiness Score\")\n\n# plot density graph of GDP vs Score\nsns.kdeplot(data=happycat1['GDP'], label=\"2-4\", color='salmon',ax=ax2)\nsns.kdeplot(data=happycat2['GDP'], label=\"4-5\", color='y',ax=ax2)\nsns.kdeplot(data=happycat3['GDP'], label=\"5-6\", color='mediumseagreen',ax=ax2)\nsns.kdeplot(data=happycat4['GDP'], label=\"6-7\", color='deepskyblue',ax=ax2)\nsns.kdeplot(data=happycat5['GDP'], label=\"7-10\", color='violet',ax=ax2)\n\nax2.title.set_text(\"Happiness Score vs GDP Density\")\nax2.set_xlabel(\"GDP\")\nax2.set_ylabel(\"Happiness Score\")\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot line and density together\nplt.style.use('ggplot')\n\nfig, (ax1,ax2) = plt.subplots(1,2)\nfig.set_size_inches(20,10)\n\n# plot line graph of Family vs Score\nx = cleaned_df['Family']\ny = cleaned_df['Score']\nx = x.sort_values(ascending = False)\n\nax1.plot(x,y,color='black',linewidth=1)\nax1.title.set_text(\"Happiness Score vs Family\")\nax1.set_xlabel(\"Family\")\nax1.set_ylabel(\"Happiness Score\")\n\n# plot density graph of Family vs Score\nsns.kdeplot(data=happycat1['Family'], label=\"2-4\", color='salmon',ax=ax2)\nsns.kdeplot(data=happycat2['Family'], label=\"4-5\", color='y',ax=ax2)\nsns.kdeplot(data=happycat3['Family'], label=\"5-6\", color='mediumseagreen',ax=ax2)\nsns.kdeplot(data=happycat4['Family'], label=\"6-7\", color='deepskyblue',ax=ax2)\nsns.kdeplot(data=happycat5['Family'], label=\"7-10\", color='violet',ax=ax2)\n\nax2.title.set_text(\"Happiness Score vs Family Density\")\nax2.set_xlabel(\"Family\")\nax2.set_ylabel(\"Happiness Score\")\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot line and density together\nplt.style.use('ggplot')\n\nfig, (ax1,ax2) = plt.subplots(1,2)\nfig.set_size_inches(20,10)\n\n# plot line graph of LifeExpectancy vs Score\nx = cleaned_df['LifeExpectancy']\ny = cleaned_df['Score']\nx = x.sort_values(ascending = False)\n\nax1.plot(x,y,color='black',linewidth=1)\nax1.title.set_text(\"Happiness Score vs Life Expectancy\")\nax1.set_xlabel(\"Life Expectancy\")\nax1.set_ylabel(\"Happiness Score\")\n\n# plot density graph of LifeExpectancy vs Score\nsns.kdeplot(data=happycat1['LifeExpectancy'], label=\"2-4\", color='salmon',ax=ax2)\nsns.kdeplot(data=happycat2['LifeExpectancy'], label=\"4-5\", color='y',ax=ax2)\nsns.kdeplot(data=happycat3['LifeExpectancy'], label=\"5-6\", color='mediumseagreen',ax=ax2)\nsns.kdeplot(data=happycat4['LifeExpectancy'], label=\"6-7\", color='deepskyblue',ax=ax2)\nsns.kdeplot(data=happycat5['LifeExpectancy'], label=\"7-10\", color='violet',ax=ax2)\n\nax2.title.set_text(\"Happiness Score vs Life Expectancy Density\")\nax2.set_xlabel(\"Life Expectancy\")\nax2.set_ylabel(\"Happiness Score\")\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot line and density together\nplt.style.use('ggplot')\n\nfig, (ax1,ax2) = plt.subplots(1,2)\nfig.set_size_inches(20,10)\n\n# plot line graph of Corruption vs Score\nx = cleaned_df['Corruption']\ny = cleaned_df['Score']\nx = x.sort_values(ascending = False)\n\nax1.plot(x,y,color='black',linewidth=1)\nax1.title.set_text(\"Happiness Score vs Corruption\")\nax1.set_xlabel(\"Corruption\")\nax1.set_ylabel(\"Happiness Score\")\n\n# plot density graph of Corruption vs Score\nsns.kdeplot(data=happycat1['Corruption'], label=\"2-4\", color='salmon',ax=ax2)\nsns.kdeplot(data=happycat2['Corruption'], label=\"4-5\", color='y',ax=ax2)\nsns.kdeplot(data=happycat3['Corruption'], label=\"5-6\", color='mediumseagreen',ax=ax2)\nsns.kdeplot(data=happycat4['Corruption'], label=\"6-7\", color='deepskyblue',ax=ax2)\nsns.kdeplot(data=happycat5['Corruption'], label=\"7-10\", color='violet',ax=ax2)\n\nax2.title.set_text(\"Happiness Score vs Corruption Density\")\nax2.set_xlabel(\"Corruption\")\nax2.set_ylabel(\"Happiness Score\")\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot line and density together\nplt.style.use('ggplot')\n\nfig, (ax1,ax2) = plt.subplots(1,2)\nfig.set_size_inches(20,10)\n\n# plot line graph of Generosity vs Score\nx = cleaned_df['Generosity']\ny = cleaned_df['Score']\nx = x.sort_values(ascending = False)\n\nax1.plot(x,y,color='black',linewidth=1)\nax1.title.set_text(\"Happiness Score vs Generosity\")\nax1.set_xlabel(\"Generosity\")\nax1.set_ylabel(\"Happiness Score\")\n\n# plot density graph of Generosity vs Score\nsns.kdeplot(data=happycat1['Generosity'], label=\"2-4\", color='salmon',ax=ax2)\nsns.kdeplot(data=happycat2['Generosity'], label=\"4-5\", color='y',ax=ax2)\nsns.kdeplot(data=happycat3['Generosity'], label=\"5-6\", color='mediumseagreen',ax=ax2)\nsns.kdeplot(data=happycat4['Generosity'], label=\"6-7\", color='deepskyblue',ax=ax2)\nsns.kdeplot(data=happycat5['Generosity'], label=\"7-10\", color='violet',ax=ax2)\n\nax2.title.set_text(\"Happiness Score vs Generosity Density\")\nax2.set_xlabel(\"Generosity\")\nax2.set_ylabel(\"Happiness Score\")\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot line and density together\nplt.style.use('ggplot')\n\nfig, (ax1,ax2) = plt.subplots(1,2)\nfig.set_size_inches(20,10)\n\n# plot line graph of DystopiaResidual vs Score\nx = cleaned_df['DystopiaResidual']\ny = cleaned_df['Score']\nx = x.sort_values(ascending = False)\n\nax1.plot(x,y,color='black',linewidth=1)\nax1.title.set_text(\"Happiness Score vs Dystopia Residual\")\nax1.set_xlabel(\"Dystopia Residual\")\nax1.set_ylabel(\"Happiness Score\")\n\n# plot density graph of DystopiaResidual vs Score\nsns.kdeplot(data=happycat1['DystopiaResidual'], label=\"2-4\", color='salmon',ax=ax2)\nsns.kdeplot(data=happycat2['DystopiaResidual'], label=\"4-5\", color='y',ax=ax2)\nsns.kdeplot(data=happycat3['DystopiaResidual'], label=\"5-6\", color='mediumseagreen',ax=ax2)\nsns.kdeplot(data=happycat4['DystopiaResidual'], label=\"6-7\", color='deepskyblue',ax=ax2)\nsns.kdeplot(data=happycat5['DystopiaResidual'], label=\"7-10\", color='violet',ax=ax2)\n\nax2.title.set_text(\"Happiness Score vs Dystopia Residual Density\")\nax2.set_xlabel(\"Dystopia Residual\")\nax2.set_ylabel(\"Happiness Score\")\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot line and density together\nplt.style.use('ggplot')\n\nfig, (ax1,ax2) = plt.subplots(1,2)\nfig.set_size_inches(20,10)\n\n# plot line graph of CILower vs Score\nx = cleaned_df['CILower']\ny = cleaned_df['Score']\nx = x.sort_values(ascending = False)\n\nax1.plot(x,y,color='black',linewidth=1)\nax1.title.set_text(\"Happiness Score vs CILower\")\nax1.set_xlabel(\"CILower\")\nax1.set_ylabel(\"Happiness Score\")\n\n# plot density graph of CILower vs Score\nsns.kdeplot(data=happycat1['CILower'], label=\"2-4\", color='salmon',ax=ax2)\nsns.kdeplot(data=happycat2['CILower'], label=\"4-5\", color='y',ax=ax2)\nsns.kdeplot(data=happycat3['CILower'], label=\"5-6\", color='mediumseagreen',ax=ax2)\nsns.kdeplot(data=happycat4['CILower'], label=\"6-7\", color='deepskyblue',ax=ax2)\nsns.kdeplot(data=happycat5['CILower'], label=\"7-10\", color='violet',ax=ax2)\n\nax2.title.set_text(\"Happiness Score vs CILower Density\")\nax2.set_xlabel(\"CILower\")\nax2.set_ylabel(\"Happiness Score\")\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot line and density together\nplt.style.use('ggplot')\n\nfig, (ax1,ax2) = plt.subplots(1,2)\nfig.set_size_inches(20,10)\n\n# plot line graph of CILower vs Score\nx = cleaned_df['CIUpper']\ny = cleaned_df['Score']\nx = x.sort_values(ascending = False)\n\nax1.plot(x,y,color='black',linewidth=1)\nax1.title.set_text(\"Happiness Score vs CIUpper\")\nax1.set_xlabel(\"CIUpper\")\nax1.set_ylabel(\"Happiness Score\")\n\n# plot density graph of CIUpper vs Score\nsns.kdeplot(data=happycat1['CIUpper'], label=\"2-4\", color='salmon',ax=ax2)\nsns.kdeplot(data=happycat2['CIUpper'], label=\"4-5\", color='y',ax=ax2)\nsns.kdeplot(data=happycat3['CIUpper'], label=\"5-6\", color='mediumseagreen',ax=ax2)\nsns.kdeplot(data=happycat4['CIUpper'], label=\"6-7\", color='deepskyblue',ax=ax2)\nsns.kdeplot(data=happycat5['CIUpper'], label=\"7-10\", color='violet',ax=ax2)\n\nax2.title.set_text(\"Happiness Score vs CIUpper Density\")\nax2.set_xlabel(\"CIUpper\")\nax2.set_ylabel(\"Happiness Score\")\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generally most variables/ features have a linear relationship with the Happiness score, meaning that when the value of the variable increases, the happiness score increases, and when the value of the variable decreases, the happiness score also decreases. This was also confirmed in the Density Graphs, where the country groups generally have maximum density on different values.\n\nAlthough the relationship is generally linear, the line graphs are not completely linear, meaning that some countries could have a high value of a variable, but still have a slightly lower Happiness score than countries with similar values of the variable, which results in a spike like graph. \n\nFor example:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select two countries with similar GDP\ngdp_illust = {'Netherlands', 'Saudi Arabia'}\ncleaned_df[cleaned_df.Country.isin(gdp_illust)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For example, two countries with similar GDP of around 1.4 such as the Netherlands and Saudi Arabia have a difference of Happiness scores of around 1.0."},{"metadata":{},"cell_type":"markdown","source":"Data Analysis on Happiness Report done, now I'm going to do Modelling on the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# import libaries for modelling\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define Label and Data\n# Label = Score, the variable we're trying to predict\ny = cleaned_df.Score\n# Features/Factors\nhappiness_features = ['GDP', 'Family', 'LifeExpectancy', 'Corruption',\n'Generosity', 'DystopiaResidual', 'CILower', 'CIUpper']\nX = cleaned_df[happiness_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build decision tree model\ndecision_tree = DecisionTreeRegressor(random_state = 1)\n# and Fit model\ndecision_tree.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Validate model\n# Calculate mean absolute error:\ndtScores = decision_tree.predict(X)\nMAE_before = mean_absolute_error(y, dtScores)\n# Split data into training and validation\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n# Define and Fit model\ndecision_tree = DecisionTreeRegressor()\ndecision_tree.fit(train_X, train_y)\n# Get predicted results\nvalidation_predictions = decision_tree.predict(val_X)\nMAE_after = mean_absolute_error(val_y, validation_predictions)\n# Compare MAE\nprint(\"Decision Trees MAE before training: \", MAE_before)\nprint(\"Decision Trees MAE after training: \", MAE_after)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run Previous codes again to refresh\n# Set label and data\ny = cleaned_df.Score\nhappiness_features = ['GDP', 'Family', 'LifeExpectancy', 'Corruption',\n'Generosity', 'DystopiaResidual', 'CILower', 'CIUpper']\nX = cleaned_df[happiness_features]\n# Split into validation and training data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\n# Build Random Forests\nforest_model = RandomForestRegressor(random_state=1)\nforest_model.fit(train_X, train_y)\nforest_preds = forest_model.predict(val_X)\n# Output MAE\nprint(\"Random Forest MAE: \", mean_absolute_error(val_y, forest_preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Everything done on the Happiness Report, and we can conclude that GDP, Family, Life Expectancy, Dystopia Residual, CILower, and CIUpper does have a factor to the happiness scores of the countries, meaning that these factors does have an effect on an individual's Happiness. Therefore this could be improved to improve Happiness.\n\nNot only that, Decision Trees and Random Forests models performed quite well, with MAE being quite low on both, although usually Random Forests perform better than Decision Trees.\n\nNow that that's done, I am going to correlate Video Game Sales to happiness. "},{"metadata":{},"cell_type":"markdown","source":"# 3. Preprocessing on Video Game Sales and Happiness Reports"},{"metadata":{},"cell_type":"markdown","source":"First, I am going to preprocess the game sales data by cleaning the data from empty values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# load video game sales dataset and look at the head\ngame_df = pd.read_csv(\"../input/videogamesales/vgsales.csv\")\ngame_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look at number of rows with missing values\nis_NaN = game_df.isnull()\nrow_has_NaN = is_NaN.any(axis=1)\nrows_with_NaN = game_df[row_has_NaN]\n# rows_with_NaN.head()\nprint(\"No. of Rows with Missing Values:\", rows_with_NaN.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look where the missing values are \ngame_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So from the looks of it, null values are on Year and Publisher, with most of the null values existing in the Year Column, and some have both null. For Publisher, since I am going to only look at the sales, the Publisher won't be important here, so I simply impute the Publisher with unknown, and use year from game title, or just use median of the gaming platform as the year."},{"metadata":{"trusted":true},"cell_type":"code","source":"# impute null values on publisher with Unknown\nnull_pub = game_df[game_df.Publisher.isnull()]\nfor index, row in null_pub.iterrows():\n    game_df.loc[index,'Publisher'] = 'Unknown'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# impute Year value with full years on the titles\nnull_years = game_df[game_df.Year.isnull()]\n\nfor index, row in null_years.iterrows():\n    match = re.match(r'.*([1-3][0-9]{3})',row['Name'])\n    if match is not None:\n        game_df.loc[index,'Year'] = float(match.group(1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# impute Year value with simplified years (e.g. 07 = 2007) on the titles.\n# but first, look at the games with two double digit numbers on the title\n\nnull_years = game_df[game_df.Year.isnull()]\n\nfor index, row in null_years.iterrows():\n    match = re.match(r'.*([0-9]{2})',row['Name'])\n    if match is not None:\n        currentYear = float(match.group(1))\n        if (currentYear > 80 or currentYear < 17):\n            print (\"Index:\", index, \", Title:\", row['Name'], \",Year:\", currentYear)\n#             game_df.loc[index,'Year'] = currentYear","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some of the numbers here are not exactly years, so look at the games and manually analyse those. After analysis, turns out that:\n* Index: 12015, Title: Drake of the 99 Dragons, actual year: 2003\n* Index: 6283, Title: Indy 500, actual Year: 1977"},{"metadata":{"trusted":true},"cell_type":"code","source":"# and impute these games Year values with its actual year\ngame_df.loc[12015,'Year'] = float(2003)\ngame_df.loc[6283,'Year'] = float(1977)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# impute Year value with simplified years before the millenium (e.g. 99 = 1999) on the titles.\n\nnull_years = game_df[game_df.Year.isnull()]\n\nfor index, row in null_years.iterrows():\n    match = re.match(r'.*([0-9]{2})',row['Name'])\n    if match is not None:\n        currentYear = float(match.group(1))\n        if (currentYear > 80):\n            currentYear = 1900+ currentYear\n            print (\"Index:\", index, \", Title:\", row['Name'], \",Year:\", currentYear)\n            game_df.loc[index,'Year'] = float(currentYear)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# impute Year value with simplified years (e.g. 07 = 2007) on the titles.\n\nnull_years = game_df[game_df.Year.isnull()]\n\nfor index, row in null_years.iterrows():\n    match = re.match(r'.*([0-9]{2})',row['Name'])\n    if match is not None:\n        currentYear = float(match.group(1))\n        if (currentYear < 17):\n            currentYear = 2000+ currentYear\n            print (\"Index:\", index, \", Title:\", row['Name'], \",Year:\", currentYear)\n            game_df.loc[index,'Year'] = float(currentYear)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look thru remaining empty Years\n\nnull_years = game_df[game_df.Year.isnull()]\n\nnull_years","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get unique Platforms in the dataset\ngame_df.Platform.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# impute using median years of the console its released on\n# Median calculated manually using the Console Lifespans from Wikipedia\n\navgYears = {'Wii': 2009, 'NES':1990, 'GB':1996, 'DS':2008, 'X360':2011, 'PS3':2012, 'PS2':2006, 'SNES':1996, 'GBA':2006,\n            '3DS':2013, 'PS4':2014, 'N64':1999, 'PS':2000, 'XB':2005, 'PC': 2016, '2600':1985, 'PSP':2009, 'XOne':2016, \n            'GC':2004,'WiiU':2014, 'GEN':1993, 'DC':1999, 'PSV':2015, 'SAT':1997, 'SCD':1994, 'WS':2001, 'NG':1997, \n            'TG16':1990,'3DO':1995, 'GG':1993, 'PCFX':1996}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_years = game_df[game_df.Year.isnull()]\n\nfor index, row in null_years.iterrows():\n    game_df.loc[index,'Year'] = float(avgYears[game_df.iloc[index].Platform])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if there is empty left\ngame_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All empty values are handled! Now let us look at the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# describe the dataframe\ngame_sum = game_df.describe()\ngame_sum = game_sum.transpose()\ngame_sum.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see from the summary, this dataset is last updated in 2016, but there are some games that have a release year beyond that. Let's analyse it even further"},{"metadata":{"trusted":true},"cell_type":"code","source":"# look at the game list sorted based on years descending\ngame_df.sort_values(by=['Year'],ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, only one game is from 2020, which is a Nintendo DS game called \"Imagine: Make up artist\". This is weird because the NDS was discontinued in 2013, but they make a game for it in 2020? That's definitely invalid. Not only that, the video game sales dataset was last updated in October 2016, which means this data is definetly invalid, possibly also games that has a release year beyond 2016. \n\nFor these games, I need to do further analysis. When looked at in Google, \"Imagine: Make up artist\" is actually a game released in 2009, so I will set the year value to 2009 for this row. \"Brothers Conflict: Precious Baby\" was released in 2016, and \"Phantasy Star Online 2 Episode 4: Deluxe Package\" were also released in 2016, so I am going to set the years to the actual Years on these games."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fix invalid data by its index.\ngame_df.loc[5957,'Year'] = 2009\ngame_df.loc[16438,'Year'] = 2016\ngame_df.loc[14390,'Year'] = 2016\ngame_df.loc[16241,'Year'] = 2016\n\n# or optionally drop it\n# game_df.drop([5957])\n# game_df.drop([16438])\n# game_df.drop([14390])\n# game_df.drop([16241])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's look at the recent games, which are games that are released on consoles that are still active at the time of the sales dataset last update, 2016. Active consoles are retrieved manually from Wikipedia. If console lifespan is until > 2016, count that console as active."},{"metadata":{"trusted":true},"cell_type":"code","source":"active_consoles = ['3DS', 'PS4', 'PC', 'XOne', 'WiiU', 'PSV']\nrecent_games = game_df[game_df.Platform.isin(active_consoles)]\nrecent_games.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function for getting the sum of the sales of each region\ndef getSumSales(df):\n    na_sum = df['NA_Sales'].sum()\n    eu_sum = df['EU_Sales'].sum()\n    jp_sum = df['JP_Sales'].sum()\n    other_sum = df['Other_Sales'].sum()\n    global_sum = df['Global_Sales'].sum()\n\n    return na_sum, eu_sum, jp_sum, other_sum, global_sum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to make new data frame of sum sales and append the sums into the df\ndef makeSumDF(df):\n    na_sum, eu_sum, jp_sum, other_sum, global_sum = getSumSales(df)\n    newSumDF = pd.DataFrame(columns = ['Country','Sales'])\n\n    newSumDF = newSumDF.append({'Country': 'North America','Sales': na_sum},ignore_index = True)\n    newSumDF = newSumDF.append({'Country': 'European Union','Sales': eu_sum},ignore_index = True)\n    newSumDF = newSumDF.append({'Country': 'Japan','Sales': jp_sum},ignore_index = True)\n    newSumDF = newSumDF.append({'Country': 'Rest of the World','Sales': other_sum},ignore_index = True)\n    newSumDF = newSumDF.append({'Country': 'Global','Sales': global_sum},ignore_index = True)\n    return newSumDF","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make recent sales dataframe \nrecent_sumSales = makeSumDF(recent_games)\nrecent_sumSales","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make all time sales dataframe \nsumSales = makeSumDF(game_df)\nsumSales","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preprocess the Happiness dataframes for analysis with the Video Game Sales and make new dataframe for the combined values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# make new empty dataframe just for the regions in the video game sales dataset\nhappiness_regions = pd.DataFrame(columns = happiness.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop unused features and simplify feature names from 2016 Happiness dataset\nhappiness = happiness.drop(columns=['Region', 'Rank'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combine the Happiness report countries to follow the game sales regions\njpn_whr = happiness[happiness.Country == 'Japan']\njpn_whr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the countries in the regions and get its subset for insertion in new dataset later on\n\nna_countries = ['Canada', 'United States', 'Mexico']\nna_whr = happiness[happiness.Country.isin(na_countries)]\nna_whr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"happiness.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# append the regional values into the new dataframe\nhappiness_regions = happiness_regions.append({'Country': 'North America', \n                                              'Score': na_whr['Score'].mean(), \n                                              'CILower': na_whr['CILower'].mean(), \n                                              'CIUpper': na_whr['CIUpper'].mean(), \n                                              'GDP': na_whr['GDP'].mean(), \n                                              'Family': na_whr['Family'].mean(), \n                                              'LifeExpectancy': na_whr['LifeExpectancy'].mean(), \n                                              'Freedom': na_whr['Freedom'].mean(), \n                                              'Corruption': na_whr['Corruption'].mean(), \n                                              'Generosity': na_whr['Generosity'].mean(), \n                                              'DystopiaResidual': na_whr['DystopiaResidual'].mean()},\n                                             ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For EU region, for simplicity, I am going to use European Union member countries for it. I am going to count in the European Union Members in 2016, before Brexit (which officially happened in January 2020). From the [EU Official Page](https://europa.eu/european-union/about-eu/countries_en), EU members were: \nAustria, Belgium, Bulgaria, Croatia, Cyprus, Czech Republic, Denmark, Estonia, Finland,\tFrance, Germany, Greece, Hungary, Ireland, Italy, Latvia, Lithuania, Luxembourg, Malta, Netherlands, Poland, Portugal, Romania, Slovakia, Slovenia, Spain, Sweden, United Kingdom."},{"metadata":{"trusted":true},"cell_type":"code","source":"EU_countries = ['Austria', 'Belgium', 'Bulgaria', 'Croatia', 'Cyprus', 'Czech Republic', 'Denmark', 'Estonia', \n                'Finland', 'France', 'Germany', 'Greece', 'Hungary', 'Ireland', 'Italy', 'Latvia', 'Lithuania', \n                'Luxembourg', 'Malta', 'Netherlands', 'Poland', 'Portugal', 'Romania', 'Slovakia', 'Slovenia', \n                'Spain', 'Sweden', 'United Kingdom']\n\nEU_whr = happiness[happiness.Country.isin(EU_countries)]\nEU_whr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"happiness_regions = happiness_regions.append({'Country': 'European Union', \n                                              'Score': EU_whr['Score'].mean(), \n                                              'CILower': EU_whr['CILower'].mean(), \n                                              'CIUpper': EU_whr['CIUpper'].mean(), \n                                              'GDP': EU_whr['GDP'].mean(), \n                                              'Family': EU_whr['Family'].mean(), \n                                              'LifeExpectancy': EU_whr['LifeExpectancy'].mean(), \n                                              'Freedom': EU_whr['Freedom'].mean(), \n                                              'Corruption': EU_whr['Corruption'].mean(), \n                                              'Generosity': EU_whr['Generosity'].mean(), \n                                              'DystopiaResidual': EU_whr['DystopiaResidual'].mean()},\n                                             ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"happiness_regions = happiness_regions.append(jpn_whr, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add in rest of the world, which is every countries that does not belong in NA, EU or Japan\n\ngame_regions = na_countries + EU_countries \ngame_regions.append('Japan')\nRestofWorld = happiness[~happiness.Country.isin(game_regions)]\nRestofWorld","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"happiness_regions = happiness_regions.append({'Country': 'Rest of the World', \n                                              'Score': RestofWorld['Score'].mean(), \n                                              'CILower': RestofWorld['CILower'].mean(), \n                                              'CIUpper': RestofWorld['CIUpper'].mean(), \n                                              'GDP': RestofWorld['GDP'].mean(), \n                                              'Family': RestofWorld['Family'].mean(), \n                                              'LifeExpectancy': RestofWorld['LifeExpectancy'].mean(), \n                                              'Freedom': RestofWorld['Freedom'].mean(), \n                                              'Corruption': RestofWorld['Corruption'].mean(), \n                                              'Generosity': RestofWorld['Generosity'].mean(), \n                                              'DystopiaResidual': RestofWorld['DystopiaResidual'].mean()},\n                                             ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"happiness_regions = happiness_regions.append({'Country': 'Global', \n                                              'Score': happiness['Score'].mean(), \n                                              'CILower': happiness['CILower'].mean(), \n                                              'CIUpper': happiness['CIUpper'].mean(), \n                                              'GDP': happiness['GDP'].mean(), \n                                              'Family': happiness['Family'].mean(), \n                                              'LifeExpectancy': happiness['LifeExpectancy'].mean(), \n                                              'Freedom': happiness['Freedom'].mean(), \n                                              'Corruption': happiness['Corruption'].mean(), \n                                              'Generosity': happiness['Generosity'].mean(), \n                                              'DystopiaResidual': happiness['DystopiaResidual'].mean()},\n                                             ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"happiness_regions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make combined region dataframe from cleaned dataframe\ncleaned_regions = pd.DataFrame(columns = cleaned_df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"na_cleaned = cleaned_df[cleaned_df.Country.isin(na_countries)]\n# na_cleaned\neu_cleaned = cleaned_df[cleaned_df.Country.isin(EU_countries)]\n# eu_cleaned\njpn_cleaned = cleaned_df[cleaned_df.Country == 'Japan']\n# jpn_cleaned\nother_cleaned = cleaned_df[~cleaned_df.Country.isin(game_regions)]\n# other_cleaned","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# and append them into the new dataframe\ncleaned_regions = cleaned_regions.append({'Country': 'North America',\n                                          'Score': na_cleaned['Score'].mean(), \n                                          'GDP': na_cleaned['GDP'].mean(),\n                                          'Family': na_cleaned['Family'].mean(), \n                                          'LifeExpectancy': na_cleaned['LifeExpectancy'].mean(), \n                                          'Corruption': na_cleaned['Corruption'].mean(),\n                                          'Generosity': na_cleaned['Generosity'].mean(), \n                                          'DystopiaResidual': na_cleaned['DystopiaResidual'].mean(),\n                                          'CILower': na_cleaned['CILower'].mean(),\n                                          'CIUpper': na_cleaned['CIUpper'].mean()},\n                                         ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_regions = cleaned_regions.append({'Country': 'European Union',\n                                          'Score': eu_cleaned['Score'].mean(), \n                                          'GDP': eu_cleaned['GDP'].mean(),\n                                          'Family': eu_cleaned['Family'].mean(), \n                                          'LifeExpectancy': eu_cleaned['LifeExpectancy'].mean(), \n                                          'Corruption': eu_cleaned['Corruption'].mean(),\n                                          'Generosity': eu_cleaned['Generosity'].mean(), \n                                          'DystopiaResidual': eu_cleaned['DystopiaResidual'].mean(),\n                                          'CILower': eu_cleaned['CILower'].mean(),\n                                          'CIUpper': eu_cleaned['CIUpper'].mean()},\n                                         ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_regions = cleaned_regions.append({'Country': 'Japan',\n                                          'Score': jpn_cleaned['Score'].mean(), \n                                          'GDP': jpn_cleaned['GDP'].mean(),\n                                          'Family': jpn_cleaned['Family'].mean(), \n                                          'LifeExpectancy': jpn_cleaned['LifeExpectancy'].mean(), \n                                          'Corruption': jpn_cleaned['Corruption'].mean(),\n                                          'Generosity': jpn_cleaned['Generosity'].mean(), \n                                          'DystopiaResidual': jpn_cleaned['DystopiaResidual'].mean(),\n                                          'CILower': jpn_cleaned['CILower'].mean(),\n                                          'CIUpper': jpn_cleaned['CIUpper'].mean()},\n                                         ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_regions = cleaned_regions.append({'Country': 'Rest of the World',\n                                          'Score': other_cleaned['Score'].mean(), \n                                          'GDP': other_cleaned['GDP'].mean(),\n                                          'Family': other_cleaned['Family'].mean(), \n                                          'LifeExpectancy': other_cleaned['LifeExpectancy'].mean(), \n                                          'Corruption': other_cleaned['Corruption'].mean(),\n                                          'Generosity': other_cleaned['Generosity'].mean(), \n                                          'DystopiaResidual': other_cleaned['DystopiaResidual'].mean(),\n                                          'CILower': other_cleaned['CILower'].mean(),\n                                          'CIUpper': other_cleaned['CIUpper'].mean()},\n                                         ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_regions = cleaned_regions.append({'Country': 'Global',\n                                          'Score': cleaned_df['Score'].mean(), \n                                          'GDP': cleaned_df['GDP'].mean(),\n                                          'Family': cleaned_df['Family'].mean(), \n                                          'LifeExpectancy': cleaned_df['LifeExpectancy'].mean(), \n                                          'Corruption': cleaned_df['Corruption'].mean(),\n                                          'Generosity': cleaned_df['Generosity'].mean(), \n                                          'DystopiaResidual': cleaned_df['DystopiaResidual'].mean(),\n                                          'CILower': cleaned_df['CILower'].mean(),\n                                          'CIUpper': cleaned_df['CIUpper'].mean()},\n                                         ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_regions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Data Analysis on Video Game Sales and Happiness Report"},{"metadata":{"trusted":true},"cell_type":"code","source":"# use list for color mapping graphs for each regions\nregionCmap = {' North America': 'red', \n              ' European Union': 'blue', \n              ' Japan': 'salmon', \n              ' Rest of the World': 'goldenrod', \n              ' Global':'green'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recent_happiness = pd.DataFrame(recent_sumSales)\nrecent_happiness['Score'] = happiness_regions['Score']\nrecent_happiness","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = happiness_regions['Score']\ny = recent_sumSales['Sales']\n\nplt.style.use('ggplot')\nfig, ax = plt.subplots()\nfig.set_size_inches(7,5)\n\nplt.title(\"Recent Game Sales vs 2016 Happiness Score\")\nplt.xlabel(\"Happiness Score\")\nplt.ylabel(\"Game Sales (in Millions)\")\n\n# plot every points as scatter graph\nfor pointx, pointy in zip(x,y):\n    label = recent_sumSales.Country[recent_sumSales['Sales'] == pointy].to_string(index=False)\n    plt.scatter(pointx, pointy, c = regionCmap[label], label = label)\n    \n    # annotate labels on the datapoints\n    plt.annotate(label,\n                 (pointx, pointy),\n                 textcoords=\"offset points\",\n                 xytext=(0,5),\n                 ha='center')\nax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_happiness = pd.DataFrame(sumSales)\nall_happiness['Score'] = happiness_regions['Score']\nall_happiness","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = happiness_regions['Score']\ny = sumSales['Sales']\n\nplt.style.use('ggplot')\nfig, ax = plt.subplots()\nfig.set_size_inches(7,5)\n\nplt.title(\"Recent Game Sales vs 2016 Happiness Score\")\nplt.xlabel(\"Happiness Score\")\nplt.ylabel(\"Game Sales (in Millions)\")\n\n# plot every points as scatter graph\nfor pointx, pointy in zip(x,y):\n    label = sumSales.Country[sumSales['Sales'] == pointy].to_string(index=False)\n    plt.scatter(pointx, pointy, c = regionCmap[label], label = label)\n    \n    # annotate labels on the datapoints\n    plt.annotate(label,\n                 (pointx, pointy),\n                 textcoords=\"offset points\",\n                 xytext=(0,5),\n                 ha='center')\nax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recent_cleaned = pd.DataFrame(recent_sumSales)\nrecent_cleaned['Score'] = cleaned_regions['Score']\nrecent_cleaned","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = cleaned_regions['Score']\ny = recent_sumSales['Sales']\n\nplt.style.use('ggplot')\nfig, ax = plt.subplots()\nfig.set_size_inches(7,5)\n\nplt.title(\"Recent Game Sales vs Cleaned Happiness Score\")\nplt.xlabel(\"Happiness Score\")\nplt.ylabel(\"Game Sales (in Millions)\")\n\n# plot every points as scatter graph\nfor pointx, pointy in zip(x,y):\n    label = recent_sumSales.Country[recent_sumSales['Sales'] == pointy].to_string(index=False)\n    plt.scatter(pointx, pointy, c = regionCmap[label], label = label)\n    \n    # annotate labels on the datapoints\n    plt.annotate(label,\n                 (pointx, pointy),\n                 textcoords=\"offset points\",\n                 xytext=(0,10),\n                 ha='center')\n    \nax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_cleaned = pd.DataFrame(sumSales)\nall_cleaned['Score'] = cleaned_regions['Score']\nall_cleaned","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = cleaned_regions['Score']\ny = sumSales['Sales']\n\nplt.style.use('ggplot')\nfig, ax = plt.subplots()\nfig.set_size_inches(7,5)\n\nplt.title(\"All Time Game Sales vs Cleaned Happiness Score\")\nplt.xlabel(\"Happiness Score\")\nplt.ylabel(\"Game Sales (in Millions)\")\n\n# plot every points as scatter graph\nfor pointx, pointy in zip(x,y):\n    label = sumSales.Country[sumSales['Sales'] == pointy].to_string(index=False)\n    plt.scatter(pointx, pointy, c = regionCmap[label], label = label)\n\n    # annotate labels on the datapoints\n    plt.annotate(label,\n                 (pointx, pointy),\n                 textcoords=\"offset points\",\n                 xytext=(0,10),\n                 ha='center')\n\nax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot combined bar chart"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = happiness_regions['Score']\ny = sumSales['Sales']\naltY = recent_sumSales['Sales']\n\nplt.style.use('ggplot')\nfig, ax = plt.subplots()\nfig.set_size_inches(7,5)\n\nplt.title(\"Game Sales vs 2016 Happiness Scores\")\nplt.xlabel(\"Happiness Score\")\nplt.ylabel(\"Game Sales (in Millions)\")\n\n# Plot the All-time sales Bar\nfor pointx, pointy in zip(x,y):\n    label = sumSales.Country[sumSales['Sales'] == pointy].to_string(index=False)\n    plt.bar(pointx, pointy, 0.1, color = regionCmap[label], label = label, alpha = 0.5)\n    # annotate label with the Happiness Scores\n    plt.annotate(round(pointx,2),\n                 (pointx, pointy),\n                 textcoords=\"offset points\",\n                 xytext=(0,10),\n                 ha='center')\n\n# Plot the recent sales Bar\nfor pointx, pointy in zip(x,altY):\n    label = recent_sumSales.Country[recent_sumSales['Sales'] == pointy].to_string(index=False)\n    plt.bar(pointx, pointy, 0.1, color = regionCmap[label], alpha = 0.5)\n    \nax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = cleaned_regions['Score']\ny = sumSales['Sales']\naltY = recent_sumSales['Sales']\n\nplt.style.use('ggplot')\nfig, ax = plt.subplots()\nfig.set_size_inches(7,5)\n\nplt.title(\"Game Sales vs Cleaned Happiness Scores\")\nplt.xlabel(\"Happiness Score\")\nplt.ylabel(\"Game Sales (in Millions)\")\n\n# Plot the All-time sales Bar\nfor pointx, pointy in zip(x,y):\n    label = sumSales.Country[sumSales['Sales'] == pointy].to_string(index=False)\n    plt.bar(pointx, pointy, 0.1, color = regionCmap[label], label = label, alpha = 0.5)\n    # annotate label with the Happiness Scores\n    plt.annotate(round(pointx,2),\n                 (pointx, pointy),\n                 textcoords=\"offset points\",\n                 xytext=(0,10),\n                 ha='center')\n\n# Plot the recent sales Bar\nfor pointx, pointy in zip(x,altY):\n    label = recent_sumSales.Country[recent_sumSales['Sales'] == pointy].to_string(index=False)\n    plt.bar(pointx, pointy, 0.1, color = regionCmap[label], alpha = 0.5)\n    \nax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Conclusions"},{"metadata":{},"cell_type":"markdown","source":"In conclusion, some factors could possibly affect the happiness of a country or a region. Factors such as GDP, Family, Life Expectancy, CILower, and CIUpper have a linear effect on the Happiness Scores, meaning that the higher these values are, the higher the happiness scores will be. Whereas Corruption, Generosity and Dystopia Residual do not really have a big factor to the Happiness scores, as most countries are all dense on a similar value. Video Game sales do have an effect on the happiness scores of a region, the higher the game sales of a region is, the higher the happiness scores is. But globally, the Happiness Scores are still quite low, although the video game sales were high, which means Video Games definitely affect Happiness, but is not the main factor.\n\nFrom the results of the analysis from this project, a conclusion could be drawn that the higher the GDP of a country, the stronger the family aspect, the higher the life expectancy, and the higher the video game sales are, the happier the population of the country/region is. Therefore, in order to increase Happiness of the population of the country/region, increasing GDP, Family, Life Expectancy, and Video Games could be done.\n"},{"metadata":{},"cell_type":"markdown","source":"# 6. Appendix"},{"metadata":{},"cell_type":"markdown","source":"Appendix contains extra graphs, and modelling attempt on the Video Game sales and Happiness Report"},{"metadata":{"trusted":true},"cell_type":"code","source":"# GDP vs Happiness vs Game Sales\nx = cleaned_regions['Score']\ny = sumSales['Sales']\nz = cleaned_regions['GDP']\n\nfig = plt.figure()\nplt.style.use('ggplot')\nfig, ax = plt.subplots()\nfig.set_size_inches(7,5)\nax = plt.axes(projection='3d')\n\nplt.title(\"All Time Game Sales vs Cleaned Happiness Score and GDP\")\n\nax.set_xlabel('Happiness Score')\nax.set_ylabel('Game Sales (in Millions)')\n\nfor pointx, pointy, pointz in zip(x,y,z):\n    label = sumSales.Country[sumSales['Sales'] == pointy].to_string(index=False)\n    ax.scatter3D(pointx, pointy, pointz, c = regionCmap[label], label = label)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GDP vs Happiness vs Game Sales\nx = cleaned_regions['Score']\ny = recent_sumSales['Sales']\nz = cleaned_regions['GDP']\n\nfig = plt.figure()\nplt.style.use('ggplot')\nfig, ax = plt.subplots()\nfig.set_size_inches(7,5)\nax = plt.axes(projection='3d')\n\nplt.title(\"Recent Game Sales vs Cleaned Happiness Score and GDP\")\nax.set_xlabel('Happiness Score')\nax.set_ylabel('Game Sales (in Millions)')\n\nfor pointx, pointy, pointz in zip(x,y,z):\n    label = recent_sumSales.Country[recent_sumSales['Sales'] == pointy].to_string(index=False)\n    ax.scatter3D(pointx, pointy, pointz, c = regionCmap[label], label = label)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Modelling on VGSales + Happiness Report Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# try modelling\n\n# create dataframes used for predicting\n# create deep copy of cleaned_regions with addition of all sales.\ncleaned_sumSales = cleaned_regions.copy(deep=True)\ncleaned_sumSales['VGSales'] = sumSales['Sales']\n# cleaned_sumSales\n\n# create deep copy of cleaned_regions with addition of recent sales.\ncleaned_RsumSales = cleaned_regions.copy(deep=True)\ncleaned_RsumSales['VGSales'] = recent_sumSales['Sales']\n# cleaned_RsumSales","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = cleaned_RsumSales.Score\n\nhappiness_features = ['GDP', 'Family', 'LifeExpectancy', 'Corruption',\n                      'Generosity', 'DystopiaResidual', 'CILower', 'CIUpper', 'VGSales']\nX = cleaned_RsumSales[happiness_features]\ntest_X = cleaned_sumSales[happiness_features]\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"happiness_model = DecisionTreeRegressor(random_state = 1)\n# Fit model\nhappiness_model.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate mean absolute error:\nhappiness_prices = happiness_model.predict(X)\nMAE_before = mean_absolute_error(y, happiness_prices)\n\n# Split data into training and validation\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n\n# Define/Fit model\nhappiness_model = DecisionTreeRegressor()\nhappiness_model.fit(train_X, train_y)\n\n# Get predicted results\nvalidation_predictions = happiness_model.predict(val_X)\nMAE_after = mean_absolute_error(val_y, validation_predictions)\n\n# Compare MAE\nprint(\"MAE Before training: \", MAE_before)\nprint(\"MAE After training: \", MAE_after)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So because of small size of data, prediciting is not possible, either will overfit, or underfit."},{"metadata":{},"cell_type":"markdown","source":"Attempted combined plot using scatter plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = cleaned_regions['Score']\ny = recent_sumSales['Sales']\naltX = happiness_regions['Score']\naltY = sumSales['Sales']\n# plt.plot(x,y,'ro')\n# plt.bar(x,y, 0.1)\n\nplt.style.use('ggplot')\nfig, ax = plt.subplots()\nfig.set_size_inches(7,5)\n\nplt.title(\"Recent Game Sales vs All Time on 2016 Happiness Score\")\nplt.xlabel(\"Happiness Score\")\nplt.ylabel(\"Game Sales (in Millions)\")\nfor pointx, pointy in zip(x,y):\n    label = recent_sumSales.Country[recent_sumSales['Sales'] == pointy].to_string(index=False)\n    plt.scatter(pointx, pointy, c = regionCmap[label], label = label)\n\nfor pointx, pointy in zip(x,altY):\n    label = sumSales.Country[sumSales['Sales'] == pointy].to_string(index=False)\n    plt.scatter(pointx, pointy, c = regionCmap[label], label = label)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = cleaned_regions['Score']\ny = recent_sumSales['Sales']\naltX = happiness_regions['Score']\naltY = sumSales['Sales']\n\nplt.style.use('ggplot')\nfig, ax = plt.subplots()\nfig.set_size_inches(7,5)\n\nplt.title(\"Recent Game Sales vs All Time on Cleaned Happiness Score\")\nplt.xlabel(\"Happiness Score\")\nplt.ylabel(\"Game Sales (in Millions)\")\n# plt.annotate('Japan', (6,100))\nfor pointx, pointy in zip(x,y):\n    label = recent_sumSales.Country[recent_sumSales['Sales'] == pointy].to_string(index=False)\n    plt.scatter(pointx, pointy, c = regionCmap[label], label = label)\n\nfor pointx, pointy in zip(x,altY):\n    label = sumSales.Country[sumSales['Sales'] == pointy].to_string(index=False)\n    plt.scatter(pointx, pointy, c = regionCmap[label], label = label)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thank you for reading my notebook! Many thank to you, and also my groupmates for helping with the project."},{"metadata":{},"cell_type":"markdown","source":"P.S. this is my first Public Kaggle Notebook, so apalogies for any mistakes :) Also, note that for the work done on the Happiness Report, it was done by the other group members (although I also contributed for it as well), the one I used in the notebook is a simplified version of it. Whereas the video game sales were all done by myself."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}