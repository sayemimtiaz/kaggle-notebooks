{"cells":[{"metadata":{},"cell_type":"markdown","source":"<div align=\"right\"> <span style=\"font-size:0.8em\"> Przemysław Bedełek, Mateusz Marciniewicz </span></div>\n\n# <div align=\"center\"> What do we know about COVID-19 risk factors? </div>  \n# <div align=\"center\"> <span style=\"color:gray; font-size:0.5em;\">(COVID-19 Open Research Dataset Challenge - task 2)</span></div>\n  \n    \n     \n### 1. **Interpretation**\nThis notebook is dedicated to solve the problem mentioned in the 2<sup>nd </sup> task.  \nIn our approach, we are searching the dataset in order to find those articles that refer to given questions, namely:  \n> * Data on potential **risks factors** - smoking, pulmonary diseases, co-infections etc.\n* **Transmission dynamics** of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors\n* **Severity of disease**, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups\n* **Susceptibility of populations**\n* Public health **mitigation measures** that could be effective for control\n\n### 2. **Approach**\nSome of readers may not be familiar with NLP methods (yet).  \nIn order to make analysis of our code easier we will introduce a brief step-by-step description of our solution: \n> 1. Data preprocessing:\n    * removal of stopwords\n    * removal of redundant information - links, references, two letter words, letter-digit sequences and external links\n    * tokenization\n    * stemming\n2. Articles evaluation using [TF-IDF](https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089): \n    * Text vectorizing \n    * calculating each article's TF-IDF value basing on initial keywords (arbitrarily chosen with hope for success :) )\n    * collecting articles with the best TF-IDF values\n    * searching for extra keywords based on collected articles\n    * removal of the \"least informative\" keywords (appearing in too big number of articles)\n    * re-collecting the best articles\n    \n### 3. **Pros and cons of our solution**\n> Advantages:\n    * Simplicity - the main idea behind the solution is to use basic nltk text preprocessing and tfidf which are both well documented\n    and easy to decipher.\n    * Potential for universal usage - it can be used for any set of inital keywords and it will still provide valuable articles.\n> Disadvantages:\n    * The algorithm is sensitive to the quality of the provided keywords.\n    * The text preprocessing is quite harsh because limiting the number of tokens in each article increases the computation speed and removes noise. However this may result in some loss of information in certain articles.\n    * There is no duplicate removal in the final articles.\n\n\n### 3. **Result**\n\nThe final result of our computations will be:\n> * for each problem above, a set of top *n* best matching articles\n* word clouds of the most frequent occuring words in each topic\n* plots evaluating how accurate our approach was\n\n\n\n\n\n\n\n# <div align=\"center\"> Let's begin! </div>\n\n# 1. Setup"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\nimport json\nimport nltk\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport string\nimport os\nfrom datetime import datetime\n\nstart = datetime.now()\nprint(f'Execution started at: {start.strftime(\"%m/%d/%Y, %H:%M:%S\")}')\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" * Create an auxiliary class that will store article's file path and the main text"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Article:\n    def __init__(self, file_path):\n        content = json.load(open(file_path))\n        self.paper_id = file_path\n        self.body_text = ''   \n        for input in content['body_text']:\n            self.body_text += input['text'] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Prepare paths to read files"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"root_path = '/kaggle/input/CORD-19-research-challenge'\nsub_dirs = ['/biorxiv_medrxiv/biorxiv_medrxiv/pdf_json',\n            '/comm_use_subset/comm_use_subset/pdf_json',\n            '/comm_use_subset/comm_use_subset/pmc_json',\n            '/custom_license/custom_license/pdf_json',\n            '/custom_license/custom_license/pmc_json',\n            '/noncomm_use_subset/noncomm_use_subset/pdf_json',\n            '/noncomm_use_subset/noncomm_use_subset/pmc_json'\n           ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Create an absolute file path for each article\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_paths = []\n\nfor sub_dir in sub_dirs:\n    all_paths.append(glob.glob(f'{root_path}{sub_dir}/*.json'))\n\n# merging sublists into a single list\nall_paths = [item for sublist in all_paths for item in sublist]\nlen(all_paths)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Load articles. Only 10000 articles will be loaded in order to accelerate the computation.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading articles\narticles_number = 10000      # to speed up computing, we will work on a smaller number than 50k. However, feel free to type a bigger one. \narticles = []\nfor index in range(articles_number):\n    if index % (articles_number//5) == 0:\n        print(f'{index/(articles_number)*100}% of files processed: {index}')\n    articles.append(Article(all_paths[index]))\nprint('Files loading finished')    \n\narticles[3].body_text\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.Text preprocessing"},{"metadata":{},"cell_type":"markdown","source":"* Removal of digits and punctuation characters\n* Tokenization\n* Strings with length under 3 characters are omitted as they don't carry much meaning "},{"metadata":{"trusted":true},"cell_type":"code","source":"# In the first step of data preprocessing the punctation and numerical characters are removed.\n# Besides that words under 3 letters are omitted as they don't carry any significant meaning.\nfor i in range(articles_number):\n    articles[i].body_text = articles[i].body_text.replace('-',' ')\n    articles[i].body_text = articles[i].body_text.translate(str.maketrans('','',string.punctuation + string.digits))\n    articles[i].body_text = [w.lower() for w in articles[i].body_text.split() if len(w)>2 and w.isalpha()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Noise words list consists of words that refer to the copyrights and usage licences. They don't carry any information as they are present in almost all articles.\n* Removal of stopwords, noise words and http links"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords \nnoise_words = ['medrxiv','biorxiv','covid','sars','preprint','authorfunder','license','available','copyright','peer','granted','perpetuityis','display','coronavirus','doi','also']\n\nfor index in range(articles_number):    \n    articles[index].body_text = [word for word in articles[index].body_text if word not in stopwords.words('english') + noise_words and word[:4] != 'http']  \n    if index % (articles_number//5) == 0:\n        print(f'{index/(articles_number)*100}% of files processsed: {index}')\nprint('Stopwords removal finished')\n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Stemming using the snowball stemmer"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stemming using the nltk SnowballStemmer\nfrom nltk.stem import SnowballStemmer\n\nstemmer = SnowballStemmer('english')\n\nfor i in range(len(articles)):    \n    articles[i].body_text = [stemmer.stem(word) for word in articles[i].body_text]\n\" \".join(articles[3].body_text)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Articles evaluation\n\n#### Creating a sparse matrix containing the tf-idf values for all articles. The articles are already tokenized and therefore the tokenizer in the TfidfVectorizer is swapped with a dummy function. We use CSR and CSC sparse matrices to minimize the cost of iterating over key words(columns) and articles(rows) in the Tf-idf sparse matrix.****\n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tf-idf function\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.sparse import coo_matrix\nfrom scipy.sparse import csr_matrix\nimport matplotlib.pyplot as plt\n\n# The text is already tokenized so the default tf-idf tokenizer is not needed here.\n# It will be swapped with the dummy function below\n\ndef dummy_fun(gunwo):\n    return gunwo\n\ntfidf = TfidfVectorizer(analyzer='word',\n                        tokenizer=dummy_fun,\n                        preprocessor=dummy_fun,\n                        token_pattern=None)\n\ntfidf_matrix = coo_matrix(tfidf.fit_transform([article.body_text for article in articles]))\ntfidf_csr = tfidf_matrix.tocsr()\ntfidf_csc = tfidf_matrix.tocsc()\n\nprint('Matrix size: (articles, unique_tokens) = ' + str(tfidf_matrix.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Declaring initial keywords that will identify articles from different topics."},{"metadata":{"trusted":true},"cell_type":"code","source":"# In the second task there are 5 major fields in which we want to harvest information \ntopics = ['Risk factors', 'Transmission dynamics', 'Severity of disease', 'Susceptibility of population', 'Mitigation measures']\n\n# Listed below are the arbitrarily chosen keywords for each topic\ninit_keywords = {\n 'Risk factors':['risk', 'factors', 'smoke', 'tobacco', 'cigarette', 'pneumonic', 'pulmonary', 'coexisting', 'coinfections', 'comorbidities', 'preexisting', 'chronic', 'neonates', 'mother', 'child', 'pregnancy', 'cancer', 'addiction', 'rich', 'poor', 'background', 'welfare', 'prosperity', 'immune'],\n 'Transmission dynamics': ['reproductive', 'number', 'incubation', 'period', 'serial', 'interval', 'transmission', 'spread', 'environment', 'circumstances', 'respiratory', 'droplets'],\n 'Severity of disease': ['fatality', 'risk', 'severe', 'hospitalize', 'mortality', 'death', 'rate', 'serious', 'mild'],\n 'Susceptibility of population': ['susceptibility', 'receptivity', 'sensitivity', 'age', 'old', 'young', 'ill', 'cold'],\n 'Mitigation measures': ['mitigate', 'measures', 'action', 'public', 'health', 'healthcare', 'reaction', 'counteraction', 'flatten', 'capacity', 'mask', 'gloves', 'soap', 'lockdown', 'wash', 'clean', 'sterile', 'prevent', 'slow', 'fast', 'block']}\n\nkeywords = init_keywords.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stemming the keywords so they match the stemmed tokens from the tfidf vect\n\nfor topic in topics:    \n    keywords[topic] = [stemmer.stem(word) for word in keywords[topic]]\n\n# Remove those initial keywords that don't appear in articles\nfor topic in topics:    \n    keywords[topic] = [word for word in keywords[topic] if word in tfidf.get_feature_names()]\n    \n# Getting indices of our keywords in the tfidf_array\nkeywords_indices = {}\n\nfor topic in topics:    \n    keywords_indices[topic] = [tfidf.get_feature_names().index(word) for word in keywords[topic]]\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Create auxiliary functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_tfidf_value(article,keyword):\n    start_index = tfidf_csr.indptr[article]\n    end_index = tfidf_csr.indptr[article+1]\n    for i in tfidf_csr.indices[start_index:end_index]:\n        if tfidf_csr.indices[i] == keyword:\n            return tfidf_csr.data[i]\n    return 0.0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_article(article, keywords):\n    start_index = tfidf_csr.indptr[article] \n    end_index = tfidf_csr.indptr[article+1] \n    article_value = 0\n    matching_indices = [i for i in range(start_index,end_index) if tfidf_csr.indices[i] in keywords]\n    for i in matching_indices:\n        article_value += tfidf_csr.data[i]\n    return article_value    \n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_keyword(articles, keyword):\n    articles_indices = [article[0] for article in articles]\n    start_index = tfidf_csc.indptr[keyword]\n    end_index = tfidf_csc.indptr[keyword+1]\n    keyword_value = 0\n    matching_indices = [i for i in range(start_index,end_index) if tfidf_csc.indices[i] in articles_indices]\n    for i in matching_indices:\n        keyword_value += tfidf_csc.data[i]\n    return keyword_value  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Insert adds an item into a sorted list\ndef insert(list, doc, doc_value): \n    global top_number\n    if len(list) < top_number: \n            list.append([doc,doc_value])\n            return list\n    for index in range(top_number):\n        if list[index][1] <= doc_value:\n            list = list[:index] + [[doc,doc_value]] + list[index:]\n            return list[:top_number]\n    return list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_best_articles(keywords_indices):\n    best_articles = {topic: [] for topic in topics}\n    for topic in topics:\n        for article in range(len(articles)):        \n            article_value = evaluate_article(article, keywords_indices[topic])\n            best_articles[topic] = insert(best_articles[topic],article,article_value)\n    return best_articles","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_col(col):\n    start_index = tfidf_csc.indptr[col]\n    end_index = tfidf_csc.indptr[col+1]\n    value = 0\n    for i in tfidf_csc.data[start_index:end_index]:\n        value += i\n        \n    return value","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Collect articles with the best TF-IDF values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Best articles contains the most informative articles in each topic\ntop_number = 20 # number of top articles that we would like to assign to each topic\nbest_articles = get_best_articles(keywords_indices)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Search for extra keywords based on collected articles"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extra kewords holds the most important keywords in each topic\nextra_keywords = {topic: [] for topic in topics}\nfor topic in topics:\n    for keyword in range(len(tfidf.get_feature_names())):\n        keyword_value = evaluate_keyword(best_articles[topic],keyword)\n        extra_keywords[topic] = insert(extra_keywords[topic],keyword,keyword_value)\n\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top20_keywords = {}\nfor topic in topics:\n    top20_keywords[topic] =  [tfidf.get_feature_names()[extra_keywords[topic][doc][0]] for doc in range(len(extra_keywords[topic]))]\n    print(f'{topic}: {top20_keywords[topic][1:-1]}')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Amongst the new keywords there are some words like infect, cov or case. Those words appear in all topics, hence they don't provide us with valuable information. Therefore they need to be removed."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some of the extra_keywords don't hold much information as they appear in articles from various fields i.e: case, covid\n\nto_remove = {topic: [] for topic in topics}\n\n# Collect these keywords\nfor topic in topics:\n    for keyword in extra_keywords[topic]:\n        keyword_value = evaluate_col(keyword[0])/(0.3 * len(articles))\n        if(keyword_value > keyword[1]/top_number):\n            to_remove[topic].append(keyword)\n    print(f'{topic}: { [tfidf.get_feature_names()[to_remove[topic][doc][0]] for doc in range(len(to_remove[topic]))] }')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for topic in topics: \n    extra_keywords[topic] = [keyword for keyword in extra_keywords[topic] if keyword not in to_remove[topic]]\n    print(f'{topic}: { [tfidf.get_feature_names()[extra_keywords[topic][doc][0]] for doc in range(len(extra_keywords[topic]))] }')    \n\nextra_keywords_indices = {}\nfor topic in topics:\n    extra_keywords_indices[topic] = [word[0] for word in extra_keywords[topic]]\n    \nfor topic in topics:\n    keywords_indices[topic] += extra_keywords_indices[topic]\n    keywords_indices[topic] = list(set(keywords_indices[topic]))   \n\nnew_articles = get_best_articles(keywords_indices)\n\nkeywords = {}\nfor topic in topics:\n    keywords[topic] = {}\n    for i in keywords_indices[topic]:\n        keyword_value = 0\n        for article in new_articles[topic]:\n            keyword_value += get_tfidf_value(article[0],i)\n        if keyword_value != 0.0:\n            keywords[topic][tfidf.get_feature_names()[i]] = keyword_value\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Normalize TF-IDF values"},{"metadata":{"trusted":true},"cell_type":"code","source":"for topic in topics: \n    best_articles[topic] = [ [item[0], item[1]/len(init_keywords[topic])] for item in best_articles[topic]]\n    new_articles[topic] = [ [item[0], item[1]/len(keywords_indices[topic])] for item in new_articles[topic]]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Distribution of the best articles in each topic. The y-axis represents the average tf-idf value for the article per keyword. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\ndef create_boxplot(articles):\n    data = {}\n    for topic in topics:\n        data[topic] = [item[1] for item in articles[topic]]\n    sns.set(palette='Blues_d', style=\"whitegrid\")\n    df = pd.DataFrame.from_dict(data)\n    boxplot = df.boxplot(figsize=(16,8))\n    \ncreate_boxplot(best_articles)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Before the addition of extra keywords."},{"metadata":{"trusted":true},"cell_type":"code","source":"for topic in topics: \n    print(len(init_keywords[topic]))\n    print(len(keywords_indices[topic]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"create_boxplot(new_articles)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Create a boxplot for a new set of best articles in order to compare those two sets "},{"metadata":{},"cell_type":"markdown","source":"# 4. Result\n* Create a wordcloud of keywords for each topic"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nword_clouds = {}\nfor topic in topics:\n    print(topic)\n    word_clouds[topic] = WordCloud(background_color='white').generate_from_frequencies(keywords[topic])\n    plt.figure(figsize=(16,8))\n    plt.imshow(word_clouds[topic])\n    plt.axis('off')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def present_article(file_path):\n    content = json.load(open(file_path))\n    title = content['metadata']['title']\n    body_text = ''\n\n    print(f'\\nTitle: {title}\\n')\n      \n    for input in content['body_text']:\n        body_text += input['text']\n    print(f'Text: {body_text[:300]}')\n   \n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for topic in topics:\n    print(f'{topic}: \\n')\n    \n    for i in range(3):\n        present_article(articles[new_articles[topic][i][0]].paper_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"end = datetime.now()\ntotal = end - start\nprint(f'Execution finished at: {end.strftime(\"%m/%d/%Y, %H:%M:%S\")} \\nDuration: {total}')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}