{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Bank marketing campaigns dataset analysis"},{"metadata":{},"cell_type":"markdown","source":"### Abstract\nThis is dataset that describe Portugal bank marketing campaigns results. \nConducted campaigns were based mostly on direct phone calls, offering  bank's clients to place a term deposit. \nIf after all marking afforts client had agreed to place deposit - target variable marked 'yes', otherwise 'no'.\n\nSourse of the data\nhttps://archive.ics.uci.edu/ml/datasets/bank+marketing\n\nDataset description\nhttps://www.kaggle.com/volodymyrgavrysh/bank-marketing-campaigns-data-set-description\n\n**Citation Request:**\n> This dataset is public available for research. The details are described in S. Moro, P. Cortez and P. Rita. \"A Data-Driven Approach to Predict the Success of Bank Telemarketing.\" Decision Support Systems, Elsevier, 62:22-31, June 2014 <\n\n### Task\n\n* predicting the future results of marketing companies based on available statistics and, accordingly, formulating recommendations for such companies in the future.\n* building a profile of a consumer of banking services (deposits).\n\n### Approach\n\nThe following steps will be performed to complete the task:\n1. Loading data and holding a short Expanatory Data Analysis (EDA).\n2. Formulating hypotheses regarding individual factors (features) for conducting correct data clearining and data preparation for modeling.\n3. The choice of metrics result.\n4. Building a pipline for Cross Validation and Grid Search procedures (search for optimal parameters of the model)\n5. The choice of the most effective model **, build learninig curve rate\n6. Conclusions and recomendations.\n\n > ****** we intentionally use most basic machine learning models to increase the level of interprebility of the solution\n"},{"metadata":{},"cell_type":"markdown","source":"### Feature description\n**Bank client data:**\n* 1 - age (numeric)\n* 2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n* 3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n* 4 - education (categorical: basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n* 5 - default: has credit in default? (categorical: 'no','yes','unknown')\n* 6 - housing: has housing loan? (categorical: 'no','yes','unknown')\n* 7 - loan: has personal loan? (categorical: 'no','yes','unknown')\n\n***Related with the last contact of the current campaign:***\n* 8 - contact: contact communication type (categorical: 'cellular','telephone')\n* 9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n* 10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n* 11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n\n***other attributes:***\n* 12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n* 13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n* 14 - previous: number of contacts performed before this campaign and for this client (numeric)\n* 15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n\n***social and economic context attributes***\n* 16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)\n* 17 - cons.price.idx: consumer price index - monthly indicator (numeric)\n* 18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)\n* 19 - euribor3m: euribor 3 month rate - daily indicator (numeric)\n* 20 - nr.employed: number of employees - quarterly indicator (numeric)\n\n***Output variable (desired target):***\n* 21 - y - has the client subscribed a term deposit? (binary: 'yes','no')"},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"# Import libraries \nimport pandas as pd\nimport numpy as np\nimport time\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import SGDClassifier\n\nimport category_encoders as ce\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objs as go\nfrom tqdm import tqdm\n\n''' Citing libraries \nscikit-learn\nauthors={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\n         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\n         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\n         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.}\n\n category Encoders\n author = Will McGinnis\n \n matplotlib\n author = Hunter, J. D.\n \n seaborn\n author = Michael Waskom\n \n \n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Loading data and holding a short Expanatory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read data\ndata = pd.read_csv('../input/bank-additional-full.csv', sep=';')\ndisplay(data.head(3))\ndisplay('There is {} observations with {} features'.format(data.shape[0], data.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Explore categorical features (EDA)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build a function to show categorical values disribution\ndef plot_bar(column):\n    # temp df \n    temp_1 = pd.DataFrame()\n    # count categorical values\n    temp_1['No_deposit'] = data[data['y'] == 'no'][column].value_counts()\n    temp_1['Yes_deposit'] = data[data['y'] == 'yes'][column].value_counts()\n    temp_1.plot(kind='bar')\n    plt.xlabel(f'{column}')\n    plt.ylabel('Number of clients')\n    plt.title('Distribution of {} and deposit'.format(column))\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_bar('job'), plot_bar('marital'), plot_bar('education'), plot_bar('contact'), plot_bar('loan'), plot_bar('housing')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Primary analysis of several categorical features reveals:**\n1. Administrative staff and technical specialists opened the deposit most of all. In relative terms, a high proportion of pensioners and students might be mentioned as well.\n2. Although in absolute terms married consumers more often agreed to the service, in relative terms the single was responded better.\n3. Best communication channel is secullar.\n4. The difference is evident between consumers who already use the services of banks and received a loan.\n5. Home ownership does not greatly affect marketing company performance."},{"metadata":{},"cell_type":"markdown","source":"### Explore numerical features (EDA)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert target variable into numeric\ndata.y = data.y.map({'no':0, 'yes':1}).astype('uint8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build correlation matrix\ncorr = data.corr()\ncorr.style.background_gradient(cmap='PuBu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** From correlation matrix we observe next:** \n* most correlated with target feature is call duration. So we need to transform it to reduce the influence\n* higly correlated features (employment rate, consumer confidence index, consumer price index) may describe clients state from different social-economic angles. Their variance might support model capacity for generalization."},{"metadata":{},"cell_type":"markdown","source":"### 2. Formulating hypotheses regarding individual factors (features) for conducting correct data clearining and data preparation for modeling\n\n** Data cleaning stategy **\n\nSince categorical variables dominate in dataset and the number of weakly correlated numeric variables is not more than 4, we need to transform categorical variables to increase the model's ability to generalize data. (we can not drop them)\n\nParticular attention should be paid to the Duration Feature and categories that can be treated as binary. It suggests using binning and simple transformation accordingly (0 and 1)\n\nFor categories of more than 3 types of possible option (marital and education) it is proposed to use the encode targeting - it will allow correctly relate the values to the target variable and use indicated categories in numerical form.\n\nIn some cases, rescaling is proposed to normalize the data.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing values with binary ()\ndata.contact = data.contact.map({'cellular': 1, 'telephone': 0}).astype('uint8') \ndata.loan = data.loan.map({'yes': 1, 'unknown': 0, 'no' : 0}).astype('uint8')\ndata.housing = data.housing.map({'yes': 1, 'unknown': 0, 'no' : 0}).astype('uint8')\ndata.default = data.default.map({'no': 1, 'unknown': 0, 'yes': 0}).astype('uint8')\ndata.pdays = data.pdays.replace(999, 0) # replace with 0 if not contact \ndata.previous = data.previous.apply(lambda x: 1 if x > 0 else 0).astype('uint8') # binary has contact or not\n\n# binary if were was an outcome of marketing campane\ndata.poutcome = data.poutcome.map({'nonexistent':0, 'failure':0, 'success':1}).astype('uint8') \n\n# change the range of Var Rate\ndata['emp.var.rate'] = data['emp.var.rate'].apply(lambda x: x*-0.0001 if x > 0 else x*1)\ndata['emp.var.rate'] = data['emp.var.rate'] * -1\ndata['emp.var.rate'] = data['emp.var.rate'].apply(lambda x: -np.log(x) if x < 1 else np.log(x)).astype('uint8')\n\n# Multiply consumer index \ndata['cons.price.idx'] = (data['cons.price.idx'] * 10).astype('uint8')\n\n# change the sign (we want all be positive values)\ndata['cons.conf.idx'] = data['cons.conf.idx'] * -1\n\n# re-scale variables\ndata['nr.employed'] = np.log2(data['nr.employed']).astype('uint8')\ndata['cons.price.idx'] = np.log2(data['cons.price.idx']).astype('uint8')\ndata['cons.conf.idx'] = np.log2(data['cons.conf.idx']).astype('uint8')\ndata.age = np.log(data.age)\n\n# less space\ndata.euribor3m = data.euribor3m.astype('uint8')\ndata.campaign = data.campaign.astype('uint8')\ndata.pdays = data.pdays.astype('uint8')\n\n# fucntion to One Hot Encoding\ndef encode(data, col):\n    return pd.concat([data, pd.get_dummies(col, prefix=col.name)], axis=1)\n\n# One Hot encoding of 3 variable \ndata = encode(data, data.job)\ndata = encode(data, data.month)\ndata = encode(data, data.day_of_week)\n\n# Drop tranfromed features\ndata.drop(['job', 'month', 'day_of_week'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Drop the dublicates'''\ndata.drop_duplicates(inplace=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Convert Duration Call into 5 category'''\ndef duration(data):\n    data.loc[data['duration'] <= 102, 'duration'] = 1\n    data.loc[(data['duration'] > 102) & (data['duration'] <= 180)  , 'duration'] = 2\n    data.loc[(data['duration'] > 180) & (data['duration'] <= 319)  , 'duration'] = 3\n    data.loc[(data['duration'] > 319) & (data['duration'] <= 645), 'duration'] = 4\n    data.loc[data['duration']  > 645, 'duration'] = 5\n    return data\nduration(data);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Target encoding for two categorical feature '''\n# save target variable before transformation\ny = data.y\n# Create target encoder object and transoform two value\ntarget_encode = ce.target_encoder.TargetEncoder(cols=['marital', 'education']).fit(data, y)\nnumeric_dataset = target_encode.transform(data)\n# drop target variable\nnumeric_dataset.drop('y', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Check numerical data set'''\ndisplay(numeric_dataset.head(3), numeric_dataset.shape, y.shape)\ndisplay('We observe 41175 rows and 44 numerical features after transformation. Target variable shape is (41175, 0 ) as expected')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. The choice of metrics result\n\nIt is proposed to use ROC_AUC metrics for evaluating different models with additional monitoring of the accuracy metric dynamic.\n\nThis approach will allow us to explore models from different angles."},{"metadata":{},"cell_type":"markdown","source":"### 4. Building a pipline for Cross Validation and Grid Search procedures (search for optimal parameters of the model)"},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Split data on train and test'''\n# set global random state\nrandom_state = 11\n# split data\nX_train, X_test, y_train, y_test = train_test_split(numeric_dataset, y, test_size=0.2, random_state=random_state)\n# collect excess data\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display('check the shape of splitted train and test sets', X_train.shape, y_train.shape, X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Build pipline of classifiers'''\n# set all CPU\nn_jobs = -1\n# LogisticRegression\npipe_lr = Pipeline([('lr', LogisticRegression(random_state=random_state, n_jobs=n_jobs, max_iter=500))])\n# RandomForestClassifier\npipe_rf = Pipeline([('rf', RandomForestClassifier(random_state=random_state, oob_score=True, n_jobs=n_jobs))])\n# KNeighborsClassifier\npipe_knn = Pipeline([('knn', KNeighborsClassifier(n_jobs=n_jobs))])\n# DecisionTreeClassifier\npipe_dt = Pipeline([('dt', DecisionTreeClassifier(random_state=random_state, max_features='auto'))])\n# BaggingClassifier\n# note we use SGDClassifier as classier inside BaggingClassifier\npipe_bag = Pipeline([('bag',BaggingClassifier(base_estimator=SGDClassifier(random_state=random_state, n_jobs=n_jobs, max_iter=1500),\\\n                                              random_state=random_state,oob_score=True,n_jobs=n_jobs))])\n# SGDClassifier\npipe_sgd = Pipeline([('sgd', SGDClassifier(random_state=random_state, n_jobs=n_jobs, max_iter=1500))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Set parameters for Grid Search '''\n# set number \ncv = StratifiedKFold(shuffle=True, n_splits=5, random_state=random_state)\n# set for LogisticRegression\ngrid_params_lr = [{\n                'lr__penalty': ['l2'],\n                'lr__C': [0.3, 0.6, 0.7],\n                'lr__solver': ['sag']\n                }]\n# set for RandomForestClassifier\ngrid_params_rf = [{\n                'rf__criterion': ['entropy'],\n                'rf__min_samples_leaf': [80, 100],\n                'rf__max_depth': [25, 27],\n                'rf__min_samples_split': [3, 5],\n                'rf__n_estimators' : [60, 70]\n                }]\n# set for KNeighborsClassifier\ngrid_params_knn = [{'knn__n_neighbors': [16,17,18]}]\n\n# set for DecisionTreeClassifier\ngrid_params_dt = [{\n                'dt__max_depth': [8, 10],\n                'dt__min_samples_leaf': [1, 3, 5, 7]\n                  }]\n# set for BaggingClassifier\ngrid_params_bag = [{'bag__n_estimators': [10, 15, 20]}]\n\n# set for SGDClassifier\ngrid_params_sgd = [{\n                    'sgd__loss': ['log', 'huber'],\n                    'sgd__learning_rate': ['adaptive'],\n                    'sgd__eta0': [0.001, 0.01, 0.1],\n                    'sgd__penalty': ['l1', 'l2', 'elasticnet'], \n                    'sgd__alpha':[0.1, 1, 5, 10]\n                    }]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Grid search objects'''\n# for LogisticRegression\ngs_lr = GridSearchCV(pipe_lr, param_grid=grid_params_lr,\n                     scoring='accuracy', cv=cv) \n# for RandomForestClassifier\ngs_rf = GridSearchCV(pipe_rf, param_grid=grid_params_rf,\n                     scoring='accuracy', cv=cv)\n# for KNeighborsClassifier\ngs_knn = GridSearchCV(pipe_knn, param_grid=grid_params_knn,\n                     scoring='accuracy', cv=cv)\n# for DecisionTreeClassifier\ngs_dt = GridSearchCV(pipe_dt, param_grid=grid_params_dt,\n                     scoring='accuracy', cv=cv)\n# for BaggingClassifier\ngs_bag = GridSearchCV(pipe_bag, param_grid=grid_params_bag,\n                     scoring='accuracy', cv=cv)\n# for SGDClassifier\ngs_sgd = GridSearchCV(pipe_sgd, param_grid=grid_params_sgd,\n                     scoring='accuracy', cv=cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# models that we iterate over\nlook_for = [gs_lr, gs_rf, gs_knn, gs_dt, gs_bag, gs_sgd]\n# dict for later use \nmodel_dict = {0:'Logistic_reg', 1:'RandomForest', 2:'Knn', 3:'DesionTree', 4:'Bagging with SGDClassifier', 5:'SGD Class'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Function to iterate over models and obtain results'''\n# set empty dicts and list\nresult_acc = {}\nresult_auc = {}\nmodels = []\n\nfor index, model in enumerate(look_for):\n        start = time.time()\n        print()\n        print('+++++++ Start New Model ++++++++++++++++++++++')\n        print('Estimator is {}'.format(model_dict[index]))\n        model.fit(X_train, y_train)\n        print('---------------------------------------------')\n        print('best params {}'.format(model.best_params_))\n        print('best score is {}'.format(model.best_score_))\n        auc = roc_auc_score(y_test, model.predict_proba(X_test)[:,1])\n        print('---------------------------------------------')\n        print('ROC_AUC is {} and accuracy rate is {}'.format(auc, model.score(X_test, y_test)))\n        end = time.time()\n        print('It lasted for {} sec'.format(round(end - start, 3)))\n        print('++++++++ End Model +++++++++++++++++++++++++++')\n        print()\n        print()\n        models.append(model.best_estimator_)\n        result_acc[index] = model.best_score_\n        result_auc[index] = auc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. The choice of the most effective model, build learninig curve rate\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(model_dict.values(), result_acc.values(), c='r')\nplt.plot(model_dict.values(), result_auc.values(), c='b')\nplt.xlabel('Models')\nplt.xticks(rotation=45)\nplt.ylabel('Accouracy and ROC_AUC')\nplt.title('Result of Grid Search')\nplt.legend(['Accuracy', 'ROC_AUC'])\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\" Model performance during Grid Search \"\"\"\npd.DataFrame(list(zip(model_dict.values(), result_acc.values(), result_auc.values())), \\\n                  columns=['Model', 'Accuracy_rate','Roc_auc_rate'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Best model \nOur best performed model with ROC_AUC (0.9269) metric is ** Random forest **. \nThis classifier could achive accuracy rate 0.903 that is average accuracy among all classifiers (0.904)."},{"metadata":{},"cell_type":"markdown","source":"We can build graph to check RandomForestClassifier performatce with OOB score to be sure that critical hyperparametr was correctly selected during Grid Search. \nAs you may see it almost the same - 80 estimators with best ROC_AUC score and 90 estimators with maximun of OOB score\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def graph(model, X_train, y_train):\n    obb = []\n    est = list(range(5, 200, 5))\n    for i in tqdm(est):\n        random_forest = model(n_estimators=i, criterion='entropy', random_state=11, oob_score=True, n_jobs=-1, \\\n                           max_depth=25, min_samples_leaf=80, min_samples_split=3,)\n        random_forest.fit(X_train, y_train)\n        obb.append(random_forest.oob_score_)\n    display('max oob {} and number of estimators {}'.format(max(obb), est[np.argmax(obb)]))\n    plt.plot(est, obb)\n    plt.title('model')\n    plt.xlabel('number of estimators')\n    plt.ylabel('oob score')\n    plt.show();\n    \ngraph(RandomForestClassifier, X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let see the ROC_AUC graph"},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Build graph for ROC_AUC '''\n\nfpr, tpr, threshold = roc_curve(y_test, models[1].predict_proba(X_test)[:,1])\n                                        \ntrace0 = go.Scatter(\n    x=fpr,\n    y=tpr,\n    text=threshold,\n    fill='tozeroy',\n    name='ROC Curve')\n\ntrace1 = go.Scatter(\n    x=[0,1],\n    y=[0,1],\n    line={'color': 'red', 'width': 1, 'dash': 'dash'},\n    name='Baseline')\n\ndata = [trace0, trace1]\n\nlayout = go.Layout(\n    title='ROC Curve',\n    xaxis={'title': 'False Positive Rate'},\n    yaxis={'title': 'True Positive Rate'})\n\nfig = go.Figure(data, layout)\nfig.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Curve is well distributed with tendency to False Positive Rate.  The roc auc values of the best model of 0.9269 is quite high level to make later assumptions about the data."},{"metadata":{},"cell_type":"markdown","source":"#### We can build feature importance of RandomForestClassifier with best ROC_AUC score"},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Build bar plot of feature importance of the best model '''\n\ndef build_feature_importance(model, X_train, y_train):\n    \n    models = RandomForestClassifier(criterion='entropy', random_state=11, oob_score=True, n_jobs=-1, \\\n                           max_depth=25, min_samples_leaf=80, min_samples_split=3, n_estimators=70)\n    models.fit(X_train, y_train)\n    data = pd.DataFrame(models.feature_importances_, X_train.columns, columns=[\"feature\"])\n    data = data.sort_values(by='feature', ascending=False).reset_index()\n    plt.figure(figsize=[6,6])\n    sns.barplot(x='index', y='feature', data=data[:10], palette=\"Blues_d\")\n    plt.title('Feature inportance of Random Forest after Grid Search')\n    plt.xticks(rotation=45)\n    plt.show();\n    \nbuild_feature_importance(RandomForestClassifier, X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6. Conclusions and recomendations"},{"metadata":{},"cell_type":"markdown","source":"#### What general recommendations can be offered for a successful marketing company in the future?"},{"metadata":{},"cell_type":"markdown","source":"This analysis can be carried out at the level of individual bank branches as does not require much resources and special knowledge (the model itself can be launched automatically with a certain periodicity). \n\nPotentially similar micro-targeting will increase the overall effectiveness of the entire marketing company."},{"metadata":{},"cell_type":"markdown","source":"1. Take into account the time of the company (May is the most effective)\n2. Increase the time of contact with customers (perhaps in a different way formulating the goal of the company). It is possible to use other means of communication.\n3. Focus on specific categories. The model shows that students and senior citizens respond better to proposal.\n4. It is imperative to form target groups based on socio-economic categories. Age, income level (not always high), profession can accurately determine the marketing profile of a potential client.\n\nGiven these factors, it is recommended to **concentrate on those consumer groups** that are potentially more promising.\n\nThe concentration of the bank’s efforts will effectively distribute the company’s resources to the main factor - the bank’s contact time with the client - it affects most of all on conversion.\n\nThe continuation of such a study may be the **formation of a clear customer profile** - by age, gender, income and other factors, as well as the adaptation of the product itself (deposit) for a specific category of consumer.\n"},{"metadata":{},"cell_type":"markdown","source":"Thank you for your time! \n\nFeel free to conctact https://www.linkedin.com/in/volodymyrgavrish/"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}