{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Importing all the tools we need\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score,f1_score\nfrom sklearn.metrics import plot_roc_curve\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Definition\nSee if you can find any other trends in heart data to predict certain cardiovascular events or find any clear indications of heart health.\n## 2. Data\nTaken from https://www.kaggle.com/ronitf/heart-disease-uci\n## 3. Features\n\nThis is where you'll get different information about each of the features in your data.\n\n**Create data dictionary**\n\n1. age - age in years\n2. sex - (1 = male; 0 = female)\n3. cp - chest pain type\n    * 0: Typical angina: chest pain related decrease blood supply to the heart\n    * 1: Atypical angina: chest pain not related to heart\n    * 2: Non-anginal pain: typically esophageal spasms (non heart related)\n    * 3: Asymptomatic: chest pain not showing signs of disease\n4. trestbps - resting blood pressure (in mm Hg on admission to the hospital) anything above 130-140 is typically cause for concern\n5. chol - serum cholestoral in mg/dl\n    * serum = LDL + HDL + .2 * triglycerides\n    * above 200 is cause for concern\n6. fbs - (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n    * '>126' mg/dL signals diabetes\n7. restecg - resting electrocardiographic results\n    * 0: Nothing to note\n    * 1: ST-T Wave abnormality\n        * can range from mild symptoms to severe problems\n        * signals non-normal heart beat\n    * 2: Possible or definite left ventricular hypertrophy\n        * Enlarged heart's main pumping chamber\n8. thalach - maximum heart rate achieved\n9. exang - exercise induced angina (1 = yes; 0 = no)\n10. oldpeak - ST depression induced by exercise relative to rest looks at stress of heart during excercise unhealthy heart will stress more\n11. slope - the slope of the peak exercise ST segment\n    * 0: Upsloping: better heart rate with excercise (uncommon)\n    * 1: Flatsloping: minimal change (typical healthy heart)\n    * 2: Downslopins: signs of unhealthy heart\n12. ca - number of major vessels (0-3) colored by flourosopy\n    * colored vessel means the doctor can see the blood passing through\n    * the more blood movement the better (no clots)\n13. thal - thalium stress result\n    * 1,3: normal\n    * 6: fixed defect: used to be defect but ok now\n    * 7: reversable defect: no proper blood movement when excercising\n14. target - have disease or not (1=yes, 0=no) (= the predicted attribute)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Read data from csv\ndf = pd.read_csv(\"../input/heart-disease-uci/heart.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get some rows\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check if everything is a number\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Are there any missing data?\ndf.isna().sum() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"target\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"target\"].value_counts().plot(kind=\"bar\", color=[\"salmon\",\"lightblue\"]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a plot of crosstab\npd.crosstab(df.target, df.sex).plot(kind=\"bar\",\n                                    figsize=(10,6),\n                                    color=[\"salmon\",\"lightblue\"])\nplt.title(\"Heart Disease Frequency for Sex\")\nplt.xlabel(\"0 = No Disease, 1 = Disease\")\nplt.legend([\"Female\",\"Male\"])\nplt.xticks(rotation=0);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create another figure\nplt.figure(figsize=(10, 6))\n\n# Scatter with positive example\nplt.scatter(df.age[df.target==1], df.thalach[df.target==1], color=\"salmon\")\n\n#Scatter with negative examples\nplt.scatter(df.age[df.target==0], df.thalach[df.target==0], color=\"lightblue\");\n\n# Add some helpful info\nplt.title(\"Heart Disease in function of Age and Max Heart Rate\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Max Hear Rate\")\nplt.legend([\"Disease\", \"No Disease\"]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the distribution of the age column with histogram\ndf.age.plot.hist();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a crosstab more visual\npd.crosstab(df.cp, df.target).plot(kind=\"bar\",\n                                  figsize=(10,6),\n                                  color=[\"salmon\",\"lightblue\"])\n\n#Add some communication\nplt.title(\"Heart Disease Frequency Per Chest Pain Type\")\nplt.xlabel(\"Chest Pain Type\")\nplt.ylabel(\"Amount\")\nplt.legend([\"Disease\",\"No Disease\"])\nplt.xticks(rotation=0);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a correlation matrix\ncorr_matrix = df.corr()\nfig,ax=plt.subplots(figsize=(15,10))\nax = sns.heatmap(corr_matrix,\n                annot=True,\n                linewidths=0.5,\n                fmt=\".2f\",\n                cmap=\"YlGnBu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split data into X and y\nX = df.drop(\"target\",axis=1)\ny = df[\"target\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split into train and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We're going to try 3 different machine learning models:\n1. Logistic regression\n2. K-Nearest Neighbours Classifier\n3. Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Put models in a dictionary\nmodels = {\"Logistic Regression\": LogisticRegression(),\n         \"KNN\": KNeighborsClassifier(),\n         \"Random Forest\": RandomForestClassifier()}\n\n# Create a function to fit and score models\n\ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n    \"\"\"\n    Fits and evaluates given machine learning models.\n    models: a dict of different Scikit-Learn machine learning models\n    X_train : training data (no labels)\n    X_test : testing data (no labels)\n    y_train : training labels\n    y_test : testing labels\n    \"\"\"\n    # Set random seed\n    np.random.seed(42)\n    \n    #Make a dictionary to keep model scores\n    model_scores={}\n    #Loop through models\n    for name, model in models.items():\n        #Fit the model to the data\n        model.fit(X_train,y_train)\n        #Evaluate the model and append its score to model_scores\n        model_scores[name] = model.score(X_test,y_test)\n    return model_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_scores = fit_and_score(models=models,\n                             X_train=X_train,\n                             X_test=X_test,\n                             y_train=y_train,\n                             y_test=y_test)\nmodel_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_compare = pd.DataFrame(model_scores, index=[\"accuracy\"])\nmodel_compare.T.plot.bar();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Results: KNN is staying behind. We will check how accuracy changes with a little tweaking. If it's not increasing, KNN will be droppped for further tuning."},{"metadata":{},"cell_type":"markdown","source":"# Tuning"},{"metadata":{},"cell_type":"markdown","source":"## Checking accuracy based on different parameters for KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_scores = []\ntest_scores = []\n\n# Create a list of different values for n_neighbors\nneighbors = range(1,40)\n\n#Setup KNN instance\nknn=KNeighborsClassifier()\n\n# Loop through different n_neighbors\nfor i in neighbors:\n    knn.set_params(n_neighbors=i)\n    \n    #Fit the algorithm\n    knn.fit(X_train,y_train)\n    \n    # Update the training scores list\n    train_scores.append(knn.score(X_train, y_train))\n    \n    #Update the test scores list\n    test_scores.append(knn.score(X_test, y_test ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(neighbors, train_scores, label = \"train score\")\nplt.plot(neighbors, test_scores, label = \"test scores\")\nplt.xticks(np.arange(1,21,1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.legend()\n\nprint(f\"Maximum KNN score on the test data: {max(test_scores)*100:.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Trying to get better parameters for LogisticRegression() and RandomForestClassfier() using RandomizedGridCV"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a hyperparameter grid for Logistic regression\nlog_reg_grid = {\"C\": np.logspace(-4,4,20),\n               \"solver\": [\"liblinear\"]}\n\n# Create hyperparameter grid for RandomForestClassfier\nrf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n          \"max_depth\": [None, 3, 5, 10],\n          \"min_samples_split\": np.arange(2, 20, 2),\n          \"min_samples_leaf\": np.arange(1, 20, 2)}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tune LogsticRegression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tune LogsticRegression\n\n# Setup random hyperparameter search for LogisticRegression\nrs_log_reg = RandomizedSearchCV(LogisticRegression(),\n                                param_distributions=log_reg_grid, \n                                cv=5,\n                               n_iter=20,\n                               verbose = True)\n\n# Fit random hyperparametr search model for LogisticRegression\nrs_log_reg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#best parameters are:\nrs_log_reg.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#best accuracy score for Logistic regression\nrs_log_reg.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tune RandomizedForestClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setup random hyperparameter search for RandomizedForestClassifier\nrs_rf = RandomizedSearchCV(RandomForestClassifier(),\n                           param_distributions=rf_grid,\n                           cv = 5,\n                           n_iter=20,\n                           verbose=True,\n                           n_jobs=-1)\n\n# Fit random Hyperparameter srach model for RandomForestClassifier()\nrs_rf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#best parameters are:\nrs_rf.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# best accuracy score for RandomForestClassifier\nrs_rf.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Results: Since our LogisticRegression model provides the best scores so far, we'll try and imporve them again using GfridSearchCV"},{"metadata":{},"cell_type":"markdown","source":"## Hyperparameter Tuning with GridSearchCV\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Different hyperparameters for our LogisticsRegression model\nlog_reg_grid = {\"C\": np.logspace(-4,4,30),\n               \"solver\": [\"liblinear\"]}\n\n#Setup grid hyperparameter search for LogisticRegression\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                         param_grid=log_reg_grid,\n                         cv=5,\n                         verbose=True)\n\n# Fit grid hyperparameter search model\ngs_log_reg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best parameters are:\ngs_log_reg.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate the grid search LogisticRegression model\ngs_log_reg.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluating our tuned machine learning classifier, beyond accuracy\n\nAfter acuring best parameters for LogisticRegression. We will evaluate following metrics:\n* ROC curve and AUC score\n* Confusion matrix\n* Classification report\n* Precision\n* Recall\n* F1-score"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions with tuned model\ny_preds = gs_log_reg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ROC curve and AUC metric"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot ROC curve and calculate AUC metric\nplot_roc_curve(gs_log_reg, X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(font_scale=1.5)\n\ndef plot_conf_mat(y_test, y_preds):\n    \"\"\"\n    Plots a nice looking confusion matrix using Seaborn's heatmap()\n    \"\"\"\n    fig, ax = plt.subplots (figsize=(3,3))\n    ax= sns.heatmap(confusion_matrix(y_test,y_preds),\n                   annot=True,\n                   cbar=False)\n    plt.ylabel(\"True label\")\n    plt.xlabel(\"Predicted label\")\n    \nplot_conf_mat(y_test,y_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Classification report"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Calculate evaluation metrics using cross-validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check best params\ngs_log_reg.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making it dynamically, my best params is commented\nclf = LogisticRegression()\nclf.set_params(**gs_log_reg.best_params_)\n#clf = LogisticRegression(C = 0.38566204211634725,solver = \"liblinear\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cross validated accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_acc=cross_val_score(clf,\n                       X,y,\n                       cv=5,scoring=\"accuracy\"\n)\ncv_acc = np.mean(cv_acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cross validated precision"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_precision=cross_val_score(clf,\n                       X,y,\n                       cv=5,scoring=\"precision\"\n)\ncv_precision = np.mean(cv_precision)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cross validated recall"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_recall=cross_val_score(clf,\n                       X,y,\n                       cv=5,scoring=\"recall\"\n)\ncv_recall = np.mean(cv_recall)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cross validated f1-score"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_f1=cross_val_score(clf,\n                       X,y,\n                       cv=5,scoring=\"f1\"\n)\ncv_f1 = np.mean(cv_f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Accuracy: {cv_acc*100:.2f}%   Overall, how often is the classifier correct?\")\nprint(f\"Recall (Sensitivity): {cv_recall*100:.2f}%    When it's actually yes, how often does it predict yes?\")\nprint(f\"Precision: {cv_precision*100:.2f}%    When it predicts yes, how often is it correct?\")\nprint(f\"F1 score: {cv_f1*100:.2f}%    good F1 score means that you have low false positives and low false negatives\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize cross-validated metrics\ncv_metrics = pd.DataFrame({\"Accuracy\":cv_acc,\n                          \"Precision\":cv_precision,\n                          \"Recall\": cv_recall,\n                          \"F1 score\": cv_f1},\n                         index=[0])\ncv_metrics.T.plot.bar(title=\"Cross-validated metrics\", legend=False);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" clf.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Match coef's of features to columns\nfeature_dict = dict (zip(df.columns, list(clf.coef_[0])))\nfeature_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize feature importance\nfeature_df = pd.DataFrame(feature_dict, index=[0])\nfeature_df.T.plot.bar(title=\"feature Importance\", legend=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Helper function for plotting feature importance\ndef plot_features(columns, importances, n=20):\n    df = (pd.DataFrame({\"features\": columns,\n                       \"feature_importances\": importances})\n          .sort_values(\"feature_importances\",ascending = False)\n          .reset_index(drop=True))\n    fig, ax =plt.subplots()\n    ax.barh(df[\"features\"][:n], df[\"feature_importances\"][:20])\n    ax.set_ylabel(\"Features\")\n    ax.set_xlabel(\"Feature importance\")\n    ax.invert_yaxis()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# another visualization\nplot_features(X_train.columns, clf.coef_[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results (as a non expert in field):"},{"metadata":{},"cell_type":"markdown","source":"1. Dataset has strange Heart Disease Frequency according to Sex distribution\n\nIf the patient is female, she has a higher chances of heart disease.\n\nIs it a dataset problem? Or women generally go to hospitals only with serious pains?"},{"metadata":{},"cell_type":"markdown","source":"2. Correlation coefficient and feature importance for cholesterol is non existent.\n\nThat's a surprising finding, every media is saying there is a correlation between cholesterol levels and heart diseases."},{"metadata":{},"cell_type":"markdown","source":"3. There is a positive correlation between cp (chest pain) and heart disease\n\ncp - chest pain type\n  * 0: Typical angina: chest pain related decrease blood supply to the heart\n  * 1: Atypical angina: chest pain not related to heart\n  * 2: Non-anginal pain: typically esophageal spasms (non heart related)\n  * 3: Asymptomatic: chest pain not showing signs of disease\n\nSo, basically, if there is no chest pain, it's more likely patient have a heart disease. Which can be a sign for dataset problem."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}