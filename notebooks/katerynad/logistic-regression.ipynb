{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"4b38403d-2919-bfd4-d630-9de8c31464d4"},"source":"*0 are classified better then in Naive Bayes but still an issue*"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e9f3d07e-d859-4694-269e-52f48f5e909f"},"outputs":[],"source":"import pandas as pd\nfrom pandas import Series,DataFrame\nimport numpy as np\n\nfrom datetime import date\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem.porter import PorterStemmer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve, auc\nfrom scipy import interp\n\n#Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7558513c-62f9-4007-3b6f-08ec349c2d54"},"outputs":[],"source":"#data\ndf=pd.read_csv('../input/Combined_News_DJIA.csv')\ndf['Combined']=df.iloc[:,2:27].apply(lambda row: ''.join(str(row.values)), axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"95ea4b8c-7b4e-0e69-162e-8d3a114ba8af"},"outputs":[],"source":"#train data\ntrain=df.loc[(pd.to_datetime(df[\"Date\"]) <= date(2014,12,31)),['Label','Combined']]\ntrain.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7622c8d0-f5b3-4ce3-a3b9-c4f3868266fd"},"outputs":[],"source":"#test data\ntest=df.loc[(pd.to_datetime(df[\"Date\"]) > date(2014,12,31)),['Label','Combined']]\ntest.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"c7f3a78c-c4b7-0cf1-739a-83918b04041d"},"source":"I run different classification models on the same data to compare the results. So I combine text processing, plotting and evaluation in specific functions."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f96b2026-e97e-146d-4cd5-2f9d36d66251"},"outputs":[],"source":"#Text pre-processing\n\ndef text_process(text):\n    \"\"\"\n    Takes in a string of text, then performs the following:\n    1. Tokenizes and removes punctuation\n    2. Removes  stopwords\n    3. Stems\n    4. Returns a list of the cleaned text\n    \"\"\"\n     \n    # tokenizing\n    tokenizer = RegexpTokenizer(r'\\w+')\n    text_processed=tokenizer.tokenize(text)\n    \n    # removing any stopwords\n    text_processed = [word.lower() for word in text_processed if word.lower() not in stopwords.words('english')]\n    \n    # steming\n    porter_stemmer = PorterStemmer()\n    \n    text_processed = [porter_stemmer.stem(word) for word in text_processed]\n    \n    try:\n        text_processed.remove('b')\n    except: \n        pass\n\n    return text_processed"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"22fb6320-7c1c-9f6f-e68c-c500993b60cc"},"outputs":[],"source":"def ROCCurves (Actual, Predicted):\n    '''\n    Plot ROC curves for the multiclass problem\n    based on http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n    '''\n    # Compute ROC curve and ROC area for each class\n    n_classes=2\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(Actual.values, Predicted)\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n    # Compute micro-average ROC curve and ROC area\n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Actual.ravel(), Predicted.ravel())\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n    ##############################################################################\n    # Plot ROC curves for the multiclass problem\n\n    # Compute macro-average ROC curve and ROC area\n\n    # First aggregate all false positive rates\n\n    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n    # Then interpolate all ROC curves at this points\n    mean_tpr = np.zeros_like(all_fpr)\n    for i in range(n_classes):\n        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\n    # Finally average it and compute AUC\n    mean_tpr /= n_classes\n\n    fpr[\"macro\"] = all_fpr\n    tpr[\"macro\"] = mean_tpr\n    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n    # Plot all ROC curves\n    plt.figure()\n    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='micro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"micro\"]),\n         linewidth=2)\n\n    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n         label='macro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"macro\"]),\n         linewidth=2)\n\n    for i in range(n_classes):\n        plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n                                   ''.format(i, roc_auc[i]))\n\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Some extension of Receiver operating characteristic to multi-class')\n    plt.legend(loc=\"lower right\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"52d8f57b-f83e-3213-3391-9255ebe5f957"},"outputs":[],"source":"def heatmap(data):\n  fig, ax = plt.subplots()\n  heatmap = sns.heatmap(data, cmap=plt.cm.Blues,annot=True, annot_kws={\"size\": 8})\n  # put the major ticks at the middle of each cell\n  # want a more natural, table-like display\n  ax.xaxis.tick_top()\n  # rotate the\n  plt.xticks(rotation=90)\n  plt.yticks(rotation=0)\n  plt.tight_layout()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b0b5c903-5200-3850-de14-0567072ae0ea"},"outputs":[],"source":"def plot_classification_report(classification_report):\n    lines = classification_report.split('\\n')\n    classes = []\n    plotMat = []\n    for line in lines[2 : (len(lines) - 3)]:\n        t = line.split()\n        classes.append(t[0])\n        v = [float(x) for x in t[1: len(t) - 1]]\n        plotMat.append(v)\n    aveTotal = lines[len(lines) - 1].split()\n    classes.append('avg/total')\n    vAveTotal = [float(x) for x in t[1:len(aveTotal) - 1]]\n    plotMat.append(vAveTotal)\n    df_classification_report = DataFrame(plotMat, index=classes,columns=['precision', 'recall', 'f1-score'])\n    heatmap(df_classification_report)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6bf28c99-6b75-a9f4-7738-e33e64f9d2dc"},"outputs":[],"source":"def plot_confusion_matrix(confusion_matrix,classes=['0','1']):\n    df_confusion_matrix = DataFrame(confusion_matrix, index=classes,columns=classes)\n    df_confusion_matrix\n    heatmap(df_confusion_matrix)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"00f98fe1-8a17-538c-79c9-701c1d270d2c"},"outputs":[],"source":"def Evaluation (Actual, Predicted):\n    '''\n        Prints and plots\n        - classification report\n        - confusion matrix\n        - ROC-AUC\n    '''\n    print (classification_report(Actual,Predicted))\n    plot_classification_report(classification_report(Actual,Predicted))\n    print ('Confussion matrix:\\n', confusion_matrix(Actual,Predicted))\n    plot_confusion_matrix(confusion_matrix(Actual,Predicted))\n    print ('ROC-AUC: ' + str(roc_auc_score(Actual,Predicted)))\n    ROCCurves (Actual,Predicted)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79e54595-a967-69e4-6bcc-8f59451fdd19"},"outputs":[],"source":"#Creating a Data Pipeline for LogisticRegression classifier\nlgr_pipeline = Pipeline([\n    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n    ('classifier', LogisticRegression()),  # train on TF-IDF vectors w/ LogisticRegression classifier\n])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6c6f11be-e08e-a1b1-89cf-e716be14295e"},"outputs":[],"source":"lgr_pipeline.fit(train['Combined'],train['Label'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b28b7064-3707-2ff5-ba68-2661976700e3"},"outputs":[],"source":"predictions = lgr_pipeline.predict(test['Combined'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0d4d7959-e3e9-7523-67c3-bc3e2961def5"},"outputs":[],"source":"Evaluation (test[\"Label\"], predictions)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}