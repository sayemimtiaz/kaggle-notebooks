{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"8340d1eb-868c-8d45-945d-9c84399010db"},"source":"The script compares scores for Naive Bayes (baseline), Bernoulli Naive Bayes and different n-grams, n days shifts (how yesterday's news impact today's index), smoothing parameters. Different combinations of Top news columns are used in the analysis.\n\nThe best result  for all Top news columns combination (AUC - 55%) is achieved with 3-days shift and 0.5 smoothing parameter.\n\nI did not find any n-grams change the result.\n\n1-day shift and smoothing parameter 0  provides almost the same AUC but precision and recall scores are better for 3-days shift and 0.5 smoothing parameter\n\nOther combination of Top news columns can give us 56 - 59% AUC\n\nCombined Top3, Top12 and Top25, 2 days shift - 59%, Top10 and Top25, no days shift - 58%\nTop25 only and 0 days shift, combined Top1 and Top6 3-days shift etc - 56%\n\n\nLooks like  the rating from RedditNews is not important for the index, maybe the source of the news is more significant\n\nPlease see below the detail results"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"99ad2797-36a7-e8ed-081f-f3c70d5d62ad"},"outputs":[],"source":"import pandas as pd\nfrom pandas import Series,DataFrame\nimport numpy as np\n\n\nfrom datetime import date\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem.porter import PorterStemmer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve, auc,precision_score, accuracy_score, recall_score, f1_score\nfrom scipy import interp\n\n#Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b80cfb49-7da7-60a7-fc9f-9901f85c7897"},"outputs":[],"source":"#List to keep different methods scores to compare\nScoreSummaryByMethod=[]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"78735ad8-a191-1bf3-e332-b7fb9852f3f7"},"outputs":[],"source":"#data\ndf=pd.read_csv('../input/Combined_News_DJIA.csv')\ndf['Combined']=df.iloc[:,2:27].apply(lambda row: ''.join(str(row.values)), axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"db782ac8-ac48-b31b-3a9a-ddeac5be1b77"},"outputs":[],"source":"#train data\ntrain=df.loc[(pd.to_datetime(df[\"Date\"]) <= date(2014,12,31)),['Label','Combined']]\ntrain.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"92ca95d6-c2d2-7dfd-7c59-8f73290a792d"},"outputs":[],"source":"#test data\ntest=df.loc[(pd.to_datetime(df[\"Date\"]) > date(2014,12,31)),['Label','Combined']]\ntest.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"e79c709f-350c-d5cd-1d2d-09d4b111c067"},"source":"I run different classification models on the same data to compare the results. So I combine text processing, plotting and evaluation in specific functions."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"be1c5e9b-5656-6946-6753-43e773c646ae"},"outputs":[],"source":"#Text pre-processing\n\ndef text_process(text):\n    \"\"\"\n    Takes in a string of text, then performs the following:\n    1. Tokenizes and removes punctuation\n    2. Removes  stopwords\n    3. Stems\n    4. Returns a list of the cleaned text\n    \"\"\"\n    if pd.isnull(text):\n        return []\n    # tokenizing\n    tokenizer = RegexpTokenizer(r'\\w+')\n    text_processed=tokenizer.tokenize(text)\n    \n    # removing any stopwords\n    text_processed = [word.lower() for word in text_processed if word.lower() not in stopwords.words('english')]\n    \n    # steming\n    porter_stemmer = PorterStemmer()\n    \n    text_processed = [porter_stemmer.stem(word) for word in text_processed]\n    \n    try:\n        text_processed.remove('b')\n    except: \n        pass\n\n    return text_processed\n    "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4bb4d28f-1d4f-2759-e234-3a674603e9f0"},"outputs":[],"source":"def ROCCurves (Actual, Predicted):\n    '''\n    Plot ROC curves for the multiclass problem\n    based on http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n    '''\n    # Compute ROC curve and ROC area for each class\n    n_classes=2\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(Actual.values, Predicted)\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n    # Compute micro-average ROC curve and ROC area\n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Actual.ravel(), Predicted.ravel())\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n    ##############################################################################\n    # Plot ROC curves for the multiclass problem\n\n    # Compute macro-average ROC curve and ROC area\n\n    # First aggregate all false positive rates\n\n    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n    # Then interpolate all ROC curves at this points\n    mean_tpr = np.zeros_like(all_fpr)\n    for i in range(n_classes):\n        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\n    # Finally average it and compute AUC\n    mean_tpr /= n_classes\n\n    fpr[\"macro\"] = all_fpr\n    tpr[\"macro\"] = mean_tpr\n    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n    # Plot all ROC curves\n    plt.figure()\n    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='micro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"micro\"]),\n         linewidth=2)\n\n    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n         label='macro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"macro\"]),\n         linewidth=2)\n\n    for i in range(n_classes):\n        plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n                                   ''.format(i, roc_auc[i]))\n\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Some extension of Receiver operating characteristic to multi-class')\n    plt.legend(loc=\"lower right\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"93efd6fb-d9f5-49b8-b41a-dfb5226288c5"},"outputs":[],"source":"def heatmap(data, rotate_xticks=True):\n  fig, ax = plt.subplots()\n  heatmap = sns.heatmap(data, cmap=plt.cm.Blues)\n  ax.xaxis.tick_top()\n  if rotate_xticks:\n      plt.xticks(rotation=90)\n  plt.yticks(rotation=0)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b84dc8b1-37a3-b86c-4656-062bc340a6f5"},"outputs":[],"source":"def plot_classification_report(classification_report):\n    lines = classification_report.split('\\n')\n    classes = []\n    plotMat = []\n    for line in lines[2 : (len(lines) - 3)]:\n        t = line.split()\n        classes.append(t[0])\n        v = [float(x) for x in t[1: len(t) - 1]]\n        plotMat.append(v)\n    aveTotal = lines[len(lines) - 1].split()\n    classes.append('avg/total')\n    vAveTotal = [float(x) for x in t[1:len(aveTotal) - 1]]\n    plotMat.append(vAveTotal)\n    df_classification_report = DataFrame(plotMat, index=classes,columns=['precision', 'recall', 'f1-score'])\n    heatmap(df_classification_report)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"51aa12a7-a6fa-0652-ca7c-9b494706f370"},"outputs":[],"source":"def plot_confusion_matrix(confusion_matrix,classes=['0','1']):\n    df_confusion_matrix = DataFrame(confusion_matrix, index=classes,columns=classes)\n    heatmap(df_confusion_matrix,False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"11d2e03d-b0ef-235c-5b29-27aaf44b07c0"},"outputs":[],"source":"def Evaluation (Method,Comment,Actual, Predicted):\n    '''\n        Prints and plots\n        - classification report\n        - confusion matrix\n        - ROC-AUC\n    '''\n    print (Method)\n    print (Comment)\n    print (classification_report(Actual,Predicted))\n    #plot_classification_report(classification_report(Actual,Predicted))\n    print ('Confussion matrix:\\n', confusion_matrix(Actual,Predicted))\n    #plot_confusion_matrix(confusion_matrix(Actual,Predicted))\n    ROC_AUC=roc_auc_score(Actual,Predicted)\n    print ('ROC-AUC: ' + str(ROC_AUC))\n    #ROCCurves (Actual,Predicted)\n    Precision=precision_score(Actual,Predicted)\n    Accuracy=accuracy_score(Actual,Predicted)\n    Recall=recall_score(Actual,Predicted)\n    F1=f1_score(Actual,Predicted)\n    ScoreSummaryByMethod.append([Method,Comment,ROC_AUC,Precision,Accuracy,Recall,F1])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5ad4039f-9121-74c6-cab0-da940cea6455"},"outputs":[],"source":"#Creating a Data Pipeline for Naive Bayes classifier classifier - baseline\nnb_pipeline = Pipeline([\n    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n])\nnb_pipeline.fit(train['Combined'],train['Label'])\npredictions = nb_pipeline.predict(test['Combined'])\nEvaluation ('MultinomialNB','no shift, no n-grams, combined Top news',test[\"Label\"], predictions)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1b099461-0372-9eff-bab4-02694c8abc4b"},"outputs":[],"source":"#Creating a Data Pipeline for Bernoulli Naive Bayes classifier classifier and n-grams, default alpha=1\nbnb_2ngram_pipeline = Pipeline([\n    ('bow', CountVectorizer(analyzer=text_process,ngram_range=(1, 2))),  # strings to token integer counts\n    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n    ('classifier', BernoulliNB(binarize=0.0)),  # train on TF-IDF vectors w/ Bernoulli Naive Bayes classifier\n])\nbnb_2ngram_pipeline.fit(train['Combined'],train['Label'])\npredictions = bnb_2ngram_pipeline.predict(test['Combined'])\nEvaluation ('BernoulliNB(binarize=0.0)','default alpha=1,no shift, ngram_range=(1, 2), combined Top news',test[\"Label\"], predictions)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"63667f23-fa70-20e2-ccde-46064a243e21"},"outputs":[],"source":"#1 days shift\ndf.Label = df.Label.shift(-1)\ndf.drop(df.index[len(df)-1], inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2151fbf0-cdc4-d6ba-20c5-f475c47e4361"},"outputs":[],"source":"#new train data\ntrain=df.loc[(pd.to_datetime(df[\"Date\"]) <= date(2014,12,31)),['Label','Combined']]\ntrain.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d9a453ae-db95-f266-8f77-79419c6a201b"},"outputs":[],"source":"#new test data\ntest=df.loc[(pd.to_datetime(df[\"Date\"]) > date(2014,12,31)),['Label','Combined']]\ntest.tail()"},{"cell_type":"markdown","metadata":{"_cell_guid":"a8fadbac-294e-8c15-1548-0bce2a8213e4"},"source":"****The best result for Bernoulli Naive Bayes classifier, 1-2 n-grams and  1-day shift is smoothing alpha = 0****"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e627f55e-6e85-086c-ba9f-426dc035594d"},"outputs":[],"source":"#The best result for Bernoulli Naive Bayes classifier, 1-2 n-grams and 1-day shift is smoothing alpha = 0 \nbnb_2ngram_pipeline = Pipeline([\n    ('bow', CountVectorizer(analyzer=text_process,ngram_range=(1, 2))),\n    ('tfidf', TfidfTransformer()), \n    ('classifier', BernoulliNB(alpha=0.0, binarize=0.0))])\nbnb_2ngram_pipeline.fit(train['Combined'],train['Label'])\npredictions = bnb_2ngram_pipeline.predict(test['Combined'])\nEvaluation ('BernoulliNB(alpha=0.0,binarize=0.0)','1-day shift, ngram_range=(1, 2), combined Top news',test[\"Label\"], predictions)"},{"cell_type":"markdown","metadata":{"_cell_guid":"cf5aa165-79b8-f255-9011-40e664ab3354"},"source":"**2-days shift produces with any smoothing alpha gives worse results then 3-days shift\nI skip 2-days shift and demo the best result for 3-days shift**\n#The best result for Bernoulli Naive Bayes classifier, 1-2 n-grams and 3-day shift is smoothing alpha = 0.5 "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1264e666-9615-d818-55ea-c688e1a1649e"},"outputs":[],"source":"#3 days shift\ndf.Label = df.Label.shift(-2)\ndf.drop(df.index[len(df)-1], inplace=True)\ndf.drop(df.index[len(df)-1], inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a76b148f-40c9-59d6-4261-f48027ed9422"},"outputs":[],"source":"#new train data\ntrain=df.loc[(pd.to_datetime(df[\"Date\"]) <= date(2014,12,31)),['Label','Combined']]\ntrain.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1bc7e136-9830-fed2-3d40-b6d91164537c"},"outputs":[],"source":"#new test data\ntest=df.loc[(pd.to_datetime(df[\"Date\"]) > date(2014,12,31)),['Label','Combined']]\ntest.tail()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8e2209dd-6ff3-3502-9849-4c15277a44dc"},"outputs":[],"source":"#The best result for Bernoulli Naive Bayes classifier, 1-2 n-grams and 3-day shift is smoothing alpha = 0.5 \nbnb_2ngram_pipeline = Pipeline([\n    ('bow', CountVectorizer(analyzer=text_process,ngram_range=(1, 2))),\n    ('tfidf', TfidfTransformer()), \n    ('classifier', BernoulliNB(alpha=0.5, binarize=0.0))])\nbnb_2ngram_pipeline.fit(train['Combined'],train['Label'])\npredictions = bnb_2ngram_pipeline.predict(test['Combined'])\nEvaluation ('BernoulliNB(alpha=0.5,binarize=0.0)','3-days shift, ngram_range=(1, 2), combined Top news',test[\"Label\"], predictions)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5b8d9cb6-fdd7-f859-8d8a-ea3a5dd410ca"},"outputs":[],"source":"ROCCurves (test[\"Label\"], predictions)"},{"cell_type":"markdown","metadata":{"_cell_guid":"81f4f2ce-276c-bf60-2ed4-0546e5ce3f73"},"source":"**Let's explore different combinations of Top news columns**\n*Here are few the best:*"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"05446ca2-0c61-f623-5198-0b053187cd24"},"outputs":[],"source":"#Here is the pipeline we use for the differenet data sets\nbnb_2ngram_pipeline = Pipeline([\n    ('bow', CountVectorizer(analyzer=text_process,ngram_range=(1, 2))),\n    ('tfidf', TfidfTransformer()), \n    ('classifier', BernoulliNB(alpha=0.5, binarize=0.0))])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9766c267-e8a2-a928-7120-28033f9c4de9"},"outputs":[],"source":"#data re-new\ndf=pd.read_csv('../input/Combined_News_DJIA.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cc4ce750-2aa1-c3ac-575d-8eea8a78b278"},"outputs":[],"source":"#Combination 10 and 25\ndf['Combined10_25']=df.iloc[:,[11,26]].apply(lambda row: ''.join(str(row.values)), axis=1)\n#Combination 12 and 25\ndf['Combined12_25']=df.iloc[:,[13,26]].apply(lambda row: ''.join(str(row.values)), axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8477c289-399c-80d8-370f-83d6cb35c79c"},"outputs":[],"source":"#train data\ntrain=df.loc[(pd.to_datetime(df[\"Date\"]) <= date(2014,12,31)),['Label','Top1','Top12','Top25','Combined10_25','Combined12_25','Combined3_12_25']]\ntrain.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"de79cbbf-13a5-ea31-ee39-5c9d3ef1833f"},"outputs":[],"source":"#test data\ntest=df.loc[(pd.to_datetime(df[\"Date\"]) > date(2014,12,31)),['Label','Top1','Top12','Top25','Combined10_25','Combined12_25','Combined3_12_25']]\ntest.tail()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"02f58d47-23b0-2006-1fff-dfccc3ddfe7c"},"outputs":[],"source":"#no changes in the pipeline. We just use other data sets\n#Top1, no shift, baseline\nbnb_2ngram_pipeline.fit(train['Top1'],train['Label'])\npredictions = bnb_2ngram_pipeline.predict(test['Top1'])\nEvaluation ('BernoulliNB(alpha=0.5,binarize=0.0)','no shift, ngram_range=(1, 2),Top1 only',test[\"Label\"], predictions)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"42860a1c-5fed-ff4a-84a2-3e238a9bbef6"},"outputs":[],"source":"#no changes in the pipeline. We just use other data sets\n#Top25, no shift\nbnb_2ngram_pipeline.fit(train['Top25'],train['Label'])\npredictions = bnb_2ngram_pipeline.predict(test['Top25'])\nEvaluation ('BernoulliNB(alpha=0.5,binarize=0.0)','no shift, ngram_range=(1, 2),Top25 only',test[\"Label\"], predictions)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6e3cc3e5-bc96-97b4-3afe-404e8ae79fc4"},"outputs":[],"source":"#no changes in the pipeline. We just use other data sets\n#Combined12_25, no shift\nbnb_2ngram_pipeline.fit(train['Combined12_25'],train['Label'])\npredictions = bnb_2ngram_pipeline.predict(test['Combined12_25'])\nEvaluation ('BernoulliNB(alpha=0.5,binarize=0.0)','no shift, ngram_range=(1, 2),Combined Top12 and Top25',test[\"Label\"], predictions)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4cb01773-5de2-bf54-262a-feb2cb8e5256"},"outputs":[],"source":"#no changes in the pipeline. We just use other data sets\n#Combined10_25, no shift\nbnb_2ngram_pipeline.fit(train['Combined10_25'],train['Label'])\npredictions = bnb_2ngram_pipeline.predict(test['Combined10_25'])\nEvaluation ('BernoulliNB(alpha=0.5,binarize=0.0)','no shift, ngram_range=(1, 2),Combined Top10 and Top25',test[\"Label\"], predictions)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2239d6d1-5c57-1fe5-ecf1-699aab9023fc"},"outputs":[],"source":"ROCCurves (test[\"Label\"], predictions)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d3b59849-cc4b-9c54-17b0-e63379f03e7e"},"outputs":[],"source":"#let's shift the data and explore Top3, Top12 and Top25 combination for 2 days shift\ndf.Label = df.Label.shift(-1)\ndf.drop(df.index[len(df)-1], inplace=True)\ndf.Label = df.Label.shift(-1)\ndf.drop(df.index[len(df)-1], inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"20582a0c-cd47-dd57-364c-3d5812954e3f"},"outputs":[],"source":"#Combination 3,12 and 25\ndf['Combined3_12_25']=df.iloc[:,[4,13,26]].apply(lambda row: ''.join(str(row.values)), axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ae463f14-f27b-ba27-7ba1-a46699c1e360"},"outputs":[],"source":"#train data\ntrain=df.loc[(pd.to_datetime(df[\"Date\"]) <= date(2014,12,31)),['Label','Combined3_12_25']]\ntrain.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aca1600e-92a4-b88d-4b38-f5c807d62f68"},"outputs":[],"source":"#test data\ntest=df.loc[(pd.to_datetime(df[\"Date\"]) > date(2014,12,31)),['Label','Combined3_12_25']]\ntest.tail()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b6351512-92c3-426e-bb4c-b0e872c496dd"},"outputs":[],"source":"#no changes in the pipeline. We just use other data sets\n#Combined Top3, Top12 and Top25, 3-days shift\nbnb_2ngram_pipeline.fit(train['Combined3_12_25'],train['Label'])\npredictions = bnb_2ngram_pipeline.predict(test['Combined3_12_25'])\nEvaluation ('BernoulliNB(alpha=0.5,binarize=0.0)','2-days shift, ngram_range=(1, 2),Combined Top3,top12 and Top25',test[\"Label\"], predictions)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fe70574a-6b84-a4c8-dc6e-4f82674a7065"},"outputs":[],"source":"ROCCurves (test[\"Label\"], predictions)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e31d72e9-3e64-c777-d3f4-2248438d9e24"},"outputs":[],"source":"#let's shift the data and explore Top1 and Top6 combination for 3 days shift\ndf.Label = df.Label.shift(-1)\ndf.drop(df.index[len(df)-1], inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e79a652a-c01e-1cec-c15f-18c6371c5c98"},"outputs":[],"source":"#Combination 1 and 6\ndf['Combined1_6']=df.iloc[:,[2,7]].apply(lambda row: ''.join(str(row.values)), axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9cb9dcc4-bfa2-5318-343f-8ae45a9c9017"},"outputs":[],"source":"#train data\ntrain=df.loc[(pd.to_datetime(df[\"Date\"]) <= date(2014,12,31)),['Label','Combined1_6']]\ntrain.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0a672e4d-8519-d324-6e5e-a141c4883b53"},"outputs":[],"source":"#test data\ntest=df.loc[(pd.to_datetime(df[\"Date\"]) > date(2014,12,31)),['Label','Combined1_6']]\ntest.tail()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0f79e812-fe76-83be-6564-00140afd9ed2"},"outputs":[],"source":"#no changes in the pipeline. We just use other data sets\n#Combined Top1 and Top6, 3-days shift\nbnb_2ngram_pipeline.fit(train['Combined1_6'],train['Label'])\npredictions = bnb_2ngram_pipeline.predict(test['Combined1_6'])\nEvaluation ('BernoulliNB(alpha=0.5,binarize=0.0)','3-days shift, ngram_range=(1, 2),Combined Top1 and Top6',test[\"Label\"], predictions)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8b094a52-5c79-6bdb-abb1-6a0a7aeb3870"},"outputs":[],"source":"ROCCurves (test[\"Label\"], predictions)"},{"cell_type":"markdown","metadata":{"_cell_guid":"37ec1641-4d5d-acd7-c553-50804613db8a"},"source":"**Score Summary by Method**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"470144d0-a54f-2a67-99a9-3d5d7f1ab35e"},"outputs":[],"source":"df_ScoreSummaryByMethod=DataFrame(ScoreSummaryByMethod,columns=['Method','Comment','ROC_AUC','Precision','Accuracy','Recall','F1'])\ndf_ScoreSummaryByMethod.sort_values(['ROC_AUC'],ascending=False,inplace=True)\ndf_ScoreSummaryByMethod.head(20)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}