{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"2facc2d0-d7c4-d4da-5c04-79faf93b118e"},"source":"Data exploring\n--------------\n\nPart 5\n------\n\nLet's explore how many clusters we need and visualize the data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fa6e5565-7062-3680-d6f0-7a6719bd8750"},"outputs":[],"source":"import pandas as pd\nfrom pandas import Series,DataFrame\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\n\nfrom scipy.spatial import distance\nfrom scipy.spatial.distance import cdist, pdist\nfrom sklearn.metrics import euclidean_distances\nfrom sklearn.metrics import silhouette_score\n\n# For Visualization\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom math import sqrt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\n%matplotlib inline"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b5ad8b8c-4b8e-388c-4a66-ad22e1882834"},"outputs":[],"source":"#data\ndf=pd.read_csv('../input/indicators_by_company.csv')\ndf.head(5)"},{"cell_type":"markdown","metadata":{"_cell_guid":"18b3907b-4537-22f9-fb74-8bc78a2866dc"},"source":"Most popular indicators in 2011 discovered in Part 1\n===================================================="},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"37d76d83-9f01-6da3-56a5-b00d332b6fe6"},"outputs":[],"source":"indicators=['Assets','LiabilitiesAndStockholdersEquity',\n'StockholdersEquity',\n'CashAndCashEquivalentsAtCarryingValue',\n'NetCashProvidedByUsedInOperatingActivities',\n'NetIncomeLoss',\n'NetCashProvidedByUsedInFinancingActivities',\n'CommonStockSharesAuthorized',\n'CashAndCashEquivalentsPeriodIncreaseDecrease',\n'CommonStockValue',\n'CommonStockSharesIssued',\n'RetainedEarningsAccumulatedDeficit',\n'CommonStockParOrStatedValuePerShare',\n'NetCashProvidedByUsedInInvestingActivities',\n'PropertyPlantAndEquipmentNet',\n'AssetsCurrent',\n'LiabilitiesCurrent',\n'CommonStockSharesOutstanding',\n'Liabilities',\n'OperatingIncomeLoss' ]"},{"cell_type":"markdown","metadata":{"_cell_guid":"5f503927-c341-65eb-2ede-cd6cffbb013d"},"source":"Data Preparation\n----------------\n\n- Unpivot from existing format (years as columns)\n- Pivot to indicator ids as columns\n- Remove nulls\n- Scale the data so that the distribution of the indicators is centered around 0 with a standard deviation of 1\n- Let's review 2011 and most popular indicators in this year\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"58717a71-68e4-bbd4-6fe2-49d518dbd841"},"outputs":[],"source":"Values=df.loc[df['indicator_id'].isin(indicators),['company_id','indicator_id','2011']]\nValues=pd.melt(Values, id_vars=['company_id', 'indicator_id'], var_name='year', value_name='value')\nValues=Values.loc[Values['year']=='2011',['company_id','indicator_id','value']].pivot(index='company_id',columns='indicator_id', values='value').dropna()\nValues.head(5)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"382ea018-9881-5a61-1375-068c50c9feda"},"outputs":[],"source":"scaler = StandardScaler().fit(Values[indicators])\nValues_Scaled = scaler.transform(Values[indicators])"},{"cell_type":"markdown","metadata":{"_cell_guid":"9d5d9406-935b-a245-f52c-bbd641505f79"},"source":"For 2D visualization we need 2-components reduced data\n------------------------------------------------------"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a2c6425b-fc12-a955-ee53-616ed3c69efb"},"outputs":[],"source":"Values_Reduced_2D = PCA(n_components=2).fit_transform(Values_Scaled)"},{"cell_type":"markdown","metadata":{"_cell_guid":"0ebbafb2-7e5d-b816-a1cd-1367a96916bb"},"source":"For 3D visualization we need 3-components reduced data\n------------------------------------------------------"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5a631034-7c79-b5d9-c0c2-515f644c9343"},"outputs":[],"source":"Values_Reduced_3D = PCA(n_components=3).fit_transform(Values_Scaled)"},{"cell_type":"markdown","metadata":{"_cell_guid":"876e00ae-dc1b-af43-7da2-fbe341705175"},"source":"As was found in Part 4 - 6 principal components provides 90% of explained variance\n------------------------------------------------------------------------"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9044be83-f1a6-ca06-01e9-44048ce7cde2"},"outputs":[],"source":"Values_Reduced_6pc = PCA(n_components=6).fit_transform(Values_Scaled)"},{"cell_type":"markdown","metadata":{"_cell_guid":"ad5fca97-eb2f-8c6f-2a57-ce577276ebe4"},"source":"How many clusters do we need?\n-----------------------------\n\nLet's try to use elbow approach"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"93c23261-87c6-3127-9343-2435d54053b4"},"outputs":[],"source":"def Elbow_Method(data):\n    K = range(1,50)\n    KM = [KMeans(n_clusters=k).fit(data) for k in K]\n    centroids = [k.cluster_centers_ for k in KM]\n    D_k = [cdist(data, cent, 'euclidean') for cent in centroids]\n    cIdx = [np.argmin(D,axis=1) for D in D_k]\n    dist = [np.min(D,axis=1) for D in D_k]\n    avgWithinSS = [sum(d)/data.shape[0] for d in dist]\n    # Total with-in sum of square\n    wcss = [sum(d**2) for d in dist]\n    tss = sum(pdist(data)**2)/data.shape[0]\n    bss = tss-wcss\n    \n\n    # elbow curve\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    \n    kIdx = 7\n    ax.plot(K, avgWithinSS, 'b*-')\n    ax.plot(K[kIdx], avgWithinSS[kIdx], marker='o', markersize=10, \n    markeredgewidth=2, markeredgecolor='r', markerfacecolor='None')\n    \n    kIdx = 8\n    ax.plot(K, avgWithinSS, 'b*-')\n    ax.plot(K[kIdx], avgWithinSS[kIdx], marker='o', markersize=10, \n    markeredgewidth=2, markeredgecolor='b', markerfacecolor='None')\n    \n    kIdx = 9\n    ax.plot(K, avgWithinSS, 'b*-')\n    ax.plot(K[kIdx], avgWithinSS[kIdx], marker='o', markersize=10, \n    markeredgewidth=2, markeredgecolor='g', markerfacecolor='None')   \n    \n    plt.grid(True)\n    plt.xlabel('Number of clusters')\n    plt.ylabel('Average within-cluster sum of squares')\n    plt.title('Elbow for KMeans clustering')\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.plot(K, bss/tss*100, 'b*-')\n    plt.grid(True)\n    plt.xlabel('Number of clusters')\n    plt.ylabel('Percentage of variance explained')\n    plt.title('Elbow for KMeans clustering')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"df2683c1-08ee-d6af-e8f1-2edd44c4ddff"},"outputs":[],"source":"Elbow_Method(Values_Scaled)"},{"cell_type":"markdown","metadata":{"_cell_guid":"f920e013-f96d-81f1-2a7d-4d478daa9bce"},"source":"**Using full dataset (20 indocators) is not very clear where is the \"elbow\"**\n\nI would say 6 - 10 cluster\n\nLet's try a reduced dataset. First 6 principal compo"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d1a94b1c-fe23-797a-0dd3-8acc7af242f0"},"outputs":[],"source":"Elbow_Method(Values_Reduced_6pc)"},{"cell_type":"markdown","metadata":{"_cell_guid":"bf2c8508-f678-c65a-536f-6b929aab258e"},"source":"Still no clear elbow\n--------------------\n\nLet's try less components - 3 for now"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a0a6bb5a-fd12-8fc0-11c1-2ed629725e20"},"outputs":[],"source":"Elbow_Method(Values_Reduced_3D)"},{"cell_type":"markdown","metadata":{"_cell_guid":"09b80a81-82af-8619-23cb-845ac9ecdf42"},"source":"Much better\n-----------\n\nBut still not clear 7 or 10 clusters Let's explore the data reduced to 2 components"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2daf8a25-793d-d22d-ed9a-1d93e4e750ce"},"outputs":[],"source":"Elbow_Method(Values_Reduced_2D)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d221b8a7-b399-e07d-9be8-cbc29152c7b4"},"source":"Let's stop on 7 clusters and do 2D and 3D visualization\n-------------------------------------------------------"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2853d9f7-8915-575a-bb31-6f448e2b1a2a"},"outputs":[],"source":"# 2D Visualization\n\nkmeans = KMeans(init='k-means++', n_clusters=7, n_init=10)\nkmeans.fit(Values_Reduced_2D)\n\n# Step size of the mesh. Decrease to increase the quality of the VQ.\nh = .02     # point in the mesh [x_min, m_max]x[y_min, y_max].\n\n# Plot the decision boundary. For that, we will assign a color to each\nx_min, x_max = Values_Reduced_2D[:, 0].min() - 1, Values_Reduced_2D[:, 0].max() + 1\ny_min, y_max = Values_Reduced_2D[:, 1].min() - 1, Values_Reduced_2D[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n# Obtain labels for each point in mesh. Use last trained model.\nZ = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure(1, figsize=(8, 6))\nplt.imshow(Z, interpolation='nearest',\n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap=plt.cm.Paired,\n           aspect='auto', origin='lower')\n\nplt.plot(Values_Reduced_2D[:, 0], Values_Reduced_2D[:, 1], 'k.', markersize=2)\n# Plot the centroids as a white X\ncentroids = kmeans.cluster_centers_\nplt.scatter(centroids[:, 0], centroids[:, 1],\n            marker='x', s=169, linewidths=3,\n            color='w', zorder=10)\nplt.title('K-means clustering (PCA-reduced data)\\n'\n          'Centroids are marked with white cross')\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.xlabel(\"PC-1\")\nplt.ylabel(\"PC-2\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8fe3b8ef-60b6-7a84-376c-93dca9c090d7"},"outputs":[],"source":"fig = plt.figure(1, figsize=(8, 6))\nax = Axes3D(fig, elev=-150, azim=250)\nkmeans = KMeans(init='k-means++', n_clusters=7, n_init=10)\nkmeans.fit(Values_Reduced_3D)\nax.scatter(Values_Reduced_3D[:, 0], Values_Reduced_3D[:, 1], Values_Reduced_3D[:, 2], \n           c=kmeans.labels_.astype(np.float)\n          )\ncentroids = kmeans.cluster_centers_\nax.scatter(centroids[:, 0], centroids[:, 1],  centroids[:, 2],\n            marker='x', s=169, linewidths=3, \n           c='r')\nax.set_title('K-means clustering (PCA-reduced data)\\n'\n             'Centroids are marked with red cross')\nax.set_xlabel(\"PC-1\")\nax.w_xaxis.set_ticklabels([])\nax.set_ylabel(\"PC-2\")\nax.w_yaxis.set_ticklabels([])\nax.set_zlabel(\"PC-3\")\nax.w_zaxis.set_ticklabels([])"},{"cell_type":"markdown","metadata":{"_cell_guid":"6ab89f0e-a028-7ae5-5388-459f77212feb"},"source":"The other methods (BIC and Siluet analysis) do not clarify elbows on any data set\n------------------------------------------------------------------------"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0c13c6c2-7e94-415a-a336-ae6a72cefda6","collapsed":true},"outputs":[],"source":"def bic(clusters, centroids):\n    num_points = sum(len(cluster) for cluster in clusters)\n    num_dims = clusters[0][0].shape[0]\n    log_likelihood = _loglikelihood(num_points, num_dims, clusters, centroids)\n    num_params = _free_params(len(clusters), num_dims)\n    return log_likelihood - num_params / 2.0 * np.log(num_points)\n\n\ndef _free_params(num_clusters, num_dims):\n    return num_clusters * (num_dims + 1)\n\n\ndef _loglikelihood(num_points, num_dims, clusters, centroids):\n    ll = 0\n    for cluster in clusters:\n        fRn = len(cluster)\n        t1 = fRn * np.log(fRn)\n        t2 = fRn * np.log(num_points)\n        variance = _cluster_variance(num_points, clusters, centroids) or np.nextafter(0, 1)\n        t3 = ((fRn * num_dims) / 2.0) * np.log((2.0 * np.pi) * variance)\n        t4 = (fRn - 1.0) / 2.0\n        ll += t1 - t2 - t3 - t4\n    return ll\n\ndef _cluster_variance(num_points, clusters, centroids):\n    s = 0\n    denom = float(num_points - len(centroids))\n    for cluster, centroid in zip(clusters, centroids):\n        distances = euclidean_distances(cluster, centroid)\n        s += (distances*distances).sum()\n    return s / denom\n\n\ndef compute_bic(kmeans,X):\n    \"\"\"\n    Computes the BIC metric for a given clusters\n\n    Parameters:\n    -----------------------------------------\n    kmeans:  List of clustering object from scikit learn\n\n    X     :  multidimension np array of data points\n\n    Returns:\n    -----------------------------------------\n    BIC value\n    \"\"\"\n    # assign centers and labels\n    centers = [kmeans.cluster_centers_]\n    labels  = kmeans.labels_\n    #number of clusters\n    m = kmeans.n_clusters\n    # size of the clusters\n    n = np.bincount(labels)\n    #size of data set\n    N, d = X.shape\n\n    #compute variance for all clusters beforehand\n    cl_var = (1.0 / (N - m) / d) * sum([sum(distance.cdist(X[np.where(labels == i)], [centers[0][i]], 'euclidean')**2) for i in range(m)])\n\n    const_term = 0.5 * m * np.log(N) * (d+1)\n\n    BIC = np.sum([n[i] * np.log(n[i]) -\n               n[i] * np.log(N) -\n             ((n[i] * d) / 2) * np.log(2*np.pi*cl_var) -\n             ((n[i] - 1) * d/ 2) for i in range(m)]) - const_term\n\n    return(BIC)\n\ndef BIC_Method(data):\n    sns.set_style(\"ticks\")\n    sns.set_palette(sns.color_palette(\"Blues_r\"))\n    bics = []\n    for n_clusters in range(2,50):\n        kmeans = KMeans(n_clusters=n_clusters)\n        kmeans.fit(data)\n\n        labels = kmeans.labels_\n        centroids = kmeans.cluster_centers_\n\n        clusters = {}\n        for i,d in enumerate(kmeans.labels_):\n            if d not in clusters:\n                clusters[d] = []\n            clusters[d].append(data[i])\n\n        bics.append(compute_bic(kmeans,data))#-bic(clusters.values(), centroids))\n\n    plt.plot(bics)\n    plt.ylabel(\"BIC score\")\n    plt.xlabel(\"Number of clusters\")\n    plt.title(\"BIC scoring for K-means cell's behaviour\")\n    sns.despine()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dd5b2e7e-8ffa-f59c-c9b6-1c205333ad7c"},"outputs":[],"source":"BIC_Method(Values_Scaled)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"14e6a561-7301-7631-d541-dfe0f78790b4"},"outputs":[],"source":"BIC_Method(Values_Reduced_6pc)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"03dfd4f7-cf5e-067e-ca79-e8212af0a53f"},"outputs":[],"source":"BIC_Method(Values_Reduced_3D)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ea6f60eb-7e82-56b2-1f4b-4c510f7ebdec"},"outputs":[],"source":"BIC_Method(Values_Reduced_2D)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e123163a-b948-6987-90ab-2379e5d62cec"},"outputs":[],"source":"def Silhouette_Method(data):\n    s = []\n    for n_clusters in range(2,50):\n        kmeans = KMeans(n_clusters=n_clusters)\n        kmeans.fit(data)\n\n        labels = kmeans.labels_\n        centroids = kmeans.cluster_centers_\n\n        s.append(silhouette_score(data, labels, metric='euclidean'))\n\n    plt.plot(s)\n    plt.ylabel(\"Silouette\")\n    plt.xlabel(\"Number of clusters\")\n    plt.title(\"Silouette for K-means cell's behaviour\")\n    #sns.despine()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bcd71a2b-7324-4eac-d2a2-4f1c4997600d"},"outputs":[],"source":"Silhouette_Method(Values_Scaled)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"721d1ddc-59eb-41b0-86f5-470fb49e5987"},"outputs":[],"source":"Silhouette_Method(Values_Reduced_6pc)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"046ba4da-6a38-158d-94fe-459ad7e4a7fe"},"outputs":[],"source":"Silhouette_Method(Values_Reduced_3D)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2fc630cc-24d3-ba18-dbdd-57a6feb71d9f"},"outputs":[],"source":"Silhouette_Method(Values_Reduced_2D)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f7ebd63a-d58a-2378-555e-a9dd90335ffd"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}