{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"We are trying to predict who gets diabetes and who doesn't (Outcome = 1 and 0, respectively). This notebook shows a simple Random Forest implementation that gets 83% accuracy (TP+TN). I just drop 2 columns, impute missing values for some other columns, and apply the default random forest model. This is my first notebook, so please let me know if there are any ways I can improve!","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load data\nX = pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')\ny = X.pop('Outcome')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-23T17:41:54.32826Z","iopub.execute_input":"2021-07-23T17:41:54.328565Z","iopub.status.idle":"2021-07-23T17:41:54.380566Z","shell.execute_reply.started":"2021-07-23T17:41:54.328538Z","shell.execute_reply":"2021-07-23T17:41:54.379835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"execution":{"iopub.status.busy":"2021-07-23T17:41:54.381628Z","iopub.execute_input":"2021-07-23T17:41:54.381869Z","iopub.status.idle":"2021-07-23T17:41:54.408191Z","shell.execute_reply.started":"2021-07-23T17:41:54.381847Z","shell.execute_reply":"2021-07-23T17:41:54.407404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"0 is an impossible value for some fields like \"Glucose\"--therefore, it probably denotes missing value. Plotting distributions with and without 0 values shows for which fields 0 is not a natural part of the distribution, and therefore is a null value. This is true for Glucose and the 4 columns after it.","metadata":{}},{"cell_type":"code","source":"numRows = 8\nnumCols = 2\nfigWidth = 5\nfigHeight = 5\nfig, axes = plt.subplots(numRows, numCols, sharex=False, figsize=(numCols * figWidth, numRows * figHeight))\n\nfor i in range(numRows):\n    colname = X.columns[i]\n    var2plot = X[colname]\n\n    # left = plot of original column\n    sns.histplot(x=var2plot, hue = y, ax=axes[i][0], common_bins=False, element='step')\n    \n    # right = plot of column with 0's filtered out\n    sns.histplot(x=var2plot[var2plot>0], hue = y, ax=axes[i][1], common_bins=False, element='step')\n    \n    plt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T17:41:54.409508Z","iopub.execute_input":"2021-07-23T17:41:54.409767Z","iopub.status.idle":"2021-07-23T17:41:56.925542Z","shell.execute_reply.started":"2021-07-23T17:41:54.409743Z","shell.execute_reply":"2021-07-23T17:41:56.92457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code below reveals what percentage of the data are missing for those columns.","metadata":{}},{"cell_type":"code","source":"for i in range(1, 6):\n    value_counts = X.iloc[:, i].value_counts().sort_index()\n    print(\"{} \\t {}\".format(X.columns[i], value_counts[0] / 768))\n    #print(X.columns[i], value_counts.index[0], value_counts[0] / 768)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T17:41:56.927472Z","iopub.execute_input":"2021-07-23T17:41:56.928037Z","iopub.status.idle":"2021-07-23T17:41:56.940682Z","shell.execute_reply.started":"2021-07-23T17:41:56.927998Z","shell.execute_reply":"2021-07-23T17:41:56.939757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Impute missing values as the median.\n\nfrom sklearn.impute import SimpleImputer\nstart_i = 1\nend_i = 5\n\n# Imputation: missing values in columns 1-5 (\"Glucose\" to \"BMI\") are denoted by 0. I replace these with the median of the column.\nmy_imputer = SimpleImputer(missing_values=0, strategy='median')\nimputed_cols = pd.DataFrame(my_imputer.fit_transform(X.iloc[:, start_i : end_i+1]))\n\n# Imputation removed column names; put them back\nimputed_cols.columns = X.columns[start_i : end_i+1]","metadata":{"execution":{"iopub.status.busy":"2021-07-23T17:41:56.94172Z","iopub.execute_input":"2021-07-23T17:41:56.941989Z","iopub.status.idle":"2021-07-23T17:41:56.95369Z","shell.execute_reply.started":"2021-07-23T17:41:56.941965Z","shell.execute_reply":"2021-07-23T17:41:56.952791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replace original columns in X with imputed columns, call it \"X_i\"\nX_i = pd.concat([X.Pregnancies, imputed_cols, X.iloc[:, 6:8]], axis=1)\nX_i","metadata":{"execution":{"iopub.status.busy":"2021-07-23T17:41:56.954662Z","iopub.execute_input":"2021-07-23T17:41:56.955021Z","iopub.status.idle":"2021-07-23T17:41:56.976454Z","shell.execute_reply.started":"2021-07-23T17:41:56.955Z","shell.execute_reply":"2021-07-23T17:41:56.975917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Define the models\n#model = DecisionTreeClassifier(random_state=0) # 2% less accurate\nmodel = RandomForestClassifier(random_state=0)\n\n### Select data to use for modelling ###\n\n#my_X = X.iloc[:, :1] # Using first variable alone (Pregnancies) gets 73% accuracy\n#my_X = X.iloc[:, 0:4] # Using first 4 variables boosts accuracy to 77%\n#my_X = X.iloc[:, :8] # Using all 8 gets 79% accuracy\n\n#my_X = X_i # 82% after imputing missing values\nmy_X = pd.concat([X_i.iloc[:, :3], X_i.iloc[:, 5:]], axis=1) # 83% by dropping SkinThickness and Insulin, which have many missing values\n\n# Break off validation set from training data\nX_t, X_v, y_t, y_v = train_test_split(my_X, y, train_size=0.8, test_size=0.2,random_state=0)\n\nmodel.fit(X_t, y_t)\npreds = model.predict(X_v)\n\n\n# Make confusion matrix\nconfusion = [[0,0],[0,0]]\nfor i in range(len(preds)):\n    confusion[y_v.iloc[i]][preds[i]] += 1\n    \nconfusion_perc = [[0,0],[0,0]]\nfor i in range(2):\n    for j in range(2):\n        confusion_perc[i][j] = confusion[i][j] / len(preds)\nprint('Confusion matrix:')\nprint(confusion_perc)\n\nprint('Accuracy: ' + str( confusion_perc[0][0] + confusion_perc[1][1] ))","metadata":{"execution":{"iopub.status.busy":"2021-07-23T17:46:07.480023Z","iopub.execute_input":"2021-07-23T17:46:07.480353Z","iopub.status.idle":"2021-07-23T17:46:07.719014Z","shell.execute_reply.started":"2021-07-23T17:46:07.480327Z","shell.execute_reply":"2021-07-23T17:46:07.718177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}