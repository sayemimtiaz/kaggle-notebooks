{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport re\nimport string\n\nimport nltk\nfrom nltk.probability import FreqDist\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom nltk import pos_tag\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\nfrom wordcloud import WordCloud\n\nimport matplotlib.style as style\nstyle.use('dark_background')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/covid19-containment-and-mitigation-measures/COVID 19 Containment measures data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().sum()/len(data)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Date Start'] = pd.to_datetime(data['Date Start'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"amount = data.groupby('Country')['Description of measure implemented'].nunique().sort_values(ascending=False).head(20)\namount = amount.to_frame().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"co = sns.cubehelix_palette(n_colors=20,\n                           start=0,\n                           rot=0.4,\n                           gamma=1.0,\n                           hue=0.8,\n                           light=0.85,\n                           dark=0.15,\n                           reverse=True,\n                           as_cmap=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\n\nsns.barplot(data = amount, y = 'Country', x = 'Description of measure implemented', palette=co)\nplt.title('Number of implemented measures and restrictions by country', fontsize=24)\nplt.xlabel('Measures implemented', fontsize=18)\nplt.ylabel('Country', fontsize=18)\nax = plt.gca()\nax.tick_params(axis = 'both', which = 'major', labelsize = 16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Quick text mining"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_line_breaks(text):\n    text = text.replace('\\r', ' ').replace('\\n', ' ')\n    return text\n\n#remove punctuation\ndef remove_punctuation(text):\n    re_replacements = re.compile(\"__[A-Z]+__\")  # such as __NAME__, __LINK__\n    re_punctuation = re.compile(\"[%s]\" % re.escape(string.punctuation))\n    '''Escape all the characters in pattern except ASCII letters and numbers: word_tokenize('ebrahim^hazrati')'''\n    tokens = word_tokenize(text)\n    tokens_zero_punctuation = []\n    for token in tokens:\n        if not re_replacements.match(token):\n            token = re_punctuation.sub(\" \", token)\n        tokens_zero_punctuation.append(token)\n    return ' '.join(tokens_zero_punctuation)\n\ndef remove_special_characters(text):\n    text = re.sub('[^a-zA-z0-9\\s]', '', text)\n    return text\n\ndef lowercase(text):\n    text_low = [token.lower() for token in word_tokenize(text)]\n    return ' '.join(text_low)\n\ndef remove_stopwords(text):\n    stop = set(stopwords.words('english'))\n    word_tokens = nltk.word_tokenize(text)\n    text = \" \".join([word for word in word_tokens if word not in stop])\n    return text\n\n#remove punctuation\ndef remove_punctuation(text):\n    re_replacements = re.compile(\"__[A-Z]+__\")  # such as __NAME__, __LINK__\n    re_punctuation = re.compile(\"[%s]\" % re.escape(string.punctuation))\n    '''Escape all the characters in pattern except ASCII letters and numbers: word_tokenize('ebrahim^hazrati')'''\n    tokens = word_tokenize(text)\n    tokens_zero_punctuation = []\n    for token in tokens:\n        if not re_replacements.match(token):\n            token = re_punctuation.sub(\" \", token)\n        tokens_zero_punctuation.append(token)\n    return ' '.join(tokens_zero_punctuation)\n\n#remobe one character words\ndef remove_one_character_words(text):\n    '''Remove words from dataset that contain only 1 character'''\n    text_high_use = [token for token in word_tokenize(text) if len(token)>1]      \n    return ' '.join(text_high_use)   \n\n##remove specific word list\ndef remove_special_words(text):\n    '''Remove the User predefine useless words from the text. The list should be in the lowercase.'''\n    special_words_list=['af', 'iv', 'ivm', 'mg', 'dd', 'vrijdag','afspraak','over','met', 'van', 'patient', 'dr', 'geyik','heyman','bekker','dries','om', 'sel', 'stipdonk', 'eurling', 'knackstedt'\n                        'lencer','volder','schalla']# list : words\n    querywords=text.split()\n    textwords = [word for word in querywords if word.lower() not in special_words_list]\n    text=' '.join(textwords)\n    return text\n    \n#%%\n# Stemming with 'Snowball Dutch stemmer\" package\ndef stem(text):\n    stemmer = nltk.stem.snowball.SnowballStemmer('english')\n    text_stemmed = [stemmer.stem(token) for token in word_tokenize(text)]        \n    return ' '.join(text_stemmed)\n\ndef lemma(text):\n    wordnet_lemmatizer = WordNetLemmatizer()\n    word_tokens = nltk.word_tokenize(text)\n    text_lemma = \" \".join([wordnet_lemmatizer.lemmatize(word) for word in word_tokens])       \n    return ' '.join(text_lemma)\n\n\n#break sentences to individual word list\ndef sentence_word(text):\n    word_tokens = nltk.word_tokenize(text)\n    return word_tokens\n#break paragraphs to sentence token \ndef paragraph_sentence(text):\n    sent_token = nltk.sent_tokenize(text)\n    return sent_token    \n\n\ndef tokenize(text):\n    \"\"\"Return a list of words in a text.\"\"\"\n    return re.findall(r'\\w+', text)\n\n\n#%% make a text c' '.join(data['le'][6])learning function specific for pitch decks so far \ndef normalization_pitchdecks(text):\n    _steps = [\n    remove_line_breaks,\n    remove_one_character_words,\n    remove_special_characters,\n    lowercase,\n    remove_punctuation,\n    remove_stopwords,\n    remove_special_words,\n    stem\n]\n    for step in _steps:\n        text=step(text)\n    return text   \n#%%","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = data['Description of measure implemented']\nexceptions = data['Exceptions']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" data = data.replace(np.nan, '', regex=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"common_words = []\n\nfor i in data['Description of measure implemented']:\n    common_words.append(normalization_pitchdecks(i))\n    \n    \n    \ncommon_exceptions = []\n\nfor i in data['Exceptions']:\n    common_exceptions.append(normalization_pitchdecks(i))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['text_clean'] = pd.Series(common_words)\ndata['exceptions_clean'] = pd.Series(common_exceptions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_clean = data['text_clean'].dropna().to_list()\nexceptions_clean = data['exceptions_clean'].dropna().to_list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,13))\nwc = WordCloud(background_color=\"black\", max_words=1000, max_font_size= 200,  width=1600, height=800)\ng = wc.generate(\" \".join(exceptions_clean))\nplt.title(\"Most discussed Exceptions\", fontsize=27)\nplt.imshow(wc.recolor( colormap= 'gist_rainbow' , random_state=17), alpha=0.98, interpolation=\"bilinear\", )\nplt.axis('off')\n\nplt.figure(figsize=(16,13))\nwc = WordCloud(background_color=\"black\", max_words=1000, max_font_size= 200,  width=1600, height=800)\nq = wc.generate(\" \".join(text_clean))\nplt.title(\"Most discussed terms\", fontsize=27)\nplt.imshow(wc.recolor( colormap= 'gist_rainbow' , random_state=17), alpha=0.98, interpolation=\"bilinear\", )\nplt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls= []\n\nfor i in text_clean:\n    ls.append(str(i).split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fdist = FreqDist()\n\nfor sentence in ls:\n    for token in sentence:\n        fdist[token] +=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_title = fdist.most_common(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls = []\nfor i in top_title:\n    ls.append({'Word': i[0], 'Num': i[1]})\n\ndf = pd.DataFrame(ls)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"co = sns.cubehelix_palette(n_colors=20,\n                           start=0,\n                           rot=-0.4,\n                           gamma=1.0,\n                           hue=0.8,\n                           light=0.85,\n                           dark=0.15,\n                           reverse=True,\n                           as_cmap=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\n\nsns.barplot(data = df, y = 'Word', x = 'Num', palette=co)\nplt.title('Most discussed words', fontsize = 24)\nplt.xlabel('Occurrences', fontsize=18)\nplt.ylabel('Word', fontsize=18)\nax = plt.gca()\nax.tick_params(axis = 'both', which = 'major', labelsize = 16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}