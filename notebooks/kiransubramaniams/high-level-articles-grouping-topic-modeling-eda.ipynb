{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Identifying the relevant subsets and discovering if we can group the topics\n## Update: 19-03-2020 - Updated to perform the same task on abstract instead of title\n\nThis work is to identify the right subset of the literature articles corresponding to coronavirus,\nto also include most related articles that speak about respiratory syndrome, SARS, MERS, etc. Rather\nthan defining these keywords explicitly, we go around to discover these relevant keywords, and \nperform a topic modeling to create 4 groups of topics from the literature titles.\n\n![Topic Space](https://i.imgur.com/nYgkHWJ.png)\n\n\n### What are the detected four groups?\n* `Group 1: population study information`\n* `Group 2: gene & protein related`\n* `Group 3: between #1 & #2 has topics about both analysis and gene information`\n* `Group 4: has topics on drug discovery, clinical trials, vaccine, etc., maybe drug discovery`\n\n### Next Tasks?\n* `can we break down these groups further to mine and answer our questions?`\n* `Should we reduce the groups to 3, so we can merge parts of Group #3 between the 1,2 & 4?`\n\n### Why title and not full text?\n* `It is not resource intensive`\n* `Some articles have non-english text, makes it hard until/unless we translate`\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm\ndf = pd.read_csv('../input/CORD-19-research-challenge/2020-03-13/all_sources_metadata_2020-03-13.csv')\n# biorxiv_clean = pd.read_csv(\"../input/cord-19-eda-parse-json-and-generate-clean-csv/biorxiv_clean.csv\")\n# clean_comm_use = pd.read_csv(\"../input/cord-19-eda-parse-json-and-generate-clean-csv/clean_comm_use.csv\")\n# clean_noncomm_use = pd.read_csv(\"../input/cord-19-eda-parse-json-and-generate-clean-csv/clean_noncomm_use.csv\")\n# clean_pmc = pd.read_csv(\"../input/cord-19-eda-parse-json-and-generate-clean-csv/clean_pmc.csv\")\n# df = pd.concat([biorxiv_clean, clean_comm_use, clean_noncomm_use, clean_pmc]).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicate_paper = ~(df.title.isnull() | df.abstract.isnull()) & (df.duplicated(subset=['title', 'abstract']))\ndf = df[~duplicate_paper].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk import ngrams\nstop_words = set(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pos_tag_text(actual_text, print_ = False):\n    p_tg = (nltk.pos_tag(nltk.word_tokenize(actual_text)))\n    if(print_):\n        for e in p_tg:\n            print(e[0]+' : '+e[1])\n    return p_tg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reject = ['(', ')', 'IN', 'DT', ':', 'CC', ',', '.']\ndef prep_text(text_in):\n    try:\n        rfa = pos_tag_text(text_in)\n        ret_tex = ''\n        for ev in rfa:\n            if(ev[1] not in reject):\n                ret_tex = ret_tex + ' ' + ev[0]\n    except:\n        ret_tex = str(text_in)\n    return(ret_tex.strip().lower())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title_without_sw = []\nfor et in tqdm(df['abstract']):\n    title_without_sw.append(prep_text(et))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['title_without_sw'] = title_without_sw","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# keywords trend lookup: 1-gram, 2-gram and 3-gram"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_n_grams(df_c, ngr):\n    kwrds_list = []\n    for et in df_c:\n        split_up = str(et).lower().split()\n        for ew in ngrams(split_up, ngr):\n            bg = ''\n            for w in ew:\n                bg = bg + ' '+ w\n            kwrds_list.append(bg.strip())\n    return(kwrds_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"key_w_l = []\nkey_w_l_bi = []\nkey_w_l_tri = []\nfor et in tqdm(df['title_without_sw']):\n    split_up = []\n    for evwd in str(et).lower().split():\n        if(evwd not in stop_words):\n            split_up.append(evwd)\n    #code to create single keywords list\n    tem_ = []\n    for eww in split_up:\n        if eww not in tem_:\n            tem_.append(eww)\n            key_w_l.append(eww)\n        #bigrams\n    for ew in ngrams(split_up, 2):\n        bg = ''\n        for w in ew:\n            bg = bg + ' '+ w\n        key_w_l_bi.append(bg.strip())\n        #tri-grams\n    for ew in ngrams(split_up, 3):\n        bg = ''\n        for w in ew:\n            bg = bg + ' '+ w\n        key_w_l_tri.append(bg.strip())\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(key_w_l).value_counts()[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(key_w_l_bi).value_counts()[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(key_w_l_tri).value_counts()[:25]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finding all abstracts that are part of top counts about coronavirus and perform analysis on that subset: We are using tri-grams going forward as bi-grams have very general terms"},{"metadata":{"trusted":true},"cell_type":"code","source":"def is_in_top(text, top_key_ws_lst):\n    #code to remove stopwords\n    ret_v = ''\n    for ewd in text.split():\n        if ewd not in stop_words:\n            ret_v = ret_v+' '+ ewd\n    ret_v = ret_v.strip()\n    fl = False\n    for ek in top_key_ws_lst:\n        for ev in ek.split():\n            #print(ev)\n            if(ev in text):\n                if (len(ev)>3):\n                    fl = True\n                    for ef in ret_v.split():\n                        if(ev in ef):\n                            ret_v = ret_v.replace(ef,'').replace('  ',' ')\n    return [ret_v,fl]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_ct = 25\nis_in_top_tri = []\ntop_text_tri = []\ntri = pd.Series(key_w_l_tri).value_counts()[:top_ct].index.tolist()\nfor et in tqdm(df['title_without_sw']):\n    \n    te = is_in_top(et, tri)\n    is_in_top_tri.append(te[1])\n    top_text_tri.append(te[0])\n    \ndf['is_in_top_tri'] = is_in_top_tri\ndf['top_text_tri'] = top_text_tri","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['is_in_top_tri'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Meaning we have 20315 literature article abstracts talking about coronavirus or relevant topics(respiratory, syndrome, etc.)"},{"metadata":{},"cell_type":"markdown","source":"# Preparing data for topic modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tri = df[df['is_in_top_tri'] == True]\ndf_tri.reset_index(inplace = True) \n# for i, r in df_tri[:3].iterrows():\n#     print('\\n====================\\n'+r['title']+'\\n'+r['top_text_tri'])\n#     pos_tag_text(r['top_text_tri'], True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tri.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tri.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now that we have identified topics talking about the corona virus (with different relevant keywords), let us do topic modeling on that subset\nInspired from work [Topic Modeling: Finding Related Articles](https://www.kaggle.com/danielwolffram/topic-modeling-finding-related-articles)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nfrom sklearn.feature_extraction import text\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\n#import scispacy\n#import spacy\n\nfrom scipy.spatial.distance import jensenshannon\n\nimport joblib\n\nimport pyLDAvis\nimport pyLDAvis.sklearn\npyLDAvis.enable_notebook()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import en_core_sci_sm\n#nlp = en_core_sci_sm.load()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#def spacy_tokenizer(sentence):\n#    return [word.lemma_ for word in nlp(sentence)]\ndef print_top_words(model, feature_names, n_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        message = \"\\nTopic #%d: \" % topic_idx\n        message += \" \".join([feature_names[i]\n                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n        print(message)\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n                                stop_words = stop_words,\n                                lowercase = True\n                               )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf = tf_vectorizer.fit_transform(df_tri['top_text_tri'].str.replace('\\n\\n', ''))\ntf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda_tf = LatentDirichletAllocation(n_components=4, random_state=0)\nlda_tf.fit(tf)\ntfidf_feature_names = tf_vectorizer.get_feature_names()\n#print_top_words(lda_tf, tfidf_feature_names, 5)\n#viz = pyLDAvis.sklearn.prepare(lda_tf, tf, tf_vectorizer)\n#pyLDAvis.display(viz)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Looking at the above, we have articles broken down with following top keywords / topics:\n* `Group 1: population study information`\n* `Group 2: gene & protein related`\n* `Group 3: between #1 & #2 has topics about both analysis and gene information`\n* `Group 4: has topics on clinical trials, vaccine, etc., maybe drug discovery`"},{"metadata":{"trusted":true},"cell_type":"code","source":"topic_dist = pd.DataFrame(lda_tf.transform(tf))\ntopic_dist.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_k_nearest_docs(doc_dist, k=5, use_jensenshannon=True):\n    '''\n    doc_dist: topic distribution (sums to 1) of one article\n    \n    Returns the index of the k nearest articles (as by Jensenâ€“Shannon divergence/ Euclidean distance in topic space). \n    '''\n    \n    if use_jensenshannon:\n            distances = topic_dist.apply(lambda x: jensenshannon(x, doc_dist), axis=1)\n    else:\n        diff_df = topic_dist.sub(doc_dist)\n        distances = np.sqrt(np.square(diff_df).sum(axis=1)) # euclidean distance (faster)\n        \n    return distances[distances != 0].nsmallest(n=k).index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topic_dist.head(5)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"def get_titles(vectr, k, condition=['']):\n    recommended = get_k_nearest_docs(vectr, k=k)\n    for i in recommended:\n        title_ = df_tri['title'][i]\n        pr = False\n        for l in condition:\n            if l in title_:\n                pr = True\n        if(pr):\n            print('- ', df_tri['title'][i] )\n            print('==========================')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# printing articles containing 'virus' related keywords under group 1: looks more like population study information by keywords"},{"metadata":{"trusted":true},"cell_type":"code","source":"viru_kws = ['virus', 'corona', 'middle', 'east', 'respiratory']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_titles([1,0,0,0], 20, viru_kws)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# printing articles containing 'virus' under group 2: Looks more like gene & protein related"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_titles([0,1,0,0], 20, viru_kws)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# printing articles containing 'virus' under group 3: more like between #1 & #2 has topics about both analysis and gene information"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_titles([0,0,1,0], 20, viru_kws)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# printing articles containing 'virus' under group 4: has topics on clinical trials, vaccine, drug discovery, etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"get_titles([0,0,0,1], 20, viru_kws)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}