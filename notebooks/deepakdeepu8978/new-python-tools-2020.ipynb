{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"**n this kernel i will try to explane how we can make  Data visualization with some awesome python libraries which can real come in handy for the Data Analysis and Machine learning tasks, Data visualization is the art of telling your story through data so lets get started**\n\n# 1.dabl(Data Analysis Baseline Library)\n\ndabl(Data Analysis Baseline Library) tries to help make supervised machine learning more accessible for beginners, and reduce boiler plate for common tasks.\nlets see how it works "},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install dabl \nimport dabl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from dabl import plot\nfrom dabl.datasets import load_ames\nimport matplotlib.pyplot as plt\n\n# load the ames housing dataset\n# returns a plain dataframe\ndata = load_ames()\n\nplot(data, 'SalePrice')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import fetch_openml\nfrom dabl import plot\n\nX, y = fetch_openml('diamonds', as_frame=True, return_X_y=True)\n\nplot(X, y)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import load_wine\nfrom dabl.utils import data_df_from_bunch\n\nwine_bunch = load_wine()\nwine_df = data_df_from_bunch(wine_bunch)\n\nplot(wine_df, 'target')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import dabl\nimport pandas as pd\ntitanic = pd.read_csv(dabl.datasets.data_path(\"titanic.csv\"))\ntitanic_df_clean = dabl.clean(titanic, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"types = dabl.detect_types(titanic_df_clean)\nprint(types) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Initial Model Building\n\nThe SimpleClassifier first tries several baseline and instantaneous models, potentially on subsampled data, to get an idea of what a low baseline should be. This again is a good place to surface data leakage, as well as find the main discriminative features in the dataset. The SimpleClassifier allows specifying data in the scikit-learn-style fit(X, y) with a 1d y and features X, or with X being a dataframe and specifying the target column inside of X as target_col.\n\nThe SimpleClassifier also performs preprocessing such as missing value imputation and one-hot encoding."},{"metadata":{"trusted":true},"cell_type":"code","source":"ec = dabl.SimpleClassifier(random_state=0).fit(titanic, target_col=\"ticket\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Searching optimal parameters with successive halving\n\ndabl provides the   ***dabl.search.GridSuccessiveHalving***   and dabl.search.RandomSuccessiveHalving estimators that can be used to search a parameter space using successive halving 1 2. Successive halving is an iterative selection process where all candidates are evaluated with a small amount of resources at the first iteration. Only a subset of these candidates are selected for the next iteration, which will be allocated more resources. What defines a resource is typically the number of samples to train on, or the number of trees for a gradient boosting / decision forest estimator."},{"metadata":{},"cell_type":"markdown","source":"# 2.missingno \n\nMessy datasets? Missing values? missingno provides a small toolset of flexible and easy-to-use missing data visualizations and utilities that allows you to get a quick visual summary of the completeness (or lack thereof) of your dataset. Just pip install missingno to get started."},{"metadata":{"trusted":true},"cell_type":"code","source":"import missingno as msno\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"lets use this vehicle-collisions for example "},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\ndatabase = pd.read_csv(\"../input/vehicle-collisions/database.csv\")\ndatabase.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ohhh .... there are so may missing values ok lets see how missingno will give us :) **"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\ncollisions = database.replace(\"nan\", np.nan)\nmsno.matrix(collisions.sample(250))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At a glance, date, time, the distribution of vehicle type, and the location name of the first vehicle appear to be more NAN, while geographic information seems mostly complete, but spottier.\n\nThe sparkline at right summarizes the general shape of the data completeness and points out the rows with the maximum and minimum nullity in the dataset.\n\nThis visualization will comfortably accommodate up to 50 labelled variables. Past that range labels begin to overlap or become unreadable, and by default large displays omit them.\n\nIf you are working with time-series data, you can [specify a periodicity](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html) using the freq keyword parameter:"},{"metadata":{"trusted":true},"cell_type":"code","source":"null_pattern = (np.random.random(1000).reshape((50, 20)) > 0.5).astype(bool)\nnull_pattern = pd.DataFrame(null_pattern).replace({False: None})\nmsno.matrix(null_pattern.set_index(pd.period_range('1/1/2011', '2/1/2015', freq='M')) , freq='BQ')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Bar Chart** :\n\nmsno.bar is a simple visualization of nullity by column:"},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.bar(collisions.sample(1000))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can switch to a logarithmic scale by specifying log=True. bar provides the same information as matrix, but in a simpler format.\n\n**Heatmap** :\n\nThe missingno correlation heatmap measures nullity correlation: how strongly the presence or absence of one variable affects the presence of another:"},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.heatmap(collisions)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this example, it seems that reports which are filed with an OFF STREET NAME variable are less likely to have complete geographic data.\n\nNullity correlation ranges from -1 (if one variable appears the other definitely does not) to 0 (variables appearing or not appearing have no effect on one another) to 1 (if one variable appears the other definitely also does).\n\nVariables that are always full or always empty have no meaningful correlation, and so are silently removed from the visualization—in this case for instance the datetime and injury number columns, which are completely filled, are not included.\n\nThe heatmap works great for picking out data completeness relationships between variable pairs, but its explanatory power is limited when it comes to larger relationships and it has no particular support for extremely large datasets.\n\n**Dendrogram**:\n\nThe dendrogram allows you to more fully correlate variable completion, revealing trends deeper than the pairwise ones visible in the correlation heatmap:"},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.dendrogram(collisions)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dendrogram uses a [hierarchical clustering algorithm](https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html)(courtesy of scipy) to bin variables against one another by their nullity correlation (measured in terms of binary distance). At each step of the tree the variables are split up based on which combination minimizes the distance of the remaining clusters. The more monotone the set of variables, the closer their total distance is to zero, and the closer their average distance (the y-axis) is to zero.\n\nTo interpret this graph, read it from a top-down perspective. Cluster leaves which linked together at a distance of zero fully predict one another's presence—one variable might always be empty when another is filled, or they might always both be filled or both empty, and so on. In this specific example the dendrogram glues together the variables which are required and therefore present in every record.\n\nAs with matrix, only up to 50 labeled columns will comfortably display in this configuration. However the dendrogram more elegantly handles extremely large datasets by simply flipping to a horizontal configuration."},{"metadata":{},"cell_type":"markdown","source":"# 3.PyFlux\n\nTime series analysis is one of the most frequently encountered problems in the Machine learning domain. PyFlux is an open source library in Python explicitly built for working with time series problems. The library has an excellent array of modern time series models including but not limited to ARIMA, GARCH, and VAR models. In short, PyFlux offers a probabilistic approach to time series modeling. Worth trying out.\n\nTime series analysis is a subfield of statistics and econometrics. Time series data yt is indexed by time t and ordered sequentially. This presents unique challenges including autocorrelation within the data, non-exchangeability of data points, and non-stationarity of data and parameters. Because of the sequential nature of the data, time series analysis has particular goals."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# installing and importing the library\n\n!pip install pyflux\n\nimport pyflux as pf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas_datareader.data import DataReader\nfrom datetime import datetime\n\na = DataReader('JPM',  'yahoo', datetime(2006,6,1), datetime(2016,6,1))\na_returns = pd.DataFrame(np.diff(np.log(a['Adj Close'].values)))\na_returns.index = a.index.values[1:a.index.values.shape[0]]\na_returns.columns = [\"JPM Returns\"]\n\na_returns.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 5))\nplt.ylabel(\"Returns\")\nplt.plot(a_returns)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lets apply this on NIFTY-50 Stock Market Data (2000-2019) dataset and lets see the results ..!!\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"maruti = pd.read_csv(\"../input/nifty50-stock-market-data/MARUTI.csv\")\n# Convert string to datetime64\nmaruti ['Date'] = maruti ['Date'].apply(pd.to_datetime)\nmaruti.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maruti_df = maruti[['Date','VWAP']]\n\n#Set Date column as the index column.\nmaruti_df.set_index('Date', inplace=True)\nmaruti_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### .. Visualise the data and modelling with pyflux "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 5))\nplt.ylabel(\"Volume Weighted Average Price'\")\nplt.plot(maruti_df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's run an ARIMA Model. We can build an ARIMA model as follows, specifying the order of model we want, as well as a pandas DataFrame or numpy array carrying the data.\n\n## ARIMA :\n\nAutoregressive integrated moving average (ARIMA) models were popularised by Box and Jenkins (1970). An ARIMA model describes a univariate time series as a combination of autoregressive (AR) and moving average (MA) lags which capture the autocorrelation within the time series. The order of integration denotes how many times the series has been differenced to obtain a stationary series.\n\nWe write an ARIMA(p,d,q) model for some time series data yt, where p is the number of autoregressive lags, d is the degree of differencing and q is the number of moving average lags as:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#codes from Rodrigo Lima  @rodrigolima82\nfrom IPython.display import Image\nImage(url = 'https://www.machinelearningplus.com/wp-content/uploads/2019/02/Equation-3-min.png',width=400,height=400)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ARIMA models are associated with a Box-Jenkins approach to time series. According to this approach, you should difference the series until it is stationary, and then use information criteria and autocorrelation plots to choose the appropriate lag order for an ARIMA process. You then apply inference to obtain latent variable estimates, and check the model to see whether the model has captured the autocorrelation in the time series. For example, you can plot the autocorrelation of the model residuals. Once you are happy, you can use the model for retrospection and forecasting."},{"metadata":{"trusted":true},"cell_type":"code","source":"my_model = pf.ARIMA(data=maruti_df, ar=4, ma=4, family=pf.Normal())\nprint(my_model.latent_variables)\n\nresult = my_model.fit(\"MLE\")\nresult.summary()\n\nmy_model.plot_z(figsize=(15,5))\nmy_model.plot_fit(figsize=(15,10))\nmy_model.plot_predict_is(h=50, figsize=(15,5))\nmy_model.plot_predict(h=20,past_values=20,figsize=(15,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ARIMAX models : \n\nAutoregressive integrated moving average (ARIMAX) models extend ARIMA models through the inclusion of exogenous variables X. We write an ARIMAX(p,d,q) model for some time series data yt and exogenous data Xt, where p is the number of autoregressive lags, d is the degree of differencing and q is the number of moving average lags as:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from IPython.display import display\nfrom IPython.display import Image\nImage(url = 'https://publichealth.jmir.org/api/download?filename=1aa9764d6789a491635ab5a3238e9da6.png&alt_name=10827-230395-1-PB.png',width=400,height=400)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/MASS/drivers.csv\")\ndata.index = data['time']\ndata.loc[(data['time']>=1983.05), 'seat_belt'] = 1\ndata.loc[(data['time']<1983.05), 'seat_belt'] = 0\ndata.loc[(data['time']>=1974.00), 'oil_crisis'] = 1\ndata.loc[(data['time']<1974.00), 'oil_crisis'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.plot(data.index,data=data)\nplt.ylabel('Driver Deaths')\nplt.title('Deaths of Car Drivers in Great Britain 1969-84')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = pf.ARIMAX(data=data, formula='time~1+seat_belt+oil_crisis',\n                  ar=1, ma=1, family=pf.Normal())\n\nx = model.fit(\"MLE\")\nx.summary()\n\n\nmodel.plot_z(figsize=(15,5))\nmodel.plot_fit(figsize=(15,10))\nmodel.plot_predict_is(h=50, figsize=(15,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**there are so many modelling algorithms are there in pyflux **\nlike :\n\n* DAR models\n* Dynamic Linear regression models\n* Beta-t-EGARCH models\n* Beta-t-EGARCH in-mean models\n* Beta-t-EGARCH in-mean regression models\n\nlike wish so many ..!!  "},{"metadata":{},"cell_type":"markdown","source":"# Bokeh\n\n\nBokeh is an interactive visualization library for modern web browsers. It provides elegant, concise construction of versatile graphics, and affords high-performance interactivity over large or streaming datasets. Bokeh can help anyone who would like to quickly and easily make interactive plots, dashboards, and data applications."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install bokeh","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"here in this package there are some sampledata so lets download that and check thous "},{"metadata":{"trusted":true},"cell_type":"code","source":"import bokeh \nbokeh.sampledata.download()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bokeh packages\nfrom bokeh.io import output_file,show,output_notebook,push_notebook\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource,HoverTool,CategoricalColorMapper\nfrom bokeh.layouts import row,column,gridplot\nfrom bokeh.models.widgets import Tabs,Panel\nfrom bokeh.sampledata.stocks import AAPL, GOOG, IBM, MSFT\noutput_notebook()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N = 4000\nx = np.random.random(size=N) * 150\ny = np.random.random(size=N) * 150\nradii = np.random.random(size=N) * 1.5\ncolors = [\n    \"#%02x%02x%02x\" % (int(r), int(g), 150) for r, g in zip(50+2*x, 30+2*y)\n]\n\nTOOLS=\"hover,crosshair,pan,wheel_zoom,zoom_in,zoom_out,box_zoom,undo,redo,reset,tap,save,box_select,poly_select,lasso_select,\"\n\np = figure(tools=TOOLS)\n\np.scatter(x, y, radius=radii,\n          fill_color=colors, fill_alpha=0.6,\n          line_color=None)\n\nshow(p)  # open a browser","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.integrate import odeint\nsigma = 10\nrho = 28\nbeta = 8.0/3\ntheta = 3 * np.pi / 4\ndef lorenz(xyz, t):\n    x, y, z = xyz\n    x_dot = sigma * (y - x)\n    y_dot = x * rho - x * z - y\n    z_dot = x * y - beta* z\n    return [x_dot, y_dot, z_dot]\ninitial = (-10, -7, 35)\nt = np.arange(0, 100, 0.006)\n\nsolution = odeint(lorenz, initial, t)\n\nx = solution[:, 0]\ny = solution[:, 1]\nz = solution[:, 2]\nxprime = np.cos(theta) * x - np.sin(theta) * y\ncolors = [\"#C6DBEF\", \"#9ECAE1\", \"#6BAED6\", \"#4292C6\", \"#2171B5\", \"#08519C\", \"#08306B\",]\np = figure(title=\"Lorenz attractor example\", background_fill_color=\"#fafafa\")\np.multi_line(np.array_split(xprime, 7), np.array_split(z, 7),\n             line_color=colors, line_alpha=0.8, line_width=1.5)\n\nshow(p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def datetime(x):\n    return np.array(x, dtype=np.datetime64)\n\np1 = figure(x_axis_type=\"datetime\", title=\"Stock Closing Prices\")\np1.grid.grid_line_alpha=0.3\np1.xaxis.axis_label = 'Date'\np1.yaxis.axis_label = 'Price'\n\np1.line(datetime(AAPL['date']), AAPL['adj_close'], color='#A6CEE3', legend_label='AAPL')\np1.line(datetime(GOOG['date']), GOOG['adj_close'], color='#B2DF8A', legend_label='GOOG')\np1.line(datetime(IBM['date']), IBM['adj_close'], color='#33A02C', legend_label='IBM')\np1.line(datetime(MSFT['date']), MSFT['adj_close'], color='#FB9A99', legend_label='MSFT')\np1.legend.location = \"top_left\"\n\naapl = np.array(AAPL['adj_close'])\naapl_dates = np.array(AAPL['date'], dtype=np.datetime64)\n\nwindow_size = 30\nwindow = np.ones(window_size)/float(window_size)\naapl_avg = np.convolve(aapl, window, 'same')\n\np2 = figure(x_axis_type=\"datetime\", title=\"AAPL One-Month Average\")\np2.grid.grid_line_alpha = 0\np2.xaxis.axis_label = 'Date'\np2.yaxis.axis_label = 'Price'\np2.ygrid.band_fill_color = \"olive\"\np2.ygrid.band_fill_alpha = 0.1\n\np2.circle(aapl_dates, aapl, size=4, legend_label='close',\n          color='darkgrey', alpha=0.2)\n\np2.line(aapl_dates, aapl_avg, legend_label='avg', color='navy')\np2.legend.location = \"top_left\"\n\n\nshow(gridplot([[p1,p2]], plot_width=400, plot_height=400)) # open a browser","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from bokeh.sampledata.periodic_table import elements\nfrom bokeh.transform import dodge, factor_cmap\n\n\nperiods = [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\"]\ngroups = [str(x) for x in range(1, 19)]\n\ndf = elements.copy()\ndf[\"atomic mass\"] = df[\"atomic mass\"].astype(str)\ndf[\"group\"] = df[\"group\"].astype(str)\ndf[\"period\"] = [periods[x-1] for x in df.period]\ndf = df[df.group != \"-\"]\ndf = df[df.symbol != \"Lr\"]\ndf = df[df.symbol != \"Lu\"]\n\ncmap = {\n    \"alkali metal\"         : \"#a6cee3\",\n    \"alkaline earth metal\" : \"#1f78b4\",\n    \"metal\"                : \"#d93b43\",\n    \"halogen\"              : \"#999d9a\",\n    \"metalloid\"            : \"#e08d49\",\n    \"noble gas\"            : \"#eaeaea\",\n    \"nonmetal\"             : \"#f1d4Af\",\n    \"transition metal\"     : \"#599d7A\",\n}\n\nTOOLTIPS = [\n    (\"Name\", \"@name\"),\n    (\"Atomic number\", \"@{atomic number}\"),\n    (\"Atomic mass\", \"@{atomic mass}\"),\n    (\"Type\", \"@metal\"),\n    (\"CPK color\", \"$color[hex, swatch]:CPK\"),\n    (\"Electronic configuration\", \"@{electronic configuration}\"),\n]\n\np = figure(title=\"Periodic Table (omitting LA and AC Series)\", plot_width=1000, plot_height=450,\n           x_range=groups, y_range=list(reversed(periods)),\n           tools=\"hover\", toolbar_location=None, tooltips=TOOLTIPS)\n\nr = p.rect(\"group\", \"period\", 0.95, 0.95, source=df, fill_alpha=0.6, legend_field=\"metal\",\n           color=factor_cmap('metal', palette=list(cmap.values()), factors=list(cmap.keys())))\n\ntext_props = {\"source\": df, \"text_align\": \"left\", \"text_baseline\": \"middle\"}\n\nx = dodge(\"group\", -0.4, range=p.x_range)\n\np.text(x=x, y=\"period\", text=\"symbol\", text_font_style=\"bold\", **text_props)\n\np.text(x=x, y=dodge(\"period\", 0.3, range=p.y_range), text=\"atomic number\",\n       text_font_size=\"8pt\", **text_props)\n\np.text(x=x, y=dodge(\"period\", -0.35, range=p.y_range), text=\"name\",\n       text_font_size=\"5pt\", **text_props)\n\np.text(x=x, y=dodge(\"period\", -0.2, range=p.y_range), text=\"atomic mass\",\n       text_font_size=\"5pt\", **text_props)\n\np.text(x=[\"3\", \"3\"], y=[\"VI\", \"VII\"], text=[\"LA\", \"AC\"], text_align=\"center\", text_baseline=\"middle\")\n\np.outline_line_color = None\np.grid.grid_line_color = None\np.axis.axis_line_color = None\np.axis.major_tick_line_color = None\np.axis.major_label_standoff = 0\np.legend.orientation = \"horizontal\"\np.legend.location =\"top_center\"\np.hover.renderers = [r] # only hover element boxes\n\nshow(p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install vega_datasets","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Altair \n\nAltair is a declarative statistical visualization library for Python, based on Vega and Vega-Lite.\n\nAltair offers a powerful and concise visualization grammar that enables you to build a wide range of statistical visualizations quickly. Here is an example of using the Altair API to quickly visualize a dataset with an interactive scatter plot,not just scatter plot there many like \n\n\n* sample chart \n* bar chart \n* line chart \n* area chart\n* scatter plots \n* Maps \n* case studies\n* interactive charts \n* histograms\n\nlets see each example one by one :)"},{"metadata":{},"cell_type":"markdown","source":"* sample cahrt : normaly this sample charts will be there is seaborns so we will see something new ..! \n\n* i know this is also a normal one but there are so intersting one lets see\n\n\n**there is something called vega dataset is lets use that**\n\n-------->> **Bar Chart with Negative Values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import altair as alt\nfrom vega_datasets import data\n\nsource = data.us_employment()\n\nalt.Chart(source).mark_bar().encode(\n    x=\"month:T\",\n    y=\"nonfarm_change:Q\",\n    color=alt.condition(\n        alt.datum.nonfarm_change > 0,\n        alt.value(\"steelblue\"),  # The positive color\n        alt.value(\"orange\")  # The negative color\n    )\n).properties(width=600)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-------->> **Stacked Bar Chart with Text Overlay**"},{"metadata":{"trusted":true},"cell_type":"code","source":"source=data.barley()\nbars = alt.Chart(source).mark_bar().encode(\n    x=alt.X('sum(yield):Q', stack='zero'),\n    y=alt.Y('variety:N'),\n    color=alt.Color('site')\n)\ntext = alt.Chart(source).mark_text(dx=-15, dy=3, color='white').encode(\n    x=alt.X('sum(yield):Q', stack='zero'),\n    y=alt.Y('variety:N'),\n    detail='site:N',\n    text=alt.Text('sum(yield):Q', format='.1f')\n)\nbars + text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lets see few in \n\n* line chart\n\n------>> **Line Chart with Layered Aggregates**"},{"metadata":{"trusted":true},"cell_type":"code","source":"source = data.stocks()\nbase = alt.Chart(source).properties(width=550)\nline = base.mark_line().encode(\n    x='date',\n    y='price',\n    color='symbol'\n)\nrule = base.mark_rule().encode(\n    y='average(price)',\n    color='symbol',\n    size=alt.value(2)\n)\nline + rule","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"------->> **Scatter Plot and Histogram with Interval Selection**"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.random.normal(size=100)\ny = np.random.normal(size=100)\n\nm = np.random.normal(15, 1, size=100)\n\nsource = pd.DataFrame({\"x\": x, \"y\":y, \"m\":m})\n\n# interval selection in the scatter plot\npts = alt.selection(type=\"interval\", encodings=[\"x\"])\n\n# left panel: scatter plot\npoints = alt.Chart().mark_point(filled=True, color=\"black\").encode(\n    x='x',\n    y='y'\n).transform_filter(\n    pts\n).properties(\n    width=300,\n    height=300\n)\n\n# right panel: histogram\nmag = alt.Chart().mark_bar().encode(\n    x='mbin:N',\n    y=\"count()\",\n    color=alt.condition(pts, alt.value(\"black\"), alt.value(\"lightgray\"))\n).properties(\n    width=300,\n    height=300\n).add_selection(pts)\n\n# build the chart:\nalt.hconcat(\n    points,\n    mag,\n    data=source\n).transform_bin(\n    \"mbin\",\n    field=\"m\",\n    bin=alt.Bin(maxbins=20)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Area chart \n\n----->>  **Faceted Density Estimates**        "},{"metadata":{"trusted":true},"cell_type":"code","source":"source = data.iris()\n\nalt.Chart(source).transform_fold(\n    ['petalWidth',\n     'petalLength',\n     'sepalWidth',\n     'sepalLength'],\n    as_ = ['Measurement_type', 'value']\n).transform_density(\n    density='value',\n    bandwidth=0.3,\n    groupby=['Measurement_type'],\n    extent= [0, 8]\n).mark_area().encode(\n    alt.X('value:Q'),\n    alt.Y('density:Q'),\n    alt.Row('Measurement_type:N')\n).properties(width=300, height=50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"------>> **Stacked Density Estimates**\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsource = data.iris()\n\nalt.Chart(source).transform_fold(\n    ['petalWidth',\n     'petalLength',\n     'sepalWidth',\n     'sepalLength'],\n    as_ = ['Measurement_type', 'value']\n).transform_density(\n    density='value',\n    bandwidth=0.3,\n    groupby=['Measurement_type'],\n    extent= [0, 8],\n    counts = True,\n    steps=200\n).mark_area().encode(\n    alt.X('value:Q'),\n    alt.Y('density:Q', stack='zero'),\n    alt.Color('Measurement_type:N')\n).properties(width=400, height=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Scatter Plots\n\n------->> **Scatter Plot with Rolling Mean**"},{"metadata":{"trusted":true},"cell_type":"code","source":"source = data.seattle_weather()\n\nline = alt.Chart(source).mark_line(\n    color='red',\n    size=3\n).transform_window(\n    rolling_mean='mean(temp_max)',\n    frame=[-15, 15]\n).encode(\n    x='date:T',\n    y='rolling_mean:Q'\n)\n\npoints = alt.Chart(source).mark_point().encode(\n    x='date:T',\n    y=alt.Y('temp_max:Q',\n            axis=alt.Axis(title='Max Temp'))\n)\n\npoints + line","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* maps \n\ni seen maps are more intersting here in this \n\n### im making only fwe example i will make one more kernal for full detials :) \n\nlets goo "},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsource = alt.topo_feature(data.world_110m.url, 'countries')\n\nbase = alt.Chart(source).mark_geoshape(\n    fill='#666666',\n    stroke='white'\n).properties(\n    width=300,\n    height=180\n)\n\nprojections = ['equirectangular', 'mercator', 'orthographic', 'gnomonic']\ncharts = [base.project(proj).properties(title=proj)\n          for proj in projections]\n\nalt.concat(*charts, columns=2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counties = alt.topo_feature(data.us_10m.url, 'counties')\nsource = data.unemployment.url\n\nalt.Chart(counties).mark_geoshape().encode(\n    color='rate:Q'\n).transform_lookup(\n    lookup='id',\n    from_=alt.LookupData(source, 'id', ['rate'])\n).project(\n    type='albersUsa'\n).properties(\n    width=500,\n    height=300\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airports = data.airports.url\nstates = alt.topo_feature(data.us_10m.url, feature='states')\n\n# US states background\nbackground = alt.Chart(states).mark_geoshape(\n    fill='lightgray',\n    stroke='white'\n).properties(\n    width=500,\n    height=300\n).project('albersUsa')\n\n# airport positions on background\npoints = alt.Chart(airports).transform_aggregate(\n    latitude='mean(latitude)',\n    longitude='mean(longitude)',\n    count='count()',\n    groupby=['state']\n).mark_circle().encode(\n    longitude='longitude:Q',\n    latitude='latitude:Q',\n    size=alt.Size('count:Q', title='Number of Airports'),\n    color=alt.value('steelblue'),\n    tooltip=['state:N','count:Q']\n).properties(\n    title='Number of airports in US'\n)\n\nbackground + points","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# this is just part 1 soon i will come up with part 2 :-)\n# in part i will explaining each libary in detials :-)\n# please upvote if you like your upvote can make me to work more and more :) \n# happy kaggling :)\n# thank you :-)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}