{"cells":[{"metadata":{"toc":true},"cell_type":"markdown","source":"<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Libraries\" data-toc-modified-id=\"Libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Libraries</a></span></li><li><span><a href=\"#Reading-and-Exploring-the-Data\" data-toc-modified-id=\"Reading-and-Exploring-the-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Reading and Exploring the Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Numerical-Features-Analysis\" data-toc-modified-id=\"Numerical-Features-Analysis-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Numerical Features Analysis</a></span></li><li><span><a href=\"#Categorical-Features-Analysis\" data-toc-modified-id=\"Categorical-Features-Analysis-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Categorical Features Analysis</a></span></li><li><span><a href=\"#Insights-from-Data\" data-toc-modified-id=\"Insights-from-Data-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Insights from Data</a></span></li><li><span><a href=\"#Clustering-Data\" data-toc-modified-id=\"Clustering-Data-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Clustering Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Age-and-Duration\" data-toc-modified-id=\"Age-and-Duration-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Age and Duration</a></span></li><li><span><a href=\"#Credit-Amount-and-Duration\" data-toc-modified-id=\"Credit-Amount-and-Duration-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>Credit Amount and Duration</a></span></li></ul></li></ul></li><li><span><a href=\"#Data-Prep\" data-toc-modified-id=\"Data-Prep-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Prep</a></span><ul class=\"toc-item\"><li><span><a href=\"#Preprocessing-Pipeline\" data-toc-modified-id=\"Preprocessing-Pipeline-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Preprocessing Pipeline</a></span></li><li><span><a href=\"#Processing-Pipeline\" data-toc-modified-id=\"Processing-Pipeline-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Processing Pipeline</a></span></li></ul></li><li><span><a href=\"#Training-and-Evaluating-a-Model\" data-toc-modified-id=\"Training-and-Evaluating-a-Model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Training and Evaluating a Model</a></span></li><li><span><a href=\"#Production-Script\" data-toc-modified-id=\"Production-Script-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Production Script</a></span></li></ul></div>"},{"metadata":{},"cell_type":"markdown","source":"Hi everyone! The _German Credit Risk_ dataset has been the first place I started my kaggle journey. As long as I've been studying Data Science and Machine Learning, I always saw Kaggle as one of the greates ways to learn more about everything related on these two topics. So here I am!"},{"metadata":{},"cell_type":"markdown","source":"**UPDATE:** I just finished my \"German Credit Risk Project\" with a DevOps approach for training a Machine Learning model and scoring production data received for a source. The idea is to simulate what we get in practice. So if you want to take a look at it, please visit my github at [ThiagoPaniniGithub](https://github.com/ThiagoPanini/kaggle_challenges/tree/master/kernels/01_german_credit_risk) - you will see that the project strucuture have a `/dev`, `/models`, `/pipelines`, `´/prod` and other folders, simulating what I consider an organized Machine Learning project for real. The scripts we can pay attention is the `train.py` (for reading the data, building pipelines and training a model) and the `score.py` (for receiving new data, applying the pipelines and scoring the data using the trained model).\n\nI really like you enjoy! At the end of this notebook you can see a little taste of it."},{"metadata":{},"cell_type":"markdown","source":"# Libraries"},{"metadata":{"ExecuteTime":{"start_time":"2020-07-01T00:12:59.022046Z","end_time":"2020-07-01T00:13:05.975826Z"},"trusted":true},"cell_type":"code","source":"# Stsandard libs\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom datetime import datetime\nimport time\n\n# Utilities\nfrom viz_utils import *\nfrom ml_utils import *\nfrom custom_transformers import *\n\n# Pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\n# Modeling\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score, cross_val_predict, \\\n                                    learning_curve\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, \\\n    accuracy_score, precision_score, recall_score, f1_score\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading and Exploring the Data"},{"metadata":{},"cell_type":"markdown","source":"Here we will answear the following questions:\n    - What's the content of data?\n    - How are the numerical attributes distributed?\n    - How are the categorical attributes distributed?\n    - What's the influence of Risk and Credit Amount in other features?\n    - Is there any other pattern that may be related to credit risk?"},{"metadata":{"ExecuteTime":{"start_time":"2020-07-01T00:13:05.980813Z","end_time":"2020-07-01T00:13:06.056489Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Data path\ndf_ori = import_data('../input/german-credit-data-with-risk/german_credit_data.csv', optimized=True)\ndf = df_ori.iloc[:, 1:]\ndf.columns = [col.lower().strip().replace(' ', '_') for col in df.columns]\n\n# Results\nprint(f'Data dimension: {df.shape}')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"___\n* _Target class balance_\n___"},{"metadata":{"ExecuteTime":{"start_time":"2020-07-01T00:13:06.062157Z","end_time":"2020-07-01T00:13:06.255685Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Target class balance\nfig, ax = plt.subplots(figsize=(7, 7))\nlabel_names = ['Good', 'Bad']\ncolor_list = ['navy', 'mediumvioletred']\ntext = f'Total\\n{len(df_ori)}'\ntitle = 'Target Class Balance'\n\n# Visualizing it through a donut chart\ndonut_plot(df, col='risk', ax=ax, label_names=label_names, colors=color_list, title=title, text=text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"An overview of the data."},{"metadata":{"ExecuteTime":{"start_time":"2020-07-01T00:13:06.257682Z","end_time":"2020-07-01T00:13:06.318341Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Overview from the data\ndf_overview = data_overview(df)\ndf_overview","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Numerical Features Analysis"},{"metadata":{},"cell_type":"markdown","source":"___\n* _Distribution_\n___"},{"metadata":{"ExecuteTime":{"start_time":"2020-07-01T00:13:06.320269Z","end_time":"2020-07-01T00:13:09.305128Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"num_cols = ['age', 'credit_amount', 'duration']\ncolor_sequence = ['navy', 'mediumseagreen', 'navy']\nnumplot_analysis(df[num_cols], color_sequence=color_sequence, hist=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"___\n* _Distribution by risk_\n___"},{"metadata":{"ExecuteTime":{"start_time":"2020-07-01T00:13:09.30879Z","end_time":"2020-07-01T00:13:11.497759Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"num_cols += ['risk']\nnumplot_analysis(df[num_cols], hue='risk', color_hue=color_list)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2020-07-01T00:13:11.500925Z","end_time":"2020-07-01T00:13:13.573284Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"boxenplot(df, features=['age', 'credit_amount', 'duration'], hue='risk', fig_cols=3, figsize=(15, 5), \n          palette=color_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By looking at the the plots above, it's reasonble to point that:\n\n* **Young people tend to present more risk than the older ones**\n    * To me this statement makes sense because maybe older people usually have more financial stability than the younger ones;\n    * By the perspective of a bank or a loan establishment, probably it's safer to offer better credit programs to people who has condition to pay back.\n    \n    \n* **Higher credit amount presents more risk than lower ones**\n    * Well, this is kind of intuitive and makes total sense to me;\n    * By the perspective of a customer who took a loan or entered a credit program, it's easier to pay back a low amount.\n    \n    \n* **Higher duration is related to a higher risk**\n    * Once again this is perfectly fine to say;\n    * The common credit programs are based on interest or fees that gets higher through the time;\n    * A costumer who took money for a long time will have to pay much more by the end."},{"metadata":{},"cell_type":"markdown","source":"## Categorical Features Analysis"},{"metadata":{},"cell_type":"markdown","source":"___\n* _Quantity for each category_\n___"},{"metadata":{"ExecuteTime":{"start_time":"2020-07-01T00:13:13.577298Z","end_time":"2020-07-01T00:13:18.242953Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cat_features = [col for col, dtype in df.dtypes.items() if dtype == 'object']\ncatplot_analysis(df[cat_features], palette='plasma')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These categorical analysis provided an overview of our dataset in terms of quantity. For each categorical column, now is possible to know what are the marjority ones. Meanwhile, to give more power to the analysis, let's separate each categorical entry into the risk approach."},{"metadata":{},"cell_type":"markdown","source":"___\n* _Quantity for each category by risk_\n___"},{"metadata":{"ExecuteTime":{"start_time":"2020-07-01T00:13:18.249931Z","end_time":"2020-07-01T00:13:22.082453Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"catplot_analysis(df[cat_features], hue='risk', palette=color_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On the first look, maybe the main category that demands some attention if `checking_account`. If we look at those ones with _little_ checking account and its distribution by risk, it's almost 50/50. To make it cleaner for us to conclude, let's repeat this analysis but in a percentage approach."},{"metadata":{"ExecuteTime":{"start_time":"2020-07-01T00:13:22.088723Z","end_time":"2020-07-01T00:13:25.639683Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"rev_color_list = ['mediumvioletred', 'navy']\ncatplot_percentage_analysis(df[cat_features], hue='risk', palette=rev_color_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The insights we can take from the plot above are:\n\n* People with `little` saving account and checking account presents higher risk;\n* When customers take some credit for `vacation/others`, it's a sign that the risk could be high;\n* People who have their own house have lower risk"},{"metadata":{},"cell_type":"markdown","source":"## Insights from Data"},{"metadata":{},"cell_type":"markdown","source":"In this session we will go deeper into the dataset to make some graphical analysis in order to take valuable insights that could be used on understanding the problem or even on training a model."},{"metadata":{},"cell_type":"markdown","source":"___\n* _For what purpose people tend to apply for higher amount loans in average? In terms of total amount, what's the main purpose?_\n___"},{"metadata":{"ExecuteTime":{"start_time":"2020-07-01T00:13:25.642677Z","end_time":"2020-07-01T00:13:27.192704Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"mean_sum_analysis(df, group_col='purpose', value_col='credit_amount')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We could see by the graphic that, in average, people often request higher credit amount for `vacation/other`. Meanwhile this purpose is only the 6th in terms of total amount given by the loan establishment.\n\nBy the way, people who request credit for `car` purpose representes the main public for the bank/loan establishment."},{"metadata":{},"cell_type":"markdown","source":"___\n* _Is there any purpose in particular with long duration credit requests?_\n___"},{"metadata":{"ExecuteTime":{"start_time":"2020-07-01T00:13:27.202347Z","end_time":"2020-07-01T00:13:29.01606Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"mean_sum_analysis(df, group_col='purpose', value_col='duration')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"___\n* _How credit amount is related to gender?_\n___"},{"metadata":{"ExecuteTime":{"start_time":"2020-07-01T00:13:29.020576Z","end_time":"2020-07-01T00:13:29.607709Z"},"code_folding":[],"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"gender_palette = ['cornflowerblue', 'salmon']\nmean_sum_analysis(df, group_col='sex', value_col='credit_amount', orient='horizontal', \n                  palette=gender_palette, figsize=(12, 4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"___\n* _Is there any correlation between features?_\n___"},{"metadata":{},"cell_type":"markdown","source":"For this let's use the `pairplot` function from `seaborn`."},{"metadata":{"ExecuteTime":{"start_time":"2020-07-01T00:13:29.609707Z","end_time":"2020-07-01T00:13:38.81555Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sns.pairplot(df[num_cols], hue='risk', palette=color_list)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The scatterplots built into the `pairplot` function show us that maybe there is a positive correlation between `credit amount` and `duration` of the credit request. It makes sense, since long term credits could be related to higher amounts."},{"metadata":{},"cell_type":"markdown","source":"___\n* _How much credit was given for bad risk customers?_\n___"},{"metadata":{},"cell_type":"markdown","source":"Just to clarify, this is a really important question by the way. The answear we search here is to see the profile of the bank/credit stablishment in terms of risk."},{"metadata":{"ExecuteTime":{"start_time":"2020-07-01T00:13:38.817576Z","end_time":"2020-07-01T00:13:38.947788Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"amount_risk = df.groupby(by='risk', as_index=False).sum().loc[:, ['risk', 'credit_amount']]\namount_risk['percentage'] = amount_risk['credit_amount'] / amount_risk['credit_amount'].sum()\namount_risk.style.background_gradient(cmap='Reds_r')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2020-07-01T00:13:38.950167Z","end_time":"2020-07-01T00:13:39.228564Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Creating figure\nfig, ax = plt.subplots(figsize=(7, 7))\n\n# Defining useful elements for the donut chart\nvalues = amount_risk['credit_amount']\nlabels = amount_risk['risk']\ncenter_circle = plt.Circle((0, 0), 0.8, color='white')\n\n# Plotting the pizza chart and the center circle\nax.pie(values, labels=labels, colors=['darkred', 'cadetblue'], autopct=make_autopct(values))\nax.add_artist(center_circle)\n\nkwargs = dict(size=20, fontweight='bold', va='center')\nax.text(0, 0, f'Total Amount\\n${values.sum()}', ha='center', **kwargs)\nax.set_title('Credit Amount Made Available to Customers by Risk', size=14, color='dimgrey')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"___\n* _Analysing how housing and job attributes is related to age and credit amount_\n___"},{"metadata":{"ExecuteTime":{"start_time":"2020-07-01T00:13:39.230562Z","end_time":"2020-07-01T00:13:41.146464Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Creating figure\nfig, axs = plt.subplots(ncols=2, figsize=(15, 5))\n\n# Scatterplot\nsns.scatterplot(x='age', y='credit_amount', data=df, hue='housing', ax=axs[0], palette='magma', alpha=.8)\nsns.scatterplot(x='age', y='credit_amount', data=df, hue='job', ax=axs[1], palette='YlGnBu')\n\n# Customizing plot\nformat_spines(axs[0], right_border=False)\nformat_spines(axs[1], right_border=False)\naxs[0].set_title('Credit Amomunt and Age Distribution by Housing', size=12, color='dimgrey')\naxs[1].set_title('Credit Amomunt and Age Distribution by Job', size=12, color='dimgrey')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"___\n* _How the duration of loan requests are associated with age and credit amount?_\n___"},{"metadata":{"ExecuteTime":{"start_time":"2020-07-01T00:13:43.405875Z","end_time":"2020-07-01T00:13:45.563589Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"g = (sns.jointplot(x='credit_amount', y='duration', data=df, color='seagreen', kind='hex'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create bins for duration and see its behavior along a age and credit amount scatterplot"},{"metadata":{"ExecuteTime":{"start_time":"2020-07-01T00:13:45.564586Z","end_time":"2020-07-01T00:13:46.301868Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Creating new categories for duration col\nbins = [0, 10, 30, 50, np.inf]\nlabels = ['<= 10', 'between 10 and 30', 'between 30 and 50', '> 50']\ndf['cat_duration'] = pd.cut(df['duration'], bins=bins, labels=labels)\n\n# Creating figure\nfig, ax = plt.subplots(figsize=(10, 5))\n\n# Scatterplot\nsns.scatterplot(x='age', y='credit_amount', data=df, hue='cat_duration', palette='YlGnBu')\n\n# Customizing plot\nformat_spines(ax, right_border=False)\nax.set_title('Credit Amomunt and Age Distribution by Duration Category', size=14, color='dimgrey')\nax.legend(loc='upper right', fancybox=False, framealpha=0.2)\ndf.drop('cat_duration', axis=1, inplace=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Clustering Data"},{"metadata":{},"cell_type":"markdown","source":"In this session, we will select some useful features to split the data into different clusters. The aim is to present a better way to understanding the relationship between features and to study how our public could be splitted into clusters."},{"metadata":{},"cell_type":"markdown","source":"### Age and Duration"},{"metadata":{"ExecuteTime":{"start_time":"2020-07-05T15:26:12.507807Z","end_time":"2020-07-05T15:26:13.69069Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Definindo melhor número de clusters\ncolumns = ['age', 'duration']\ncluster_data = df.loc[:, columns]\nK_min, K_max = 1, 8\nelbow_method_kmeans(cluster_data, K_min, K_max)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2020-07-05T15:31:16.041789Z","end_time":"2020-07-05T15:31:16.651233Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Treinando algoritmo KMeans\nk_means = KMeans(n_clusters=3)\nk_means.fit(cluster_data)\ny_kmeans = k_means.predict(cluster_data)\ncenters = k_means.cluster_centers_\n\n# Plotando resultado do agrupamento\nplot_kmeans_clusters(cluster_data, y_kmeans, centers, cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's really interesting to see the three main group on the data when we look at `age` and `duration` features. To present you some description with that I thought on:\n\n* **Group 1 (purple):** Here we have young people (from 18 to 40 years approximately) who usually request short term loans (from 0 to 30 years approximately).\n\n* **Group 2 (yellow):** On the cluster above the others, we have people who take loans for a long time (higher than 30 years).\n\n* **Group 3 (green):** The third cluster have older people (with age higher than 40) and short term loans."},{"metadata":{},"cell_type":"markdown","source":"### Credit Amount and Duration"},{"metadata":{"ExecuteTime":{"start_time":"2020-07-05T15:39:11.091088Z","end_time":"2020-07-05T15:39:12.082594Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Definindo melhor número de clusters\ncolumns = ['credit_amount', 'duration']\ncluster_data = df.loc[:, columns]\nK_min, K_max = 1, 8\nelbow_method_kmeans(cluster_data, K_min, K_max)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2020-07-05T15:39:17.17746Z","end_time":"2020-07-05T15:39:17.811945Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Treinando algoritmo KMeans\nk_means = KMeans(n_clusters=2)\nk_means.fit(cluster_data)\ny_kmeans = k_means.predict(cluster_data)\ncenters = k_means.cluster_centers_\n\n# Plotando resultado do agrupamento\nplot_kmeans_clusters(cluster_data, y_kmeans, centers, cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are using the `duration` and `credit_amount` features. The elbow method showed us that 2 is optimal number of clusters that we can get between these two features. So we have:\n\n* **Group 1 (purple):** On this cluster we have customers who take lower amount for credit for a middle term duration (between 0 and 50 years).\n\n* **Group 2 (yellow):** This cluster shows us customers that request higher amount for credit for a usually higher duration."},{"metadata":{},"cell_type":"markdown","source":"# Data Prep"},{"metadata":{},"cell_type":"markdown","source":"After a pool of visualizations build in order to help us to get insights from the data, we can now safely go through the steps needed to prepare our data to a predictive model.\n\nFor this session, I prepared a utility model called `custom_transformers.py` with some classes already built right for this task."},{"metadata":{"ExecuteTime":{"start_time":"2020-07-01T00:13:46.303804Z","end_time":"2020-07-01T00:13:46.333104Z"},"trusted":true},"cell_type":"code","source":"# Creating a target column\ndf['target'] = df['risk'].apply(lambda x: 1 if x == 'bad' else 0)\ndf.drop('risk', axis=1, inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing Pipeline"},{"metadata":{},"cell_type":"markdown","source":"In this topic, I purpose the application of a preprocessing Pipeline to:\n\n    - Remove duplicated instances;\n    - Splitting data into train and test sets."},{"metadata":{"ExecuteTime":{"start_time":"2020-07-01T00:13:46.33599Z","end_time":"2020-07-01T00:13:46.356508Z"},"trusted":true},"cell_type":"code","source":"# Building the preprocessing Pipeline\npreprocessing_pipeline = Pipeline([\n    ('dup_dropped', DropDuplicates()),\n    ('data_splitter', SplitData(target='target'))\n])\n\n# Applying this pipeline\nX_train, X_test, y_train, y_test = preprocessing_pipeline.fit_transform(df)\nprint(f'Shape of X_train: {X_train.shape}')\nprint(f'Shape of X_test: {X_test.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Processing Pipeline"},{"metadata":{},"cell_type":"markdown","source":"In this step, the point is to construct pipelines for numerical and categorical data so we can:\n\n**Numerical pipeline:**\n    - Filling null data (if exists)\n    - Scalling with StandardScaler\n    \n**Categorical pipeline:**\n    - Filling null data (if exists - and we already know that it exists)\n    - Apply encoding (with get_dummies)"},{"metadata":{"ExecuteTime":{"start_time":"2020-07-01T00:13:46.365813Z","end_time":"2020-07-01T00:13:46.381772Z"},"trusted":true},"cell_type":"code","source":"# Splitting the data by dtype\nnum_features = [col for col, dtype in X_train.dtypes.items() if dtype != 'object']\ncat_features = [col for col, dtype in X_train.dtypes.items() if dtype == 'object']\n\n# Building a numerical pipeline\nnum_pipeline = Pipeline([\n    ('scaler', StandardScaler())\n])\n\n# Building a categorical pipeline\ncat_pipeline = Pipeline([\n    ('encoder', DummiesEncoding(dummy_na=True))\n])\n\n# Building a complete pipeline\nfull_pipeline = ColumnTransformer([\n    ('num', num_pipeline, num_features),\n    ('cat', cat_pipeline, cat_features)\n])","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2020-07-01T00:13:46.389782Z","end_time":"2020-07-01T00:13:46.472744Z"},"trusted":true},"cell_type":"code","source":"# Applying the data prep pipeline\nX_train_prep = full_pipeline.fit_transform(X_train)\nX_test_prep = full_pipeline.fit_transform(X_test)\n\nprint(f'Shape of X_train_prep: {X_train_prep.shape}')\nprint(f'Shape of X_test_prep: {X_test_prep.shape}')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2020-07-01T00:13:46.47519Z","end_time":"2020-07-01T00:13:46.533489Z"},"trusted":true},"cell_type":"code","source":"# Returning the final features of the dataset\nencoded_features = full_pipeline.named_transformers_['cat']['encoder'].features_after_encoding\nmodel_features = num_features + encoded_features\ndf_train_prep = pd.DataFrame(X_train_prep, columns=model_features)\ndf_train_prep.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training and Evaluating a Model"},{"metadata":{},"cell_type":"markdown","source":"It seems we finally have a final dataset to analyze a classifying model. Our goal here is to use the features we prepared to predict the loan risk and, for that, we will go through evaluation metrics, ROC Curve, Confusion Matrix and other tools to show how good our models really are."},{"metadata":{},"cell_type":"markdown","source":"First let's setup all the hyperparmeters for the classifiers we pretend to evaluate"},{"metadata":{"ExecuteTime":{"start_time":"2020-07-01T00:13:46.536481Z","end_time":"2020-07-01T00:13:46.557505Z"},"trusted":true},"cell_type":"code","source":"# Logistic Regression hyperparameters\nlogreg_param_grid = {\n    'C': np.linspace(0.1, 10, 20),\n    'penalty': ['l1', 'l2'],\n    'class_weight': ['balanced', None],\n    'random_state': [42],\n    'solver': ['liblinear']\n}\n\n# Decision Trees hyperparameters\ntree_param_grid = {\n    'criterion': ['entropy', 'gini'],\n    'max_depth': [3, 5, 10, 20],\n    'max_features': np.arange(1, X_train.shape[1]),\n    'class_weight': ['balanced', None],\n    'random_state': [42]\n}\n\n# Random Forest hyperparameters\nforest_param_grid = {\n    'bootstrap': [True, False],\n    'max_depth': [3, 5, 10, 20, 50],\n    'n_estimators': [50, 100, 200, 500],\n    'random_state': [42],\n    'max_features': ['auto', 'sqrt'],\n    'class_weight': ['balanced', None]\n}\n\n# LightGBM hyperparameters\nlgbm_param_grid = {\n    'num_leaves': list(range(8, 92, 4)),\n    'min_data_in_leaf': [10, 20, 40, 60, 100],\n    'max_depth': [3, 4, 5, 6, 8, 12, 16],\n    'learning_rate': [0.1, 0.05, 0.01, 0.005],\n    'bagging_freq': [3, 4, 5, 6, 7],\n    'bagging_fraction': np.linspace(0.6, 0.95, 10),\n    'reg_alpha': np.linspace(0.1, 0.95, 10),\n    'reg_lambda': np.linspace(0.1, 0.95, 10),\n}\n\nlgbm_fixed_params = {\n    'application': 'binary',\n    'objective': 'binary',\n    'metric': 'auc',\n    'is_unbalance': 'true',\n    'boosting_type': 'gbdt',\n    'num_leaves': 31,\n    'feature_fraction': 0.5,\n    'bagging_fraction': 0.5,\n    'bagging_freq': 20,\n    'learning_rate': 0.05,\n    'verbose': 0\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At least, let's prepare a dictionary with the instances of our classifiers and the respective hyperparameters"},{"metadata":{"ExecuteTime":{"start_time":"2020-07-01T00:13:46.559502Z","end_time":"2020-07-01T00:13:46.568988Z"},"trusted":true},"cell_type":"code","source":"# Setting up classifiers\nset_classifiers = {\n    'LogisticRegression': {\n        'model': LogisticRegression(),\n        'params': logreg_param_grid\n    },\n    'DecisionTrees': {\n        'model': DecisionTreeClassifier(),\n        'params': tree_param_grid\n    },\n    'RandomForest': {\n        'model': RandomForestClassifier(),\n        'params': forest_param_grid\n    },\n    'LightGBM': {\n        'model': lgb.LGBMClassifier(**lgbm_fixed_params),\n        'params': lgbm_param_grid\n    }\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"___\n* _Training and evaluating classifiers_\n___"},{"metadata":{"ExecuteTime":{"start_time":"2020-07-01T00:13:46.57198Z","end_time":"2020-07-01T00:14:06.370876Z"},"trusted":true},"cell_type":"code","source":"# Creating an instance for the homemade class BinaryClassifiersAnalysis\nclf_tool = BinaryClassifiersAnalysis()\nclf_tool.fit(set_classifiers, X_train_prep, y_train, random_search=True, cv=5, verbose=5)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2020-07-01T00:14:06.373896Z","end_time":"2020-07-01T00:14:35.04338Z"},"trusted":true},"cell_type":"code","source":"# Evaluating metrics\ndf_performances = clf_tool.evaluate_performance(X_train_prep, y_train, X_test_prep, y_test, cv=5)\ndf_performances.reset_index(drop=True).style.background_gradient(cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"___\n* _Looking at the feature importance of a specific model_\n___"},{"metadata":{"ExecuteTime":{"start_time":"2020-07-03T01:22:37.769918Z","end_time":"2020-07-03T01:22:38.570874Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(13, 11))\nlgbm_feature_importance = clf_tool.feature_importance_analysis(model_features, specific_model='LightGBM', ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"___\n* _Plotting the ROC Curve for the classifiers (train and test)_\n___"},{"metadata":{"ExecuteTime":{"start_time":"2020-07-03T01:22:38.572867Z","end_time":"2020-07-03T01:22:39.568145Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"clf_tool.plot_roc_curve()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"___\n* _Plotting Confusion Matrix for each classifier (train and test)_\n___"},{"metadata":{"ExecuteTime":{"start_time":"2020-07-03T01:22:39.57014Z","end_time":"2020-07-03T01:22:46.219948Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"clf_tool.plot_confusion_matrix(classes=['Good', 'Bad'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"___\n* _Learning curve for a specific model_\n___"},{"metadata":{"ExecuteTime":{"start_time":"2020-07-04T00:06:13.619885Z","end_time":"2020-07-04T00:06:17.196544Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12, 6))\nclf_tool.plot_learning_curve('LightGBM', ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"___\n* _Looking at score (proba) distribution for a specific model_\n___"},{"metadata":{"ExecuteTime":{"start_time":"2020-07-04T00:06:17.199502Z","end_time":"2020-07-04T00:06:17.644083Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"clf_tool.plot_score_distribution('LightGBM', shade=True)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2020-07-04T00:06:24.108077Z","end_time":"2020-07-04T00:06:25.421521Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"clf_tool.plot_score_bins('LightGBM', bin_range=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By looking at the graphs and the model results, it seems we've done a good work on preparing and training the data. By the end, the model LightGBM showd us the best performance among the classifiers used on analysis. Meanwhile, the model **learning curve** shows us that there is space for improvement: the high gap between training scores and validation scores is a proxy that the model could is overfitting the data and so we could work on that to raise up the validation score metric.\n\nFor the next steps, thinking on putting this model on production, we could encapsulate every step in a pipeline and build a python script to receive some entrance base and return the instance score or the instance prediction based on the business problem."},{"metadata":{},"cell_type":"markdown","source":"# Production Script"},{"metadata":{},"cell_type":"markdown","source":"Let's imagine that we have already trained and selected the best model for this task. After that, we receive new data for scoring and we want to use our built pipeline to make this process easier. So, considering the data we have is the same we would use for scoring, the script could be constructed as:"},{"metadata":{"ExecuteTime":{"start_time":"2020-07-07T16:39:41.955994Z","end_time":"2020-07-07T16:39:42.080588Z"},"trusted":true},"cell_type":"code","source":"# Libs\nimport pandas as pd\nfrom custom_transformers import *\nfrom sklearn.pipeline import Pipeline\n\n# Reading the raw data\ndf_ori = import_data('../input/german-credit-data-with-risk/german_credit_data.csv', optimized=True)\ndf = df_ori.iloc[:, 1:]\ndf.columns = [col.lower().strip().replace(' ', '_') for col in df.columns]\ndf['target'] = df['risk'].apply(lambda x: 1 if x == 'bad' else 0)\ndf.drop('risk', axis=1, inplace=True)\n\n# Applying the data prep pipeline (the pkl file could be read from a specific path)\nscoring_data = full_pipeline.fit_transform(df)\n\n# Using the trained model for predicting (the pkl file could be read from a specific path)\nmodel = clf_tool.classifiers_info['LightGBM']['estimator']\ny_pred = model.predict(scoring_data)\ny_score = model.predict_proba(scoring_data)[:, 1]\n\n# Appending the predictions to the data\ndf['y_score'] = y_score\ndf['y_pred'] = y_pred\ndf['y_class'] = df['y_pred'].apply(lambda x: 'Bad' if x == 1 else 'Good')\n\n# Creating bins\nbins = df['y_score'].quantile(np.arange(0, 1.01, 0.1)).values\nlabels = ['Faixa ' + str(i) for i in range(len(bins)-1, 0, -1)]\ndf['faixa'] = pd.cut(df['y_score'], bins=bins, labels=labels, include_lowest=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"black\"><b>Please visit my other kernels by clicking on the buttons</b></font><br>\n\n<a href=\"https://www.kaggle.com/thiagopanini/predicting-restaurant-s-rate-in-bengaluru\" class=\"btn btn-primary\" style=\"color:white;\">Bengaluru's Restaurants</a>\n<a href=\"https://www.kaggle.com/thiagopanini/sentimental-analysis-on-e-commerce-reviews\" class=\"btn btn-primary\" style=\"color:white;\">Sentimental Analysis E-Commerce</a>\n<a href=\"https://www.kaggle.com/thiagopanini/credit-fraud-how-to-choose-the-best-classifier\" class=\"btn btn-primary\" style=\"color:white;\">Credit Card Fraud Detection</a>\n<a href=\"https://www.kaggle.com/thiagopanini/global-terrorism-eda-nlp\" class=\"btn btn-primary\" style=\"color:white;\">Global Terrorism</a>"},{"metadata":{},"cell_type":"markdown","source":"If you like this kernel, please **upvote**!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}