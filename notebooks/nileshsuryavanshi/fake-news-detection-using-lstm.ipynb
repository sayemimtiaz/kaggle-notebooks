{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **<font color='#7868e6'>Fake news detection</font>**\n\nIn this notebook, we are going to work on a dataset which consists of 3988 news articles. We will do some text processing and will build a deep learning model which will try to predict whether a particular news is fake or not. This is a Natural Language Processing problem, where we deal with text data and convert texts into some vectors it means a word will be defined as a vector. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# libraries for data manipulation and exploration\nimport pandas as pd\nimport numpy as np\n\n# for text processing\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# libraries for data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading the dataset\nfile_path = '../input/fake-news-detection/data.csv'\ndata = pd.read_csv(file_path)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking the dimension of dataset\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **<font color='#7868e6'>Data description</font>**\nThis dataset contains 4009 rows and 4 columns, here we will see the what each column describe.\n- `URLs` : URL of the news\n- `Headline` : Headline of the news\n- `Body` : News description\n- `Label` : **0** for fake news and **1** for true news"},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking if there is any null value\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are few null values in `Body` and `Website` columns, so we can remove those rows because we are dropping only 21 rows, and not loosing much information."},{"metadata":{},"cell_type":"markdown","source":"### **<font color='#7868e6'>Analysis</font>**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping rows where there is any null value\ndata.dropna(inplace=True, axis=0)\n\n# taking only the name of website from the URLs\npattern = 'https?://([\\w.]+)/'\ndata['Website'] = data.URLs.str.extract(pattern)\ndata.drop('URLs', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# making a bar plot to count the frequency of \n# fake and real news\nsns.countplot(x='Label', \n              data=data,\n              palette=['#ffaec0', '#b8b5ff'],\n              saturation=1)\nsns.despine()\nplt.xticks([0,1], ['Fake', 'Real'])\nplt.title('Fake Vs Real news');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In our dataset, we have more number of fake news as compare to real news. However, there is no huge difference in the ratio so the dataset is somewhat unbalanced and it is fine. \n\nWe can start working on it."},{"metadata":{},"cell_type":"markdown","source":"Let's see which websites contain more number of real or fake news."},{"metadata":{"trusted":true},"cell_type":"code","source":"fake_news_count = data[data.Label == 0]['Website'].value_counts()\nreal_news_count = data[data.Label == 1]['Website'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fdf = pd.DataFrame({\n    'Web':fake_news_count.index,\n    'Fake':fake_news_count.values\n})\n\nrdf = pd.DataFrame({\n    'Web':real_news_count.index,\n    'Real':real_news_count.values\n})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_count = pd.merge(rdf, fdf, on='Web', how='outer').fillna(0)\nrf_count['Real'] = rf_count['Real'].astype(int)\nrf_count['Fake'] = rf_count['Fake'].astype(int)\nrf_count","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we observed that websites are only posting either fake news or real news but not both of them means one website is only posting one type of news either fake or real but not both. \n\nMost of the websites are posting real news, only four websites are there posting fake news.\n\nSo, we want to see the following:\n- Top 5 websites posting real news\n- Top 5 websites posting fake news"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(y=real_news_count[:5].index, x=real_news_count[:5].values, \n            palette=['#7868e6', '#b8b5ff', '#ffaec0', 'grey', '#a7c5eb'])\nsns.despine(bottom=True, left=True)\nplt.title('Top 5 websites posting Real News');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(y=fake_news_count.index, x=fake_news_count.values,\n            palette=['#7868e6', '#b8b5ff', '#ffaec0', 'grey'])\nsns.despine(bottom=True, left=True)\nplt.title('Top websites posting Fake News');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **<font color='#7868e6'>Text cleaning</font>**\nHere, we will remove the unwanted words such as puntuation marks and stop words like - the, is, there, his etc. beacause these words are more frequent and don't have much importance. So, by removing these words we will get important words which will help us to find patterns regarding fake and real news.\n\n **Steps for text cleaning :**\n- convert each word to lower case\n- remove any words apart from alphabets\n- remove stop words\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"lm = WordNetLemmatizer()\ndef clean_text(column):\n    corpus = []\n    for value in column:\n        # convert each word into lower case\n        value = value.lower()\n        # take only alphabets\n        value = re.sub('[^a-z]', ' ', value)\n        value = value.split()\n        # lemmatizing those words which are not stop words\n        value = [lm.lemmatize(word) for word in value\\\n                 if word not in set(stopwords.words('english'))]\n        corpus.append(' '.join(value))\n    return corpus    \n\n# taking both headline and body of news into a single column\ndata['Text'] = data['Headline'] + ' ' + data['Body']\ndata['Text'] = clean_text(data.Text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After cleaning we will try to know about the most frequent words come in real and fake news. To get this thing done, we can use a cloud of words which shows words frequency on the basis of their size means words with bigger fontsize appear mostly as compare to other.\n\nWe have combined both **Headline** and **Body** column."},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nsw = set(STOPWORDS)\n\nfake_news = data[data.Label == 0]\nreal_news = data[data.Label == 1]\n\n# wordcloud for fake news\nfake_wc = WordCloud(width = 1200, height = 600, \n                    background_color ='white', \n                    stopwords = sw, \n                    min_font_size = 10).generate(' '.join(fake_news.Text)) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (12, 6), facecolor = None) \nplt.imshow(fake_wc) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.title('Word Cloud for fake news', fontsize=20);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# wordcloud for real news\nreal_wc = WordCloud(width = 1200, height = 600, \n                    background_color ='white', \n                    stopwords = sw, \n                    min_font_size = 10).generate(' '.join(real_news.Text)) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (12, 6), facecolor = None) \nplt.imshow(real_wc) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.title('Word Cloud for real news', fontsize=20);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **<font color='#7868e6'>Modeling</font>**\nHere, we are going to build a deep learning model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM, Dense","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one hot representation of words\noh_repr = [one_hot(words, 5000) for words in data.Text]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list to store length of words in each news\nlen_list = []\nfor w in data.Text:\n    w = w.split()\n    len_list.append(len(w))\n\nprint('Summary of word length :')\npd.Series(len_list).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# taking sentences length as 400\nsent_length = 400\n# padding\nembedded_doc = pad_sequences(oh_repr, padding='pre', maxlen=sent_length)\nembedded_doc[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initializing model\nmodel = Sequential()\n# adding embedding layer\nmodel.add(Embedding(5000, 100, input_length=sent_length))\nmodel.add(LSTM(150))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx = np.array(embedded_doc)\ny = data.Label\n\n# splitting the dataset into train and test\ntrain_x, test_x, train_y, test_y = train_test_split(x, y, \n                                                    test_size=0.25, random_state=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fitting the model\nhistory = model.fit(train_x, train_y, validation_data=(test_x, test_y), epochs=10, batch_size=64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score\npred = model.predict_classes(test_x);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting graph for confusion matrix\ncm = confusion_matrix(test_y, pred)\nplt.figure(figsize=(5,5))\nsns.heatmap(cm, annot=True, fmt='', cbar=False, linewidths=2,\n            xticklabels = ['Fake','Real'], yticklabels = ['Fake','Real'])\nplt.title('confusion matrix')\nplt.xlabel('Predicted', color='navy', fontsize=15)\nplt.ylabel('Actual', color='navy', fontsize=15);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# accuracy of the model we built\naccuracy_score(test_y, pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, we built a LSTM model with 97% accuracy which is quit amazing. We can try other parameters in order to increase the accuracy."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}