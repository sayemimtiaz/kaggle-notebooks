{"cells":[{"metadata":{"_uuid":"b8164d2a8a23f9cbc0bc79b20322c3aafe98960f"},"cell_type":"markdown","source":"# House Sales in King County"},{"metadata":{"_uuid":"0b295dad4e6c2ee656c3a4303dbd4dbe788360c3"},"cell_type":"markdown","source":"In this project, I am analyzing a dataset of about 21,000 house sales in King County (Seattle) over the course of a year (in 2014/15). The data are  taken from a [Kaggle competition](https://www.kaggle.com/harlfoxem/housesalesprediction). The goal is to build a model predicting sales prices.\n\nIn the first section, I will explore the correlation between the different features and the target variable (price) and visualize the data. In the second part, I will build a predictive regression model. In order to do so, I will use different regression algorithms and various different feature selection and engineering techniques. The best model is linear regression with data preprocessed by creating quadratic polynomial features. This model has an accuracy of 0.82 (R2 score).\n\n[1. Exploratory Data Analysis](#eda)<br>\n[2. Predictive Model for House Prices](#model)<br>\n[2.1. Simple Model: Linear Regression](#simple)<br>\n[2.2. Feature Selection](#sel)<br>\n[2.3. Different Regression Algorithms](#alg)<br>\n[2.4. Feature Engineering](#fe)<br>\n[2.5. Summary of Models](#25)<br>\n[3. Visualization on a Map](#map)<br>"},{"metadata":{"trusted":false,"_uuid":"3ff88e69f93d5beff578710df0d5f60f73e65c23"},"cell_type":"code","source":"from IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:100% !important; }</style>\"))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"9da5ef6d74f652aea0a3f92c59dcf1cdd8446a43"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d8c8fd3e10b46f1d0eb86703c5149ba2bb2466a"},"cell_type":"markdown","source":"First, the dataset is loaded from a csv file and the date column is converted from string to a timestamp."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"32a888b0bb531ff822fac799e263c5091085043c"},"cell_type":"code","source":"df = pd.read_csv('../input/kc_house_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"90b736aafff22f3bd0da99d3aa31b44ec80b4310"},"cell_type":"code","source":"# Convert date to timestamp\ndf['date'] = pd.to_datetime(df['date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"16e084629977a11d02effd2ecf9106bdb7307f36"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"142d8e104602230957da75c805289dae90144c67"},"cell_type":"markdown","source":"# 1. Exploratory Data Analysis<a id='eda'></a>"},{"metadata":{"_uuid":"2e07fb8d5cc1b34c56e69643ccf90f82f278f0eb"},"cell_type":"markdown","source":"First, I'm going to gather some basic information about the data set:"},{"metadata":{"trusted":false,"_uuid":"1fba2308e15c5226baaf96da9959eaac044fe7f1"},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3d48e4f252692fc88e602c9e0518b399af1647c4"},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3bbe03139c5c94644ff27a720600a43a147daa7"},"cell_type":"markdown","source":"There are 21,613 house sales in this data set, each of which has 21 features (listed above)."},{"metadata":{"trusted":false,"_uuid":"844aaec6dbb6fc34516663ad94ac101be024b1fe"},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"914ab6b92bef48cfd13ca9f7c28c28f3a6b3a999"},"cell_type":"markdown","source":"All of the features are numerical, except for the date of the transaction."},{"metadata":{"_uuid":"5bb8e2a3d9354d87e8101b580fa9a865286ab46e"},"cell_type":"markdown","source":"## 1.1. Correlation Matrix"},{"metadata":{"trusted":false,"_uuid":"21ccc6b649b18f3cdc79998aa664edd8f0500e88"},"cell_type":"code","source":"df.corr()['price'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14ffa5c103cce06f6e85be916c4a4064b86823b6"},"cell_type":"markdown","source":"Looking at the correlation between `price` and the other features, one can notice that the strongest correlation is between price and `sqft_living` (0.7), `grade` (0.67), and `sqft_above` (0.61). Not surprisingly, there is virtually no correlation between `price` and `zipcode`, `id`, and `longitude`."},{"metadata":{"_uuid":"6bd6b8d016268ef38d833ed76f8fb756eb8d5bf1"},"cell_type":"markdown","source":"The following correlation matrix shows the correlation between any two features of the data set."},{"metadata":{"trusted":false,"_uuid":"e3e9ed1c872fd381e84ab12feb16f349e2a48dfe"},"cell_type":"code","source":"plt.figure(figsize=(12,12))\nsns.heatmap(df.corr(),vmax=1.0,vmin=-1.0, square=True, fmt='.2f',\n            annot=True, cbar_kws={\"shrink\": .75}, cmap='coolwarm')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77649326ba64e8c4f20bdc8047655ad34280a2f3"},"cell_type":"markdown","source":"In addition to the correlations between `price` and other features, there are some other noteworthy facts about the correlation matrix: the strongest correlation is between `sqft_living` and `sqft_above`, the srongest negative correlation is between `zipcode` and `longitode`, suggesting that zipcodes in King County were drawn from east to west. Generally, there is a strong correlation between the quantities that relate to the size of the house: `sqft_living`, `sqft_living15`, `sqft_above`, `bedrooms`, and `bathrooms`."},{"metadata":{"_uuid":"b9e2ed861af12a07935155747befce489427ebb8"},"cell_type":"markdown","source":"# 1.2. House Prices"},{"metadata":{"_uuid":"dc43a93a849907953162d9deeda4c8277a8df201"},"cell_type":"markdown","source":"Below is a histogram of the sales prices. Most prices are between \\$200,000 and \\$1,000,000."},{"metadata":{"trusted":false,"_uuid":"7f395ed684db499c0103aa4af56bd9b67e1e03bc"},"cell_type":"code","source":"plt.hist(df['price'], bins=200, alpha=0.3)\nplt.grid()\nplt.xlabel('House price ($)')\nplt.ylabel('Transactions')\nplt.xlim(0,1200000)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ce686c4753f93a92c34aa4f09889148f2fa5dbb9"},"cell_type":"code","source":"print('Maximum house price: \\t${:0,.2f}'.format(df['price'].max()))\nprint('Minimum house price: \\t${:0,.2f}'.format(df['price'].min()))\nprint('Mean house price: \\t${:0,.2f}'.format(df['price'].mean()))\nprint('Median house price: \\t${:0,.2f}'.format(df['price'].median()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2fcacddee8002787926c1ebd7f470d71f887c62"},"cell_type":"markdown","source":"Next, I will group the data by date and then day of the week to get a sense for how the number of transactions varies over the course of a year and a week. The a significant dips in transactions in the winter and on the weekend."},{"metadata":{"trusted":false,"_uuid":"520d01b3cabbe7782535589a6e7c37e7b915f3a2"},"cell_type":"code","source":"df_week = df.copy()\ndf_week['day_of_week'] = df_week['date'].dt.dayofweek\ndf_week = df_week.groupby('day_of_week', as_index=False).count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"63d7fb34c3c23726ed4441ca209c9c42658a654b"},"cell_type":"code","source":"# Group transactions by date\ndf_date = df.groupby('date', as_index=False).count()\n\n# Plot transactions throughout a year\nplt.figure(figsize=(12,6))\nplt.plot(df_date['date'], df_date['id'])\nplt.xlabel('Date')\nplt.ylabel('Transactions')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2a4484ac1a7f1b9b7c2d406567f3d64d9ce66a7c"},"cell_type":"code","source":"plt.bar(df_week['day_of_week'], df_week['id'])\nplt.xlabel('Day of Week')\nplt.ylabel('Transactions')\nplt.xticks(np.arange(7), ('Mon', 'Tue', 'Wed', 'Thu', 'Fri','Sat','Sun'))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b89c7bbfba743292e3118183f1f656d650bec6a"},"cell_type":"markdown","source":"The feature most correlated with price is the size of the living area `sqft_living`. Here, I'm visualizing the relationship of these two quantities in a hexagonal binning plot."},{"metadata":{"trusted":false,"_uuid":"476e99e9daad765d3e929cf2a06f7f178592fc28"},"cell_type":"code","source":"plt.hexbin(df['sqft_living'], df['price'], gridsize=150, cmap='Blues')\nplt.xlim(0,4000)\nplt.ylim(0,1500000)\nplt.xlabel('Square Footage of Living Space')\nplt.ylabel('Price (USD)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b25836bf729e5e270a73ce0ca778cc7256d18cf8"},"cell_type":"markdown","source":"Some `id`s appear more than once in the dataset, indicating that the house was sold twice within a year. By grouping the data by `id` and filtering the original data, I can investigate these properties. For example, the below histogram shows their sales price distribution."},{"metadata":{"trusted":false,"_uuid":"270748d08d0b4c27d4550f7de6fee8dc72ab8e3b"},"cell_type":"code","source":"# Houses sold more than once\ndf_id = df.groupby('id', as_index=False).count()[['id','date']]\ndf_id = df_id[df_id['date']>1][['id']]\ndf_twice = df_id.merge(df, on='id', how='inner')\ndf_twice['price'].hist(bins=50)\nplt.xticks(rotation=60)\nplt.xlabel('Price (USD)')\nplt.ylabel('Transactions')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb3283489dd73a3ad194fc65ffd5a8d0e1f1d6ed"},"cell_type":"markdown","source":"One example is the house with `id` 7200179 which was sold in October 2014 for \\$150,000 and then sold again for \\$175,000 six months later."},{"metadata":{"trusted":false,"_uuid":"60d26b5cdbf10250bd862793360a91dfa9265f39"},"cell_type":"code","source":"df_twice[df_twice['id']==7200179]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06f6d9af0701b06e1cf7647e9381d4278cb423a6"},"cell_type":"markdown","source":"# 2. Pridictive Model for House Prices<a id='model'></a>"},{"metadata":{"_uuid":"047c6833983dfc2a528745a268619feaede6c529"},"cell_type":"markdown","source":"In the following, I will build a predictive regression model which will predic the price of a house based on the various features of the house. In order to prepare the data set for model building, the `date` column is transformed into numerical features (year, month, day, and day of the week) and the data are split into a training and a test set. In addition, a standardized feature matrix `X_std` is created."},{"metadata":{"trusted":false,"_uuid":"5d0f9c43f995e16edbf753324913942b84bca583"},"cell_type":"code","source":"# Split data into features and target\nX = df.drop('price', axis=1)\ny = df['price']\n\n# Transform date from single timestamp feature to multiple numerical \n# features\nX['date_day'] = X['date'].dt.day\nX['date_month'] = X['date'].dt.month\nX['date_year'] = X['date'].dt.year\nX['date_DoW'] = X['date'].dt.dayofweek\nX = X.drop('date', axis=1)\n\n# Split data set into training and test sets\nX_train, X_test, y_train, y_test = \\\n        train_test_split(X, y, test_size=0.2, random_state=0)\n    \n# Create standardized feature matrix\nX_std = StandardScaler().fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82a67a93c4f3a5c14d9c220e4c7598910b7d6e3f"},"cell_type":"markdown","source":"## 2.1. Simple Model: Linear Regression<a id='simple'></a>"},{"metadata":{"_uuid":"e34be0167a3d408b048794455f2fde7850f57512"},"cell_type":"markdown","source":"The simplest approach to this regression task is an ordinary least squares regression. I'm using the `LinearRegression` regressor implemented in `scikit-learn`. The performance of the regressor is evaluated by 10-fold cross-validation on the training set using the R2 score as the scoring metric."},{"metadata":{"trusted":false,"_uuid":"a62f945d25d64f146f803978eb97a84195a96800"},"cell_type":"code","source":"# Linear Regression\nlr = LinearRegression()\n\n# Evaluate model with cross-validation\ncvs = cross_val_score(estimator=lr, X=X_train, \n                                    y=y_train, \n                                    cv=10, scoring='r2')\nprint('CV score: %.3f ± %.3f' % (cvs.mean(), cvs.std()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40865d2b31d898524571d970d25d9b6878b853f8"},"cell_type":"markdown","source":"This simple model achieves an R2 score of 0.70 and we can use the coefficients of the individual features to gain quantiative insights into what affects the pricing of the houses."},{"metadata":{"trusted":false,"_uuid":"67387a2fe1abb63076b2f4ef0041c48db04f33e6"},"cell_type":"code","source":"lr.fit(X_train, y_train)\ncoef_list = list(lr.coef_)\nname_list = list(X_train.columns)\npd.Series(coef_list, index=name_list)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78a52c545fb3cee211fb3fb1d55e410b47194f4a"},"cell_type":"markdown","source":"According to this model, every square foot of living space adds \\$112 to the property value, every bathroom adds \\$39,528, and a waterfron view adds \\$606,251. While these numbers may sound reasonable, the is cause for scepticism looking at some of the other predictions: it seems counter-intuitive that each bedroom would *reduce* the value by \\$34,997 and that the size of the lot has no influence on the pricing.\n\nSince I will be evaluating various regressors in combination with other estimators, the following function is defined as a shortcut to evaluate a model:"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"a59722d082468ad37636c3e1f0aa2af4f096c3c1"},"cell_type":"code","source":"def eval_model(estimator, X=X_train, y=y_train, out=True):\n    '''\n    Evaluates a model provided in 'estimator' and returns the \n    cross-validation score (as a list of 10 results)\n    X: Feature matrix\n    y: Targets\n    '''\n    cvs = cross_val_score(estimator=estimator, X=X, y=y, \n                          cv=10, scoring='r2')\n    \n    if out == True:\n        print('CV score: %.3f ± %.3f' % (cvs.mean(), cvs.std()))\n    \n    return cvs","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ebbda42e748c2f50f8a41ea08e293fe1778b32f"},"cell_type":"markdown","source":"## 2.2. Feature Selection<a id='sel'></a>"},{"metadata":{"_uuid":"7254648d5203aa83f5b0f37880c690fd7272e034"},"cell_type":"markdown","source":"In this section I will try to reduce the number of features in order to possibly reduce the effects of overfitting and reduce training time. First, I will build a model with successively more relevant features, second, I will use recursive feature elimination (RFE)."},{"metadata":{"_uuid":"7120e3f854f9611aa48c12c004ca3714a16be512"},"cell_type":"markdown","source":"### 2.2.1. Add features according to their correlation with price"},{"metadata":{"_uuid":"ac4faa64fbe595cfd4167bc4b3fb9c8031fa100f"},"cell_type":"markdown","source":"The first idea is to start building a very simple model with only one feature, namely the one with the highest correlation with `price`, namely `sqft_living`. Then, more features are added in order of their correlation with `price`. For each model, a score is calculated.\n\nThe following code segment produces a `Series` of features and their respective correlations with `price`."},{"metadata":{"trusted":false,"_uuid":"0a2e247308314872be2bcba8c53ecdbfa7fbb807"},"cell_type":"code","source":"# Calculate correlation or each feature and create sorted \n# pandas Series of correlation factors\ncorr_ranking = df.corr()['price'].sort_values(ascending=False)[1:]\ncorr_ranking","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a9ecf3510e63690f7dcdd7062d6c9680f484bf0"},"cell_type":"markdown","source":"Now, I will loop over the features sorted by correlation and successively add more and more features to the model. At each step, the CV score is calculated and stored in `score_list`. The order in which features were added is stored in `feat_list`."},{"metadata":{"trusted":false,"_uuid":"8ce0107f2ed81f6a8cb8e5c58a65db495002bd0e"},"cell_type":"code","source":"feat_list = []       # List of features added to the model \nscore_list = []      # List of CV scores obtained after adding features\n\n# Loop over features in order of correlation with price\nfor feat in list(corr_ranking.index):\n    \n    feat_list.append(feat)      # Add feature name to feat_list\n    \n    # Calculate CV score\n    cvs = eval_model(lr, X=X_train[feat_list], out=False)     \n    \n    score_list.append(cvs.mean())   # Add score to score_list","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2cb30f0a2c9c64e01f9057cdeb7e807452dfbfe"},"cell_type":"markdown","source":"The following plot shows the improvement of the model with each new feature added. `sqft_living` along yields a score of 0.49 and the score continuously increases with every feature. There are only a few features that do not inprove the model and none that reduce its score. The contributions of the individual features is indicated by the orange bars."},{"metadata":{"trusted":false,"_uuid":"0e700423c6b6dbfdab339d097f3406031a0ee9a8"},"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize=(12,6))\n\n# Accumulated CV score\nax.plot(score_list)\nax.set_xticks(list(range(len(feat_list))))\nax.set_xticklabels(feat_list, rotation=60)\nax.set_xlabel('Added feature')\nax.set_ylabel('CV score (R2) (blue)')\nax.grid()\n\n# Derivative\naxR = ax.twinx()\naxR.set_ylabel('Individual feature contribution (orange)')\naxR.bar(left=list(range(19)), height=np.diff([0]+score_list),\n        alpha=0.4, color='orange')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c63c8b44db4335b7e3a98d22c0e95b457a35a2d9"},"cell_type":"markdown","source":"### 2.2.2. Recursive Feature Elimination (RFE)"},{"metadata":{"_uuid":"0403d1e7570f228c0419917a2c3be0f2db8d8515"},"cell_type":"markdown","source":"Next, I will use `scikit-learn`'s RFE implementation to eliminate features in a more sophisticated manner. In the following, RFE is performed for any an increasing number of features to be selected `n` and for each model, the score is recorded in `scores_list`. The below plot shows how the score increases are `n` increases."},{"metadata":{"trusted":false,"_uuid":"3676664fc908e02d4579affaed3e4895a0bb4ba6"},"cell_type":"code","source":"features_range = range(1,24)\nscores_list = []\n\nfor n in features_range:\n    rfe = RFE(lr, n, step=1)\n    rfe.fit(X_train, y_train)\n    scores_list.append(rfe.score(X_train, y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7e291311a04bc4282c6c0fcd2ad841316435bf1f"},"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize=(12,6))\n\n# Regular sized plot\nax.plot(features_range, scores_list)\nax.set_xlabel('Number of features')\nax.set_ylabel('CV score (R2)')\nax.set_xticks(features_range)\nax.grid()\n\n# Vertical magnification\naxR = ax.twinx()\naxR.plot(features_range, scores_list/max(scores_list), linestyle='--')\naxR.set_ylabel('% of maximum CV score (dashed)')\naxR.set_ylim(0.94,1.01)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1b8dd3130de30abd10437e241d1202a7878dca8"},"cell_type":"markdown","source":"The model's score approaches the maximum value relatively quickly and already after 13 features, the score is above 99% of the final score, meaning that 10 features (almost half) can be discarded without significantly impacting the model accuracy. These are those 13 features:"},{"metadata":{"trusted":false,"_uuid":"2de00309bab9b0fff039e5e457b715b57d10fd3f"},"cell_type":"code","source":"# Fit RFE model with n_features_to_select=13 to X_train\nrfe = RFE(lr, 13, step=1)\nrfe.fit(X_train, y_train)\n\n# Print list of selected columns\nlist(X_train.columns[rfe.support_])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2904ca4a521e8b552502e9c36dc05a5cfaec3ad1"},"cell_type":"markdown","source":"I'm creating a new feature matrix `X_rfe` containing only the 13 most relevant features."},{"metadata":{"trusted":false,"_uuid":"5a2b9be23f703f751cff0a0e0997a47089f68bda"},"cell_type":"code","source":"X_rfe = X_train[list(X_train.columns[rfe.support_])]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41d3e4a3c606f13916a176988169a7cddcdaab78"},"cell_type":"markdown","source":"## 2.3. Different Regression Algorithms<a id='alg'></a>"},{"metadata":{"_uuid":"1efc29d4998dd3a9195ee664f7c4d0361feb6ddb"},"cell_type":"markdown","source":"So far I've only used ordiniary least squares regression, `lr`. The highest score achieved with this model was 0.70."},{"metadata":{"trusted":false,"_uuid":"b3705fc7b03d6cbf21ef320ae633ef97e1650886"},"cell_type":"code","source":"eval_model(lr, X=X_std);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b03d8cb1a325cf1958f60367679b2ab55dbafb63"},"cell_type":"markdown","source":"### 2.3.1. Ridge"},{"metadata":{"_uuid":"ab0df33e486dcb93eb5fe16d57485a47a6673a82"},"cell_type":"markdown","source":"Ridge regression is an alternative to ordinary least squares regression, the main difference being that large coefficients are penalized in this model. The strength of this regularization is determined by the hyperparameter `alpha`. With the default of `alpha`=1.0, ridge regression yields the same R2 score:"},{"metadata":{"trusted":false,"_uuid":"3ee1294dd616ba2950c9faeb667112d0b785ae27"},"cell_type":"code","source":"ridge = linear_model.Ridge()\neval_model(ridge, X=X_std);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82e39588b1ee209cc6b2f0b35927ccf3f8322bf2"},"cell_type":"markdown","source":"In the following, a validation curve is used to determine whether other parameters of `alpha` yield improved scores. As the plot below shows, this is not the case. In fact, the model score seems independent of `alpha`, except for values exceeding 1,000 for which the model quickly deteriorates."},{"metadata":{"trusted":false,"_uuid":"201d53fd93a60129a525b9542df0a38aac18458d"},"cell_type":"code","source":"# Trying different alpha parameters for Ridge regression\n\ntest_int = np.logspace(-10, 6, 17)\n\ntrain_scores, valid_scores = \\\n            validation_curve(ridge, X_std, y_train, 'alpha',\n                                test_int, cv=5)\n\n# Plot validation curve\nplt.figure(figsize=(12,6))\nplt.title('Validation curve')\nplt.semilogx(test_int, np.mean(train_scores, axis=1), \n             marker='o', label='Training data')\nplt.semilogx(test_int, np.mean(valid_scores, axis=1), \n             marker='o', label='Validation data')\nplt.legend()\nplt.ylabel('CV score (R2)')\nplt.xlabel('alpha parameter')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f858533a67b71cdfe3ec395232122a3fedecb2eb"},"cell_type":"markdown","source":"### 2.3.2. Lasso"},{"metadata":{"_uuid":"78a6263df57f738c365b1a41ac9d793be1dbc19a"},"cell_type":"markdown","source":"The same behavior (and no improvement in the R2 score) is observed for regression with lasso (L1) regularization. This is likely due to the relatively small number of features."},{"metadata":{"trusted":false,"_uuid":"1c9c37aea87aa137da81be7a8070cc8c735874e3"},"cell_type":"code","source":"lasso = linear_model.Lasso()\neval_model(lasso, X=X_std);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b390ed938bcd280d013d42b77b30c13871661f0b"},"cell_type":"code","source":"# Trying different alpha parameters for Lasso regression\n\ntest_int = np.logspace(-10, 6, 17)\n\ntrain_scores, valid_scores = \\\n            validation_curve(lasso, X_std, y_train, 'alpha',\n                                test_int, cv=5)\n\n# Plot validation curve\nplt.figure(figsize=(12,6))\nplt.title('Validation curve')\nplt.semilogx(test_int, np.mean(train_scores, axis=1), \n             marker='o', label='Training data')\nplt.semilogx(test_int, np.mean(valid_scores, axis=1), \n             marker='o', label='Validation data')\nplt.legend()\nplt.ylabel('CV score (R2)')\nplt.xlabel('alpha parameter')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6be0098bec51d7dbc73189847b8fc0db24345e0b"},"cell_type":"markdown","source":"### 2.3.3. SVR"},{"metadata":{"_uuid":"e84fa2b962af3614df6ae8360153dde7a12ff147"},"cell_type":"markdown","source":"Another alternative regression model is Support Vector Regression (SVR), which in this case proves much worse than the linear model:"},{"metadata":{"trusted":false,"_uuid":"e85f61284a81d7c4567d52275e2297950b0e9cb3"},"cell_type":"code","source":"svr = SVR(kernel='linear', C=1)\neval_model(svr, X=X_std);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5820b638b95f9ef8a24d489dcb6c324ce2c7244e"},"cell_type":"markdown","source":"### 2.3.4. Random Forest Regressor"},{"metadata":{"_uuid":"bbd5cf5ad984f5e2ea86a4a63ae82e94b12c028d"},"cell_type":"markdown","source":"On the other hand, the Random Forest Regressor proves the most accurate model with an R2 score of 0.72, however this comes at the substantial computational cost. The random forest regressor takes over 400 times longer, although using the reduced feature set `X_rfe`, the computation takes less than half as long."},{"metadata":{"trusted":false,"_uuid":"566da524a30ba6b2b07f43cd9739dff342ce3046"},"cell_type":"code","source":"forest = RandomForestRegressor(max_depth=4, random_state=0,\n                              n_estimators=200)\neval_model(forest, X=X_std);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4c7fec3470fe5f5d2f957a0df8823548e8c0e1cc"},"cell_type":"code","source":"%timeit eval_model(lr, X=X_std, out=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3879177cad6e34c22e57f7368357c251367e7cf2"},"cell_type":"code","source":"%timeit eval_model(forest, X=X_std, out=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c38246ea9fa907b40fdde10973368e38760f6ec6"},"cell_type":"code","source":"%timeit eval_model(forest, X=X_rfe, out=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"833a089a6c8f141248496774d08c6e52a91c6c76"},"cell_type":"markdown","source":"## 2.4. Feature Engineering<a id='fe'></a>"},{"metadata":{"_uuid":"4ac51ebccdc9da60ddb0ff84e841d964a18abaf6"},"cell_type":"markdown","source":"After having investigated options to eliminate irrelevant features above, I will now create additional features that improve the quality of the model."},{"metadata":{"_uuid":"921681e916e3a3139a98142bd1c40aea8ee813d0"},"cell_type":"markdown","source":"### 2.4.1. Add polynomial features"},{"metadata":{"_uuid":"58891f8cd9c06bd5430368597bb384e35b917d3b"},"cell_type":"markdown","source":"Using `sciki-learn`'s `PolynomialFeatures()`, I am expanding the feature matrix by quadratic and cubic terms. This improves the model quality to a score of 0.81 in case of polynomial order 2 and a standardized input matrix."},{"metadata":{"trusted":false,"_uuid":"7c8a302a95fbfb054aedc92a207e8113f6abc624"},"cell_type":"code","source":"# Second order (quadratic) polynomials with linear regression\npipe = Pipeline([('poly', PolynomialFeatures(2)),\n                 ('lr', lr)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"187bde18349879b772c6302d3f7c2448f25a1cc2"},"cell_type":"code","source":"eval_model(pipe, X=X_train); # quadratic terms, unstandardized features","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b103acde8eb3e8e202a04c7cd98fed3d90270d47"},"cell_type":"code","source":"eval_model(pipe, X=X_std);   # quadratic terms, standardized features","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"0131bbd57e17e03fef7e0ea10a21c09b7ccb59e2"},"cell_type":"code","source":"# Third order (cubic) polynomials with linear regression\npipe = Pipeline([('poly', PolynomialFeatures(3)),\n                 ('lr', lr)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e86c68280c4b46f752e8a398ae4211422d47bda3"},"cell_type":"code","source":"eval_model(pipe, X=X_train); # cubic terms, unstandardized features","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34508a132ebb7c1c268851f1964bf03bd563f2be"},"cell_type":"markdown","source":"Training the model on a feature matrix containing cubic terms increases the training time substantially. Therefore, I will try to reduce the number of features in the following by using RFE to reduce the size of the feature matrix before adding cubic terms. However, this model does not yield useful predictions."},{"metadata":{"trusted":false,"_uuid":"8fb4bcfd6966ca58b0b9aeea756f7cddcb14ac98"},"cell_type":"code","source":"pipe = Pipeline([('rfe', RFE(lr, 13, step=1)),\n                 ('poly', PolynomialFeatures(3)),\n                 ('lr', lr)])\neval_model(pipe, X=X_std);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b92c14b2eabf092fecdfb8249795c488157f8dbb"},"cell_type":"markdown","source":"### 2.4.3. Combine polynomial features with PCA"},{"metadata":{"_uuid":"2d0f1f9b3f21cc3fdb52b1f4d0b479eb2d38a052"},"cell_type":"markdown","source":"In this section, the size of the feature matrix will be reduced not through RFE before adding cubic terms, but using principal component analysis (PCA). First, I will train a linear regression model on the PCA-reduced feature matrix alone. Not surprisingly, this reduces the score:"},{"metadata":{"trusted":false,"_uuid":"444722df4edb0b9a54695c3eabaf0795d95a2280"},"cell_type":"code","source":"pipe = Pipeline([('pca', PCA(n_components=13)),\n                 ('lr', lr)])\neval_model(pipe, X=X_std);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac0896e7eacc66c3bdd5183a7ee153e1c59cec65"},"cell_type":"markdown","source":"Now, I'm adding cubic terms after reducing the feature matrix size. The combination of PCA and cubic terms yields a score of 0.77 which is significantly better than the score of the linear model alone."},{"metadata":{"trusted":false,"_uuid":"aa69f6bd1fb23de8784ae394c15078cf880f909d"},"cell_type":"code","source":"pipe = Pipeline([('pca', PCA(n_components=13)),\n                 ('poly', PolynomialFeatures(3)),\n                 ('lr', lr)])\neval_model(pipe, X=X_std);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26bce4673dc0e6d88b6d36e954cee5769c3b65b2"},"cell_type":"markdown","source":"In the following, I will test PCA with different values for `n_components`."},{"metadata":{"trusted":false,"_uuid":"1809025859238fed5b4852ef6efa96b7fab44926"},"cell_type":"code","source":"scores_list = []  # List in which scores will be saved\n\n# Loop over values for n_components\nfor n_components in range(7,17):\n    \n    # Build pipe\n    pipe = Pipeline([('pca', PCA(n_components=n_components)),\n                     ('poly', PolynomialFeatures(3)),\n                     ('lr', lr)])\n    \n    # Evaluate model, print, and save score in scores_list\n    cvs = eval_model(pipe, X=X_std, out=False);\n    print('n_components = %d :  %.3f pm %.3f' % \\\n          (n_components, cvs.mean(), cvs.std()))\n    scores_list.append(cvs.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8881f6a541936deb8f0e812b903a581dde3681ef"},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nplt.xlabel('n_components')\nplt.ylabel('CV score (R2)')\nplt.plot(range(7,17), scores_list, marker='o')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56e406e6cd7b7dc53d83e4537e39e4f686c87b4a"},"cell_type":"markdown","source":"As the above plot shows, the ideal number for `n_components` is 14, yielding a score of 0.78. Thus, the combination of PCA and adding cubic terms does not yield a better result than square terms alone."},{"metadata":{"_uuid":"d2b2c1c6f629f549396a1f1f1aef727f453397de"},"cell_type":"markdown","source":"## 2.5. Summary of Models<a id='25'></a>"},{"metadata":{"_uuid":"778ebf0fdcad1167ca82f63d9e3f408b04718c6d"},"cell_type":"markdown","source":"__Different regressors__: I started with a simple linear model (`LinearRegression()`), which achieved a score of 0.70. The related ridge and lasso models (`Ridge()`, `Lasso()`) which penalize large weights yield the same result, 0.70. A support vector classifier (`SVR()`) gave a score of only 0.09 but random forest regression (`RandomForestRegressor()`) gave a slightly better (the best) score with 0.72, but at significant computational cost.\n\n__Feature engineering__: I then modified the feature set used by the model, and evaluated them with the linear model. First, I used polynomial features (`PolynomialFeatures()`) which improved the score to 0.74 but this result was improved even further to __0.82__ when using scaled features. This is the overall best result. \n\nMoving further to cubic terms reduced the score to 0.63, in addition the computational cost went up substantially. To combate that, I tried preprocessing the data with RFE (which yielded bad results) and PCA. After just reducing the dimensionality to 13 with PCA, the score was 0.66. But when PCA was used for preprocessing before creating cubic terms with `PolynomialFeatures(3)`, the score went up to 0.78."},{"metadata":{"_uuid":"ca79caadf71a003da76bd9415000892041318a52"},"cell_type":"markdown","source":"# 3. Visualization on a Map<a id='map'></a>"},{"metadata":{"_uuid":"c848661b57fe53452f8b15431cbc26d91020cc53"},"cell_type":"markdown","source":"The data includes geographic information in the form of latitude and longitude of the sold properties. In the following, I use `Basemap` to plot a fraction (10%) of the houses on a map, with their price colorcoded, blue being cheap and red being expensive."},{"metadata":{"trusted":false,"_uuid":"8f75724acd02ef3104e7b803bf5fead7c9da881e"},"cell_type":"code","source":"from mpl_toolkits.basemap import Basemap\nfrom matplotlib.cm import bwr # import color map\n\nplt.figure(figsize=(12, 12))\n\n# Create map with basemap\nm = Basemap(projection='cyl', resolution='i',\n            llcrnrlat = df['lat'].min()+0.1, \n            llcrnrlon = df['long'].min()-0.1,\n            urcrnrlat = df['lat'].max(), \n            urcrnrlon = df['long'].max()-0.6)\n\n# Load satellite image (deactivated for Kaggle)\n#m.arcgisimage(service='ESRI_Imagery_World_2D', \n#              xpixels=1500, verbose=True)\n\n# Reducing number of properties shown\nplot_df = df[::10]\n\nfor index, house in plot_df.iterrows(): # Loop over houses\n    \n    # Get position on the map from geo coordinates\n    x,y = m(house['long'], house['lat'])\n    \n    #  Get color from price\n    price_min = 200000\n    price_max = 800000\n    price_norm = (house['price'] - price_min) / (price_max - price_min)\n\n    rgb_exp = price_norm\n    rgb=[0,0,0]\n    for i in range(3): rgb[i] = int(bwr(rgb_exp)[i]*255)\n    color_hex = \"#%02x%02x%02x\" % (rgb[0], rgb[1], rgb[2])\n    \n    #Plot data point\n    plt.plot(x,y, 'o', markersize=6, color=color_hex, alpha=0.6)\n   \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2cc60e33ec36d6ebaeddb50d54c5c6027ba5acad"},"cell_type":"markdown","source":"It becomes clear from this map that house prices are higher in the downtown area (near Elliott Bay in top left of the map) and in Redmond (east of Lake Washington). House prices are intermediate north and south of downtown, and are lowest south of the city, near the airport. In addition we can see that waterfront property is more expensive."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}