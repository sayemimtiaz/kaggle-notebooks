{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Gender Recognition from voice using Deep Learning And Neural Networks"},{"metadata":{},"cell_type":"markdown","source":"# **** AIM****\n1. To build neural networks to classify the gender of the voice and maximise the accuracy of the model\n2. To compare the accuracy of Deep learning-Neural Network model with machine learning classifiers"},{"metadata":{},"cell_type":"markdown","source":"Before we dive in let me give a brief of what we are upto. We have a dataset which based on certain paramaters classifies a voice based on gender. How do humans do it?\n\n    Sound waves travel into the ear canal until they reach the eardrum. The eardrum passes the vibrations through the middle ear bones or ossicles into the inner ear. The inner ear is shaped like a snail and is also called the cochlea. Inside the cochlea, there are thousands of tiny hair cells. Hair cells change the vibrations into electrical signals that are sent to the brain through the hearing nerve. The brain tells you that you are hearing a sound and what that sound is.\n\nWhat happens in the brain is neurons perform certain operations to classify the sound, this is exactly what we will be trying to simulate. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 1\nImport the libraries\n1. matplotlib :: To plot graphs \n2. numpy :: To perform operations and manipulate arrays \n3. pandas :: To read and manage the data from the file\n4. Import ML basic classification models :: from sklearn for classification\n5. Import Neural network building libraries :: from keras"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pylab as pl\n\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport sklearn.metrics as metrics\nfrom mpl_toolkits.mplot3d import Axes3D\n\n#Ignore  the warnings\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\n#plotting missing data\nimport missingno as msno\n\n#classification models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\n\n#Neural network building libraries\nimport keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.callbacks import History \nfrom keras.utils import plot_model\nfrom keras.optimizers import SGD","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 2\nLoading the dataset and performing EDA (Exploratory Data Analysis) over the dataset. \nTo analyse and understand the dataset, its features and target classes"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"voice=pd.read_csv(\"../input/voicegender/voice.csv\")\nvoice.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\n\",voice.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"voice.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualizing no missing value.\nmsno.matrix(voice)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Shows no null values so cleaning not required**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating a copy\ndata=voice.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of target varibles\ncolors = ['pink','Lightblue']\ndf = data[data.columns[-1]]\nplt.pie(df.value_counts(),colors=colors,labels=['female','male'])\nplt.axis('equal')\nprint (data['label'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Radviz circle \n#Good to compare every feature\npd.plotting.radviz(data,\"label\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pairplotting\nsns.pairplot(data[['meanfreq', 'Q25', 'Q75',\n                'skew', 'centroid', 'label']], \n                 hue='label', size=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop('label' ,axis=1).hist(bins=30, figsize=(12,12))\npl.suptitle(\"Histogram for each numeric input variable\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#corelation matrix.\ncor_mat= data[:].corr()\nmask = np.array(cor_mat)\nmask[np.tril_indices_from(mask)] = False\nfig=plt.gcf()\nfig.set_size_inches(15,15)\nsns.heatmap(data=cor_mat,square=True,annot=True,cbar=True,cmap='Spectral')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this section the corelation between different features is analyzed. 'Heat map' is plotted which clearly visulizes the corelation between different features"},{"metadata":{},"cell_type":"markdown","source":"# Step 3\n\nNow since we have the feature set and the set of dependent variables, We observe that the 'label' has strings and in maths we need values so  we will convert it to numerical values Male=1 and Female=0"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert string label to float : male = 1, female = 0\ndict = {'label':{'male':1,'female':0}}      # label = column name\ndata.replace(dict,inplace = True)           # replace = str to numerical\nx = data.loc[:, data.columns != 'label']\ny = data.loc[:,'label']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 4\nWe need to separate the dependent and independent variables. Here the first 20 set columns consists of the features and the last coloumn is the dependent variable, which takes two integer values i.e 1 (Male) and 0 (Female)\n\nX as feature columns and Y as dependent column"},{"metadata":{"trusted":true},"cell_type":"code","source":"array = data.values\nX = array[:,0:20]\nY = array[:,20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 5\n\nDivide the data into training set and test set, One set to train the neural Network and the other set to test the neural network. "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 6 \n**Scaling**\n\n\nNow if we observe the values in various coloumns we see that there is a problem either the values are extremely close to zero or the all the coloumn are of not the same scale.  there is a lot of times where we will need to calculate slopes assume in the denominator two point are really close to zero, subtracting will lead it much more closer to zero and the slope assumes an amazingly huge value, so to prevent this kind of problems we generally use scaling in Neural Networks."},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 7\nBuilding **different Machine Learning classifiers** and finding the accuracy score of each model.\n\nAlso to find the accurate model with highest accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Appending different Models to a list\n\nmodels = []\n\nmodels.append(( 'LR ', LogisticRegression()))\nmodels.append(( 'SVC', SVC(kernel='linear', C=1.0, random_state=0)))\nmodels.append(( 'LDA', LinearDiscriminantAnalysis()))\nmodels.append(( 'KNN', KNeighborsClassifier(n_neighbors=9, p=2, metric='minkowski')))\nmodels.append(( 'CLF', DecisionTreeClassifier(criterion=\"entropy\",max_depth=3)))\nmodels.append(( 'RFC', RandomForestClassifier(max_depth=2, random_state=0)))\nmodels.append(( 'MLP', MLPClassifier(hidden_layer_sizes=(3,3),max_iter=3000, activation = 'relu',solver='adam',random_state=1)))\nmodels.append(( 'GNB', GaussianNB()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding Mean Accuracy for Models\n\nresults = []\nnames = []\nmeanscore=[]\nscoring = 'accuracy'\n\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=10)\n    cv_results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: (%f)\" % (name, cv_results.mean()*100)\n    meanscore.append(cv_results.mean()*100)\n    print(\"Mean Accuracy score\", msg)\n\nprint(\"\\nHighest Mean Accuracy is for the classifer LDA\", max(meanscore))\nplt.plot(names,meanscore,marker='o')\nplt.xlabel('Models')\nplt.ylabel('Model Accuracy')\nplt.title('ML classifiers and Accuracy score',size=25)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 8\n\nNow starts the actual  building of the neural network using Keras.\n\nBefore we get into the code let us try to understand the neural network structure we are aiming to build. Here in the data set there are 20 paramaters which can also be called features and these are fed to the nodes on a one-to-one basis that is one node recieves one input. We will call this the first layer and this is what this piece of code does.\n\nThe Dense is used to specify the fully connected layer.\n\n    classifier.add(Dense(output_dim=16,init='uniform',activation='relu',input_dim=20))\n\nnext we pass this sound to the processing unit the brain where we have a lot of itermediate processing neurons before we actually get the output. In this case we will add just 2 intermediate stages of processing neurons with 16 nodes in each layer.\n\n    classifier.add(Dense(output_dim=16,init='uniform',activation='relu'))\n\n    classifier.add(Dense(output_dim=6,init='uniform',activation='relu'))\n\nNow we need to get the output and one node will do the job\n\n    classifier.add(Dense(output_dim=1,init='uniform',activation='sigmoid'))\n\nIf you are wondering what is relu and sigmoid well these are the functions which are used to calculate the weights/loss etc.\n\nNow we need to specify the loss function and the optimizer. It is done using compile function in keras.\n\n    classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\nIn the end we feed out data to the neural Network and wait for the magic to happen."},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier=Sequential()\nhistory = History()\n\n#number of input variables = 20 so input_dim is only for the first layer\nclassifier.add(Dense(output_dim=16,init='uniform',activation='relu',input_dim=20)) #first layer\nclassifier.add(Dense(output_dim=16,init='uniform',activation='relu'))   #first Hidden layer\nclassifier.add(Dense(output_dim=6,init='uniform',activation='relu'))    #Second Hidden layer\n\nclassifier.add(Dense(output_dim=1,init='uniform',activation='sigmoid')) #output layer\n\n#Running the artificial neural network\nclassifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\nclassifier.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 9\n**Training model :**\nNow we are done with building a neural network and we will train it.Training step is simple in keras. \n\n       ( classifier.fit)  is used to train it.\nIt is always important to see, what actually is happening and how the model is learning. So with every epoch there is some learning which happens. The model is capable of calculating the loss it is facing from the actual result and then correspondingly adjusts its weight in automatically"},{"metadata":{"trusted":true},"cell_type":"code","source":"trained=classifier.fit(X_train,Y_train,batch_size=5,epochs=20,validation_split=0.2,callbacks=[history],shuffle=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 10 (Final Step)\n\nNow we can check the modelâ€™s performance on test data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=classifier.predict(X_train)\ny_pred = np.round(y_pred)\n\nprint('Accuracy by the Neural Network on train dataset is',metrics.accuracy_score(y_pred,Y_train)*100,'%')\n\ny_pred=classifier.predict(X_test)\ny_pred = np.round(y_pred)\n\nprint('Accuracy by the Neural Network on test dataset is ',metrics.accuracy_score(y_pred,Y_test)*100,'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \npd.DataFrame(trained.history).plot()\nplt.grid(True)\nplt.xlabel(\"epochs\") \nplt.ylabel(\"loss/accuracy\")\nplt.title(\"Training and validation plot\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'], color = 'red',label='Variaton Loss over the epochs',)\nplt.plot(history.history['accuracy'],color='green',label='Variation in Accuracy over the epochs')\n\nplt.xlabel('Epochs')\nplt.title('Loss/Accuracy VS Epoch on test Dataset using our model')\nplt.ylabel('Loss/Accuracy')\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we observe the graph, Over a period of time it clear that the loss is gradually hitting zero and the Accuracy is increasing at a considerable rate."},{"metadata":{},"cell_type":"markdown","source":"# Visualization of model accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(trained.history['accuracy'])\nplt.plot(trained.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'])\nplt.show()\n\nplt.plot(trained.history['loss'])\nplt.plot(trained.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(Y_test[-30:],linestyle='--',label='Actual value',\n         linewidth=3,marker='o' ,\n         markerfacecolor='green',markersize=15,color='green')\nplt.plot(y_pred[-30:],linestyle='-.',label='Predicted value',\n         linewidth=3,marker='o',\n         markerfacecolor='red',markersize=10,color='red')\nplt.title('Validating the Model for 30 voices',size=15)\nplt.xlabel(\"Voice notes\")\nplt.ylabel(\"Male(1)/ female(0)\")\nplt.legend(loc='center left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The actual and predicted value is visualized"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loss, test_accuracy = classifier.evaluate(X_test, Y_test) \nprint(\"Test-loss: %f, Test-accuracy: %f\" % (test_loss, test_accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}