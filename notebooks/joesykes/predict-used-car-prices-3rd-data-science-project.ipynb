{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Used Car Sale Prices\n\nThe goal of this project is to create a model that accurately predicts the sale price of a used car.\n\nThe data used for this project was found on Kaggle.com, uploaded by Aditya. The data contains 9 csv files, with each file storing the information about one make of car, including Audi, BMW, Ford, Hyundai, Mercedes, Skoda, Toyota, Vauxhall and Volkswagen. \n\nIn this project we shall undertake the following tasks:\n\n0. Data and Package Imports\n1. Exploratory Data Analysis\n2. Data Preprocessing\n3. Model Creation and Evaluation\n4. Conclusions\n\n## 0: Data and Package Imports\n\nIn this section we shall import the 9 different csv files and the necessary visualisation libraries from Python. We shall first import the visualisation packages.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We shall now import each csv file into a Pandas dataframe.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"audi = pd.read_csv('/kaggle/input/used-car-dataset-ford-and-mercedes/audi.csv')\nbmw = pd.read_csv('/kaggle/input/used-car-dataset-ford-and-mercedes/bmw.csv')\nford = pd.read_csv('/kaggle/input/used-car-dataset-ford-and-mercedes/ford.csv')\nhyundai = pd.read_csv('/kaggle/input/used-car-dataset-ford-and-mercedes/hyundi.csv')\nmercedes = pd.read_csv('/kaggle/input/used-car-dataset-ford-and-mercedes/merc.csv')\nskoda = pd.read_csv('/kaggle/input/used-car-dataset-ford-and-mercedes/skoda.csv')\ntoyota = pd.read_csv('/kaggle/input/used-car-dataset-ford-and-mercedes/toyota.csv')\nvauxhall = pd.read_csv('/kaggle/input/used-car-dataset-ford-and-mercedes/vauxhall.csv')\nvw = pd.read_csv('/kaggle/input/used-car-dataset-ford-and-mercedes/vw.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Rather than working with 9 different dataframes, it will be simpler if we were able to concatenate each dataframe into one larger dataframe. Let us first check the columns of each dataframe to ensure that the information stored about each vehicle is the same.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"Columns in the Audi dataframe:\") \nprint(list(audi.columns))\nprint(\"-\" * 50)\nprint(\"Columns in the BMW dataframe:\")\nprint(list(bmw.columns))\nprint(\"-\" * 50)\nprint(\"Columns in the Ford dataframe:\")\nprint(list(ford.columns))\nprint(\"-\" * 50)\nprint(\"Columns in the Hyundai dataframe:\")\nprint(list(hyundai.columns))\nprint(\"-\" * 50)\nprint(\"Columns in the Mercedes dataframe:\")\nprint(list(mercedes.columns))\nprint(\"-\" * 50)\nprint(\"Columns in the Skoda dataframe:\")\nprint(list(skoda.columns))\nprint(\"-\" * 50)\nprint(\"Columns in the Toyota dataframe:\")\nprint(list(toyota.columns))\nprint(\"-\" * 50)\nprint(\"Columns in the Vauxhall dataframe:\")\nprint(list(vauxhall.columns))\nprint(\"-\" * 50)\nprint(\"Columns in the VW dataframe:\")\nprint(list(vw.columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the columns within each dataframe are the same, with the exception of the \"Tax\" column in the Hyundai dataframe. Let us change the name of this column so that we are able to join the dataframes together.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"hyundai.rename({'tax(£)': 'tax'},axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us check the columns of the Hyundai dataframe now.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"print(list(hyundai.columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We notice that the column names are now identical for each dataframe, meaning that we are able to merge the dataframes together. First, let us create a new column in each dataframe called \"make\", which is simply the name of the manufacturer who produced the car, so that this information is not lost in our new dataframe.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"audi['make'] = 'Audi'\nbmw['make'] = 'BMW'\nford['make'] = 'Ford'\nhyundai['make'] = 'Hyundai'\nmercedes['make'] = 'Mercedes'\nskoda['make'] = 'Skoda'\ntoyota['make'] = 'Toyota'\nvauxhall['make'] = 'Vauxhall'\nvw['make'] = 'Volkswagen'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are now able to join the dataframes into a single larger dataframe which contains all the information about every car within our dataset. ","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df = pd.concat([audi, bmw, ford, hyundai, mercedes, skoda, toyota, vauxhall, vw], axis=0, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us now check the info of the new dataframe.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that our dataframe contains nearly data regarding nearly 100000 used cars from across the UK. Let us reorder the columns so that the data is presented in a logical order. ","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df = df[['make','model','year','fuelType','mileage','engineSize','transmission','mpg','tax','price']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us now check the head of the dataframe.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see some examples of the types of data stored within each column. We note that the \"Model\", \"fuelType\" and \"transmission\" variables are stored in the \"object\" format, which means we will have to use label encoding or dummy variables in order to input them into our machine learning algorithms. \n\n## 1: Exploratory Data Analysis\n\nIn this section we shall begin to explore the data in order to identify and key relationships.\n\n### 1.1: Understanding the Variables and Cleaning the Dataset\n\nLet us begin investigating the range of different values possible for each variable.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df.nunique(axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that, particulary for our categorical variables, that there are a range of different values that could be taken. Let us investigate these further. ","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Unique values for \"fuelType\" column:', sorted(list(df['fuelType'].unique())))\nprint('Unique values for \"transmission\" column:', sorted(list(df['transmission'].unique())))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The options shown within these catgeorical variables are completely independent and we are therefore not able to reduce the number of categories within these features. Let us now investigate the numerical columns.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df.describe().apply(lambda s: s.apply(lambda x: format(x, 'f')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Year\n\nWe immediately notice that there seems to be an isue with the \"year\" column, with at least 1 vehicle having a value of 2060. Let us remove that datapoint.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df[df['year'] == 2060]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We shall remove this entry from our dataset, as it is difficult to determine when this vehicle was first registered.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df = df.drop(df.index[39175])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also notice that at least 1 vehicle was first registered in 1970. Let us investigate this date in more detail.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df[df['year'] == 1970]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us remove these two vehicles from the dataset also.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"indexNames = df[df['year'] == 1970].index\ndf = df.drop(indexNames)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Engine Size\n\nFrom the output of the describe method above, we also observe that there are some vehicles recorded with an engine size of 0. This is obviously impossible and these vehicles must be investigated. Let us first look at which vehicles have been recorded with this value.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df[df['engineSize'] == 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are 272 vehicles that supposedly have an engine size of 0. Let us determine whcih percentage of our dataset this is. ","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"len(df[df['engineSize'] == 0]) * 100 / len(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that these vehicles account for less than a quarter of a percent of the total dataset. As a result, we can remove them.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"engineIndex = df[df['engineSize']==0].index\ndf = df.drop(engineIndex)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that the vehicles with an engine size of 0 have been removed, let us investigate the vehicles with particularly low mileage. \n\n#### Mileage\n\nWe observe that there are vehicles with a recorded mileage of 1. Let us find these vehicles.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df[df['mileage']==1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some of these vehicles were first registered in 2020 and therefore a mileage of 1 is understandable. However, for vehicles registered before this, a mileage value this low does not make sense. Let us see the percentage of our dataset which are vehicles registered in 2019 or before than have a mileage figure of 1.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"len(df[(df['mileage']==1) & (df['year']<= 2019)]) * 100 / len(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once again, this is such a small percentage of our entire dataset that removing them will not affect our ability to accurately predict the prices of used cars. As a result, these vehicles shall be removed.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"mileageIndex = df[(df['mileage']==1) & (df['year']<= 2019)].index\ndf = df.drop(mileageIndex)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Tax\n\nSince tax payments are made on almost all vehicles purchased in the UK, let us now investigate the vehicles that have a recorded tax value of 0.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df[df['tax'] == 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(df[df['tax'] == 0]) * 100 / len(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since car tax payments within the UK are based on the age of the vehicle, its CO2 emissions, and various other factors, we are not able to impute these values with estimates of the tax that should be paid. As a result, since the vehicles in question account for only 6% of the total dataset, and the fact the our dataset is rather large, we can simply remove these cars.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"taxindex = df[df['tax']==0].index\ndf = df.drop(taxindex)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### MPG\n\nFinally, let us now investigate vehicles that have a recorded MPG value of less than 5.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df[df['mpg'] < 5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We notice that all of these vehicles, except for the Volkswagen Golf SV, have either Diesel or Hybrid engines. The diesel vehicles within this subset of data are all pickup trucks and as a result the low mpg figure is understandable and could easily be correct. The hybrid vehicles do not solely depend on their petrol or diesel engine as a result of the combination with electricity, which could explain the low mpg figure in these cases. In the case of the Volkswagen Golf SV, the low mpg figure is difficult to explain and as a result we shall drop this vehicle from the dataset.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df = df.drop(df[df['mpg']==0.3].index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us now check the info method on our dataframe again.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df.describe().apply(lambda s: s.apply(lambda x: format(x, 'f')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us now check that we have no missing entries with our dataset.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We do not have any missing entries within our dataset. Our data has been successfully cleaned.\n\n### 1.2: Analysing Relationships Between Variables\n\n#### 1.2.1: Numerical Variables\n\nIn this section we shall analyse the relationships between the variables in our dataset. We shall start by creating a correlation heatmap of the numerical variables.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.heatmap(df.corr(),annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Firstly we notice that there is an extremely positive correlation between year and price and an extremely negative correlation between mileage and price. This makes sense, since newer cars are generally more expensive and cars with more mileage are relatively cheaper. We also notice a negative correlation between mileage and year - the newer a car is the less miles it is likely to have travelled. Furthermore, we notice a positive correlation between engine size and price, as well as engine size and tax. This follows expectation, since it is common practice for manufacturers to sell models with larger engines for a higher price in comparison to the same model with a smaller engine. As a result, due to the higher price, a larger tax payment is required, hence the positive correlation. This also explains the positive correlation between tax and price. \n\nLet us highlight these observations through the use of scatterplots.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.scatterplot(x='mileage',y='price',data=df)\nplt.title('Scatter plot of Mileage against Price')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We notice that the earlier mileage on a vehicle has the most negative impact on the price. This can be seen since the slope on the plot is much steeper for lower mileage, while the rate of decrease of the price reduces as the mileage increases.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.scatterplot(x='engineSize',y='price',data=df)\nplt.title('Scatter Plot of Engine Size against Price')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We clearly see that as the engine size of the vehicle increases, the price tends to increase too. Let us produce a pairplot to discover any relationships that have not been noticed yet.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.pairplot(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no clear and obvious variable relationships shown here that have not already been discussed. \n\n#### 1.2.2: Categorical Variables\n\nIn this section we shall attempt to identify any key trends between our categorical variables and the target variable.\n\n##### 1.2.2.1: Make and Model\n\nLet us investigate how the make and model of the vehicle affects the price. ","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.boxplot(x='make',y='price',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We notice that the German made cars, namely Audi, BMW, Mercedes and Volkswagen, all have a higher price on average than the rest of the manufacturers within the dataset. It appears that there are vehicles from Hyundai and Skoda where the price seems to be an outlier. Let us investigate these two vehicles, starting with the Hyundai.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df[(df['make'] == 'Hyundai') & (df['price'] > 80000)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After doing some research, it can be seen that this make and model of vehicle would cost a customer in the region of £15,000 to buy brand new. As a result, the price of £92,000 for this 3 year old version is clearly a mistake and as a result we shall drop this point from the dataset.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"hyundai_error = df[(df['make'] == 'Hyundai') & (df['price'] > 80000)].index\ndf = df.drop(hyundai_error)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us now investigate the problematic Skoda.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df[(df['make'] == 'Skoda') & (df['price']> 80000)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once again, it can be found that a brand new model of this car costs around £25,000 to purchase. Similarly to above, the price entered here is clearly in error and we shall remove this point from the dataset also.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"skoda_error = df[(df['make'] == 'Skoda') & (df['price']> 80000)].index\ndf = df.drop(skoda_error)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Within this dataset we have over 190 different models of vehicle. Therefore, producing box plots to investigate this in detail would be too complex. However, it is somewhat obvious that the model purchased does have a clear effect on the price of the vehicle.\n\n##### 1.2.2.2: Fuel Type\n\nLet us determine the effect that the fuel type of a vehicle has on its price.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.boxplot(x='fuelType',y='price',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that, on average, petrol vehicles are cheaper to purchase than vehicles with different fuel types. Hybrid vehicles are the most expensive to purchase on average, possibly due to the advanced technology required in order to merge petrol and electric motors. We can clearly observe that the fuel type is an important feature in determining a vehicles sale price.\n\n##### 1.2.2.3: Transmission\n\nLet us determine if the transmission type of a vehicle has an influence on the sale price.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.boxplot(x='transmission',y='price',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first observartion here is that vehicles with manual transmission tend to be cheaper to purchase than other transmission types. This may be due to advanced resources required to design and implement automatic transmission systems. We can clearly see that this feature has a significant influence on the price.\n\n#### 1.2.3: Analysing the Distribution of Numerical Variables\n\nIn order to achieve optimised prediction results, we must first ensure that our numerical features are normally distributed. To do this, we produced histograms and check that the follow the \"bell\" shaped curve.\n\n##### 1.2.3.1: Price","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.distplot(df['price'],bins=50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that our target variable is extremely positively skewed. We shall apply a log transformation of this feature in the data preprocessing section.\n\n##### 1.2.3.2: Year","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.distplot(df['year'],bins=50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this scenario, we notice that our data is negatively skewed.\n\n##### 1.2.3.3: Mileage","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.distplot(df['mileage'],bins=50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our mileage data is positively skewed.\n\n##### 1.2.3.4: Tax","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.distplot(df['tax'],bins=50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have slight positive skew in this case.\n\n##### 1.2.3.5:  MPG","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.distplot(df['mpg'],bins=50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe a positive skew for our \"MPG\" data.\n\n##### 1.2.6: Engine Size","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.distplot(df['engineSize'],bins=50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also notice positive skew in this feature too.\n\n## 2: Data Preprocessing\n\nIn this section, we shall deal with skewed data and create dummy variables for our categorical features. \n\n### 2.1: Dealing with Skewed Data\n\nLet us apply a log transform to our numerical columns in an attempt to reduce skewness.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df['price'] = np.log(df['price'])\ndf['year'] = np.log(df['year'])\ndf['mileage'] = np.log(df['mileage'])\ndf['tax'] = np.log(df['tax'])\ndf['mpg'] = np.log(df['mpg'])\ndf['engineSize'] = np.log(df['engineSize'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2: Creating Dummy Variables\n\nIn order to use our categorical variables in the machine learning algorithms, we must create dummy variables for them. However, let us begin by removing the \"make\" column from our dataset since we can infer this information from the vehicles \"model\".","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df = df.drop('make',axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now create dummy variables for the categorical features within our dataset. We must set the parameter \"drop_first\" to be true in order to reduce multicolinearity.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"transmission = pd.get_dummies(df['transmission'],drop_first=True)\nmodel = pd.get_dummies(df['model'],drop_first=True)\nfueltype = pd.get_dummies(df['fuelType'],drop_first=True)\ndf = pd.concat([df,transmission,model,fueltype],axis=1)\ndf = df.drop(['transmission','model','fuelType'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us check the head of the dataframe to ensure that the dummy variables were created successfully.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3: Creating Training and Test Sets\n\nWe must now create training and test sets for our data.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"X = df.drop('price',axis=1)\ny = df['price']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state=101)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our dataset is now ready for use in the machine learning algorithms.\n\n## 3: Model Creation and Analysis\n\nIn this section we shall implement a range of machine learning algorithms in order to predict the prices of used cars, whilst simultaneously investigating the effects of scaling our data.\n\n### 3.1: Non-scaled Data\n\n#### 3.1.1: Linear Regression\n\nThe first model that we shall implement will be a linear regression model. First we must fit our model to the training data.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now create predictions using our trained model.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"linreg_preds = linreg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since this is a continuous regression problem, our key metrics for analysis purposes are the root mean squared error and the R2 score. Let us import these metrics from scikit-learn and use them to analyse our linear regression model. ","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import r2_score, mean_squared_error\nlinreg_r2 = r2_score(np.exp(y_test),np.exp(linreg_preds))\nlinreg_RMSE = np.sqrt(mean_squared_error(np.exp(y_test),np.exp(linreg_preds)))\nprint(\"Linear Regression R2 Score: {}\".format(linreg_r2))\nprint(\"Linear Regression RMSE: {}\".format(linreg_RMSE))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our R2 score of 0.9 is very good and represents 90% of the variance of the price of a used car based on the independent variables we have used here. Our RMSE value of roughly £3000 is large, but in comparison to the average price of a car in our dataset, this value is reasonable. One reason for this larger value may be due to the fact there remains some vehicles which have an extremely large price, which would be considered outliers due to the being over 1.5x larger than the Upper Quartile price value.\n\n#### 3.1.2: Decision Tree\n\nThe process of training a machine learning model and generating predictions will be the same as described above for all machine learning algorithms implemented from this point forward. We shall now implement the decision tree algorithm.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\ndtr = DecisionTreeRegressor()\ndtr.fit(X_train, y_train)\ndtr_preds = dtr.predict(X_test)\ndtr_r2 = r2_score(np.exp(y_test),np.exp(dtr_preds))\ndtr_RMSE = np.sqrt(mean_squared_error(np.exp(y_test),np.exp(dtr_preds)))\nprint(\"Decision Tree R2 Score: {}\".format(dtr_r2))\nprint(\"Decision Tree RMSE: {}\".format(dtr_RMSE))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The decision tree regressor has managed to explain just over 93% of the variance within the price feature based on the independent variables used in the model. We have also managed to reduce the RMSE by approximately £500 in comparison to the Linear Regression model. \n\n#### 3.1.3: Random Forest","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor()\nrfr.fit(X_train, y_train)\nrfr_preds = rfr.predict(X_test)\nrfr_r2 = r2_score(np.exp(y_test),np.exp(rfr_preds))\nrfr_RMSE = np.sqrt(mean_squared_error(np.exp(y_test),np.exp(rfr_preds)))\nprint(\"Random Forest R2 Score: {}\".format(rfr_r2))\nprint(\"Random Forest RMSE: {}\".format(rfr_RMSE))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our random forest regressor manages to explain approximately 96% of the variance within the dependent variable. The root mean squared error is also the lowest that we have seen, at approximately £1800, which equates to roughly 10% error based on the average price of vehicles within the dataset.\n\n#### 3.1.4: Support Vector Regression","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.svm import SVR\nsvr = SVR()\nsvr.fit(X_train, y_train)\nsvr_preds = svr.predict(X_test)\nsvr_r2 = r2_score(np.exp(y_test),np.exp(svr_preds))\nsvr_RMSE = np.sqrt(mean_squared_error(np.exp(y_test),np.exp(svr_preds)))\nprint(\"Support Vector Regression R2 Score: {}\".format(svr_r2))\nprint(\"Support Vector Regression RMSE: {}\".format(svr_RMSE))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that Support Vector Regression is currently the worst model we have implemented, achieving a R2 score of approximately 87% and a RMSE of approximately £3500.\n\n#### 3.1.5: MLP Regressor","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.neural_network import MLPRegressor\nmlp = MLPRegressor()\nmlp.fit(X_train, y_train)\nmlp_preds = mlp.predict(X_test)\nmlp_r2 = r2_score(np.exp(y_test),np.exp(mlp_preds))\nmlp_RMSE = np.sqrt(mean_squared_error(np.exp(y_test),np.exp(mlp_preds)))\nprint(\"MLP Regressor R2 Score: {}\".format(mlp_r2))\nprint(\"MLP Regressor RMSE: {}\".format(mlp_RMSE))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our MLP regressor managed to achieve an R2 score of approximately 93%, but with a RMSE of around £2600.\n\n#### 3.1.6: Summary of Findings","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"d = {'Model': ['Linear Regression', 'Decision Tree', 'Random Forest', 'Support Vector Regressor', 'MLP Regressor'],\n    'R2 Score': [linreg_r2, dtr_r2, rfr_r2, svr_r2, mlp_r2],\n    'RMSE': [linreg_RMSE, dtr_RMSE, rfr_RMSE, svr_RMSE, mlp_RMSE]}\nresults = pd.DataFrame(data=d)\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that our random forest model achieved both the best R2 score and RMSE. The support vector regressor acheived the worst R2 score. Let us plot this information for visual understanding.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.barplot(x='Model',y='R2 Score',data=results,order=['Support Vector Regressor', 'Linear Regression', 'MLP Regressor','Decision Tree','Random Forest'])\nplt.title('R2 Score for Each Model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.scatterplot(x='R2 Score',y='RMSE',data=results,hue='Model')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2: Scaled Data\n\nIn this section, we shall scale the data and reimplement the same models implemented above. We shall use the Standard Scaler to scale our training and test sets.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us now implement the same models form section 3.1 above.\n\n#### 3.2.1: Decision Tree","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"s_dtr = DecisionTreeRegressor()\ns_dtr.fit(X_train,y_train)\ns_dtr_preds = s_dtr.predict(X_test)\ns_dtr_r2 = r2_score(np.exp(y_test),np.exp(s_dtr_preds))\ns_dtr_RMSE = np.sqrt(mean_squared_error(np.exp(y_test),np.exp(s_dtr_preds)))\nprint(\"Scaled Decision Tree R2 Score: {}\".format(s_dtr_r2))\nprint(\"Scaled Decision Tree RMSE: {}\".format(s_dtr_RMSE))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our decision tree model manages to explain 93% of the variance in our target variable, whilst producing a root mean squared error of approximately £2500.\n\n#### 3.2.2: Random Forest","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"s_rfr = RandomForestRegressor()\ns_rfr.fit(X_train, y_train)\ns_rfr_preds = s_rfr.predict(X_test)\ns_rfr_r2 = r2_score(np.exp(y_test),np.exp(s_rfr_preds))\ns_rfr_RMSE = np.sqrt(mean_squared_error(np.exp(y_test),np.exp(s_rfr_preds)))\nprint(\"Scaled Random Forest R2 Score: {}\".format(s_rfr_r2))\nprint(\"Scaled Random Forest RMSE: {}\".format(s_rfr_RMSE))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The random forest regressor on our scaled training and test sets acheives an R2 score of approximately 96%, with a root mean squared error of roughly £1850.\n\n#### 3.2.3: Support Vector Regression","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"s_svr = SVR()\ns_svr.fit(X_train, y_train)\ns_svr_preds = s_svr.predict(X_test)\ns_svr_r2 = r2_score(np.exp(y_test),np.exp(s_svr_preds))\ns_svr_RMSE = np.sqrt(mean_squared_error(np.exp(y_test),np.exp(s_svr_preds)))\nprint(\"Scaled Support Vector Regression R2 Score: {}\".format(s_svr_r2))\nprint(\"Scaled Support Vector Regression RMSE: {}\".format(s_svr_RMSE))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our support vector regression model based on the scaled data acheives an R2 score of approximately 92% and an RMSE value of roughly £2300.\n\nLet us join these results into a dataframe for a more direct comparison.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"d = {'Model': ['Scaled Decision Tree', 'Scaled Random Forest', 'Scaled Support Vector Regressor'],\n    'R2 Score': [s_dtr_r2,s_rfr_r2,s_svr_r2],\n    'RMSE': [s_dtr_RMSE,s_rfr_RMSE,s_svr_RMSE]}\nscaled_results = pd.DataFrame(d)\nscaled_results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We notice that our scaled random forest regressor is able to explain the most variance within the price of used cars in comaprison to the other models built using the scaled data. We also have the lowest root mean squared error as a result of using this model. \n\nLet us now analyse the results obtained by all of the models we have implemented so far.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"full_results = pd.concat([results,scaled_results],ignore_index=True)\nfull_results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that in both the unscaled and scaled sets, the random forest regressor performed better than all of the other types of regressors implemented within this project. In the case of the random forest regressor, scaling the data only slightly imporved the model, whereas the scaling of the data led to a reduction in the performance in the model. However, scaling the data led to an approximate 7% increase in the amount of variance explained by the support vector regression model. ","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}