{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Read data from dataset and print sample out of it.","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('../input/default-of-credit-card-clients-dataset/UCI_Credit_Card.csv')\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Print metainformation of the dataset","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data preprocessing tasks:\n1. Cleaning - the dataset shows that for certain PAY_* columns, it has -2 as a value, we can assume that any negative value means paying duly, therefore, we should update -2 with -1 or zero for that matter. Once updated, describe data to see the changes taking place in the row depicting minimum value. Further, we'll remove noisy data i.e. unwanted rows from the dataset for e.g. empty rows.\n","metadata":{}},{"cell_type":"code","source":"for col in ['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']:\n    data[col] = np.where(data[col]<=0, 0, data[col]) \ndata = data.dropna()\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Renaming the PAY_0 to PAY_1 because since the column PAY_1 is missing it would make sense to rename the PAY_0 to PAY_1.\nSimilarly, we'll rename the column 'Marriage' to 'Marital_Status', also, 'SEX' to 'GENDER'.","metadata":{}},{"cell_type":"code","source":"data = data.rename(columns={'PAY_0': 'PAY_1', 'MARRIAGE':'MARITAL_STATUS', 'SEX':'GENDER'})\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Reduction - we'll remove remove the dimension ID as we don't need it.","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data analysis using histograms","metadata":{}},{"cell_type":"code","source":"output = 'default.payment.next.month'\ncols = [ f for f in data.columns if data.dtypes[ f ] != \"object\"]\ncols.remove(\"ID\")\ncols.remove(output)\nf = pd.melt( data, id_vars=output, value_vars=cols)\ng = sns.FacetGrid( f, hue=output, col=\"variable\", col_wrap=5, sharex=False, sharey=False )\ng = g.map( sns.histplot, \"value\", kde=True).add_legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations:\n1. Lower the balance limit, higher the chance of defaults\n2. Females are least likely to default\n3. Larger proportion of highly educated are least likely to default\n4. Large proportion of age group 25-45 are least likely to default\n","metadata":{}},{"cell_type":"code","source":"# The quantitative vars:\nquant = [\"LIMIT_BAL\", \"AGE\"]\n\n# The qualitative but \"Encoded\" variables (ie most of them)\nqual_Enc = cols\nqual_Enc.remove(\"LIMIT_BAL\")\nqual_Enc.remove(\"AGE\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logged = []\nfor ii in range(1,6):\n    qual_Enc.remove(\"PAY_AMT\" + str( ii ))\n    data[ \"log_PAY_AMT\" + str( ii )]  = data[\"PAY_AMT\"  + str( ii )].apply( lambda x: np.log1p(x) if (x>0) else 0 )\n    logged.append(\"log_PAY_AMT\" + str( ii ) )\n\nfor ii in range(1,6):\n    qual_Enc.remove(\"BILL_AMT\" + str( ii ))\n    data[ \"log_BILL_AMT\" + str( ii )] = data[\"BILL_AMT\" + str( ii )].apply( lambda x: np.log1p(x) if (x>0) else 0 )\n    logged.append(\"log_BILL_AMT\" + str( ii ) )\n\nf = pd.melt( data, id_vars=output, value_vars=logged)\ng = sns.FacetGrid( f, hue=output, col=\"variable\", col_wrap=3, sharex=False, sharey=False )\ng = g.map( sns.histplot, \"value\", kde=True).add_legend()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = quant + qual_Enc + logged + [output]\ncorr = data[features].corr()\nplt.subplots(figsize=(30,10))\nsns.heatmap( corr, square=True, annot=True, fmt=\".1f\" )  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfeatures = quant + qual_Enc + logged   \nX = data[features].values    \ny = data[ output ].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2)\n\nfrom sklearn.preprocessing import StandardScaler\nscX = StandardScaler()\nX_train = scX.fit_transform( X_train )\nX_test = scX.transform( X_test )\n\n# We'll need some metrics to evaluate our models\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data Analysis/Predictions using various techniques","metadata":{}},{"cell_type":"code","source":"# Random Forest \n\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators=10)\nclassifier.fit( X_train, y_train )\ny_pred = classifier.predict( X_test )\n\ncm = confusion_matrix( y_test, y_pred )\nprint(\"Accuracy on Test Set for RandomForest = %.2f\" % ((cm[0,0] + cm[1,1] )/len(X_test)))\nscoresRF = cross_val_score( classifier, X_train, y_train, cv=10)\nprint(\"Mean RandomForest CrossVal Accuracy on Train Set %.2f, with std=%.2f\" % (scoresRF.mean(), scoresRF.std() ))\n\n# kernel SVM \n\nfrom sklearn.svm import SVC\nclassifier1 = SVC(kernel=\"rbf\")\nclassifier1.fit( X_train, y_train )\ny_pred = classifier1.predict( X_test )\n\ncm = confusion_matrix( y_test, y_pred )\nprint(\"Accuracy on Test Set for kernel-SVM = %.2f\" % ((cm[0,0] + cm[1,1] )/len(X_test)))\nscoresSVC = cross_val_score( classifier1, X_train, y_train, cv=10)\nprint(\"Mean kernel-SVM CrossVal Accuracy on Train Set %.2f, with std=%.2f\" % (scoresSVC.mean(), scoresSVC.std() ))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logistic Regression \n\nfrom sklearn.linear_model import LogisticRegression\nclassifier2 = LogisticRegression()\nclassifier2.fit( X_train, y_train )\ny_pred = classifier2.predict( X_test )\n\ncm = confusion_matrix( y_test, y_pred )\nprint(\"Accuracy on Test Set for LogReg = %.2f\" % ((cm[0,0] + cm[1,1] )/len(X_test)))\nscoresLR = cross_val_score( classifier2, X_train, y_train, cv=10)\nprint(\"Mean LogReg CrossVal Accuracy on Train Set %.2f, with std=%.2f\" % (scoresLR.mean(), scoresLR.std() ))\n\n# Naive Bayes \n\nfrom sklearn.naive_bayes import GaussianNB\nclassifier3 = GaussianNB()\nclassifier3.fit( X_train, y_train )\ny_pred = classifier3.predict( X_test )\ncm = confusion_matrix( y_test, y_pred )\nprint(\"Accuracy on Test Set for NBClassifier = %.2f\" % ((cm[0,0] + cm[1,1] )/len(X_test)))\nscoresNB = cross_val_score( classifier3, X_train, y_train, cv=10)\nprint(\"Mean NaiveBayes CrossVal Accuracy on Train Set %.2f, with std=%.2f\" % (scoresNB.mean(), scoresNB.std() ))\n\n# K-NEIGHBOURS \n\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier4 = KNeighborsClassifier(n_neighbors=5)\nclassifier4.fit( X_train, y_train )\ny_pred = classifier4.predict( X_test )\ncm = confusion_matrix( y_test, y_pred )\nprint(\"Accuracy on Test Set for KNeighborsClassifier = %.2f\" % ((cm[0,0] + cm[1,1] )/len(X_test)))\nscoresKN = cross_val_score( classifier3, X_train, y_train, cv=10)\nprint(\"Mean KN CrossVal Accuracy on Train Set Set %.2f, with std=%.2f\" % (scoresKN.mean(), scoresKN.std() ))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conclusion: \n\n1. The kernel-SVM classifier and logistic regression gives an accuracy of ~82%, therefore, we can use either to predict whether a customer is likely to default next month.\n\n2. We see that being Female, More educated, Single and between 25-45 years old means a customer is more likely to make payments on time.","metadata":{}}]}