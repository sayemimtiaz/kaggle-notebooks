{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport os\nprint(os.listdir(\"../input\"))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Importing the data and forming the inner join between two tables\n\n\ncountry_code= pd.read_excel(\"../input/Country-Code.xlsx\")\ncountry_code.head()\nprint('Shape of country code : ',country_code.shape)\n\nzomato=pd.read_csv('../input/zomato.csv',encoding='latin-1')\nzomato.head()\nprint('Shape of main data : ',zomato.shape)\n\ndata=pd.merge(zomato,country_code,how='inner')\nprint('Shape of data after merging  : ',data.shape)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns=['Restaurant_ID', 'Restaurant_Name', 'Country_Code', 'City', 'Address',\n       'Locality', 'Locality_Verbose', 'Longitude', 'Latitude', 'Cuisines',\n       'Average_Cost_for_two', 'Currency', 'Has_Table_booking',\n       'Has_Online_delivery', 'Is_delivering_now', 'Switch_to_order_menu',\n       'Price_range', 'Aggregate_rating', 'Rating_color', 'Rating_text',\n       'Votes', 'Country']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Helps in finding the number of null values in the whole dataset\n\ndata.isnull().sum() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### There are 9 null values in the cuisines column which will be treated further\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets see the records or ros that have null values in the Cuisines features\n\ndata[data.Cuisines.isnull()==True]\n\n#The common thing that can be observed from the records belo is that,  all the records with null vaues in the cuisine feature belongs to \"United States\"\n#Lets treat the null values after splitting the data into train ans test data. Becuase, we re however going to dropo the Cusines feature in further code.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns #Gives the number of columns that are in the dataset that we want to work on","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Switch_to_order_menu'].value_counts() # Gives us the number of unique responses that are in \"Switch to order menu \" feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%config InlineBackend.figure_format = 'retina'\nplt.figure(figsize=(6,4),dpi=100)\ndata.groupby(['Country']).mean()['Price_range'].sort_values().plot(kind='barh',figsize=(10,6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#In this plot, number of votes given to a restauatnt on an average can be known.\n#We can observe that the votes given are highest for Asian countries(4 out of 5 are Asian) \n\n%config InlineBackend.figure_format = 'retina'\ndf=data.groupby(['Country']).mean()\nplt.figure(figsize=(8,5),frameon=True,dpi=100)\ndf['Votes'].sort_values().plot(kind='barh',figsize=(10,6))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### As we can see that switch to order is not presenet in any of the restaurants, it does not give much information"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Rating_color'].value_counts() # Gives us the number of records that has Orange, White, Yellow etc,. in 'Rating Color' feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Rating_text'].value_counts() # Gives us the number of records that has AVergae, Not rated, Good etc,. in 'Rating text' feature","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above 2 cells, we can see that Rating text and Rating color are representing the same thing in a different format. So, we can delete either of the columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Features with the following features are droppped :\n\n# That are producing redundant information\n# That has 100% percent co relation with other features\n# The features that has no significance in predicting the rating\n# That requires NLP or other complex algorithms to analyse\n\ndata.drop(['Country_Code','Restaurant_ID', 'Restaurant_Name','Address','Locality','Locality_Verbose','Longitude', 'Latitude', 'Switch_to_order_menu','Rating_color'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets rename the feature names without any gaps. Because, gaps in the olumn names may create trouble while indexing.\n#It is good practice to avoid spaces, gaps in the column name\n\ndata.columns=['City', 'Cuisines', 'Avg_cost', 'Currency',\n       'Table_booking', 'Online_delivery', 'Delivering_now',\n       'Price_range', 'Rating', 'Rating_text', 'Votes', 'Country']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets create 2 dataframes in which one has target variable (i.e. Rating) and latter has predictor variables\n# X datframe has predictor variables while y datframe has target variable\n\nX=data.drop('Rating',axis=1)\ny=data['Rating']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here, I am splitting 2 dataframes into 4 parts.\n#They are 2 target variable datframes (i.e. y_train and y_test) and 2 predictor variables datframes (X_train and X_test)\n#X_train is used to train the model using predictor variables while X_test has same features as X_train which is used while testing the model\n#y_train is used as a target variable while training the model while y_test is compared with the predicted values after testing the model\n\n#Importing essential library\nfrom sklearn.model_selection import train_test_split\n\n\n#SPlitting the data into 4 datframes\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature Engineering**"},{"metadata":{},"cell_type":"markdown","source":"### Lets create a new feature which counts the number of cuisines that a restaurant offers. The column Cuisines has few hunderds of different cuisines and some of the same cuisines are written with different spellings. Hence, it is very difficult to rectify those. However, while building a model, when encoding will be done, it will be heavy on computing and also may lead to Curse of Dimensionality as the number of features will become few hundreds."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Helps in finding the popular cuisines\n\ndata.Cuisines.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#As the number of cuisines are very high and to make the code identify each cuisines and count it, NLP might be needed.\n#So, instead of including the cuisisnes itself, we can also count the number of cuisines offered by the restaurant\n# Lets create a new feature that gives the number of cuisines offered by each restaurant\n\nX_train['no_of_cuisines'] = data.Cuisines.str.count(',')+1\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test['no_of_cuisines'] = data.Cuisines.str.count(',')+1\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### As there are 9 null values in the cuisisnes, there are also 9 null values in the no_of_cuisines feature.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#In the X_train datframe, lets see the number of records with different number of cuisines\n\nX_train.no_of_cuisines.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#In the X_test datframe, lets see the number of records with diferent number of cuisisnes\n\nX_test.no_of_cuisines.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Since 2 number of cuisines is mode for United States in both train and test, lets impute the null values with 3 in both X_train and X-test dataframes "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Imputing the null values with the model  in \"no_of_cusisnes\"\n\nX_train[\"no_of_cuisines\"].fillna(2, inplace = True)\nX_test[\"no_of_cuisines\"].fillna(2, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets create a new feature named Continent which inlcudes respective countries that belong to a continent. This feature gives the model some extra information."},{"metadata":{"trusted":true},"cell_type":"code","source":"#A function is being created that helps in assigning continnets to their respective countries\n\ndef continent (x):\n    if (x in ['United States','Canada','Brazil']):\n        return ('Americas')\n    elif (x in ['India','Phillipines','Sri Lanka','UAE' ,'Indonesia' ,'Qatar','Singapore']):\n        return ('Asia')\n    elif (x in ['Australia','New Zealand']):\n        return ('Australia_continent')\n    elif (x in ['Turkey','United Kingdom']):\n        return ('Europe')\n    else:\n        return ('Africa')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here the fuction is being called which creates a new feature named continent by checking with the \"Country\" feature\n\nX_train['Continent']=X_train['Country'].apply(continent)\nX_test['Continent']=X_test['Country'].apply(continent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets plot the amount spent at restaurants in different countries\n\n%config InlineBackend.figure_format = 'retina'\nplt.figure(figsize=(8,5),dpi=100)\ndata.groupby(['Country']).mean()['Avg_cost'].sort_values().plot(kind='barh',figsize=(10,6))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From the above plot, we can see that the cost in Indonesia is very high which is due to the amount they pay for food. In the given data, each country's average cost is mentioned in their respective currency. Lets standardize the currency unit to Dollar. Below conversion rates are according to Google on the day of writing this code"},{"metadata":{"trusted":true},"cell_type":"code","source":"conversion_rates= {'Botswana Pula(P)':0.095, 'Brazilian Real(R$)':0.266,'Dollar($)':1,'Emirati Diram(AED)':0.272,\n    'Indian Rupees(Rs.)':0.014,'Indonesian Rupiah(IDR)':0.00007,'NewZealand($)':0.688,'Pounds(£)':1.314,\n    'Qatari Rial(QR)':0.274,'Rand(R)':0.072,'Sri Lankan Rupee(LKR)':0.0055,'Turkish Lira(TL)':0.188}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['New_cost'] = X_train['Avg_cost'] * X_train['Currency'].map(conversion_rates)\nX_test['New_cost'] = X_test['Avg_cost'] * X_test['Currency'].map(conversion_rates)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5),dpi=100)\nX_train.groupby(['Country']).mean()['New_cost'].sort_values().plot(kind='barh',figsize=(10,6))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now, after bringing all country's cost into Dollar, we can observe that Singapore has the expensive food while India has the least expensive food. "},{"metadata":{},"cell_type":"markdown","source":"Lets create a new feature named Continent which inlcudes respective countries that belong to a continent. This feature gives the model some extra information."},{"metadata":{"trusted":true},"cell_type":"code","source":"def continent (x):\n    if (x in ['United States','Canada','Brazil']):\n        return ('Americas')\n    elif (x in ['India','Phillipines','Sri Lanka','UAE' ,'Indonesia' ,'Qatar','Singapore']):\n        return ('Asia')\n    elif (x in ['Australia','New Zealand']):\n        return ('Australia_continent')\n    elif (x in ['Turkey','United Kingdom']):\n        return ('Europe')\n    else:\n        return ('Africa')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here the fuction is being called which creates a new feature named continent by checking with the \"Country\" feature\n\nX_train['Continent']=X_train['Country'].apply(continent)\nX_test['Continent']=X_test['Country'].apply(continent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#As model can only read numeric values, lets assign values to the rating text, Excellent being the highest(i.e. 5) and poor being the least (i.e. 1)\n# These texts will be replaced by the given numbers in train and test data. So that we can include this feature in the model \n\ndictionary = {'Excellent': 5,'Very Good': 4,'Average': 2,'Good': 3,'Not rated': 2,'Poor': 1} \nX_train.Rating_text = [dictionary[item] for item in X_train.Rating_text] \nX_test.Rating_text = [dictionary[item] for item in X_test.Rating_text] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here encoding is being done in both X_train and X_test dataframes\n\nBinary= {'Yes': 1,'No': 0} \n\nX_train.Online_delivery = [Binary[item] for item in X_train.Online_delivery] \nX_train.Table_booking = [Binary[item] for item in X_train.Table_booking] \nX_train.Delivering_now = [Binary[item] for item in X_train.Delivering_now] \n\nX_test.Online_delivery = [Binary[item] for item in X_test.Online_delivery] \nX_test.Table_booking = [Binary[item] for item in X_test.Table_booking] \nX_test.Delivering_now = [Binary[item] for item in X_test.Delivering_now] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of cities in the data : ',len(data.City.unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets drop avg_cost feature as new feature is created (i.e. New_cost)\n#Cuisines feature is not required as numer of cuisines is created\n# Currecny feature is not required as we have standardized everything into dollars\n# Lets drop city feature also as there are 141 different cities and when encoding is done, it may create a curse of dimensionality\n# If we feel city feature is mandatory, 141 new features will be created and to reduce the dimensions, we need to do PCA\n# In this code, as PCA is not being done, lets drop City feature too\n\nX_train.drop(['Avg_cost','Cuisines','Currency','City'],axis=1,inplace=True)\nX_test.drop(['Avg_cost','Cuisines','Currency','City'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Brazil has the least number of votes while Indonesia has the maximum"},{"metadata":{"trusted":true},"cell_type":"code","source":"#ENcoding is being done for continents in train and test data sets\n\ntrain_conti=pd.DataFrame(pd.get_dummies(X_train.Continent))\ntest_conti=pd.get_dummies(X_test.Continent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ENcoding is being done for countries in train and test data sets\n\ntrain_countr=pd.get_dummies(X_train.Country)\ntest_countr=pd.get_dummies(X_test.Country)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The encoded dataframes are being merged to the train and test datasets\n\nX_train=pd.concat([X_train,train_conti,train_countr],axis=1)\nX_test=pd.concat([X_test,test_conti,test_countr],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#As country and continent features are included in the datasets in the form of encoded data, lets drop the orginal features\n\nX_train.drop(['Country','Continent'],axis=1,inplace=True)\nX_test.drop(['Country','Continent'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets rename the columns in both train and test datasets\n\nX_train.columns=['Table_booking', 'Online_delivery', 'Delivering_now', 'Price_range',\n       'Rating_text', 'Votes', 'no_of_cuisines', 'New_cost', 'Africa',\n       'Americas', 'Asia', 'Australia_continent', 'Europe', 'Australia',\n       'Brazil', 'Canada', 'India', 'Indonesia', 'NewZealand', 'Phillipines',\n       'Qatar', 'Singapore', 'SouthAfrica', 'SriLanka', 'Turkey', 'UAE',\n       'UnitedKingdom', 'UnitedStates']\nX_test.columns=['Table_booking', 'Online_delivery', 'Delivering_now', 'Price_range',\n       'Rating_text', 'Votes', 'no_of_cuisines', 'New_cost', 'Africa',\n       'Americas', 'Asia', 'Australia_continent', 'Europe', 'Australia',\n       'Brazil', 'Canada', 'India', 'Indonesia', 'NewZealand', 'Phillipines',\n       'Qatar', 'Singapore', 'SouthAfrica', 'SriLanka', 'Turkey', 'UAE',\n       'UnitedKingdom', 'UnitedStates']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scaling. Lets apply zscore for the purpose of data scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing libraries\n\nfrom sklearn import model_selection\nfrom scipy.stats import zscore\nfrom sklearn.metrics import explained_variance_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Zscore scaling is being done here in both train and test datsets\n\ntrain_scale=pd.DataFrame(zscore(X_train,axis=1))\ntest_scale=pd.DataFrame(zscore(X_test,axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#After scaling the dataset, there will be no feature names. So, lets give feature names\n\ntrain_scale.columns=['Table_booking', 'Online_delivery', 'Delivering_now', 'Price_range',\n       'Rating_text', 'Votes', 'no_of_cuisines', 'New_cost', 'Africa',\n       'Americas', 'Asia', 'Australia_continent', 'Europe', 'Australia',\n       'Brazil', 'Canada', 'India', 'Indonesia', 'NewZealand', 'Phillipines',\n       'Qatar', 'Singapore', 'SouthAfrica', 'SriLanka', 'Turkey', 'UAE',\n       'UnitedKingdom', 'UnitedStates']\ntest_scale.columns=['Table_booking', 'Online_delivery', 'Delivering_now', 'Price_range',\n       'Rating_text', 'Votes', 'no_of_cuisines', 'New_cost', 'Africa',\n       'Americas', 'Asia', 'Australia_continent', 'Europe', 'Australia',\n       'Brazil', 'Canada', 'India', 'Indonesia', 'NewZealand', 'Phillipines',\n       'Qatar', 'Singapore', 'SouthAfrica', 'SriLanka', 'Turkey', 'UAE',\n       'UnitedKingdom', 'UnitedStates']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets see how the scaling has transformed our original data\n\ntrain_scale.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets round off the values as after certan  nnumber of decimal places, the values does not make significant difference and becomes heavy on the computation\n\ntrain_scale=np.round(train_scale,decimals=4)\ntest_scale=np.round(test_scale,decimals=4)\ny_train=np.round(y_train,decimals=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LINEAR REGRESSION"},{"metadata":{"trusted":true},"cell_type":"code","source":"# implementation of Linear Regression model using scikit-learn\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LinearRegression() # Defing the linear regression model \nlr.fit(train_scale,y_train) #Fitting the data into the algorithm\nlr_pred = lr.predict(test_scale) #Predicting using Linear regression model\n\n#Metrics for comaprision between prediction and original values\nprint(r2_score(y_test,np.round(lr_pred,decimals=1))) \nprint('RMSE score through Linear regression : ',np.sqrt(metrics.mean_squared_error(y_test,np.round(lr_pred,decimals=1))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\ndt= DecisionTreeRegressor()\ndt.fit(train_scale,y_train)\ndt_pred=dt.predict(test_scale)\nprint('RMSE score through Decision tree regression : ',np.sqrt(metrics.mean_squared_error(y_test,np.round(dt_pred,decimals=1))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RANDOM FOREST"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrf= RandomForestRegressor()\nrf.fit(train_scale,y_train)\nrf_pred=rf.predict(test_scale)\nprint('RMSE score through Random Forest : ',np.sqrt(metrics.mean_squared_error(y_test,np.round(rf_pred,decimals=1))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### In the following plots, x axis shos the actual rating values while y axis shows the predicted ratings. Ideally when the modes predict exactly same as the actual values, there has to be a straight line passing through origin. \n\n\n#### Since, 100% accuracy is not possible, a slim distribution of points is acceptable. Slimmer the distrbution better the accuracy of the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=(6,4),dpi=100)\nplt.plot(y_test,np.round(rf_pred,decimals=1),'*')\nplt.xlabel('Actual Rating',size=11)\nplt.ylabel('Predicted Ratinge using Random Forest',size=11)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,4),dpi=100)\nplt.plot(y_test,rf_pred,'*',label='Random forest')\nplt.plot(y_test,dt_pred,'o',color='red',label='Decision tree',marker='s',markersize=4)\nplt.legend()\nplt.xlabel('Actual VRating',size=11)\nplt.ylabel('Predicted Rating',size=11)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### From the above scatter plots, we can observe that the decision tree has more spread out predictions when compared to slimmer scatter points of Random Forest. We can visually see that the Random forest has better performance than the decision tree."},{"metadata":{},"cell_type":"markdown","source":"### XG BOOST REGRESSOR"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost\nxgb = xgboost.XGBRegressor(n_estimators=100, learning_rate=0.3, max_depth=4)\nxgb.fit(train_scale,y_train)\nxgb_pred= xgb.predict(test_scale)\nprint('RMSE score through XGBoost : ',np.sqrt(metrics.mean_squared_error(y_test,np.round(xgb_pred,decimals=1))))\nprint('R square value using XGBoost',r2_score(y_test,xgb_pred))\nprint('Variance covered by XG Boost Regression : ',explained_variance_score(xgb_pred,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Learning rate and max depth re taken in such a way that it gives the least rmse score. Mutliple scores are obtained by trial and error method."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,4),dpi=100)\nplt.plot(y_test,np.round(xgb_pred,decimals=1),'*')\nplt.xlabel('Actual Value',size=11)\nplt.ylabel('Predicted Value using Random Forest',size=11)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comparison among different models predicting the continous variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('RMSE score through Linear Regression : ',np.sqrt(metrics.mean_squared_error(y_test,lr_pred)))\nprint('R square value using Linear Regression',r2_score(y_test,np.round(lr_pred,decimals=1)))\nprint('Variance covered by Linear Regression : ',explained_variance_score(lr_pred,y_test))\nprint('\\n')\nprint('RMSE score through Decision tree Regression : ',np.sqrt(metrics.mean_squared_error(y_test,dt_pred)))\nprint('R square value using Decision Tree Regression',r2_score(y_test,np.round(dt_pred,decimals=1)))\nprint('Variance covered by Decision Tree Regression : ',explained_variance_score(dt_pred,y_test))\nprint('\\n')\nprint('RMSE score through Random Forest : ',np.sqrt(metrics.mean_squared_error(y_test,np.round(rf_pred,decimals=1))))\nprint('R square value using Random Forest',r2_score(y_test,rf_pred))\nprint('Variance covered by Random Forest : ',explained_variance_score(rf_pred,y_test))\nprint('\\n')\nprint('RMSE score through XGBoost : ',np.sqrt(metrics.mean_squared_error(y_test,np.round(xgb_pred,decimals=1))))\nprint('R square value using XGBoost',r2_score(y_test,xgb_pred))\nprint('Variance covered by XG Boost Regression : ',explained_variance_score(xgb_pred,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusons\n\n>- XG Boost regressor has the least rmse score which indirectly tells us that it is the most accurte model among the 4 models\n>- R square value and Variance explained are almost equal\n>- Variance explained by XG Boost regressor and Random forest are amost equal\n>- Generally, Random Forest is expected to have lower RMSE than a Decsion Tree which is evident in our case study\n>- Due to the assumptions of Linear Regression algorithm which is a bit offset from the real case scenaro, Linear Regression has under performed"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}