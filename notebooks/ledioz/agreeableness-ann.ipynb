{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/big-five-personality-test/IPIP-FFM-data-8Nov2018'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysing AGR4 based on response latency**\n\nIn this notebook, the goal is to create a ANN model, that predicts how much one sympathizes with other's feelings based on the response latency of all 50 statements. So based on how much you think of choosing a number from 1 to 5 for each statement, you can predict different psychological characteristics.\n\nFor this analysis I used only 60000 samples from the dataset, because of my CPU limitations. First we need to clean the rows that hold any NaN values. Also I removed the rows that had a 0 value in the **y** column, because the values should only range from 1 to 5."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport csv\nimport numpy as np\n\ndf = pd.read_csv('/kaggle/input/big-five-personality-test/IPIP-FFM-data-8Nov2018/data-final.csv',sep='\\t')\ndf =df[~df.isin([np.nan, np.inf, -np.inf]).any(1)]\ndf.dropna(axis = 0, how= 'any')\n\ndf = df[~(df == 0).any(axis=1)]\n\nx = df.iloc[0:60000,:].values\n\nx_str = str(x)\nx_str = x_str.replace('[','')\nx_str = x_str.replace(']','')\n\ncount = len(x_str.split('\\n'))\na_split = np.array_split(x, count)\n\nwith open('/kaggle/working/OCEAN_60000.csv', 'a', newline='') as f:\n    writer = csv.writer(f)\n    for i in a_split:\n         writer.writerows(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we take all the data samples for response latency for each statement, so it is 50 columns in total. The column that we are trying to predict is the 24th, which stands for AGR4 \"I sympathize with others' feelings\". \n\nSince this is a multiclass analysis we have to keep in mind that for ANN we have to use Softmax activation layer, but first we need to preprocess the dataset. We have to use One Hot Encoding on the y column before we add it to the train & test split.\n\nAfter splitting the data for training and testing, we scale the data and than we get into the ANN model. I have used two hidden layers in this case, with dropout at 0.1 because we don't want our model to overfit.\n\nIn this case I have used a batch size of 150 and 100 epochs to train the model. The accuracy of the model is at 80.5 % with a test score at 0.42. \n\nOfcourse this is far from the ideal accuracy, but I uploaded this notebook to give an analysis idea on this interesting dataset. It can further improve with more epochs and with the full dataset."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Importing data\ndata = pd.read_csv('/kaggle/working/OCEAN_60000.csv')\n\nX = data.iloc[:, 49:99].values\ny = data.iloc[:, 23].values\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlable_y = LabelEncoder()\ny = lable_y.fit_transform(y)\n\nonehot = OneHotEncoder()\ny = onehot.fit_transform(y.reshape(-1,1)).toarray()\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)\n\n#Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\n\n# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(output_dim=60, init='uniform', activation='relu', input_dim=50))\nclassifier.add(Dropout(p = 0.1))\n\nclassifier.add(Dense(output_dim=30, init='uniform', activation='relu'))\nclassifier.add(Dropout(p = 0.1))\n\nclassifier.add(Dense(output_dim=5, init='uniform', activation='softmax'))\n\n# Compiling the ANN\nclassifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Fitting the ANN to the Training set\nclassifier.fit(X_train, y_train, batch_size=150, nb_epoch=100)\n\nscore = classifier.evaluate(X_test, y_test, batch_size = 150)\nprint(\"Test score: \", score[0])\nprint(\"Test accuracy: \", score[1]*100, \"%\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}