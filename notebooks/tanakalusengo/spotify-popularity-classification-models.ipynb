{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Spotify - Popularity Classification \n### All Time Top 2000s Mega Dataset\n","metadata":{}},{"cell_type":"markdown","source":"TABLE OF CONTENT\n\n0. INTRODUCTION & PROJECT GOAL\n1. IMPORTING LIBRARIES\n2. DATA DESCRIPTION & CLEANING\n3. EXPLORATORY ANALYSIS & VISUALISATIONS\n4. MODELLING DATA\n5. FINAL CONCLUSIONS","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-09-07T14:23:40.851467Z","iopub.execute_input":"2021-09-07T14:23:40.853471Z","iopub.status.idle":"2021-09-07T14:23:40.865498Z","shell.execute_reply.started":"2021-09-07T14:23:40.853404Z","shell.execute_reply":"2021-09-07T14:23:40.863253Z"}}},{"cell_type":"markdown","source":"### 0 INTRODUCTION\n\n#### Data Collection:\n\nThis Dataset was collected from Kaggle.com\n\n- Link: https://www.kaggle.com/iamsumat/spotify-top-2000s-mega-dataset\n\n#### Context\n\n- This dataset contains audio statistics of roughly the top 2000 tracks from between 1956 to 2019 on Spotify. The data contains 15 columns each describing the track and it's qualities. \n\n#### Acknowledgements\n\nThis data is extracted from the Spotify playlist - Top 2000s on PlaylistMachinery(@plamere) using Selenium with Python. More specifically, it was scraped from http://sortyourmusic.playlistmachinery.com/. \n\n#### Content Variables\n\n1. Index: ID\n2. Title: Name of the Track\n3. Artist: Name of the Artist\n4. Top Genre: Genre of the track\n5. Year: Release Year of the track\n6. Beats per Minute(BPM): The tempo of the song\n7. Energy: The energy of a song - the higher the value, the more energtic. song\n8. Danceability: The higher the value, the easier it is to dance to this song.\n9. Loudness: The higher the value, the louder the song.\n10. Valence: The higher the value, the more positive mood for the song.\n11. Length: The duration of the song.\n12. Acoustic: The higher the value the more acoustic the song is.\n13. Speechiness: The higher the value the more spoken words the song contains\n15. Popularity: The higher the value the more popular the song is.","metadata":{}},{"cell_type":"markdown","source":"# PROJECT GOAL & INTERESTS:\n\n1. The goal is to build a classification models using: **Linear Regression, Decision Tree Classifier & Naive Bayes.**\n- Will look to classify a songs level of popularity based off of given feature metrics as mention above.\n\nAlong the way we will look at other interests such as:\n\n2. Most popular Genres and Artists of all time from 1950s to 2000s?\n3. Is there a trend in genres preferred back in the day vs now?\n4. What other variables have an impact on the popularity metric? ","metadata":{}},{"cell_type":"markdown","source":"# 1 IMPORTING LIBRARIES","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy import stats, special\n\nimport seaborn as sns\nfrom seaborn import pairplot, heatmap\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.offline import plot, iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom sklearn import model_selection, metrics, linear_model, tree, datasets, feature_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2 DATA DESCRIPTION & CLEANING\n\n#### 2.1 DATA DESCRIPTION","metadata":{}},{"cell_type":"code","source":"# Loading datadataset & View\n\nspotify_df = pd.read_csv('../input/spotify-top-2000s-mega-dataset/Spotify-2000.csv') \nspotify_df.head()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Overview of Dataset information and data types\n\nspotify_df.info() ","metadata":{"_kg_hide-input":false,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Overview of Dataset numerical data\n\nspotify_df.describe()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Number of genres that have featured in the all time top 2000.\n\nlen(spotify_df[\"Top Genre\"].unique())","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of times each genre features in the all time top 2000.\n\nspotify_df[\"Top Genre\"].value_counts()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Raw Dataset  Summary:\n\n#### The Dataset contains:\n- 1994 entries\n- 1994 non-null entries\n- 15 total variable columns\n- 149 Genre entries\n\n#### Data Types:\n- 4 categorical columns\n- 11 numerical columns\n\n#### Numerical Data:\n- The data set is between years 1994 - 2019. Just about 63 years worth of most popular songs as classified by spotify.\n- min Popularity of a song is 11 and max is 100.\n\n\n### Initial Analysis & Progression:\n\n1. It's clear that Rock music seems to be the all time favourite genre with the most features. But that being said the data is lob-sidded towards pre-2000s and music taste does tend to change over the years so this can be be investigated further in the EDA.","metadata":{}},{"cell_type":"markdown","source":"### 2.2 Data Cleaning\n\n#### Action:\n\n1. Convert column data types.\n2. Remove unecessary columns. \n3. Adjust column titles.\n4. consolidate genre column as there are many variations of a single genre e.g. 'dutch pop' and 'dance pop' or 'album rock'and alternative rock. we will make these columns just 'pop' or just 'rock' as to provide a more accurate summarised representation of that genres.","metadata":{}},{"cell_type":"code","source":"#Converting Length (Duration) to an integer data type\n\nspotify_df.replace(',','', regex=True, inplace=True)\nspotify_df['Length (Duration)']= spotify_df['Length (Duration)'].apply(pd.to_numeric,errors='coerce')\nprint('Length (Duration) is now a -->',spotify_df['Length (Duration)'].dtype, 'data type')","metadata":{"scrolled":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Removing the Index column.\nspotify_df.drop(columns = ['Index'], inplace = True)\n\n#Converting all column titles to lowercase.\nspotify_df.columns = map(str.lower, spotify_df.columns)\n\n#Coverting column names to have no space between, if they do, replace space with an underscore \"_\"\nspotify_df.rename(columns = {'top genre' : 'genre', 'beats per minute (bpm)':'beats_per_minute','loudness (db)': 'loudness','length (duration)': 'duration'}, inplace = True)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spotify_df.info()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spotify_df.head(3)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Consolidating genre column","metadata":{}},{"cell_type":"code","source":"# function to split the genre column\n    \ndef genre_splitter(genre):\n    result = genre.copy()\n    result = result.str.split(\" \",1)\n    for i in range(len(result)):\n        if (len(result[i]) > 1):\n            result[i] = [result[i][1]]\n    return result.str.join('')\n\n#loop until the genre cannot be split any further\n\nnew_genre = spotify_df['genre'].copy()\nwhile(max((new_genre.str.split(\" \", 1)).str.len()) > 1):\n    new_genre = genre_splitter(new_genre)\n    \nprint('New Total of Genres from 146 to -->', len(new_genre.unique()))","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_genre.value_counts()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analysis:\n- Above shows consolidated genre column into single/more generalistic genres into rock, pop etc.\n- There is also an expected increase in values due to the consolidating the genres.","metadata":{}},{"cell_type":"code","source":"#inputting new column values from new_genre to genre in dataframe.\n\nspotify_df['genre'] = new_genre\nspotify_df['genre']","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3 EXPLORATORY ANALYSIS & VISUALISATIONS\n\n\n-  In this section we will investigate the data. Taking a particular look at our target features: \"Popularity\" & \"Genre\" and their correlating variables within the data set.","metadata":{}},{"cell_type":"markdown","source":"### 3.1 Most popular Genres & Artists from 1950s to 2000s?","metadata":{}},{"cell_type":"code","source":"# Create a function top_10, which takes a single parameter for a column. \n# Group the data by the desired column input, sum the values the remaining columns, sort sum values by 'Popularity' column \n# from highest to lowest, the print the top 10 rows. \n\ndef top_10(column):\n    top_10_songs = spotify_df.groupby([column]).sum().sort_values('popularity', ascending=False).head(10)\n    return(top_10_songs[['popularity']])  # Only show 'popularity' column.\n\ntop_10('genre')","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#Use the same function for the Artists\n\ntop_10('artist')","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.1 Conclusion:\n\n- The above shows the accumulation of most popular Genres & Artists of all time. We can also see there is a significant amount of Rock music genre entries compared to the rest of the genres. So there was always a high possibility this will be the most popular overall. Though Michael Jackson, would be an outlier here due to his popularity. \n\n- But in terms of 'pure' popularity as seen below, values from 0-100, the story is a little different. We can see that pop music and its many variations has a majority in populularity in more recent years. This may indicate a shift in popularity over the years as well as musical listening trends due to advancements in technology.","metadata":{}},{"cell_type":"code","source":"pure_popularity = spotify_df.sort_values('popularity', ascending=False).head(10)\npure_popularity[['genre', 'year', 'popularity']]","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2  Is there a trend/shift in genres prefered to pre-2000s vs now over the years?\n\n#### - To approach this, I will **split the dataset into quarters (n/4) and track the popularity change** in genre, as well as the genre entry count over the years. ","metadata":{}},{"cell_type":"code","source":"# Split of 'Year' column data into 4x equal-dispersed buckets in ascending order from 1956 - 2019:\n\nspotify_df['year'] = pd.qcut(spotify_df['year'], q=4, labels=[1, 2, 3, 4]) \nspotify_df['year'].value_counts()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=['YB-1', 'YB-2', 'YB-3', 'YB-4']\ny=spotify_df['year'].value_counts()\n\n_ = sns.barplot(x=x, y=y, palette=\"BrBG\")\n_.set(xlabel='Year Block (YB) divided into 4/4', ylabel='Year Count')","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function for creating year block sets (1 - 4):\n\ndef year_block(year_no):\n    block = spotify_df.loc[spotify_df['year'] == year_no]\n    return block[['genre', 'year', 'popularity']].sort_values('popularity', ascending=False)\n\n# Function for creating Top 5 genre value counts for Pie Chart visual:\n\ndef genre_count(year_block):\n    return year_block['genre'].value_counts().head()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pie Chart Visuals of Year Blocks showing trend/shift in genres between (1956 - 2019)","metadata":{}},{"cell_type":"code","source":"#Creation of Year block 1 & Genre counter for Pie Chart Viaual:\nyear_block_1 = year_block(1) \ngenre_count(year_block_1)\n   \n#Pie chart for year block 1:\nvalues = genre_count(year_block_1).values\nnames = genre_count(year_block_1).index\ncolors = ['gold', 'mediumturquoise', 'darkorange', 'lightgreen', 'AntiqueWhite']\n\nfig = go.Figure(data=[go.Pie(labels=names, values=values, pull=[0.1, 0, 0, 0, 0])])\nfig.update_traces(hoverinfo='label+percent', textinfo='value', textfont_size=20,\n                  marker=dict(colors=colors, line=dict(color='#000000', width=1.5)))\nfig.update_layout(title_text='Year Block 1')\nfig.show()\n   \n    \n#Creation of Year block 2 & Genre counter for Pie Chart Viaual:\nyear_block_2 = year_block(2) \ngenre_count(year_block_2)\n   \n#Pie chart for year block 2:\nvalues = genre_count(year_block_2).values\nnames = genre_count(year_block_2).index\ncolors = ['gold', 'mediumturquoise', 'darkorange', 'lightgreen', 'AntiqueWhite']\n\nfig = go.Figure(data=[go.Pie(labels=names, values=values, pull=[0.1, 0, 0, 0, 0])])\nfig.update_traces(hoverinfo='label+percent', textinfo='value', textfont_size=20,\n                  marker=dict(colors=colors, line=dict(color='#000000', width=1.5)))\nfig.update_layout(title_text='Year Block 2')\nfig.show()\n\n\n#Creation of Year block 3 & Genre counter for Pie Chart Viaual:\nyear_block_3 = year_block(3) \ngenre_count(year_block_3)\n   \n#Pie chart for year block 3:\nvalues = genre_count(year_block_3).values\nnames = genre_count(year_block_3).index\ncolors = ['gold', 'mediumturquoise', 'darkorange', 'lightgreen', 'AntiqueWhite']\n\nfig = go.Figure(data=[go.Pie(labels=names, values=values, name=\"Year Block 3\", pull=[0.1, 0, 0, 0, 0])])\nfig.update_traces(hoverinfo='label+percent', textinfo='value', textfont_size=20,\n                  marker=dict(colors=colors, line=dict(color='#000000', width=1.5)))\nfig.update_layout(title_text='Year Block 3')\nfig.show()\n\n\n#Creation of Year block 4 & Genre counter for Pie Chart Viaual:\nyear_block_4 = year_block(4) \ngenre_count(year_block_4)\n   \n#Pie chart for year block 4:\nvalues = genre_count(year_block_4).values\nnames = genre_count(year_block_4).index\ncolors = ['mediumturquoise','gold', 'darkorange', 'lightgreen', 'AntiqueWhite']\n\nfig = go.Figure(data=[go.Pie(labels=names, values=values, pull=[0.1, 0, 0, 0, 0])])\nfig.update_traces(hoverinfo='label+percent', textinfo='value', textfont_size=20,\n                  marker=dict(colors=colors, line=dict(color='#000000', width=1.5)))\nfig.update_layout(title_text='Year Block 4')\nfig.show()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.2 Conclusion: Pie Chart Analysis:\n- As per above, we can clearly see a shift in genre preference, from Rock leaning into pop music at the end of the quater 4/4. But, the level of popularity for pop music has a much higher rate than that of rock music at it's peak as we shall see below.","metadata":{}},{"cell_type":"markdown","source":"## Genre Popularity Count over the years","metadata":{}},{"cell_type":"code","source":"# Joining the year_block heads for top genre count:\n\nframes = [year_block_1.head(), year_block_2.head(), year_block_3.head(), year_block_4.head()]\ntop_genre_df = pd.concat(frames)\ntop_genre_df","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 What other features have an impact on the popularity of a song?\n\n- We shall look to explore numerical variables such as the audio features and look for correlation bewteen each feature in order to help define usable features to build an accurate model. First let's remove the 'title' and 'year column as all values are unique and will not help with our classification model.","metadata":{}},{"cell_type":"code","source":"spotify_df.drop(columns = ['title','year'], inplace = True)\nspotify_df.head()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Note:\n- Music genres that have a single values would make our model inefficient, since it does not have enough data to work off of, so these values and corresponding rows in the original dataframe will be removed. Therefore: \n\n#### - Genre's with a value count less than 20x shall be removed","metadata":{}},{"cell_type":"code","source":"unique = spotify_df['genre'].unique()\nto_remove = [] \n\n# genres that have a single instance only will be placed within the to_remove array\nfor genre in unique:\n    if spotify_df['genre'].value_counts()[genre] < 20: \n        to_remove += [genre]\n\nprint('Genre Values to be removed from data set =', len(to_remove))","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### - Now to replace our original genre column with the updated version","metadata":{}},{"cell_type":"code","source":"spotify_df.set_index([\"genre\"],drop = False, inplace = True)\nfor name in to_remove:\n    type(name)\n    spotify_df.drop(index = str(name), inplace = True)\n    \nspotify_df.head()","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spotify_df = spotify_df.reset_index(drop=True)\nspotify_df.head()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### - As you can see genre's have been removed with those having an instance less than 20.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.heatmap(spotify_df.corr(),annot=True,cmap='BrBG')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3 Conclusion:\n\n#### Target Feature: 'Popularity'\nFrom the above heat map, though not very dtrong, we can see that the strongest features that correlate with Popularity is:\n- loudness (17%)\n- danceability (13%)\n- energy (12%)\n- valence (10%)\n\n#### Variable relationships: \nWe can see the strongest relationship between the variables excl. paopularity are:\n- loudness & energy (74%)\n- valence & danceability (53%)\n- valence & energy (43%)","metadata":{}},{"cell_type":"markdown","source":"# 4 MODELLING DATA","metadata":{}},{"cell_type":"markdown","source":"## --> Model (Popularity Classification)\n\n**Popularity predictor Model:** Will look to classify a songs level of popularity based off of given feature metrics as mention above.\n\n**Steps:**\n1. We need to one hot-encode (get dummy variables) for the genre, artist & popularity feature for higher accuracy.\n2. Split & Scale Data\n3. Modele & Train Data","metadata":{}},{"cell_type":"markdown","source":"### Step 1: One hot-encode Genre & Artist Features:\n\n#### - Create dummy variables for the genre column.","metadata":{}},{"cell_type":"code","source":"# Function for creating dummy variables\n\ndef dummies(df, column, prefix):\n    df = df.copy()\n    dummies = pd.get_dummies(df[column], prefix=prefix)\n    df = pd.concat([df, dummies], axis=1)\n    df = df.drop(column, axis=1)\n    return df\n\nspotify_df = dummies(spotify_df, 'genre', 'genre')\nspotify_df = dummies(spotify_df, 'artist', 'artist')\nspotify_df.head()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### - Classify target feature: 'popularity' into 2x bins, to help better classify our data for our predictive modeling.\n\n- 1 = 'most popular' \n- 0 = 'least popular'  ","metadata":{"execution":{"iopub.status.busy":"2021-09-07T23:34:04.060509Z","iopub.execute_input":"2021-09-07T23:34:04.060955Z","iopub.status.idle":"2021-09-07T23:34:04.068567Z","shell.execute_reply.started":"2021-09-07T23:34:04.060919Z","shell.execute_reply":"2021-09-07T23:34:04.067063Z"}}},{"cell_type":"code","source":"spotify_df['popularity'] = pd.qcut(spotify_df['popularity'], q=2, labels=[0, 1]) \nspotify_df[['popularity']].head()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_v = spotify_df['popularity'].value_counts(normalize=True).round(3)\ntarget_v","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.barplot(x=target_v.index, y=target_v, palette=\"BrBG\")","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analysis\n\n- As we see above, we have a very good split for our target variable. This will greatly minimise class imbalance and will allow our model to be more accurate.","metadata":{}},{"cell_type":"markdown","source":"### Step 2: Splitting & Scaling Data","metadata":{}},{"cell_type":"code","source":"# Chosing independant and dependant variables:\n\ny = spotify_df.loc[:,'popularity'] #dependant \nX = spotify_df[['loudness', 'danceability', 'energy', 'valence']] #independant \n\n# Split data into training and test sets: 80% training, 20% test split.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Shape of Data:')\nprint('-')\nprint('X_train:', X_train.shape, 'y_train:', y_train.shape)\nprint('X_test:', X_test.shape,'y_test:', y_test.shape)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scale the independant variable for relatively normal distribution of the data. \n\nX_train = StandardScaler().fit_transform(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling & Training\n\nDue to the nature of the dataset, we will be implementing classification type machine learning with the following:\n- Logistic Regression\n- Decision Tree Classifier\n- Naive Bayes ","metadata":{"execution":{"iopub.status.busy":"2021-09-07T23:12:20.468925Z","iopub.execute_input":"2021-09-07T23:12:20.469536Z","iopub.status.idle":"2021-09-07T23:12:20.475209Z","shell.execute_reply.started":"2021-09-07T23:12:20.469491Z","shell.execute_reply":"2021-09-07T23:12:20.474207Z"}}},{"cell_type":"markdown","source":"## Training Model Accuracy Outputs:","metadata":{}},{"cell_type":"code","source":"# Creating Models\n\nlogr_model = linear_model.LogisticRegression(solver='liblinear')\ndtree_model = tree.DecisionTreeClassifier()\nnb_model = GaussianNB()\n\n# Training the models\n\nlogr_model.fit(X_train, y_train)\ndtree_model.fit(X_train, y_train)\nnb_model.fit(X_train, y_train)\n\n# Accuracy of trained models with training data:\n\nlogr_train_acc = logr_model.score(X_train, y_train)\ndtree_train_acc = dtree_model.score(X_train, y_train)\nnb_train_acc = nb_model.score(X_train, y_train)\n\nprint('Training Model Accuracy Outputs:')\nprint('-')\nprint('Logistic Regression:', round(logr_train_acc*100,2),'%')\nprint('Decision Tree:', round(dtree_train_acc*100,2),'%')\nprint('Naive Bayes:', round(nb_train_acc*100,2),'%')","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.bar(x=['Logistic Regression', 'Decision Tree model', 'Naive Bayes'], \n             y=[logr_train_acc, dtree_train_acc, nb_train_acc], \n            color=['Logistic Regression', 'Decision Tree model', 'Naive Bayes'],\n             labels={'x': 'Model', 'y': 'Accuracy'},\n            title='Accuracy of trained models with training data')\nfig.show()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analysis:\n- As seen above it appears that the Decision Tree Model has a really high accuracy rate against the training data. But this seems a bit too high and I suspect over fitting. I will validate this through k-fold cross validation.","metadata":{}},{"cell_type":"markdown","source":"## Validating models with k-fold cross validation:","metadata":{}},{"cell_type":"code","source":"# Validating models with k-fold cross validation method\n\nkf = model_selection.KFold(n_splits=10, shuffle=True, random_state=24)\n\naccuracy_logr = cross_val_score(logr_model, X_train, y_train, scoring=\"accuracy\", cv=kf)\naccuracy_dtree = cross_val_score(dtree_model, X_train, y_train, scoring=\"accuracy\", cv=kf)\naccuracy_nb = cross_val_score(nb_model, X_train, y_train, scoring=\"accuracy\", cv=kf)\n\naccuracy_logr = accuracy_logr.mean()\naccuracy_dtree = accuracy_dtree.mean()\naccuracy_nb = accuracy_nb.mean()\n\nprint('k-fold Cross Validation Accuracy Outputs:')\nprint('-')\nprint(\"Logistic Regression:\", round(accuracy_logr*100,2),\"%\")\nprint(\"Decision Tree:\", round(accuracy_dtree*100,2),\"%\")\nprint(\"Naive Bayes:\", round(accuracy_nb*100,2),\"%\")","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.bar(x=['Logistic Regression', 'Decision Tree model', 'Naive Bayes'], \n             y=[accuracy_logr, accuracy_dtree, accuracy_nb], \n            color=['Logistic Regression', 'Decision Tree model', 'Naive Bayes'],\n             labels={'x': 'Model', 'y': 'Accuracy'},\n            title='Validating models with k-fold cross validation')\nfig.show()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analysis:\n- As seen above this seems a little more accurate with the best accuracy being the Logistic Regression Model. It would appear that with a larger dataset the Decision Tree performs well but when there is minimal data, not so much. ","metadata":{}},{"cell_type":"markdown","source":"## Test Data Output from trained models:","metadata":{}},{"cell_type":"code","source":"# using the test data for our trained models:\n\nprint('Test data - Model Accuracy Outputs:')\nprint('-')\nprint('Logistic Regression:', round(logr_model.score(X_test, y_test)*100,2),'%')\nprint('Decision Tree:', round(dtree_model.score(X_test, y_test)*100,2),'%')\nprint('Naive Bayes:', round(nb_model.score(X_test, y_test)*100,2),'%')","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analysis and Next Steps:\n- With the test data, we have an identical match with Logistic Regression and Decision Tree.\n- Accuracy ratings are very low.\n- I will look to now include all variables as to help improve the accuracy of the models.","metadata":{"execution":{"iopub.status.busy":"2021-09-08T00:47:51.036438Z","iopub.execute_input":"2021-09-08T00:47:51.03704Z","iopub.status.idle":"2021-09-08T00:47:51.044628Z","shell.execute_reply.started":"2021-09-08T00:47:51.037Z","shell.execute_reply":"2021-09-08T00:47:51.043359Z"}}},{"cell_type":"markdown","source":"# --> Model (Popularity Classification) Pt.2 \n\n- Using all independant variables to try and achieve a greater accuracy","metadata":{"execution":{"iopub.status.busy":"2021-09-08T00:52:31.397919Z","iopub.execute_input":"2021-09-08T00:52:31.39833Z","iopub.status.idle":"2021-09-08T00:52:31.404515Z","shell.execute_reply.started":"2021-09-08T00:52:31.398297Z","shell.execute_reply":"2021-09-08T00:52:31.403089Z"}}},{"cell_type":"markdown","source":"### Splitting & Scaling Data.2","metadata":{}},{"cell_type":"code","source":"# Chosing independant and dependant variables:\n\ny_1 = spotify_df.loc[:,'popularity'] #dependant/target \nX_1 = spotify_df.drop('popularity', axis=1)#independant \n\n# Split data into training and test sets: 80% training, 20% test split.\n\nX_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1, test_size=0.2, random_state=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Shape of Data.2:')\nprint('-')\nprint('X_train_1:', X_train_1.shape, 'y_train_1:', y_train_1.shape)\nprint('X_test_1:',X_test_1.shape,'y_test_1:', y_test_1.shape)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scale the independant variable for relatively normal distribution of the data. \n\nX_train_1 = StandardScaler().fit_transform(X_train_1)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling & Training.2\n\n- Same a previosuly done with the 3x classification models.","metadata":{"execution":{"iopub.status.busy":"2021-09-08T00:53:56.582649Z","iopub.execute_input":"2021-09-08T00:53:56.583129Z","iopub.status.idle":"2021-09-08T00:53:56.589463Z","shell.execute_reply.started":"2021-09-08T00:53:56.583089Z","shell.execute_reply":"2021-09-08T00:53:56.588009Z"}}},{"cell_type":"markdown","source":"#### Training Model Accuracy Outputs.2","metadata":{}},{"cell_type":"code","source":"# Creating Models\n\nlogr_model_1 = linear_model.LogisticRegression(solver='liblinear')\ndtree_model_1 = tree.DecisionTreeClassifier()\nnb_model_1 = GaussianNB()\n\n# Training the models\n\nlogr_model_1.fit(X_train_1, y_train_1)\ndtree_model_1.fit(X_train_1, y_train_1)\nnb_model_1.fit(X_train_1, y_train_1)\n\n# Accuracy of trained models with training data:\n\nlogr_train_acc_1 = logr_model_1.score(X_train_1, y_train_1)\ndtree_train_acc_1 = dtree_model_1.score(X_train_1, y_train_1)\nnb_train_acc_1 = nb_model_1.score(X_train_1, y_train_1)\n\nprint('Training Model Accuracy Outputs.2:')\nprint('-')\nprint('Logistic Regression:', round(logr_train_acc_1*100,2),'%')\nprint('Decision Tree:', round(dtree_train_acc_1*100,2),'%')\nprint('Naive Bayes:', round(nb_train_acc_1*100,2),'%')","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.bar(x=['Logistic Regression', 'Decision Tree model', 'Naive Bayes'], \n             y=[logr_train_acc_1, dtree_train_acc_1, nb_train_acc_1], \n            color=['Logistic Regression', 'Decision Tree model', 'Naive Bayes'],\n             labels={'x': 'Model', 'y': 'Accuracy'},\n            title='Accuracy of trained models with tusing all independant variables on training data')\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analysis:\n- As seen above it appears that the accuracy has increased quite a bit for all models, again, Decision Tree Model has a really high accuracy rate against the training data. So will validate this through k-fold cross validation.","metadata":{}},{"cell_type":"markdown","source":"#### Validating models with k-fold cross validation.2","metadata":{}},{"cell_type":"code","source":"# Validating new models with k-fold cross validation method\n\naccuracy_logr_1 = cross_val_score(logr_model_1, X_train_1, y_train_1, scoring=\"accuracy\", cv=kf)\naccuracy_dtree_1 = cross_val_score(dtree_model_1, X_train_1, y_train_1, scoring=\"accuracy\", cv=kf)\naccuracy_nb_1 = cross_val_score(nb_model_1, X_train_1, y_train_1, scoring=\"accuracy\", cv=kf)\n\naccuracy_logr_1 = accuracy_logr_1.mean()\naccuracy_dtree_1 = accuracy_dtree_1.mean()\naccuracy_nb_1 = accuracy_nb_1.mean()\n\nprint('k-fold Cross Validation Accuracy Outputs.2:')\nprint('-')\nprint(\"Logistic Regression:\", round(accuracy_logr_1*100,2),\"%\")\nprint(\"Decision Tree:\", round(accuracy_dtree_1*100,2),\"%\")\nprint(\"Naive Bayes:\", round(accuracy_nb_1*100,2),\"%\")","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.bar(x=['Logistic Regression', 'Decision Tree model', 'Naive Bayes'], \n             y=[accuracy_logr_1, accuracy_dtree_1, accuracy_nb_1], \n            color=['Logistic Regression', 'Decision Tree model', 'Naive Bayes'],\n             labels={'x': 'Model', 'y': 'Accuracy'},\n            title='Validating new models with k-fold cross validation')\nfig.show()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analysis:\n- With the increase in data for the feature variables, we have increased validation of the models, with logistic regression coming out on top and decision trees coming in as least favourable.","metadata":{"execution":{"iopub.status.busy":"2021-09-08T01:10:19.050998Z","iopub.execute_input":"2021-09-08T01:10:19.051657Z","iopub.status.idle":"2021-09-08T01:10:19.060218Z","shell.execute_reply.started":"2021-09-08T01:10:19.051602Z","shell.execute_reply":"2021-09-08T01:10:19.058251Z"}}},{"cell_type":"markdown","source":"#### Test Data Output from trained models.2","metadata":{}},{"cell_type":"code","source":"# using the test data for our trained models:\n\nprint('Test data - Model Accuracy Outputs.2:')\nprint('-')\nprint('Logistic Regression:', round(logr_model_1.score(X_test_1, y_test_1)*100,2))\nprint('Decision Tree:', round(dtree_model_1.score(X_test_1, y_test_1)*100,2))\nprint('Naive Bayes:', round(nb_model_1.score(X_test_1, y_test_1)*100,2))","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Summary of Model Accuracy:","metadata":{}},{"cell_type":"markdown","source":"#### Model (Popularity Classification) \n- With 'loudness', 'danceability', 'energy', 'valence' as independant variables due to highest correlation.\n\nTrained Model:\n- LR Accuracy: 56%\n- DT Accuracy: 99%\n- NB Accuracy: 55%\n\nK-fold Cross Val:\n- LR Accuracy: 56%\n- DT Accuracy: 54%\n- NB Accuracy: 55%\n\nTest Data Result:\n- LR Accuracy: 53%\n- DT Accuracy: 53%\n- NB Accuracy: 53%\n\n\n#### Model (Popularity Classification) pt.2\n- With all independant data variables included.\n\nTrained Model.2:\n- LR Accuracy: 85%\n- DT Accuracy: 100%\n- NB Accuracy: 77%\n\nK-fold Cross Val.2:\n- LR Accuracy: 69%\n- DT Accuracy: 62%\n- NB Accuracy: 63%\n\nTest Data Result.2:\n- LR Accuracy: 55%\n- DT Accuracy: 50%\n- NB Accuracy: 62%","metadata":{"execution":{"iopub.status.busy":"2021-09-08T10:19:51.31293Z","iopub.execute_input":"2021-09-08T10:19:51.313313Z","iopub.status.idle":"2021-09-08T10:19:51.387914Z","shell.execute_reply.started":"2021-09-08T10:19:51.313281Z","shell.execute_reply":"2021-09-08T10:19:51.386378Z"}}},{"cell_type":"markdown","source":"# 5 FINAL CONCLUSIONS","metadata":{"execution":{"iopub.status.busy":"2021-09-08T12:55:03.964337Z","iopub.execute_input":"2021-09-08T12:55:03.964737Z","iopub.status.idle":"2021-09-08T12:55:03.968841Z","shell.execute_reply.started":"2021-09-08T12:55:03.964702Z","shell.execute_reply":"2021-09-08T12:55:03.96791Z"}}},{"cell_type":"markdown","source":"1. The accuracy of the data sets are quite vaired throughout the models used.\n2. Overall, it seems the models perform better with more data than less. My assumption is this is due to the lack of strong correlation between what makes a song popular. As well as a shift in music trends over the years which makes prediction more challenging. In retrospect I could have split the modeling over the year blocks created. This may produce a more accurate model and relevant model.\n3. In conclusion, the best performing in general, either the **Linear Regression** should be used with a larger dataset (+1000) with an 85% accuracy . And **Naive Bayes** models should be used with a smaller data set with a 62% accuracy.","metadata":{"execution":{"iopub.status.busy":"2021-09-08T13:05:20.764076Z","iopub.execute_input":"2021-09-08T13:05:20.764426Z","iopub.status.idle":"2021-09-08T13:05:20.772938Z","shell.execute_reply.started":"2021-09-08T13:05:20.764376Z","shell.execute_reply":"2021-09-08T13:05:20.77138Z"}}}]}