{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Bank Churn Prediction : Grid Search VS Threshold modification"},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nThe data set contains details of a bank's customers and the target variable is a binary variable reflecting the fact whether the customer left the bank (closed his account : Exited = 0) or he continues to be a customer.\n\nThe problem present here is a classification problem, or more specificaly binary classification problem. We can solve this kind of problems using famous models like Logistic Regression, Linear Discriminant Analysis, SVC, XGBoost, LGBM Classifier, Decision Tree Based models ...\n\nThe model we choose for deployment will be the one who "},{"metadata":{},"cell_type":"markdown","source":"# 1. Import Libraries:"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport sklearn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier)\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import KFold\n\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n\n\nfrom sklearn.metrics import accuracy_score, recall_score,roc_curve, auc\n\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\n\nfrom catboost import CatBoostClassifier\n\n\nfrom mlxtend.classifier import EnsembleVoteClassifier\n\n\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.max_colwidth = 200\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Import Data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Load in the dataset\nDF = pd.read_csv('../input/churn-modelling/Churn_Modelling.csv')\nDF.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. EDA & Data Preprocessing:"},{"metadata":{},"cell_type":"markdown","source":"The variables in this dataset are represented as follow: \n\n* Surname : The surname of the customer\n* CreditScore : The credit score of the customer\n* Geography : The country of the customer(Germany/France/Spain)\n* Gender : The gender of the customer (Female/Male)\n* Age : The age of the customer\n* Tenure : The customer's number of years in the bank\n* Balance : The customer's account balance\n* NumOfProducts : The number of bank products that the customer uses\n* HasCrCard : Does the customer has a card? (0=No,1=Yes)\n* IsActiveMember : Does the customer has an active membership (0=No,1=Yes)\n* EstimatedSalary : The estimated salary of the customer\n* Exited : Churned or not? (0=No,1=Yes)"},{"metadata":{"trusted":true},"cell_type":"code","source":"DF.info()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Types of our varibles:\n\nCategorical: Exited, Gender,HasCrCard, IsActiveMember, and Geography.\n\nContinous: Age, CreditScore, EstimatedSalary and Balance. Discrete: Tenure, NumOfProducts.\n\n2. Null and Missing values:\n\nThe data contains no Null values, so no deletion or imputation are needed."},{"metadata":{"trusted":true},"cell_type":"code","source":"DF[\"IsActiveMember\"] = DF[\"IsActiveMember\"].map({1:\"Yes\" , 0:\"No\"})\nDF[\"Exited\"] = DF[\"Exited\"].map({1:\"Yes\" , 0:\"No\"})\nDF[\"HasCrCard\"] = DF[\"HasCrCard\"].map({1:\"Yes\" , 0:\"No\"})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DF.iloc[:,3:len(DF)].describe([0.01,0.1,0.25,0.5,0.75,0.99])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First thing we can observe from the summary statistics table, is that we have an umbalanced dataset. 20% of customers churned and about 80% don't. So, with this information, we had to change our metric for choosing the best medel, instead of using the accuracy, we will use the recall.\n\nRecall,  also referred to as the true positive rate or sensitivity, is the True Positive Rate.\n\nIn other term, recall attempts to answer the following question: What proportion of actual positives was identified correctly? So our metric will be the proportion of customers the model predect them to churn among the total number of true churners."},{"metadata":{"trusted":true},"cell_type":"code","source":"DF.iloc[:,3:len(DF)].describe(include=['O'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DF[\"IsActiveMember\"] = DF[\"IsActiveMember\"].map({\"Yes\":1 , \"No\":0}).astype(int)\nDF[\"Exited\"] = DF[\"Exited\"].map({\"Yes\":1 , \"No\":0}).astype(int)\nDF[\"HasCrCard\"] = DF[\"HasCrCard\"].map({\"Yes\":1 , \"No\":0}).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also conclude from the summary statistics that the age variable has some outliers with very high age (up to 92), and that few customers have more than 3 products.\n\nNow let's move to some more exploratory analysis for each variable.\n\n### 1. CreditScore:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(DF[\"CreditScore\"], label=\"Skewness : %.2f\"%(DF[\"CreditScore\"].skew()))\nplt.legend(loc=\"best\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = sns.kdeplot(DF[\"CreditScore\"][(DF[\"Exited\"] == 0) & (DF[\"CreditScore\"].notnull())], color=\"Red\", shade = True)\nplot = sns.kdeplot(DF[\"CreditScore\"][(DF[\"Exited\"] == 1) & (DF[\"CreditScore\"].notnull())], ax =plot, color=\"Blue\", shade= True)\nplot.set_xlabel(\"Credit Score\")\nplot.set_ylabel(\"Frequency\")\nplot = plot.legend([\"Not Churn\",\"Churn\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, CreditScore distribution is bell shaped. So no transformation is needed.\n\n\n### 2. Geography:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = sns.barplot(x = \"Geography\" , y = \"Exited\" , data = DF , errwidth = 0)\n\nplt.ylim(0,0.5)\nplt.yticks([0.1,0.2,0.3,0.4])\nfor p in plot.patches:\n    plot.annotate(format(p.get_height(), '.2f'), \n                   (p.get_x() + p.get_width() / 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\n    \nplt.ylabel(\"Churn proportion\")\nplt.title(\"Percentage of churning by the geography of the customer\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot=sns.countplot(DF[\"Geography\"], hue=DF[\"Exited\"])\n\nplt.ylim(0,4700)\n\nfor p in plot.patches:\n    plot.annotate(format(p.get_height(), '.2f'), \n                   (p.get_x() + p.get_width() / 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\n\nplt.legend(loc=\"best\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems like the customers from Germany are more likely to churn comparing the the ones from France or Spain.\n\n### 3. Gender:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = sns.barplot(x = \"Gender\" , y = \"Exited\" , data = DF , errwidth = 0)\n\nplt.ylim(0,0.5)\nplt.yticks([0.1,0.2,0.3,0.4])\nfor p in plot.patches:\n    plot.annotate(format(p.get_height(), '.2f'), \n                   (p.get_x() + p.get_width() / 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\n\nplt.ylabel(\"Churn proportion\")\nplt.title(\"Percentage of churning by the gender of the customer\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot=sns.countplot(DF[\"Gender\"], hue=DF[\"Exited\"])\n\nplt.ylim(0,5000)\n\nfor p in plot.patches:\n    plot.annotate(format(p.get_height(), '.2f'), \n                   (p.get_x() + p.get_width() / 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\n\nplt.legend(loc=\"best\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no significant diffirence in the churn proportion between male and female customers.\n\n### 4. Age:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(DF[\"Age\"], label=\"Skewness : %.2f\"%(DF[\"Age\"].skew()))\nplt.legend(loc=\"best\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Age distribution is very skewed, so we will try to transform the Age variable with the log transformation to get better distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"DF[\"LogAge\"] = np.log(DF[\"Age\"])\n\nsns.distplot(DF[\"LogAge\"], label=\"Skewness : %.2f\"%(DF[\"LogAge\"].skew()))\nplt.legend(loc=\"best\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we got a bell curve distribution better than the original age distribution. So we will use the LogAge as new varible in the modelisation."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = sns.kdeplot(DF[\"Age\"][(DF[\"Exited\"] == 0) & (DF[\"Age\"].notnull())], color=\"Red\", shade = True)\nplot = sns.kdeplot(DF[\"Age\"][(DF[\"Exited\"] == 1) & (DF[\"Age\"].notnull())], ax =plot, color=\"Blue\", shade= True)\nplot.set_xlabel(\"Age\")\nplot.set_ylabel(\"Frequency\")\nplot = plot.legend([\"Not Churn\",\"Churn\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The young cutomers have less chance to churn comparing to old customers.\n\n### 5. Tenure:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = sns.barplot(x = \"Tenure\" , y = \"Exited\" , data = DF , errwidth = 0)\n\nplt.ylim(0,0.5)\nplt.yticks([0.1,0.2,0.3,0.4])\n\nfor p in plot.patches:\n    plot.annotate(format(p.get_height(), '.2f'), \n                   (p.get_x() + p.get_width() / 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\n    \nplt.ylabel(\"Churn proportion\")\nplt.title(\"Percentage of churning by the customer's number of years in the bank\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(DF[\"Tenure\"], hue=DF[\"Exited\"])\nplt.legend(loc=\"best\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution of Tenure is uniform, and there is no difference between the two target categories.\n\n### 6. Balance:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(DF[\"Balance\"], label=\"Skewness : %.2f\"%(DF[\"Balance\"].skew()))\nplt.legend(loc=\"best\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = sns.kdeplot(DF[\"Balance\"][(DF[\"Exited\"] == 0) & (DF[\"Balance\"].notnull())], color=\"Red\", shade = True)\nplot = sns.kdeplot(DF[\"Balance\"][(DF[\"Exited\"] == 1) & (DF[\"Balance\"].notnull())], ax =plot, color=\"Blue\", shade= True)\nplot.set_xlabel(\"Balance\")\nplot.set_ylabel(\"Frequency\")\nplot = plot.legend([\"Not Churn\",\"Churn\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The balance variable has an extrem amount of low values, maybe refers to student or non income customers. For the rest of values, the variable follow a bell curve distribution.\n\nThe cutomers with low balance have less chance to churn comparing to the ones with hight Balance.\n\n### 7. NumOfProducts"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = sns.barplot(x = \"NumOfProducts\" , y = \"Exited\" , data = DF , errwidth = 0)\n\nplt.ylim(0,1.2)\nfor p in plot.patches:\n    plot.annotate(format(p.get_height(), '.2f'), \n                   (p.get_x() + p.get_width() / 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\n    \nplt.ylabel(\"Churn proportion\")\nplt.title(\"Percentage of churning by the Number of Products\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplot = sns.countplot(DF[\"NumOfProducts\"], hue=DF[\"Exited\"])\n\nplt.ylim(0,4700)\n\nfor p in plot.patches:\n    plot.annotate(format(p.get_height(), '.2f'), \n                   (p.get_x() + p.get_width() / 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\n    \n\nplt.legend(loc=\"right\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Number Of Products seems to be an effective variable to detect the customers who have more chance to churn. The customers with 1 or 2 products have a greatter chance to not churn, and in the other hand, the customers with 3 or 4 products have bigger chance to churn.\n\n### 8. HasCrCard:"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplot = sns.countplot(DF[\"HasCrCard\"], hue=DF[\"Exited\"])\n\nplt.ylim(0,6200)\n\nfor p in plot.patches:\n    plot.annotate(format(p.get_height(), '.2f'), \n                   (p.get_x() + p.get_width() / 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\n\nplt.legend(loc=\"best\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = sns.barplot(x = \"HasCrCard\" , y = \"Exited\" , data = DF , errwidth = 0)\n\nplt.ylim(0,0.5)\nfor p in plot.patches:\n    plot.annotate(format(p.get_height(), '.2f'), \n                   (p.get_x() + p.get_width() / 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\n    \nplt.ylabel(\"Churn proportion\");\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no difference for churn variable between the customers who have a Credit Card or haven't.\n\n### 9. IsActiveMember:"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplot = sns.countplot(DF[\"IsActiveMember\"], hue=DF[\"Exited\"])\n\nplt.ylim(0,5500)\n\nfor p in plot.patches:\n    plot.annotate(format(p.get_height(), '.2f'), \n                   (p.get_x() + p.get_width() / 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\n\nplt.legend(loc=\"best\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = sns.barplot(x = \"IsActiveMember\" , y = \"Exited\" , data = DF , errwidth = 0)\n\nplt.ylim(0,0.5)\nfor p in plot.patches:\n    plot.annotate(format(p.get_height(), '.2f'), \n                   (p.get_x() + p.get_width() / 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\n    \nplt.ylabel(\"Churn proportion\");\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the customer who have an active membership the chance of churn is less comparing to customers with no active membership.\n\n### 10. EstimatedSalary:"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsns.distplot(DF[\"EstimatedSalary\"], label=\"Skewness : %.2f\"%(DF[\"Balance\"].skew()))\nplt.legend(loc=\"best\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = sns.kdeplot(DF[\"EstimatedSalary\"][(DF[\"Exited\"] == 0) & (DF[\"EstimatedSalary\"].notnull())], color=\"Red\", shade = True)\nplot = sns.kdeplot(DF[\"EstimatedSalary\"][(DF[\"Exited\"] == 1) & (DF[\"EstimatedSalary\"].notnull())], ax =plot, color=\"Blue\", shade= True)\nplot.set_xlabel(\"Estimated Salary\")\nplot.set_ylabel(\"Frequency\")\nplot = plot.legend([\"Not Churn\",\"Churn\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution of salary follows a uniform distribution for both categories.\n\n### 11. Exited:"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(12,5))\nDF['Exited'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Exited')\nax[0].set_ylabel('')\nsns.countplot('Exited',data=DF,ax=ax[1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlations:"},{"metadata":{"trusted":true},"cell_type":"code","source":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(DF.drop([\"RowNumber\",\"CustomerId\",\"Surname\",\"LogAge\"],axis =1, inplace =False).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no significant correlation between any two features in train dataset.\n\n\n## Data Preprocessing:"},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = DF[\"Exited\"]\nX = DF.drop(['RowNumber', 'CustomerId', 'Surname' ,'Age' ,'Exited'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dummy variables for Gender & Geography:"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX = pd.get_dummies(X, columns = [\"Geography\"],drop_first = True)\nX = pd.get_dummies(X, columns = [\"Gender\"],drop_first = True)\n\nX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Standardization:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X[\"Balance\"] = (X[\"Balance\"] - X[\"Balance\"].mean())/X[\"Balance\"].std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X[\"EstimatedSalary\"] = (X[\"EstimatedSalary\"] - X[\"EstimatedSalary\"].mean())/X[\"EstimatedSalary\"].std()\n\nX[\"CreditScore\"] = (X[\"CreditScore\"] - X[\"CreditScore\"].mean())/X[\"CreditScore\"].std()\n\nX.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Modeling:\n\nNow we are ready to train a model and predict the required solution. There are a lot of predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a selected few models which we can evaluate. Our problem is a classification problem, we want to identify relationship between output (Churn or not) with other features or informations about customers. The most used models in our case can be set as follows:\n\n* Logistic Regression\n* XGBoost\n* AdaBoost\n* Gradient Boosting\n* Linear Discriminant Analysis\n* Logistic Regression\n* KNN or k-Nearest Neighbors\n* Support Vector Machines\n* Naive Bayes classifier\n* Decision Tree\n* Random Forrest\n* Perceptron\n* Artificial neural network"},{"metadata":{"trusted":true},"cell_type":"code","source":"RS=121\n\n\nclassifiers = [\n    KNeighborsClassifier(),\n    SVC(random_state=RS, probability = True),\n    DecisionTreeClassifier(random_state=RS),\n    RandomForestClassifier(random_state=RS),\n    AdaBoostClassifier(random_state=RS),\n    GradientBoostingClassifier(random_state=RS),\n    ExtraTreesClassifier(random_state=RS),\n    GaussianNB(),\n    XGBClassifier(random_state=RS),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis(),\n    LogisticRegression(random_state=RS),\n    MLPClassifier(random_state=RS, activation=\"logistic\"),\n    LGBMClassifier(random_state=RS),\n    CatBoostClassifier(verbose = False)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use the stratified K fold Cross Validation to split the data for training because we have an unbalanced datset with minority of target class. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross validate model with Kfold stratified cross val\nkfold = StratifiedKFold(n_splits=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As first try, we gonna use all different models already set with thier default parameters.\n\nHowever, as metric, we will use the recall instead of accuaracy, the reason is we want to detect as much as possible customers from the positive class, i.e customers with hight chance to churn. The accuracy in case of unbalanced data can give us a wrong idea about the target variable, It favors the majority class. \n\nEven when this creteria, we must keep a reasonable value of accuracy to avoid classing all cutomers as churners. For this reason, we will choose a threshold that leave a balance between the recall and precision, and we will comapre the results between the default 0.5 threshold and the new one."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nresults = {}\nNames=[]\nfor classifier in classifiers :\n    Name = classifier.__class__.__name__\n    Names.append(Name)\n    AUC=[]\n    Recall_Before = []\n    Recall_After = []\n    Accuaracy_Before = []\n    Accuaracy_After = []\n    Threshold = []\n    Metrics = [AUC , Accuaracy_Before, Accuaracy_After, Threshold,Recall_Before, Recall_After]   \n    for train_index, test_index in kfold.split(X.values, Y.values):\n        X_train, X_test = X.values[train_index], X.values[test_index]\n        Y_train, Y_test = Y.values[train_index], Y.values[test_index]\n        classifier.fit(X_train,Y_train)\n        FPR, TPR, thresholds = roc_curve(Y_test, classifier.predict_proba(X_test)[:, 1])\n        \n        Accuaracy_Before.append(accuracy_score(Y_test,classifier.predict(X_test)))\n        Recall_Before.append(recall_score(Y_test,classifier.predict(X_test)))\n        AUC.append(auc(FPR, TPR))\n        \n        best_thresh_pos = np.argmax(np.abs(FPR - TPR))\n        best_thresh = thresholds[best_thresh_pos]\n        Threshold.append(best_thresh)\n        \n        pred = np.where(classifier.predict_proba(X_test)[:,1]>best_thresh,1,0)\n\n        \n        Accuaracy_After.append(accuracy_score(Y_test,pred))\n        Recall_After.append(recall_score(Y_test,pred))\n        \n    results[Name] = [np.mean(L) for L in Metrics]\n\n        \n        \n        \n        \n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"ClassifiersRes = pd.DataFrame(results)\nClassifiersRes.index = [\"AUC\" , \"Accuaracy_Before\", \"Accuaracy_After\", \"Threshold\",\"Recall_Before\", \"Recall_After\"]\nClassifiersRes = ClassifiersRes.T.sort_values(by = [\"Recall_After\" , \"Accuaracy_After\"] , ascending = False)\n\nClassifiersRes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's very clear that as we change the threshold to the optimal one, the recall incearses significantly, and the accuracy decreases but not too much. \n\n4 models are the best in term of model recall: Gradient Boosting Classifier, LGBM Classifier, CatBoost Classifier and Multi Layers Perceptron classifier.\n\nThe question now, is it possible to make those recalls better if we use grid search for each classifier then repeat the same procedure like before. Let's Try to answer that.\n\nP.S : the following scripts take a very long time to finish, so be patient if you want to check the results for yourself. "},{"metadata":{},"cell_type":"markdown","source":"First we set the hyperparamets for each classifier, then we stack the classifiers and parameters in lists for the for loop."},{"metadata":{"trusted":true},"cell_type":"code","source":"LogReg_params= {\"C\":np.logspace(-1, 1, 10),\n                \"penalty\": [\"l1\",\"l2\"], \n                \"solver\":['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], \n                \"max_iter\":[1000]}\n\nNB_params = {'var_smoothing': np.logspace(0,-9, num=100)}\nKNN_params= {\"n_neighbors\": np.linspace(1,19,10, dtype = int).tolist(),\n            \"weights\": [\"uniform\",\"distance\"],\n            \"metric\":[\"euclidean\",\"manhattan\"]}\nSVC_params= {\"kernel\" : [\"rbf\"],\n             \"gamma\": [0.001, 0.01, 0.1],\n             \"C\": [1,10,100,1000]}\nDT_params = {\"min_samples_split\" : range(10,500,20),\n             \"max_depth\": range(1,20,2)}\nRF_params = {\"max_features\": [\"log2\",\"Auto\",\"None\"],\n                \"min_samples_split\":[2,3,5],\n                \"bootstrap\":[True,False],\n                \"n_estimators\":[100,150],\n                \"criterion\":[\"gini\",\"entropy\"]}\nGB_params = {\"learning_rate\" : [ 0.01, 0.1, 0.05],\n             \"n_estimators\": [500,1000],\n             \"max_depth\": [3,5],\n             \"min_samples_split\": [2,5,10]}\n\n\nXGB_params ={\n        'n_estimators': [100, 200],\n        'subsample': [ 0.6, 0.8, 1.0],\n        'max_depth': [1,2,3],\n        'learning_rate': [0.1,0.2, 0.3]}\n\nMLPC_params = {\"alpha\": [0.1, 0.01, 0.001, 0.0001,0.00001],\n              \"hidden_layer_sizes\": [(100,100,100),\n                                     (100,100)],\n              \"solver\" : [\"lbfgs\",\"adam\",\"sgd\"],\n              \"activation\": [\"relu\",\"logistic\"]}\nCATB_params =  {'depth':[4],\n              'loss_function': ['CrossEntropy'],\n              'l2_leaf_reg':np.arange(2,28)}\n\nLGBM_params = { 'boosting_type':['gbdt','dart','goss','rf'],\n            'learning_rate':[0.1, 0.2, 0.5, 0.01],\n            'n_estimators':[100, 500, 1000],\n            'objective':[\"binary\"]}\n\nLDA_params = {'solver':['svd', 'lsqr']}\n\nQDA_params = {'reg_param': [0.1, 0.2, 0.3, 0.4, 0.5]}\n\nAdaB_params ={\"learning_rate\" : [0.001, 0.01, 0.1, 0.05],\n             \"algorithm\" : ['SAMME', 'SAMME.R'],\n             'n_estimators':[100, 500, 1000]}\n\nRS=121\nclassifiers2 = [LogisticRegression(random_state=RS),GaussianNB(), KNeighborsClassifier(),\n          SVC(random_state=RS,probability=True),DecisionTreeClassifier(random_state=RS),\n          RandomForestClassifier(random_state=RS), GradientBoostingClassifier(random_state=RS),\n          XGBClassifier(random_state=RS), MLPClassifier(random_state=RS),\n          CatBoostClassifier(random_state=RS,verbose = False), LGBMClassifier(random_state=RS),\n          LinearDiscriminantAnalysis(), QuadraticDiscriminantAnalysis(), AdaBoostClassifier(random_state=RS)]\n\n\nclassifier_params = [LogReg_params,NB_params,KNN_params,SVC_params,DT_params,RF_params,\n                     GB_params, XGB_params,MLPC_params,CATB_params, LGBM_params, LDA_params,\n                     QDA_params, AdaB_params] \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# Tuning by Cross Validation  \ncv_result = {}\nbest_estimators = []\n\nfor classifier,classifier_param in zip(classifiers2,classifier_params):\n    name = classifier.__class__.__name__\n    cv_result[name] = []\n    clf = GridSearchCV(classifier, param_grid=classifier_param, cv =10, \n                       scoring = \"recall\", n_jobs = -1)\n    clf.fit(X,Y)\n    cv_result[name].append(clf.best_params_)\n    best_estimators.append(clf.best_estimator_)    \n    \n    AUC=[]\n    Recall_Before = []\n    Recall_After = []\n    Accuaracy_Before = []\n    Accuaracy_After = []\n    Threshold = []\n    Metrics = [AUC , Accuaracy_Before, Accuaracy_After, Threshold,Recall_Before, Recall_After]   \n    for train_index, test_index in kfold.split(X.values, Y.values):\n        X_train, X_test = X.values[train_index], X.values[test_index]\n        Y_train, Y_test = Y.values[train_index], Y.values[test_index]\n        classifier = clf.best_estimator_.fit(X_train,Y_train)\n        FPR, TPR, thresholds = roc_curve(Y_test, classifier.predict_proba(X_test)[:, 1])\n        \n        Accuaracy_Before.append(accuracy_score(Y_test,classifier.predict(X_test)))\n        Recall_Before.append(recall_score(Y_test,classifier.predict(X_test)))\n        AUC.append(auc(FPR, TPR))\n        \n        best_thresh_pos = np.argmax(np.abs(FPR - TPR))\n        best_thresh = thresholds[best_thresh_pos]\n        Threshold.append(best_thresh)\n        \n        pred = np.where(classifier.predict_proba(X_test)[:,1]>best_thresh,1,0)\n\n        \n        Accuaracy_After.append(accuracy_score(Y_test,pred))\n        Recall_After.append(recall_score(Y_test,pred))\n        \n    cv_result[name].extend([np.mean(L) for L in Metrics])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CV_ClassifiersRes = pd.DataFrame(cv_result)\nCV_ClassifiersRes.index = [\"Params\", \"AUC\" , \"Accuaracy_Before\", \"Accuaracy_After\", \"Threshold\",\"Recall_Before\", \"Recall_After\"]\nCV_ClassifiersRes = CV_ClassifiersRes.T.sort_values(by = [\"Recall_After\" , \"Accuaracy_After\"] , ascending = False)\n\nCV_ClassifiersRes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the opposite of what we expect the grid search to do, the recalls as not increasing at all, instead, there are decresed.\n\nSo we can say that the best parameters for a model, using recall as metric, not necessary to be the best after changing the threshold. The threshold in this case make a bigger importance for the model performance than the hyperparameters tunning."},{"metadata":{"trusted":true},"cell_type":"code","source":"classifiers11 = [\n    SVC(random_state=RS, probability = True),\n    AdaBoostClassifier(random_state=RS),\n    GradientBoostingClassifier(random_state=RS),\n    XGBClassifier(random_state=RS),\n    MLPClassifier(random_state=RS, activation=\"logistic\"),\n    LGBMClassifier(random_state=RS),\n    CatBoostClassifier(verbose = False)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"HardVotingEstimator = EnsembleVoteClassifier(classifiers11 , voting='hard')\nSoftVotingEstimator = EnsembleVoteClassifier(classifiers11 , voting='soft')\n\nVoting_Classifiers = [HardVotingEstimator]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results__hard_vote = {}\nclassifier = HardVotingEstimator\nName = classifier.__class__.__name__+\" \"+classifier.voting\nNames_vote.append(Name)\nAUC=[]\nRecall_Before = []\nRecall_After = []\nAccuaracy_Before = []\nAccuaracy_After = []\nThreshold = []\nMetrics = [AUC , Accuaracy_Before, Accuaracy_After, Threshold,Recall_Before, Recall_After]   \nfor train_index, test_index in kfold.split(X.values, Y.values):\n    X_train, X_test = X.values[train_index], X.values[test_index]\n    Y_train, Y_test = Y.values[train_index], Y.values[test_index]\n    classifier.fit(X_train,Y_train)\n    FPR, TPR, thresholds = roc_curve(Y_test, classifier.predict_proba(X_test)[:, 1])\n        \n    Accuaracy_Before.append(accuracy_score(Y_test,classifier.predict(X_test)))\n    Recall_Before.append(recall_score(Y_test,classifier.predict(X_test)))\n    AUC.append(auc(FPR, TPR))\n        \n    best_thresh_pos = np.argmax(np.abs(FPR - TPR))\n    best_thresh = thresholds[best_thresh_pos]\n    Threshold.append(best_thresh)\n        \n    pred = np.where(classifier.predict_proba(X_test)[:,1]>best_thresh,1,0)\n\n        \n    Accuaracy_After.append(accuracy_score(Y_test,pred))\n    Recall_After.append(recall_score(Y_test,pred))\n        \nresults__hard_vote[Name] = [np.mean(L) for L in Metrics]\n\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Hard_Voting_ClassifiersRes = pd.DataFrame(results__hard_vote)\nHard_Voting_ClassifiersRes.index = [\"AUC\" , \"Accuaracy_Before\", \"Accuaracy_After\", \"Threshold\",\"Recall_Before\", \"Recall_After\"]\nHard_Voting_ClassifiersRes = Hard_Voting_ClassifiersRes.T.sort_values(by = [\"Recall_After\" , \"Accuaracy_After\"] , ascending = False)\n\nHard_Voting_ClassifiersRes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results__soft_vote = {}\nclassifier = SoftVotingEstimator\nName = classifier.__class__.__name__+\" \"+classifier.voting\nNames_vote.append(Name)\nAUC=[]\nRecall_Before = []\nRecall_After = []\nAccuaracy_Before = []\nAccuaracy_After = []\nThreshold = []\nMetrics = [AUC , Accuaracy_Before, Accuaracy_After, Threshold,Recall_Before, Recall_After]   \nfor train_index, test_index in kfold.split(X.values, Y.values):\n    X_train, X_test = X.values[train_index], X.values[test_index]\n    Y_train, Y_test = Y.values[train_index], Y.values[test_index]\n    classifier.fit(X_train,Y_train)\n    FPR, TPR, thresholds = roc_curve(Y_test, classifier.predict_proba(X_test)[:, 1])\n        \n    Accuaracy_Before.append(accuracy_score(Y_test,classifier.predict(X_test)))\n    Recall_Before.append(recall_score(Y_test,classifier.predict(X_test)))\n    AUC.append(auc(FPR, TPR))\n        \n    best_thresh_pos = np.argmax(np.abs(FPR - TPR))\n    best_thresh = thresholds[best_thresh_pos]\n    Threshold.append(best_thresh)\n        \n    pred = np.where(classifier.predict_proba(X_test)[:,1]>best_thresh,1,0)\n\n        \n    Accuaracy_After.append(accuracy_score(Y_test,pred))\n    Recall_After.append(recall_score(Y_test,pred))\n        \nresults__soft_vote[Name] = [np.mean(L) for L in Metrics]\n\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Soft_Voting_ClassifiersRes = pd.DataFrame(results__soft_vote)\nSoft_Voting_ClassifiersRes.index = [\"AUC\" , \"Accuaracy_Before\", \"Accuaracy_After\", \"Threshold\",\"Recall_Before\", \"Recall_After\"]\nSoft_Voting_ClassifiersRes = Soft_Voting_ClassifiersRes.T.sort_values(by = [\"Recall_After\" , \"Accuaracy_After\"] , ascending = False)\n\nSoft_Voting_ClassifiersRes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion:\n\nThe model that fit the best our data and make us predict customer churn with the highest recall is Ensemble Vote Classifier with a threshold of 0.234271 and recall of 0.78."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}