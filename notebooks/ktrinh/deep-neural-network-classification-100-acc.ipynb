{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nThis data set consists of 8,124 labelled samples (edible or poisonous) with 22 categorical variables. As a greenhorn data scientist, I found this data set to be especially useful for practicing feature engineering with non-numeric data and working with panda dataframes. However, I did get some suspiciously good results (100% accuracy), though other Kernels I've looked at obtained similar results for various ML algorithms. I'd love for a data expert to comment on my results!\n\n**Goal:** Classify edible mushrooms using a deep neural network.\n\n\n# Approach\n1. Data visualization\n2. Feature engineering\n3. Neural network (NN) classification\n4. Discussion\n\n\n# Data visualization\n\n    Goals\n    - Use dual histograms to compare edible and poisonous mushrooms across all features\n    - Qualitatively guess which features are collinear or heavily skewed\n\nLet's explore our data with histograms for each feature. Run the script below for the figures.\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"'''\nProject:    Mushroom Classification - Data Visualizer\nPurpose:    Explore the Mushroom data set prior to ML\n\n@author:    Kevin Trinh\n'''\n\nimport numpy as np\nfrom numpy.core.defchararray import add\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\n\n\ndef histCompare(edf, pdf, feature):\n    '''Plot a dual histogram of edible and poisonous mushrooms for a \n    certain feature.\n    @param edf --> (pandas dataframe) a dataframe of edible mushrooms\n    @param pdf --> (pandas dataframe) a dataframe of poisonous mushrooms\n    @param feature --> (string) the name of the feature to be compared\n    '''\n    e_array = list(edf[feature])\n    p_array = list(pdf[feature])\n    plt.hist([e_array, p_array], color=['b', 'g'], alpha=0.5)\n    plt.xlabel(feature)\n    plt.title('Histogram (n = 8124)')\n    plt.legend(['Edible', 'Poisonous'])\n    plt.show()\n\n# read in .csv data as pandas dataframe\nmushroom_df = pd.read_csv('../input/mushroom-classification/mushrooms.csv', encoding='utf-8')\n\n# separate dataframe by class\nedible_df = mushroom_df.loc[mushroom_df['class'] == 'e']\npoisonous_df = mushroom_df.loc[mushroom_df['class'] == 'p']\n\n# obtain list of features\nfeatures = list(mushroom_df)\n\n# generate comparative histograms for each feature\nfor feat in features:\n    histCompare(edible_df, poisonous_df, feat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here are some noteworthy observations about our data:\n\n* We have a roughly equal number of positive (edible) and negative (poisonious) examples, so we don't need to worry about balancing each training batch with enough of both classes.\n\n* Without any formal test for correlation, stalk characteristics above and below the ring seem to be collinear. Let's omit the stalk features below the ring.\n\n* Veil type and veil color are heavily skewed towards one of its categories, and the edibility of mushrooms don't seem to make a difference here. Let's omit these features too.\n\n* Odor, spore print color, gill color, gill size, stalk surface above/below ring, and bruises seem to have lots of valuable information in determining the edibility of a mushroom.\n\nUnderstanding the distribution of our data helps us decide on the best features to use for our NN.\n\n# Feature engineering\n\n    Goals\n    - Use dummy variables to represent categorical data containing > 2 categories\n    - Use binary variables to represent features containing 2 categories\n    - Omit collinear, heavily skewed, or otherwise flawed features\n   \n\nAll categorical features (each with m categories) will be handled in one of three ways:\n\n1. Converted into dummy variables (m - 1 columns) \n2. Converted into a binary variable (1 column)\n3. Dropped from the pandas dataframe (0 columns)\n\nWe also drop the original feature after creating new artificial features to avoid collinearity. Having redundant features will lead to overfitting.\n\nThe following script will save a .csv file that encodes the mushrooms.csv file in the following way:\n\n**Dummy Features:** Cap shape, cap surface, cap color, odor, gill color, stalk root, stalk surface above ring, stalk color above ring, stalk color below ring, ring type, spore print color, population, and habitat.\n\n**Binary Features:** Class, bruises, gill attachment, gill spacing, gill size, stalk shape, and ring number.\n\n**Omitted Features:** Stalk surface below ring, veil type, veil color.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nProject:    Mushroom Classification -- Feature Engineering\nPurpose:    - Encode categorical data\n            - omit redundant and highly skewed features\n\n@author:    Kevin Trinh\n\"\"\"\n\n\nimport numpy as np\nfrom numpy.core.defchararray import add\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\n\ndef encodeDummy(df, feature):\n    '''Encode a given feature into dummy variables, omitting the first\n    alphabetically-sorted category. Remove the original feature.\n    \n    @param df --> (pandas dataframe) dataframe to be modified\n    @param feature --> (str) name of feature\n    @return df --> (pandas dataframe) modified dataframe\n    '''\n    labels = np.unique(df[feature])\n    labels = add(feature, labels)\n    le = LabelEncoder()\n    dummy_labels = le.fit_transform(df[feature])\n    df[feature] = dummy_labels\n    dummy_features = pd.get_dummies(df[feature], drop_first=True)\n    df[labels[1:]] = dummy_features\n    return df.drop(feature, axis=1)\n    \n\ndef encodeBinary(df, feature, positive):\n    '''Encode a given feature into a binary variable with 'positive' as 1 and\n    all other values as 0.\n    \n    @param df --> (pandas dataframe) dataframe to be modified\n    @param feature --> (str) name of feature\n    @param positive --> (str) category to be a positive binary\n    @return df --> (pandas dataframe) modified dataframe\n    '''\n    positive_arr = df[feature] == positive\n    df.loc[positive_arr, feature] = 1\n    df.loc[~positive_arr, feature] = 0\n    return df\n\ndef encodeOmit(df, feature):\n    '''Omit feature from dataframe.\n    \n    @param df --> (pandas dataframe) dataframe to be modified\n    @param feature --> (str) name of feature\n    @return df --> (pandas dataframe) modified dataframe\n    '''\n    return df.drop(feature, axis=1)\n\n\n# read in .csv data as pandas dataframe\nmushroom_df = pd.read_csv('mushrooms.csv', encoding='utf-8')\n\n# select features to encode or omit\nmy_dummies = ['cap-shape', 'cap-surface', 'cap-color', 'odor', 'gill-color',\n              'stalk-root', 'stalk-surface-above-ring', \n              'stalk-color-above-ring', 'ring-type', 'spore-print-color', \n              'population', 'habitat']\n\nmy_binaries = [('class', 'e'), ('bruises', 't'), ('gill-attachment', 'f'),\n               ('gill-spacing', 'c'), ('gill-size', 'b'), ('stalk-shape', 't'), \n               ('ring-number', 'o')]\n\nmy_omissions = ['stalk-surface-below-ring', 'stalk-color-below-ring',\n                'veil-type', 'veil-color']\n\n\n# encode dataframe\nfor feat in my_dummies:\n    mushroom_df = encodeDummy(mushroom_df, feat)\nfor feat, pos in my_binaries:\n    mushroom_df = encodeBinary(mushroom_df, feat, pos)\nfor feat in my_omissions:\n    mushroom_df = encodeOmit(mushroom_df, feat)\n\n\n\nmushroom_df.to_csv('mushrooms_encoded.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Neural Network\n\n    Goals\n    - Shuffle and partition data into training, validation, and test sets\n    - Construct and train neural network\n    - After satisfactory performance on the validation set, make predictions ONLY ONCE on the test data\n\nNow that we have feature engineered our data set, we are ready to do some machine learning.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nProject:    Mushroom Classification -- Neural Network\nPurpose:    Construct a neural network to predict mushroom edibility\n\n            Note: Run mushroomEncoder.py before running mushroomClassifier.py\n\n@author:    Kevin Trinh\n\"\"\"\n\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.metrics import binary_crossentropy\nimport matplotlib.pyplot as plt\n\n\n# read in and shuffle encoded data\nmushroom_df = pd.read_csv('../input/mushrooms-encoded/mushrooms_encoded.csv', encoding='utf-8')\nmushroom_df = mushroom_df.drop(mushroom_df.columns[0], axis=1) # omit index column\nmushroom_df = mushroom_df.sample(frac=1)\n\n# partition into training (60%), validation (20%), and test set (20%)\nsamples = mushroom_df.shape[0]\ntrain_count = round(samples * 0.6)\nval_count = round(samples * 0.2)\ntest_count = samples - train_count - val_count\n\ntrain_df = mushroom_df.iloc[:train_count]\nvalidation_df = mushroom_df.iloc[train_count:train_count + val_count]\ntest_df = mushroom_df.iloc[-test_count:]\n\nX_train = train_df.drop(['class'], axis=1)\nX_validation = validation_df.drop(['class'], axis=1)\nX_test = test_df.drop(['class'], axis=1)\n\ny_train = train_df['class']\ny_validation = validation_df['class']\ny_test = test_df['class']\n\n\n### Build neural network architecture ###\nnum_features = mushroom_df.shape[1] - 1\n\nmodel = Sequential()\nmodel.add(Dense(16, input_dim=num_features, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(12, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(4, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(1, activation='sigmoid', name='output'))\n  \nmodel.compile(loss='binary_crossentropy', optimizer='adam', \n              metrics=['binary_accuracy'])\n\n# train NN\nmy_epochs = 50\nhistory = model.fit(X_train, y_train, epochs=my_epochs, batch_size=20,\n                    validation_data=(X_validation, y_validation))\n\n# plot model loss while training\nepochs_arr = np.arange(1, my_epochs + 1, 1)\nmy_history = history.history\nline1 = plt.plot(epochs_arr, my_history['loss'], 'r-', label='training loss')\nline2 = plt.plot(epochs_arr, my_history['val_loss'], 'b-', label='validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Model loss')\nplt.legend()\nplt.show()\n\n# plot model accuracy while training\nline1 = plt.plot(epochs_arr, my_history['binary_accuracy'], 'r-', label='training accuracy')\nline2 = plt.plot(epochs_arr, my_history['val_binary_accuracy'], 'b-', label='validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model accuracy')\nplt.legend()\nplt.show()\n\n\n# evaluate the keras model against the test set (DO ONCE)\n_, accuracy = model.evaluate(X_test, y_test)\nprint('Test Accuracy: %.2f' % (accuracy*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Discussion\n\n**Results**\n\nMy regularized deep neural network obtained a 100% (or very close to perfect) classification accuracy on my validation and test data while achieving about 97% accuracy on my training set. Oddly, my validation and test accuracy outperformed my training accuracy, contradicting the idea that ML algorithms are optimized for its given training data.\n\nIf I remove my drop out layers (i.e. don't apply regularization to my model), then my training, validation, and test sets all achieve ~100% accuracy. The regularization appears to slow down my models convergence to 100% accuracy for all data sets, but convergence happens fast (i.e. small number of epochs) nonetheless.\n    \n\n**Interpretation**\n\nI was pretty skeptical about my 100% classification accuracy on my validation and test set, but after comparing with other kernels, I've noticed that such a high accuracy is not uncommon on the mushroom classification data set given the right ML algorithm. However, there still is the issue of having a lower training accuracy than that of my validation and test set. I have a possible explanation for this weird result, but I would love to see what others think!\n\nPerhaps the mushroom data set is inherently easy to classify edible mushrooms (i.e. various independent features are strongly correlated with the output label) such that a good ML algorithm is bound to have ~100% accuracy. However, when we introduce drop out layers, we are randomly disabling neurons which makes it artificially harder for our model to learn from our training data. This loss of information puts a cap on how well our model can perform on our training data.\n\nIn general, regularization should decrease performance on our training data to match that of unseen data, but this decrease in training accuracy is only favorable when validation and test accuracy is not extraordinarily high.\n\nNote: I doubt that I'm overfitting my training data because 1) I'm applying regularization and 2) an overfit model should have a high training accuracy and low validation/test accuracy.\n\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}