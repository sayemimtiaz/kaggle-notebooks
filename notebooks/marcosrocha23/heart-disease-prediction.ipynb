{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\n\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Load the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/heart-disease-uci/heart.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Data preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for missing data\n\nprint(data.isnull().sum())\nprint('\\n No missing data')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Simple exploratory analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking numbers of \"Not Survived\" (1) and \"Survived\" (0)\n\ndata[\"target\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Data modeling + Tuning parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining variables and split data\n\ny = data[\"target\"]\nX = data.drop(labels=[\"target\"], axis = 1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\n# Data dimension\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pip install 'grid search'\n\n!pip install scikit-optimize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a function to train a model that will be used like parameters in 'tuning parameter method'\n\ndef treinar_modelo(params):\n  max_leaf_nodes = params[0]\n  n_estimators = params[1]\n\n  rf = RandomForestClassifier(max_leaf_nodes = max_leaf_nodes, n_estimators = n_estimators)\n  rf.fit(X_train, y_train)\n  predict_rf = rf.predict_proba(X_test)[:,1]\n\n  return -roc_auc_score(y_test, predict_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the library to tuning parameters\nfrom skopt import dummy_minimize \n\n# Dummy minimize will be use to find parameters at randomly from a sample\nspace = [(2, 145), (50, 1000)]\nresultado_random = dummy_minimize(treinar_modelo, dimensions=space, random_state=42, verbose=0)\n\n# Best parameters\nprint(resultado_random.x)\n\n# Score of the best model\nprint(resultado_random.fun)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bayesian optimization\n\nfrom skopt import gp_minimize\nresultados_bayesian = gp_minimize(treinar_modelo, space, n_calls=30, n_random_starts=20, random_state=42, verbose=0)\n\n# Best parameters\nprint(resultados_bayesian.x)\n\n# Score of the best model\nprint(resultados_bayesian.fun)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The best model is:\n\nbest_rf = RandomForestClassifier(n_estimators=51, max_leaf_nodes=114)\nbest_rf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. ML Explainability"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Install library shap for understand the model\n!pip install shap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import library\nimport shap\n\n# Create the objects to understand the model\nexplainer = shap.TreeExplainer(best_rf)\nshap_values = explainer.shap_values(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# init js\nshap.initjs()\n\n# Force plot\nshap.force_plot(explainer.expected_value[1], shap_values[1][0,:], X_train.iloc[0,:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summary plot - find the most important features\n\nshap.initjs()\nshap.summary_plot(shap_values[1], X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dependence plot - select one feature to especfic analysis\n\nshap.initjs()\nshap.dependence_plot(\"thal\", shap_values[1], X_train, interaction_index=None)\nshap.dependence_plot(\"thalach\", shap_values[1], X_train, interaction_index=None)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}