{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting Heart Disease using Machine Learning\nThis notebook will introduce some foundation machine learning and data science concepts by exploring the problem of heart disease classification.\n\nIt is intended to be an end-to-end example of what a data science and machine learning proof of concept might look like.","metadata":{}},{"cell_type":"markdown","source":"# What is classification?\nClassification involves deciding whether a sample is part of one class or another (single-class classification). If there are multiple class options, it's referred to as multi-class classification.","metadata":{}},{"cell_type":"markdown","source":"# we'll look at the following topics.\n\n- Exploratory data analysis (EDA) - the process of going through a dataset and finding out more about it.\n- Model training - create model(s) to learn to predict a target variable based on other variables.\n- Model evaluation - evaluating a models predictions using problem-specific evaluation metrics.\n- Model comparison - comparing several different models to find the best one.\n- Model fine-tuning - once we've found a good model, how can we improve it?\n- Feature importance - since we're predicting the presence of heart disease, are there some things which are more important for prediction?\n- Cross-validation - if we do build a good model, can we be sure it will work on unseen data?\n- Reporting what we've found - if we had to present our work, what would we show someone?\nTo work through these topics, we'll use pandas, Matplotlib and NumPy for data anaylsis, as well as, Scikit-Learn for machine learning and modelling tasks.","metadata":{}},{"cell_type":"markdown","source":"We'll work through each step and by the end of the notebook, we'll have a handful of models, all which can predict whether or not a person has heart disease based on a number of different parameters at a considerable accuracy.\n\nYou'll also be able to describe which parameters are more indicative than others, for example, sex may be more important than age.","metadata":{}},{"cell_type":"markdown","source":"## Problem Definition\nIn our case, the problem we will be exploring is binary classification (a sample can only be one of two things).\n\nThis is because we're going to be using a number of differnet features (pieces of information) about a person to predict whether they have heart disease or not.\n\nIn a statement,\n\nGiven clinical parameters about a patient, can we predict whether or not they have heart disease?","metadata":{}},{"cell_type":"markdown","source":"## Data\n\nWhat you'll want to do here is dive into the data your problem definition is based on. This may involve, sourcing, defining different parameters, talking to experts about it and finding out what you should expect.\n\nThe original database contains 76 attributes, but here only 14 attributes will be used. Attributes (also called features) are the variables what we'll use to predict our target variable.","metadata":{}},{"cell_type":"markdown","source":"Attributes and features are also referred to as independent variables and a target variable can be referred to as a dependent variable.\n\nWe use the independent variables to predict our dependent variable.\n\nOr in our case, the independent variables are a patients different medical attributes and the dependent variable is whether or not they have heart disease.","metadata":{}},{"cell_type":"markdown","source":"# **Evaluation**\n\nThe evaluation metric is something you might define at the start of a project.\n\nSince machine learning is very experimental, you might say something like,\n\nIf we can reach 95% accuracy at predicting whether or not a patient has heart disease during the proof of concept, we'll pursure this project.\n\nThe reason this is helpful is it provides a rough goal for a machine learning engineer or data scientist to work towards.\n\nHowever, due to the nature of experimentation, the evaluation metric may change over time.","metadata":{}},{"cell_type":"markdown","source":"# **Features**\n\nFeatures are different parts of the data. During this step, you'll want to start finding out what you can about the data.\n\nOne of the most common ways to do this, is to create a data dictionary.","metadata":{}},{"cell_type":"markdown","source":"## Heart Disease Data Dictionary\nA data dictionary describes the data you're dealing with. Not all datasets come with them so this is where you may have to do your research or ask a subject matter expert (someone who knows about the data) for more.\n\nThe following are the features we'll use to predict our target variable (heart disease or no heart disease).\n\n1. age - age in years\n2. sex - (1 = male; 0 = female)\n3. cp - chest pain type\n    - 0: Typical angina: chest pain related decrease blood supply to the heart\n    - 1: Atypical angina: chest pain not related to heart\n    - 2: Non-anginal pain: typically esophageal spasms (non heart related)\n    - 3: Asymptomatic: chest pain not showing signs of disease .\n4. trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n    - anything above 130-140 is typically cause for concern\n5. chol - serum cholestoral in mg/dl\n    - serum = LDL + HDL + .2 * triglycerides\n    - above 200 is cause for concern\n6. fbs - (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n    - '>126' mg/dL signals diabetes\n7. restecg - resting electrocardiographic results\n    - 0: Nothing to note\n    - 1: ST-T Wave abnormality\n        - can range from mild symptoms to severe problems\n        - signals non-normal heart beat\n    - 2: Possible or definite left ventricular hypertrophy\n        - Enlarged heart's main pumping chamber\n8. thalach - maximum heart rate achieved\n9. exang - exercise induced angina (1 = yes; 0 = no)\n10. oldpeak - ST depression induced by exercise relative to rest\n    - looks at stress of heart during excercise\n    - unhealthy heart will stress more\n11. slope - the slope of the peak exercise ST segment\n    - 0: Upsloping: better heart rate with excercise (uncommon)\n    - 1: Flatsloping: minimal change (typical healthy heart)\n    - 2: Downslopins: signs of unhealthy heart\n12. ca - number of major vessels (0-3) colored by flourosopy\n    - colored vessel means the doctor can see the blood passing through\n    - the more blood movement the better (no clots)\n13. thal - thalium stress result\n    - 1,3: normal\n    - 6: fixed defect: used to be defect but ok now\n    - 7: reversable defect: no proper blood movement when excercising\n14. target**(output)** - have disease or not (1=yes, 0=no) (= the predicted attribute)\n*Note: No personal identifiable information (PPI) can be found in the dataset.*","metadata":{}},{"cell_type":"code","source":"# Regular EDA and plotting libraries\nimport numpy as np # np is short for numpy\nimport pandas as pd # pandas is so commonly used, it's shortened to pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns # seaborn gets shortened to sns\n\n# We want our plots to appear in the notebook\n%matplotlib inline \n\n## Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n## Model evaluators\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/heart-attack-analysis-prediction-dataset/heart.csv\") # 'DataFrame' shortened to 'df'\ndf.shape # (rows, columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration (exploratory data analysis or EDA)\n","metadata":{}},{"cell_type":"code","source":"# Let's check the top 5 rows of our dataframe\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.rename(columns={'output':'target'},inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see how many positive (1) and negative (0) samples we have in our dataframe\ndf.target.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalized value counts\ndf.target.value_counts(normalize=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the value counts with a bar graph\ndf.target.value_counts().plot(kind=\"bar\", color=[\"salmon\", \"lightblue\"]);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Heart Disease Frequency according to Gender\nIf you want to compare two columns to each other, you can use the function pd.crosstab(column_1, column_2).\n\nThis is helpful if you want to start gaining an intuition about how your independent variables interact with your dependent variables.\n\nLet's compare our target column with the sex column.\n\nRemember from our data dictionary, for the target column, 1 = heart disease present, 0 = no heart disease. And for sex, 1 = male, 0 = female.","metadata":{}},{"cell_type":"code","source":"df.sex.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### There are 207 males and 96 females in our study.","metadata":{}},{"cell_type":"code","source":"# Compare target column with sex column\npd.crosstab(df.target, df.sex)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**What can we infer from this? Let's make a simple heuristic.**\n\nSince there are about 100 women and 72 of them have a postive value of heart disease being present, we might infer, based on this one variable if the participant is a woman, there's a 75% chance she has heart disease.\n\nAs for males, there's about 200 total with around half indicating a presence of heart disease. So we might predict, if the participant is male, 50% of the time he will have heart disease.\n\nAveraging these two values, we can assume, based on no other parameters, if there's a person, there's a 62.5% chance they have heart disease.\n\nThis can be our very simple baseline, we'll try to beat it with machine learning.","metadata":{}},{"cell_type":"markdown","source":"## Making our crosstab visual\nYou can plot the crosstab by using the plot() function and passing it a few parameters such as, kind (the type of plot you want), figsize=(length, width) (how big you want it to be) and color=[colour_1, colour_2] (the different colours you'd like to use).\n\nDifferent metrics are represented best with different kinds of plots. In our case, a bar graph is great. We'll see examples of more later. And with a bit of practice, you'll gain an intuition of which plot to use with different variables.","metadata":{}},{"cell_type":"code","source":"# Create a plot\npd.crosstab(df.target, df.sex).plot(kind=\"bar\", figsize=(10,6), color=[\"salmon\", \"lightblue\"])\n\n# Add some attributes to it\nplt.title(\"Heart Disease Frequency for Sex\")\nplt.xlabel(\"0 = No Disease, 1 = Disease\")\nplt.ylabel(\"Amount\")\nplt.legend([\"Female\", \"Male\"])\nplt.xticks(rotation=0); # keep the labels on the x-axis vertical","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Age vs Max Heart rate for Heart Disease\ntry to combining a couple of independent variables, such as, age and thalach (maximum heart rate) and then comparing them to our target variable heart disease.\n\nBecause there are so many different values for age and thalach, we'll use a scatter plot.","metadata":{}},{"cell_type":"code","source":"# Create another figure\nplt.figure(figsize=(10,6))\n\n# Start with positve examples\nplt.scatter(df.age[df.target==1], \n            df.thalachh[df.target==1], \n            c=\"salmon\") # define it as a scatter figure\n\n# Now for negative examples, we want them on the same plot, so we call plt again\nplt.scatter(df.age[df.target==0], \n            df.thalachh[df.target==0], \n            c=\"lightblue\") # axis always come as (x, y)\n\n# Add some helpful info\nplt.title(\"Heart Disease in function of Age and Max Heart Rate\")\nplt.xlabel(\"Age\")\nplt.legend([\"Disease\", \"No Disease\"])\nplt.ylabel(\"Max Heart Rate\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nWhat can we infer from this?\n\nIt seems the younger someone is, the higher their max heart rate (dots are higher on the left of the graph) and the older someone is, the more green dots there are. But this may be because there are more dots all together on the right side of the graph (older participants).\n\nBoth of these are observational of course, but this is what we're trying to do, build an understanding of the data.\n\nLet's check the age distribution.","metadata":{}},{"cell_type":"code","source":"# Histograms are a great way to check the distribution of a variable\ndf.age.plot.hist();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see it's a normal distribution but slightly swaying to the right, which reflects in the scatter plot above.\n\nLet's keep going.","metadata":{}},{"cell_type":"markdown","source":"## Heart Disease Frequency per Chest Pain Type\nLet's try another independent variable. This time, cp (chest pain).\n\nWe'll use the same process as we did before with sex.","metadata":{}},{"cell_type":"code","source":"pd.crosstab(df.cp, df.target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a new crosstab and base plot\npd.crosstab(df.cp, df.target).plot(kind=\"bar\", \n                                   figsize=(10,6), \n                                   color=[\"lightblue\", \"salmon\"])\n\n# Add attributes to the plot to make it more readable\nplt.title(\"Heart Disease Frequency Per Chest Pain Type\")\nplt.xlabel(\"Chest Pain Type\")\nplt.ylabel(\"Frequency\")\nplt.legend([\"No Disease\", \"Disease\"])\nplt.xticks(rotation = 0);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What can we infer from this?\n\nRemember from our data dictionary what the different levels of chest pain are.\n\ncp - chest pain type\n0: Typical angina: chest pain related decrease blood supply to the heart\n1: Atypical angina: chest pain not related to heart\n2: Non-anginal pain: typically esophageal spasms (non heart related)\n3: Asymptomatic: chest pain not showing signs of disease\nIt's interesting the atypical agina (value 1) states it's not related to the heart but seems to have a higher ratio of participants with heart disease than not.","metadata":{}},{"cell_type":"markdown","source":"# Correlation between independent variables\nFinally, we'll compare all of the independent variables in one hit.\n\nWhy?\n\nBecause this may give an idea of which independent variables may or may not have an impact on our target variable.\n\nWe can do this using df.corr() which will create a correlation matrix for us, in other words, a big table of numbers telling us how related each variable is the other.","metadata":{}},{"cell_type":"code","source":"# Find the correlation between our independent variables\ncorr_matrix = df.corr()\ncorr_matrix ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's make it look a little prettier\ncorr_matrix = df.corr()\nplt.figure(figsize=(15, 10))\nsns.heatmap(corr_matrix, \n            annot=True, \n            linewidths=0.5, \n            fmt= \".2f\", \n            cmap=\"YlGnBu\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A higher positive value means a potential positive correlation (increase) and a higher negative value means a potential negative correlation (decrease).","metadata":{}},{"cell_type":"markdown","source":"# **let's model**","metadata":{}},{"cell_type":"markdown","source":"Aside from our basline estimate using sex, the rest of the data seems to be pretty distributed.","metadata":{}},{"cell_type":"markdown","source":"**We're trying to predict our target variable using all of the other variables.**\n\n**To do this, we'll split the target variable from the rest.**","metadata":{}},{"cell_type":"code","source":"# Everything except target variable\nX = df.drop(\"target\", axis=1)\n\n# Target variable\ny = df.target.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Independent variables (no target column)\nX.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Targets\ny","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training and test split\nNow comes one of the most important concepts in machine learning, the training/test split.\n\nThis is where you'll split your data into a training set and a test set.\n\nYou use your training set to train your model and your test set to test it.\n\nThe test set must remain separate from your training set.","metadata":{}},{"cell_type":"markdown","source":"### Why not use all the data to train a model?\nLet's say you wanted to take your model into the hospital and start using it on patients. How would you know how well your model goes on a new patient not included in the original full dataset you had?\n\nThis is where the test set comes in. It's used to mimic taking your model to a real environment as much as possible.\n\nAnd it's why it's important to never let your model learn from the test set, it should only be evaluated on it.\n\nTo split our data into a training and test set, we can use Scikit-Learn's train_test_split() and feed it our independent and dependent variables (X & y).","metadata":{}},{"cell_type":"code","source":"# Random seed for reproducibility\nnp.random.seed(42)\n\n# Split into train & test set\nX_train, X_test, y_train, y_test = train_test_split(X, # independent variables \n                                                    y, # dependent variable\n                                                    test_size = 0.2) # percentage of data to use for test set","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The ' test_size ' parameter is used to tell the 'train_test_split()' function how much of our data we want in the test set.\n\nA rule of thumb is to use 80% of your data to train on and the other 20% to test on.\n\nFor our problem, a train and test set are enough. But for other problems, you could also use a validation (train/validation/test) set or cross-validation (we'll see this in a second).","metadata":{}},{"cell_type":"code","source":"X_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train, len(y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Beautiful, we can see we're using 242 samples to train on. Let's look at our test data.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"X_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test, len(y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And we've got 61 examples we'll test our model(s) on. Let's build some.","metadata":{}},{"cell_type":"markdown","source":"### Model choices\n\nNow we've got our data prepared, we can start to fit models. We'll be using the following and comparing their results.\n\n1. Logistic Regression - [`LogisticRegression()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n2. K-Nearest Neighbors - [`KNeighboursClassifier()`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n3. RandomForest - [`RandomForestClassifier()`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)","metadata":{}},{"cell_type":"code","source":"# Put models in a dictionary\nmodels = {\"KNN\": KNeighborsClassifier(),\n          \"Logistic Regression\": LogisticRegression(), \n          \"Random Forest\": RandomForestClassifier()}\n\n# Create function to fit and score models\ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n    \"\"\"\n    Fits and evaluates given machine learning models.\n    models : a dict of different Scikit-Learn machine learning models\n    X_train : training data\n    X_test : testing data\n    y_train : labels assosciated with training data\n    y_test : labels assosciated with test data\n    \"\"\"\n    # Random seed for reproducible results\n    np.random.seed(42)\n    # Make a list to keep model scores\n    model_scores = {}\n    # Loop through models\n    for name, model in models.items():\n        # Fit the model to the data\n        model.fit(X_train, y_train)\n        # Evaluate the model and append its score to model_scores\n        model_scores[name] = model.score(X_test, y_test)\n    return model_scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_scores = fit_and_score(models=models,\n                             X_train=X_train,\n                             X_test=X_test,\n                             y_train=y_train,\n                             y_test=y_test)\nmodel_scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"Beautiful! Since our models are fitting, let's compare them visually.","metadata":{}},{"cell_type":"markdown","source":"## Model Comparison\n\nSince we've saved our models scores to a dictionary, we can plot them by first converting them to a DataFrame.","metadata":{}},{"cell_type":"code","source":"model_compare = pd.DataFrame(model_scores, index=['accuracy'])\nmodel_compare.T.plot.bar();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Beautiful! We can't really see it from the graph but looking at the dictionary, the [LogisticRegression()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model performs best.\n\nSince you've found the best model. Let's take it to the boss and show her what we've found.\n\n> **You:** I've found it!\n\n> **Her:** Nice one! What did you find?\n    \n> **You:** The best algorithm for prediting heart disease is a LogisticRegrssion!\n\n> **Her:** Excellent. I'm surprised the hyperparameter tuning is finished by now.\n\n> **You:** *wonders what **hyperparameter tuning** is*\n    \n> **You:** Ummm yeah, me too, it went pretty quick.\n    \n> **Her:** I'm very proud, how about you put together a **classification report** to show the team, and be sure to include a **confusion matrix**, and the **cross-validated precision**, **recall** and **F1 scores**. I'd also be curious to see what **features are most important**. Oh and don't forget to include a **ROC curve**.\n    \n> **You:** *asks self, \"what are those???\"*\n    \n> **You:** Of course! I'll have to you by tomorrow.\n","metadata":{}},{"cell_type":"markdown","source":"### Tune KNeighborsClassifier (K-Nearest Neighbors or KNN) by hand\n\nThere's one main hyperparameter we can tune for the K-Nearest Neighbors (KNN) algorithm, and that is number of neighbours. The default is 5 (`n_neigbors=5`).","metadata":{}},{"cell_type":"code","source":"# Create a list of train scores\ntrain_scores = []\n\n# Create a list of test scores\ntest_scores = []\n\n# Create a list of different values for n_neighbors\nneighbors = range(1, 21) # 1 to 20\n\n# Setup algorithm\nknn = KNeighborsClassifier()\n\n# Loop through different neighbors values\nfor i in neighbors:\n    knn.set_params(n_neighbors = i) # set neighbors value\n    \n    # Fit the algorithm\n    knn.fit(X_train, y_train)\n    \n    # Update the training scores\n    train_scores.append(knn.score(X_train, y_train))\n    \n    # Update the test scores\n    test_scores.append(knn.score(X_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(neighbors, train_scores, label=\"Train score\")\nplt.plot(neighbors, test_scores, label=\"Test score\")\nplt.xticks(np.arange(1, 21, 1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.legend()\n\nprint(f\"Maximum KNN score on the test data: {max(test_scores)*100:.2f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the graph, `n_neighbors = 11` seems best.\n\nEven knowing this, the `KNN`'s model performance didn't get near what `LogisticRegression` or the `RandomForestClassifier` did.\n\nBecause of this, we'll discard `KNN` and focus on the other two.\n\nWe've tuned `KNN` by hand but let's see how we can `LogisticsRegression` and `RandomForestClassifier` using [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html).\n\nInstead of us having to manually try different hyperparameters by hand, `RandomizedSearchCV` tries a number of different combinations, evaluates them and saves the best.\n\n### Tuning models with with [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n\nReading the Scikit-Learn documentation for [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV), we find there's a number of different hyperparameters we can tune.\n\nThe same for [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n\nLet's create a hyperparameter grid (a dictionary of different hyperparameters) for each and then test them out.","metadata":{}},{"cell_type":"code","source":"# Different LogisticRegression hyperparameters\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n                \"solver\": [\"liblinear\"]}\n\n# Different RandomForestClassifier hyperparameters\nrf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n           \"max_depth\": [None, 3, 5, 10],\n           \"min_samples_split\": np.arange(2, 20, 2),\n           \"min_samples_leaf\": np.arange(1, 20, 2)}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's use `RandomizedSearchCV` to try and tune our `LogisticRegression` model.\n\nWe'll pass it the different hyperparameters from `log_reg_grid` as well as set `n_iter = 20`. This means, `RandomizedSearchCV` will try 20 different combinations of hyperparameters from `log_reg_grid` and save the best ones.","metadata":{}},{"cell_type":"code","source":"# Setup random seed\nnp.random.seed(42)\n\n# Setup random hyperparameter search for LogisticRegression\nrs_log_reg = RandomizedSearchCV(LogisticRegression(),\n                                param_distributions=log_reg_grid,\n                                cv=5,\n                                n_iter=20,\n                                verbose=True)\n\n# Fit random hyperparameter search model\nrs_log_reg.fit(X_train, y_train);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rs_log_reg.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rs_log_reg.score(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we've tuned `LogisticRegression` using `RandomizedSearchCV`, we'll do the same for `RandomForestClassifier`.","metadata":{}},{"cell_type":"code","source":"# Setup random seed\nnp.random.seed(42)\n\n# Setup random hyperparameter search for RandomForestClassifier\nrs_rf = RandomizedSearchCV(RandomForestClassifier(),\n                           param_distributions=rf_grid,\n                           cv=5,\n                           n_iter=20,\n                           verbose=True)\n\n# Fit random hyperparameter search model\nrs_rf.fit(X_train, y_train);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find the best parameters\nrs_rf.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the randomized search random forest model\nrs_rf.score(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Excellent! Tuning the hyperparameters for each model saw a slight performance boost in both the `RandomForestClassifier` and `LogisticRegression`.\n\nThis is akin to tuning the settings on your oven and getting it to cook your favourite dish just right.\n\nBut since `LogisticRegression` is pulling out in front, we'll try tuning it further with [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n\n### Tuning a model with [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n\nThe difference between `RandomizedSearchCV` and `GridSearchCV` is where `RandomizedSearchCV` searches over a grid of hyperparameters performing `n_iter` combinations, `GridSearchCV` will test every single possible combination.\n\nIn short:\n* `RandomizedSearchCV` - tries `n_iter` combinations of hyperparameters and saves the best.\n* `GridSearchCV` - tries every single combination of hyperparameters and saves the best.\n\nLet's see it in action.","metadata":{}},{"cell_type":"code","source":"# Different LogisticRegression hyperparameters\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n                \"solver\": [\"liblinear\"]}\n\n# Setup grid hyperparameter search for LogisticRegression\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                          param_grid=log_reg_grid,\n                          cv=5,\n                          verbose=True)\n\n# Fit grid hyperparameter search model\ngs_log_reg.fit(X_train, y_train);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the best parameters\ngs_log_reg.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model\ngs_log_reg.score(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this case, we get the same results as before since our grid only has a maximum of 20 different hyperparameter combinations.\n\n**Note:** If there are a large amount of hyperparameters combinations in your grid, `GridSearchCV` may take a long time to try them all out. This is why it's a good idea to start with `RandomizedSearchCV`, try a certain amount of combinations and then use `GridSearchCV` to refine them.","metadata":{}},{"cell_type":"markdown","source":"## Evaluating a classification model, beyond accuracy\n\nNow we've got a tuned model, let's get some of the metrics we discussed before.\n\nWe want:\n* ROC curve and AUC score - [`plot_roc_curve()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_roc_curve.html#sklearn.metrics.plot_roc_curve)\n* Confusion matrix - [`confusion_matrix()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n* Classification report - [`classification_report()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)\n* Precision - [`precision_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)\n* Recall - [`recall_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)\n* F1-score - [`f1_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)\n\nLuckily, Scikit-Learn has these all built-in.\n\nTo access them, we'll have to use our model to make predictions on the test set. You can make predictions by calling `predict()` on a trained model and passing it the data you'd like to predict on.\n\nWe'll make predictions on the test data.","metadata":{}},{"cell_type":"code","source":"# Make preidctions on test data\ny_preds = gs_log_reg.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we've got our prediction values we can find the metrics we want.\n\nLet's start with the ROC curve and AUC scores.\n\n### ROC Curve and AUC Scores\n\nWhat's a ROC curve?\n\nIt's a way of understanding how your model is performing by comparing the true positive rate to the false positive rate.\n\nIn our case...\n\n> To get an appropriate example in a real-world problem, consider a diagnostic test that seeks to determine whether a person has a certain disease. A false positive in this case occurs when the person tests positive, but does not actually have the disease. A false negative, on the other hand, occurs when the person tests negative, suggesting they are healthy, when they actually do have the disease.\n\nScikit-Learn implements a function `plot_roc_curve` which can help us create a ROC curve as well as calculate the area under the curve (AUC) metric.\n\nReading the documentation on the [`plot_roc_curve`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_roc_curve.html) function we can see it takes `(estimator, X, y)` as inputs. Where `estiamator` is a fitted machine learning model and `X` and `y` are the data you'd like to test it on.\n\nIn our case, we'll use the GridSearchCV version of our `LogisticRegression` estimator, `gs_log_reg` as well as the test data, `X_test` and `y_test`.","metadata":{}},{"cell_type":"code","source":"# Import ROC curve function from metrics module\nfrom sklearn.metrics import plot_roc_curve\n\n# Plot ROC curve and calculate AUC metric\nplot_roc_curve(gs_log_reg, X_test, y_test);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is great, our model does far better than guessing which would be a line going from the bottom left corner to the top right corner, AUC = 0.5. But a perfect model would achieve an AUC score of 1.0, so there's still room for improvement.\n\nLet's move onto the next evaluation request, a confusion matrix.\n\n### Confusion matrix \n\nA confusion matrix is a visual way to show where your model made the right predictions and where it made the wrong predictions (or in other words, got confused).\n\nScikit-Learn allows us to create a confusion matrix using [`confusion_matrix()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) and passing it the true labels and predicted labels.","metadata":{}},{"cell_type":"code","source":"# Display confusion matrix\nprint(confusion_matrix(y_test, y_preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, Scikit-Learn's built-in confusion matrix is a bit bland. For a presentation you'd probably want to make it visual.\n\nLet's create a function which uses Seaborn's [`heatmap()`](https://seaborn.pydata.org/generated/seaborn.heatmap.html) for doing so.","metadata":{}},{"cell_type":"code","source":"# Import Seaborn\nimport seaborn as sns\nsns.set(font_scale=1.5) # Increase font size\n\ndef plot_conf_mat(y_test, y_preds):\n    \"\"\"\n    Plots a confusion matrix using Seaborn's heatmap().\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n                     annot=True, # Annotate the boxes\n                     cbar=False)\n    plt.xlabel(\"true label\")\n    plt.ylabel(\"predicted label\")\n    \nplot_conf_mat(y_test, y_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Beautiful! That looks much better. \n\nYou can see the model gets confused (predicts the wrong label) relatively the same across both classes. In essence, there are 4 occasaions where the model predicted 0 when it should've been 1 (false negative) and 3 occasions where the model predicted 1 instead of 0 (false positive).","metadata":{}},{"cell_type":"markdown","source":"### Classification report\n\nWe can make a classification report using [`classification_report()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) and passing it the true labels as well as our models predicted labels. \n\nA classification report will also give us information of the precision and recall of our model for each class.","metadata":{}},{"cell_type":"code","source":"# Show classification report\nprint(classification_report(y_test, y_preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What's going on here?\n\nLet's get a refresh.\n\n* **Precision** - Indicates the proportion of positive identifications (model predicted class 1) which were actually correct. A model which produces no false positives has a precision of 1.0.\n* **Recall** - Indicates the proportion of actual positives which were correctly classified. A model which produces no false negatives has a recall of 1.0.\n* **F1 score** - A combination of precision and recall. A perfect model achieves an F1 score of 1.0.\n* **Support** - The number of samples each metric was calculated on.\n* **Accuracy** - The accuracy of the model in decimal form. Perfect accuracy is equal to 1.0.\n* **Macro avg** - Short for macro average, the average precision, recall and F1 score between classes. Macro avg doesnâ€™t class imbalance into effort, so if you do have class imbalances, pay attention to this metric.\n* **Weighted avg** - Short for weighted average, the weighted average precision, recall and F1 score between classes. Weighted means each metric is calculated with respect to how many samples there are in each class. This metric will favour the majority class (e.g. will give a high value when one class out performs another due to having more samples).\n\nOk, now we've got a few deeper insights on our model. But these were all calculated using a single training and test set.\n\nWhat we'll do to make them more solid is calculate them using cross-validation.\n\nHow?\n\nWe'll take the best model along with the best hyperparameters and use [`cross_val_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) along with various `scoring` parameter values.\n\n`cross_val_score()` works by taking an estimator (machine learning model) along with data and labels. It then evaluates the machine learning model on the data and labels using cross-validation and a defined `scoring` parameter.\n\nLet's remind ourselves of the best hyperparameters and then see them in action.","metadata":{}},{"cell_type":"code","source":"# Check best hyperparameters\ngs_log_reg.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import cross_val_score\nfrom sklearn.model_selection import cross_val_score\n\n# Instantiate best model with best hyperparameters (found with GridSearchCV)\nclf = LogisticRegression(C=0.23357214690901212,\n                         solver=\"liblinear\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cross-validated accuracy score\ncv_acc = cross_val_score(clf,\n                         X,\n                         y,\n                         cv=5, # 5-fold cross-validation\n                         scoring=\"accuracy\") # accuracy as scoring\ncv_acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_acc = np.mean(cv_acc)\ncv_acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cross-validated precision score\ncv_precision = np.mean(cross_val_score(clf,\n                                       X,\n                                       y,\n                                       cv=5, # 5-fold cross-validation\n                                       scoring=\"precision\")) # precision as scoring\ncv_precision","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cross-validated recall score\ncv_recall = np.mean(cross_val_score(clf,\n                                    X,\n                                    y,\n                                    cv=5, # 5-fold cross-validation\n                                    scoring=\"recall\")) # recall as scoring\ncv_recall","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cross-validated F1 score\ncv_f1 = np.mean(cross_val_score(clf,\n                                X,\n                                y,\n                                cv=5, # 5-fold cross-validation\n                                scoring=\"f1\")) # f1 as scoring\ncv_f1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing cross-validated metrics\ncv_metrics = pd.DataFrame({\"Accuracy\": cv_acc,\n                            \"Precision\": cv_precision,\n                            \"Recall\": cv_recall,\n                            \"F1\": cv_f1},\n                          index=[0])\ncv_metrics.T.plot.bar(title=\"Cross-Validated Metrics\", legend=False);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great! This looks like something we could share. An extension might be adding the metrics on top of each bar so someone can quickly tell what they were.\n\nWhat now?\n\nThe final thing to check off the list of our model evaluation techniques is feature importance.\n\n## Feature importance\n\nFeature importance is another way of asking, \"which features contributing most to the outcomes of the model?\"\n\nOr for our problem, trying to predict heart disease using a patient's medical characterisitcs, which charateristics contribute most to a model predicting whether someone has heart disease or not?\n\nUnlike some of the other functions we've seen, because how each model finds patterns in data is slightly different, how a model judges how important those patterns are is different as well. This means for each model, there's a slightly different way of finding which features were most important.\n\nYou can usually find an example via the Scikit-Learn documentation or via searching for something like \"[MODEL TYPE] feature importance\", such as, \"random forest feature importance\".\n\nSince we're using `LogisticRegression`, we'll look at one way we can calculate feature importance for it.\n\nTo do so, we'll use the `coef_` attribute. Looking at the [Scikit-Learn documentation for `LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), the `coef_` attribute is the coefficient of the features in the decision function.\n\nWe can access the `coef_` attribute after we've fit an instance of `LogisticRegression`.","metadata":{}},{"cell_type":"code","source":"# Fit an instance of LogisticRegression (taken from above)\nclf.fit(X_train, y_train);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check coef_\nclf.coef_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at this it might not make much sense. But these values are how much each feature contributes to how a model makes a decision on whether patterns in a sample of patients health data leans more towards having heart disease or not.\n\nEven knowing this, in it's current form, this `coef_` array still doesn't mean much. But it will if we combine it with the columns (features) of our dataframe.","metadata":{}},{"cell_type":"code","source":"# Match features to columns\nfeatures_dict = dict(zip(df.columns, list(clf.coef_[0])))\nfeatures_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we've match the feature coefficients to different features, let's visualize them. ","metadata":{}},{"cell_type":"code","source":"# Visualize feature importance\nfeatures_df = pd.DataFrame(features_dict, index=[0])\nfeatures_df.T.plot.bar(title=\"Feature Importance\", legend=False);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You'll notice some are negative and some are positive.\n\nThe larger the value (bigger bar), the more the feature contributes to the models decision.\n\nIf the value is negative, it means there's a negative correlation. And vice versa for positive values. \n\nFor example, the `sex` attribute has a negative value of -0.904, which means as the value for `sex` increases, the `target` value decreases.\n\nWe can see this by comparing the `sex` column to the `target` column.","metadata":{}},{"cell_type":"code","source":"pd.crosstab(df[\"sex\"], df[\"target\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can see, when `sex` is 0 (female), there are almost 3 times as many (72 vs. 24) people with heart disease (`target` = 1) than without.\n\nAnd then as `sex` increases to 1 (male), the ratio goes down to almost 1 to 1 (114 vs. 93) of people who have heart disease and who don't.\n\nWhat does this mean?\n\nIt means the model has found a pattern which reflects the data. Looking at these figures and this specific dataset, it seems if the patient is female, they're more likely to have heart disease.\n\nHow about a positive correlation?","metadata":{}},{"cell_type":"code","source":"# Contrast slope (positive coefficient) with target\npd.crosstab(df[\"slp\"], df[\"target\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking back the data dictionary, we see `slope` is the \"slope of the peak exercise ST segment\" where:\n* 0: Upsloping: better heart rate with excercise (uncommon)\n* 1: Flatsloping: minimal change (typical healthy heart)\n* 2: Downslopins: signs of unhealthy heart\n    \nAccording to the model, there's a positive correlation of 0.470, not as strong as `sex` and `target` but still more than 0.\n\nThis positive correlation means our model is picking up the pattern that as `slope` increases, so does the `target` value.\n\nIs this true?\n\nWhen you look at the contrast (`pd.crosstab(df[\"slope\"], df[\"target\"]`) it is. As `slope` goes up, so does `target`. \n\nWhat can you do with this information?\n\nThis is something you might want to talk to a subject matter expert about. They may be interested in seeing where machine learning model is finding the most patterns (highest correlation) as well as where it's not (lowest correlation). \n\nDoing this has a few benefits:\n1. **Finding out more** - If some of the correlations and feature importances are confusing, a subject matter expert may be able to shed some light on the situation and help you figure out more.\n2. **Redirecting efforts** - If some features offer far more value than others, this may change how you collect data for different problems. See point 3.\n3. **Less but better** - Similar to above, if some features are offering far more value than others, you could reduce the number of features your model tries to find patterns in as well as improve the ones which offer the most. This could potentially lead to saving on computation, by having a model find patterns across less features, whilst still achieving the same performance levels.","metadata":{}},{"cell_type":"markdown","source":"Remember we defined one in step 3.\n\n> If we can reach 95% accuracy at predicting whether or not a patient has heart disease during the proof of concept, we'll pursure this project.\n\nIn this case, we didn't. The highest accuracy our model achieved was below 90%.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}