{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1> <u>Stroke prediction</u></h1>","metadata":{}},{"cell_type":"markdown","source":"<img src=\"img.jpg\" width=\"60%\">","metadata":{}},{"cell_type":"markdown","source":"# Problem statement\nStroke is sometimes termed as brain attack or a cardiovascular accident (CVA). It is much like a heart attack, only it occurs in the brain.<br>\n\nIt occurs when the supply of blood to the brain is reduced or blocked completely, which prevents brain tissue from getting oxygen and nutrients.<br>\n\nAccording to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.<br>\nEarly identification of stroke can help doctors to give necessary medication to the patient.\n\n\n## Machine Learning problem\nPredict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status.<br>\n<b>Type</b> : Supervised Learning<br>\n<b>Task</b> : Binary classification<br>\n<b>Performance metric</b> : F1 score (since imbalanced classes)<br>","metadata":{}},{"cell_type":"markdown","source":"## About Dataset\nSource : https://www.kaggle.com/fedesoriano/stroke-prediction-dataset\n\n### Attribute Information\n<b>id</b>: unique identifier<br>\n<b>gender</b>: Male, Female or Other<br>\n<b>age</b>: age of the patient<br>\n<b>hypertension</b>: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension<br>\n<b>heart_disease</b>: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease<br>\n<b>ever_married</b>: No or Yes<br>\n<b>work_type</b>: children, Govt_jov, Never_worked, Private or Self-employed<br>\n<b>Residence_type</b>: Rural or Urban<br>\n<b>avg_glucose_level</b>: average glucose level in blood<br>\n<b>bmi</b>: body mass index<br>\n<b>smoking_status</b>: formerly smoked, never smoked, smokes or Unknown<br>\n<b>stroke</b>: 1 if the patient had a stroke or 0 if not (target)<br>","metadata":{}},{"cell_type":"markdown","source":"# Libraries\nImporting all the necessary python modules","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.impute import KNNImputer\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import f1_score, confusion_matrix\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\nimport pickle\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# settings\nsns.set_style('whitegrid')\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('display.max_columns', None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### K-Fold Cross-Validation\nStep 1: Randomly divide a dataset into k groups, or ‚Äúfolds‚Äù, of roughly equal size.<br>\nStep 2: Choose one of the folds to be the holdout set. Fit the model on the remaining k-1 folds.<br>\nStep 3: Calculate the test F1-score on the observations in the fold that was held out.<br>\nStep 4: Repeat this process k times, using a different set each time as the holdout set.<br>\nStep 5: Calculate the average of the k test F1-scores to get the overall test F1-score.","metadata":{}},{"cell_type":"code","source":"# Below function implements above steps.\ndef run_kfold(model, X_train, y_train, N_SPLITS = 10):\n    f1_list = []\n    oofs = np.zeros(len(X_train))\n    folds = StratifiedKFold(n_splits=N_SPLITS)\n    for i, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n        \n        print(f'\\n------------- Fold {i + 1} -------------')\n        X_trn, y_trn = X_train.iloc[trn_idx], y_train.iloc[trn_idx]\n        X_val, y_val = X_train.iloc[val_idx], y_train.iloc[val_idx]\n        \n        model.fit(X_trn, y_trn)\n        # Instead of directly predicting the classes we will obtain the probability of positive class.\n        preds_val = model.predict_proba(X_val)[:,1]\n        \n        fold_f1 = f1_score(y_val, preds_val.round())\n        f1_list.append(fold_f1)\n        \n        print(f'\\nf1 score for validation set is {fold_f1}') \n        \n        oofs[val_idx] = preds_val\n        \n    mean_f1 = sum(f1_list)/N_SPLITS\n    print(\"\\nMean validation f1 score :\", mean_f1)\n    \n    oofs_score = f1_score(y_train, oofs.round())\n    print(f'\\nF1 score for oofs is {oofs_score}')\n    return oofs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preprocessing","metadata":{}},{"cell_type":"code","source":"# Load data into memory\ndata = pd.read_csv('/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')\n\nprint(\"No of columns in the data : \", len(data.columns))\nprint(\"No of rows in the data : \", len(data))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# random sample of data\ndata.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# statistical summary of the data\ndata.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# null values\ndata.isna().sum().to_frame(name=\"Null count\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Variable separation","metadata":{}},{"cell_type":"code","source":"# features\nfeatures = ['gender', 'age', 'hypertension', 'heart_disease',\n            'ever_married','work_type','Residence_type','avg_glucose_level',\n            'bmi','smoking_status']\n\n#target\ntarget = 'stroke'\n\nnumerical_features = ['age', 'avg_glucose_level', 'bmi']\n\ncategorical_features = ['gender', 'hypertension', 'heart_disease',\n                        'ever_married', 'work_type', 'Residence_type', \n                        'smoking_status']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting features into required datatypes\ndata[numerical_features] = data[numerical_features].astype(np.float64)\n\ndata[categorical_features] = data[categorical_features].astype('category')\n\n# Replace Other label in gender with Female\ndata.gender.replace({'Other':\"Female\"}, inplace=True)\n\n# Remove id column\ndata.drop('id', axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data types\ndata[features+[target]].dtypes.to_frame(name=\"Data type\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Test Split\n- Dividing the total dataset into training and testing sets\n- For Training 75% of data\n- For Testing 25% of data","metadata":{}},{"cell_type":"code","source":"train, test = train_test_split(data, random_state=1,\n                               test_size=0.25,\n                               stratify=data.stroke)\n\nprint(\"No. of data points in training set : \", len(train))\nprint(\"No. of data points in testing set : \", len(test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fill Missing values\nUsing K-nearest neighbors of numerical features to fill the missing values in bmi","metadata":{}},{"cell_type":"code","source":"imputer = KNNImputer(n_neighbors = 5)\n\ntrain[numerical_features] = imputer.fit_transform(train[numerical_features])\ntest[numerical_features] = imputer.transform(test[numerical_features])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis\nExploratory data analysis (EDA) is used to analyze and investigate data sets and summarize their main characteristics, often employing data visualization methods.\n\nPerforming EDA on Training set only (best practice to avoid overfitting)\n## Univariate analysis\n- Univariate analysis refers to the analysis of one variable.\n- The purpose of univariate analysis is to understand the distribution of values for a single variable.\n\n### A. Target distribution","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=2,figsize=(12, 4))\ntrain[target].value_counts(normalize=True).plot \\\n.bar(width=0.2, color=('red','green'), ax=axes[0], title=\"Train\")\n\ntest[target].value_counts(normalize=True).plot \\\n.bar(width=0.2, color=('red','green'), ax=axes[1], title=\"Test\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### B. Histogram","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=3,figsize=(8, 8))\nfor i, c in enumerate(numerical_features):\n    hist = train[c].plot(kind = 'hist', ax=axes[i], \n                         title=c, color='blue', bins=30)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Boxplot (Outliers)\nAn outlier is a data point that differs significantly from other observations.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=3, figsize=(8, 7))\nfor i, c in enumerate(numerical_features):\n    box = train[c].plot(kind = 'box', ax=axes[i],\n                        vert=False, color='blue')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### KDE Plot","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=3, figsize=(8, 7))\nfor i, c in enumerate(numerical_features):\n    plot = sns.kdeplot(data=train, x=c, ax=axes[i],\n                       fill=True, color='blue')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pie-Charts\nPercentage of labels in categorical features","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(4, 2, figsize=(12,16))\naxes = [ax for axes_row in axes for ax in axes_row]\n\nfor i,c in enumerate(categorical_features):\n    train[c].value_counts() \\\n    .plot(kind='pie', ax=axes[i], title=c, autopct=\"%.2f\", fontsize=14)\n    axes[i].set_ylabel('')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Bivariate analysis\nIt involves the analysis of two variables, for the purpose of determining the empirical relationship between them.\n\nWe perform bivariate analysis of features with respect to target.\n\n### Box plots","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=3, figsize=(8, 8))\nfor i, c in enumerate(numerical_features): \n    plot = sns.boxplot(x=train[target], y=train[c], ax=axes[i])\n    axes[i].set_ylabel(c, fontsize=13)\n    axes[i].set_xlabel(target, fontsize=13)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Target vs Mean of Numerical features","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=3, figsize=(20, 5))\nfor i, c in enumerate(numerical_features):\n    train.groupby(target)[c].mean().plot(kind = 'bar', ax=axes[i], color=('red','green'))\n    axes[i].set_ylabel(f'Mean_{c}', fontsize=14)\n    axes[i].set_xlabel('stroke', fontsize=14)\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Target vs categorical features","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 4, figsize=(20,10))\naxes = [ax for axes_row in axes for ax in axes_row]\n\nfor i, c in enumerate(categorical_features):\n    df = train[[c,target]].groupby(c).mean().reset_index()\n    sns.barplot(df[c], df[target], ax=axes[i])\n    axes[i].set_ylabel('Target mean', fontsize=14)\n    axes[i].set_xlabel(c, fontsize=14)\n    \nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering\nFeature engineering is the process of using domain knowledge to extract features from raw data. These features can be used to improve the performance of machine learning algorithms.","metadata":{}},{"cell_type":"markdown","source":"## New Features with Age and bmi","metadata":{}},{"cell_type":"code","source":"def age_group(x):\n    if x<13: return \"Child\"\n    elif 13<x<20: return \"Teenager\"\n    elif 20<x<=60: return \"Adult\"\n    else: return \"Elder\"\n    \ntrain[\"age_group\"] = train.age.apply(age_group)\ntest['age_group'] = test.age.apply(age_group)\n\ndef bmi_group(x):\n    if x<18.5 : return \"UnderWeight\"\n    elif 18.5<x<25: return \"Healthy\"\n    elif 25<x<30: return \"OverWeight\"\n    else: return \"Obese\"\n\ntrain[\"bmi_group\"] = train.bmi.apply(bmi_group)\ntest['bmi_group'] = test.bmi.apply(bmi_group)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## OneHot encoding\n- Replaces categorical column(s) with the binary value for each category.","metadata":{}},{"cell_type":"code","source":"# add new features\ncategorical_features.extend([\"age_group\", \"bmi_group\"])\n\nencoder = OneHotEncoder(drop='first', sparse=False)\nencoder.fit(train[categorical_features])\n\ncols = encoder.get_feature_names(categorical_features)\n\ntrain.loc[:, cols] = encoder.transform(train[categorical_features])\ntest.loc[:, cols] = encoder.transform(test[categorical_features])\n\n# Drop categorical features\ntrain.drop(categorical_features, axis=1, inplace=True)\ntest.drop(categorical_features, axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Scaling\nStandardize the numerical features","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(train[numerical_features])\n\ntrain.loc[:, numerical_features] = scaler.transform(train[numerical_features])\ntest.loc[:, numerical_features] = scaler.transform(test[numerical_features])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ## Correlation\n Correalation between features and target","metadata":{}},{"cell_type":"code","source":"# Correlation with Target\n\ncorr = train.corr()[target].sort_values(ascending=False).to_frame()\nplt.figure(figsize=(2,8))\nsns.heatmap(corr, cmap='Blues', cbar=False, annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessed data","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inputs and Target \nX_train = train.drop(target, axis=1)\ny_train = train[target]\n\nX_test = test.drop(target, axis=1)\ny_test = test[target]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Machine Learning\n## Decision tree classifier","metadata":{}},{"cell_type":"code","source":"# Base model\nclf = DecisionTreeClassifier(random_state=1)\nclf.fit(X_train, y_train)\ntrain_preds = clf.predict(X_train)\ntest_preds = clf.predict(X_test)\nprint(\"Train f1 Score :\", f1_score(y_train, train_preds))\nprint(\"Test f1 Score :\", f1_score(y_test, test_preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hyperparameter tuning\nparams = {\n    'max_depth': [4, 6, 8, 10, 12, 14, 16, 20],\n    'criterion': ['gini', 'entropy'],\n    'min_samples_split': [5, 10, 20, 30, 40, 50],\n    'max_features': [0.2, 0.4, 0.6, 0.8, 1],\n    'max_leaf_nodes': [8, 16, 32, 64, 128,256],\n    'class_weight': [{0: 1, 1: 9}, {0: 1, 1: 4},\n                     {0: 1, 1: 5}, {0: 1, 1: 6}, \n                     {0: 1, 1: 7}, {0: 1, 1: 8}]\n}\n\nclf = RandomizedSearchCV(DecisionTreeClassifier(random_state=1),\n                         params,\n                         scoring='f1',\n                         verbose=1,\n                         random_state=1,\n                         cv=5,\n                         n_iter=50)\n\nsearch = clf.fit(X_train, y_train)\n\nprint(\"\\nBest f1-score:\",search.best_score_)\nprint(\"\\nBest params:\",search.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cross validation\nclf = DecisionTreeClassifier(random_state = 1,\n                             **search.best_params_)\noofs = run_kfold(clf, X_train, y_train, N_SPLITS=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final Decision tree classifier\nclf = DecisionTreeClassifier(random_state = 1, \n                             **search.best_params_)\nclf.fit(X_train, y_train)\n\npreds_test = clf.predict_proba(X_test)[:, 1]\n    \ncm = confusion_matrix(y_test,preds_test.round(),normalize='true')\nplt.figure(figsize=(5,5))\nsns.heatmap(cm, annot=True, cmap='Blues', cbar=False,fmt='.2f')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"code","source":"# Base model\nclf = LogisticRegression(random_state=1, \n                         class_weight='balanced')\n\nclf.fit(X_train, y_train)\ntrain_preds = clf.predict(X_train)\ntest_preds = clf.predict(X_test)\nprint(\"Train f1 Score :\", f1_score(y_train, train_preds))\nprint(\"Test f1 Score :\", f1_score(y_test, test_preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hyperparameter tuning\nparams = {\n    'penalty': ['l1', 'l2','elasticnet'],\n    'C':[0.0001, 0.001, 0.1, 1, 10, 100,1000],\n    'fit_intercept':[True, False],\n    'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n}\n\nclf = RandomizedSearchCV(LogisticRegression(random_state=1,\n                                            class_weight='balanced'),\n                         params,\n                         scoring='f1',\n                         verbose=1,\n                         random_state=1,\n                         cv=5,\n                         n_iter=20)\n\nsearch = clf.fit(X_train, y_train)\n\nprint(\"\\nBest f1-score:\",search.best_score_)\nprint(\"\\nBest params:\",search.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cross validation\nclf = LogisticRegression(random_state = 1,\n                         class_weight='balanced', \n                         **search.best_params_)\noofs = run_kfold(clf, X_train, y_train, N_SPLITS=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final Logistic regression\n\nclf = LogisticRegression(random_state = 1,\n                         class_weight='balanced',\n                         **search.best_params_)\nclf.fit(X_train, y_train)\n\npreds_test = clf.predict_proba(X_test)[:, 1]\n\ncm = confusion_matrix(y_test, preds_test.round(), normalize='true')\nplt.figure(figsize=(5,5))\nsns.heatmap(cm, annot=True, cmap='Blues', cbar=False, fmt='.2f')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Weigths or Coefficents learnt by Logistic regression for each feature","metadata":{}},{"cell_type":"code","source":"imp = pd.DataFrame([X_train.columns, \n                    clf.coef_[0]]).T.sort_values(1, ascending=False).reset_index(drop=True)\nimp.columns=['feature', 'coeff']\nimp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save all the transformers\nOne hot encoder<br>\nStandard scaler<br>\nLogistic regression<br>\n<b> Logistic regression is giving the high true positive rate i.e., performing better at predicting the likelihood of Stroke. Which is what we want.!! ","metadata":{}},{"cell_type":"code","source":"with open(\"onehotencoder.pkl\", 'wb') as f:\n    pickle.dump(encoder, f)\n\nwith open(\"scaler.pkl\", 'wb') as f:\n    pickle.dump(scaler, f)\n\nwith open(\"model.pkl\", 'wb') as f:\n    pickle.dump(clf, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction on single data point","metadata":{}},{"cell_type":"code","source":"def predict(x):\n    X = pd.DataFrame(x, columns=features)\n    # converting numerical features as float dtype\n    X.loc[:, numerical_features] = X.loc[:, numerical_features].astype('float64')\n    # add new features\n    X[\"age_group\"] = X.age.apply(age_group)\n    X[\"bmi_group\"] = X.age.apply(bmi_group)\n    \n    # converting categorical features as category dtype\n    X.loc[:, categorical_features] = X.loc[:, categorical_features].astype('category')\n    # Categorical encoding\n    cols = encoder.get_feature_names(categorical_features)\n\n    X.loc[:, cols] = encoder.transform(X[categorical_features])\n\n    # Drop categorical features\n    X.drop(categorical_features, axis=1, inplace=True)\n\n    # Feature scaling\n    X.loc[:, numerical_features] = scaler.transform(X[numerical_features])\n    return clf.predict(X)[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random data point\nx = [['Male', 67.0, 0, 1, 'Yes', 'Private', 'Urban', 228.69, 36.6, 'formerly smoked']]\ny_true = 1\nprint(\"y_true :\", y_true)\ny_pred = predict(x)\nprint(\"y_pred :\",y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Next steps\n\n#### Building a web application for this problem!! Updating soon.üëç\n## Thanks for reading!! Please upvote if you like it.üòÄ","metadata":{}}]}