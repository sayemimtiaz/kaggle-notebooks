{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-23T06:58:51.96608Z","iopub.execute_input":"2021-05-23T06:58:51.966454Z","iopub.status.idle":"2021-05-23T06:58:51.994858Z","shell.execute_reply.started":"2021-05-23T06:58:51.966414Z","shell.execute_reply":"2021-05-23T06:58:51.993794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q efficientnet==1.1.0","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:58:52.926242Z","iopub.execute_input":"2021-05-23T06:58:52.926571Z","iopub.status.idle":"2021-05-23T06:59:02.464415Z","shell.execute_reply.started":"2021-05-23T06:58:52.926543Z","shell.execute_reply":"2021-05-23T06:59:02.463005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nuser_credential = user_secrets.get_gcloud_credential()\nuser_secrets.set_tensorflow_credential(user_credential)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:02.466522Z","iopub.execute_input":"2021-05-23T06:59:02.466966Z","iopub.status.idle":"2021-05-23T06:59:10.565608Z","shell.execute_reply.started":"2021-05-23T06:59:02.4669Z","shell.execute_reply":"2021-05-23T06:59:10.564462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_datasets import KaggleDatasets","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:10.567514Z","iopub.execute_input":"2021-05-23T06:59:10.567828Z","iopub.status.idle":"2021-05-23T06:59:10.574353Z","shell.execute_reply.started":"2021-05-23T06:59:10.567796Z","shell.execute_reply":"2021-05-23T06:59:10.573126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math, re, os, glob, time, random\nfrom collections import namedtuple\nfrom functools import partial\nfrom tensorflow.keras import regularizers\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\n# from classification_models.tfkeras import Classifiers\nimport efficientnet.tfkeras as efn\n\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nprint(\"Tensorflow version \" + tf.__version__)\n\nK = tf.keras.backend\nL = tf.keras.layers\nAUTO = tf.data.experimental.AUTOTUNE","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:10.576153Z","iopub.execute_input":"2021-05-23T06:59:10.576486Z","iopub.status.idle":"2021-05-23T06:59:12.041259Z","shell.execute_reply.started":"2021-05-23T06:59:10.576454Z","shell.execute_reply":"2021-05-23T06:59:12.040094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GCS_PATH_2 = KaggleDatasets().get_gcs_path('chinese-rec')  # 'sp-tfrecords' or 'sp-tfrecords-sz3","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:38.716184Z","iopub.execute_input":"2021-05-23T06:59:38.716548Z","iopub.status.idle":"2021-05-23T06:59:39.280264Z","shell.execute_reply.started":"2021-05-23T06:59:38.716517Z","shell.execute_reply":"2021-05-23T06:59:39.279078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GCS_PATH_2","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:40.332609Z","iopub.execute_input":"2021-05-23T06:59:40.332972Z","iopub.status.idle":"2021-05-23T06:59:40.341496Z","shell.execute_reply.started":"2021-05-23T06:59:40.332926Z","shell.execute_reply":"2021-05-23T06:59:40.3402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH_2 +'/Chinese_Rec/trainrecord/*.tfrec')\n","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:41.400727Z","iopub.execute_input":"2021-05-23T06:59:41.401118Z","iopub.status.idle":"2021-05-23T06:59:41.569779Z","shell.execute_reply.started":"2021-05-23T06:59:41.401088Z","shell.execute_reply":"2021-05-23T06:59:41.568739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAINING_FILENAMES","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:42.717296Z","iopub.execute_input":"2021-05-23T06:59:42.717658Z","iopub.status.idle":"2021-05-23T06:59:42.723594Z","shell.execute_reply.started":"2021-05-23T06:59:42.717622Z","shell.execute_reply":"2021-05-23T06:59:42.722717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ll ../input/tfrecord/trainrecord","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:43.708017Z","iopub.execute_input":"2021-05-23T06:59:43.708611Z","iopub.status.idle":"2021-05-23T06:59:44.450471Z","shell.execute_reply.started":"2021-05-23T06:59:43.708558Z","shell.execute_reply":"2021-05-23T06:59:44.44944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_strategy():\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    else:\n        strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\n    print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n    \n    return strategy\n\nstrategy = get_strategy()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:45.27783Z","iopub.execute_input":"2021-05-23T06:59:45.278242Z","iopub.status.idle":"2021-05-23T06:59:50.979057Z","shell.execute_reply.started":"2021-05-23T06:59:45.278205Z","shell.execute_reply":"2021-05-23T06:59:50.977881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 1029\n\nIM_SZ = 224  # 224, 320, 384, 448, 512\nIMAGE_SIZE = [IM_SZ, IM_SZ]\nWHICH_FOLD = 0\n\n# training setup\nEPOCHS = 8\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nLR = 6e-4\n\nrandom.seed(SEED)\nnp.random.seed(SEED)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:50.980561Z","iopub.execute_input":"2021-05-23T06:59:50.980968Z","iopub.status.idle":"2021-05-23T06:59:50.986641Z","shell.execute_reply.started":"2021-05-23T06:59:50.980902Z","shell.execute_reply":"2021-05-23T06:59:50.985472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_datasets import KaggleDatasets\n","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:50.98881Z","iopub.execute_input":"2021-05-23T06:59:50.989269Z","iopub.status.idle":"2021-05-23T06:59:51.006025Z","shell.execute_reply.started":"2021-05-23T06:59:50.989223Z","shell.execute_reply":"2021-05-23T06:59:51.004758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GCS_PATH_2 = KaggleDatasets().get_gcs_path('tfrecord')  # 'sp-tfrecords' or 'sp-tfrecords-sz3","metadata":{"execution":{"iopub.status.busy":"2021-05-23T06:59:52.919259Z","iopub.execute_input":"2021-05-23T06:59:52.919619Z","iopub.status.idle":"2021-05-23T06:59:58.73316Z","shell.execute_reply.started":"2021-05-23T06:59:52.919574Z","shell.execute_reply":"2021-05-23T06:59:58.730103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH_2 +'/trainrecord/*.tfrec')\n","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:06:12.586062Z","iopub.execute_input":"2021-05-20T09:06:12.586391Z","iopub.status.idle":"2021-05-20T09:06:12.666128Z","shell.execute_reply.started":"2021-05-20T09:06:12.586361Z","shell.execute_reply":"2021-05-20T09:06:12.664545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAINING_FILENAMES","metadata":{"execution":{"iopub.status.busy":"2021-05-20T09:06:16.633762Z","iopub.execute_input":"2021-05-20T09:06:16.634149Z","iopub.status.idle":"2021-05-20T09:06:16.642825Z","shell.execute_reply.started":"2021-05-20T09:06:16.634115Z","shell.execute_reply":"2021-05-20T09:06:16.641667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VALID_FILENAMES = [TRAINING_FILENAMES[WHICH_FOLD]]\nTRAIN_FILENAMES = TRAINING_FILENAMES[:WHICH_FOLD] + TRAINING_FILENAMES[WHICH_FOLD+1:]","metadata":{"execution":{"iopub.status.busy":"2021-05-23T07:00:14.572878Z","iopub.execute_input":"2021-05-23T07:00:14.573449Z","iopub.status.idle":"2021-05-23T07:00:14.577211Z","shell.execute_reply.started":"2021-05-23T07:00:14.573412Z","shell.execute_reply":"2021-05-23T07:00:14.576167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MEAN_C = [123.68, 116.779, 103.939]\n\n# for torch preproc mode\nMEAN_T = [0.485, 0.456, 0.406]\nSTD = [0.229, 0.224, 0.225]\n\ndef decode_image(image, mode='torch'):\n    assert mode in (None, 'tf', 'torch', 'caffe'), \"mode must be one of None, 'tf', 'torch', 'caffe'\"\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.cast(image, tf.float32)\n    image = tf.image.resize(image, IMAGE_SIZE)\n    if mode is None:\n        return image\n    if mode == 'tf':\n        image = image / 255.0  # convert image to floats in [0, 1] range\n    if mode == 'torch':\n        image = image / 255.0\n        image = (image - MEAN_T) / STD\n    if mode == 'caffe':\n        image = image - MEAN_C\n    return image\n\ndef deprocessing(image, mode='torch'):\n    assert mode in (None, 'tf', 'torch', 'caffe'), \"mode must be one of None, 'tf', 'torch', 'caffe'\"\n    def rescale(x):\n        low, high = x.min(), x.max()\n        x_rescaled = (x - low) / (high - low)\n        return x_rescaled\n    if mode is None or mode == 'tf':\n        return rescale(image)\n    if mode == 'torch':\n        return rescale(image * STD + MEAN_T)\n    if mode == 'caffe':\n        return rescale(image + MEAN_C)\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"label\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    label = tf.one_hot(tf.cast(example['label'], tf.int32), CLASSES)\n    return image, label\n\ndef read_unlabeled_tfrecord(example):\n    UNLABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string),  # tf.string means bytestring\n        \"filename\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n    }\n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    filename = example['filename']\n    return image, filename\n\ndef data_augment(image, label, p_hsv=0.6, p_affine=0.75, p_cutout=0.):\n    # data augmentation. Thanks to the dataset.prefetch(AUTO) statement in the next function (below),\n    # this happens essentially for free on TPU. Data pipeline code is executed on the \"CPU\" part\n    # of the TPU while the TPU itself is computing gradients.\n    r_hsv = tf.random.uniform([1], minval=0, maxval=1, dtype='float32', seed=SEED)\n    r_affine = tf.random.uniform([1], minval=0, maxval=1, dtype='float32', seed=SEED)\n    r_cutout = tf.random.uniform([1], minval=0, maxval=1, dtype='float32', seed=SEED)\n    \n    image = tf.image.random_flip_left_right(image)\n    if r_hsv < p_hsv:\n        image = tf.image.random_brightness(image, 0.15)\n        image = tf.image.random_saturation(image, 0.8, 2.3)\n        image = tf.image.random_contrast(image, 0.8, 1.3)\n    if r_affine < p_affine:\n        image = shift_scale_rotate(image, h_shift=0.1*IM_SZ, w_shift=0.1*IM_SZ)\n    if r_cutout < p_cutout:\n        image = cutout(image)\n    \n    return image, label\n\ndef get_training_dataset():\n    dataset = tf.data.TFRecordDataset(TRAIN_FILENAMES, num_parallel_reads=AUTO)\n    return (\n        dataset\n        .map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n         .cache()\n        .map(data_augment, num_parallel_calls=AUTO)\n        .repeat()\n        .shuffle(100000)\n        .batch(BATCH_SIZE)\n        .prefetch(AUTO)\n    )\n\ndef get_validation_dataset():\n    dataset = tf.data.TFRecordDataset(VALID_FILENAMES, num_parallel_reads=AUTO)\n    return (\n        dataset\n        .map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n        .batch(BATCH_SIZE)\n        .cache()\n        .prefetch(AUTO)\n    )\ndef get_test_dataset():\n    dataset = tf.data.TFRecordDataset(TEST_FILENAMES, num_parallel_reads=AUTO)\n    return (\n        dataset\n        .map(read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n        .batch(BATCH_SIZE)\n    )\n\ndef count_data_items(filenames):\n    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T07:00:25.288119Z","iopub.execute_input":"2021-05-23T07:00:25.288502Z","iopub.status.idle":"2021-05-23T07:00:25.313043Z","shell.execute_reply.started":"2021-05-23T07:00:25.288466Z","shell.execute_reply":"2021-05-23T07:00:25.312046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transform(image, matrix, border_mode=0):\n    BORDERS = ['constant', 'nearest']\n    border_mode = BORDERS[border_mode]\n    \n    DIM = IMAGE_SIZE[0]\n    C = DIM // 2\n    \n    # LIST DESTINATION PIXEL INDICES\n    y, x = tf.meshgrid(tf.range(DIM), tf.range(DIM))\n    x_c, y_c = tf.reshape(x - C, [-1]), tf.reshape(y - C, [-1])\n    x, y = tf.reshape(x, [-1]), tf.reshape(y, [-1])\n    if matrix.shape[0] == 2:\n        coord = tf.stack( [x_c,y_c] )   # (2, DIM*DIM)\n    else:\n        z = tf.ones([DIM*DIM], dtype='int32')\n        coord = tf.stack( [x_c,y_c,z] )   # (3, DIM*DIM)\n    \n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    coord_pr = tf.matmul(matrix, tf.cast(coord, dtype='float32'))\n    coord_pr = tf.cast(tf.round(coord_pr[:2,:] + C), dtype='int32')   # (2, DIM*DIM)\n    \n    # FIND ORIGIN PIXEL VALUES\n    if border_mode == 'constant':\n        x_pr, y_pr = coord_pr[0,:], coord_pr[1,:]\n        outside_ind = tf.logical_or( tf.logical_or(y_pr>DIM-1 , y_pr<0), tf.logical_or(x_pr>DIM-1 , x_pr<0))\n\n        x_pr = tf.boolean_mask(x_pr, tf.logical_not(outside_ind))   # (<DIM*DIM, )\n        y_pr = tf.boolean_mask(y_pr, tf.logical_not(outside_ind))   # (<DIM*DIM, )\n        x    = tf.boolean_mask(x, tf.logical_not(outside_ind))   # (<DIM*DIM, )\n        y    = tf.boolean_mask(y, tf.logical_not(outside_ind))   # (<DIM*DIM, )\n\n        coord_pr = tf.transpose( tf.stack( [x_pr, y_pr] ) )   # (<DIM*DIM, 2)\n        coord = tf.cast(tf.transpose( tf.stack( [x, y] ) ), 'int64')   # (<DIM*DIM, 2)\n\n        im_channels = tf.split(image, 3, axis=-1)\n        rot_channels = []\n        for im_val in im_channels:\n            rot_val = tf.squeeze(tf.gather_nd(im_val, coord_pr), axis=-1)   # (<DIM*DIM, )\n            rot = tf.SparseTensor(coord, rot_val, [DIM, DIM])\n            rot_channels.append(tf.sparse.to_dense(rot, default_value=0, validate_indices=False))\n\n        rot_image = tf.transpose(tf.stack(rot_channels), [1, 2, 0])   # (DIM, DIM, 3)\n    \n    if border_mode == 'nearest':\n        coord_pr = tf.clip_by_value(coord_pr, 0, DIM - 1)   # (2, DIM*DIM)\n        rot_image = tf.reshape( tf.gather_nd(image, tf.transpose(coord_pr)), [DIM, DIM, 3] )\n    \n    return rot_image\n\ndef rotate(image, angle):\n    angle = math.pi * angle / 180.\n    c1 = tf.math.cos(angle)\n    s1 = tf.math.sin(angle)\n    t_matrix = tf.reshape( tf.stack([c1,s1, -s1,c1]), [2,2] )\n    return transform(image, t_matrix)\n\ndef shear(image, angle):\n    angle = math.pi * angle / 180.\n    c1 = tf.math.cos(angle)\n    s1 = tf.math.sin(angle)\n    one = tf.constant([1], dtype='float32')\n    zero = tf.constant([0], dtype='float32')\n    t_matrix = tf.reshape( tf.stack([one[0],s1, zero[0],c1]), [2,2] )\n    return transform(image, t_matrix)\n\ndef zoom(image, height_zoom, width_zoom):\n    one = tf.constant([1], dtype='float32')\n    zero = tf.constant([0], dtype='float32')\n    t_matrix = tf.reshape( tf.stack([one[0]/height_zoom,zero[0], zero[0],one[0]/width_zoom]), [2,2] )\n    return transform(image, t_matrix)\n\ndef shift(image, height_shift, width_shift):\n    one = tf.constant([1], dtype='float32')\n    zero = tf.constant([0], dtype='float32')\n    t_matrix = tf.reshape( tf.stack([one[0],zero[0],height_shift, zero[0],one[0],width_shift, zero[0],zero[0],one[0]]), [3,3] )\n    return transform(image, t_matrix)\n\ndef shift_scale_rotate(image, rotation=15, shear=8, h_zoom=1, w_zoom=1, h_shift=20, w_shift=20):\n    rot = rotation * tf.random.normal([1], dtype='float32')\n    shr = shear * tf.random.normal([1], dtype='float32')\n    h_zoom = h_zoom + tf.random.normal([1], dtype='float32') / 10.\n    w_zoom = w_zoom + tf.random.normal([1], dtype='float32') / 10.\n    h_shift = h_shift * tf.random.normal([1], dtype='float32')\n    w_shift = w_shift * tf.random.normal([1], dtype='float32')\n    \n    angle = math.pi * rot / 180.\n    c1 = tf.math.cos(angle)\n    s1 = tf.math.sin(angle)\n    one = tf.constant([1], dtype='float32')\n    zero = tf.constant([0], dtype='float32')\n    rot_matrix = tf.reshape( tf.concat([c1,s1, -s1,c1], axis=0), [2,2] )\n    \n    angle = math.pi * shr / 180.\n    c1 = tf.math.cos(angle)\n    s1 = tf.math.sin(angle)\n    shr_matrix = tf.reshape( tf.concat([one,s1, zero,c1], axis=0), [2,2] )\n    \n    z_matrix = tf.reshape( tf.concat([one/h_zoom,zero, zero,one/w_zoom], axis=0), [2,2] )\n    \n    t_matrix = K.dot(K.dot(rot_matrix, shr_matrix), z_matrix)\n    t_image = transform(image, t_matrix)\n    \n    s_matrix = tf.reshape( tf.concat([one,zero,h_shift, zero,one,w_shift, zero,zero,one], axis=0), [3,3] )\n    return transform(t_image, s_matrix)\n\ndef cutout(image, min_height=0.4, min_width=0.4, max_height=0.6, max_width=0.6):\n    DIM = IMAGE_SIZE[0]\n    \n    cut_height = tf.cast(tf.round(tf.random.uniform([], minval=min_height*DIM, maxval=max_height*DIM)), 'int32')\n    cut_width = tf.cast(tf.round(tf.random.uniform([], minval=min_width*DIM, maxval=max_width*DIM)), 'int32')\n    x_min = tf.random.uniform([], minval=-cut_width//2, maxval=DIM-1-cut_width//2, dtype='int32')\n    x_max = x_min + cut_width\n    y_min = tf.random.uniform([], minval=-cut_height//2, maxval=DIM-1-cut_height//2, dtype='int32')\n    y_max = y_min + cut_height\n    if x_min < 0:\n        cut_width -= 0 - x_min\n        x_min = tf.clip_by_value(x_min, 0, x_max)\n    if y_min < 0:\n        cut_height -= 0 - y_min\n        y_min = tf.clip_by_value(y_min, 0, y_max)\n    if x_max > DIM:\n        cut_width -= x_max - DIM\n        x_max = tf.clip_by_value(x_max, x_min, DIM)\n    if y_max > DIM:\n        cut_height -= y_max - DIM\n        y_max = tf.clip_by_value(y_max, y_min, DIM)\n    \n    cut_area = tf.zeros([cut_height, cut_width, 3], dtype='float32')\n    pad_top = y_min\n    pad_bottom = DIM - y_max\n    pad_left = x_min\n    pad_right = DIM - x_max\n    cut_mask = tf.pad(cut_area, [[pad_top, pad_bottom], [pad_left, pad_right], [0,0]], constant_values=1)\n    \n    cut_image = tf.multiply(image, cut_mask)\n    return cut_image","metadata":{"execution":{"iopub.status.busy":"2021-05-23T07:00:26.330197Z","iopub.execute_input":"2021-05-23T07:00:26.330608Z","iopub.status.idle":"2021-05-23T07:00:26.373012Z","shell.execute_reply.started":"2021-05-23T07:00:26.330541Z","shell.execute_reply":"2021-05-23T07:00:26.371686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    SCHEDULES = set(['cosine', 'linear', 'linear_cosine'])\n    \n    def __init__(self, iterations, sched_profile='cosine', max_lr=3e-3,\n                 div_factor=40, warmup=0.3, final_div=None):\n\n        assert sched_profile in self.SCHEDULES\n        self.sched_profile = sched_profile\n\n        self.max_lr = max_lr\n        self.init_lr = max_lr/div_factor\n\n        self.final_div = final_div\n        if self.final_div is None: self.final_div = div_factor*1e4\n        self.final_lr = self.max_lr/self.final_div\n\n        self.total_iteration = iterations\n        self.up_iteration = int(self.total_iteration * warmup)\n        self.down_iteration = self.total_iteration - self.up_iteration\n\n    def _annealing_cos(self, start, end, pct):\n        cos_out = tf.math.cos(tf.constant(np.pi) * pct) + 1\n        return end + (start-end)/2 * cos_out\n\n    def _annealing_linear(self, start, end, pct):\n        return start + pct * (end-start)\n    \n    def _annealing_function(self):\n        if self.sched_profile == 'cosine':\n            return self._annealing_cos\n        if self.sched_profile == 'linear':\n            return self._annealing_linear\n    \n    @tf.function\n    def __call__(self, step):\n        if self.sched_profile != 'linear_cosine':\n            anneal = self._annealing_function()\n\n        if step <= self.up_iteration:\n            if self.sched_profile == 'linear_cosine':\n                anneal = self._annealing_linear\n            pct = step / self.up_iteration\n            curr_lr = anneal(self.init_lr, self.max_lr, pct)\n        else:\n            if self.sched_profile == 'linear_cosine':\n                anneal = self._annealing_cos\n            pct = (step-self.up_iteration) / self.down_iteration\n            curr_lr = anneal(self.max_lr, self.final_lr, pct)\n\n        return curr_lr\n    \n    def plot_sched(self):\n        fig_sz = (6,4)\n        subplts = (111,)\n        \n        init_lr = self.init_lr\n        max_lr = self.max_lr\n        final_lr = self.final_lr\n        if self.sched_profile == 'linear_cosine':\n            anneal_up = self._annealing_linear\n            anneal_down = self._annealing_cos\n        else:\n            anneal_up = anneal_down = self._annealing_function()\n        lrs = [anneal_up(init_lr, max_lr, it/self.up_iteration) for it in range(self.up_iteration)] + \\\n              [anneal_down(max_lr, final_lr, it/self.down_iteration) for it in range(self.down_iteration)]\n        \n        plt.subplots(figsize=fig_sz)\n        plt.tight_layout()\n        ax = plt.subplot(subplts[0])\n        ax.plot(range(self.total_iteration), lrs)\n        ax.set_xlabel('Iteration')\n        ax.set_ylabel('Learning rate')\n        \n        plt.show()\n\nclass ConcatPooling2D(L.Layer):\n    \"Layer that concats `GlobalAveragePooling2D` and `GlobalMaxPooling2D`,\"\n    def __init__(self):\n        \"Output will be 2*output_size or 2 if output_size is None\"\n        super().__init__()\n        self.ap = L.GlobalAveragePooling2D()\n        self.mp = L.GlobalMaxPooling2D()\n    def call(self, x): return tf.concat([self.mp(x), self.ap(x)], 1)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T07:00:27.21928Z","iopub.execute_input":"2021-05-23T07:00:27.219682Z","iopub.status.idle":"2021-05-23T07:00:27.239609Z","shell.execute_reply.started":"2021-05-23T07:00:27.219646Z","shell.execute_reply":"2021-05-23T07:00:27.23827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_TRAIN_IMAGES = count_data_items(TRAIN_FILENAMES)\nNUM_VALIDATION_IMAGES = count_data_items(VALID_FILENAMES)\nSTEPS_PER_EPOCH = math.ceil(NUM_TRAIN_IMAGES / BATCH_SIZE)\n# VALIDATION_STEPS = math.ceil(NUM_VALIDATION_IMAGES / BATCH_SIZE)\nprint('Dataset: {} training images, {} validation images'.format(NUM_TRAIN_IMAGES, NUM_VALIDATION_IMAGES))","metadata":{"execution":{"iopub.status.busy":"2021-05-23T07:00:28.389615Z","iopub.execute_input":"2021-05-23T07:00:28.389968Z","iopub.status.idle":"2021-05-23T07:00:28.398718Z","shell.execute_reply.started":"2021-05-23T07:00:28.389923Z","shell.execute_reply":"2021-05-23T07:00:28.397554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = fold_start_time = time.time()\n\nwhich_folds = [0] + np.random.choice(np.arange(1,10), 2).tolist()\noof_labels = []\noof_preds = []","metadata":{"execution":{"iopub.status.busy":"2021-05-23T07:00:36.153709Z","iopub.execute_input":"2021-05-23T07:00:36.154128Z","iopub.status.idle":"2021-05-23T07:00:36.160155Z","shell.execute_reply.started":"2021-05-23T07:00:36.154093Z","shell.execute_reply":"2021-05-23T07:00:36.158975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Tensorflow version \" + tf.__version__)\nAUTO = tf.data.experimental.AUTOTUNE # used in tf.data.Dataset API\n\nSEED = 1029\nDATA_PATH = '../input/shopee-product-detection-open/'\nCLASSES=800\nIM_SZ = 224  # 320, 384, 448, 512\nIMAGE_SIZE = [IM_SZ, IM_SZ]\nN_SPLITS = 10","metadata":{"execution":{"iopub.status.busy":"2021-05-23T07:00:36.624644Z","iopub.execute_input":"2021-05-23T07:00:36.625026Z","iopub.status.idle":"2021-05-23T07:00:36.630562Z","shell.execute_reply.started":"2021-05-23T07:00:36.62499Z","shell.execute_reply":"2021-05-23T07:00:36.629762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = fold_start_time = time.time()\n\nwhich_folds = [0] + np.random.choice(np.arange(1,10), 2).tolist()\noof_labels = []\noof_preds = []\nEPOCHS = 20\n\nfor i, fd in enumerate(which_folds):\n    print(f\"Fold {i+1}\")\n    \n    VALID_FILENAMES = [TRAINING_FILENAMES[fd]]\n    TRAIN_FILENAMES = TRAINING_FILENAMES[:fd] + TRAINING_FILENAMES[fd+1:]\n    train_ds = get_training_dataset()\n    valid_ds = get_validation_dataset()\n    \n    strategy = get_strategy()\n    with strategy.scope():\n        pretrained_model = efn.EfficientNetB5(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n        pretrained_model.trainable = True  # False = transfer learning, True = fine-tuning\n\n        model = tf.keras.Sequential([\n            pretrained_model,\n            L.GlobalAveragePooling2D(),\n            L.Dense(CLASSES, activation='softmax')\n        ])\n\n        model.compile(\n            optimizer=tf.keras.optimizers.Adam(LRSchedule(STEPS_PER_EPOCH * EPOCHS, max_lr=LR)),\n            loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n            metrics=[tf.keras.metrics.CategoricalAccuracy(), tfa.metrics.F1Score(CLASSES\n                                                                                 , average='macro', threshold=None)]\n        )\n\n        ckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n            filepath=f'./best_model_3{i}.h5',\n            save_weights_only=True,\n            monitor='val_categorical_accuracy',\n            mode='max',\n            save_best_only=True\n        )\n    \n    model.fit(\n        train_ds,\n        epochs=EPOCHS,\n        steps_per_epoch=STEPS_PER_EPOCH,\n             validation_data=valid_ds,\n        callbacks=[ckpt_callback]\n    )\n    model.load_weights(f'best_model_3{i}.h5')\n    \n    images_ds = valid_ds.map(lambda image, label: image)\n    labels_ds = valid_ds.map(lambda image, label: label).unbatch()\n    valid_labels = next(iter(labels_ds.batch(NUM_VALIDATION_IMAGES))).numpy()\n    valid_labels = np.argmax(valid_labels, -1)\n    valid_preds = model.predict(images_ds)\n    valid_preds = np.argmax(valid_preds, axis=-1)\n    oof_labels.append(valid_labels)\n    oof_preds.append(valid_preds)\n    \n    training_time = time.time() - fold_start_time\n    print(\"FOLD TRAINING TIME: {:0.1f}s\".format(training_time))\n    fold_start_time = time.time()\n    print()\n    \nkeras_fit_training_time = time.time() - start_time\nprint(\"KERAS FIT TRAINING TIME: {:0.1f}s\".format(keras_fit_training_time))","metadata":{"execution":{"iopub.status.busy":"2021-05-21T06:07:41.33531Z","iopub.execute_input":"2021-05-21T06:07:41.335645Z","iopub.status.idle":"2021-05-21T06:07:48.900176Z","shell.execute_reply.started":"2021-05-21T06:07:41.335617Z","shell.execute_reply":"2021-05-21T06:07:48.898571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"which_folds = [0] + np.random.choice(np.arange(1,10), 2).tolist()\n","metadata":{"execution":{"iopub.status.busy":"2021-05-21T06:07:53.573347Z","iopub.execute_input":"2021-05-21T06:07:53.573695Z","iopub.status.idle":"2021-05-21T06:07:53.579052Z","shell.execute_reply.started":"2021-05-21T06:07:53.573649Z","shell.execute_reply":"2021-05-21T06:07:53.578111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"which_folds","metadata":{"execution":{"iopub.status.busy":"2021-05-21T06:07:54.524117Z","iopub.execute_input":"2021-05-21T06:07:54.524607Z","iopub.status.idle":"2021-05-21T06:07:54.529855Z","shell.execute_reply.started":"2021-05-21T06:07:54.524576Z","shell.execute_reply":"2021-05-21T06:07:54.528815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def categorical_focal_loss(num_classes, gamma=2., alpha=.25, smooth_alpha=0.05):\n    \"\"\"\n    Softmax version of focal loss.\n           m\n      FL = âˆ‘  -alpha * (1 - p_o,c)^gamma * y_o,c * log(p_o,c)\n          c=1\n      where m = number of classes, c = class and o = observation\n    Parameters:\n      alpha -- the same as weighing factor in balanced cross entropy\n      gamma -- focusing parameter for modulating factor (1-p)\n    Default value:\n      gamma -- 2.0 as mentioned in the paper\n      alpha -- 0.25 as mentioned in the paper\n    References:\n        Official paper: https://arxiv.org/pdf/1708.02002.pdf\n        https://www.tensorflow.org/api_docs/python/tf/keras/backend/categorical_crossentropy\n    Usage:\n     model.compile(loss=[categorical_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n    \"\"\"\n    def categorical_focal_loss_fixed(y_true, y_pred):\n        \"\"\"\n        :param y_true: A tensor of the same shape as `y_pred`\n        :param y_pred: A tensor resulting from a softmax\n        :return: Output tensor.\n        \"\"\"\n        if smooth_alpha > 0:\n            y_true = y_true * (1 - smooth_alpha) + smooth_alpha / num_classes\n\n        # Scale predictions so that the class probas of each sample sum to 1\n        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n\n        # Clip the prediction value to prevent NaN's and Inf's\n        epsilon = K.epsilon()\n        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n\n        # Calculate Cross Entropy\n        cross_entropy = -y_true * K.log(y_pred)\n\n        # Calculate Focal Loss\n        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n\n        # Sum the losses in mini_batch\n        return K.sum(loss, axis=1)\n\n    return categorical_focal_loss_fixed  ","metadata":{"execution":{"iopub.status.busy":"2021-05-23T02:12:01.462729Z","iopub.execute_input":"2021-05-23T02:12:01.463079Z","iopub.status.idle":"2021-05-23T02:12:01.47021Z","shell.execute_reply.started":"2021-05-23T02:12:01.463048Z","shell.execute_reply":"2021-05-23T02:12:01.469462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Generalized_mean_pooling2D(tf.keras.layers.Layer):\n    def __init__(self, p=3, epsilon=1e-6, name='', **kwargs):\n      super(Generalized_mean_pooling2D, self).__init__(name, **kwargs)\n      self.init_p = p\n      self.epsilon = epsilon\n    \n    def build(self, input_shape):\n      if isinstance(input_shape, list) or len(input_shape) != 4:\n        raise ValueError('`GeM` pooling layer only allow 1 input with 4 dimensions(b, h, w, c)')\n      self.build_shape = input_shape\n      self.p = self.add_weight(\n              name='p',\n              shape=[1,],\n              initializer=tf.keras.initializers.Constant(value=self.init_p),\n              regularizer=None,\n              trainable=True,\n              dtype=tf.float32\n              )\n      self.built=True\n\n    def call(self, inputs):\n      input_shape = inputs.get_shape()\n      if isinstance(inputs, list) or len(input_shape) != 4:\n        raise ValueError('`GeM` pooling layer only allow 1 input with 4 dimensions(b, h, w, c)')\n      return (tf.reduce_mean(tf.abs(inputs**self.p), axis=[1,2], keepdims=False) + self.epsilon)**(1.0/self.p)\n\n\nclass CosFace(L.Layer):\n    def __init__(self, n_classes=10, s=30.0, m=0.35, regularizer=None, **kwargs):\n        super(CosFace, self).__init__(**kwargs)\n        self.n_classes = n_classes\n        self.s = s\n        self.m = m\n        self.regularizer = regularizers.get(regularizer)\n        print('aaaaaa',s,m,n_classes)\n\n    def build(self, input_shape):\n        super(CosFace, self).build(input_shape[0])\n        self.W = self.add_weight(shape=(input_shape[0][-1], self.n_classes),\n                                initializer='glorot_uniform',\n                                trainable=True,\n                                regularizer=self.regularizer)\n\n    def call(self, inputs):\n        x, y = inputs\n        c = K.shape(x)[-1]\n        \n        # normalize weights\n        W = tf.nn.l2_normalize(self.W, axis=0)\n        # dot product\n        logits = x @ W\n        # add margin\n        target_logits = logits - self.m\n        logits = logits * (1 - y) + target_logits * y\n        # feature re-scale\n        logits *= self.s\n        out = tf.nn.softmax(logits)\n\n        return out\n\n    def compute_output_shape(self, input_shape):\n        return (None, self.n_classes)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T07:00:55.505309Z","iopub.execute_input":"2021-05-23T07:00:55.50584Z","iopub.status.idle":"2021-05-23T07:00:55.52255Z","shell.execute_reply.started":"2021-05-23T07:00:55.505805Z","shell.execute_reply":"2021-05-23T07:00:55.521633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input = L.Input(shape=(28, 28, 1))\nlabel = L.Input(shape=(10,))\n\nx = L.Conv2D(32, kernel_size=(3, 3), activation='relu')(input)\nx = L.MaxPooling2D(pool_size=(2, 2))(x)\nx = L.Conv2D(64, kernel_size=(3, 3), activation='relu')(x)\nx = L.MaxPooling2D(pool_size=(2, 2))(x)\n\nx = L.BatchNormalization()(x)\nx = L.Dropout(0.5)(x)\nx = L.Flatten()(x)\nx = L.Dense(512, kernel_initializer='he_normal')(x)\nx = L.BatchNormalization()(x)\noutput = CosFace(10)([x, label])\n\nmodel =  tf.keras.Model([input, label], output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_input = L.Input(shape=[*IMAGE_SIZE, 3], name='imgs', dtype='float32')\nr_label = L.Input(shape=(800,))\nprint(x_input)\n#pretrained_model = efn.EfficientNetB5(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n#pretrained_model.trainable = True  \nbackbone='efficientnet-b5'\nweights='imagenet'\nmodel_fn = getattr(efn, f'EfficientNetB{backbone[-1]}')\nx = model_fn(input_shape=[*IMAGE_SIZE, 3], weights=weights, include_top=False)(x_input)\nx = Generalized_mean_pooling2D()(x)\n\n    # feature vector\nweight_decay = 1e-4\nx = L.BatchNormalization()(x)\nx = L.Dropout(0.2)(x)\nx = L.Flatten()(x)\n\n    # root\nx1 = L.Dense(512, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(x)\nx1 = L.BatchNormalization()(x1)\nx1 = tf.nn.l2_normalize(x1, axis=1)\nroot = CosFace(800, regularizer=regularizers.l2(weight_decay), name='root')([x1, r_label])\nx1 = L.Dense(800, use_bias=False)(x1)\nroot2 = L.Lambda(lambda x: K.softmax(x), name='root2')(x1)\n\n    # model\nmodel = tf.keras.Model(\n        inputs = [x_input,r_label],\n        outputs = [root, root2]\n    )\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef prepare_metric_learning(image, label, mode='train'):\n    if mode == 'train':\n        return (image, label), label\n    else:\n        return (image, tf.zeros_like(label)), label","metadata":{"execution":{"iopub.status.busy":"2021-05-23T07:01:04.190786Z","iopub.execute_input":"2021-05-23T07:01:04.191303Z","iopub.status.idle":"2021-05-23T07:01:04.196278Z","shell.execute_reply.started":"2021-05-23T07:01:04.191269Z","shell.execute_reply":"2021-05-23T07:01:04.195283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"which_folds = [0] + np.random.choice(np.arange(1,10), 2).tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"which_folds","metadata":{"execution":{"iopub.status.busy":"2021-05-21T06:08:09.937145Z","iopub.execute_input":"2021-05-21T06:08:09.937495Z","iopub.status.idle":"2021-05-21T06:08:09.943402Z","shell.execute_reply.started":"2021-05-21T06:08:09.937466Z","shell.execute_reply":"2021-05-21T06:08:09.942266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = fold_start_time = time.time()\n\noof_labels = []\noof_preds = []\nEPOCHS = 100\nfor i, fd in enumerate(which_folds):\n    print(f\"Fold {i+1}\")\n    \n    VALID_FILENAMES = [TRAINING_FILENAMES[fd]]\n    TRAIN_FILENAMES = TRAINING_FILENAMES[:fd] + TRAINING_FILENAMES[fd+1:]\n    train_ds = get_training_dataset()\n    train_ds = train_ds.map(lambda a, b: prepare_metric_learning(a, b, 'train'))\n\n    valid_ds = get_validation_dataset().map(lambda a, b: prepare_metric_learning(a, b, 'valid'))\n    weight_decay = 1e-4\n \n    strategy = get_strategy()\n    with strategy.scope():\n        x_input = L.Input(shape=[*IMAGE_SIZE, 3], name='imgs', dtype='float32')\n        r_label = L.Input(shape=(800,))\n        print(x_input)\n        #pretrained_model = efn.EfficientNetB5(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n        #pretrained_model.trainable = True  \n        backbone='efficientnet-b5'\n        weights='imagenet'\n        model_fn = getattr(efn, f'EfficientNetB{backbone[-1]}')\n        x = model_fn(input_shape=[*IMAGE_SIZE, 3], weights=weights, include_top=False)(x_input)\n        x = Generalized_mean_pooling2D()(x)\n\n            # feature vector\n        weight_decay = 1e-4\n        x = L.BatchNormalization()(x)\n        x = L.Dropout(0.2)(x)\n        x = L.Flatten()(x)\n\n            # root\n        x1 = L.Dense(512, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(x)\n        x1 = L.BatchNormalization()(x1)\n        x1 = tf.nn.l2_normalize(x1, axis=1)\n        root = CosFace(800, regularizer=regularizers.l2(weight_decay), name='root')([x1, r_label])\n        x1 = L.Dense(800, use_bias=False)(x1)\n        root2 = L.Lambda(lambda x: K.softmax(x), name='root2')(x1)\n\n            # model\n        model = tf.keras.Model(\n                inputs = [x_input,r_label],\n                outputs = [root, root2]\n            )\n\n\n        model.compile(\n            optimizer=tf.keras.optimizers.Adam(LRSchedule(STEPS_PER_EPOCH * EPOCHS, max_lr=LR)),\n            loss=categorical_focal_loss(800),\n            metrics=[tf.keras.metrics.CategoricalAccuracy(), tfa.metrics.F1Score(CLASSES\n                                                                                 , average='macro', threshold=None)]\n        )\n\n        ckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n            filepath=f'./best_modelmetric_{i}.h5',\n            save_weights_only=True,\n            monitor='val_loss',\n            mode='min',\n            save_best_only=True\n        )\n    print('i am model', train_ds)\n    model.fit(\n        train_ds,\n        epochs=EPOCHS,\n        steps_per_epoch=STEPS_PER_EPOCH,\n             validation_data=valid_ds,\n        callbacks=[ckpt_callback]\n    )\n    model.load_weights(f'best_modelmetric_{i}.h5')\n    #images_ds = valid_ds.map(lambda image, label: image)\n\n    images_ds = valid_ds\n    labels_ds = valid_ds.map(lambda image, label: label).unbatch()\n\n    valid_labels = next(iter(labels_ds.batch(NUM_VALIDATION_IMAGES))).numpy()\n    valid_labels = np.argmax(valid_labels, -1)\n    valid_preds = model.predict(images_ds)\n    valid_preds = np.argmax(valid_preds, axis=-1)\n    oof_labels.append(valid_labels)\n    oof_preds.append(valid_preds)\n    \n    training_time = time.time() - fold_start_time\n    print(\"FOLD TRAINING TIME: {:0.1f}s\".format(training_time))\n    fold_start_time = time.time()\n    print()\n    \nkeras_fit_training_time = time.time() - start_time\nprint(\"KERAS FIT TRAINING TIME: {:0.1f}s\".format(keras_fit_training_time))","metadata":{"execution":{"iopub.status.busy":"2021-05-21T06:09:38.017425Z","iopub.execute_input":"2021-05-21T06:09:38.017802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input = L.Input(shape=(28, 28, 1))\nlabel = L.Input(shape=(10,))\n\nx = L.Conv2D(32, kernel_size=(3, 3), activation='relu')(input)\nx = L.MaxPooling2D(pool_size=(2, 2))(x)\nx = L.Conv2D(64, kernel_size=(3, 3), activation='relu')(x)\nx = L.MaxPooling2D(pool_size=(2, 2))(x)\n\nx = L.BatchNormalization()(x)\nx = L.Dropout(0.5)(x)\nx = L.Flatten()(x)\nx = L.Dense(512, kernel_initializer='he_normal')(x)\nx = L.BatchNormalization()(x)\noutput = CosFace(10)([x, label])\n\nmodel =  tf.keras.Model([input, label], output)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ArcFace(L.Layer):\n    def __init__(self, n_classes=10, s=30.0, m=0.50, regularizer=None, **kwargs):\n        super(ArcFace, self).__init__(**kwargs)\n        self.n_classes = n_classes\n        self.s = s\n        self.m = m\n        self.regularizer = regularizers.get(regularizer)\n\n    def build(self, input_shape):\n        super(ArcFace, self).build(input_shape[0])\n        self.W = self.add_weight(name='W',\n                                shape=(input_shape[0][-1], self.n_classes),\n                                initializer='glorot_uniform',\n                                trainable=True,\n                                regularizer=self.regularizer)\n\n    def call(self, inputs):\n        x, y = inputs\n        c = K.shape(x)[-1]\n        # normalize feature\n        x = tf.nn.l2_normalize(x, axis=1)\n        # normalize weights\n        W = tf.nn.l2_normalize(self.W, axis=0)\n        # dot product\n        logits = x @ W\n        # add margin\n        # clip logits to prevent zero division when backward\n        theta = tf.acos(K.clip(logits, -1.0 + K.epsilon(), 1.0 - K.epsilon()))\n        target_logits = tf.cos(theta + self.m)\n        # sin = tf.sqrt(1 - logits**2)\n        # cos_m = tf.cos(logits)\n        # sin_m = tf.sin(logits)\n        # target_logits = logits * cos_m - sin * sin_m\n        #\n        logits = logits * (1 - y) + target_logits * y\n        # feature re-scale\n        logits *= self.s\n        out = tf.nn.softmax(logits)\n\n        return out\n\n    def compute_output_shape(self, input_shape):\n        return (None, self.n_classes)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T07:01:16.277893Z","iopub.execute_input":"2021-05-23T07:01:16.278325Z","iopub.status.idle":"2021-05-23T07:01:16.290518Z","shell.execute_reply.started":"2021-05-23T07:01:16.278289Z","shell.execute_reply":"2021-05-23T07:01:16.28935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"which_folds = [0] + np.random.choice(np.arange(1,10), 3).tolist()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T07:01:59.853916Z","iopub.execute_input":"2021-05-23T07:01:59.85431Z","iopub.status.idle":"2021-05-23T07:01:59.859235Z","shell.execute_reply.started":"2021-05-23T07:01:59.854277Z","shell.execute_reply":"2021-05-23T07:01:59.858076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"which_folds","metadata":{"execution":{"iopub.status.busy":"2021-05-23T11:36:27.742334Z","iopub.execute_input":"2021-05-23T11:36:27.742732Z","iopub.status.idle":"2021-05-23T11:36:27.815261Z","shell.execute_reply.started":"2021-05-23T11:36:27.742649Z","shell.execute_reply":"2021-05-23T11:36:27.813905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"start_time = fold_start_time = time.time()\n\noof_labels = []\noof_preds = []\nEPOCHS = 80\nfor i, fd in enumerate(which_folds):\n    print(f\"Fold {i+1}\")\n    \n    VALID_FILENAMES = [TRAINING_FILENAMES[fd]]\n    TRAIN_FILENAMES = TRAINING_FILENAMES[:fd] + TRAINING_FILENAMES[fd+1:]\n    train_ds = get_training_dataset()\n    train_ds = train_ds.map(lambda a, b: prepare_metric_learning(a, b, 'train'))\n\n    valid_ds = get_validation_dataset().map(lambda a, b: prepare_metric_learning(a, b, 'valid'))\n    weight_decay = 1e-4\n \n    strategy = get_strategy()\n    with strategy.scope():\n        x_input = L.Input(shape=[*IMAGE_SIZE, 3], name='imgs', dtype='float32')\n        r_label = L.Input(shape=(800,))\n        print(x_input)\n        #pretrained_model = efn.EfficientNetB5(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n        #pretrained_model.trainable = True  \n        backbone='efficientnet-b5'\n        weights='imagenet'\n        model_fn = getattr(efn, f'EfficientNetB{backbone[-1]}')\n        x = model_fn(input_shape=[*IMAGE_SIZE, 3], weights=weights, include_top=False)(x_input)\n        x = Generalized_mean_pooling2D()(x)\n\n            # feature vector\n        weight_decay = 1e-4\n        x = L.BatchNormalization()(x)\n        x = L.Dropout(0.2)(x)\n        x = L.Flatten()(x)\n\n            # root\n        x = L.Dense(512, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(x)\n        x = L.BatchNormalization()(x)\n        x = tf.nn.l2_normalize(x, axis=1)\n        output = ArcFace(n_classes=800)([x, r_label])\n\n\n\n            # model\n        model = tf.keras.Model(\n                inputs = [x_input,r_label],\n                outputs = output\n            )\n\n\n        model.compile(\n            optimizer=tf.keras.optimizers.Adam(LRSchedule(STEPS_PER_EPOCH * EPOCHS, max_lr=LR)),\n            loss='categorical_crossentropy',\n            metrics=[tf.keras.metrics.CategoricalAccuracy(), tfa.metrics.F1Score(CLASSES\n                                                                                 , average='macro', threshold=None)]\n        )\n\n        ckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n            filepath=f'./best_modelmetric2_{i}.h5',\n            save_weights_only=True,\n            monitor='val_loss',\n            mode='min',\n            save_best_only=True\n        )\n    print('i am model', train_ds)\n    model.fit(\n        train_ds,\n        epochs=EPOCHS,\n        steps_per_epoch=STEPS_PER_EPOCH,\n             validation_data=valid_ds,\n        callbacks=[ckpt_callback]\n    )\n    model.load_weights(f'best_modelmetric2_{i}.h5')\n    #images_ds = valid_ds.map(lambda image, label: image)\n\n    images_ds = valid_ds\n    labels_ds = valid_ds.map(lambda image, label: label).unbatch()\n\n    valid_labels = next(iter(labels_ds.batch(NUM_VALIDATION_IMAGES))).numpy()\n    valid_labels = np.argmax(valid_labels, -1)\n    valid_preds = model.predict(images_ds)\n    valid_preds = np.argmax(valid_preds, axis=-1)\n    oof_labels.append(valid_labels)\n    oof_preds.append(valid_preds)\n    \n    training_time = time.time() - fold_start_time\n    print(\"FOLD TRAINING TIME: {:0.1f}s\".format(training_time))\n    fold_start_time = time.time()\n    print()\n    \nkeras_fit_training_time = time.time() - start_time\nprint(\"KERAS FIT TRAINING TIME: {:0.1f}s\".format(keras_fit_training_time))","metadata":{"execution":{"iopub.status.busy":"2021-05-23T07:02:31.435687Z","iopub.execute_input":"2021-05-23T07:02:31.436093Z"}}},{"cell_type":"code","source":"start_time = fold_start_time = time.time()\n\noof_labels = [] oof_preds = [] EPOCHS = 80 for i, fd in enumerate(which_folds): print(f\"Fold {i+1}\")\n\nVALID_FILENAMES = [TRAINING_FILENAMES[fd]]\nTRAIN_FILENAMES = TRAINING_FILENAMES[:fd] + TRAINING_FILENAMES[fd+1:]\ntrain_ds = get_training_dataset()\ntrain_ds = train_ds.map(lambda a, b: prepare_metric_learning(a, b, 'train'))\n\nvalid_ds = get_validation_dataset().map(lambda a, b: prepare_metric_learning(a, b, 'valid'))\nweight_decay = 1e-4\n\nstrategy = get_strategy()\nwith strategy.scope():\n    x_input = L.Input(shape=[*IMAGE_SIZE, 3], name='imgs', dtype='float32')\n    r_label = L.Input(shape=(800,))\n    print(x_input)\n    #pretrained_model = efn.EfficientNetB5(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n    #pretrained_model.trainable = True  \n    backbone='efficientnet-b5'\n    weights='imagenet'\n    model_fn = getattr(efn, f'EfficientNetB{backbone[-1]}')\n    x = model_fn(input_shape=[*IMAGE_SIZE, 3], weights=weights, include_top=False)(x_input)\n    x = Generalized_mean_pooling2D()(x)\n\n        # feature vector\n    weight_decay = 1e-4\n    x = L.BatchNormalization()(x)\n    x = L.Dropout(0.2)(x)\n    x = L.Flatten()(x)\n\n        # root\n    x = L.Dense(512, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(x)\n    x = L.BatchNormalization()(x)\n    x = tf.nn.l2_normalize(x, axis=1)\n    output = ArcFace(n_classes=800)([x, r_label])\n\n\n\n        # model\n    model = tf.keras.Model(\n            inputs = [x_input,r_label],\n            outputs = output\n        )\n\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(LRSchedule(STEPS_PER_EPOCH * EPOCHS, max_lr=LR)),\n        loss='categorical_crossentropy',\n        metrics=[tf.keras.metrics.CategoricalAccuracy(), tfa.metrics.F1Score(CLASSES\n                                                                             , average='macro', threshold=None)]\n    )\n\n    ckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n        filepath=f'./best_modelmetric2_{i}.h5',\n        save_weights_only=True,\n        monitor='val_loss',\n        mode='min',\n        save_best_only=True\n    )\nprint('i am model', train_ds)\nmodel.fit(\n    train_ds,\n    epochs=EPOCHS,\n    steps_per_epoch=STEPS_PER_EPOCH,\n         validation_data=valid_ds,\n    callbacks=[ckpt_callback]\n)\nmodel.load_weights(f'best_modelmetric2_{i}.h5')\n#images_ds = valid_ds.map(lambda image, label: image)\n\nimages_ds = valid_ds\nlabels_ds = valid_ds.map(lambda image, label: label).unbatch()\n\nvalid_labels = next(iter(labels_ds.batch(NUM_VALIDATION_IMAGES))).numpy()\nvalid_labels = np.argmax(valid_labels, -1)\nvalid_preds = model.predict(images_ds)\nvalid_preds = np.argmax(valid_preds, axis=-1)\noof_labels.append(valid_labels)\noof_preds.append(valid_preds)\n\ntraining_time = time.time() - fold_start_time\nprint(\"FOLD TRAINING TIME: {:0.1f}s\".format(training_time))\nfold_start_time = time.time()\nprint()\nkeras_fit_training_time = time.time() - start_time print(\"KERAS FIT TRAINING TIME: {:0.1f}s\".format(keras_fit_training_time))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"start_time = fold_start_time = time.time()\n\noof_labels = []\noof_preds = []\nEPOCHS = 80\nfor i, fd in enumerate(which_folds):\n    print(f\"Fold {i+1}\")\n    \n    VALID_FILENAMES = [TRAINING_FILENAMES[fd]]\n    TRAIN_FILENAMES = TRAINING_FILENAMES[:fd] + TRAINING_FILENAMES[fd+1:]\n    train_ds = get_training_dataset()\n    train_ds = train_ds.map(lambda a, b: prepare_metric_learning(a, b, 'train'))\n\n    valid_ds = get_validation_dataset().map(lambda a, b: prepare_metric_learning(a, b, 'valid'))\n    weight_decay = 1e-4\n \n    strategy = get_strategy()\n    with strategy.scope():\n        x_input = L.Input(shape=[*IMAGE_SIZE, 3], name='imgs', dtype='float32')\n        r_label = L.Input(shape=(800,))\n        print(x_input)\n        #pretrained_model = efn.EfficientNetB5(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n        #pretrained_model.trainable = True  \n        backbone='efficientnet-b5'\n        weights='imagenet'\n        model_fn = getattr(efn, f'EfficientNetB{backbone[-1]}')\n        x = model_fn(input_shape=[*IMAGE_SIZE, 3], weights=weights, include_top=False)(x_input)\n        x = Generalized_mean_pooling2D()(x)\n\n            # feature vector\n        weight_decay = 1e-4\n        x = L.BatchNormalization()(x)\n        x = L.Dropout(0.2)(x)\n        x = L.Flatten()(x)\n\n            # root\n        x = L.Dense(512, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(x)\n        x = L.BatchNormalization()(x)\n        x = tf.nn.l2_normalize(x, axis=1)\n        output = ArcFace(n_classes=800)([x, r_label])\n\n\n\n            # model\n        model = tf.keras.Model(\n                inputs = [x_input,r_label],\n                outputs = output\n            )\n\n\n        model.compile(\n            optimizer=tf.keras.optimizers.Adam(LRSchedule(STEPS_PER_EPOCH * EPOCHS, max_lr=LR)),\n            loss='categorical_crossentropy',\n            metrics=[tf.keras.metrics.CategoricalAccuracy(), tfa.metrics.F1Score(CLASSES\n                                                                                 , average='macro', threshold=None)]\n        )\n\n        ckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n            filepath=f'./best_modelmetric2_{i}.h5',\n            save_weights_only=True,\n            monitor='val_loss',\n            mode='min',\n            save_best_only=True\n        )\n    print('i am model', train_ds)\n    model.fit(\n        train_ds,\n        epochs=EPOCHS,\n        steps_per_epoch=STEPS_PER_EPOCH,\n             validation_data=valid_ds,\n        callbacks=[ckpt_callback]\n    )\n    model.load_weights(f'best_modelmetric2_{i}.h5')\n    #images_ds = valid_ds.map(lambda image, label: image)\n\n    images_ds = valid_ds\n    labels_ds = valid_ds.map(lambda image, label: label).unbatch()\n\n    valid_labels = next(iter(labels_ds.batch(NUM_VALIDATION_IMAGES))).numpy()\n    valid_labels = np.argmax(valid_labels, -1)\n    valid_preds = model.predict(images_ds)\n    valid_preds = np.argmax(valid_preds, axis=-1)\n    oof_labels.append(valid_labels)\n    oof_preds.append(valid_preds)\n    \n    training_time = time.time() - fold_start_time\n    print(\"FOLD TRAINING TIME: {:0.1f}s\".format(training_time))\n    fold_start_time = time.time()\n    print()\n    \nkeras_fit_training_time = time.time() - start_time\nprint(\"KERAS FIT TRAINING TIME: {:0.1f}s\".format(keras_fit_training_time))","metadata":{"execution":{"iopub.status.busy":"2021-05-23T07:02:31.435687Z","iopub.execute_input":"2021-05-23T07:02:31.436093Z"}}}]}