{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-01T02:55:56.933772Z","iopub.execute_input":"2021-09-01T02:55:56.934359Z","iopub.status.idle":"2021-09-01T02:55:56.954881Z","shell.execute_reply.started":"2021-09-01T02:55:56.934269Z","shell.execute_reply":"2021-09-01T02:55:56.953849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this project, the objective is to predict the occurance of rain in Australia. We are given 10 years worth of historical data collected from various part of Australia. We will be using this data to create a Machine Learning model using Supervised Learning for Classification. \n\nFor this project, first we will try to explore the dataset to gain potential insight and information to help us with the modelling, then we will process the given data, and finally use it to create our model.  \n\nTo start our project, we will load relavant library, and import the dataset. ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:55:56.956389Z","iopub.execute_input":"2021-09-01T02:55:56.9567Z","iopub.status.idle":"2021-09-01T02:55:58.534891Z","shell.execute_reply.started":"2021-09-01T02:55:56.956664Z","shell.execute_reply":"2021-09-01T02:55:58.53408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:55:58.53672Z","iopub.execute_input":"2021-09-01T02:55:58.537305Z","iopub.status.idle":"2021-09-01T02:55:58.545254Z","shell.execute_reply.started":"2021-09-01T02:55:58.537263Z","shell.execute_reply":"2021-09-01T02:55:58.544112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = '/kaggle/input/weather-dataset-rattle-package/weatherAUS.csv'\n\ndf = pd.read_csv(data)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:55:58.546936Z","iopub.execute_input":"2021-09-01T02:55:58.547449Z","iopub.status.idle":"2021-09-01T02:55:59.241366Z","shell.execute_reply.started":"2021-09-01T02:55:58.547409Z","shell.execute_reply":"2021-09-01T02:55:59.240508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"First we need to understand what kind of data that we have. We will start by doing some basic analysis of the dataset. ","metadata":{}},{"cell_type":"markdown","source":"## Data Types","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:55:59.242781Z","iopub.execute_input":"2021-09-01T02:55:59.243182Z","iopub.status.idle":"2021-09-01T02:55:59.249828Z","shell.execute_reply.started":"2021-09-01T02:55:59.243147Z","shell.execute_reply":"2021-09-01T02:55:59.248578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:55:59.251169Z","iopub.execute_input":"2021-09-01T02:55:59.251482Z","iopub.status.idle":"2021-09-01T02:55:59.387361Z","shell.execute_reply.started":"2021-09-01T02:55:59.25145Z","shell.execute_reply":"2021-09-01T02:55:59.386201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It appears that we have around **145.460** rows of data. We have a total of **23 column**, **16 numerical column** and **7 object column**. \n\nNote that, the **Date** column is still an object type, which mean that for later on we need to change it into date format. \n\nFor our Target Feature, **RainTomorrow**, the data type is string. ","metadata":{}},{"cell_type":"code","source":"#Group the Categorical And Numerical column\ncategorical = ['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow']\nnumerical = ['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm']","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:55:59.388511Z","iopub.execute_input":"2021-09-01T02:55:59.3888Z","iopub.status.idle":"2021-09-01T02:55:59.394537Z","shell.execute_reply.started":"2021-09-01T02:55:59.388772Z","shell.execute_reply":"2021-09-01T02:55:59.393188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, to create a more comprehensive analysis for our **Column, or Features,** we create a group for the Numerical Features and Categorical Features. ","metadata":{}},{"cell_type":"markdown","source":"## Categorical Data","metadata":{}},{"cell_type":"markdown","source":"For our Categorical Features, we have Location, WindGustDir, WinDir9am, WinDir3pm, RainToday, and RainTomorrow. We will begin by describing the features. ","metadata":{}},{"cell_type":"code","source":"df[categorical].describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:55:59.397447Z","iopub.execute_input":"2021-09-01T02:55:59.397774Z","iopub.status.idle":"2021-09-01T02:55:59.752999Z","shell.execute_reply.started":"2021-09-01T02:55:59.39774Z","shell.execute_reply":"2021-09-01T02:55:59.752003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the description, we can take 2 important notes. First, from the count of data in each features, we can see that **there is a missing data in all of our Categorical Features, except for Location**. Also, Location have 49 unique value, WindGustDir,WindDir9am, and WindDir3pm have 16 unique value, and RainToday and RainTomorrow have 2 unique value. ","metadata":{}},{"cell_type":"code","source":"print(df['Location'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:55:59.755001Z","iopub.execute_input":"2021-09-01T02:55:59.75537Z","iopub.status.idle":"2021-09-01T02:55:59.795224Z","shell.execute_reply.started":"2021-09-01T02:55:59.755335Z","shell.execute_reply":"2021-09-01T02:55:59.794042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see all of the **49 unique values** that the **Location Feature** have, and how much row it contains","metadata":{}},{"cell_type":"code","source":"print(df['WindGustDir'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:55:59.796807Z","iopub.execute_input":"2021-09-01T02:55:59.797209Z","iopub.status.idle":"2021-09-01T02:55:59.844606Z","shell.execute_reply.started":"2021-09-01T02:55:59.797171Z","shell.execute_reply":"2021-09-01T02:55:59.843568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df['WindDir9am'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:55:59.845928Z","iopub.execute_input":"2021-09-01T02:55:59.846356Z","iopub.status.idle":"2021-09-01T02:55:59.888156Z","shell.execute_reply.started":"2021-09-01T02:55:59.846312Z","shell.execute_reply":"2021-09-01T02:55:59.887009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df['WindDir3pm'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:55:59.889829Z","iopub.execute_input":"2021-09-01T02:55:59.890276Z","iopub.status.idle":"2021-09-01T02:55:59.935524Z","shell.execute_reply.started":"2021-09-01T02:55:59.890242Z","shell.execute_reply":"2021-09-01T02:55:59.934731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next is the **WinGustDir, WindDir9am, and WindDir3pm**, all which have **16 unique value**, as seen above, and all **3 of them have 16 identical unique values**, since all of them represent the wind direction. ","metadata":{}},{"cell_type":"code","source":"print(df['RainToday'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:55:59.93691Z","iopub.execute_input":"2021-09-01T02:55:59.937278Z","iopub.status.idle":"2021-09-01T02:55:59.98665Z","shell.execute_reply.started":"2021-09-01T02:55:59.937243Z","shell.execute_reply":"2021-09-01T02:55:59.985811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df['RainTomorrow'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:55:59.987783Z","iopub.execute_input":"2021-09-01T02:55:59.988127Z","iopub.status.idle":"2021-09-01T02:56:00.036639Z","shell.execute_reply.started":"2021-09-01T02:55:59.988094Z","shell.execute_reply":"2021-09-01T02:56:00.034769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lastly, it is the **RainToday and RainTomorrow** Features, which both have the same **2 unique values**, which are Yes and No. This would make both of these collumn a **Boolean type** features. \n\nBoolean data is a data that contains two different value, usually contains value of Yes or No, True or Fales, Correct or Wrong, and some more. ","metadata":{}},{"cell_type":"markdown","source":"## Numerical Data","metadata":{}},{"cell_type":"markdown","source":"Since we have done the basic analysis of Categorical Features, now we can continute to explore our Numerical Features.","metadata":{}},{"cell_type":"code","source":"df[numerical].describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:56:00.037908Z","iopub.execute_input":"2021-09-01T02:56:00.038222Z","iopub.status.idle":"2021-09-01T02:56:00.19735Z","shell.execute_reply.started":"2021-09-01T02:56:00.03819Z","shell.execute_reply":"2021-09-01T02:56:00.196266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the description of our Numerical Features, we can see the **Count**(how many row of data), **Mean** (average value of the data), **Standard Deviation or STD**(variation or dispersion of a set of values), **Min**(smallest value), **25%** (Q1), **50%** (Q2), **75%** (Q3), and **Max**(largest value). \n\n**Evaporation, Rainfall, WindGustSpeed, and WindSpeed9am** caught my eyes since the it's Q3 and Q4, or 75% and Max of it's data have a big value differences, and can be concluded that both of the features **a lot of outliers**. ","metadata":{}},{"cell_type":"markdown","source":"## Data Visualization","metadata":{}},{"cell_type":"markdown","source":"From our initial analysis, we can conclude that we gain a number of insight from our dataset. But with that insight, comes a **few question that we want to answer before proceeding with our analysis**. Some of the questions are : \n\n1. Since the count of rows for Location features are different for each, did it effects the distribution of rain in the city?\n2. Since we have identify 4 features with possible outliers value, is it possible that the rest of the features also have outliers?\n3. How do we check the distribution of data in each of the features?\n4. How about Null Values? How many does each features have?\n5. Can we check the correlation of Independent Features with Target Features?\n\nFor this questions, we will try to answer them using visualization. ","metadata":{}},{"cell_type":"markdown","source":"### Rain Distribution in Each City","metadata":{}},{"cell_type":"code","source":"rain_chance = df.loc[(df.RainToday == 'Yes')]\n\nfig, ax= plt.subplots(figsize=(10,15))\n\nsns.histplot(y='Location' ,hue='RainTomorrow', data=rain_chance, ax=ax, bins=49)\nax.set_title('Number of Rain for Each City', size=17, pad=17)\nplt.tight_layout()\nax.set_xlabel('Count', size=13, labelpad=11)\nax.set_ylabel('Location', size=13)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:56:00.198866Z","iopub.execute_input":"2021-09-01T02:56:00.199424Z","iopub.status.idle":"2021-09-01T02:56:01.600502Z","shell.execute_reply.started":"2021-09-01T02:56:00.199369Z","shell.execute_reply":"2021-09-01T02:56:01.59928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We create this visualization to answer about the different distribution of rain in each city. From the visualization, we can see more clearly about the different amount of data each city have. **Not every cities have the same amount of data**. \n\nAlso, if we try to aggregate the distribution of rain from each city, **36 Cities have higher count of rain**. This would mean that we are dealing with **Imbalance Dataset**. \n\nImbalance Dataset is when the one of the value of Target Feature have higher count than the other. This would need to be addressed later if we want to create an ideal model. ","metadata":{}},{"cell_type":"markdown","source":"### Outliers and Distribution","metadata":{}},{"cell_type":"code","source":"#Visualizing the features to determine the outlier and distribution of data\nnull_cols = df[numerical]\n\nfig, axis = plt.subplots(16, 2, figsize = (20, 30))\n\nfor x, null in enumerate(null_cols):\n  sns.boxplot(y = null, data = df, ax = axis[x][0], color = 'gold')\n#Box plost will be use to check the outliers\n  sns.histplot(data=df, x=null, color=\"skyblue\", ax=axis[x][1])\n#Histogram will be used to see the distribution of the data\n\nfig.suptitle('Rain in Australia Numerical Feature Visualization', fontsize = 16, y=1)\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:56:01.602033Z","iopub.execute_input":"2021-09-01T02:56:01.602642Z","iopub.status.idle":"2021-09-01T02:56:40.83193Z","shell.execute_reply.started":"2021-09-01T02:56:01.602597Z","shell.execute_reply":"2021-09-01T02:56:40.830862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this visualization, we can see the Outliers of the Data in the left column using a Box Plot, and the Distribution of Data in the right using a Histogram. After carefull study of the outliers and distribution, **the features can be divided into 3 category**. \n\nFirst is the features which have a **normal distribution**, and **no outliers**. The features are **MinTemp, MaxTemp, Temp9am, Temp3pm, Cloud9am, Cloud3pm, and Humidity**. \n\nSecond is the features with **normal distribution** but have **some outliers**. The features are **Humidity9am, Pressure9am, and Pressure3pm**. \n\nThird is the features with **skewed distribution, and have a lot of outliers**. The features are **WindSpeed9am, WindSpeed3pm, WindGustSpeed, Sunshine, Rainfall, Evaporation**.","metadata":{}},{"cell_type":"markdown","source":"### Null Values","metadata":{}},{"cell_type":"code","source":"total = df.isnull().sum().sort_values(ascending=False)\npercent_1 = df.isnull().sum()/df.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\nmissing_data","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:56:40.83336Z","iopub.execute_input":"2021-09-01T02:56:40.833954Z","iopub.status.idle":"2021-09-01T02:56:41.166076Z","shell.execute_reply.started":"2021-09-01T02:56:40.833908Z","shell.execute_reply":"2021-09-01T02:56:41.164906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we are going to inspect the **Null Values of each features**. This is important because later we need to input these missing values. \n\nAt the top we have **Sunshine, Evaporation, Cloud3pm, and Cloud9am with more tha 35% missing value**. since these features have a high number of missing value, it will be best to drop the features, to avoid any bias or noise when we finally create our model. \n\nNext we have **some features with missing value around 0-10%**, now these missing value can be input, and we will determine the method of filling them based on their distribution and outliers. \n\nAlso we need to check about our **Target Feature, RainTomorrow, which have 3267 missing value**, or around 2.2%, which we **need to remove**. This is because as a target feature, it can't have any missing value, and to rather than filling them it would be best to simply remove them. ","metadata":{}},{"cell_type":"markdown","source":"### Correlation of Features","metadata":{}},{"cell_type":"markdown","source":"Lastly we want to check the **correlation of each features**. Now since we have a lot of features, we will **focused on the correlation of our Independent Features with Target Features**. ","metadata":{}},{"cell_type":"code","source":"data = df.apply(lambda x: x.factorize()[0]).corr(method='pearson')\nplt.figure(figsize=(15,11))\nsns.heatmap(data, linecolor='white',linewidths=1, cmap=\"YlGnBu\", annot=True)\nplt.title('Rain in Australia Feature Correlation', size=30)\nfigure = plt.gcf()\nfigure.set_size_inches(20, 20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:56:41.16945Z","iopub.execute_input":"2021-09-01T02:56:41.169799Z","iopub.status.idle":"2021-09-01T02:56:44.304431Z","shell.execute_reply.started":"2021-09-01T02:56:41.169761Z","shell.execute_reply":"2021-09-01T02:56:44.303691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there are **3 features that is highly correlated** to our Target Features. it is **RainFall, Humidity, and RainToday**. Out of curiosity, we will also try to visualize Temperature featuers.\n\nOut of the categorical features, it appears that **Wind Direction Features** have the **lowest correlation** with our target features. ","metadata":{}},{"cell_type":"code","source":"fig, ax= plt.subplots(2, 2, figsize=(20,16))\n\nsns.histplot(x='Rainfall', hue='RainTomorrow', data=df, ax=ax[0][0], bins=10)\nax[0][0].set_title('Rainfall', size=17, pad=17)\nax[0][0].set_xlabel('Rainfall (mm)', size=13, labelpad=11)\nax[0][0].set_ylabel('Days', size=13)\n\nsns.scatterplot(x='Humidity9am', y='Humidity3pm', hue='RainTomorrow',data=df, ax=ax[0][1])\nax[0][1].set_title('Humidity', size=17, pad=17)\nax[0][1].set_xlabel('Humidity9am', size=13, labelpad=11)\nax[0][1].set_ylabel('Humidity3on', size=13)\n\nsns.scatterplot(x='Temp9am', y='Temp3pm', hue='RainTomorrow', data=df, ax=ax[1][0])\nax[1][0].set_title('Temperature', size=17, pad=17)\nax[1][0].set_xlabel('Temp9am', size=13, labelpad=11)\nax[1][0].set_ylabel('Temp3pm', size=13)\n\nsns.scatterplot(x='Pressure9am', y='Pressure3pm', hue='RainTomorrow', data=df, ax=ax[1][1])\nax[1][1].set_title('Pressure', size=17, pad=17)\nax[1][1].set_xlabel('Pressure9am', size=13, labelpad=11)\nax[1][1].set_ylabel('Pressure3pm', size=13)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:56:44.305482Z","iopub.execute_input":"2021-09-01T02:56:44.305861Z","iopub.status.idle":"2021-09-01T02:57:13.261927Z","shell.execute_reply.started":"2021-09-01T02:56:44.30583Z","shell.execute_reply":"2021-09-01T02:57:13.260874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the visualization, we can see clearly that there is a **characteristic of value from each feature for our Target Feature**. This is important because without any clear line to determine our Target Feature value, we will need to process the data more carefully. \n\nThis visualization also further confirm the **outlier distribution of Rainfall feature**. ","metadata":{}},{"cell_type":"markdown","source":"# Data Processing","metadata":{}},{"cell_type":"markdown","source":"## Mising Value","metadata":{}},{"cell_type":"markdown","source":"Now that we have a more clearer picture of our dataset, we can start to process the data for Machine Learning Model. We will **begin with processing the Null Values**. ","metadata":{}},{"cell_type":"code","source":"#Remove the Feature that have >30% Null Value\nnullfeature=['Sunshine', 'Evaporation', 'Cloud3pm', 'Cloud9am']\ndf.drop(nullfeature, axis=1, inplace=True)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:57:13.263212Z","iopub.execute_input":"2021-09-01T02:57:13.263509Z","iopub.status.idle":"2021-09-01T02:57:13.308325Z","shell.execute_reply.started":"2021-09-01T02:57:13.26348Z","shell.execute_reply":"2021-09-01T02:57:13.307291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For starter, we **eleminate the feature that have more than 35% of missing value**, since synthetizing the values could create bias and noises. ","metadata":{}},{"cell_type":"code","source":"nullfeature=['WindGustDir', 'WindDir9am', 'WindDir3pm']\ndf.drop(nullfeature, axis=1, inplace=True)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:57:13.309609Z","iopub.execute_input":"2021-09-01T02:57:13.309891Z","iopub.status.idle":"2021-09-01T02:57:13.348338Z","shell.execute_reply.started":"2021-09-01T02:57:13.309863Z","shell.execute_reply":"2021-09-01T02:57:13.347321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we also **remove WindGustDir, WindDir9am, and WindDir3pm**, since they are similar feature with identical value, and have **low correlation** with our target feature. \n\nAlthough we have eleminate features with high Null Value, we still have to deal with the missing value in our remaining features. To input the missing value, we will use 3 different methods, Mean, Median, & Mode. Mean is an average value of the column, Median is the middle value of the feature, and Mode is the most occuring value in the column. \n\nTo determine which method to use, we will check using the distribution and the outlier of the column\n\nFor M**inTemp, MaxTemp, Temp9am, Temp3pm, & Humidity, we will input using Mean**, since the distribution is fairly normal, and there are no outliers. \n\nFor **Humidity9am, Pressure9am, & Pressure3pm, we will use Median**, since although the distribution is fairly normal, there are several outliers. \n\nLastly, for **WindSpeed9am, WindSpeed3pm, WindGustSpeed, & Rainfall, we will use Mode**, since the distribution is skewed, and there are many outliers. ","metadata":{}},{"cell_type":"code","source":"df['MaxTemp'].fillna(df['MaxTemp'].mean(), inplace=True)\ndf['MinTemp'].fillna(df['MinTemp'].mean(), inplace=True)\ndf['Temp9am'].fillna(df['Temp9am'].mean(), inplace=True)\ndf['Temp3pm'].fillna(df['Temp3pm'].mean(), inplace=True)\ndf['Humidity3pm'].fillna(df['Humidity3pm'].mean(), inplace=True)\n\ndf['Humidity9am'].fillna(df['Humidity9am'].median(), inplace=True)\ndf['Pressure9am'].fillna(df['Pressure9am'].median(), inplace=True)\ndf['Pressure3pm'].fillna(df['Pressure3pm'].median(), inplace=True)\n\ndf['WindSpeed9am'].fillna(int(df['WindSpeed9am'].mode()), inplace=True)\ndf['Rainfall'].fillna(int(df['Rainfall'].mode()), inplace=True)\ndf['WindSpeed3pm'].fillna(int(df['WindSpeed3pm'].mode()), inplace=True)\ndf['WindGustSpeed'].fillna(int(df['WindGustSpeed'].mode()), inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:57:13.351971Z","iopub.execute_input":"2021-09-01T02:57:13.352305Z","iopub.status.idle":"2021-09-01T02:57:13.386519Z","shell.execute_reply.started":"2021-09-01T02:57:13.352274Z","shell.execute_reply":"2021-09-01T02:57:13.385341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we that we have filled the missing value with the method that we determine before, we will continue to **fill the categorical value**. \n\nSince Location have no missing value, and we have delete 3 categorical features, WindGustDir, WinDir9am, & WinDir3pm, **we only have to deal with RainToday and RainTomorrow** missing value.\n\nFor **RainTomorrow, we will delete the missing value**, since as the target feature we won't synthesize the value, which could possible create bias in our analysis. \n\nTo input **RainToday missing value, we will use Before Fill**, which is using the value before the missing value. This is because our data is sorted by date, therefore we will make assumption that today and tomorrow will have similar data, and will be more accurate than using Mean, Median or Mode. ","metadata":{}},{"cell_type":"code","source":"df['RainToday'].fillna(method='bfill', inplace=True)\ndf.dropna(subset = [\"RainTomorrow\"], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:57:13.388119Z","iopub.execute_input":"2021-09-01T02:57:13.3884Z","iopub.status.idle":"2021-09-01T02:57:13.449436Z","shell.execute_reply.started":"2021-09-01T02:57:13.388372Z","shell.execute_reply":"2021-09-01T02:57:13.448641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:57:13.450464Z","iopub.execute_input":"2021-09-01T02:57:13.450868Z","iopub.status.idle":"2021-09-01T02:57:13.519677Z","shell.execute_reply.started":"2021-09-01T02:57:13.450837Z","shell.execute_reply":"2021-09-01T02:57:13.518609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have deal with the missing value of our dataset, we can continue to process the features for Machine Learning Model. ","metadata":{}},{"cell_type":"markdown","source":"## Data Scaling and Binning","metadata":{}},{"cell_type":"markdown","source":"### Categorical Features Processing","metadata":{}},{"cell_type":"markdown","source":"Now we will begin to process our features. **Location is an important feature**, but Machine Learning can't input categorical data. That is why we need to convert Location feature into something else. \n\nIn this case, we will **create an identifier of our Location features**. Identifier is creating a numerical values to act as an identifier for our Location. We will be creating this Identifier using data that we have about the Location. \n\nFor instance, **we will be aggregating the Temperature, Humidity, Pressure, and WindSpeed of each city**. This will create 4 new feature, and for each city we will have different values. This will allow our Model to understand the Location feature. \n\nOnce we create the identifier, we will enter the new features into a new dataset called **df_new**.","metadata":{}},{"cell_type":"code","source":"data = (df.groupby(['Location', 'WindSpeed9am'], as_index=False).mean()\n            .groupby('Location')['WindSpeed3pm'].mean())\ntotal_wind = pd.DataFrame(data).reset_index()\n\ndata2 = (df.groupby(['Location', 'Humidity9am'], as_index=False).mean()\n            .groupby('Location')['Humidity3pm'].mean())\ntotal_humidity = pd.DataFrame(data2).reset_index()\n\ndata3 = (df.groupby(['Location', 'Pressure9am'], as_index=False).mean()\n            .groupby('Location')['Pressure3pm'].mean())\ntotal_pressure = pd.DataFrame(data3).reset_index()\n\ndata4 = (df.groupby(['Location', 'Temp9am'], as_index=False).mean()\n            .groupby('Location')['Temp3pm'].mean())\ntotal_pressure = pd.DataFrame(data4).reset_index()\n\ntotal1 = pd.merge(data, data2, on='Location')\ntotal2 = pd.merge(data3, data4, on='Location')\ncity_total = pd.merge(total1, total2, on='Location')\ncity_total.columns = ['WindSpeed', 'Humidity', 'Pressure', 'Temp']","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:57:13.521138Z","iopub.execute_input":"2021-09-01T02:57:13.52146Z","iopub.status.idle":"2021-09-01T02:57:13.696924Z","shell.execute_reply.started":"2021-09-01T02:57:13.521429Z","shell.execute_reply":"2021-09-01T02:57:13.696011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_new = pd.merge(df, city_total, on='Location')\ndf_new.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:57:13.698359Z","iopub.execute_input":"2021-09-01T02:57:13.698789Z","iopub.status.idle":"2021-09-01T02:57:13.775887Z","shell.execute_reply.started":"2021-09-01T02:57:13.698747Z","shell.execute_reply":"2021-09-01T02:57:13.774968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have 4 new feature that will act as an identifier for our Location. Since we no longer need the Location feature, **we will delete the feature**, and continue to process the RainToday and RainTomorrow features. ","metadata":{}},{"cell_type":"code","source":"df_new.drop(['Location'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:57:13.777083Z","iopub.execute_input":"2021-09-01T02:57:13.777376Z","iopub.status.idle":"2021-09-01T02:57:13.815322Z","shell.execute_reply.started":"2021-09-01T02:57:13.777346Z","shell.execute_reply":"2021-09-01T02:57:13.814212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_new['RainToday'] = df_new.RainToday.astype('category')\ndf_new['RainTomorrow'] = df_new.RainTomorrow.astype('category')\n\ndf_new.RainToday = pd.Categorical(df_new.RainToday)\ndf_new.RainTomorrow = pd.Categorical(df_new.RainTomorrow)\n\ndf_new['RainToday'] = df_new.RainToday.cat.codes\ndf_new['RainTomorrow'] = df_new.RainTomorrow.cat.codes","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:57:13.816809Z","iopub.execute_input":"2021-09-01T02:57:13.81714Z","iopub.status.idle":"2021-09-01T02:57:13.853711Z","shell.execute_reply.started":"2021-09-01T02:57:13.817109Z","shell.execute_reply":"2021-09-01T02:57:13.852746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_new.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:57:13.855103Z","iopub.execute_input":"2021-09-01T02:57:13.855396Z","iopub.status.idle":"2021-09-01T02:57:13.884694Z","shell.execute_reply.started":"2021-09-01T02:57:13.855367Z","shell.execute_reply":"2021-09-01T02:57:13.883615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since **RainToday and RainTomorrow is a Boolean Data**, it's quite simple to process them since they can be easily be **converted into 1 and 0**, 1 for Yes and 0 for No. Now that all our Categorical Features have been process, we can continue to process our Numerical Features. ","metadata":{}},{"cell_type":"markdown","source":"### Numerical Features Processing","metadata":{}},{"cell_type":"markdown","source":"Now we will begin to **scale our Numerical Features**. Scaling is important to create the same magnitudes, units and range between features. We will process them using StandarScaler, MinMaxScaler and Bining.\n\n**StandardScaler** is converting the value of our data so that the feature will have Mean equal to 0, and Standard Deviation equal to 1. \n\n**MinMaxScaler** is converting the value into the range of 0 and 1. \n\n**Binning** is changing the value of our feature into an ordinal range, which we will have to define manually. \n\nFirst, lets describe our Dataset to give a broader context. ","metadata":{}},{"cell_type":"code","source":"df_new.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:57:13.886214Z","iopub.execute_input":"2021-09-01T02:57:13.886616Z","iopub.status.idle":"2021-09-01T02:57:14.025193Z","shell.execute_reply.started":"2021-09-01T02:57:13.886573Z","shell.execute_reply":"2021-09-01T02:57:14.023981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First we will process the **feature with high number of outliers**. That would be WindGustSpeed, WindSpeed9am, WindSpeed3pm, and Rainfall, which we will scaled using **MinMaxScaler**. \n\nFor our identifier for the Location feature, which are Temp, WindSpeed, Humidity, and Pressure, we will use **StandardScaler**.","metadata":{}},{"cell_type":"code","source":"df_new['Rainfall'] = MinMaxScaler().fit_transform(df_new['Rainfall'].values.reshape(len(df_new), 1))\ndf_new['WindGustSpeed'] = MinMaxScaler().fit_transform(df_new['WindGustSpeed'].values.reshape(len(df_new), 1))\ndf_new['WindSpeed9am'] = MinMaxScaler().fit_transform(df_new['WindSpeed9am'].values.reshape(len(df_new), 1))\ndf_new['WindSpeed3pm'] = MinMaxScaler().fit_transform(df_new['WindSpeed3pm'].values.reshape(len(df_new), 1))\ndf_new['Temp'] = StandardScaler().fit_transform(df_new['Temp'].values.reshape(len(df_new), 1))\ndf_new['WindSpeed'] = StandardScaler().fit_transform(df_new['WindSpeed'].values.reshape(len(df_new), 1))\ndf_new['Humidity'] = StandardScaler().fit_transform(df_new['Humidity'].values.reshape(len(df_new), 1))\ndf_new['Pressure'] = StandardScaler().fit_transform(df_new['Pressure'].values.reshape(len(df_new), 1))","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:57:14.026359Z","iopub.execute_input":"2021-09-01T02:57:14.026701Z","iopub.status.idle":"2021-09-01T02:57:14.052777Z","shell.execute_reply.started":"2021-09-01T02:57:14.02667Z","shell.execute_reply":"2021-09-01T02:57:14.051843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_new.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:57:14.054129Z","iopub.execute_input":"2021-09-01T02:57:14.054447Z","iopub.status.idle":"2021-09-01T02:57:14.195942Z","shell.execute_reply.started":"2021-09-01T02:57:14.054418Z","shell.execute_reply":"2021-09-01T02:57:14.194801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have scale some of our Numerical Features, it's time to scale the rest. **For the rest of the numerical features, we will be using Binning Method**. \n\nBinning Method is changing the values of each features into an ordinal range, by defining the range of values for features. This method is great to simplified our data, and create an easier data for our model to learn, but it's dangerous because we risk the data to be over-generalized. \n\nTo avoid that, we will define the range for the binning using the description of each feature that we have run before.","metadata":{}},{"cell_type":"code","source":"data = [df_new]\n\nfor dataset in data:\n  dataset['MinTemp'] = dataset['MinTemp'].astype(int)\n  dataset.loc[ dataset['MinTemp'] <= 0, 'MinTemp'] = 0\n  dataset.loc[(dataset['MinTemp'] > 0) & (dataset['MinTemp'] <= 5), 'MinTemp'] = 1\n  dataset.loc[(dataset['MinTemp'] > 5) & (dataset['MinTemp'] <= 10), 'MinTemp'] = 2\n  dataset.loc[(dataset['MinTemp'] > 10) & (dataset['MinTemp'] <= 15), 'MinTemp'] = 3\n  dataset.loc[(dataset['MinTemp'] > 15) & (dataset['MinTemp'] <= 20), 'MinTemp'] = 4\n  dataset.loc[(dataset['MinTemp'] > 20) & (dataset['MinTemp'] <= 25), 'MinTemp'] = 5\n  dataset.loc[(dataset['MinTemp'] > 25) & (dataset['MinTemp'] <= 30), 'MinTemp'] = 6\n  dataset.loc[ dataset['MinTemp'] > 30, 'MinTemp'] = 7\n\nfor dataset in data:\n  dataset['MaxTemp'] = dataset['MaxTemp'].astype(int)\n  dataset.loc[ dataset['MaxTemp'] <= 0, 'MaxTemp'] = 0\n  dataset.loc[(dataset['MaxTemp'] > 0) & (dataset['MaxTemp'] <= 5), 'MaxTemp'] = 1\n  dataset.loc[(dataset['MaxTemp'] > 5) & (dataset['MaxTemp'] <= 10), 'MaxTemp'] = 2\n  dataset.loc[(dataset['MaxTemp'] > 10) & (dataset['MaxTemp'] <= 15), 'MaxTemp'] = 3\n  dataset.loc[(dataset['MaxTemp'] > 15) & (dataset['MaxTemp'] <= 20), 'MaxTemp'] = 4\n  dataset.loc[(dataset['MaxTemp'] > 20) & (dataset['MaxTemp'] <= 25), 'MaxTemp'] = 5\n  dataset.loc[(dataset['MaxTemp'] > 25) & (dataset['MaxTemp'] <= 30), 'MaxTemp'] = 6\n  dataset.loc[(dataset['MaxTemp'] > 30) & (dataset['MaxTemp'] <= 35), 'MaxTemp'] = 7\n  dataset.loc[(dataset['MaxTemp'] > 35) & (dataset['MaxTemp'] <= 40), 'MaxTemp'] = 8\n  dataset.loc[(dataset['MaxTemp'] > 40) & (dataset['MaxTemp'] <= 45), 'MaxTemp'] = 9\n  dataset.loc[ dataset['MaxTemp'] > 45, 'MaxTemp'] = 10\n\nfor dataset in data:\n  dataset['Humidity9am'] = dataset['Humidity9am'].astype(int)\n  dataset.loc[(dataset['Humidity9am'] > 0) & (dataset['Humidity9am'] <= 10), 'Humidity9am'] = 0\n  dataset.loc[(dataset['Humidity9am'] > 10) & (dataset['Humidity9am'] <= 20), 'Humidity9am'] = 1\n  dataset.loc[(dataset['Humidity9am'] > 20) & (dataset['Humidity9am'] <= 30), 'Humidity9am'] = 2\n  dataset.loc[(dataset['Humidity9am'] > 30) & (dataset['Humidity9am'] <= 40), 'Humidity9am'] = 3\n  dataset.loc[(dataset['Humidity9am'] > 40) & (dataset['Humidity9am'] <= 50), 'Humidity9am'] = 4\n  dataset.loc[(dataset['Humidity9am'] > 50) & (dataset['Humidity9am'] <= 60), 'Humidity9am'] = 5\n  dataset.loc[(dataset['Humidity9am'] > 60) & (dataset['Humidity9am'] <= 70), 'Humidity9am'] = 6\n  dataset.loc[(dataset['Humidity9am'] > 70) & (dataset['Humidity9am'] <= 80), 'Humidity9am'] = 7\n  dataset.loc[(dataset['Humidity9am'] > 80) & (dataset['Humidity9am'] <= 90), 'Humidity9am'] = 8\n  dataset.loc[ dataset['Humidity9am'] > 90, 'Humidity9am'] = 9\n\nfor dataset in data:\n  dataset['Humidity3pm'] = dataset['Humidity3pm'].astype(int)\n  dataset.loc[(dataset['Humidity3pm'] > 0) & (dataset['Humidity3pm'] <= 10), 'Humidity3pm'] = 0\n  dataset.loc[(dataset['Humidity3pm'] > 10) & (dataset['Humidity3pm'] <= 20), 'Humidity3pm'] = 1\n  dataset.loc[(dataset['Humidity3pm'] > 20) & (dataset['Humidity3pm'] <= 30), 'Humidity3pm'] = 2\n  dataset.loc[(dataset['Humidity3pm'] > 30) & (dataset['Humidity3pm'] <= 40), 'Humidity3pm'] = 3\n  dataset.loc[(dataset['Humidity3pm'] > 40) & (dataset['Humidity3pm'] <= 50), 'Humidity3pm'] = 4\n  dataset.loc[(dataset['Humidity3pm'] > 50) & (dataset['Humidity3pm'] <= 60), 'Humidity3pm'] = 5\n  dataset.loc[(dataset['Humidity3pm'] > 60) & (dataset['Humidity3pm'] <= 70), 'Humidity3pm'] = 6\n  dataset.loc[(dataset['Humidity3pm'] > 70) & (dataset['Humidity3pm'] <= 80), 'Humidity3pm'] = 7\n  dataset.loc[(dataset['Humidity3pm'] > 80) & (dataset['Humidity3pm'] <= 90), 'Humidity3pm'] = 8\n  dataset.loc[ dataset['Humidity3pm'] > 90, 'Humidity3pm'] = 9\n\nfor dataset in data:\n  dataset['Pressure9am'] = dataset['Pressure9am'].astype(int)\n  dataset.loc[(dataset['Pressure9am'] > 0) & (dataset['Pressure9am'] <= 1000), 'Pressure9am'] = 0\n  dataset.loc[(dataset['Pressure9am'] > 1000) & (dataset['Pressure9am'] <= 1005), 'Pressure9am'] = 1\n  dataset.loc[(dataset['Pressure9am'] > 1005) & (dataset['Pressure9am'] <= 1010), 'Pressure9am'] = 2\n  dataset.loc[(dataset['Pressure9am'] > 1010) & (dataset['Pressure9am'] <= 1013), 'Pressure9am'] = 3\n  dataset.loc[(dataset['Pressure9am'] > 1013) & (dataset['Pressure9am'] <= 1015), 'Pressure9am'] = 4\n  dataset.loc[(dataset['Pressure9am'] > 1015) & (dataset['Pressure9am'] <= 1017), 'Pressure9am'] = 5\n  dataset.loc[(dataset['Pressure9am'] > 1017) & (dataset['Pressure9am'] <= 1019), 'Pressure9am'] = 6\n  dataset.loc[(dataset['Pressure9am'] > 1019) & (dataset['Pressure9am'] <= 1021), 'Pressure9am'] = 7\n  dataset.loc[(dataset['Pressure9am'] > 1021) & (dataset['Pressure9am'] <= 1031), 'Pressure9am'] = 8\n  dataset.loc[ dataset['Pressure9am'] > 1031, 'Pressure9am'] = 9\n\nfor dataset in data:\n  dataset['Pressure3pm'] = dataset['Pressure3pm'].astype(int)\n  dataset.loc[(dataset['Pressure3pm'] > 0) & (dataset['Pressure3pm'] <= 980), 'Pressure3pm'] = 0\n  dataset.loc[(dataset['Pressure3pm'] > 980) & (dataset['Pressure3pm'] <= 990), 'Pressure3pm'] = 1\n  dataset.loc[(dataset['Pressure3pm'] > 990) & (dataset['Pressure3pm'] <= 1000), 'Pressure3pm'] = 2\n  dataset.loc[(dataset['Pressure3pm'] > 1000) & (dataset['Pressure3pm'] <= 1005), 'Pressure3pm'] = 2\n  dataset.loc[(dataset['Pressure3pm'] > 1005) & (dataset['Pressure3pm'] <= 1011), 'Pressure3pm'] = 3\n  dataset.loc[(dataset['Pressure3pm'] > 1011) & (dataset['Pressure3pm'] <= 1013), 'Pressure3pm'] = 4\n  dataset.loc[(dataset['Pressure3pm'] > 1013) & (dataset['Pressure3pm'] <= 1015), 'Pressure3pm'] = 5\n  dataset.loc[(dataset['Pressure3pm'] > 1015) & (dataset['Pressure3pm'] <= 1017), 'Pressure3pm'] = 6\n  dataset.loc[(dataset['Pressure3pm'] > 1017) & (dataset['Pressure3pm'] <= 1019), 'Pressure3pm'] = 7\n  dataset.loc[(dataset['Pressure3pm'] > 1019) & (dataset['Pressure3pm'] <= 1024), 'Pressure3pm'] = 8\n  dataset.loc[(dataset['Pressure3pm'] > 1024) & (dataset['Pressure3pm'] <= 1029), 'Pressure3pm'] = 9\n  dataset.loc[(dataset['Pressure3pm'] > 1029) & (dataset['Pressure3pm'] <= 1035), 'Pressure3pm'] = 10\n  dataset.loc[ dataset['Pressure3pm'] > 1035, 'Pressure3pm'] = 11 \n\nfor dataset in data:\n  dataset['Temp9am'] = dataset['Temp9am'].astype(int)\n  dataset.loc[ dataset['Temp9am'] <= 0, 'Temp9am'] = 0\n  dataset.loc[(dataset['Temp9am'] > 0) & (dataset['Temp9am'] <= 5), 'Temp9am'] = 1\n  dataset.loc[(dataset['Temp9am'] > 5) & (dataset['Temp9am'] <= 10), 'Temp9am'] = 2\n  dataset.loc[(dataset['Temp9am'] > 10) & (dataset['Temp9am'] <= 15), 'Temp9am'] = 3\n  dataset.loc[(dataset['Temp9am'] > 15) & (dataset['Temp9am'] <= 20), 'Temp9am'] = 4\n  dataset.loc[(dataset['Temp9am'] > 20) & (dataset['Temp9am'] <= 25), 'Temp9am'] = 5\n  dataset.loc[(dataset['Temp9am'] > 25) & (dataset['Temp9am'] <= 30), 'Temp9am'] = 6\n  dataset.loc[(dataset['Temp9am'] > 30) & (dataset['Temp9am'] <= 35), 'Temp9am'] = 7\n  dataset.loc[ dataset['Temp9am'] > 35, 'Temp9am'] = 8\n\nfor dataset in data:\n  dataset['Temp3pm'] = dataset['Temp3pm'].astype(int)\n  dataset.loc[ dataset['Temp3pm'] <= 0, 'Temp3pm'] = 0 \n  dataset.loc[(dataset['Temp3pm'] > 0) & (dataset['Temp3pm'] <= 5), 'Temp3pm'] = 1\n  dataset.loc[(dataset['Temp3pm'] > 5) & (dataset['Temp3pm'] <= 10), 'Temp3pm'] = 2\n  dataset.loc[(dataset['Temp3pm'] > 10) & (dataset['Temp3pm'] <= 15), 'Temp3pm'] = 3\n  dataset.loc[(dataset['Temp3pm'] > 15) & (dataset['Temp3pm'] <= 20), 'Temp3pm'] = 4\n  dataset.loc[(dataset['Temp3pm'] > 20) & (dataset['Temp3pm'] <= 25), 'Temp3pm'] = 5\n  dataset.loc[(dataset['Temp3pm'] > 25) & (dataset['Temp3pm'] <= 30), 'Temp3pm'] = 6\n  dataset.loc[(dataset['Temp3pm'] > 30) & (dataset['Temp3pm'] <= 35), 'Temp3pm'] = 7\n  dataset.loc[(dataset['Temp3pm'] > 35) & (dataset['Temp3pm'] <= 40), 'Temp3pm'] = 8\n  dataset.loc[ dataset['Temp3pm'] > 40, 'Temp3pm'] = 9","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:57:14.197403Z","iopub.execute_input":"2021-09-01T02:57:14.197717Z","iopub.status.idle":"2021-09-01T02:57:14.474155Z","shell.execute_reply.started":"2021-09-01T02:57:14.197685Z","shell.execute_reply":"2021-09-01T02:57:14.473181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_new.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:57:14.475479Z","iopub.execute_input":"2021-09-01T02:57:14.475795Z","iopub.status.idle":"2021-09-01T02:57:14.614485Z","shell.execute_reply.started":"2021-09-01T02:57:14.475764Z","shell.execute_reply":"2021-09-01T02:57:14.613506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have finished Binning our data, our last step for the processing is to **breakdown the Date Feature**. Although Date is a numerical data, Machine Learning Model can't understand Date Type Data.\n\nTherefore, we will be **breaking our Date feature, into 3 different feature, which are Day, Month, and Year**. ","metadata":{}},{"cell_type":"code","source":"df_new['Date'] = pd.to_datetime(df_new['Date'])\n\ndf_new['Year'] = df_new['Date'].dt.year\ndf_new['Month'] = df_new['Date'].dt.month\ndf_new['Day'] = df_new['Date'].dt.day\n\ndf_new.drop(['Date'], axis=1, inplace=True)\ndf_new.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:57:14.615872Z","iopub.execute_input":"2021-09-01T02:57:14.616187Z","iopub.status.idle":"2021-09-01T02:57:14.743546Z","shell.execute_reply.started":"2021-09-01T02:57:14.616156Z","shell.execute_reply":"2021-09-01T02:57:14.742597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Selection","metadata":{}},{"cell_type":"markdown","source":"Before we create a Machine Learning Model with our data, we need to **reduce the dimensionality of our data**, by removing some of it's Features. We need to do this because not all of the Features that we have is useful for making the predictions. \n\nBut we can't just delete Features without any analyzation. This is where Feature Selection Method become useful. We will be removing Features using the **Embedded Method**. \n\nEmbedded Method is a feature selection technic using Machine Learning, by analyzing the correlation of each feature with each others. This method is choosed because it's have a fast computation speed, calculate for the interaction of each features, great accuracy, and reduce the chance of over-fitting. \n\nFor this method, we first need to **break our data into two**, one for Independent Features and the other for Target Features. ","metadata":{}},{"cell_type":"code","source":"X = df_new.drop('RainTomorrow', axis=1)\nY = df_new['RainTomorrow']","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:57:14.744742Z","iopub.execute_input":"2021-09-01T02:57:14.745037Z","iopub.status.idle":"2021-09-01T02:57:14.755783Z","shell.execute_reply.started":"2021-09-01T02:57:14.745008Z","shell.execute_reply":"2021-09-01T02:57:14.754307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = RandomForestClassifier()\nclf = clf.fit(X,Y)\n\nfeat_importance = pd.Series(clf.feature_importances_, index=X.columns)\nfeat_importance.plot(kind='barh')\nplt.title('Feature Importances')\nplt.show()\nprint(feat_importance)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:57:14.757276Z","iopub.execute_input":"2021-09-01T02:57:14.757702Z","iopub.status.idle":"2021-09-01T02:57:40.563521Z","shell.execute_reply.started":"2021-09-01T02:57:14.757666Z","shell.execute_reply":"2021-09-01T02:57:40.562506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this method, we can see the **value of each features to our Target features**. Now that we know the value, we can set the lower limit of the value. **We will set the lower limit to 0.04** so that we can remove some of the less useful features. ","metadata":{}},{"cell_type":"code","source":"embed = SelectFromModel(clf,threshold = 0.04, prefit=True)\nX_new = embed.transform(X)\n\nprint('Before Embed', X.shape)\nprint('After Embed', X_new.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T03:01:49.196241Z","iopub.execute_input":"2021-09-01T03:01:49.196788Z","iopub.status.idle":"2021-09-01T03:01:49.290518Z","shell.execute_reply.started":"2021-09-01T03:01:49.196754Z","shell.execute_reply":"2021-09-01T03:01:49.289508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have remove some of the less important features, we can continue to create our Machine Learning Model. \n\n","metadata":{}},{"cell_type":"markdown","source":"# Machine Learning Model","metadata":{}},{"cell_type":"markdown","source":"For our Machine Learning Model, there are two points to highlight. **First since we are dealing with Imbalance Dataset**, we will need to do either an Over-Sampling, or Under-Sampling. \n\nOver-Sampling have a risk of over-fitting our model. but Under-Sampling have a risk of losing valuable informations. With this consideration, we will use Over-Sampling for our model, using SMOTE.\n\nSecond, since we don't know the best model for our analysis, based on our Exploratory Data Analysis and Data Processing, we c**onclude that two possible best model** for our analysis would be **K Nearest Neighbor (KNN), or Random Forest Classifier**. We will try both of these model. \n\nWe Choose **KNN Model** since it is a model that calculate the distance of each features, using it to classify new data. **Random Forest** is choosen since it create a Tree-like calculation to determine the classification, and create a several iteration of the features to determine how to best classify a new data. \n\nFor the Scoring of our Model, there are **4 metrics to consider**. First is **Accuracy**, which is the ratio of correct prediction of the whole dataset. Second is **Precision**, which is the ratio of correctly predicted positive observations to the total predicted positive observations. **Recalls** correctly predicted positive observations to the all observations in actual class - yes, and **F1 Score** is weighted average of Precision and Recall.","metadata":{}},{"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X_new, Y, test_size=0.3, random_state=42)\n\noversample = SMOTE()\nX_over, Y_over = oversample.fit_resample(X_new, Y)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T03:01:53.964575Z","iopub.execute_input":"2021-09-01T03:01:53.966737Z","iopub.status.idle":"2021-09-01T03:01:55.342629Z","shell.execute_reply.started":"2021-09-01T03:01:53.966688Z","shell.execute_reply":"2021-09-01T03:01:55.341423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## K Nearest Neighbor","metadata":{}},{"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 3) \nknn.fit(X_over, Y_over)\nknn_pred = knn.predict(X_test)\nknn_test_score = accuracy_score(Y_test, knn_pred)\nacc_knn = cross_val_score(knn, X_over, Y_over, cv=5)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T03:01:56.682235Z","iopub.execute_input":"2021-09-01T03:01:56.682596Z","iopub.status.idle":"2021-09-01T03:02:27.127604Z","shell.execute_reply.started":"2021-09-01T03:01:56.682565Z","shell.execute_reply":"2021-09-01T03:02:27.126654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"random_forest = RandomForestClassifier(max_depth=5)\nrandom_forest.fit(X_over, Y_over)\nrandom_pred = random_forest.predict(X_test)\nrandom_test_score = accuracy_score(Y_test, random_pred)\nacc_random = cross_val_score(random_forest, X_over, Y_over, cv=5)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T03:02:27.129105Z","iopub.execute_input":"2021-09-01T03:02:27.129397Z","iopub.status.idle":"2021-09-01T03:03:56.308116Z","shell.execute_reply.started":"2021-09-01T03:02:27.129368Z","shell.execute_reply":"2021-09-01T03:03:56.307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Results","metadata":{}},{"cell_type":"code","source":"results = pd.DataFrame({\n    'Model': ['Random Forest', 'KNN Model'],\n    'Train Score': [acc_random.mean(), acc_knn.mean()],\n    'Test Score': [random_test_score, knn_test_score]          \n              })\nresult_df = results.sort_values(by='Train Score', ascending=False)\nresult_df = result_df.set_index('Model')\nresult_df","metadata":{"execution":{"iopub.status.busy":"2021-09-01T03:03:56.309972Z","iopub.execute_input":"2021-09-01T03:03:56.310419Z","iopub.status.idle":"2021-09-01T03:03:56.326842Z","shell.execute_reply.started":"2021-09-01T03:03:56.310372Z","shell.execute_reply":"2021-09-01T03:03:56.325857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now from the result, we can see that **KNN perform better than Random Forest**. Normally models are scored by 4 different methods, Accuracy, Precision, Recall, and F1. But for our **initial analysis**, we will simply refeer them to Train & Test Score. ","metadata":{}},{"cell_type":"markdown","source":"# HyperParameter Tuning","metadata":{}},{"cell_type":"markdown","source":"Both of our models already have a good performance, but we can still improve the performance using **HyperParameter Tuning**. HyperParameter is parameter of a model whose value is defined by the user. If we define them correctly, we can increase the score of our model. \n\nBut to find the optimal value for our HyperParameter, we need to search it first. To find the optimal value, we will be using **RandomizedSearchCV**, which is a HyperParameter searching method by iterating each possible combination for the HyperParameter that we define. \n\nFor HyperParamter Tuning, we will **try and find the best value to increase our F1 Score**. This is because eventhough we use accuracy for our initial test, for our **final result we will determine the best model using F1 score, since we are dealing with an Imbalance Dataset**, which is more accurate metrics for our model, since it's calculate the average of Recall and Precision. Still, we still will show the score of all of the 4 metrics for the final result, to show a broader context in scoring our models. . ","metadata":{}},{"cell_type":"markdown","source":"## RandomizedSearchCV","metadata":{}},{"cell_type":"code","source":"param = {'n_neighbors' : [3,5,7],\n         'p' : [1, 2],\n         'algorithm' : ['ball_tree', 'kd_tree']}\n\nsearch_knn = RandomizedSearchCV(knn, param_distributions = param, n_iter=25, scoring='f1', n_jobs=-1, cv=3, random_state=1)\nresult_knn = search_knn.fit(X_over, Y_over)\n\nprint('Best Score: %s' % result_knn.best_score_)\nprint('Best Hyperparameters: %s' % result_knn.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T03:06:21.051011Z","iopub.execute_input":"2021-09-01T03:06:21.051567Z","iopub.status.idle":"2021-09-01T03:11:42.656151Z","shell.execute_reply.started":"2021-09-01T03:06:21.051531Z","shell.execute_reply":"2021-09-01T03:11:42.654984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param = {'n_estimators': [100, 300, 500],\n         'max_depth': [4, 5, 6],\n         'min_samples_split':[2, 4, 6],\n         'min_samples_leaf': [1, 3, 5]}\n\nrandom_forest = RandomForestClassifier()\nsearch = RandomizedSearchCV(random_forest, param_distributions = param, n_iter=25, scoring='f1', n_jobs=-1, cv=3, random_state=1)\nresult = search.fit(X_over, Y_over)\n\nprint('Best Score: %s' % result.best_score_)\nprint('Best Hyperparameters: %s' % result.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T03:12:15.162707Z","iopub.execute_input":"2021-09-01T03:12:15.163358Z","iopub.status.idle":"2021-09-01T03:31:57.17884Z","shell.execute_reply.started":"2021-09-01T03:12:15.163301Z","shell.execute_reply":"2021-09-01T03:31:57.177682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have the **values for our HyperParameter Tuning**, the next step would be to enter the values into our Machine Learning Model. \n\nWe will input the values into both of our Machine Learning Model, K Nearest Neighbor and Random Forest Classifier. ","metadata":{}},{"cell_type":"markdown","source":"## K Nearest Neighbor","metadata":{}},{"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 3, algorithm = 'kd_tree', p = 1)\nknn.fit(X_over,Y_over)\nknn_pred = knn.predict(X_test)\nacc_knn = cross_val_score(knn, X_over, Y_over, cv=5)\nknn_acc_score = accuracy_score(Y_test, knn_pred)\nknn_prec_score = precision_score(Y_test, knn_pred)\nknn_rec_score = recall_score(Y_test, knn_pred)\nknn_f1_score = f1_score(Y_test, knn_pred)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T03:31:57.180687Z","iopub.execute_input":"2021-09-01T03:31:57.181006Z","iopub.status.idle":"2021-09-01T03:32:38.234881Z","shell.execute_reply.started":"2021-09-01T03:31:57.180972Z","shell.execute_reply":"2021-09-01T03:32:38.233897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest","metadata":{}},{"cell_type":"code","source":"random_forest = RandomForestClassifier(n_estimators = 500, min_samples_split = 2, min_samples_leaf = 1,  max_depth = 6)\nrandom_forest.fit(X_over, Y_over)\nrandom_pred = random_forest.predict(X_test)\nacc_random = cross_val_score(random_forest, X_over, Y_over, cv=5)\nrandom_acc_score = accuracy_score(Y_test, random_pred)\nrandom_prec_score = precision_score(Y_test, random_pred)\nrandom_rec_score = recall_score(Y_test, random_pred)\nrandom_f1_score = f1_score(Y_test, random_pred)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T03:32:55.799684Z","iopub.execute_input":"2021-09-01T03:32:55.800088Z","iopub.status.idle":"2021-09-01T03:41:31.141888Z","shell.execute_reply.started":"2021-09-01T03:32:55.800038Z","shell.execute_reply":"2021-09-01T03:41:31.140775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Results","metadata":{}},{"cell_type":"code","source":"results = pd.DataFrame({\n    'Score Index': ['Random Forest Classifier', 'K Nearest Neighbor'],\n    'Train Score': [acc_random.mean(), acc_knn.mean()],\n    'Accuracy Test' : [random_acc_score, knn_acc_score],\n    'Precision Test' : [random_prec_score, knn_prec_score],\n    'Recall Test' : [random_rec_score, knn_rec_score],\n    'F1 Test' : [random_f1_score, knn_f1_score]\n    })\nresult_forest = results\nresult_forest","metadata":{"execution":{"iopub.status.busy":"2021-09-01T03:41:31.143251Z","iopub.execute_input":"2021-09-01T03:41:31.143545Z","iopub.status.idle":"2021-09-01T03:41:31.161595Z","shell.execute_reply.started":"2021-09-01T03:41:31.143516Z","shell.execute_reply":"2021-09-01T03:41:31.160312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After doing HyperParameter tuning, we were able to increase the result from our model. The **KNN Model Training Score have increased** from 85,7% to 88,3%. Also, our KNN model have a high accuracy and recall score. **The Random Forest model also increased it's training score** after HyperParameter Tuning, from 80,1% to 81,1%. From this, we conclude that the **HyperParameter Tuning successfully increase our model score**. \n\nSince we are dealing with Imbalance Dataset, the best metric to determine the **best score for our analysis is F1 Score**, which is the weighted average of Precision and Recall.\n\nTherefore, we can conclude that the **best model for this analysis is K Nearest Neighbor Model**, with F1 Score of 86,2%. ","metadata":{}}]}