{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Query Based Relevant Arrangement of Medical Papers and Getting Suitable Answer"},{"metadata":{},"cell_type":"markdown","source":"## Acknowledgements:\nI am grateful to [xhlulu](https://www.kaggle.com/xhlulu) for the [useful notebook](https://www.kaggle.com/xhlulu/cord-19-eda-parse-json-and-generate-clean-csv/notebook).\n\nI am thankful to [Ivan](https://www.kaggle.com/ivanbagmut), [Sergey](https://www.kaggle.com/sergeypashchenko), and [Lvennak](https://www.kaggle.com/lvennak) for fruitful discussions on COVID-19.\n"},{"metadata":{},"cell_type":"markdown","source":"## Step 0. Set up packages"},{"metadata":{},"cell_type":"markdown","source":" Install the [End-To-End Closed Domain Question Answering System](https://pypi.org/project/cdqa/)"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install cdqa","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"import the required modules:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport json\nimport nltk\nfrom math import log, sqrt\nfrom collections import defaultdict\nfrom copy import deepcopy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 1.   Extraction of data from json files to dataframe format"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Paths to json files\npath_1 = '/kaggle/input/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv/pdf_json/'\npath_2 = '/kaggle/input/CORD-19-research-challenge/comm_use_subset/comm_use_subset/pdf_json/'\npath_3 = '/kaggle/input/CORD-19-research-challenge/noncomm_use_subset/noncomm_use_subset/pdf_json/'\npath_4 = '/kaggle/input/CORD-19-research-challenge/custom_license/custom_license/pdf_json/'\n\n# List of folder names\nfolder_names = ['biorxiv_medrxiv','comm_use_subset']\nfolder_paths = [path_1, path_2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This piece of code was adopted from the original source at:\n# https://www.kaggle.com/xhlulu/cord-19-eda-parse-json-and-generate-clean-csv/notebook \n\ndef format_name(author):\n    middle_name = \" \".join(author['middle'])\n    if author['middle']:\n        return \" \".join([author['first'], middle_name, author['last']])\n    else:\n        return \" \".join([author['first'], author['last']])\n\ndef format_affiliation(affiliation):\n    text = []\n    location = affiliation.get('location')\n    if location:\n        text.extend(list(affiliation['location'].values()))\n    institution = affiliation.get('institution')\n    if institution:\n        text = [institution] + text\n    return \", \".join(text)\n\ndef format_authors(authors, with_affiliation=False):\n    name_ls = []\n    for author in authors:\n        name = format_name(author)\n        if with_affiliation:\n            affiliation = format_affiliation(author['affiliation'])\n            if affiliation:\n                name_ls.append(f\"{name} ({affiliation})\")\n            else:\n                name_ls.append(name)\n        else:# First, for each query the system arranges all the scientific papers within the corpus in the relevant order.\n# Second, the system analize texts of top N the mosr relevant papers to answer to the query in the best way.\n            name_ls.append(name)\n    return \", \".join(name_ls)\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    for section, text in texts:\n        texts_di[section] += text\n    body = \"\"\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    return body\n\ndef format_bib(bibs):\n    if type(bibs) == dict:\n        bibs = list(bibs.values())\n    bibs = deepcopy(bibs)\n    formatted = []\n    for bib in bibs:\n        bib['authors'] = format_authors(\n            bib['authors'], \n            with_affiliation=False\n        )\n        formatted_ls = [str(bib[k]) for k in ['title', 'authors', 'venue', 'year']]\n        formatted.append(\", \".join(formatted_ls))\n    return \"; \".join(formatted)\n\ndef load_files(dirname, filename=None):\n    filenames = os.listdir(dirname)\n    raw_files = []\n    if filename:\n        filename = dirname + filename\n        raw_files = [json.load(open(filename, 'rb'))]\n    else:\n        #for filename in tqdm(filenames):\n        for filename in filenames:\n            filename = dirname + filename\n            file = json.load(open(filename, 'rb'))\n            raw_files.append(file)\n    return raw_files\n\ndef generate_clean_df(all_files):\n    cleaned_files = []\n    #for file in tqdm(all_files):\n    for file in all_files:\n        features = [\n            file['paper_id'],\n            file['metadata']['title'],\n            format_authors(file['metadata']['authors']),\n            format_authors(file['metadata']['authors'], \n                           with_affiliation=True),\n            format_body(file['abstract']),\n            format_body(file['body_text']),\n            format_bib(file['bib_entries']),\n            file['metadata']['authors'],\n            file['bib_entries']\n        ]\n        cleaned_files.append(features)\n    col_names = ['paper_id', 'title', 'authors',\n                 'affiliations', 'abstract', 'text', \n                 'bibliography','raw_authors','raw_bibliography']\n    clean_df = pd.DataFrame(cleaned_files, columns=col_names)\n    clean_df = clean_df.drop(columns=['authors','affiliations','bibliography',\n                                      'raw_authors','raw_bibliography'])\n    return clean_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2. Corpus formation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_corpus(folder_paths = folder_paths):\n    num_of_papers = {}\n    corpus = pd.DataFrame(columns=['paper_id','title','abstract','text'])\n    for i in range(len(folder_paths)):\n        filenames = os.listdir(folder_paths[i])\n        print('Reading {0} json files from folder {1} ...'.format(len(filenames), folder_names[i]))\n        num_of_papers[folder_names[i]] = len(filenames)\n        files = load_files(folder_paths[i])\n        df = generate_clean_df(files)\n        corpus = pd.concat([corpus, df], ignore_index=True, sort=False)\n    print('Corpus includes {0} scientific articles.'.format(len(corpus)))\n    return corpus, num_of_papers\n\ncorpus, num_of_papers = get_corpus()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 3. Processing of Corpus"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This processing algorithm can originaly be found at:\n# https://github.com/nilayjain/text-search-engine\n\ninverted_index = defaultdict(list)\nnum_of_documents = len(corpus)\nvects_for_docs = []  # we will need nos of docs number of vectors, each vector is a dictionary\ndocument_freq_vect = {}  # sort of equivalent to initializing the number of unique words to 0\n\n# It updates the vects_for_docs variable with vectors of all the documents.\ndef iterate_over_all_docs():\n    print('Processing corpus...')\n    for i in range(num_of_documents):\n        if np.mod(i, 1000) == 0:\n            print('{0} of {1}'.format(str(i).zfill(len(str(num_of_documents))),num_of_documents))\n        doc_text = corpus['title'][i] + ' ' + corpus['abstract'][i] + ' ' + corpus['text'][i]\n        token_list = get_tokenized_and_normalized_list(doc_text)\n        vect = create_vector(token_list)\n        vects_for_docs.append(vect)\n    print('{0} of {1}'.format(num_of_documents, num_of_documents))\n\ndef create_vector_from_query(l1):\n    vect = {}\n    for token in l1:\n        if token in vect:\n            vect[token] += 1.0\n        else:\n            vect[token] = 1.0\n    return vect\n\ndef generate_inverted_index():\n    count1 = 0\n    for vector in vects_for_docs:\n        for word1 in vector:\n            inverted_index[word1].append(count1)\n        count1 += 1\n\ndef create_tf_idf_vector():\n    vect_length = 0.0\n    for vect in vects_for_docs:\n        for word1 in vect:\n            word_freq = vect[word1]\n            temp = calc_tf_idf(word1, word_freq)\n            vect[word1] = temp\n            vect_length += temp ** 2\n        vect_length = sqrt(vect_length)\n        for word1 in vect:\n            vect[word1] /= vect_length\n\ndef get_tf_idf_from_query_vect(query_vector1):\n    vect_length = 0.0\n    for word1 in query_vector1:\n        word_freq = query_vector1[word1]\n        if word1 in document_freq_vect:\n            query_vector1[word1] = calc_tf_idf(word1, word_freq)\n        else:\n            query_vector1[word1] = log(1 + word_freq) * log(\n                num_of_documents)\n        vect_length += query_vector1[word1] ** 2\n    vect_length = sqrt(vect_length)\n    if vect_length != 0:\n        for word1 in query_vector1:\n            query_vector1[word1] /= vect_length\n\ndef calc_tf_idf(word1, word_freq):\n    return log(1 + word_freq) * log(num_of_documents / document_freq_vect[word1])\n\ndef get_dot_product(vector1, vector2):\n    if len(vector1) > len(vector2):\n        temp = vector1\n        vector1 = vector2\n        vector2 = temp\n    keys1 = vector1.keys()\n    keys2 = vector2.keys()\n    sum = 0\n    for i in keys1:\n        if i in keys2:\n            sum += vector1[i] * vector2[i]\n    return sum\n\ndef get_tokenized_and_normalized_list(doc_text):\n    tokens = nltk.word_tokenize(doc_text)\n    ps = nltk.stem.PorterStemmer()\n    stemmed = []\n    for words in tokens:\n        stemmed.append(ps.stem(words))\n    return stemmed\n\ndef create_vector(l1):\n    vect = {}  # this is a dictionary\n    global document_freq_vect\n    for token in l1:\n        if token in vect:\n            vect[token] += 1\n        else:\n            vect[token] = 1\n            if token in document_freq_vect:\n                document_freq_vect[token] += 1\n            else:\n                document_freq_vect[token] = 1\n    return vect\n\ndef get_result_from_query_vect(query_vector1):\n    parsed_list = []\n    for i in range(num_of_documents - 0):\n        dot_prod = get_dot_product(query_vector1, vects_for_docs[i])\n        parsed_list.append((i, dot_prod))\n        parsed_list = sorted(parsed_list, key=lambda x: x[1])\n    return parsed_list\n\niterate_over_all_docs()\ngenerate_inverted_index()\ncreate_tf_idf_vector()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 4. Using pretrained BERT model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The End-To-End Closed Domain Question Answering System is used here.\n# It is available at: https://pypi.org/project/cdqa/\n\nfrom cdqa.utils.filters import filter_paragraphs\nfrom cdqa.utils.download import download_model, download_bnpp_data\nfrom cdqa.pipeline.cdqa_sklearn import QAPipeline\n\ndownload_bnpp_data(dir='./data/bnpp_newsroom_v1.1/')\ndownload_model(model='bert-squad_1.1', dir='./models')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 5. Search of the most relevant articles and competent answer on the query"},{"metadata":{},"cell_type":"markdown","source":"First, for each query the system arranges all the scientific papers within the corpus in the relevant order.\n\nSecond, the system analize texts of top N the mosr relevant papers to answer to the query in the best way."},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_relevant_articles(query=None, top_n_papers=20, min_n_papers=3):\n    if query == None:\n        query = input('Please enter your query...')\n    print('\\n\\n'+'*'*34+' PROCESSING NEW QUERY '+'*'*34+'\\n')   \n    query_list = get_tokenized_and_normalized_list(query)\n    query_vector = create_vector_from_query(query_list)\n    get_tf_idf_from_query_vect(query_vector)\n    result_set = get_result_from_query_vect(query_vector)\n    papers_info = {'query':query, 'query list':query_list, 'query vector':query_vector,\n                   'id':[], 'title':[], 'abstract':[], 'text':[], 'weight':[], 'index':[]}\n    for i in range(1, top_n_papers+1):\n        tup = result_set[-i]\n        papers_info['id'].append(corpus['paper_id'][tup[0]])\n        papers_info['title'].append(corpus['title'][tup[0]])\n        papers_info['abstract'].append(corpus['abstract'][tup[0]])\n        papers_info['text'].append(corpus['text'][tup[0]])\n        papers_info['weight'].append(tup[1])\n        papers_info['index'].append(tup[0])\n    colms = ['date', 'title', 'category', 'link', 'abstract', 'paragraphs']\n    df = pd.DataFrame(columns=colms)\n    for i in range(len(papers_info['text'])):\n        papers_info['text'][i] = papers_info['text'][i].replace('\\n\\n', ' ')\n        CurrentText = papers_info['text'][i]\n        CurrentText = CurrentText.split('. ')\n        #CurrentList = [\"None\", papers_info['title'][i], \"None\", \"None\", \"None\", CurrentText]\n        CurrentList = [\"None\", papers_info['title'][i], \"None\", \"None\", papers_info['abstract'][i], CurrentText]\n        CurrentList = np.array(CurrentList)\n        CurrentList = CurrentList.reshape(1, CurrentList.shape[0])\n        CurrentList = pd.DataFrame(data = CurrentList, columns=colms)\n        df = pd.concat([df, CurrentList], ignore_index=True)\n    df = filter_paragraphs(df)\n    # Loading QAPipeline with CPU version of BERT Reader pretrained on SQuAD 1.1\n    cdqa_pipeline = QAPipeline(reader='models/bert_qa.joblib')\n    # Fitting the retriever to the list of documents in the dataframe\n    cdqa_pipeline.fit_retriever(df=df)\n    # Sending a question to the pipeline and getting prediction\n    query = papers_info['query']\n    prediction = cdqa_pipeline.predict(query=query)\n    for i in range(top_n_papers):\n        if papers_info['title'][i] == prediction[1]:\n            pid = papers_info['id'][i]\n    response = {query:{'id':pid,'title':prediction[1],'answer':prediction[0],'summary':prediction[2],\n                       'important papers':{'id':papers_info['id'],'title':papers_info['title']}}}\n    print('QUERY: {0}\\n'.format(query))\n    print('ANSWER MINED FROM PAPER: {0}\\n'.format(prediction[0]))\n    print('PAPER TITLE: {0}\\n'.format(prediction[1]))\n    print('PARAGRAPH IN PAPER: {0}\\n'.format(prediction[2]))\n    show_paper = np.min([min_n_papers, top_n_papers])\n    print('\\nTOP {0} MOST RELEVANT PAPERS RELATED TO THE QUERY:\\n'.format(show_paper))\n    for i in range(show_paper):\n        print('PAPER #{0}. \\nID: {1} \\nTITLE: {2}\\n'.format(i+1, papers_info['id'][i], papers_info['title'][i]))\n    return response, papers_info, prediction, result_set, df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 6. Getting practical answers and the most relevant papers (query based approach)"},{"metadata":{},"cell_type":"markdown","source":"Below one can see a list of 10 queries and answers, which have been found by the system due to text mining. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of queries\nqueries = ['What is range of incubation period for coronavirus SARS-CoV-2 COVID-19 in humans',\n           'What is optimal quarantine period for coronavirus COVID-19',\n           'What is effective quarantine period for coronavirus COVID-19',\n           'What is percentage of death cases for coronavirus SARS-CoV-2 COVID-19',\n           'What is death rate for coronavirus COVID-19 and air pollution',\n           'At which temperature coronavirus COVID-19 can survive',\n           'How long coronavirus SARS-CoV-2 can survive on plastic surface',\n           'What are risk factors for coronavirus COVID-19',\n           'What is origin of coronavirus COVID-19',\n           'At which temperature coronavirus cannot survive']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for query in queries:\n    response, papers_info, prediction, result_set, df = find_relevant_articles(query, top_n_papers=50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## How to use the system"},{"metadata":{},"cell_type":"markdown","source":"    When Steps 0-6 have been completed with a corpus of scientific papers, the system is ready to process your queries. To get an answer to a query, follow two steps: \n\n    1. Input any query in the form of string type variable.\n    \n    For example,"},{"metadata":{"trusted":true},"cell_type":"code","source":"query = 'What is coronavirus SARS-CoV-2'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    2. Call the function find_relevant_articles().\n\n    For example,"},{"metadata":{"trusted":true},"cell_type":"code","source":"find_relevant_articles(query=query, top_n_papers=50, min_n_papers=5);","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}