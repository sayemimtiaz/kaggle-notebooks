{"cells":[{"metadata":{"_uuid":"1e12282742e247fff3ae6ad037a172114452fba1"},"cell_type":"markdown","source":"# Clinical Data Explanation and Standard EDA"},{"metadata":{"_uuid":"08bc4a5cab8812bc595927202f5de19170b63645"},"cell_type":"markdown","source":"In this kernel, I'm going to explain the data with some related information about heart disease.\n\nThere are two parts in this kernel.  \nIn the first part, I will answer the following two questions:\n\n1. How these fields are related to heart disease?\n2. Why these fields can be used to diagnose heart disease?\n\nIn the second part, I will give a basic classification solution with 3-layer NN and lightgbm.\nThe best accuracy recorded is *95%* with lightgbm.\n\n**Note that the original data descriptions do not correspond to the data, thus I've manually modified them to make them look more realistic.**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"340bca1f48978904ba5ba43d3681a1c0ccb8b8a9"},"cell_type":"markdown","source":"First, we need to handle null values.  \nHopefully there are no null values in this dataset."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":false},"cell_type":"code","source":"df = pd.read_csv(\"../input/heart.csv\")\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21bab26db7fa18d9e956f5e59fc147a3dfd8989d"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28e27d081781fa591d9cf40a8c52d02110d954cd"},"cell_type":"markdown","source":"Here is other fields' correlation with target.\nMost fields are highly correlated with heart disease."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"cb57983e734740b7230eb2bf9afebfcbca9f9e69"},"cell_type":"code","source":"corr = df.corr()['target'].abs().sort_values()\ncorr","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a127ff0676a3c6d0b72f2f2a7882c0e6c1608540"},"cell_type":"markdown","source":"Let's plot the differences between people with and without heart diseases side by side.\n\nFields that are more correlated with heart diseases are put at the bottom."},{"metadata":{"_uuid":"9d1b72b6da64cccc123a6a7f3857b80238af9476"},"cell_type":"markdown","source":"## Part I: Comparison between people with(left) and without(right) heart diseases\nIn this part, I will answer the following two questions with side-by-side plot of each feature:\n\n1. How these fields are related to heart disease?\n2. Why these fields can be used to diagnose heart disease?"},{"metadata":{"trusted":true,"_uuid":"3d4d11742822a98c9dc42ae80dbed5e1ff072f91"},"cell_type":"code","source":"# Helper function for plotting side by side\ndef sideplot(df, col, kind=\"bar\", title=None):\n    assert kind in [\"bar\", \"hist\"]\n    fig = plt.figure(figsize=(10, 6))\n    if kind == \"bar\":\n        ax1 = plt.subplot(2, 2, 1)\n        df[df.target == 1][['target', col]].groupby(col).count().plot(kind='bar', rot=0, legend=False, ax=ax1, color=\"#268bd2\")\n        ax2 = plt.subplot(2, 2, 2)\n        df[df.target == 0][['target', col]].groupby(col).count().plot(kind='bar', rot=0, legend=False, ax=ax2, color=\"#268bd2\")\n    else:\n        ax1 = plt.subplot(2, 2, 1)\n        plt.hist(df[df.target == 1][col], color=\"#268bd2\")\n        plt.xlabel(col)\n        ax2 = plt.subplot(2, 2, 2)\n        plt.hist(df[df.target == 0][col], color=\"#268bd2\")\n        plt.xlabel(col)\n    # Re-adjusting\n    ylim = (0, max(ax1.get_ylim()[1], ax2.get_ylim()[1]))\n    ax1.set_ylim(ylim)\n    ax2.set_ylim(ylim)\n    xlim = (min(ax1.get_xlim()[0], ax2.get_xlim()[0]), max(ax1.get_xlim()[1], ax2.get_xlim()[1]))\n    ax1.set_xlim(xlim)\n    ax2.set_xlim(xlim)\n    if title is not None:\n        fig.suptitle(title)\n    #plt.subplots_adjust(top=0.99)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0abbbdeee21e8a4c03e3f294bd9530f3e84d59a3"},"cell_type":"markdown","source":"### Fasting blood sugar(FBS)\nPeople with FBS higher than 120 mg/dl are marked as 1, else 0."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"d1f18119b23f7bba321249673e3d91bc659c6859"},"cell_type":"code","source":"sideplot(df, \"fbs\", kind=\"bar\", title=\"Comparison of fasting blood sugar\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57b679ae08d566ce06d3555695b895ef8c92474e"},"cell_type":"markdown","source":"Fasting blood sugar is an important feature for measuring diabetes.  \nThough it is not directly related to heart diseases, It is pointed out that people with diabetes have a higher epidemiological estimate lifetime risk (LTR) of chronical heart disease<sup>1</sup>.\n\nJin C<sup>2</sup> finds out that the trajectories of fasting blood sugar are significantly associated with the risk of myocardio infarction. Since we only have a single value instead of continuous data, it does not look quite useful here to analyse heart disease.\n\n<hr />\n\n1: Turin TC, Okamura T, Rumana N, Afzal AR, Watanabe M, Higashiyama A, Nakao YM, \nNakai M, Takegami M, Nishimura K, Kokubo Y, Okayama A, Miyamoto Y. Diabetes and\nlifetime risk of coronary heart disease. Prim Care Diabetes. 2017\nOct;11(5):461-466. doi: 10.1016/j.pcd.2017.04.007. Epub 2017 May 22. PubMed PMID:\n28545843.\n\n2: Jin C, Chen S, Vaidya A, Wu Y, Wu Z, Hu FB, Kris-Etherton P, Wu S, Gao X.\nLongitudinal Change in Fasting Blood Glucose and Myocardial Infarction Risk in a \nPopulation Without Diabetes. Diabetes Care. 2017 Nov;40(11):1565-1572. doi:\n10.2337/dc17-0610. Epub 2017 Sep 8. PubMed PMID: 28887409; PubMed Central PMCID: \nPMC5652588."},{"metadata":{"_uuid":"b4e25009a5605b0f8b55ff669774efaad3abc3e1"},"cell_type":"markdown","source":"### Serum cholestoral(chol)"},{"metadata":{"trusted":true,"_uuid":"8a5fa23520f5661ce2dae5fd8471ab5696e91be1"},"cell_type":"code","source":"sideplot(df, \"chol\", kind=\"hist\", title=\"Comparison of serum cholestoral\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd239ddfe3144e4236fad212853bbc4f0abf162e"},"cell_type":"markdown","source":"People who have higher cholesterol in their blood (or hypercholesterolemia) are more likely to have their blood vessels narrowed with more cholesterol (mostly LDL cholesterol) precipited, which is known as atherosclerosis.\n\nCoronary heart disease, which is one of the most common heart diseases, comes from lack of blood supply for the heart because of atherosclerosis of coronary arteries."},{"metadata":{"_uuid":"3751031c3ff27a99149a78d79ce93f851608573d"},"cell_type":"markdown","source":"### Resting electrocardiographic results (restecg)\n- Value 0: normal\n- Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n- Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria "},{"metadata":{"trusted":true,"_uuid":"a4fd84599a6535ffe359ae57241700262e0835f5"},"cell_type":"code","source":"sideplot(df, \"restecg\", kind=\"bar\", title=\"Comparison of resting ECG results\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dec1240fa525e7ab5c5c88982c8e99f9ff06a1df"},"cell_type":"markdown","source":"In ECG, all the three ST-T wave abnormality menstioned(T wave inversion; ST elevation; ST depression of greater than 0.05 mV) are possible signs of myo infarction.  \nT wave inversion is a sign of lacking blood in endocardium, while ST abnomality indicates there are some damage to the myocardium due to lack of blood for a relatively long period of time."},{"metadata":{"_uuid":"eb11b90ea2a225562b95fa4a4ea04eef2d8172c0"},"cell_type":"markdown","source":"### Resting blood pressure (trestbps)\nThe data seems to be systolic blood pressure.\n\nIn most countries, the normal blood pressure is around 120/80. The 2019 standard of high blood pressure (or hypertension) is over 130/80 in America."},{"metadata":{"trusted":true,"_uuid":"5a294a9c1e245e850133a56a77d426a1c2b3b460"},"cell_type":"code","source":"sideplot(df, \"trestbps\", kind=\"hist\", title=\"Comparison of resting blood pressure\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94e17a0c14bd01022dbb4e2bdfaf0e5998e70148"},"cell_type":"markdown","source":"If not controlled, continuous status of high blood pressure can result in congestive heart failure.  \nIn most circumstances, people with high blood pressure also suffered from coronary heart diseases, as high blood pressure can contribute to atherosclerosis."},{"metadata":{"_uuid":"64eb287f3bfcd9cb231030945cbc3ef8ea92dfe7"},"cell_type":"markdown","source":"### Age"},{"metadata":{"trusted":true,"_uuid":"babaf1fa5726f3b7c30f1ebaf8f4a6751c098040"},"cell_type":"code","source":"sideplot(df, \"age\", kind=\"hist\", title=\"Comparison of age\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e983ec969ca8bcd87e5aea3e75015b1a8d6eff6"},"cell_type":"markdown","source":"### Sex\n1 = male; 0 = female"},{"metadata":{"trusted":true,"_uuid":"b61479cf303b2f8f28e019bfb9972faded27eff4"},"cell_type":"code","source":"sideplot(df, \"sex\", kind=\"bar\", title=\"Comparison of sex\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"578eb18827ee99b57d4a3f3bc284dd2644f71a65"},"cell_type":"markdown","source":"### (thal)\nSorry, I don't know what `thal` means in the dataset. The description shows that it has 3 acceptable values:\n\n- 3 = normal\n- 6 = fixed defect\n- 7 = reversable defect\n\nBut the data here are labeled 0-4. Maybe we should contact the dataset provider."},{"metadata":{"trusted":true,"_uuid":"be759d60de1fe18e06f1880f6481e4fe91617e40"},"cell_type":"code","source":"sideplot(df, \"thal\", kind=\"bar\", title=\"Comparison of (thal)\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65b8b8bf6393303d66136b1a0d3e9501d9443312"},"cell_type":"markdown","source":"### the slope of the peak exercise ST segment (slope)\n*Adjusted from data description*\n- Value 0: upsloping\n- Value 1: flat\n- Value 2: downsloping"},{"metadata":{"trusted":true,"_uuid":"1b92b360221a5df9fdbd169e6129e0ba92f0359b"},"cell_type":"code","source":"sideplot(df, \"slope\", kind=\"bar\", title=\"Comparison of the slope of the peak exercise ST segment\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"493311f1d765b1dffeac31c879972f6bd387215b"},"cell_type":"markdown","source":"### number of major vessels colored by fluorosopy (ca)\n*The data description says that the number of major vessels ranges from 0 to 3, but actually it ranges from 0 to 4*"},{"metadata":{"trusted":true,"_uuid":"d35143f31ea1a26c20f1ba768ee4918bfde62126"},"cell_type":"code","source":"sideplot(df, \"ca\", kind=\"bar\", title=\"Comparison of the number of major visible vessels under fluorosopy\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa74d7f79c9812e9845e708ba6b0cf87f9b17ed9"},"cell_type":"markdown","source":"There are two major coronary arteries providing nutrition for the heart. The left one has 3 major branches, and the right one has 5 major branches.\n\nUnder fluorosopy, only arteries that have a stable blood transmission can be shown clearly.  \nWhich means the less visible vessels, the worse the heart can receive adequate nutrient and oxygen, and the greater the possibility the patient will suffer a heart attack."},{"metadata":{"_uuid":"340be7fc2c31704d736dda9ae58ae091730ba4fe"},"cell_type":"markdown","source":"### maximum heart rate achieved(thalach)"},{"metadata":{"trusted":true,"_uuid":"c0771bf1fea3e4920fa7d1690cf254e3daf87575","scrolled":true},"cell_type":"code","source":"sideplot(df, \"thalach\", kind=\"hist\", title=\"Comparison of maximum heart rate achieved\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eab58b662343b44029beb57ecf1d3f8a36ecc307"},"cell_type":"markdown","source":"### ST depression induced by exercise relative to rest (oldpeak)"},{"metadata":{"trusted":true,"_uuid":"fb5a04d838fee5317b27633439d19cc22058e508"},"cell_type":"code","source":"sideplot(df, \"oldpeak\", kind=\"hist\", title=\"Comparison of ST depression induced by exercise relative to rest\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"132f057d8ee2e8628b1a66179d5ff4349bb3c614"},"cell_type":"markdown","source":"In exercise, the heart will consume more oxygen and nutrient, which can introduce shortage of blood transmitted to the heart by coronary arteries.\n\nAs we learned above, if myocardium lack blood for a long time, the cells in myocardium will be damaged, which can result in depression in ST segment. This feature will expose some not-easy-to-detect heart diseases."},{"metadata":{"_uuid":"49797fb41295d337bb40d62480ec1a0a1adde9de"},"cell_type":"markdown","source":"### chest pain type (cp)\n- Value 0: asymptomatic\n- Value 1: typical angina\n- Value 2: atypical angina\n- Value 3: non-anginal pain"},{"metadata":{"trusted":true,"_uuid":"4c6f1c2b7b63642e577a0b95db5f36550ba71281"},"cell_type":"code","source":"sideplot(df, \"cp\", kind=\"bar\", title=\"Comparison of chest pain type\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1c5185c56d102157a31f34ec9b8bd3c6f35845b"},"cell_type":"markdown","source":"Present research <sup>1, 2</sup> shows that the number of patients with typical angina are not significantly different from that with atypical angina and non-anginal pain.\n\nIt seems that patients with atypical angina and non-anginal pain are more likely to have a heart disease here, which are contradict with the research results, may come from the following reasons:\n\n- The result may be true due to lack of data (we only have 306 records here).\n- Indeed atypical angina and non-anginal pain are more likely to have a heart disease.\n\n<hr />\n\n1: Bugaenko VV, LomakovskiÄ­ AN. [Alterations in coronary arteries and frequency\nof episodes of transient myocardial ischemia in patients with ischemic heart\ndisease with typical and atypical angina pectoris]. Lik Sprava. 2002;(2):27-31.\nRussian. PubMed PMID: 12073253.\n\n\n2: Hermann LK, Weingart SD, Yoon YM, Genes NG, Nelson BP, Shearer PL, Duvall WL, \nHenzlova MJ. Comparison of frequency of inducible myocardial ischemia in patients\npresenting to emergency department with typical versus atypical or nonanginal\nchest pain. Am J Cardiol. 2010 Jun 1;105(11):1561-4. doi:\n10.1016/j.amjcard.2010.01.014. Epub 2010 Apr 10. PubMed PMID: 20494662.\n"},{"metadata":{"_uuid":"bce891ccc9591f95b011ad729bbfcb411f63f048"},"cell_type":"markdown","source":"### exercise induced angina (exang)"},{"metadata":{"trusted":true,"_uuid":"bbd3a9bee22356bdfe28da4619d3053a66aaf96f"},"cell_type":"code","source":"sideplot(df, \"exang\", kind=\"bar\", title=\"Comparison of exercise induced angina\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11a16b7d8a5a2059bfa2f2ea3b6b5f9a74e470e6"},"cell_type":"markdown","source":"## Part II: Basic classification solution\n\nIn this part, I am going to give a basic heart disease classification solution with pytorch.\n\nFirst, let's predict how well the classification model can perform by using linear discriminant analysis (or LDA)  \nThe LDA process similarly as PCA by reducing dimensions of the data, but it tends to make data with different labels most discriminable."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"0cd02fff4c57e8b3bfe24dc48571326ee8e5f1d3"},"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nclf = LDA(n_components=1)\n\ny = df[\"target\"].values\nX = clf.fit(df[df.columns[:-1]].values, y).transform(df[df.columns[:-1]].values)\nX = X[:, 0]\n\nsns.swarmplot(X[y == 0], color=\"b\", label=\"without HD\")\nsns.swarmplot(X[y == 1], color=\"r\", label=\"with HD\")\nplt.title(\"LDA analysis of heart disease classification\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7a116a70bf23eb41a3056699d48118734430ba3"},"cell_type":"markdown","source":"We can see from the LDA graph above that the labelled data are approximately separable.  \nIt seems the model we are going to train will have a comparably good performance."},{"metadata":{"_uuid":"7608996e8286b37c60d72cb4fa10a2053532a99b"},"cell_type":"markdown","source":"### Preprocessing\n**NOTE: PREPROCESSING IS CRITICAL TO IMPROVE ACCURACY!!!**"},{"metadata":{"_uuid":"5106e5943a8baf5bae5e8e094a9e5d2c615d07e4"},"cell_type":"markdown","source":"Before we train the classifier, we should generally:\n\n1. encode the enum-like features, as non-continuous values should not be considered as continuous ones, since they can have some bad effect on the final result.\n2. analyse & drop the outliers.\n3. normalize the features to be centered to 0, as some models will be badly affected by that."},{"metadata":{"_uuid":"707e5c69162fb9c83dfe8732e7a1390c82f7a3b5"},"cell_type":"markdown","source":"First, let's encode the enum-like features."},{"metadata":{"trusted":true,"_uuid":"fac7be13f807dff53267dda896b207b1e9a11ddc"},"cell_type":"code","source":"def onehot(ser, num_classes=None):\n    \"\"\"\n    One-hot encode the series.\n    Example: \n    >>> onehot([1, 0, 2], 3)\n    array([[0., 1., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.]])\n    \"\"\"\n    if num_classes == None:\n        num_classes = len(np.unique(ser))\n    return np.identity(num_classes)[ser]\n\nnew_col_names = []\nneed_encode_col = [\"restecg\", \"thal\", \"slope\", \"cp\"]\nno_encode_col = [col for col in df.columns if col not in need_encode_col]\nnew_df = df[no_encode_col]\nfor col in need_encode_col:\n    num_classes = len(df[col].unique())\n    new_col_names = [f\"{col}_{i}\" for i in range(num_classes)]\n    encoded = pd.DataFrame(onehot(df[col], num_classes), columns=new_col_names, dtype=int)\n    new_df = pd.concat([new_df, encoded], axis=1)\nnew_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d96a19673cdfb7aaa49674c9b267564cc62ca7d6"},"cell_type":"markdown","source":"Then we can look at if there are some outliers, for example, with principle component analysis (or PCA).  \nPCA will try to maximize the variance of data."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"4167a144a0662f6743d76994af15588e53d1f3aa"},"cell_type":"code","source":"from sklearn.decomposition import PCA\nclf = PCA(n_components=2)\ndata_cols = [col for col in new_df.columns if col != \"target\"]\nX = new_df[data_cols]\ny = new_df[\"target\"]\nX_trans = clf.fit(X, y).transform(X)\nsns.scatterplot(X_trans[y == 0][:, 0], X_trans[y == 0][:, 1], color=\"b\", label=\"without HD\")\nsns.scatterplot(X_trans[y == 1][:, 0], X_trans[y == 1][:, 1], color=\"r\", label=\"with HD\")\nplt.title(\"PCA analysis of heart disease classification\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e996467ec1ec4433da4dea24bec90c107b647cce"},"cell_type":"markdown","source":"We can see a handful of outliers at about (320, -20) and maybe around (150, -10).  \nWe can use sklearn.covariance.EllipticEnvelope for example to drop the outliers if your model will be greatly affected by outliers."},{"metadata":{"_uuid":"54e68631a8f70d74d127e88e125a492b87a87ebb"},"cell_type":"markdown","source":"### Neural network"},{"metadata":{"_uuid":"7ee81964324f0496d9e7e6823522c5b3ba0a99dc"},"cell_type":"markdown","source":"First let's train a 3-layer neural network and see what it gave us.\n\nNeural networks are one of the most widely used models today."},{"metadata":{"trusted":true,"_uuid":"e276b9d32a4bcf6976e6cd0eaab0980f652c4dcb"},"cell_type":"code","source":"from sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\n\nnew_df_shfl = shuffle(new_df, random_state=443)\nX = new_df_shfl[data_cols].values\ny = new_df_shfl[\"target\"].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=80)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa210c0fb067cc50555b0211552c3f4591f6a01b"},"cell_type":"code","source":"num_epochs = 5000\nlog_inteval = 250\ntotal_losses = []\ntotal_val_losses = []\nlr = 1e-4\nlr_decay_inteval = 2500\nlr_decay_rate = 0.3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f32a79308bc530c266625ebc1b8284fb31333a18"},"cell_type":"code","source":"import torch\nfrom torch import nn, optim\nmodel = nn.Sequential(\n    nn.Linear(len(data_cols), 80),\n    nn.ReLU(),\n    nn.Dropout(0.3),\n    nn.Linear(80, 256),\n    nn.ReLU(),\n    nn.Dropout(0.6),\n    nn.Linear(256, 1),\n)\nloss_fn = torch.nn.BCELoss()\nopt = optim.Adam(model.parameters(), lr=lr)\n\ndef init_normal(m):\n    if type(m) == nn.Linear:\n        nn.init.xavier_normal_(m.weight, 0.06)\n\nmodel.apply(init_normal)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e9456e194d78c7a17fa822172d8e5a824ab923f","scrolled":false},"cell_type":"code","source":"for epoch in range(1, num_epochs+1):\n    y_pred = model(torch.tensor(X_train, dtype=torch.float))\n    y_pred = torch.sigmoid(y_pred)\n    opt.zero_grad()\n    loss = loss_fn(y_pred[:, 0], torch.tensor(y_train, dtype=torch.float))\n    loss.backward()\n    opt.step()\n    total_losses.append(loss.item())\n    if epoch % log_inteval == 0: # Logging\n        epochs_ran = epoch\n        model.eval()\n        with torch.no_grad():\n            y_pred = model(torch.tensor(X_test, dtype=torch.float))\n            y_pred = torch.sigmoid(y_pred)\n            val_loss = loss_fn(y_pred[:, 0], torch.tensor(y_test, dtype=torch.float))\n            total_val_losses.append(val_loss.item())\n        model.train()\n        print(f\"total loss in epoch {epoch} = {'%.4f'%loss}, validation loss = {'%.4f'%val_loss}, lr = {'%.2e'%lr}\")\n        if len(total_val_losses) > 3 and val_loss.item() > total_val_losses[-2] and val_loss.item() > total_val_losses[-3]:\n            print(f\"Validation loss not improving for {log_inteval * 2} epochs, stopping...\")\n            break\n    if epoch % lr_decay_inteval == 0: # Learning rate decay\n        lr *= lr_decay_rate\n        for param_group in opt.param_groups:\n            param_group['lr'] = lr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7398bdaa9094cda41645b7ec2037e836577d7522"},"cell_type":"code","source":"plt.plot(total_losses, 'b', label=\"train\")\nplt.plot(np.array(range(epochs_ran // log_inteval)) * log_inteval + log_inteval, total_val_losses, 'r', label=\"valid\")\nplt.ylim([0, 1])\nplt.title(\"Learning curve\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"afb8eefbc0620838bb79c1bbc567b52cb2242116","scrolled":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nwith torch.no_grad():\n    model.eval()\n    y_pred = model(torch.tensor(X_test, dtype=torch.float))\n    y_pred_lbl = np.where(y_pred.numpy() > 0, 1, 0)\ncm = pd.DataFrame(confusion_matrix(y_test, y_pred_lbl), columns=[\"T\", \"F\"], index=[\"P\", \"N\"])\nprint(\"Accuracy = %.2f%%\" % ((cm.iloc[1, 1] + cm.iloc[0, 0]) / cm.values.sum() * 100))\ncm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"719021df83fae232abb099d956c6c54a33a5b2b1"},"cell_type":"markdown","source":"### Gradient boost"},{"metadata":{"_uuid":"4b232d44516bd32eef7f9567179fd7cf4e4a575a"},"cell_type":"markdown","source":"Let's use lightgbm to train the model.\n\nLightgbm instead use gradient boost to train a decision tree model."},{"metadata":{"trusted":true,"_uuid":"44ab2f1670e275d3149ea0b20c8e4bd28364ca06"},"cell_type":"code","source":"import lightgbm as lgb\n\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_valid = lgb.Dataset(X_valid, y_valid, reference=lgb_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa210c0fb067cc50555b0211552c3f4591f6a01b"},"cell_type":"code","source":"params = {\n    \"num_iterations\": 1000,\n    \"num_leaves\": 63,\n    \"max_depth\": 7,\n    \"max_bin\": 500,\n    \"learning_rate\": 0.001,\n    \"min_data_in_leaf\": 1,\n    \"objective\": \"binary\",\n    \"metric\": [\"binary\"],\n}\n\nbst = lgb.train(params, lgb_train, valid_sets=[lgb_valid], early_stopping_rounds=50, verbose_eval=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5eb5a2626444a45bd0adb90faae2ede28728ccd8"},"cell_type":"code","source":"y_pred = bst.predict(X_test)\ny_pred_lbl = np.where(y_pred > 0.5, 1, 0)\ncm = pd.DataFrame(confusion_matrix(y_test, y_pred_lbl), columns=[\"T\", \"F\"], index=[\"P\", \"N\"])\nprint(\"Accuracy = %.2f%%\" % ((cm.iloc[1, 1] + cm.iloc[0, 0]) / cm.values.sum() * 100))\ncm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41e5f392a193e77456fffb7050ee313e3e6c14a0"},"cell_type":"markdown","source":"This is my first public kernel.  \nAs a student studying clinical medicine, I put the emphasis on clinical explanation of the data rather than the implementation of classification model.\n\nIf you think this kernel is useful, I would appreciate it if you give me a vote up.  \nIf you find some mistakes, contact me and I will correct them as soon as possible.  \n**Thanks for reading!**"},{"metadata":{"_uuid":"95fa65462b5c922153db67603b4fea1020556e54"},"cell_type":"markdown","source":"TODOS:\n\n1. I saw many kernels have [code/hide] buttons on the upper right of the cell. How to do that?\n2. Fine-tune the model."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}