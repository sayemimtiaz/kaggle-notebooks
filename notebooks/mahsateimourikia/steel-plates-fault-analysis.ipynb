{"cells":[{"metadata":{"_uuid":"df125a13195842a103ae6537f1f20ee362e7a17b"},"cell_type":"markdown","source":"# Steel Plates Fault Analysis\n\nThe goal of this document is to describe the approach used to analyse the dataset related to the Steel Plates Faults at the following [link](http://archive.ics.uci.edu/ml/datasets/steel+plates+faults).\n\nThe dataset includes 1941 observations, and 27 features. The data is already labeled, and there are 7 types of steel plate faults that are added to the dataset as 7 fields representing the one-hot-encoding of the label."},{"metadata":{"_uuid":"74363a2c311feec93b97c258691434f1e7822d1a"},"cell_type":"markdown","source":"## Installations and Usage Giudelines\n\nFor this project I have been using the Anaconda distribution version 1.8.7 with python version 3.6.5. However, the normal installation of python should also work.\n\nHere you may find the required modules that have been used in this project. The codes are written in Python 3."},{"metadata":{"trusted":true,"_uuid":"98ce2e80daf5a1bd92724e8176b380e7e06fe79e"},"cell_type":"code","source":"import numpy\nimport pandas\nimport sklearn\nimport scipy\nimport collections\nimport matplotlib\nimport seaborn\nimport sys","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"9cadedc76420c74fe1cfc9288d1999977518064d"},"cell_type":"code","source":"modules = list(set(sys.modules) & set(globals()))\nfor module_name in modules:\n    module = sys.modules[module_name]\n    print(module_name, getattr(module, '__version__', 'unknown'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c5938089b5d8af1c4ba5ede949cd7ca06573333"},"cell_type":"markdown","source":"For the complete list of requirements and their dependencies please take a look at the __requirements.txt__ file provided. To replicate my exact environment in anaconda distribution you may use the following command using the __machine-learning.yml__ file provided:\n\n    conda env create -f machine-learnin.yml\n\n"},{"metadata":{"_uuid":"9b8528bc71e92d3c4cd3bcafd19259abffe2ef26"},"cell_type":"markdown","source":"## The Analysis\n\nI am going to start by importing the relevant libraries and the dataset. Here Numpy and Pandas libraries are used for computations and handling the dataset. Scikit-Learn has been utilized for the machine learning methods that are used for analyzing the data, and Scipy has provided some more advanced statistical tools that were needed for this analysis."},{"metadata":{"trusted":true,"_uuid":"b0cf373450b4ae9b5ead71e2537123f95e3f94dd"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import DBSCAN, KMeans, AgglomerativeClustering\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom scipy.cluster.hierarchy import fcluster\nfrom scipy.cluster.hierarchy import cophenet\nfrom scipy.spatial.distance import pdist\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\nfrom collections import Counter\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e87471c8fe7f21a4a532a9c022dd6b3236e6e09a"},"cell_type":"markdown","source":"Let's read the dataset and look inside:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b06f2e08582c7788698d5d12f19546ebd5f39382"},"cell_type":"code","source":"data = pd.read_csv('../input/faulty-steel-plates/faults.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1023072356fbd957be81690f14d993d3ba62a4d7"},"cell_type":"markdown","source":"From the data documentation we how that we have no missing values in our dataset. Therefore, there is no need to do the fixing for missing values.\n\nNow that the dataset is created let's take a look at the first 5 rows to see if the data is ok:"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"57fc7331be50b52c98be1c279a0eedd67735b9d5"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c667e1460e41d27bacc0f6e37023ad6b36a60fbf"},"cell_type":"markdown","source":"At the moment the data is consist of 1941 observations and 32 columns from which 7 are the encoded fault types, we can also look at the descriptive statistics on the dataset:"},{"metadata":{"trusted":true,"_uuid":"0af2071cc11eb729ec83c9db8e14bff43ebe0aa6"},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6dfad720fe338fc75cae3078d7b05c618d83503"},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8cf09bc802a8eeedbf094b70a6f57bd5f7155d8"},"cell_type":"markdown","source":"To be able to use the data, I will drop the one-hot encoding of the types and add a single __Target__ column with the fault type names:"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"645ac569dd73f4be9e9f62c8ef8c6b7e6420d2a7"},"cell_type":"code","source":"targets = data.iloc[:, 27:35]\ndata.drop(targets.columns, axis=1, inplace=True)\ndata['Target'] = targets.idxmax(1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25abf100e31c19d8dae72a5953929a67b983b5bd"},"cell_type":"markdown","source":"Let's create a copy of the data that remains unchanged with the pre-procecssing:"},{"metadata":{"trusted":true,"_uuid":"9ec5982083ec20a738d7da4ddd4bae2a79a741e5"},"cell_type":"code","source":"origina_data = data.copy()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8de44ca8b81df1ffa78d5967213eb639c7debc5e"},"cell_type":"markdown","source":"Let's take a look at the classes of faults:"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"dd174bfc64ccab5038eb05e861d5c4b9910c854d"},"cell_type":"code","source":"target_counts= data['Target'].value_counts()\n\nfig, ax = plt.subplots(1, 2, figsize=(15,7))\ntarget_counts_barplot = sns.barplot(x = target_counts.index,y = target_counts.values, ax = ax[0])\ntarget_counts_barplot.set_ylabel('Number of classes in the dataset')\n\ncolors = ['#8d99ae','#ffe066', '#f77f00','#348aa7','#bce784','#ffcc99',  '#f25f5c']\ntarget_counts.plot.pie(autopct=\"%1.1f%%\", ax=ax[1], colors=colors)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"724793c7d084d0a30bb158ba57826e88d9e21ae0"},"cell_type":"markdown","source":"There are 7 fault classes as follows: Dirtiness, Stains, Pastry, Z_Scratch, K_Scratch, Bumps, and Other_Faults.\n\nIt can be seen that there are many observations (34.7%) that are labeled as __Other_Faults__. And besides __Bumps__, and __k_Scatch__ that cover considerable number of observations, other fault types do not have very high number of observations. This shows that in this set of observations the other four fault types do not accure often. However, this might cause an issue as classification methods might not perform well in recognizing the less populated faults. Therefore, a balanced sampling method might provide some improvements in the performance of the classification method. However, it should be studied and tested to give a final opinion.\n\nIn the following, I would like to look at the pairplot to take a look at the distribution of the features. The pairplot will be very crowded as there are many features to be examined. However, with zooming in it is possible to get some ideas:"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"4dc065b8bf93e4c02fcb6e6c0f8fa03798600b41"},"cell_type":"code","source":"sns.pairplot(data, hue='Target')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d6b0da59750aaa3f51d3bf3b28a9105b050b500"},"cell_type":"markdown","source":"From the pair plot I can see that the _TypeOfSteel_A300_, _TypeOfSteel_A400_ and _Outside_Global_Index_ are actually categorical featrues:"},{"metadata":{"trusted":true,"_uuid":"44f8e91851a57d5910e31fdf08511fe3eb186e93"},"cell_type":"code","source":"data['TypeOfSteel_A300'] = data['TypeOfSteel_A300'].astype('category',copy=False)\ndata['TypeOfSteel_A400'] = data['TypeOfSteel_A400'].astype('category',copy=False)\ndata['Outside_Global_Index'] = data['Outside_Global_Index'].astype('category',copy=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7382f233ed48f8d269b7f0c25304441795fde203"},"cell_type":"markdown","source":"Also, I could notice some correlations between some of the features. To investigate it more deeply I will use Peason's Correlation of the numerical features to see if the features are mutually independent. I use a heatmap to visualize the correlation:"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"8346c782daf4dfbbbdacb1cd01c95d37e1684952"},"cell_type":"code","source":"plt.figure(figsize=(20,15))\nsns.heatmap(data.corr(), cmap='seismic')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bca1d80fae8aedce6892bd12ec525e58cf8568c3"},"cell_type":"markdown","source":"From the heatmap, it can be seen that there are some features that are highly correlated (shown in dark red for positive correlation and dark blue for negative correlation), with each other and cannot be considered as indipendent variables that we need for modeling. The features with white or very light colors however, can be condidered as indipendent. Let's examine some of the examples of correlated features below:"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"2c4c4db00c65b27c6c4c55a84457a5ac2c8da321"},"cell_type":"code","source":"sns.regplot(x='X_Minimum', y='X_Maximum', data = data, scatter = True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"5c2aa83fed87269e8ef22be7353a12a8be792ac7"},"cell_type":"code","source":"sns.regplot(x='Pixels_Areas', y='Sum_of_Luminosity', data = data, scatter = True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"73f32e39b6d85b61482673061874fed9e135a2a3"},"cell_type":"code","source":"sns.regplot(x='Maximum_of_Luminosity', y='Luminosity_Index', data = data, scatter = True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"366dff2bc762b9b394c6ede3ffdb309cde0096c4"},"cell_type":"code","source":"sns.regplot(x='Edges_Y_Index', y='Log_X_Index', data = data, scatter = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8acb6a02fae5b7ded62945a231285d9390a55d6"},"cell_type":"markdown","source":"The linear relationship between the two features shows their correlation. To resolve this issue the Principal Component Analysis (PCA) method might come handy to produce the indipendent components. However, the issue with this method is that the results of the classification based on PCA components is not easily interpretable based on the features."},{"metadata":{"_uuid":"e511919515f221372f304dbdfc12d6d791b51cb8"},"cell_type":"markdown","source":"## Feature Skewness\n\nWhen we talk about normality what we mean is that the data should look like a normal distribution. This is important because several statistic tests rely on this (e.g. t-statistics).\n\nIf the dataset is skewed, then the Machine Learning model wouldnâ€™t be able to do a good job on predictions. To resolve the issue of the skewed features we can apply the a log transform of the same data, or to use the Box-Cox Transformation. Let's see how skewed are our numerical features:"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"d67aa60e88327cee3b71eb0c6af57fdc0e1ba4ab"},"cell_type":"code","source":"numeric_features = data.dtypes[data.dtypes != \"object\"].index\n\nskewed_features = data[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nskewed_features_df = pd.DataFrame(skewed_features, columns={'Skew'})\nskewed_features_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"3ac776edeaeec34cf605366488228962efc43f30"},"cell_type":"code","source":"skewed_features_df.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fde25fd315428958cde6c9f803d80f70c95f8b4"},"cell_type":"markdown","source":"I remove our categorical features from the list before applying the Box-Cox transformation:"},{"metadata":{"trusted":true,"_uuid":"d2071c1e51ecb7a28ec97ab511750e42045132c8"},"cell_type":"code","source":"skewed_features_df.drop(['TypeOfSteel_A400','TypeOfSteel_A300', 'Outside_Global_Index'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49f7789f3ee65dd30b2e73c8a6e14c3c43ebfe8b"},"cell_type":"markdown","source":"From what we can see, there are many features that are skewed, therefore, a method to resolve this issue might provide better performances in many of the models that would be applied on this dataset. Let's see one of the skewed features and compare the results after applying the Box-Cox Transformation:"},{"metadata":{"trusted":true,"_uuid":"5a28cd88db449fe513580c70765be6494659a3aa"},"cell_type":"code","source":"sns.distplot(data['Sum_of_Luminosity'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b31b14ac52b970f2f86479b2fc6e9b8d219c807d"},"cell_type":"markdown","source":"Now I apply the Box-Cox transformation that is available in scipy special module:"},{"metadata":{"trusted":true,"_uuid":"4b0ab755f2c68b1dce2aa37c14e36e2d1fcfb27a"},"cell_type":"code","source":"skewed_features_df = skewed_features_df[abs(skewed_features_df) > 0.75]\n\nfrom scipy.special import boxcox1p\nlam = 0.15\ncols = skewed_features_df.index\n\nfor c in cols:\n    data[c] = boxcox1p(data[c], lam)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56b43d72d7dd23c23236a20c423767b63d20851d"},"cell_type":"code","source":"sns.distplot(data['Sum_of_Luminosity'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1661827efff26e88f32fd4b6450992fe050a4323"},"cell_type":"markdown","source":"It can be seen that the sample feature that we examined is now very close to a normal distribution."},{"metadata":{"_uuid":"35d341593d72071ed16283dae64e01c42442a5cc"},"cell_type":"markdown","source":"## Scaling the Features\n\nIn many models the features need to be scaled for the model to perform well. Features with high values may effect the performance of many models. I will first separate the features and the target from our dataset and then apply the standard scaler on the features:"},{"metadata":{"trusted":true,"_uuid":"a2209b2429fa11358b7c0257897aa476cb39f3d4"},"cell_type":"code","source":"features = data.drop('Target', axis=1)\ntarget = data['Target']\n\nscaler = StandardScaler()\nfeatures_scaled = pd.DataFrame(scaler.fit_transform(features), columns=features.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e726daadff45b3a2471e9db72ddf801df0f58772"},"cell_type":"markdown","source":"## Outlier Analysis\n\nFrom the pairplot that was examined previously, I also noticed many outliers. There are basically three types of outliers:\n* __Point Outliers:__ observations anomalous with respect to the majority of observations in a feature (aka univariate outlier).\n* __Contextual Outliers:__ observations considered anomalous given a specific context.\n* __Collective Outliers:__ a collection of observations anomalous but appear close to one another because they all have a similar anomalous value.\n\nIn what follows I try to analyse the outliers and remove them if possible. \n\nIn a more thorough analysis it would be useful to utilize the expert's knowledge to understand if the found outliers should be actually removed or they are revealing useful information.\n\n### Univariate Method:\n\nTukey Boxplot is used to detect unusually high or low data points, which can be considered as potential outliers to be investigated further. Even if it would be very crowded, I would like to take a look of the boxplot of the all features to get an idea about how many outliers we are talking about:"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"11639667a0ec658b64d526306e15dfc748ff5171"},"cell_type":"code","source":"data_boxplot = features_scaled.boxplot(return_type='dict', vert=False, figsize=(20,20))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bcef2b8ee00f588da13e02a284c639ff392fffe2"},"cell_type":"markdown","source":"It seems that there are many outliers in different features that is expected as we are looking at the fault data. However, the outliers should be studied, and if they are real outliers they should be removed. Let's first look at the extreme outliers:"},{"metadata":{"trusted":true,"_uuid":"ab7465a5a38be14af8613ac33ef61dfd2f3ca6e2"},"cell_type":"code","source":"features_scaled[features_scaled['Pixels_Areas']>4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38a5f2a96fa2d4639d26a28f68cf73b9e79d1387"},"cell_type":"code","source":"features_scaled[features_scaled['Sum_of_Luminosity']>4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33b4162c2ffb2d804736b7361babc6a99150f41b"},"cell_type":"code","source":"features_scaled[features_scaled['X_Perimeter']>4]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"612693822ff188e8a982f1b5800f43b7f1ed6a84"},"cell_type":"code","source":"features_scaled[features_scaled['Y_Perimeter']>4]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12f58506632f3d1c00204d97cceee8bb70b913b1"},"cell_type":"markdown","source":"It seems that these outliers are related to the same observations. This is not very suprising as we have seen previously that these 4 features are highly correlated. \n\nThe univariate study shows that there are many outliers, however, more study is required to decide how to manage them. Therefore, it should be taken into consideration in a more thorough analysis."},{"metadata":{"_uuid":"64a8866562317ed370c37958ad163ae557d16688"},"cell_type":"markdown","source":"### Multivariate Method"},{"metadata":{"_uuid":"fb15c10e7b4b244dc02277542dc306bd31ee2f45"},"cell_type":"markdown","source":"I would also like to do a multivariate analysis to reveal the outliers in the features with regards to the target classes. \n\nTo go forward with the analysis, I create the scaled dataset from the scaled features and I add the target, and also a numeric code to represent the target class:"},{"metadata":{"trusted":true,"_uuid":"e31fd0ec36b15aa799186c39e27de850c3aa59c5"},"cell_type":"code","source":"data_scaled = features_scaled.copy()\ndata_scaled['Target'] = target\n\ndata_scaled['Target'] = pd.Categorical(data_scaled['Target'])\ndata_scaled['Target_Code'] = data_scaled.Target.cat.codes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2bf2bb3e9ad41d27c6f02c45c5b412bda8029534"},"cell_type":"markdown","source":"After some try and error I selected some boxplots that will give some information about the amount of outliers that we are facing:"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"113ad15904ae4d9d4c209eeed1437ac74a843a8c"},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.boxplot(x='Target', y='X_Maximum', data=data_scaled)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"c7d42bb3168ea94c71bee4e516a864936263aaad"},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.boxplot(x='Target', y='Steel_Plate_Thickness', data=data_scaled)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"ffc6ceb70bf56b3596ecb75986970a08dbe5c74b"},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.boxplot(x='Target', y='Luminosity_Index', data=data_scaled)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"0c3b83f6bd45895bc49986098289dd6abb0b7881"},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.boxplot(x='Target', y='Square_Index', data=data_scaled)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"b333c2107fe9fb026ba722aac5ae20a25febb6fd"},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.boxplot(x='Target', y='Edges_Index', data=data_scaled)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"915feb8c845526549e9b97e162dff37f3eb34a29"},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.boxplot(x='Target', y='LogOfAreas', data=data_scaled)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"cad0f8e59a05e103d41ee9c11ecdfecc0569369b"},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.boxplot(x='Target', y='Y_Maximum', data=data_scaled)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"2f7d523bc4ff71f9714b6d16330f2c44703f8c03"},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.boxplot(x='Target', y='Orientation_Index', data=data_scaled)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"abc9317d11ecb97652924026eb014b2c19a14614"},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.boxplot(x='Target', y='Minimum_of_Luminosity', data=data_scaled)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"e98b24a5c370d7f64f2d8d53f535233e7f78f9c7"},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.boxplot(x='Target', y='Length_of_Conveyer', data=data_scaled)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9b106e321f86b41d1ed7c5c2354d6cb2e6f66db"},"cell_type":"markdown","source":"As it can be seen in the plots, there are many outliers, including some very extreme ones that would reduce the performance of the models. As in the data documentations they aren't enough information on the features and expert knowledge cannot not be accessed at the current time, it is not possible to examine the outliers to decide which ones can be removed. In the following, I will use DBSCAN as a method to remove the outliers in a more automatic way:"},{"metadata":{"_uuid":"eed3beefc22fca58ae21181ecc544335c3605e7f"},"cell_type":"markdown","source":"### DBSCAN for Outlier Detection\n\nDBSCAN is a unsupervised method that clusters core samples and denotes non-core samples. It can be used to identify collective outliers. I start from $eps =0.1$ and increase it untill I arrive to maximum 5% of the dataset to be considered as outliers. _eps_ is the maximum distance between two samples for them to be considered as in the same neighborhood. I also play with the _min_samples_ parameter which defiens the minimum number of the samples in a cluster."},{"metadata":{"trusted":true,"_uuid":"a1fde44a049f38237faa9cce6c465ccf2cf03c6a"},"cell_type":"code","source":"dbscan_model = DBSCAN(eps=3.3, min_samples=7).fit(features_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b12a3543a561f7b1d63d0bf459de9a0dee2853a"},"cell_type":"code","source":"print(Counter(dbscan_model.labels_))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82c62bcc1e3ae16eae24231f218503976b6a7e42"},"cell_type":"markdown","source":"Here the DBSCAN has found 95 outliers which is about 4.89% of our dataset which is acceptable as we should remain under 5% when detecting the outliers. This is not the optimal output that was expected, as it wasn't possible to detect more clusters and there was always too many observations considered as outliers.\n\nAs there aren't any other information to decide about the outliers I will simply drop the outliers from the dataset:"},{"metadata":{"trusted":true,"_uuid":"49aa24c9539824e36f4d3f36e988f04e496d77a3"},"cell_type":"code","source":"outliers = features_scaled[dbscan_model.labels_ == -1]\noutliers.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"9a9b7d9d5b9bca78ee7f000f1ede0d2185acc468"},"cell_type":"code","source":"features_scaled.drop(outliers.index, axis=0, inplace=True)\ntarget.drop(outliers.index, axis=0, inplace=True)\ndata_scaled.drop(outliers.index, axis=0, inplace=True)\nfeatures_scaled.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc363b6b3ce676d14a4421d6c69497ab66d75372"},"cell_type":"markdown","source":"## Principal Component Analysis"},{"metadata":{"_uuid":"5d15e4b586df52f47d75401917d25d707afb9dee"},"cell_type":"markdown","source":"As it has been discusssed before, there are many correlated variables in the dataset. Therefore, PCA might help in creating independent components that can be used in the final models.\n\nHere I try to run the PCA to create components representing all the features. I will use this to select the right number of components that better represent the variation in the dataset:"},{"metadata":{"trusted":true,"_uuid":"cca89471e264cc6ebbf31713f0f3f5bbca6a532d"},"cell_type":"code","source":"pca = PCA(random_state=101)\nfeatures_pca = pca.fit_transform(features_scaled.values)\npd.DataFrame(pca.explained_variance_ratio_, columns=['Explained Variance Ratio'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc78299c8c1d59fb97b4d8fba3422bb4cf1dfc55"},"cell_type":"code","source":"pca.explained_variance_ratio_[0:15].sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de79e3d53344f06c4364b2ecb62c1c12fe7b0122"},"cell_type":"markdown","source":"It seems that we can explain the 99% of the variability in the data set using the first 15 components from the PCA.\n\nLet's see how the PCA components correlate with our featuers:"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"5e9433c4e6aef9d14b3f693138f8b8153ac2c78c"},"cell_type":"code","source":"pca_components = pd.DataFrame(pca.components_, columns= features.columns)\nplt.figure(figsize=(20,20))\nsns.heatmap(pca_components, cmap='seismic')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb47b548b3035411a69454372de0c7d8acb9794b"},"cell_type":"markdown","source":"Now I choose the number of components to 15, and run the PCA again to create the PCA components:"},{"metadata":{"trusted":true,"_uuid":"619a364201be53f4e2781df941b920b399049433"},"cell_type":"code","source":"def pca_dataset(features, n_components):\n    \n    pca_n = PCA(n_components=n_components, random_state=101)\n    features_pca_n = pca_n.fit_transform(features)\n    \n    column_pca = []\n    for i in range(0,n_components):\n        column_pca.append('Component'+np.str(i))\n    return pd.DataFrame(features_pca_n, columns=column_pca)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d535df1bee1e58b0ba3dd8a3f306921cde1fa794"},"cell_type":"markdown","source":"I create the dataset based on the results, I simply call the columns 'Component0'...'Component14':"},{"metadata":{"trusted":true,"_uuid":"4989740ac7d65da0f9eba16cc74ce2fd366a4d40"},"cell_type":"code","source":"data_pca15 = pca_dataset(features_scaled, n_components=15)\ndata_pca15['Target'] = target","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b394838cee862a1b4d1e585faf4ef00163a5c9e"},"cell_type":"markdown","source":"Let's run the paiplot one more time to see how the components are distributed:"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"826fda0628e1c0c1b0e3204e0b798b24470d6751"},"cell_type":"code","source":"sns.pairplot(data_pca15, hue='Target')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97d75cffb2b28832e7c5f57fe1554b2898187e14"},"cell_type":"markdown","source":"Here we can see the components are mutually independent from one another. However, still it seems like the last components do not play an important role in separating the classes. I will also create another dataset using only 5 components that make up to the 76% variability as can be seen below:"},{"metadata":{"trusted":true,"_uuid":"38af936210537a5aa7115bc4cfc06c82c04334f0"},"cell_type":"code","source":"pca.explained_variance_ratio_[0:5].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61e1872b193ce021767cb83efe73141acdb5cdff"},"cell_type":"code","source":"data_pca5 = pca_dataset(features_scaled, n_components=5)\ndata_pca5['Target'] = target","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b29475db088e9e1189f8dab833b47bf961e730c"},"cell_type":"markdown","source":"I will also add the target codes to the created dataset which will be useful later on:"},{"metadata":{"trusted":true,"_uuid":"34753a2ef3108dc5edd89bb211228afd9f7759d1"},"cell_type":"code","source":"data_pca15['Target'] = pd.Categorical(data_pca15['Target'])\ndata_pca15['Target_Code'] = data_pca15.Target.cat.codes\n\ndata_pca5['Target'] = pd.Categorical(data_pca5['Target'])\ndata_pca5['Target_Code'] = data_pca5.Target.cat.codes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4f0abacc8831577ebb968788051289cb0b934b4"},"cell_type":"markdown","source":"## Cluster Analysis\n\n### Grouping with K-Means Clustering\n\nThe K-Means clustering algorithm is a simple unsupervised algorithm that's used for quickly predicting groupings."},{"metadata":{"_uuid":"5334fcfa604268295adf9c693349a63b09ddb982"},"cell_type":"markdown","source":"Now let's take a look at the groups that we can identify by K-Means. We already know that we have 7 types of fault. Therefore, we can consider the number of clusters to be 7."},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"50ece9fd0333d5f416773649a7d02e8f582bdf08"},"cell_type":"code","source":"kmeans_model = KMeans(n_clusters=7, random_state=54)\nkmeans_model.fit(features_scaled)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5854f0fe2fb8f1ce8705b8c29125e09944cf5adc"},"cell_type":"markdown","source":"To see the results, I will choose one example features and we can compare how the ground truth classification (with the labels) will compare to the clusters that are recognized with the K-Means:"},{"metadata":{"trusted":true,"_uuid":"964df5966a7462f5a61f0ca57e4c18c7bb291e2f"},"cell_type":"code","source":"kmeans_labels = np.choose(kmeans_model.labels_, [0,1,2,3,4,5,6]).astype(np.int64)\ndata_scaled['kmeans_labels'] = kmeans_labels","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"bcfb5c5454fd7f4298f1316050c641132bcf1f2e"},"cell_type":"code","source":"color_themes = {0:'#8d99ae',1:'#ffe066', 2:'#f77f00',3:'#348aa7',4:'#bce784',5:'#ffcc99',  6:'#f25f5c'}\n\n\nsns.lmplot(x='Orientation_Index', y='Log_X_Index', data=data_scaled, fit_reg=False, hue='Target', col='Target', size=8)\nplt.title(\"Ground Truth Classification\")\n\nsns.lmplot(x='Orientation_Index', y='Log_X_Index', data=data_scaled,  fit_reg=False, hue='kmeans_labels', col='kmeans_labels',size=8)\nplt.title(\"KMean Clustering\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02d8025f7cf5073548bda7ba06c2550869ca9179"},"cell_type":"markdown","source":"From this example we can see that K-Means could some how find the right clusters however it doesn't seem close enough. Let's look at the classification metrics to get a more detailed evaluation:"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"297a3bf448ee690bb0f021b04c2eb3cb7eb145d8"},"cell_type":"code","source":"print(classification_report(data_scaled['Target_Code'], kmeans_labels))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"553002b7f83f95fcf04b4ce59260d166dceedcae"},"cell_type":"markdown","source":"As the results show the precision and recall suffer, and K-Means is not able to cluster the data accurately. Let's try the same approach using the 15 PCA components that we selected:"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"53ddf3712077ea417ccd17c6334c49cffbe0d6fe"},"cell_type":"code","source":"kmeans_model_pca15 = KMeans(n_clusters=7, random_state=54)\nkmeans_model_pca15.fit(data_pca15.drop(['Target','Target_Code'], axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6206ab84407f830d25364042a7448b3c49acb97c"},"cell_type":"code","source":"kmeans_labels_pca15 = np.choose(kmeans_model.labels_, [0,1,2,3,4,5,6]).astype(np.int64)\ndata_pca15['kmeans_labels'] = kmeans_labels_pca15","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f2c383882728c661eeb550e96ba324bf93b4fa9"},"cell_type":"code","source":"sns.lmplot(x='Component0', y='Component1', data=data_pca15, fit_reg=False, hue='Target', col='Target', size=8)\nplt.title(\"Ground Truth Classification\")\n\nsns.lmplot(x='Component0', y='Component1', data=data_pca15,  fit_reg=False, hue='kmeans_labels', col='kmeans_labels',size=8)\nplt.title(\"KMean Clustering\")","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"30343a49c5b4fad1ed3c5e06838915557a59116f"},"cell_type":"code","source":"print(classification_report(data_pca15['Target_Code'], kmeans_model_pca15.labels_))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cda46bc1dd521f8142cd23b5a39b84a349d8e70e"},"cell_type":"markdown","source":"It can be seen from the results that the 15 PCA components have led to much better precision and recall in K-Means. Let's also try with the 5 PCA components:"},{"metadata":{"trusted":true,"_uuid":"e4fb2bd0d633e327997f3ce8076dff08b030849c"},"cell_type":"code","source":"kmeans_model_pca5 = KMeans(n_clusters=7, random_state=54)\nkmeans_model_pca5.fit(data_pca5.drop(['Target','Target_Code'], axis=1))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"c0be6c264790fbf9583bc93f238aa7c9a7d0e139"},"cell_type":"code","source":"print(classification_report(data_pca5['Target_Code'], kmeans_model_pca15.labels_))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb8baf1ed4d169199dd49fec8a77100876f409de"},"cell_type":"markdown","source":"We can see the using the 5 PCA components we arrive at the same results. Therefore, it can be considered to use less components would not impact the performance of the model negatively. Let's now try another clustering method that might lead to better results compared to K-Means:"},{"metadata":{"_uuid":"d170872a69eca1d2b081d3cf6b1ed7ab2e546947"},"cell_type":"markdown","source":"### Hierarchical Clustering Method\n\nHierarchical clustering methods predict subgroups within data by finding the distance between each data point and its nearest neighbors. As we have irregular classes that are not normally distributed, this method might have an advantage to K-Means in producing more accurate clusters.\n\nHowever, as the original shape of the distribution is important here and this model doesn't perform well on normal distributions, I will use the original dataset without the transformations and scaling for this model.\n\nFirst, I use scipy yo generate dendograms:"},{"metadata":{"trusted":true,"_uuid":"c8e032d647586e7e9970d79cec31c0b90282be66"},"cell_type":"code","source":"original_features = origina_data.drop(['Target'], axis=1).copy()\norigina_data['Target'] = pd.Categorical(origina_data['Target'])\norigina_data['Target_Code'] = origina_data.Target.cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96448d422e65008f7e0adfaeca9d8563d2d18291"},"cell_type":"code","source":"linkage_model = linkage(original_features, method='ward')\ndendrogram(linkage_model, truncate_mode='lastp', p=12, leaf_rotation=45, leaf_font_size=12, show_contracted=True)\nplt.title('Truncated Hierarchical Clustering Dendrogram')\nplt.xlabel('Cluster Size')\nplt.ylabel('Distance')\n\nplt.axhline(y=0.4*10**(8))\nplt.axhline(y=0.2*10**(8))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e87f9383e7efb83f073721f1a53a53c88b4caecb"},"cell_type":"markdown","source":"As it can be seen in the dendrogram, the closest that we can arrive to the desired 7 clusters is if we deside to have the distance around 0.2le8 which is quite strange. Let's see how the hierarchical clustering will perform here:\n\n### Generating hierarchical clusters"},{"metadata":{"trusted":true,"_uuid":"0ff77b333fdaf86796ed74c672429e176b00f2e8"},"cell_type":"code","source":"k = 7\nh_clustering = AgglomerativeClustering(n_clusters=k, affinity='euclidean', linkage='ward')\nh_clustering.fit(original_features)\n\naccuracy_score(origina_data['Target_Code'], h_clustering.labels_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"29e2e7d79d2f8d421b473dbd36b2542fcdda8308"},"cell_type":"code","source":"h_clustering = AgglomerativeClustering(n_clusters=k, affinity='manhattan', linkage='complete' )\nh_clustering.fit(original_features)\n\naccuracy_score(origina_data['Target_Code'], h_clustering.labels_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"742afe74e157fe5137c45d015f8d07809103e145"},"cell_type":"code","source":"h_clustering = AgglomerativeClustering(n_clusters=k, affinity='manhattan', linkage='average')\nh_clustering.fit(original_features)\n\naccuracy_score(origina_data['Target_Code'], h_clustering.labels_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11ab371bf34bcb42ff046708cafe8c86d174e31b"},"cell_type":"markdown","source":"After some try and error, it seems like the distance metric Euclidean and the linkage \"Ward\" work the best on our data, however, the results are still far from desired. Let's also use the PCA components and see if we can get better results:"},{"metadata":{"trusted":true,"_uuid":"c2c946f91c5f4edcee22f9a517b312d2c0d50d4c"},"cell_type":"code","source":"k = 7\nh_clustering_pca5 = AgglomerativeClustering(n_clusters=k, affinity='euclidean', linkage='ward' )\nh_clustering_pca5.fit(data_pca5.drop(['Target','Target_Code'], axis=1))\n\naccuracy_score(data_pca5['Target_Code'], h_clustering_pca5.labels_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8a6edf201887591652aacd20cf9b8e08adcd2ff"},"cell_type":"markdown","source":"Using the 5 selected components of the PCA method, it can be seen the results are far from satisfying."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":1}