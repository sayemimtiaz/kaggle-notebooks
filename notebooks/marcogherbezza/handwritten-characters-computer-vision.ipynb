{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  **Welcome my friends**","metadata":{}},{"cell_type":"markdown","source":"**Import libraries**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport math\nimport seaborn as sns\nimport random\nimport cv2 as cv\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout, Activation\nfrom keras import optimizers\nfrom keras_preprocessing.image import ImageDataGenerator\nfrom keras.models import load_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Read csv file**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/english-handwritten-characters-dataset/english.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**What are the classes?**","metadata":{}},{"cell_type":"code","source":"classes = df['label'].unique()\nquantity = len(df['label'].unique())\nprint(f'\\n The %d Classes are: \\n {classes}' %quantity)    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create validation and testing dataset**","metadata":{}},{"cell_type":"code","source":"#Create a variable that contains the general file path\nDATAPATH = '../input/english-handwritten-characters-dataset'\n#Read another time the csv file\ndataset = pd.read_csv(DATAPATH + '/english.csv')\n#Pick randomly 600 values\nrand = random.sample(range(len(dataset)), 300)\n#Create validation set of data\nvalidation_set = pd.DataFrame(dataset.iloc[rand,:].values, columns = ['image','label'])\n#Drop Validation data from dataset\ndataset.drop(rand, inplace = True)\n#Pick randomly 15 values\nrand = random.sample(range(len(validation_set)), 15)\n#Create test set of data\ntest_set = pd.DataFrame(validation_set.iloc[rand,:].values, columns = ['image','label'])\n#Drop Test data from validation set\nvalidation_set.drop(rand, inplace = True)\n#Show Validation dataset\nvalidation_set","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data Augmentation","metadata":{}},{"cell_type":"code","source":"train_data_generator = ImageDataGenerator(\n            rescale=1/255,\n            shear_range=0.2,\n            zoom_range=0.2,\n            width_shift_range=0.2,\n            height_shift_range=0.2,)\ndata_generator = ImageDataGenerator(rescale = 1/255)\ntraining_data_frame = train_data_generator.flow_from_dataframe( \n                                            dataframe = dataset, \n                                            directory = DATAPATH,\n                                            x_col = 'image',\n                                            y_col = 'label',\n                                            target_size = (64,64),\n                                            classe_mode = 'categorical')\nvalidation_data_frame = data_generator.flow_from_dataframe( \n                                            dataframe = validation_set, \n                                            directory = DATAPATH + '',\n                                            x_col = 'image',\n                                            y_col = 'label',\n                                            target_size = (64,64),\n                                            classe_mode = 'categorical')\ntest_data_frame = data_generator.flow_from_dataframe( \n                                            dataframe = test_set, \n                                            directory = DATAPATH,\n                                            x_col = 'image',\n                                            y_col = 'label',\n                                            target_size = (64,64),\n                                            classe_mode = 'categorical')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = keras.Sequential([\n        layers.BatchNormalization(),\n        # Add first Convolutional Layer\n        layers.Conv2D(filters=32, kernel_size=3, activation='relu',\n                      padding='same', input_shape=(64,64,3)),\n        # Add a second Convolutional Layer\n        layers.Conv2D(filters=32, kernel_size=3, activation='relu'),\n        # Add a Max pooling layer\n        layers.MaxPool2D(pool_size=(2, 2)),\n        # Add a Dropout layer\n        layers.Dropout(0.25),\n    \n        layers.BatchNormalization(),\n        # Add third Convolutional Layer\n        layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding = 'same'),\n        # Add Fourth Convolutional Layer\n        layers.Conv2D(filters=32, kernel_size=3, activation='relu'),\n        # Add a Max pooling layer\n        layers.MaxPool2D(pool_size=(2, 2)),\n        # Add a Dropout Layer\n        layers.Dropout(0.25),\n\n        layers.BatchNormalization(),\n        # Add Fifth Convolutional Layer\n        layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding = 'same'),\n        # Add a sixth Convolutional Layer\n        layers.Conv2D(filters=32, kernel_size=3, activation='relu'),\n        # Add a Max Pooling Layer\n        layers.MaxPool2D(pool_size=(2, 2)),\n        # Add a Dropout Layer\n        layers.Dropout(0.25),\n\n        layers.BatchNormalization(),\n        # Add a Flatten Layer\n        layers.Flatten(),\n        # Add a Dense layer Layer\n        layers.Dense(512, activation = 'relu'),\n        # Add a Dropout Layer\n        layers.Dropout(0.5),\n        # Add the Output Dense Layer\n        layers.Dense(62, activation='softmax')\n])\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(epsilon=0.01),\n    loss= 'categorical_crossentropy',\n    metrics = ['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train the module**","metadata":{}},{"cell_type":"code","source":"history = model.fit(training_data_frame, validation_data = validation_data_frame, \n                    epochs = 50\n                )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(50)\n\nplt.figure(figsize=(15, 15))\nplt.subplot(2, 2, 1)\nplt.plot(epochs_range, accuracy, label='Training Accuracy')\nplt.plot(epochs_range, val_accuracy, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model as model.h5\nmodel.save('model.h1')\n# Load the model\nmodel = load_model('model.h1')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the class indices \nprint(\"Prediction Dict: \", training_data_frame.class_indices)\n# Predict on the test data \npred = model.predict(test_data_frame)\n# Create a class/labels dictionary\nclassDict = {\n            0: \"0\", 1: \"1\", 2: \"2\", 3: \"3\", 4: \"4\", 5: \"5\", 6: \"6\", 7: \"7\", 8: \"8\", 9: \"9\", 10: \"A\",\n            11: \"B\", 12: \"C\", 13: \"D\", 14: \"E\", 15: \"F\", 16: \"G\", 17: \"H\", 18: \"I\", 19: \"J\", 20: \"K\",\n            21: \"L\", 22: \"M\", 23: \"N\", 24: \"O\", 25: \"P\", 26: \"Q\", 27: \"R\", 28: \"S\", 29: \"T\", 30: \"U\",\n            31: \"V\", 32: \"W\", 33: \"X\", 34: \"Y\", 35: \"Z\", 36: \"a\", 37: \"b\", 38: \"c\", 39: \"d\", 40: \"e\",\n            41: \"f\", 42: \"g\", 43: \"h\", 44: \"i\", 45: \"j\", 46: \"k\", 47: \"l\", 48: \"m\", 49: \"n\", 50: \"o\",\n            51: \"p\", 52: \"q\", 53: \"r\", 54: \"s\", 55: \"t\", 56: \"u\", 57: \"v\", 58: \"w\", 59: \"x\", 60: \"y\",\n            61: \"z\"}\n\n# Make a data frame that contains the probability for each class\noutputDf = pd.DataFrame(pred)\n# Get the index of the max probability from the output Data frame\nmaxIndex = list(outputDf.idxmax(axis=1))\n# Print the max index\nprint(\"Max index: \", maxIndex)\n# Make a loop in range the length of the test data (20)\nfor i in range(len(test_set)):\n    # Read the image \n    image = cv.imread(DATAPATH + '/' + test_set.at[i, 'image'])\n    # The title of the plot which is the predicted label\n    plt.title(classDict.get(maxIndex[i], \"error\"))\n    # Show the actual image\n    plt.imshow(image)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}