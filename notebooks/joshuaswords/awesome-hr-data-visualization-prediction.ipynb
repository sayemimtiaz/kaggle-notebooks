{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# HR Data visualization, analysis, and prediction\n\n","metadata":{}},{"cell_type":"markdown","source":"Welcome!\n\nThis will be an extensive visualization, analysis, and prediction notebook. \n\nHere are some of my other notebooks:\n\n**Gold price prediction using Prophet**\n\nhttps://www.kaggle.com/joshuaswords/eda-gold-price-prediction-prophet\n\n\n**Computer Vision with FastAI - Pneumonia prediction**\n\nhttps://www.kaggle.com/joshuaswords/computer-vision-pneumonia-prediction-fastai\n\n\n**Stroke Prediction with SMOTE and LIME explainer**\n\nhttps://www.kaggle.com/joshuaswords/predicting-a-stroke-95-acc-with-lime-explainer\n\n\n**2021 World Happiness Index EDA**\n\nhttps://www.kaggle.com/joshuaswords/awesome-eda-2021-happiness-population\n\n\nLet's get to it...\n\n\n\n# Context\n\n**Problem Statement**: Predict the probability of a candidate looking for a new job.\n\nEssentially, I'll be attempting to predict **who is job-seeking and who is not.** \n\n**Note** - The main focus of this notebook is on **data visualization** so the predictive models can almost certainly be improved upon with some basic tweaks. \n\nRemember though, in industry it is difficult to deploy models that take hours to retrain daily. A good model is often better - and more practical - than a perfect one. Although this should be assessed on a case-by-case basis of course.\n\n","metadata":{}},{"cell_type":"code","source":"\nimport numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport warnings\nwarnings.filterwarnings(\"ignore\")        \nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport matplotlib.gridspec as grid_spec\nimport seaborn as sns\nimport squarify\nfrom sklearn import preprocessing\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n\n\nplt.rcParams[\"font.family\"] = \"serif\"\nplt.rcParams['figure.dpi'] = 200","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the data","metadata":{}},{"cell_type":"code","source":"aug_train = pd.read_csv('../input/hr-analytics-job-change-of-data-scientists/aug_train.csv')\naug_test = pd.read_csv('../input/hr-analytics-job-change-of-data-scientists/aug_test.csv')\naug_train.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dealing with missing values\n\nAs I have said in previous notebooks, dealing with missing values is often subjective. \n\nI like to keep categorical data where I can, as perhaps the fact that we do not have a company size or the company type could actually tell us something.\n\nAdditionally, I will keep the null gender values as of these, 30% have a target of 1 in the dataset, so this could be a telling data point, even if we don't quite understand it yet. \n\nAs a reminder:\n**Target: 0 – Not looking for job change, 1 – Looking for a job change**","metadata":{}},{"cell_type":"code","source":"# Check for nulls\naug_train.isna().sum()/len(aug_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I like to keep categorical data where I can, as perhaps the fact that we do not have a company size or the company type\n# could actually tell us something.\n\n\naug_train['last_new_job'] = aug_train['last_new_job'].apply(lambda x: 'Never' if x == 'never' else x) #just reads nicer\naug_train['enrolled_university'][aug_train['enrolled_university'] == 'no_enrollment'] = 'No Enrollment' #just reads nicer\naug_train['company_size'] = aug_train['company_size'].apply(lambda x: '10-49' if x == '10/49' else x) #diff replacement method\n\naug_train['experience'] = aug_train['experience'].apply(lambda x: '0' if x == '<1' else x)\naug_train['experience'] = aug_train['experience'].apply(lambda x: '20' if x == '>20' else x)\n\n\naug_train['company_size'].fillna('0',inplace=True)\naug_train['company_type'].fillna('Unknown',inplace=True)\naug_train['major_discipline'].fillna('Unknown',inplace=True)\naug_train['gender'].fillna('Not provided',inplace=True)\n\n\n# Repeat steps on Test set\n\naug_test['last_new_job'] = aug_test['last_new_job'].apply(lambda x: 'Never' if x == 'never' else x) #just reads nicer\naug_test['enrolled_university'][aug_test['enrolled_university'] == 'no_enrollment'] = 'No Enrollment' #just reads nicer\naug_test['company_size'] = aug_test['company_size'].apply(lambda x: '10-49' if x == '10/49' else x) #diff replacement method\n\naug_test['experience'] = aug_test['experience'].apply(lambda x: '0' if x == '<1' else x)\naug_test['experience'] = aug_test['experience'].apply(lambda x: '20' if x == '>20' else x)\n\naug_test['company_size'].fillna('0',inplace=True)\naug_test['company_type'].fillna('Unknown',inplace=True)\naug_test['major_discipline'].fillna('Unknown',inplace=True)\naug_test['gender'].fillna('Not provided',inplace=True)\n\n\n\n# This is what I mean with respect to the gender categories:\n\naug_train['target'].groupby(aug_train['gender']).mean()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I will drop the remaining missing data\n\naug_train.dropna(inplace=True)\naug_test.dropna(inplace=True)\naug_train.isna().sum()/len(aug_train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting objects to integers\n\naug_train['experience'] = aug_train['experience'].astype(str).astype(int)\naug_test['experience'] = aug_test['experience'].astype(str).astype(int)\n\naug_train['training_hours'] = aug_train['training_hours'].astype(str).astype(int)\naug_test['training_hours'] = aug_test['training_hours'].astype(str).astype(int)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I like to order my categorical variables. This gives much more control over your visuals and enables you to tell the story you're trying to tell","metadata":{}},{"cell_type":"code","source":"# Orders\n\ned_order = ['Primary School','High School','Graduate','Masters','Phd']\nenroll_order = ['No Enrollment','Part time course','Full time course']\ndisc_order = ['STEM','Unknown','Humanities','Other','Business Degree','Arts','No Major']\nexp_yrs_order = ['<1','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','>20']\nexp_yrs_order_2 = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\nsize_order = ['0','<10', '10-49', '50-99', '100-500', '500-999', '1000-4999', '5000-9999', '10000+']\njob_order = ['Never', '1', '2', '3', '4', '>4']\nexp_order =['No relevant experience','Has relevant experience']\ngender_order = ['Male','Female','Other','Not provided']\ncompany_order = ['Pvt Ltd','Unknown','Funded Startup','Public Sector','Early Stage Startup','NGO','Other']\n \n# for visuals\ntrain_df = aug_train\ntest_df = aug_test\n\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Visualization \n\n**This will be an EXPLORATORY visualization, as opposed to EXPLANATORY.**\n\n\nI will also make use of **GridSpec** as I want to practice this technique. \n\n\nLet's see if we can understand why people might look for a new job...\n\n\nThe colour palette I will use is below:","metadata":{}},{"cell_type":"code","source":"sns.palplot(['#002d1d','#0e4f66','gray','#fbfbfb'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# How many job-seekers are there?\n\nFirst of all, I want to see how many job-seekers there are in our training set. \n\nDo we have a balanced dateset? Or Imbalanced? The answer to these question may influence our models later on.","metadata":{}},{"cell_type":"code","source":"x=train_df.groupby(['target'])['target'].count()\ny=len(train_df)\nr=((x/y)).round(2)\nratio = pd.DataFrame(r).T\n\n\nfig, ax = plt.subplots(1,1,figsize=(6.5, 2),dpi=150)\nbackground_color = \"#fbfbfb\"\nfig.patch.set_facecolor(background_color)\nax.set_facecolor(background_color) \n\nax.barh(ratio.index, ratio[1.0], color='#0e4f66', alpha=0.9, ec=background_color, label='Job-Seeker')\nax.barh(ratio.index, ratio[0.0], left=ratio[1.0], color='gray', alpha=0.9,ec=background_color, label='Non Job-Seeker')\n\nax.set_xlim(0, 1)\nax.set_xticks([])\nax.set_yticks([])\nax.legend().set_visible(False)\nfor s in ['top', 'left', 'right', 'bottom']:\n    ax.spines[s].set_visible(False)\n    \nfor i in ratio.index:\n    ax.annotate(f\"{int(ratio[1.0][i]*100)}%\", xy=(ratio[1.0][i]/2, i),va = 'center', ha='center',fontsize=32, fontweight='light', fontfamily='serif',color='white')\n    ax.annotate(\"Job-Seeker\", xy=(ratio[1.0][i]/2, -0.25),va = 'center', ha='center',fontsize=12, fontweight='light', fontfamily='serif',color='white')\n    \n    \nfor i in ratio.index:\n    ax.annotate(f\"{int(ratio[0.0][i]*100)}%\", xy=(ratio[1.0][i]+ratio[0.0][i]/2, i),va = 'center', ha='center',fontsize=32, fontweight='light', fontfamily='serif',color='white')\n    ax.annotate(\"Non Job-Seeker\", xy=(ratio[1.0][i]+ratio[0.0][i]/2, -0.25),va = 'center', ha='center',fontsize=12, fontweight='light', fontfamily='serif',color='white')\n\n\nfig.text(0.125,1.1,'How many are looking for a new role?', fontfamily='serif',fontsize=15, fontweight='bold')\nfig.text(0.125,0.915,'We see an imbalanced dataset;\\nmost people are not job-seeking',fontfamily='serif',fontsize=12)  \n\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have an imbalanced dataset - that is,cmany **more non job-seekers than job-seekers**.\n\nThis is a problem that we can address later. For now, let's continue **exploring the data**","metadata":{}},{"cell_type":"markdown","source":"# How do the Train & Test set compare to each other?","metadata":{}},{"cell_type":"markdown","source":"Here I'll show an example of how you can use GridSpec.\n\nGridSpec enables you to plot multiple plots and is highly customisable - a skill worth picking up!\n\nThis is really valuable because you can convey a lot of information in a small amount of space. \n\nI'll compare the Train & Test sets here...","metadata":{}},{"cell_type":"code","source":"background_color = \"#fbfbfb\"\n\nfig = plt.figure(figsize=(22,15),dpi=150)\nfig.patch.set_facecolor(background_color)\ngs = fig.add_gridspec(3, 3)\ngs.update(wspace=0.35, hspace=0.27)\nax0 = fig.add_subplot(gs[0, 0])\nax1 = fig.add_subplot(gs[0, 1])\nax2 = fig.add_subplot(gs[0, 2])\nax3 = fig.add_subplot(gs[1, 0])\nax4 = fig.add_subplot(gs[1, 1])\nax5 = fig.add_subplot(gs[1, 2])\nax6 = fig.add_subplot(gs[2, 0])\nax7 = fig.add_subplot(gs[2, 1])\nax8 = fig.add_subplot(gs[2, 2])\n\n\n# Ax0 - EDUCATION LEVEL\ntrain = pd.DataFrame(train_df[\"education_level\"].value_counts())\ntrain[\"Percentage\"] = train[\"education_level\"].apply(lambda x: x/sum(train[\"education_level\"])*100)\ntrain = train.sort_index()\n\ntest = pd.DataFrame(test_df[\"education_level\"].value_counts())\ntest[\"Percentage\"] = test[\"education_level\"].apply(lambda x: x/sum(test[\"education_level\"])*100)\ntest = test.sort_index()\n\nax0.bar(np.arange(len(train.index)), height=train[\"Percentage\"], zorder=3, color=\"gray\", width=0.05)\nax0.scatter(np.arange(len(train.index)), train[\"Percentage\"], zorder=3,s=200, color=\"gray\")\nax0.bar(np.arange(len(test.index))+0.4, height=test[\"Percentage\"], zorder=3, color=\"#0e4f66\", width=0.05)\nax0.scatter(np.arange(len(test.index))+0.4, test[\"Percentage\"], zorder=3,s=200, color=\"#0e4f66\")\nax0.text(-0.5, 68.5, 'Education Level', fontsize=14, fontweight='bold', fontfamily='serif', color=\"#323232\")\nax0.yaxis.set_major_formatter(mtick.PercentFormatter())\nax0.yaxis.set_major_locator(mtick.MultipleLocator(10))\nax0.set_xticks(np.arange(len(train.index))+0.4 / 2)\nax0.set_xticklabels(list(train.index),rotation=0)\n\n\n# Ax1 - ENROLLED IN UNIVESITY\ntrain = pd.DataFrame(train_df[\"enrolled_university\"].value_counts())\ntrain[\"Percentage\"] = train[\"enrolled_university\"].apply(lambda x: x/sum(train[\"enrolled_university\"])*100).loc[enroll_order]\ntest = pd.DataFrame(test_df[\"enrolled_university\"].value_counts())\ntest[\"Percentage\"] = test[\"enrolled_university\"].apply(lambda x: x/sum(test[\"enrolled_university\"])*100).loc[enroll_order]\n\nax1.text(0, 2.5, 'University Enrollment', fontsize=14, fontweight='bold', fontfamily='serif', color=\"#323232\")\nax1.barh(train.index, train['Percentage'], color=\"gray\", zorder=3, height=0.6)\nax1.barh(test.index, test['Percentage'], color=\"#0e4f66\", zorder=3, height=0.4)\nax1.xaxis.set_major_formatter(mtick.PercentFormatter())\nax1.xaxis.set_major_locator(mtick.MultipleLocator(10))\n\n###\n# Ax2 - GENDER \ntrain = pd.DataFrame(train_df[\"gender\"].value_counts())\ntrain[\"Percentage\"] = train[\"gender\"].apply(lambda x: x/sum(train[\"gender\"])*100)\ntest = pd.DataFrame(test_df[\"gender\"].value_counts())\ntest[\"Percentage\"] = test[\"gender\"].apply(lambda x: x/sum(test[\"gender\"])*100)\n\nx = np.arange(len(train))\nax2.text(-0.6, 76, 'Gender', fontsize=14, fontweight='bold', fontfamily='serif', color=\"#323232\")\nax2.grid(color='gray', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\nax2.bar(x, height=train[\"Percentage\"], zorder=3, color=\"gray\", width=0.4)\nax2.bar(x+0.4, height=test[\"Percentage\"], zorder=3, color=\"#0e4f66\", width=0.4)\nax2.set_xticks(x + 0.4 / 2)\nax2.set_xticklabels(['Male','Female','Other','Not provided'])\nax2.yaxis.set_major_formatter(mtick.PercentFormatter())\nax2.yaxis.set_major_locator(mtick.MultipleLocator(10))\nfor i,j in zip([0, 1], train[\"Percentage\"]):\n    ax2.annotate(f'{j:0.0f}%',xy=(i, j/2), color='#f6f6f6', horizontalalignment='center', verticalalignment='center')\nfor i,j in zip([0, 1], test[\"Percentage\"]):\n    ax2.annotate(f'{j:0.0f}%',xy=(i+0.4, j/2), color='#f6f6f6', horizontalalignment='center', verticalalignment='center')\n    \n\n    \n## Ax 3 - CDI\n\nax3.grid(color='gray', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\ntrain = pd.DataFrame(train_df[\"city_development_index\"])\ntest = pd.DataFrame(test_df[\"city_development_index\"])\nsns.kdeplot(train[\"city_development_index\"], ax=ax3,color=\"gray\", shade=True, label=\"Train\")\nsns.kdeplot(test[\"city_development_index\"], ax=ax3, color=\"#0e4f66\", shade=True, label=\"Test\")\nax3.text(0.29, 13, 'City Development Index', fontsize=14, fontweight='bold', fontfamily='serif', color=\"#323232\")\nax3.yaxis.set_major_locator(mtick.MultipleLocator(2))\nax3.set_ylabel('')    \nax3.set_xlabel('')\n\n## AX4 - TITLE\n\nax4.spines[\"bottom\"].set_visible(False)\nax4.tick_params(left=False, bottom=False)\nax4.set_xticklabels([])\nax4.set_yticklabels([])\nax4.text(0.5, 0.6, 'How do our\\n\\n datasets compare?', horizontalalignment='center', verticalalignment='center',fontsize=22, fontweight='bold', fontfamily='serif', color=\"#323232\")\nax4.text(0.28,0.57,\"Train\", fontweight=\"bold\", fontfamily='serif', fontsize=22, color='gray')\nax4.text(0.5,0.57,\"&\", fontweight=\"bold\", fontfamily='serif', fontsize=22, color='#323232')\nax4.text(0.58,0.57,\"Test\", fontweight=\"bold\", fontfamily='serif', fontsize=22, color='#0e4f66')\n\n\n\n\n### Ax5 - RELEVANT EXPERIENCE\ntrain = pd.DataFrame(train_df[\"relevent_experience\"].value_counts())\ntrain[\"Percentage\"] = train[\"relevent_experience\"].apply(lambda x: x/sum(train[\"relevent_experience\"])*100)\ntest = pd.DataFrame(test_df[\"relevent_experience\"].value_counts())\ntest[\"Percentage\"] = test[\"relevent_experience\"].apply(lambda x: x/sum(test[\"relevent_experience\"])*100)\nx = np.arange(len(train))\nax5.text(-0.4, 80, 'Relevant Experience', fontsize=14, fontweight='bold', fontfamily='serif', color=\"#323232\")\nax5.grid(color='gray', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\nax5.bar(x, height=train[\"Percentage\"], zorder=3, color=\"gray\", width=0.4)\nax5.bar(x+0.4, height=test[\"Percentage\"], zorder=3, color=\"#0e4f66\", width=0.4)\nax5.set_xticks(x + 0.4 / 2)\nax5.set_xticklabels(['No relevant experience','Has relevant experience'])\nax5.yaxis.set_major_formatter(mtick.PercentFormatter())\nax5.yaxis.set_major_locator(mtick.MultipleLocator(10))\n\nfor i,j in zip([0, 1, 2], train[\"Percentage\"]):\n    ax5.annotate(f'{j:0.0f}%',xy=(i, j/2), color='#f6f6f6', horizontalalignment='center', verticalalignment='center')\nfor i,j in zip([0, 1, 2], test[\"Percentage\"]):\n    ax5.annotate(f'{j:0.0f}%',xy=(i+0.4, j/2), color='#f6f6f6', horizontalalignment='center', verticalalignment='center')\n\n    \n    \n# Ax6 - TRAINING HOURS\ntrain = pd.DataFrame(train_df[\"training_hours\"])\ntrain[\"TrainTest\"] = \"Train\"\ntest = pd.DataFrame(test_df[\"training_hours\"])\ntest[\"TrainTest\"] = \"Test\"\nax6.text(-0.65, 370, 'Training Hours', fontsize=14, fontweight='bold', fontfamily='serif', color=\"#002d1d\")\ncomb_graph_temp_df = pd.concat([train, test], axis=0)\nsns.boxenplot(ax=ax6, y=\"training_hours\", x=\"TrainTest\", data=comb_graph_temp_df, palette=[\"gray\", \"#0e4f66\"])\nax6.set_xlabel(\"\")\nax6.set_ylabel(\"\")\n\n\n# Ax7 - EXPERIENCE YRS\ntrain = pd.DataFrame(train_df[\"experience\"].value_counts())\ntrain[\"Percentage\"] = train[\"experience\"].apply(lambda x: x/sum(train[\"experience\"])*100)\ntrain = train.sort_index()\ntest = pd.DataFrame(test_df[\"experience\"].value_counts())\ntest[\"Percentage\"] = round(test[\"experience\"].apply(lambda x: x/sum(test[\"experience\"])*100),).astype(int)\ntest = test.sort_index()\nax7.grid(color='gray', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\nax7.plot(train.index, train[\"Percentage\"], zorder=3, color=\"gray\", marker='o')\nax7.plot(test.index, test[\"Percentage\"], zorder=3, color=\"#0e4f66\", marker='o')\nax7.text(-1.5, 20.5, 'Years Experience', fontsize=14, fontweight='bold', fontfamily='serif', color=\"#323232\")\nax7.set_yticklabels(labels = ['0   ', '5%','10%','15%'])\nax7.xaxis.set_major_locator(mtick.MultipleLocator(5))\nax7.yaxis.set_major_locator(mtick.MultipleLocator(5))\n\n\n# Ax8 - MAJOR DISCIPLINE\ntrain = pd.DataFrame(train_df[\"major_discipline\"].value_counts())\ntrain[\"Percentage\"] = train[\"major_discipline\"].apply(lambda x: x/sum(train[\"major_discipline\"])*100)\ntest = pd.DataFrame(test_df[\"major_discipline\"].value_counts())\ntest[\"Percentage\"] = test[\"major_discipline\"].apply(lambda x: x/sum(test[\"major_discipline\"])*100)\n\nax8.barh(np.arange(len(train.index)), train[\"Percentage\"], zorder=3, color=\"gray\", height=0.4)\nax8.barh(np.arange(len(test.index))+0.4, test[\"Percentage\"], zorder=3, color=\"#0e4f66\", height=0.4)\nax8.text(-5, -0.8, 'Major Discipline', fontsize=14, fontweight='bold', fontfamily='serif', color=\"#323232\")\nax8.xaxis.set_major_formatter(mtick.PercentFormatter())\nax8.yaxis.set_major_locator(mtick.MultipleLocator(1))\nax8.set_yticks(np.arange(len(test.index))+0.4 / 2)\nax8.set_yticklabels(list(test.index))\nax8.invert_yaxis()\n\n\n\nfor i in range(0,9):\n    locals()[\"ax\"+str(i)].set_facecolor(background_color) \n    \nfor i in range(0,9):\n    locals()[\"ax\"+str(i)].tick_params(axis=u'both', which=u'both',length=0)\n\n\nfor s in [\"top\",\"right\",\"left\"]:\n    for i in range(0,9):\n        locals()[\"ax\"+str(i)].spines[s].set_visible(False)\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Train & Tests sets are similar - that's good news\n\nIf the training set has wildy different characteristics to our test set then we really are in for a difficult time.\n\nWe'd need to ask if the training population can really help us predict the target. \n\nIn this case though, we're fine. \n\n# Now let's focus on the Training set and explore the data...","metadata":{}},{"cell_type":"markdown","source":"You'll note that I often incoroprate text in to my visuals. I'll often an annotations to the plots themselves, for example at the 'mean', or at peaks in the data etc. \n\nIn this case, I've included an explanation of what we're seeing and what it might mean. This helps your audience to understand your data, but it also helps them to get thinking in a way that is in line with the story you are trying to craft.","metadata":{}},{"cell_type":"code","source":"color_palette=[\"gray\",\"#0e4f66\"]\nfig = plt.figure(figsize=(18,15), dpi=150)\nfig.patch.set_facecolor(background_color) # figure background color\ngs = fig.add_gridspec(3, 3)\ngs.update(wspace=0.4, hspace=0.6)\nax0 = fig.add_subplot(gs[0, 0])\nax1 = fig.add_subplot(gs[0, 1])\nax2 = fig.add_subplot(gs[1, 0])\nax3 = fig.add_subplot(gs[1, 1])\nax4 = fig.add_subplot(gs[2, 0])\nax5 = fig.add_subplot(gs[2, 1])\n\n# Distribution\nax0.text(-1, 19000, 'Who is looking for a new job?', fontsize=20, fontweight='bold', fontfamily='serif', color=\"#323232\")\nax0.text(-1, 17500, 'Most job-seekers appear to be male', fontsize=14, fontweight='light', fontfamily='serif', color=\"#323232\")\nax0.text(-1, 14050, 'Overall', fontsize=14, fontweight='bold', fontfamily='serif', color=\"#323232\")\nsns.countplot(x=train_df[\"gender\"], color=\"#247747\", ax=ax0, zorder=3,alpha=0.9)\n\n\n\n# Gender\nax1.text(-1, 11000, 'Job searching by gender', fontsize=14, fontweight='bold', fontfamily='serif', color=\"#323232\")\nsns.countplot(x=\"gender\", hue=\"target\", data=train_df, palette=color_palette, ax=ax1, zorder=3)\nlegend_labels, _= ax1.get_legend_handles_labels()\nax1.legend(legend_labels, [\"Non-Job Seeker\", \"Job Seeker\"], ncol=2, bbox_to_anchor=(-0.52, 1.28), facecolor=background_color, edgecolor=background_color)\n\n\n\n# CDI\nax2.text(0.3, 16, 'Does the City Development Index play a role?', fontsize=20, fontweight='bold', fontfamily='serif', color=\"#323232\")\nax2.text(0.3, 14.5, 'Interestingly, we see Job Seekers are frequently from cities with a lower CDI score', fontsize=14, fontweight='light', fontfamily='serif', color=\"#323232\")\nax2.text(0.3, 13, 'Overall', fontsize=14, fontweight='bold', fontfamily='serif', color=\"#323232\")\nsns.kdeplot(train_df[\"city_development_index\"], color=\"#247747\", shade=True, ax=ax2, zorder=3)\n\n\n\nax3.text(0.33, 15.5, 'Job Seeker / Non-Job Seeker', fontsize=14, fontweight='bold', fontfamily='serif', color=\"#323232\")\nsns.kdeplot(train_df.loc[(train_df[\"target\"]==0), \"city_development_index\"], color=\"gray\", label=\"Not Survived\", ax=ax3)\nsns.kdeplot(train_df.loc[(train_df[\"target\"]==1), \"city_development_index\"], color=\"#0e4f66\", label=\"Survived\", ax=ax3)\n\n\n\n###\naug_train['count'] = 1\njob_hunt_only = aug_train[aug_train['target']==1]\nno_job_hunt_only = aug_train[aug_train['target']==0]\n\njob_change = aug_train.groupby(['education_level','last_new_job'])['experience'].sum().unstack().loc[ed_order,job_order]\n\njob_hunt_only.groupby(['target','last_new_job'])['count'].sum().unstack()\nnotseek_job_change = no_job_hunt_only.groupby(['target','last_new_job'])['count'].sum().unstack().T\nseek_job_change = job_hunt_only.groupby(['target','last_new_job'])['count'].sum().unstack().T\n\nnotseek_job_change.columns = ['count']\nseek_job_change.columns = ['count']\n\nnotseek_job_change[\"percentage\"] = notseek_job_change[\"count\"].apply(lambda x: x/sum(notseek_job_change[\"count\"])) *100\nseek_job_change[\"percentage\"] = seek_job_change[\"count\"].apply(lambda x: x/sum(seek_job_change[\"count\"])) *100\n\n\ned_notseek_job_change = no_job_hunt_only.groupby(['target','education_level'])['count'].sum().unstack().T.loc[ed_order]\ned_seek_job_change = job_hunt_only.groupby(['target','education_level'])['count'].sum().unstack().T.loc[ed_order]\n\ned_notseek_job_change.columns = ['count']\ned_seek_job_change.columns = ['count']\n\ned_notseek_job_change[\"percentage\"] = ed_notseek_job_change[\"count\"].apply(lambda x: x/sum(ed_notseek_job_change[\"count\"])) *100\ned_seek_job_change[\"percentage\"] = ed_seek_job_change[\"count\"].apply(lambda x: x/sum(ed_seek_job_change[\"count\"])) *100\n\n###\n\n\nax4.barh(notseek_job_change.index, notseek_job_change['percentage'], color=\"gray\", zorder=3, height=0.7)\nax4.barh(seek_job_change.index, seek_job_change['percentage'], color=\"#0e4f66\", zorder=3, height=0.3)\nax4.xaxis.set_major_locator(mtick.MultipleLocator(10))\n\n\n##\nax5.barh(ed_notseek_job_change.index, ed_notseek_job_change['percentage'], color=\"gray\", zorder=3, height=0.7)\nax5.barh(ed_seek_job_change.index, ed_seek_job_change['percentage'], color=\"#0e4f66\", zorder=3, height=0.3)\nax5.xaxis.set_major_locator(mtick.MultipleLocator(10))\n\n##\nax4.text(-1, 5.7, 'Last job change (yrs)',fontsize=15, fontweight='bold', fontfamily='serif',color='#323232')\nax5.text(0, 4.55, 'Education level', fontsize=15, fontweight='bold', fontfamily='serif',color='#323232')\n\nax4.text(-2.5, 7.5, 'Are there other differences?', \n         fontsize=20, fontweight='bold', fontfamily='serif',color='#323232')\n\nax4.text(-2.5, 6.75, \n         'We see broadly similar patterns, but notable areas of difference', \n         fontsize=14, fontweight='light', fontfamily='serif')\n\n\n####\n\nfig.text(0.77, 0.89\n         , 'Insight', fontsize=15, fontweight='bold', fontfamily='serif',color='#323232')\n\nfig.text(0.77, 0.39, '''\nWe note that most job-seekers are Male.\nThis is not all that surprising as in this\ndataset Males make up the majority of the\nsample population.\n\nWhat is more interesting though is\nthe City Development Index (CDI) chart.\nThere we see that there are two peaks\nfor job-seekers. \nThe peaks are at high and low CDI scores. \n\n\nWe can ponder why this might be;\nin high CDI areas perhaps there are a \nlot of opportunities and therefore\npeople feel encouraged to seek better roles.\n\n\nPerhaps in lower CDI areas candidates\nwant to improve their circumstances by\nsearching for new jobs, maybe in new areas. \n\n\nThis is all conjecture, but interesting\nnonetheless.\n\nIt is also interesting to see that job-seekers\nhave changed job more often that non-job seekers\nwithin that past 1 year, and also\nthose that have never looked for a job\nalso seem to be ready for a new\nchallenge.\n'''\n         , fontsize=14, fontweight='light', fontfamily='serif',color='#323232')\n\n\n\nimport matplotlib.lines as lines\nl1 = lines.Line2D([0.7, 0.7], [0.1, 0.9], transform=fig.transFigure, figure=fig,color='black',lw=0.2)\nfig.lines.extend([l1])\n\nfor s in [\"top\",\"right\",\"left\"]:\n    for i in range(0,6):\n        locals()[\"ax\"+str(i)].spines[s].set_visible(False)\n        \nfor i in range(0,6):\n        locals()[\"ax\"+str(i)].set_facecolor(background_color)\n        locals()[\"ax\"+str(i)].tick_params(axis=u'both', which=u'both',length=0)\n        locals()[\"ax\"+str(i)].grid(color='gray', linestyle=':', axis='y', zorder=0,  dashes=(1,5))      \n\n        \nfor x in range(0,4):\n    for y in range(0,4):\n        locals()[\"ax\"+str(x)].set_xlabel(\"\")\n        locals()[\"ax\"+str(y)].set_ylabel(\"\")\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Let's now explore other factors like company size & employee experience...\n\nDo more experienced employees seek new challenges?\n\nDo employees at larger companies feel less valued?\n\nDo employees at smaller companies crave new opportunities?\n\nBoth seem plausible. These are the questions that a good EDA can answer!\n","metadata":{}},{"cell_type":"code","source":"pv_gen_size = pd.pivot_table(aug_train, values='count',index=['gender'],columns=['company_size'],aggfunc=np.sum).loc[gender_order, size_order]\nct_gen_size = pd.crosstab(aug_train['company_size'],aug_train['experience'], normalize='index').loc[size_order,exp_yrs_order_2]\n\n\nfig = plt.figure(figsize=(15,10), dpi=150) \nfig.patch.set_facecolor(background_color)\ngs = fig.add_gridspec(2, 1)\ngs.update(wspace=0, hspace=-0.09)\nax0 = fig.add_subplot(gs[:,:])\n\ncolors = [\"#fbfbfb\", \"#4b4b4c\",\"#0e4f66\"]\ncolormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\nsns.heatmap(ax=ax0, data=ct_gen_size, linewidths=.1, vmin=0, vmax=0.075,\n            square=True, cbar=False, cmap=colormap,linewidth=3, annot=True, fmt='1.0%',annot_kws={\"fontsize\":14})\n\n \nax0.set_facecolor(background_color) \nax0.set_xlabel(\"Employee Experience [Years]\",fontfamily='serif',fontsize=12,loc='left')\nax0.set_ylabel(\"\")\n\nfor s in [\"top\",\"right\",\"left\"]:\n    ax0.spines[s].set_visible(False)\n\n    \nax0.text(0, -1.4, \n         'Company size & employee experience', \n         fontsize=20, \n         fontweight='bold', \n         fontfamily='serif',\n        )\n\nax0.text(0, -0.9, \n         'Those with over 20 yrs experience dominate the work force at all company sizes.', \n         fontsize=13, \n         fontweight='light', \n         fontfamily='serif',\n        )\nax0.text(0, -0.5, \n         'We also observe some heat around the lower experience range and at smaller companies.', \n         fontsize=13, \n         fontweight='light', \n         fontfamily='serif',\n        )\n\nax0.tick_params(axis=u'both', which=u'both',length=0)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It appears the as employee experience increases, they tend to work for larger companies.\n\nWhy might this be? Perhaps larger companies pay better, or perhaps more job security.\n\n# Employee Experience & Company Size\n\nIs there a noticeable difference between job-seekers and non-job-seekers?","metadata":{}},{"cell_type":"code","source":"job_hunt_only = aug_train[aug_train['target']==1]\nno_job_hunt_only = aug_train[aug_train['target']==0]\n\njob_seek = pd.crosstab(job_hunt_only['company_size'],job_hunt_only['experience'], normalize='index').loc[size_order,exp_yrs_order_2]\nno_seek = pd.crosstab(no_job_hunt_only['company_size'],no_job_hunt_only['experience'], normalize='index').loc[size_order,exp_yrs_order_2]\n\n###\nfig = plt.figure(figsize=(14,14),dpi=150)\nfig.patch.set_facecolor(background_color)\ngs = fig.add_gridspec(2, 3)\ngs.update(wspace=0.2, hspace=0.3)\nax0 = fig.add_subplot(gs[0,:])\nax1 = fig.add_subplot(gs[1,:])\n\n\ncolors = [\"#fbfbfb\", \"#4b4b4c\",\"#0e4f66\"]\ncolormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\nsns.heatmap(ax=ax0, data=job_seek, linewidths=.1, vmin=0, vmax=0.075,\n            square=True, cbar_kws={\"orientation\": \"horizontal\"}, cbar=False, cmap=colormap,linewidth=3, annot=False, fmt='1.0%',annot_kws={\"fontsize\":14})\n\nsns.heatmap(ax=ax1, data=no_seek, linewidths=.1, vmin=0, vmax=0.075,\n            square=True, cbar_kws={\"orientation\": \"horizontal\"}, cbar=False, cmap=colormap,linewidth=3, annot=False, fmt='1.0%',annot_kws={\"fontsize\":14})\n\nax0.set_facecolor(background_color) \nax0.set_xlabel(\"\")\nax0.set_ylabel(\"\")\n\nfor s in [\"top\",\"right\",\"left\"]:\n    ax0.spines[s].set_visible(False)\n\n    \nax0.set_xlabel(\"\")\nax0.set_ylabel(\"\")\nax1.set_xlabel(\"Employee Experience [Years]\",fontfamily='serif',fontsize=14,loc='left')\nax1.set_ylabel(\"\")\n\nfor s in [\"top\",\"right\",\"left\"]:\n    ax0.spines[s].set_visible(False)\n    ax1.spines[s].set_visible(False)\n\n    \nax0.text(0, -1.3, \n         'Job Seekers: Company size & employee experience', \n         fontsize=20, \n         fontweight='bold', \n         fontfamily='serif',\n        )\n\nax1.text(0, -0.7, \n         'Those with over 20 yrs experience dominate the work force at all company sizes.', \n         fontsize=13, \n         fontweight='light', \n         fontfamily='serif',\n        )\nax1.text(0, -0.35, \n         'People with 20+ years of experience are not seeking new roles; perhaps they are at their desired seniority level.', \n         fontsize=13, \n         fontweight='light', \n         fontfamily='serif',\n        )\n    \nax1.text(0, -1.3, \n         'Non-Job Seekers: Company size & employee experience', \n         fontsize=20, \n         fontweight='bold', \n         fontfamily='serif',\n        )\n\nax0.text(0, -0.7, \n         'We see that most job seekers have between 2 - 6 years experience.', \n         fontsize=13, \n         fontweight='light', \n         fontfamily='serif',\n        )\nax0.text(0, -0.35, \n         'Anecdotally this seems correct; people build their skills in the first few years of their career then seek new challenges.', \n         fontsize=13, \n         fontweight='light', \n         fontfamily='serif',\n        )\n\nax0.tick_params(axis=u'both', which=u'both',length=0)\nax1.tick_params(axis=u'both', which=u'both',length=0)\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Education\n\nWe've explored some interesting feautures of our data, including employee experience and company size. \n\nLet's throw education in to the mix. \n\nDo job-seekers have a higher education level? Are they less educated? Or is there no difference at all?","metadata":{}},{"cell_type":"code","source":"job_hunt_only = aug_train[aug_train['target']==1]\nno_job_hunt_only = aug_train[aug_train['target']==0]\n\njob_seek = pd.crosstab(job_hunt_only['education_level'],job_hunt_only['company_type'], normalize='index').loc[ed_order,company_order]\nno_seek = pd.crosstab(no_job_hunt_only['education_level'],no_job_hunt_only['company_type'], normalize='index').loc[ed_order,company_order]\n\n\nfig = plt.figure(figsize=(15,15),dpi=150)\nfig.patch.set_facecolor(background_color)\n\ngs = fig.add_gridspec(1, 2)\ngs.update(wspace=0.05, hspace=0.3)\nax0 = fig.add_subplot(gs[0,0])\nax1 = fig.add_subplot(gs[0,1])\n\n\ncolors = [\"#fbfbfb\", \"#4b4b4c\",\"#0e4f66\"]\ncolormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\nsns.heatmap(ax=ax0, data=job_seek, linewidths=.1, vmin=-0.01, vmax=0.075,\n            square=True, cbar_kws={\"orientation\": \"horizontal\"}, cbar=False, cmap=colormap,linewidth=3, annot=True, fmt='1.0%',annot_kws={\"fontsize\":14})\n\nsns.heatmap(ax=ax1, data=no_seek, linewidths=.1, vmin=-0.01, vmax=0.075,\n            square=True, cbar_kws={\"orientation\": \"horizontal\"}, cbar=False,yticklabels=False, cmap=colormap,linewidth=3, annot=True, fmt='1.0%',annot_kws={\"fontsize\":14})\n\n\nax0.set_facecolor(background_color)\nax1.set_facecolor(background_color) \nax0.set_xlabel(\"\")\nax0.set_ylabel(\"\")\n\nfor s in [\"top\",\"right\",\"left\"]:\n    ax0.spines[s].set_visible(False)\n\n\nax1.set_xlabel(\"\")\nax1.set_ylabel(\"\")\n\nfor s in [\"top\",\"right\",\"left\"]:\n    ax0.spines[s].set_visible(False)\n    ax1.spines[s].set_visible(False)\n    \n    \nax0.text(0, -1, \n         'Education level & company type', \n         fontsize=20, \n         fontweight='bold', \n         fontfamily='serif',\n        )\n\nax0.text(0, -0.6, \n         'Job Seekers', \n         fontsize=15, \n         fontweight='bold', \n         fontfamily='serif',\n        )\n\nax0.text(0, -0.2, \n         'Seem to be at earlier stages of their education.', \n         fontsize=13, \n         fontweight='light', \n         fontfamily='serif',\n        )\n\n\n###\n\nax1.text(0, -0.6, \n         'Non-Job Seekers', \n         fontsize=15, \n         fontweight='bold', \n         fontfamily='serif',\n        )\nax1.text(0, -0.2, \n         'We see a slightly higher education level.', \n         fontsize=13, \n         fontweight='light', \n         fontfamily='serif',\n        )\n    \n\nax0.tick_params(axis=u'both', which=u'both',length=0)\nax1.tick_params(axis=u'both', which=u'both',length=0)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These results are interesting. It does appear that job-seekers are less educated - I would suggest that this is becuase they are younger and still on their education journey, and are also seeking new challenges.\n\nWhat do you think?","metadata":{}},{"cell_type":"code","source":"# Orders\n\ned_order = ['Primary School','High School','Graduate','Masters','Phd']\nenroll_order = ['No Enrollment','Part time course','Full time course']\ndisc_order = ['STEM','Unknown','Humanities','Other','Business Degree','Arts','No Major']\nexp_yrs_order = ['<1','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','>20']\nsize_order = ['0','<10', '10-49', '50-99', '100-500', '500-999', '1000-4999', '5000-9999', '10000+']\njob_order = ['Never', '1', '2', '3', '4', '>4']\nexp_order =['No relevant experience','Has relevant experience']\ngender_order = ['Male','Female','Other','Not provided']\ncompany_order = ['Pvt Ltd','Unknown','Funded Startup','Public Sector','Early Stage Startup','NGO','Other']\n\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = train_df\ndata['count'] = 1\n\ndata_plot = pd.pivot_table(data, values='count', index=['major_discipline'], columns=['company_size'], aggfunc=np.sum).fillna(0).astype(int).stack().loc[disc_order, size_order]\ndata_job_seek = pd.pivot_table(data[data['target']==1], values='count', index=['major_discipline'], columns=['company_size'], aggfunc=np.sum).fillna(0).astype(int).stack().loc[disc_order, size_order]\ndata_no_job_seek = pd.pivot_table(data[data['target']==0], values='count', index=['major_discipline'], columns=['company_size'], aggfunc=np.sum).fillna(0).astype(int).stack().loc[disc_order, size_order]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://stackoverflow.com/questions/56337732/how-to-plot-scatter-pie-chart-using-matplotlib\ndef drawPieMarker(xs, ys, ratios, sizes, colors, ax):\n    markers = []\n    previous = 0\n    # calculate the points of the pie pieces\n    for color, ratio in zip(colors, ratios):\n        this = 2 * np.pi * ratio + previous\n        x  = [0] + np.cos(np.linspace(previous, this, 30)).tolist() + [0]\n        y  = [0] + np.sin(np.linspace(previous, this, 30)).tolist() + [0]\n        xy = np.column_stack([x, y])\n        previous = this\n        markers.append({'marker':xy, 's':np.abs(xy).max()**2*np.array(sizes), 'facecolor':color})\n\n    # scatter each of the pie pieces to create pies\n    for marker in markers:\n        ax.scatter(xs, ys, **marker, alpha=0.9, ec=background_color)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Going Deeper\n\nA plot like this is not precise - but it does give viewers a **quick overview** of a lot of information. It engages the reader, and encourages them to spend time looking at the plot - it gets them thinking.\n\nWe see that the vast majority of employees are STEM graduates. These graduates also are also quite well represented at all company sizes.\n","metadata":{}},{"cell_type":"code","source":"# Plot inspired by Subin An\n\nfig = plt.figure(figsize=(13, 13), dpi=150)\ngs = fig.add_gridspec(5, 5)\ngs.update(wspace=0.1, hspace=0.1)\nfig.patch.set_facecolor(background_color) \n\n# Pie \n\nax_centre = fig.add_subplot(gs[1:4, 0:4]) \nfor cl_idx in disc_order[::-1]:\n    for age_idx in size_order:\n        seek = data_job_seek[cl_idx][age_idx]\n        no_seek = data_no_job_seek[cl_idx][age_idx]\n        total = data_job_seek[cl_idx][age_idx]\n        drawPieMarker([age_idx],[cl_idx], [seek/(seek+no_seek), no_seek/(seek+no_seek)] ,[total*2.5], [\"#0e4f66\", \"gray\"], ax=ax_centre)\n\nax_centre.grid(linewidth=0.1)        \nax_centre.set_facecolor(background_color)\nax_centre.set_xticklabels(size_order,fontfamily='serif', fontsize=11, rotation=90)\n\n # Top\nax_top = fig.add_subplot(gs[0, :4], sharex=ax_centre) \nc_size_non = data[data['target']==0]['company_size'].value_counts()[size_order]\nax_top.bar(c_size_non.index, c_size_non, width=0.45, alpha=0.9,ec=background_color, color='gray')\n\nc_size = data[data['target']==1]['company_size'].value_counts()[size_order]\nax_top.bar(c_size.index, c_size, bottom=c_size_non , width=0.45, alpha=0.9, ec=background_color,color='#0e4f66')\n\nplt.setp(ax_top.get_xticklabels(), visible=False)\nax_top.set_facecolor(background_color)\n\n# Side \nax_side = fig.add_subplot(gs[1:4, 4], sharey=ax_centre) \ndisc_no = data[data['target']==0]['major_discipline'].value_counts()[disc_order]\nax_side.barh(disc_no.index[::-1], disc_no[::-1], height=0.55, alpha=0.9,ec=background_color, color='gray')\n\ndisc_yes = data[data['target']==1]['major_discipline'].value_counts()[disc_order]\nax_side.barh(disc_yes.index[::-1], disc_yes[::-1], left= disc_no[::-1],height=0.55, alpha=0.9, ec=background_color,color='#0e4f66')\n\nplt.setp(ax_side.get_yticklabels(), visible=False)\nax_side.set_facecolor(background_color)\n\n# Spines\nfor s in ['top', 'left', 'right', 'bottom']:\n    ax_centre.spines[s].set_visible(False)\n    ax_top.spines[s].set_visible(False)\n    ax_side.spines[s].set_visible(False)\nax_centre.set_axisbelow(True)    \n\nfig.text(0.9, 0.9, 'Job seeking, company size, and major discipline', fontweight='bold', fontfamily='serif', fontsize=20, ha='right') \nfig.text(0.9, 0.87, 'Stacked Bar Chart & Categorical Bubble Pie Chart', fontweight='light', fontfamily='serif', fontsize=13, ha='right')\n\nfig.text(0.633,0.84,\"Job Seeking\", fontweight=\"bold\", fontfamily='serif', fontsize=15, color='#0e4f66')\nfig.text(0.745,0.84,\"|\", fontweight=\"bold\", fontfamily='serif', fontsize=15, color='black')\nfig.text(0.755,0.84,\"Not Job Seeking\", fontweight=\"bold\", fontfamily='serif', fontsize=15, color='gray')\n\nax_centre.tick_params(axis=u'both', which=u'both',length=0)\nax_top.tick_params(axis=u'both', which=u'both',length=0)\nax_side.tick_params(axis=u'both', which=u'both',length=0)\n\nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ned_order = ['Primary School','High School','Graduate','Masters','Phd']\nenroll_order = ['No Enrollment','Part time course','Full time course']\ndisc_order = ['STEM','Unknown','Humanities','Other','Business Degree','Arts','No Major']\nexp_yrs_order = ['<1','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','>20']\nexp_yrs_order_2 = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\nsize_order = ['0','<10', '10-49', '50-99', '100-500', '500-999', '1000-4999', '5000-9999', '10000+']\njob_order = ['Never', '1', '2', '3', '4', '>4']\nexp_order =['No relevant experience','Has relevant experience']\ngender_order = ['Male','Female','Other','Not provided']\n\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"job_change = aug_train.groupby(['education_level','last_new_job'])['experience'].sum().unstack().loc[ed_order,job_order]\njc_never = job_change['Never']\n\n\njob_change_norm = pd.crosstab(aug_train['education_level'],aug_train['last_new_job'],normalize='columns').loc[ed_order,job_order,]\njob_change_norm = round(job_change_norm*100,1).astype(int)\n\n\n\n\n##\njob_hunt_only = aug_train[aug_train['target']==1]\nno_job_hunt_only = aug_train[aug_train['target']==0]\n\n\nseekers_job_change_norm = pd.crosstab(job_hunt_only['education_level'],job_hunt_only['last_new_job'],normalize='columns').loc[ed_order,job_order,]\nseekers_job_change_norm = round(seekers_job_change_norm*100,1).astype(int)\nseekers_job_change_norm\n\nnon_seekers_job_change_norm = pd.crosstab(no_job_hunt_only['education_level'],no_job_hunt_only['last_new_job'],normalize='columns').loc[ed_order,job_order,]\nnon_seekers_job_change_norm = round(non_seekers_job_change_norm*100,1).astype(int)\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction\n\nThe main purpose of the notebook is of course **visualization**, but I wanted to finish the process with the process of selecting a predictive model\n\nI won't spend much time fine-tuning these models, but as long as we get a fairly decent result, above the Null Accuracy Score, I will be content. I will also visualize the results.\n","metadata":{}},{"cell_type":"code","source":"# Extra libs\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nimport xgboost as xgb\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import accuracy_score, recall_score, roc_auc_score, precision_score\nfrom sklearn.svm import LinearSVC\nfrom imblearn.over_sampling import BorderlineSMOTE\nfrom numpy import where","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_of_columns = ['city', 'gender', 'relevent_experience', 'enrolled_university', 'education_level',\n                   'major_discipline', 'experience', 'company_size', 'company_type', 'last_new_job']\n\naug_train_dummies_df = []\n\nfor col in list_of_columns:\n    dummy_train_df = pd.get_dummies(aug_train[col])\n    aug_train_dummies_df.append(dummy_train_df)\n\n\nlist_of_columns.append('enrollee_id')    \naug_train_dummies_df.insert(0, aug_train.drop(columns=list_of_columns))\n\naug_train = pd.concat(aug_train_dummies_df,axis=1)\naug_train = aug_train.drop('count', 1)\n\n# Final look at our df\naug_train.head(3)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling\n\nI will try a series of algorithms on this problem. Let's see how we do...","metadata":{}},{"cell_type":"code","source":"X = aug_train.dropna().drop(columns=['target']).values\ny = aug_train.dropna()['target'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=80)\n\n\n# Classification with a linear SVM\nsvc = LinearSVC(dual=False, random_state=123)\nparams_grid = {\"C\": [10 ** k for k in range(-3, 4)]}\nclf = GridSearchCV(svc, params_grid)\nclf.fit(X_train, y_train)\nprint(\n    \"Accuracy on the test set with raw data: {:.3f}\".format(clf.score(X_test, y_test))\n)\n\nprint(clf.best_params_)","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = aug_train.dropna().drop(columns=['target']).values\ny = aug_train.dropna()['target'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=80)","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Support Vector Machine (SVM / SVC)\npipeline = make_pipeline(StandardScaler(), SVC(kernel='sigmoid'))\npipeline.fit(X_train, y_train)\nsvc_prediction = pipeline.predict(X_test)\ncm_svc = confusion_matrix(y_test, svc_prediction)\nsvc_df = pd.DataFrame(data=[accuracy_score(y_test, svc_prediction), recall_score(y_test, svc_prediction),\n                   precision_score(y_test, svc_prediction), roc_auc_score(y_test, svc_prediction)], \n             columns=['SVC Score'],\n             index=[\"Accuracy\", \"Recall\", \"Precision\", \"ROC AUC Score\"])","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Basic Decision Tree\ndtree = DecisionTreeClassifier()\ndtree.fit(X_train,y_train)\ndtree_prediction = dtree.predict(X_test)\ncm_dtree = confusion_matrix(y_test, dtree_prediction)\ndtree_df = pd.DataFrame(data=[accuracy_score(y_test, dtree_prediction), recall_score(y_test, dtree_prediction),\n                   precision_score(y_test, dtree_prediction), roc_auc_score(y_test, dtree_prediction)], \n             columns=['Decision Tree Score'],\n             index=[\"Accuracy\", \"Recall\", \"Precision\", \"ROC AUC Score\"])","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest\nrfc = RandomForestClassifier(n_estimators=600)\nrfc.fit(X_train,y_train)\nrfc_prediction = rfc.predict(X_test)\ncm_rfc = confusion_matrix(y_test, rfc_prediction)\nrfc_df = pd.DataFrame(data=[accuracy_score(y_test, rfc_prediction), recall_score(y_test, rfc_prediction),\n                   precision_score(y_test, rfc_prediction), roc_auc_score(y_test, rfc_prediction)], \n             columns=['Random Forest Score'],\n             index=[\"Accuracy\", \"Recall\", \"Precision\", \"ROC AUC Score\"])","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#param_grid = { \n#    'n_estimators': [ 500,800],\n#    'max_features': ['auto', 'sqrt'],\n#    'max_depth' : [7,8,9],\n#    'criterion' :['gini', 'entropy']\n#}\n\n#CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n#CV_rfc.fit(X_train,y_train)\n#CV_rfc.best_params_","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tuned Random Forest\nrfc1=RandomForestClassifier(random_state=0, n_estimators= 800, criterion = 'gini',max_features = 'auto',max_depth = 8)\nrfc1.fit(X_train,y_train)\nprediction_rf1= rfc1.predict(X_test)\ncm_trfc = confusion_matrix(y_test, prediction_rf1)\ntrfc_df = pd.DataFrame(data=[accuracy_score(y_test, prediction_rf1), recall_score(y_test, prediction_rf1),\n                   precision_score(y_test, prediction_rf1), roc_auc_score(y_test, prediction_rf1)], \n             columns=['Tuned Random Forest Score'],\n             index=[\"Accuracy\", \"Recall\", \"Precision\", \"ROC AUC Score\"])","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Logistic Regression\nlogmodel = LogisticRegression()\nlogmodel.fit(X_train,y_train)\nlog_prediction = logmodel.predict(X_test)\ncm_log = confusion_matrix(y_test, log_prediction)\nlog_df = pd.DataFrame(data=[accuracy_score(y_test, log_prediction), recall_score(y_test, log_prediction),\n                   precision_score(y_test, log_prediction), roc_auc_score(y_test, log_prediction)], \n             columns=['Logisitc Regression Score'],\n             index=[\"Accuracy\", \"Recall\", \"Precision\", \"ROC AUC Score\"])","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = aug_train.dropna().drop(columns=['target']).values\ny = aug_train.dropna()['target'].values\n\nscaler = StandardScaler()\nscaler.fit(aug_train.drop('target',axis=1))\nscaled_features = scaler.transform(aug_train.drop('target',axis=1))\n\n# re split\nX_train, X_test, y_train, y_test = train_test_split(scaled_features,aug_train['target'],\n                                                    test_size=0.30)","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# K-Nearest Neighbours (KNN)\n# searched already to find optimal neighbours, removed from notebook as took a long time\nknn = KNeighborsClassifier(n_neighbors=17)\nknn.fit(X_train,y_train)\nknn_prediction = knn.predict(X_test)\ncm_knn = confusion_matrix(y_test, knn_prediction)\nknn_df = pd.DataFrame(data=[accuracy_score(y_test, knn_prediction), recall_score(y_test, knn_prediction),\n                   precision_score(y_test, knn_prediction), roc_auc_score(y_test, knn_prediction)], \n             columns=['KNN Score'],\n             index=[\"Accuracy\", \"Recall\", \"Precision\", \"ROC AUC Score\"])","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# So far...","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(15,18), dpi=150)\ngs = fig.add_gridspec(4, 2)\ngs.update(wspace=0.1, hspace=0.5)\nax0 = fig.add_subplot(gs[0, :])\n\ncolors = [\"lightgray\",\"lightgray\"]\ncolormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n# Change background color\nbackground_color = \"#fbfbfb\"\nfig.patch.set_facecolor(background_color) # figure background color\nax0.set_facecolor(background_color)\n\n# Overall\ndf_models = round(pd.concat([svc_df,dtree_df,rfc_df,trfc_df,log_df,knn_df], axis=1),3)\nsns.heatmap(df_models.T, cmap=colormap,annot=True,fmt=\".1%\", linewidths=2.5,cbar=False,ax=ax0)\n\nax0.tick_params(axis=u'both', which=u'both',length=0)\nax0.text(0,-2,'Our results so far',fontfamily='serif',fontsize=20,fontweight='bold')\nax0.text(0,-0.7,'We saw earlier that our dataset was imbalanced.\\nWill using SMOTE improve our scores? We have the Random Forest scores to beat.',fontfamily='serif',fontsize=14)\n\nfrom matplotlib.patches import Rectangle\n\nrect = ax0.add_patch(Rectangle((0, 2), 5, 1, fill=True,color='#0e4f66', edgecolor='white', lw=0,alpha=0.5))\n\n\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Implementing SMOTE\n\nSMOTE is a technique that helps deal with imbalanced data sets. \n\nA great introductory article can be found here:\n\nhttps://www.geeksforgeeks.org/ml-handling-imbalanced-data-with-smote-and-near-miss-algorithm-in-python/\n\nThe common error I see people making is to use SMOTE and THEN split their data in to train & test sets. This is a big mistake as you will get some serious data leakage and end up predicting synthetic results that you have just created - it does not make sense. \n\nInstead, split your data first, and THEN use SMOTE on the training data only.\n\nLet's see if it helps here...","metadata":{}},{"cell_type":"code","source":"# Our data is unbalanced, we can fix this with SMOTE\n\nfrom imblearn.over_sampling import SMOTE\n\nX = aug_train.dropna().drop(columns=['target']).values\ny = aug_train.dropna()['target'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=80)\n\noversample = SMOTE()\nX_train_resh, y_train_resh = oversample.fit_resample(X_train, y_train.ravel())","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scale our data in pipeline\n\nrf_pipeline = Pipeline(steps = [('scale',StandardScaler()),('RF',RandomForestClassifier(random_state=42))])\nsvm_pipeline = Pipeline(steps = [('scale',StandardScaler()),('SVM',SVC(random_state=42))])\nlogreg_pipeline = Pipeline(steps = [('scale',StandardScaler()),('LR',LogisticRegression(random_state=42))])\n","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#rf_cv = cross_val_score(rf_pipeline,X_train_resh,y_train_resh,cv=10,scoring='f1')\n#svm_cv = cross_val_score(svm_pipeline,X_train_resh,y_train_resh,cv=10,scoring='f1')\n#logreg_cv = cross_val_score(logreg_pipeline,X_train_resh,y_train_resh,cv=10,scoring='f1')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Mean f1 scores:')\nprint('Random Forest mean :',cross_val_score(rf_pipeline,X_train_resh,y_train_resh,cv=10,scoring='f1').mean())\n# removed svm as took an incredibly long time to fit\n#print('SVM mean :',cross_val_score(svm_pipeline,X_train_resh,y_train_resh,cv=10,scoring='f1').mean())\nprint('Logistic Regression mean :',cross_val_score(logreg_pipeline,X_train_resh,y_train_resh,cv=10,scoring='f1').mean())","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On the training set at least, we are doing very well. \n\nI used f1 score as the metric as this is a weighted blend of accuracy & recall.\n\nLet's try the test data...","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, precision_score, f1_score\n\nrf_pipeline.fit(X_train_resh,y_train_resh)\n#svm_pipeline.fit(X_train_resh,y_train_resh)\nlogreg_pipeline.fit(X_train_resh,y_train_resh)\n\nrf_pred   =rf_pipeline.predict(X_test)\n#svm_pred  = svm_pipeline.predict(X_test)\nlogreg_pred   = logreg_pipeline.predict(X_test)\n\nrf_cm  = confusion_matrix(y_test,rf_pred )\n#svm_cm = confusion_matrix(y_test,svm_pred)\nlogreg_cm  = confusion_matrix(y_test,logreg_pred )\n\nrf_f1  = f1_score(y_test,rf_pred)\n#svm_f1 = f1_score(y_test,svm_pred)\nlogreg_f1  = f1_score(y_test,logreg_pred)","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Mean scores:')\nprint('RF F1 score:',rf_f1)\nprint('RF Accuracy:',accuracy_score(y_test,rf_pred))\nprint('LR F1 score :',logreg_f1)\nprint('LR Accuracy:',accuracy_score(y_test,logreg_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The results are looking great. Let's see our models side-by-side so we can select the one that acheives our aims the best","metadata":{}},{"cell_type":"code","source":"smote_rf_df = pd.DataFrame(data=[accuracy_score(y_test, rf_pred), recall_score(y_test, rf_pred),\n                   precision_score(y_test, rf_pred), roc_auc_score(y_test, rf_pred)], \n             columns=['SMOTE Random Forest Score'],\n             index=[\"Accuracy\", \"Recall\", \"Precision\", \"ROC AUC Score\"])\n\nsmote_logreg_df = pd.DataFrame(data=[accuracy_score(y_test, logreg_pred), recall_score(y_test, logreg_pred),\n                   precision_score(y_test, logreg_pred), roc_auc_score(y_test, logreg_pred)], \n             columns=['SMOTE Logistic Regression Score'],\n             index=[\"Accuracy\", \"Recall\", \"Precision\", \"ROC AUC Score\"])\n\n\nsmote_cm_log = confusion_matrix(y_test, logreg_pred)\nsmote_cm_rf = confusion_matrix(y_test, rf_pred)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Viewing our results in an accesible way\n\nWe now now need to find a way to view our results which can be easily explained to business stakeholders. \n\nA simple annotated heatmap works well for this!","metadata":{}},{"cell_type":"code","source":"# Plotting our results\n\ncolors = [\"lightgray\",\"lightgray\"]\ncolormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\n\nfig = plt.figure(figsize=(15,18), dpi=150) # create figure\ngs = fig.add_gridspec(5, 2)\ngs.update(wspace=0.1, hspace=0.5)\nax0 = fig.add_subplot(gs[0, :])\nax1 = fig.add_subplot(gs[1, 0])\nax2 = fig.add_subplot(gs[1, 1])\nax3 = fig.add_subplot(gs[2, 0])\nax4 = fig.add_subplot(gs[2, 1])\nax5 = fig.add_subplot(gs[3, 0])\nax6 = fig.add_subplot(gs[3,1])\nax7 = fig.add_subplot(gs[4,0])\nax8 = fig.add_subplot(gs[4,1])\n\n# Overall\ndf_models = round(pd.concat([svc_df,dtree_df,rfc_df,trfc_df,smote_rf_df,log_df,smote_logreg_df,knn_df], axis=1),3)\nsns.heatmap(df_models.T, cmap=colormap,annot=True,fmt=\".1%\", linewidths=2.5,cbar=False,ax=ax0)\n\n\nax0.set_yticklabels(ax0.get_yticklabels(), fontfamily='serif', rotation = 0, fontsize=12)\nax0.set_xticklabels(ax0.get_xticklabels(), fontfamily='serif', rotation=0, fontsize=12)\n\nax0.text(0,-4,'Model Performance Overview & Selection',fontfamily='serif',fontsize=20,fontweight='bold')\nax0.text(0,-0.55,\n         '''\nBased on the problem statement: \"Predict employees who are job seeking\", our model of choice will be Logistic Regression using SMOTE.\nThis model has the highest recall score, a high accuracy overall, and is quick to run & re-train if required.\n'''\n         ,fontfamily='serif',fontsize=14)\n\n\n\nfor lab, annot in zip(ax0.get_yticklabels(), ax0.texts):\n    text =  lab.get_text()\n    if text == 'SMOTE Logistic Regression Score': \n        # set the properties of the ticklabel\n        lab.set_weight('bold')\n        lab.set_size(15)\n        lab.set_color('black')\n        \n\n\n\nfrom matplotlib.patches import Rectangle\n\nrect = ax0.add_patch(Rectangle((0, 6), 5, 1, fill=True,color='#0e4f66', edgecolor='white', lw=0,alpha=0.5))\n\n\n\n# svc\nsns.heatmap(cm_svc, linewidths=2.5,yticklabels=['Actual Non-Job Seeker','Actual Job Seeker'],xticklabels=['Predicted Non-Job Seeker','Predicted Job Seeker'], cmap=colormap, cbar=None,annot=True,fmt='d',ax=ax1,annot_kws={\"fontsize\":15})\n\n# dtree\nsns.heatmap(cm_dtree, linewidths=2.5,yticklabels=False,xticklabels=['Predicted Non-Job Seeker','Predicted Job Seeker'], cmap=colormap, cbar=None,annot=True,fmt='d',ax=ax2,annot_kws={\"fontsize\":15})\n\n# rf\nsns.heatmap(cm_rfc, linewidths=2.5,yticklabels=['Actual Non-Job Seeker','Actual Job Seeker'],xticklabels=['Predicted Non-Job Seeker','Predicted Job Seeker'], cmap=colormap, cbar=None,annot=True,fmt='d',ax=ax3,annot_kws={\"fontsize\":15})\n\n# tuned rf\nsns.heatmap(cm_trfc, linewidths=2.5,yticklabels=False,xticklabels=['Predicted Non-Job Seeker','Predicted Job Seeker'], cmap=colormap, cbar=None,annot=True,fmt='d',ax=ax4,annot_kws={\"fontsize\":15})\n\n# log\nsns.heatmap(cm_log, linewidths=2.5,yticklabels=['Actual Non-Job Seeker','Actual Job Seeker'],xticklabels=['Predicted Non-Job Seeker','Predicted Job Seeker'], cmap=colormap, cbar=None,annot=True,fmt='d',ax=ax5,annot_kws={\"fontsize\":15})\n\n# knn\nsns.heatmap(cm_knn, linewidths=2.5,yticklabels=False,xticklabels=['Predicted Non-Job Seeker','Predicted Job Seeker'], cmap=colormap, cbar=None,annot=True,fmt='d',ax=ax6,annot_kws={\"fontsize\":15})\n\n# smote rf\nsns.heatmap(smote_cm_rf, linewidths=2.5,yticklabels=['Actual Non-Job Seeker','Actual Job Seeker'],xticklabels=['Predicted Non-Job Seeker','Predicted Job Seeker'], cmap=colormap, cbar=None,annot=True,fmt='d',ax=ax7,annot_kws={\"fontsize\":15})\n\n# smote log reg\nsns.heatmap(smote_cm_log, linewidths=2.5,yticklabels=False,xticklabels=['Predicted Non-Job Seeker','Predicted Job Seeker'], cmap=colormap, cbar=None,annot=True,fmt='d',ax=ax8,annot_kws={\"fontsize\":15})\n\n\n\nbackground_color = \"#fbfbfb\"\nfig.patch.set_facecolor(background_color) \n\nax0.set_xlabel(\"\")\nax0.set_ylabel(\"\")\n\n\nax1.text(0, -0.3, 'Support Vector Machine (SVM)',fontsize=15, fontweight='bold', fontfamily='serif')\nax2.text(0, -0.3, 'Decision Tree', fontsize=15, fontweight='bold', fontfamily='serif')\nax3.text(0, -0.3, 'Random Forest',fontsize=15, fontweight='bold', fontfamily='serif')\nax4.text(0, -0.3, 'Random Forest (w/Adjustments)', fontsize=15, fontweight='bold', fontfamily='serif')\nax5.text(0, -0.3, 'Logistic Regression',fontsize=15, fontweight='bold', fontfamily='serif')\nax6.text(0, -0.3, 'K-Nearest Neighbours (KNN)', fontsize=15, fontweight='bold', fontfamily='serif')\nax7.text(0, -0.3, 'SMOTE Random Forest',fontsize=15, fontweight='bold', fontfamily='serif')\nax8.text(0, -0.3, 'SMOTE Logistic Regression', fontsize=15, fontweight='bold', fontfamily='serif')\n\n\nax7.text(0, 3, 'Review',fontsize=20, fontweight='bold', fontfamily='serif')\nax7.text(0, 6, \n'''\nBy using SMOTE, which is Synthetic Minority Oversampling Technique, we have dramatically improved\nour results.\n\nOur final Logistic Regression model performs well on all metrics - particularly recall. but wihtout\nsacrificing performance in other metrics.\nWith the highest recall & ROC AUC scores, this model would certainly be valuable for this HR department.\nIt has the added advantage of being very quick to train, too.\n\nThe only downside with the model is that it often predicts that a person is looking for a new role even\nif they are not. \nHowever, on balance, I think this model is still worthy of selection. \n''',fontsize=14, fontfamily='serif')\n\n\nfrom matplotlib.patches import Rectangle\n\n\nax8.add_patch(Rectangle((1, 1), 1, 1, fill=True,color='#0e4f66', edgecolor='white', lw=0,alpha=0.5))\n\ncolors = ['gray','#0e4f66','#002d1d']\ncolormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\nfor i in range(0,9):\n    locals()[\"ax\"+str(i)].set_facecolor(background_color) \n    \nfor i in range(0,9):\n    locals()[\"ax\"+str(i)].tick_params(axis=u'both', which=u'both',length=0)\n\n\nfor s in [\"top\",\"right\",\"left\"]:\n    for i in range(0,9):\n        locals()[\"ax\"+str(i)].spines[s].set_visible(False)\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Something else... Borderline SMOTE\n\nThere are many oversampling techniques that one could employ. \n\nA variation of the technique used above is **Borderline SMOTE**.\n\nBorderline SMOTE involves selecting those instances of the minority class that are misclassified.\n\nWe can then oversample just those difficult instances, providing more resolution only where it may be required\n\nA great article cab be found here:\n\nhttps://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/\n\n","metadata":{}},{"cell_type":"code","source":"oversample = BorderlineSMOTE()\nX_train_resh_bord, y_train_resh_bord = oversample.fit_resample(X_train, y_train.ravel())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_pipeline.fit(X_train_resh_bord,y_train_resh_bord)\nlogreg_pipeline.fit(X_train_resh_bord,y_train_resh_bord)\n\nrf_pred   =rf_pipeline.predict(X_test)\nlogreg_pred   = logreg_pipeline.predict(X_test)\n\nrf_cm  = confusion_matrix(y_test,rf_pred )\nlogreg_cm  = confusion_matrix(y_test,logreg_pred )\n\nrf_f1  = f1_score(y_test,rf_pred)\nlogreg_f1  = f1_score(y_test,logreg_pred)\n\nprint('Mean scores:')\nprint('RF F1 score:',rf_f1)\nprint('RF Accuracy:',accuracy_score(y_test,rf_pred))\nprint('LR F1 score :',logreg_f1)\nprint('LR Accuracy:',accuracy_score(y_test,logreg_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(15,18), dpi=150) \nfig.patch.set_facecolor(background_color) \ngs = fig.add_gridspec(5, 2)\ngs.update(wspace=0.1, hspace=0.5)\nax0 = fig.add_subplot(gs[0, :])\n\n# borderline smote log reg\nsns.heatmap(logreg_cm, linewidths=2.5,yticklabels=['Actual Non-Job Seeker','Actual Job Seeker'],xticklabels=['Predicted Non-Job Seeker','Predicted Job Seeker'], cmap=colormap, cbar=None,annot=True,fmt='d',ax=ax0,annot_kws={\"fontsize\":15})\n\nax0.set_facecolor(background_color) \nax0.tick_params(axis=u'both', which=u'both',length=0)\n\n\nfor s in [\"top\",\"right\",\"left\"]:\n    ax0.spines[s].set_visible(False)\n    \nax0.text(0, -0.3, 'Borderline SMOTE Logistic Regression',fontsize=15, fontweight='bold', fontfamily='serif')\n    \nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see above, that by using Borderline SMOTE **we have managed to improve recall**, capturing 964 job seekers as opposed to 960 with regular SMOTE.","metadata":{}},{"cell_type":"markdown","source":"# Conclusion\n\n**So there we have it**\n\nWe have explored and visualized our data in interesting ways - designed to enageg the reader.\n\nAlong the way we dealt with null values & performed feauture engineering so get a more complete view of our data.\n\nFinally, we performed several machine learning algorithms to try to predict whether or not someone would be a job seeker or not.\nWe looked at:\n- SVM\n- Decision Trees\n- Random Forests\n- Logisitic Regression\n- KNN\n\nNext, we looked at addressing the imbalance in our data. Could this improve our models?\n\nI used SMOTE, and re-trained Logistics Regression & Random Forest models. The Logistic Regression model performed great and I decided that would be the model I would select.\n\nWe did not do much model tuning, so the final score could almost certainly be improved upon - but we still achieved good results!\n\n\nEasy to implement next steps could be:\n\n- Hyperparameter tuning\n- Threshold manipulation \n\nNext, I also presented my findings in a way such that we could **explain our results** to business stakeholders.\n\n# Thank you for reading\n\nIf you found value in this notebook, please **upvote** \n\nHave a great day!\n\n\n# More of my work\n\n\n\n**Gold price prediction using Prophet**\n\nhttps://www.kaggle.com/joshuaswords/eda-gold-price-prediction-prophet\n\n\n**Computer Vision with FastAI - Pneumonia prediction**\n\nhttps://www.kaggle.com/joshuaswords/computer-vision-pneumonia-prediction-fastai\n\n\n**Stroke Prediction with SMOTE and LIME explainer**\n\nhttps://www.kaggle.com/joshuaswords/predicting-a-stroke-95-acc-with-lime-explainer\n\n\n**2021 World Happiness Index EDA**\n\nhttps://www.kaggle.com/joshuaswords/awesome-eda-2021-happiness-population","metadata":{}},{"cell_type":"markdown","source":"The pie chart visual was inspired by Subin An, check out his page if you get the chance\n\nhttps://www.kaggle.com/subinium\n","metadata":{}}]}