{"cells":[{"metadata":{},"cell_type":"markdown","source":"# What do we know about COVID-19 risk factors?\n*About this Notebook*\n\n    In this notebook,we have answered the questions given in the task by using lexical model(TF-IDF). Moreover, literature clustering is also being performed to better understand the domains of articles/documents and efficiently target/search the specific domain of articles. We worked on paragraph rather than documents to get the better results in question answering and literature clustering, and it worked.However, the end result is document (not paragraph: paragraphs are used only for training). Original dataset contains around 50k papers and many of them are not specifically about COVID-19. The dataset we have used is acquired by filtering the original 'CORD-19-research-challenge' dataset for specific COVID-19 papers/articles.  [this](https://www.kaggle.com/massiq/doctovec) notebook is used for filtering COVID-19 related papers/articles.[this](https://www.kaggle.com/maksimeren/covid-19-literature-clustering) notebook and [this](https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089) article helped us doing literature clustering and lexical model respectively.By combining clustering and information retrieval system (TF-IDF) we can better understand and explore our own interest articles/papers."},{"metadata":{},"cell_type":"markdown","source":"*PROS*: \n*     It is very simple and efficient method.\n*     We can easily compute the similarity between different documents using it.\n*     It is very effective in extracting descriptive documents.\n*     Base of almost every search engine.\n    \n*CONS*: \n*     This method is based on the bag-of-words (BoW) model, therefore it does not capture position in text, semantics, co-occurrences in different documents.\n\nAs our scenario is inclined more towards lexical approach,our model will work good in this scenario."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see where our dataset resides."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!ls /kaggle/input//covid19-filtered-dataset/","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Importing some basic libraries which will be helpful in coming operations."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport glob\nimport json\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Read dataset file."},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_df = pd.read_csv('/kaggle/input//covid19-filtered-dataset/covid_19_full_text_files.csv', dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\nmeta_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can find meta information of dataset with .info() function."},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before we go towards information extraction part with TF-IDF, let's do clustering to get the overall insights of dataset. So, first of all we do preprocessing of data.\n* Remove punctuations.\n* Convert into lowercase.\n\nFor now, we will do these two operations."},{"metadata":{"trusted":true},"cell_type":"code","source":"import re,string\n\nmeta_df['text'] = meta_df['text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\nmeta_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lower_case(input_str):\n    input_str = input_str.lower()\n    return input_str\n\nmeta_df['text'] = meta_df['text'].apply(lambda x: lower_case(x))\nmeta_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Optional:**\nAs we do not need all the columns, we will keep only main columns so we can read and compare results easily."},{"metadata":{"trusted":true},"cell_type":"code","source":"text = meta_df.drop([\"source_x\",\"pmcid\",\"pubmed_id\",\"license\",\"publish_time\",\"Microsoft Academic Paper ID\",\"WHO #Covidence\",\"has_pdf_parse\",\"has_pmc_xml_parse\",\"full_text_file\",\"tag_disease_covid19\"], axis=1)\ntext.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data may have duplicates informations, it is better to drop those rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"text.drop_duplicates(['text'], inplace=True)\nlen(text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It droped two rows, let's see the current count. "},{"metadata":{"trusted":true},"cell_type":"code","source":"text['text'].describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Point to be noted****\nHere is the main operations which will play an important role in getting better results in this notebook. As we know that an article/research paper does not describe a single topic but collection of closely related topics. Our dataset mainly concern about COVID-19. Every document is describing different aspects of this topic. On the other hand, paragraph is more specific about a single aspect of a specific topic. It is better if we look deeply into documents: take every paragraph as a document.It means that we will have documents and in a document there will be paragraph documents which will point towards that document.  Now, it make sense that we will get better results if we divide every document into paragraph and do operations on paragraph rather than a whole document. However, rest of the details will be same for every paragraph of a single document. You will understand better once you see the results. So, let's convert the documents."},{"metadata":{"trusted":true},"cell_type":"code","source":"para_list=pd.DataFrame(columns=['cord_uid','sha','paper_id','doi','journal','title','authors','abstract','text','url'])\ni=0\nfor index,bodyText in text.iterrows(): \n    big_data_list=[]\n    para=bodyText['text'].split('\\n')\n    for par in para:\n      data_list=[]\n      data_list.append(bodyText['cord_uid'])\n      data_list.append(bodyText['sha'])\n      data_list.append(bodyText['paper_id'])\n      data_list.append(bodyText['doi'])\n      data_list.append(bodyText['journal'])\n      data_list.append(bodyText['title'])\n      data_list.append(bodyText['authors'])\n      data_list.append(bodyText['abstract'])\n      data_list.append(par)\n      data_list.append(bodyText['url'])\n      big_data_list.append(data_list)\n    para_df=pd.DataFrame(columns=['cord_uid','sha','paper_id','doi','journal','title','authors','abstract','text','url'], data=big_data_list)\n    para_list=para_list.append(para_df)\n    i+=1\n#     print(i)\nprint(\"length: \"+str(len(para_list)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can see that the 1737 documents are now converted into 53701 paragraph documents.Let's check the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"para_list.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you see the paragraph which are from same document have same rest of the information i.e title,abstract etc.\nAs, documents also have headings, notes and empty lines which are not of our concern because those are very small in length and also present in paragraph. We can get those informations from the paragraph. So, we will eliminate rows which have text less than 65 ((max words in titles/headings * average characters in a word)+spaces)"},{"metadata":{"trusted":true},"cell_type":"code","source":"para_list=para_list[para_list['text'].map(len) > 65]\nlen(para_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, our dataset length reduced to 13404. let's take a look at the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"para_list.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From here, we will start our clustering related operations. One thing we need to point out here is that we will work on body text only becuase it is our main concern but we can add title and abstract later to get more better results. \n\nFirst of all we need to vectorize the text(convert text into vector form). TF-IDF vectorizer is being used for vectorizing. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(max_features=2**12)\nX = vectorizer.fit_transform(para_list['text'].values)\nX.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try to get our labels (which text comes under which cluster). we will use MiniBatchKMeans to clusterize the text(in vector form) as it is faster with more data (we can also use kMeans:it is a bit slower). Number of clusters is 20."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import MiniBatchKMeans\n\nk = 20\nkmeans = MiniBatchKMeans(n_clusters=k)\ny_pred = kmeans.fit_predict(X)\ny = y_pred\ny","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we have gotten our labels, we can plot them but vectorizer vectorizes the text in higher dimension, first we need to reduce our high dimensional features vector into 2 dimensional plane. For this process, we will use PCA as it scales very well with larger datasets and dimensions. It will keep similar instances together while trying to push different instances far from each other."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=3)\npca_result = pca.fit_transform(X.toarray())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" It is easier to see the results in a 3 dimensional plot. So let's try to do that:"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nax = plt.figure(figsize=(16,10)).gca(projection='3d')\nax.scatter(\n    xs=pca_result[:,0], \n    ys=pca_result[:,1], \n    zs=pca_result[:,2], \n    c=y, \n    cmap='tab10'\n)\nax.set_xlabel('pca-one')\nax.set_ylabel('pca-two')\nax.set_zlabel('pca-three')\nplt.title(\"PCA Covid-19 Articles (paragraph) (3D) - Clustered (K-Means,k=20) - Tf-idf with Plain Text\")\n# plt.savefig(\"plots/pca_covid19_label_TFID_3d.png\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can see in the graph that there are 6,7 clusters which dominates the other clusters. It means that those paragraph documents are very closely related to each other.\n\nNow we will add those clusters/labels to our dataset to increase our understanding."},{"metadata":{"trusted":true},"cell_type":"code","source":"para_list['Cluster']=y\npara_list.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From here, we will start working on our TF-IDF model. Special thanks to William Scott, who did a complete TF-IDF model(https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089). It helped us a lot. We changed it with respect to our requirements and did some of changes according to our method.\n\nFirst of all we will do preprocessing of paragraph document. If you remember we did preprocessing but that was for clustering, it is better if we do more deep preprocessing as now we will be extracting information not just clustering. It is needed that we make each text in specified format and follow certain rules."},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nimport os\nimport string\nimport numpy as np\nimport copy\nimport pandas as pd\nimport pickle\nimport re\nimport math\n\nnltk.download(\"popular\")\n!pip install num2words\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom collections import Counter\nfrom num2words import num2words\n\n\n\ndef remove_stop_words(data):\n    stop_words = stopwords.words('english')\n    words = word_tokenize(str(data))\n    new_text = \"\"\n    for w in words:\n        if w not in stop_words and len(w) > 1:\n            new_text = new_text + \" \" + w\n    return new_text\n\ndef stemming(data):\n    stemmer= PorterStemmer()\n    \n    tokens = word_tokenize(str(data))\n    new_text = \"\"\n    for w in tokens:\n        new_text = new_text + \" \" + stemmer.stem(w)\n    return new_text\ndef convert_numbers(data):\n    tokens = word_tokenize(str(data))\n    new_text = \"\"\n    for w in tokens:\n        try:\n            w = num2words(int(w))\n        except:\n            a = 0\n        new_text = new_text + \" \" + w\n    new_text = np.char.replace(new_text, \"-\", \" \")\n    return new_text\n\ndef convert_lower_case(data):\n    return np.char.lower(data)\n\ndef remove_punctuation(data):\n    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n    for i in range(len(symbols)):\n        data = np.char.replace(data, symbols[i], ' ')\n        data = np.char.replace(data, \"  \", \" \")\n    data = np.char.replace(data, ',', '')\n    return data\n\ndef remove_apostrophe(data):\n    return np.char.replace(data, \"'\", \"\")\n\n\ndef preprocess(data):\n    data = convert_lower_case(data)\n    data = remove_punctuation(data) #remove comma seperately\n    data = remove_apostrophe(data)\n    data = remove_stop_words(data)\n    data = convert_numbers(data)\n    data = stemming(data)\n    data = remove_punctuation(data)\n    data = convert_numbers(data)\n    data = stemming(data) #needed again as we need to stem the words\n    data = remove_punctuation(data) #needed again as num2word is giving few hypens and commas fourty-one\n    data = remove_stop_words(data) #needed again as num2word is giving stop words 101 - one hundred and one\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_text=[]\ni=0\nfor t in para_list['text']:\n    processed_text.append(word_tokenize(str(preprocess(t))))\n    if i%1000==0:\n        print(\"text: \"+str(i))\n    i+=1\n\nprint(len(processed_text))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's define these two terms, so every person can understand what is going on.\n* **TF of a document:** It is the term frequency in a document: how many times a word appears in a document.\n* **IDF:** It is the inverse document frequency. It is the inverse of In how many documents a word appear. The less the number of document a word appears in, the more score it will have. It basically tells that how much a word uniquely defines a document.\n\nTo learn more about approach you can follow William's article. \nLet's find the term frequency."},{"metadata":{"trusted":true},"cell_type":"code","source":"N=len(processed_text)\nDF = {}\n\nfor i in range(N):\n    tokens = processed_text[i]\n    for w in tokens:\n        try:\n            DF[w].add(i)\n        except:\n            DF[w] = {i}\nfor i in DF:\n    DF[i] = len(DF[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have term frequency of every unique word in our dataset. Let's store the length of unique words which will be our vocabulary size."},{"metadata":{"trusted":true},"cell_type":"code","source":"total_vocab_size = len(DF)\ntotal_vocab_size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's store our vocabulary in a separate place."},{"metadata":{"trusted":true},"cell_type":"code","source":"total_vocab = [x for x in DF]\ntotal_vocab[:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have defined a function which will give us document frequency. we will use it to calculate document frequency on runtime and take it inverse to find IDF."},{"metadata":{"trusted":true},"cell_type":"code","source":"def doc_freq(word):\n    c = 0\n    try:\n        c = DF[word]\n    except:\n        pass\n    return c","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we have both TF and IDF calculator, now we will find TF-IDF score."},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\ndoc = 0\n\ntf_idf = {}\n\nfor i in range(N):\n    \n    tokens = processed_text[i]\n    \n    counter = Counter(tokens)\n    words_count = len(tokens)\n    \n    for token in np.unique(tokens):\n        \n        tf = counter[token]/words_count\n        df = doc_freq(token)\n        idf = np.log((N+1)/(df+1))\n        \n        tf_idf[doc, token] = tf*idf\n\n    doc += 1\n    \nlen(tf_idf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As now we have TF-IDF score of every word in every document, we can calculate TF-IDF score of text in every document by simply adding the TF-IDF score of every word of text in every document. At the end we will sort them by high scores and get the K documents which have top high scores."},{"metadata":{"trusted":true},"cell_type":"code","source":"def matching_score(k, query):\n    tokens = word_tokenize(str(query))\n\n    print(\"Matching Score\")\n    print(\"\\nQuery:\", query)\n    print(\"\")\n    print(tokens)\n    \n    query_weights = {}\n\n    for key in tf_idf:\n        \n        if key[1] in tokens:\n            try:\n                query_weights[key[0]] += tf_idf[key]\n            except:\n                query_weights[key[0]] = tf_idf[key]\n    \n    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)\n\n    print(\"\")\n    \n    l = []\n    \n    for i in query_weights[:10]:\n        l.append(i[0])\n    \n    print(l)\n    print(para_list.iloc[l[0]])\n    \n\nmatching_score(10, \"pregnant woman\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This approach of finding query related document is good and simple but this does not work well when query length becomes large i.e 5,15,20 etc words length query. It will not work good in our scanerio because user may give longer query. Thus we will shift to cosine similarity finder.\n\nlet's define our cosine similarity score function."},{"metadata":{"trusted":true},"cell_type":"code","source":"def cosine_sim(a, b):\n    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n    return cos_sim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(tf_idf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's do some preprocessing. We have defined a vector generator function which will create vector in space of given text."},{"metadata":{"trusted":true},"cell_type":"code","source":"D = np.zeros((N, total_vocab_size))\ncnt=0\nfor i in tf_idf:\n    if cnt%100000==0:\n        print(cnt)\n    try:\n        ind = total_vocab.index(i[1])\n        D[i[0]][ind] = tf_idf[i]\n    except:\n        pass\n    cnt+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gen_vector(tokens):\n\n    Q = np.zeros((len(total_vocab)))\n    \n    counter = Counter(tokens)\n    words_count = len(tokens)\n\n    query_weights = {}\n    \n    for token in np.unique(tokens):\n        \n        tf = counter[token]/words_count\n        df = doc_freq(token)\n        idf = math.log((N+1)/(df+1))\n\n        try:\n            ind = total_vocab.index(token)\n            Q[ind] = tf*idf\n        except:\n            pass\n    return Q","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have every modular function to help, at first we will give query, query will be preprocessed in the same way every document is preprocessed so we can have better results. Then it will generate vector of query in space and check the similarity of query vector with every paragraph document vector by measuring distance between them and get the top k paragraph document which has high similarity scores.\n\nAfter getting the paragraph document it will merge those paragraph which points to same document and show the results including documents informations i.e title,abstract,paper_id,text,sha,url/doi etc. It shows the specific paragraph of a document which is related to your query, you can also view the whole document by following url/doi given in the result.\n\nMoreover, 'Cluster' field also added to results/Output, so you can see that in which cluster these documents falls in. you can also find dominating cluster in these results and search for documents of dominating cluster and read it."},{"metadata":{"trusted":true},"cell_type":"code","source":"def cosine_similarity(k, query):\n    # print(\"Cosine Similarity\")\n    preprocessed_query = preprocess(query)\n    tokens = word_tokenize(str(preprocessed_query))\n    \n    # print(\"\\nQuery:\", query)\n    # print(\"\")\n    # print(tokens)\n    \n    d_cosines = []\n    \n    query_vector = gen_vector(tokens)\n    \n    for d in D:\n        d_cosines.append(cosine_sim(query_vector, d))\n        \n    out = np.array(d_cosines).argsort()[-k:][::-1]\n    \n    result=pd.DataFrame(columns=['cord_uid','sha','paper_id','doi','journal','title','authors','abstract','text','url'])\n    for file in out:\n      found=result[result['paper_id'].str.contains(str(para_list.iloc[file]['paper_id']),na=False)]\n      if len(found)!=0:\n        indx=result.index[result['paper_id'] == para_list.iloc[file]['paper_id']]\n        result.loc[indx]['text']=result.loc[indx]['text'] +\"***************************************************\"+ para_list.iloc[file]['text']\n      else:\n        result=result.append(para_list.iloc[file])\n    return result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These were the question from the tasks. You can see their result."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.max_colwidth=70\nquestions=['Smoking, pre-existing pulmonary disease',\n          'Co-infections (determine whether co-existing respiratory/viral infections make the virus more transmissible or virulent) and other co-morbidities',\n          'Neonates and pregnant women',\n          'Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences',\n          'Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors',\n          'Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups',\n          'Susceptibility of populations',\n          'Public health mitigation measures that could be effective for control']\nQ = cosine_similarity(10, questions[0])\nQ2 = cosine_similarity(10, questions[1])\nQ3 = cosine_similarity(10, questions[2])\nQ4 = cosine_similarity(10, questions[3])\nQ5 = cosine_similarity(10, questions[4])\nQ6 = cosine_similarity(10, questions[5])\nQ7 = cosine_similarity(10, questions[6])\nQ8 = cosine_similarity(10, questions[7])\nQ.to_csv('./smokingPreExistingPulmonaryDisease.csv', index=False)\nQ2.to_csv('./coinfectionsComorbities.csv', index=False)\nQ3.to_csv('./neonatesPregnantWomen.csv', index=False)\nQ4.to_csv('./socioeconomicBehavioralFactors.csv', index=False)\nQ5.to_csv('./transmissionDynamicsVirus.csv', index=False)\nQ6.to_csv('./severityDiseaseIncludingRisk.csv', index=False)\nQ7.to_csv('./susceptibilityOfPopulations.csv', index=False)\nQ8.to_csv('./publicHealthMitigationMeasures.csv', index=False)\nQ8['title']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the whole result."},{"metadata":{"trusted":true},"cell_type":"code","source":"Q.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As, in above result(in my case) dominating clusters are 18 and 9. So you can check both clusters documents as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"(para_list.loc[para_list['Cluster'] == 18]).head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}