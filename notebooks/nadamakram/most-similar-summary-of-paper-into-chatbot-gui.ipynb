{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\nCOVID-19 Open Research Dataset (CORD-19) has been prepared which is a resource of over 138,000 scholarly articles, including over 69,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. The CORD-19 dataset represents the most extensive machine-readable coronavirus literature collection available for data mining to date which we can apply text and data mining approaches to find answers to questions within this content in support of the ongoing COVID-19 response efforts worldwide. \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Problem Statement:\nthe rapid increase in coronavirus scientific literature, making it harder to keep track and updated with all new publications that let us understand the virus better or find the relevant publications to our work as contributors in fighting COVID-19 effects.\nWe are focusing on what do we know about COVID-19 risk factors? what have we learned from epidemiological studies?\nSpecifically, we want to know what the literature reports about:\nData on potential risks factors\nSmoking, pre-existing pulmonary disease\nCo-infections (determine whether co-existing respiratory/viral infections make the virus more transmissible or virulent) and other co-morbidities\nNeonates and pregnant women\nSocio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.\nTransmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors\nSeverity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups\nSusceptibility of populations\nPublic health mitigation measures that could be effective for control","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Our Solution:\nWe are aiming to make the search of a question related to COVID-19 (specially COVID-19 risk factors) easier by finding the most similar scientific paper and present it to the searcher in a chatbot GUI.\n### DEMO LINK: [here](https://youtu.be/cEH3M1woqVI)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We use Doc2Vec algorithm to produce vector representation of documents, train the model, have the most similar papers to query, summarize the paper, and then send the response via API to DialogFlow chatbot for easy UX.\n\n![](https://imgur.com/8PTzABg.jpg)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Methodology:\n* Understand the problem\n* Data Exploration \n* How can you use data to answer the question? Information Retrieval\n* Data Requirements:\n    * Collection\n    * Cleaning\n        \n* Choosing the right model\n* Train model\n* Model deployment/Our results in the production\n    * Build chatbot GUI\n    * Chatbot and model integration\n* Result","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Import Libraries ","execution_count":null},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\n\nimport json\nimport re\n\nfrom nltk.stem import WordNetLemmatizer #for stemming\nfrom nltk.collocations import *\nfrom nltk.tokenize import word_tokenize #for tokenization\nfrom nltk.corpus import stopwords #for removing stop words\nimport nltk\n\nimport csv #for saving the data after preprocessing, and loading it later\n\nimport gensim #for training the model\nimport joblib #for saving the model on disk\n\n#plotting libraries\nimport matplotlib.pylab as plt\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\nimport random\n\nimport heapq","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Collecting Data:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#preparing list of the files path for later use in reading the files itself\nroot_path = '/kaggle/input/CORD-19-research-challenge'\n\nfiles_path = []\nfor dirname, _, filenames in os.walk(f'{root_path}/document_parses/pmc_json/'):\n    for filename in filenames:\n        files_path.append(os.path.join(dirname, filename))\n        \nfor dirname, _, filenames in os.walk(f'{root_path}/document_parses/pdf_json/'):\n    for filename in filenames:\n        files_path.append(os.path.join(dirname, filename))\n        \nfiles_path = random.sample(files_path, 10000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning:\n### 1. Formatting:\n    * Lemmatization: it has pros over stemming that it is more accurate as it uses semantics, not only syntax. Unfortunately it is slower than stemming.\n    * Removing Stop Words: should be used wisely, As it may give a negative effect if used before sentimant analysis.\n    * Removing URLs\n    * Removing HTML\n    * Removing Punctuations\n### 2. Removing duplicates:\n    * removing duplicates can be done using one of two features:\n        * Title: it would've been the more efficient way but we proved that it wasn't good enough as it had about 40% null values when we tested upon a random sample\n        * Body Text: it is a slower but more efficient considering the high percentage of null values in the title\n### 3. Dropping Rows With Empty Body Text:\n    * As our algorithm mainly depends on the body text values, it would be useless if we used article with empty body text in the model\n### 4. Feature Selection:\n    * Selecting useful features or columns\n    * Filter Body Text's Sections","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(text):\n    wordnet_lemmatizer = WordNetLemmatizer()\n    stop_words = set(stopwords.words('english'))\n    \n    text = re.sub('<.*?>','',text) #removing html that is read unintentionally when collecting the data\n    text = re.sub('https?:\\/\\/[^\\s]+', '', text) #removing the URLs, as they won't make use to us\n    text = \" \".join([wordnet_lemmatizer.lemmatize(t) for t in text.split()]) #lemmatizing\n    \n    #removing punctuations\n    for punc in '!\"#$%&()*+,./:;<=>?@[\\\\]^_`{|}~':\n        text = ' '.join(text.split(punc))\n    \n    text = ' '.join([word for word in text.split() if word not in stop_words])#removing stop_words\n    \n    return text.lower().strip()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#counting the number of occurences of the body sections we desire\nbody_sections = {}\n\ndef body_sections_dic(section_name):\n    global body_sections\n    if section_name in body_sections: \n        body_sections[section_name] += 1\n    else: \n        body_sections[section_name] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning\n* Feature Selection","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"since some sections are more important than others to give the answer of the user's question, we decided to only include the sections that would give the greatest useful information","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#saving only the sections that would matter to us\nbody_sections_matters = ['abstract','introduction','summary','discussion','conclusion','diagnosis', 'method','treatment','result','concluding','method','background','measures','transmission period','incubation']\n\ndef fileRead(file_path):\n    with open(file_path) as file:\n        content = json.load(file)\n        \n        body_text = []\n        for entry in content['body_text']:\n            preprocessed_section = preprocess(entry['section'])\n            body_sections_dic(preprocessed_section)\n            for i in body_sections_matters:\n                if i in preprocessed_section or preprocessed_section == '':\n                    body_text.append(entry['text'])\n                    break\n        return preprocess('\\n'.join(body_text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing entries with empty body and repeated ones\nduplicates_cnt = 0 #counter repeated or duplicated articles \nemptyBodies_cnt = 0 #counter of articles with empty body \n\ndef removeEmptyRows():\n    global duplicates_cnt\n    global emptyBodies_cnt\n    \n    empty_body_ind = []\n    \n    for indx, file in enumerate(files):\n        if(file == ''):\n            empty_body_ind.append(indx)\n        elif file in files[:indx]:\n            duplicates_cnt += 1\n            empty_body_ind.append(indx)\n            \n    emptyBodies_cnt = len(empty_body_ind)\n    empty_body_ind.reverse()\n    \n    for ind in empty_body_ind:\n        files.pop(ind)\n        files_path.pop(ind)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"files_flag = 1 #if the flag is == 1, then we should read the files from their locations, otherwise, it should be loaded from csv file\n\nif files_flag:\n    #read the files from their locations\n    files = [fileRead(eachfile) for eachfile in files_path]\n    removeEmptyRows()\n    \n    #save a csv file of the output\n    with open('articles_body.csv', 'w', newline='') as file:\n        writer = csv.writer(file)\n        for f in files:\n            writer.writerow(f)\nelse: #load the data from a csv file\n    files = []\n    with open('../input/titles/articles_body_.csv', encoding='latin-1') as csv_file:\n        csv_reader = csv.reader(csv_file, delimiter=',')\n        for row in csv_reader:\n            files.append(row[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pylab as plt\nfrom matplotlib.pyplot import figure\nfigure(num=None, figsize=(20, 10), dpi=80, facecolor='w')\nplt.rc('xtick', labelsize=20) \nplt.rc('ytick', labelsize=20)\nplt.xticks(rotation=90)\n\nlists = sorted(body_sections.items(),key=lambda x: x[1], reverse=True) # sorted by key, return a list of tuples\nx, y = zip(*lists[:20]) # unpack a list of pairs into two tuples\n\nplt.plot(x, y)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see in the graph above the frequency of different section names.\nAs a result, we used the most frequent sections that matter on the paper. In addition, we considered sessions that have the same meaning. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"figure(num=None, figsize=(6, 6), dpi=80, facecolor='w')\nplt.rc('xtick', labelsize=10) \nplt.rc('ytick', labelsize=10)\n#plt.xticks(rotation=90)\n#EDIT: note that there is only two or three duplicates in all articles, I guess we should use another graph type to show it\nx, y = ['Nomber of empty body papers', 'Number of duplicated papers'], [emptyBodies_cnt, duplicates_cnt]\n\nplt.bar(x, y, align='center')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see in the graph above that we dropped the empty body papers and duplicated papers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#for insertnig the files into the model they should be tonkenized and tagged, so here is a function to do that\ndef tagFiles(indx, file):\n    tokens = word_tokenize(file)\n    return gensim.models.doc2vec.TaggedDocument(tokens, [indx])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tag and tokenize all files\nfiles_flag = 1 #if the flag is == 1, then we should read the files from their locations, otherwise, it should be loaded from csv file\ntaggedFiles_flag = 1\n\nif taggedFiles_flag:\n    #read the files from their locations\n    taggedFiles = [tagFiles(indx, file) for indx, file in enumerate(files)]\n    \n    #save a csv file of the output\n    with open('taggedFiles.csv', 'w', newline='') as file:\n        writer = csv.writer(file)\n        for tf in taggedFiles:\n            writer.writerow([tf])\n    \nelse: #load the data from a csv file\n    taggedFiles_flag = []\n    with open('../input/titles/taggedFiles.csv', encoding='latin-1') as csv_file:\n        csv_reader = csv.reader(csv_file, delimiter=',')\n        for row in csv_reader:\n            taggedFiles.append(row)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We used Document Embeddings not word embeddings though there’s a way of having document embeddings by adding directly adding word vectors constituting a sentence/document but we found that document embedding models do better than just summation.\nWe want to produce a semantic representation of documents so we chose Doc2Vec unsupervised algorithm to generate vectors for papers.\n![](https://imgur.com/JYJwWHB.jpg)\nref: https://shuzhanfan.github.io/2018/08/understanding-word2vec-and-doc2vec/\n<br>\nPros: \n1. Perform well in words which don’t have spelling mistakes or informal text\n2. Adaptation of word2vec - and can be used to generate sentence/document embeddings\n\nCons: \n1. Generally, it doesn't always perform well, in some cases it is insufficient and in others like our case it performs well on sentence similarity tasks\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_flag = 1\n\nif model_flag:\n    #build the model\n    model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n    model.build_vocab(taggedFiles)\n    model.train(taggedFiles, total_examples=model.corpus_count, epochs=model.epochs)\n    \n    # Save the model to disk\n    joblib.dump(model, 'nlp_model.pkl')\nelse:\n    model = joblib.load('nlp_model.pkl') #Load \"model.pkl\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getMostSimilar(question):\n    question = preprocess(question) #QUESTION: what infer_vector does\n    query_token = model.infer_vector(word_tokenize(question)) #tokenizing the question\n    similar_docs = model.docvecs.most_similar([query_token], topn=20) #get the top 20 similar articles to the question\n    documents = [files[similar_doc[0]] for similar_doc in similar_docs] #get the data to return from the top 20 articles or documents\n    return documents","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#storing the questions on task\nquestions = [\"What do we know about potential risks factors?\",\n             \"what is the effect of Smoking, pre-existing pulmonary disease?\",\n             \"Do co-existing respiratory/viral infections make the virus more transmissible or virulent and other comorbidities?\",\n             \"What is the effect on Neonates and pregnant women?\",\n             \"What are the Socio-economic and behavioral factors on COVID-19?\",\n             \"What is the economic impact of the virus?\",\n             \"What are Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors?\",\n             \"Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups\",\n             \"What are the Susceptibility of populations?\",\n             \"What are the Public health mitigation measures that could be effective for control?\"]\n#getting the answers of the questions\nquestionsAnswer = [getMostSimilar(q) for q in questions]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"queries_token = [model.infer_vector(word_tokenize(question)) for question in questions]\ntsne = TSNE(verbose=1, perplexity=100, random_state=42)\nQ_embedded = tsne.fit_transform(queries_token)\nQ_embedded.shape\ndocuments_vetors = [model.infer_vector(word_tokenize(file)) for file in files]\n\nD_embedded = tsne.fit_transform(documents_vetors)\nD_embedded.shape\n\n#Normalization\nmean = (0,0)\nfor q in Q_embedded:\n    mean += q\n    \nnormaliz_factor = mean/len(Q_embedded)\n\nD_embedded = D_embedded * normaliz_factor\n\n# sns settings\nsns.set(rc={'figure.figsize':(10,8)})\n\n# colors\npalette = sns.hls_palette(40, l=.4, s=.9)\n\n# plot\nsns.scatterplot(D_embedded[:,0],D_embedded[:,1], palette=palette)\n    \n# plot\nsns.scatterplot(Q_embedded[:,0], Q_embedded[:,1], palette=palette)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"similar_docs_percentage = [model.docvecs.most_similar([query_token], topn=20) for query_token in queries_token]\nsimilar_documents = [documents_vetors[similar_doc[0][0]] for similar_doc in similar_docs_percentage]\ntsne = TSNE(verbose=1, perplexity=100, random_state=42)\nQ_embedded = tsne.fit_transform(queries_token)\nQ_embedded.shape\n#TODO:DIAA input the questions token\n#Normalization\nmean = (0,0)\nfor q in Q_embedded:\n    mean += q\nnormaliz_factor = mean/len(Q_embedded)\n# sns settings\nsns.set(rc={'figure.figsize':(10,8)})\n# colors\npalette = sns.hls_palette(40, l=.4, s=.9) \nfor doc in similar_documents:\n    D_embedded = tsne.fit_transform(similar_documents)\n    D_embedded.shape\n    D_embedded = D_embedded * normaliz_factor\n    # plot\n    sns.scatterplot(D_embedded[:,0],D_embedded[:,1], palette=palette)\n# plot\nsns.scatterplot(Q_embedded[:,0], Q_embedded[:,1], palette=palette)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this scatter plot we can see the papers in blue dots and questions in red dots with their similarity.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_summery(text):   \n    sentence_list = nltk.sent_tokenize(text)\n    preprocessed_list = [preprocess(sent) for sent in sentence_list]\n    preprocessed_text = ' '.join(preprocessed_list)\n\n    #count vectorizing\n    word_frequencies = {}\n    for word in nltk.word_tokenize(preprocessed_text):\n        if word not in word_frequencies.keys():\n            word_frequencies[word] = 1\n        else:\n            word_frequencies[word] += 1\n\n    maximum_frequncy = max(word_frequencies.values())\n\n    for word in word_frequencies.keys():\n        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n\n    sentence_scores = {}\n    for sent in sentence_list:\n        for word in nltk.word_tokenize(sent.lower()):\n            if word in word_frequencies.keys():\n                if len(sent.split(' ')) < 30:\n                    if sent not in sentence_scores.keys():\n                        sentence_scores[sent] = word_frequencies[word]\n                    else:\n                        sentence_scores[sent] += word_frequencies[word]\n\n    summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n\n    summary = ' '.join(summary_sentences)\n    return summary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"finalAnswers = [[text_summery(qa) for qa in questionAnswer] for questionAnswer in questionsAnswer]\nprint(\"What do we know about potential risks factors?\")\nprint(finalAnswers)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Deployment In Production\nAfter training the model and getting the results we saved the model as a pickle file to use it in a chatbot interface.\nDialogflow link: https://bot.dialogflow.com/fedf78f4-0592-4455-86ac-b4bf76336d1f\nFlask API GitHub repository: https://github.com/nadamakram/cord-19\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}