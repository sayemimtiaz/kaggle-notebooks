{"cells":[{"metadata":{},"cell_type":"markdown","source":"# House Sales in King Country, USA\n\nHere [House Sales in King County, USA](https://www.kaggle.com/harlfoxem/housesalesprediction) dataset by [harlfoxem](https://www.kaggle.com/harlfoxem) is used to perform `EDA` on housing prices and creating `machine lerning model` to predict house prices.\n\n**About data source**: This dataset contains house sale prices for King County, which includes Seattle. It includes homes sold between `May 2014 and May 2015`.\n\n![](https://media.giphy.com/media/3o6Mba1qerHR51rl9C/giphy.gif)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n\nimport xgboost\n\nfrom scipy.stats import zscore, pearsonr\n\nfrom joblib import dump","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pandas config\ndef pandas_config():\n    # display 10 rows and all the columns\n    pd.set_option('display.max_rows', 10)\n    pd.set_option('display.max_columns', None)\n    \npandas_config()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading the dataset\nfile_path = '/kaggle/input/housesalesprediction/kc_house_data.csv'\ndf = pd.read_csv(file_path)\ndf.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No missing data"},{"metadata":{},"cell_type":"markdown","source":"## Data preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop df column\ndef drop_df_column(df, column_name, inplace=True):\n    return df.drop([column_name], axis='columns', inplace=inplace)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_df_column(df, 'id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.select_dtypes('object').columns.tolist())\ndf.drop(['date'], axis='columns', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_corr(df, figsize=(16, 12)):\n    # the `corr` method uses pearson correaltion\n    corr = df.corr()\n    \n    _, ax = plt.subplots(1, 1, figsize=figsize)\n    g = sns.heatmap(corr, ax=ax, annot=True, cmap=sns.cubehelix_palette(start=.5, rot=-.5, as_cmap=True))\n\n    for _ax in g.get_xticklabels():\n        _ax.set_rotation(75)\n    \n    \nplot_corr(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get columns names in as sentence (instead of getting long list \n# of column names)\ndef get_column_names(df):\n    for column_name in df.columns.tolist():\n        print(f'{column_name} | ', end='')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_column_names(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_base_relation(df, figsize=(20, 200)):\n    columns = df.columns.tolist()\n    _, axs = plt.subplots(len(columns), 4, figsize=figsize)\n    \n    for idx, column in enumerate(columns):\n        # To get distribution of data\n        sns.histplot(\n            x=df[column],\n            kde=False,\n            color='#65b87b', alpha=.7,\n            ax=axs[idx][0]\n        )\n\n        # To get knowledge about outliers\n        sns.boxplot(\n            x=df[column],\n            color='#6fb9bd',\n            ax=axs[idx][1]\n        )\n\n        # To get its realtion with price\n        sns.scatterplot(\n            x=column, y='price', data=df,\n            color='#706dbd', alpha=.7, s=80,\n            ax=axs[idx][2]\n        )\n        \n        # To get count plot for `column`\n        sns.countplot(\n            x=column, data=df,\n            color='#42b0f5', alpha=.7,\n            ax=axs[idx][3]\n        )\n        \n        \nplot_base_relation(df, figsize=(20, 70))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dealing with outliers\n\nA lot of columns have issue of outliers. Using `IQR` & `Zscores` method to deal with it."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing outliers using IQR method\ndef rm_outliers_in_col_using_iqr(df, col):\n    # col here is df.column_name (or df[column_name])\n\n    Q1 = col.quantile(0.25)\n    Q3 = col.quantile(0.75)\n    IQR = Q3 - Q1\n\n    outliers_row_idx = col.loc[\n        (col < (Q1 - 1.5 * IQR)) | (col > (Q3 + 1.5 * IQR))\n    ].index.tolist()\n\n    df = df.drop(outliers_row_idx, axis='rows')\n    return (outliers_row_idx, df)\n\n\n# Removing outliers using the Zscore method\ndef rm_outliers_in_col_using_zscore(df, col, column_name):\n    # col here is df.column_name (or df[column_name])\n    \n    zscores_df = pd.DataFrame({\n        f'{column_name}': col.to_numpy()\n    }, df.index.tolist())\n    \n    zscores_df['zscores'] = zscores_df.apply(lambda x: zscore(x))\n    outliers_row_idx = zscores_df[np.abs(zscores_df.zscores) > 3].index.tolist()\n\n    df = df.drop(outliers_row_idx, axis='rows')\n    return (outliers_row_idx, df)\n\n\n# Remove outliers of a column using iqr & zscore methods\ndef remove_outliers_of_a_column(df, column_name):\n    rm_idxs = []\n    for _ in range(10):\n        outliers_row_idx, df = rm_outliers_in_col_using_iqr(df, df[column_name])\n        rm_idxs.extend(outliers_row_idx)\n        \n        outliers_row_idx, df = rm_outliers_in_col_using_zscore(df, df[column_name], column_name)\n        rm_idxs.extend(outliers_row_idx)\n    return rm_idxs, df\n\n\n# Remove outliers of a df using iqr & zscore methods\ndef remove_outliers_of_df(df):\n    rm_rows_idxs = []\n    for column in df.columns.tolist():\n        if column == 'price':\n            # As we don't want to do anything with `price`\n            continue\n            \n        rm_idxs, df = remove_outliers_of_a_column(df, column)\n        rm_rows_idxs.extend(rm_idxs)\n    return rm_rows_idxs, df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n    If you removed all the outliers in `continuous_df` then only 1/10th of the data is remaining.\n    So instead of going through all the columns in df at once, we will go through each column at \n    a time and if the columns has 2% of outliners then we drop that column. Keeping the threshold \n    as 2% because if keep threshold higher then collectivetly a lot of rows will drop which in turn\n    reduces our dataset \n    \n    eg. if threshold is 20% then column1 (if there are 15% outliers then remove the rows) then \n    column2 (if there are 10% outliers then remove the rows) so in total we end up dropping \n    15% + 10% = 30% of our rows.\n    \n    So to avoid this we are keeping threshold as 2%\n'''\n\n# To do the above thing we can just modify the `remove_outliers_of_df` func\ndef remove_outliers_of_df_with_threshold(df, threshold=2):\n    # `threshold` here is the percent above which the entire \n    # column will be dropped \n\n    rm_rows_idxs = []\n    for column in df.columns.tolist():\n        if column == 'price':\n            # As we don't want to do anything with `price`\n            continue\n            \n        rm_idxs, tmp_df = remove_outliers_of_a_column(df, column)\n\n        if round(len(rm_idxs) / len(df), 2) * 100 > threshold:\n            drop_df_column(df, column)\n        else:\n            df = tmp_df.copy()\n            del tmp_df\n            rm_rows_idxs.extend(rm_idxs)\n   \n    return rm_rows_idxs, df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Dataset size before removing outliers: {len(df)}')\n\nwith np.errstate(divide='ignore', invalid='ignore'):\n    RM_ROWS_IDXS, df = remove_outliers_of_df_with_threshold(df, threshold=4)\n\nprint(f'Dataset size after removing outliers: {len(df)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'{len(RM_ROWS_IDXS)} columns are dropped while removing outliers')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_base_relation(df, (20, 38))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove columns which have only one unique value as they won't be useful\ndrop_df_column(df, 'waterfront')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_corr(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_df_column(df, 'yr_renovated')\ndrop_df_column(df, 'zipcode')\ndrop_df_column(df, 'lat')\ndrop_df_column(df, 'long')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_scatterplot(x, y, ax=None):\n    sns.scatterplot(\n        x=x, y=y,\n        color='#706dbd', alpha=.7, s=80,\n        ax=ax\n    )\n    \n    \ndef plot_boxplot(x, ax=None):\n    sns.boxplot(x=x, color='#6fb9bd', ax=ax)\n    \n    \ndef plot_barplot(x, y, ax=None):\n    sns.barplot(x=x, y=y, data=df, palette='rocket', ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_, ax = plt.subplots(2, 2, figsize=(16, 8))\n\n\nplot_barplot(df.bedrooms, df.price, ax=ax[0][0])\nplot_barplot(df.condition, df.price, ax=ax[0][1])\nplot_barplot(df.bathrooms, df.price, ax=ax[1][0])\nplot_barplot(df.floors, df.price, ax=ax[1][1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_, ax = plt.subplots(2, 3, figsize=(16, 8))\n\nplot_scatterplot(df.sqft_living, df.price, ax=ax[0][0])\nplot_scatterplot(df.sqft_above, df.price, ax=ax[0][1])\nplot_scatterplot(df.sqft_basement, df.price, ax=ax[0][2])\nplot_scatterplot(df.yr_built, df.price, ax=ax[1][0])\nplot_scatterplot(df.sqft_living15, df.price, ax=ax[1][1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_corr(df, figsize=(14, 8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`sqft_above` has strong positive correlation with `sqft_living` and moderate positive correlation with `sqft_living15` and `sqft_living` has positive correlation with `sqft_living15`. In short there is `multi-collinearity` issue here, so dropping any 2 columns out of 3. "},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_df_column(df, 'sqft_above')\ndrop_df_column(df, 'sqft_living15')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp_df = df[['yr_built', 'price']].sort_values(by=['yr_built'])\n\ngroup = tmp_df.groupby(['yr_built'])['price'].mean()\navg_price_of_the_year = [avg_price for avg_price in group]\n\nplt.plot(tmp_df.yr_built.unique(), avg_price_of_the_year, linestyle='solid')\nplt.xticks(rotation=16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling int & float dtype column\ndef standard_scaler(column):\n    # Bumping up the ndim by np.newaxis as column.values is 1D & fit_transform needs 2D\n    return StandardScaler().fit_transform(column.values[:, np.newaxis])\n\n\n# Scaling all int & float dtype columns \ndef scaling_df(df):\n    # Selecting columns which have number dtype\n    numbers_df = df.select_dtypes(include=[np.int64, np.float64])\n\n    for column_name in numbers_df.columns.tolist():\n        df[column_name] = standard_scaler(df[column_name])\n    return df\n\n\n# Scaling `continuous_df` for EDA \nscaling_df(df)\n\ndf.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = df.columns.tolist()\ncolumns.remove('price')\n\nx = df[columns]\ny = df['price']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the dataset\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross validation\n\nkf = KFold(n_splits=10)\n\nscore = cross_val_score(Ridge(), x_train, y_train, cv=kf)\nprint(score.mean())\n\npr = PolynomialFeatures(degree=4)\nx_train_pr = pr.fit_transform(x_train)\nx_test_pr = pr.fit_transform(x_test)\n\nscore = cross_val_score(Ridge(), x_train_pr, y_train, cv=kf)\nprint(score.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using XGBoost\n\nxgb = xgboost.XGBRegressor()\ntry:\n    xgb.fit(x_train_pr, y_train)\nexcept KeyError:\n    pass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predictions\nxgb_y_test_pred = xgb.predict(x_test_pr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rms_error = mean_squared_error(y_test, xgb_y_test_pred, squared=False)\nr2_score_value = r2_score(y_test, xgb_y_test_pred)\n\nprint(f\"Root mean squared error: {rms_error}\")\nprint(f\"R2-score: {r2_score_value}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a pipeline\n\nscaling = ('scale', StandardScaler())\nploy = ('ploy', PolynomialFeatures(degree=4))\nmodel = ('model', xgboost.XGBRegressor())\n\n# Steps in the pipeline\nsteps = [scaling, ploy, model]\n\npipe = Pipeline(steps=steps)\n\n# Fiitting the model\nmodel = pipe.fit(x_train, y_train)\n\n# Out-Of-Sample Forecast\ny_test_pred = model.predict(x_test)\n\n# Evaluation\nrms_error = mean_squared_error(y_test, y_test_pred, squared=False)\nr2_score_value = r2_score(y_test, y_test_pred)\n\nprint(f\"Root mean squared error: {rms_error}\")\nprint(f\"R2-score: {r2_score_value}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Saving the model\ndump(model, 'model.joblib')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualizing our prediction against actual values"},{"metadata":{},"cell_type":"markdown","source":"### Visualizing entire prediction vs actual value"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(20, 6))\n\nax1.plot(np.arange(len(y_test)), y_test, label='Actual')\nax2.plot(np.arange(len(y_test_pred)), y_test_pred, label='Prediction')\n\nax1.legend()\nax2.legend()\n\nf, ax3 = plt.subplots(nrows=1, ncols=1, figsize=(20, 6))\n\nax3.plot(np.arange(len(y_test)), y_test, label='Actual')\nax3.plot(np.arange(len(y_test_pred)), y_test_pred, label='Prediction')\n\nax3.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing prediction vs actual values in interval of 100"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_result(start, end):\n    f, ax3 = plt.subplots(nrows=1, ncols=1, figsize=(13, 5))\n\n    ax3.plot(np.arange(len(y_test[start:end+1])), y_test[start:end+1], label='Actual')\n    ax3.plot(np.arange(len(y_test_pred[start:end+1])), y_test_pred[start:end+1], label='Prediction')\n\n    ax3.set_title(f'{start} - {end}')\n    ax3.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0, 4480, 100):\n    start = i\n    end = start + 100\n    plot_result(start, end)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\nI'll wrap things up there. If you want to find some other answers then go ahead `edit` this kernel. If you have any `questions` then do let me know.\n\nIf this kernel helped you then don't forget to ðŸ”¼ `upvote` and share your ðŸŽ™ `feedback` on improvements of the kernel.\n\n![](https://media.giphy.com/media/cp7bUxkodNBHW/giphy.gif)\n\n---"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}