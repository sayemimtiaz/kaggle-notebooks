{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Package imports","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\nfrom torch.utils.tensorboard import SummaryWriter\nimport torchvision.models as models\nfrom torchvision import datasets, transforms, utils\n\nimport cv2\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nfrom skimage import io, transform\nimport tensorboard as tb\nfrom dataclasses import dataclass\nfrom typing import Iterable\nfrom sklearn.model_selection import KFold\n\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Configs and hyper-params","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"@dataclass\nclass SystemConfig:\n    '''Describes the common system setting needed for reproducible training'''\n    seed: int = 11 \n    cudnn_benchmark_enabled: bool = True\n    cudnn_deterministic: bool = True \n   \n\n@dataclass\nclass TrainingConfig:\n    '''Describes configuration of the training process'''\n    device: str = 'cuda'\n    model_save_best: bool = True\n    batch_size: int = 32\n    epochs_count: int = 12\n    log_interval: int = 5  \n    test_interval: int = 1  \n    model_name: str = 'resnet50'\n    num_workers: int = 4\n    num_classes: int = 43\n    data_augmentation: bool = False\n    mean = [0.3447, 0.3131, 0.3243]\n    std = [0.1565, 0.1575, 0.1670]\n\n@dataclass\nclass DataConfig:\n    root_dir: str = '../input/gtsrb-german-traffic-sign'\n    train_dir: str = 'train/'\n    test_dir: str = 'test/'\n    train_csv: str = 'Train.csv'\n    test_csv: str = 'Test.csv'\n    model_dir: str = os.path.join('/kaggle/working/', 'models/')\n    log_dir: str = os.path.join('/kaggle/working/', 'logs/')\n\n@dataclass\nclass OptimizerConfig:\n    init_learning_rate: float = 0.0001 \n    weight_decay: float = 0.0001\n    scheduler_step_size: int = 6\n    scheduler_gamma: float = 0.1\n\n    \ndef setup_system(system_config: SystemConfig) -> None:\n    torch.manual_seed(system_config.seed)\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda:0\")\n        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic\n        \nsc = SystemConfig()\ntc = TrainingConfig()\ndc = DataConfig()\noc = OptimizerConfig()\n\nsetup_system(sc)\n\n\nfor path in [dc.log_dir, dc.model_dir]:\n    if not os.path.exists(path):\n        os.makedirs(path)\n        \ntb_writer = SummaryWriter(dc.log_dir)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data\n\nLoad the csv files and view some of the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_file = os.path.join(dc.root_dir, dc.train_csv)\ntest_file = os.path.join(dc.root_dir, dc.test_csv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(train_file)\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(test_file)\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Train csv shape: {df_train.shape}, \\nTest csv shape: {df_test.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_list = []\nimgs = []\nfor index, data in df_train.iterrows():\n    file_name = data['Path']\n    class_id = data['ClassId']\n    if class_id not in class_list:\n      class_list.append(class_id)\n      imgs.append(mpimg.imread(os.path.join(dc.root_dir,file_name)))\n    if index % 10000 == 0:\n      print(f'Currently on row {index} of 39209')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,15))\nplt.suptitle('Sample images from each class')\ncolumns = 10\nfor i, image in enumerate(imgs):\n    plt.subplot(len(imgs) / columns + 1, columns, i + 1)\n    plt.title(f'Class: {i+1}', color='black')\n    plt.imshow(image)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data distribution\n\nDisplaying a bar chart which shows the distribution of class occurences within the dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"c = df_train['ClassId'].nunique()\nx = df_train['ClassId'].value_counts()\n\nplt.bar(x=x.index.sort_values(), height=x, color='#0066ff')\nplt.title('Distibution of occurences in each class', color='black')\nplt.xlabel(\"Classes\", color='black')\nplt.ylabel(\"Occurences\", color='black')\nplt.tick_params(colors='black')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Custom Dataset\nCreate a custom dataset class with the proper methods for importing the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class GTSR43Dataset(Dataset):\n    \"\"\"German Traffic Sign Recognition dataset.\"\"\"\n    def __init__(self, root_dir, train_file, transform=None):\n        self.root_dir = root_dir\n        self.train_file_path = train_file\n        self.label_df = pd.read_csv(os.path.join(self.root_dir, self.train_file_path))\n        self.transform = transform\n        self.classes = list(self.label_df['ClassId'].unique())\n\n    def __getitem__(self, idx):\n        \"\"\"Return (image, target) after resize and preprocessing.\"\"\"\n        img = os.path.join(self.root_dir, self.label_df.iloc[idx, 7])\n        \n        X = Image.open(img)\n        y = self.class_to_index(self.label_df.iloc[idx, 6])\n\n        if self.transform:\n            X = self.transform(X)\n\n        return X, y\n    \n    def class_to_index(self, class_name):\n        \"\"\"Returns the index of a given class.\"\"\"\n        return self.classes.index(class_name)\n    \n    def index_to_class(self, class_index):\n        \"\"\"Returns the class of a given index.\"\"\"\n        return self.classes[class_index] \n    \n    def get_class_count(self):\n        \"\"\"Return a list of label occurences\"\"\"\n        cls_count = dict(self.label_df.ClassId.value_counts())\n#         cls_percent = list(map(lambda x: (1 - x / sum(cls_count)), cls_count))\n        return cls_count\n    \n    def __len__(self):\n        \"\"\"Returns the length of the dataset.\"\"\"\n        return len(self.label_df)\n\nclass GTSR43Subset(GTSR43Dataset):\n    \"\"\"A subset helper class for splitting the main dataset\"\"\"\n    def __init__(self, subset, transform=None):\n        self.subset = subset\n        self.transform = transform\n\n    def __getitem__(self, idx):\n        \"\"\"Retrieves one item from the dataset.\"\"\"\n        X, y = self.subset[idx]\n        \n        if self.transform:\n            X = self.transform(X)\n\n        return X, y\n\n    def __len__(self):\n        return len(self.subset)\n    \nclass GTSR43Testset(Dataset):\n    \"\"\"German Traffic Sign Recognition dataset\"\"\"\n    def __init__(self, root_dir, test_file, transform=None):\n        self.root = root_dir\n        self.test_file_path = test_file\n        self.label_df = pd.read_csv(os.path.join(self.root_dir, self.test_file_path))\n        self.transform = transform\n\n    def __getitem__(self, idx):\n        \"\"\"Retrieves one item from the dataset.\"\"\"\n        img = os.path.join(self.root_dir, self.label_df.iloc[idx, 7])\n        \n        image = Image.open(img)\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image\n    \n    def __len__(self):\n        return len(self.label_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Verify that the custom dataset and methods work","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ds = GTSR43Dataset(dc.root_dir, dc.train_csv)\nX, y = ds.__getitem__(5)\nprint(f'Train image: {X}\\nTarget class: {y}\\nClass count: {ds.get_class_count()}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data augmentation\nHelper functions for handling data augmenation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def image_resize():\n    \"\"\"Transforms for resizing, cropping.\"\"\"\n    resize_transforms = transforms.Compose([transforms.Resize(32),\n                                            transforms.CenterCrop(30),\n                                           ])\n    return resize_transforms\n\ndef image_preprocess():\n    \"\"\"Transforms for resizing, cropping, then converting to Tensor.\"\"\"\n    preprocess_transforms = transforms.Compose([transforms.Resize(32),\n                                                transforms.CenterCrop(30),\n                                                transforms.ToTensor()\n                                               ])\n    return preprocess_transforms\n\ndef common_transforms(mean, std):\n    \"\"\"Transforms which are common to both the train and test set.\"\"\" \n    common_transforms = transforms.Compose([image_preprocess(),\n                                            transforms.Normalize(mean, std)\n                                           ])\n    return common_transforms\n\ndef data_aug(mean, std):\n    \"\"\"Data augmentation transforms.\"\"\"\n    data_aug_transforms = transforms.Compose([transforms.Resize(32),\n                                              transforms.CenterCrop(30),\n                                              transforms.RandomVerticalFlip(),\n                                              transforms.RandomHorizontalFlip(),\n                                              transforms.ColorJitter(),\n                                              transforms.ToTensor(),\n                                              transforms.Normalize(mean, std),\n                                              transforms.RandomErasing(),\n                                             ])        \n    return data_aug_transforms","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mean and Std Dev\nHelper function for extracting the mean and std dev of the images in the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# def get_mean_std():\n#     \"\"\"Gets mean and standard deviation.\"\"\"  \n#     ds = GTSR43Dataset(dc.root_dir, \n#                        dc.train_csv, \n#                        transform=image_preprocess())       \n#     loader = DataLoader(ds, \n#                         batch_size = 10, \n#                         shuffle = False, \n#                         num_workers = 4)\n    \n#     mean = 0.0\n#     std = 0.0\n    \n#     for images, _ in loader:\n#         batch_samples = images.size(0) # the last batch can have smaller size\n#         images = images.view(batch_samples, images.size(1), -1)\n#         mean += images.mean(2).sum(0)\n#         std += images.std(2).sum(0)\n\n#     mean /= len(loader.dataset)\n#     std /= len(loader.dataset)\n    \n#     return mean, std\n\n# mean, std = get_mean_std()\n# print(mean, std)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Helper function for loading the data into Train/Test subsets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_data(root_dir, batch_size, num_workers=4, data_augmentation=False):\n    \"\"\"Loads and splits data into train and test subsets. \"\"\"  \n    dataset = GTSR43Dataset(root_dir, dc.train_csv)\n    \n    if data_augmentation:\n        X_transforms = data_aug(tc.mean, tc.std)\n    else:\n        X_transforms = common_transforms(tc.mean, tc.std)\n    \n    y_transforms = common_transforms(tc.mean, tc.std)\n    \n    X_size = int(0.8 * len(dataset))\n    y_size = len(dataset) - X_size\n    \n    X_dataset, y_dataset = torch.utils.data.random_split(dataset, [X_size, y_size])\n    \n    X_subset = GTSR43Subset(X_dataset, X_transforms)\n    y_subset = GTSR43Subset(y_dataset, y_transforms)\n\n    X_loader = DataLoader(X_subset, \n                          batch_size=batch_size, \n                          shuffle=False,\n                          num_workers=num_workers)  \n    \n    y_loader = DataLoader(y_subset,\n                          batch_size=batch_size, \n                          shuffle=False, \n                          num_workers=num_workers)\n    \n    return X_loader, y_loader","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(tc: TrainingConfig, \n          model: nn.Module, \n          optimizer: torch.optim.Optimizer,\n          X_loader: torch.utils.data.DataLoader, \n          epoch_idx: int) -> None:\n    \n    model.train()\n    \n    batch_loss = np.array([])\n    batch_acc = np.array([])\n    \n    for batch, (X, y) in enumerate(X_loader):\n        \n        if batch == 0:\n            print(f'Device: {torch.cuda.get_device_name(0)}, Data: {X.shape}, Target: {y.shape}')       \n\n        index_y = y.clone()\n        X = X.to(tc.device)\n        y = y.to(tc.device)\n        \n        optimizer.zero_grad()\n        output = model(X)\n        loss = F.cross_entropy(output, y)\n        loss.backward()\n        optimizer.step()\n        \n        batch_loss = np.append(batch_loss, [loss.item()])\n        prob = F.softmax(output, dim=1)\n        pred = prob.data.max(dim=1)[1]  \n        correct = pred.cpu().eq(index_y).sum()\n        acc = float(correct) / float(len(X))\n        batch_acc = np.append(batch_acc, [acc])\n            \n    epoch_loss = batch_loss.mean()\n    epoch_acc = 100. * batch_acc.mean()\n    \n    print(f'Training   - loss: {epoch_loss:.4f}, accuracy: {epoch_acc:.2f}%')\n    \n    return epoch_loss, epoch_acc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test Function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def validate(tc: TrainingConfig,\n             model: nn.Module,\n             y_loader: torch.utils.data.DataLoader) -> float:\n\n    model.eval()\n    \n    loss = 0.0\n    correct = 0.0\n    for X, y in y_loader:\n        index_y = y.clone()\n        \n        X = X.to(tc.device)\n        y = y.to(tc.device)\n\n        output = model(X)\n        loss += F.cross_entropy(output, y).item()\n        prob = F.softmax(output, dim=1)\n        pred = prob.data.max(dim=1)[1] \n        correct += pred.cpu().eq(index_y).sum()\n\n    loss = loss / len(y_loader)  \n    accuracy = 100. * correct / len(y_loader.dataset)\n    \n    print(f'Validation - loss: {loss:.4f}, accuracy: {accuracy:.2f}%, {correct}/{len(y_loader.dataset)}')\n    \n    return loss, accuracy/100.0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load pretrained model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def trainable_parameters(model):\n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            print(name)\n            \n\ndef pretrained_net():        \n    model_path = os.path.join(dc.model_dir, tc.model_name)\n    try:\n        model = torch.load('/kaggle/working/models/resnet50_3_pretrained.pt')\n    except FileNotFoundError:\n        os.environ['TORCH_HOME'] = '/kaggle/working/german_traffic_sign_recognition'\n        model = models.resnet50(pretrained=True, progress=True)\n        torch.save(model, '/kaggle/working/models/resnet50_pretrained.pt')\n        model = torch.load('/kaggle/working/models/resnet50_pretrained.pt')\n        \n    for param in model.parameters():\n        param.requires_grad = False\n        \n    model.fc = nn.Sequential(nn.Linear(2048, 512),\n                             nn.ReLU(inplace=True),\n                             nn.Dropout(0.5),\n                             \n                             nn.Linear(512, 512), \n                             nn.ReLU(inplace=True),\n                             nn.Dropout(0.2),\n                             \n                             nn.Linear(512, tc.num_classes))\n        \n    layers = [\n        model.layer2,\n        model.layer3,\n        model.layer4,\n        model.avgpool,\n      ]\n    \n    for layer in layers:\n        for param in layer.parameters():\n            param.requires_grad = True\n    \n    return model\n\npt_model = pretrained_net()\n\nprint(\"Layers: \\n\")\nfor name, child in pt_model.named_children():\n    print(name)\n\nprint(\"\\nCurrent Trainable Parameters: \\n\")\nprint(trainable_parameters(pt_model))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load and Save functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_model(model, device, accuracy):\n    \n    if not os.path.exists(dc.model_dir):\n        os.makedirs(dc.model_dir)\n\n    model_path = os.path.join(dc.model_dir, tc.model_name)\n\n    if device == 'cuda':\n        model.to('cpu')\n\n    torch.save(model.state_dict(), model_path + '_retrained.pt' )\n\n    if device == 'cuda':\n        model.to('cuda')\n    return\n\ndef load_model(model):\n    \n    model_path = os.path.join(dc.model_dir, tc.model_name)\n    model.load_state_dict(torch.load(model_path + '_retrained.pt'))\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Main function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def main(model, optimizer, tb_writer, scheduler = None, data_augmentation = True):\n\n    if torch.cuda.is_available():\n        tc.device = \"cuda\"\n    else:\n        tc.device = \"cpu\"\n        batch_size_to_set = 10\n        num_workers_to_set = 2\n    \n    model.to(tc.device)\n    \n    X_loader, y_loader = get_data(root_dir=dc.root_dir, \n                                  batch_size=tc.batch_size, \n                                  num_workers=tc.num_workers, \n                                  data_augmentation=tc.data_augmentation)\n    \n    best_loss = torch.tensor(np.inf)\n    \n    epoch_X_loss = np.array([])\n    epoch_y_loss = np.array([])\n    epoch_X_acc = np.array([])\n    epoch_y_acc = np.array([])\n    \n    t_begin = time.time()      \n                            \n    for epoch in range(tc.epochs_count):\n        print(f'\\nEpoch: {epoch + 1}/{tc.epochs_count}')    \n        \n        X_loss, X_acc = train(tc=tc, \n                              model=model, \n                              optimizer=optimizer, \n                              X_loader=X_loader, \n                              epoch_idx=epoch)\n\n        epoch_X_loss = np.append(epoch_X_loss, [X_loss])\n        epoch_X_acc = np.append(epoch_X_acc, [X_acc])\n                                \n        elapsed_time = time.time() - t_begin\n        speed_epoch = elapsed_time / (epoch + 1)\n        speed_batch = speed_epoch / len(X_loader)\n        eta = speed_epoch * tc.epochs_count - elapsed_time\n\n        tb_writer.add_scalar('Loss/Train', X_loss, epoch)\n        tb_writer.add_scalar('Accuracy/Train', X_acc, epoch)\n        tb_writer.add_scalar('Time/elapsed_time', elapsed_time, epoch)\n        tb_writer.add_scalar('Time/speed_epoch', speed_epoch, epoch)\n        tb_writer.add_scalar('Time/speed_batch', speed_batch, epoch)\n        tb_writer.add_scalar('Time/eta', eta, epoch)\n                             \n        if epoch % tc.test_interval == 0:\n\n            current_loss, current_acc = validate(tc, model, y_loader)\n                                                 \n            epoch_y_loss = np.append(epoch_y_loss, [current_loss])\n            epoch_y_acc = np.append(epoch_y_acc, [current_acc])\n                                    \n            if current_loss < best_loss:\n                best_loss = current_loss\n                save_model(model, device = tc.device, accuracy = current_acc)\n                print('Model Improved! Saved!')\n\n            tb_writer.add_scalar('Loss/Validation', current_loss, epoch)\n            tb_writer.add_scalar('Accuracy/Validation', current_acc, epoch)\n            tb_writer.add_scalars('Loss/train-val', {'train': X_loss, 'validation': current_loss}, epoch)\n            tb_writer.add_scalars('Accuracy/train-val', {'train': X_acc,'validation': current_acc}, epoch)\n                                  \n        if scheduler is not None:\n            scheduler.step()  \n\n        print(f'Time: {elapsed_time:.2f}s, {speed_epoch:.2f} s/epoch, {speed_batch:.2f} s/batch, Learning rate: {scheduler.get_last_lr()[0]}') \n        \n    print(f'Total time: {time.time() - t_begin:.2f}, Best loss: {best_loss:.3f}')    \n    \n    return model, epoch_X_loss, epoch_X_acc, epoch_y_loss, epoch_y_acc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Putting it all together","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = pretrained_net()\n\noptimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n                             lr=oc.init_learning_rate,\n                             weight_decay=oc.weight_decay)\n\nscheduler = optim.lr_scheduler.StepLR(optimizer, \n                                      step_size=oc.scheduler_step_size, \n                                      gamma=oc.scheduler_gamma)\n\nprint(f'Device: {tc.device}\\n\\\nEpochs: {tc.epochs_count}\\n\\\nBatch size: {tc.batch_size}\\n\\\nData Augmentation: {tc.data_augmentation}\\n\\\nScheduler step size: {oc.scheduler_step_size}\\n\\\nScheduler gamma: {oc.scheduler_gamma}\\n\\\nLearning rate: {oc.init_learning_rate}\\n\\\nL2 weight decay: {oc.weight_decay}')\n\nmodel, train_loss, train_acc, val_loss, val_acc = main(model, \n                                                       optimizer, \n                                                       tb_writer, \n                                                       scheduler=scheduler, \n                                                       data_augmentation=tc.data_augmentation)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}