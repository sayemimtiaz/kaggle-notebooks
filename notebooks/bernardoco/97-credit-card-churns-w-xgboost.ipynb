{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Abstract**\n* The objective of this notebook is to predict customers churns (cancellation) among credit card customers"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\ndata = pd.read_csv('../input/credit-card-customers/BankChurners.csv')\n\n# Remove Last 2 Columns\ndata = data.iloc[:, :-2]\ndata = data.iloc[:, 1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='Attrition_Flag', data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking if we have some NaN values in our dataset\ndata.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis\n\n* Doing some visualization with our data;\n\n* Correlation between features."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.boxplot(x='Income_Category', \n            y='Credit_Limit',\n            order=['Unknown', 'Less than $40K', '$40K - $60K', '$60K - $80K', '$80K - $120K', '$120K +'],\n            data=data).set_title('Income x Credit_Limit Boxplot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.boxplot(x='Education_Level', \n            y='Credit_Limit',\n            order=['Unknown', 'Uneducated', 'High School', 'College', 'Graduate', 'Doctorate', 'Post-Graduate'],\n            data=data).set_title('Boxplot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(data.corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mapping target feature 'Attrition Flag' to binary\ndata['Attrition_Flag'] = data['Attrition_Flag'].map({'Existing Customer': 0, 'Attrited Customer': 1})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Pearson Correlation between features and target variable Attrtion_Flag**"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = data.corr()\ncorr.sort_values('Attrition_Flag', ascending=False, inplace=True)\nprint(corr.Attrition_Flag)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One Hot Encoding Categorical Features\ndata = pd.get_dummies(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Churns vs Credit Card Category (Blue, Silver, Gold, Platinum)\nprint('Card        % of Customers     % of Churns')\nprint('------------------------------------------')\nprint('Blue: %15.2f %17.2f' % (data['Card_Category_Blue'].mean()*100, \n                               (data['Card_Category_Blue'] == data['Attrition_Flag']).mean()*100))\n\nprint('Silver: %13.2f %17.2f' % (data['Card_Category_Silver'].mean()*100, \n                                 (data['Card_Category_Silver'] == data['Attrition_Flag']).mean()*100))\n\nprint('Gold: %15.2f %17.2f' % (data['Card_Category_Gold'].mean()*100, \n                               (data['Card_Category_Gold'] == data['Attrition_Flag']).mean()*100))\n\nprint('Platinum: %11.2f %17.2f' % (data['Card_Category_Platinum'].mean()*100, \n                                   (data['Card_Category_Platinum'] == data['Attrition_Flag']).mean()*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separating features into x and target y Attrition_Flag (churns)\nx = data.iloc[:, 1:].values\ny = data.iloc[:, 0].values\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualizing our with some Dimensionality Reduction Algorithms**\n\nUsing PCA decomposition and t-SNE to see if we can find some clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling features\nscaler = StandardScaler()\nscaler.fit(x_train)\n\nscaled_x_train = scaler.transform(x_train)\nscaled_x_test = scaler.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=3)\npc = pca.fit_transform(scaled_x_train)\n\nprint('Components Variance:')\nprint('1st: %.3f' % pca.explained_variance_ratio_[0])\nprint('2nd: %.3f' % pca.explained_variance_ratio_[1])\nprint('3rd: %.3f' % pca.explained_variance_ratio_[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(pc[:, 0], pc[:, 1], c=y_train)\nplt.title(\"PCA\")\nplt.xlabel(\"1st Component\")\nplt.ylabel(\"2nd Component\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(8,8))\nax = Axes3D(fig)\nax.scatter(pc[:, 0], pc[:, 1], pc[:, 2], c=y_train)\nax.set_xlabel('1st Component')\nax.set_ylabel('2nd Component')\nax.set_zlabel('3rd Component')\nax.set_title('PCA')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=3)\ntsne_comp = tsne.fit_transform(scaled_x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(tsne_comp[:, 0], tsne_comp[:, 1], c=y_train)\nplt.title(\"t-SNE\")\nplt.xlabel(\"1st Component\")\nplt.ylabel(\"2nd Component\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8,8))\nax = Axes3D(fig)\nax.scatter(tsne_comp[:, 0], tsne_comp[:, 1], tsne_comp[:, 2], c=y_train)\nax.set_xlabel('1st Component')\nax.set_ylabel('2nd Component')\nax.set_zlabel('3rd Component')\nax.set_title('t-SNE')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing Classifiers\n\nWe'll test some classifiers using cross validation to determine which one we will fine tune and test it with our test set.\n\n* Logistic Regression\n* Decision Tree\n* LinearSVC\n* Random Forest\n* XGBoost\n* Gaussian Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import KFold, cross_val_score\n\n# Evaluate Models\n\nn_folds = 10\nmodels = []\n\nsvc_clf = LinearSVC(random_state=0, tol=1e-5, dual=False)\nsvc_clf.fit(scaled_x_train, y_train)\n\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('Tree', DecisionTreeClassifier()))\nmodels.append(('SVC', LinearSVC(dual=False)))\nmodels.append(('Forest', RandomForestClassifier()))\nmodels.append(('XGB', XGBClassifier(use_label_encoder=False, eval_metric='logloss')))\nmodels.append(('NB', GaussianNB()))\n\nfor name, model in models:\n    kfold = KFold(n_splits=n_folds)\n    cv_results = cross_val_score(model, scaled_x_train, y_train, cv=kfold, scoring='accuracy')\n    print(\"%6s %.3f %.3f \" % (name, cv_results.mean(), cv_results.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Grid Search Hyperparameter Tuning for XGBoost**\n\nNow we will use sklearn's grid search to find the best parameters for our XGBoost model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\nparams = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [5, 6, 7, 8],\n        }\n\nxgb_clf = xgb.XGBClassifier(learning_rate=0.3, n_estimators=600, objective='binary:logistic',\n                            nthread=1, use_label_encoder=False, eval_metric='logloss')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\n\nfolds = 3\nparam_comb = 5\n\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state = 25)\n\nsearch = RandomizedSearchCV(xgb_clf, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(x_train, y_train), random_state=25)\n\nsearch.fit(x_train, y_train)\n\nprint('Best hyperparameters:')\nprint(search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n        'min_child_weight': 1,\n        'gamma': 2,\n        'subsample': 1,\n        'colsample_bytree': 0.8,\n        'max_depth': 8,\n        }\n\nxgb_clf = xgb.XGBClassifier(learning_rate=0.3, n_estimators=600, objective='binary:logistic',\n                            nthread=1, use_label_encoder=False, eval_metric='logloss')\n\nxgb_clf.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model accuracy in our test set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_predictions = xgb_clf.predict(x_test)\nxgb_predictions = np.round(xgb_predictions)\nprint('XGBoost Test Set')\nprint('Accuracy: %.2f' % ((xgb_predictions == y_test).mean()*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_clf.get_booster().feature_names = list(data.columns[1:])\nxgb_clf.get_booster().get_score(importance_type=\"gain\")\n\nfig, ax = plt.subplots(1,1,figsize=(10,10))\nxgb.plot_importance(xgb_clf, max_num_features=35, ax=ax)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}