{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"63de4277-c079-1630-2e47-44d138f3e7bf"},"source":"# Library Import #"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"193a91a4-7404-a01b-2c78-6b7385979b9c"},"outputs":[],"source":"import re\nimport nltk\nimport pandas as pd\nimport numpy as np\n\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nenglish_stemmer=nltk.stem.SnowballStemmer('english')\n\nfrom sklearn.feature_selection.univariate_selection import SelectKBest, chi2, f_classif\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom sklearn.linear_model import SGDClassifier, SGDRegressor,LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport random\nimport itertools\n\nimport sys\nimport os\nimport argparse\nfrom sklearn.pipeline import Pipeline\nfrom scipy.sparse import csr_matrix\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport six\nfrom abc import ABCMeta\nfrom scipy import sparse\nfrom scipy.sparse import issparse\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils import check_X_y, check_array\nfrom sklearn.utils.extmath import safe_sparse_dot\nfrom sklearn.preprocessing import normalize, binarize, LabelBinarizer\nfrom sklearn.svm import LinearSVC\n\nfrom keras.preprocessing import sequence\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation, Lambda\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.recurrent import LSTM, SimpleRNN, GRU\nfrom keras.preprocessing.text import Tokenizer\nfrom collections import defaultdict\nfrom keras.layers.convolutional import Convolution1D\nfrom keras import backend as K\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n%matplotlib inline\nplt.style.use('ggplot')"},{"cell_type":"markdown","metadata":{"_cell_guid":"f98318d0-c122-24a1-a9a6-62ab93985428"},"source":"# Data Import"},{"cell_type":"markdown","metadata":{"_cell_guid":"c39b53ac-2080-bfb4-124d-5e775af2cec2"},"source":"we'll use all of the dates up to the end of 2014 as our training data and everything after as testing data."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"28217d5a-e580-fa92-fdc2-45a4842812e0"},"outputs":[],"source":"data = pd.read_csv('../input/Combined_News_DJIA.csv')\ntrain = data[data['Date'] < '2015-01-01']\ntest = data[data['Date'] > '2014-12-31']"},{"cell_type":"markdown","metadata":{"_cell_guid":"3d372c26-3c4c-7890-3089-c8ddb17ca81a"},"source":"# Data Process"},{"cell_type":"markdown","metadata":{"_cell_guid":"b43212e0-9344-a216-b86b-37cfbfb37752"},"source":"First, we transform the string of news into the  number of words as input."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a87f0d3d-80f3-af90-9368-dedde8c1154f"},"outputs":[],"source":"trainheadlines = []\nfor row in range(0,len(train.index)):\n    trainheadlines.append(' '.join(str(x) for x in train.iloc[row,2:27]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"66913298-12f3-da89-030a-90ba38b1e11b"},"outputs":[],"source":"basicvectorizer = CountVectorizer()\nbasictrain = basicvectorizer.fit_transform(trainheadlines)\nprint(basictrain.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"4647ab33-fd7e-0d98-9afd-7ea21eb02cf8"},"source":"## Logic Regression"},{"cell_type":"markdown","metadata":{"_cell_guid":"ee566614-2f26-df99-3c54-700a07cf83bd"},"source":"### Logic Regression 1"},{"cell_type":"markdown","metadata":{"_cell_guid":"dc0ac1c7-f4fe-3bbe-18fc-f578947626b1"},"source":"Algorithm: Logic Regression"},{"cell_type":"markdown","metadata":{"_cell_guid":"5a6e98cc-ec7f-3dd2-5dce-2c2c04cff260"},"source":"Input: the counts of single words"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5adfb2da-7467-505b-d44b-0a028e60227b"},"outputs":[],"source":"basicmodel = LogisticRegression()\nbasicmodel = basicmodel.fit(basictrain, train[\"Label\"])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"56d1ea73-641b-6fdf-698f-32518fe99dec"},"outputs":[],"source":"testheadlines = []\nfor row in range(0,len(test.index)):\n    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\nbasictest = basicvectorizer.transform(testheadlines)\npreds1 = basicmodel.predict(basictest)\nacc1=accuracy_score(test['Label'], preds1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bc13f327-70bd-2f8f-4ab7-6a9dfae356e9"},"outputs":[],"source":"print('Logic Regression 1 accuracy: ',acc1 )"},{"cell_type":"markdown","metadata":{"_cell_guid":"c08a6f88-d723-97b5-dd09-a61b1f3f7829"},"source":"The accuracy is only 0.42."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2303559d-4ad8-7d30-ff3d-ee3bb91440ae"},"outputs":[],"source":"basicwords = basicvectorizer.get_feature_names()\nbasiccoeffs = basicmodel.coef_.tolist()[0]\ncoeffdf = pd.DataFrame({'Word' : basicwords, \n                        'Coefficient' : basiccoeffs})\ncoeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\ncoeffdf.head(5)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"949e33a5-f63d-0059-2ee0-17e625aede36"},"outputs":[],"source":"coeffdf.tail(5)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6aa2253b-0cf6-4853-4ff5-6143dae71fb1"},"source":"### Logic Regression 2"},{"cell_type":"markdown","metadata":{"_cell_guid":"65239831-43f2-290b-8e21-b4a9f82e5a54"},"source":"Algorithm: Logic Regression"},{"cell_type":"markdown","metadata":{"_cell_guid":"a108baea-371d-d93f-7d65-213ecd4337ec"},"source":"Input: the counts of phrases with two connected words(exclude words which are too common like \"a\" ,\"an\" ,\"the\" and words too uncommon of which counts are too small )"},{"cell_type":"markdown","metadata":{"_cell_guid":"15df5336-f6f9-cd66-b020-6072aff3083b"},"source":"We delete phrases of which frequency lower than 0.03 or higher than 0.97"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dbdf7b4f-4673-49f1-d510-377026c95611"},"outputs":[],"source":"advancedvectorizer = TfidfVectorizer( min_df=0.03, max_df=0.97, max_features = 200000, ngram_range = (2, 2))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b6bf747c-021c-eb7f-cd50-2240138ed555"},"outputs":[],"source":"print(advancedtrain.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8c225d22-d997-5db8-877e-03022a0785b4"},"outputs":[],"source":"advancedmodel = LogisticRegression()\nadvancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1b3421b9-b326-4ef5-4235-2e764e5cd3a3"},"outputs":[],"source":"testheadlines = []\nfor row in range(0,len(test.index)):\n    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\nadvancedtest = advancedvectorizer.transform(testheadlines)\npreds2 = advancedmodel.predict(advancedtest)\nacc2=accuracy_score(test['Label'], preds2)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"422fb025-f985-e010-3e9e-2ee8c4494c3e"},"outputs":[],"source":"print('Logic Regression 2 accuracy: ', acc2)"},{"cell_type":"markdown","metadata":{"_cell_guid":"8f5a4ca4-75a0-6a24-2c34-61855d2b102a"},"source":"The accuracy is higher than input of single words."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3af2a0fb-a8ef-84f2-2a80-3568d327baec"},"outputs":[],"source":"advwords = advancedvectorizer.get_feature_names()\nadvcoeffs = advancedmodel.coef_.tolist()[0]\nadvcoeffdf = pd.DataFrame({'Words' : advwords, \n                        'Coefficient' : advcoeffs})\nadvcoeffdf = advcoeffdf.sort_values(['Coefficient', 'Words'], ascending=[0, 1])\nadvcoeffdf.head(5)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1fd8c2b-23fd-7dda-cf8d-0381c3d0736f"},"outputs":[],"source":"advcoeffdf.tail(5)"},{"cell_type":"markdown","metadata":{"_cell_guid":"39b0d7a4-cedc-d51c-11f4-d3a1d454682f"},"source":"### Logic Regression 3"},{"cell_type":"markdown","metadata":{"_cell_guid":"4cf79ce1-930d-474a-f07b-dd397f9a72eb"},"source":"Algorithm: Logic Regression"},{"cell_type":"markdown","metadata":{"_cell_guid":"a62cefba-3f14-ce21-49a5-1fdd09556dde"},"source":"Input: the counts of phrases with three connected words(exclude words which are too common like \"a\" ,\"an\" ,\"the\" and words too uncommon of which counts are too small )"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"886f33fe-d47f-d545-1b73-29f1e3618e01"},"outputs":[],"source":"advancedvectorizer = TfidfVectorizer( min_df=0.0039, max_df=0.1, max_features = 200000, ngram_range = (3, 3))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5437284a-b14e-96cd-e6d5-97c41790d42a"},"outputs":[],"source":"print(advancedtrain.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5e29bfca-0041-5b86-fdff-9c502f4aa1ad"},"outputs":[],"source":"advancedmodel = LogisticRegression()\nadvancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d5717cfb-3f8c-1af4-665b-6a22e6e9c9ba"},"outputs":[],"source":"testheadlines = []\nfor row in range(0,len(test.index)):\n    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\nadvancedtest = advancedvectorizer.transform(testheadlines)\npreds3 = advancedmodel.predict(advancedtest)\nacc3 = accuracy_score(test['Label'], preds3)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b700896c-936b-004d-2d70-50fefee2129c"},"outputs":[],"source":"print('Logic Regression 3 accuracy: ', acc3)"},{"cell_type":"markdown","metadata":{"_cell_guid":"2a438254-8acb-4db7-4f38-bb07be962dc2"},"source":"The accuracy is lower than input of phrases with two connected words."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"66fee024-03b4-5964-f925-cfb3746a4741"},"outputs":[],"source":"advwords = advancedvectorizer.get_feature_names()\nadvcoeffs = advancedmodel.coef_.tolist()[0]\nadvcoeffdf = pd.DataFrame({'Words' : advwords, \n                        'Coefficient' : advcoeffs})\nadvcoeffdf = advcoeffdf.sort_values(['Coefficient', 'Words'], ascending=[0, 1])\nadvcoeffdf.head(5)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c50f97d8-a6f7-0a60-9edf-42bb8b7aa492"},"outputs":[],"source":"advcoeffdf.tail(5)"},{"cell_type":"markdown","metadata":{"_cell_guid":"a4229efa-49d5-82fb-5a1d-e1ca4001e0ac"},"source":"## Naive Bayes"},{"cell_type":"markdown","metadata":{"_cell_guid":"5ccc2e71-28c6-f425-f229-f1395293bffc"},"source":"### NBayes 1"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"476ba54c-4b47-6029-e0de-29e6fcb64845"},"outputs":[],"source":"advancedvectorizer = TfidfVectorizer( min_df=0.1, max_df=0.7, max_features = 200000, ngram_range = (1, 1))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1ce37cf1-5c09-9fe7-09ec-a23d6d3b749e"},"outputs":[],"source":"print(advancedtrain.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4726d110-1821-ea71-dc5c-7a21ad628dd1"},"outputs":[],"source":"advancedmodel = MultinomialNB(alpha=0.01)\nadvancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\ntestheadlines = []\nfor row in range(0,len(test.index)):\n    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\nadvancedtest = advancedvectorizer.transform(testheadlines)\npreds4 = advancedmodel.predict(advancedtest)\nacc4=accuracy_score(test['Label'], preds4)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ff1f469d-1c12-5a3d-1aed-148179c26824"},"outputs":[],"source":"print('NBayes 1 accuracy: ', acc4)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"02a9958e-ab18-bf74-58f1-160c5b47cbff"},"outputs":[],"source":"advwords = advancedvectorizer.get_feature_names()\nadvcoeffs = advancedmodel.coef_.tolist()[0]\nadvcoeffdf = pd.DataFrame({'Words' : advwords, \n                        'Coefficient' : advcoeffs})\nadvcoeffdf = advcoeffdf.sort_values(['Coefficient', 'Words'], ascending=[0, 1])\nadvcoeffdf.head(5)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6223b466-48eb-0a0a-5d36-c082e50b1fe5"},"outputs":[],"source":"advcoeffdf.tail(5)"},{"cell_type":"markdown","metadata":{"_cell_guid":"dcc814d2-014d-5cb9-b106-52ff648d9c18"},"source":"### NBayes 2"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dfe60202-6348-d724-c860-c3b46c66c447"},"outputs":[],"source":"advancedvectorizer = TfidfVectorizer( min_df=0.03, max_df=0.2, max_features = 200000, ngram_range = (2, 2))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"28b21c32-28de-1bc4-161e-dcf63737caca"},"outputs":[],"source":"print(advancedtrain.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"23ab2ed6-8e9a-4291-2f9b-6fc2563f33ca"},"outputs":[],"source":"advancedmodel = MultinomialNB(alpha=0.0001)\nadvancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\ntestheadlines = []\nfor row in range(0,len(test.index)):\n    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\nadvancedtest = advancedvectorizer.transform(testheadlines)\npreds5 = advancedmodel.predict(advancedtest)\nacc5 = accuracy_score(test['Label'], preds5)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aa7d48c3-60cc-0895-7ed3-ddbf796339ff"},"outputs":[],"source":"print('NBayes 2 accuracy: ', acc5)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f49cb3c4-b80b-48a8-69be-ac6c579e2ed4"},"outputs":[],"source":"advwords = advancedvectorizer.get_feature_names()\nadvcoeffs = advancedmodel.coef_.tolist()[0]\nadvcoeffdf = pd.DataFrame({'Words' : advwords, \n                        'Coefficient' : advcoeffs})\nadvcoeffdf = advcoeffdf.sort_values(['Coefficient', 'Words'], ascending=[0, 1])\nadvcoeffdf.head(5)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"087f9b18-c50b-17b5-60b6-2cf2dada7c3c"},"outputs":[],"source":"advcoeffdf.tail(5)"},{"cell_type":"markdown","metadata":{"_cell_guid":"4bfcc58e-1ed4-18d5-a321-e58a4744dd8c"},"source":"## Random Forest"},{"cell_type":"markdown","metadata":{"_cell_guid":"a77b2afd-5654-aaff-74a9-0013fe4e8164"},"source":"### RF 1"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"91fe0b03-37a0-eb19-2632-cdc04067b12f"},"outputs":[],"source":"advancedvectorizer = TfidfVectorizer( min_df=0.01, max_df=0.99, max_features = 200000, ngram_range = (1, 1))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"878756f6-f57d-88cb-fe82-5181c1e40901"},"outputs":[],"source":"print(advancedtrain.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b561af90-db7d-ef58-26a2-5721d5432615"},"outputs":[],"source":"advancedmodel = RandomForestClassifier()\nadvancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\ntestheadlines = []\nfor row in range(0,len(test.index)):\n    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\nadvancedtest = advancedvectorizer.transform(testheadlines)\npreds6 = advancedmodel.predict(advancedtest)\nacc6 = accuracy_score(test['Label'], preds6)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"91fe3c5f-b005-97f0-da7c-597c29521877"},"outputs":[],"source":"print('RF 1 accuracy: ', acc6)"},{"cell_type":"markdown","metadata":{"_cell_guid":"76a971f3-878d-33e2-249a-a2b063aa5460"},"source":"### RF 2"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1db5461b-1ee9-2196-6725-66a6c2376a19"},"outputs":[],"source":"advancedvectorizer = TfidfVectorizer( min_df=0.03, max_df=0.2, max_features = 200000, ngram_range = (2, 2))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e1fb8019-c981-9d18-8f54-2750387ec071"},"outputs":[],"source":"print(advancedtrain.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bcddf034-4a10-2026-55ea-dd3d969a297b"},"outputs":[],"source":"advancedmodel = RandomForestClassifier()\nadvancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\ntestheadlines = []\nfor row in range(0,len(test.index)):\n    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\nadvancedtest = advancedvectorizer.transform(testheadlines)\npreds7 = advancedmodel.predict(advancedtest)\nacc7 = accuracy_score(test['Label'], preds7)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e395e3a6-7bc8-a434-16af-be13aebbabbd"},"outputs":[],"source":"print('RF 2 accuracy: ', acc7)"},{"cell_type":"markdown","metadata":{"_cell_guid":"0a12329a-12c7-aa90-a138-025087dd76ba"},"source":"## Gradient Boosting Machines"},{"cell_type":"markdown","metadata":{"_cell_guid":"ea1681ab-166f-20a6-3c29-f34f84438a36"},"source":"### GBM 1"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4ec11f0e-6c50-8149-93c3-acd17d1e7caa"},"outputs":[],"source":"advancedvectorizer = TfidfVectorizer( min_df=0.1, max_df=0.9, max_features = 200000, ngram_range = (1, 1))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3c5d312d-555f-f353-606e-2f560631033a"},"outputs":[],"source":"print(advancedtrain.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8f9d72ff-fb8e-5d1a-6016-3ebdb76a2ffd"},"outputs":[],"source":"advancedmodel = GradientBoostingClassifier()\nadvancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\ntestheadlines = []\nfor row in range(0,len(test.index)):\n    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\nadvancedtest = advancedvectorizer.transform(testheadlines)\npreds8 = advancedmodel.predict(advancedtest.toarray())\nacc8 = accuracy_score(test['Label'], preds8)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ba8ae675-7e71-e24e-69bf-99d5d19590d8"},"outputs":[],"source":"print('GBM 1 accuracy: ', acc8)"},{"cell_type":"markdown","metadata":{"_cell_guid":"e39653db-00b9-d1e0-d82e-87105e0eba1e"},"source":"### GBM 2"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d35d6a94-69f4-5370-6d0f-5b98c91d2565"},"outputs":[],"source":"advancedvectorizer = TfidfVectorizer( min_df=0.02, max_df=0.175, max_features = 200000, ngram_range = (2, 2))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b83bc122-5059-665a-28c1-85145787155e"},"outputs":[],"source":"print(advancedtrain.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e98ca989-15e6-b785-f282-4e59f849cd1f"},"outputs":[],"source":"advancedmodel = GradientBoostingClassifier()\nadvancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\ntestheadlines = []\nfor row in range(0,len(test.index)):\n    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\nadvancedtest = advancedvectorizer.transform(testheadlines)\npreds9 = advancedmodel.predict(advancedtest.toarray())\nacc9 = accuracy_score(test['Label'], preds9)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6df5f34f-a315-c1c4-fdbe-c54127468d5e"},"outputs":[],"source":"print('GBM 2 accuracy: ', acc9)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b7bd5500-e077-8225-31b5-a9c3f1015ca5"},"source":"## Stochastic Gradient Descent Classifier"},{"cell_type":"markdown","metadata":{"_cell_guid":"8f139d81-25c5-ba11-8924-e68b6e2e55f6"},"source":"### SGDClassifier 1"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d071bb2f-df6a-1657-1af8-5d31ee8e4a19"},"outputs":[],"source":"advancedvectorizer = TfidfVectorizer( min_df=0.2, max_df=0.8, max_features = 200000, ngram_range = (1, 1))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"24d0156a-2705-fece-001e-0426da57d383"},"outputs":[],"source":"print(advancedtrain.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"30e4b846-af7e-1eda-dade-f1550d6e043b"},"outputs":[],"source":"advancedmodel = SGDClassifier(loss='modified_huber', n_iter=5, random_state=0, shuffle=True)\nadvancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\ntestheadlines = []\nfor row in range(0,len(test.index)):\n    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\nadvancedtest = advancedvectorizer.transform(testheadlines)\npreds10 = advancedmodel.predict(advancedtest.toarray())\nacc10 = accuracy_score(test['Label'], preds10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79a91d72-1cd5-4308-9b0e-9810314cdd1a"},"outputs":[],"source":"print('SGDClassifier 1: ', acc10)"},{"cell_type":"markdown","metadata":{"_cell_guid":"0f46cdfe-20b2-9bc6-103a-1f44e2172789"},"source":"### SGDClassifier 2"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8ccd7ace-7326-4dfe-b93d-a8377e1a3ba6"},"outputs":[],"source":"advancedvectorizer = TfidfVectorizer( min_df=0.03, max_df=0.2, max_features = 200000, ngram_range = (2, 2))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c012b95d-8575-ef34-d699-d240ade631f0"},"outputs":[],"source":"print(advancedtrain.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5e4f2994-e199-976d-f54b-0e676b6b11be"},"outputs":[],"source":"advancedmodel = SGDClassifier(loss='modified_huber', n_iter=5, random_state=0, shuffle=True)\nadvancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\ntestheadlines = []\nfor row in range(0,len(test.index)):\n    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\nadvancedtest = advancedvectorizer.transform(testheadlines)\npreds11 = advancedmodel.predict(advancedtest.toarray())\nacc11 = accuracy_score(test['Label'], preds11)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a9d8499e-ae6a-6197-c86f-c31bf73f3f98"},"outputs":[],"source":"print('SGDClassifier 2: ', acc11)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d6aa0178-e7f3-6427-f88f-aa6e436fc87d"},"source":"## Naive Bayes SVM"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cd1391ca-1b70-13c0-3a62-151fc84f6069"},"outputs":[],"source":"class NBSVM(six.with_metaclass(ABCMeta, BaseEstimator, ClassifierMixin)):\n\n    def __init__(self, alpha=1.0, C=1.0, max_iter=10000):\n        self.alpha = alpha\n        self.max_iter = max_iter\n        self.C = C\n        self.svm_ = [] # fuggly\n\n    def fit(self, X, y):\n        X, y = check_X_y(X, y, 'csr')\n        _, n_features = X.shape\n\n        labelbin = LabelBinarizer()\n        Y = labelbin.fit_transform(y)\n        self.classes_ = labelbin.classes_\n        if Y.shape[1] == 1:\n            Y = np.concatenate((1 - Y, Y), axis=1)\n\n        # LabelBinarizer().fit_transform() returns arrays with dtype=np.int64.\n        # so we don't have to cast X to floating point\n        Y = Y.astype(np.float64)\n\n        # Count raw events from data\n        n_effective_classes = Y.shape[1]\n        self.class_count_ = np.zeros(n_effective_classes, dtype=np.float64)\n        self.ratios_ = np.full((n_effective_classes, n_features), self.alpha,\n                                 dtype=np.float64)\n        self._compute_ratios(X, Y)\n\n        # flugglyness\n        for i in range(n_effective_classes):\n            X_i = X.multiply(self.ratios_[i])\n            svm = LinearSVC(C=self.C, max_iter=self.max_iter)\n            Y_i = Y[:,i]\n            svm.fit(X_i, Y_i)\n            self.svm_.append(svm) \n\n        return self\n\n    def predict(self, X):\n        n_effective_classes = self.class_count_.shape[0]\n        n_examples = X.shape[0]\n\n        D = np.zeros((n_effective_classes, n_examples))\n\n        for i in range(n_effective_classes):\n            X_i = X.multiply(self.ratios_[i])\n            D[i] = self.svm_[i].decision_function(X_i)\n        \n        return self.classes_[np.argmax(D, axis=0)]\n        \n    def _compute_ratios(self, X, Y):\n        \"\"\"Count feature occurrences and compute ratios.\"\"\"\n        if np.any((X.data if issparse(X) else X) < 0):\n            raise ValueError(\"Input X must be non-negative\")\n\n        self.ratios_ += safe_sparse_dot(Y.T, X)  # ratio + feature_occurrance_c\n        normalize(self.ratios_, norm='l1', axis=1, copy=False)\n        row_calc = lambda r: np.log(np.divide(r, (1 - r)))\n        self.ratios_ = np.apply_along_axis(row_calc, axis=1, arr=self.ratios_)\n        check_array(self.ratios_)\n        self.ratios_ = sparse.csr_matrix(self.ratios_)\n\n        #p_c /= np.linalg.norm(p_c, ord=1)\n        #ratios[c] = np.log(p_c / (1 - p_c))\n\n\ndef f1_class(pred, truth, class_val):\n    n = len(truth)\n\n    truth_class = 0\n    pred_class = 0\n    tp = 0\n\n    for ii in range(0, n):\n        if truth[ii] == class_val:\n            truth_class += 1\n            if truth[ii] == pred[ii]:\n                tp += 1\n                pred_class += 1\n                continue;\n        if pred[ii] == class_val:\n            pred_class += 1\n\n    precision = tp / float(pred_class)\n    recall = tp / float(truth_class)\n\n    return (2.0 * precision * recall) / (precision + recall)\n\n\ndef semeval_senti_f1(pred, truth, pos=2, neg=0): \n\n    f1_pos = f1_class(pred, truth, pos)\n    f1_neg = f1_class(pred, truth, neg)\n\n    return (f1_pos + f1_neg) / 2.0;\n\n\ndef main(train_file, test_file, ngram=(1, 3)):\n    print('loading...')\n    train = pd.read_csv(train_file, delimiter='\\t', encoding='utf-8', header=0,\n                        names=['text', 'label'])\n\n    # to shuffle:\n    #train.iloc[np.random.permutation(len(df))]\n\n    test = pd.read_csv(test_file, delimiter='\\t', encoding='utf-8', header=0,\n                        names=['text', 'label'])\n\n    print('vectorizing...')\n    vect = CountVectorizer()\n    classifier = NBSVM()\n\n    # create pipeline\n    clf = Pipeline([('vect', vect), ('nbsvm', classifier)])\n    params = {\n        'vect__token_pattern': r\"\\S+\",\n        'vect__ngram_range': ngram, \n        'vect__binary': True\n    }\n    clf.set_params(**params)\n\n    #X_train = vect.fit_transform(train['text'])\n    #X_test = vect.transform(test['text'])\n\n    print('fitting...')\n    clf.fit(train['text'], train['label'])\n\n    print('classifying...')\n    pred = clf.predict(test['text'])\n   \n    print('testing...')\n    acc = accuracy_score(test['label'], pred)\n    f1 = semeval_senti_f1(pred, test['label'])\n    print('NBSVM: acc=%f, f1=%f' % (acc, f1))"},{"cell_type":"markdown","metadata":{"_cell_guid":"7f0e000a-ca1a-5611-0ca8-c8bc40eafecb"},"source":"### NBSVM 1"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e054e5ea-c8be-cb98-90a1-2512b3478b6e"},"outputs":[],"source":"advancedvectorizer = TfidfVectorizer( min_df=0.1, max_df=0.8, max_features = 200000, ngram_range = (1, 1))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)\nprint(advancedtrain.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b57a2b30-d69f-cf0a-05d0-8a065d5a758b"},"outputs":[],"source":"advancedmodel = NBSVM(C=0.01)\nadvancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\ntestheadlines = []\nfor row in range(0,len(test.index)):\n    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\nadvancedtest = advancedvectorizer.transform(testheadlines)\npreds12 = advancedmodel.predict(advancedtest)\nacc12 = accuracy_score(test['Label'], preds12)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6acdade4-665e-5c06-95af-3e5ddcfd33d6"},"outputs":[],"source":"print('NBSVM 1: ', acc12)"},{"cell_type":"markdown","metadata":{"_cell_guid":"aca410f2-118d-f973-6b14-a63048444f26"},"source":"### NBSVM 2"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"663e5e98-7ccd-29a4-40a8-10a46327c06b"},"outputs":[],"source":"advancedvectorizer = TfidfVectorizer( min_df=0.031, max_df=0.2, max_features = 200000, ngram_range = (2, 2))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)\nprint(advancedtrain.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0c67fed4-8110-2a52-bb33-1da6f294b592"},"outputs":[],"source":"advancedmodel = NBSVM(C=0.01)\nadvancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\ntestheadlines = []\nfor row in range(0,len(test.index)):\n    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\nadvancedtest = advancedvectorizer.transform(testheadlines)\npreds13 = advancedmodel.predict(advancedtest)\nacc13 = accuracy_score(test['Label'], preds13)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"803861e3-d740-c2ed-2b2e-57221ba9b8c8"},"outputs":[],"source":"print('NBSVM 2: ', acc13)"},{"cell_type":"markdown","metadata":{"_cell_guid":"682e897f-169c-8694-d05d-66e9fc844158"},"source":"## Deep Learning"},{"cell_type":"markdown","metadata":{"_cell_guid":"1de5a061-52d5-2578-9cb0-2602499ad87b"},"source":"### MLP"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ac32628f-8080-5d53-e3a2-3d6986431f15"},"outputs":[],"source":"batch_size = 32\nnb_classes = 2\nadvancedvectorizer = TfidfVectorizer( min_df=0.04, max_df=0.3, max_features = 200000, ngram_range = (2, 2))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)\ntestheadlines = []\nfor row in range(0,len(test.index)):\n    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\nadvancedtest = advancedvectorizer.transform(testheadlines)\nprint(advancedtrain.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eca75f7e-4fe5-0be4-85ee-47fecf3ad4d8"},"outputs":[],"source":"X_train = advancedtrain.toarray()\nX_test = advancedtest.toarray()\n\nprint('X_train shape:', X_train.shape)\nprint('X_test shape:', X_test.shape)\ny_train = np.array(train[\"Label\"])\ny_test = np.array(test[\"Label\"])\n\nY_train = np_utils.to_categorical(y_train, nb_classes)\nY_test = np_utils.to_categorical(y_test, nb_classes)\n\n\n# pre-processing: divide by max and substract mean\nscale = np.max(X_train)\nX_train /= scale\nX_test /= scale\n\nmean = np.mean(X_train)\nX_train -= mean\nX_test -= mean\n\ninput_dim = X_train.shape[1]\n\n# Here's a Deep Dumb MLP (DDMLP)\nmodel = Sequential()\nmodel.add(Dense(256, input_dim=input_dim))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\n\n# we'll use categorical xent for the loss, and RMSprop as the optimizer\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n\nprint(\"Training...\")\nmodel.fit(X_train, Y_train, nb_epoch=2, batch_size=16, validation_split=0.15, show_accuracy=True)\n\nprint(\"Generating test predictions...\")\npreds14 = model.predict_classes(X_test, verbose=0)\nacc14 = accuracy_score(test[\"Label\"], preds14)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"39345ebd-2cf2-8332-6fb2-d00bc801af57"},"outputs":[],"source":"print('prediction accuracy: ', acc14)"},{"cell_type":"markdown","metadata":{"_cell_guid":"143c5d6d-4259-c945-f1dd-b07a4c388526"},"source":"### LSTM"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"db232b70-f3a0-0952-e7ab-9c0b768e8025"},"outputs":[],"source":"max_features = 10000\nEMBEDDING_DIM = 100\nVALIDATION_SPLIT = 0.1\nmaxlen = 200\nbatch_size = 32\nnb_classes = 2"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e5f6e70a-325d-5a94-0c16-c852f951d475"},"outputs":[],"source":"# vectorize the text samples into a 2D integer tensor\ntokenizer = Tokenizer(nb_words=max_features)\ntokenizer.fit_on_texts(trainheadlines)\nsequences_train = tokenizer.texts_to_sequences(trainheadlines)\nsequences_test = tokenizer.texts_to_sequences(testheadlines)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a9224d6a-3bca-ef84-9817-a72b41241f54"},"outputs":[],"source":"print('Pad sequences (samples x time)')\nX_train = sequence.pad_sequences(sequences_train, maxlen=maxlen)\nX_test = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n\nY_train = np_utils.to_categorical(y_train, nb_classes)\nY_test = np_utils.to_categorical(y_test, nb_classes)\n\n\nprint('X_train shape:', X_train.shape)\nprint('X_test shape:', X_test.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"29359f0d-8d42-2038-c04f-2cd5bc87bc93"},"outputs":[],"source":"print('Build model...')\nmodel = Sequential()\nmodel.add(Embedding(max_features, 128, dropout=0.2))\nmodel.add(LSTM(128, dropout_W=0.2, dropout_U=0.2)) \nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nprint('Train...')\nmodel.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=3,\n          validation_data=(X_test, Y_test))\nscore, acc = model.evaluate(X_test, Y_test,\n                            batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)\n\n\nprint(\"Generating test predictions...\")\npreds15 = model.predict_classes(X_test, verbose=0)\nacc15 = accuracy_score(test['Label'], preds15)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"365d4d4b-57cd-c678-6e62-133aa32586fb"},"outputs":[],"source":"print('prediction accuracy: ', acc15)"},{"cell_type":"markdown","metadata":{"_cell_guid":"8afb0d07-7d0d-dd18-b19c-be603f301b52"},"source":"## CNN"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7271a1d1-cde0-a26c-de99-edfb7dfbf918"},"outputs":[],"source":"nb_filter = 120\nfilter_length = 2\nhidden_dims = 120\nnb_epoch = 2"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c071e2fc-2106-4bc5-2e2f-dde4a84dabdf"},"outputs":[],"source":"print('Build model...')\nmodel = Sequential()\nmodel.add(Embedding(max_features, 128, dropout=0.2))\n# we add a Convolution1D, which will learn nb_filter\n# word group filters of size filter_length:\nmodel.add(Convolution1D(nb_filter=nb_filter,\n                        filter_length=filter_length,\n                        border_mode='valid',\n                        activation='relu',\n                        subsample_length=1))\n\ndef max_1d(X):\n    return K.max(X, axis=1)\n\nmodel.add(Lambda(max_1d, output_shape=(nb_filter,)))\nmodel.add(Dense(hidden_dims)) \nmodel.add(Dropout(0.2)) \nmodel.add(Activation('relu'))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"61e07990-43ef-ff27-3b36-dca146f8b4a3"},"outputs":[],"source":"print('Train...')\nmodel.fit(X_train, Y_train, batch_size=32, nb_epoch=1,\n          validation_data=(X_test, Y_test))\nscore, acc = model.evaluate(X_test, Y_test,\n                            batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)\n\n\nprint(\"Generating test predictions...\")\npreds16 = model.predict_classes(X_test, verbose=0)\nacc16 = accuracy_score(test['Label'], preds16)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"49e9abbe-d224-b757-c17b-bd4cfa5da818"},"outputs":[],"source":"print('prediction accuracy: ', acc16)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"21b86295-1a3b-a1b8-78cf-b032b9d74eaa"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"91aa05fb-b8fe-fe99-d191-c08b8dc5620c"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"60fa6ce4-1658-11d2-c0ad-3b43a542fff0"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"03273578-b025-4e87-6088-a993fce31e2f"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}