{"cells":[{"metadata":{},"cell_type":"markdown","source":"## COVID-19 Open Research Dataset Challenge (CORD-19)\n\nAn AI challenge to to the world's artificial intelligence experts to develop text and data mining tools that can help the medical community develop answers to high priority scientific questions. The CORD-19 dataset represents the most extensive machine-readable coronavirus literature collection available for data mining to date. This allows the worldwide AI research community the opportunity to apply text and data mining approaches to find answers to questions within, and connect insights across, this content in support of the ongoing COVID-19 response efforts worldwide. There is a growing urgency for these approaches because of the rapid increase in coronavirus literature, making it difficult for the medical community to keep up.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Our approach\n\nIn this attempt of solving the above mentioned challenge, we use community creation technique for cluster creations for query expansion of the search. Most of this work is highly influenced by our paper - https://arxiv.org/pdf/2002.02238.pdf, which somewhat works on the same ground. \n\nThe various steps of our method are as follows:\n1. We include all the documents present in the dataset, remove duplicates present in the dataset based on 'SHA' values.\n2. We consider the abstract of the papers retrieved and processed considering the above process. We only considered papers whose abstract was present, removed the rest.\n3. We removed stopwords ,numerical values and punctuations from the processed dataset and then lemmatized the words for model training.\n4. We train Word2Vec model on the dataset with the embedding size of 256. (The dimension of embedding was calculated from this work - https://github.com/ziyin-dl/word-embedding-dimensionality-selection)\n5. Calculating the embedding size based on the dataset helped us to generate knowledge infused word embeddings tailored to domain knowledge.\n6. After the embedding generation, we used the top 50000 words present in the word2vec model with their 10 most similar words for generating semantic clusters and capture contextual information of these words.\n7. The above mentioned words were used to built a corpus graph which helps in finding similar communities present in the dataset and using this information for answering the queries of the task and challenge.\n8. Then, we built a BM25 search index on the dataset with the keywords of the abstract paper infused by knowledge of corpus graph for Query Expansion methodology.\n9. Lastly, we retrieve the results of the query of the task through two different techniques- Token based search and Embedding based search to demonstrate the efficacy of our methods.\n\n\n## Enter your search query here\n\nEnter the search query here.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"search_query = 'anticoagulant therapy treatment hypercoagulable state thrombophilia anticoagulation'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_md-0.2.4.tar.gz\n!pip install en_core_sci_md\n!pip install gensim\n!pip install sklearn\n!pip install matplotlib\n!pip install networkx\n!pip install wordcloud\n!pip install rank-bm25","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing relavant packages and setting packages\nimport numpy as np\nimport pandas as pd\nimport os\nimport json\nimport glob\nimport sys\n\nfrom rank_bm25 import BM25Okapi\nfrom networkx.algorithms.community import k_clique_communities\n\n#search_query = \"Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery.\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting all the files saved into a list and then iterate over them like below\n# to extract relevant information\n\n# Hold this information in a dataframe and then move forward from there.\n\n# Just set up a quick blank dataframe to hold all these medical papers.\n\ncorona_features = {\"doc_id\": [None], \"source\": None, \"title\": [None], \"abstract\": [None],\n                  \"text_body\": [None]}\n\ncorona_df = pd.DataFrame.from_dict(corona_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# accessing all the json files. Using glob for the job\nroot_dir = '/kaggle/input/CORD-19-research-challenge'\n#json_filenames = glob.glob(f'{root_dir}/**/*.json', recursive=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# we read through the metadata file for finding duplicate papers and relevant data points for the search engine\nsources = pd.read_csv(f'{root_dir}/metadata.csv')\nsources.drop_duplicates(subset=['sha'], inplace=True)\n\ndef doi_url(d):\n    if d.startswith('http://'):\n        return d\n    elif d.startswith('doi.org'):\n        return f'http://{d}'\n    else:\n        return f'http://doi.org/{d}'\n    \nsources.doi = sources.doi.fillna('').apply(doi_url)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(sources)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sources.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_texts = sources.abstract\nall_texts.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Text Preprocessing\n\nUsing spacy's biomedical tokenizer for text cleaning and basic preprocessing like stopwords removal, word in lemma form, etc.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# text preprocessing step\nimport en_core_sci_md\nnlp = en_core_sci_md.load(disable=[\"tagger\", \"parser\", \"ner\"])\nnlp.max_length = 2000000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenizer function to generate tokens from the text file based on spacy's biomedical tokenizer\ndef spacy_tokenizer(sentence):\n    # remove numbers (e.g. from references [1], etc.)\n    try:\n        return [word.lemma_ for word in nlp(sentence) if not (word.like_num or word.is_stop or word.is_punct or word.is_space)]\n    except:\n        return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# removing customized stopwords prevalent in the domain of medicine research\ncustomize_stop_words = ['doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', \n    'et', 'al', 'author', 'figure', 'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'fig', 'fig.', \n    'al.', 'di', 'la', 'il', 'del', 'le', 'della', 'dei', 'delle', 'una', 'da',  'dell',  'non', 'si']\n\n# Mark them as stop words\nfor w in customize_stop_words:\n    nlp.vocab[w].is_stop = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#saving merge dataframe\nall_texts = all_texts.apply(spacy_tokenizer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word2Vec Model Creation\n\nCreating Word2Vec for the biomedical domain to understand relevant words and generating their embeddings.\nThis is useful in generating a word cluster graph on the dataset and finding relevance between different words and clusters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import Word2Vec\nfrom matplotlib import pyplot\nimport gensim\n\nmodel = Word2Vec(all_texts, min_count=1, size=256, workers=4)\n\nX = model[model.wv.vocab]\nwords = model.wv.vocab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using a frequency distribution calculation to find relevant words present in the dataset to emphasis on important\n# terms and find relationship between these words\n\nword_freq = dict()\nwords = list(model.wv.vocab)\nfor i in range(len(words)):\n    words[i] = words[i].replace(\"'\",\"\")\n    if words[i] not in word_freq:\n        if len(words[i]) > 2:\n            word_freq[words[i]] = 0\n        \nprint(len(list(word_freq.keys())))\n\nno_list = all_texts.index\nfor i in no_list:\n    for j in range(len(all_texts[i])):\n        tmp = all_texts[i][j]\n        if tmp in word_freq:\n            word_freq[tmp] += 1 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_freq = {k: v for k, v in sorted(word_freq.items(), key=lambda item: item[1], reverse=True)}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Graph Generation\n\nGraph is created to find relationship between words and finding various communities present in the dataset\n\nFinding topn similar words for word in our vocabulary. This will help in recognizing clusters present in the data ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import networkx as nx\nfrom scipy.spatial.distance import cosine\nG2 = nx.Graph()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#G2 = nx.Graph()\nwords = list(word_freq.keys())\nfor i in range(len(words)):\n       \n    try:\n        other_words = list(model.similar_by_word(words[i], topn=10))\n    except:\n        pass\n    # print(other_words[0][0])\n    \n    if len(words[i]) > 2:\n        G2.add_node(words[i])\n        \n        for j in range(len(other_words)):\n            words[i] = words[i].replace(\"'\",\"\")\n            w = other_words[j][0].replace(\"'\",\"\") \n            if len(w) > 2:\n                G2.add_node(w)\n                try:\n                    sim_score = 1/(1 - cosine(model[words[i]], model[w]))\n                    G2.add_weighted_edges_from([(words[i], w, sim_score)])\n                except:\n                    pass\n        \nprint(G2.number_of_edges())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"root_out_dir = '/kaggle/working'\nnx.write_gpickle(G2, f\"{root_out_dir}/graph_2_256.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"G2.number_of_nodes()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"G2.number_of_edges()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Writing communities created in a file for better evaluation of the algorithm on the dataset\ndef comm_hyper(G2,k):\n    c = list(k_clique_communities(G2, k))\n    print(len(c))\n    with open(\"set_\"+str(k)+\".txt\",\"w\", encoding='utf8') as f:\n        for i in range(len(c)):\n            f.write(\"\\n------------------------------------------------------\\n\")\n            f.write(str(len(c[i]))+\"\\n\")\n            f.write(str(c[i]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Community Identification\n\nThe important point is to look at the communities generated and choose the best set of generation among that. Too big a community with various informational sections isn't preferred so we need to narrow such communities down with the help of hyperparameter 'k'.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Performing Clique Perlocation Technique for community finding in the graph created above.\n# This step is more like tuning step where the value of \"k\" can be changed for various reasons\n# comm_hyper(graph, k)\n\ncomm_hyper(G2, 4)\ncomm_hyper(G2, 5)\ncomm_hyper(G2, 6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comm_4 = list(k_clique_communities(G2, 5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_1 = list(comm_4[14])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Word Cloud creation for visualization \nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\ncloud_word = ' '\nfor word in word_1:\n    cloud_word = cloud_word + word + ' '\n    \nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(width=800, height=800, background_color='white',\n                     stopwords=stopwords, min_font_size=10).generate(cloud_word)\n\n# plotting the word cloud image\nplt.figure(figsize=(8,8), facecolor=None)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Community Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Statistical analysis for communities based on number of words and different overlapping topics\n\nmax_len = 0\navg_len = 0\nmin_len = len(comm_4[0])\n\nfor i in range(len(comm_4)):\n    temp_len = len(comm_4[i])\n    if temp_len > max_len:\n        max_len = temp_len\n        \n    if temp_len < min_len:\n        min_len = temp_len\n        \n    avg_len += temp_len\n    \nprint(max_len)\nprint(min_len)\nprint(avg_len/(len(comm_4)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Community Selection\n\nWe looked at all the communities and figured out the fact that communities with less members are mostly insignificant to major knowledge discovery and analysis, so we removed communities with length less than the average length of communities. This step helps us in keeping the information boom in check for retrieval purposes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"comm = dict()\nfor i in range(len(comm_4)):\n    if len(comm_4[i]) > 10:\n        comm[i] = list()\n        word_list = comm_4[i]\n        for word in word_list:\n            comm[i].append(word.lower())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(list(comm.keys())))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"columns = list()\ncolumns.append(\"doc_id\")\nfor cid in comm.keys():\n    columns.append(cid)\ncomm_data = pd.DataFrame(columns=columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Text Retrieval Part\n\nPerforming query expansion with community matching using both embedding techniques and token based matching","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#search_query = search_query +\" therapeutics, interventions and clinical studies. hypercoagulable state in COVID-19. efficacy of novel therapeutics anticoagulant therapy treatment hypercoagulable state thrombophilia anticoagulation\"\n\n# matching query with communities - token search\ncomm_list = dict()\ncheck = spacy_tokenizer(search_query)\nmax_key = list(comm.keys())[0]\nfor key in comm.keys():\n    freq = [1 if word in check else 0 for word in comm[key]]\n    comm_list[key] = sum(freq)\n    if comm_list[key] > comm_list[max_key]:\n        max_key = key\n    \nprint(comm_list[max_key])\nprint(max_key)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_word_frequency(word, text):\n    count = 0\n    for wd in text:\n        if wd == word:\n            count += 1\n            \n    return count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using smooth inverse frequency for calculation of sentence embedding\ndef query_vector(query, embedding_size=256, a=1e-3):\n    vs = np.zeros(embedding_size)\n    for word in query:\n        a_value = a /(a + get_word_frequency(word, query))\n        vs = np.add(vs, np.multiply(a_value, model[word]))\n        \n    vs = np.divide(vs, len(query))\n    return vs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# consolidated sentence embedding from word2vec\nvs = query_vector(check)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comm_vec = dict()\nfor key in comm.keys():\n    try:\n        temp = query_vector(comm[key])\n        comm_vec[key] = temp\n    except:\n        pass","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"from scipy.spatial.distance import cosine\nmax_score = 0\nfor key in comm_vec.keys():\n    temp_score = 1/(1-cosine(vs, comm_vec[key]))\n    if temp_score > max_score:\n        max_score = temp_score\n        key_val = key","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def embedding_community_matching(query, comm, key_val):\n    # embedding based community search scope expansion\n    for word in comm[key_val]:\n        query.append(word)\n        \n    return query","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def community_matching(query, comm):\n    # token based community search\n    temp = 0\n    for key in comm.keys():\n        res = 0\n        for word in query:\n            if word in comm[key]:\n                res += 1\n        if res >= temp:\n            temp = res\n            comm_key = key\n            \n    for word in comm[comm_key]:\n        query.append(word)\n    return query","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BM25 Search Engine\n\nFor indexing the documents and searching the query based on query expansion from community generation to retreive relevant documents","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bm25_index = BM25Okapi(all_texts.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def search(search_tokens, num_results=10):\n    scores = bm25_index.get_scores(search_tokens)\n    top_indexes = np.argsort(scores)[::-1][:num_results]\n    \n    return top_indexes,scores\n\n# text = \"novel corona virus\"\n# indexes = search(spacy_tokenizer(text))\n# indexes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Token Based Search with Community Information","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"query_text = spacy_tokenizer(search_query)\nquery_text = community_matching(query_text, comm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indexes, scores = search(query_text, 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sources.loc[sources.index[indexes]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Embedding based Search with Community Information","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"query_text = spacy_tokenizer(search_query)\nquery_text = embedding_community_matching(query_text, comm, key_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indexes2, scores = search(query_text, 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sources.loc[sources.index[indexes2]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Finding Query Words in Papers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import html\nimport random\nfrom IPython.core.display import display, HTML\n\n# Prevent special characters like & and < to cause the browser to display something other than what you intended.\ndef html_escape(text_body):\n    return html.escape(text_body)\n\n\ndef text_highlight(text_body, query_text):\n    # Remove duplicate words from text_body\n    seen = set()\n\n    for item in text_body:\n        if item not in seen:\n            seen.add(item)\n\n    # Create random sample weights for each unique word\n    weights = []\n    for i in range(len(query_text)):\n        weights.append(0.5)\n\n    df_coeff = pd.DataFrame({'word': query_text, 'num_code': weights})\n\n    # Select the code value to generate different weights\n    word_to_coeff_mapping = {}\n\n    for row in df_coeff.iterrows():\n        row = row[1]\n        word_to_coeff_mapping[row[0]] = row[1]\n\n    max_alpha = 0.8\n    highlighted_text_body = []\n    for word in text_body:\n        try:\n            weight = word_to_coeff_mapping[word]\n        except:\n            weight = None\n        \n        if weight is not None:\n            highlighted_text_body.append('<span style=\"background-color:rgba(135,206,250,' + str(weight/max_alpha) + ');\">' + html_escape(word) + '</span>')\n        else:\n            highlighted_text_body.append(word)\n\n    highlighted_text_body = ' '.join(highlighted_text_body)\n    display(HTML(highlighted_text_body))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_highlight(all_texts.loc[sources.index[indexes2[2]]], query_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_index = sources.index[indexes2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now we iterate over the files and populate the data frame\n\ndef return_corona_df(json_filenames, df, sources, new_index):\n\n# len(json_filenames)\n    for i in range(len(json_filenames)):\n        file_name = json_filenames[i]\n        row = {\"doc_id\": None, \"source\": None, \"title\":None, \"abstract\":None, \n               \"text_body\":None}\n\n        with open(file_name) as json_data:\n            data = json.load(json_data)\n\n            row['doc_id'] = data['paper_id']\n            row['title'] = data['metadata']['title']\n\n            # Now need all of abstract. Put it all in a list then use str.join()\n            # to split it into paragraphs\n\n\n            if sources.loc[new_index[i]]['pdf_json_files'] != 'NaN':\n                row['abstract'] = sources.loc[new_index[i]]['abstract']\n\n            # And lastly the body of the text to be added.\n\n            body_list = []\n            for _ in range(len(data['body_text'])):\n                try:\n                    body_list.append(data['body_text'][_]['text'])\n                except:\n                    pass\n\n            body = \"\\n\".join(body_list)\n            row['text_body'] = body\n\n            # Now just add to the dataframe\n            df = df.append(row, ignore_index=True)\n            del row\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"json_filenames = list()\nfor i in range(len(new_index)):\n    if sources.loc[new_index[i]]['pdf_json_files'] != 'NaN':\n        json_filenames.append(f'{root_dir}/'+sources.loc[new_index[i]]['pdf_json_files'].split(';')[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df = return_corona_df(json_filenames, corona_df, sources, new_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df = corona_df.iloc[1:].reset_index(drop=True)\ncorona_df.drop_duplicates(subset=['doc_id'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Drug Discovery","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from bs4 import BeautifulSoup\nimport requests as r\nimport string\nimport time\nimport re\n\nalphabet = list(string.ascii_lowercase)\n\nlinks = []\nfor letter in alphabet:\n    links.append(f'https://druginfo.nlm.nih.gov/drugportal/drug/names/{letter}')\n    \npages = []\nfor link in links:\n    page = r.get(link)\n    pages.append(page.text)\n    time.sleep(3)\n    \ndrug_tables = []\nfor p in pages:    \n    parser = BeautifulSoup(p, 'html.parser')\n    tables = parser.find_all('table')\n    drug_tables.append(tables[2])\n    \ndrug_names = []\nfor table in drug_tables:\n    a_refs = table.find_all('a')\n    for a in a_refs:\n        drug_names.append(a.string)\n        \ndrug_names.append('oseltamivir') #adding other relevant drugs by hand\ndrug_names.append('lopinavir') #adding other relevant drugs by hand\n\nnih_dnames = pd.DataFrame({'drug_name':drug_names})\nnih_dnames.drug_name = nih_dnames.drug_name.str.lower()\nnih_dnames.to_csv('nih_dnames.csv', index=False)\n\n\ndrug_names = nih_dnames.drug_name\n\ndef find_drugs(df):\n    mentioned_drugs = []\n    for article in df.iterrows():\n        article_drugs = ''\n        for drug in drug_names:\n            if re.search(fr'([\\W\\b])({drug})([\\b\\W])', article[1]['text_body'].lower()) != None:\n                article_drugs = article_drugs + drug + ';'\n        if (len(article_drugs) > 0):\n            article_drugs = article_drugs[:-1]\n        mentioned_drugs.append(article_drugs)\n    return mentioned_drugs\n\n\nsearch_drugs = find_drugs(corona_df)\ncorona_df['mentioned_drugs'] = search_drugs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"page = r.get('https://www.ef.edu/english-resources/english-grammar/numbers-english/')\nparser = BeautifulSoup(page.content, 'html.parser')\ntables = parser.find_all('table')\ncardinal_numbers = pd.read_html(str(tables[0]))[0].loc[0:37, \"Cardinal\"]\n\ndef find_sample_sizes(df):\n    patient_num_list = []\n    for abstract in df.text_body:\n        matches = re.findall(r'(\\s)([0-9,]+)(\\s|\\s[^0-9\\s]+\\s)(patients)', abstract)\n        num_patients = ''\n        for match in matches:\n            num_patients = num_patients + ''.join(match[1:]) + ';'\n        for number in cardinal_numbers:\n            cardinal_regex_search = re.search(fr'({number})(\\s)(patients)', abstract)\n            if cardinal_regex_search != None:\n                num_patients = num_patients + cardinal_regex_search[0] + ';'\n        num_patients = num_patients[:-1]\n        patient_num_list.append(num_patients)\n    return patient_num_list\n\ncorona_df['sample_size'] = find_sample_sizes(corona_df)\ncorona_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\ndef remove_stopwords(words):\n    stop_words = stopwords.words('english')\n    clean = [word for word in words if word not in stop_words]\n    return clean\n\nstudy_type_adjs = ['cohort', 'observational', 'clinical', \n              'randomized', 'open-label', 'control',\n              'case', 'meta-analysis', 'systematic',\n              'narrative', 'literature', 'critical',\n              'retrospective', 'controlled', 'non-randomized',\n              'secondary', 'rapid', 'double-blinded',\n              'open-labelled', 'concurrent', 'pilot', \n              'empirical', 'retrospective', 'single-center',\n              'collaborative', 'case-control']\nstudy_type_nouns = ['study', 'trial', 'report', 'review',\n                   'analysis', 'studies']\n\ndef clean(string):\n    temp = re.sub(r'[^\\w\\s]', '', string)\n    temp = re.sub(r'\\b\\d+\\b', '', temp)\n    temp = re.sub(r'\\s+', ' ', temp)\n    temp = re.sub(r'^\\s', '', temp)\n    temp = re.sub(r'\\s$', '', temp)\n    return temp.lower()\n\ndef find_study_types(df):\n    study_types = []\n    for article in df.iterrows():\n        title = article[1].title.lower()\n        clean_title = clean(title)\n        title_tokens = word_tokenize(clean_title)\n        title_tokens = remove_stopwords(title_tokens)\n        study_type = ''\n        study_type_name = False\n\n        article_study_type = []\n        for word in title_tokens:\n            if word in study_type_adjs:\n                study_type = study_type + word + ' '\n                study_type_name = True\n\n            elif word in study_type_nouns:\n                study_type = study_type + word + ' '\n                study_type_name = True\n\n            if (study_type_name == False):\n                study_type = ''\n\n        study_types.append(study_type[:-1])\n\n    return study_types\n\ncorona_stypes = find_study_types(corona_df)\ncorona_df['study_types'] = corona_stypes\ncorona_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"severity_types = ['severe', 'critical', 'icu', 'mild']\n\ndef find_severities(df):\n    severity_of_disease = []\n    for abstract in df.text_body:\n        text = re.sub('Severe acute respiratory', 'sar', abstract)\n        severities = []\n        tokens = word_tokenize(text)\n        for severity_type in severity_types:\n            if severity_type in tokens:\n                severities.append(severity_type)\n        severity_of_disease.append(';'.join(severities))\n    return severity_of_disease\n\ncorona_severities = find_severities(corona_df)\ncorona_df['severity_of_disease'] = corona_severities\ncorona_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"page = r.get('https://www.ef.edu/english-resources/english-grammar/numbers-english/')\nparser = BeautifulSoup(page.content, 'html.parser')\ntables = parser.find_all('table')\ncardinal_numbers = pd.read_html(str(tables[0]))[0].loc[0:37, \"Cardinal\"]\n\ndef find_sample_sizes(df):\n    patient_num_list = []\n    for abstract in df.text_body:\n        matches = re.findall(r'(\\s)([0-9,]+)(\\s|\\s[^0-9\\s]+\\s)(patients)', abstract)\n        num_patients = ''\n        for match in matches:\n            num_patients = num_patients + ''.join(match[1:]) + ';'\n        for number in cardinal_numbers:\n            cardinal_regex_search = re.search(fr'({number})(\\s)(patients)', abstract)\n            if cardinal_regex_search != None:\n                num_patients = num_patients + cardinal_regex_search[0] + ';'\n        num_patients = num_patients[:-1]\n        patient_num_list.append(num_patients)\n    return patient_num_list\n\ncorona_df['sample_size'] = find_sample_sizes(corona_df)\ncorona_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def output_csv(df, new_index):\n    \n    i = 0\n    results = pd.DataFrame(columns=['cord_id','date','study','study_link','journal', \n                        'study_type', 'therapeutic_method', 'sample_size', \n                        'severity_of_disease', 'conclusion_excerpt', 'primary_endpoints', \n                        'clinical_imporovement', 'added_on'])\n    for j in df.index:\n        #print(paper_row)\n        new_row = {'cord_id': sources.loc[new_index[i]]['cord_uid'],\n                   'date':sources.loc[new_index[i]]['publish_time'],\n                   'study_link':[sources.loc[new_index[i]]['url']], \n                    'journal':[sources.loc[new_index[i]]['journal']], \n                   'study_type':[df.loc[j].study_types], \n                   'therapeutic_method':[df.loc[j].mentioned_drugs], \n                    'sample_size':[df.loc[j].sample_size], \n                   'severity_of_disease':[df.loc[j].severity_of_disease], \n                   'conclusion_excerpt':[df.loc[j].text_body], \n                    'primary_endpoints':[''], \n                   'clinical_improvement':[''], \n                   'added_on':['']}\n        \n        results = results.append(new_row, ignore_index=True)\n        i = i+1\n        \n    return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = output_csv(corona_df, new_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.to_csv('results.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}