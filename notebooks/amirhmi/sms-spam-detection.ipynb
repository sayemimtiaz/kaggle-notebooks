{"cells":[{"metadata":{},"cell_type":"markdown","source":"> <h1>SMS Spam Detection</h1>"},{"metadata":{},"cell_type":"markdown","source":"**Define Problem **\n\nFirst part of dealing with a machine learning and data science problem is defining the problem.\nHere, our problem is easy to understand. We have two kind of SMS : <br> 1. Spam<br> 2. Ham<br>\n\nWe have a dataset which contains 5574 English SMS that each SMS labeled that is spam or ham. So we have a supervised classification problem."},{"metadata":{},"cell_type":"markdown","source":"**Loading Data**\n\nAs we use kaggle dataset, we don't need gathering data. We use pandas library to read csv file and loading it in a pandas dataframe.\n\nThe dataset file name is <mark style=\"background-color: LightYellow\">spam.csv</mark> and it exists in <mark style=\"background-color: LightYellow\">input</mark> directory."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<mark style=\"background-color: LightYellow\">load_data</mark> function, get path of directory(../input), filename(spam.csv)  and data file coded as it's parameters and load csv file with pandas and finally return a pandas dataframe.\n\nWe call load_data with <mark style=\"background-color: LightYellow\">latin1</mark> coded and load it in <mark style=\"background-color: LightYellow\">spam</mark> variable."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport os\n\ndef load_data(path, filename, codec='utf-8'):\n  csv_path = os.path.join(path, filename)\n  print(csv_path)\n  return pd.read_csv(csv_path, encoding=codec)\n\nspam = load_data('../input', 'spam.csv', codec='latin1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"pandas <mark style=\"background-color: LightYellow\">head</mark> method, returns 5 first row of dataframe.\n\nWe can see that dataframe has 5 column :\n1. **v1:** dataset label that categorized to <mark style=\"background-color: LightYellow\">spam</mark> and <mark style=\"background-color: LightYellow\">ham</mark> label\n2. **v2:** first line of SMS that can not be empty\n3. **Unnamed: 2** second line of SMS\n4. **Unnamed: 3** third line of SMS\n5. **Unnamed: 4** fourth line of SMS"},{"metadata":{"trusted":true},"cell_type":"code","source":"spam.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this step, we <mark style=\"background-color: LightYellow\">rename</mark> dataframe column for simplifying dealing with it.\n\nThen in spam <mark style=\"background-color: LightYellow\">describe</mark> method, you can see that column names are renamed. We also get some information from this table, see that a few number of messages are more than one line and also most of the messages are unique."},{"metadata":{"trusted":true},"cell_type":"code","source":"spam.columns = ['label', 'line1', 'line2', 'line3', 'line4']\nspam.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualize Data**\n\nWe use <mark style=\"background-color: LightYellow\">seaborn</mark> and <mark style=\"background-color: LightYellow\">matplotlib</mark> libraries for visualiztion and plotting.\n\nIn this two plot we that over 86% of 5574 SMSs are ham and 13.4% of them are spam."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, axs = plt.subplots(1, 2, figsize=(12, 6))\nsns.countplot(spam['label'], ax=axs[0])\naxs[1].pie(spam.groupby(spam['label'])['line1'].count(), labels=['ham', 'spam'], autopct='%1.1f%%', startangle=90, pctdistance=0.85)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also need to know how many SMSs which are more than two lines are spam. we can see 90% of such SMSs are ham"},{"metadata":{"trusted":true},"cell_type":"code","source":"spam_with_more_line = spam[spam['line2'].notnull()]\nf, axs = plt.subplots(1, 2, figsize=(12, 6))\nsns.countplot(spam_with_more_line['label'], ax=axs[0])\naxs[1].pie(spam_with_more_line.groupby(spam_with_more_line['label'])['line1'].count(), labels=['ham', 'spam'], autopct='%1.1f%%',\n           startangle=90, pctdistance=0.85)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Cleaning**\n\nNow we should <mark style=\"background-color: LightYellow\">concat</mark> sencod, third and fourth line, with the first line. For this purpose we should fill their <mark style=\"background-color: LightYellow\">NaN</mark> values with empty string. Then we can add lines with each other and save it new column called <mark style=\"background-color: LightYellow\">text</mark>.\n\nNow we can drop line1, line2, line3 and line4 from spam."},{"metadata":{"trusted":true},"cell_type":"code","source":"spam['line2'].fillna('', inplace=True)\nspam['line3'].fillna('', inplace=True)\nspam['line4'].fillna('', inplace=True)\n\nspam['text'] = spam['line1'] + ' ' + spam['line2'] + ' ' + spam['line3'] + ' ' + spam['line4']\n\nspam.drop(['line1', 'line2', 'line3', 'line4'], axis=1, inplace=True)\n\nspam.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Adding new feature**\n\nFrom the histogram below, we can see that spam SMSs have more characters than ham SMSs. So we add new feature called len to our data and fill it with length of each text message."},{"metadata":{"trusted":true},"cell_type":"code","source":"spam_with_len = spam.copy()\nspam_with_len['len'] = spam['text'].str.len()\n\nspam_with_len.hist(column='len', by='label', bins=25, figsize=(15, 6), color = \"skyblue\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Create Train and Test Set**\n\nWe load data once again to prevent changes. (we change data in pipeline in next steps)<br>\n\nWe use 20% of data as test set and remaining data as train set."},{"metadata":{"trusted":true},"cell_type":"code","source":"spam = load_data('../input', 'spam.csv', 'latin1')\ntxts = spam.drop(['v1'], axis=1)\nlabels = spam['v1']\nx_train, x_test, y_train, y_test = txts[:4457], txts[4457:], labels[:4457], labels[4457:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We should categorize labels, map spam to 1 and ham to 0. We can also use sklearn LabelEncoder."},{"metadata":{"trusted":true},"cell_type":"code","source":"label_map_func = lambda x: 1 if x == 'spam' else 0\n\ny_test = list(map(label_map_func, y_test))\ny_train = list(map(label_map_func, y_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"x_test data indexes starts from 4457, we should reset it's indexing."},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = x_test.reset_index().drop(['index'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Prepare Data for Machine Learning Algorithm**\n\nFor this goal we use <mark style=\"background-color: LightYellow\">nltk library</mark>.\nNLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries.\n\nWe write our own custom pipline friendly transformation and use them step by step in pipeline."},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize.casual import TweetTokenizer\nfrom nltk.stem import PorterStemmer ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. **Concat All Lines**<br>\nConcat all lines and remove additional column as we said in previous parts."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass ConcatLines(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        pass\n    def transform(self, X):\n        X['Unnamed: 2'].fillna('', inplace=True)\n        X['Unnamed: 3'].fillna('', inplace=True)\n        X['Unnamed: 4'].fillna('', inplace=True)\n\n        X['text'] = X['v2'] + ' ' + X['Unnamed: 2'] + ' ' + X['Unnamed: 3'] + ' ' + X['Unnamed: 4']\n        X.drop(['v2', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1, inplace=True)\n        return X\nspam = ConcatLines().transform(spam)\nspam.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. **Add Length Feature**\n\nIn this transformation, we add <mark style=\"background-color: LightYellow\">length</mark> of each SMS in column called len"},{"metadata":{"trusted":true},"cell_type":"code","source":"class AddLength(BaseEstimator, TransformerMixin):\n    def __init__(self, textAttr='text', lenAttr='len'):\n        self.lenAttr = lenAttr\n        self.textAttr = textAttr\n    def fit(self, X, y=None):\n        pass\n    def transform(self, X):\n        X[self.lenAttr] = X[self.textAttr].str.len()\n        return X\nspam = AddLength().transform(spam)\nspam.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. **LowerCase All Words**<br>\nAll words in setence should be <mark style=\"background-color: LightYellow\">lowercase</mark>, because we have to check if two words are equal or not, So both of them should be lowercase or uppercase."},{"metadata":{"trusted":true},"cell_type":"code","source":"class ToLowerCase(BaseEstimator, TransformerMixin):\n    def __init__(self, textAttr='text', lenAttr='len'):\n        self.lenAttr = lenAttr\n        self.textAttr = textAttr\n    def fit(self, X, y=None):\n        pass\n    def transform(self, X):\n        X[self.textAttr] = X[self.textAttr].str.lower()\n        return X\n    \nspam = ToLowerCase().transform(spam)\nspam.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. **Tokenize String**:\n\nTokenizing string using nltk tweet tokenizer. It tokenize text to list of words and it can find out emojies and not delete them."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Tokenize(BaseEstimator, TransformerMixin):\n    def __init__(self, textAttr='text', lenAttr='len'):\n        self.lenAttr = lenAttr\n        self.textAttr = textAttr\n    def fit(self, X, y=None):\n        pass\n    def transform(self, X):\n        x_len = X[self.lenAttr]\n        x_text = X[self.textAttr]\n        x_text = [TweetTokenizer().tokenize(str(x)) for x in x_text]\n        X['text'] = x_text\n        X['len'] = x_len\n        return X\n    \nspam = Tokenize().transform(spam)\nspam.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5. **Removing Stop word**:\n\nIn case of omitting stopwords, we should select which words are stopwords. Fortunately nltk give us a huge list of stop words. We also choose one char english alphabet and punctuaions as stopwords. We could simply remove words equal to stopwords.\n\nWe also remove empty words and words finished with dot. \n\n**Stemming Words**\n\nStemming is the process of producing morphological variants of a root/base word. Stemming programs are commonly referred to as stemming algorithms or stemmers. A stemming algorithm reduces the words “chocolates”, “chocolatey”, “choco” to the root word, “chocolate” and “retrieval”, “retrieved”, “retrieves” reduce to the stem “retrieve”.<br>\n[Refrence link](https://www.geeksforgeeks.org/python-stemming-words-with-nltk/)\n![](https://www.wolfram.com/language/11/text-and-language-processing/assets.en/generate-and-verify-stemmed-words/O_51.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\n\nclass RemoveStopWordsAndStem:\n    def __init__(self, textAttr='text', lenAttr='len'):\n        self.lenAttr = lenAttr\n        self.textAttr = textAttr\n        self.ps = ps = PorterStemmer()\n    def fit(self, X, y=None):\n        pass\n    def transform(self, X):\n        alphabet = list(string.ascii_lowercase)\n        stop_words = list(stopwords.words('english'))\n        puncs = list(string.punctuation)\n        stop_words = stop_words + puncs + alphabet\n        x_text = X[self.textAttr]\n        text = []\n        for i in range(len(X)):\n            filtered_sentence = []\n            for w in x_text[i]: \n                if w not in stop_words:\n                    w = w.rstrip(\".\")\n                    if w is not \"\":\n                        filtered_sentence.append(self.ps.stem(w))\n            text.append(filtered_sentence)\n        X[self.textAttr] = text\n        return X\nspam = RemoveStopWordsAndStem().transform(spam)\nspam.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"6. **Substitute Emoji, Website and Number**:\n\n\n* In this step we want to substitute emojies with \"emoji\" string. we know that almost all emojies start with : ; = > and since we have deleted punctuation marks from words, so words with 2 or more characters starting with char above are labeled as emoji.\n\n* We know that websites are start with https:// or www. or ends with .com or something like that. So we substitute website urls with website label.\n\n* This dataset has a lot of numbers, we should label number and they can find with [isnumeric()](https://www.programiz.com/python-programming/methods/string/isnumeric) function and they labeled as digitnumber."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Substitute:\n    def __init__(self, textAttr='text', lenAttr='len'):\n        self.lenAttr = lenAttr\n        self.textAttr = textAttr\n        self.emoji_list = emoji_list = [':', ';', '>', '=']\n        self.website_list = ['.com', '.org', '.co.uk', '.net', 'http', 'www.']\n    def fit(self, X, y=None):\n        pass\n    def transform(self, X):\n        x_text = X[self.textAttr]\n        text = []\n        for i in range(len(X)):\n            text.append(self.substitute(x_text[i]))\n        X[self.textAttr] = text\n        return X\n    \n    def substitute(self, words):\n        for i in range(len(words)):\n            if self.is_emoji(words[i]):\n                words[i] = 'emoji'\n            elif words[i].isnumeric():\n                words[i] = 'digitnumber'\n            else :\n                for site in self.website_list:\n                    if site in words[i]:\n                        words[i] = 'website'\n        return words\n    def is_emoji(self, word):\n        return word[0] in self.emoji_list and len(word) > 1\n\nspam = Substitute().transform(spam)\nspam.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"7. **Creating Sparse Matrix**\n\nIn the last part of pipeline we should create the existance matrix that told us that a row contains which words. We first find all words after data cleaning from train data, then if fill column with repeating number of column's word.\n\nThen we should Transform it to sparse matrix that only saves non zero column. This step done for better performance during machine learning algorithms."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import sparse\nimport numpy as np\n\nclass ToSparseMatrix:\n    def __init__(self, train_set, textAttr='text', lenAttr='len'):\n        self.lenAttr = lenAttr\n        self.textAttr = textAttr\n        self.train_words = tokenize_pipeline.transform(train_set.copy())\n    def fit(self, X, y=None):\n        pass\n    def transform(self, X):\n        if self.train_words is None:\n            self.train_words = X\n        self.final_words = np.array([x for t in self.train_words[self.textAttr] for x in t])\n        self.final_words = np.unique(self.final_words)\n        matrix = np.array([[0 for x in range(len(self.final_words) + 1)] for y in range(len(X))])\n        x_texts = list(X[self.textAttr])\n        x_len = list(X[self.lenAttr])\n        for i in range(len(x_texts)):\n            for token in x_texts[i]:\n                cond = np.where(self.final_words == token)\n                if(len(cond[0]) > 0):\n                    matrix[i][cond[0][0]] += 1\n            matrix[i][-1] = x_len[i]\n        return sparse.csr_matrix(matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creating Pipeline**\n\nWe should create a pipeline from previous steps because we need all of them together for both training set and test set in case of creating sparse matrix. Also we need them for for finding final words from training set.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\n\ntokenize_pipeline = Pipeline([\n    ('concat lines', ConcatLines()),\n    ('add length', AddLength()),\n    ('lower case words', ToLowerCase()),\n    ('tokenize words', Tokenize()),\n    ('remove stopwords', RemoveStopWordsAndStem()),\n    ('substitute emoji, web, number', Substitute()),\n])\n\nsparse_pipeline = Pipeline([\n    ('sparse pipeline', ToSparseMatrix(x_train.copy(deep=True)))\n])\n\nx_train_prepared = tokenize_pipeline.transform(x_train.copy(deep=True))\nx_train_prepared = sparse_pipeline.transform(x_train_prepared)\nx_train_prepared","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Try to find Best Model**\n\nWe should import all classification models that guess to work for this problem, then create a model variable to work with."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nmodels = [\n    ('svc', SVC(kernel='rbf')),\n    ('neighbors', KNeighborsClassifier(3)),\n    ('random_forest', RandomForestClassifier()),\n    ('sgd', SGDClassifier()), \n    ('mutlinomial_nb', MultinomialNB()),\n    ('complement_nb', ComplementNB()),\n    ('bernoli_nb', BernoulliNB()),\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it's time to compute score for each model. For this goal, we use cross_val_score function from sklearn and show scores for each model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score, roc_auc_score\nscores = pd.DataFrame([], columns=['model', 'accuracy', 'auc', 'precision', 'recall', 'f1'])\nfor model in models:\n    pred = cross_val_predict(model[1], x_train_prepared, y_train, cv=3, n_jobs=-1)\n    scores = scores.append({\n        'model':model[0],\n        'accuracy' : accuracy_score(y_train, pred),\n        'auc' : roc_auc_score(y_train, pred),\n        'precision': precision_score(y_train, pred),\n        'recall': recall_score(y_train, pred),\n        'f1': f1_score(y_train, pred),\n    }, ignore_index=True)\nscores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Score Analysis **\n\nWe now that in this classification problem, accuracy is important but recall and precesion is more important than accuracy. As we can see almost all models have high accuracy but some of them have very low f1-score which calculate from presicion and recall. so we must to select the best model but which one of them is the best?"},{"metadata":{},"cell_type":"markdown","source":"**Why The Naive Bayes Model Works So Well**\n\nThe Naive Bayes model works on the assumption that the features of the dataset are independent of each other — hence called Naive.<br>\nThis works well for bag-of-words models a.k.a text documents since:\n* words in a text document are independent of each other.\n* the location of one word doesn’t depend on another word.\n\nThus satisfying the independence assumption of the Naive Bayes model. Hence, it is most commonly used for text classification, sentiment analysis, spam filtering & recommendation systems.\n\n[Reference](https://towardsdatascience.com/sms-text-classification-a51defc2361c)"},{"metadata":{},"cell_type":"markdown","source":"**Find Best Hyperparameter**\n\nWe choose multinomial model and use GridSearchCV to find the best hyperparameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nestimator = BernoulliNB()\ngrid_params = {\n    'alpha': [0, 0.08, 0.09, 0.10, 0.11, 0.15],\n    'binarize': [0, 0.1, 0.3, 0.5],\n    'fit_prior': [True, False],\n    'class_prior': [None, [0.4, 0.6]],\n}\ngrid_search = GridSearchCV(estimator, grid_params, scoring='recall')\ngrid_search.fit(x_train_prepared, y_train)\ngrid_search.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"best model from grid search"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model = grid_search.best_estimator_\nfinal_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Find scores on the test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_prepared = tokenize_pipeline.transform(x_test.copy(deep=True))\nx_test_prepared = sparse_pipeline.transform(x_test_prepared)\npred = cross_val_predict(final_model, x_test_prepared, y_test, cv=3)\n\nprint(\"Precision: \", precision_score(y_test, pred))\nprint(\"Recall: \", recall_score(y_test, pred))\nprint(\"f1_score: \", f1_score(y_test, pred))\nprint(\"Accuracy: \", accuracy_score(y_test, pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that by tuning model's hyperparameter, we can reach to 98% accuracy and f1_score 93%."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}