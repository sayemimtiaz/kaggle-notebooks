{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Iris Flower Dataset"},{"metadata":{},"cell_type":"markdown","source":"### Context\n\nThe Iris flower data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems. It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species. The data set consists of 50 samples from each of three species of Iris (Iris Setosa, Iris virginica, and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters."},{"metadata":{},"cell_type":"markdown","source":"#### Import libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom pandas.plotting import parallel_coordinates\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn import metrics\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EDA"},{"metadata":{},"cell_type":"markdown","source":"### Import dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/iris-flower-dataset/IRIS.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check if we have any missing values ? "},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's analyse quickly our target "},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.species.value_counts().plot(kind=\"bar\",color='green')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 50 species in each categories"},{"metadata":{},"cell_type":"markdown","source":"There are different types of plots like bar plot, box plot, scatter plot etc.\nScatter plot is very useful when we are analyzing the relation ship between 2 features on x and y axis.\nIn seaborn library we have pairplot function which is very useful to scatter plot all the features at once instead of plotting them individually."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#sns.pairplot(dataset)\nsns.pairplot(dataset, hue=\"species\", height = 2, palette = 'colorblind');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that some variables seem to be highly correlated, e.g. petal_length and petal_width. In addition, the petal measurements separate the different species better than the sepal ones."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,11))\nsns.heatmap(dataset.corr(),annot=True)\nplt.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will see how these features are correlated to each other using heatmap in seaborn library. We can see that Sepal Length and Sepal Width features are slightly correlated with each other."},{"metadata":{},"cell_type":"markdown","source":"Let’s see how our data is distributed based on Sepal Length and Width features using scatterplot."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.FacetGrid(dataset,hue=\"species\").map(plt.scatter,\"sepal_length\",\"sepal_width\").add_legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similarly scatter plot of data based on Petal Length and Width features\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.FacetGrid(dataset,hue=\"species\").map(plt.scatter,\"petal_length\",\"petal_width\").add_legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have now a better overview of our dataset. \nLet's conclude this EDA with my favorite automatic EDA libraries --- Pandas Profiling"},{"metadata":{},"cell_type":"markdown","source":"### Pandas Profiling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas_profiling import ProfileReport","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"design_report = ProfileReport(dataset)\ndesign_report.to_file(output_file='report.html')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another cool visualization tool is parallel coordinate plot, which represents each row as a line."},{"metadata":{"trusted":true},"cell_type":"code","source":"parallel_coordinates(dataset, \"species\", color = ['blue', 'red', 'green']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we have seen before, petal measurements can separate species better than the sepal ones."},{"metadata":{},"cell_type":"markdown","source":"## Train-Test Split\n"},{"metadata":{},"cell_type":"markdown","source":"Now, we can split the dataset into a training set and a test set. Usually, we should also have a validation set, which is used to evaluate the performance of each classifier, fine-tune, and determine the best model. The test set is mainly used for reporting. However, due to the small size of this dataset, we can simplify it by using the test set to serve the purpose of the validation set."},{"metadata":{"trusted":true},"cell_type":"code","source":"list(dataset.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = dataset.drop('species',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = dataset.species","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,stratify=y,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build Classifiers"},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier_dt = DecisionTreeClassifier(max_depth = 3, random_state = 1)\nclassifier_dt.fit(X_train,y_train)\nprediction=classifier_dt.predict(X_test)\nprint(\"Accuracy Score : \" , accuracy_score(prediction,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This decision tree predicts 89.4% of the test data correctly."},{"metadata":{},"cell_type":"markdown","source":"We can use feature_importances to understand the importance of each predictor. "},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier_dt.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the output and based on the indices of the four features, we know that the first two features (sepal measurements) are of no importance, and only the petal ones are used to build this tree."},{"metadata":{},"cell_type":"markdown","source":"*We can also visiualize the classification rules*"},{"metadata":{"trusted":true},"cell_type":"code","source":"fn = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\ncn = ['setosa', 'versicolor', 'virginica']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,8))\nplot_tree(classifier_dt, feature_names = fn, class_names = cn, filled = True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Another way to show the prediction results is through a confusion matrix:"},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_mat = metrics.plot_confusion_matrix(classifier_dt, X_test, y_test,\n                                 display_labels=cn,\n                                 cmap=plt.cm.Blues,\n                                 normalize=None)\nconf_mat.ax_.set_title('Decision Tree Confusion matrix, without normalization');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thanks to classification report we can see precision,recal and f1 of each class."},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion = confusion_matrix(y_test, prediction)\nprint('Confusion Matrix\\n')\nprint(confusion)\n\n#importing accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nprint('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test, prediction)))\n\n\n\nfrom sklearn.metrics import classification_report\nprint('\\nClassification Report\\n')\nprint(classification_report(y_test, prediction, target_names=['Class 1', 'Class 2', 'Class 3']))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Precision: It tells you what fraction of predictions as a positive class were actually positive. To calculate precision, use the following formula: TP/(TP+FP).\n- Recall: It tells you what fraction of all positive samples were correctly predicted as positive by the classifier. It is also known as True Positive Rate (TPR), Sensitivity, Probability of Detection. To calculate Recall, use the following formula: TP/(TP+FN).\n- F1-score: It combines precision and recall into a single measure. Mathematically it’s the harmonic mean of precision and recall. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}