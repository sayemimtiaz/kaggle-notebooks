{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Churn Problem for Bank Customer\n\n## Can you develop a machine learning model that can predict the customers who will leave the company?\n\n- The aim is to estimate whether a bank's customers leave the bank or not.\n\n- The event that defines the customer abandonment is the closing of the customer's bank account.\n\n### Data Set Story:\n\n- It consists of 10000 observations and 12 variables.\n- Independent variables contain information about customers.\n- Dependent variable refers to customer abandonment.\n\n### Features:\n\n- Surname: Surname\n- CreditScore: Credit score\n- Geography: Country (Germany / France / Spain)\n- Gender: Gender (Female / Male)\n- Age: Age\n- Tenure: How many years of customer\n- Balance: Balance\n- NumOfProducts: Bank product used\n- HasCrCard: Credit card status (0 = No, 1 = Yes)\n- IsActiveMember: Active membership status (0 = No, 1 = Yes)\n- EstimatedSalary: Estimated salary\n- Exited: Abandoned or not? (0 = No, 1 = Yes)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. Installing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.linear_model import LogisticRegression  \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score, GridSearchCV,train_test_split,cross_val_score\nimport itertools\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neighbors import LocalOutlierFactor # çok değişkenli aykırı gözlem incelemesi\nfrom sklearn.preprocessing import scale,StandardScaler, MinMaxScaler,Normalizer,RobustScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import  accuracy_score, f1_score, precision_score,confusion_matrix, recall_score, roc_auc_score\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nwarnings.filterwarnings(\"ignore\", category=FutureWarning) \nwarnings.filterwarnings(\"ignore\", category=UserWarning) \n\n%config InlineBackend.figure_format = 'retina'\n\n# to display all columns and rows:\npd.set_option('display.max_columns', None); pd.set_option('display.max_rows', None);  # to display all columns and rows\npd.set_option('display.float_format', lambda x: '%.2f' % x) # The number of numbers that will be shown after the comma.\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/churn-for-bank-customers/churn.csv\", index_col=0)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. EDA (Exploratory of Data Analysis)\n### 2.1. Data Preperation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dependent_variable_name = \"Exited\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_prepare():\n    df_prep = df.copy()\n    \n    \n    missing_value_len = df.isnull().any().sum()\n    if missing_value_len == 0:\n        print(\"No Missing Value\")\n    else:\n        print(\"Investigate Missing Value, Missing Value : \" + str(missing_value_len))\n    print(\"\\n\")\n    \n    show_unique_count_variables(df = df_prep)\n    \n    \n    df_prep['Tenure'] =  df_prep.Tenure.astype(np.float)\n    df_prep['NumOfProducts'] =  df_prep.NumOfProducts.astype(np.float)\n    return df_prep\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unique value data representation on all variables:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_unique_count_variables(df):\n    for index, value in df.nunique().items():\n        print(str(index) + \"\\n\\t\\t\\t:\" + str(value))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2. Outliers Observe (LOF method and Supress)\n#### 2.2.1. Outlier Editing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef show_outliers(df):\n    \n    #tüm sütünları yukardan bakarak  outlier feature'ları gözlemleme\n    \n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    df_num_cols = df.select_dtypes(include=numerics)\n    sns.set(font_scale = 0.7) \n    fig, axes = plt.subplots(nrows = 2, ncols = 5, gridspec_kw =  dict(hspace=0.3), figsize = (12,9))\n    fig.tight_layout()\n    for ax,col in zip(axes.flatten(), df_num_cols.columns):\n        sns.boxplot(x = df_num_cols[col], color='green', ax = ax)\n    fig.suptitle('Observing Outliers', color = 'r', fontsize = 14)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.2.2. Visualization of outliers according to the LOF method","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef lof_observation(df):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    df_num_cols = df.select_dtypes(include=numerics)\n    df_outlier = df_num_cols.astype(\"float64\")\n    clf = LocalOutlierFactor(n_neighbors = 20, contamination = 0.1)\n    clf.fit_predict(df_outlier)\n    df_scores = clf.negative_outlier_factor_\n    scores_df = pd.DataFrame(np.sort(df_scores))\n    \n    scores_df.plot(stacked=True, xlim = [0,20], color='r', title='Visualization of outliers according to the LOF method', style = '.-');                # first 20 observe\n    th_val = np.sort(df_scores)[2]\n    outliers = df_scores > th_val\n    df = df.drop(df_outlier[~outliers].index)\n    df.shape\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.2.3. Outiler suppression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef clear_outliers(df):\n    \n    #yas ve kredi_skoru baskılama\n    \n    Q1 = df[\"Age\"].quantile(0.25)\n    Q3 = df[\"Age\"].quantile(0.75)\n    IQR = Q3 - Q1\n    lower = Q1 - 1.5 * IQR\n    upper = Q3 + 1.5 * IQR\n    print(\"When age and credit score is printed below lower score: \", lower, \"and upper score: \", upper)\n    df_outlier = df[\"Age\"][(df[\"Age\"] > upper)]\n    df[\"Age\"][df_outlier.index] = upper\n    \n    \n    #kredi_skoru\n    \n    Q1 = df[\"CreditScore\"].quantile(0.25)\n    Q3 = df[\"CreditScore\"].quantile(0.75)\n    IQR = Q3 - Q1\n    lower = Q1 - 1.5 * IQR\n    upper = Q3 + 1.5 * IQR\n    print(\"When age and credit score is printed above lower score: \", lower, \"and upper score: \", upper)\n    df_outlier = df[\"CreditScore\"][(df[\"CreditScore\"] < lower)]\n    df[\"CreditScore\"][df_outlier.index] = lower\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.2.4. Outiler Process","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef outlier_process(df):\n    #show_outliers(df = df)\n    df_outlier = lof_observation(df = df)\n    df_outlier = clear_outliers(df = df_outlier)\n    return df_outlier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3. Data Analysis\n#### 2.3.1. Dependent variable distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef show_dependent_variable(df):\n#     sns.countplot(data = df, x = dependent_variable_name, label = 'Count') \\\n#     .set_title(dependent_variable_name + ' dependent variable situation', fontsize = 18, color = 'r')\n    fig, axarr = plt.subplots(2, 3, figsize=(18, 6))\n    sns.countplot(x = 'Geography', hue = 'Exited',data = df, ax = axarr[0][0])\n    sns.countplot(x = 'Gender', hue = 'Exited',data = df, ax = axarr[0][1])\n    sns.countplot(x = 'HasCrCard', hue = 'Exited',data = df, ax = axarr[0][2])\n    sns.countplot(x = 'IsActiveMember', hue = 'Exited',data = df, ax = axarr[1][0])\n    sns.countplot(x = 'NumOfProducts', hue = 'Exited',data = df, ax = axarr[1][1])\n    sns.countplot(x = 'Tenure', hue = 'Exited',data = df, ax = axarr[1][2])\n    zero, one = df[dependent_variable_name].value_counts()\n    print(\"Dependent variable distribution;\")\n    print(dependent_variable_name + \" 0 count:\", zero)\n    print(dependent_variable_name + \" 1 count:\", one)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.3.2. Numeric columns distribution observation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef show_numeric_columns_distributions(df):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    df_num_cols = df.select_dtypes(include=numerics)\n    columns = df_num_cols.columns[: len(df_num_cols.columns)]\n    fig = plt.figure()\n    fig.set_size_inches(18, 15)\n    #plt.subplots(figsize=(22,22))\n    length = len(columns)\n    for i,j in itertools.zip_longest(columns, range(length)):\n        plt.subplot((length / 2), 3, j+1)\n        plt.subplots_adjust(wspace = 0.2, hspace = 0.5)\n        df_num_cols[i].hist(bins = 20, edgecolor = 'black')\n        plt.title(i)\n    fig = fig.suptitle('Structures of numeric variables', color = 'r' ,fontsize = 18)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.3.3. Status of other variables according to dependent variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef show_dependent_variable_cross_others_distributions(df):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    df_dependent_var = df[df[dependent_variable_name] == 1]\n    df_num_cols = df_dependent_var.select_dtypes(include = numerics)\n    columns = df_num_cols.columns[: len(df_num_cols.columns)]\n    \n    fig = plt.figure()\n    fig.set_size_inches(18, 15)\n    length = len(columns)\n    for i,j in itertools.zip_longest(columns, range(length)):\n        plt.subplot((length / 2), 3, j+1)\n        plt.subplots_adjust(wspace = 0.2, hspace = 0.5)\n        df_num_cols[i].hist(bins = 20, edgecolor = 'black')\n        plt.title(i)\n    fig = fig.suptitle(dependent_variable_name + ' Status of other variables according to 1 dependent variable', color = 'r', fontsize = 18)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.3.4. Categorical variables are observed according to the dependent variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef show_dependent_variable_cross_categorical_distributions(df, categorical_columns):\n    sns.set(font_scale = 0.7) \n    fig, axes = plt.subplots(nrows = int( len(categorical_columns) / 2 ) , ncols = 2, figsize = (7,9))\n    fig.tight_layout()\n    for ax,col in zip(axes.flatten(), categorical_columns):\n        sns.countplot(x = df[col], hue = dependent_variable_name, data = df, ax = ax)\n    fig.suptitle('Categorical variables are monitored according to the dependent variable', color = 'r', fontsize = 1)\n  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.3.5. The main method that started all data analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_data_analysis(df):\n    show_dependent_variable(df)\n    show_numeric_columns_distributions(df)\n    show_dependent_variable_cross_others_distributions(df)\n    show_dependent_variable_cross_categorical_distributions(df = df_outlier, categorical_columns = [\"Gender\",\"Geography\",\"HasCrCard\",\"IsActiveMember\"])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Feature Engineering\n### 3.1. Credit Score grouping ( min= 358 and max= 800)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef credit_score_table(row):\n    \n    credit_score = row.CreditScore\n    if credit_score >= 300 and credit_score < 500:\n        return \"Very_Poor\"\n    elif credit_score >= 500 and credit_score < 601:\n        return \"Poor\"\n    elif credit_score >= 601 and credit_score < 661:\n        return \"Fair\"\n    elif credit_score >= 661 and credit_score < 781:\n        return \"Good\"\n    elif credit_score >= 851:\n        return \"Top\"\n    elif credit_score >= 781 and credit_score < 851:\n        return \"Excellent\"\n    elif credit_score < 300:\n        return \"Deep\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2. Product utilization RATE by YEAR","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef product_utilization_rate_by_year(row):\n    number_of_products = row.NumOfProducts\n    tenure = row.Tenure\n    \n    if number_of_products == 0:\n        return 0\n    \n    if tenure == 0:\n        return number_of_products\n    \n    rate = number_of_products / tenure\n    return rate","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.3. Product utilization rate by estimated SALARY","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef product_utilization_rate_by_estimated_salary(row):\n    number_of_products = row.number_of_products\n    estimated_salary = row.EstimatedSalary\n    \n    if number_of_products == 0:\n        return 0\n\n    \n    rate = number_of_products / estimated_salary\n    return rate","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.4. According to countries monthly average salaries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef countries_monthly_average_salaries(row):\n    #brutto datas from  https://tr.wikipedia.org/wiki/Aylık_ortalama_ücretlerine_göre_Avrupa_ülkeleri_listesi\n    fr = 3696    \n    de = 4740\n    sp = 2257\n    salary = row.EstimatedSalary / 12\n    country = row.Geography              # Germany, France and Spain\n    \n    if country == 'Germany':\n        return salary / de\n    elif country == \"France\":\n        return salary / fr\n    elif country == \"Spain\": \n        return salary / sp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.5. The main method that started all Feature Engineering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_engineering(df, is_show_graph = False):\n    df_fe = df.copy()\n    \n    #bakiye_maas_orani\n    balance_salary_rate = 'balance_salary_rate'\n    df_fe[balance_salary_rate] = df_fe.Balance / df_fe.EstimatedSalary\n    \n    #yila_gore_urun_kullanim_orani\n    df_fe = df_fe.assign(product_utilization_rate_by_year=df_fe.apply(lambda x: product_utilization_rate_by_year(x), axis=1)) \n    \n    #tahmini_maasa_gore_urun_kullanim_orani\n    #df_fe = df_fe.assign(product_utilization_rate_by_estimated_salary = df_fe.apply(lambda x: product_utilization_rate_by_estimated_salary(x), axis=1)) \n    \n    \n    #musteri_yilina göre yaşa göre standardize edilmesi, oranlanmasi - ergenlik donemini cikariyoruz!\n    tenure_rate_by_age = 'tenure_rate_by_age'\n    df_fe[tenure_rate_by_age] = df_fe.Tenure / (df_fe.Age-17)\n    \n    #yaşa göre kredi_skoru standardize edilmesi, oranlanmasi - ergenlik donemini cikariyoruz!\n    credit_score_rate_by_age = 'credit_score_rate_by_age'\n    df_fe[credit_score_rate_by_age] = df_fe.CreditScore / (df_fe.Age-17)\n    \n    #maaşa gore kullanilan urun oran, oranlanmasi\n    product_utilization_rate_by_salary = 'product_utilization_rate_by_salary'\n    #df_fe[product_utilization_rate_by_salary] = df_fe.Tenure / (df_fe.EstimatedSalary)\n   \n    #maaşa göre kredi_skoru urun oran, oranlanmasi\n    credit_score_rate_by_salary = 'credit_score_rate_by_salary'\n    df_fe[credit_score_rate_by_salary] = df_fe.CreditScore / (df_fe.EstimatedSalary)\n    \n    #Feature Eng. oluşturulan değişkenlerin bağımlı değişkene göre grafikleri gösterilsin mi?\n    if is_show_graph:\n        fig, axes = plt.subplots(nrows = 2, ncols = 2, figsize = (20,12))\n        fig.tight_layout()\n        sns.boxplot(y = balance_salary_rate, x = dependent_variable_name, hue = dependent_variable_name, data = df_fe, ax = axes[0][0])\n        sns.boxplot(y = product_utilization_rate_by_year, x = dependent_variable_name, hue = dependent_variable_name, data = df_fe, ax = axes[0][1])\n        #sns.countplot(x = credit_score_rate_by_age, hue = dependent_variable_name, data = df_fe, ax = axes[1][0])\n        #sns.countplot(x = credit_score_rate_by_age, hue = dependent_variable_name, data = df_fe, ax = axes[1][1])\n        plt.ylim(-1, 5)\n    \n    \n    #feature engineering add- kredi_skor_tablosu\n    df_fe = df_fe.assign(credit_score_table=df_fe.apply(lambda x: credit_score_table(x), axis=1))\n    \n    #feature engineering add- ulkelere ortalama  maas durum\n    df_fe = df_fe.assign(countries_monthly_average_salaries = df_fe.apply(lambda x: countries_monthly_average_salaries(x), axis=1)) \n    \n    return df_fe","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Data Encoding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# vucut_yag_orani, vucut_yag_orani_kategori,beden_kitle_kategori,insulin_kategori,kucuk_tansiyon_kategori\n\ndef data_encoding(df):\n    df_model = df.copy()\n    '''\n    # It was attempted to reduce the number of 0 observations.\n    churn_zero = df_model[df_model.apply(lambda x: True if x['Exited'] == 0 else False , axis=1)]\n    df_train = churn_zero.sample(frac=0.1,random_state=100)\n    df_model = df_model.drop(df_train.index)\n    '''\n    \n    \n    # >>>> Categorical columns <<<<<\n    \n    non_encoding_columns = [\"Geography\",\"HasCrCard\",\"IsActiveMember\",\"Gender\",\"NumOfProducts\",\"Tenure\",\"credit_score_table\"]\n    \n    df_non_encoding = df_model[non_encoding_columns]\n    df_model = df_model.drop(non_encoding_columns,axis=1)\n    \n    \n    df_encoding = df_non_encoding.copy()\n    \n    from sklearn.preprocessing import LabelEncoder\n    encoder = LabelEncoder()\n    df_encoding[\"gender_category\"] = encoder.fit_transform(df_non_encoding.Gender)\n    df_encoding[\"country_category\"] = encoder.fit_transform(df_non_encoding.Geography)\n    df_encoding[\"credit_score_category\"] = encoder.fit_transform(df_non_encoding.credit_score_table)\n\n    \n\n    df_encoding.reset_index(drop=True, inplace=True)\n    df_model.reset_index(drop=True, inplace=True)\n    df_model = pd.concat([df_model,df_encoding],axis=1)\n\n    df_model = df_model.drop([\"Geography\",\"Gender\",\"CustomerId\",\"Surname\",\"credit_score_table\",\"CreditScore\",\"EstimatedSalary\"],axis=1)\n    df_model = df_model.reset_index()\n    df_model = df_model.drop('index',axis=1)\n    \n    df_model.loc[df_model.HasCrCard == 0, 'credit_card_situation'] = -1\n    df_model.loc[df_model.IsActiveMember == 0, 'is_active_member'] = -1\n    return df_model\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Model Preparation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_prepare(df_model):\n    y = df_model[dependent_variable_name]\n    X = df_model.loc[:, df_model.columns != dependent_variable_name]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 12345)\n    from sklearn.preprocessing import StandardScaler\n    sc = StandardScaler()\n    X_train = sc.fit_transform(X_train)\n    X_test = sc.transform (X_test)\n    return X_train, X_test, y_train, y_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.1 Part of Data TRAIN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_training(X_train, X_test, y_train, y_test):\n\n    models = []\n    models.append(('LOGR',LogisticRegression()))\n    models.append(('KNN',KNeighborsClassifier()))\n    models.append(('CART',DecisionTreeClassifier()))\n    models.append(('RF',RandomForestClassifier()))\n    #models.append(('SVC',SVC()))\n    models.append(('GBM',GradientBoostingClassifier()))\n    models.append(('XGBoost',XGBClassifier()))\n    models.append(('LightGBM',LGBMClassifier()))\n    models.append(('CatBoost',CatBoostClassifier()))\n\n    df_result = pd.DataFrame(columns=[\"model\",\"accuracy_score\",\"scale_method\",\"0_precision\",\"0_recall\",\"1_precision\",\"1_recall\"])\n    index = 0\n    for name,model in models:\n        model.fit(X_train,y_train)\n        y_pred = model.predict(X_test)\n        score = accuracy_score(y_test,y_pred)\n        class_report = classification_report(y_test,y_pred,digits=2,output_dict=True)\n        zero_report = class_report['0']\n        one_report = class_report['1']\n        df_result.at[index,['model','accuracy_score','scale_method',\"0_precision\",\"0_recall\",\"1_precision\",\"1_recall\"]] = [name,score,\"NA\",zero_report['precision'],zero_report['recall'],one_report['precision'],one_report['recall']]\n        index += 1\n    return df_result.sort_values(\"accuracy_score\",ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. HELPing Functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to give best model score and parameters\n\ndef best_model(model):\n    print(model.best_score_)    \n    print(model.best_params_)\n    print(model.best_estimator_)\n    \ndef get_auc_scores(y_actual, method,method2):\n    auc_score = roc_auc_score(y_actual, method); \n    fpr_df, tpr_df, _ = roc_curve(y_actual, method2); \n    return (auc_score, fpr_df, tpr_df)\n\n\nfrom matplotlib import rc,rcParams\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n    plt.rcParams.update({'font.size': 16})\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45, color=\"blue\")\n    plt.yticks(tick_marks, classes, color=\"blue\")\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"red\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n''' inf yakalama\nindices_to_keep = df_encoded.isin([np.nan, np.inf, -np.inf]).any(1)\ndf_encoded[indices_to_keep].astype(np.float64)\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Data Cleaning","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"df_prep = data_prepare()\n\ndf_outlier = outlier_process(df = df_prep)\n\n\nshow_data_analysis(df_prep)\nshow_outliers(df = df_outlier)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see after feature engineering:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fe = feature_engineering(df = df_outlier)\ndf_fe.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And see after data encoding:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_encoded = data_encoding(df_fe)\ndf_encoded.drop(['credit_card_situation', 'is_active_member'], axis=1, inplace=True)\ndf_encoded.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's see correlation graph:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation = df_encoded.corr().abs()\nplt.figure(figsize=(10,10))\nsns.heatmap(correlation, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"List of correlation scores:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corrs_results = df_encoded.corrwith(df_encoded[\"Exited\"]).abs().nlargest(24)\ncorrs_results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8. Apply Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_prepare test, train split 0.2\nX_train, X_test, y_train, y_test = model_prepare(df_model = df_encoded)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.1. Logistic Regression Model\n\nHave a look with Logistic Regression.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"logr_model = LogisticRegression().fit(X_train,y_train)\ny_pred = logr_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(classification_report(y_test, y_pred, digits=4))\nprint(\"Accuracy score of Logistic Regression: \", accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.2. Model Training\n\nThis is for all LogisticRegression, RandomForestClassifier, GradientBoostingClassifier, XGBClassifier, and LGBMClassifier model objects:","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"training_result = data_training(X_train, X_test, y_train, y_test)\ntraining_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's have a look which model is best for us:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"training_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9. Model Tunning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 9.1. XGBoost Tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model=XGBClassifier(silent=0, learning_rate=0.23, max_delta_step=5,\n                            objective='reg:logistic',n_estimators=92, \n                            max_depth=5, eval_metric=\"logloss\", gamma=3,base_score=0.5)\nxgb_model.fit(X_train, y_train)\ny_pred = xgb_model.predict(X_test)\nprint(classification_report(y_test,y_pred,digits=2))\nprint(\"Accuracy score of Tuned XGBoost Regression: \", accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 9.2. Random Forest Tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'max_depth': [3, 5, 6, 7, 8], 'max_features': [2,4,6,7,8,9],'n_estimators' : [50,100], 'min_samples_split': [3, 5, 6, 7]}\nrandFor_grid = GridSearchCV(RandomForestClassifier(), param_grid, cv = 5, refit = True, verbose = 0)\nrandFor_grid.fit(X_train,y_train)\nbest_model(randFor_grid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the parameters and get final version accuracy score.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rnd_model = RandomForestClassifier(max_depth=8, max_features=6, min_samples_split=6,n_estimators=50)\nrnd_model.fit(X_train, y_train)\ny_pred = rnd_model.predict(X_test)\nprint(classification_report(y_test,y_pred,digits=2))\nprint(\"Accuracy score of tuned Random Forest model: \", accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 9.3. LightGBM Tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_model = LGBMClassifier(silent = 0, learning_rate = 0.09, max_delta_step = 2, n_estimators = 100, boosting_type = 'gbdt',\n                            max_depth = 10, eval_metric = \"logloss\", gamma = 3, base_score = 0.5)\nlgbm_model.fit(X_train, y_train)\ny_pred = lgbm_model.predict(X_test)\nprint(classification_report(y_test, y_pred, digits=2))\nprint(\"Accuracy score of tuned LightGBM model: \", accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 10. Confussion Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cfm = confusion_matrix(y_test, y_pred=y_pred)\nplot_confusion_matrix(cfm, classes=['Non Churn','Churn'],\n                      title='Churn Confusion matrix')\n\n\ntn, fp, fn, tp = cfm.ravel()\nprint(\"True Negatives: \",tn)\nprint(\"False Positives: \",fp)\nprint(\"False Negatives: \",fn)\nprint(\"True Positives: \",tp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install scikit-plot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 11. ROC Curve","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import scikitplot as skplt\ny_pred_proba = lgbm_model.predict_proba(X_test)\nskplt.metrics.plot_roc_curve(y_test, y_pred_proba, figsize=(8,8))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 12. Feature Importance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_index = df_encoded.loc[:, df_encoded.columns != dependent_variable_name]\n\nfeature_importance = pd.Series(lgbm_model.feature_importances_, \n                               index=feature_index.columns).sort_values(ascending=False)\nsns.barplot(x = feature_importance, y = feature_importance.index, color='r', saturation=1)\nplt.xlabel('Variable Severity Scores')\nplt.ylabel('Variables')\nplt.title('Variable Severity Levels')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 13. Conclusion\n\n\n- Our aim in this project was to develop a churn prediction model using machine learning algorithms.\n- There were 10000 rows in the data set and there were no missing values. and the dataset consisted of 13 variables.\n\n\n- The following conclusions came from the analysis on the features:\n\n    * Most customers who using products 3 and 4 stopped working with the bank. In fact, all customers using product number 4 were gone.\n    * Customers between the ages of 40 and 65 were more likely to quit the bank.\n    * Those who had a credit score below 450 had high abandonment rates.\n    * Predictions were made with a total of 8 classification models. The highest head was taken with LightGBM method.\n    * Accuracy and cross validation scores were calculated for each model and results were displayed.\n\n\n\n#### Note:\n\n    * After this notebook, my aim is to prepare 'kernel' which is 'not clear' data set.\n\n    * If you have any suggestions, please could you write for me? I wil be happy for comment and critics!\n\n    * Thank you for your suggestion and votes ;)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}