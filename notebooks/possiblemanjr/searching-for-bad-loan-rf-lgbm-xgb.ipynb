{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score , average_precision_score \nfrom sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve ,auc , log_loss ,  classification_report \nfrom sklearn.preprocessing import StandardScaler , Binarizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nimport time\nimport os, sys, gc, warnings, random, datetime\nimport math\nimport shap\nimport joblib\nwarnings.filterwarnings('ignore')\n\nimport xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold , cross_val_score\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_pickle('../input/searching-for-bad-loan-data-preprocessing/df_pp.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df = pd.read_pickle('../input/loan-include-chargeoff/df_pp.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndf['Loan_status'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Loan_status')\nax[0].set_ylabel('')\nsns.countplot('Loan_status',data=df,ax=ax[1])\nax[1].set_title('Loan_status')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('Loan_status', axis=1)\ny = df['Loan_status']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2 , random_state = 2020, stratify = y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Xgboost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"params_xGB = {\n    'nthread':16, \n    'gamma': 0, \n    'max_depth': 6, \n    'min_child_weight': 1, \n    'max_delta_step': 0, \n    'subsample': 1.0,\n        \n    'colsample_bytree': 1.0, \n       \n    'objective':'binary:logistic',\n    'num_class':1,\n    'eval_metric':'logloss',\n    'seed':2020,\n#     'tree_method' : 'gpu_hist',\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainingScores = []\ncvScores = []\npredictionsBasedOnKFolds = pd.DataFrame(data=[],\n                                    index=y_train.index,columns=['prediction'])\nk_fold = StratifiedKFold(n_splits=5,shuffle=True,random_state=2020)\nstart = time.time() \nfor train_index, cv_index in k_fold.split(np.zeros(len(X_train)),\n                                          y_train.ravel()):\n    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], \\\n        X_train.iloc[cv_index,:]\n    y_train_fold, y_cv_fold = y_train.iloc[train_index], \\\n        y_train.iloc[cv_index]\n    \n    dtrain = xgb.DMatrix(data=X_train_fold, label=y_train_fold)\n    dCV = xgb.DMatrix(data=X_cv_fold)\n    \n    bst = xgb.cv(params_xGB, dtrain, num_boost_round=2000, \n                 nfold=5, early_stopping_rounds=200, verbose_eval=100)\n    \n    best_rounds = np.argmin(np.array(bst['test-logloss-mean']))\n    bst = xgb.train(params_xGB, dtrain, best_rounds)\n    \n    loglossTraining = log_loss(y_train_fold, bst.predict(dtrain))\n    trainingScores.append(loglossTraining)\n    \n    predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'] = \\\n        bst.predict(dCV)\n    loglossCV = log_loss(y_cv_fold, \\\n        predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'])\n    cvScores.append(loglossCV)\n    \n    print('Training Log Loss: ', loglossTraining)\n    print('CV Log Loss: ', loglossCV)\n    \nxgb_runtime = time.time() - start    \nloglossXGBoostGradientBoosting = \\\n    log_loss(y_train, predictionsBasedOnKFolds.loc[:,'prediction'])\n\nprint( 'XGBoost Gradient Boosting Log Loss : {0:.4f} ,  XGBoost Runtime : {1:.4f}'.format(loglossXGBoostGradientBoosting ,xgb_runtime ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = pd.concat([y_train,predictionsBasedOnKFolds.loc[:,'prediction']], axis=1)\npreds.columns = ['trueLabel','prediction']\npredictionsBasedOnKFoldsXGBoostGradientBoosting = preds.copy()\n\nprecision, recall, thresholds = \\\n    precision_recall_curve(preds['trueLabel'],preds['prediction'])\naverage_precision = \\\n    average_precision_score(preds['trueLabel'],preds['prediction'])\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = roc_curve(preds['trueLabel'],preds['prediction'])\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\n        Area under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_estimators = 10\nmax_features = 'auto'\nmax_depth = None\nmin_samples_split = 2\nmin_samples_leaf = 1\nmin_weight_fraction_leaf = 0.0\nmax_leaf_nodes = None\nbootstrap = True\noob_score = False\nn_jobs = -1\nrandom_state = 2018\nclass_weight = 'balanced'\n\nRFC = RandomForestClassifier(n_estimators=n_estimators, \n        max_features=max_features, max_depth=max_depth,\n        min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n        min_weight_fraction_leaf=min_weight_fraction_leaf, \n        max_leaf_nodes=max_leaf_nodes, bootstrap=bootstrap, \n        oob_score=oob_score, n_jobs=n_jobs, random_state=random_state, \n        class_weight=class_weight)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##RandomForest with stratified 5 Fold\n\ntrainingScores = []\ncvScores = []\npredictionsBasedOnKFolds = pd.DataFrame(data=[],\n                                        index=y_train.index,columns=[0,1])\n\nstart = time.time() \nclf = RandomForestClassifier(n_estimators=30, min_samples_leaf=20, max_features=0.7, n_jobs=-1, random_state = 2020, oob_score=True)\ncv = StratifiedKFold(n_splits=5,random_state = 2020)\ny_preds_rf = np.zeros(X_test.shape[0])\nn_iter = 0 \nfor train_index,test_index in cv.split(X_train,y_train):\n    trx , tsx = X_train.iloc[train_index] , X_train.iloc[test_index]\n    vly , vlt = y_train.iloc[train_index] , y_train.iloc[test_index]\n    RFC = RFC.fit(trx,vly)   \n    loglossTraining = log_loss(vly, \\\n                                RFC.predict_proba(trx))\n    trainingScores.append(loglossTraining)\n    \n    predictionsBasedOnKFolds.loc[tsx.index,:] = \\\n        RFC.predict_proba(tsx)  \n    loglossCV = log_loss(vlt, \\\n        predictionsBasedOnKFolds.loc[tsx.index,1])\n    cvScores.append(loglossCV)\n    print('Training Log Loss: ', loglossTraining)\n    print('CV Log Loss: ', loglossCV)\n    \n    n_iter += 1\n    cv_roc_score = roc_auc_score(y_test, RFC.predict_proba(X_test)[:,1], average = 'macro')\n    cv_precision, cv_recall, _ = precision_recall_curve(y_test,RFC.predict_proba(X_test)[:,1])\n    cv_pr_auc = auc(cv_recall, cv_precision)\n    print( '\\n#{0}, CV_ROC_AUC : {1} , RF_CV_PR_AUC : {2} '.format(n_iter ,cv_roc_score, cv_pr_auc))\n    y_preds_rf += RFC.predict_proba(X_test)[:,1]/ cv.n_splits\nrf_runtime = time.time() - start \nrf_cv_roc_score = roc_auc_score(y_test, y_preds_rf, average = 'macro')\nrf_cv_precision, rf_cv_recall, _ = precision_recall_curve(y_test,y_preds_rf)\nrf_cv_pr_auc = auc(rf_cv_recall, rf_cv_precision)    \nloglossRandomForestsClassifier = log_loss(y_train, \n                                          predictionsBasedOnKFolds.loc[:,1])\nprint( 'Random Forest Log Loss : {0:.4f} ,  Random Forest Runtime : {1:.4f}'.format(loglossRandomForestsClassifier ,rf_runtime ))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = pd.concat([y_train,predictionsBasedOnKFolds.loc[:,1]], axis=1)\npreds.columns = ['trueLabel','prediction']\npredictionsBasedOnKFoldsRandomForests = preds.copy()\n\nprecision, recall, thresholds = precision_recall_curve(preds['trueLabel'],\n                                                       preds['prediction'])\naverage_precision = average_precision_score(preds['trueLabel'],\n                                            preds['prediction'])\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = roc_curve(preds['trueLabel'],preds['prediction'])\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\n          Area under the curve = {0:0.2f}'.format(\n          areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LightGBM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"params_lightGB = {\n    'task': 'train',\n    'application':'binary',\n    'num_class':1,\n    'boosting': 'gbdt',\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'metric_freq':50,\n    'is_training_metric':False,\n    'max_depth':4,\n    'num_leaves': 31,\n#     'learning_rate': 0.01,\n    'feature_fraction': 1.0,\n    'bagging_fraction': 1.0,\n    'bagging_freq': 0,\n    'bagging_seed': 2020,\n    'verbose': 50,\n    'num_threads':16,\n    'random_state ' : 2020\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainingScores = []\ncvScores = []\npredictionsBasedOnKFolds = pd.DataFrame(data=[],\n                                index=y_train.index,columns=['prediction'])\nstart = time.time() \nfor train_index, cv_index in k_fold.split(np.zeros(len(X_train)),\n                                          y_train.ravel()):\n    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], \\\n        X_train.iloc[cv_index,:]\n    y_train_fold, y_cv_fold = y_train.iloc[train_index], \\\n        y_train.iloc[cv_index]\n    \n    lgb_train = lgb.Dataset(X_train_fold, y_train_fold)\n    lgb_eval = lgb.Dataset(X_cv_fold, y_cv_fold, reference=lgb_train)\n    gbm = lgb.train(params_lightGB, lgb_train, num_boost_round=10000,\n                   valid_sets=lgb_eval, early_stopping_rounds=200 , verbose_eval = 500)\n    \n    loglossTraining = log_loss(y_train_fold, \\\n                gbm.predict(X_train_fold, num_iteration=gbm.best_iteration))\n    trainingScores.append(loglossTraining)\n    \n    predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'] = \\\n        gbm.predict(X_cv_fold, num_iteration=gbm.best_iteration) \n    loglossCV = log_loss(y_cv_fold, \\\n        predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'])\n    cvScores.append(loglossCV)\n    \n    print('Training Log Loss: ', loglossTraining)\n    print('CV Log Loss: ', loglossCV)\nlgbm_runtime = time.time() - start     \nloglossLightGBMGradientBoosting = \\\n    log_loss(y_train, predictionsBasedOnKFolds.loc[:,'prediction'])\nprint( 'LightGBM Log Loss : {0:.4f} ,  LightGBM Runtime : {1:.4f}'.format(loglossLightGBMGradientBoosting ,lgbm_runtime ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = pd.concat([y_train,predictionsBasedOnKFolds.loc[:,'prediction']], axis=1)\npreds.columns = ['trueLabel','prediction']\npredictionsBasedOnKFoldsLightGBMGradientBoosting = preds.copy()\n\nprecision, recall, thresholds = \\\n    precision_recall_curve(preds['trueLabel'],preds['prediction'])\naverage_precision = \\\n    average_precision_score(preds['trueLabel'],preds['prediction'])\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = roc_curve(preds['trueLabel'],preds['prediction'])\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\nArea under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apply to Test Set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictionsTestSetRandomForests = \\\n    pd.DataFrame(data=[],index=y_test.index,columns=['prediction'])\npredictionsTestSetRandomForests.loc[:,'prediction'] = \\\n    RFC.predict_proba(X_test)[:,1]\nlogLossTestSetRandomForests = \\\n    log_loss(y_test, predictionsTestSetRandomForests)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictionsTestSetXGBoostGradientBoosting = \\\n    pd.DataFrame(data=[],index=y_test.index,columns=['prediction'])\ndtest = xgb.DMatrix(data=X_test)\npredictionsTestSetXGBoostGradientBoosting.loc[:,'prediction'] = \\\n    bst.predict(dtest)\nlogLossTestSetXGBoostGradientBoosting = \\\n    log_loss(y_test, predictionsTestSetXGBoostGradientBoosting)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictionsTestSetLightGBMGradientBoosting = \\\n    pd.DataFrame(data=[],index=y_test.index,columns=['prediction'])\npredictionsTestSetLightGBMGradientBoosting.loc[:,'prediction'] = \\\n    gbm.predict(X_test, num_iteration=gbm.best_iteration)\nlogLossTestSetLightGBMGradientBoosting = \\\n    log_loss(y_test, predictionsTestSetLightGBMGradientBoosting)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Log Loss of Random Forests on Test Set: \", \\\n          logLossTestSetRandomForests)\nprint(\"Log Loss of XGBoost Gradient Boosting on Test Set: \", \\\n          logLossTestSetXGBoostGradientBoosting)\nprint(\"Log Loss of LightGBM Gradient Boosting on Test Set: \", \\\n          logLossTestSetLightGBMGradientBoosting)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RF\nprecision, recall, thresholds = \\\n    precision_recall_curve(y_test,predictionsTestSetRandomForests)\naverage_precision = \\\n    average_precision_score(y_test,predictionsTestSetRandomForests)\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = roc_curve(y_test,predictionsTestSetRandomForests)\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\nArea under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#XGB\nprecision, recall, thresholds = \\\n    precision_recall_curve(y_test,predictionsTestSetXGBoostGradientBoosting)\naverage_precision = \\\n    average_precision_score(y_test,predictionsTestSetXGBoostGradientBoosting)\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = \\\n    roc_curve(y_test,predictionsTestSetXGBoostGradientBoosting)\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\nArea under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LGB\nprecision, recall, thresholds = \\\n    precision_recall_curve(y_test,predictionsTestSetLightGBMGradientBoosting)\naverage_precision = \\\n    average_precision_score(y_test,predictionsTestSetLightGBMGradientBoosting)\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = \\\n    roc_curve(y_test,predictionsTestSetLightGBMGradientBoosting)\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\nArea under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_clf_eval(y_test, pred):\n    confusion = confusion_matrix(y_test, pred)\n    accuracy = accuracy_score(y_test , pred)\n    precision = precision_score(y_test, pred)\n    recall = recall_score(y_test,pred)\n    f1 = f1_score(y_test, pred)\n    print('Confusion Matrix')\n    print(confusion)\n    print('Auccuracy : {0:.4f}, Precision : {1:.4f} , Recall : {2:.4f} , F1_Score : {3:.4f}'.format(accuracy , precision, recall, f1))\n    print('------------------------------------------------------------------------------')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"thresholds = {0.1,0.15, 0.2,0.25, 0.3,0.35, 0.4 , 0.45 , 0.5}\n\ndef get_eval_by_threshold(y_test, pred_proba_c1, thresholds):\n    for custom_threshold in thresholds:\n        binarizer = Binarizer(threshold = custom_threshold).fit(pred_proba_c1)\n        custom_predict = binarizer.transform(pred_proba_c1)\n        print('threshold:', custom_threshold)\n        get_clf_eval(y_test, custom_predict)\n\n## get_eval_by_threshold(y_test, pred_proba[:,1].reshape(-1,1), thresholds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Using CPU only\n\nstart = time.time()\n\nparams_lgb={'boosting_type':'gbdt',\n           'objective': 'binary',\n           'random_state':2020,\n           'metric':'binary_logloss',\n            'metric_freq' : 50,\n            'max_depth' :4, \n            'num_leaves' : 31,\n            'learning_rate' : 0.01,\n            'feature_fraction' : 1.0,\n            'bagging_fraction' : 1.0,\n            'bagging_freq' : 0,\n            'bagging_seed' : 2020,\n            'num_threads' : 16\n           }\n\n\nlgbm_clf = LGBMClassifier(boosting_type = 'gbdt',\n           objective= 'binary',\n           metric='auc',\n#             metric_freq = 50,\n#             max_depth =4, \n#             num_leaves = 31,\n#             learning_rate = 0.01,\n#             feature_fraction = 1.0,\n#             bagging_fraction = 1.0,\n#             bagging_freq = 0,\n# #             bagging_seed = 2020,\n#             num_threads = 16,\n                          random_state = 2020)\n\nevals = [(X_test, y_test)]\nlgbm_clf.fit(X_train, y_train,  verbose = 50)\n\n\nlgbm_cpu_runtime = time.time() - start\n\nget_eval_by_threshold(y_test, lgbm_clf.predict_proba(X_test)[:,1].reshape(-1,1), thresholds)\nlgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:,1], average = 'macro')\nlgbm_precision, lgbm_recall, _ = precision_recall_curve(y_test,lgbm_clf.predict_proba(X_test)[:,1])\nlgbm_pr_auc = auc(lgbm_recall, lgbm_precision)\n\n\n\nprint( 'LightGBM_ROC_AUC : {0:.4f} , LightGBM_PR_AUC : {1:.4f} ,Runtime : {2:.4f}'.format(lgbm_roc_score ,lgbm_pr_auc, lgbm_cpu_runtime))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\n\nxgb_clf = XGBClassifier(random_state = 2020)\nxgb_clf.fit(X_train, y_train, verbose = 50)\n\nxgb_gpu_runtime = time.time() - start\n\npred = xgb_clf.predict(X_test)\n\nget_eval_by_threshold(y_test, xgb_clf.predict_proba(X_test)[:,1].reshape(-1,1), thresholds)\n\nxgb_gpu_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:,1], average = 'macro')\n\nxgb_precision, xgb_recall, _ = precision_recall_curve(y_test,xgb_clf.predict_proba(X_test)[:,1])\nxgb_gpu_pr_auc = auc(xgb_recall, xgb_precision)\n\n\n\nprint( 'XGboost_gpu_ROC_AUC : {0:.4f} , XGboost_gpu_PR_AUC : {1:.4f} , Runtime : {2:.4f}'.format(xgb_gpu_roc_score ,xgb_gpu_pr_auc, xgb_gpu_runtime ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"perm_xgb = PermutationImportance(xgb_clf, random_state=2020).fit(X_test, y_test)\neli5.show_weights(perm_xgb, feature_names = X_test.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import shap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('Loan_status', axis=1)\ny = df['Loan_status']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2 , random_state = 2020, stratify = y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_trainin = y_train.to_frame()\nfor_sample_train_df = pd.concat([X_train, y_trainin], axis=1)\ny_testet = y_test.to_frame()\nfor_sample_test_df = pd.concat([X_test, y_testet], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_ = for_sample_train_df.drop('Loan_status', axis=1)\ny_ = for_sample_train_df['Loan_status']\n\nsample_train_x, sample_test_x, sample_train_y, sample_test_y = train_test_split(X_, y_, test_size = 0.8 , random_state = 2020, stratify = y_)\n\ndel X_train, X_test, y_train, y_test , y_trainin, y_testet,","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Make sample for faster computation\n\nX_ = for_sample_train_df.drop('Loan_status', axis=1)\ny_ = for_sample_train_df['Loan_status']\n\nsample_train_x, sample_test_x, sample_train_y, sample_test_y = train_test_split(X_, y_, test_size = 0.80 , random_state = 2020, stratify = y_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_train_yin = sample_train_y.to_frame()\nfor__sample_train_df = pd.concat([sample_train_x, sample_train_yin], axis=1)\nsample_test_yin = sample_test_y.to_frame()\nfor__sample_test_df = pd.concat([sample_test_x, sample_test_yin], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for__sample_train_df.head(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_sampled = sample_train_x.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LightGBM\nimport shap\nshap.initjs()\n\n# (same syntax works for LightGBM, CatBoost, and scikit-learn models)\n\nexplainer = shap.TreeExplainer(lgbm_clf)\nshap_values = explainer.shap_values(X_sampled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.force_plot(explainer.expected_value[1], shap_values[1][0,:], X_sampled.iloc[0,:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.force_plot(explainer.expected_value[1], shap_values[1][1,:], X_sampled.iloc[1,:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.force_plot(explainer.expected_value[1], shap_values[1][15,:], X_sampled.iloc[15,:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.force_plot(explainer.expected_value[1], shap_values[1][3,:], X_sampled.iloc[3,:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.force_plot(explainer.expected_value[1], shap_values[1][3,:], X_sampled.iloc[3,:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.force_plot(explainer.expected_value[1], shap_values[1][4,:], X_sampled.iloc[4,:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # summarize the effects of all the features\n# shap.summary_plot(shap_values, X_sampled, plot_type=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shap.force_plot(base_value=explainer.expected_value[1], shap_values=shap_values[1], features=X_sampled.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}