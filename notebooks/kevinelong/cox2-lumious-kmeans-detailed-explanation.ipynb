{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"## <u> K-means Clustering </u>\n\nK-means clustering is a type of unsupervised learning, which is used with unlabeled dataset. The goal of this algorithm is to find K groups in the data. The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided. Data points are clustered based on feature similarity. The results of the K-means clustering algorithm are:\n\n- The centroids of the K clusters, which can be used to label new data\n- Labels for the training data (each data point is assigned to a single cluster)\n\nK-means works by defining spherical clusters that are separable in a way so that the mean value converges towards the cluster center. Because of this, K-Means may underperform sometimes.\n\n<b><u>Use Cases:</u></b>\n- Document Classification\n- Delivery Store Optimization\n- Customer Segmentation\n- Insurance Fraud Detection etc.\n\n### <u> Algorithm </u>:\n\nΚ-means clustering algorithm inputs are the number of clusters Κ and the data set. Algorithm starts with initial estimates for the Κ centroids, which can either be randomly generated or randomly selected from the data set. The algorithm then iterates between two steps:\n\n<b><u>1. Data assigment step:</u></b>\n\nEach centroid defines one of the clusters. In this step, each data point based on the squared Euclidean distance is assigned to its nearest centroid. If $c_i$ is the collection of centroids in set C, then each data point x is assigned to a cluster based on\n\n$$\\underset{c_i \\in C}{\\min} \\; dist(c_i,x)^2$$\n\nwhere dist( · ) is the standard (L2) Euclidean distance.\n\n<b><u>2. Centroid update step:</u></b>\n\nCentroids are recomputed by taking the mean of all data points assigned to that centroid's cluster.\n\nThe algorithm iterates between step one and two until a stopping criteria is met (no data points change clusters, the sum of the distances is minimized, or some maximum number of iterations is reached).\n\n<b>This algorithm may converge on a local optimum. </b> Assessing more than one run of the algorithm with randomized starting centroids may give a better outcome.\n\n<b><u>Choosing K</u></b>\n\nIf the true label is not known in advance, then K-Means clustering can be evaluated using <b> Elbow Criterion </b>, <b> Silhouette Coefficient </b> , cross-validation, information criteria, the information theoretic jump method, and the G-means algorithm. .\n\n\n<b><u>Elbow Criterion Method:</u></b>\n\nThe idea behind elbow method is to run k-means clustering on a given dataset for a range of values of k (e.g k=1 to 10), for each value of k, calculate sum of squared errors (SSE).\n\nCalculate the mean distance between data points and their cluster centroid. Increasing the number of clusters(K) will always reduce the distance to data points, thus decrease this metric, to the extreme of reaching zero when K is as same as the number of data points. So the goal is to choose a small value of k that still has a low SSE.\n\nWe run the algorithm for different values of K(say K = 10 to 1) and plot the K values against SSE(Sum of Squared Errors). And select the value of K for the elbow point.\n\n<b><u>Silhouette Coefficient Method:</u></b>\n\nA higher Silhouette Coefficient score relates to a model with better-defined clusters. The Silhouette Coefficient is defined for each sample and is composed of two scores:\n\n- The mean distance between a sample and all other points in the same class.\n- The mean distance between a sample and all other points in the next nearest cluster.\n\nThe Silhouette Coefficient is for a single sample is then given as:\n\n$$s=\\frac{b-a}{max(a,b)}$$\n\nTo find the optimal value of k for KMeans, loop through 1..n for n_clusters in KMeans and calculate Silhouette Coefficient for each sample.\n\nA higher Silhouette Coefficient indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n\n<u>K-Means algorithm uses Eucledean Distance, other popular distance metrics in Machine Learning are:</u>\n\n1. <b> Cosine distance </b>: It determines the cosine of the angle between the point vectors of the two points in the n dimensional space. Closer the point vectors are by angle, the higher is the Cosine Similarity\n\n$$\\cos\\theta = \\frac{\\overrightarrow{a} . \\overrightarrow{b}}{\\parallel \\overrightarrow{a} \\parallel \\parallel \\overrightarrow{b} \\parallel} = \\frac{\\sum_{i=1}^{n} a_ib_i}{\\sqrt{\\sum_{i=1}^{n}a_i^2 \\sqrt{\\sum_{i=1}^{n}b_i^2}}}$$\n\nwhere $\\overrightarrow{a} . \\overrightarrow{b} = \\sum_{i=1}^{n}a_ib_i = a_1b_1 + a_2b_2 + ... + a_nb_n$\n\n2. <b> Manhattan distance </b>: is the total sum of the difference between the x-coordinates  and y-coordinates.\n\n$$ManhattanDistance = |x1 – x2| + |y1 – y2|$$\n\nBoth the RMSE and the MAE are ways to measure the distance between two vectors: the vector of predictions and the vector of target values. Various distance measures, or norms, are possible:\n\n- Computing the root of a sum of squares (RMSE) corresponds to the Euclidian norm: it is the notion of distance you are familiar with. It is also called the ℓ2 norm(...)\n\n- Computing the sum of absolutes (MAE) corresponds to the ℓ1 norm,(...). It is sometimes called the Manhattan norm because it measures the distance between two points in a city if you can only travel along orthogonal city blocks.\n\n### <u> Example </u>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"raw_df = pd.read_csv('../input/ccdata/CC GENERAL.csv')\nraw_df = raw_df.drop('CUST_ID', axis = 1) \nraw_df.fillna(method ='ffill', inplace = True) \nraw_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standardize data\nscaler = StandardScaler() \nscaled_df = scaler.fit_transform(raw_df) \n  \n# Normalizing the Data \nnormalized_df = normalize(scaled_df) \n  \n# Converting the numpy array into a pandas DataFrame \nnormalized_df = pd.DataFrame(normalized_df) \n  \n# Reducing the dimensions of the data \npca = PCA(n_components = 2) \nX_principal = pca.fit_transform(normalized_df) \nX_principal = pd.DataFrame(X_principal) \nX_principal.columns = ['P1', 'P2'] \n  \nX_principal.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Elbow Criterion:</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"sse = {}\nfor k in range(1, 10):\n    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(X_principal)\n    sse[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center\nplt.figure()\nplt.plot(list(sse.keys()), list(sse.values()))\nplt.xlabel(\"Number of cluster\")\nplt.ylabel(\"SSE\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Silhouette Coefficient Method:</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"silhouette_scores = [] \n\nfor n_cluster in range(2, 8):\n    silhouette_scores.append( \n        silhouette_score(X_principal, KMeans(n_clusters = n_cluster).fit_predict(X_principal))) \n    \n# Plotting a bar graph to compare the results \nk = [2, 3, 4, 5, 6,7] \nplt.bar(k, silhouette_scores) \nplt.xlabel('Number of clusters', fontsize = 10) \nplt.ylabel('Silhouette Score', fontsize = 10) \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters=3)\nkmeans.fit(X_principal)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizing the clustering \nplt.scatter(X_principal['P1'], X_principal['P2'],  \n           c = KMeans(n_clusters = 3).fit_predict(X_principal), cmap =plt.cm.winter) \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step size of the mesh. Decrease to increase the quality of the VQ.\nh = .01     # point in the mesh [x_min, x_max]x[y_min, y_max].\n\n# Plot the decision boundary. For that, we will assign a color to each\nx_min, x_max = X_principal['P1'].min() - 1, X_principal['P1'].max() + 1\ny_min, y_max = X_principal['P2'].min() - 1, X_principal['P2'].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n# Obtain labels for each point in mesh. Use last trained model.\n\n# https://www.quora.com/Can-anybody-elaborate-the-use-of-c_-in-numpy\n# https://www.geeksforgeeks.org/differences-flatten-ravel-numpy/\n# Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n\nZ = kmeans.predict(np.array(list(zip(xx.ravel(), yy.ravel()))))\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure(1)\n# https://stackoverflow.com/questions/16661790/difference-between-plt-close-and-plt-clf\nplt.clf()\nplt.imshow(Z, interpolation='nearest',\n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap=plt.cm.winter,\n           aspect='auto', origin='lower')\n\nplt.plot(X_principal['P1'], X_principal['P2'], 'k.', markersize=2)\n# Plot the centroids as a white X\ncentroids = kmeans.cluster_centers_\nplt.scatter(centroids[:, 0], centroids[:, 1],\n            marker='o', s=10, linewidths=3,\n            color='w', zorder=10)\nplt.title('K-means clustering on the credit card fraud dataset (PCA-reduced data)\\n'\n          'Centroids are marked with white circle')\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <u>Accuracy metrics</u>\n\nAs opposed to classfication, it is difficult to assess the quality of results from clustering. Here, a metric cannot depend on the labels but only on the goodness of split. Secondly, we do not usually have true labels of the observations when we use clustering.\n\nThere are internal and external goodness metrics. External metrics use the information about the known true split while internal metrics do not use any external information and assess the goodness of clusters based only on the initial data. The optimal number of clusters is usually defined with respect to some internal metrics.\n\n<u> <b>External Goodness Metrics</b></u>\n\nF-measure, Normalized Mutual Information(the average mutual information between every pair of clusters and their class), Rand Index etc.\n\n<u> <b>Internal Goodness Metrics</b></u>\n\nDavies-Bouldin index,Silhouette index,Dunn index,Partition Coefficient, Entropy,Separation Index,Xie and Beni's Index etc.\n\n<u> <b>Normalized Mutual Information (NMI)</b></u>\n\nMutual Information of two random variables is a measure of the mutual dependence between the two variables. Normalized Mutual Information is a normalization of the Mutual Information (MI) score to scale the results between 0 (no mutual information) and 1 (perfect correlation). In other words, 0 means dissimilar and 1 means a perfect match.\n\n<u> <b>Adjusted Rand Score (ARS)</b></u>\n\nAdjusted Rand Score on the other hand, computes a similarity measure between two clusters. ARS considers all pairs of samples and counts pairs that are assigned in the same or different clusters in the predicted and true clusters. 0 is the lowest similarity and 1 is the highest."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}