{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Credit Data Analysis\n### Author: Alex Correa\n### Last Publish Date: 12/6/20\n### Submission Version: 2.0\n\n## Problem Statement\nUsing the provided attrition data from the bank, generate a model that predicts customer attrition.\n\n## Findings Summary\nUsing Random Forest Classification with one hot encoding for categorical data, we're able to achieve an accuracy rating of 96.10% based on a train/test/split sampling with 20% split (20% of the data reserved for testing and 80% for training).\n\n## Future Iteration Goals\n- Further validate accuracy with cross validation\n- Further tune model with XGBoost\n- Add further detailed commentary on charts and data exploration\n\n## Notes\nThe cleanliness of this data really made this project enjoyable and able to stay focused on the exploration/modeling.  My goal in this notebook was to approach a data set different from most I've used in learning but still apply some of the tools I've gained from the Kaggle courses.  \n\n## Changelog\n- Version 1.0: Initial Publish\n- Version 1.1: Title modification and changelog inclusion\n- Version 2.0: We add charts to view the data in different dimensions"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## Data Import\nWe start by getting the data imported with pandas to make our data frame. We're told that 'CLIENTNUM' is our unique id for customers so we'll use it as our index (if this were real world we'd validate by checking duplicates and ensuring it wasn't a relevant feature).\n\nAfter the import we look at the first rows and print columns to get a quick peek."},{"metadata":{"trusted":true},"cell_type":"code","source":"holdingData = pd.read_csv(\"../input/credit-card-customers/BankChurners.csv\",index_col='CLIENTNUM')\ncreditData = holdingData.drop(['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1','Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## First thoughts\nAfter looking WOW we got a lot of features.  We were encouraged to drop the last two columns so it would likely help with readability.\n\n## Check for Missing Data\nBefore we get too crazy let's see what columns are missing data.  We wont condition the data here but use it to determine what's viable.  If we were to condition now we may risk data leakage."},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values_count = creditData.isnull().sum()\nprint(missing_values_count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing Data Findings\nSoooo we're missing no data.  That's because we were given the data so we will assume someone did a lot of work for us.\n\n## Data Exploration\nThe following cells will be used to explore the data.  We'll do this with some graphs and charts.  Seaborn and matplotlib will be our tool of choice for the time being."},{"metadata":{"trusted":true},"cell_type":"code","source":"creditData.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# The below code for charts hangs a bit too long and includes charts that \n# clearly aren't the most appropriate for their data type.  I'm including\n# it as a demonstration of some issues quick solutions like this can cause.\n\nbarColumns = ['Attrition_Flag','Customer_Age','Gender','Dependent_count','Education_Level','Marital_Status',\n             'Income_Category','Card_Category','Months_on_book','Total_Relationship_Count','Months_Inactive_12_mon']\n\nfor i in creditData[barColumns]:\n    cat_num = creditData[i].value_counts()\n    # print(\"Graph for %s: total=%d\" % (i,len(cat_num)))\n    chart = sns.barplot(x=cat_num.index, y=cat_num)\n    chart.set_xticklabels(chart.get_xticklabels(),rotation=90)\n    chart.set_title(\"Graph for %s: total=%d\" % (i,len(cat_num)))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This version of the plotting gives us the rest of the DF data to use for \n# Hues.  Surface level very few features look valuable aside from card_category\n\nfor i in creditData[barColumns]:\n    cat_num = creditData[i].value_counts()\n    chart = sns.countplot(x=i,data=creditData,hue=\"Attrition_Flag\")\n    chart.set_xticklabels(chart.get_xticklabels(),rotation=90)\n    chart.set_title(\"Graph for %s: total=%d\" % (i,len(cat_num)))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we need to chart some of our numeric values.  Here we look at \nnumColumns = ['Credit_Limit', 'Total_Revolving_Bal',\n       'Avg_Open_To_Buy', 'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt',\n       'Total_Trans_Ct', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio']\n\n# we're using histograms for this. In seaborn 11 we'll be able to do \n# these with a \"hue\" using displot (but oh well for now)\nfor i in creditData[numColumns]:\n    chart = sns.distplot(a=creditData[i], kde=False)\n    chart.set_title(\"Graph for %s: total=%d\" % (i,len(cat_num)))\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Don't know what to chart? Chart EVERYTHING.\nsns.pairplot(creditData, hue='Attrition_Flag')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Quick Chart Thoughts\nUltimately we charted a ton of different things.  Bars for counts on categorical data and histograms for the numerical data.  Finally we use the pairplot to visualize graphs of many features against each other numerically. \n\nI have some concerns about the credit utilization stats being sources of data leak. Yes, we can see some discrepencies from attrited and existing customers, but they're related to credit utilization. My immediate questions are:\n- Do credit utilization categories include data collected for 12 months before?\n- How could we count utilization if people attrited in February?  Naturally people with low utilization could imply attrite because they literally couldn't purchase 10 of the months.\n\nI'll wrap this part up with a quick heatmap "},{"metadata":{"trusted":true},"cell_type":"code","source":"cmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(creditData.corr(), vmax=.3, center=0, cmap=cmap,\n           square = True, linewidths=.5, cbar_kws={\"shrink\":.5})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering\nHere we begin to create our \"features\".  Honestly I'm going to do a run with all of them and create a classifier, but that will end poorly for me soon enough if we don't do the effort to convert data with encoding etc. Categorical data is definitely not the easiest thing to work with.\n\nIn future iterations the feature engineering section will be more scientific with us understanding the data better and making choices that prevent data leak."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Features?\n# here we will define smaller feature sets in the future\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Code Overview\nComing from a computer science background, I have a personal preference of leaving code blocks sequential to allow straightforward reading.  In the future I'll likely begin adding more context between blocks, but for now I'll do it all at the beginning.  One thing of note here is that I did not condition the categorical data before this step.  It could have been done for exploration, but I want to practice building scalable solutions and conditioning data in the pipeline for training and testing separately is a small step towards that.\n\nThe code executes as follows:\n1. We generate our training and testing data from the entire sample.\n2. We define numerical and categorical columns by type and use them to generate our working dataframe.\n3. We begin building our pipeline that defines the rules for handling data and using the model.\n4. We define an imputer (for potential missing data) and define one-hot-encoding for our categorical data.\n5. We define the model we'll use which in this case is a random forest classifier.\n6. We fit/train the model\n7. We make predictions based on the newly trained model\n8. We analyze the accuracy of our predictions\n9. We map our predictions to the reality\n10. We take a step back and think on how to iterate."},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's get some train test split going\nfrom sklearn.model_selection import train_test_split\n\nX = creditData.drop(\"Attrition_Flag\",axis=1)\ny = creditData.Attrition_Flag\n\nX_train_full, X_test_full, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now we do some pipeline work\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_cols = [cname for cname in X_train_full.columns if\n                    X_train_full[cname].nunique() < 10 and \n                    X_train_full[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X_train_full.columns if \n                X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For a future opportunity we could create a new heatmap after \n# converting categorical data to empirical.  Because this is wrapped in \n# a pipeline for train and test we'll skip it.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import accuracy_score, plot_confusion_matrix\n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='constant')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Define model\nmodel = RandomForestClassifier(n_estimators=100, random_state=0)\n\n# Bundle preprocessing and modeling code in a pipeline\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)\n                     ])\n\n# Preprocessing of training data, fit model \nclf.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = clf.predict(X_test)\n\n# MAE does not work for classified data so we use accuracy \npreds_score=accuracy_score(y_test,preds)\nprint(\"Random Forest Classifier Success Rate :\", \"{:.2f}%\".format(100*preds_score))\nplot_confusion_matrix(clf, X_test, y_test)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}