{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### The data has been obtained from the National Institute of Diabetes and Digestive and Kidney Diseases in India.","metadata":{}},{"cell_type":"markdown","source":"# 1. Importing relevant libraries and dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.stats import chi2_contingency\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC  \nfrom sklearn.naive_bayes import GaussianNB\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-17T04:06:10.605773Z","iopub.execute_input":"2021-08-17T04:06:10.6067Z","iopub.status.idle":"2021-08-17T04:06:12.291486Z","shell.execute_reply.started":"2021-08-17T04:06:10.60656Z","shell.execute_reply":"2021-08-17T04:06:12.290336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = pd.read_csv(\"diabetes.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-17T04:06:12.293526Z","iopub.execute_input":"2021-08-17T04:06:12.293864Z","iopub.status.idle":"2021-08-17T04:06:12.429402Z","shell.execute_reply.started":"2021-08-17T04:06:12.29383Z","shell.execute_reply":"2021-08-17T04:06:12.42721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Data Exploration","metadata":{}},{"cell_type":"code","source":"dataset.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 9 columns and 768 rows","metadata":{}},{"cell_type":"code","source":"dataset.iloc[:,:-1].info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no independent Categorical variables. Hence, none need to be one hot encoded, and are apt in their current float/integer form","metadata":{}},{"cell_type":"code","source":"dataset.describe()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.isna().sum()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### There are no null values. Hence, we do not require filling or dropping of any rows.","metadata":{}},{"cell_type":"code","source":"discrete_feature = ['Pregnancies','Age','Outcome']\n\nprint(\"Discrete Variables Count: {}\".format(len(discrete_feature)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"overall_diabetes_rate = dataset['Outcome'].mean()\noverall_diabetes_rate","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"group_by_pregnancy = dataset.groupby(\"Pregnancies\").agg({'Outcome': np.mean})\ngroup_by_pregnancy","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"axes = plt.axes()\naxes.axhline(overall_diabetes_rate, color = 'red')\ngroup_by_pregnancy.plot(marker='x', legend= False, ax = axes)\naxes.set_ylabel('Proportion of  Diabetic people')\naxes.legend(['Enite dataset', 'Numbers of pregancnies'])","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature = 'Age'\n\nfig, (ax1,ax2) = plt.subplots(1,2, figsize = (20,5),constrained_layout=True)\nbin_x = range(25,80,2)\n\nax1.hist(dataset[feature],bins=bin_x,rwidth=0.9)\nax1.set_xticks(range(25,80,2))\nax1.set_xlabel('Age',fontsize=15)\nax1.set_ylabel('Count',fontsize=15)\nax1.set_title('Age Distribution',fontsize=20)\n\nax2.hist(dataset[dataset['Outcome']==1][feature], label = 'Positive',bins=bin_x,rwidth=0.9)\nax2.hist(dataset[dataset['Outcome']==0][feature], label = 'Negative',bins=bin_x,rwidth=0.5)\nax2.legend()\nax2.set_xticks(range(25,80,2))\nax2.set_xlabel('Age',fontsize=15)\nax2.set_ylabel('Count',fontsize=15)\nax2.set_title('Diabetes: Positive vs Negative',fontsize=20)\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Groups of people with higher number of pregnancies tend to have a higher diabetes rate.\n\n#### Younger cohorts have a lower diabetes rate as oppoed to older people","metadata":{}},{"cell_type":"code","source":"continuous_feature=[feature for feature in dataset.columns if feature not in discrete_feature]\nprint(\"Continuous feature Count {}\".format(len(continuous_feature)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for feature in continuous_feature:\n    dataset[feature].hist(bins=25)\n    plt.xlabel(feature)\n    plt.ylabel(\"Count\")\n    plt.title(feature)\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Some instances are observed to have Glucose/ Skin Thickness/ BMI/ Blood Pressure equal to 0. This is biologically impossible for a living human. The simple answer would be to drop the rows with any of these features equal to 0. However, this can lead to a major loss of information or bias. As such, we must devise a way to deal with the missing values whilst abstaining from dropping the respective rows. We also avoid removing columns with a high number of null values as they are all crucial to our analysis\n\n\n#### I advoacte for imputation of data rather than removal of data","metadata":{}},{"cell_type":"code","source":"columns_incorrect = [\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\"]\n\ndef replace_(x):\n    if x == 0:\n        return np.nan\n    return x\n\nfor column in columns_incorrect:\n    dataset[column] = dataset[column].map(replace_).values","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.isna().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.isnull().sum(axis=1).value_counts()\n#Number of rows by number of missing values","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### The  ðœ’2 could be used to test goodness of fit, homogenity test and independence test. The latter will be used in this case to figure out if the missigness of data in the Height column is dependent (or not) on the other variables (columns).\n\n#### The test starts by stating a first hypothesis called (the null hypothesis) and calculates a measure of closness between the observed data and the expected data (in the case where the null hypothesis is satisfied).\n\n#### The null hypothesis in this case is the following: There is no association between the missingness in the Height column and the dependent variable\n\n#### Alpha = 0.05","metadata":{}},{"cell_type":"code","source":"def check_mcar(data, incorrect_columns, dependent_categorical_variable):\n    \"\"\" To check whether the missingness of data is dependent on the outcome variable. A significant relationship can bias our results.\"\"\"\n    new_columns = []\n    \n    for column in incorrect_columns:\n        data[column+\"_missing\"]  = False\n        data.loc[data[data[column].isnull()].index, column+\"_missing\"] = True\n        new_columns.append(column+\"_missing\")\n        \n    for column in incorrect_columns:\n        grouped_true =data[data[column+\"_missing\"]==True].groupby(dependent_categorical_variable)[column+\"_missing\"].count()\n        grouped_false =data[data[column+\"_missing\"]==False].groupby(dependent_categorical_variable)[column+\"_missing\"].count()\n        table = [[grouped_true[0], grouped_false[0]],[grouped_true[1],grouped_false[1]]]\n        chi2, p, dof, ex = chi2_contingency(table, correction=True)\n        print(\"The p-value of chi-square test between\", column +\" and \"+dependent_categorical_variable,  \"is equal to {}\".format(p))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"check_mcar(dataset, columns_incorrect, \"Outcome\")","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dropping columns to check missingness\nprint(\"Shape before dropping\", dataset.shape)\nfor column in columns_incorrect:\n    dataset.drop((column+\"_missing\"), inplace=True,axis=1)\nprint(\"Shape after dropping\", dataset.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.head()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Since all p-values are > than 0.05, we do not reject the null hypothesis, and thus there is no relationship between the missingness of any data and whether the person is diabetic. Although it is hard to tell with certitude whether the data is missing at random or not, the above test tells us that there is no evidence to tell that the data is not missing at random baleful for the outcome of interest.\n\n#### Hence, safely assuming that the data is missing at random, we can imputate the missing values using the mean and median.\n\n#### Since Glucose and BloodPressure follow a normal distribution and are without outliers, missing data for these variables can be imputated using the mean. Since the other three columns â€“ SkinThickness, BMI and Insulin have presence of outliers, the missing values will be imputated with the mean for these.","metadata":{}},{"cell_type":"code","source":"imputer1 = SimpleImputer(strategy=\"mean\")\nimputer2 = SimpleImputer(strategy=\"mean\")\nimputer3 = SimpleImputer(strategy=\"median\")\nimputer4 = SimpleImputer(strategy=\"median\")\nimputer5 = SimpleImputer(strategy=\"median\")\n\n\ndataset[\"Glucose\"] = imputer1.fit_transform(dataset[\"Glucose\"].values.reshape(-1, 1)).copy()\ndataset[\"BloodPressure\"] = imputer2.fit_transform(dataset[\"BloodPressure\"].values.reshape(-1, 1)).copy()\ndataset[\"SkinThickness\"] = imputer3.fit_transform(dataset[\"SkinThickness\"].values.reshape(-1, 1)).copy()\ndataset[\"Insulin\"] = imputer4.fit_transform(dataset[\"Insulin\"].values.reshape(-1, 1)).copy()\ndataset[\"BMI\"] = imputer5.fit_transform(dataset[\"BMI\"].values.reshape(-1, 1)).copy()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.isna().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for feature in continuous_feature:\n    dataset[feature].hist(bins=25)\n    plt.xlabel(feature)\n    plt.ylabel(\"Count\")\n    plt.title(feature)\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(dataset, hue=\"Outcome\")","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Skin thickness and BMI seem to be correlated. No other pair of independent variables seems to be highly correlated","metadata":{}},{"cell_type":"code","source":"fig=plt.figure(figsize=(10,7))\nbackgroundcolor='#f6f5f7'\nfig.patch.set_facecolor(backgroundcolor)\nsns.heatmap(data=dataset.corr(),annot=True,cmap='OrRd')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_redundant_pairs(df):\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef get_top_abs_correlations(df, n=5):\n    au_corr = df.corr().abs().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[0:n]\n\nprint(\"Top Absolute Correlations\")\nprint(get_top_abs_correlations(dataset, 20))","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Using a threshold of 0.7, we conclude that there is no multicollinearity between any independent variables. As such, we do not drop any features to resolve this. I will get back to this later during feature selection.","metadata":{}},{"cell_type":"code","source":"sns.set(font_scale=1)\nLABELS = [\"Negative\", \"Positive\"]\ncount_classes = pd.value_counts(dataset['Outcome'], sort = True)\ncount_classes.plot(kind = 'pie', rot=0)\nplt.title(\"Visualization of Value of Label\")\nplt.xticks(range(2), LABELS)\nplt.ylabel(\"Frequency\")","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### There are a lot of instances with a positive outcome for Diabetes (Taking a threshold of 1:10). Hence, due to it not being rare, balancing the dataset is not necessary","metadata":{}},{"cell_type":"code","source":"dataset.groupby('Outcome').mean()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We notice that at an average people diabetes tend to have a higher number of pregnancies, higher glucose levels, higher blood pressure, thicker skin, a higher score on the insulin test, higher BMI levels, and a higher age.","metadata":{}},{"cell_type":"markdown","source":"### Train-test split","metadata":{}},{"cell_type":"code","source":"y = dataset['Outcome']\ndataset.drop(columns=['Outcome'],inplace=True)\nX= dataset\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test= train_test_split(X,y,test_size=0.2,random_state=54,stratify=y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Scaling the features","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Modelling","metadata":{}},{"cell_type":"markdown","source":"### F-test","metadata":{}},{"cell_type":"code","source":"features_response = dataset.columns.tolist()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"[f_stat,f_p_value] = f_classif(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f_test_df = pd.DataFrame({'Feature': features_response,'F-statistic': f_stat, 'p-value': f_p_value})\nf_test_df.sort_values('p-value')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Glucose and BMI seem to be the most useful determiners for predicting diabetes. All predictors seem to be related with the response variable, and thus will be useful in our model ","metadata":{}},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"code","source":"lr = LogisticRegression(random_state=42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_lr = dict()\nparam_lr['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\nparam_lr['penalty'] = ['none', 'l1', 'l2', 'elasticnet']\nparam_lr['C'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100,1e+3]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_search = GridSearchCV(lr, param_lr, scoring='accuracy', n_jobs=-1, cv=5)\nlr_search.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_search.best_params_\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LogisticRegression(penalty = \"none\", solver= \"newton-cg\",C=1e-05,random_state=42)\nlr.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = lr.predict(X_test)\naccuracy = accuracy_score(y_test,y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(font_scale=1.5)\ncm = confusion_matrix(y_pred, y_test)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## K-nearest Neighbours","metadata":{}},{"cell_type":"code","source":"knn = KNeighborsClassifier()\nparam_knn = {'n_neighbors':np.arange(2, 50)}  \ngrid_knn = GridSearchCV(knn, param_grid=param_knn,scoring='accuracy', cv=5)\n\ngrid_knn.fit(X_train, y_train)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_knn.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors= 13)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\naccuracy = accuracy_score(y_test,y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(font_scale=1.5)\ncm = confusion_matrix(y_pred, y_test)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decision Trees","metadata":{}},{"cell_type":"code","source":"dt = DecisionTreeClassifier()\nparam_dt = {'criterion':['gini','entropy'],'max_depth':np.arange(1, 50), 'min_samples_leaf':[1,2,4,5,10,20,30,40,80,100]}\ngrid_dt = GridSearchCV(dt, param_grid=param_dt, cv=5)\ngrid_dt.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_dt.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt = DecisionTreeClassifier(criterion= 'entropy', max_depth= 5, min_samples_leaf= 20)\ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)\naccuracy = accuracy_score(y_test,y_pred)\naccuracy\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(font_scale=1.5)\ncm = confusion_matrix(y_pred, y_test)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"rf = RandomForestClassifier(random_state=42)\nparam_rf = {'n_estimators':[100, 350, 500], 'min_samples_leaf':[2, 10, 30]}\ngrid_rf = GridSearchCV(rf, param_grid=param_rf, cv=5)\ngrid_rf.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_rf.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf = RandomForestClassifier(min_samples_leaf= 10, n_estimators= 100,random_state=42)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\naccuracy = accuracy_score(y_test,y_pred)\naccuracy","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(font_scale=1.5)\ncm = confusion_matrix(y_pred, y_test)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SVM","metadata":{}},{"cell_type":"code","source":"svc = SVC(random_state=42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_svc = GridSearchCV(svc, param_grid=param_grid, cv=5)\ngrid_svc.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_svc.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc = SVC(random_state=42,C= 100, gamma= 0.01, kernel= 'sigmoid')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\naccuracy = accuracy_score(y_test,y_pred)\naccuracy","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(font_scale=1.5)\ncm = confusion_matrix(y_pred, y_test)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Naive Bayes","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gnb = GaussianNB()\ngnb.fit(X_train, y_train)\ny_pred = gnb.predict(X_test)\naccuracy_score(y_test,y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(font_scale=1.5)\ncm = confusion_matrix(y_pred, y_test)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVM is the best model with an accuracy of 78%","metadata":{}}]}