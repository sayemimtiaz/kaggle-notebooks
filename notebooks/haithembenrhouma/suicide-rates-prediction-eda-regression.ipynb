{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Suicide Rate Prediction\n\n## EDA, Decision Tree, Linear Regression and much more! \n\nHey guys, I hope you are doing good! So I'm still a beginner in the Data Science field, if you have any recommendations or suggestions please put them in the comments down below! Thanks :D \n\nToday in our analysis we will look at the \"Suicide Rates Overview 1985 to 2016\" dataset. This dataset contains 12 features: \n- country: The country of residence of the individual\n- year: The year the suicide happend \n- sex: The gender of the individual (male/female)\n- age group: The age group of the individual \n- count of suicides: The count number of suicides that happend \n- population: The overall population of the country \n- suicide rate: The number of suicides per 100,000 person\n- country-year composite key: A code containing the country name plus the year of the suicide \n- HDI for year: Human Development Index is a statistic composite index of life expectancy, education, and per capita income indicators.\n- gdpforyear: The GDP of the country at that year\n- gdppercapita: The GDP per capita of the country at that year\n- generation: The name of the corresponding generation \n\nDuring this analysis, we will first clean our data and make the needed transformation(We will keep transforming our data whenever it's needed during the analysis). Second, we will perform an exploratory data analysis where we will try to extract a few usefull insights from our dataset. Finally we will try to predict the number of suicides given the features we have. \n\n![](https://uploads-ssl.webflow.com/5a4c78412b69220001d82c7d/5a4c78412b69220001d82dc2_171128_cropped_tsis-lowres.jpg)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing:","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Importing needed packages\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nsns.set()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- First, we'll start by importing our data. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing the dataset\nraw_data = pd.read_csv('../input/suicide-rates-overview-1985-to-2016/master.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Now, we'll check for any null values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking for any null values\nraw_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The HDI for year column has 2/3 of its data as missing values, unfortunatly, we'll have to drop this column. The country-year column isn't that useful also. We'll discard it too. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Getting the names of the columns we have\nprint(raw_data.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing the HDI and country-year columns\nno_na_data = raw_data[['country', 'year', 'sex', 'age', 'suicides_no', 'population',\n       'suicides/100k pop', ' gdp_for_year ($) ', 'gdp_per_capita ($)', 'generation']]\nno_na_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Let's now check for any outliers or wrong entires through the Pandas describe method that gives us a small summary of our numerical features. This will help us detect any visible anomalies in our data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Describing our dataset\nno_na_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- All the features seem to be in good order, except the suicides rates variables. It is very odd to have 0 suicides in a year! Let's check the values for which we have no suicides in a given year. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the entries where suicides_no = 0 \nno_na_data[no_na_data['suicides_no']==0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The entries that had 0 values didn't represent the whole country in a given year, they actually represented a specific age category. So we are good to go, it's safe to say that our data is ready and we can work with it. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a new dataframe 'clean_data' to work with \nclean_data = no_na_data.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Grouping our data by year\ngp_year_data = clean_data.groupby('year', as_index=False).mean()\n\n#Plotting the suicides rates by years \nfig, ax = plt.subplots(figsize=(12,4))\nsns.lineplot(x='year', y='suicides/100k pop', color=sns.husl_palette(6)[5], data=gp_year_data, ax=ax)\nplt.xlabel('Year')\nplt.ylabel('Suicides/100k')\nplt.title('Evolution of suicide rates\\nthroughout the years', size=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- From the line plot above we can see that the suicide rates kept oscilating between 11 and 16 suicide per 100,000 person. In 1995 the suicde rates peaked reaching almost 16 suicide per 100,000 individual. It started dropping afterwards to reach a minimum of 11 suicide per 100,000 individual in 2010 and 2014. Let's check if we can determine which countries have the highest suicide rates. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Grouping the data by country\ngp_cnt_data = clean_data.groupby('country', as_index=False).sum()\ntop_ten = gp_cnt_data.nlargest(10, 'suicides_no').sort_values('suicides_no', ascending=False)\n\n#Plotting the number of suicides according to the countries \nfig, ax = plt.subplots(figsize=(12,4))\nsns.barplot(x='suicides_no', y='country', palette='husl', data=top_ten, ax=ax)\nplt.xlabel('Suicides')\nplt.ylabel('Country')\nplt.title('Suicides according\\nto the country', size=15)\nax.ticklabel_format(style='plain', axis='x')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The countries that have the highest number of suicides from 1985 untill 2016 are: \n    1. Russia \n    2. USA\n    3. Japan\n    \nLet's check if we can determine which generation has the highest suicide rates. ;","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Grouping the data by generations\ngp_gen_data = clean_data.groupby('generation', as_index=False).mean()\n\n#Plotting the suicide rates according to the generations \nfig, ax = plt.subplots(figsize=(12,4))\nsns.barplot(x='generation', y='suicides/100k pop', palette='husl', data=gp_gen_data, ax=ax, \n            order=['G.I. Generation', 'Silent', 'Boomers', 'Generation X', 'Millenials', 'Generation Z'])\nplt.xlabel('Generation')\nplt.ylabel('Suicides/100k')\nplt.title('Suicide rates according\\nto the generation', size=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- As we can see, the G.I. Generation or the Greatest Generation (the generation who lived during the WWII) has the highest suicide rate with almost 25 suicides per 100,000 person. This is a very big number compared to younger generations, this might be due to the fact that this generation suffered a lot during the WWII, many of them lost their loved ones and experienced different traumatic events. The suicide rates decrease from a generation to another, where Generation Z has the lowest suicide rates with 1 suicide per 100,000 person. Let's check the suicide rates according to the age categories.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Grouping the data by age\ngp_age_data = clean_data.groupby('age', as_index=False).mean()\n\n#Plotting the suicide rates according to the age categories\nfig, ax = plt.subplots(figsize=(12,4))\nsns.barplot(x='age', y='suicides/100k pop', palette='husl', data=gp_age_data, ax=ax, \n           order=['5-14 years', '15-24 years', '25-34 years', '35-54 years', '55-74 years', '75+ years'])\nplt.xlabel('Age categories')\nplt.ylabel('Suicides/100k')\nplt.title('Suicide rates according\\nto the age categories', size=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see that as the person gets older it tends to be more suicidal. This could be explained by the fact that important life changes that happen as we get older may cause feelings of uneasiness, stress, and sadness. But this might be due to the fact that old people (75+ years) belong to the G.I. Generation which already has the highest suicide rates. To further explore this we must check the number of people that commited suicide within each age category with respect to their generation. This way we can find out the distribution of ages of suicidal people within each generation. This will help us to identify if suicide is due to the age factor or to the generation. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Grouping our data by generation and age \ngp_gen_age_data = clean_data.groupby(['generation', 'age'], as_index=False).mean()\n\n#Making a list containing all the gens \ngens = ['G.I. Generation', 'Silent', 'Boomers', 'Generation X', 'Millenials', 'Generation Z']\n\n#Creating the axis of the plots\nplt.figure(figsize=(12,18))\nax1 = plt.subplot2grid((6,1),(0,0))\nax2 = plt.subplot2grid((6,1),(1,0))\nax3 = plt.subplot2grid((6,1),(2,0))\nax4 = plt.subplot2grid((6,1),(3,0))\nax5 = plt.subplot2grid((6,1),(4,0))\nax6 = plt.subplot2grid((6,1),(5,0))\n\n#Making a list containing all the axes\naxes = [ax1, ax2, ax3, ax4, ax5, ax6]\n\n#Making a for loop to plot the needed plots \nfor gen, ax in zip(gens, axes):\n    sns.barplot(x='age', y='suicides/100k pop', palette='husl', \n                data=gp_gen_age_data[gp_gen_age_data['generation'] == gen],\n                ax=ax, order=['5-14 years', '15-24 years', '25-34 years', '35-54 years', \n                          '55-74 years', '75+ years'])\n    ax.set_xlabel('Age categories')\n    ax.set_ylabel('Suicides/100k')\n    ax.set_title(gen, size=15)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- From the plot above we can see that unfortunatly, we don't have every age category for each generation. This is due to the fact the data collection started from around the year 1985, that means that: \n\n    1. people who belong to the G.I. Generation at the year of 1985, will be already 55+ years, so we won't have any people from this generation who belong to younger age category who commited suicide. \n    2. equivalently, we won't have any 25-34 years (or younger) people who belong to the Silent Generation and commited suicide because this generation in 1985 were at least 35 years old. \n    3. The same goes for younger generations, we might have the young age categories but since the data collection stopped at around 2015 or so, we won't be seeing any boomers older than 75, or millenials older than 35 years old. \n    \nDue to these reasons, we can't check if the elevated suicide rates are due to the generation or to the age factor. Let's see if the economy of a country has any effect on suicide rates.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Grouping the data by country\ngp_cnt_data = clean_data.groupby('country', as_index=False).mean()\n\n#Plotting the suicide rates according to the generations \nfig, ax = plt.subplots(figsize=(12,4))\nsns.scatterplot(x='gdp_per_capita ($)', y='suicides/100k pop', color=sns.husl_palette(6)[4], data=gp_cnt_data, ax=ax)\nplt.xlabel('GDP per capita')\nplt.ylabel('Suicides/100k')\nplt.title('Suicide rates according\\nto the GDP per capita', size=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- From the scatterplot above we can't really see a relation between the suicide rates and the GDP per capita, let's present the data in a diffrent way maybe this will help us to detect a pattern. We're going to split the data we have into bins. This way we'll present the different GDP per capita categories and each category will have it's corresponding suicide rate.  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making bins and labels for the gdp_per_capita feature\nbins = list(range(0, 160000, 20000))\nlabels = ['0-20,000', '20,000-40,000', '40,000-60,000', '60,000-80,000', '80,000-100,000', '100,000-120,000', '+120,000']\nclean_data['gdp_per_capita_bins'] = pd.cut(clean_data['gdp_per_capita ($)'], bins=bins, labels=labels)\n\n#Plotting the suicide rates according to the gbp per capita bins \nfig, ax = plt.subplots(figsize=(12,4))\nsns.barplot(x='gdp_per_capita_bins', y='suicides/100k pop', palette='husl', data=clean_data, ax=ax)\nplt.xlabel('GDP per capita')\nplt.ylabel('Suicides/100k')\nplt.title('Suicide rates according\\nto the GDP per capita', size=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Even after transformation we can't detect a clear pattern from our data. All the GDP per capita values give a suicide rate between 10 and 13 suicides per 100k person. If we could say something about the data we have, we would say that the GDP doesn't have any effect on the suicide rates.  \n\n\n## Modelling and predictions: \n\n\n- In this part of the notebook, we will try to fit two diffrent models to our data. First we are going to try the linear regression model. Second we will use the decision tree regressor. We will use the SkLearn library for both algorithms and for data preprocessing too. We will first start by selecting the independent features and storing them in a variable named X, and our independent feature in a variable named y. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Selecting the dependent and independent features\nX = clean_data[['country', 'sex', 'population', 'age', 'gdp_per_capita ($)', 'generation']]\ny = clean_data['suicides/100k pop']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- As we know, the linear regression algorithm doesn't work with categorical features. To deal with this problem we'll need to transform our categorical data to dummy variables. For this we will use the Pandas method get_dummies. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Transforming the categorical variables to dummy variables\nX = pd.get_dummies(X, drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- After dealing with categorical variables, we can move to scaling our data in order to normalise the data within a particular range.Also, scaling helps in speeding up the calculations in an algorithm.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing needed package for scaling\nfrom sklearn.preprocessing import StandardScaler ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Scaling our data \nsc = StandardScaler()\nX[['population', 'gdp_per_capita ($)']] = sc.fit_transform(X[['population', 'gdp_per_capita ($)']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing needed package for splitting the dataset\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Finally, we'll split our data into two sets: training and testing. The sizes will be 80% for the training data and 20% for testing data. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting the dataset \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Now we can move to creating our models. \n\n### Linear Regression: \n\n- We'll start with the linear regression, first we'll import the needed tools from SkLearn. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing the Linear Regression algorithm \nfrom sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Afterwards, we'll need to initialize our model and fit it to the training set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Initializing our Linear Regression\nlr = LinearRegression()\nlr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Now we can make predictions, for this we will use the X_test. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predicting the test values\nlr_y_pred = lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We saved the predictions in a variable named lr_y_pred. In order to compare them to the real values, we will plot them in the same figure. The values should be aligned on a 45Â° dergrees line.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting the results\nfig, ax = plt.subplots(figsize=(12,4))\nsns.scatterplot(lr_y_pred, y_test, ax=ax, color=sns.husl_palette(10)[0])\nsns.lineplot([0, 175], [0, 175], color=sns.husl_palette(10)[5], ax=ax)\nplt.xlabel('Actual values')\nplt.ylabel('Predicted values')\nplt.title('Prediction evaluation (Linear Regression)', size=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Our model doesn't seem to be doing a good job, this might be due to fact that the features we selected aren't good enough, or it might be due the fact that the data we have isn't linear so a similar model won't be any good to estimate the values. Maybe the decision tree will perfom better, let's check it out. \n\n### Decision Tree Regressor: \n\n- We will use exactly the same staps we did for the linear regression. First we'll import the tools from SkLearn.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing the Decision Tree algorithm \nfrom sklearn.tree import DecisionTreeRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Then we'll initialize the model and fit it to the train data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Initializing our Decision Tree\ndt = DecisionTreeRegressor()\ndt.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Now, we'll predicted the test set results.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predicting the test values\ndt_y_pred = dt.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Finally we will compare our predictions to the real ones by plotting the on the same figure.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting the results\nfig, ax = plt.subplots(figsize=(12,4))\nsns.scatterplot(dt_y_pred, y_test, ax=ax, color=sns.husl_palette(10)[0])\nsns.lineplot([0, 175], [0, 175], color=sns.husl_palette(10)[5], ax=ax)\nplt.xlabel('Actual values')\nplt.ylabel('Predicted values')\nplt.title('Prediction evaluation (Decision Tree)', size=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The predictions are visibly much better than the one that the linear regression produced. Still we can't say that model produced good results. The decision tree needs further tuning in order to produce better results. Our data might also need more transformation or we might need more features in order to produce more accurate results. ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}