{"cells":[{"metadata":{"_uuid":"9e2bd514fbb5fa41ff5faa1edae0899cc14d3e85"},"cell_type":"markdown","source":"![](https://i.imgur.com/sWyuy4Y.jpg)"},{"metadata":{"_uuid":"929bb7239d67aa71cd065830de8680218023d9ae"},"cell_type":"markdown","source":"In this notebook, I'll use the dataset \"StackSample: 10% of Stack Overflow Q&A\", I'll only use the questions and the tags. \nI will implement a tag suggestion system. I'll both try machine learning models and deep learning models like Word2Vec. I'll then compare the performance of both approaches. \n\nThis notebook will be divided in 2 parts:\n* PART 1 : Cleaning data and EDA\n* PART 2 : Classical classifiers implemented (SGC classifier, MultiNomial Naive Bayes Classifier, Random Forest Classfier, ...\n"},{"metadata":{"_uuid":"a7cf70d54413735fe632aaa5df43f34e353e0ff4"},"cell_type":"markdown","source":"**PART 1: Cleaning Data and Exploratory Data Analysis**"},{"metadata":{"_uuid":"abe58b6f63ad6fc975f6b62c1a47d3af54a943e5"},"cell_type":"markdown","source":"**1.1 Setting up the dataset for later training**"},{"metadata":{"_uuid":"6ddb1e9c1032972f4c87204650195ee00fb3bb29"},"cell_type":"markdown","source":"Importing useful libraries at first"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.lines as mlines\nimport seaborn as sns\n\nimport warnings\n\nimport pickle\nimport time\n\nimport re\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom nltk.tokenize import ToktokTokenizer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom string import punctuation\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import model_selection\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import hamming_loss\nfrom sklearn.cluster import KMeans\n\n\nimport logging\n\nfrom scipy.sparse import hstack\n\nwarnings.filterwarnings(\"ignore\")\nplt.style.use('bmh')\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Setting a random seed in order to keep the same random results each time I run the notebook\nnp.random.seed(seed=11)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc1c92bf4166de1e2ce4bfb229b40b32eb94374a"},"cell_type":"code","source":"import os \nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"120edf8b2306a6391ad992cf3f48598739ec1940"},"cell_type":"code","source":"# Importing the database \n\ndf = pd.read_csv(\"../input/Questions.csv\", encoding=\"ISO-8859-1\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40c3e09d38aecf03ce13fd0d88809e80a67d3d1a"},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9beed1bea78dd1ed930caeef2923aef03420e1cc"},"cell_type":"code","source":"tags = pd.read_csv(\"../input/Tags.csv\", encoding=\"ISO-8859-1\", dtype={'Tag': str})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f4a7e6a7ee2745fc207f19fccb9c0dbc3a571a4"},"cell_type":"code","source":"tags.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab191660e879734a6c26d1c1f2afd0e1218a50fc"},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b71d45e2c86152d95f22679f01d8f855b042a62e"},"cell_type":"code","source":"tags.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec6a03e07edd3ce3de1f20f073bdf18ffca87f4c"},"cell_type":"markdown","source":"First, what I want to do is to merge both dataframes. In order to do that, I'll have to group tags by the id of the post since a post can have multiple tags. I'll just use the groupeby function and then merge the dataframes on the id. "},{"metadata":{"trusted":true,"_uuid":"933761c8b2bdb668e0bca3c9ea9cab36dd1a4391"},"cell_type":"code","source":"tags['Tag'] = tags['Tag'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"267d7c8f01a3b427ab139d689ce5d13ab8820913","_kg_hide-output":true},"cell_type":"code","source":"grouped_tags = tags.groupby(\"Id\")['Tag'].apply(lambda tags: ' '.join(tags))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4096130e47ec904f9bbe27822d1ddf403a675dc"},"cell_type":"code","source":"grouped_tags.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"a9ecae8a6445c671d9c0a1b207df8b530bae70ab"},"cell_type":"code","source":"grouped_tags.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8da81a33552cd273c80e159e0c1cc8dddc98399c"},"cell_type":"code","source":"grouped_tags_final = pd.DataFrame({'Id':grouped_tags.index, 'Tags':grouped_tags.values})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9968760c364a005b7de54bd908cfc612fc9defe8"},"cell_type":"code","source":"grouped_tags_final.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2612dea031562d420f8dac14801d167721d2bcc9"},"cell_type":"code","source":"df.drop(columns=['OwnerUserId', 'CreationDate', 'ClosedDate'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd794a28df6e19818c754702ed5f3745262d866f"},"cell_type":"code","source":"df = df.merge(grouped_tags_final, on='Id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"132c4501747d2832eaf8d95f5d9cb3acaadc97f4","scrolled":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df4a7b2e88af524f4bd778c223d580de76c57cf7"},"cell_type":"markdown","source":"Now, I'll take only quesions witha score greater than 5. I'm doing that for 2 reasons:\n* 1- I'll require less computational resources from kaggle.\n* 2- The posts will probably be with a better quality and will be better tagged since they have lots of upvotes. \n"},{"metadata":{"trusted":true,"_uuid":"ecebe90606c70e84e4e87e1661e49a11c37e529f"},"cell_type":"code","source":"new_df = df[df['Score']>5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e462e56ec2d84a0f3810d53bee3715cb570907f1"},"cell_type":"markdown","source":"**1.2 Cleaning Data**"},{"metadata":{"trusted":true,"_uuid":"2f966f3733dfc5668fd20af7850302c8c2bbdbf7"},"cell_type":"code","source":"plt.figure(figsize=(5, 5))\nnew_df.isnull().mean(axis=0).plot.barh()\nplt.title(\"Ratio of missing values per columns\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1550d62d082219a8c8564f8a95afb121d9096c67","scrolled":true},"cell_type":"code","source":"print('Dupplicate entries: {}'.format(new_df.duplicated().sum()))\nnew_df.drop_duplicates(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"097a6864ce893217365a3dca22bb68b442746740"},"cell_type":"markdown","source":"This is a very good dataset since there are no missing valeus or dupplicate values. "},{"metadata":{"trusted":true,"_uuid":"6ed21b85891cb57be8273aa41443e6c8198c0d0c"},"cell_type":"code","source":"new_df.drop(columns=['Id', 'Score'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c6033e87a6ca08eeca49f64b8ca1bf6a9a2d11b"},"cell_type":"markdown","source":"Now we only need 3 columns: Body, Title and Tags. "},{"metadata":{"_uuid":"d3a6dc67414c2faf61fc32adf821d5d6c5a00e4e"},"cell_type":"markdown","source":"**1.2.1 Tags**"},{"metadata":{"_uuid":"7d52b54ca515eb7ef5a9545b9f722090abc4c683"},"cell_type":"markdown","source":"Let's do some cleaning on the tags' column. Furthermore, I decided to keep the 100 most popular tags because I'll be easier to predict the right tag from 100 words than from 14,000 and because we want to keep macro tags and not be too specific since it's only a recommendation for a post, the user can add more specific tags himself. "},{"metadata":{"trusted":true,"_uuid":"f52f8b1079382f88516dd1df91aeaebe6d4f1dc4"},"cell_type":"code","source":"new_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d112c6cca7ac7b8fecfafdecfe3c75820a8b6f8c","_kg_hide-output":true},"cell_type":"code","source":"new_df['Tags'] = new_df['Tags'].apply(lambda x: x.split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84505cde455a6d528b08e72688caa6c092893388"},"cell_type":"code","source":"all_tags = [item for sublist in new_df['Tags'].values for item in sublist]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79be7667451d41ac7cd9ef52b7edae19a8c9e846"},"cell_type":"code","source":"len(all_tags)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a7e3c76664f2bbcbab52d1ecbf445e8451d877d"},"cell_type":"code","source":"my_set = set(all_tags)\nunique_tags = list(my_set)\nlen(unique_tags)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56cecaeb6a313c81202a08fbcca65e33da95b3be"},"cell_type":"code","source":"flat_list = [item for sublist in new_df['Tags'].values for item in sublist]\n\nkeywords = nltk.FreqDist(flat_list)\n\nkeywords = nltk.FreqDist(keywords)\n\nfrequencies_words = keywords.most_common(100)\ntags_features = [word[0] for word in frequencies_words]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a97a5e5d2e1031394382558d4dc3b54530400005"},"cell_type":"code","source":"tags_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4a1cb0882531f0de6aef354e468cd0711bbb432"},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15, 10))\nkeywords.plot(100, cumulative=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3dabce5e92dc9314e7042384add46fc2c31f3112"},"cell_type":"code","source":"def most_common(tags):\n    tags_filtered = []\n    for i in range(0, len(tags)):\n        if tags[i] in tags_features:\n            tags_filtered.append(tags[i])\n    return tags_filtered","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1b1f8eefaf9b94b5bffdbd295b5770fae7b2984"},"cell_type":"code","source":"new_df['Tags'] = new_df['Tags'].apply(lambda x: most_common(x))\nnew_df['Tags'] = new_df['Tags'].apply(lambda x: x if len(x)>0 else None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be6aae9b05c9d212cb26adb487df1476e6040c48"},"cell_type":"code","source":"new_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f9ffd9e9e0dfe7c2e4c595e31559e2f5ec513f4"},"cell_type":"code","source":"new_df.dropna(subset=['Tags'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"608e36814e0b2befb41bb9dc6eca073edf48fbe4"},"cell_type":"code","source":"new_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8864db3bccbe04abb7656fa580fbbb87b0528c43"},"cell_type":"markdown","source":"We are here loosing 10000 rows but the it's for the greater good. "},{"metadata":{"trusted":true,"_uuid":"0240dc179aeabfdd8bbd6638d0e1b3ae5cb56877"},"cell_type":"markdown","source":"**1.2.2 Body**"},{"metadata":{"_uuid":"598869f86fd4f9a6fe1e41f9acce43d392cb50aa"},"cell_type":"markdown","source":"In the next two columns: Body and Title, I'll use lots of text processing:\n* Removing html format \n* Lowering text\n* Transforming abbreviations \n* Removing punctuation (but keeping words like c# since it's the most popular tag)\n* Lemmatizing words\n* Removing stop words"},{"metadata":{"trusted":true,"_uuid":"e1b8ccc185a38ec08ee74deecd749b187053e444"},"cell_type":"code","source":"# Converting html to text in the body\n\nnew_df['Body'] = new_df['Body'].apply(lambda x: BeautifulSoup(x).get_text()) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b25cd0dd5fd7bc22eaa629bdabfa9ce882661f85"},"cell_type":"code","source":"def clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"can not \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n    text = re.sub(r\"\\'\\n\", \" \", text)\n    text = re.sub(r\"\\'\\xa0\", \" \", text)\n    text = re.sub('\\s+', ' ', text)\n    text = text.strip(' ')\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d39a9d9c3221288d7d72076070219cb861ad399e"},"cell_type":"code","source":"new_df['Body'] = new_df['Body'].apply(lambda x: clean_text(x)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2b3cfc4fbb0482324dc1937d8411feb5334f7d2"},"cell_type":"code","source":"token=ToktokTokenizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e96435d0bb6d758f5868ae7cf1561e28293b21e0"},"cell_type":"code","source":"punctuation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52c525eabf07b51a54af954f4adc7404febdf27e"},"cell_type":"code","source":"punct = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7681b82ec0d4e8fb87fc7c2a2d7e90f00184bdb7"},"cell_type":"code","source":"def strip_list_noempty(mylist):\n    newlist = (item.strip() if hasattr(item, 'strip') else item for item in mylist)\n    return [item for item in newlist if item != '']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8d4bb83559025f06a5bae00aa511f5985bb56bd"},"cell_type":"code","source":"def clean_punct(text): \n    words=token.tokenize(text)\n    punctuation_filtered = []\n    regex = re.compile('[%s]' % re.escape(punct))\n    remove_punctuation = str.maketrans(' ', ' ', punct)\n    for w in words:\n        if w in tags_features:\n            punctuation_filtered.append(w)\n        else:\n            punctuation_filtered.append(regex.sub('', w))\n  \n    filtered_list = strip_list_noempty(punctuation_filtered)\n        \n    return ' '.join(map(str, filtered_list))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"997064cd595544af307751a19ac0680a392a79a4"},"cell_type":"code","source":"new_df['Body'] = new_df['Body'].apply(lambda x: clean_punct(x)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c32de0e6fb753123485706a2655a68260de9741a"},"cell_type":"code","source":"new_df['Body'][2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b18fd3ceb79801c3737758607e39ff6881dc1f0b"},"cell_type":"code","source":"lemma=WordNetLemmatizer()\nstop_words = set(stopwords.words(\"english\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7adfb53ebffef9af3068171d4f4e8e7b4b959527"},"cell_type":"code","source":"def lemitizeWords(text):\n    words=token.tokenize(text)\n    listLemma=[]\n    for w in words:\n        x=lemma.lemmatize(w, pos=\"v\")\n        listLemma.append(x)\n    return ' '.join(map(str, listLemma))\n\ndef stopWordsRemove(text):\n    \n    stop_words = set(stopwords.words(\"english\"))\n    \n    words=token.tokenize(text)\n    \n    filtered = [w for w in words if not w in stop_words]\n    \n    return ' '.join(map(str, filtered))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32e75ae4b01dcafb295fceb59a1f973b1d5d4b86"},"cell_type":"code","source":"new_df['Body'] = new_df['Body'].apply(lambda x: lemitizeWords(x)) \nnew_df['Body'] = new_df['Body'].apply(lambda x: stopWordsRemove(x)) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ae7426282e33a923238b13dddf083700bfaa360"},"cell_type":"markdown","source":"**1.2.3 Title**"},{"metadata":{"trusted":true,"_uuid":"5151ce53b67e2a7a94565e28171d4dded9f3c390"},"cell_type":"code","source":"new_df['Title'] = new_df['Title'].apply(lambda x: str(x))\nnew_df['Title'] = new_df['Title'].apply(lambda x: clean_text(x)) \nnew_df['Title'] = new_df['Title'].apply(lambda x: clean_punct(x)) \nnew_df['Title'] = new_df['Title'].apply(lambda x: lemitizeWords(x)) \nnew_df['Title'] = new_df['Title'].apply(lambda x: stopWordsRemove(x)) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe90f28a386d20375d1ec8c40dec91684802142e"},"cell_type":"markdown","source":"**1.3 EDA**"},{"metadata":{"_uuid":"7103674c5e35809eb0b89c0fa86bdb1ec2f76af9"},"cell_type":"markdown","source":"Here I'll just use some LDA to see if shows any paterns in words and the main topics.  "},{"metadata":{"trusted":true,"_uuid":"14aee1be48cf9da95ba0b6f4425ba99a13713d40"},"cell_type":"code","source":"no_topics = 20","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bdba110b6091d86d5af652c2c54caf688690e82f"},"cell_type":"code","source":"text = new_df['Body']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42b4c014172c9e2b55705d5e80f7e0a0d5d0424f"},"cell_type":"code","source":"vectorizer_train = TfidfVectorizer(analyzer = 'word',\n                                       min_df=0.0,\n                                       max_df = 1.0,\n                                       strip_accents = None,\n                                       encoding = 'utf-8', \n                                       preprocessor=None,\n                                       token_pattern=r\"(?u)\\S\\S+\", # Need to repeat token pattern\n                                       max_features=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70c71f3bd686f989e504d8574b645b3bc3026e84"},"cell_type":"code","source":"TF_IDF_matrix = vectorizer_train.fit_transform(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f8c7f50bea68a8b3878bdf91da12d87434863de"},"cell_type":"code","source":"lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50,random_state=11).fit(TF_IDF_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07526c92eb492e38f47d9d1bddeedbfdb36a25a9"},"cell_type":"code","source":"def display_topics(model, feature_names, no_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"--------------------------------------------\")\n        print(\"Topic %d:\" % (topic_idx))\n        print(\" \".join([feature_names[i]\n                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n        print(\"--------------------------------------------\")\n        \n\nno_top_words = 10\ndisplay_topics(lda, vectorizer_train.get_feature_names(), no_top_words)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"709f989864c43312c2a3606583be434572b75e40"},"cell_type":"markdown","source":"It's a bit disappointing but I'm certain that it can be done better. "},{"metadata":{"_uuid":"119c8c35293cccd9627b58544acaaf06b4cb32d2"},"cell_type":"markdown","source":"**PART 2: Classical classifiers**"},{"metadata":{"_uuid":"4d7d89ca6323645d6bf722bd572852edc2b9571c"},"cell_type":"markdown","source":"**2.1 Data preparation**"},{"metadata":{"_uuid":"ccbc4065cdf68e9ef58fcad2889fc7454372425a"},"cell_type":"markdown","source":"Now our data is almost ready to be put into a classifier. I just need to:\n* Binarize the tags\n* Use a TFIDF for body and Title\nThe parameters in the TFIDF are very important for the performance of our tags since we don't want him to delete words like c# or.net. To do that we need to use the following pattern : token_pattern=r\"(?u)\\S\\S+\""},{"metadata":{"trusted":true,"_uuid":"8640de349b18ea297f15c3d3eed3836d3ed19e14"},"cell_type":"code","source":"X1 = new_df['Body']\nX2 = new_df['Title']\ny = new_df['Tags']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4669f647cb5a147710294f1116ac4e2f29f50d7"},"cell_type":"code","source":"multilabel_binarizer = MultiLabelBinarizer()\ny_bin = multilabel_binarizer.fit_transform(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8fd01469710da8a69fc45de00d37901d570814a"},"cell_type":"code","source":"vectorizer_X1 = TfidfVectorizer(analyzer = 'word',\n                                       min_df=0.0,\n                                       max_df = 1.0,\n                                       strip_accents = None,\n                                       encoding = 'utf-8', \n                                       preprocessor=None,\n                                       token_pattern=r\"(?u)\\S\\S+\",\n                                       max_features=1000)\n\nvectorizer_X2 = TfidfVectorizer(analyzer = 'word',\n                                       min_df=0.0,\n                                       max_df = 1.0,\n                                       strip_accents = None,\n                                       encoding = 'utf-8', \n                                       preprocessor=None,\n                                       token_pattern=r\"(?u)\\S\\S+\",\n                                       max_features=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fa0c121a6510f161df99e7fd882534214a8e380"},"cell_type":"code","source":"X1_tfidf = vectorizer_X1.fit_transform(X1)\nX2_tfidf = vectorizer_X2.fit_transform(X2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d241fca148de1384e5323d2b194feb52727d14e1"},"cell_type":"code","source":"X_tfidf = hstack([X1_tfidf,X2_tfidf])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1219d6cdc1f1d27c90910659f1be21a917f9288e"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_bin, test_size = 0.2, random_state = 0) # Do 80/20 split","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9ba6707c5aef3d55a7a5a8d33182c87d83af703"},"cell_type":"markdown","source":"Now it's finally ready. "},{"metadata":{"_uuid":"6755c0843211713e0b4bf90cb4618cfdaa9fef85"},"cell_type":"markdown","source":"**2.2 One vs Rest**"},{"metadata":{"_uuid":"c4448315a460e141e1c8b079b809c1040e4fc050"},"cell_type":"markdown","source":"To evaluate our models, I'll use the jacard score since it's the best fitted for multi label classification. "},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"03e10593770e8b27cb2a9574efcc9cd8b6dabbd6"},"cell_type":"code","source":"def avg_jacard(y_true,y_pred):\n    '''\n    see https://en.wikipedia.org/wiki/Multi-label_classification#Statistics_and_evaluation_metrics\n    '''\n    jacard = np.minimum(y_true,y_pred).sum(axis=1) / np.maximum(y_true,y_pred).sum(axis=1)\n    \n    return jacard.mean()*100\n\ndef print_score(y_pred, clf):\n    print(\"Clf: \", clf.__class__.__name__)\n    print(\"Jacard score: {}\".format(avg_jacard(y_test, y_pred)))\n    print(\"Hamming loss: {}\".format(hamming_loss(y_pred, y_test)*100))\n    print(\"---\")    ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"6fb3326bc2c812cd991ca969762e54190b548150"},"cell_type":"code","source":"dummy = DummyClassifier()\nsgd = SGDClassifier()\nlr = LogisticRegression()\nmn = MultinomialNB()\nsvc = LinearSVC()\nperceptron = Perceptron()\npac = PassiveAggressiveClassifier()\n\nfor classifier in [dummy, sgd, lr, mn, svc, perceptron, pac]:\n    clf = OneVsRestClassifier(classifier)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    print_score(y_pred, classifier)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7615d72e56406afe2d981b72b0b69ceff34aca91"},"cell_type":"markdown","source":"**2.3 MLP Classifier**"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"43560fbdf7c0232998cbb9ff303b257fae491eab"},"cell_type":"code","source":"mlpc = MLPClassifier()\nmlpc.fit(X_train, y_train)\n\ny_pred = mlpc.predict(X_test)\n\nprint_score(y_pred, mlpc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4bbd3c1f3db69cfa7d59e9bb77a89e1ee21620b9"},"cell_type":"markdown","source":"**2.4 Random Forest**"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"237dd4c165fc9433a1d146a2ba16bd7c085d90c5"},"cell_type":"code","source":"rfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)\n\ny_pred = rfc.predict(X_test)\n\nprint_score(y_pred, rfc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b565bd1188ff3a760d2c687052bcbdd730661c25"},"cell_type":"markdown","source":"**2.5 GridSearch CV on the best classifier **"},{"metadata":{"trusted":true,"_uuid":"8296f35a8035826033f78093a6d6826ceeaf6bb3"},"cell_type":"code","source":"param_grid = {'estimator__C':[1,10,100,1000]\n              }","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1bbe3e12fa2b1e55af3030d57a7212d46c6156e4","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"svc = OneVsRestClassifier(LinearSVC())\nCV_svc = model_selection.GridSearchCV(estimator=svc, param_grid=param_grid, cv= 5, verbose=10, scoring=make_scorer(avg_jacard,greater_is_better=True))\nCV_svc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e192adaeaf60e362ee8cf71c3e2439aa15067576","trusted":true},"cell_type":"code","source":"CV_svc.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20290a9fdc51cf5155dd062462c7910fd6fe8b6e"},"cell_type":"code","source":"best_model = CV_svc.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2029b6994027cd3bba479ccf3c155a8b77ec17df"},"cell_type":"code","source":"y_pred = best_model.predict(X_test)\n\nprint_score(y_pred, best_model)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c14aca676e8fe5e65beba7b40ce7d6295e8a4242"},"cell_type":"markdown","source":"**2.6 Confusion matrix**"},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"444d6f85d6c9c121c3ecf4654cb7a37e21974321"},"cell_type":"code","source":"for i in range(y_train.shape[1]):\n    print(multilabel_binarizer.classes_[i])\n    print(confusion_matrix(y_test[:,i], y_pred[:,i]))\n    print(\"\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5917ac43e2fc1dd4e09cbcd3eaf9d209860c371b"},"cell_type":"markdown","source":"**2.7 Exctracting feature importance**"},{"metadata":{"trusted":true,"_uuid":"cb0790a18efaf21740784c00df81289725a66598"},"cell_type":"code","source":"def print_top10(feature_names, clf, class_labels):\n    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n    for i, class_label in enumerate(class_labels):\n        top10 = np.argsort(clf.coef_[i])[-10:]\n        print(\"--------------------------------------------\")\n        print(\"%s: %s\" % (class_label,\n              \" \".join(feature_names[j] for j in top10)))\n        print(\"--------------------------------------------\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4445db4758034e793a38710a09779bfb9f447c5c"},"cell_type":"code","source":"feature_names = vectorizer_X1.get_feature_names() + vectorizer_X2.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b586230068efc6554c0c126a2a1aa5dbe33040e8"},"cell_type":"code","source":"print_top10(feature_names, best_model, multilabel_binarizer.classes_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12cc691fb0614e001b802a55f9955edddef6fb05"},"cell_type":"markdown","source":"**If you have any comment or improvement I'm all ears. **\n![](https://i.imgur.com/yO8v1sI.png)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}