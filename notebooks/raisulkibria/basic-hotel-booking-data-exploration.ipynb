{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Importing Libraries: ##\n\nAt first, all the necessary libraries are imported. These are:\n- **Matplotlib** is a comprehensive library for creating static, animated, and interactive visualizations in Python. Although there are other popular libraries like seaborn, I decided to stick with matplotlib for this basic project\n- **Numpy** provides a high-performance multidimensional array and basic tools to compute with and manipulate these arrays.\n- **Pandas** is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool. Used here for dataframe operations and visualization.\n- **Sklearn** is a machine learning library. Here, sklearns' support vector classifier, random forest classifier, MLP classifier and train_test_split has been used.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, the dataset has to be imported. The dataset used here is a hotel bookings dataset containing various information of different customers who booked either a resort or city hotel during 2015-17. The dataset is in .csv format and it is imported and stored as a pandas dataframe 'df'. The df.head() function returns the first few (default = 5) rows along with headers of the dataframe. It is very useful for a quick glimpse.","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/hotel-booking-demand/hotel_bookings.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A quick summary of the dataframe can also be viewed with the df.describe() function. Many attributes for each column is summarised for further information on the data. The most frequent entries in each column and other statiscal parameters like- mean, standard deviation etc is presented.\nNote the include = \"all\" parameter ensures that every column of the input will be included in the output","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"df.describe(include = 'all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To see which column contains which type of data, df.dtypes attribute is called. This attributes contain each column's datatype. In this dataset there are three types of data- int64, float64 and objects ","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset contains many empty cells or 'None' entries. These hinder further data analysis or classification. So these must be handeled as a data preprocessing step. To first see which column contains how many of these 'NA' values in total, df.isna().sum() is used.","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Four columns (children, country, agent, company) contain many 'NA' entries. These are handeled differently here-\n- 'NA' countries are replced with term 'NRF' (no record found)\n- agent and company 'NA' entries are simply replaced with 0\n- Number of childrens is calculated by taking the mean over the whole column\n<br>Another call to df.isna().sum() validates that no 'NA' entries remain.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['country']=df['country'].fillna('NRF')\ndf['agent'] = df['agent'].fillna(0.0)\ndf['company'] = df['company'].fillna(0.0)\ndf['children'] = df['children'].fillna(int(df['children'].mean()))\ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"df.corr() computes pairwise correlation of columns. So here calling the function returns a 32x32 matrix with the correlation of each pairwise correlation in their respected cell entry. <br>\n**Remember:** A positive correlation is a relationship between two variables in which both variables move in the same direction. Negative correlation is a relationship between two variables in which one variable increases as the other decreases and vice versa","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = df.corr()\ncorr_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just to see the column names again to determine which data to view and use for classification, df.columns attribute can be called. This returns all the column labels of the DataFrame. To see which column represents what information, the original paper can be sought [1].","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pandas also contains direct visualization methods, providing a wrapper around plt.plot(). On DataFrame, plot() is a convenience to plot all of the columns with labels. Here a Box plot over every column is viewed to find the outliers in each column. As there are many columns in the dataset, all the labels in the x-axis are overlapping. In such cases, unimportant columns can be excluded.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.plot(kind = 'box', figsize = (9, 5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Visualization ##\n\nData visualization is the graphical representation of information and data. It provides with a quick, clear understanding of the information. Thanks to graphic representations, we can visualize large volumes of data in an understandable and coherent way, which in turn helps us comprehend the information and draw conclusions and insights.<br>\nAs the first step, to look at how the total number of customer varries in each month for the years 2015,16 & 17, The value_counts() method can be used. The function returns the total frequency of cutomers in each year. sort_values() is used to sort the outcome values(for dataframe with multiple columns, the 'by' parameter, defining sorting by which column, must be specified) and to_dict() is used to get the value in python dictionary format.<br>\nA loop over each year is used and the dataframe containing arrival year and arrival month is grouped over these years by combining [groupby()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html?highlight=groupby#pandas.DataFrame.groupby) and [get_group()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.get_group.html?highlight=get_group#pandas.core.groupby.GroupBy.get_group). A lineplot of maxplotlib is used here 3 different lines representing 3 different years.","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12, 4))\nyear_freq = df['arrival_date_year'].value_counts().sort_values().to_dict()\nyear, freq = zip(*year_freq.items())\nytick = []\nfor i in range(len(year)):\n    month_f = df[['arrival_date_year', 'arrival_date_month']]\n    month_f = month_f.groupby(['arrival_date_year']).get_group(year[i])\n    month_f = pd.Categorical(month_f['arrival_date_month'], categories = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', \"December\"])\n    month_f = month_f.value_counts().to_dict()\n    month, m_freq = zip(*month_f.items())\n    ytick.append(m_freq)\n    plt.plot(month, m_freq)\nytick = np.array(ytick).reshape(3,12)\nfor j in range(3):\n    for i in range(12):\n        if ytick[j, i]!=0:\n            plt.text(i, ytick[j, i], str(ytick[j, i]))\n\nplt.legend(year)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the image, it can be seen how the number of cusyomers follow a similar pattern in the same months of different year. As no data for  early 2015 and late 2017 is present in the dataset the trend can not be exactly defined. However it is obvious that during April to June the number of bookings increase as well as for September-October.<br>\nNow to see the top 10 countries with most bookings in this dataset agaian the value_counts() function is used. But here a Barchart is used with plt.text() function to note down the exact number of bookings for each country.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"country_freq = df['country'].value_counts().iloc[0:10].to_dict()\ncountry, freq = zip(*country_freq.items()) \nplt.bar(country, freq)\nplt.xticks(country)\npeak = plt.plot(np.arange(10), freq, 'r.')\nfor i in range(len(freq)):\n    plt.text(i, freq[i]+1000, str(freq[i]))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To visualize how much a customer has to wait in the queue for a booking depending on their date of arrival, first a summation for a dataframe grouped by each date of month is computed to a dictionary. A horizontal barchart is used here with y axis representing each date of month. This data may not be very significant (eg- day 31).","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(5,6))\nwait = df[['days_in_waiting_list','arrival_date_day_of_month']]\nwait = wait.groupby(['arrival_date_day_of_month']).sum().to_dict()\nx, y = zip(*wait['days_in_waiting_list'].items())\nplt.barh(x, y)\nplt.yticks(x)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The frame contains infromation of customer bookings coming from different distribution channels. A pandas piechart is used here on a frame with 'distribution_channel' column with value_counts(). The chart conveniently describes how most bookings came from 'TA/TO' meaning “Travel Agents” and “Tour Operators”.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dist_chnl = df['distribution_channel'].value_counts()\ndist_chnl.iloc[:4].plot.pie(figsize=(5,5), fontsize =10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, if we want to see which country has the most cancellation rates, both country value counts and cancellation frequency is calculated. The rates are calculated by dividing the cancellations per country by total entries for that country. As there are entries for 177 different countries, to prevent overpopulating the diagram, rates less than 50% are excluded.","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(60,7))\nc = df[['country', 'is_canceled']].groupby('country').sum().sort_values(by='is_canceled', ascending=False).to_dict()\ncountry_freq = df['country'].value_counts().to_dict()\nfor _, (ctr, cnc) in enumerate(c['is_canceled'].items()):\n    for _, (ctr2, freq) in enumerate(country_freq.items()):\n        if (ctr == ctr2):\n            cnc = cnc/freq\n            if(cnc>.5):\n                plt.bar(ctr, cnc)\n                print(ctr, cnc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Classification: ##\n\nFor classification, columns with 'object' data types can not be used. In order to transform these columns into usable format, first we find all the columns with 'object' data type.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_type = df.dtypes.to_dict()\ncat = []\nfor _, (i, j) in enumerate(dt_type.items()):\n    if j == object:\n        cat.append(str(i))\nprint(cat, len(cat))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pandas provide a very useful tool for this transformation. Each target column is transformed into categorical data, then int type. Again checking the df.dtypes attribute shows that.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in cat:\n    df[col] = df[col].astype('category')\n    df[col] = df[col].cat.codes\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, if we want to determine whether or not a booking has chances of being canceled, we have to predict that from other attributes of the booking. Although feature selection is an important step to decide which attributes are most significant in such classification scenario, we have taken columns arbitrarily. The correlation matrix also could have been useful here. The Y data contains the target variable('is_canceled' column here).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df[['hotel', 'lead_time','arrival_date_year', 'arrival_date_month', 'arrival_date_week_number', 'arrival_date_day_of_month', 'adults', 'children', 'babies',\n       'country', 'is_repeated_guest', 'previous_cancellations', 'booking_changes', 'deposit_type', 'agent', 'company', 'days_in_waiting_list', 'customer_type']]\nY = df['is_canceled']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets see a glimpse of X data with head function and the shapes of both X and Y.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X.shape)\nprint(Y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**train_test_split** is a function in Sklearn model selection for splitting data arrays into two subsets: for training data and for testing data. With this function, we don't need to divide the dataset manually. By default, Sklearn train_test_split will make random partitions for the two subsets. Here 80% data is used for training and 20% for testing.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = .2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is always better to cross-check the shapes of traing and testing data. The number of columns for both X and Y must match for training and testing.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)\n\nprint(Y_train.shape)\nprint(Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First classifier we are going to use is Support vector classifier. The make_pipeline() function sequentially apply a list of transforms and a final estimator. <br>\nThe purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters.\n<br>StandardScaler() standardizes features by removing the mean and scaling to unit variance.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf1 = make_pipeline(StandardScaler(), SVC(gamma='auto'))\nclf1.fit(X_train, Y_train)\nscore1 = clf1.score(X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After defining the classifier the training data is fit into the classifier. the score() function on testing data returns a accuracy score for the classifier. The first classifier on this experiment scores in a 79.68% accuracy.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"score1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Multi-layer Perceptron classifier model is a neural network that optimizes the log-loss function using LBFGS or stochastic gradient descent. Similar to previous way, the classifier is trained and tested and accuracy is observed. MLP does better than SVC with ~80.96% accuracy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf2 =  make_pipeline(StandardScaler(), MLPClassifier(alpha=.001, max_iter=2000))\nclf2.fit(X_train, Y_train)\nscore2 = clf2.score(X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, a random forest classifier is used as the third classifer. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The default parameters has been used here.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf3 = RandomForestClassifier()\nclf3.fit(X_train, Y_train)\nscore3 = clf3.score(X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With 85.77% accuracy the Random forest classifier performs the best of the three in the presented scenario. So, this model can be used predict whether a booking will be cancelled approximately 86 out of 100 times correctly which can provide many important business decisions.<br>\nIn the conclusion, this was a very basic demonstration just to get some familiarity with basic matplotlib and pandas functions for data exploration. If there are any mistakes in the explanation or code, or some function that are presented complicatedly here, please do let me know [here](mailto:raisul.inc@gmail.com).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"References:<br>\n- [1] Antonio, Nuno, Ana de Almeida, and Luis Nunes. \"Hotel booking demand datasets.\" Data in brief 22 (2019): 41-49.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}