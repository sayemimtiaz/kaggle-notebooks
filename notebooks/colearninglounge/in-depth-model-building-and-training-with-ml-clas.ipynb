{"cells":[{"metadata":{},"cell_type":"markdown","source":"## NLP Workshop-2: Model building and Training\n\n\nAuthored by [abhilash1910](https://www.kaggle.com/abhilash1910)\n\n\n### Movie Reviews !!\n\nThe second and most interesting part of the curriculum is building different models - both statistical and deep learning so as to provide a proper classification model for our use case. In this case, we will create an initial baseline with statistical classification algorithms by using both non semantic vectors as well as semantic vectors. Later we will try to improvise on these baselines with standard neural models like Bidirectional LSTMs, Convolution Networks, Gated Recurrecnt Units. The improvements of these traditional neural models over the baselines would be further investigated when we will explore advanced architectures, particularly that of an encoder decoder . Further advancement would be made on attention based encoder-decoder modules like Transformers and using the different flavours from BERT to GPT.\n\nThe following is the workflow of this notebook:\n\n- Statistical Classifiers\n  - With Non Semantic TfIdf Baseline\n  - With Semantic Static Embeddings\n  - With Semantic Dynamic Embeddings \n  - With Transformer Embeddings\n  - Models: LR,SVM,NB,XGboost,DT,RF,LGBM,LDA,KNN\n  \n  \n- Traditional NN models\n  - With Static Embeddings\n  - With Dynamic Embeddings\n  - Models: LSTM,CNN,BiLSTM,GRU,Encoder-Decoders\n\n\n- Advanced Architectures\n  - Transformers\n  - Attention\n  - BERT TPU\n  - All BERT variants\n  - GPT TPU\n  - All variants of GPT2\n  - Hybrid Transformer\n  \n\nThis is an in depth approach to analyse the performance of different models on this task.\n\n<img src=\"https://lumiere-a.akamaihd.net/v1/images/eu_bpan_showcase_hero_v4_m_823e00f3.jpeg?region=0,0,750,668\">\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Recap from Workshop-1\n\nIn the previous [Notebook](https://www.kaggle.com/colearninglounge/nlp-end-to-end-cll-nlp-workshop), we learned about cleaning,EDA, vectorization and embeddings. We saw how cleaning played a significant role in determining knowledge embeddings from the corpus and also determined the similarity between words and sentences. \n\nWe will be following a few steps from the elaborate Notebook:\n\n- Use the cleaning methods (regex) for redundancy removal\n- Lemmatization\n- Vectorization\n- Embeddings (Static,Dynamic,Transformer)\n\nSince we have already implemented the Regex cleaning method, we can apply the same here. In the first section of this notebook, we will be running statistical classifiers, with 3 different vectorized data.\n\n- Non semantic TFIDF Vectorized Data\n- Static Embedding Vectorized Data\n- Dynamic Embedding Vectorized Data\n\nFor the first use case of statistical models, we will be relying on TFIDF Baseline with Statistical classifiers.\n\n\n<img src=\"https://i.pinimg.com/originals/b0/ec/e4/b0ece436f4244f1f97bab3facf4d6b8a.gif\">\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Load the set\ntrain_df=pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing,metrics,manifold\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV,cross_val_predict\nfrom imblearn.over_sampling import ADASYN,SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nimport collections\nimport matplotlib.patches as mpatches\nfrom sklearn.metrics import accuracy_score\n%matplotlib inline\nfrom sklearn.preprocessing import RobustScaler\nimport xgboost\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import classification_report,roc_auc_score,roc_curve,r2_score,recall_score,confusion_matrix,precision_recall_curve\nfrom collections import Counter\nfrom sklearn.model_selection import StratifiedKFold,KFold,StratifiedShuffleSplit\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD,SparsePCA\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom nltk.tokenize import word_tokenize\nfrom collections import defaultdict\nfrom collections import Counter\nimport seaborn as sns\nfrom wordcloud import WordCloud,STOPWORDS\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nimport xgboost\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import classification_report,roc_auc_score,roc_curve,r2_score,recall_score,confusion_matrix,precision_recall_curve\nfrom collections import Counter\nfrom sklearn.model_selection import StratifiedKFold,KFold,StratifiedShuffleSplit\nfrom xgboost import XGBClassifier as xg\nfrom lightgbm import LGBMClassifier as lg\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingClassifier,RandomForestClassifier,AdaBoostClassifier,BaggingClassifier,ExtraTreesClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#Convert the labels into integers (numerics) for reference.\n\ntrain_li=[]\nfor i in range(len(train_df)):\n    if (train_df['sentiment'][i]=='positive'):\n        train_li.append(1)\n    else:\n        train_li.append(0)\ntrain_df['Binary']=train_li\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#Running the Preprocessing and cleaning phase as well as the TFIDF Vectorization\n\nimport re\n#Removes Punctuations\ndef remove_punctuations(data):\n    punct_tag=re.compile(r'[^\\w\\s]')\n    data=punct_tag.sub(r'',data)\n    return data\n\n#Removes HTML syntaxes\ndef remove_html(data):\n    html_tag=re.compile(r'<.*?>')\n    data=html_tag.sub(r'',data)\n    return data\n\n#Removes URL data\ndef remove_url(data):\n    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\n\n#Removes Emojis\ndef remove_emoji(data):\n    emoji_clean= re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    data=emoji_clean.sub(r'',data)\n    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\n#Lemmatize the corpus\ndef lemma_traincorpus(data):\n    lemmatizer=WordNetLemmatizer()\n    out_data=\"\"\n    for words in data:\n        out_data+= lemmatizer.lemmatize(words)\n    return out_data\n\ndef tfidf(data):\n    tfidfv = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), lowercase=True, max_features=150000)\n    fit_data_tfidf=tfidfv.fit_transform(data)\n    return fit_data_tfidf\n\n\ntrain_df['review']=train_df['review'].apply(lambda z: remove_punctuations(z))\ntrain_df['review']=train_df['review'].apply(lambda z: remove_html(z))\ntrain_df['review']=train_df['review'].apply(lambda z: remove_url(z))\ntrain_df['review']=train_df['review'].apply(lambda z: remove_emoji(z))\ncount_good=train_df[train_df['sentiment']=='positive']\ncount_bad=train_df[train_df['sentiment']=='negative']\ntrain_df['review']=train_df['review'].apply(lambda z: lemma_traincorpus(z))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Revisiting TFIDF for our Baseline\n\nTFIDF vectorization is non semantic frequency based algorithm which uses a logarithmic distribution over document frequencies to embed vectors based on normalized frequency of occurence of words in the corpus. A descriptive formulation is provided here:\n\n<img src=https://plumbr.io/app/uploads/2016/06/tf-idf.png>\n\n\nThe logical inference for using TFIDF vectorization over other vectorization strategies to embed vectors in HD space is to capture rare words occuring across the corpus. This vectorized embeddings can be applied on a statistical model for training.\n\n<img src=\"https://cdn-images-1.medium.com/max/876/1*_OsV8gO2cjy9qcFhrtCdiw.jpeg\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#TFIDF Vectorize the Data\n\ntrain_set=tfidf(train_df['review'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Statistical Training Without Balancing\n\nWe have heard of balancing techniques in the previous [Notebook](https://www.kaggle.com/colearninglounge/nlp-end-to-end-cll-nlp-workshop), and approaches like [SMOTE](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/), [Adasyn](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.ADASYN.html),are used to balance the classes. In the first case, we will not be balancing the set and evaluate the preliminary training of the statistical models.This initial benchmark can be used for further improvement by balancing the dataset.\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Splitting the set\n\n\nOriginally we have the transformed /vectorized data as we saw in previous notebook, the TSNE transformation of which looked like :\n\n<img src=\"https://www.kaggleusercontent.com/kf/48903343/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..jgD4rWmavXEjaRs2D5hj1A.oiGPZCbEUh-SE708RH5SVGYnriaNgG8ZQr-jCjhIhA_NfrLPNRkn5yemLGwtV2_YHBt5K5pxw5oS3IlczfFEFbPHuJVfiSXazjeucA_MICTf8t7GQ0Qp8Eb-2uR2vchwySVuto8Sox5FOwbWswurk-VZKPIOn4whx2pUdeGIe2uEVEd6LcklBb_J0OlYGjytC-4Qh32eyWE1q5KrXIktcUXPDeuChQQYOvKiWJlK_Sz6mLz4bW4ZuYvgcOtmOc8IMx_muTyVbmd5RlBycC4cdme6Q5qHVS7SR-2eM9FLxKpyCtnor5sdDCSVZ749-eylO0KQ2xKX_KPUnYAXKVwuHPalAUNcWylpjR67Q_SxVckm5qzT1mI_iBUh4fKqe0Fq4QyoQt8E1ulug_AdE9UhyGENgn2AYa2WyueUKXPXc3xZSrWvI7AwPmvy9vV5Bh96qDod5vOYtkKHofOzAMXMjvB4JgGyYQoN4l39XKwu99RZGN0V8nUmugorm_kSxvKrqeZDGXNiU0OZZo6XXbO9NGtG5XU8gcZ2DzGqonuI8p0sROGP_nhybo28Z-MXkTelqS_ZSMkEbNN-2uJI0EsZACwPBj0xRRVYc-lkqcGkklAbWUkJO-pNYozWTED9uqPjNuJocY1DXFPJZX-4eHxliWUCV5-9zRrYekkx7zXZ2ds.I4RU-iNAzcSSEj2dlQ6ZnA/__results___files/__results___50_0.png\">\n\n\nIn generic supervised learning, we are implying model training after splitting the tfidf vectorized data. This transformation is linear and just splits depending on the ratio/test size provided by us. For this strategy, we will be using the sklearn split module.There are many versions of splitting the data based on the data, and some of them include:\n\n- [Test Train Split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n- [Stratified Split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n- [Stratified Shuffle Split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html)\n- [Shuffle Split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html)\n- [Stratified K Fold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)\n\nJason's [blog](https://machinelearningmastery.com/train-test-split-for-evaluating-machine-learning-algorithms/) provides a good idea about splitting techniques for generic classification problems."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#Normal Train Test Split\n\ntrain_y=train_df['sentiment']\ntrain_x,test_x,train_y,test_y=train_test_split(train_set,train_y,test_size=0.2,random_state=42)\ntrain_x.shape,train_y.shape,test_x.shape,test_y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting in Imbalanced Classes\n\nThere are several ways to strategize and split imbalanced classes. One of the ways is to use the \"stratify\" (Stratified Split) option during train_test_split. this splitting allows propertional splitting between the  classes, and also maintains the proportionality on the training and testing dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#Stratified Train Test Split\n\ntrain_stratify_y=train_df['sentiment']\ntrain_stratified_x,test_stratified_x,train_stratified_y,test_stratified_y=train_test_split(train_set,train_stratify_y,test_size=0.2,random_state=42,stratify=train_stratify_y)\ntrain_stratified_x.shape,train_stratified_y.shape,test_stratified_x.shape,test_stratified_y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysing TFIDF-LR Baseline with simple split \n\nIn this case, we want to evaluate the performance of a Logistic Regression classifier on the tfidf vectorized data sampled with normal train_test_split. Logistic Regression Classifier uses a sigmoid kernel for training. In a supervised learning mode , Logistic Regression is one of the standardised models under generalized linear models which tries a convex optimization by passing the cost function through the sigmoid kernel. The sigmoid function is denoted by the formulation:\n\n<img src=\"https://www.gstatic.com/education/formulas2/-1/en/sigmoid_function.svg\">\n\n\nThis equation due to its convergence property (+/- infinity) and due to its differentiability , the sigmoid kernel allows clamping of predicted values to binary labels. The sigmoid curve actually has optima at x=0 and x=1.Now in the case of supervised logistic regression, when we try to optimize the cost function (in this case a linear sum of weights & biases passed through sigmoid kernel), by applying stochastic gradient descent. Since by gradient descent, the steepest slope is considered, the change in derivatives (gradients) at each stage is computed and the weights of the cost function are updated. The effective loss function for logistic regression is E=(|y_predicted -y_actual|^2). This [blog](https://machinelearningmastery.com/gradient-descent-for-machine-learning/) provides an idea. \n\n\n\n<img src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--xoKf0Xfi--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn-images-1.medium.com/max/2000/1%2AXisqJi764DTxragiRFexSQ.png\">\n\nSome resources:\n\n- [Blog](https://www.analyticsvidhya.com/blog/2020/10/how-does-the-gradient-descent-algorithm-work-in-machine-learning/)\n- [Sklearn documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n- [KDNuggets](https://www.kdnuggets.com/2017/04/simple-understand-gradient-descent-algorithm.html)\n- [Blog](https://towardsdatascience.com/machine-learning-101-an-intuitive-introduction-to-gradient-descent-366b77b52645)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying Logistic Regression on split tfidf baseline\nmodel=LogisticRegression()\nmodel.fit(train_x,train_y)\npred=model.predict(test_x)\nprint(\"Evaluate confusion matrix for LR\")\nprint(confusion_matrix(test_y,pred))\nprint(f\"Accuracy Score for LR with C=1.0  ={accuracy_score(test_y,pred)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MultiNomial Naive Bayes on TFIDF Baseline\n\n[MultiNomial NB](https://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/modules/generated/sklearn.naive_bayes.MultinomialNB.html) is a probabilistic statistical classification model which uses conditional probability to segregate or classify samples. This works well with discrete integer valued features (such as count vectorization) but can also be used with TFIDF vectors. Particularly, this uses the Bayes Theorem which tries to determine conditional probability using prior and posterior probabilities as shown in the figure:\n\n<img src=\"https://storage.googleapis.com/coderzcolumn/static/tutorials/machine_learning/article_image/Scikit-Learn%20-%20Naive%20Bayes.jpg\">\n\nThe major concept under this category is statistics of [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html). There are many variants:\n\n- Gaussian NB which relies on Gaussian distribution of the input features\n\n<img src=\"https://www.researchgate.net/profile/Yune_Lee/publication/255695722/figure/fig1/AS:297967207632900@1448052327024/Illustration-of-how-a-Gaussian-Naive-Bayes-GNB-classifier-works-For-each-data-point.png\">\n\n\n- Complement NB which is suited for unbalanced classes and relies on the statistics of complement of each class to generate the weights. It is better than Multinomial NB for textual classification as it has a normalization factor (and a smoothing hyperparameter alpha) which tends to capture information from longer sequences of text.\n\n\n<img src=\"https://www.researchgate.net/profile/Motaz_Saad/publication/231521157/figure/fig7/AS:667829850345476@1536234452166/Figure-31-Complement-Naive-Bayes-Algorithm-72.png\">\n\n- Bernoulli NB which relies on multivariate bernoulli distributions of the input features and also expects the data to be in binary format.\n\n<img src=\"https://www.astroml.org/_images/fig_simple_naivebayes_1.png\">\n\nOther variants include:\n\n- Categorical NB\n- Partial Fit of NB models\n\nResources:\n\n- [Blog](https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/)\n- [Kernel](https://www.kaggle.com/abhilash1910/nlp-workshop-ml-india#Vectorization-and-Benchmarking)\n- [Jason's Blog](https://machinelearningmastery.com/classification-as-conditional-probability-and-the-naive-bayes-algorithm/)\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying MultiNomial Naive Bayes on split tfidf baseline\nmodel=MultinomialNB()\nmodel.fit(train_x,train_y)\npred=model.predict(test_x)\nprint(\"Evaluate confusion matrix for NB\")\nprint(confusion_matrix(test_y,pred))\nprint(f\"Accuracy Score for NB ={accuracy_score(test_y,pred)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multiple Baseline computation using KFold and Cross Validation\n\n\nIn this concept, we will be training multiple statistical models based on KFold and Cross Validation Technique.\nKFold cross validators provide train/test split indices for splitting the dataset into 'k' folds without shuffling. The general methodology for using Kfold and Cross Validation is provided below:\n\n- Shuffle the dataset randomly.\n- Split the dataset into k groups\n- For each unique group:\n   - Take the group as a hold out or test data set\n   - Take the remaining groups as a training data set\n   - Fit a model on the training set and evaluate it on the test set\n   - Retain the evaluation score and discard the model\n- Summarize the skill of the model using the sample of model evaluation scores\n\nThis technique has a following rule: The first ``` n_samples % n_splits``` folds have size ``` n_samples // n_splits + 1```, other folds have size ``` n_samples // n_splits```, where n_samples is the number of samples.\n\nA typical flowchart of cross validation is provided below:\n\n<img src=\"https://scikit-learn.org/stable/_images/grid_search_workflow.png\">\n\nThis allows for better hyperparameter search using GridSearch CV algorithms which will be covered later.\nThe following procedure is followed for each of the k “folds”:\n\n- A model is trained using  of the folds as training data;\n\n- The resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n\nThe performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.\n\n<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\">\n\nSome resources:\n\n- [Blog](https://machinelearningmastery.com/k-fold-cross-validation/)\n- [Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)\n- [Documentation](https://scikit-learn.org/stable/modules/cross_validation.html)\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Brief Introduction of Statistical Models\n\n\nThis is going to be a brief introduction of different statistical models which will be used simultaeneously with k fold and cross validation techniques for examining the accuracy of the models. In this case, we will be focussing on accuracy as the major KPI and later we will be running on different observations such as f1,ROC etc.\n\n\n### Decision Trees\n\n[Decision Trees](https://scikit-learn.org/stable/modules/tree.html) is a supervised model for classification/regression. This works on creating decision branches which evolves a criteria and is often acknowledged as a simplistic classification (white box) model as the stages of decision can be easily derived. A regression tree appears as follows:\n\n<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_tree_regression_0011.png\">\n\nThe [algorithms](https://scikit-learn.org/stable/modules/tree.html) include ID3,C4.5/C5.0,CART which can be analysed as follows:\n\n- ID3(Iterative Dichotomiser 3) was developed in 1986 by Ross Quinlan. The algorithm creates a multiway tree, finding for each node (i.e. in a greedy manner) the categorical feature that will yield the largest information gain for categorical targets. Trees are grown to their maximum size and then a pruning step is usually applied to improve the ability of the tree to generalise to unseen data.\n\n- C4.5 is the successor to ID3 and removed the restriction that features must be categorical by dynamically defining a discrete attribute (based on numerical variables) that partitions the continuous attribute value into a discrete set of intervals. C4.5 converts the trained trees (i.e. the output of the ID3 algorithm) into sets of if-then rules. These accuracy of each rule is then evaluated to determine the order in which they should be applied. Pruning is done by removing a rule’s precondition if the accuracy of the rule improves without it.\n\n- C5.0 is Quinlan’s latest version release under a proprietary license. It uses less memory and builds smaller rulesets than C4.5 while being more accurate.\n\n- CART (Classification and Regression Trees) is very similar to C4.5, but it differs in that it supports numerical target variables (regression) and does not compute rule sets. CART constructs binary trees using the feature and threshold that yield the largest information gain at each node.\n\n\nIn general, the major logics involved in Decision Trees involves computation of Entropy or Gini Index, which is as follows:\n\n<img src=\"https://qph.fs.quoracdn.net/main-qimg-690a5cee77c5927cade25f26d1e53e77\">\n\nTypically a Gini Coefficient  is evaluated as the area between the ```y=x``` line and Lorentz curve\n\n<img src=\"https://i.stack.imgur.com/iawuF.jpg\">\n\n\nMisclassification is another criteria:\n\n<img src=\"https://miro.medium.com/max/2180/1*O5eXoV-SePhZ30AbCikXHw.png\">\n\nTypically a decision tree appears as follows:\n\n<img src=\"https://scikit-learn.org/stable/_images/iris.png\">\n\nSome resources:\n\n- [Blog](https://towardsdatascience.com/scikit-learn-decision-trees-explained-803f3812290d)\n- [Blog](https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/)\n- [Blog](https://machinelearningmastery.com/cost-sensitive-decision-trees-for-imbalanced-classification/)\n\n\n\n### Random Forests \n\n\n[Random Forests](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)  is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the ```max_samples``` parameter ```if bootstrap=True (default)```, otherwise the whole dataset is used to build each tree. When splitting each node during the construction of a tree, the best split is found either from all input features or a random subset of size ```max_features```. (See the parameter tuning guidelines for more details).The purpose of these two sources of randomness is to decrease the variance of the forest estimator. Indeed, individual decision trees typically exhibit high variance and tend to overfit. The injected randomness in forests yield decision trees with somewhat decoupled prediction errors. By taking an average of those predictions, some errors can cancel out. Random forests achieve a reduced variance by combining diverse trees, sometimes at the cost of a slight increase in bias. In practice the variance reduction is often significant hence yielding an overall better model.\n\n<img src=\"https://www.researchgate.net/profile/Hung_Cao12/publication/333438248/figure/fig6/AS:763710377299970@1559094151459/Random-Forest-model-with-majority-voting.ppm\">\n\n\n### Gradient Boosting Forests and Trees\n\n[Gradient Boosting](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/) is a central part of ensemble modelling in sklearn.\n\nThe goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.\n\nTwo families of ensemble methods are usually distinguished:\n\n- In averaging methods, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.\n\n  Examples: Bagging methods, Forests of randomized trees\n\n- By contrast, in boosting methods, base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.\n\n  Examples: AdaBoost, Gradient Tree Boosting\n\n\nPictorially these can be represented as :\n\n<img src=\"https://miro.medium.com/max/3908/1*FoOt85zXNCaNFzpEj7ucuA.png\">\n\n\nSeveral Boosting Models can be found under this criteria:\n\n#### [AdaBoosting](https://blog.paperspace.com/adaboost-optimizer/#:~:text=AdaBoost%20is%20an%20ensemble%20learning,turn%20them%20into%20strong%20ones.): \nThe core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consist of applying weights  to each of the training samples. Initially, those weights are all set to (1/N), so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence\n\n<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_adaboost_hastie_10_2_0011.png\">\n\n\n### [LightGBM](https://lightgbm.readthedocs.io/en/latest/)\n\nLight GBM is another gradient boosting strategy which relies on trees.It has the following advantages:\n\n- Faster training speed and higher efficiency.\n\n- Lower memory usage.\n\n- Better accuracy.\n\n- Support of parallel and GPU learning.\n\n- Capable of handling large-scale data.\n\nLightGBM grows leaf-best wise and will choose the leaf with maximum max delta loss to grow.\n\n<img src=\"https://lightgbm.readthedocs.io/en/latest/_images/leaf-wise.png\">\n\n\nSome resources:\n\n- [XGB](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)\n- [Blogs](https://www.analyticsvidhya.com/blog/tag/gradient-boosting/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#KFold and cross validation on tfidf baseline\nmodels=[]\nmodels.append(('LogisticRregression',LogisticRegression(C=1.0,penalty='l2')))\nmodels.append(('KNearestNeighbors',KNeighborsClassifier()))\nmodels.append(('DecisionTree',DecisionTreeClassifier(criterion='entropy')))\n#models.append(('RandomForestRegressor',RandomForestRegressor(n_estimators = 1000, random_state = 42)))\n# models.append(('RandomForestClassifier',RandomForestClassifier(n_estimators = 1000, criterion='gini')))\nmodels.append(('GradientBoostClassifier',GradientBoostingClassifier(learning_rate=1e-2, loss='deviance',n_estimators=100)))\nmodels.append(('AdaBoostClassifier',AdaBoostClassifier(learning_rate=1e-2,algorithm='SAMME.R',n_estimators=100)))\nmodels.append(('ExtraTreesClassifier',ExtraTreesClassifier(n_estimators=10, max_depth=None,min_samples_split=2)))\nmodels.append(('BagClassifier',BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.5)))\n# models.append(('HistGradientBoostClassifier',HistGradientBoostingClassifier(max_iter=100)))\n# models.append(('SupportVectorClassifier',SVC(C=1.0,kernel='sigmoid')))\nmodel_result=[]\nscoring='accuracy'\nprint(\"Statistical Model TFIDF- Baseline Evaluation\")\nfor name,model in models:\n    kfold=KFold(n_splits=10,random_state=7)\n    results=cross_val_score(model,train_x,train_y,cv=kfold)\n    print(\"=======================\")\n    print(\"Classifiers: \",name, \"Has a training score of\", round(results.mean(), 2) * 100, \"% accuracy score\")\n    model_result.append(results.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Applying SMOTE balancing on TFIDF Baseline\n\nHere, we will be applying SMOTE on TFIDF vectorized data for creating a different baseline.Since in our case, the data is balanced, applying SMOTE on balanced data would only reduce the efficiency of the models.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Balancing The Sampple for TFIDF Baseline\n#SMOTE oversampling\nsmote=SMOTE(random_state=42,k_neighbors=2)\nsmote_train_x,smote_train_y=smote.fit_sample(train_x,train_y)\nsmote_train_x.shape,smote_train_y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#Applying SMOTE TFIDF Balanced Baseline with KFold\n\nmodels=[]\nmodels.append(('LogisticRregression',LogisticRegression(C=1.0,penalty='l2')))\nmodels.append(('KNearestNeighbors',KNeighborsClassifier()))\nmodels.append(('DecisionTree',DecisionTreeClassifier(criterion='entropy')))\n#models.append(('RandomForestRegressor',RandomForestRegressor(n_estimators = 1000, random_state = 42)))\n#models.append(('RandomForestClassifier',RandomForestClassifier(n_estimators = 1000, criterion='gini')))\nmodels.append(('GradientBoostClassifier',GradientBoostingClassifier(learning_rate=1e-2, loss='deviance',n_estimators=100)))\nmodels.append(('AdaBoostClassifier',AdaBoostClassifier(learning_rate=1e-2,algorithm='SAMME.R',n_estimators=100)))\nmodels.append(('ExtraTreesClassifier',ExtraTreesClassifier(n_estimators=10, max_depth=None,min_samples_split=2)))\nmodels.append(('BagClassifier',BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.5)))\n# models.append(('HistGradientBoostClassifier',HistGradientBoostingClassifier(max_iter=100)))\n# models.append(('SupportVectorClassifier',SVC(C=1.0,kernel='sigmoid')))\nmodel_training_result,model_validation_result=[],[]\nscoring='accuracy'\nprint(\"Statistical Model SMOTE TFIDF- Baseline Evaluation\")\nfor name,model in models:\n    kfold=KFold(n_splits=10,random_state=7)\n    results=cross_val_score(model,smote_train_x,smote_train_y,cv=kfold)\n    print(\"=======================\")\n    print(\"Classifiers: \",name, \"Has a training score of\", round(results.mean(), 2) * 100, \"% accuracy score\")\n    predictions=cross_val_predict(model,test_x,test_y)\n    accuracy = accuracy_score(predictions,test_y)\n    model_training_result.append(results.mean())\n    model_validation_result.append(accuracy)\n\nfinal_outcomes=pd.DataFrame(columns=['Model','Training Acc','Validation Acc'])\nfinal_outcomes['Model']=models\nfinal_outcomes['Training Acc']=model_training_result\nfinal_outcomes['Validation Acc']=model_validation_result\nfinal_outcomes.to_csv('TFIDF-SMOTE-Baseline.csv',index=False)\nfinal_outcomes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Concluding Non Semantic Baseline Techniques\n\n\nWe have seen the effect of applying a statistical classifier on the basis of non semantic TFIDF vectorized data and also attained a parallel analysis of the accuracy of the different algorithms. The inference for using these statistical models is that it provides an initial benchmark which has to be improved further by trying different models as such. This provides a quick overview of how  a traditional classifier can be used for non semantic classification and in the next case we will be using semantic embeddings (vectors) with these traditional classifiers.\n\n\n<img src=\"https://media2.giphy.com/media/118u58QrLaLnDG/giphy.gif\">"},{"metadata":{},"cell_type":"markdown","source":"## Static Semantic Embedding Baseline\n\n\nIn this context, we will be applying the traditional ML classifiers on Word2Vec, Glove and Fasttext data to create a classification model. We have already realised the concept of these embeddings and some of the images of applying these embeddings on the corpus is provided in the Notebook [here](https://www.kaggle.com/colearninglounge/nlp-end-to-end-cll-nlp-workshop):\n\n### Word2Vec From [Gensim](https://www.analyticsvidhya.com/blog/tag/word2vec/)\n\n<img src=\"https://www.kaggleusercontent.com/kf/48903343/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..C4q3IW8yHfXenn6fNzGSXQ.eHWK4_UfbdG6wh4wTDUBKfapys5hjPEyZqNdH-szO_domUFCC5uIpgEqg6IWleffop3sIT6lPyKsKPaMMNxUR9GtqbkZroidkwVVGL_4hdybmE0o6-yCbbAGxJfO44uwvSIc8ak_QqhFLLxhANMRRceiSBn7jK4mK9iUhnx4EBXhj9JQfuwlHDlCyWrL9FtyQV0a_iY0yRpJ39EvAjGAwYSnwk2FqjADkptTUaKA3liDy_ZohvtUGXZh4BEX0SCLLXpfKkleqq5sTeTLMU_h-AHH2z8AyVXFpSTMVAmXh2urgGjl1BbQjyf1fhxETZFj1eoCnpFddvNuK8hrIqdvuDyaybFnb_MTFScC3104AWu7sI7ke3-7fUFc0dGSzBzY2RE1s17MdaepXCNmDuBr40Yd2O4fN9VziTSgAZUKEhnbJe1C_MZtuYkUMZY7kRjiHstQQrS4lVYnALNlFlqzDy61-dl7MMgQlcK-EePn6lK4umB94lq1EC-AFLzFaoMeX4iC6z8LUPcSaQjjlkuOHBWpdfj3fYCQ_uaXeD7-vKuqTr-c1EAhrdXJjgOCzQjLmlvw1oisiTPpVDSjl2P4J6fL8WxEG4TmiBIHhQJywYr0lMN2kZdLDa3U98KtbFf5ed4_7upTP_IzQ2g8VzKvZa8W-9qRGFWLoW1hehO0DxQ.dFpEskCVbLZ4qvYaycKJcg/__results___files/__results___63_1.png\">\n\n<img src=\"https://www.kaggleusercontent.com/kf/48903343/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..C4q3IW8yHfXenn6fNzGSXQ.eHWK4_UfbdG6wh4wTDUBKfapys5hjPEyZqNdH-szO_domUFCC5uIpgEqg6IWleffop3sIT6lPyKsKPaMMNxUR9GtqbkZroidkwVVGL_4hdybmE0o6-yCbbAGxJfO44uwvSIc8ak_QqhFLLxhANMRRceiSBn7jK4mK9iUhnx4EBXhj9JQfuwlHDlCyWrL9FtyQV0a_iY0yRpJ39EvAjGAwYSnwk2FqjADkptTUaKA3liDy_ZohvtUGXZh4BEX0SCLLXpfKkleqq5sTeTLMU_h-AHH2z8AyVXFpSTMVAmXh2urgGjl1BbQjyf1fhxETZFj1eoCnpFddvNuK8hrIqdvuDyaybFnb_MTFScC3104AWu7sI7ke3-7fUFc0dGSzBzY2RE1s17MdaepXCNmDuBr40Yd2O4fN9VziTSgAZUKEhnbJe1C_MZtuYkUMZY7kRjiHstQQrS4lVYnALNlFlqzDy61-dl7MMgQlcK-EePn6lK4umB94lq1EC-AFLzFaoMeX4iC6z8LUPcSaQjjlkuOHBWpdfj3fYCQ_uaXeD7-vKuqTr-c1EAhrdXJjgOCzQjLmlvw1oisiTPpVDSjl2P4J6fL8WxEG4TmiBIHhQJywYr0lMN2kZdLDa3U98KtbFf5ed4_7upTP_IzQ2g8VzKvZa8W-9qRGFWLoW1hehO0DxQ.dFpEskCVbLZ4qvYaycKJcg/__results___files/__results___59_1.png\">\n\n### Google News Vectors Variant of [Word2Vec](https://code.google.com/archive/p/word2vec/)\n\n\n<img src=\"https://www.kaggleusercontent.com/kf/48903343/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..C4q3IW8yHfXenn6fNzGSXQ.eHWK4_UfbdG6wh4wTDUBKfapys5hjPEyZqNdH-szO_domUFCC5uIpgEqg6IWleffop3sIT6lPyKsKPaMMNxUR9GtqbkZroidkwVVGL_4hdybmE0o6-yCbbAGxJfO44uwvSIc8ak_QqhFLLxhANMRRceiSBn7jK4mK9iUhnx4EBXhj9JQfuwlHDlCyWrL9FtyQV0a_iY0yRpJ39EvAjGAwYSnwk2FqjADkptTUaKA3liDy_ZohvtUGXZh4BEX0SCLLXpfKkleqq5sTeTLMU_h-AHH2z8AyVXFpSTMVAmXh2urgGjl1BbQjyf1fhxETZFj1eoCnpFddvNuK8hrIqdvuDyaybFnb_MTFScC3104AWu7sI7ke3-7fUFc0dGSzBzY2RE1s17MdaepXCNmDuBr40Yd2O4fN9VziTSgAZUKEhnbJe1C_MZtuYkUMZY7kRjiHstQQrS4lVYnALNlFlqzDy61-dl7MMgQlcK-EePn6lK4umB94lq1EC-AFLzFaoMeX4iC6z8LUPcSaQjjlkuOHBWpdfj3fYCQ_uaXeD7-vKuqTr-c1EAhrdXJjgOCzQjLmlvw1oisiTPpVDSjl2P4J6fL8WxEG4TmiBIHhQJywYr0lMN2kZdLDa3U98KtbFf5ed4_7upTP_IzQ2g8VzKvZa8W-9qRGFWLoW1hehO0DxQ.dFpEskCVbLZ4qvYaycKJcg/__results___files/__results___66_0.png\">\n\n### [Glove](https://www.analyticsvidhya.com/blog/tag/glove/) [Embeddings](https://nlp.stanford.edu/projects/glove/)\n\n<img src=\"https://www.kaggleusercontent.com/kf/48903343/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..C4q3IW8yHfXenn6fNzGSXQ.eHWK4_UfbdG6wh4wTDUBKfapys5hjPEyZqNdH-szO_domUFCC5uIpgEqg6IWleffop3sIT6lPyKsKPaMMNxUR9GtqbkZroidkwVVGL_4hdybmE0o6-yCbbAGxJfO44uwvSIc8ak_QqhFLLxhANMRRceiSBn7jK4mK9iUhnx4EBXhj9JQfuwlHDlCyWrL9FtyQV0a_iY0yRpJ39EvAjGAwYSnwk2FqjADkptTUaKA3liDy_ZohvtUGXZh4BEX0SCLLXpfKkleqq5sTeTLMU_h-AHH2z8AyVXFpSTMVAmXh2urgGjl1BbQjyf1fhxETZFj1eoCnpFddvNuK8hrIqdvuDyaybFnb_MTFScC3104AWu7sI7ke3-7fUFc0dGSzBzY2RE1s17MdaepXCNmDuBr40Yd2O4fN9VziTSgAZUKEhnbJe1C_MZtuYkUMZY7kRjiHstQQrS4lVYnALNlFlqzDy61-dl7MMgQlcK-EePn6lK4umB94lq1EC-AFLzFaoMeX4iC6z8LUPcSaQjjlkuOHBWpdfj3fYCQ_uaXeD7-vKuqTr-c1EAhrdXJjgOCzQjLmlvw1oisiTPpVDSjl2P4J6fL8WxEG4TmiBIHhQJywYr0lMN2kZdLDa3U98KtbFf5ed4_7upTP_IzQ2g8VzKvZa8W-9qRGFWLoW1hehO0DxQ.dFpEskCVbLZ4qvYaycKJcg/__results___files/__results___70_0.png\">\n\n### [Fasttext Embeddings](https://fasttext.cc/docs/en/supervised-tutorial.html)\n\n<img src=\"https://www.kaggleusercontent.com/kf/48903343/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..C4q3IW8yHfXenn6fNzGSXQ.eHWK4_UfbdG6wh4wTDUBKfapys5hjPEyZqNdH-szO_domUFCC5uIpgEqg6IWleffop3sIT6lPyKsKPaMMNxUR9GtqbkZroidkwVVGL_4hdybmE0o6-yCbbAGxJfO44uwvSIc8ak_QqhFLLxhANMRRceiSBn7jK4mK9iUhnx4EBXhj9JQfuwlHDlCyWrL9FtyQV0a_iY0yRpJ39EvAjGAwYSnwk2FqjADkptTUaKA3liDy_ZohvtUGXZh4BEX0SCLLXpfKkleqq5sTeTLMU_h-AHH2z8AyVXFpSTMVAmXh2urgGjl1BbQjyf1fhxETZFj1eoCnpFddvNuK8hrIqdvuDyaybFnb_MTFScC3104AWu7sI7ke3-7fUFc0dGSzBzY2RE1s17MdaepXCNmDuBr40Yd2O4fN9VziTSgAZUKEhnbJe1C_MZtuYkUMZY7kRjiHstQQrS4lVYnALNlFlqzDy61-dl7MMgQlcK-EePn6lK4umB94lq1EC-AFLzFaoMeX4iC6z8LUPcSaQjjlkuOHBWpdfj3fYCQ_uaXeD7-vKuqTr-c1EAhrdXJjgOCzQjLmlvw1oisiTPpVDSjl2P4J6fL8WxEG4TmiBIHhQJywYr0lMN2kZdLDa3U98KtbFf5ed4_7upTP_IzQ2g8VzKvZa8W-9qRGFWLoW1hehO0DxQ.dFpEskCVbLZ4qvYaycKJcg/__results___files/__results___73_0.png\">\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n## Load word2vec algorithm from gensim and vectorize the words\nfrom gensim.models import Word2Vec,KeyedVectors\ncheck_df=list(train_df['review'].str.split())\nmodel=Word2Vec(check_df,min_count=1,iter=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Label Encode the labels\nfrom sklearn.preprocessing import LabelEncoder\nlabel_y= LabelEncoder()\nlabels=label_y.fit_transform(train_df['sentiment'])\nlabels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating Sentence Vectors from Word2Vec\n\nSince Word2Vec creates vector embeddings for individual words in a corpus by transforming them to a manifold, we need effective document /sentence vectors from these individual vectorized words.  The concept of [pooling](https://medium.com/technologymadeeasy/the-best-explanation-of-convolutional-neural-networks-on-the-internet-fbb8b1ad5df8#:~:text=A%20pooling%20layer%20is%20another,in%20pooling%20is%20max%20pooling) is derived from Neural Networks particularly [Convolution Neural Network Architectures](https://analyticsindiamag.com/max-pooling-in-convolutional-neural-network-and-its-features/) where MaxPooling signifies taking the maximum from a range (particularly a kernel/filter or a window of input features). A typical Maxpooling diagram is as follows:\n\n\n<img src=\"https://i.redd.it/61tcfy2xy2u41.png\">\n\n\n\nBut in the case of creating document embeddings, a general notion is to use [Average pooling](https://i.redd.it/61tcfy2xy2u41.png). Mean pooling is generally used to create document vectors by taking the average of all the vectors in the context. A schematic diagram of the same is provided:\n\n\n\n<img src=\"https://yashuseth.files.wordpress.com/2018/08/5.jpg?w=834\">\n\n\nAs we move forward towards using complex embeddings, we will be using Mean Pooling to create sentence/paragraph vectors from the individual word vectors. There are also other strategies involving Max Pooling and then applying Mean Pooling on the word Vectors to create complete vectors.\n\n<img src=\"https://www.researchgate.net/profile/Xingsheng_Yuan/publication/332810604/figure/fig2/AS:754128875683841@1556809743129/Simple-word-embedding-based-model-with-modified-hierarchical-pooling-strategy.png\">\n\n\nSome resources\n\n- [Research](https://www.researchgate.net/figure/Simple-word-embedding-based-model-with-modified-hierarchical-pooling-strategy_fig2_332810604)\n- [Some paper](https://www.cs.tau.ac.il/~wolf/papers/qagg.pdf)\n- [Huggingface](https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convert word vectors to sentence vectors/sentence vectors and apply mean pooling\n\ndef convert_sentence(data):\n    vocab=[w for w in data if w in model.wv.vocab]\n    avg_pool=np.mean(model[vocab],axis=0)\n#     sum_pool=np.sum(model[vocab],axis=0)\n#     min_pool=np.min(model[vocab],axis=0)\n#     max_pool=np.max(model[vocab],axis=0)\n    return avg_pool\n\ntrain_df['Vectorized_Reviews']=train_df['review'].apply(convert_sentence)\n\n#Split the dataset into training and testing sets\ntrain_y=train_df['sentiment']\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['Vectorized_Reviews'],train_y,test_size=0.2,random_state=42)\ntrain_x.shape,train_y.shape,test_x.shape,test_y.shape\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Convert the sentence vectors to List\n\nThis is done to ensure the dimensionality of the input sentence vectors is that of an array (list). This can be easily fed into any statistical classifier for our use case.\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_x=list(test_x)\ntrain_x=list(train_x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Apply Statistical Models on Static Embeddings\n\nNow we move forward to apply the statsitical models on the Compressed Sentence Vectors . This will allow us to apply gradient boosting algorithms on the static embeddings computed by taking the mean of the word vectors.We have also compressed the train and test dataset for our purposes.\n\nSteps:\n\n- Apply Word2Vec on the Corpus\n- Create Sentence Vectors by Mean Pooling\n- Run the input sentence vectors with Kfold Cross Validation on Traditional and gradient boosting classifiers.\n\n\n<img src=\"https://media1.tenor.com/images/0e438477bb88b5683690bfe101cf1181/tenor.gif?itemid=10724659\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#Applying W2V Balanced Baseline with KFold\n\nmodels=[]\nmodels.append(('LogisticRregression',LogisticRegression(C=1.0,penalty='l2')))\nmodels.append(('KNearestNeighbors',KNeighborsClassifier()))\nmodels.append(('DecisionTree',DecisionTreeClassifier(criterion='entropy')))\n#models.append(('RandomForestRegressor',RandomForestRegressor(n_estimators = 1000, random_state = 42)))\n#models.append(('RandomForestClassifier',RandomForestClassifier(n_estimators = 1000, criterion='gini')))\nmodels.append(('GradientBoostClassifier',GradientBoostingClassifier(learning_rate=1e-2, loss='deviance',n_estimators=100)))\nmodels.append(('AdaBoostClassifier',AdaBoostClassifier(learning_rate=1e-2,algorithm='SAMME.R',n_estimators=100)))\nmodels.append(('ExtraTreesClassifier',ExtraTreesClassifier(n_estimators=10, max_depth=None,min_samples_split=2)))\nmodels.append(('BagClassifier',BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.5)))\n# models.append(('HistGradientBoostClassifier',HistGradientBoostingClassifier(max_iter=100)))\n#models.append(('SupportVectorClassifier',SVC(C=1.0,kernel='sigmoid')))\nw2v_model_training_result,w2v_model_validation_result=[],[]\nscoring='accuracy'\nprint(\"Statistical Model Word2Vec- Baseline Evaluation\")\nfor name,model in models:\n    kfold=KFold(n_splits=10,random_state=7)\n    results=cross_val_score(model,train_x,train_y,cv=kfold)\n    print(\"=======================\")\n    print(\"Classifiers: \",name, \"Has a training score of\", round(results.mean(), 2) * 100, \"% accuracy score\")\n    w2v_model_training_result.append(results.mean())\n    predictions=cross_val_predict(model,test_x,test_y)\n    accuracy = accuracy_score(predictions,test_y)\n    w2v_model_validation_result.append(accuracy)\n\nfinal_w2v_outcomes=pd.DataFrame(columns=['Model','Training Acc','Validation Acc'])\nfinal_w2v_outcomes['Model']=models\nfinal_w2v_outcomes['Training Acc']=w2v_model_training_result\nfinal_w2v_outcomes['Validation Acc']=w2v_model_validation_result\nfinal_w2v_outcomes.to_csv('W2V-Baseline.csv',index=False)\nfinal_w2v_outcomes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying XGBoost and LightGBM\n\nThis will allow us to further analyse the  results of XGBoost over LightGBM.\n\n<img src=\"https://miro.medium.com/max/1554/1*FLshv-wVDfu-i54OqvZdHg.png\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluating XGBoost & Light GBM on the dataset\nfrom xgboost import XGBClassifier as xg\nfrom lightgbm import LGBMClassifier as lg\nmodel_xgb= xg(n_estimators=100,random_state=42)\nmodel_xgb.fit(train_x,train_y)\ny_pred_xgb=model_xgb.predict(test_x)\nprint(accuracy_score(test_y,y_pred_lgbm.round()))\n# print(\"Confusion matrix\")\nmodel_lgbm= lg(n_estimators=100,random_state=42)\nmodel_lgbm.fit(train_x,train_y)\ny_pred_lgbm=model_lgbm.predict(test_x)\n# print(\"Confusion matrix\")\n# print(confusion_matrix(test_y,y_pred_lgbm))\nprint(accuracy_score(test_y,y_pred_lgbm.round()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Converting Other Vectors (Glove,Fasttext) to Word2Vec \n\nIn this case, we will be using Glove,Fasttext by converting them to Word2Vec embeddings and then applying mean pooling on them. The method of conversion is taken from the [previous Notebook](https://www.kaggle.com/colearninglounge/nlp-end-to-end-cll-nlp-workshop). The steps involved are as follows:\n\n- Convert Glove,Fasttext,Google News to Word2Vec by using Gensim\n- Apply Mean Pooling on the Word Vectors to create Sentence Vectors\n- Apply Statistical Classifiers with Kfold Cross Validation\n\nThere are alternate strategies to apply bu the this is by far the simplest one with minimalistic code.\nSome resource:\n\n- [Good Alternate Script](https://www.kaggle.com/eswarbabu88/toxic-comment-glove-logistic-regression)\n- [Blog](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import Word2Vec,KeyedVectors\nfrom gensim.scripts.glove2word2vec import glove2word2vec\n\n##Google News Vectors to word2vec format for mean pooling\ngoogle_news_embed='../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin'\ngoogle_loaded_model=KeyedVectors.load_word2vec_format(google_news_embed,binary=True)\nprint(google_loaded_model)\n##Glove Vectors to word2vec format for mean pooling\nglove_file='../input/glove-global-vectors-for-word-representation/glove.6B.50d.txt'\nword2vec_output_file = 'glove.6B.100d.txt.word2vec'\nglove_loaded=glove2word2vec(glove_file, word2vec_output_file)\nglove_loaded = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\nprint(glove_loaded)\n##Fasttext to word2vec format for mean pooling\nfasttext_file=\"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\"\nfasttext_model = KeyedVectors.load_word2vec_format(fasttext_file, binary=False)\nprint(fasttext_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_sentence_embeddings(data,model):\n    vocab=[w for w in data if w in model.wv.vocab]\n    avg_pool=np.mean(model[vocab],axis=0)\n#     sum_pool=np.sum(model[vocab],axis=0)\n#     min_pool=np.min(model[vocab],axis=0)\n#     max_pool=np.max(model[vocab],axis=0)\n    return avg_pool\n\n#Google vectors\nprint('Google Vectors')\ntrain_google_df=train_df\ntrain_google_df['Google_News_Vectorized_Reviews']=train_df['review'].apply(lambda z: convert_sentence_embeddings(z,google_loaded_model) )\n#Split the dataset into training and testing sets\ntrain_google_y=train_df['sentiment']\ntrain_google_x,test_google_x,train_google_y,test_google_y=train_test_split(train_google_df['Google_News_Vectorized_Reviews'],train_google_y,test_size=0.2,random_state=42)\ntrain_google_x=list(train_google_x)\ntest_google_x=list(test_google_x)\n# train_google_x.shape,train_google_y.shape,test_google_x.shape,test_google_y.shape\n\n\n# #Glove Vectors\n# print('Glove Vectors')\n# train_glove_df=train_df\n# train_glove_df['Glove_Vectorized_Reviews']=train_df['review'].apply(lambda z: convert_sentence_embeddings(z,glove_loaded) )\n# #Split the dataset into training and testing sets\n# train_glove_y=train_df['sentiment']\n# train_glove_x,test_glove_x,train_glove_y,test_glove_y=train_test_split(train_glove_df['Glove_Vectorized_Reviews'],train_glove_y,test_size=0.2,random_state=42)\n# train_glove_x=list(train_glove_x)\n# test_glove_x=list(test_glove_x)\n# # train_glove_x.shape,train_glove_y.shape,test_glove_x.shape,test_glove_y.shape\n\n# #FastText Vectors\n# print('Fasttext Vectors')\n# train_fasttext_df=train_df\n# train_fasttext_df['Fasttext_Vectorized_Reviews']=train_df['review'].apply(lambda z: convert_sentence_embeddings(z,fasttext_model) )\n# #Split the dataset into training and testing sets\n# train_fasttext_y=train_df['sentiment']\n# train_fasttext_x,test_fasttext_x,train_fasttext_y,test_fasttext_y=train_test_split(train_fasttext_df['Fasttext_Vectorized_Reviews'],train_fasttext_y,test_size=0.2,random_state=42)\n# train_fasttext_x=list(train_fasttext_x)\n# test_fasttext_x=list(test_fasttext_x)\n# # train_fasttext_x.shape,train_fasttext_y.shape,test_fasttext_x.shape,test_fasttext_y.shape\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#FastText Vectors\nprint('Fasttext Vectors')\ntrain_fasttext_df=train_df\ntrain_fasttext_df['Fasttext_Vectorized_Reviews']=train_df['review'].apply(lambda z: convert_sentence_embeddings(z,fasttext_model) )\n#Split the dataset into training and testing sets\ntrain_fasttext_y=train_df['sentiment']\ntrain_fasttext_x,test_fasttext_x,train_fasttext_y,test_fasttext_y=train_test_split(train_fasttext_df['Fasttext_Vectorized_Reviews'],train_fasttext_y,test_size=0.2,random_state=42)\ntrain_fasttext_x=list(train_fasttext_x)\ntest_fasttext_x=list(test_fasttext_x)\n# train_fasttext_x.shape,train_fasttext_y.shape,test_fasttext_x.shape,test_fasttext_y.shape\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying the Classifiers\n\nHere in this sequence of codebases we apply the classifiers for our use case. "},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#Applying Google Vectors Balanced Baseline with KFold\n\nmodels=[]\nmodels.append(('LogisticRregression',LogisticRegression(C=1.0,penalty='l2')))\nmodels.append(('KNearestNeighbors',KNeighborsClassifier()))\nmodels.append(('DecisionTree',DecisionTreeClassifier(criterion='entropy')))\n#models.append(('RandomForestRegressor',RandomForestRegressor(n_estimators = 1000, random_state = 42)))\n#models.append(('RandomForestClassifier',RandomForestClassifier(n_estimators = 1000, criterion='gini')))\nmodels.append(('GradientBoostClassifier',GradientBoostingClassifier(learning_rate=1e-2, loss='deviance',n_estimators=100)))\nmodels.append(('AdaBoostClassifier',AdaBoostClassifier(learning_rate=1e-2,algorithm='SAMME.R',n_estimators=100)))\nmodels.append(('ExtraTreesClassifier',ExtraTreesClassifier(n_estimators=10, max_depth=None,min_samples_split=2)))\nmodels.append(('BagClassifier',BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.5)))\n# models.append(('HistGradientBoostClassifier',HistGradientBoostingClassifier(max_iter=100)))\n#models.append(('SupportVectorClassifier',SVC(C=1.0,kernel='sigmoid')))\nmodels.append(('XGBoosting',xg(n_estimators=100,random_state=42)))\nmodels.append(('LightGBM',lg(n_estimators=100,random_state=42)))\ngoogle_model_training_result,google_model_validation_result=[],[]\nscoring='accuracy'\nprint(\"Statistical Model Google Vectors- Baseline Evaluation\")\nfor name,model in models:\n    kfold=KFold(n_splits=10,random_state=7)\n    results=cross_val_score(model,train_google_x,train_google_y,cv=kfold)\n    print(\"=======================\")\n    print(\"Classifiers: \",name, \"Has a training score of\", round(results.mean(), 2) * 100, \"% accuracy score\")\n    w2v_model_training_result.append(results.mean())\n    predictions=cross_val_predict(model,test_google_x,test_google_y)\n    accuracy = accuracy_score(predictions,test_google_y)\n    w2v_model_validation_result.append(accuracy)\n\nfinal_google_outcomes=pd.DataFrame(columns=['Model','Training Acc','Validation Acc'])\nfinal_google_outcomes['Model']=models\nfinal_google_outcomes['Training Acc']=google_model_training_result\nfinal_google_outcomes['Validation Acc']=google_model_validation_result\nfinal_google_outcomes.to_csv('GoogleNewsVectors-Baseline.csv',index=False)\nfinal_google_outcomes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#Applying Glove Vectors Balanced Baseline with KFold\n\nmodels=[]\nmodels.append(('LogisticRregression',LogisticRegression(C=1.0,penalty='l2')))\nmodels.append(('KNearestNeighbors',KNeighborsClassifier()))\nmodels.append(('DecisionTree',DecisionTreeClassifier(criterion='entropy')))\n#models.append(('RandomForestRegressor',RandomForestRegressor(n_estimators = 1000, random_state = 42)))\n#models.append(('RandomForestClassifier',RandomForestClassifier(n_estimators = 1000, criterion='gini')))\nmodels.append(('GradientBoostClassifier',GradientBoostingClassifier(learning_rate=1e-2, loss='deviance',n_estimators=100)))\nmodels.append(('AdaBoostClassifier',AdaBoostClassifier(learning_rate=1e-2,algorithm='SAMME.R',n_estimators=100)))\nmodels.append(('ExtraTreesClassifier',ExtraTreesClassifier(n_estimators=10, max_depth=None,min_samples_split=2)))\nmodels.append(('BagClassifier',BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.5)))\n# models.append(('HistGradientBoostClassifier',HistGradientBoostingClassifier(max_iter=100)))\n#models.append(('SupportVectorClassifier',SVC(C=1.0,kernel='sigmoid')))\nmodels.append(('XGBoosting',xg(n_estimators=100,random_state=42)))\nmodels.append(('LightGBM',lg(n_estimators=100,random_state=42)))\nglove_model_training_result,glove_model_validation_result=[],[]\nscoring='accuracy'\nprint(\"Statistical Model Google Vectors- Baseline Evaluation\")\nfor name,model in models:\n    kfold=KFold(n_splits=10,random_state=7)\n    results=cross_val_score(model,train_glove_x,train_glove_y,cv=kfold)\n    print(\"=======================\")\n    print(\"Classifiers: \",name, \"Has a training score of\", round(results.mean(), 2) * 100, \"% accuracy score\")\n    w2v_model_training_result.append(results.mean())\n    predictions=cross_val_predict(model,test_glove_x,test_glove_y)\n    accuracy = accuracy_score(predictions,test_glove_y)\n    w2v_model_validation_result.append(accuracy)\n\nfinal_glove_outcomes=pd.DataFrame(columns=['Model','Training Acc','Validation Acc'])\nfinal_glove_outcomes['Model']=models\nfinal_glove_outcomes['Training Acc']=glove_model_training_result\nfinal_glove_outcomes['Validation Acc']=glove_model_validation_result\nfinal_glove_outcomes.to_csv('GoogleNewsVectors-Baseline.csv',index=False)\nfinal_glove_outcomes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#Applying Fasttext Vectors Balanced Baseline with KFold\n\nmodels=[]\nmodels.append(('LogisticRregression',LogisticRegression(C=1.0,penalty='l2')))\nmodels.append(('KNearestNeighbors',KNeighborsClassifier()))\nmodels.append(('DecisionTree',DecisionTreeClassifier(criterion='entropy')))\n#models.append(('RandomForestRegressor',RandomForestRegressor(n_estimators = 1000, random_state = 42)))\n#models.append(('RandomForestClassifier',RandomForestClassifier(n_estimators = 1000, criterion='gini')))\nmodels.append(('GradientBoostClassifier',GradientBoostingClassifier(learning_rate=1e-2, loss='deviance',n_estimators=100)))\nmodels.append(('AdaBoostClassifier',AdaBoostClassifier(learning_rate=1e-2,algorithm='SAMME.R',n_estimators=100)))\nmodels.append(('ExtraTreesClassifier',ExtraTreesClassifier(n_estimators=10, max_depth=None,min_samples_split=2)))\nmodels.append(('BagClassifier',BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.5)))\n# models.append(('HistGradientBoostClassifier',HistGradientBoostingClassifier(max_iter=100)))\n#models.append(('SupportVectorClassifier',SVC(C=1.0,kernel='sigmoid')))\nmodels.append(('XGBoosting',xg(n_estimators=100,random_state=42)))\nmodels.append(('LightGBM',lg(n_estimators=100,random_state=42)))\nfasttext_model_training_result,fasttext_model_validation_result=[],[]\nscoring='accuracy'\nprint(\"Statistical Model Google Vectors- Baseline Evaluation\")\nfor name,model in models:\n    kfold=KFold(n_splits=10,random_state=7)\n    results=cross_val_score(model,train_fasttext_x,train_fasttext_y,cv=kfold)\n    print(\"=======================\")\n    print(\"Classifiers: \",name, \"Has a training score of\", round(results.mean(), 2) * 100, \"% accuracy score\")\n    w2v_model_training_result.append(results.mean())\n    predictions=cross_val_predict(model,test_fasttext_x,test_fasttext_y)\n    accuracy = accuracy_score(predictions,test_fasttext_y)\n    w2v_model_validation_result.append(accuracy)\n\nfinal_fasttext_outcomes=pd.DataFrame(columns=['Model','Training Acc','Validation Acc'])\nfinal_fasttext_outcomes['Model']=models\nfinal_fasttext_outcomes['Training Acc']=fasttext_model_training_result\nfinal_fasttext_outcomes['Validation Acc']=fasttext_model_validation_result\nfinal_fasttext_outcomes.to_csv('GoogleNewsVectors-Baseline.csv',index=False)\nfinal_fasttext_outcomes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Standard Neural Networks with Static Semantic Embeddings Baseline\n\n\n<img src=\"https://miro.medium.com/max/688/1*zR61FG9RUd6ul4ecXA_euQ.jpeg\">\n\n\nIn this context, we will be building a preliminary deep model using sophisticated neural networks and variants of RNNs. We will be building a simple LSTM model for validating the influence of deep models with respect to the statistical ones. In the first case, we will be using the Keras Embedding layer and visualize the results before using the embedding models.\n\n[Keras LSTM](https://keras.io/api/layers/recurrent_layers/lstm/)\n[Keras](https://keras.io/)\n[Keras Starter Guides](https://keras.io/examples/nlp/)\n[Tensorflow Starter](https://www.tensorflow.org/tutorials/keras/text_classification)\n[Tensorflow Hub](https://www.tensorflow.org/tutorials/keras/text_classification_with_hub)\n[Jason's Blog-Best practises](https://machinelearningmastery.com/best-practices-document-classification-deep-learning/)\n[Jason's Blog-Convolution Networks](https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/)\n\n\nMore resources will be provided, and for now we will be focussing on creating specific  RNN (Recurrent Neural Variants) with/without Static Semantic Embeddings to create a Neural Model Baseline. \n\n\n<img src=\"https://miro.medium.com/max/875/1*n-IgHZM5baBUjq0T7RYDBw.gif\">\n\n\n### Recurrent Neural Networks\n\nRecurrent neural networks (RNN) are a class of neural networks that is powerful for modeling sequence data such as time series or natural language.\n\nSchematically, a RNN layer uses a for loop to iterate over the timesteps of a sequence, while maintaining an internal state that encodes information about the timesteps it has seen so far.\n\nThe Keras RNN API is designed with a focus on:\n\n- Ease of use: the built-in keras.layers.RNN, keras.layers.LSTM, keras.layers.GRU layers enable you to quickly build recurrent models without having to make difficult configuration choices.\n\n- Ease of customization: You can also define your own RNN cell layer (the inner part of the for loop) with custom behavior, and use it with the generic keras.layers.RNN layer (the for loop itself). This allows you to quickly prototype different research ideas in a flexible way with minimal code.\n\n\nA classic RNN appears as follows:\n\n<img src=\"https://miro.medium.com/max/627/1*go8PHsPNbbV6qRiwpUQ5BQ.png\">\n\nThis [video](https://youtu.be/8HyCNIVRbSU) provides a good description of how RNNs work.\n\n\nParticulary a RNN works on the logic:\n\n\n<img src=\"https://miro.medium.com/max/875/1*3mDe6V5DRXqpHYKDfxN4Rg.png\">\n\n\nThere are various kinds of such networks:\n\n\n- Encoding Recurrent Neural Networks are just folds. They’re often used to allow a neural network to take a variable length list as input, for example taking a sentence as input.\n\n\n<img src=\"https://colah.github.io/posts/2015-09-NN-Types-FP/img/RNN-encoding.png\">\n\n\n- Generating Recurrent Neural Networks are just unfolds. They’re often used to allow a neural network to produce a list of outputs, such as words in a sentence.\n\n\n<img src=\"https://colah.github.io/posts/2015-09-NN-Types-FP/img/RNN-generating.png\">\n\n\n- General Recurrent Neural Networks are accumulating maps. They’re often used when we’re trying to make predictions in a sequence. For example, in voice recognition, we might wish to predict a phenome for every time step in an audio segment, based on past context.\n\n\n<img src=\"https://colah.github.io/posts/2015-09-NN-Types-FP/img/RNN-general.png\">\n\n\n- Bidirectional Recursive Neural Networks are a more obscure variant, which I mention primarily for flavor. In functional programming terms, they are a left and a right accumulating map zipped together. They’re used to make predictions over a sequence with both past and future context.\n\n<img src=\"https://colah.github.io/posts/2015-09-NN-Types-FP/img/RNN-bidirectional.png\">\n\n \n \nSome resources for understanding the derivatives and optimization inside the RNNs:\n\n- [Maths](https://www.cs.toronto.edu/~tingwuwang/rnn_tutorial.pdf)\n- [Blog](https://colah.github.io/posts/2015-09-NN-Types-FP/)\n- [Blog](https://towardsdatascience.com/under-the-hood-of-neural-networks-part-2-recurrent-af091247ba78)\n- [Kernel](https://www.kaggle.com/abhilash1910/nlp-workshop-ml-india#Neural-Networks)\n\n\nThese are some starter resources for creating preliminary networks for sentiment analysis, text/intent classifications. There will be some advanced architectures which will be focussed later.\n\n\n### Long Short Term Memory (LSTM)\n\n[Drawbacks of RNNS](https://colah.github.io/posts/2015-08-Understanding-LSTMs/): One of the appeals of RNNs is the idea that they might be able to connect previous information to the present task, such as using previous video frames might inform the understanding of the present frame. If RNNs could do this, they’d be extremely useful. But can they? It depends.Sometimes, we only need to look at recent information to perform the present task. For example, consider a language model trying to predict the next word based on the previous ones. If we are trying to predict the last word in “the clouds are in the sky,” we don’t need any further context – it’s pretty obvious the next word is going to be sky. In such cases, where the gap between the relevant information and the place that it’s needed is small, RNNs can learn to use the past information.But there are also cases where we need more context. Consider trying to predict the last word in the text “I grew up in France… I speak fluent French.” Recent information suggests that the next word is probably the name of a language, but if we want to narrow down which language, we need the context of France, from further back. It’s entirely possible for the gap between the relevant information and the point where it is needed to become very large.\nUnfortunately, as that gap grows, RNNs become unable to learn to connect the information.In theory, RNNs are absolutely capable of handling such “long-term dependencies.” A human could carefully pick parameters for them to solve toy problems of this form. Sadly, in practice, RNNs don’t seem to be able to learn them. The problem was explored in depth by Hochreiter (1991) [German] and Bengio, et al. (1994), who found some pretty fundamental reasons why it might be difficult.\nThankfully, LSTMs don’t have this problem!\n\n- LSTMs:\n \n <img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\">\n \n The first step in our LSTM is to decide what information we’re going to throw away from the cell state. This decision is made by a sigmoid layer called the “forget gate layer.” It looks at ```ht−1``` and ```xt```, and outputs a number between 0 and 1 for each number in the cell state ```Ct−1```. A 1 represents “completely keep this” while a 0 represents “completely get rid of this.”\n\nLet’s go back to our example of a language model trying to predict the next word based on all the previous ones. In such a problem, the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject.\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png\">\n\n\nThe next step is to decide what new information we’re going to store in the cell state. This has two parts. First, a sigmoid layer called the “input gate layer” decides which values we’ll update. Next, a tanh layer creates a vector of new candidate values, ```C~t```, that could be added to the state. In the next step, we’ll combine these two to create an update to the state.\n\nIn the example of our language model, we’d want to add the gender of the new subject to the cell state, to replace the old one we’re forgetting.\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png\">\n\nIt’s now time to update the old cell state, ```Ct−1```, into the new cell state ```Ct```. The previous steps already decided what to do, we just need to actually do it.\n\nWe multiply the old state by ```ft```, forgetting the things we decided to forget earlier. Then we add ```it∗C~t```. This is the new candidate values, scaled by how much we decided to update each state value.\n\nIn the case of the language model, this is where we’d actually drop the information about the old subject’s gender and add the new information, as we decided in the previous steps.\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png\">\n\nFinally, we need to decide what we’re going to output. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we’re going to output. Then, we put the cell state through tanh (to push the values to be between −1 and 1) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.\n\nFor the language model example, since it just saw a subject, it might want to output information relevant to a verb, in case that’s what is coming next. For example, it might output whether the subject is singular or plural, so that we know what form a verb should be conjugated into if that’s what follows next.\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png\">\n\n\nAn illustrated working of the LSTM is provided:\n\n\n<img src=\"https://miro.medium.com/max/1900/1*GjehOa513_BgpDDP6Vkw2Q.gif\">\n\n\nSome blogs:\n\n- [Blog](https://www.google.com/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21&psig=AOvVaw3GJ2-g9jyCgtlUxlTAmyJ8&ust=1608535825759000&source=images&cd=vfe&ved=0CA0QjhxqFwoTCLjax4eF3O0CFQAAAAAdAAAAABAD)\n- [Blog](https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/)\n- [Blog](https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/)\n- [Paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43905.pdf)\n\nThere are several Variants of LSTMs some of the most famous being Depth GRU /Gated Recurrent Units:\n\nA slightly more dramatic variation on the LSTM is the Gated Recurrent Unit, or GRU, introduced by Cho, et al. (2014). It combines the forget and input gates into a single “update gate.” It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, and has been growing increasingly popular.\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png\">\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import libraries\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.layers import LSTM, Dense,Flatten,Conv2D,Conv1D,GlobalMaxPooling1D,GlobalMaxPool1D\nfrom keras.optimizers import Adam\nimport numpy as np  \nimport pandas as pd \nimport keras.backend as k\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional,GRU\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.preprocessing import OneHotEncoder\nfrom keras.utils import to_categorical\nfrom keras.utils.vis_utils import plot_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating a Basic LSTM Neural Model without Embeddings\n\nIn this case, we will not be using an y pretrained static/dynamic embeddings but will be using a simple Neural Network model of LSTM to create our network.The steps are as follows:\n\n\n- Tokenize the input data (Keras.Preprocessing)\n- Creating the limits of Maxlen, Max Features and Embedding Size for our Embedding Matrix\n- Pad the tokenized data to maintain uniformity in length of the input features\n\nA more descriptive overview is found [here](https://www.kaggle.com/abhilash1910/nlp-workshop-ml-india#Neural-Networks) . This also provides an [idea](https://www.tensorflow.org/guide/keras/rnn)\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"##First Step is to test model performance without pretrained Embeddings\n## Will be using only Keras Embeddings in this case with a minimal neural network model\n\nmaxlen=1000\nmax_features=5000 \nembed_size=300\n\n#clean some null words or use the previously cleaned & lemmatized corpus\n\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\n\nval_x=test_x\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating the Model architecture\n\nHere we creating a [sequential model](https://keras.io/api/models/sequential/) and the embedding always has to be the first layer for our use case. In any neural model, Embedding layer always comes first followed by other layers- LSTM/GRU and others. The heirarchy of the model can be represented as below:\n\n<img src=\"https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs12859-019-3079-8/MediaObjects/12859_2019_3079_Fig2_HTML.png\">\n\n\nA proper model overview comprising of LSTMs and Embeddings is provided here:\n\n<img src=\"https://d3i71xaburhd42.cloudfront.net/6ac8328113639044d2beb83246b9d07f513ac6c8/3-Figure1-1.png\">\n\nSome resources:\n\n- [Kernels](https://www.kaggle.com/rajmehra03/a-detailed-explanation-of-keras-embedding-layer)\n- [Kernels](https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"##Design a Simple Network\n\nmodel=Sequential()\nmodel.add(Embedding(max_features,embed_size,input_length=maxlen))\nmodel.add(LSTM(60))\nmodel.add(Dense(16,activation='relu'))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"simple_model.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\nmodel.fit(train_x,train_y,batch_size=512,epochs=3,verbose=2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Visualization\n\nThe model with its individual layers can be visualized as follows:\n\n<img src=\"https://i.imgur.com/qI6zRqM.png\">\n\n\nWe can also look into the model parameters and the weights of the intermediate layers. We can visualize the sizes of the hidden layers and the output of each sequential layer in the model. Some resources:\n\n- [Keras](https://keras.io/getting_started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer-feature-extraction)\n- [Stack Overflow](https://stackoverflow.com/questions/41711190/keras-how-to-get-the-output-of-each-layer)\n- [Kite](https://www.kite.com/python/answers/how-to-get-the-output-of-each-layer-of-a-keras-model-in-python#:~:text=A%20Keras%20model%20runs%20data,and%20applies%20the%20layer%20funtion.)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Get to know individual layer sizes and parameters \n\nfrom keras import backend as k\ninputs=model.input\noutputs=[layer.output for layer in model.layers]\nprint(f\"Outputs of the sequential layers{outputs}\")\nfunctions=[k.function([inputs],[outs]) for outs in outputs]\nprint(f'Sequential Model Layers{functions}')\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inference from the Model\n\nThe Model has almost 1.5 million parameters to be trained even without any embeddings.This is the simplicity of using Keras to train such large parametric mdoels.\n\n<img src=\"https://media1.tenor.com/images/54603c681d37cecb2973e7974dea7f43/tenor.gif?itemid=16430080\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fit and validate\nmodel.fit(train_x,train_y,batch_size=128,epochs=3,verbose=2,validation_data=(val_x,val_y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build a Static Semantic Embedding Neural Network(LSTM) Baseline\n\nIn this case, we will be using pretrained embeddings for ouruse case. For this we will be using the embedding matrix creation  code from our [previous Notebook](https://www.kaggle.com/colearninglounge/nlp-end-to-end-cll-nlp-workshop).\n\nParticularly this lines of code:\n\n```python\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow import keras\nfrom keras.preprocessing.text import Tokenizer\nmaxlen=1000\nmax_features=5000 \nembed_size=300\n\ntrain_sample=train_df['review']\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_sample))\ntrain_sample=tokenizer.texts_to_sequences(train_sample)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_sample=pad_sequences(train_sample,maxlen=maxlen)\n\n\n\nEMBEDDING_FILE = '../input/wikinews300d1msubwordvec/wiki-news-300d-1M-subword.vec'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\nplt.plot(embedding_matrix[20])\nplt.show()\n```"},{"metadata":{"trusted":true},"cell_type":"code","source":"##Build Static Embedding on top of a Neural Model\n\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow import keras\nfrom keras.preprocessing.text import Tokenizer\nmaxlen=1000\nmax_features=5000 \nembed_size=300\n\ntrain_sample=train_df['review']\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_sample))\ntrain_sample=tokenizer.texts_to_sequences(train_sample)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_sample=pad_sequences(train_sample,maxlen=maxlen)\n\n\n\nEMBEDDING_FILE = '../input/glove-global-vectors-for-word-representation/glove.6B.50d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Run the same model with the Pretrained Embeddings\n\nNow we will run the same model as before with the pretrained static embeddings- Glove in our use case. I have trained it for 2 epochs but this can be made to train on an even larger epoch size."},{"metadata":{"trusted":true},"cell_type":"code","source":"inp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,weights=[embedding_matrix])(inp)\nz=Bidirectional(LSTM(60,return_sequences='True'))(z)\nz=GlobalMaxPool1D()(z)\nz=Dense(16,activation='relu')(z)\nz=Dense(1,activation='sigmoid')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"glove_simple_model.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\nmodel.fit(train_x,train_y,batch_size=128,epochs=3,verbose=2,validation_data=(val_x,val_y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Visualization\n\nThe Simple LSTM Model with Glove Pretrained embeddings:\n\n<img src=\"https://i.imgur.com/7FpjJVP.png\">"},{"metadata":{},"cell_type":"markdown","source":"## Working With ELMO\n\n[ELMO](https://tfhub.dev/google/elmo/3) is a deep contextual embedding model comprising of stacked dual lstm layers.Salient features of the model:\n\n- Computes contextualized word representations using character-based word representations and bidirectional LSTMs, as described in the paper \"Deep contextualized word representations\" [1].\n\n- This modules supports inputs both in the form of raw text strings or tokenized text strings.\n\n- The module outputs fixed embeddings at each LSTM layer, a learnable aggregation of the 3 layers, and a fixed mean-pooled vector representation of the input.\n\n- The complex architecture achieves state of the art results on several benchmarks.\n\nThe entire architectural model is as follows:\n\n\n\n<img src=\"http://jalammar.github.io/images/Bert-language-modeling.png\">\n\n\nThe deep contextual representations are retained with the help of bidirectional lstm layers, which uses look-ahead and look-back mechanisms combined to provide a correct representation of the context.\n\n\n<img src=\"http://jalammar.github.io/images/elmo-forward-backward-language-model-embedding.png\">\n\n\nSome resources:\n\n- [Paper](https://arxiv.org/abs/1802.05365)\n- [Kernel](https://www.kaggle.com/sarthak221995/textclassification-95-5-accuracy-elmo)\n- [Blog](https://towardsdatascience.com/elmo-contextual-language-embedding-335de2268604)\n- [Blog](https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/)\n- [Jay's Blog](http://jalammar.github.io/illustrated-bert/)\n"},{"metadata":{},"cell_type":"markdown","source":"## Some Issues with ELMO\n\n\nELMO appears to be working well with TF 1.15 (rather any  TF version <2.0.0) . For using ELMO from Tensorflow [Hub](https://tfhub.dev/google/elmo/3), we have to follow the steps:\n\n- Restart the Kernel\n- Run the cell containing:\n  ```python\n   !pip install -U tensorflow==1.15\n  ```\n- Check if the older version of tensorflow is installed (the session will automatically get restarted).\n  ```python\n   import tensorflow as tf\n   tf.__version__\n  ```\n- Make sure the ELMO embeddings are loaded and can be used by clicking on the example cell below this markdown.\n- Create the ELMO embeddings and feed it into the classifier.\n\n\nSince ELMO has several benchmarks due to its bidirectionality (LSTMs), the only layers which provide a proper accuracy are Dense layers (when placed after the Embedding Layer).ELMO embeddings generally have a shape of (?,?,1024), and hence compressing these multidimensional embeddings to a Dense Layer (eg.256 units) takes a huge computation time. The program for the ELMO embedding classifier is written in tensorflow."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Make sure ELMO embeddings are working in tf version 1.15\n\n# !pip install tensorflow==1.15\nimport tensorflow as tf\nimport tensorflow_hub as tf_hub\n\nelmo = tf_hub.Module(\"https://tfhub.dev/google/elmo/2\")\nembeddings = elmo(\n    [\"the cat is on the mat\", \"dogs are in the fog\"],\n    signature=\"default\",\n    as_dict=True)[\"elmo\"]\nembeddings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as tf_hub\n\n# from keras.layers import Input, Lambda, Dense\n# from keras import backend as k\nelmo_embed=tf_hub.Module(\"https://tfhub.dev/google/elmo/2\",trainable=True)\n\n#Creating the elmo embeddings by squeezing the inputs\ndef create_embedding(z):\n    return elmo_embed(tf.squeeze(tf.cast(z,tf.string)),signature='default',as_dict=True)[\"default\"]\n\ntrain_y=labels\nX=train_df['review'].tolist()\ntrain_x,test_x,train_y,test_y=train_test_split(np.asarray(X),train_y,test_size=0.2,random_state=42)\n#Create the model with ELMO Embeddings and Dense Layers\n\ninp=tf.keras.layers.Input(shape=(1,),dtype=tf.string)\nz=tf.keras.layers.Lambda(create_embedding,output_shape=(1024,))(inp)\nz=tf.keras.layers.Dense(128,activation='relu')(z)\nz=tf.keras.layers.Dense(1,activation='sigmoid')(z)\nmodel=tf.keras.Model(inputs=inp,outputs=z)\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,to_file=\"seq2seq_encoder_decoder_model.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n\n\nwith tf.Session() as session:\n#     K.set_session(session)\n    session.run(tf.global_variables_initializer())  \n    session.run(tf.tables_initializer())\n    history = model.fit(train_x, train_y, epochs=1, batch_size=16)\n    model.save_weights('./response-elmo-model.h5')\n\n# with tf.Session() as session:\n#     K.set_session(session)\n#     session.run(tf.global_variables_initializer())\n#     session.run(tf.tables_initializer())\n#     model.load_weights('./response-elmo-model.h5')  \n#     predicts = model.predict(x_test, batch_size=16)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip uninstall tensorflow -y\n!pip uninstall tensorflow-cloud -y\n!pip install -U tensorflow==1.15","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip uninstall pytorch-lightning -y\n!pip uninstall tensorflow-probability -y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking for the Installed Tensorflow Version\n\nThis code block checks for the tensorflow version (downgraded)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\ntf.__version__\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion of ELMO Embeddings\n\nWe used ELMO embeddings to see the performance of ELMO on our classification model. For the actual implementation of the ELMO (Peters etal), the resources are provided:\n\n- [Github](https://github.com/allenai/bilm-tf)\n- [Resources](https://paperswithcode.com/paper/deep-contextualized-word-representations)\n\n\n<img src=\"https://media3.giphy.com/media/3o7budMRwZvNGJ3pyE/giphy.gif\">\n"},{"metadata":{},"cell_type":"markdown","source":"## Encoder Decoder Architectures\n\n[Encoder-Decoders](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html) is a classic architecture mostly popular for sequence2sequence learning. Encoder-Decoders are most popularly used for neural machine translation (seq2seq learning with attention). The general workflow revolves around stacks of RNNs (LSTMs/GRUs/TimeDistributed Cells) which behaves as an encoder takes as input 3 parameters (max_features,embed_size,maxlen in our example) and returns an output. We then save the 2 output LSTM cell states ,the h and c states. We design the decoder model in a similar manner (if the internal layers are modified it becomes a hybrid decoder). And while passing the inputs of the decoder, we also pass the 2 output LSTM cell states from the encoder output (namely the h and c states). The output of the decoder is then passed through a activation/distribution function to optimize our target loss function.\n\n\n<img src=\"https://miro.medium.com/max/875/1*CkeGXClZ5Xs0MhBc7xFqSA.png\">\n\n\n\n### Descriptive overview of NMT with Encoder Decoders:\n\n\n\nIn the context of NMT, the words of one language should be mapped to a different language (machine translation). An example of such an architecture is as follows:\n\n\n\n<img src=\"https://blog.keras.io/img/seq2seq/seq2seq-teacher-forcing.png\">\n\n\n\nIn the general case, input sequences and output sequences have different lengths (e.g. machine translation) and the entire input sequence is required in order to start predicting the target. This requires a more advanced setup, which is what people commonly refer to when mentioning \"sequence to sequence models\" with no further context. Here's how it works:\n\n- A RNN layer (or stack thereof) acts as \"encoder\": it processes the input sequence and returns its own internal state. Note that we discard the outputs of the encoder RNN, only recovering the state. This state will serve as the \"context\", or \"conditioning\", of the decoder in the next step.\n\n- Another RNN layer (or stack thereof) acts as \"decoder\": it is trained to predict the next characters of the target sequence, given previous characters of the target sequence. Specifically, it is trained to turn the target sequences into the same sequences but offset by one timestep in the future, a training process called \"teacher forcing\" in this context. Importantly, the encoder uses as initial state the state vectors from the encoder, which is how the decoder obtains information about what it is supposed to generate. Effectively, the decoder learns to generate targets[t+1...] given targets[...t], conditioned on the input sequence.\n\n\n\n### Pictorial Representation of Encoder-Decoders for Generative Modelling\n\n\n\n<img src=\"https://miro.medium.com/max/1250/1*LYGO4IxqUYftFdAccg5fVQ.png\">\n\n\n\nSome resources:\n\n- [TF-Blog](https://www.tensorflow.org/tutorials/text/nmt_with_attention)\n- [Blog](https://towardsdatascience.com/how-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639)\n- [Blog](https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"maxlen=1000\nmax_features=5000 \nembed_size=300\n\n#clean some null words or use the previously cleaned & lemmatized corpus\n\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\n\nval_x=test_x\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n#sequence to sequence basic lstm encoder gru decoders\ndef seq2seq_encoder_decoder(maxlen,max_features,embed_size):\n    #Creating LSTM  encoder neural model with no pretrained embeddings\n    encoder_inp=Input(shape=(maxlen,))\n    encoder_embed=Embedding(max_features,embed_size,input_length=maxlen,trainable=True)(encoder_inp)\n    encoder_lstm_cell=LSTM(60,return_state='True')\n    encoder_outputs,encoder_state_lstm_h,encoder_state_lstm_c=encoder_lstm_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    #Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(max_features,embed_size,input_length=maxlen,trainable=True)(decoder_inp)\n    decoder_lstm_cell=LSTM(60,return_sequences='True',return_state=True)\n    decoder_outputs,decoder_state_lstm_h,decoder_state_lstm_c=decoder_lstm_cell(decoder_embed,initial_state=[encoder_state_lstm_h,encoder_state_lstm_c])\n    decoder_dense_cell=Dense(16,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(1,activation='sigmoid')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    return model\n    \n    \nmodel=seq2seq_encoder_decoder(maxlen,max_features,embed_size)  \nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nplot_model(\n    model,to_file=\"seq2seq_encoder_decoder_model.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n    \nmodel.fit([train_x,train_x],train_y,batch_size=512,epochs=3,verbose=2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoder-Decoder Model Architecture\n\nThe model architecture by using trainable Embeddings (no pretrained embeddings):\n\n<img src=\"https://i.imgur.com/sGBqoBB.png\">\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Homologous Encoder Decoder With Pretrained Embeddings\n\nIn this context, we will be applying a pretrained static embedding (Glove-embedding matrix) to our Encoder Decoder model (comprising of LSTM units). Then we will visualize the training and performance on our dataset.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def seq2seq_encoder_decoder_glove(maxlen,max_features,embedding_matrix):\n    #Creating LSTM  encoder neural model with no pretrained embeddings\n    encoder_inp=Input(shape=(maxlen,))\n    encoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(encoder_inp)\n    encoder_lstm_cell=LSTM(60,return_state='True')\n    encoder_outputs,encoder_state_lstm_h,encoder_state_lstm_c=encoder_lstm_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    #Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(decoder_inp)\n    decoder_lstm_cell=LSTM(60,return_sequences='True',return_state=True)\n    decoder_outputs,decoder_state_lstm_h,decoder_state_lstm_c=decoder_lstm_cell(decoder_embed,initial_state=[encoder_state_lstm_h,encoder_state_lstm_c])\n    decoder_dense_cell=Dense(16,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(1,activation='sigmoid')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    return model\n    \n    \nmodel=seq2seq_encoder_decoder_glove(maxlen,max_features,embedding_matrix)  \nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nplot_model(\n    model,to_file=\"seq2seq_encoder_decoder_model_glove.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n    \nmodel.fit([train_x,train_x],train_y,batch_size=512,epochs=2,verbose=2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Homologous Encoder-Decoder Model Architecture\n\nThe model architecture with pretrained Glove 50d embeddings :\n\n<img src=\"https://i.imgur.com/WQww2yU.png\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bidirectional LSTM Encoder-Decoder\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\ndef seq2seq_encoder_decoder_glove_bilstm(maxlen,max_features,embedding_matrix):\n    #Creating LSTM  encoder neural model with no pretrained embeddings\n    encoder_inp=Input(shape=(maxlen,))\n    encoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(encoder_inp)\n    encoder_lstm_cell=Bidirectional(LSTM(60,return_state='True'))\n    encoder_outputs,encoder_state_flstm_h,encoder_state_flstm_c,encoder_state_blstm_h,encoder_state_blstm_c=encoder_lstm_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_state_flstm_h,encoder_state_flstm_c,encoder_state_blstm_h,encoder_state_blstm_c]\n    #Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(decoder_inp)\n    \n    decoder_lstm_cell=Bidirectional(LSTM(60,return_sequences='True',return_state=True),merge_mode=\"concat\")\n    decoder_outputs,decoder_state_lstm_h,decoder_state_lstm_c,_,_=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n#     decoderoutputs,_,_=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n    \n    decoder_dense_cell=Dense(16,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(1,activation='sigmoid')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    return model\n    \n    \nmodel=seq2seq_encoder_decoder_glove_bilstm(maxlen,max_features,embedding_matrix)  \nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nplot_model(\n    model,to_file=\"seq2seq_encoder_decoder_model_glove_bilstm.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n    \nmodel.fit([train_x,train_x],train_y,batch_size=512,epochs=2,verbose=2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Architecture\n\nThe model architecture is shown here:\n\n<img src=\"https://i.imgur.com/5FAd57t.png\">"},{"metadata":{},"cell_type":"markdown","source":"## Hybrid Encoder Decoder Models\n\nThese classes of encoder decoders allow different variants of RNNs (LSTM/Bilstm) which acts as a variational circuit. Hybrid deccoder models generally have a compression decoder which implies that the decoder can be GRU/LSTM while the encoder can be any Bidirectional version of that. This allows a smooth compression of the tensors by concatenating the hidden and cell state channels."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bidirectional LSTM Encoder-Decoder\n# maxlen=1000\n# max_features=5000 \n# embed_size=300\n\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\ndef seq2seq_encoder_decoder_glove_bilstm_hybrid(maxlen,max_features,embedding_matrix):\n    #Creating LSTM  encoder neural model with no pretrained embeddings\n    encoder_inp=Input(shape=(maxlen,))\n    encoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(encoder_inp)\n    encoder_lstm_cell=Bidirectional(LSTM(60,return_state='True'),merge_mode='sum')\n    encoder_outputs,encoder_state_flstm_h,encoder_state_flstm_c,encoder_state_blstm_h,encoder_state_blstm_c=encoder_lstm_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_state_flstm_h+encoder_state_blstm_h,encoder_state_flstm_c+encoder_state_blstm_c]\n    #Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(decoder_inp)\n    \n    decoder_lstm_cell=LSTM(60,return_sequences='True',return_state=True)\n    decoder_outputs,decoder_state_lstm_h,decoder_state_lstm_c=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n#     decoderoutputs,_,_=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n    \n    decoder_dense_cell=Dense(16,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(1,activation='sigmoid')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    return model\n    \n    \nmodel=seq2seq_encoder_decoder_glove_bilstm_hybrid(maxlen,max_features,embedding_matrix)  \nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nplot_model(\n    model,to_file=\"seq2seq_encoder_decoder_model_glove_bilstm_hybrid.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n    \nmodel.fit([train_x,train_x],train_y,batch_size=512,epochs=2,verbose=2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hybrid Encoder Decoder With Attention\n\nThis section will comprise of Hybrid Encoder Decoder Architectures with variants of Attention Mechanisms. For an introduction attention refers to allowing certain neural weights to be focussed during training and this in turn assists in model performance.\n\nThe main paper behind this is [Attention is all you need](https://paperswithcode.com/paper/attention-is-all-you-need)\nA preview of this is provided in the images below:\n\n<img src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\">\n\n<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg\">\n\n\nMore details will be explained soon on the different variants . For now, this tf resource should [help](https://www.tensorflow.org/tutorials/text/nmt_with_attention)"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install MiniAttention","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bidirectional LSTM Hybrid Encoder-Decoder with Hierarchical Attention\nimport MiniAttention.MiniAttention as ma\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\ndef seq2seq_encoder_decoder_glove_bilstm_hybrid_attention(maxlen,max_features,embedding_matrix):\n    #Creating LSTM  encoder neural model with no pretrained embeddings\n    encoder_inp=Input(shape=(maxlen,))\n    encoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(encoder_inp)\n    encoder_embed_attention=ma.MiniAttentionBlock(None,None,None,keras.regularizers.L2(l2=0.02),None,None,None,None,None)(encoder_embed)\n    encoder_lstm_cell=Bidirectional(LSTM(60,return_state='True'),merge_mode=\"sum\")\n    encoder_outputs,encoder_state_flstm_h,encoder_state_flstm_c,encoder_state_blstm_h,encoder_state_blstm_c=encoder_lstm_cell(encoder_embed_attention)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_state_flstm_h+encoder_state_blstm_h,encoder_state_flstm_c+encoder_state_blstm_c]\n    #Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(decoder_inp)\n    \n    decoder_lstm_cell=LSTM(60,return_sequences='True',return_state=True)\n    decoder_outputs,decoder_state_lstm_h,decoder_state_lstm_c=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n#     decoderoutputs,_,_=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n    \n    decoder_dense_cell=Dense(16,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(1,activation='sigmoid')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    return model\n    \n    \nmodel=seq2seq_encoder_decoder_glove_bilstm_hybrid_attention(maxlen,max_features,embedding_matrix)  \nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nplot_model(\n    model,to_file=\"seq2seq_encoder_decoder_model_glove_bilstm_hybrid.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n    \nmodel.fit([train_x,train_x],train_y,batch_size=512,epochs=2,verbose=2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bahdanau Attention\n\n\n\n<img src=\"https://miro.medium.com/max/639/1*qhOlQHLdtfZORIXYuoCtaA.png\">\n\nBahdanau et al. proposed an attention mechanism that learns to align and translate jointly. It is also known as Additive attention as it performs a linear combination of encoder states and the decoder states.\n\nlet’s understand the Attention mechanism suggested by Bahdanau\n\n- All hidden states of the encoder(forward and backward) and the decoder are used to generate the context vector, unlike how just the last encoder hidden state is used in seq2seq without attention.\n- The attention mechanism aligns the input and output sequences, with an alignment score parameterized by a feed-forward network. It helps to pay attention to the most relevant information in the source sequence.\n- The model predicts a target word based on the context vectors associated with the source position and the previously generated target words.\n\n\n### Alignment Score\n\nThe alignment score maps how well the inputs around position “j” and the output at position “i” match. The score is based on the previous decoder’s hidden state, s₍ᵢ₋₁₎ just before predicting the target word and the hidden state, hⱼ of the input sentence.\n\n\n<img src=\"https://miro.medium.com/max/535/1*u2YdTRPjN34Fpr-zxvoJsg.png\">\n\n\nThe decoder decides which part of the source sentence it needs to pay attention to, instead of having encoder encode all the information of the source sentence into a fixed-length vector.\nThe alignment vector that has the same length with the source sequence and is computed at every time step of the decode.\n\n\n### Attention Weights\n\n\nWe apply a softmax activation function to the alignment scores to obtain the attention weights.\n\n<img src=\"https://miro.medium.com/max/685/1*3aCyU9aSVHvxzOwvQdExdQ.png\">\n \n\nSome resources:\n\n\n- [Paper](https://arxiv.org/abs/1409.0473)\n- [Lilian's Blog](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#:~:text=Self%2Dattention%2C%20also%20known%20as,summarization%2C%20or%20image%20description%20generation.)\n- [Nice Blog](https://towardsdatascience.com/sequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a)\n- [Blog](https://medium.com/analytics-vidhya/neural-machine-translation-using-bahdanau-attention-mechanism-d496c9be30c3)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import MiniAttention.MiniAttention as ma\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n\n\nclass Simple_Attention(tf.keras.layers.Layer):\n    def __init__(self,units):\n        super(Simple_Attention,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,v):\n        self.q=q\n        self.v=v\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n        score=self.Wv(tf.nn.tanh(self.Wq(self.q)+self.Wk(self.v)))\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n#         context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n    \n\n        \ndef seq2seq_encoder_decoder_glove_bilstm_hybrid_bahdanau(maxlen,max_features,embedding_matrix):\n    #Creating LSTM  encoder neural model with no pretrained embeddings\n    encoder_inp=Input(shape=(maxlen,))\n    encoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(encoder_inp)\n#     encoder_embed_attention=ma.MiniAttentionBlock(None,None,None,keras.regularizers.L2(l2=0.02),None,None,None,None,None)(encoder_embed)\n    encoder_lstm_cell=LSTM(60,return_state='True')\n    encoder_outputs,encoder_state_flstm_h,encoder_state_flstm_c=encoder_lstm_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_state_flstm_h,encoder_state_flstm_c]\n    \n    #Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(decoder_inp)\n    bahdanau_attention=Simple_Attention(60)\n    \n    decoder_embed_attention_h,decoder_embed_wghts_h=bahdanau_attention(encoder_state_flstm_h,encoder_outputs)\n    decoder_embed_attention_c,decoder_embed_wghts_c=bahdanau_attention(encoder_state_flstm_c,encoder_outputs)\n#     print(decoder_embed_wghts)\n    decoder_lstm_cell=LSTM(60,return_sequences='True',return_state=True)\n    decoder_outputs,decoder_state_lstm_h,decoder_state_lstm_c=decoder_lstm_cell(decoder_embed,initial_state=[decoder_embed_wghts_h,decoder_embed_wghts_c])\n#     decoderoutputs,_,_=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n    \n    decoder_dense_cell=Dense(16,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(1,activation='sigmoid')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    return model\nmodel=seq2seq_encoder_decoder_glove_bilstm_hybrid_bahdanau(maxlen,max_features,embedding_matrix)  \nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nplot_model(\n    model,to_file=\"seq2seq_encoder_decoder_model_glove_bilstm_bahdanau_attention.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n\nmodel.fit([train_x,train_x],train_y,batch_size=512,epochs=2,verbose=2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoder Decoder Model With Bahdanau Attention\n\nThe model architecture is as follows:\n\n<img src=\"https://i.imgur.com/lIMEt59.png\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import MiniAttention.MiniAttention as ma\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n\n\nclass Simple_Attention(tf.keras.layers.Layer):\n    def __init__(self,units):\n        super(Simple_Attention,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,v):\n        self.q=q\n        self.v=v\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n        score=self.Wv(tf.nn.tanh(self.Wq(self.q)+self.Wk(self.v)))\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n#         context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n    \n\n        \ndef seq2seq_encoder_decoder_glove_bilstm_hybrid_bahdanau(maxlen,max_features,embedding_matrix):\n    #Creating LSTM  encoder neural model with no pretrained embeddings\n    encoder_inp=Input(shape=(maxlen,))\n    encoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(encoder_inp)\n#     encoder_embed_attention=ma.MiniAttentionBlock(None,None,None,keras.regularizers.L2(l2=0.02),None,None,None,None,None)(encoder_embed)\n    encoder_lstm_cell=GRU(60,return_state='True')\n    encoder_outputs,encoder_state_flstm_h=encoder_lstm_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_state_flstm_h]\n    \n    #Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(decoder_inp)\n    bahdanau_attention=Simple_Attention(60)\n    \n    decoder_embed_attention_h,decoder_embed_wghts_h=bahdanau_attention(encoder_state_flstm_h,encoder_outputs)\n#     decoder_embed_attention_c,decoder_embed_wghts_c=bahdanau_attention(encoder_state_flstm_c,encoder_outputs)\n#     print(decoder_embed_wghts)\n    decoder_lstm_cell=GRU(60,return_sequences='True',return_state=True)\n    decoder_outputs,decoder_state_lstm_h=decoder_lstm_cell(decoder_embed,initial_state=[decoder_embed_wghts_h])\n#     decoderoutputs,_,_=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n    \n    decoder_dense_cell=Dense(16,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(1,activation='sigmoid')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    return model\nmodel=seq2seq_encoder_decoder_glove_bilstm_hybrid_bahdanau(maxlen,max_features,embedding_matrix)  \nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nplot_model(\n    model,to_file=\"seq2seq_encoder_decoder_model_glove_bilstm_bahdanau_attention.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n\nmodel.fit([train_x,train_x],train_y,batch_size=512,epochs=2,verbose=2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoder Decoder With Luong Attention\n\nIn this case, we are  going to replicate the training process with [Luong Dot Product (Multiplicative Style) Attention Mechanism](https://arxiv.org/abs/1508.04025).\n\n\n### Global Attention\n\n\n<img  src=\"https://miro.medium.com/max/626/1*LhEapXF1mtaB3rDgIjcceg.png\">\n\n\nLuong, et al., 2015 proposed the “global” and “local” attention. The global attention is similar to the soft attention, while the local one is an interesting blend between hard and soft, an improvement over the hard attention to make it differentiable: the model first predicts a single aligned position for the current target word and a window centered around the source position is then used to compute a context vector.\n\nThe commonality between Global and Local attention\n\n- At each time step t, in the decoding phase, both approaches, global and local attention, first take the hidden state hₜ at the top layer of a stacking LSTM as an input.\n- The goal of both approaches is to derive a context vector 𝒸ₜ to capture relevant source-side information to help predict the current target word yₜ\n- Attentional vectors are fed as inputs to the next time steps to inform the model about past alignment decisions.\n- Global and local attention models differ in how the context vector 𝒸ₜ is derived\n- Before we discuss the global and local attention, let’s understand the conventions used by Luong’s attention mechanism for any given time t\n  - 𝒸ₜ : context vector\n  - aₜ : alignment vector\n  - hₜ : current target hidden state\n  - hₛ : current source hidden state\n  - yₜ: predicted current target word\n  - h˜ₜ : Attentional vectors\n\n- The global attentional model considers all the hidden states of the encoder when calculating the context vector 𝒸ₜ.\n- A variable-length alignment vector aₜ equal to the size of the number of time steps in the source sequence is derived by comparing the current target hidden state hₜ with each of the source hidden state hₛ\n- The alignment score is referred to as a content-based function for which we consider three different alternatives\n\n\n### Local Attention\n\n\n<img src=\"https://miro.medium.com/max/538/1*YXjdGl3CnSfHfzYpQiObgg.png\">\n\n\n- Local attention only focuses on a small subset of source positions per target words unlike the entire source sequence as in global attention\n- Computationally less expensive than global attention\n- The local attention model first generates an aligned position Pₜ for each target word at time t.\n- The context vector 𝒸ₜ is derived as a weighted average over the set of source hidden states within selected the window\n- The aligned position can be monotonically or predictively selected\n\n\n### Formula \n\n<img src=\"https://miro.medium.com/max/875/1*_Ta67S8_lXTbVzJMztkxKg.png\">\n\n\nSome resources:\n\n- [Lilan's Blog](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#:~:text=Self%2Dattention%2C%20also%20known%20as,summarization%2C%20or%20image%20description%20generation.)\n- [Paper](https://arxiv.org/pdf/1508.04025.pdf)\n- [Paper](https://arxiv.org/pdf/1508.04025.pdf)\n- [Paper](https://arxiv.org/pdf/1508.4025.pdf)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import MiniAttention.MiniAttention as ma\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n\n\n\nclass Luong_Attention(tf.keras.layers.Layer):\n    def __init__(self,units):\n        super(Luong_Attention,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,v):\n        self.q=q\n        self.v=v\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n#         self.q=tf.transpose(self.q)\n        score=(self.q)*(self.v)\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n#         context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n    \n\n        \ndef seq2seq_encoder_decoder_glove_bilstm_hybrid_luong(maxlen,max_features,embedding_matrix):\n    #Creating LSTM  encoder neural model with no pretrained embeddings\n    encoder_inp=Input(shape=(maxlen,))\n    encoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(encoder_inp)\n#     encoder_embed_attention=ma.MiniAttentionBlock(None,None,None,keras.regularizers.L2(l2=0.02),None,None,None,None,None)(encoder_embed)\n    encoder_lstm_cell=LSTM(60,return_state='True')\n    encoder_outputs,encoder_state_flstm_h,encoder_state_flstm_c=encoder_lstm_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_state_flstm_h,encoder_state_flstm_c]\n    \n    #Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(decoder_inp)\n    luong_attention=Luong_Attention(128)\n    \n    decoder_embed_attention_h,decoder_embed_wghts_h=luong_attention(encoder_state_flstm_h,encoder_outputs)\n    decoder_embed_attention_c,decoder_embed_wghts_c=luong_attention(encoder_state_flstm_c,encoder_outputs)\n#     print(decoder_embed_wghts)\n    decoder_lstm_cell=LSTM(60,return_sequences='True',return_state=True)\n    decoder_outputs,decoder_state_lstm_h,decoder_state_lstm_c=decoder_lstm_cell(decoder_embed,initial_state=[decoder_embed_wghts_h,decoder_embed_wghts_c])\n#     decoderoutputs,_,_=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n    \n    decoder_dense_cell=Dense(16,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(1,activation='sigmoid')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    return model\nmodel=seq2seq_encoder_decoder_glove_bilstm_hybrid_luong(maxlen,max_features,embedding_matrix)  \nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nplot_model(\n    model,to_file=\"seq2seq_encoder_decoder_model_glove_bilstm_luong_attention.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n\nmodel.fit([train_x,train_x],train_y,batch_size=512,epochs=2,verbose=2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoder Decoder Model Architecture with Luong Attention\n\nThe architecture is as follows:\n\n<img src=\"https://i.imgur.com/ZAJ2iTH.png\">"},{"metadata":{},"cell_type":"markdown","source":"## Graves Cosine Attention\n\nHere we apply ,cosine transformation on the Dot product Attention.\n\n<img src=\"https://theaisummer.com/assets/img/posts/attention/attention-calculation.png\">\n\n\nArchitecture\n\n<img src=\"https://miro.medium.com/max/2048/0*hMbmU5-BjN-i6mZh.jpg\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"import MiniAttention.MiniAttention as ma\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n\n\n\nclass Graves_Attention(tf.keras.layers.Layer):\n    def __init__(self,units):\n        super(Graves_Attention,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,v):\n        self.q=q\n        self.v=v\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n#         self.q=tf.transpose(self.q)\n        score=tf.math.cos((self.q)*(self.v))\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n#         context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n    \n\n        \ndef seq2seq_encoder_decoder_glove_bilstm_hybrid_graves(maxlen,max_features,embedding_matrix):\n    #Creating LSTM  encoder neural model with no pretrained embeddings\n    encoder_inp=Input(shape=(maxlen,))\n    encoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(encoder_inp)\n#     encoder_embed_attention=ma.MiniAttentionBlock(None,None,None,keras.regularizers.L2(l2=0.02),None,None,None,None,None)(encoder_embed)\n    encoder_lstm_cell=LSTM(60,return_state='True')\n    encoder_outputs,encoder_state_flstm_h,encoder_state_flstm_c=encoder_lstm_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_state_flstm_h,encoder_state_flstm_c]\n    \n    #Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(decoder_inp)\n    graves_attention=Graves_Attention(128)\n    \n    decoder_embed_attention_h,decoder_embed_wghts_h=graves_attention(encoder_state_flstm_h,encoder_outputs)\n    decoder_embed_attention_c,decoder_embed_wghts_c=graves_attention(encoder_state_flstm_c,encoder_outputs)\n#     print(decoder_embed_wghts)\n    decoder_lstm_cell=LSTM(60,return_sequences='True',return_state=True)\n    decoder_outputs,decoder_state_lstm_h,decoder_state_lstm_c=decoder_lstm_cell(decoder_embed,initial_state=[decoder_embed_wghts_h,decoder_embed_wghts_c])\n#     decoderoutputs,_,_=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n    \n    decoder_dense_cell=Dense(16,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(1,activation='sigmoid')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    return model\nmodel=seq2seq_encoder_decoder_glove_bilstm_hybrid_graves(maxlen,max_features,embedding_matrix)  \nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nplot_model(\n    model,to_file=\"seq2seq_encoder_decoder_model_glove_bilstm_graves_attention.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n\nmodel.fit([train_x,train_x],train_y,batch_size=512,epochs=2,verbose=2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoder Decoder Model Architecture with Graves Attention\n\nThe architecture is as follows:\n\n<img src=\"https://i.imgur.com/2AInGw0.png\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"import MiniAttention.MiniAttention as ma\nimport math\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n\n\n\nclass Scaled_Dot_Product_Attention(tf.keras.layers.Layer):\n    def __init__(self,units):\n        super(Scaled_Dot_Product_Attention,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,v,n):\n        self.q=q\n        self.v=v\n        self.n=n\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n#         self.q=tf.transpose(self.q)\n        score=((self.q)*(self.v))/math.sqrt(self.n)\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n#         context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n    \n\n        \ndef seq2seq_encoder_decoder_glove_bilstm_hybrid_scaled_dot_product(maxlen,max_features,embedding_matrix):\n    #Creating LSTM  encoder neural model with no pretrained embeddings\n    encoder_inp=Input(shape=(maxlen,))\n    encoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(encoder_inp)\n#     encoder_embed_attention=ma.MiniAttentionBlock(None,None,None,keras.regularizers.L2(l2=0.02),None,None,None,None,None)(encoder_embed)\n    encoder_lstm_cell=LSTM(60,return_state='True')\n    encoder_outputs,encoder_state_flstm_h,encoder_state_flstm_c=encoder_lstm_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_state_flstm_h,encoder_state_flstm_c]\n    \n    #Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(decoder_inp)\n    scaled_dot_product_attention=Scaled_Dot_Product_Attention(128)\n    \n    decoder_embed_attention_h,decoder_embed_wghts_h=scaled_dot_product_attention(encoder_state_flstm_h,encoder_outputs,64)\n    decoder_embed_attention_c,decoder_embed_wghts_c=scaled_dot_product_attention(encoder_state_flstm_c,encoder_outputs,64)\n#     print(decoder_embed_wghts)\n    decoder_lstm_cell=LSTM(60,return_sequences='True',return_state=True)\n    decoder_outputs,decoder_state_lstm_h,decoder_state_lstm_c=decoder_lstm_cell(decoder_embed,initial_state=[decoder_embed_wghts_h,decoder_embed_wghts_c])\n#     decoderoutputs,_,_=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n    \n    decoder_dense_cell=Dense(16,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(1,activation='sigmoid')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    return model\nmodel=seq2seq_encoder_decoder_glove_bilstm_hybrid_scaled_dot_product(maxlen,max_features,embedding_matrix)  \nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nplot_model(\n    model,to_file=\"seq2seq_encoder_decoder_model_glove_bilstm_scaled_dot_product_attention.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n\nmodel.fit([train_x,train_x],train_y,batch_size=512,epochs=2,verbose=2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoder Decoder Model Architecture with Scaled Dot Product Attention\n\nThe architecture is as follows:\n\n<img src=\"https://i.imgur.com/pBnshbv.png\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"import MiniAttention.MiniAttention as ma\nimport math\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n\n\n\nclass Scaled_Dot_Product_Self_Attention(tf.keras.layers.Layer):\n    def __init__(self,units):\n        super(Scaled_Dot_Product_Self_Attention,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,k,v,n):\n        self.q=q\n        self.v=v\n        self.n=n\n        self.k=k\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n#         self.q=tf.transpose(self.q)\n        score=(self.Wq(self.q)*self.Wk(self.k))/math.sqrt(n)\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n        context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n    \n\n        \ndef seq2seq_encoder_decoder_glove_bilstm_hybrid_scaled_dot_product_self(maxlen,max_features,embedding_matrix):\n    #Creating LSTM  encoder neural model with no pretrained embeddings\n    encoder_inp=Input(shape=(maxlen,))\n    print(embedding_matrix.shape)\n    encoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(encoder_inp)\n#     encoder_embed_attention=ma.MiniAttentionBlock(None,None,None,keras.regularizers.L2(l2=0.02),None,None,None,None,None)(encoder_embed)\n    encoder_lstm_cell=LSTM(60,return_state='True')\n    encoder_outputs,encoder_state_flstm_h,encoder_state_flstm_c=encoder_lstm_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_state_flstm_h,encoder_state_flstm_c]\n    \n    #Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(decoder_inp)\n    scaled_dot_product_attention=Scaled_Dot_Product_Self_Attention(60)\n    \n    decoder_embed_attention_h,decoder_embed_wghts_h=scaled_dot_product_attention(encoder_state_flstm_h,encoder_state_flstm_h,encoder_outputs,64)\n    decoder_embed_attention_c,decoder_embed_wghts_c=scaled_dot_product_attention(encoder_state_flstm_c,encoder_state_flstm_c,encoder_outputs,64)\n#     print(decoder_embed_wghts)\n    decoder_lstm_cell=LSTM(60,return_sequences='True',return_state=True)\n    decoder_outputs,decoder_state_lstm_h,decoder_state_lstm_c=decoder_lstm_cell(decoder_embed,initial_state=[decoder_embed_wghts_h,decoder_embed_wghts_c])\n#     decoderoutputs,_,_=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n    \n    decoder_dense_cell=Dense(16,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(1,activation='sigmoid')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    return model\nmodel=seq2seq_encoder_decoder_glove_bilstm_hybrid_scaled_dot_product_self(maxlen,max_features,embedding_matrix)  \nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nplot_model(\n    model,to_file=\"seq2seq_encoder_decoder_model_glove_bilstm_scaled_dot_self_product_attention.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n\nmodel.fit([train_x,train_x],train_y,batch_size=512,epochs=2,verbose=2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoder Decoder Model Architecture with Self Attention\n\nThe architecture is as follows:\n\n<img src=\"https://i.imgur.com/3sdKFMW.png\">"},{"metadata":{},"cell_type":"markdown","source":"## Self Attention \n\n\nThis is produced from [Google-research](http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf).Self-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing. [Jay's Blog](http://jalammar.github.io/illustrated-transformer/) provide a very good idea of this logic.\n\n<img src=\"http://jalammar.github.io/images/t/transformer_self-attention_visualization.png\">\n\n\nThree vectors q,k and v (query,key and value) are taken into consideration for computation of the self attention mechanism.The q,k and v are normally of 64 dimensions.\n\n\n<img src=\"http://jalammar.github.io/images/t/transformer_self_attention_vectors.png\">\n\n\nThe score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring. So if we’re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.\n\n\n<img  src=\"http://jalammar.github.io/images/t/transformer_self_attention_score.png\">\n\n\nThe third and forth steps are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper – 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1.\n\n\n<img src=\"http://jalammar.github.io/images/t/self-attention_softmax.png\">\n\n\n\nThis softmax score determines how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it’s useful to attend to another word that is relevant to the current word.\nThe fifth step is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).\nThe sixth step is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).\n\n\n<img  src=\"http://jalammar.github.io/images/t/self-attention-output.png\">\n\nThe cumulative computation can be thought of like this:\n\n<img src=\"http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\">\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import MiniAttention.MiniAttention as ma\nimport math\nimport transformers\nfrom transformers import AutoTokenizer,AutoModelForQuestionAnswering\n\nmaxlen=1000\nembed_size=768\nmax_features=1000\ntrain_df=train_df[:1000]\nlabels=label_y.fit_transform(train_df['sentiment'])\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\ndef build_model(transformer, max_len=maxlen):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    #Replaced from the Embedding+LSTM/CoNN layers\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    return cls_token,sequence_output\n    \n\n\nclass Scaled_Dot_Product_Self_Attention(tf.keras.layers.Layer):\n    def __init__(self,units):\n        super(Scaled_Dot_Product_Self_Attention,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,k,v,n):\n        self.q=q\n        self.v=v\n        self.n=n\n        self.k=k\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n#         self.q=tf.transpose(self.q)\n        score=(self.Wq(self.q)*self.Wk(self.k))/math.sqrt(n)\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n        context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n    \n\ndef chunks(l, n):\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n    for i in range(0, len(l), n):\n        yield l[i:i + n]\n        \n        \ndef fetch_vectors(string_list,pretrained_model, batch_size=64):\n    # inspired by https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n    model = transformers.TFDistilBertModel.from_pretrained(pretrained_model)\n    \n    fin_features = []\n    for data in chunks(string_list, batch_size):\n        tokenized = []\n        for x in data:\n            x = \" \".join(x.strip().split()[:300])\n            tok = tokenizer.encode(x, add_special_tokens=True)\n            tokenized.append(tok[:512])\n\n        max_len = 512\n        #bert variants have attention id, input id and segment id\n        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n        attention_mask = np.where(padded != 0, 1, 0)\n        input_ids = tf.convert_to_tensor(padded)\n        attention_mask = tf.convert_to_tensor(attention_mask)\n\n        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n\n        features = last_hidden_states[0][:, 0, :].cpu().numpy()\n        fin_features.append(features)\n\n    fin_features = np.vstack(fin_features)\n    return fin_features\n\n        \ndef distilbert_encoder_decoder_attention(maxlen,max_features,distilbert_embeddings):\n    #Creating LSTM  encoder neural model with no pretrained embeddings\n    encoder_inp=Input(shape=(maxlen,))\n    encoder_embed=Embedding(distilbert_embeddings.shape[0],embed_size,weights=[distilbert_embeddings])(encoder_inp)\n    print(encoder_inp.shape)\n#     encoder_embed_attention=ma.MiniAttentionBlock(None,None,None,keras.regularizers.L2(l2=0.02),None,None,None,None,None)(encoder_embed)\n    encoder_lstm_cell=LSTM(60,return_state='True')\n    encoder_outputs,encoder_state_flstm_h,encoder_state_flstm_c=encoder_lstm_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_state_flstm_h,encoder_state_flstm_c]\n    \n    #Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(distilbert_embeddings.shape[0],embed_size,weights=[distilbert_embeddings])(decoder_inp)\n    scaled_dot_product_attention=Scaled_Dot_Product_Self_Attention(60)\n    \n    decoder_embed_attention_h,decoder_embed_wghts_h=scaled_dot_product_attention(encoder_state_flstm_h,encoder_state_flstm_h,encoder_outputs,64)\n    decoder_embed_attention_c,decoder_embed_wghts_c=scaled_dot_product_attention(encoder_state_flstm_c,encoder_state_flstm_c,encoder_outputs,64)\n#     print(decoder_embed_wghts)\n    decoder_lstm_cell=LSTM(60,return_sequences='True',return_state=True)\n    decoder_outputs,decoder_state_lstm_h,decoder_state_lstm_c=decoder_lstm_cell(decoder_embed,initial_state=[decoder_embed_wghts_h,decoder_embed_wghts_c])\n#     decoderoutputs,_,_=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n    \n    decoder_dense_cell=Dense(16,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(1,activation='sigmoid')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    return model\n\n\n\n# transformer_layer = (\n#         transformers.TFDistilBertModel\n#         .from_pretrained('distilbert-base-multilingual-cased'))\n# embedding_matrix,embedding_vector=build_model(transformer_layer,maxlen)\n# embedding_matrix=tf.keras.layers.Reshape((maxlen, 768))(embedding_matrix)\ndistilbert_embeddings = fetch_vectors(train_df.review.values,'distilbert-base-uncased')\n\nmodel=distilbert_encoder_decoder_attention(maxlen,max_features,distilbert_embeddings)  \nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nplot_model(\n    model,to_file=\"distilbert_encoder_decoder_attention.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n\nmodel.fit([train_x,train_x],train_y,batch_size=512,epochs=2,verbose=2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion on Attention\n\nWe have seen different flavours of attention mechanism and in the next section, we move forward to transformers.\n\n\n<img src=\"https://i.pinimg.com/originals/4a/2e/2b/4a2e2b7a3aabd03daebf11d1a2e970cc.gif\">"},{"metadata":{},"cell_type":"markdown","source":"## Enter Transformers\n\n\n### Multi Head Self Scaled Dot Product Attention\n\n\nThis improves the performance of the attention layer in two ways:\n\n- It expands the model’s ability to focus on different positions. Yes, in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the the actual word itself. It would be useful if we’re translating a sentence like “The animal didn’t cross the street because it was too tired”, we would want to know which word “it” refers to.\n\n- It gives the attention layer multiple “representation subspaces”. As we’ll see next, with multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.\n\n\n<img src=\"http://jalammar.github.io/images/t/transformer_attention_heads_qkv.png\">\n\n\nThe entire modelling of multi head attention can be summed up in this image:\n\n\n<img src=\"http://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png\">\n\n\n### Combining Multi HeadAttention with Encoder Decoders With Layer Normalization\n\n\nIn the classic encoder-decoder model, we will add the Multi head attention mechanism along with some modifications. The Encoder  contains a self attention head along with a Addition and Layer Normalization Layer. [Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf) tries to apply normalization (mean and variance)on the cumulative hidden units present in a particular layer rather than minibatch sampling (as in the case of batch normalization). The \"covariate shift\" issue can be resolved to an extent using this normalization technique.\n\n<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTJyynaajlr9lye4s0p28jOsE2VLZQ1R3l9rw&usqp=CAU\">\n\n\n#### Encoder\n\n\nThe encoder model with this modification appears as follows:\n\n\n<img src=\"http://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png\">\n\n \nThe classic transformer consists of 8 stacked encoder decoder units with self attention(multihead) and layer normalization ,along with FFNN Dense Network inside each of them.This leads to a more robust architecture as compared to standard Encoder Decoder models.\n\n\n<img src=\"http://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png\">\n\n\n#### Decoder\n\n\nNow that we’ve covered most of the concepts on the encoder side, we basically know how the components of decoders work as well. But let’s take a look at how they work together.\n\nThe encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence:\n\n\n<img src=\"http://jalammar.github.io/images/t/transformer_decoding_1.gif\">\n\n\nThe following steps repeat the process until a special symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.\n\n\n<img src=\"http://jalammar.github.io/images/t/transformer_decoding_2.gif\">\n\n\n#### Masking\n\n\nThe self attention layers in the decoder operate in a slightly different way than the one in the encoder:\n\nIn the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation.\n\nThe “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.\n\n\n### Final Linear and Softmax Activation\n\n\nThis is similar to the softmax activated output of the final FFNN layer.The decoder stack outputs a vector of floats. How do we turn that into a word? That’s the job of the final Linear layer which is followed by a Softmax Layer.\n\nThe Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.\n\nLet’s assume that our model knows 10,000 unique English words (our model’s “output vocabulary”) that it’s learned from its training dataset. This would make the logits vector 10,000 cells wide – each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer.\n\nThe softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.\n\n\n<img src=\"http://jalammar.github.io/images/t/transformer_decoder_output_softmax.png\">\n\n\nSome resources:\n\n- [Research](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)\n- [Repo](https://github.com/tensorflow/tensor2tensor)\n- [Jupyter](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb)\n- [Talk](https://www.youtube.com/watch?v=rBCqOTEfxvg)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Taken from Google-research implementation of transformer\n\nimport random, os, sys\nimport numpy as np\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.callbacks import *\nfrom keras.initializers import *\nimport tensorflow as tf\nfrom keras.engine.topology import Layer\nimport keras.backend as K\ntry:\n    from dataloader import TokenList, pad_to_longest\n    # for transformer\nexcept: pass\n\n#Layer normalization class\nclass LayerNormalization(Layer):\n    def __init__(self, eps=1e-6, **kwargs):\n        self.eps = eps\n        super(LayerNormalization, self).__init__(**kwargs)\n    def build(self, input_shape):\n        #Adding custom weights\n        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n                                     initializer=Ones(), trainable=True)\n        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n                                    initializer=Zeros(), trainable=True)\n        super(LayerNormalization, self).build(input_shape)\n    def call(self, x):\n        mean = K.mean(x, axis=-1, keepdims=True)\n        std = K.std(x, axis=-1, keepdims=True)\n        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n#Division by 8 (q.k/d^0.5)\nclass ScaledDotProductAttention():\n    def __init__(self, d_model, attn_dropout=0.1):\n        self.temper = np.sqrt(d_model)\n        self.dropout = Dropout(attn_dropout)\n    def __call__(self, q, k, v, mask):\n        attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])\n        if mask is not None:\n            mmask = Lambda(lambda x:(-1e+10)*(1-x))(mask)\n            attn = Add()([attn, mmask])\n        attn = Activation('softmax')(attn)\n        attn = self.dropout(attn)\n        output = Lambda(lambda x:K.batch_dot(x[0], x[1]))([attn, v])\n        return output, attn\n\nclass MultiHeadAttention():\n    # mode 0 - big martixes, faster; mode 1 - more clear implementation\n    def __init__(self, n_head, d_model, d_k, d_v, dropout, mode=0, use_norm=True):\n        self.mode = mode\n        self.n_head = n_head\n        self.d_k = d_k\n        self.d_v = d_v\n        self.dropout = dropout\n        if mode == 0:\n            self.qs_layer = Dense(n_head*d_k, use_bias=False)\n            self.ks_layer = Dense(n_head*d_k, use_bias=False)\n            self.vs_layer = Dense(n_head*d_v, use_bias=False)\n        elif mode == 1:\n            self.qs_layers = []\n            self.ks_layers = []\n            self.vs_layers = []\n            for _ in range(n_head):\n                self.qs_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n                self.ks_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n                self.vs_layers.append(TimeDistributed(Dense(d_v, use_bias=False)))\n        #Joining scaled dot product\n        self.attention = ScaledDotProductAttention(d_model)\n        self.layer_norm = LayerNormalization() if use_norm else None\n        self.w_o = TimeDistributed(Dense(d_model))\n\n    def __call__(self, q, k, v, mask=None):\n        d_k, d_v = self.d_k, self.d_v\n        n_head = self.n_head\n\n        if self.mode == 0:\n            qs = self.qs_layer(q)  # [batch_size, len_q, n_head*d_k]\n            ks = self.ks_layer(k)\n            vs = self.vs_layer(v)\n\n            def reshape1(x):\n                s = tf.shape(x)   # [batch_size, len_q, n_head * d_k]\n                x = tf.reshape(x, [s[0], s[1], n_head, d_k])\n                x = tf.transpose(x, [2, 0, 1, 3])  \n                x = tf.reshape(x, [-1, s[1], d_k])  # [n_head * batch_size, len_q, d_k]\n                return x\n            qs = Lambda(reshape1)(qs)\n            ks = Lambda(reshape1)(ks)\n            vs = Lambda(reshape1)(vs)\n\n            if mask is not None:\n                mask = Lambda(lambda x:K.repeat_elements(x, n_head, 0))(mask)\n            head, attn = self.attention(qs, ks, vs, mask=mask)  \n                \n            def reshape2(x):\n                s = tf.shape(x)   # [n_head * batch_size, len_v, d_v]\n                x = tf.reshape(x, [n_head, -1, s[1], s[2]]) \n                x = tf.transpose(x, [1, 2, 0, 3])\n                x = tf.reshape(x, [-1, s[1], n_head*d_v])  # [batch_size, len_v, n_head * d_v]\n                return x\n            head = Lambda(reshape2)(head)\n        elif self.mode == 1:\n            heads = []; attns = []\n            for i in range(n_head):\n                qs = self.qs_layers[i](q)   \n                ks = self.ks_layers[i](k) \n                vs = self.vs_layers[i](v) \n                head, attn = self.attention(qs, ks, vs, mask)\n                heads.append(head); attns.append(attn)\n            head = Concatenate()(heads) if n_head > 1 else heads[0]\n            attn = Concatenate()(attns) if n_head > 1 else attns[0]\n\n        outputs = self.w_o(head)\n        outputs = Dropout(self.dropout)(outputs)\n        if not self.layer_norm: return outputs, attn\n        outputs = Add()([outputs, q])\n        return self.layer_norm(outputs), attn\n#Feedforward layer using COnv1D and Layer normalization.\nclass PositionwiseFeedForward():\n    def __init__(self, d_hid, d_inner_hid, dropout=0.1):\n        self.w_1 = Conv1D(d_inner_hid, 1, activation='relu')\n        self.w_2 = Conv1D(d_hid, 1)\n        self.layer_norm = LayerNormalization()\n        self.dropout = Dropout(dropout)\n    def __call__(self, x):\n        output = self.w_1(x) \n        output = self.w_2(output)\n        output = self.dropout(output)\n        output = Add()([output, x])\n        return self.layer_norm(output)\n#Encoder layer containing self/multi head attention with positionwisefeedforward\nclass EncoderLayer():\n    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):\n        self.self_att_layer = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n        self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)\n    def __call__(self, enc_input, mask=None):\n        output, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)\n        output = self.pos_ffn_layer(output)\n        return output, slf_attn\n#Decoder layer with same architecture as the encoder.\nclass DecoderLayer():\n    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):\n        self.self_att_layer = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n        self.enc_att_layer  = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n        self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)\n    def __call__(self, dec_input, enc_output, self_mask=None, enc_mask=None):\n        output, slf_attn = self.self_att_layer(dec_input, dec_input, dec_input, mask=self_mask)\n        output, enc_attn = self.enc_att_layer(output, enc_output, enc_output, mask=enc_mask)\n        output = self.pos_ffn_layer(output)\n        return output, slf_attn, enc_attn\n#This is from the paper \"Attention is all you need\" which hypothesizes sin and cosine for positional encoding\ndef GetPosEncodingMatrix(max_len, d_emb):\n    pos_enc = np.array([\n        [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)] \n        if pos != 0 else np.zeros(d_emb) \n            for pos in range(max_len)\n            ])\n    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2]) # dim 2i\n    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2]) # dim 2i+1\n    return pos_enc\n\n#normal padding class for masking\ndef GetPadMask(q, k):\n    ones = K.expand_dims(K.ones_like(q, 'float32'), -1)\n    mask = K.cast(K.expand_dims(K.not_equal(k, 0), 1), 'float32')\n    mask = K.batch_dot(ones, mask, axes=[2,1])\n    return mask\n\ndef GetSubMask(s):\n    len_s = tf.shape(s)[1]\n    bs = tf.shape(s)[:1]\n    mask = K.cumsum(tf.eye(len_s, batch_shape=bs), 1)\n    return mask\n\nclass Encoder():\n    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, \\\n                layers=6, dropout=0.1, word_emb=None, pos_emb=None):\n        self.emb_layer = word_emb\n        self.pos_layer = pos_emb\n        self.emb_dropout = Dropout(dropout)\n        self.layers = [EncoderLayer(d_model, d_inner_hid, n_head, d_k, d_v, dropout) for _ in range(layers)]\n        \n    def __call__(self, src_seq, src_pos, return_att=False, active_layers=999):\n        x = self.emb_layer(src_seq)\n        if src_pos is not None:\n            pos = self.pos_layer(src_pos)\n            x = Add()([x, pos])\n        x = self.emb_dropout(x)\n        if return_att: atts = []\n        mask = Lambda(lambda x:GetPadMask(x, x))(src_seq)\n        for enc_layer in self.layers[:active_layers]:\n            x, att = enc_layer(x, mask)\n            if return_att: atts.append(att)\n        return (x, atts) if return_att else x\n\n\nclass Transformer():\n    def __init__(self, len_limit, d_model=embed_size, \\\n              d_inner_hid=512, n_head=10, d_k=64, d_v=64, layers=2, dropout=0.1, \\\n              share_word_emb=False, **kwargs):\n        self.name = 'Transformer'\n        self.len_limit = len_limit\n        self.src_loc_info = True\n        self.d_model = d_model\n        self.decode_model = None\n        d_emb = d_model\n\n        pos_emb = Embedding(len_limit, d_emb, trainable=False, \\\n                            weights=[GetPosEncodingMatrix(len_limit, d_emb)])\n\n        i_word_emb = Embedding(max_features, d_emb, weights=[embedding_matrix]) # Add Kaggle provided embedding here\n\n        self.encoder = Encoder(d_model, d_inner_hid, n_head, d_k, d_v, layers, dropout, \\\n                               word_emb=i_word_emb, pos_emb=pos_emb)\n\n        \n    def get_pos_seq(self, x):\n        mask = K.cast(K.not_equal(x, 0), 'int32')\n        pos = K.cumsum(K.ones_like(x, 'int32'), 1)\n        return pos * mask\n\n    def compile(self, active_layers=999):\n        src_seq_input = Input(shape=(None,))\n        src_seq = src_seq_input\n        src_pos = Lambda(self.get_pos_seq)(src_seq)\n        if not self.src_loc_info: src_pos = None\n\n        x = self.encoder(src_seq, src_pos, active_layers=active_layers)\n        # x = GlobalMaxPool1D()(x) # Not sure about this layer. Just wanted to reduce dimension\n        x = GlobalAveragePooling1D()(x)\n        outp = Dense(1, activation=\"sigmoid\")(x)\n\n        self.model = Model(inputs=src_seq_input, outputs=outp)\n        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nclassic_transformer = Transformer(maxlen, layers=1)\nclassic_transformer.compile()\nmodel = classic_transformer.model\nmodel.summary()\nplot_model(\n    model,to_file=\"Classic_Transformer.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classic Transformer Architecture -MultiHead Self Attention\n\nThe following is the architecture:\n\n<img src=\"https://i.imgur.com/nImDpAf.png\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(train_x[:1000],train_y[:1000],epochs=2,verbose=2,batch_size=512)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom transformers import AutoTokenizer, pipeline, TFDistilBertModel\nfrom scipy.spatial.distance import cosine\ndef transformer_embedding(name,inp,model_name):\n\n    model = model_name.from_pretrained(name)\n    tokenizer = AutoTokenizer.from_pretrained(name)\n    pipe = pipeline('feature-extraction', model=model, \n                tokenizer=tokenizer)\n    features = pipe(inp)\n    features = np.squeeze(features)\n    return features\nembedding_features1=transformer_embedding('distilbert-base-uncased',z[0],TFDistilBertModel)\nembedding_features2=transformer_embedding('distilbert-base-uncased',z[1],TFDistilBertModel)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Huggingface Transformers\n\n\nWe will be leveraging the power of Transformers (from [Huggingface](https://huggingface.co/)) for training the corpus using any variant of transformer architecture.  Some information regarding TPU usage:\n\n\n### TPU\n\n\n<img src=\"https://storage.googleapis.com/kaggle-media/tpu/tpuv3angle.jpg\">\n\n\nIn this context, we will be using the TPU cluster from the Notebook (Hardware accelerations). TPUs provide a better performance with respect to Tensorflow and Keras computations on tensors against GPUs.But it has to be explicitly called out in the code segment.\n\n[Kaggle Documentation on TPUs provide an excellent starting point for this.Highly recommend to go through it.](https://www.kaggle.com/docs/tpu)\n\nSteps to check and run the TPU cluster:\n\n```python\ndetect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver() tf.config.experimental_connect_to_cluster(tpu) tf.tpu.experimental.initialize_tpu_system(tpu)\n\ninstantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\ninstantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope(): model = tf.keras.Sequential( … ) # define your model normally model.compile( … )\n\ntrain model normally\nmodel.fit(training_dataset, epochs=EPOCHS, steps_per_epoch=…)\n```\n\nSome points on TPUs:\n\n- TPUs are network-connected accelerators and you must first locate them on the network. This is what TPUClusterResolver() does.\n- To go fast on a TPU, increase the batch size. The rule of thumb is to use batches of 128 elements per core (ex: batch size of 128*8=1024 for a TPU with 8 cores). At this size, the 128x128 hardware matrix multipliers of the TPU (see hardware section below) are most likely to be kept busy. You start seeing interesting speedups from a batch size of 8 per core though. In the sample above, the batch size is scaled with the core count through this line of code:\n\n```python\nBATCH_SIZE = 16 * tpu_strategy.num_replicas_in_sync\n```\n\n<img src=\"https://storage.googleapis.com/kaggle-media/tpu/tpu_rule_of_thumb.png\">\n\n- With larger batch sizes, TPUs will be crunching through the training data faster. This is only useful if the larger training batches produce more “training work” and get your model to the desired accuracy faster. That is why the rule of thumb also calls for increasing the learning rate with the batch size. You can start with a proportional increase but additional tuning may be necessary to find the optimal learning rate schedule for a given model and accelerator\n\n- Because TPUs are very fast, many models ported to TPU end up with a data bottleneck. The TPU is sitting idle, waiting for data for the most part of each training epoch. TPUs read training data exclusively from GCS (Google Cloud Storage). And GCS can sustain a pretty large throughput if it is continuously streaming from multiple files in parallel. Following a couple of best practices will optimize the throughput:For TPU training, organize your data in GCS in a reasonable number (10s to 100s) of reasonably large files (10s to 100s of MB).\n\n- To enable parallel streaming from multiple TFRecord files, we can modify :\n\n   - num_parallel_reads=AUTO instructs the API to read from multiple files if available. It figures out how many automatically.\n   - experimental_deterministic = False disables data order enforcement. We will be shuffling the data anyway so order is not important. With this setting the API can use any TFRecord as soon as it is streamed in.\n   \n\n#### TPU Hardware\n\nAt approximately 20 inches (50 cm), a TPU v3-8 board is a fairly sizeable piece of hardware. It sports 4 dual-core TPU chips for a total of 8 TPU cores.Each TPU core has a traditional vector processing part (VPU) as well as dedicated matrix multiplication hardware capable of processing 128x128 matrices. This is the part that specifically accelerates machine learning workloads.\n\nTPUs are equipped with 128GB of high-speed memory allowing larger batches, larger models and also larger training inputs. In the sample above, you can try using 512x512 px input images, also provided in the dataset, and see the TPU v3-8 handle them easily.\n\n<img src=\"https://storage.googleapis.com/kaggle-media/tpu/tpu_cores_and_chips.png\">\n\nSome resources:\n\n- [Cloud TPU](https://cloud.google.com/tpu/docs/tpus)\n- [Tensorflow TPU](https://www.tensorflow.org/tfrc)\n"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://i.pinimg.com/originals/73/d3/a1/73d3a14d212314ab1f7268b71d639c15.gif\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\nfrom tqdm.notebook import tqdm\nfrom tokenizers import BertWordPieceTokenizer\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom transformers import AutoTokenizer, pipeline, TFDistilBertModel\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n#allow experimental tf\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Data access\nGCS_DS_PATH = KaggleDatasets().get_gcs_path('imdb-dataset-of-50k-movie-reviews')\n\n# Configuration of hyperparameters\nEPOCHS = 3\n#batch size denotes the partitioning amongst the cluster replicas.\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN = 192\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_datasets import KaggleDatasets\nGCS_PATH = KaggleDatasets().get_gcs_path('imdb-dataset-of-50k-movie-reviews')\n!gsutil ls $GCS_PATH","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls /kaggle/input","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Transformer Workflow with TPU\n\n\nIn this case, we will be producing a robust workflow using Tensorflow TPU with Google Cloud Storage bucket data for training any Transformer models. The following are the steps:\n\n\n-  Load the TPU cluster\n- Fast Encode the data with tokenizer from [Huggingface](https://github.com/huggingface/tokenizers).This is done by chunks of window sizes (batches)\n- We will use the Transformer Embeddings (which we created in [Notebook-1](https://www.kaggle.com/colearninglounge/nlp-end-to-end-cll-nlp-workshop))\n- For all transformer models of BERT variants,the standard is to abstract the last hidden layer of the outputs.\n- This particular layer contains the embedding vectors , generally of size (?,768) for BERT base and (?,1024) for BERT large variants.\n- Then the model uses a FFNN Dense Network with a sigmoid/softmax activation to get the output weights.\n- Then we convert the input data (train data/validation data) to a [Tensorflow Dataset](https://www.tensorflow.org/guide/data) which can leverage the power of TPU.\n- Then we use a distributed training pattern on TPU using [tf.strategy] (https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy)\n\nIn this case,we are using [Huggingface](https://huggingface.co/transformers/pretrained_models.html) models. The first in this case,is using [DistilBERT model](https://huggingface.co/transformers/model_doc/distilbert.html).\n\n\n<img src=\"https://cdn.nextjournal.com/data/QmNQFSULXLPYnGhHSCxmeGk8oHjfdWnybmZGFztfS26fgZ?filename=2019-05-26%2023-43-43%20%E7%9A%84%E8%9E%A2%E5%B9%95%E6%93%B7%E5%9C%96.png&content-type=image/png\">\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"## BERT \n\n[BERT](https://arxiv.org/abs/1810.04805) is [bidirectional encoder Transformer model](https://github.com/google-research/bert)\n\n\n<img src=\"http://jalammar.github.io/images/distilBERT/bert-output-tensor.png\">\n\n\n\nThe entire workflow can be designed as follows:\n\n\nThis image can be used to describe the workflow:\n\n\n<img src=\"http://jalammar.github.io/images/distilBERT/bert-input-to-output-tensor-recap.png\">\n\n\nSlicing the important part\nFor sentence classification, we’re only only interested in BERT’s output for the [CLS] token, so we select that slice of the cube and discard everything else.\n\n\n<img src=\"http://jalammar.github.io/images/distilBERT/bert-output-tensor-selection.png\">\n\n\nBERT Model\n\n<img src=\"https://miro.medium.com/max/740/1*G6PYuBxc7ryP4Pz7nrZJgQ@2x.png\">\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"## DistilBERT Model\n\n\nThe distilbert performs better than Bert in most cases owing to continuous feedback of attention weights from the teacher to the student network. Where the weights change by a large extent in case of Bert, this fails to happen in DistilBert.\n\n\n<img src=\"https://storage.googleapis.com/groundai-web-prod/media%2Fusers%2Fuser_14%2Fproject_391208%2Fimages%2FKD_figures%2Ftransformer_distillation.png\">\n\n\nDistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT’s performances as measured on the GLUE language understanding benchmark. 2 significant benchmarks aspects of this Model:\n\n- Quantization :This leads to approximation of internal weight vectors to a numerically smaller precision\n- Weights Pruning: Removing some connections from the network.\n\nKnowledge distillation (sometimes also referred to as teacher-student learning) is a compression technique in which a small model is trained to reproduce the behavior of a larger model (or an ensemble of models). It was introduced by Bucila et al. and generalized by Hinton et al. a few years later. We will follow the latter method.Rather than training with a cross-entropy over the hard targets (one-hot encoding of the gold class), we transfer the knowledge from the teacher to the student with a cross-entropy over the soft targets (probabilities of the teacher). Our training loss thus becomes:\n\n<img src=\"https://miro.medium.com/max/311/1*GZkQPjKC_Wqx1F4Uu3FdiQ.png\">\n\nThis loss is a richer training signal since a single example enforces much more constraint than a single hard target.\nTo further expose the mass of the distribution over the classes, Hinton et al. introduce a softmax-temperature:\n\n<img src=\"https://miro.medium.com/max/291/1*BaVyKMXRWaudFvcI9So8MQ.png\">\n\nWhen T → 0, the distribution becomes a Kronecker (and is equivalent to the one-hot target vector), when T →+∞, it becomes a uniform distribution. The same temperature parameter is applied both to the student and the teacher at training time, further revealing more signals for each training example. At inference, T is set to 1 and recover the standard Softmax.\n\n\nSome resources:\n\n- [Blog](https://medium.com/huggingface/distilbert-8cf3380435b5)\n- [Huggingface](https://huggingface.co/transformers/model_doc/distilbert.html)\n- [Paper](https://arxiv.org/abs/1910.01108)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tokenize the data and separate them in chunks of 256 units\n\nmaxlen=512\nchunk_size=256\ndef fast_encode(texts, tokenizer, chunk_size=chunk_size, maxlen=maxlen):\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n    #sliding window methodology\n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)\ndef build_model(transformer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    #Replaced from the Embedding+LSTM/CoNN layers\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    model.summary()\n    return model\n\n# First load the real tokenizer\ntokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n# Save the loaded tokenizer locally\ntokenizer.save_pretrained('.')\n# Reload it with the huggingface tokenizers library\nfast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\nval_y=test_y\ntrain_x = fast_encode(train_x.astype(str), fast_tokenizer, maxlen=MAX_LEN)\nval_x = fast_encode(test_x.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_x, train_y))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((val_x, val_y))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    transformer_layer = (\n        transformers.TFDistilBertModel\n        .from_pretrained('distilbert-base-multilingual-cased')\n    )\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()\nplot_model(\n    model,to_file=\"Distilbert_Transformer.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\nn_steps = train_x.shape[0] // BATCH_SIZE\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DistilBERT base Architecture -768 D\n\nThe model architecture for Distilbert for classification is provided here:\n\n<img src=\"https://i.imgur.com/uUzt9dk.png\">"},{"metadata":{},"cell_type":"markdown","source":"## Albert Transformer\n\n[Albert](https://arxiv.org/abs/1909.11942) is a lightweight bert which introduces parameter sharing, caching, and intermediate repeated splitting of the embedding matrix for efficient modelling tasks.\n\nAccording to the paper:\n\n\n'The first one is a factorized embedding parameterization. By decomposing\nthe large vocabulary embedding matrix into two small matrices, we separate the size of the hidden\nlayers from the size of vocabulary embedding. This separation makes it easier to grow the hidden\nsize without significantly increasing the parameter size of the vocabulary embeddings. The second\ntechnique is cross-layer parameter sharing. This technique prevents the parameter from growing\nwith the depth of the network. Both techniques significantly reduce the number of parameters for\nBERT without seriously hurting performance, thus improving parameter-efficiency. An ALBERT\nconfiguration similar to BERT-large has 18x fewer parameters and can be trained about 1.7x faster.\nThe parameter reduction techniques also act as a form of regularization that stabilizes the training\nand helps with generalization.\nTo further improve the performance of ALBERT, we also introduce a self-supervised loss for\nsentence-order prediction (SOP). SOP primary focuses on inter-sentence coherence and is designed\nto address the ineffectiveness (Yang et al., 2019; Liu et al., 2019) of the next sentence prediction\n(NSP) loss proposed in the original BERT.'\n\n\nResources:\n\n- [Github](https://github.com/google-research/albert)\n- [Huggingface](https://huggingface.co/transformers/model_doc/albert.html)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Testing AlbertTransformer\n\ntokenizer = transformers.AlbertTokenizer.from_pretrained('albert-base-v1')\n# Save the loaded tokenizer locally\ntokenizer.save_pretrained('.')\n# Reload it with the huggingface tokenizers library\nfast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\nval_y=test_y\ntrain_x = fast_encode(train_x.astype(str), fast_tokenizer, maxlen=MAX_LEN)\nval_x = fast_encode(test_x.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_x, train_y))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((val_x, val_y))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\nwith strategy.scope():\n    transformer_layer = (\n        transformers.TFAlbertModel\n        .from_pretrained('albert-base-v1')\n    )\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()\nplot_model(\n    model,to_file=\"AlbertTransformer.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n\nn_steps = train_x.shape[0] // BATCH_SIZE\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Albert Base Architecture -768 D\n\nThe model architecture for Albert is as follows:\n\n<img src=\"https://i.imgur.com/ztcIbsb.png\">"},{"metadata":{},"cell_type":"markdown","source":"## XLM Roberta/Roberta\n\n\n[XLM](https://arxiv.org/pdf/1907.11692.pdf) builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining objective and training with much larger mini-batches and learning rates.\n\n<img src=\"https://camo.githubusercontent.com/f5c0d05eb0635cdd0e17e137265af23fa825b1d4/68747470733a2f2f646c2e666261697075626c696366696c65732e636f6d2f584c4d2f786c6d5f6669677572652e6a7067\">Tips:\n\n\nThis implementation is the same as BertModel with a tiny embeddings tweak as well as a setup for Roberta pretrained models.\n\nRoBERTa has the same architecture as BERT, but uses a byte-level BPE as a tokenizer (same as GPT-2) and uses a different pretraining scheme.\n\nRoBERTa doesn’t have token_type_ids, you don’t need to indicate which token belongs to which segment. Just separate your segments with the separation token tokenizer.sep_token (or </s>)\n\nCamemBERT is a wrapper around RoBERTa.\n\n\nResources:\n\n- [FAIR](https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/)\n- [Pytorch](https://pytorch.org/hub/pytorch_fairseq_roberta/)\n- [Github](https://github.com/pytorch/fairseq/tree/master/examples/roberta)\n- [Huggingface](https://huggingface.co/transformers/model_doc/roberta.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Testing AlbertTransformer\nimport os\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\nfrom tqdm.notebook import tqdm\nfrom tokenizers import BertWordPieceTokenizer\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom transformers import AutoTokenizer, pipeline, TFDistilBertModel\n\ntokenizer = transformers.XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n# Save the loaded tokenizer locally\ntokenizer.save_pretrained('.')\n# Reload it with the huggingface tokenizers library\nfast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\nval_y=test_y\ntrain_x = fast_encode(train_x.astype(str), fast_tokenizer, maxlen=MAX_LEN)\nval_x = fast_encode(test_x.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_x, train_y))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((val_x, val_y))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\nwith strategy.scope():\n    transformer_layer = (\n        transformers.TFRobertaModel\n        .from_pretrained('roberta-base')\n    )\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()\nplot_model(\n    model,to_file=\"Roberta-Transformer.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n\nn_steps = train_x.shape[0] // BATCH_SIZE\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Roberta Base Architecture -768 D\n\nThe model architecture for Albert is as follows:\n\n<img src=\"https://i.imgur.com/n6wDjpP.png\">"},{"metadata":{},"cell_type":"markdown","source":"## Conclusion of BERT-base Transformers\n\n\nThis section concludes the classification models created using all BERT-based transformer models ranging from Bert to Albert /Roberta. These classes of Bidirectional Encoder Models are based on Discriminatory Transformer architectures and are well suited for classification tasks in general (although they are used for language modelling, question answering).\n\nNow we move forward to some Generative transformers like GPT.\n\n\n\n<img src=\"https://i.pinimg.com/originals/76/04/48/760448c0de6bed1e9b810b006d264561.gif\">\n"},{"metadata":{},"cell_type":"markdown","source":"## GPT-Generative Pretraining\n\n\n<img src=\"https://jalammar.github.io/images/gpt2/gpt2-self-attention-split-attention-heads-1.png\">\n\n[GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)a large transformer-based language model with 1.5 billion parameters, trained on a dataset[1] of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data.\n\nTips:\n\n- GPT-2 is a model with absolute position embeddings so it’s usually advised to pad the inputs on the right rather than the left.\n\n- GPT-2 was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next token in a sequence. Leveraging this feature allows GPT-2 to generate syntactically coherent text as it can be observed in the run_generation.py example script.\n\n- The PyTorch models can take the past as input, which is the previously computed key/value attention pairs.\n\n Resource:\n \n - [Jay's Blog](http://jalammar.github.io/illustrated-gpt2/)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Testing GPT2Transformer\nimport os\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\nfrom tqdm.notebook import tqdm\nfrom tokenizers import BertWordPieceTokenizer\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom transformers import AutoTokenizer, pipeline, TFDistilBertModel\n\ntokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2-medium')\n# Save the loaded tokenizer locally\ntokenizer.save_pretrained('.')\n# Reload it with the huggingface tokenizers library\nfast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\nval_y=test_y\ntrain_x = fast_encode(train_x.astype(str), fast_tokenizer, maxlen=MAX_LEN)\nval_x = fast_encode(test_x.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_x, train_y))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((val_x, val_y))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\nwith strategy.scope():\n    transformer_layer = (\n        transformers.TFGPT2Model\n        .from_pretrained('gpt2-medium')\n    )\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()\nplot_model(\n    model,to_file=\"GPT2-Transformer.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n\nn_steps = train_x.shape[0] // BATCH_SIZE\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion of Notebook\n\n\nThis terminates the notebook -2 for the workshop. The same codebase can be used  with any model from the huggingface repository. In the next [notebook](https://www.kaggle.com/colearninglounge/nlp-end-to-end-cll-nlp-workshop-3), we will breifly look into tensorboard graphs and training a simple model with tensorboard.\n\n\n\n<img src=\"https://i.pinimg.com/originals/1d/cd/04/1dcd045c688cb9b8c85c79ab05834094.gif\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Testing Classical-Quantum Circuits \nimport pennylane as qml\nimport tensorflow as tf\nfrom tensorflow import keras\nn_qubits = 2\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\n@qml.qnode(dev)\ndef qnode(inputs, weights):\n    qml.templates.AngleEmbedding(inputs, wires=range(n_qubits))\n    qml.templates.BasicEntanglerLayers(weights, wires=range(n_qubits))\n    return [qml.expval(qml.PauliZ(wires=i)) for i in range(n_qubits)]\nn_layers = 6\nweight_shapes = {\"weights\": (n_layers, n_qubits)}\nqlayer = qml.qnn.KerasLayer(qnode, weight_shapes, output_dim=n_qubits)\nclayer_1 = tf.keras.layers.Dense(2)\nclayer_2 = tf.keras.layers.Dense(2, activation=\"softmax\")\nmodel = tf.keras.models.Sequential([clayer_1, qlayer, clayer_2])\nopt = tf.keras.optimizers.SGD(learning_rate=0.2)\nmodel.compile(opt, loss=\"mae\", metrics=[\"accuracy\"])\nmodel.fit(train_x,train_y,batch_size=128,epochs=1,verbose=2,validation_data=(val_x,val_y))\nmodel.summary()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}