{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"background-color:#A8A8A8; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% / 10% 40%\">Stroke Prediction</h1>\n\n\n\n\n![picture](https://imgk.timesnownews.com/story/silent-stroke.gif?tr=w-400,h-300,fo-auto)\n# Introduction\nAccording to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.This dataset is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. Each row in the data provides relavant information about the patient.Notebook summary\n* [Data understanding](#1)\n* [Exploratory Data Analysis](#2)\n* [Re-sampling](#7)\n* [Data Preprocessing](#3)\n* [Modeling](#4)\n* [Model Evaluation(k cross validation & ROC Auc curve)](#5)\n* [Feature importance](#6)\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing Libraries\n\n# linear algebra\nimport numpy as np \n\n# data processing\nimport pandas as pd\n\n# data visualization(for EDA)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.style.use('ggplot')\nsns.set(color_codes=True)\nimport plotly.express as px\nimport shap\nimport plotly.offline as pyo\nimport plotly.graph_objs as go\npyo.init_notebook_mode()\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n# Importing sklearn methods\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import model_selection\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\n# import labelencoder\nfrom sklearn.preprocessing import LabelEncoder\n\n#Feature Scaling\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n<h1 style=\"background-color:#A8A8A8; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% / 10% 40%\">Data understanding</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"print('Number of rows and  number of columns in our dataset  :',df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"print(f'Categorical features in our data {df.columns[df.dtypes==object].tolist()}')\nprint(f'Numerical features in our data {df.columns[df.dtypes!=object].tolist()}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#null values\n# Draw plot\nfig, ax = plt.subplots(figsize=(20,5))\nx=df.isnull().sum().index\ny=df.isnull().sum()\nax.vlines(x, ymin=0, ymax=y, color='firebrick', alpha=0.7, linewidth=2)\nax.scatter(x, y, s=75, color='firebrick', alpha=0.7)\n\n\n# Title, Label, Ticks and Ylim\nax.set_title('Missing values in a data', fontdict={'size':22})\nax.set_ylabel('Null Values', fontdict={'size':18})\nax.set_xlabel('Column Name', fontdict={'size':18})\nax.set_xticks(df.columns)\nax.set_xticklabels(df.columns.str.upper(),rotation=0, fontdict={'horizontalalignment': 'center', 'size':10})\nax.set_ylim(-10)\n\n# Annotate\n\nfor index, value in enumerate(y):\n    plt.text(index, value, str(value),horizontalalignment= 'center', verticalalignment='bottom', fontsize=17)\nfig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's drop the null values\ndf = df.dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n<h1 style=\"background-color:#A8A8A8; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% / 10% 40%\">Exploratory Data Analysis</h1>"},{"metadata":{},"cell_type":"markdown","source":"It is easy to see that the data is very unbalanced and we will look at that after the EDA."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"value=df.stroke.value_counts()\nx=['Stroke -VE','Stroke +VE']\nfig = go.Figure(data=[go.Pie(labels=x,values=value , name=\"Smoking Status\",hole=0.4,pull=[0.1],\n                     textinfo=\"label+percent\")])\nfig.update_layout(title_text='Target Feature Pie Chart',autosize=False,\n                  title=dict(x=0.5))\nfig.show()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nThe graph shows that stroke positive does not correlate with smokers, since the proportion of people with stroke is about the same among other smoking status.\n."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"value=df.smoking_status.value_counts()\ny1=df.query('stroke==0')['smoking_status'].value_counts()\ny2=df.query('stroke==1')['smoking_status'].value_counts()\nx=df.smoking_status.unique()\n\nfig = make_subplots(rows=1, cols=2, subplot_titles=(\"Smoking Status\",'smoking status vs stroke'),specs=[[{'type':'domain'}, {\"type\": \"bar\"}]])\nfig.add_trace(go.Pie(labels=x,values=value , name=\"Smoking Status\",hole=0.4,pull=[0.02,0.02,0.02,0.02],\n                     textinfo=\"label+percent\"),\n              1, 1)\nfig.add_trace(go.Bar(name='Stroke -VE', x=x, y=y1),1, 2)\nfig.add_trace(go.Bar(name='Stroke +VE', x=x, y=y2),1, 2)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that the Residence Type is uniformly distributed and has no relationship with stroke-positive individuals."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"value=df.Residence_type.value_counts()\ny1=df.query('stroke==0')['Residence_type'].value_counts()\ny2=df.query('stroke==1')['Residence_type'].value_counts()\nx=df.Residence_type.unique()\n\n\nfig = make_subplots(rows=1, cols=2,specs=[[{'type':'domain'}, {\"type\": \"bar\"}]],subplot_titles=(\"Residence type\",'Residence type vs stroke'))\nfig.add_trace(go.Pie(labels=x, values=value, name=\"Residence_type\",hole=0.4,pull=[0.02,0.02,0.02,0.02],\n                     textinfo=\"label+percent\"),\n              1, 1)\nfig.add_trace(go.Bar(name='Stroke -VE', x=x, y=y1),1, 2)\nfig.add_trace(go.Bar(name='Stroke +VE', x=x, y=y2),1, 2)\n\n\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At first glance, it is clear that the elderly are more likely to have a stroke"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"\nfig = px.histogram(df, x=\"age\", color=\"stroke\",marginal=\"box\",\n                   hover_data=df.columns,title='Distribution of Age')\nfig.update_layout(autosize=False,width=500,height=350,title=dict(x=0.5))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the figure, it is clear that glucose level and stroke positivity are not related. This means that people can have a stroke at every glucose level"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"x1=df.query('stroke==0')['avg_glucose_level']\nx2=df.query('stroke==1')['avg_glucose_level']\n\ngroup_labels = ['Stroke -VE', 'Stroke +VE']\n\ncolors = ['slategray', 'magenta']\n\n# Create distplot with curve_type set to 'normal'\nfig = ff.create_distplot([x1,x2], group_labels, bin_size=5,curve_type='normal', colors=colors)\n\n# Add title\nfig.update_layout(title_text='Distribution of Glucose Level',autosize=False,\n    width=500,\n    height=400,title=dict(x=0.5))\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"People with a low BMI have a higher risk of stroke"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import plotly.figure_factory as ff\nimport numpy as np\n\nx1=df.query('stroke==0')['bmi']\nx2=df.query('stroke==1')['bmi']\n\ngroup_labels = ['Stroke -VE', 'Stroke +VE']\n\ncolors = ['slategray', 'magenta']\n\n# Create distplot with curve_type set to 'normal'\nfig = ff.create_distplot([x1,x2], group_labels, bin_size=5,\n                         curve_type='normal',\n                         colors=colors)\n\n# Add title\nfig.update_layout(title_text='Distribution of BMI(Body mass index)',autosize=False,\n    width=500,\n    height=400,title=dict(x=0.5))\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Findings:\n* There are more women than men in our dataset.\n* The number of married people are shown more in our data.\n* In both sexes, married people are more likely to have a stroke."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"Data=df[['gender','ever_married','stroke']]\nfig = px.parallel_categories(Data, dimensions=['gender','ever_married','stroke'])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7\"></a> <br>\n<h1 style=\"background-color:#A8A8A8; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% / 10% 40%\">Random Oversampling Imbalanced Datasets</h1>"},{"metadata":{},"cell_type":"markdown","source":"Before applying any classifier algorithm we should balance over dataset because imbalanced classifications pose a challenge for predictive modeling as most of the machine learning algorithms used for classification were designed around the assumption of an equal number of examples for each class. This results in models that have poor predictive performance, specifically for the minority class. And now we will Apply re-sampling strategies to obtain a more balanced data distribution is an effective solution to the imbalance problem."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"\n# Class count\ncount_class_0, count_class_1 = df.stroke.value_counts()\n\n# Divide by class\ndf_class_0 = df[df['stroke'] == 0]\ndf_class_1 = df[df['stroke'] == 1]\n\n\ndf_class_1_over = df_class_1.sample(count_class_0, replace=True)\ndf = pd.concat([df_class_0, df_class_1_over], axis=0)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Draw pie chart\nvalue=df.stroke.value_counts()\nx=['Stroke -VE','Stroke +VE']\nfig = go.Figure(data=[go.Pie(labels=x,values=value , name=\"Smoking Status\",hole=0.4,pull=[0.1],\n                     textinfo=\"label+percent\")])\nfig.update_layout(title_text='Target Feature after Re-sampling',autosize=False,\n                  title=dict(x=0.5))\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n<h1 style=\"background-color:#A8A8A8; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% / 10% 40%\">Data Preprocessing</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Encoding categorical data values \ncategorical_feature_mask = df.dtypes==object\ncategorical_features =df.columns[categorical_feature_mask].tolist()\nle = LabelEncoder()\ndf[categorical_features] =df[categorical_features].apply(lambda col: le.fit_transform(col))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting the dataset into the Training set and Test set\nX=df.drop(columns=['stroke','id'])\ny=df['stroke']\nx_train, x_test,  y_train, y_test = train_test_split(X, y,  random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n<h1 style=\"background-color:#A8A8A8; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% / 10% 40%\">Modeling</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#setting up models that weâ€™ll be testing out\nmodels = [('LogReg', LogisticRegression()), \n          ('RF' , RandomForestClassifier(n_estimators=20, random_state=0)),\n           ('DecTree', DecisionTreeClassifier()),\n          ('KNN', KNeighborsClassifier()),\n          ('LinDisc', LinearDiscriminantAnalysis()),\n          ('GaussianNB', GaussianNB())]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#k-fold validation to evaluate each algorithm\nscores=[]\ncross_val_scores = []\nmodel_names=[]\nfor model_name, model in models:\n    \n    \n    results = model_selection.cross_val_score(model, X, y, cv=5, scoring='f1') \n    model.fit(x_train, y_train)\n    y_pred =model.predict(x_test)\n    score = f1_score(y_pred, y_test)\n    cross_val_scores.append(results)\n    scores.append(score)\n    model_names.append(model_name)\n    \n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a> <br><h1 style=\"background-color:#A8A8A8; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% / 10% 40%\">Model Evaluation</h1>"},{"metadata":{},"cell_type":"markdown","source":"## k cross validation scores of different classifiers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare Data\ndata=[]\ndata=np.array(cross_val_scores)\ndata = data.transpose()\nData=pd.DataFrame(data, columns=model_names)\n#Draw graph\nfig = px.line(Data,x=['one','Two','Three','Four','Five'],y=Data.columns,title='Comparing models k Cross validation scores', labels=dict(value=\"f1 Score\", x=\"k(FOLD)\", variable=\"Classifier\"))\nfig.update(layout=dict(title=dict(x=0.5)))\nfig.update_traces(mode=\"markers+lines\",marker=dict(\n            color='Gray'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Comparing model performance on testing data\nFindings:RandomForest has a higher F1 score than the other classifiers."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.line(x=model_names,y=scores,labels=dict(y=\"f1 Score\", x=\"classfier\"),title='Comparing All classifiers Performance on testing dataset')\nfig.update_traces(mode=\"markers+lines\",marker=dict(\n            color='Red'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ROC AUC Curve\nAnother way to evaluate and compare your binary classifier is provided by the ROC AUC Curve.I will draw both models whose f1 is higher and lower in order to see the difference in curves. The Black line in the middel represents a purely random classifier and therefore your classifier should be as far away from it as possible. Our Random Forest model seems to do a good job as it far from that line.The ROC AUC Score is the corresponding score to the ROC AUC Curve. It is simply computed by measuring the area under the curve, which is called AUC.A classifiers that is 100% correct, would have a ROC AUC Score of 1 and a completely random classiffier would have a score of 0.5.."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"AUC_models = [('GaussianNB', GaussianNB()), \n          ('RF' , RandomForestClassifier())]\n\n\n# Create an empty figure, and iteratively add new lines\nfig = go.Figure()\nfig.add_shape(\n    type='line', line=dict(dash='dash',color='Black'),\n    x0=0, x1=1, y0=0, y1=1\n)\n\nfor model_name, model in AUC_models:\n\n    model.fit(x_train, y_train)\n    y_pred =model.predict(x_test)\n    fpr, tpr, _ = roc_curve(y_test, y_pred)\n    auc_score = roc_auc_score(y_test, y_pred)\n    name = f\"{model_name}(AUC={auc_score:.2f})\"\n    fig.add_trace(go.Scatter(x=fpr, y=tpr, name=name, mode='lines'))\n\nfig.update_layout(title='ROC CURVE',\n    xaxis_title='False Positive Rate',\n    yaxis_title='True Positive Rate',\n    yaxis=dict(scaleanchor=\"x\", scaleratio=1),\n    xaxis=dict(constrain='domain'),\n    width=500, height=500\n    \n)\nfig.update(layout=dict(title=dict(x=0.5)))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\"></a> <br><h1 style=\"background-color:#A8A8A8; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% / 10% 40%\">Feature Importance<a id=\"1\"></a> <br></h1>\n\n"},{"metadata":{},"cell_type":"markdown","source":"The figure shows that Age, glucose level and BMI are the most important characteristics for predicting stroke patients."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"rf=RandomForestClassifier().fit(x_train, y_train)\nexplainer = shap.TreeExplainer(rf)\nshap_values = explainer.shap_values(x_test)\nshap.summary_plot(shap_values, x_test, plot_type=\"bar\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}