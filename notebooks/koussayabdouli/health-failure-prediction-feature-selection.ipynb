{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:green\">Health Failure Prediction </h1>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"color :red\"> Columns Meaning : </h3>\n- Age : Age of patient <br></br>\n- Anemina : 1 if True , 0 if False <br></br>\n- Diabetes : 1 if True , 0 if False <br></br>\n- High blood pressure : 1 if True , 0 if False <br></br>\n- Sex : 1 Male , 0 Female <br></br>\n- Smoking : 1 if True , 0 if False  <br></br>\n- Other features are numerical features .\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# data shape\nprint('Data Shape is : ', data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check null values\ndata.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Features correlation\nplt.figure(figsize=(12,8))\nsns.heatmap(data.corr(),annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Age , Ejection_fraction , Serum_creatine and time are highly correlated with the target than other features ."},{"metadata":{},"cell_type":"markdown","source":"We should divide our data to two categories : \n- Numerical Values \n- Categorical Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Get_Features_types(data):\n    categorical_features = []\n    numerical_features = []\n    for col in data.iloc[:,:-1]:\n        if data[col].nunique() < 10:\n            categorical_features.append(col)\n        else : \n            numerical_features.append(col)\n    return categorical_features , numerical_features\n\nGet_Features_types(data)\n\ncategorical_features = Get_Features_types(data)[0]\nnumerical_features = Get_Features_types(data)[1]\n\nprint(categorical_features)\nprint(numerical_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:red\">Categorical Values</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot( x = 'DEATH_EVENT' , data = data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def CountPlot_hue_Categorical_data(data):\n    for feature in categorical_features:\n        plt.figure()\n        sns.countplot(x = \"DEATH_EVENT\" , data = data , hue=feature)\n        plt.title(feature)\n\nCountPlot_hue_Categorical_data(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Get_Percentage_data(data):\n    percentage_death_true_case = []\n    percentage_death_false_case = []\n    for col in categorical_features:\n        true_case = round(data['DEATH_EVENT'][data[col] == 1].value_counts(normalize = True)[1] * 100 , 2)\n        false_case = round(data['DEATH_EVENT'][data[col] == 0].value_counts(normalize = True)[1] * 100,2)\n        percentage_death_true_case.append(true_case)\n        percentage_death_false_case.append(false_case)\n    Percentage = pd.DataFrame(list(zip(percentage_death_true_case , percentage_death_false_case)) ,\n                              index = categorical_features ,\n                              columns = ['% Percentage Death (IF 1)' , '% Percentage Death (IF 0)'])\n    return Percentage\n        \nGet_Percentage_data(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This data can show : \n-  People who smoke has a percentage of death (31,25 ) , otherwise peole who don't has a percentage of death ( 32,51) [so strange seriously]\n- Percentage of Men's death is roughly equivalent to women's death ( Same for poeple who has diabetes).\n- In High blood pressure and Amenia  , we can see obviously a little difference between percentage of death ( IF True)\n"},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:red\">Numerical Values</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def BOX_Plot_numerical_features(data):\n    for col in numerical_features:\n        plt.figure()\n        sns.boxplot(data[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BOX_Plot_numerical_features(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Numerical Features are \" , numerical_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drawing box plot can show many details : \n- Detecting outliers in ejection_fraction \n- No outliers in features like : Age and time \n- In Medical fields , values for features : <br></br>\n   Ejection_fraction <br></br>Creatinine_phosphokinase <br></br> Platelets <br></br> Serum_creatinine <br></br>Serum_sodium <br></br>are possibles in range of values , so we will keep them in order to have a reasonable prediction \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data['ejection_fraction'] > 65]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Age \nsns.distplot(data[data['DEATH_EVENT'] == 0]['age'] , label =\"Death\")\nsns.distplot(data[data['DEATH_EVENT'] == 1]['age'] , label = \"Survived\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see here that people with under than 70 have a probability of surviving more than death  ."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nsns.countplot(data[(data['age'] < 70)]['DEATH_EVENT'])\nprint(data[(data['age'] < 70)]['DEATH_EVENT'].value_counts())\nplt.title(\"Age Under than 70\")\nplt.show()\n####\nplt.figure()\nsns.countplot(data[(data['age'] > 70)]['DEATH_EVENT'])\nprint(data[(data['age'] > 70)]['DEATH_EVENT'].value_counts())\nplt.title(\"Age Higher than 70\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Creatinine_phosphokinase\nsns.distplot(data[data['DEATH_EVENT'] == 0]['creatinine_phosphokinase'] , label =\"Death\")\nsns.distplot(data[data['DEATH_EVENT'] == 1]['creatinine_phosphokinase'] , label = \"Survived\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Platelets\nsns.distplot(data[data['DEATH_EVENT'] == 0]['platelets'] , label =\"Death\")\nsns.distplot(data[data['DEATH_EVENT'] == 1]['platelets'] , label = \"Survived\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Serum_creatinine\nsns.distplot(data[data['DEATH_EVENT'] == 0]['serum_creatinine'] , label =\"Death\")\nsns.distplot(data[data['DEATH_EVENT'] == 1]['serum_creatinine'] , label = \"Survived\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Serum_sodium\nsns.distplot(data[data['DEATH_EVENT'] == 0]['serum_sodium'] , label =\"Death\")\nsns.distplot(data[data['DEATH_EVENT'] == 1]['serum_sodium'] , label = \"Survived\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Time\nsns.distplot(data[data['DEATH_EVENT'] == 0]['time'] , label =\"Death\")\nsns.distplot(data[data['DEATH_EVENT'] == 1]['time'] , label = \"Survived\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"it's obivious that people more than a follow-up period (days) more than 90 have a probalibity of surviving more than death .\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nsns.countplot(data[data['time'] > 90]['DEATH_EVENT'])\nprint(data[data['time'] > 90]['DEATH_EVENT'].value_counts())\nplt.title(\"Time Higher than 90\")\nplt.show()\n####\nplt.figure()\nsns.countplot(data[data['time'] < 90]['DEATH_EVENT'])\nprint(data[data['time'] < 90]['DEATH_EVENT'].value_counts())\nplt.title(\"Time lower than 90\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ejection_fraction\nsns.distplot(data[data['DEATH_EVENT'] == 0]['ejection_fraction'] , label =\"Death\")\nsns.distplot(data[data['DEATH_EVENT'] == 1]['ejection_fraction'] , label = \"Survived\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"People with ejection fracrtion more than 30 have a probability of surviving more than death ."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data[data['ejection_fraction'] > 28]['DEATH_EVENT'])\nprint(data[data['ejection_fraction'] > 28]['DEATH_EVENT'].value_counts())\nplt.title(\"Ejection Fraction more than 30\")\nplt.show()\n#####\nsns.countplot(data[data['ejection_fraction'] < 30]['DEATH_EVENT'])\nprint(data[data['ejection_fraction'] < 30]['DEATH_EVENT'].value_counts())\nplt.title(\"Ejection Fraction lower than 30\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:red\">Feature Engineering</h1>"},{"metadata":{},"cell_type":"markdown","source":"- Using domain knowledge to extract features from raw data via data mining techniques is one of the best typical methods to improve our model "},{"metadata":{"trusted":true},"cell_type":"code","source":"data_copy= data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_copy['platelets/age'] = data['platelets'] / data['age']\ndata_copy['time/age'] = data['time'] / data['age']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_copy.drop(['platelets' , 'time'] , axis = 1 , inplace = True ) \ndata_copy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<h1 style=\"color:red\">Modelisation</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier ,AdaBoostClassifier\nfrom sklearn.metrics import f1_score as f1\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectKBest , f_classif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split the data\nX = data_copy.drop(['DEATH_EVENT'] , axis = 1)\ny = data_copy['DEATH_EVENT']\nx_train , x_test , y_train , y_test = train_test_split(X,y,test_size = 0.2 ,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will use KNN with differents values of k in order to have an idea about our score accuracy and f1-score\ndef KNN_best_scores(n_iterations):\n    acc_score = []\n    f1_score = []\n    for k in range(1,n_iterations):\n        model = KNeighborsClassifier(n_neighbors= k)\n        model.fit(x_train , y_train)\n        y_pred = model.predict(x_test)\n        acc_score.append(model.score(x_test , y_test))\n        f1_score.append(f1(y_pred , y_test))\n    Knn_scores = pd.DataFrame(list(zip(acc_score , f1_score)) , index = range(1,n_iterations) ,\n                              columns = ['Accuracy Score' , 'F1_Score'])\n    print('Best values for f1_score are : \\n',Knn_scores.nlargest(5, ['F1_Score']))\n    return  Knn_scores.nlargest(5, ['Accuracy Score'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Selecting a range of value between (1,20)\nKNN_best_scores(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## RandomForest\ndef RandomForest_best_score():\n    model = RandomForestClassifier()\n    model.fit(x_train,y_train)\n    acc_score = model.score(x_test,y_test)\n    y_pred = model.predict(x_test)\n    f1_score = f1(y_pred , y_test)\n    return pd.DataFrame([[acc_score , f1_score]] , columns = ['Accuracy Score' , 'F1_Score'])\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"RandomForest : Accuracy Score / F1_Score\")\nRandomForest_best_score()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gradient Boosting Classifier\ndef GradientBoosting_best_score():\n    model = GradientBoostingClassifier()\n    model.fit(x_train , y_train)\n    y_pred = model.predict(x_test)\n    acc_score = model.score(x_test , y_test)\n    f1_score = f1(y_pred , y_test)\n    \n    return pd.DataFrame([[acc_score , f1_score]] , columns = ['Accuracy Score' , 'F1_Score'])\n\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"GradientBoosting : Accuracy Score / F1_Score\")\n\nGradientBoosting_best_score()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It combines multiple classifiers to increase the accuracy of classifiers. AdaBoost is an iterative ensemble method\ndef AdaBoostClassifier_best_score(list_values):\n    acc_scores = []\n    f1_scores = []\n    for n in list_values:\n        model = AdaBoostClassifier(n_estimators=n,learning_rate=0.01)\n        model.fit(x_train , y_train)\n        acc_score = model.score(x_test , y_test)\n        y_pred = model.predict(x_test)\n        f1_score = f1(y_pred,y_test)\n        acc_scores.append(acc_score)\n        f1_scores.append(f1_score)\n    return pd.DataFrame(list(zip(acc_scores,f1_scores)) , columns = ['Accuracy Score' , 'F1_Score'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AdaBoostClassifier_best_score([i for i in range(100,1000,100)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:red\">Modelisation Using features selection Techniques</h1>"},{"metadata":{},"cell_type":"markdown","source":"- We will use SelectKBest as a feature selector to improve our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"## SelectKBest with range of values (5,6,7,8) and try to find difference between models with and without feature selection\ndef KNN_With_Feature_Selection():\n    acc = []\n    f1_scores = []\n    features_names = []\n    for n in range(5,9):\n        feature_selection = SelectKBest(f_classif , k = n)\n        features_name = feature_selection.fit_transform(data_copy[X.columns], data_copy['DEATH_EVENT'])\n        cols = feature_selection.get_support(indices=True)\n        features_name = data_copy.columns[cols]\n        features_names.append(features_name)\n        knn_processor = make_pipeline(StandardScaler() , feature_selection )\n        for i in range(1,20):\n            KNN_model = make_pipeline(knn_processor , KNeighborsClassifier(n_neighbors = i))\n            KNN_model.fit(x_train , y_train)\n            acc_score = KNN_model.score(x_test,y_test)\n            y_pred = KNN_model.predict(x_test)\n            f1_score = f1(y_pred , y_test)\n            acc.append(acc_score)\n            f1_scores.append(f1_score)\n    DF1 = pd.DataFrame(list(zip(acc[:19] , f1_scores[:19])) , columns = ['Accuracy(n = 5)' , 'F1_Score(n = 5)'])\n    DF2 = pd.DataFrame(list(zip(acc[20:37] , f1_scores[20:37])) , columns = ['Accuracy(n = 6)' , 'F1_Score(n = 6)'])\n    DF3 = pd.DataFrame(list(zip(acc[38:56] , f1_scores[38:56])) , columns = ['Accuracy(n = 7)' , 'F1_Score(n = 7)'])\n    DF4 = pd.DataFrame(list(zip(acc[57:] , f1_scores[57:])) , columns = ['Accuracy(n = 8)' , 'F1_Score(n = 8)'])\n    \n    return (list(features_names[0]) , DF1) , (list(features_names[1]) , DF2) , (list(features_names[2]) , DF3) , (list(features_names[3]) , DF2)\n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(FN0,DF1),(FN1,DF2) ,(FN2,DF3) ,(FN3,DF4) = KNN_With_Feature_Selection()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Feature Selection with n =5 \" , FN0)\nDF1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Feature Selection with n =6\" , FN1)\nDF2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Feature Selection with n =7 \" , FN2)\nDF3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Feature Selection with n =8 \" , FN3)\nDF4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def AdaBoostClassifier_best_score_with_feature_selection(list_values):\n    train_score = []\n    acc = []\n    f1_scores = []\n    feature_selection = SelectKBest(f_classif , k = 5)\n    adaboost_processor = make_pipeline(StandardScaler() , feature_selection )\n    for i in list_values:\n        adabooost_model = make_pipeline(adaboost_processor , AdaBoostClassifier(n_estimators= i,learning_rate=0.09))\n        adabooost_model.fit(x_train , y_train)\n        train_sc= adabooost_model.score(x_train,y_train)\n        train_score.append(train_sc)\n        acc_score = adabooost_model.score(x_test,y_test)\n        y_pred = adabooost_model.predict(x_test)\n        f1_score = f1(y_pred , y_test)\n        acc.append(acc_score)\n        f1_scores.append(f1_score)\n    return pd.DataFrame(list(zip(train_score,acc,f1_scores)) , columns = ['Train Score','Accuracy Score' , 'F1_Score'])\n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AdaBoostClassifier_best_score_with_feature_selection([i for i in range(100,1000,100)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AdaBoostClassifier_best_score_with_feature_selection([800])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h5>-Feature selection is the process of reducing the number of input variables when developing a predictive model. It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model.</h5>"},{"metadata":{},"cell_type":"markdown","source":"<h1>I'll use an RandomForestClassifier as my final model because it gives good results .\nWe can see how feature selection can improve our model </h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Final Model :\nfeature_selection = SelectKBest(f_classif , k = 5)\nadaboost_processor = make_pipeline(StandardScaler() , feature_selection )\nadaboost_model = make_pipeline(adaboost_processor , AdaBoostClassifier(n_estimators= 800,learning_rate=0.09))\nadaboost_model.fit(X , y)\nmy_predictions = adaboost_model.predict(X)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')\nsubmission['Predictions'] = my_predictions\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}