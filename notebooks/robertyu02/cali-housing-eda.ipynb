{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Summary\n\n### Did exploratory analysis on 1990 CA housing price data\n\n### Computed a population density proxy measure (done on my computer using ad-hoc C++ algo; uploaded onto Kaggle)\n\n### Analyzed distribution of datapoints w/ respect to the density measure. Found clear clustering\n\n### Ran a quick multiple regression model","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib\nfrom matplotlib import cm\nimport matplotlib.pyplot as plt\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nFPATH = '/kaggle/input/housing/'\nGPATH = '/kaggle/input/caliboundaries/'\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Processing dataframes","metadata":{}},{"cell_type":"markdown","source":"## Main dataframe and feature engineering","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(FPATH+\"housing.csv\")\nl0 = -125\ndf['long_mercator'] = df['longitude']-l0\ndf['lat_mercator'] = np.arcsinh(np.tan(df['latitude']*np.pi/180))\ndf['pop_per_household'] = df['population']/df['households']\ndf['rooms_per_house'] = df['total_rooms']/df['households']\ndf['bedrooms_per_house'] = df['total_bedrooms']/df['households']\ndf['bedrooms_per_room'] = df['total_bedrooms']/df['total_rooms']\ndf['rooms_per_person'] = df['total_rooms']/df['population']\ndf['bedrooms_per_person'] = df['total_bedrooms']/df['population']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading CA boundary points\n(for visualization purposes, mainly)","metadata":{}},{"cell_type":"code","source":"f = open(GPATH+'pts.txt', 'r')\npts = []\nfor i in range(1500):\n    a = f.readline()\n    if a == '':\n        break\n    if ',' not in a:\n        pts.append([])\n    else:\n        tup = list(map(float,a[:-1].split(',')))\n        tup[0] -= l0\n        tup[1] = np.arcsinh(np.tan(tup[1]*np.pi/180))\n        pts[-1].append(tup)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for poly in pts:\n    x = [t[0] for t in poly]\n    y = [t[1] for t in poly]\n    plt.fill(x, y, facecolor='lightblue')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading population density proxy measure\n(I calculated this using population and long/lat data via a separate C++ program)","metadata":{}},{"cell_type":"code","source":"f2 = open(GPATH+'densities.txt', 'r')\nf2.readline()\nds = []\nfor i in range(20640):\n    ds.append(float(f2.readline()[:-1]))\ndf['density'] = ds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preliminary Analysis","metadata":{}},{"cell_type":"markdown","source":"## Function to easily display summary statistics for any sub-dataframe of df\nI decided not to use .describe() since each block group (geographical district) has a different # households. The \"cumulative\" function calculates summary stats, weighted by # households","metadata":{}},{"cell_type":"code","source":"critical = ['housing_median_age', 'median_income', 'median_house_value', 'pop_per_household', 'rooms_per_house',\n            'bedrooms_per_house', 'bedrooms_per_room', 'rooms_per_person', 'bedrooms_per_person', 'density']\nsumlist = ['longitude','latitude','housing_median_age','total_rooms','total_bedrooms','median_income','median_house_value',\n           'long_mercator','lat_mercator','pop_per_household','rooms_per_house','bedrooms_per_house','bedrooms_per_room',\n           'rooms_per_person','bedrooms_per_person','density']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This calculates summary statistics for a df, weighted by # households\n\ndef cumulative(duf, cols):\n    df_summary = pd.DataFrame(index=['count', 'mean', 'std' , 'min' , '25%' , '50%' , '75%', 'max'])\n    dtf = duf.copy()\n    for s in cols:\n        df_summary[s] = pd.Series(float)\n        dtf.sort_values(s, inplace=True)\n        cumsum = dtf['households'].cumsum()\n        df_summary[s].loc['count'] = dtf[s].count()\n        df_summary[s].loc['min'] = dtf[s].min()\n        df_summary[s].loc['25%'] = dtf[s][cumsum >= dtf['households'].sum() / 4.0].iloc[0]\n        df_summary[s].loc['50%'] = dtf[s][cumsum >= dtf['households'].sum() / 2.0].iloc[0]\n        df_summary[s].loc['75%'] = dtf[s][cumsum >= 3*dtf['households'].sum() / 4.0].iloc[0]\n        df_summary[s].loc['max'] = dtf[s].max()\n\n        avg = (dtf[s]*dtf['households']).sum()/dtf['households'].sum()\n        df_summary[s].loc['mean'] = avg\n        df_summary[s].loc['std'] = np.sqrt(((dtf[s]-avg)**2 * dtf['households']).sum()/dtf['households'].sum())\n\n    return df_summary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cumulative(df,sumlist)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlation and Outlier Analysis\nI chose certain variables, examined their relationships with other variables, and looked at outliers","metadata":{}},{"cell_type":"markdown","source":"### Each variable vs. median_house_value","metadata":{}},{"cell_type":"code","source":"for c in critical:\n    plt.scatter(df[c], df['median_house_value'], s=.1)\n    plt.title(c)\n#     if c == 'bedrooms_per_room':\n#         plt.xlim([0,0.5])\n    plt.show()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Density proxy measure vs. each variable","metadata":{}},{"cell_type":"code","source":"for c in critical:\n    plt.scatter(df['density'], df[c], s=.1)\n    plt.title(c)\n    plt.ylim([0,df[c].quantile(.95)])\n#     if c == 'bedrooms_per_room':\n#         plt.xlim([0,0.5])\n    plt.show()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(df['density'], df['median_house_value'], s=.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cumulative(df, critical)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Price vs. Median Income","metadata":{}},{"cell_type":"code","source":"plt.scatter(df['median_income'], df['median_house_value'], s=.1)\nplt.plot([6, 14], [0, 400000])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_outlier1 = df.loc[(df['median_house_value']-0) < (400000-0)/(14-6)*(df['median_income']-6)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cumulative(df, critical)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_outlier1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 18501 is the most visible outlier (i.e. high income, low house value)\n    # State Park. \n# 19006 has an insanely high pop_per_household\n    # Rooms per house stats are all fine, but WTF with household size\n    # Checked coordinates on Google Maps. Turns out it's a state prison + med facility\n# Outliers are in much less dense areas\n# More rooms per house\n# But ppl per house roughly the same\ndf_outlier1[critical]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Price vs. Rooms per House","metadata":{}},{"cell_type":"code","source":"plt.scatter(df['rooms_per_house'], df['median_house_value'], s = .1)\n# plt.xlim([0,15])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# don't know why 1914 is so whack\n    # all really small looking houses\n    # maybe a 'house' is broken into many small structures?\n    # maybe clerical error?\n    \n# all seem pretty rural. SUPER LOW DENSITY\n    # a bunch seem clustered around Lake Tahoe (i created a map)\n    # less pop per house, med house value, income\ndf[df['rooms_per_house'] > 40]\n\n# df.loc[df['rooms_per_house'] > 40][critical].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Where are districts with abnormally high rooms_per_house?","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize = (10,15))\naxs = fig.add_axes([0,0,1,1])\n\nfor poly in pts:\n    x = [t[0] for t in poly]\n    y = [t[1] for t in poly]\n    plt.fill(x, y,facecolor='whitesmoke')\n\ndef f(x):\n    if (x['median_house_value']-0) < (400000-0)/(14-6)*(x['median_income']-6):\n        return 'red'\n    else:\n        return 'whitesmoke'\n\ndf['A'] = df[['median_income','median_house_value']].apply(f, axis=1)\n\n# .loc[(df['median_house_value']-0) < (400000-0)/(14-6)*(df['median_income']-6)]\n\n\n# df.plot.scatter('long_mercator', 'lat_mercator', 20, ax = axs, c=df['A'], figsize = (10,15), cmap=cm.get_cmap('Spectral'), zorder = 2, alpha = 1)\ndf.loc[df['rooms_per_house'] > 40].plot.scatter('long_mercator', 'lat_mercator', 10, ax=axs, figsize = (10,15), zorder = 2, alpha=1)\nfig.savefig(\"densities.svg\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Created overlapping histograms comparing high rooms-per-house districts (orange) vs. all districts (blue) on several important variables","metadata":{}},{"cell_type":"code","source":"for c in critical:\n    print (\"PLOTTING: \", c)\n    fig, ax1 = plt.subplots()\n    ax2 = ax1.twinx()\n    x = df[c]\n    ydf = df.loc[df['rooms_per_house'] > 40]\n    y = ydf[c]\n    y.hist(ax=ax2, color='orange')\n    x.hist(ax=ax1)\n    plt.tight_layout()\n    if (c == 'density'):\n        plt.xlim(0,5000)\n    plt.show()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Price vs. Bedrooms_per_person","metadata":{}},{"cell_type":"code","source":"plt.scatter(df['bedrooms_per_person'], df['median_house_value'], s = .1)\n# plt.xlim([0,4])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1979 is a ski resort. A lot of overlap with rooms per house outliers. Again, very few houses\n\ndf.loc[df['bedrooms_per_person'] > 6]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Price vs. bedrooms_per_room","metadata":{}},{"cell_type":"code","source":"cumulative(df,critical)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rooms_per_house is centered about 1.5-2 (vs. 5-ish for total), which means these houses are generally quite small\n# Surprisingly, these are more expensive than on average. But that's also bc they're in denser regions (higher rent)\ncumulative(df.loc[df['bedrooms_per_room'] > .6], critical)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Geographical Analysis","metadata":{}},{"cell_type":"markdown","source":"### In the next 2 cells, we examine the distribution of population density values for the dataset. We find several distinct clusters.","metadata":{}},{"cell_type":"code","source":"l0 = -125\n# df['R'] = df['housing_median_age'].apply(lambda x: 'blue' if x > 30 else 'red')\ncol = ['darkred', 'tomato', 'darkorange','forestgreen','lightskyblue','thistle']\n# col = ['whitesmoke', 'darkorange','whitesmoke','whitesmoke','whitesmoke']\ndf['R'] = df['density'].apply(lambda x: col[0] if x >= 20000 else (col[1] if x >= 9200 else (col[2] if x >= 6500 else (col[3] if x >= 3700 else (col[4] if x >= 500 else col[5])))))\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(20,8))\naxs = fig.add_axes([0,0,1,1])\ndf.hist('density', ax=axs, bins=800)\nplt.title(\"Population Density Histogram\", fontsize = 30)\nplt.ylabel(\"Frequency\", fontsize = 20)\nplt.xlabel(\"Density Estimates\", fontsize = 20)\nplt.axvline(x=500, c='red', linestyle='--')\nplt.axvline(x=3700, c='red', linestyle='--')\nplt.axvline(x=6500, c='red', linestyle='--')\nplt.axvline(x=9200, c='red', linestyle='--')\nplt.axvline(x=20000, c='red', linestyle='--')\nplt.savefig('histo.svg')\nplt.ylim([0,600])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We find clear clustering on this histogram of density proxy values. Let's break the data points into 6 clusters, as seen above, and explore each. (NOTE: the cluster with the least density seems arbitrarily determined. I just used 500 ppl/mi, as defined in https://www.ers.usda.gov/topics/rural-economy-population/rural-classifications/what-is-rural/) ","metadata":{}},{"cell_type":"markdown","source":"### Cool visualization below","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize = (10,15))\naxs = fig.add_axes([0,0,1,1])\n\nfor poly in pts:\n    x = [t[0] for t in poly]\n    y = [t[1] for t in poly]\n    plt.fill(x, y,facecolor='whitesmoke')\n\ndf.plot.scatter('long_mercator', 'lat_mercator', .4, ax = axs, c=df['R'], figsize = (13,15), zorder = 2, alpha = 1)\nfig.savefig(\"densities.svg\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now, let's name the 6 groups\n* RR: rural\n* RS: rural suburban\n* SS: suburban\n* SU: suburban-urban\n* UU: urban\n* CC: city center","metadata":{}},{"cell_type":"code","source":"df_RR = df.loc[df['density'] < 500]\ndf_RS = df.loc[(500 <= df['density']) & (df['density'] < 3700)]\ndf_SS = df.loc[(3700 <= df['density']) & (df['density'] < 6500)]\ndf_SU = df.loc[(6500 <= df['density']) & (df['density'] < 9200)]\ndf_UU = df.loc[(9200 <= df['density']) & (df['density'] < 20000)]\ndf_CC = df.loc[20000 <= df['density']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_list = [df_RR, df_RS, df_SS, df_SU, df_UU, df_CC]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### For each of the critical variables, we plot its median (red dot) and IQR (blue line) for each of the 6 groups (labeled 0 to 5)\n#### There are some nice trends","metadata":{}},{"cell_type":"code","source":"for c in critical:\n    fig = plt.figure(figsize=(10,10))\n    axs = fig.add_axes([0,0,1,1])\n    plt.title(c)\n    for i, sub in enumerate(df_list):\n        cm = cumulative(sub, critical)\n        plt.plot([i,i],[cm[c].loc['25%'],cm[c].loc['75%']], color='lightblue')\n        plt.plot(i,cm[c].loc['50%'], 'ro', markersize=12)\n    plt.show()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We unravel some clean relationships between population density clusters and other variables in df.\n\nNow let's analyze each cluster separately","metadata":{}},{"cell_type":"markdown","source":"SS: weaker pop per household,\nSU, UU, and (less so) CC: weak bedrooms/rooms per person, pop per household, med income\nCC: {weak: }, {moderate: }","metadata":{}},{"cell_type":"code","source":"for c in critical:\n    plt.scatter(df_UU[c], df_UU['median_house_value'], s=.1)\n    plt.title(c)\n    plt.xlim([0,df_UU[c].quantile(.95)])\n    plt.show()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We find below that breaking the dataset into the 6 groups improves the \"predictive power\" (as judged by Pearson correlation coefficient) of some variables","metadata":{}},{"cell_type":"code","source":"df.corr()['median_house_value']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lizt = ['RR','RS','SS','SU','UU','CC']\nfor i, d in enumerate(df_list):\n    print('-'*20+lizt[i]+'-'*20)\n    cmat = d.corr()\n    print(cmat.loc[abs(cmat['median_house_value']) > .4]['median_house_value'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Housing Value Prediction","metadata":{}},{"cell_type":"markdown","source":"## Multiple Regression","metadata":{}},{"cell_type":"code","source":"means = [154375.371074, 221558.781652, 209589.774185, 216382.263378, 209971.951837, 239581.831597]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making the ocean_proximity variable quantitative\n\ndef makebin(x):\n    d = x['ocean_proximity']\n    mp = {'NEAR BAY':2, '<1H OCEAN':1, 'INLAND':0, 'NEAR OCEAN':3, 'ISLAND':4}\n    return mp[d]\n\ndf['ocean_proximity_bin'] = df[['ocean_proximity']].apply(makebin, axis=1)\ndf = df.drop(['R', 'A', 'long_mercator', 'lat_mercator', 'ocean_proximity'],axis=1)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Multiple regression for each cluster, separately\n(Printed RMSE, but didn't do further analysis of model)","metadata":{}},{"cell_type":"code","source":"from sklearn import linear_model\n\ndf_list_names = ['RR','RS','SS','SU','UU','CC']\n\nfor i, d in enumerate(df_list):\n    d['ocean_proximity_bin'] = d[['ocean_proximity']].apply(makebin, axis=1)\n    d = d.drop(['R', 'A', 'long_mercator', 'lat_mercator', 'ocean_proximity'],axis=1)\n#     print(d.columns)\n    variables = list(d.columns)\n    variables.remove('median_house_value')\n#     variables = ['median_income']\n    d = d.dropna()\n    X = d[variables]\n    y = d['median_house_value']\n    regr = linear_model.LinearRegression()\n    regr.fit(X, y)\n    \n    # evaluate\n    pred = regr.predict(np.array(d.drop('median_house_value', axis=1)))\n#     pred = regr.predict(np.array(pd.DataFrame(d['median_income'])))\n    actu = np.array(d['median_house_value'])\n    dumb = np.full((len(d.index),), means[i])\n    print(df_list_names[i]+' RMSE: ', (sum((pred-actu)**2)/len(d.index))**.5)\n    \n# 1: 60363.56455980125","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}