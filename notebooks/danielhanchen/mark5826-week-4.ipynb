{"cells":[{"metadata":{"_uuid":"7ed5dfc8e60b2a43c72b53902380d2475e6728e2"},"cell_type":"markdown","source":"<h2> Welcome to MARK5826 Week 4!</h2>\n\nLecturer In Charge: Junbum Kwon;Teaching Assistant: Daniel Han-Chen & James Lin\n\nIn week 3, we focused on Recommendation Algorithms!\n\nThis week (week 4), we will focus on **CONTENT OPTIMIZATION**. This week is the first week where we will use **machine learning**.\n\n<h2>AIM</h2>\n\nGiven movie data, find out and predict revenue / ratings, and understand WHAT contributes to the prediction.\n\n<h2>TABLEAU</h2>\n\nWe will be also using TABLEAU this week. Next week also.\n\n<h2>ASSIGNMENT 1 (week 3 to 5) 7 marks - DUE SATURDAY WEEK 5 10pm</h2>\n\nAssignment 1 is due Saturday week 5 10pm. We need you to submit a URL / LINK to your assignment notebook on Kaggle.\n\nIT MUST BE **PUBLIC**. Also, download the Kaggle file, and upload it to Moodle by 10pm Saturday week 5.\n\n<h2> Suggestions from Previous Weeks </h2>\n\n1. **|Slides|** We will upload the slides through email / moodle before the lectures!\n2. **|Applications|** Week 1 was intro. Week 2+ will be all industry applications.\n3. **|Lab Delivery|**  Speaking will be slower now (and content has been cut back a lot. We will explain more --> more quality over quantity).\n4. **|Too fast|** We will go slowly on the hard parts of the class. In fact, we will go through how to do Lab Questions as a class.\n5. **|Heavy Content|** Sorry about the overwhelming content! I'm sure from week 2 onwards, the code you'll see should look familiar.\n6. **|Slow Computers|** Some people's computers are slow. We have decided to optimise the code below (removing superfluous code)\n7. **|Heavy Content|** Lab Organisation has been improved, with one streamlined marking system.\n\n\n<h2>Week Topics</h2>\n\n(You can click the links below)\n\n[SKIP BELOW CODE TO CONTENT](#Content)\n<hr>\n\n1.[Reading in Data](#Content)\n\n2.[Plotting and Tabulating - Tableau](#Plotting and Tabulating)\n\n3.[Data Cleaning - Natural Language Processing](#Data Cleaning - Natural Language Processing)\n\n4.[Machine Learning - Supervised Learning](#Machine Learning - Supervised Learning)\n\n5.[Machine Learning - Interpretation Traps](#Machine Learning - Interpretation Traps)\n\n6.[Lab Questions](#Lab)"},{"metadata":{"_uuid":"5587c3a76d021bbd3a72cdae5da32e8bcfca2ecf"},"cell_type":"markdown","source":"[<h1>CLICK to SKIP BELOW CODE TO CONTENT</h1>](#Content)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import pandas as pd, numpy as np, os, gc, matplotlib.pyplot as plt, seaborn as sb, re, warnings, calendar, sys\nfrom numpy import arange\nget_ipython().run_line_magic('matplotlib', 'inline')\nwarnings.filterwarnings('ignore'); np.set_printoptions(suppress=True); pd.options.mode.chained_assignment = None\npd.set_option('display.float_format', lambda x: '%.3f' % x); pd.options.display.max_rows = 15\nglobal directory; directory = '../input'\n\ndef files(): return os.listdir(directory)\n\ndef read_clean(data):\n    data.columns = [str(x.lower().strip().replace(' ','_')) for x in data.columns]\n    seen = {}; columns = []; i = 0\n    for i,x in enumerate(data.columns):\n        if x in seen: columns.append(x+'_{}'.format(i))\n        else: columns.append(x)\n        seen[x] = None\n        \n    for x in data.columns[data.count()/len(data) < 0.0001]: del data[x];\n    gc.collect();\n    try: data = data.replace({'':np.nan,' ':np.nan});\n    except: pass;\n    \n    if len(data) < 10000: l = len(data);\n    else: l = 10000;\n    sample = data.sample(l);size = len(sample);\n    \n    for x in sample.columns:\n        ints = pd.to_numeric(sample[x], downcast = 'integer', errors = 'coerce')\n        if ints.count()/size > 0.97:\n            minimum = ints.min()\n            if minimum > 0: data[x] = pd.to_numeric(data[x], downcast = 'unsigned', errors = 'coerce')\n            else: data[x] = pd.to_numeric(data[x], downcast = 'integer', errors = 'coerce')\n        else:\n            floats = pd.to_numeric(sample[x], downcast = 'float', errors = 'coerce')\n            if floats.count()/size > 0.97: data[x] = pd.to_numeric(data[x], downcast = 'float', errors = 'coerce')\n            else:\n                dates = pd.to_datetime(sample[x], errors = 'coerce')\n                if dates.count()/size > 0.97: data[x] = pd.to_datetime(data[x], errors = 'coerce')\n    return data.reset_index(drop = True)\n\ndef read(x):\n    '''Kaggle Reading in CSV files.\n    Just type read('file.csv'), and you'll get back a Table.'''\n    \n    file = '{}/{}'.format(directory,x)\n    try:     data = pd.read_csv(file)\n    except:  data = pd.read_csv(file, encoding = 'latin-1')\n    return read_clean(data)\n\ndef tally(column, minimum = 0, top = None, graph = False, percent = False, multiple = False, lowercase = False, min_count = 1):\n    '''Provides a tally count of all values in a COLUMN.\n        1. minimum  =  (>0)          Least count of item to show.\n        2. top      =  (-1,>0)       Only show top N objects\n        3. graph    =  (False,True)  Show bar graph of results\n        4. percent  =  (False,>0)    Instead of showing counts, show percentages of total count\n        \n       multiple = False/True.\n       If True, counts and tallies objects in list of lists (Count Vectorizer)\n       \n       lowercase = True / False.\n       If True, lowers all text firsrt. So A == a\n       \n       min_count >= 1\n       If a column sum for tag has less than min_count, discard whole column\n    '''\n    if multiple == False:\n        counts = column.value_counts().astype('uint')\n        counts = counts[counts >= minimum][:top]\n        counts = pd.DataFrame(counts).reset_index()\n        counts.columns = [column.name, 'tally']\n        if percent: \n            counts['tally'] /= counts['tally'].sum()/100\n            counts['tally'] = counts['tally']\n        if graph:\n            C = counts[::-1]\n            C.plot.barh(x = column.name, y = 'tally', legend = False); plt.show();\n        return counts\n    else:\n        from sklearn.feature_extraction.text import CountVectorizer\n        column = column.fillna('<NAN>')\n        if type(column.iloc[0]) != list: column = column.apply(lambda x: [x])\n        counter = CountVectorizer(lowercase = lowercase, tokenizer = lambda x: x, dtype = np.uint32, min_df = min_count)\n        counter.fit(column)\n        counts = pd.DataFrame(counter.transform(column).toarray())\n        counts.columns = [column.name+'_('+str(x)+')' for x in counter.get_feature_names()]\n        return counts\n    \n    \ndef describe(data):\n    '''Provides an overview of your data\n        1. dtype    =  Column type\n        2. missing% =  % of the column that is missing\n        3. nunique  =  Number of unique values in column\n        4. top3     =  Top 3 most occuring items\n        5. min      =  Minimum value. If not a number column, then empty\n        6. mean     =  Average value. If not a number column, then empty\n        7. median   =  Middle value. So sort all numbers, and get middle. If not a number column, then empty\n        8. max      =  Maximum value. If not a number column, then empty\n        9. sample   =  Random 2 elements\n        10. name    =  Column Name\n    '''\n    dtypes = dtype(data)\n    length = len(data)\n    missing = ((length - data.count())/length*100)\n    \n    N = [];    most3 = []\n    for dt,col in zip(dtypes,data.columns):\n        if dt != 'datetime':\n            U = data[col].value_counts()\n            N.append(len(U))\n            if U.values[0] > 1: most3.append(U.index[:3].tolist())\n            else: most3.append([]);\n        else: N.append(0); most3.append([]);\n            \n    df = pd.concat([dtypes, missing], 1)\n    df.columns = ['dtype','missing%']\n    df['nunique'] = N; df['top3'] = most3\n    \n    numbers = list(data.columns[df['dtype'].isin(('uint','int','float'))])\n    df['min'] = data.min()\n    df['mean'] = data[numbers].mean()\n    df['median'] = data[numbers].median()\n    df['max'] = data.max()\n    df['sample'] = data.apply(lambda x : x.sample(2).values.tolist())\n    df['name'] = list(data.columns)\n    return df.sort_values(['missing%', 'nunique', 'dtype'], ascending = [False, False, True]).reset_index(drop = True)\n\n\ndef Checker(x):\n    if type(x) is pd.DataFrame: return 0\n    elif type(x) is pd.Series: return 1\n    else: return -1\n\ndef columns(data): return list(data.columns)\ndef rows(data): return list(data.index)\ndef index(data): return list(data.index)\ndef head(data, n = 10): return data.head(n)\ndef tail(data, n = 10): return data.tail(n)\ndef sample(data, n = 10): return data.sample(n)\n\ndef dtype(data):\n    what = Checker(data)\n    if what == 0:\n        dtypes = data.dtypes.astype('str')\n        dtypes = dtypes.str.split(r'\\d').str[0]\n    else:\n        dtypes = str(data.dtypes)\n        dtypes = re.split(r'\\d', dtypes)[0]\n    return dtypes\n\ndef mean(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].mean()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.mean()\n        else: return np.nan\n    else:\n        try:     return np.nanmean(data)\n        except:  return np.nan\n        \ndef std(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].std()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.std()\n        else: return np.nan\n    else:\n        try:     return np.nanstd(data)\n        except:  return np.nan\n        \ndef var(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].var()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.var()\n        else: return np.nan\n    else:\n        try:     return np.nanvar(data)\n        except:  return np.nan\n        \ndef log(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        x = np.log(data[numbers])\n        x[np.isinf(x)] = np.nan\n        return pd.Series(x)\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt:\n            x = np.log(data)\n            x[np.isinf(x)] = np.nan\n            return x\n        else: return np.nan\n    else:\n        try:\n            x = np.log(data)\n            x[np.isinf(x)] = np.nan\n            return x\n        except:  return np.nan\n        \ndef median(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].median()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.median()\n        else: return np.nan\n    else:\n        try:     return np.nanmedian(data)\n        except:  return np.nan\n        \ndef minimum(data):\n    what = Checker(data)\n    if what == 0:      return data.min()\n    elif what == 1:    return data.min()\n    else:              return np.min(data)\n        \ndef maximum(data):\n    what = Checker(data)\n    if what == 0:      return data.max()\n    elif what == 1:    return data.max()\n    else:              return np.max(data)\n    \ndef missing(data):\n    what = Checker(data)\n    if what >= 0:      return pd.isnull(data)\n    else:              return np.isnan(data)\n    \ndef count(data):\n    what = Checker(data)\n    if what >= 0:      return data.count()\n    else:              return len(data)\n    \ndef nunique(data):\n    what = Checker(data)\n    if what >= 0:      return data.nunique()\n    else:              return len(np.unique(data))\n    \ndef unique(data):\n    if type(data) is pd.DataFrame:\n        uniques = []\n        for x in data.columns:\n            uniques.append(data[x].unique())\n        df = pd.Series(uniques)\n        df.index = data.columns\n        return df\n    elif type(data) is pd.Series: return data.unique()\n    else:              return np.unique(data)\n    \ndef total(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].sum()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.sum()\n        else: return np.nan\n    else:\n        try:     return np.nansum(data)\n        except:  return np.nan\n        \ndef time_number(date): return hours(date)+minutes(date)/60+seconds(date)/60**2\ndef hours_minutes(date): return hours(date)+minutes(date)/60\ndef hours(date): return date.dt.hour\ndef minutes(date): return date.dt.minute\ndef seconds(date): return date.dt.second\ndef month(date): return date.dt.month\ndef year(date): return date.dt.year\ndef day(date): return date.dt.day\ndef weekday(date): return date.dt.weekday\ndef leap_year(date): return year(date).apply(calendar.isleap)\ndef date_number(date): return year(date)+month(date)/12+day(date)/(365+leap_year(date)*1)\ndef year_month(date): return year(date)+month(date)/12\n\ndef hcat(*columns):\n    cols = []\n    for c in columns:\n        if c is None: continue;\n        if type(c) in (list, tuple): \n            for i in c:\n                if type(i) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(i))\n                else: cols.append(i)\n        elif type(c) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(c))\n        else: cols.append(c)\n    return pd.concat(cols, 1)\n\ndef vcat(*columns):\n    cols = []\n    for c in columns:\n        if c is None: continue;\n        if type(c) in (list, tuple): \n            for i in c:\n                if type(i) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(i))\n                else: cols.append(i)\n        elif type(c) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(c))\n        else: cols.append(c)\n    return pd.concat(cols, 0)\n\ndef melt(data, columns):\n    '''Converts a dataset into long form'''\n    return data.melt(id_vars = columns)\n    \ndef tabulate(*columns, method = 'count'):\n    '''Splits columns into chunks, and counts the occurences in each group.\n        Remember - tabulate works on the LAST column passed.\n        Options:\n            1. count            = Pure Count in group\n            2. count_percent    = Percentage of Count in group\n            3. mean             = Mean in group\n            4. median           = Median in group\n            5. max              = Max in group\n            6. min              = Min in group\n            7. sum_percent      = Percentage of Sum in group\n        Eg:\n            Apple | 1\n            ---------\n            Orange| 3\n            ---------\n            Apple | 2\n            ---------\n        Becomes:\n            Apple | 1 | 1\n            -------------\n                  | 2 | 1\n            -------------\n            Orange| 3 | 1\n        \n        NOTE --------\n            method can be a list of multiple options.\n    '''\n    if type(method) in (list, tuple):\n        xs = []\n        for x in method:\n            g = tabulate(*columns, method = x)\n            xs.append(g)\n        xs = hcat(xs)\n        xs = xs.T.drop_duplicates().T\n        return read_clean(xs)        \n    else:\n        def percent(series):\n            counts = series.count()\n            return counts.sum()\n\n        data = hcat(*columns)\n        columns = data.columns.tolist()\n\n        if method in ('count', 'count_percent'):\n            groups = data.groupby(data.columns.tolist()).apply(lambda x: x[data.columns[-1]].count())\n\n            if method == 'count_percent':\n                groups = groups.reset_index()\n                groups.columns = list(groups.columns[:-1])+['Group_Count']\n                right = data.groupby(columns[:-1]).count().reset_index()\n                right.columns = list(right.columns[:-1])+['Group_Sum']\n\n                groups = pd.merge(left = groups, right = right, left_on = columns[:-1], right_on = columns[:-1])\n                groups['Percent%'] = groups['Group_Count']/groups['Group_Sum']*100\n                groups = groups[columns+['Percent%']]\n                return groups\n\n        elif method == 'mean': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].mean())\n        elif method == 'median': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].median())\n        elif method == 'max': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].max())\n        elif method == 'min': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].min())\n        elif method == 'sum_percent':\n            groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].sum()).reset_index()\n            groups.columns = list(groups.columns[:-1])+['Group_Count']\n            right = data.groupby(columns[:-1]).sum().reset_index()\n            right.columns = list(right.columns[:-1])+['Group_Sum']\n\n            groups = pd.merge(left = groups, right = right, left_on = columns[:-1], right_on = columns[:-1])\n            groups['Sum%'] = groups['Group_Count']/groups['Group_Sum']*100\n            groups = groups[cols+['Sum%']]\n            return groups\n        else:\n            print('Method does not exist. Please choose count, count_percent, mean, median, max, min, sum_percent.'); return None;\n        #except: print('Method = {}'.format(method)+' cannot work on Object, Non-Numerical data. Choose count.'); return None;\n\n        groups = pd.DataFrame(groups)\n        groups.columns = [method]\n        groups.reset_index(inplace = True)\n        return groups\n\n\ndef sort(data, by = None, how = 'ascending', inplace = False):\n    ''' how can be 'ascending' or 'descending' or 'a' or 'd'\n    It can also be a list for each sorted column.\n    '''\n    replacer = {'ascending':True,'a':True,'descending':False,'d':False}\n    if by is None and type(data) is pd.Series:\n        try:    x = replacer[how]\n        except: print(\"how can be 'ascending' or 'descending' or 'a' or 'd'\"); return None;\n        return data.sort_values(ascending = x, inplace = inplace)\n    elif type(how) is not list:\n        try:    how = replacer[how]\n        except: print(\"how can be 'ascending' or 'descending' or 'a' or 'd'\"); return None;\n    else:\n        for x in how: \n            try:    x = replacer[x]\n            except: print(\"how can be 'ascending' or 'descending' or 'a' or 'd'\"); return None;\n    return data.sort_values(by, ascending = how, inplace = inplace)\n\ndef keep(data, what, inplace = False):\n    '''Keeps data in a column if it's wanted.\n    Everything else is filled with NANs'''\n    if type(what) not in (list,tuple,np.array,np.ndarray): what = [what]\n    need = data.isin(what)\n    if inplace: \n        df = data\n        df.loc[~need] = np.nan\n    else: \n        df = data.copy()\n        df.loc[~need] = np.nan\n        return df\n\ndef remove(data, what, inplace = False):\n    '''Deletes data in a column if it's not wanted.\n    Everything else is filled with NANs'''\n    if type(what) not in (list,tuple): what = [what]\n    need = data.isin(what)\n    if inplace: \n        df = data\n        df.loc[need] = np.nan\n    else: \n        df = data.copy()\n        df.loc[need] = np.nan\n        return df\n    \n    \ndef ternary(data, condition, true, false = np.nan, inplace = False):\n    '''C style ternary operator on column.\n    Condition executes on column, and if true, is filled with some value.\n    If false, then replaced with other value. Default false is NAN.'''\n    try:\n        execute = 'data {}'.format(condition)\n        series = eval(execute)\n        try: series = series.map({True:true, False:false})\n        except: series = series.replace({True:true, False:false})\n        return series\n    except: print('Ternary accepts conditions where strings must be enclosed.\\nSo == USD not allowed. == \"USD\" allowed.'); return False;\n\n    \ndef locate(data, column):\n    '''Use ternary to get result and then filter with notnull'''\n    if dtype(column) == 'bool': return data.loc[column]\n    return data.loc[column.notnull()]\n    \ndef query(data, column = None, condition = None):\n    '''Querying data based on conditions'''\n    def Q(data, column, condition):\n        if column is not None:\n            if type(condition) in (np.array, np.ndarray, list, tuple, set):\n                cond = keep(data[column], tuple(condition))\n                cond = (cond.notnull())\n            else: cond = ternary(data[column], condition, True, False)\n            return data.loc[cond]\n        else:\n            if type(condition) in (np.array, np.ndarray, list, tuple, set):\n                cond = keep(data, tuple(condition))\n            else: cond = ternary(data, condition, True, False)\n            return data.loc[cond]\n    try:\n        return Q(data, column, condition)\n    except:\n        condition = condition.replace('=','==')\n        return Q(data, column, condition)\n        \ndef keep_top(x, n = 5):\n    '''Keeps top n (after tallying) in a column'''\n    df = keep(x, tally(x)[x.name][:n].values)\n    return df\n\ndef keep_bot(x, n = 5):\n    '''Keeps bottom n (after tallying) in a column'''\n    df = keep(x, tally(x)[x.name][:-n].values)\n    return df\n\n\ndef remove_outlier(x, method = 'iqr', range = 1.5):\n    '''Removes outliers in column with methods:\n        1. mean     =    meean+range (normally 3.5)\n        2. median   =    median+range (normally 3.5)\n        3. iqr      =    iqr+range (normally 1.5)\n    '''\n    i = x.copy()\n    if method == 'iqr':\n        first = np.nanpercentile(x, 0.25)\n        third = np.nanpercentile(x, 0.75)\n        iqr = third-first\n        i[(i > third+iqr*range) | (i < first-iqr*range)] = np.nan\n    else:\n        if method == 'mean': mu = np.nanmean(x)\n        else: mu = np.nanmedian(x)\n        std = np.nanstd(x)\n        i[(i > mu+std*range) | (i < mu-std*range)] = np.nan\n    return i\n\n\ndef cut(x, bins = 5, method = 'range'):\n    '''Cut continuous column into parts.\n        Method options:\n            1. range\n            2. quantile (number of quantile cuts)'''\n    if method == 'range': return pd.cut(x, bins = bins, duplicates = 'drop')\n    else: return pd.qcut(x, q = bins, duplicates = 'drop')\n    \n    \ndef plot(x, y = None, colour = None, column = None, data = None, size = 5, top = 10, wrap = 4, \n         subset = 5000, method = 'mean', quantile = True, bins = 10,\n         style = 'lineplot', logx = False, logy = False, logc = False, power = 1):\n    '''Plotting function using seaborn and matplotlib\n        Options:\n        x, y, colour, column, subset, style, method\n        \n        Plot styles:\n            1. boxplot\n            2. barplot\n            3. tallyplot (counting number of appearances)\n            4. violinplot (boxplot just fancier)\n            5. lineplot (mean line plot)\n            6. histogram\n            7. scatterplot (X, Y must be numeric --> dates will be converted)\n            8. bivariate (X, Y must be numeric --> dates will be converted)\n            9. heatmap (X, Y will be converted into categorical automatically --> bins)\n            10. regplot (X, Y must be numeric --> dates will be converted)\n    '''\n    if type(x) in (np.array,np.ndarray): x = pd.Series(x); x.name = 'x';\n    if type(y) in (np.array,np.ndarray): y = pd.Series(y); y.name = 'y';\n    if type(column) in (np.array,np.ndarray): column = pd.Series(column); column.name = 'column';\n    if type(colour) in (np.array,np.ndarray): colour = pd.Series(colour); colour.name = 'colour';\n        \n    if type(x) == pd.Series: \n        data = pd.DataFrame(x); x = x.name\n        if type(x) is not str:\n            data.columns = [str(x)]\n            x = str(x)\n    if method == 'mean': estimator = np.nanmean\n    elif method == 'median': estimator = np.nanmedian\n    elif method == 'min': estimator = np.min\n    elif method == 'max': estimator = np.max\n    else: print('Wrong method. Allowed = mean, median, min, max'); return False;\n    #----------------------------------------------------------\n    sb.set(rc={'figure.figsize':(size*1.75,size)})\n    dtypes = {'x':None,'y':None,'c':None,'col':None}\n    names = {'x':None,'y':None,'c':None,'col':None}\n    xlim = None\n    #----------------------------------------------------------\n    if data is not None:\n        if type(x) is str: x = data[x];\n        if type(y) is str: y = data[y]; \n        if type(colour) is str: colour = data[colour]; \n        if type(column) is str: column = data[column]; \n    if type(x) is str: print('Please specify data.'); return False;\n    #----------------------------------------------------------\n    if x is not None:\n        dtypes['x'] = dtype(x); names['x'] = x.name\n        if dtypes['x'] == 'object': x = keep_top(x, n = top)\n        elif dtypes['x'] == 'datetime': x = date_number(x)\n        if logx and dtype(x) != 'object': x = log(x)\n    if y is not None: \n        dtypes['y'] = dtype(y); names['y'] = y.name\n        if dtypes['y'] == 'object': y = keep_top(y, n = top)\n        elif dtypes['y'] == 'datetime': y = date_number(y)\n        if logy and dtype(y) != 'object': y = log(y)\n    if colour is not None:\n        dtypes['c'] = dtype(colour); names['c'] = colour.name\n        if dtypes['c'] == 'object': colour = keep_top(colour, n = top)\n        elif dtypes['c'] == 'datetime': colour = date_number(colour)\n        if logc and dtype(colour) != 'object': colour = log(colour)\n    if column is not None:\n        dtypes['col'] = dtype(column); names['col'] = column.name\n        if dtypes['col'] == 'object': column = keep_top(column, n = top)\n        elif dtypes['col'] == 'datetime': column = date_number(column)\n    #----------------------------------------------------------\n    df = hcat(x, y, colour, column)\n    if subset > len(df): subset = len(df)\n    df = sample(df, subset)\n    #----------------------------------------------------------\n    if column is not None:\n        if dtype(df[names['col']]) not in ('object', 'uint',' int') and nunique(df[names['col']]) > top: \n            if quantile: df[names['col']] = cut(df[names['col']], bins = bins, method = 'quantile')\n            else: df[names['col']] = cut(df[names['col']], bins = bins, method = 'range')\n    \n    try: df.sort_values(names['y'], inplace = True);\n    except: pass;\n    #----------------------------------------------------------\n    replace = {'boxplot':'box', 'barplot':'bar', 'tallyplot':'count', 'violinplot':'violin', \n               'lineplot': 'point', 'histogram':'lv'}\n    \n    if style == 'histogram' and y is None:\n        plot = sb.distplot(df[names['x']].loc[df[names['x']].notnull()], bins = bins)\n    elif style == 'lineplot' and y is None:\n        plot = plt.plot(df[names['x']]);\n        plt.show(); return;\n    elif style == 'barplot' and y is None:\n        plot = df.sort_values(names['x']).plot.bar();\n        plt.show(); return;\n    elif style in replace.keys():\n        if dtype(df[names['x']]) not in ('object', 'uint',' int') and nunique(df[names['x']]) > top: \n            if quantile: df[names['x']] = cut(df[names['x']], bins = bins, method = 'quantile')\n            else: df[names['x']] = cut(df[names['x']], bins = bins, method = 'range')\n        \n        if names['col'] is not None:\n            plot = sb.factorplot(x = names['x'], y = names['y'], hue = names['c'], data = df, kind = replace[style], col = names['col'],\n                             n_boot = 1, size = size, estimator = estimator, col_wrap = wrap)\n        else:\n            plot = sb.factorplot(x = names['x'], y = names['y'], hue = names['c'], data = df, kind = replace[style], col = names['col'],\n                             n_boot = 1, size = size, estimator = estimator)\n            \n        for ax in plot.axes.flatten(): \n            for tick in ax.get_xticklabels(): \n                tick.set(rotation=90)\n    \n    elif style == 'heatmap':\n        if dtype(df[names['x']]) != 'object'and nunique(df[names['x']]) > top:\n            if quantile: df[names['x']] = cut(df[names['x']], bins = bins, method = 'quantile')\n            else: df[names['x']] = cut(df[names['x']], bins = bins, method = 'range')\n                \n        if dtype(df[names['y']]) != 'object'and nunique(df[names['y']]) > top:\n            if quantile: df[names['y']] = cut(df[names['y']], bins = bins, method = 'quantile')\n            else: df[names['y']] = cut(df[names['y']], bins = bins, method = 'range')     \n\n        df = tabulate(df[names['x']], df[names['y']]).pivot(index = names['x'], columns = names['y'], values = 'count')\n        plot = sb.heatmap(df, cmap=\"YlGnBu\")\n\n        \n    elif dtype(df[names['x']]) == 'object' or dtype(df[names['y']]) == 'object':\n            print('{} can only take X = number and Y = number.'.format(style)); return False;\n        \n    elif style  in ('regplot', 'scatterplot'):\n        if column is None: col_wrap = None\n        else: col_wrap = wrap\n        if style == 'regplot': reg = True\n        else: reg = False\n        \n        plot = sb.lmplot(x = names['x'], y = names['y'], hue = names['c'], data = df, col = names['col'],\n                             n_boot = 2, size = size, ci = None, scatter_kws={\"s\": 50,'alpha':0.5},\n                        col_wrap = col_wrap, truncate = True, fit_reg = reg, order = power)\n        plot.set_xticklabels(rotation=90)\n        \n    elif style == 'bivariate':\n        plot = sb.jointplot(x = names['x'], y = names['y'], data = df, dropna = True, size = size, kind = 'reg',\n                           scatter_kws={\"s\": 50,'alpha':0.5}, space = 0)\n    plt.show()\n    \n    \ndef match_pattern(x, pattern, mode = 'find'):\n    '''Regex pattern finds in data and returns only match\n        \\d = digits\n        \\l = lowercase alphabet\n        \\p = uppercase alphabet\n        \\a = all alphabet\n        \\s = symbols and punctuation\n        \\e = end of sentence\n        \n        Modes =\n            1. find:   True/False if find or not\n            2. keep:   Output original string if match, else NAN\n            3. match:  Output only the matches in the string, else NAN\n        '''\n    pattern = pattern.replace('\\d','[0-9]').replace('\\l','[a-z]').replace('\\p','[A-Z]').replace('\\a','[a-zA-Z]')\\\n                .replace('\\s','[^0-9a-zA-Z]').replace('\\e', '(?:\\s|$)')\n    if dtype(x) != 'object': print('Data is not string. Convert first'); return False;\n\n    regex = re.compile(r'{}'.format(pattern))\n    \n    def patternFind(i):\n        try: j = re.match(regex, i).group(); return True\n        except: return False;\n    def patternKeep(i):\n        try: j = re.match(regex, i).group(); return i\n        except: return np.nan;\n    def patternMatch(i):\n        try: j = re.match(regex, i).group(); return j\n        except: return np.nan;\n    \n    if mode == 'find':        return x.apply(patternFind)\n    elif mode == 'keep':      return x.apply(patternKeep)\n    elif mode == 'match':     return x.apply(patternMatch)\n    \n    \ndef split(x, pattern):\n    '''Regex pattern finds in data and returns match. Then, it is splitted accordingly.\n        \\d = digits\n        \\l = lowercase alphabet\n        \\p = uppercase alphabet\n        \\a = all alphabet\n        \\s = symbols and punctuation\n        \\e = end of sentence\n        '''\n    pattern2 = pattern.replace('\\d','[0-9]').replace('\\l','[a-z]').replace('\\p','[A-Z]').replace('\\a','[a-zA-Z]')\\\n                .replace('\\s','[^0-9a-zA-Z]').replace('\\e', '(?:\\s|$)')\n    \n    if dtype(x) != 'object': print('Data is not string. Convert first'); return False;\n    \n    regex = re.compile(r'{}'.format(pattern))\n    if pattern == pattern2: return x.str.split(pattern)\n    else: return x.apply(lambda i: re.split(regex, i))\n    \ndef replace(x, pattern, with_ = None):\n    '''Regex pattern finds in data and returns match. Then, it is replaced accordingly.\n        \\d = digits\n        \\l = lowercase alphabet\n        \\p = uppercase alphabet\n        \\a = all alphabet\n        \\s = symbols and punctuation\n        \\e = end of sentence\n        '''\n    if type(pattern) is list:\n        d = {}\n        for l in pattern: d[l[0]] = l[1]\n        try:\n            return x.replace(d)\n        except:\n            return x.astype('str').replace(d)\n            \n    pattern2 = pattern.replace('\\d','[0-9]').replace('\\l','[a-z]').replace('\\p','[A-Z]').replace('\\a','[a-zA-Z]')\\\n                .replace('\\s','[^0-9a-zA-Z]').replace('\\e', '(?:\\s|$)')\n    \n    if dtype(x) != 'object': print('Data is not string. Convert first'); return False;\n    \n    regex = re.compile(r'{}'.format(pattern))\n    if pattern == pattern2: return x.str.replace(pattern, with_)\n    else: return x.apply(lambda i: re.sub(regex, with_, i))\n    \ndef remove(x, what):\n    return replace(x, what, '')\n    \ndef notnull(data, loc = None):\n    '''Returns the items that are not null in a column / dataframe'''\n    if loc is not None:\n        return data.loc[loc.notnull()]\n    else:\n        return data.loc[data.notnull().sum(1) == data.shape[1]]\n    \n    \ndef exclude(data, col):\n    '''Only returns a dataframe where the columns in col are not included'''\n    if type(col) is str: col = [col]\n    columns = list(data.columns)\n    leave = list(set(columns) - set(col))\n    return data[leave]\n\n################### -----------------------------------------------------------------#######################\n#Recommendation Systems\ndef pivot(index, columns, values):\n    '''Creates a table where rows = users, columns = items, and cells = values / ratings'''\n    from scipy.sparse import dok_matrix\n    S = dok_matrix((nunique(index), nunique(columns)), dtype=np.float32)\n    \n    mins = np.abs(np.min(values))+1\n    indexM = {}\n    for i,x in enumerate(unique(index)): indexM[x] = i;\n    columnsM = {}\n    for i,x in enumerate(unique(columns)): columnsM[x] = i;\n        \n    for i,c,v in zip(index, columns, values+mins): S[indexM[i],columnsM[c]] = v;\n    \n    S = S.toarray(); S[S == 0] = np.nan; S -= mins\n    S = pd.DataFrame(S)\n    S.index = indexM.keys(); S.columns = columnsM.keys();\n    return S\n\ndef row_operation(data, method = 'sum'):\n    '''Apply a function to a row\n        Allowed functions:\n            1. sum\n            2. median\n            3. mean\n            4. max\n            5. min\n            6. count\n            7. count_percent\n            8. sum_percent\n            9. mean_zero         (Mean but zeroes arent counted)\n            10. count_zero       (Count but zeroes arent counted)\n        Own functions are also allowed.\n    '''\n    if method in ('sum','median','mean','max','min','count'):\n        x = eval('data.{}(1)'.format(method))\n    elif method in ('count_percent', 'sum_percent'):\n        x = eval('data.{}(1)'.format(method.split('_')[0]))\n        x /= x.sum()\n        x *= 100\n    elif method in ('mean_zero', 'count_zero'):\n        df = data.copy()\n        df[df == 0] = np.nan\n        x = eval('df.{}(1)'.format(method.split('_')[0]))\n    else: return data.apply(method, axis = 1)\n    x.name = 'row_operation'\n    return x\n\n\ndef col_operation(data, method = 'sum'):\n    '''Apply a function to a column\n        Allowed functions:\n            1. sum\n            2. median\n            3. mean\n            4. max\n            5. min\n            6. count\n            7. count_percent\n            8. sum_percent\n            9. mean_zero         (Mean but zeroes arent counted)\n            10. count_zero       (Count but zeroes arent counted)\n        Own functions are also allowed.\n        '''\n    if method in ('sum','median','mean','max','min','count'):\n        x = eval('data.{}(0)'.format(method))\n    elif method in ('count_percent', 'sum_percent'):\n        x = eval('data.{}(0)'.format(method.split('_')[0]))\n        x /= x.sum()\n        x *= 100\n    elif method in ('mean_zero', 'count_zero'):\n        df = data.copy()\n        df[df == 0] = np.nan\n        x = eval('df.{}(0)'.format(method.split('_')[0]))\n    else: return data.apply(method, axis = 0)\n    x.name = 'col_operation'\n    return x\n\n    \ndef random(obj, n = 1, p = None):\n    if p is not None:\n        if type(p) is pd.Series: p = p.values\n        if p.sum() > 2: p /= 100\n    return list(np.random.choice(obj, size = n, replace = False, p = p))\n\ndef row(data, n): return data.loc[n]\n\ndef distances(source, target):\n    '''Returns all distances between target and source (L2)'''\n    Y = np.tile(target.values, (source.shape[0],1))\n    nans = np.isnan(Y)\n    X = source.values; X[np.isnan(X)] = 0;\n    Y[nans] = 0;\n    diff = X - Y;\n    diff[nans] = 0;\n    d = np.linalg.norm(diff, axis = 1)\n    j = pd.Series(d)\n    j.index = source.index\n    return j\n\n################### -----------------------------------------------------------------#######################\n#Natural Language Processing & Machine Learning\n\ndef multiply(left, right):\n    ''' Multiplies 2 tables or columns together.\n        Will do automatic type casting'''\n\n    if len(left.shape) == 1:\n        try: return left.values.reshape(-1,1)*right\n        except: return left.reshape(-1,1)*right\n    elif len(right.shape) == 1:\n        try: return right.values.reshape(-1,1)*left\n        except: return right.reshape(-1,1)*left\n    else:\n        return left*right\n    \n    \ndef clean(data, missing = 'mean', remove_id = True):\n    '''Cleans entire dataset.\n    1. missing =\n        mean, max, median, min\n        Fills all missing values with column mean/median etc\n\n    2. remove_id = True/False\n        Checks data to see if theres an ID column.\n        Removes it (not perfect)\n    '''\n    x = data[data.columns[dtype(data) != 'object']].copy()\n    for c in x.columns[x.count()!=len(x)]:\n        x[c] = eval('x[c].fillna(x[c].{}())'.format(missing))\n    if remove_id:\n        for c in x.columns[(dtype(x) == 'int')|(dtype(x) == 'uint')]:\n            if x[c].min() >= 0:\n                j = (x[c] - x[c].min()).sort_values().diff().sum()\n                if j <= 1.001*len(x) and j >= len(x)-1: x.pop(c);\n    return x\n\n\ndef scale(data):\n    columns = data.columns\n    from sklearn.preprocessing import StandardScaler\n    scaler = StandardScaler().fit(data)\n    X = pd.DataFrame(scaler.transform(data))\n    X.columns = columns\n    return X\n\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom keras.models import Sequential, load_model\nfrom sklearn.linear_model import LassoLarsIC\nfrom sklearn.linear_model import Ridge\nfrom sklearn.utils import class_weight\n# from keras.layers import Dense, Activation, GaussianNoise, BatchNormalization, Dropout\n# from keras.initializers import glorot_normal\n# from keras.callbacks import *\n# from keras.optimizers import Nadam, SGD\n\nclass LinearModel(BaseEstimator, RegressorMixin):\n\n    def __init__(self, lasso = False, scale = True, logistic = False, layers = 0, activation = 'tanh', epochs = 50,\n                    time = None, shift = 1, test_size = 0.2, early_stopping = 7, lr = 0.1):\n        self.scale = scale; self.logistic = logistic; self.lasso = lasso; self.layers = layers;\n        assert activation in ['tanh','relu','sigmoid','linear']\n        assert shift > 0;\n        self.activation = activation; self.epochs = epochs; self.time = time; self.shift = shift\n        if logistic or (self.logistic == False and self.layers > 0): self.model = Sequential()\n        elif lasso: self.model = LassoLarsIC()\n        else: self.model = Ridge()\n        self.mapping = {}; self.test_size = test_size; self.early_stopping = early_stopping\n        self.lr = lr\n\n    def fit(self, X, Y):\n        print('Model now fitting...')\n        X = self._process_X(X.copy())\n        X = self._time_transform(X)\n        \n        self.uniques, self.columns = X.apply(self._range_unique), list(X.columns)\n        if self.scale: X, self.means, self.stds = self._scaler(X)\n        self.uniques_scale = X.apply(self._range_unique)\n        \n        if self.logistic:\n            Y = self._process_Y(Y)\n            self._fit_keras(X, Y)\n        else:\n            try: \n                if self.layers == 0: self._fit_sklearn(X, Y)\n                else: \n                    self.out = 1\n                    if Y.min() >= 0:\n                        if Y.max() <= 1: self.activ = 'sigmoid'\n                        else: self.activ = 'relu'\n                    elif Y.min() >= -1 and Y.max() <= 1:\n                        self.activ = 'tanh'\n                    else: self.activ = 'linear'\n                    self.loss = 'mse'\n                    self._fit_keras(X, Y)\n            except: \n                print('Y is not numeric. Choose logistic = True for classification'); return None\n\n        self._store_coefficients()\n        self._df = self._store_analysis(X)\n        print('Model finished fitting')\n\n        \n    def predict(self, X):\n        X = self._process_X(X.copy())\n        X = self._time_transform(X)\n        X = self._transform(X)\n        if self.logistic:\n            prob = self.model.predict(X)\n            if self.activ == 'sigmoid': prob = prob.round().astype('int').flatten()\n            else: prob = prob.argmax(1).astype('int').flatten()\n            prob = pd.Series(prob).replace(self.mapping)\n            return prob\n        else: return self.model.predict(X).flatten()\n\n\n    def predict_proba(self, X):\n        if self.logistic:\n            X = self._process_X(X.copy())\n            X = self._time_transform(X)\n            X = self._transform(X)\n            prob = self.model.predict(X).flatten()\n            return prob\n        else: print('Predict Probabilities only works for logisitc models.'); return None;\n\n        \n    def coefficients(self, plot = False, top = None):\n        df = self.coef\n        if self.layers == 0:\n            if top is not None: df = df[:top]\n            if plot:\n                df = df.fillna('')\n                if len(self.mapping) > 2:\n                    df = df.style.bar(subset = [x for x in df.columns if 'Y=(' in x], align='mid', color=['#d65f5f', '#5fba7d'])\n                else:\n                    df = df.style.bar(subset = ['Coefficient'], align='mid', color=['#d65f5f', '#5fba7d'])\n        return df\n    \n    \n    def plot(self, predictions, real_Y):\n        if self.logistic:\n            from sklearn.metrics import confusion_matrix\n            conf = pd.DataFrame(confusion_matrix(real_Y, predictions))\n            try: \n                conf.index = [f'True({x}/{i})' for i,x in zip(self.mapping.keys(), self.mapping.values())]\n                conf.columns = [f'{x}/{i}' for i,x in zip(self.mapping.keys(), self.mapping.values())]\n            except: \n                conf.index = [f'True({x})' for x in range(nunique(real_Y))]\n            conf = conf.divide(conf.sum(1), axis = 'index')*100\n            return sb.heatmap(conf, cmap=\"YlGnBu\", vmin = 0, vmax = 100, annot = True)\n        else:\n            return plot(x = predictions, y = real_Y, style = 'regplot')\n    \n    \n    def score(self, predictions, real_Y):\n        if self.logistic:\n            from sklearn.metrics import matthews_corrcoef\n            coef = matthews_corrcoef(real_Y, predictions)\n            if np.abs(coef) < 0.3: print('Model is not good. Score is between (-0.3 and 0.3). A score larger than 0.3 is good, or smaller than -0.3 is good.') \n            else: print('Model is good.')\n            return coef\n        else:\n            from sklearn.metrics import mean_squared_error\n            error = np.abs(np.sqrt(mean_squared_error(real_Y, predictions))/np.mean(real_Y))\n            if error > 0.4: print('Model is not good. Score is larger than 40%. Smaller than 40% relative error is good.')\n            else: print('Model is good.')\n            return error\n    \n    \n    def analyse(self, column = None, plot = False, top = 20):\n        if self.layers == 0:\n            df = self._df.round(2)\n            if self.logistic:\n                if column is not None: df = df.loc[column]\n                else: df = df[:top]\n                def color_negative_red(val):\n                    color = 'lightgreen' if val == 'Add 1' else 'pink'\n                    return 'background-color: %s' % color\n\n                def highlight_max(s):\n                    is_max = s == s.max()\n                    return ['color: lightgreen' if v else '' for v in is_max]\n\n                if plot:\n                    df = df.fillna('')\n                    if self.activ == 'sigmoid':\n                        df = df.style.bar(align = 'mid', width = 75, color = ['gray'])\\\n                                    .applymap(color_negative_red, subset = ['Effect'])\n                    else:\n                        df = df.style.bar(align = 'mid', width = 75, color = ['gray'])\\\n                                    .applymap(color_negative_red, subset = ['Effect'])\\\n                                    .apply(highlight_max, subset = df.columns[2:], axis = 1)\n            else:\n                if column is not None: df = pd.DataFrame(df.loc[[column]])\n                else: df = df[:top]\n                if plot:\n                    cols = list(df.columns); cols.remove('If Stays'); cols.remove('Change if Removed')\n                    df[cols] = df[cols].fillna('')\n                    def color_negative_red(val):\n                        if val == True: color = 'cyan'\n                        elif val == False: color = 'pink'\n                        else: color = ''\n                        return 'background-color: %s' % color\n\n                    df = df.style.bar(subset = ['Coefficient','If Stays','Change if Removed','Best Addon',\n                                               'Worst Reduced'], align='mid', color=['#d65f5f', '#5fba7d'])\\\n                            .applymap(color_negative_red, subset = ['Stay'])\\\n                            .bar(subset = ['Best Contrib','Worst Contrib'], align='mid', color=['pink', 'cyan'])\n            return df\n        else:\n            print(\"Can't analyse since it's a neural network. I can only give you the model layout and loss graphs\")\n            print(self.model.summary()); history = self.history\n            plt.plot(history.history['loss']); plt.plot(history.history['val_loss'])\n            plt.title('model loss'); plt.ylabel('loss');plt.xlabel('epoch')\n            plt.legend(['train', 'test'], loc='upper left')\n            plt.show()\n        \n        \n    def degrees(self, prediction, real_Y):\n        '''The closer the degree of the fit line to 45*, the better!'''\n        if not self.logistic:\n            from sklearn.linear_model import Ridge as modeller\n            models = modeller().fit(prediction.reshape(-1,1),real_Y)\n            deg = np.round((np.arctan(models.coef_[0]))/np.pi*180, 3)\n            if deg <= 50 and deg > 45: print('Prediction seems good, but probably overpredicting')\n            elif deg > 50: print(\"Prediction doesn't seem good. It's overpredicting\")\n            elif deg == 45: print(\"Prediction looks ideal! It's quite smooth\")\n            elif deg <= 45 and deg > 40: print(\"Prediction seems good, but probably underpredicting\")\n            else: print(\"Prediction doesn't seem good. It's underpredicting\")\n            return deg\n        else: print('Model is not regression. Use score instead'); return None;\n        \n        \n    def _process_X(self, X):\n        try: X.shape[1]\n        except: X = X.reshape(-1,1)\n        if type(X) is not pd.DataFrame: X = pd.DataFrame(X)\n        try: X = X[self.columns]\n        except: pass\n        return X\n\n\n    def _process_Y(self, Y):\n        if type(Y) is not pd.Series: Y = pd.Series(Y)\n        n = nunique(Y); Y = Y.astype('category')\n        self.mapping = dict(enumerate(Y.cat.categories))\n        self.reverse_mapping = dict(zip(self.mapping.values(), self.mapping.keys()))\n        Y = Y.cat.codes\n        \n        class_weights = class_weight.compute_class_weight('balanced', list(self.mapping.keys()), Y)\n        self.class_weights = dict(enumerate(class_weights))\n        \n        if n == 2:\n            self.activ, self.loss, self.out = 'sigmoid', 'binary_crossentropy', 1\n        else:\n            self.activ, self.loss = 'softmax', 'categorical_crossentropy'\n            Y = pd.get_dummies(Y); self.out = Y.shape[1]\n        return Y\n    \n    \n    def _time_transform(self, X):\n        if self.time is not None:\n            X.sort_values(self.time, inplace = True)\n            alls = [X]\n            for s in range(1,self.shift+1):\n                ss = X.shift(s); ss.columns = [x+f'({-s})' for x in ss.columns]\n                alls.append(ss)\n            X = pd.concat(alls, 1)\n            X.fillna(method = 'backfill', inplace = True); X.sort_index(inplace = True)\n        return X\n    \n        \n    def _store_coefficients(self):\n        if self.logistic:\n            if self.layers == 0:\n                coefs = pd.DataFrame(self.coef_)\n                if len(self.mapping) > 2: coefs.columns, coefs.index = [f'Y=({x})' for x in self.mapping.values()], self.columns\n                else: coefs.columns, coefs.index = ['Coefficient'], self.columns\n                coefs['Abs'] = np.abs(coefs).sum(1)\n                coefs['Mean'], coefs['Std'], coefs['Range'], coefs['Scale'] = self.means, self.stds, self.uniques, self.uniques_scale\n                coefs.sort_values('Abs', inplace = True, ascending = False); coefs.pop('Abs');\n                self.coef = coefs\n            else: self.coef = self.coef_\n        else:\n            if self.layers == 0:\n                df = pd.DataFrame({'Coefficient':self.coef_ , 'Abs' : np.abs(self.coef_),\n                                    'Mean':self.means, 'Std':self.stds, 'Range':self.uniques, 'Scale':self.uniques_scale})\n                df.index = self.columns; df.sort_values('Abs', ascending = False, inplace = True)\n                df.pop('Abs');\n                self.coef = df\n            else: self.coef = self.coef_\n                \n\n    def _store_analysis(self, X):\n        if self.logistic:\n            if self.layers == 0:\n                coefs = pd.DataFrame(self.coef_)\n                if self.activ == 'sigmoid':\n                    col = 'Probability (Y={})'.format(max(list(self.mapping.values())))\n                    coefs.columns = [col]\n                    coefs.index = self.columns\n                    exponential = np.exp(1*coefs + self.bias_)\n                    exponential = exponential.divide(exponential + 1)*100\n                    exponential['Effect'] = 'Add 1'\n\n                    neg_exponential = np.exp(-1*coefs + self.bias_)\n                    neg_exponential = neg_exponential.divide(neg_exponential + 1)*100\n                    neg_exponential['Effect'] = 'Minus 1'\n\n                    coefs = pd.concat([exponential, neg_exponential]).round(2)\n                    coefs.reset_index(inplace = True); coefs.columns = ['Column']+list(coefs.columns[1:])\n                    coefs.sort_values(col, ascending = False, inplace = True)\n                else:\n                    coefs.columns, coefs.index = [f'Y=({x})' for x in self.mapping.values()], self.columns\n                    exponential = np.exp(1*coefs + self.bias_)\n                    exponential = exponential.divide(exponential.sum(1), axis = 0)*100\n                    exponential['Effect'] = 'Add 1'\n\n                    neg_exponential = np.exp(-1*coefs + self.bias_)\n                    neg_exponential = neg_exponential.divide(neg_exponential.sum(1), axis = 0)*100\n                    neg_exponential['Effect'] = 'Minus 1'\n\n                    coefs = pd.concat([exponential, neg_exponential])\n                    coefs.reset_index(inplace = True)\n                    coefs.columns = ['Column']+list(coefs.columns[1:])\n                    coefs = coefs[['Column','Effect']+list(coefs.columns)[1:-1]].round(2)\n\n                    coefs['Max'] = coefs.max(1); coefs.sort_values('Max', ascending = False, inplace = True); del coefs['Max'];\n                return coefs\n            else: return None\n        else:\n            if self.layers == 0:\n                full = X*self.coef_\n                transformed = full.sum(1) + self.bias_\n                selects, unselects, worst, best, W, B, L, original_G, original_B, overall = [],[],[],[],[],[],[],[],[],[]\n\n                for i, (col, mu) in enumerate(zip(self.columns, self.means)):\n                    if np.isnan(mu):\n                        cond = (X[col]!=0)\n                        select = transformed.loc[cond]\n                        unselect = transformed.loc[~cond]\n                        selects.append(select.mean())\n                        unselects.append(unselect.mean())\n\n                        original = X.loc[cond].mean(0)\n                        d = full.loc[cond].mean(0)\n                        dx = full.loc[~cond].mean(0)\n\n                        d = pd.DataFrame({col: d, 'Abs': np.abs(d)}).sort_values('Abs', ascending = False)[col]\n                        s = (d.index == col)\n                        d = d.loc[~s].sort_values(ascending = False)\n                        first = d.index[0]; end = d.index[-1]\n                        best.append(first)\n                        B.append(d[0]-dx.loc[first])\n                        worst.append(d.index[-1])\n                        W.append(d[-1]-dx.loc[end])\n                        L.append(len(select))\n\n                        original_G.append(original.loc[first])\n                        original_B.append(original.loc[end])\n                    else:\n                        selects.append(np.nan); unselects.append(np.nan); L.append(np.nan)\n\n                        gt = (full.gt(full[col], axis = 'index')*full)\n                        gt[gt == 0] = np.nan; gt_means = gt.mean(0).sort_values(ascending = False)\n                        changes = gt.subtract(full[col], axis = 'index').mean(0)\n                        b = gt_means.index[0]; b_add = changes.loc[b]; b_contrib = gt_means.iloc[0]\n                        best.append(b); B.append(b_add); original_G.append(b_contrib)\n\n                        lt = (full.lt(full[col], axis = 'index')*full)\n                        lt[lt == 0] = np.nan; lt_means = lt.mean(0).sort_values(ascending = True)\n                        changes = lt.subtract(full[col], axis = 'index').mean(0)\n                        w = lt_means.index[0]; w_add = changes.loc[w]; w_contrib = lt_means.iloc[0]\n                        worst.append(w); W.append(w_add); original_B.append(w_contrib)\n\n\n                df = pd.DataFrame({'Coefficient':self.coef_, 'N':L,'If Stays':selects, 'Removed':unselects, 'Change if Removed': 0, 'Stay' : 0,\n                                  'Best Combo':best, 'Best Addon':B,'Best Contrib':original_G,'Worst Combo':worst, 'Worst Reduced':W, 'Worst Contrib':original_B})\n\n                df['Change if Removed'] = df['Removed'] - df['If Stays']\n                df['Stay'] = (df['Change if Removed'] < 0); df['Abs'] = np.abs(df['Change if Removed'])\n                df.loc[df['N'].isnull(), 'Stay'] = np.nan\n                df['Abs_Coef'] = np.abs(df['Coefficient'])\n                df.index = self.columns\n                df.sort_values(['Abs','Abs_Coef'], ascending = [False,False], inplace = True)\n                df.pop('Abs'); df.pop('Removed'); df.pop('Abs_Coef');\n                return df\n            else: return None\n\n    def _fit_keras(self, X, Y):\n        self.model.add(GaussianNoise(0.01, input_shape = (X.shape[1],)))\n        \n        for l in range(self.layers):\n            self.model.add(Dense(X.shape[1], kernel_initializer = glorot_normal(seed = 0)))\n            self.model.add(Activation(self.activation))\n            self.model.add(BatchNormalization())\n            self.model.add(Dropout(0.15))\n            self.model.add(GaussianNoise(0.01))\n            \n        self.model.add(Dense(self.out, kernel_initializer = glorot_normal(seed = 0)))\n        self.model.add(Activation(self.activ))\n    \n        earlyStopping = EarlyStopping(monitor = 'val_loss', patience = int(self.early_stopping*(self.layers/2+1)), verbose = 0, mode = 'min')\n        reduce_lr_loss = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = int(1*(self.layers/2+1)), verbose = 0, epsilon = 1e-4, mode = 'min')\n        cycle = CyclicLR(base_lr = 0.0005, max_lr = self.lr, step_size = 2000, mode = 'exp_range')\n        checkpoint = ModelCheckpoint('Best_Model.hdf5', save_best_only = True)\n        \n        self.metrics = ['acc']        \n        if not self.logistic: self.class_weights = None; self.metrics = None\n        \n        self.model.compile(optimizer = Nadam(), loss = self.loss, metrics = self.metrics)\n\n        if len(X) < 100: bs = 10\n        elif len(X) < 200: bs = 20\n        elif len(X) < 300: bs = 30\n        else: bs = 32\n\n        self.history = self.model.fit(X, Y, epochs = self.epochs, batch_size = bs, verbose = 2, validation_split = self.test_size, shuffle = True,\n                    callbacks = [earlyStopping, TerminateOnNaN(), reduce_lr_loss, cycle, checkpoint], \n                   class_weight = self.class_weights)\n        self.model = load_model('Best_Model.hdf5')\n        if self.layers == 0: self.coef_, self.bias_ = self.model.get_weights()\n        else: self.coef_ = self.model.get_weights()\n        self.lr = cycle\n        \n        \n    def _fit_sklearn(self, X, Y):\n        self.model.fit(X, Y)\n        self.coef_, self.bias_ = self.model.coef_, self.model.intercept_\n\n\n    def _range_unique(self, x):\n        s = x.sort_values(ascending = True).values\n\n        mins, maxs = np.round(s[0], 2), np.round(s[-1], 2)\n        length = len(s)/4\n        qtr1, qtr3 = np.round(s[int(length)], 2), np.round(s[int(3*length)], 2)\n        return sorted(set([mins, qtr1, qtr3, maxs]))\n\n\n    def _scaler(self, X):\n        result = []; means = []; stds = []\n        \n        for col in X.columns:\n            df = X[col]\n            if df.nunique() == 2 and df.min() == 0 and df.max() == 1:\n                result.append(df); means.append(np.nan); stds.append(np.nan)\n            else:\n                mu, std = df.mean(), df.std()\n                means.append(mu); stds.append(std)\n                result.append((df-mu)/std)\n        return pd.concat(result, 1), np.array(means), np.array(stds)\n\n\n    def _transform(self, X):\n        if self.scale:\n            final = []\n            for col, mu, std in zip(self.columns, self.means, self.stds):\n                if np.isnan(mu): final.append(X[col])\n                else: final.append((X[col]-mu)/std)\n            X = pd.concat(final, 1)\n        return X\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0c65026adeabd49c5e5045a3720481b0b2a4593"},"cell_type":"markdown","source":"<a id='Content'></a>\n<h1> 1. Reading Data </h1>"},{"metadata":{"trusted":true,"_uuid":"1f7da9e772e5d093dbda7bc33b2e92d6a206ed74","collapsed":true},"cell_type":"code","source":"files()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c50f45ccfdf41112955e74eed5bbdc0ed0752615","collapsed":true},"cell_type":"code","source":"data = read('IMDB-Movie-Data.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2de0b0c1d016a71dc5abe46665364d64bc809c8a"},"cell_type":"markdown","source":"Let's DESCRIBE the data again using DESCRIBE"},{"metadata":{"trusted":true,"_uuid":"23c7d1213e1948563d95e91c77c7c2db400128a5","collapsed":true},"cell_type":"code","source":"describe(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec1d55ca9b9347b86babdfcda8e597354e007737","collapsed":true},"cell_type":"code","source":"head(data, 5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5de79a6d501448dc01ca8df4123d353370d66e0e"},"cell_type":"markdown","source":"<a id='Plotting and Tabulating'></a>\n<h1> 2. Plotting and Tabulating - Tableau</h1>"},{"metadata":{"_uuid":"418d898fbd998912956a2bc645e2de71dcc57023"},"cell_type":"markdown","source":"We will now use **Tableau** to do the plotting for this data!"},{"metadata":{"_uuid":"f22b0f4f39192875e84530e9bea908f657890aa6"},"cell_type":"markdown","source":"<a id='Data Cleaning - Natural Language Processing'></a>\n<h1> 3. Data Cleaning - Natural Language Processing</h1>"},{"metadata":{"_uuid":"04ecda6f67245322a31b9309985e7722160ee3e2"},"cell_type":"markdown","source":"Remember the genre and we plotted it against years?\n\nLet's investigate it again."},{"metadata":{"trusted":true,"_uuid":"e1219f59671781155794d373d9588cbb7a022313","collapsed":true},"cell_type":"code","source":"head(data['genre'],10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"99036da44ca7b13f0f8c2838f4cc543704cbaf83"},"cell_type":"markdown","source":"Clearly, theres a problem when we plotted the data. We need to SPLIT the column by the comma!\n\nBUT, before, remove all SPACES using REMOVE. Why?\n\nSay:  [apple, banana, hello] and [banana, apple, hello]\n\nIf we just split by a comma, then apple in list 1 and apple in list 2 will NOT be matched together, since in the second list, its _apple (space)."},{"metadata":{"trusted":true,"_uuid":"f9727da47b4467b11e6bc214672db8261c6408df","collapsed":true},"cell_type":"code","source":"data['genre'] = remove(data['genre'], ' ')\nsplits = split(data['genre'], ',')\nhead(splits, 10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6695c33462a61f6ce7806865de4e0e8f3ea90f9"},"cell_type":"markdown","source":"Now, after we split the column by commas, we want to transform the above into a table format of counts.\n\nThis method is called BAG OF WORDS or COUNT VECTORIZING.\n\nWe count the occurences of each object and store the count in a large table.\n\n<img src=\"https://drive.google.com/uc?id=1J3FmNZykye0SZJDH9HAu6bmuoDQfsEwd\" style=\"width: 700px;\"/>\n\nSo, to do this , use TALLY again, just set MULTIPLE = True"},{"metadata":{"trusted":true,"_uuid":"232fded972a729fd913282b60b9c03214bdd4565","collapsed":true},"cell_type":"code","source":"genres = tally(splits, multiple = True)\nhead(genres, 10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48d57d62c8d7eb6e015fb07e4244d1af16fc428f"},"cell_type":"markdown","source":"Next, let's find the COLUMN SUM using COL_OPERATION and 'sum'"},{"metadata":{"trusted":true,"_uuid":"fca19d23db443d9505466bfd732014d2bfb7c1ee","collapsed":true},"cell_type":"code","source":"col_operation(genres, 'sum')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2409377f746bf9651237872b1b113ec6a7b80188"},"cell_type":"markdown","source":"Now use PLOT and BARPLOT to plot the genres total"},{"metadata":{"trusted":true,"_uuid":"c31e5aeb9cbd09604d17e0f07b02395f0319a54c","collapsed":true},"cell_type":"code","source":"sum_genre = col_operation(genres, 'sum')\nplot(x = sum_genre, style = 'barplot')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf27fe35377017dfb5177ba896414a1813ab76c3"},"cell_type":"markdown","source":"So, after we counted GENRES separately, we want to find how RATING is affected per GENRE.\n\nNow, since the table is just 1s and 0s (for count), we want to MULTIPLY GENRES table with RATING."},{"metadata":{"trusted":true,"_uuid":"a2096d3eda8802f25fc8d4d849e6ff2de0dd8cc6","collapsed":true},"cell_type":"code","source":"M = multiply(genres, data['rating'])\nhead(M, 10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18f2bb93ac6293214d558d5e7cfd92ddf87d3411"},"cell_type":"markdown","source":"You can see how all the ratings are multiplied onto GENRES. Each row is the same number as ratings. but the whole point now is we want to have an AVERAGE rating PER GENRE!\n\nSo, how do we summarise columns? Use COL_OPERATION!"},{"metadata":{"trusted":true,"_uuid":"3918386338cf8fc61f444ad3e9ccded222cee4b2","collapsed":true},"cell_type":"code","source":"col_operation(M, 'mean')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb079d73a4dd11987af15d8a4df6f916a955388a"},"cell_type":"markdown","source":"Theres a PROBLEMMM!!! Clearly, 0.07 rating is wayyyy too low. In fact, our MEAN failed.\n\nIt's because it counted all the zeros as a variable!\n\nSee below between what's WRONG and RIGHT:\n\n<img src=\"https://drive.google.com/uc?id=1RdXbzxcqd7CJfdfjgvi6A1nCGWyoRQn8\" style=\"width: 700px;\"/>\n\nSo, instead of MEAN use MEAN_ZERO. (There's also COUNT_ZERO which omits all zeroes)"},{"metadata":{"trusted":true,"_uuid":"c17e46571ee9445ef9ce5d08248e8c53d741d403","collapsed":true},"cell_type":"code","source":"col_operation(M, 'mean_zero')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88b5f2af19b5861bbed8f7eb9cc6667c42eb89f7","collapsed":true},"cell_type":"code","source":"plot(col_operation(M, 'mean_zero'), style = 'barplot')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2dcbea20c8d2acc17ffbb6d4f5b599c102b9374"},"cell_type":"markdown","source":"Clearly, WAR genre has the average highest, whilst HORROR the lowest average.\n\nNext, we want to analyse DIRECTORS. Do they affect RATINGS??\n\nWe can use TALLY to see the count."},{"metadata":{"trusted":true,"_uuid":"04027bf542d8e6e6315a3d02b4776c95355cd9cc","collapsed":true},"cell_type":"code","source":"head(   tally(data['director'])   , 10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b7ae4027ad884c0c474689b34e44dd9212a4ea87"},"cell_type":"markdown","source":"Now, we clearly want to use MULTIPLE = TRUE in TALLY. But, the biggest problem is many directors direct once. We need to REMOVE all directors who directed say less than 4 movies.\n\nThis is because a 1 time shot at directing might not mean anything. But if you directed 4 movies or so, it could be indicative of your \"real\" rating.\n\nDon't forget to remove all SPACES using REMOVE"},{"metadata":{"trusted":true,"_uuid":"2090eea08d454f8cd6525001a41e5d82c2c155a8","collapsed":true},"cell_type":"code","source":"data['director'] = remove(data['director'], ' ')\ndirectors = tally(data['director'], multiple = True, min_count = 4)\nhead(directors, 10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5fca2b36f55e2679b998c5af8b1a8787170692a"},"cell_type":"markdown","source":"Now, use COL_OPERATION after MULTIPLYING RATING, and see who wins!"},{"metadata":{"trusted":true,"_uuid":"8d22725314e2fd478b9f66d380bf615b94dce1ff","collapsed":true},"cell_type":"code","source":"director_rating =     col_operation(   multiply(  directors,data['rating']   ),   'mean_zero')\nplot(director_rating, style = 'barplot')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"671f085ee66628651fd5490e6b6d1c7fc052a384"},"cell_type":"markdown","source":"And how about ACTORS??? Same thing we do with directors. They must have performed in at least 5 films. (NOT 4 but 5)\n\nUse SPLIT = ',' then TALLY MULTIPLE = TRUE then MULTIPLY and then COL_OPERATION (mean zero)\n\nAlso remove all SPACES using REMOVE"},{"metadata":{"trusted":true,"_uuid":"d2b9646ed22c524afa5aa1e47a92c7d1d1a3523e","collapsed":true},"cell_type":"code","source":"head(data['actors'], 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a195a205afe4da492929658569cb7eee051f29f3","collapsed":true},"cell_type":"code","source":"data['actors'] = remove(data['actors'], ' ')\nsplits = split(data['actors'], ',')\nsample(splits, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bdef3d16dd9fe6d9552e7d8f191973bd24f56045","collapsed":true},"cell_type":"code","source":"actors = tally(splits, multiple = True, min_count = 5)\ntail(actors, 5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a261e9d9440ed8194936e4c6bc69f10ddb6f6398"},"cell_type":"markdown","source":"Let's see the top 5 and bottom 5"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"3d6f0b561d167f534b0aad808d2800cabd97eb01","collapsed":true},"cell_type":"code","source":"actors_rating = multiply(actors, data['rating'])\nactors_mean = col_operation(actors_rating, 'mean_zero')\nactors_mean = sort(actors_mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46cd70f7a3a16d82c011b100d5ce12effff912d1","collapsed":true},"cell_type":"code","source":"actors_mean[0:10]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ad3f2bc807f6b8300ed4a87ca8bca4e6c53b9dd"},"cell_type":"markdown","source":"<a id='Machine Learning - Supervised Learning'></a>\n<h1> 4. Machine Learning - Supervised Learning</h1>"},{"metadata":{"_uuid":"111c54028778e0e12dae8425b0895866126749ea"},"cell_type":"markdown","source":"Finally, we get to make a model to predict RATINGS!!! We want to combine the data we have just made for ACTORS, DIRECTORS and GENRES.\n\nFirst, we need to CONCATENATE all 3 new data with the original.\n\nUse HCAT."},{"metadata":{"trusted":true,"_uuid":"0474793128e6a5754b67b5bf4a0de926e427e328","collapsed":true},"cell_type":"code","source":"X = hcat(data, actors, directors, genres)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01004201041799c251a1042904c9d45290d3d376"},"cell_type":"markdown","source":"Now, we want to CLEAN the data. Machine Learning models require that there are NO missing values.\n\nAlso, ID columns (rank in this case) need to be removed.\n\nAll TEXT / OBJECT columns must be removed.\n\nEssentially, only NUMBERS can remain.\n\nUse CLEAN"},{"metadata":{"trusted":true,"_uuid":"a4c8dbd8c9b3c4e722354870929fb98c795143d7","collapsed":true},"cell_type":"code","source":"X = clean(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a56e3945581b576a51e35fcf532efa92b8a4a18","collapsed":true},"cell_type":"code","source":"help(clean)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30d051bce32c042b2e053d726c6a9c9e09a96598"},"cell_type":"markdown","source":"Now, we want the data to predict RATING.\n\nRemove RATING from X using normal column selecting, and remove it from X using EXCLUDE.\n\nAlso remove METASCORE (might be influencing on real RATING)"},{"metadata":{"trusted":true,"_uuid":"140e6eee3dad430da13cea931976167fe8e99040","collapsed":true},"cell_type":"code","source":"head(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"32f1091e912e8e417aa57d7278ecbd90263f3230"},"cell_type":"code","source":"Y = X['rating']\nX = exclude(X, ['rating','metascore'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"250ea3acc7654227a95874563841bbac7adc01fc"},"cell_type":"markdown","source":"Next, notice how YEAR abd VOTES are on WRONG SCALES. For machine learning models to succeed, they must be on the SAME SCALE.\n\nRemember STANDARDISATION from last week? We remove the MEAN and divide by STANDARD DEVIATION.\n\nA refresher:\n\n<img src=\"https://drive.google.com/uc?id=1lzY11RT7Yi_u69eWnH9WYNlao2qw_0Rq\" style=\"width: 300px;\"/>\n\n<img src=\"http://www.statistics4u.info/fundstat_eng/img/example_ztransform.png\" style=\"width: 500px;\"/>\n\nAlso, we want to use a LINEAR MODEL to model the interaction between the data X and the target Y. But what is a LINEAR MODEL?\n\nIt's a system which tries to fit a STRAIGHT LINE in any dimensional space!\n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/1200px-Linear_regression.svg.png\" style=\"width: 500px;\"/>\n\n<img src=\"https://drive.google.com/uc?id=1dMdJh99jbw8hD0_DEtiw0SbYjZupAN6_\" style=\"width: 500px;\"/>\n\nUse the LINEARMODEL method.\n\nSet SCALE = TRUE to auto scale the data."},{"metadata":{"trusted":true,"_uuid":"76e4813191e58c4baffe30e3a1224855942d7317","collapsed":true},"cell_type":"code","source":"model = LinearModel(scale = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45ddd4b67cabf387e043a7856c8ef9610bf271bf"},"cell_type":"markdown","source":"Now, we want to FIT a model. Use model.fit(X,Y). We want to create a model mapping X to Y."},{"metadata":{"trusted":true,"_uuid":"24681f96e93621ae85e65203c380f3641b2001c5","collapsed":true},"cell_type":"code","source":"model.fit(X, Y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a87b0cd318e79c928fbca569636c77095ecfa932"},"cell_type":"markdown","source":"Then, we want predictions. Use model.PREDICT(X).\n\nIt'll do auto scaling if it was specified to do so."},{"metadata":{"trusted":true,"_uuid":"ddff40b0b7d13f0d71fd46360027e0db5341bf22","collapsed":true},"cell_type":"code","source":"predictions = model.predict(X)\nprint(predictions[0:10])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7feb861a269efe2f8f4ead4b74c31dbca1ab0242"},"cell_type":"markdown","source":"Now, after we fit a linear model, we want to know whether it was a good model or not.\n\nAn easy way to check, is to plot the real Y as X and PREDICTED Y as Y, and see if they lay on a straight line.\n\nThe closer the line is to a 45 degree line, the better the prediction.\n\nUse model.PLOT(predictions, real_Y)"},{"metadata":{"trusted":true,"_uuid":"1be23cf612fa895b5ca4733cb2bda66283decc0e","collapsed":true},"cell_type":"code","source":"model.plot(predictions, Y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7e7cf031f81ddb5530d0bbb51e5be100a5965f6"},"cell_type":"markdown","source":"Using model.COEFFICIENTS, and PLOT = TRUE with TOP = 50 (if no top, then too many rows will be outputted)"},{"metadata":{"trusted":true,"_uuid":"b331adc5b6434c0ac442e5a4a823dcc0cca854ff","collapsed":true},"cell_type":"code","source":"model.coefficients(plot = True, top = 50)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"889ec84d40a56ded07adfa57610350251332c881"},"cell_type":"markdown","source":"Negative means a BAD influence on RATING. Positive means a GOOD influence.\n\nNote - in terms of interpretation, BE VERY VERY CAREFUL. VERY CAREFUL.\n\nThe weights symbolise SCALED weights (since we standardised). This means a NEGATIVE doesnt necessarily mean \"smaller = better\"\n\nYou need to UNSCALE the data and see the weights impact.\n\nThere are also MEAN, STD, RANGE and SCALE columns.\n\nMEAN means the mean of the column after scaling. STD is the standard deviation.\n\nRANGE means original [minimum, .... maximum] of the data. It shows a snapshot of how the column looked it.\n\nSCALE means scaled [minimum, .... maximum] of the data. It shows a snapshot of how the column looked it after it scaled.\n"},{"metadata":{"_uuid":"b271b971d3520ebc60dfbd77ffcfa664a347213c"},"cell_type":"markdown","source":"<a id='Machine Learning - Interpretation Traps'></a>\n<h1> 5. Machine Learning - Interpretation Traps</h1>\n\nOne of the most important things to do is Interpreting the results / coefficients. Let's get the top 10."},{"metadata":{"trusted":true,"_uuid":"452aa2d97af67b408cfbaaa6ed0d08567cd39883","collapsed":true},"cell_type":"code","source":"model.coefficients(plot = True, top = 10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"744231d739ace2a15295ba6b729771b110cd9396"},"cell_type":"markdown","source":"Now, VOTES is 0.66. What this means is if VOTES > mean (169808), each standard deviation above the mean (or +188668) contributes 0.66 to the RATING.\n\nNow, director_(Christopher Nolan). Be careful. VERY VERY CAREFUL. It does NOT NECESSARILY mean Christopher Nolan is BAD (since negative). (or is he actually bad?)\n\nRemember we found that he was one of the HIGHEST rated directors? Then why is the Linear Model providing a negative coefficient?\n"},{"metadata":{"trusted":true,"_uuid":"0850102cd43ce9a10047d4fd8a88c6b47f64caa4","collapsed":true},"cell_type":"code","source":"sort(director_rating, how = 'descending')[0:10]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64a0373d3125990b6a5d5c7360d2105d7b3e941f"},"cell_type":"markdown","source":"Hmmmm? What's going on?\n\nSay we REMOVED NOLAN from the movies he directed. What'll happen to him?\n\nLet's find out! (Next week, we'll continue on Linear Model interpretation - since it's very important).\n\nUse ANALYSE Column = director_(ChristopherNolan), and PLOT = True"},{"metadata":{"trusted":true,"_uuid":"7dfc0f1250a121c96eb919ddfd464d7c5ac28114","collapsed":true},"cell_type":"code","source":"model.analyse(plot = True, column = 'director_(ChristopherNolan)')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5dd3e154c4e7c1bf0706c86897dc7f7d22fea11c"},"cell_type":"markdown","source":"From ANALYSE, we can see if NOLAN is not directing the 5 films (from N), he actually we REDUCE the overall mean score by -2.17!!! (Change if Removed).\n\nSo, but why is the linear model saying he has a negative influence??\n\nWE\"LL DISCUSS NEXT WEEK ABOUT THIS."},{"metadata":{"_uuid":"1791d751d592019af50c742dbee362d3a5374b78"},"cell_type":"markdown","source":"<a id='Lab'></a>\n<h1> 6. Lab Questions </h1>\n\n<img src=\"https://previews.123rf.com/images/christianchan/christianchan1503/christianchan150300425/37144675--now-it-s-your-turn-note-pinned-on-cork-.jpg\" style=\"width: 300px;\"/>"},{"metadata":{"_uuid":"fb4375d6306f2f0b7066bf395212e47846c789d4"},"cell_type":"markdown","source":"1. Review the code above. Start from:\n[Machine Learning - Supervised Learning](#Machine Learning - Supervised Learning)\n2. I want you to instead of predicting RATING, predict REVENUE\n3. This means 1. EXCLUDE REVENUE from the data\n4. Scale the data.  [negative means below average, positive means above average]\n5. Predict\n6. Use ANALYSE on the HIGHEST coefficient >> USE COEFFICIENT. and LOWEST coefficient and CONFIRM your findings.\n7.  In the future, if you wanted to make a GOOD movie, what would you recommend to say Disney or 20th Century Fox Studios?"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"42ade0c8fc956b3972afe2bf11ae45829240b879"},"cell_type":"code","source":"# Your code goes here.\n# hcat\n# remove columns\n# scale\n# model\n# plot\n# coefficients\n# analyse\n\n# Then, recommendations to studios","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33dd8f9b956a4e2307f1dbb938719742b07fe69d"},"cell_type":"markdown","source":"<h2>ASSIGNMENT 1 (week 3 to 5) 7 marks - DUE WEEK 5 SATURDAY 10pm</h2>\n\nAssignment 1 is due Saturday week 5 10pm. We need you to submit a URL / LINK to your assignment notebook on Kaggle.\n\nIT MUST BE **PUBLIC**. Also, download the Kaggle file, and upload it to Moodle by 10pm Saturday week 5.\n"},{"metadata":{"trusted":true,"_uuid":"5d111b3492c1c2a34292c6dc307ea3f657646d56"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}