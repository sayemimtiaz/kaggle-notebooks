{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # used to plot visual representation\nimport json #used for Pretty Printing dict later\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.express as px\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Import files from Input Dir and store them as Pandas DataFrame**","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_pg1 = pd.read_csv('../input/solar-power-generation-data/Plant_1_Generation_Data.csv')\ndf_wsd1 = pd.read_csv('../input/solar-power-generation-data/Plant_1_Weather_Sensor_Data.csv')\ndf_pg2 = pd.read_csv('../input/solar-power-generation-data/Plant_2_Generation_Data.csv')\ndf_wsd2 = pd.read_csv('../input/solar-power-generation-data/Plant_2_Weather_Sensor_Data.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Explore the DataFrame**","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"print(df_pg1.columns)\nprint(df_wsd1.columns)\nprint()\nprint(df_pg1.describe())\nprint()\nprint(df_wsd1.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of Inverters at Station 1: ' + str(df_pg1['SOURCE_KEY'].nunique()))\nprint('Number of Inverters at Station 2: ' + str(df_pg2['SOURCE_KEY'].nunique()))\nprint('Total Number of Inverters at Stations 1 and 2: ' + str(df_pg1['SOURCE_KEY'].nunique() + df_pg2['SOURCE_KEY'].nunique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean of Daily Yield at Station 1: ' + str(df_pg1['DAILY_YIELD'].mean()))\nprint('Mean of Daily Yield at Station 2: ' + str(df_pg2['DAILY_YIELD'].mean()))\nprint('Mean of Daily Yield across the Station 1 and 2: ' + str(pd.concat((df_pg1, df_pg2))['DAILY_YIELD'].mean()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Cast DATE_TIME strings to datetime objects**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pg1['DATE_TIME'] = pd.to_datetime(df_pg1['DATE_TIME'])\ndf_wsd1['DATE_TIME'] = pd.to_datetime(df_wsd1['DATE_TIME'], format = '%Y-%m-%d %H:%M')\ndf_pg2['DATE_TIME'] = pd.to_datetime(df_pg2['DATE_TIME'])\ndf_wsd2['DATE_TIME'] = pd.to_datetime(df_wsd2['DATE_TIME'], format = '%Y-%m-%d %H:%M')\ndf_pg1['DATE'] = df_pg1['DATE_TIME'].dt.date\ndf_pg1['TIME'] = df_pg1['DATE_TIME'].dt.time\ndf_pg2['DATE'] = df_pg2['DATE_TIME'].dt.date\ndf_pg2['TIME'] = df_pg2['DATE_TIME'].dt.time\ndf_wsd1['DATE'] = df_wsd1['DATE_TIME'].dt.date\ndf_wsd1['TIME'] = df_wsd1['DATE_TIME'].dt.time\ndf_wsd2['DATE'] = df_wsd2['DATE_TIME'].dt.date\ndf_wsd2['TIME'] = df_wsd2['DATE_TIME'].dt.time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"print('Total Irridation per day on Station 1 and 2:')\ndf_grp_irr1 = df_wsd1.groupby([df_wsd1['DATE']])['IRRADIATION'].sum()\ndf_grp_irr2 = df_wsd2.groupby([df_wsd2['DATE']])['IRRADIATION'].sum()\nprint(pd.concat([df_grp_irr1, df_grp_irr2], axis = 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Maximum Ambient Temperature at Station 1: ' + str(df_wsd1['AMBIENT_TEMPERATURE'].max()))\nprint('Maximum Ambient Temperature at Station 2: ' + str(df_wsd2['AMBIENT_TEMPERATURE'].max()))\nprint()\nprint('Maximum Module Temperature at Station 1:' + str(df_wsd1['MODULE_TEMPERATURE'].max()))\nprint('Maximum Module Temperature at Station 2:' + str(df_wsd2['MODULE_TEMPERATURE'].max()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Group DataFrame by date and then find Max/Min AC/DC Power for each Group**","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"df_grp_ac1_max = df_pg1.groupby([df_pg1['DATE']])['AC_POWER'].max()\ndf_grp_dc1_max = df_pg1.groupby([df_pg1['DATE']])['DC_POWER'].max()\ndf_grp_ac2_max = df_pg2.groupby([df_pg2['DATE']])['AC_POWER'].max()\ndf_grp_dc2_max = df_pg2.groupby([df_pg2['DATE']])['DC_POWER'].max()\ndf_grp_ac1_min = df_pg1.groupby([df_pg1['DATE']])['AC_POWER'].min()\ndf_grp_dc1_min = df_pg1.groupby([df_pg1['DATE']])['DC_POWER'].min()\ndf_grp_ac2_min = df_pg2.groupby([df_pg2['DATE']])['AC_POWER'].min()\ndf_grp_dc2_min = df_pg2.groupby([df_pg2['DATE']])['DC_POWER'].min()\ndf_grp_ac1_min_nz = df_pg1[df_pg1['AC_POWER'] != 0].groupby([df_pg1['DATE_TIME'].dt.date])['AC_POWER'].min()\ndf_grp_dc1_min_nz = df_pg1[df_pg1['DC_POWER'] != 0].groupby([df_pg1['DATE_TIME'].dt.date])['DC_POWER'].min()\ndf_grp_ac2_min_nz = df_pg2[df_pg2['AC_POWER'] != 0].groupby([df_pg2['DATE_TIME'].dt.date])['AC_POWER'].min()\ndf_grp_dc2_min_nz = df_pg2[df_pg2['DC_POWER'] != 0].groupby([df_pg2['DATE_TIME'].dt.date])['DC_POWER'].min()\n\nprint('Maximum AC and DC Power at Station 1 each day:')\nprint(pd.concat([df_grp_ac1_max, df_grp_dc1_max], axis = 1))\nprint('\\nMaximum AC and DC Power at Station 2 each day:')\nprint(pd.concat([df_grp_ac2_max, df_grp_dc2_max], axis = 1))\nprint('\\nMinimum AC and DC Power at Station 1 each day:')\nprint(pd.concat([df_grp_ac1_min, df_grp_dc1_min], axis = 1))\nprint('\\nMinimum AC and DC Power at Station 2 each day:')\nprint(pd.concat([df_grp_ac2_min, df_grp_dc2_min], axis = 1))\nprint('\\nMinimum AC and DC Power at Station 1 each day(Non Zero):')\nprint(pd.concat([df_grp_ac1_min_nz, df_grp_dc1_min_nz], axis = 1))\nprint('\\nMinimum AC and DC Power at Station 2 each day(Non Zero):')\nprint(pd.concat([df_grp_ac2_min_nz, df_grp_dc2_min_nz], axis = 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Find the index where AC\\DC Power is maximum then use the index to locate the row and extract the SOURCE_KEY**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('ID of Inverter producing Maximum AC Power at Station 1: ' + (df_pg1.iloc[df_pg1['AC_POWER'].idxmax()])['SOURCE_KEY'])\nprint('ID of Inverter producing Maximum DC Power at Station 1: ' + (df_pg1.iloc[df_pg1['DC_POWER'].idxmax()])['SOURCE_KEY'])\nprint('ID of Inverter producing Maximum AC Power at Station 2: ' + (df_pg2.iloc[df_pg2['AC_POWER'].idxmax()])['SOURCE_KEY'])\nprint('ID of Inverter producing Maximum DC Power at Station 2: ' + (df_pg2.iloc[df_pg2['DC_POWER'].idxmax()])['SOURCE_KEY'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **To rank inverters based on their performance:**\n\n* Create a Key-Value Data Structure\n* Inflate it with mean values of AC/DC Power produced by each inverter with SOURCE_KEY as their id\n* Sort this DS according to these mean values in descending order but preserve the Key-Value pair","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"dc_mean = {}\nac_mean = {}\nfor i in df_pg1['SOURCE_KEY'].unique():\n    dc_mean[i] = df_pg1[df_pg1['SOURCE_KEY'] == i]['DC_POWER'].mean()\n    ac_mean[i] = df_pg1[df_pg1['SOURCE_KEY'] == i]['AC_POWER'].mean()\ndc_mean1 = {sk: m for sk, m in sorted(dc_mean.items(), key = lambda item: item[1], reverse = True)}\nac_mean1 = {sk: m for sk, m in sorted(ac_mean.items(), key = lambda item: item[1], reverse = True)}\nfor i in df_pg2['SOURCE_KEY'].unique():\n    dc_mean[i] = df_pg2[df_pg2['SOURCE_KEY'] == i]['DC_POWER'].mean()\n    ac_mean[i] = df_pg2[df_pg2['SOURCE_KEY'] == i]['AC_POWER'].mean()\ndc_mean2 = {sk: m for sk, m in sorted(dc_mean.items(), key = lambda item: item[1], reverse = True)}\nac_mean2 = {sk: m for sk, m in sorted(ac_mean.items(), key = lambda item: item[1], reverse = True)}\n\nprint('Inverter Rank based on Mean DC Power at Station 1:\\n')\nprint(json.dumps(dc_mean1, indent = 4))\nprint('\\nInverter Rank based on Mean AC Power at Station 1:\\n')\nprint(json.dumps(ac_mean1, indent = 4))\nprint('\\nInverter Rank based on Mean DC Power at Station 2:\\n')\nprint(json.dumps(dc_mean2, indent = 4))\nprint('\\nInverter Rank based on Mean AC Power at Station 2:\\n')\nprint(json.dumps(ac_mean2, indent = 4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Missing Data**\n#### **Difference indicates missing data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of entries: ' + str(len(df_pg1.index)) + '\\nExpected entries: ' + str(34*22*24*4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## *---End Of File---*","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}