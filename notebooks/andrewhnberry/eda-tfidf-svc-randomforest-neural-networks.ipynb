{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Welcome! In this notebook, I will be going through a complete data science workflow starting with data cleaning, EDA, and various model interations! Enjoy!\n\n**Model Results**\n\n|Model|AUC Score|\n|---|---|\n|Baseline - Logistic Regression using TFIDF data| 0.58|\n|Logistic Regression using Count Vectorizer Data| 0.55|\n|KNN| 0.58|\n|SVC| 0.53|\n|Random Forest| 0.52|\n|Neural Network - MLPClassifier w/ 'lbfgs'| 0.69|\n|**Neural Network - MLPClassifer w/ 'adam'**|**0.727**|"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#importing the holy trinity of data science packages\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#Other visualization packages\nimport seaborn as sns\n\n#Importing NLP plugins\nfrom nltk.corpus import stopwords \nstop_words = stopwords.words('english')\nfrom nltk.stem import WordNetLemmatizer \nimport string\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n#Importing our Sklearn Plugins\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\n#importing our models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n#Model Evaluation\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 1: Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/real-or-fake-fake-jobposting-prediction/fake_job_postings.csv\")\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Dicitonary\nThere are 17880 rows with 18 features.\n\n|Column/Feature|Discription|\n|---|---|\n|job_id|Unique Job ID|\n|title|The title of the job ad entry.|\n|location|Geographical location of the job ad.|\n|department|Corporate department (e.g. sales).|\n|salary_range|Indicative salary range (e.g. $50,000-$60,000)|\n|company_profile|A brief company description.|\n|description|The details description of the job ad.|\n|requirment|Enlisted requirements for the job opening.|\n|benefits|Enlisted offered benefits by the employer.|\n|telecommuting|True for telecommuting positions.|\n|has_company_logo|True if company logo is present.|\n|has_questions|True if screening questions are present.|\n|employment_type|Full-type, Part-time, Contract, etc.|\n|required_experience|Executive, Entry level, Intern, etc.|\n|required_education|Doctorate, Master’s Degree, Bachelor, etc.|\n|industry|Automotive, IT, Health care, Real estate, etc.|\n|function|Consulting, Engineering, Research, Sales etc.|\n|fradulent|target - Classification attribute.|\n\n**Target Variable** = fradulent (1 or 0) with 1 being fradulent"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking our Data Types\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check percentage of data missing for each feature/column\ndf.isna().sum()/len(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking for unique elements for each column\ndf.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just from a quick glance my data, it seems that there is quite a few features that have a lot of missing rows. As such, **I will delete the various columns:**\n\n1. job_id because my DataFrame already has a built in index. \n2. salary_range because around 84% of the data is missing\n3. department because around 65% of the data is missing\n4. benefits because 40% of the data is missing\n5. company_profile because I want to combine the description + requirements columns to one features, in order to perform a tfidf vectorizer on it later on. \n\nThe rest of the columns will be filled out in a methedolical order. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Always good practice to make a copy of your dataframe ever so often,\n# so you can roll back your mistakes much easier than rerunning your whole kernal again.\ndf_2 = df.copy()\ndf_2 = df_2.drop(labels = ['job_id','salary_range',\n                    'department','benefits',\n                    'company_profile'], axis = 1) #axis = 1 to refer droping columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_2.tail(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Filling missing values for **employment_type**, **required_experience**, **required_education**, **industry**, **function** using the pandas bfill function. I did this because these features had the fewest unique elements for a non-binary feature.\n\n> *Pandas bfill is a function that is used with the fillna function to back fill the values in a dataframe. Thus, if there is a NaN cell then bfill will replace that NaN value with the next row or column based on the axis equaling to 0 or 1.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_2['employment_type'] = df_2['employment_type'].bfill(axis=0)\ndf_2['required_experience'] = df_2['required_experience'].bfill(axis = 0)\ndf_2['required_education'] = df_2['required_education'].bfill(axis = 0)\ndf_2['industry'] = df_2['industry'].bfill(axis = 0)\ndf_2['function'] = df_2['function'].bfill(axis = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next step is to append the *description* column and *requirments* column together into one column. However, before I do this, **I want to avoid the NaN values in both of these columns.** In order to do so since there is a small number of missing rows in the description column, I will drop those rows first. From there, I will fill in all NaN values in the *requirments* column with \" \" aka. blank string. \n\nIn addition I will drop duplicated description columns as well, prior to the great concatenation. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make Dataframe copy\ndf_3 = df_2.copy()\n\n# Keeping non NaN rows in my new dataframe\ndf_3 = df_3[df_3['description'].notna()]\n\n# Replacing NaNs with an empty string.\n#df_3 = df_3.replace(np.nan, '', regex = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For good measure let's drop any other Nans \ndf_3 = df_3.dropna(axis = 0, how = 'any')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'We currenlty have {len(df_3)} rows. However, let\\'s drop duplicates and compare.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop duplicates\ndf_3 = df_3.drop_duplicates(keep = 'first')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_3.isna().sum()/len(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'After dropping duplicates we have {len(df_3)} rows left. It seems there were 178 duplicate rows.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make copy\ndf_4 = df_3.copy()\n\n#concatenating our description and requirments columns\ndf_4['description'] = df_4['description'] + ' ' + df_4['requirements']\ndel df_4['requirements']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Clean DataFrame\ndf_clean = df_4.copy()\n\ndisplay(df_clean.head(7))\nprint(df_clean.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 2 - Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ploting the Target variable\nplt.figure(figsize = (10,5))\nsns.countplot(x = df.fraudulent, data = df,palette=\"Set3\")\nplt.title('Fradulent (Target Variable) Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is clear that our data is highly imbalanced. This may cause some difficulties when modeling with highly imbalanced data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Stylistic Set\nsns.set(style=\"whitegrid\")\n\nplt.figure(figsize = (14,11))\n\n#fig 1\nplt.subplot(2,2,1)\nsns.countplot(y = df.employment_type, data = df,palette=\"Set3\", \n              order = df.employment_type.value_counts().index)\nplt.title(\"Employment Type Count\")\nplt.ylabel(\"\")\n\n#fig2\nplt.subplot(2,2,2)\n#matplotlib version\n#df.required_experience.value_counts().plot(kind='barh')\n#sns version\nsns.countplot(y = df.required_experience, data = df,palette=\"Set3\",\n             order = df.required_experience.value_counts().index)\nplt.title(\"Required Experience Count\")\nplt.ylabel(\"\")\n\n#fig 3\nplt.subplot(2,2,3)\nsns.countplot(y = df.required_education, data = df,palette=\"Set3\",\n             order = df.required_education.value_counts().index)\nplt.title(\"Required Education Count\")\nplt.ylabel(\"\")\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"industry = df.industry.value_counts()[:10]\nfunction = df.function.value_counts()[:10]\n\nplt.figure(figsize = (12,12))\n\nplt.subplot(2,1,1)\nindustry.plot(kind = 'barh')\nplt.title('Top 10 Industries Represented in this Dataset.')\nplt.xlabel('Count')\n\nplt.subplot(2,1,2)\nfunction.plot(kind = 'barh')\nplt.title('Top 10 Business Functions Represented in this Dataset.')\nplt.xlabel('Count')\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EDA Insights\n\n* Most job offers were Full-time, followed by Contract work.\n\n* Most jobs required an experience of mid-senior level, followed closely by Entry Level and Associate Level. Which is similar. \n\n* Most education experience required is a Bachelor’s Degree, with very few requiring Master’s Degree. Which signals that **work experience matters more** than education experience, and that the bachelor degree is a piece paper that proves you’ve done something. \n\n* In this dataset, The top 3 Industries were all tech related.\n\n* The top 3 business functions were Information Technology, Sales, and Engineering.\n\n\n#### Future Plots\n1. A couple of word cloud images, people for some reason love world clouds.\n2. Plot of a map, showing the counts of jobs for each country. etc."},{"metadata":{},"cell_type":"markdown","source":"# Part 3 - Feature Engineering & Modeling"},{"metadata":{},"cell_type":"markdown","source":"We need to do some feature engineering. I would like to one hot encode my categorical data, as well as fit a TFIDF Vectorizer to my text data column. Might do a Count Vectorizer as well, and see if that changes anything to my model. In addition, I probably want to fit a PCA to reduce computational time. \n\n**Next Steps:**\n\n1. One Hot Encode Cateogrical Data\n2. Fit in a TFIDF Vectorizer\n3. Fit in a Count Vectorizer\n4. Determine if using a PCA would help. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Make Copy\ndf_5 = df_clean.copy()\n\n# One Hot Encoding using Pandas get dummies function\ncolumns_to_1_hot = ['employment_type','required_experience','required_education',\n                   'industry', 'function']\n\nfor column in columns_to_1_hot:\n    encoded = pd.get_dummies(df_5[column])\n    df_5 = pd.concat([df_5, encoded], axis = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_to_1_hot += ['title', 'location']\n    \n#droping the original columns that we just one hot encoded from\ndf_5 = df_5.drop(columns_to_1_hot, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_5.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Handling the description column \n\nFirst of all we need to clean up our text data a little bit. Now let us creat some helper funcitons."},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenizer(text):\n    \n    #All characters in this string will be converted to lowercase\n    text = text.lower()\n    \n    #Removing sentence punctuations\n    for punctuation_mark in string.punctuation:\n        text = text.replace(punctuation_mark,'')\n    \n    #Creating our list of tokens\n    list_of_tokens = text.split(' ')\n    #Creating our cleaned tokens list \n    cleaned_tokens = []\n    #Intatiating our Lemmatizer\n    lemmatizer = WordNetLemmatizer()\n    \n    #Removing Stop Words in our list of tokens and any tokens that happens to be empty strings\n    for token in list_of_tokens:\n        if (not token in stop_words) and (token != ''):\n            #lemmatizing our token\n            token_lemmatized = lemmatizer.lemmatize(token)\n            #appending our finalized cleaned token\n            cleaned_tokens.append(token_lemmatized)\n    \n    return cleaned_tokens","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TfidfVectorizer\n\nI will need to run a tfidf vectorizer on our description data, and append the results to our DataFrame. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_6 = df_5.copy()\n\n#Instatiating our tfidf vectorizer\ntfidf = TfidfVectorizer(tokenizer = tokenizer, min_df = 0.05, ngram_range=(1,3))\n#Fit_transform our description \ntfidf_features = tfidf.fit_transform(df_6['description']) #this will create a sparse matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#I want to append this sparse matrix to the original pandas Dataframe\ntfidf_vect_df = pd.DataFrame(tfidf_features.todense(), columns = tfidf.get_feature_names())\n\ndf_tfidf = pd.concat([df_6, tfidf_vect_df], axis = 1)\n\n#Minor Cleaning steps after appending our tfidf results to our Dataframe, we will need to drop the description column. \ndf_tfidf = df_tfidf.drop(['description'], axis = 1)\ndf_tfidf = df_tfidf.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tfidf.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Count Vectorizer\nNow let's do a similar procedure with a Count Vectorizer, so we can compare the two vectorizers in performance later on."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Instatiating our CountVectorizer\ncount_vect = CountVectorizer(tokenizer = tokenizer, min_df = 0.05, ngram_range=(1,3))\n#Fit_transform our description \ncount_vect_features = count_vect.fit_transform(df_6['description']) #this will create a sparse matrix\n\ncount_vect_df = pd.DataFrame(count_vect_features.todense(), columns = count_vect.get_feature_names())\n\ndf_count_vect = pd.concat([df_6, count_vect_df], axis = 1)\ndf_count_vect = df_count_vect.drop(['description'], axis = 1)\ndf_count_vect = df_count_vect.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_count_vect.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great, we now have two different dataframes with two different vectorizers preprocessing our description data. I will hold out on the PCA to see if I need it. I will only do it if the modelimg takes too long. \n\n**I will conduct the following steps:**\n1. Logistic Regression w/ Tfidf\n2. Logistic Regression w/ Count Vectorizer\n3. I will evaluate both models and determine which is better, and for simplicity stake pick the superior vectorizer for the other models I would like to run."},{"metadata":{},"cell_type":"markdown","source":"# Model 1 - Logistic Regresion w/ Tfidf"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = df_tfidf.fraudulent\nfeatures = df_tfidf.drop(['fraudulent'], axis = 1)\n\n#Spliting our Data into train and holdout sets to test our models\nX_train, X_hold, y_train, y_hold = train_test_split(features, target, test_size = 0.1,\n                                                    stratify = target, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Intatiating our Logistic Regression Model\nlog_reg = LogisticRegression()\n#I want to optimze the C-Value and penalty\nc_values = [.00001, .0001, .001, .1, 1, 10, 100, 1000, 10000]\npenalty_options = ['l1','l2']\n\nparam_grid = dict(C = c_values, penalty = penalty_options)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_tfidf = GridSearchCV(log_reg, param_grid= param_grid, cv = 10, scoring = 'roc_auc', n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_tfidf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(grid_tfidf.best_score_)\nprint(grid_tfidf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg_tfidf_pred = grid_tfidf.predict(X_hold)\nprint(roc_auc_score(y_hold, log_reg_tfidf_pred))\nprint(classification_report(y_hold, log_reg_tfidf_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interesting, using our holdout data our logistic regression with the tfidf data, had an AUC score of 0.58. Which is okay, that will be our baseline model. "},{"metadata":{},"cell_type":"markdown","source":"# Model 2 - Logistic Regression w/ Count Vectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"target_2 = df_count_vect.fraudulent\nfeatures_2 = df_count_vect.drop(['fraudulent'], axis = 1)\n\n#Spliting our Data into train and holdout sets to test our models\nX_train_2, X_hold_2, y_train_2, y_hold_2 = train_test_split(features_2, target_2, test_size = 0.1,\n                                                    stratify = target_2, random_state = 42)\n\n#Intiatiating our previous logistic regression model, using the count vectorizer dataset\ngrid_count_vect = GridSearchCV(log_reg, param_grid= param_grid, cv = 10, scoring = 'roc_auc', n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_count_vect.fit(X_train_2, y_train_2)\nprint(grid_count_vect.best_score_)\nprint(grid_count_vect.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg_pred_2 = grid_count_vect.predict(X_hold_2)\nprint(roc_auc_score(y_hold_2, log_reg_pred_2))\nprint(classification_report(y_hold_2, log_reg_pred_2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Count Vectorizer did not really improve from my previous model, it did worse by 3 percentage points. The AUC score on our holdout data was 0.55. Thus, I will stick using the tfidf data."},{"metadata":{},"cell_type":"markdown","source":"# Model 3 - KNearestNeighbors"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model - KNearestNeighbors\nknn = KNeighborsClassifier()\n\n#The parameters we would like to optimize for\nk_range = list(np.arange(2,23,2))\nparam_grid_knn = dict(n_neighbors=k_range)\nprint(param_grid_knn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Intatiate our knn gridsearch\ngrid_knn = GridSearchCV(knn, param_grid_knn, cv=10, scoring='roc_auc',\n                        n_jobs = -1)\n\n#Fit our grid_knn\ngrid_knn.fit(X_train, y_train)\nprint(grid_knn.best_score_)\nprint(grid_knn.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predicting on our holdout data\nknn_pred = grid_knn.predict(X_hold)\n#Printing out our evaluation metrics\nprint(roc_auc_score(y_hold, knn_pred))\nprint(classification_report(y_hold, knn_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'm a little bit dissapointed that the knn pediction on the holdout data was around the same too the original logistic regression."},{"metadata":{},"cell_type":"markdown","source":"# Model 4 - Support Vector Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Intatiating our SVM model\nsvc = SVC(kernel = 'linear', gamma = 'auto' )\n\n# I wont use a gridsearch because SVMs usually take a long looong time. I will just use a simple SVC\n# and see how it plays out\nsvc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predicting our holdout data\nsvc_pred = svc.predict(X_hold)\n\n#Printing out our evaluation metrics\nprint(roc_auc_score(y_hold, svc_pred))\nprint(classification_report(y_hold, svc_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Very Dissapointed that the SVC didn't do better either. Very dissapointed."},{"metadata":{},"cell_type":"markdown","source":"# Model 5 - Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Instatiating our random forest\n\nrf = RandomForestClassifier()\n\n#The parameters we want to tune with our random forest\nn_estimators_range = [1, 2, 4, 8, 16, 32, 64, 100, 200]\n\nparam_grid_rf = dict(n_estimators=n_estimators_range)\n\ngrid_rf = GridSearchCV(rf, param_grid_rf, cv=10, scoring='roc_auc',\n                        n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_rf.fit(X_train, y_train)\nprint(grid_rf.best_score_)\nprint(grid_rf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_pred = grid_rf.predict(X_hold)\n#Printing out our evaluation metrics\nprint(roc_auc_score(y_hold, rf_pred))\nprint(classification_report(y_hold, rf_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dissapointed."},{"metadata":{},"cell_type":"markdown","source":"# Model 6 - Neural Nets - MLPClassifier w/ solver = 'lbfgs' "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Instatiatie our MLPClassifier\nmlp = MLPClassifier(solver='lbfgs', \n                    activation = 'relu',\n                   hidden_layer_sizes = (100,50,30), \n                    max_iter = 1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mlp.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mlp_pred = mlp.predict(X_hold)\n\n#Printing out our evaluation metrics\nprint(roc_auc_score(y_hold, mlp_pred))\nprint(classification_report(y_hold, mlp_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Breath of fresh air! Finally a model that shows a **significant improvement from our baseline model.**"},{"metadata":{},"cell_type":"markdown","source":"# Model 7 - Neural Nets - MLPClassifier w/ solver = 'adam'"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Instatiatie our MLPClassifier\nmlp = MLPClassifier(solver='adam', \n                    activation = 'relu',\n                   hidden_layer_sizes = (100,50,30), \n                    max_iter = 1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mlp.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mlp_pred = mlp.predict(X_hold)\n\n#Printing out our evaluation metrics\nprint(roc_auc_score(y_hold, mlp_pred))\nprint(classification_report(y_hold, mlp_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the adam solver made our model perform even better! **an AUC score of 0.72! **"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}