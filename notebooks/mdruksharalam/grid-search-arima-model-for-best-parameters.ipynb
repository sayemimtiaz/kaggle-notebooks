{"cells":[{"metadata":{},"cell_type":"markdown","source":"Thanks to : https://www.kaggle.com/rajnaruka0698/border-crossing-eda "},{"metadata":{},"cell_type":"markdown","source":"**Here, apart from data visualization, I create a custom class called Arima to grid search and identify best parameters(p and q) with the lowest AIC, BIC, and HQIC. The best model is then used for forecasting.**\n\nIn this article, we predict/forecast the number of crossings that arrive in the USA from Canada and Mexico borders for a particular period of time. The dataset used is the Bureau of Transportation Statistics (BTS) Border Crossing Data which provides summary statistics for inbound crossings at the U.S.-Canada and the U.S.-Mexico border at the port level. Data are available for trucks, trains, containers, buses, personal vehicles, passengers, and pedestrians. Border crossing data are collected at ports of entry by U.S. Customs and Border Protection (CBP). The data reflect the number of vehicles, containers, passengers or pedestrians entering the United States. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport datetime\n\nimport plotly.graph_objects as go\nimport plotly.express as px\ndf = pd.read_csv(\"../input/border-crossing-entry-data/Border_Crossing_Entry_Data.csv\")\nprint(df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are about 345k entries in the dataset ranging from the 1st January, 1996 to the 1st March 2019. There are 8 columns. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import missingno as msno\nmsno.matrix(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the missingno plot, we see there are no missing values for any of the features. Let's modify the date column from string format to datetime format. Additionally, let's extract Year and Month from date, create separate columns for them in our dataframe, and we will use them later on."},{"metadata":{"trusted":true},"cell_type":"code","source":"    \ndf['Date'] = pd.to_datetime(df['Date'])\ndef addYearMonthColumn(dataframe):\n\n    dataframe['Year'] = dataframe['Date'].apply(lambda x : x.year)\n\n    month_mapper = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun'\n                   ,7:'Jul', 8:'Aug', 9:'Sep' ,10:'Oct', 11:'Nov', 12:'Dec'}\n    dataframe['Month'] = dataframe['Date'].apply(lambda x : x.month).map(month_mapper)\n    \n    del month_mapper\n    return dataframe\n\ndf = addYearMonthColumn(df)\ndf.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll observe the inbound traffic to US by 'Measure'. The unique values of 'Measure' columns are: 'Trucks', 'Rail Containers Full', 'Trains','Personal Vehicle Passengers', 'Bus Passengers','Truck Containers Empty', 'Rail Containers Empty', 'Personal Vehicles', 'Buses', 'Truck Containers Full','Pedestrians', 'Train Passengers'"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Measure.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def inboundTraffic(dataframe):\n    resultDf = pd.DataFrame(dataframe.groupby(by='Measure')['Value'].sum().sort_values(ascending=False)).reset_index()\n    fig = px.bar(resultDf, x='Measure', y='Value', height=400,color='Value', color_continuous_scale=px.colors.sequential.Magenta)\n    fig.show()\n    del resultDf\n    \ninboundTraffic(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above bar plot shows most inbound traffic are due to personal vehicle pessengers and personal vehicles. We'll now check out the distribution of 'Measure' in the 2 border regions (Canada and Mexico)."},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\n\nresultDf = df.groupby(by=['Border','Measure'])['Value'].sum().reset_index()\ncanada = resultDf.loc[resultDf['Border'] == 'US-Canada Border']['Value']\nmexico = resultDf.loc[resultDf['Border'] == 'US-Mexico Border']['Value']\n\nmeasures = resultDf['Measure']\n\nfig = go.Figure()\nfig.add_trace(go.Bar(\n    x=measures,\n    y=canada,\n    name='Canada Border',\n    marker_color='indianred'\n))\nfig.add_trace(go.Bar(\n    x=measures,\n    y=mexico,\n    name='Mexico Border',\n    marker_color='lightsalmon'\n))\n\n# Here we modify the tickangle of the xaxis, resulting in rotated labels.\nfig.update_layout(barmode='group', xaxis_tickangle=-45)\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this comparison bar plot, personal vehicle pessengers and personal vehicles have higher values in both the borders but there is higher traffic from Mexico. However, greater number of trucks and full container trucks arrive from Canada.\nThe pie charts below depicts the total number of crossings in percentage by border and measure. Here we see clearly that Mexico dominates the inbound traffic to USA and the lion's share of traffic is in the form of personal vehicle pessengers and personal vehicles."},{"metadata":{"trusted":true},"cell_type":"code","source":"def pieChart(dataframe, feature):\n    resultDf = dataframe.groupby(by=feature)['Value'].sum()\n    fig = go.Figure(data=[go.Pie(labels = resultDf.index, values=resultDf.values)])\n    fig.update_traces(textfont_size=15,  marker=dict(line=dict(color='#000000', width=2)))\n    fig.show()\n    del resultDf\n\npieChart(df, 'Border')\npieChart(df,'Measure')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def measure_values_by_years(dataframe, time_feature):\n    \n    plt.figure(figsize=(10,6))\n    sns.lineplot(data=dataframe, x=time_feature, y='Value', hue='Measure',legend='full')\n    plt.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")\n    plt.title('Measure Values Through ' + time_feature)\nmeasure_values_by_years(df, 'Year')\nmeasure_values_by_years(df, 'Month')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above charts show number of crossings throughout years and months respectively. Despite a decrease in crossings since 2000, a slight increment in pedestrians crossing over past few yers is observed after 2015. When analyzing crossings by month, July and August have highest crossings and February has the least number of crossings."},{"metadata":{"trusted":true},"cell_type":"code","source":"def crossing_by_post(data):\n    resultDf = pd.DataFrame(data.groupby(by='Port Name')['Value'].sum().sort_values(ascending=False)).reset_index()\n    fig = px.bar(resultDf, x='Port Name', y='Value', color='Value', color_continuous_scale=px.colors.sequential.Viridis) \n    fig.show()\ncrossing_by_post(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above graph represents ports and their number of crossings, with El Paso showing the most traffic and Boquillas showing the least traffic.\n\nWe also group measures by their size. Below there are two bar plots with sum of values of different size of measures by different states. We follow the following mapping to groups measures by their size:\n1. 'Trucks' : 'Mid_Size', \n2. 'Rail Containers Full' : 'Mid_Size', \n3. 'Trains' : 'Big_Size',\n4. 'Personal Vehicle Passengers':'Small_Size', \n5. 'Bus Passengers':'Small_Size',\n6. 'Truck Containers Empty':'Mid_Size', \n7. 'Rail Containers Empty':'Mid_Size',\n8. 'Personal Vehicles' : 'Small_Size', \n9. 'Buses' : 'Mid_Size', \n10. 'Truck Containers Full' : 'Mid_Size',\n11. 'Pedestrians':'Small_Size', \n12. 'Train Passengers':'Small_Size'"},{"metadata":{"trusted":true},"cell_type":"code","source":"measure_size = {'Trucks' : 'Mid_Size', 'Rail Containers Full' : 'Mid_Size', 'Trains' : 'Big_Size',\n       'Personal Vehicle Passengers':'Small_Size', 'Bus Passengers':'Small_Size',\n       'Truck Containers Empty':'Mid_Size', 'Rail Containers Empty':'Mid_Size',\n       'Personal Vehicles' : 'Small_Size', 'Buses' : 'Mid_Size', 'Truck Containers Full' : 'Mid_Size',\n       'Pedestrians':'Small_Size', 'Train Passengers':'Small_Size'}\n\ndf['Size'] = df['Measure'].map(measure_size)\n\ndef crossing_by_measure_size(data):\n\n    resultDf = data.groupby(by=['Size','State'])['Value'].sum().unstack()\n    resultDf.fillna(0,inplace=True)\n\n    plt.figure(figsize=(15,4))\n\n    plt.subplot(131)\n    resultDf.iloc[0].sort_values().plot(kind='bar', color='g')\n    plt.xticks(rotation=90)\n    plt.title('Big_Size')\n\n    plt.subplot(132)\n    resultDf.iloc[1].sort_values().plot(kind='bar')\n    plt.xticks(rotation=90)\n    plt.title('Mid_Size')\n\n    plt.subplot(133)\n    resultDf.iloc[2].sort_values().plot(kind='bar', color='red')\n    plt.xticks(rotation=90)\n    plt.title('Small_Size')\n\n    del resultDf\n    \ncrossing_by_measure_size(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Insights :\n- Minnesota has most number of big_size crossings but has averege on the other two categories.\n- Arizona has good number of small size crossings but average on the other two categories.\n- Ohio, Alaska and Montana has least amount of crossings in all the categories.\n- Michigan has 2nd heighest BIg and Mid size crossings but comparitively less small size crossings. For the small-size crossings California takes the 2nd place.\n- Texas has most mid_size and small size crossings and also, 3rd largest big_size crossings. This means there is considerable traffic into Texas.\n- New York also has good number of crossings in all the three size crossing categories.\n\nWe now check for seasonal variations of incoming traffic based on measure-size."},{"metadata":{"trusted":true},"cell_type":"code","source":"def seasonality_check(data, timeLabel):\n    plt.figure(figsize=(15,6))\n    g = sns.FacetGrid(data=data, col='Size', sharey=False, height=5, aspect=1)\n    g.map(sns.lineplot, timeLabel, 'Value')\n    \nseasonality_check(df,'Month')\nseasonality_check(df,'Year')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Insights :\n- Mid_Size crossings are least in December, January and July and most in October, March and August.\n- Big_Size crossings are least in February and most in October, March and August.\n- Small_Size crossings are least in January and February and most in August and July.\n- For the yearly variation, all size categories show marked increase in traffic after 2015. The years around 2010 have witnessed the least amount of traffic.\n\nFor our ARIMA model, we need to create a time series data with dates and the associated incoming traffic( values). Plotting our time series, we see that after 2002 there's a declining trend in the traffic which slightly picks up after 2014. Also, we can easily observe a yearly pattern (seasonality) where traffic is  generally low during the beginning and the end of a year but is high during mid-year.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\nts = df[['Date','Value']].groupby('Date').sum()\nprint(ts.shape)\nprint(ts.head(4))\nts = ts.loc['1997':]\nts.plot(figsize = (15,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Following is the decomposition of our time series data into trend, seasonality, and residuals. Since we observe that decreasing trend also decreases the seasonality, our time series is multiplicative. Due to the existence of trend, and seasonality, the data isn't stationary. We also check stationarity with the 'adfuller' test where the p-value>0.005 means the data is non-stationary whereas p-value<0.005 means the data is stationary. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Multiplicative Decomposition \n\nts_mult_decomposition = seasonal_decompose(ts, model='multiplicative', extrapolate_trend='freq')\nts_mult_decomposition.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ts_trend = ts_mult_decomposition.trend * ts_mult_decomposition.resid\nfrom statsmodels.tsa.stattools import adfuller\n\nresult = adfuller(ts.Value.dropna())\nprint('p-value: %f' % result[1])\n\nresult = adfuller(ts.diff().Value.dropna())\nprint('p-value: %f' % result[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The p-value for unchanged time-series dataset is 0.56 meaning it's non-stationary. We make such time series stationary by differencing. After applying differencing once, the p-value of modified time series is 0.000001 < 0.005, meaning the data has become stationary. We can conclude that in our ARIMA model we have to set the differencing factor, d=1 to difference the data. We also need to determine the values of p-factor  (significant lag of PACF) and q-factor (significant lag of ACF). For this reason we draw the ACF and PACF plots for the unmodified and one-time-differenced time series data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfig, axes = plt.subplots(2, 3, figsize=(16,10))\n\naxes[0, 0].plot(ts.Value)\naxes[0, 0].set_title('Original Series')\nplot_pacf(ts, ax=axes[0, 1])\nplot_acf(ts, ax=axes[0, 2])\n\n# 1st Differencing\naxes[1, 0].plot(ts.diff()); axes[1, 0].set_title('1st Order Differencing')\nplot_pacf(ts.diff().dropna(), ax=axes[1, 1])\nplot_acf(ts.diff().dropna(), ax=axes[1, 2])\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unmodified data has a lot of significant lags compared to one-time-differenced data meaning we need differencing in our ARIMA model to prevent overfitting/ complicating our model. To get the best model, we perform grid search with both p and q having value range between 0 and 2. The best model is chosen based on the lowest values for the metrics AIC, BIC, and HQIC. If any 2 of them, for a model, are lower than the previously calculated best metrics, that model is considered the new best model. For the training of our ARIMA model we only use the trend and the residual of time series data. The seasonality will be multiplied later during the plotting of our forecast."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_trand_and_residual():\n    trainingData = ts_mult_decomposition.trend * ts_mult_decomposition.resid\n    trainingData.plot(figsize = (15,10))\n\n    plt.show()\n    return trainingData\n\ntraining = plot_trand_and_residual()\ntraining.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.arima_model import ARIMA\n\nclass Arima:\n  def __init__(self, train_data, p,q,d=1):\n    self.train_data = train_data\n    self.p = p\n    self.q=q\n    self.d = d\n    self.best_aic = np.Inf\n    self.best_bic =np.Inf\n    self.best_hqic = np.Inf\n    self.best_order = (0,0,0)\n    self.models = []\n    self.model = 0\n  \n  def is_current_best_model(self):\n    no_of_lower_metrics = 0\n    if self.model.aic <= self.best_aic: no_of_lower_metrics+=1\n    if self.model.bic <= self.best_bic: no_of_lower_metrics+=1\n    if self.model.hqic <= self.best_hqic:no_of_lower_metrics+=1\n    return no_of_lower_metrics >= 2\n\n  def best_selection(self):\n    for p_ in self.p:\n        for q_ in self.q:\n            \n            currentOrder = (p_,q_)\n            print(\"Current Order (p,q): \"+ str(currentOrder))\n\n            self.model = ARIMA(des, order=(1,0,1)).fit(disp=0)\n            self.models.append(self.model)\n\n\n            if self.is_current_best_model() == True:\n                self.best_aic = np.round(self.model.aic,0)\n                self.best_bic = np.round(self.model.bic,0)\n                self.best_hqic = np.round(self.model.hqic,0)\n                self.best_order = (p_,self.d,q_)\n                current_best_model = self.model\n                self.models.append(self.model)\n                print('========================================================================')\n                print(\"Best model so far: ARIMA\" +  str(self.best_order) + \n                      \" AIC:{} BIC:{} HQIC:{}\".format(self.best_aic,self.best_bic,self.best_hqic)+\n                      \" resid:{}\".format(np.round(np.exp(current_best_model.resid).mean(),3)))\n                print('========================================================================')\n                print()\n\n\n    print('\\n')\n    print(current_best_model.summary())                \n    return current_best_model, self.models \n\nx=range(2)\narima =Arima(training, x,x)\nbest_model, models = arima.best_selection()\nbest_model.plot_predict()\nplt.title('Best Model')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our best model has a p and q value of 1 meaning the model uses only one significant lag from PACF and ACF respectively. The best model's prediction(from 1997 to 2019) fits quite well with the training data. Now we forecast traffic values for the near future (2019 to 2020). During forecasting, we multiply the seasonal/yearly repeating component back to both the training data and the prediction to see how our forecast will actually look like considering trend, seasonality and residuals."},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_seasonal(ts,tms) :\n    seasonal_series = ts * tms # Include the seasonality\n    seasonal_series = seasonal_series[~seasonal_series.isnull()] # trim extra values\n    return seasonal_series\n\ndef create_seasonal_component():\n    seasonal = ts_mult_decomposition.seasonal.loc['2016-01-01':'2016-12-01'].values # seasonal component, we take the 2016 ones, but they are all the same.\n    seasonal = pd.Series(np.tile(seasonal.flatten(),11), index = pd.date_range(start='2019-01-01', end = '2029-12-01', freq='MS'))  # This is just a very long series with the seasonality.\n    return seasonal\n\ndef create_forecast():\n    tms = create_seasonal_component()\n    model = ARIMA(training, order=(1,0,1))\n    model_fit = model.fit(disp=0)\n\n    fc_series, se_series, conf_series = model_fit.forecast(n_forecast)  # 2 sigma Confidence Level (95,55% conf)\n\n    # Make as pandas series and include seasonality\n    fc_series = make_seasonal(pd.Series(fc_series, index = date_rng),tms)\n    lower_series = make_seasonal(pd.Series(conf_series[:, 0], index = date_rng),tms)\n    upper_series = make_seasonal(pd.Series(conf_series[:, 1], index = date_rng),tms)\n    \n    return fc_series,lower_series,upper_series\n\ndef plot_forecast(fc_series,lower_series,upper_series):\n    plt.figure(figsize=(12,5), dpi=100)\n\n    plt.plot(training * ts_mult_decomposition.seasonal, label='Time Series Data', color='g')\n    plt.plot(fc_series , label='Forecast',color='r')\n\n    # Confidence level intervals\n    plt.fill_between(lower_series.index,lower_series, upper_series, \n                     color='k', alpha=.15, label='2$\\sigma$ Confidence level (95%)')\n    plt.title('Forecast 2019/20')\n    plt.legend(loc='upper left', fontsize=8)\n    plt.xlim('2000', '2021')\n    plt.show()\n# Forecast\n\ndef draw_forecast_main():\n\n    date_start = training.tail(1).index[0]\n    date_end = '2020-12-01'\n    print(date_start)\n    date_rng = pd.date_range(start=date_start, end=date_end, freq='MS', closed = 'right') # range for forecasting\n    n_forecast = len(date_rng) # number of steps to forecast\n    print('range of dates: '+str(n_forecast))\n\n    \n    fc_series,lower_series,upper_series = create_forecast()\n\n    \n    plot_forecast(fc_series,lower_series,upper_series)\n    \ndraw_forecast_main()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The forecast looks quite similar to the time series data meaning our model successfully captures the pattern(trend/seasonality/residuals) hidden within the dataset."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}