{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# COVID-19 Open Research Dataset (CORD-19) Analysis\n\n<p align=\"center\">\n    <img src=\"https://pages.semanticscholar.org/hs-fs/hubfs/covid-image.png?width=300&name=covid-image.png\"/>\n</p>\n\n***NOTE: There is a [Report Builder Notebook](https://www.kaggle.com/davidmezzetti/cord-19-report-builder) that runs on a prebuilt model. If you just want to try this out without a full build, this is the best choice.***\n\nCOVID-19 Open Research Dataset (CORD-19) is a free resource of scholarly articles, aggregated by a coalition of leading research groups, covering COVID-19 and the coronavirus family of viruses. The dataset can be found on [Semantic Scholar](https://pages.semanticscholar.org/coronavirus-research) and [Kaggle](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge).\n\nThis notebook builds an index over the CORD-19 dataset to assist with analysis and data discovery using the [paperai](https://github.com/neuml/paperai) project. A series of COVID-19 related research topics were explored to identify relevant articles and help find answers to key scientific questions.\n\n### Tasks\nA full list of Kaggle CORD-19 Challenge tasks are [referenced below](#Round-1-Tasks). This notebook and corresponding report notebooks won ðŸ† [7 awards](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/discussion/161447) ðŸ† in the Kaggle CORD-19 Challenge.\n\nThe latest tasks are also stored in the [cord19q repository](https://github.com/neuml/cord19q/tree/master/tasks).\n","metadata":{}},{"cell_type":"markdown","source":"# Install\n\n[paperai](https://github.com/neuml/paperai) is installed via the [cord19reports](https://www.kaggle.com/davidmezzetti/cord19reports) package. This ensures version consistency across this and all related notebooks. ","metadata":{}},{"cell_type":"code","source":"from cord19reports import install\n\n# Install paperai project\ninstall()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","scrolled":true,"_kg_hide-output":true,"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CORD-19 ETL - Articles SQLite Database\n\nThis project depends on the [CORD-19 ETL notebook](https://www.kaggle.com/davidmezzetti/cord-19-etl). The CORD-19 ETL notebook has a full overview of the raw CORD-19 dataset, parsing rules and other important information on how the data stored in SQLite is derived. Previous versions of this notebook had the full build process for both the SQLite database and embeddings index here but modularization was necessary as the dataset grew.\n\nThis following step copies over the previously constructed Articles SQLite database and related files. \n","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\n\n# Copy articles.sqlite and related files locally\nos.mkdir(\"cord19q\")\nshutil.copy(\"../input/cord-19-etl/cord19q/articles.sqlite\", \"cord19q\")\nshutil.copy(\"../input/cord-19-etl/cord19q/attribute\", \"cord19q\")\nshutil.copy(\"../input/cord-19-etl/cord19q/design\", \"cord19q\")","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build Embedding Index\n\nAn embeddings index is created with [FastText](https://fasttext.cc/) + [BM25](https://en.wikipedia.org/wiki/Okapi_BM25). Background on this method can be found in this [Medium article](https://towardsdatascience.com/building-a-sentence-embedding-index-with-fasttext-and-bm25-f07e7148d240) and an existing repository using this method [codequestion](https://github.com/neuml/codequestion).\n\nThe embeddings index takes each COVID-19 tagged, not labeled a question/fragment, having a detected study type, tokenizes the text, and builds a sentence embedding. A sentence embedding is a BM25 weighted combination of the FastText vectors for each token in the sentence. The embeddings index takes the full corpus of these embeddings and builds a [Faiss](https://github.com/facebookresearch/faiss) index to enable similarity searching. \n\nImportant source files to highlight\n\n* Indexing Process -> [index.py](https://github.com/neuml/paperai/blob/master/src/python/paperai/index.py)\n* Tokenizer -> [tokenizer.py](https://github.com/neuml/txtai/blob/master/src/python/txtai/tokenizer.py)\n* Embeddings Model -> [embeddings.py](https://github.com/neuml/txtai/blob/master/src/python/txtai/embeddings.py)\n* BM25 Scoring -> [scoring.py](https://github.com/neuml/txtai/blob/master/src/python/txtai/scoring.py)\n\nFastText vectors trained on the full CORD-19 corpus are required. A [dataset with pre-trained vectors](https://www.kaggle.com/davidmezzetti/cord19-fasttext-vectors) is included and used in this notebook. Building the vectors takes a couple of hours when locally trained and would most likely take much longer within a notebook. \n\nVectors can optionally be (re)built by running the following command with the project and articles.sqlite database installed locally:\n\n```\npython -m paperai.vectors\n```\n\nThe following code builds the embeddings index using fastText vectors trained on the full CORD-19 dataset. Alternatively, any [pymagnitude vector file](https://github.com/plasticityai/magnitude#pre-converted-magnitude-formats-of-popular-embeddings-models) can be used the build the sentence embeddings.","metadata":{}},{"cell_type":"code","source":"import shutil\n\nfrom paperai.index import Index\n\n# Copy vectors locally for predictable performance\nshutil.copy(\"../input/cord19-fasttext-vectors/cord19-300d.magnitude\", \"/tmp\")\n\n# Build the embeddings index, limit to most recent documents\nIndex.run(\"cord19q\", \"/tmp/cord19-300d.magnitude\", 100000)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Workaround for mdv terminal width issue\nos.environ[\"COLUMNS\"] = \"80\"\n\nfrom paperai.highlights import Highlights\nfrom txtai.tokenizer import Tokenizer\n\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport plotly.graph_objects as go\nimport pycountry\n\n# Use paperai + NLTK stop words\nSTOPWORDS = Highlights.STOP_WORDS | set(stopwords.words(\"english\"))\n\n# Tokenizes text and removes stopwords\ndef tokenize(text, case_sensitive=False):\n    # Get list of accepted tokens\n    tokens = [token for token in Tokenizer.tokenize(text) if token not in STOPWORDS]\n    \n    if case_sensitive:\n        # Filter original tokens to preserve token casing\n        return [token for token in text.split() if token.lower() in tokens]\n\n    return tokens\n    \n# Country data\ncountries = [c.name for c in pycountry.countries]\ncountries = countries + [\"USA\"]\n\n# Lookup country name for alpha code. If already an alpha code, return value\ndef countryname(x):\n    country = pycountry.countries.get(alpha_3=x)\n    return country.name if country else x\n    \n# Resolve alpha code for country name\ndef countrycode(x):\n    return pycountry.countries.get(name=x).alpha_3\n\n# Tokenize and filter only country names\ndef countrynames(x):\n    return [countryname(country) for country in countries if country.lower() in x.lower()]\n\n# Word Cloud colors\ndef wcolors(word=None, font_size=None, position=None,  orientation=None, font_path=None, random_state=None):\n    colors = [\"#7e57c2\", \"#03a9f4\", \"#011ffd\", \"#ff9800\", \"#ff2079\"]\n    return np.random.choice(colors)\n\n# Word Cloud visualization\ndef wordcloud(df, title = None):\n    # Set random seed to have reproducible results\n    np.random.seed(64)\n    \n    wc = WordCloud(\n        background_color=\"white\",\n        max_words=200,\n        max_font_size=40,\n        scale=5,\n        random_state=0\n    ).generate_from_frequencies(df)\n\n    wc.recolor(color_func=wcolors)\n    \n    fig = plt.figure(1, figsize=(15,15))\n    plt.axis('off')\n\n    if title:\n        fig.suptitle(title, fontsize=14)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wc),\n    plt.show()\n\n# Dataframe plot\ndef plot(df, title, kind=\"bar\", color=\"bbddf5\"):\n    # Remove top and right border\n    ax = plt.axes()\n    ax.spines[\"top\"].set_visible(False)\n    ax.spines[\"right\"].set_visible(False)\n\n    # Set axis color\n    ax.spines['left'].set_color(\"#bdbdbd\")\n    ax.spines['bottom'].set_color(\"#bdbdbd\")\n\n    df.plot(ax=ax, title=title, kind=kind, color=color);\n\n# Pie plot\ndef pie(labels, sizes, title):\n    patches, texts = plt.pie(sizes, colors=[\"#4caf50\", \"#ff9800\", \"#03a9f4\", \"#011ffd\", \"#ff2079\", \"#7e57c2\", \"#fdd835\"], startangle=90)\n    plt.legend(patches, labels, loc=\"best\")\n    plt.axis('equal')\n    plt.tight_layout()\n    plt.title(title)\n    plt.show()\n    \n# Map visualization\ndef mapplot(df, title, bartitle):\n    fig = go.Figure(data=go.Choropleth(\n        locations = df[\"Code\"],\n        z = df[\"Count\"],\n        text = df[\"Country\"],\n        colorscale = [(0,\"#fffde7\"), (1,\"#f57f17\")],\n        showscale = False,\n        marker_line_color=\"darkgray\",\n        marker_line_width=0.5,\n        colorbar_title = bartitle,\n    ))\n\n    fig.update_layout(\n        title={\n            'text': title,\n            'y':0.9,\n            'x':0.5,\n            'xanchor': 'center',\n            'yanchor': 'top'},\n        geo=dict(\n            showframe=False,\n            showcoastlines=False,\n            projection_type='equirectangular'\n        )\n    )\n    \n    fig.show(config={\"displayModeBar\": False, \"scrollZoom\": False})","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploring the data\nThe articles database has a copy of all articles that were found in metadata.csv. Pure duplicate articles (based on the sha hash) are filtered out. In addition to the metadata and text fields, a field named tags is added. Each article is tagged based on the topic. The only tag at this time is COVID-19 for articles that directly mention COVID-19 and related terms. This field is important as the embedding index and all model searches will go against the subset of data tagged as COVID-19.","metadata":{}},{"cell_type":"markdown","source":"## Articles Table\nA sample of the articles table is shown below.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport sqlite3\n\n# Connect to database\ndb = sqlite3.connect(\"cord19q/articles.sqlite\")\n\n# Articles\npd.set_option(\"max_colwidth\", 125)\narticles = pd.read_sql_query(\"select * from articles where tags is not null LIMIT 5\", db)\narticles","metadata":{"scrolled":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sections Table\nIn addition to the articles table, another table named sections is also created. The full text content is stored here. Each row is a single sentence from an article. Sentences are parsed using [NTLK's](https://www.nltk.org/) sent_tokenize method. The article id and tags are also stored with each section. The sections schema and sample rows are shown below.","metadata":{}},{"cell_type":"code","source":"# Connect to database\ndb = sqlite3.connect(\"cord19q/articles.sqlite\")\n\n# Sections\npd.set_option(\"max_colwidth\", 125)\nsections = pd.read_sql_query(\"select * from sections where tags is not null LIMIT 5\", db)\nsections","metadata":{"scrolled":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Most Frequent Words in Tagged Articles\nThe following wordcloud shows the most frequent words within the titles of tagged articles.","metadata":{}},{"cell_type":"code","source":"# Connect to database\ndb = sqlite3.connect(\"cord19q/articles.sqlite\")\n\n# Select data\narticles = pd.read_sql_query(\"select title from articles where tags is not null and title is not null order by published desc LIMIT 100000\", db)\n\n# Build word frequencies on filtered tokens\nfreqs = pd.Series(np.concatenate([tokenize(x) for x in articles.Title])).value_counts()\nwordcloud(freqs, \"Most frequent words in article titles tagged as COVID-19\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tagged Articles by Country Mentioned\nThe following map shows the Articles by Country mentioned. China is mentioned significantly more and it's count is clipped in this graphic to allow showing distribution across the globe.","metadata":{}},{"cell_type":"code","source":"# Connect to database\ndb = sqlite3.connect(\"cord19q/articles.sqlite\")\n\nsections = pd.read_sql_query(\"select text from sections where tags is not null order by id desc LIMIT 500000\", db)\n\n# Filter tokens to only country names. Build dataframe of Country, Count, Code\nmentions = pd.Series(np.concatenate([countrynames(x) for x in sections.Text])).value_counts()\nmentions = mentions.rename_axis(\"Country\").reset_index(name=\"Count\")\nmentions[\"Code\"] = [countrycode(x) for x in mentions[\"Country\"]]\n\n# Set max to 5000 to allow shading for multiple countries\nmentions[\"Count\"] = mentions[\"Count\"].clip(upper=5000)\n\nmapplot(mentions, \"Tagged Articles by Country Mentioned\", \"Articles by Country\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tagged Articles by Source\nThe following graph shows the articles grouped by the source field in the metadata. Only the Top 15 sources are shown.","metadata":{}},{"cell_type":"code","source":"# Connect to database\ndb = sqlite3.connect(\"cord19q/articles.sqlite\")\n\narticles = pd.read_sql_query(\"select source from articles where tags is not null order by published desc LIMIT 100000\", db)\n\nfreqs = articles.Source.value_counts().sort_values(ascending=True)[-15:]\nplot(freqs, \"Tagged Articles by Source\", \"barh\", \"#1976d2\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tagged Articles by Publication\nThe graph below shows the articles grouped by publication. Only the Top 15 publications are shown and many articles have no publication.","metadata":{}},{"cell_type":"code","source":"# Connect to database\ndb = sqlite3.connect(\"cord19q/articles.sqlite\")\n\narticles = pd.read_sql_query(\"select case when (Publication = '' OR Publication IS NULL) THEN '[None]' ELSE Publication END AS Publication from articles where tags is not null order by published desc LIMIT 100000\", db)\n\nfreqs = articles.Publication.value_counts().sort_values(ascending=True)[-15:]\n\nplot(freqs, \"Tagged Articles by Publication\", \"barh\", \"#7e57c2\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tagged Articles by Publication Month\nThe following graph shows articles by publication month. All of the articles have a publication date of 2020 or later (or the date is null). Many publication dates only include the year but there is a significant portion of articles this month, which shows the rapid pace things are moving. Also note that some publication dates are in the future. The articles have been released early to help find answers.","metadata":{}},{"cell_type":"code","source":"# Connect to database\ndb = sqlite3.connect(\"cord19q/articles.sqlite\")\n\narticles = pd.read_sql_query(\"select strftime('%Y-%m', published) as Published from articles where tags is not null and published >= '2020-01-01' order by published desc LIMIT 100000\", db)\n\nfreqs = articles.Published.value_counts().sort_index()\nplot(freqs, \"Tagged Articles by Publication Month\", \"bar\", \"#ff9800\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tagged Articles by Study Design\nThe chart below shows articles grouped by study design type. The study design gives researchers insight into the overall structure and quality of a study. The more rigor and hard data that goes into a study, the more reliable. This is a distinction compared to many other search systems, where we look for the best matching text. Credibility of the information is very important \nin helping judge whether the conclusions are reliable. \n\nThe medical field is rightfully a skeptical field. Many technologists are accustomed to running a web search and quickly trying the top results until you get to something that works. Lets be glad our doctors don't do the same. ","metadata":{}},{"cell_type":"code","source":"# Connect to database\ndb = sqlite3.connect(\"cord19q/articles.sqlite\")\n\narticles = pd.read_sql_query('select count(*) as count, case when design=1 then \"systematic review\" when design in (2, 3) then \"control trial\" ' + \n                             'when design in (4, 5) then \"prospective studies\" when design=6 then \"retrospective studies\" ' +\n                             'when design in (7, 8) then \"case series\" else \"modeling\" end as design from articles ' +\n                             'where tags is not null and design > 0 group by design order by published desc LIMIT 100000', db)\n\narticles = articles.groupby([\"design\"]).sum().reset_index()\n\n# Plot a pie chart of study types\npie(articles[\"design\"], articles[\"count\"], \"Tagged Articles by Study Design\")\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploration Takeaways\nGiven the urgency to find any data to help, many of the tagged articles are recent. Publications by nature put hypothesises and theories through a rigorious scientific method/peer review to ensure accuracy and reliability. It's a balancing act of not holding on to data that can help against making sure decisions are based on accurate data. Given that all searches are against this subset of data, conclusions should be carefully drawn. ","metadata":{}},{"cell_type":"markdown","source":"# Testing the model\n\nNow that both the articles.sqlite database and embeddings index are both created, lets test that everything is working properly.","metadata":{}},{"cell_type":"markdown","source":"## Word Embeddings\nThe foundation of sentence embeddings are word embeddings. As previously explained, sentence embeddings are just word embeddings joined together (each token weighted by a BM25 index). ","metadata":{}},{"cell_type":"code","source":"from txtai.embeddings import Embeddings\n\nembeddings = Embeddings()\nembeddings.load(\"cord19q\")\n\n# Get word vectors model\nvectors = embeddings.model.model\n\npd.DataFrame(vectors.most_similar(\"covid-19\", topn=10), columns=[\"key\", \"value\"])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The vector model is good at identifying near matches, which helps increase the accuracy of the overall model. Notice that the top hits are typos (covid-10 mistyped 0 instead of 9). \n\nBelow shows similarity for a list of terms, numbers look overall as expected, model has learned an association between the various diseases and knows phone is not related.","metadata":{}},{"cell_type":"code","source":"vectors.similarity(\"coronavirus\", [\"sars\", \"influenza\", \"ebola\", \"phone\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sentence Embeddings\nAt the highest level, the model builds embeddings for each sentence in the corpus. For input queries, it compares each sentence against the input query. Faiss enables that similarity search to be fast. An example of how this works at a small level below.","metadata":{}},{"cell_type":"code","source":"sentence1 = \"Range of incubation periods for the disease in humans\"\nsentence2 = \"The incubation period of 2019-nCoV is generally 3-7 days but no longer than 14 days, and the virus is infective during the incubation period\"\n\nembeddings.similarity(Tokenizer.tokenize(sentence1), [Tokenizer.tokenize(sentence2)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence1 = \"Range of incubation periods for the disease in humans\"\nsentence2 = \"The medical profession is short on facemasks during this period, more are needed\"\n\nembeddings.similarity(Tokenizer.tokenize(sentence1), [Tokenizer.tokenize(sentence2)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run a query\nRun a full query to ensure model is working.","metadata":{}},{"cell_type":"code","source":"from paperai.query import Query\n\n# Execute a test query\nQuery.run(\"antiviral covid-19 success treatment\", 5, \"cord19q\")","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building task reports\nTask reports are an aggregation of each question within a task in the challenge. For each question, a query is run and the top articles are returned. For each article, text matches are shown as bulleted points and these are the best matching sentences within the article. The full list of result sentences are also analyzed and run through a [textrank algorithm](https://en.wikipedia.org/wiki/Automatic_summarization#TextRank_and_LexRank). Highlights or top sentences within the results are also shown within the report. \n\nImportant source files to highlight\n* Report Process -> [execute.py](https://github.com/neuml/paperai/blob/master/src/python/paperai/report/execute.py)\n* Textrank algorithm to highlight best sentences -> [highlights.py](https://github.com/neuml/paperai/blob/master/src/python/paperai/highlights.py)\n\nQueries use a YAML formatted syntax that allows customizing the query string and result columns. The following example shows how to build a task report.","metadata":{}},{"cell_type":"code","source":"%%capture --no-display\n\nfrom paperai.report.execute import Execute as Report\nfrom IPython.display import display, Markdown\n\nquery = \"\"\"\nname: query\n\nantiviral covid-19 success treatment:\n    query: antiviral covid-19 success treatment\n    columns:\n        - name: Date\n        - name: Study\n        - name: Study Type\n        - name: Sample Size\n        - name: Study Population\n        - name: Matches\n        - name: Entry\n\"\"\"\n\n# Execute report query\nReport.run(query, 10, \"md\", \"cord19q\")\n\n# Render report\ndisplay(Markdown(filename=\"query.md\"))","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Round 1 Tasks\n\nThe following is a list of submitted [Round 1](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/discussion/148807) task notebooks. These notebooks are no longer updated as of 2020-05-02. \n\n- [What is known about transmission, incubation, and environmental stability?](https://www.kaggle.com/davidmezzetti/cord-19-transmission-incubation-environment)\n- [What do we know about COVID-19 risk factors?](https://www.kaggle.com/davidmezzetti/cord-19-risk-factors?scriptVersionId=33173909)\n- [What do we know about virus genetics, origin, and evolution?](https://www.kaggle.com/davidmezzetti/cord-19-virus-genetics-origin-and-evolution)\n- [What do we know about vaccines and therapeutics?](https://www.kaggle.com/davidmezzetti/cord-19-vaccines-and-therapeutics)\n- [What do we know about non-pharmaceutical interventions?](https://www.kaggle.com/davidmezzetti/cord-19-non-pharmaceutical-interventions)\n- [What has been published about medical care?](https://www.kaggle.com/davidmezzetti/cord-19-medical-care)\n- [What do we know about diagnostics and surveillance?](https://www.kaggle.com/davidmezzetti/cord-19-diagnostics-and-surveillance)\n- [What has been published about information sharing and inter-sectoral collaboration?](https://www.kaggle.com/davidmezzetti/cord-19-sharing-and-collaboration)\n- [What has been published about ethical and social science considerations?](https://www.kaggle.com/davidmezzetti/cord-19-ethical-and-social-science-considerations)\n\n# Round 2 Tasks\n\nThe following notebooks map to the tables in the [Kaggle COVID-19 Literature Review](https://www.kaggle.com/covid-19-contributions) and [Round 2](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/discussion/161447).\n\n- [Task 1: Population](https://www.kaggle.com/davidmezzetti/cord-19-population)\n- [Task 2: Relevant Factors](https://www.kaggle.com/davidmezzetti/cord-19-relevant-factors)\n- [Task 3: Patient Descriptions](https://www.kaggle.com/davidmezzetti/cord-19-patient-descriptions)\n- [Task 4: Models and Open Questions](https://www.kaggle.com/davidmezzetti/cord-19-models-and-open-questions)\n- [Task 5: Materials](https://www.kaggle.com/davidmezzetti/cord-19-materials)\n- [Task 6: Diagnostics](https://www.kaggle.com/davidmezzetti/cord-19-diagnostics)\n- [Task 7: Therapeutics](https://www.kaggle.com/davidmezzetti/cord-19-therapeutics)\n- [Task 8: Risk Factors](https://www.kaggle.com/davidmezzetti/cord-19-risk-factors)\n- [Full Task CSV Export List](https://www.kaggle.com/davidmezzetti/cord-19-task-csv-exports)\n\n","metadata":{}}]}