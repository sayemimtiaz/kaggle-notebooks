{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Bank marketing campaigns dataset analysis\n\n## Abstract\n\nThis is dataset that describe Portugal bank marketing campaigns results. Conducted campaigns were based mostly on direct phone calls, offering bank's clients to place a term deposit. If after all marking afforts client had agreed to place deposit - target variable marked 'yes', otherwise 'no'.\n\nSourse of the data https://archive.ics.uci.edu/ml/datasets/bank+marketing\n\nDataset description https://www.kaggle.com/volodymyrgavrysh/bank-marketing-campaigns-data-set-description\n\n\n## Citation Request:\n\nThis dataset is public available for research. The details are described in S. Moro, P. Cortez and P. Rita. \"A Data-Driven Approach to Predict the Success of Bank Telemarketing.\" Decision Support Systems, Elsevier, 62:22-31, June 2014 <\n\n## Task\npredicting the future results of marketing companies based on available statistics and, accordingly, formulating recommendations for such companies in the future.\nbuilding a profile of a consumer of banking services (deposits).\nApproach\n## The following steps will be performed to complete the task:\n\n1. Loading data and holding\n2. Expanatory Data Analysis (EDA).\n2. Formulating hypotheses regarding individual factors (features) for conducting correct data clearining and data preparation for modeling.\n3. The choice of metrics result.\n4. Building models\n5. The choice of the most effective model\n6. Conclusions and recomendations."},{"metadata":{},"cell_type":"markdown","source":"# 1. Loading data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nsns.set(style='whitegrid', palette='muted', font_scale=1.5)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"MAIN_PATH = '../input/'\ndf = pd.read_csv('../input/bank-marketing-dataset/bank.csv')\n# term_deposits = df.copy()\n# Have a grasp of how our data looks.\ndf.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **Input variables:**\n* Bank Client Data:\n1. 1 - age (numeric)\n1. 2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n1. 3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n1. 4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n1. 5 - default: has credit in default? (categorical: 'no','yes','unknown')\n1. 6 - housing: has housing loan? (categorical: 'no','yes','unknown')\n1. 7 - loan: has personal loan? (categorical: 'no','yes','unknown')\n* Last contact informatin:\n1. 8 - contact: contact communication type (categorical: 'cellular','telephone')\n1. 9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n1. 10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n1. 11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n* # other attributes:\n1. 12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n1. 13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n1. 14 - previous: number of contacts performed before this campaign and for this client (numeric)\n1. 15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n\n* Output variable (desired target):\n1. 21 - deposit / y - has the client subscribed a term deposit? (binary: 'yes','no')"},{"metadata":{},"cell_type":"markdown","source":"# 2. Explatory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['y'] = df['deposit']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build a function to show categorical values disribution\ndef plot_bar(column):\n    # temp df \n    temp_1 = pd.DataFrame()\n    # count categorical values\n    temp_1['No_deposit'] = df[df['y'] == 'no'][column].value_counts()\n    temp_1['Yes_deposit'] = df[df['y'] == 'yes'][column].value_counts()\n    temp_1.plot(kind='bar')\n    plt.xlabel(f'{column}')\n    plt.ylabel('Number of clients')\n    plt.title('Distribution of {} and deposit'.format(column))\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build a function to show categorical values disribution\ndef plot_bar_stacked(column):\n    # temp df \n    temp_1 = pd.DataFrame()\n    # count categorical values\n    temp_1['Open Deposit'] = df[df['y'] == 'yes'][column].value_counts()/(df[column].value_counts())\n    temp_1.plot(kind='bar')\n    plt.xlabel(f'{column}')\n    plt.ylabel('Reponse Rate %')\n    plt.title('Reponse Rate on offer'.format(column))\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_bar('job'), plot_bar_stacked('job')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_bar('marital'), plot_bar_stacked('marital')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_bar('education'), plot_bar_stacked('education')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_bar('contact'), plot_bar_stacked('contact')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_bar('poutcome'), plot_bar_stacked('poutcome')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_bar('loan'), plot_bar_stacked('loan')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_bar('housing'), plot_bar_stacked('housing')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_bar('contact'), plot_bar_stacked('contact')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert target variable into numeric\ndf.y = df.y.map({'no':0, 'yes':1}).astype('uint8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build correlation matrix\ncorr = df.corr()\ncorr.style.background_gradient(cmap='PuBu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n"},{"metadata":{},"cell_type":"markdown","source":"# 3. DATA PREPARING"},{"metadata":{},"cell_type":"markdown","source":"## 3.1 Null i NAN checking"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Verifying null values\nsns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  3.2 CREATING AND FORMATING DATA"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing values with binary ()\ndf.contact = df.contact.map({'cellular': 1, 'telephone': 0, 'unknown':0}).astype('uint8') #0 will means other\ndf.loan = df.loan.map({'yes': 1, 'unknown': 0, 'no' : 0}).astype('uint8')\ndf.housing = df.housing.map({'yes': 1, 'unknown': 0, 'no' : 0}).astype('uint8')\ndf.default = df.default.map({'no': 1, 'unknown': 0, 'yes': 0}).astype('uint8')\ndf.pdays = df.pdays.replace(999, 0) # replace with 0 if not contact \ndf.previous = df.previous.apply(lambda x: 1 if x > 0 else 0).astype('uint8') # binary has contact or not\n\n# binary if were was an outcome of marketing campane\ndf.poutcome = df.poutcome.map({'unknown':0, 'failure':0,'other':0, 'success':1}).astype('uint8')  #what mean unknow - not in campaign?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df.corr()\ncorr.style.background_gradient(cmap='PuBu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Convert Duration Call into 5 category'''\ndef duration(data):\n    data.loc[data['duration'] <= 102, 'duration'] = 1\n    data.loc[(data['duration'] > 102) & (data['duration'] <= 180)  , 'duration'] = 2\n    data.loc[(data['duration'] > 180) & (data['duration'] <= 319)  , 'duration'] = 3\n    data.loc[(data['duration'] > 319) & (data['duration'] <= 645), 'duration'] = 4\n    data.loc[data['duration']  > 645, 'duration'] = 5\n    return data\n\n\nduration(df);\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_a = df.drop(columns=['day','month'],axis = 1) #drop features which shouldn't influent outcome of campaign and remove targer feature - y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get dummies data for job,education and marital status\njob_dum = pd.get_dummies(df_a['job']).rename(columns=lambda x: 'job_' + str(x))\neducation_dum = pd.get_dummies(df['education']).rename(columns=lambda x: 'education_' + str(x))\nmarital_dum = pd.get_dummies(df['marital']).rename(columns=lambda x: 'marital' + str(x))\n## dummies for age and pdays - zrob buckety */","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create dataset with dummies variables\ndf_b = pd.concat([df_a,job_dum,education_dum,marital_dum],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_b.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_target = df_b['y']\ndf_feat =  df_b.drop(columns=['job','marital','education','y','deposit'],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.3 Scaling Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(df_feat)\nscaled_features = scaler.transform(df_feat)\ndf_feat_sc = pd.DataFrame(scaled_features,columns=df_feat.columns)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df_feat, df_target, test_size=0.30, random_state=101)\nX_train_sc, X_test_sc, y_train_sc, y_test_sc = train_test_split(df_feat_sc, df_target, test_size=0.30, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.4 Features Selections\n\nFor selectin feautres I will use random forrest method because it is robust, nonlinear"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\nclf = RandomForestClassifier(n_estimators = 50, max_depth = 4)\n\nscores = []\nnum_features = len(df_feat_sc.columns)\nfor i in range(num_features):\n    col = df_feat_sc.columns[i]\n    score = np.mean(cross_val_score(clf, df_feat_sc[col].values.reshape(-1,1), df_target, cv=10))\n    scores.append((float(score*100), col))\n\nprint(sorted(scores, reverse = True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"After checking outcome for futher modeling I will use ten 10 metrics that is:\nduration, \npdays,\nprevious, \npoutcome,\nhousing\ncontact\nage\nbalance\ncampaign\nmaritalsingle"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"df_target = df_b['y']\ndf_feat = df_feat[['duration','pdays','previous','poutcome','housing','contact','age','balance','campaign','maritalsingle']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. The choice of metrics result.\n"},{"metadata":{},"cell_type":"markdown","source":"Y is the target column in the dataset. Outcome '1' is  positive answer on campaign and '0' . Dataset is balance so for finding the best model I will use ROC Curve."},{"metadata":{},"cell_type":"markdown","source":"# 6. Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.model_selection import GridSearchCV\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nclassifier_LR = LogisticRegression(random_state = 0, penalty = 'l1')\nclassifier_LR.fit(X_train_sc, y_train_sc)\n\n# Predicting Test Set\ny_pred = classifier_LR.predict(X_test_sc)\n\nacc = accuracy_score(y_test_sc, y_pred)\nprec = precision_score(y_test_sc, y_pred)\nrec = recall_score(y_test_sc, y_pred)\nf1 = f1_score(y_test_sc, y_pred)\n\ny_pred_prob_lr = classifier_LR.predict_proba(X_test_sc)[:,1]\nfpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test_sc, y_pred_prob_lr)\nroc_auc_lr = auc(fpr_lr, tpr_lr)\nprecision_lr, recall_lr, th_lr = precision_recall_curve(y_test_sc, y_pred_prob_lr)\nprecision_recall_auc_lr = auc(recall_lr, precision_lr)\n\nresults = pd.DataFrame([['Logistic Regression', acc, prec, rec, f1, roc_auc_lr, precision_recall_auc_lr]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC auc','Precision recall auc'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Logistic Regression with grid search\npenalty = ['l1', 'l2']\n# Create regularization hyperparameter space\nC = [0.01,0.1,0.5,0.8,1.2,1.5]\n\nhyperparameters = dict(C=C, penalty=penalty)\n# Create grid search using 5-fold cross validation\nclf = GridSearchCV(classifier_LR, hyperparameters, cv=5, verbose=0,scoring='roc_auc')\n# Fit grid search\nLR_best_model = clf.fit(X_train_sc, y_train_sc)\n\n# Predicting Test Set\ny_pred = LR_best_model.predict(X_test_sc)\nacc = accuracy_score(y_test_sc, y_pred)\nprec = precision_score(y_test_sc, y_pred)\nrec = recall_score(y_test_sc, y_pred)\nf1 = f1_score(y_test_sc, y_pred)\n\n\ny_pred_prob_LR_best_model = LR_best_model.predict_proba(X_test_sc)[:,1]\nfpr_LR_best_model, tpr_LR_best_model, thresholds_LR_best_model = roc_curve(y_test_sc, y_pred_prob_LR_best_model)\nroc_auc_LR_best_model = auc(fpr_LR_best_model, tpr_LR_best_model)\nprecision_LR_best_model, recall_LR_best_model, th_LR_best_model = precision_recall_curve(y_test_sc, y_pred_prob_LR_best_model)\nprecision_recall_auc_LR_best_model = auc(recall_LR_best_model, precision_LR_best_model)\n\n\n\nmodel_results = pd.DataFrame([['Linear Regression Best Model', acc, prec, rec, f1, roc_auc_LR_best_model, precision_recall_auc_LR_best_model]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC auc','Precision recall auc'])\n\nresults = results.append(model_results, ignore_index = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nclassifier_NaiveB = GaussianNB()\nclassifier_NaiveB.fit(X_train_sc, y_train_sc)\n\n# Predicting Test Set\ny_pred = classifier_NaiveB.predict(X_test_sc)\nacc = accuracy_score(y_test_sc, y_pred)\nprec = precision_score(y_test_sc, y_pred)\nrec = recall_score(y_test_sc, y_pred)\nf1 = f1_score(y_test_sc, y_pred)\n\n\ny_pred_prob_NaiveB = classifier_NaiveB.predict_proba(X_test_sc)[:,1]\nfpr_NaiveB, tpr_NaiveB, thresholds_NaiveB = roc_curve(y_test_sc, y_pred_prob_NaiveB)\nroc_auc_NaiveB = auc(fpr_NaiveB, tpr_NaiveB)\nprecision_NaiveB, recall_NaiveB, th_NaiveB = precision_recall_curve(y_test_sc, y_pred_prob_NaiveB)\nprecision_recall_auc_NaiveB = auc(recall_NaiveB, precision_NaiveB)\n\n\n\nmodel_results = pd.DataFrame([['Naive Bayes (Gaussian)', acc, prec, rec, f1, roc_auc_NaiveB, precision_recall_auc_NaiveB]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC auc','Precision recall auc'])\n\nresults = results.append(model_results, ignore_index = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Random Forest Gini (n=100)\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier_RandomForest100 = RandomForestClassifier(random_state = 0, n_estimators = 100,\n                                    criterion = 'gini')\nclassifier_RandomForest100.fit(X_train_sc, y_train_sc)\n\n# Predicting Test Set\ny_pred = classifier_RandomForest100.predict(X_test_sc)\nacc = accuracy_score(y_test_sc, y_pred)\nprec = precision_score(y_test_sc, y_pred)\nrec = recall_score(y_test_sc, y_pred)\nf1 = f1_score(y_test_sc, y_pred)\n\ny_pred_prob_RandomForest100 = classifier_RandomForest100.predict_proba(X_test_sc)[:,1]\nfpr_RandomForest100, tpr_RandomForest100, thresholds_RandomForest100 = roc_curve(y_test_sc, y_pred_prob_RandomForest100)\nroc_auc_RandomForest100 = auc(fpr_RandomForest100, tpr_RandomForest100)\nprecision_RandomForest100, recall_RandomForest100, th_RandomForest100 = precision_recall_curve(y_test_sc, y_pred_prob_RandomForest100)\nprecision_recall_auc_RandomForest100 = auc(recall_RandomForest100, precision_RandomForest100)\n\nmodel_results = pd.DataFrame([['Random Forest Gini (n=100)', acc, prec, rec, f1, roc_auc_RandomForest100 , precision_recall_auc_RandomForest100 ]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC auc','Precision recall auc'])\n\nresults = results.append(model_results, ignore_index = True)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use random"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Random Forest Gini (n=200)\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier_RandomForest200 = RandomForestClassifier(random_state = 0, n_estimators = 200,\n                                    criterion = 'gini')\nclassifier_RandomForest200.fit(X_train_sc, y_train_sc)\n\n# Predicting Test Set\ny_pred = classifier_RandomForest200.predict(X_test_sc)\nacc = accuracy_score(y_test_sc, y_pred)\nprec = precision_score(y_test_sc, y_pred)\nrec = recall_score(y_test_sc, y_pred)\nf1 = f1_score(y_test_sc, y_pred)\n\n\ny_pred_prob_RandomForest200 = classifier_RandomForest200.predict_proba(X_test_sc)[:,1]\nfpr_RandomForest200, tpr_RandomForest200, thresholds_RandomForest200 = roc_curve(y_test_sc, y_pred_prob_RandomForest200)\nroc_auc_RandomForest200 = auc(fpr_RandomForest200, tpr_RandomForest200)\nprecision_RandomForest200, recall_RandomForest200, th_RandomForest200 = precision_recall_curve(y_test_sc, y_pred_prob_RandomForest200)\nprecision_recall_auc_RandomForest200 = auc(recall_RandomForest200, precision_RandomForest200)\n\nmodel_results = pd.DataFrame([['Random Forest Gini (n=200)', acc, prec, rec, f1, roc_auc_RandomForest200 , precision_recall_auc_RandomForest200 ]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC auc','Precision recall auc'])\n\nresults = results.append(model_results, ignore_index = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Random Forest with GridSearch\n# I use f1-score because unbalance classification\n\nrf_model = RandomForestClassifier()\nrf_params = {\n \"n_estimators\": [10,50,100,400],\n \"max_depth\": (2,5,10),\n \"min_samples_split\": (2,5,10,15),\n \"min_samples_leaf\": (1,5,10,15)\n }\nrf_grid = GridSearchCV(rf_model,\n rf_params,\nscoring='roc_auc',\n cv=5,\n verbose=1,\n n_jobs=-1)\n\nrf_grid.fit(X_train_sc, y_train_sc)\n\n# Predicting Test Set\ny_pred = rf_grid.predict(X_test_sc)\nacc = accuracy_score(y_test_sc, y_pred)\nprec = precision_score(y_test_sc, y_pred)\nrec = recall_score(y_test_sc, y_pred)\nf1 = f1_score(y_test_sc, y_pred)\n\n\ny_pred_prob_rf_grid = rf_grid.predict_proba(X_test_sc)[:,1]\nfpr_rf_grid, tpr_rf_grid, thresholds_rf_grid = roc_curve(y_test_sc, y_pred_prob_rf_grid)\nroc_auc_rf_grid = auc(fpr_rf_grid, tpr_rf_grid)\nprecision_rf_grid, recall_rf_grid, th_rf_grid = precision_recall_curve(y_test_sc, y_pred_prob_rf_grid)\nprecision_recall_auc_rf_grid = auc(recall_rf_grid, precision_rf_grid)\n\nmodel_results = pd.DataFrame([['Random Forest Grid Search', acc, prec, rec, f1, roc_auc_rf_grid , precision_recall_auc_rf_grid ]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC auc','Precision recall auc'])\n\nresults = results.append(model_results, ignore_index = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## SVM (rbf)\nfrom sklearn.svm import SVC\nclassifier_SVM = SVC(random_state = 0, kernel = 'rbf', probability= True)\nclassifier_SVM.fit(X_train_sc, y_train_sc)\n\n# Predicting Test Set\ny_pred = classifier_SVM.predict(X_test_sc)\nacc = accuracy_score(y_test_sc, y_pred)\nprec = precision_score(y_test_sc, y_pred)\nrec = recall_score(y_test_sc, y_pred)\nf1 = f1_score(y_test_sc, y_pred)\n\ny_pred_prob_SVM = classifier_SVM.predict_proba(X_test_sc)[:,1]\nfpr_SVM, tpr_SVM, thresholds_SVM = roc_curve(y_test, y_pred_prob_SVM)\nroc_auc_SVM = auc(fpr_SVM, tpr_SVM)\nprecision_SVM, recall_SVM, th_SVM = precision_recall_curve(y_test_sc, y_pred_prob_SVM)\nprecision_recall_auc_SVM = auc(recall_SVM, precision_SVM)\n\nmodel_results = pd.DataFrame([['SVM (RBF)', acc, prec, rec, f1, roc_auc_SVM , precision_recall_auc_SVM ]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC auc','Precision recall auc'])\n\nresults = results.append(model_results, ignore_index = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## SVM with GridSearch\n\nparam_grid_svm = {'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf']}  \n  \nsvm_grid_rbf = GridSearchCV(SVC(probability= True), param_grid_svm, refit = True, verbose = 3, cv = 2, scoring='roc_auc') \n  \n# fitting the model for grid search \nsvm_grid_rbf.fit(X_train_sc, y_train_sc) \n\n# Predicting Test Set\ny_pred = svm_grid_rbf.predict(X_test_sc)\nacc = accuracy_score(y_test_sc, y_pred)\nprec = precision_score(y_test_sc, y_pred)\nrec = recall_score(y_test_sc, y_pred)\nf1 = f1_score(y_test_sc, y_pred)\n\ny_pred_prob_svm_grid_rbf = svm_grid_rbf.predict_proba(X_test_sc)[:,1]\nfpr_svm_grid_rbf, tpr_svm_grid_rbf, thresholds_svm_grid_rbf = roc_curve(y_test_sc, y_pred_prob_svm_grid_rbf)\nroc_auc_SVM_grid_rbf = auc(fpr_svm_grid_rbf, tpr_svm_grid_rbf)\nprecision_svm_grid_rbf, recall_svm_grid_rbf, th_svm_grid_rbf = precision_recall_curve(y_test_sc, y_pred_prob_svm_grid_rbf)\nprecision_recall_auc_SVM_grid_rbf = auc(recall_svm_grid_rbf, precision_svm_grid_rbf)\n\nmodel_results = pd.DataFrame([['SVM (RBF) with Grid', acc, prec, rec, f1, roc_auc_SVM_grid_rbf , precision_recall_auc_SVM_grid_rbf ]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC auc','Precision recall auc'])\nmodel_results\nresults = results.append(model_results, ignore_index = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## SVM (Linear) \nfrom sklearn.svm import SVC\nclassifier_SVM = SVC(random_state = 0, kernel = 'linear', probability= True)\nclassifier_SVM.fit(X_train_sc, y_train_sc)\n\n# Predicting Test Set\ny_pred = classifier_SVM.predict(X_test_sc)\nacc = accuracy_score(y_test_sc, y_pred)\nprec = precision_score(y_test_sc, y_pred)\nrec = recall_score(y_test_sc, y_pred)\nf1 = f1_score(y_test_sc, y_pred)\n\n\ny_pred_prob_SVM = classifier_SVM.predict_proba(X_test)[:,1]\nfpr_SVM, tpr_SVM, thresholds_SVM = roc_curve(y_test, y_pred_prob_SVM)\nroc_auc_SVM = auc(fpr_SVM, tpr_SVM)\nprecision_SVM, recall_SVM, th_SVM = precision_recall_curve(y_test, y_pred_prob_SVM)\nprecision_recall_auc_SVM = auc(recall_SVM, precision_SVM)\n\nmodel_results = pd.DataFrame([['SVM (Linear)', acc, prec, rec, f1, roc_auc_SVM , precision_recall_auc_SVM ]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC auc','Precision recall auc'])\n\nresults = results.append(model_results, ignore_index = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nClassifier_KNN = KNeighborsClassifier(n_neighbors=2)\nClassifier_KNN.fit(X_train_sc, y_train_sc)\n\n# Predicting Test Set\ny_pred = Classifier_KNN.predict(X_test_sc)\nacc = accuracy_score(y_test_sc, y_pred)\nprec = precision_score(y_test_sc, y_pred)\nrec = recall_score(y_test_sc, y_pred)\nf1 = f1_score(y_test_sc, y_pred)\n\ny_pred_prob_KNN = Classifier_KNN.predict_proba(X_test_sc)[:,1]\nfpr_KNN, tpr_KNN, thresholds_KNN = roc_curve(y_test_sc, y_pred_prob_KNN)\nroc_auc_KNN = auc(fpr_KNN, tpr_KNN)\nprecision_KNN, recall_KNN, th_KNN = precision_recall_curve(y_test_sc, y_pred_prob_KNN)\nprecision_recall_auc_KNN = auc(recall_KNN, precision_KNN)\n\nmodel_results = pd.DataFrame([['KNeighborsClassifier', acc, prec, rec, f1, roc_auc_KNN , precision_recall_auc_KNN ]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC auc','Precision recall auc'])\n\nmodel_results\nresults = results.append(model_results, ignore_index = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#KNN model with grid\nClassifier_KNN_grid = KNeighborsClassifier()\n\nknn_param_grid = {'n_neighbors':[2,4,5,6,7,19],\n              'leaf_size':[1,3,5],\n              'algorithm':['auto', 'kd_tree'],\n              'n_jobs':[-1]}\n\n#Fit the model\nKNN_grid = GridSearchCV(Classifier_KNN_grid, knn_param_grid, cv=3,scoring='roc_auc')\nKNN_grid.fit(X_train_sc, y_train_sc)\n\n# Predicting Test Set\ny_pred = Classifier_KNN.predict(X_test_sc)\nacc = accuracy_score(y_test_sc, y_pred)\nprec = precision_score(y_test_sc, y_pred)\nrec = recall_score(y_test_sc, y_pred)\nf1 = f1_score(y_test_sc, y_pred)\n\ny_pred_prob_KNN = Classifier_KNN.predict_proba(X_test_sc)[:,1]\nfpr_KNN, tpr_KNN, thresholds_KNN = roc_curve(y_test_sc, y_pred_prob_KNN)\nroc_auc_KNN = auc(fpr_KNN, tpr_KNN)\nprecision_KNN, recall_KNN, th_KNN = precision_recall_curve(y_test_sc, y_pred_prob_KNN)\nprecision_recall_auc_KNN = auc(recall_KNN, precision_KNN)\n\nmodel_results = pd.DataFrame([['KNeighborsClassifier Grid', acc, prec, rec, f1, roc_auc_KNN , precision_recall_auc_KNN ]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC auc','Precision recall auc'])\n\nmodel_results\nresults = results.append(model_results, ignore_index = True)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Neural Networks \nfrom sklearn.neural_network import MLPClassifier\nClassifie_MLP = MLPClassifier(hidden_layer_sizes=(30,60,90,120),\n learning_rate='adaptive',\n batch_size=30,\n learning_rate_init=0.01,\n shuffle=True)\n\nClassifie_MLP.fit(X_train_sc, y_train_sc)\n\n\n# Predicting Test Set\ny_pred = Classifie_MLP.predict(X_test_sc)\nacc = accuracy_score(y_test_sc, y_pred)\nprec = precision_score(y_test_sc, y_pred)\nrec = recall_score(y_test_sc, y_pred)\nf1 = f1_score(y_test_sc, y_pred)\n\ny_pred_prob_MLP = Classifie_MLP.predict_proba(X_test_sc)[:,1]\nfpr_MLP, tpr_MLP, thresholds_MLP = roc_curve(y_test_sc, y_pred_prob_MLP)\nroc_auc_MLP = auc(fpr_MLP, tpr_MLP)\nprecision_MLP, recall_MLP, th_MLP = precision_recall_curve(y_test_sc, y_pred_prob_MLP)\nprecision_recall_auc_MLP = auc(recall_MLP, precision_MLP)\n\nmodel_results = pd.DataFrame([['MLP - Neural Networks', acc, prec, rec, f1, roc_auc_MLP, precision_recall_auc_MLP ]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC auc','Precision recall auc'])\n\nresults = results.append(model_results, ignore_index = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Xgboost\nfrom xgboost import XGBClassifier\nxgb_model = XGBClassifier()\nxgb_model.fit(X_train_sc, y_train_sc)\n\n# performance\ny_pred = xgb_model.predict(X_test_sc)\nacc = accuracy_score(y_test_sc, y_pred)\nprec = precision_score(y_test_sc, y_pred)\nrec = recall_score(y_test_sc, y_pred)\nf1 = f1_score(y_test_sc, y_pred)\n\ny_pred_prob_xgb_model = xgb_model.predict_proba(X_test_sc)[:,1]\nfpr_xgb_model, tpr_xgb_model, thresholds_xgb_model = roc_curve(y_test_sc, y_pred_prob_xgb_model)\nroc_auc_xgb_model = auc(fpr_xgb_model, tpr_xgb_model)\nprecision_xgb_model, recall_xgb_model, th_xgb_model = precision_recall_curve(y_test_sc, y_pred_prob_xgb_model)\nprecision_recall_auc_xgb_model = auc(recall_xgb_model, precision_xgb_model)\n\nmodel_results = pd.DataFrame([['XGBoost', acc, prec, rec, f1, roc_auc_xgb_model, precision_recall_auc_xgb_model ]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC auc','Precision recall auc'])\n\nresults = results.append(model_results, ignore_index = True)\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Xgboost with grid\nXGB_model = XGBClassifier()\n\nXGB_params = {\n \"n_estimators\": [10,20,50],\n \"max_depth\": (5,15),\n \"min_samples_split\": (2,5,10,15),\n \"min_samples_leaf\": (1,5,10,15)\n }\n\n\n\n\nXGB_grid = GridSearchCV(XGB_model,\n XGB_params,\nscoring='f1',\n cv=5,\n verbose=1,\n n_jobs=-1)\n\nXGB_grid.fit(X_train_sc, y_train_sc)\n\n\n# performance\ny_pred = XGB_grid.predict(X_test_sc)\nacc = accuracy_score(y_test_sc, y_pred)\nprec = precision_score(y_test_sc, y_pred)\nrec = recall_score(y_test_sc, y_pred)\nf1 = f1_score(y_test_sc, y_pred)\n\ny_pred_prob_XGB_grid = XGB_grid.predict_proba(X_test_sc)[:,1]\nfpr_XGB_grid, tpr_XGB_grid, thresholds_xgb_model = roc_curve(y_test_sc, y_pred_prob_XGB_grid)\nroc_auc_XGB_grid = auc(fpr_XGB_grid, tpr_XGB_grid)\nprecision_XGB_grid, recall_XGB_grid, th_XGB_grid = precision_recall_curve(y_test_sc, y_pred_prob_XGB_grid)\nprecision_recall_auc_XGB_grid = auc(recall_XGB_grid, precision_XGB_grid)\n\nmodel_results = pd.DataFrame([['XGBoost Grid', acc, prec, rec, f1, roc_auc_XGB_grid, precision_recall_auc_XGB_grid ]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC auc','Precision recall auc'])\n\nresults = results.append(model_results, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. The choice of the most effective model"},{"metadata":{"trusted":true},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on Precision Recall AUC. looks that 3 best models are: SVM (RBF) with Grid, Random Forest with Grid and Xgboost with standard setups. This 3 best model I will prescent on ROC chart and Precision-Recall Chart."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_svm_grid_rbf, tpr_svm_grid_rbf, label='SVM RBF with GridSearch (area = %0.3f)' % roc_auc_LR_best_model)\nplt.plot(fpr_xgb_model, tpr_xgb_model, label='Xbboost with deafult(area = %0.3f)' % roc_auc_xgb_model)\nplt.plot(fpr_rf_grid, tpr_rf_grid, label='Random Forest with GridSearch (area = %0.3f)' % roc_auc_rf_grid )\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC curves from the investigated models')\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Precision - recall curve\nplt.plot([1, 0], [0, 1], 'k--')\nplt.plot(recall_svm_grid_rbf, precision_svm_grid_rbf, label='SVM RBF with Grid')\nplt.plot(recall_xgb_model, precision_xgb_model, label='XGB with default')\nplt.plot(recall_rf_grid, precision_rf_grid, label='Random Forest with Grid')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Base on all outcome Xgboost with default parametrs is the best model. Based on ROC AUC."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test_sc, xgb_model.predict(X_test_sc)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use Xgboost model for clients selection. Work more on new features."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}