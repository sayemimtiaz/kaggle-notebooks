{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import time\nstart = time.time()\nimport os\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import StandardScaler\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom textblob import TextBlob\n\n#plotting\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#statistics & econometrics\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\n\n#model fiiting and selection\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import make_scorer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.linear_model import Lasso, Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor, XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/stocknews/Combined_News_DJIA.csv\",low_memory=False,\n                    parse_dates=[0])\n#use date as index\ndf.index = df.Date\ndf = df.drop([\"Date\"], axis=1)\n\nfull_stock = pd.read_csv(\"../input/stocknews/DJIA_table.csv\",low_memory=False,\n                    parse_dates=[0])\nfull_stock.index = full_stock.Date\nfull_stock = full_stock.drop([\"Date\"], axis=1)\n\n#calculate the difference between opening and closing stock value\nfull_stock['Diff'] = full_stock.Close - full_stock.Open\nfl_cols = list(full_stock.columns)\nfl_cols = fl_cols[0:6]\nfull_stock = full_stock.drop(fl_cols, axis=1)\n\n#merge the headlines together into one text\nheadlines = []\nfor row in range(0,len(df.index)):\n   headlines.append(' '.join(str(x) for x in df.iloc[row,2:27]))\n\ndf['Headlines'] = headlines\n\n#add the difference between opening and closing stock value to the df - this will be the y variable\ndf = df.merge(full_stock, left_index=True, right_index=True)\n\n#drop the Label column and Top1-Top25\ndrop_it = df.columns\ndrop_it = drop_it[0:26]\ndf = df.drop(drop_it, axis=1)\n\n#show how the dataset looks like\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data cleaning\n\n### NA treatment\nWe'll simply fill the NAs in the numerical features (Date, Close). \nIn the text features we'll fill the missing values with ''."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.replace(np.nan, ' ', regex=True)\n\n#sanity check\ndf.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove the HTML tags and digits\nThere are several non-word tags in the headlines that would just bias the sentiment analysis so we need to remove them and replace with ''. This can be done with regex."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.replace('b\\\"|b\\'|\\\\\\\\|\\\\\\\"|\\'', '', regex=True)\ndf = df.replace('[0-9]', '', regex=True)\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train-test split\nSome of the feature extraction methods require to use separate train-test sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"def ts_train_test_split(X, y, test_size):\n    \"\"\"\n        Perform train-test split with respect to time series structure\n    \"\"\"\n    \n    # get the index after which test set starts\n    test_index = int(len(X)*(1-test_size))\n    \n    X_train = X.iloc[:test_index]\n    y_train = y.iloc[:test_index]\n    X_test = X.iloc[test_index:]\n    y_test = y.iloc[test_index:]\n    \n    return X_train, X_test, y_train, y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.copy()\nX = X.drop(['Diff'],axis=1)\ny = df.Diff","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = np.mean(y)\nsd = np.std(y)\ny = (y-mean)/sd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = ts_train_test_split(X, y, test_size = 0.1)\n\n#remove first 7 rows of training set to acocunt for rows that will be removed later due to lagging\nX_train = X_train.drop(X_train.index[[np.arange(0,7)]])\ny_train = y_train[7:len(y_train)]\n\n\n#save the train-test indeces\ntest_idx = y_test.index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sentiment and subjectivity score extraction\nNow I run the sentiment analysis extracting the compound score that goes from -0.5 (most negative) to 0.5 (most positive). I'm going to use the \"dirty\" texts in this part because VADER can utilize the information such as ALL CAPS, punctuation, etc. I'll also calculate the subjectivity of each headline using the TextBlob package.\n\nInitialise the VADER analyzer."},{"metadata":{"trusted":true},"cell_type":"code","source":"Anakin = SentimentIntensityAnalyzer()\n\nAnakin.polarity_scores(\" \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Write a function to save the subjectivity score directly from TextBlob function's output. Subjectivity score might detect direct quotes in the headlines and positive stuff is rarely quoted in the headline."},{"metadata":{"trusted":true},"cell_type":"code","source":"def detect_subjectivity(text):\n    return TextBlob(text).sentiment.subjectivity\n\ndetect_subjectivity(\" \") #should return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_vect=time.time()\nprint(\"ANAKIN: 'Intializing the process..'\")\ncol=\"Headlines\"\n\n\ndf[col] = df[col].astype(str) # Make sure data is treated as a string\ndf[col+'_pos']= df[col].apply(lambda x:Anakin.polarity_scores(x)['pos'])\ndf[col+'_neg']= df[col].apply(lambda x:Anakin.polarity_scores(x)['neg'])\ndf[col+'_comp']= df[col].apply(lambda x:Anakin.polarity_scores(x)['compound'])\ndf[col+'_sub'] = df[col].apply(detect_subjectivity)\n    \nprint(\"VADER: Vaderization completed after %0.2f Minutes\"%((time.time() - start_vect)/60))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# N-grams, n=1:2"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nngrammer = CountVectorizer(ngram_range=(1, 2), lowercase=True)\nn_grams_train = ngrammer.fit_transform(X_train.Headlines)\nn_grams_test = ngrammer.transform(X_test.Headlines)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_grams_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have ~344k n-grams."},{"metadata":{"trusted":true},"cell_type":"code","source":"#the text isn't required anymore\ndf = df.drop(col,axis=1)\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explorative Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1 = go.Figure()\nfig1.add_trace(go.Scatter(x=df.index, y=df.Diff,\n                    mode='lines'))\ntitle = []\ntitle.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,\n                              xanchor='left', yanchor='bottom',\n                              text='Aug, 2008 - Jun, 2016',\n                              font=dict(family='Arial',\n                                        size=30,\n                                        color='rgb(37,37,37)'),\n                              showarrow=False))\nfig1.update_layout(xaxis_title='Date',\n                   yaxis_title='Difference between opening and closing value (in $)',\n                  annotations=title)\nfig1.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pairplot = sns.pairplot(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we'll look at some descriptive statistics about the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"def unique_ratio (col):\n    return len(np.unique(col))/len(col)\n\ncols = list(df.columns)\ncols = cols[1:len(cols)]\n\nur = []\nvar = []\nfor col in cols:\n    ur.append(unique_ratio(df[col]))\n    var.append(np.var(df[col]))\n    \nfeature_sel = pd.DataFrame({'Column': cols, \n              'Unique': ur,\n              'Variance': var})\nfeature_sel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"uniq_fig = go.Figure(data=go.Scatter(\n    x=feature_sel.Column,\n    y=feature_sel.Unique,\n    mode='markers'\n))\nuniq_fig.update_layout( yaxis_title='Unique ratio')\nuniq_fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_fig = go.Figure(data=go.Scatter(\n    x=feature_sel.Column,\n    y=feature_sel.Variance,\n    mode='markers'\n))\nvar_fig.update_layout( yaxis_title='Variance')\nvar_fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop = ['Headlines_pos']\nclean_df = df.copy()\nclean_df = clean_df.drop(drop, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Lag the extracted features\nTo allow the models to look into the past, we'll add features which are essentially just copies of rows from n-steps back. In order to not create too many new features we'll add only features from 1 week prior to the current datapoint."},{"metadata":{"trusted":true},"cell_type":"code","source":"lag_df = clean_df.copy()\nlag_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_lag = list(lag_df.columns)\nto_lag_7 = to_lag[0]\nto_lag_3 = to_lag[1:len(to_lag)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lagging text features two days back\nfor col in to_lag_3:\n    for i in range(1, 4):\n        new_name = col + ('_lag_{}'.format(i))\n        lag_df[new_name] = lag_df[col].shift(i)\n    \n#lagging closing values 7 days back\nfor i in range(1, 8):\n    new_name = to_lag_7 + ('_lag_{}'.format(i))\n    lag_df[new_name] = lag_df[to_lag_7].shift(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lag_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this process, rows with NAs were created. Unfortunately these rows will have to be removed since we simply don't have the data from the future."},{"metadata":{"trusted":true},"cell_type":"code","source":"lag_df = lag_df.drop(lag_df.index[[np.arange(0,7)]])\n\n#sanity check for NaNs\nlag_df.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lag_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Time features"},{"metadata":{"trusted":true},"cell_type":"code","source":"lag_df[\"Day\"] = lag_df.index.dayofweek\nlag_df[\"Month\"] = lag_df.index.month\nlag_df[\"Year\"] = lag_df.index.year\nlag_df[\"Quarter\"] = lag_df.index.quarter","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model training\nLet's train 3(+1) ML models. We'll do this in 3 stages. First, using the econometric features alone (7 lags of y). Second, train classifiers using n_grams and topics to predict direction. Third, including the information extracted from the headlines (compound, subjectivity and their lags) and direction of the movement predicted by the classifiers from second stage.\n\n**Models**\n- Ridge regression - punish model for using too many features but doesn't allow the coeficients drop to zero completely\n- SVM\n- XGBoost\n- Naive Bayes (only for n-grams and topics)\n\nWe'll score all models by mean squared error as it gives higher penalty to larger mistakes.\nAnd before each model training we'll standardize the training data.\n\nThe first step will be creating folds for cross-validation. We'll use the same folds for all models in order to allow for creating a meta-model. Since we're working with timeseries the folds cannot be randomly selected. Instead a fold will be a sequence of data so that we don't lose the time information."},{"metadata":{"trusted":true},"cell_type":"code","source":"# for time-series cross-validation set 5 folds \ntscv = TimeSeriesSplit(n_splits=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we split the dataset into training and testing using the indeces saved at the begining."},{"metadata":{"trusted":true},"cell_type":"code","source":"lag_df = lag_df.drop(['Diff'], axis=1)\nprint(min(test_idx))\nX_train = lag_df[lag_df.index < min(test_idx)]\nX_test = lag_df[lag_df.index >= min(test_idx)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And we also prepare the feature sets for each of the models. \n - Classifier 1 - n-grams\n - Classifier 2 - sentiment and lags of y\n - Econometric - using only lags of y\n - NLP - using only sentiment and subjectivity"},{"metadata":{"trusted":true},"cell_type":"code","source":"#classifier 1 - already prepared from before\n#n_grams_train\n#n_grams_test\nX_train.columns\n#classifier 2 - the same as X_train\nX_train_c2 = X_train\nX_test_c2 = X_test\n\n#Econometric\ndrop_e = list(X_train.columns)\ndrop_e = drop_e[0:12]\nX_train_e = X_train.drop(drop_e, axis=1)\nX_test_e = X_test.drop(drop_e, axis=1)\n\n#NLP\nX_train_n = X_train[drop_e]\nX_test_n = X_test[drop_e]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classifiers to predict direction of stock change"},{"metadata":{},"cell_type":"markdown","source":"We need to convert the y variable to binary. 1=up or same, 0=down"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_dir = []\nfor i in range(0,len(y_train)):\n    if y_train[i]<0: y_train_dir.append(0)\n    else: y_train_dir.append(1)\n        \ny_test_dir = []\nfor i in range(0,len(y_test)):\n    if y_test[i]<0: y_test_dir.append(0)\n    else: y_test_dir.append(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And define a scorer for classifiers, we'll use balanced accuracy since there's slight imbalance in the data, no-information rate is around 0.52."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import balanced_accuracy_score\nscorer_class = make_scorer(balanced_accuracy_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we're ready to fit the models"},{"metadata":{"trusted":true},"cell_type":"code","source":"class_perf = pd.DataFrame(columns=['Model','Acc', 'SD'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classifiers 1 - ngram"},{"metadata":{},"cell_type":"markdown","source":"### Naive Bayes"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nNB = MultinomialNB()\n\nnb_param = {'alpha': list(np.arange(0,1,0.01))}\nsearch_nb = GridSearchCV(estimator=NB,\n                          param_grid = nb_param,\n                          scoring = scorer_class,\n                          cv = tscv,\n                          n_jobs=4,\n                          verbose=2\n                         )\nsearch_nb.fit(X=n_grams_train, y=y_train_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_c1 = search_nb.best_estimator_\n\n#get cv results of the best model + confidence intervals\nfrom sklearn.model_selection import cross_val_score\ncv_score = cross_val_score(nb_c1, n_grams_train, y_train_dir, cv=tscv, scoring=scorer_class)\nclass_perf = class_perf.append({'Model':'NB_c1', 'Acc':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)\nnb_c1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic regression"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(class_weight='balanced') #using l2\n\nlr_param = {'C': list(np.arange(0.1,1,0.1))}\nsearch_lr = GridSearchCV(estimator=lr,\n                          param_grid = lr_param,\n                          scoring = scorer_class,\n                          cv = tscv,\n                          n_jobs=4,\n                          verbose=2\n                         )\nsearch_lr.fit(n_grams_train, y_train_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_c1 = search_lr.best_estimator_\n\n#get cv results of the best model + confidence intervals\nfrom sklearn.model_selection import cross_val_score\ncv_score = cross_val_score(lr_c1, n_grams_train, y_train_dir, cv=tscv, scoring=scorer_class)\nclass_perf = class_perf.append({'Model':'Logistic Regression_c1', 'Acc':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)\nlr_c1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classifiers 2\n\n### Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nnb_c2 = GaussianNB()\n\nnb_c2.fit(X=X_train, y=y_train_dir)\n\n#get cv results of the best model + confidence intervals\nfrom sklearn.model_selection import cross_val_score\ncv_score = cross_val_score(nb_c2, X_train, y_train_dir, cv=tscv, scoring=scorer_class)\nclass_perf = class_perf.append({'Model':'NB_c2', 'Acc':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)\nnb_c2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic regression"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"lr = LogisticRegression(class_weight='balanced') #using l2\n\nlr_param = {'C': list(np.arange(0.1,1,0.1))}\nsearch_lr = GridSearchCV(estimator=lr,\n                          param_grid = lr_param,\n                          scoring = scorer_class,\n                          cv = tscv,\n                          n_jobs=4,\n                          verbose=2\n                         )\nsearch_lr.fit(X_train, y_train_dir)\n\nlr_c2 = search_lr.best_estimator_\n\n#get cv results of the best model + confidence intervals\nfrom sklearn.model_selection import cross_val_score\ncv_score = cross_val_score(lr_c2, X_train, y_train_dir, cv=tscv, scoring=scorer_class)\nclass_perf = class_perf.append({'Model':'Logistic Regression_c2', 'Acc':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)\nlr_c2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_c2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stack the classifiers with logistic regression\n\nFirst we evaluate all the classifiers and decide which to use for stacking."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(class_perf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both c2 models seem to be the most promissing, so we'll stack just these 2."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_predict\nstack = LogisticRegression(class_weight='balanced')\n\nstack_train = pd.DataFrame(pd.DataFrame(columns=['nb_c2', 'lr_c2']))\nstack_train['nb_c2'] = nb_c2.predict(X_train)\nstack_train['lr_c2'] = lr_c2.predict(X_train)\n\nstack_test = pd.DataFrame(pd.DataFrame(columns=['nb_c2', 'lr_c2']))\nstack_test['nb_c2'] = nb_c2.predict(X_test)\nstack_test['lr_c2'] = lr_c2.predict(X_test)\n\n\nstack.fit(stack_train, y_train_dir)\n\npred_dir_train = cross_val_predict(stack, stack_train, y_train_dir)\npred_dir_test = stack.predict(stack_test)\n\n#add the stack to the classifier performance table\ncv_score = cross_val_score(stack, stack_train, y_train_dir, cv=tscv, scoring=scorer_class)\nclass_perf = class_perf.append({'Model':'Stack', 'Acc':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_perf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add these direction prediction to the data for NLP models"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"X_train_n.loc[:,'Direction'] = pred_dir_train\nX_test_n.loc[:,'Direction'] = pred_dir_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The cost function to minimize is mean squared error because this function assigns cost proportionally to the error size. The mean absolute percentage error will be used for plotting and easier interpretation. It's much easier to understand the errors of a model in terms of percentage.\nEach training set is scaled (normalized) independently to minimize data leakage."},{"metadata":{"trusted":true},"cell_type":"code","source":"def mape(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\nscorer = make_scorer(mean_squared_error)\nscaler = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Function for plotting coeficients of models (lasso and XGBoost)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotCoef(model,train_x):\n    \"\"\"\n        Plots sorted coefficient values of the model\n    \"\"\"\n    \n    coefs = pd.DataFrame(model.coef_, train_x.columns)\n    coefs.columns = [\"coef\"]\n    coefs[\"abs\"] = coefs.coef.apply(np.abs)\n    coefs = coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\n    \n    plt.figure(figsize=(15, 7))\n    coefs.coef.plot(kind='bar')\n    plt.grid(True, axis='y')\n    plt.hlines(y=0, xmin=0, xmax=len(coefs), linestyles='dashed');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8.1 Econometric models\nFirst let's train models using only the lags of the y variable (i.e. diff)."},{"metadata":{"trusted":true},"cell_type":"code","source":"econ_perf = pd.DataFrame(columns=['Model','MSE', 'SD'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ridge regression"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"ridge_param = {'model__alpha': list(np.arange(1,10,0.1))}\nridge = Ridge(max_iter=5000)\npipe = Pipeline([\n    ('scale', scaler),\n    ('model', ridge)])\nsearch_ridge = GridSearchCV(estimator=pipe,\n                          param_grid = ridge_param,\n                          scoring = scorer,\n                          cv = tscv,\n                          n_jobs=4,\n                          verbose=2\n                         )\nsearch_ridge.fit(X_train_e, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_e = search_ridge.best_estimator_\n\n#get cv results of the best model + confidence intervals\nfrom sklearn.model_selection import cross_val_score\ncv_score = cross_val_score(ridge_e, X_train_e, y_train, cv=tscv, scoring=scorer)\necon_perf = econ_perf.append({'Model':'Ridge', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)\nridge_e","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotCoef(ridge_e['model'], X_train_e)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coefs = ridge_e['model'].coef_\nridge_coefs = pd.DataFrame({'Coef': coefs,\n                           'Name': list(X_train_e.columns)})\nridge_coefs[\"abs\"] = ridge_coefs.Coef.apply(np.abs)\nridge_coefs = ridge_coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\nridge_coefs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"rf_param = {'model__n_estimators': [10, 100, 300],\n            'model__max_depth': [10, 20, 30, 40],\n            'model__min_samples_split': [2, 5, 10],\n            'model__min_samples_leaf': [1, 2, 3],\n            'model__max_features': [\"auto\", 'sqrt']}\n\nrf = RandomForestRegressor()\npipe = Pipeline([\n    ('scale', scaler),\n    ('model', rf)])\ngridsearch_rf = GridSearchCV(estimator=pipe,\n                          param_grid = rf_param,\n                          scoring = scorer,\n                          cv = tscv,\n                          n_jobs=4,\n                          verbose=3\n                         )\ngridsearch_rf.fit(X_train_e, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_e = gridsearch_rf.best_estimator_\n\n#get cv results of the best model + confidence intervals\ncv_score = cross_val_score(rf_e, X_train_e, y_train, cv=tscv, scoring=scorer)\necon_perf = econ_perf.append({'Model':'RF', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"xgb_param = {'model__lambda': list(np.arange(0.1,3, 0.1)), #L2 regularisation\n             'model__alpha': list(np.arange(0.1,3, 0.1)),  #L1 regularisation\n            }\n\nxgb = XGBRegressor(booster='gblinear', feature_selector='shuffle', objective='reg:squarederror')\n\npipe = Pipeline([\n    ('scale', scaler),\n    ('model', xgb)])\ngridsearch_xgb = GridSearchCV(estimator=pipe,\n                          param_grid = xgb_param,\n                          scoring = scorer,\n                          cv = tscv,\n                          n_jobs=4,\n                          verbose=3\n                         )\ngridsearch_xgb.fit(X_train_e, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_e = gridsearch_xgb.best_estimator_\n\n#get cv results of the best model + confidence intervals\ncv_score = cross_val_score(xgb_e, X_train_e, y_train, cv=tscv, scoring=scorer)\necon_perf = econ_perf.append({'Model':'XGB', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)\nxgb_e","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotCoef(xgb_e['model'], X_train_e)\n\ncoefs = xgb_e['model'].coef_\nxgb_coefs = pd.DataFrame({'Coef': coefs,\n                           'Name': list(X_train_e.columns)})\nxgb_coefs[\"abs\"] = xgb_coefs.Coef.apply(np.abs)\nxgb_coefs = xgb_coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\nxgb_coefs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(econ_perf)\necon_fig = px.scatter(econ_perf, x=\"Model\", y='MSE', color='Model', error_y=\"SD\")\necon_fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## NLP models\nLet's try now predict the stock value using only information from the news headlines."},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp_perf = pd.DataFrame(columns=['Model','MSE', 'SD'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ridge"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"ridge_param = {'model__alpha': list(np.arange(1,10,0.1))}\nridge = Ridge(max_iter=5000)\npipe = Pipeline([\n    ('scale', scaler),\n    ('model', ridge)\n])\nsearch_ridge = GridSearchCV(estimator=pipe,\n                          param_grid = ridge_param,\n                          scoring = scorer,\n                          cv = tscv,\n                          n_jobs=4\n                         )\nsearch_ridge.fit(X_train_n, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_n = search_ridge.best_estimator_\n\n#get cv results of the best model + confidence intervals\ncv_score = cross_val_score(ridge_n, X_train_n, y_train, cv=tscv, scoring=scorer)\nnlp_perf = nlp_perf.append({'Model':'Ridge', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)\nridge_n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotCoef(ridge_n['model'], X_train_n)\n\ncoefs = ridge_n['model'].coef_\nridge_coefs = pd.DataFrame({'Coef': coefs,\n                           'Name': list(X_train_n.columns)})\nridge_coefs[\"abs\"] = ridge_coefs.Coef.apply(np.abs)\nridge_coefs = ridge_coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\nridge_coefs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"rf_param = {'model__n_estimators': [10, 100, 300],\n            'model__max_depth': [10, 20, 30, 40],\n            'model__min_samples_split': [2, 5, 10],\n            'model__min_samples_leaf': [1, 2, 3],\n            'model__max_features': [\"auto\", 'sqrt']}\nrf = RandomForestRegressor()\npipe = Pipeline([\n    ('scale', scaler),\n    ('model', rf)])\ngridsearch_rf = GridSearchCV(estimator=pipe,\n                          param_grid = rf_param,\n                          scoring = scorer,\n                          cv = tscv,\n                          n_jobs=4,\n                          verbose=3\n                         )\ngridsearch_rf.fit(X_train_n, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_n = gridsearch_rf.best_estimator_\n\n#get cv results of the best model + confidence intervals\ncv_score = cross_val_score(rf_n, X_train_n, y_train, cv=tscv, scoring=scorer)\nnlp_perf = nlp_perf.append({'Model':'RF', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"xgb_param = {'model__lambda': list(np.arange(1,10, 1)), #L2 regularisation\n             'model__alpha': list(np.arange(1,10, 1)),  #L1 regularisation\n            }\nxgb = XGBRegressor(booster='gblinear', feature_selector='shuffle', objective='reg:squarederror')\n\npipe = Pipeline([\n    ('scale', scaler),\n    ('model', xgb)])\ngridsearch_xgb = GridSearchCV(estimator=pipe,\n                          param_grid = xgb_param,\n                          scoring = scorer,\n                          cv = tscv,\n                          n_jobs=4,\n                          verbose=3\n                         )\ngridsearch_xgb.fit(X_train_n, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_n = gridsearch_xgb.best_estimator_\n\n#get cv results of the best model + confidence intervals\ncv_score = cross_val_score(xgb_n, X_train_n, y_train, cv=tscv, scoring=scorer)\nnlp_perf = nlp_perf.append({'Model':'XGB', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)\nxgb_n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotCoef(xgb_n['model'], X_train_n)\n\ncoefs = xgb_n['model'].coef_\nxgb_coefs = pd.DataFrame({'Coef': coefs,\n                           'Name': list(X_train_n.columns)})\nxgb_coefs[\"abs\"] = xgb_coefs.Coef.apply(np.abs)\nxgb_coefs = xgb_coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\nxgb_coefs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(nlp_perf)\n\nnlp_fig = px.scatter(nlp_perf, x=\"Model\", y='MSE', color='Model', error_y=\"SD\")\nnlp_fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Save predictions to compare the performance and interpretation"},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_compare = pd.DataFrame(pd.DataFrame(columns=['y_true', 'econ_r', 'econ_rf', 'econ_x', 'nlp_r', 'nlp_rf', 'nlp_x']))\nprediction_compare['y_true'] = y_test\nprediction_compare['econ_r'] = ridge_e.predict(X_test_e)\nprediction_compare['econ_rf'] = rf_e.predict(X_test_e)\nprediction_compare['econ_x'] = xgb_e.predict(X_test_e)\nprediction_compare['nlp_r'] = ridge_n.predict(X_test_n)\nprediction_compare['nlp_rf'] = rf_n.predict(X_test_n)\nprediction_compare['nlp_x'] = xgb_n.predict(X_test_n)\n\nprediction_compare.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"class_perf.to_csv('class_perf.csv')\necon_perf.to_csv(\"econ_perf.csv\")\nnlp_perf.to_csv(\"nlp_perf.csv\")\nprediction_compare.to_csv('compare_predictions.csv')\n\nX_test = pd.DataFrame(data=X_test[1:,1:], \n                      index=X_test[1:,0],\n                      columns=X_test[0,1:])\nX_test.to_csv(\"X_test.csv\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}