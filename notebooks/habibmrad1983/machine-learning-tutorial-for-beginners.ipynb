{"cells":[{"metadata":{"_uuid":"819c6b57207e720308ea29491d85a6e8af8c253e","_cell_guid":"dcc6c619-c02a-4bea-b322-4f32779ae672"},"cell_type":"markdown","source":"# DATA SCIENTIST\n**In this tutorial, I only explain you what you need to be a data scientist neither more nor less.**\n\nData scientist need to have these skills:\n1. Basic Tools: Like python, R or SQL. You do not need to know everything. What you only need is to learn how to use **python**\n1. Basic Statistics: Like mean, median or standart deviation. If you know basic statistics, you can use **python** easily. \n1. Data Munging: Working with messy and difficult data. Like a inconsistent date and string formatting. As you guess, **python** helps us.\n1. Data Visualization: Title is actually explanatory. We will visualize the data with **python** like matplot and seaborn libraries.\n1. Machine Learning: You do not need to understand math behind the machine learning technique. You only need is understanding basics of machine learning and learning how to implement it while using **python**.\n\n### As a summary we will learn python to be data scientist !!!\n## For parts 1, 2, 3, 4, 5 and 6, look at DATA SCIENCE TUTORIAL for BEGINNERS\nhttps://www.kaggle.com/kanncaa1/data-sciencetutorial-for-beginners/\n## In this tutorial, I am not going to learn machine learning to you, I am going to explain how to learn something by yourself.\n# *Confucius: Give a man a fish, and you feed him for a day. Teach a man to fish, and you feed him for a lifetime*\n**Content:**\n1. Introduction to Python:\n    1. Matplotlib\n    1. Dictionaries \n    1. Pandas\n    1. Logic, control flow and filtering\n    1. Loop data structures\n1. Python Data Science Toolbox:\n    1. User defined function \n    1. Scope\n    1. Nested function\n    1. Default and flexible arguments\n    1. Lambda function\n    1. Anonymous function\n    1. Iterators\n    1. List comprehension\n1. Cleaning Data\n    1. Diagnose data for cleaning\n    1. Explotary data analysis\n    1. Visual exploratory data analysis\n    1. Tidy data\n    1. Pivoting data\n    1. Concatenating data\n    1. Data types\n    1. Missing data and testing with assert\n1. Pandas Foundation\n    1. Review of pandas\n    1. Building data frames from scratch\n    1. Visual exploratory data analysis\n    1. Statistical explatory data analysis\n    1. Indexing pandas time series\n    1. Resampling pandas time series\n1. Manipulating Data Frames with Pandas\n    1. Indexing data frames\n    1. Slicing data frames\n    1. Filtering data frames\n    1. Transforming data frames\n    1. Index objects and labeled data\n    1. Hierarchical indexing\n    1. Pivoting data frames\n    1. Stacking and unstacking data frames\n    1. Melting data frames\n    1. Categoricals and groupby\n1. Data Visualization\n    1. Seaborn: https://www.kaggle.com/kanncaa1/seaborn-for-beginners\n    1. Bokeh: https://www.kaggle.com/kanncaa1/interactive-bokeh-tutorial-part-1\n    1. Bokeh: https://www.kaggle.com/kanncaa1/interactive-bokeh-tutorial-part-2\n1. Statistical Thinking\n    1. https://www.kaggle.com/kanncaa1/basic-statistic-tutorial-for-beginners\n1. [Machine Learning](#1)\n    1. [Supervised Learning](#2)\n        1. [EDA(Exploratory Data Analysis)](#3)\n        1. [K-Nearest Neighbors (KNN)](#4)\n        1. [Regression](#5)\n        1. [Cross Validation (CV)](#6)\n        1. [ROC Curve](#7)\n        1. [Hyperparameter Tuning](#8)\n        1. [Pre-procesing Data](#9)\n    1. [Unsupervised Learning](#10)\n        1. [Kmeans Clustering](#11)\n        1. [Evaluation of Clustering](#12)\n        1. [Standardization](#13)\n        1. [Hierachy](#14)\n        1. [T - Distributed Stochastic Neighbor Embedding (T - SNE)](#15)\n        1. [Principle Component Analysis (PCA)](#16)\n1. Deep Learning\n    1. https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners\n1. Time Series Prediction\n    1. https://www.kaggle.com/kanncaa1/time-series-prediction-tutorial-with-eda\n1. Deep Learning with Pytorch\n    1. Artificial Neural Network: https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers\n    1. Convolutional Neural Network: https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers\n    1. Recurrent Neural Network: https://www.kaggle.com/kanncaa1/recurrent-neural-network-with-pytorch","execution_count":null},{"metadata":{"_uuid":"2b90d6250c8f9c2c302c849bffa132bd3483e893","_cell_guid":"5ee3a7aa-eca4-411b-9f84-d14c09e13730","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n# import warnings\nimport warnings\n\n# ignore warnings\nwarnings.filterwarnings(\"ignore\")\n# Use warnings. filterwarnings() to ignore deprecation warnings Call warnings. filterwarnings(action, category=DeprecationWarning) with action as \"ignore\" \n# and category set to DeprecationWarning to ignore any deprecation warnings that may rise. \n# Leave category unset to ignore all warnings\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9c5426e9e5cef81c1e1639ebe57e9b45dfd2c43","_cell_guid":"32af03f6-41be-41ec-9023-8cd519040984","trusted":true},"cell_type":"code","source":"# read csv (comma separated value) into data\ndata = pd.read_csv('../input/column_2C_weka.csv')\nprint(plt.style.available) # look at available plot styles\nplt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4fa42e9a6cf069d54459be42a1726ab03c2f1e5","_cell_guid":"89a724e2-426d-427b-9107-06835010cf59"},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n# 8. MACHINE LEARNING (ML)\nIn python there are some **ML libraries like sklearn, keras or tensorflow**. We will use sklearn.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"https://www.tutorialspoint.com/machine_learning_with_python/index.htm","execution_count":null},{"metadata":{"_uuid":"f6a2b205e1e3fc647bc2ee88d702b8572bf1cc75","_cell_guid":"10f3b719-c44b-451a-b464-0adf4e1e1522"},"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n## A. SUPERVISED LEARNING\n* **Supervised learning**: **It uses data that has labels**. Example, there are orthopedic patients data that have labels *normal* and *abnormal*.\n    * There are **features (predictor variable) and target variable**. Features are like *pelvic radius* or *sacral slope*(If you have no idea what these are like me, you can look images in google like what I did :) )Target variables are labels *normal* and *abnormal*\n    * Aim is that as given features (input) predict whether target variable (output) is *normal* or *abnormal*\n    * **Classification**: *target variable consists of categories like normal or abnormal*\n    * **Regression**: *target variable is continious like stock market*\n    \nIf these explanations are not enough for you, just google them. However, be careful about terminology: \n1.     *features = predictor variable = independent variable = columns = inputs*. \n1.     *target variable = responce variable = class = dependent variable = output = result*","execution_count":null},{"metadata":{"_uuid":"a0e671bf2ef8dbe81da2705ad70b69401bb7af16","_cell_guid":"65e897a1-8259-44c5-9cb7-e5e653f9032d"},"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n### EXPLORATORY DATA ANALYSIS (EDA)\n* In order to make something in data, as you know you need to **explore data**. Detailed exploratory data analysis is in my Data Science Tutorial for Beginners\n* I always start with *head()* to see features that are *pelvic_incidence,\tpelvic_tilt numeric,\tlumbar_lordosis_angle,\tsacral_slope,\tpelvic_radius* and \t*degree_spondylolisthesis* and target variable (dependent variable - output) that is *class*\n* **head()**: default value of it shows first 5 rows (samples). If you want to see for example 100 rows just write head(100)\n","execution_count":null},{"metadata":{"_uuid":"9a5993f4962882e1156f2062b7abf706a0739d51","_cell_guid":"c1ecd622-67cc-485f-bfa7-8c682d30a5eb","trusted":true},"cell_type":"code","source":"# to see features and target variable\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(data.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.index","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b7b9addc824de1a35b67d96d3092ffcb10869947","_cell_guid":"1631690c-bb9d-4460-a7d9-a335aa914b4f","trusted":true},"cell_type":"code","source":"# Well know question is is there any NaN value and length of this data so lets look at info\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96f97c33305956eb76d8a2043fd71aff05e38548","_cell_guid":"a7dd2a6f-a81d-4dce-9d74-fd0148c446ae"},"cell_type":"markdown","source":"As you can see:\n* length: 310 (range index)\n* Features are float\n* Target variables are object that is like string\n* Okey we have some ideas about data but lets look go inside data deeper\n    * **describe**(): I explain it in previous tutorial so there is a Quiz :)\n        * Why we need to see statistics like mean, std, max or min? I hate from quizzes :) so answer: In order to visualize data, values should be closer each other. As you can see values looks like closer. At least there is no incompatible values like mean of one feature is 0.1 and other is 1000. Also there are another reasons that I will mention next parts.","execution_count":null},{"metadata":{"_uuid":"c8961b1f3a3a73547a0b7d27955f9844f6ad43eb","_cell_guid":"3776dd3d-d0aa-419e-b788-e75454e94b86"},"cell_type":"markdown","source":"pd.plotting.scatter_matrix:\n* green: *normal* and red: *abnormal*\n* **c**:  color\n* **figsize**: figure size\n* **diagonal**: histohram of each features\n* **alpha**: opacity\n* **s**: size of marker\n* **marker**: marker type ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"color_list = ['red' if i=='Abnormal' else 'green' for i in data.loc[:,'class']]\ncolor_list","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5dc0763dde8b2638a5289f0b4496f384f85aca85","_cell_guid":"fb106765-bb47-452b-8d6e-3578b479873c","trusted":true},"cell_type":"code","source":"color_list = ['red' if i=='Abnormal' else 'green' for i in data.loc[:,'class']]\npd.plotting.scatter_matrix(data.loc[:, data.columns != 'class'],\n                                       c=color_list,\n                                       figsize= [15,15],\n                                       diagonal='hist',\n                                       alpha=0.5,\n                                       s = 200,\n                                       marker = '*',\n                                       edgecolor= \"black\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d77cef37b8c5c4f07d3f4aa94cc4ad1ccbd7caca","_cell_guid":"53fc7ab6-de8b-4b8f-9e4a-38c5db72eea0"},"cell_type":"markdown","source":"Okay, as you understand in scatter matrix there are relations between each feature but how many *normal(green)* and *abnormal(red)* classes are there. \n* **Searborn library has *countplot()* that counts number of classes**\n* Also you can print it with *value_counts()* method\n\n<br> This data looks like balanced. Actually there is no definiton or numeric value of balanced data but this data is balanced enough for us.\n<br> Now lets learn first **classification method** : **KNN (K-NEAREST NEIGHBORS)** ","execution_count":null},{"metadata":{"_uuid":"e1bb9fd338e6900888e2e4717f54be46cee848a2","_cell_guid":"36243fa5-1fa6-4f8b-bc16-43449b0dc898","trusted":true},"cell_type":"code","source":"sns.countplot(x=\"class\", data=data)\ndata.loc[:,'class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9263c479815bb729dad40bf01b68aa18a3c946ac","_cell_guid":"24a5d90f-3e7d-4733-a6f9-ff4f51145155"},"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n###  K-NEAREST NEIGHBORS (KNN)\n* **KNN**: Look at the K closest labeled data points\n* **Classification method**.\n* First we need to train our data. **Train = fit**\n* **fit**(): *fits the data, train the data*.\n* **predict()**: predicts the data\n<br> If you do not understand what is KNN, look at youtube there are videos like 4-5 minutes. You can understand better with it.\n<br> Lets learn how to implement it with sklearn\n* **x**: features\n* **y**: target variables(normal, abnormal)\n* **n_neighbors**: K. In this example it is 3. it means that Look at the 3 closest labeled data points\n","execution_count":null},{"metadata":{"_uuid":"854f0a3898a928640b9714fcd584e48c9b377f9f","_cell_guid":"c717491d-2bd5-4dc7-ac13-b9f581b1cddd","trusted":true},"cell_type":"code","source":"# KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)  # k = 3 ==> Look at the 3 closest labeled data points\nx,y = data.loc[:,data.columns != 'class'], data.loc[:,'class']\nknn.fit(x,y)\nprediction = knn.predict(x)\nprint('Prediction: {}'.format(prediction))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d9c3eacd279ddf7c0ef33b9e1814cd549d0feaa","_cell_guid":"b5d85f4e-ab30-4c49-b2bc-6265b6baea9d"},"cell_type":"markdown","source":"* Well, we fit the data and predict it with KNN. \n* So, do we predict correct or what is our accuracy or the accuracy is best metric to evaluate our result? Lets give answer of this questions\n<br> Measuring model performance:\n* **Accuracy which is fraction of correct predictions** is commonly used metric. We will use it know but there is another problem\n\n<br>As you see I train data with x (features) and again predict the x(features). Yes you are reading right but yes you are right again it is absurd :)\n\n<br>Therefore **we need to split our data train and test sets**.\n* **train**: *use train set by fitting*\n* **test**: *make prediction on test set*.\n* **With train and test sets, fitted data and tested data are completely different**\n* train_test_split(x,y,test_size = 0.3,random_state = 1)\n    * x: features\n    * y: target variables (normal,abnormal)\n    * test_size: percentage of test size. Example test_size = 0.3, test size = 30% and train size = 70%\n    * random_state: sets a seed. If this seed is same number, train_test_split() produce exact same split at each time\n* **fit(x_train,y_train): fit on train sets**\n* **score(x_test,y_test)): predict and give accuracy on test sets**","execution_count":null},{"metadata":{"_uuid":"4702429fdfa62650937b09fde5a8fd3136da8c55","_cell_guid":"79865658-c89f-43c8-a1a9-75acc4feab6a","trusted":true},"cell_type":"code","source":"# train test split\nfrom sklearn.model_selection import train_test_split\nx,y = data.loc[:,data.columns != 'class'], data.loc[:,'class']\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1)\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\nprint('Prediction: {}'.format(prediction))\nprint('With KNN (K=3) accuracy is: ',knn.score(x_test,y_test)) # accuracy","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"544f51ef05efe0b3ae4b02da806778bcfa715f35","_cell_guid":"a5665258-3f7f-435a-a634-49eb0c0d66e0"},"cell_type":"markdown","source":"Accuracy is 86% so is it good ? I do not know actually, we will see at the end of tutorial.\n<br> Now the question is why we choose K = 3 or what value we need to choose K. The answer is in model complexity\n\n<br> ***Model complexity***:\n* K has general name. It is called a **hyperparameter**. For now just know K is hyperparameter and we need to choose it that gives best performace. \n* Literature says **if k is small, model is complex model and can lead to overfit. It means that model memorizes the train sets and cannot predict test set with good accuracy.**\n* **If k is big, model that is less complex model can lead to underfit**. \n* At below, I range K value from 1 to 25 (exclude) and find accuracy for each K value. As you can see in plot, when K is 1 it memorize the training sets and cannot give good accuracy on test set (overfit) - and dont Generalize to new unseen data. Also if K is 18, model (simple one) is lead to underfit . Again accuracy is not enough. However look at when K is 18 (best performance), accuracy has highest value almost 88%. \n\n","execution_count":null},{"metadata":{"_uuid":"18d8739373085a9964071f38b8f2adcb64f25491","_cell_guid":"db2c7062-ce1b-4e8b-9b2f-0ee0cd679a91","trusted":true},"cell_type":"code","source":"# Model complexity\n\nneig = np.arange(1, 25)\n# Numpy arange() method returns the ndarray object containing evenly spaced values within the given range. \n# The np. arange() function returns an evenly spaced values within a given interval. \n# For integer arguments, the method is equivalent to a Python inbuilt range function but returns the ndarray rather than a list\n\ntrain_accuracy = []\ntest_accuracy = []\n# Loop over different values of k\nfor i, k in enumerate(neig):\n    # k from 1 to 25(exclude)\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # Fit with knn\n    knn.fit(x_train,y_train)\n    #train accuracy\n    train_accuracy.append(knn.score(x_train, y_train))\n    # test accuracy\n    test_accuracy.append(knn.score(x_test, y_test))\n\n# Plot\nplt.figure(figsize=[13,8])\nplt.plot(neig, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neig, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.title('-value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neig)\nplt.savefig('graph.png')\nplt.show()\nprint(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neig","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(enumerate(neig))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96423b4f710966c8071647874623c139c1c79bb7","_cell_guid":"b598ee81-e535-49c0-b53c-b13b0a5058db"},"cell_type":"markdown","source":"### Up to this point what you learn:\n* Supervised learning\n* Exploratory data analysis\n* KNN\n    * How to split data\n    * How to fit, predict data\n    * How to measure model performance (accuracy)\n    * How to choose hyperparameter (K)\n    \n**<br> What happens if I change the title KNN and make it some other classification technique like Random Forest?**\n* The answer is **nothing**. What you need to is just watch a video about what is random forest in youtube and implement what you learn in KNN. Because the idea and even most of the codes (only KNeighborsClassifier need to be **RandomForestClassifier** ) are same. You need to split, fit, predict your data and measue performance and choose **hyperparameter of random forest(like max_depth)**. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n# train test split\nfrom sklearn.model_selection import train_test_split\nx,y = data.loc[:,data.columns != 'class'], data.loc[:,'class']\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1)\nmodel = RandomForestClassifier()\nmodel.fit(x_train,y_train)\nprediction = model.predict(x_test)\nprint('Prediction: {}'.format(prediction))\nprint('With RFC  accuracy is: ',model.score(x_test,y_test)) # accuracy","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d075fd2a7c05e5414e33b7b1314d81a6b945e7b3","_cell_guid":"f9d427a9-faa5-46cf-9e3c-2c8cea2571ad"},"cell_type":"markdown","source":"<a id=\"5\"></a> <br>\n### REGRESSION\n* Supervised learning\n* We will learn linear and logistic regressions\n* This orthopedic patients data is not proper for regression so I only use two features that are *sacral_slope* and *pelvic_incidence* of abnormal \n    * I  consider features is pelvic_incidence and target is sacral_slope \n    * Lets look at scatter plot so as to understand it better\n    * reshape(-1,1): If you do not use it shape of x or y becomes (210,) and we cannot use it in sklearn, so we use shape(-1,1) and shape of x or y be (210, 1). \n    * If you have an array of shape (2,4) then reshaping it with (-1, 1), then the array will get reshaped in such a way that the resulting array has only 1 column and this is only possible by having 8 rows, hence, (8,1)  ","execution_count":null},{"metadata":{"_uuid":"d2655c140423b1228c42d2e8dfe54344ba43dcb0","_cell_guid":"6b072c42-059f-4e45-9cfa-8ed39b274f72","trusted":true},"cell_type":"code","source":"# create data1 that includes pelvic_incidence that is feature and sacral_slope that is target variable\ndata1 = data[data['class'] =='Abnormal']  # why class == Abnormal????\nx = np.array(data1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(data1.loc[:,'sacral_slope']).reshape(-1,1)\n# Scatter\nplt.figure(figsize=[10,10])\nplt.scatter(x=x,y=y)\nplt.xlabel('pelvic_incidence')\nplt.ylabel('sacral_slope')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c84719d363ff736e96a0a575dfd0381bcbc549fb","_cell_guid":"874ac5e0-5bc0-4429-b6c5-707690b5dd77"},"cell_type":"markdown","source":"Now we have our data to make regression. **In regression problems target value is continuously varying variable such as price of house or sacral_slope**. Lets fit line into this points.\n\n<br> Linear regression\n* y = ax + b       where  y = target, x = feature and a = parameter of model\n* **We choose parameter of model(a) according to minimum error function that is loss function**\n* **In linear regression we use Ordinary Least Square (OLS) as loss function.**\n* **OLS: sum all residuals but some positive and negative residuals can cancel each other so we sum of square of residuals. It is called OLS**\n* **Score: Score uses R^2 method that is ((y_pred - y_mean)^2 )/(y_actual - y_mean)^2**","execution_count":null},{"metadata":{"_uuid":"7cdc74efa8c46dab5f14f6cc2779928c11a4fa62","_cell_guid":"fb7991f3-5869-4df0-bf6c-30f61e8215c6","trusted":true},"cell_type":"code","source":"# LinearRegression\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression()\n\n# Predict space\npredict_space = np.linspace(min(x), max(x)).reshape(-1,1)  \n# linspace Returns evenly spaced numbers over a specified interval\n\n# Fit\nreg.fit(x,y)\n\n# Predict\npredicted = reg.predict(predict_space)\n# R^2 \nprint('R^2 score: ',reg.score(x, y))\n\n# Plot regression line and scatter\nplt.plot(predict_space, predicted, color='black', linewidth=3)\nplt.scatter(x=x,y=y)\nplt.xlabel('pelvic_incidence')\nplt.ylabel('sacral_slope')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_space","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3cf254d2feef8f9f886b0e1de12ec2b155083d2b","_cell_guid":"242dd057-bb8a-463a-b04c-1072d5de1e0a"},"cell_type":"markdown","source":"<a id=\"6\"></a> <br>\n### CROSS VALIDATION\nAs you know in KNN method we use train test split with random_state that split exactly same at each time. However, if we do not use random_state, data is split differently at each time and according to split accuracy will be different. Therefore, we can conclude that **model performance is dependent on train_test_split**. For example you split, fit and predict data 5 times and accuracies are 0.89, 0.9, 0.91, 0.92 and 0.93, respectively. Which accuracy do you use? Do you know what accuracy will be at 6th times split, train and predict. The answer is I do not know but **if I use cross validation I can find acceptable accuracy**.\n<br> Cross Validation (CV)\n* **K-folds = K folds CV.**\n* Look at this image it defines better than me :)\n* When K is increase, computationally cost is increase\n* cross_val_score(reg,x,y,cv=5): use reg(linear regression) with x and y that we define at above and K is 5. It means 5 times(split, train,predict)\n\nhttps://docs.aws.amazon.com/machine-learning/latest/dg/cross-validation.html\n\nhttps://towardsdatascience.com/cross-validation-in-machine-learning-72924a69872f\n","execution_count":null},{"metadata":{"_uuid":"1f1a77c5d5e6ca52c0264875362665f228e66078","_cell_guid":"bf504792-66f4-411e-bc1d-5b049da959ac","trusted":true},"cell_type":"code","source":"# CV\nfrom sklearn.model_selection import cross_val_score\nreg = LinearRegression()\nk = 5\ncv_result = cross_val_score(reg,x,y,cv=k) # uses R^2 as score \nprint('CV Scores: ',cv_result)\nprint('CV scores average: ',np.sum(cv_result)/k)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be425f142c0acc1ab3009fafafa3616948f5c8a5","_cell_guid":"27ffec30-4951-4479-9d86-32495a80c08d"},"cell_type":"markdown","source":"### Regularized Regression\n**As we have learned, linear regression choose parameters (coefficients) while minimizing loss function. If linear regression thinks that one of the feature is important, it gives high coefficient to this feature. However, this can cause overfitting that is like memorizing in KNN. *In order to avoid overfitting, we use regularization that penalize large coefficients*.**\n\nhttps://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a#:~:text=This%20is%20a%20form%20of,avoid%20the%20risk%20of%20overfitting.\n\n\n\n* **Ridge regression: First regularization technique. Also it is called L2 regularization**. \n    * **Ridge regression loss function = OLS + alpha * sum(parameter^2)**\n    * alpha is parameter we need to choose to fit and predict. Picking alpha is similar to picking K in KNN. As you understand alpha is hyperparameter that we need to choose for best accuracy and model complexity. This process is called hyperparameter tuning.\n    * What if alpha is zero? lost function = OLS so that is linear rigression :)\n    * If alpha is small that can cause overfitting\n    * If alpha is big that can cause underfitting. But do not ask what is small and big. These can be change from problem to problem.\n* **Lasso regression: Second regularization technique. Also it is called L1 regularization**. \n    * **Lasso regression lost fuction = OLS + alpha * sum(absolute_value(parameter))**\n    * It can be used to select important features of the data. Because features whose values are not shrinked to zero, is chosen by lasso regression\n    * In order to choose feature, I add new features in our regression data\n    \n<br> **Linear vs Ridge vs Lasso**\n* First impression: Linear\n* Feature Selection: 1.Lasso 2.Ridge\n* Regression model: 1.Ridge 2.Lasso 3.Linear \n\n","execution_count":null},{"metadata":{"_uuid":"85fa872e3988295a8fbe752bf96319518ca3595b","_cell_guid":"bdc1a379-07a4-4b61-8ac1-d7007ae33783","trusted":true},"cell_type":"code","source":"# Ridge\nfrom sklearn.linear_model import Ridge\nx_train,x_test,y_train,y_test = train_test_split(x,y,random_state = 2, test_size = 0.3)\nridge = Ridge(alpha = 0.1, normalize = True)\nridge.fit(x_train,y_train)\nridge_predict = ridge.predict(x_test)\nprint('Ridge score: ',ridge.score(x_test,y_test))\nprint('Ridge coefficients: ',ridge.coef_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57f91f4b4e267bd3eb22adfcdf719778c1901c92","_cell_guid":"de031d9f-b2f8-4fb1-a305-36cbc4dc970f","trusted":true},"cell_type":"code","source":"# Lasso\nfrom sklearn.linear_model import Lasso\nx = np.array(data1.loc[:,['pelvic_incidence','pelvic_tilt numeric','lumbar_lordosis_angle','pelvic_radius']])\nx_train,x_test,y_train,y_test = train_test_split(x,y,random_state = 3, test_size = 0.3)\nlasso = Lasso(alpha = 0.1, normalize = True)\nlasso.fit(x_train,y_train)\nridge_predict = lasso.predict(x_test)\nprint('Lasso score: ',lasso.score(x_test,y_test))\nprint('Lasso coefficients: ',lasso.coef_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d70e2366ccc8797c92d2edacb4ab3b59fad4506d","_cell_guid":"84f9d88d-28e9-4b26-8824-ae32de7e143c"},"cell_type":"markdown","source":"As you can see *pelvic_incidence* and *pelvic_tilt numeric* are important features but others are not important\n\n<br> Now lets discuss accuracy. Is it enough for measurement of model selection. For example, there is a data that includes 95% normal and 5% abnormal samples and our model uses accuracy for measurement metric. *Then our model predict 100% normal for all samples and accuracy is 95% but it classify all abnormal samples wrong*. Therefore **we need to use confusion matrix as a model measurement matrix in imbalance data**.\n<br> While using confusion matrix lets use Random forest classifier to diversify classification methods.\n* tp = true positive(20), fp = false positive(7), fn = false negative(8), tn = true negative(58) \n* **tp = Prediction is positive(normal) and actual is positive(normal). **\n* **fp = Prediction is positive(normal) and actual is negative(abnormal).**\n* **fn = Prediction is negative(abnormal) and actual is positive(normal).**\n* **tn = Prediction is negative(abnormal) and actual is negative(abnormal)**\n\n* **precision = tp / (tp+fp)**\n* **recall = tp / (tp+fn**)\n* f1 = 2 times [precision * recall / ( precision + recall)]\n\n","execution_count":null},{"metadata":{"_uuid":"19fb6fb0f651e249835037a4f4b2f0b4a2619a27","_cell_guid":"c71a33f5-5784-461b-949d-cb83f23dace6","trusted":true},"cell_type":"code","source":"# Confusion matrix with random forest\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nx,y = data.loc[:,data.columns != 'class'], data.loc[:,'class']\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1)\nrf = RandomForestClassifier(random_state = 4)\nrf.fit(x_train,y_train)\ny_pred = rf.predict(x_test)\ncm = confusion_matrix(y_test,y_pred)\nprint('Confusion matrix: \\n',cm)\nprint('Classification report: \\n',classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2697bbc248102687596713406512b4cb7f24929","_cell_guid":"fcba81bb-1257-48d0-afe1-a4416237cc73","trusted":true},"cell_type":"code","source":"# visualize with seaborn library\nsns.heatmap(cm,annot=True,fmt=\"d\") \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dbe175099c0c8151c16ad0c78f1414de8fd9ebdc","_cell_guid":"4552b965-e15d-4e05-9e02-224159e8d508"},"cell_type":"markdown","source":"<a id=\"7\"></a> <br>\n### ROC Curve with Logistic Regression \n* **logistic regression output is probabilities**\n* https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc\n* If probability is higher than 0.5 data is labeled 1(abnormal) else 0(normal)\n* By default logistic regression threshold is 0.5\n* **ROC is Receiver Operating Characteristic**. **In this curve, x axis is false positive rate (x=FPR) and y axis is true positive rate (y=TPR)**\n* **If the curve in plot is closer to left-top corner, test is more accurate**.\n* **Roc curve score is AUC that is computation area under the curve from prediction scores**\n* **We want auc to closer 1**\n* **fpr = False Positive Rate**\n* **tpr = True Positive Rate**\n* If you want, I made ROC, Random forest and K fold CV in this tutorial. https://www.kaggle.com/kanncaa1/roc-curve-with-k-fold-cv/\n* An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters: True Positive Rate. False Positive Rate.\n* https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5","execution_count":null},{"metadata":{"_uuid":"c7fce3a219764388088ec5f3d57ab913f5c05f35","_cell_guid":"4f4a8f76-9792-485f-83b0-79e2524ab83c","trusted":true},"cell_type":"code","source":"# ROC Curve with logistic regression\nfrom sklearn.metrics import roc_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# abnormal = 1 and normal = 0\ndata['class_binary'] = [1 if i == 'Abnormal' else 0 for i in data.loc[:,'class']]\nx,y = data.loc[:,(data.columns != 'class') & (data.columns != 'class_binary')], data.loc[:,'class_binary']\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=42)\nlogreg = LogisticRegression()\nlogreg.fit(x_train,y_train)\ny_pred_prob = logreg.predict_proba(x_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['class_binary'][:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_prob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg.predict_proba(x_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db2a0e5e83aee71a9585ee113cb01170325c96f1","_cell_guid":"ceb21da5-4ced-400c-a017-60f735802b69"},"cell_type":"markdown","source":"<a id=\"8\"></a> <br>\n### HYPERPARAMETER TUNING\nAs I mention at KNN there are hyperparameters that are need to be tuned\n* For example: \n    * k at KNN\n    * alpha at Ridge and Lasso\n    * Random forest parameters like max_depth\n    * linear regression parameters (coefficients)\n* Hyperparameter tuning: \n    * try all of combinations of different parameters\n    * fit all of them\n    * measure prediction performance\n    * see how well each performs\n    * finally choose best hyperparameters\n* This process is most difficult part of this tutorial. Because we will write a lot of for loops to iterate all combinations. Just I am kidding sorry for this :) (We actually did it at KNN part)\n* We only need is one line code that is **GridSearchCV** \n    * grid: K is from 1 to 50(exclude)\n    * **GridSearchCV takes knn and grid and makes grid search. It means combination of all hyperparameters. Here it is k**.\n\nhttps://towardsdatascience.com/grid-search-for-hyperparameter-tuning-9f63945e8fec#:~:text=GridSearchCV%20is%20a%20library%20function,model)%20on%20your%20training%20set.&text=In%20addition%20to%20that%2C%20you,for%20each%20set%20of%20hyperparameters.\n\n    \nGrid search is the process of performing hyper parameter tuning in order to determine the optimal values for a given model. This is significant as the performance of the entire model is based on the hyper parameter values specified.\n\nhttps://medium.com/datadriveninvestor/an-introduction-to-grid-search-ff57adcc0998\n\nhttps://towardsdatascience.com/using-gridsearchcv-76614defc594\n\nhttps://medium.com/@elutins/grid-searching-in-machine-learning-quick-explanation-and-python-implementation-550552200596\n\n","execution_count":null},{"metadata":{"_uuid":"20fb485285b4d27da2e3eb89e8e43797db7c457a","_cell_guid":"d1a56a4e-a307-4376-8d06-53fc30447ce6","trusted":true},"cell_type":"code","source":"# grid search cross validation with 1 hyperparameter\nfrom sklearn.model_selection import GridSearchCV\ngrid = {'n_neighbors': np.arange(1,50)}\nknn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, grid, cv=3) # GridSearchCV\nknn_cv.fit(x,y)# Fit\n\n# Print hyperparameter\nprint(\"Tuned hyperparameter k: {}\".format(knn_cv.best_params_)) \nprint(\"Best score: {}\".format(knn_cv.best_score_))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bdc9c267b635db5696e73cb6ba36d093c8f127c5","_cell_guid":"a4401913-229c-4bcc-8dd0-3563b824f6e9"},"cell_type":"markdown","source":"Other grid search example with 2 hyperparameters\n* *First hyperparameter is C*: **logistic regression regularization parameter**\n    * If C is high: overfit\n    * If C is low: underfit\n* *Second hyperparameter is* **penalty (loss function): L1 (Lasso) or L2(Ridge)** as we learnt at linear regression part.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.logspace(-3, 3, 7)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb5961ae83511410ae491366bd12f70bef9f440b","_cell_guid":"8988b749-3bf5-448f-830e-edce366348d0","trusted":true},"cell_type":"code","source":"# grid search cross validation with 2 hyperparameters\n# 1. hyperparameter is C:logistic regression regularization parameter\n# 2. penalty L1 or L2\n\n# The logspace() function return numbers spaced evenly on a log scale.\n# In linear space, the sequence starts at base ** start (base to the power of start) and ends with base ** stop.\n# https://www.w3resource.com/numpy/array-creation/logspace.php#:~:text=numpy.-,logspace()%20function,%3DTrue%2C%20dtype%3DNone)\n\n# Hyperparameter grid\nparam_grid = {'C': np.logspace(-3, 3, 7), 'penalty': ['l1', 'l2']}\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state = 12)\nlogreg = LogisticRegression()\nlogreg_cv = GridSearchCV(logreg,param_grid,cv=3)\nlogreg_cv.fit(x_train,y_train)\n\n# Print the optimal parameters and best score\nprint(\"Tuned hyperparameters : {}\".format(logreg_cv.best_params_))\nprint(\"Best Accuracy: {}\".format(logreg_cv.best_score_))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7cd0bf0ed62c1858f5de74ee4f85c84765f8bc06","_cell_guid":"37171c8f-21ad-4628-8279-5cfedb27aafb"},"cell_type":"markdown","source":"<a id=\"9\"></a> <br>\n### PRE-PROCESSING DATA\n* In real life data can include **objects** or **categorical data**. In order to use them in sklearn, we need to encode them into numerical data.\n* In data, class is *abnormal* and *normal* (categorial data). Lets convert them into numeric value (actually I did it in logistic regression part with different method)\n* 2 different features are created with the name *class_Abnormal* and *class_Normal*\n* However we need to drop one of the column because they are duplicated","execution_count":null},{"metadata":{"_uuid":"8168cf770134142252e92022307815e31a931a34","_cell_guid":"342e6f67-afeb-42b0-9e56-6521fd15e3ad","trusted":true},"cell_type":"code","source":"# Load data\ndata = pd.read_csv('../input/column_2C_weka.csv')\n# get_dummies // The get_dummies() function is used to convert categorical variable into dummy/indicator variables\n# https://www.w3resource.com/pandas/get_dummies.php#:~:text=The%20get_dummies()%20function%20is,variable%20into%20dummy%2Findicator%20variables.&text=Data%20of%20which%20to%20get%20dummy%20indicators.&text=String%20to%20append%20DataFrame%20column%20names.&text=If%20appending%20prefix%2C%20separator%2Fdelimiter%20to%20use.\ndf = pd.get_dummies(data)\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"032c522e3f423822402c7f75fcfe3818894f68c8","_cell_guid":"e2541cfc-7d99-45ce-8486-80f7fec79257","trusted":true},"cell_type":"code","source":"# drop one of the feature\ndf.drop(\"class_Normal\",axis = 1, inplace = True) \ndf.head(10)\n# instead of two steps we can make it with one step pd.get_dummies(data,drop_first = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5465db779fed984dd89ad591889d1dadb88fc056","_cell_guid":"f55c171e-36d4-4873-a0f6-e8ee411d0b64"},"cell_type":"markdown","source":"**Other preprocessing step is centering, scaling or normalizing** \n* If you listen my advice and watch KNN in youtube, you have noticed that **KNN uses form of distance for classificaiton** like some other methods. Therefore, we need to scale data. For this reason, we use \n    * **standardization**: **( x - x.mean) / x.variance**   or   **x - x.min / x.range**\n* **pipeline**: *The purpose of the pipeline is to assemble several steps like svm (classifier) and  standardization (pre-processing)*\n* How we create parameters name: for example SVM_ _C :  stepName__parameterName\n* Then grid search to find best parameters","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**What does it mean to standardize data?**\n\n> In statistics, standardization is the process of putting different variables on the same scale. This process allows you to compare scores between different types of variables. Typically, to standardize variables, you calculate the mean and standard deviation for a variable. Then, for each observed value of the variable, you subtract the mean and divide by the standard deviation.\n\n> This process produces standard scores that represent the number of standard deviations above or below the mean that a specific observation falls. For instance, a standardized value of 2 indicates that the observation falls 2 standard deviations above the mean. This interpretation is true regardless of the type of variable that you standardize.\n\n**https://statisticsbyjim.com/glossary/standardization/**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* SVM is a supervised machine learning algorithm which can be used for classification or regression problems. It uses a technique called the kernel trick to transform your data and then based on these transformations it finds an optimal boundary between the possible outputs.\n\nhttps://www.kdnuggets.com/2017/02/yhat-support-vector-machine.html\n\nhttps://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/#:~:text=%E2%80%9CSupport%20Vector%20Machine%E2%80%9D%20(SVM,mostly%20used%20in%20classification%20problems.&text=Support%20Vectors%20are%20simply%20the%20co%2Dordinates%20of%20individual%20observation.\n","execution_count":null},{"metadata":{"_uuid":"4690eac9c778ba7230fcee5a782a46396aa8c4ef","_cell_guid":"5d4aef88-f75e-4485-b481-bdeabbc0e4c7","trusted":true},"cell_type":"code","source":"# SVM, pre-process and pipeline\nfrom sklearn.svm import SVC\n\n\n\n# The objective of a Linear SVC (Support Vector Classifier) is to fit to the data you provide, \n# returning a \"best fit\" hyperplane that divides, or categorizes, your data. From there, after getting the hyperplane, \n# you can then feed some features to your classifier to see what the \"predicted\" class is. \n# This makes this specific algorithm rather suitable for our uses, though you can use this for many situations\n\n# https://pythonprogramming.net/linear-svc-example-scikit-learn-svm-python/#:~:text=The%20objective%20of%20a%20Linear,%2C%20or%20categorizes%2C%20your%20data.&text=Other%20than%20the%20visualization%20packages,and%20numpy%20for%20array%20conversion.\n\n# The idea behind StandardScaler is that it will transform your data such that \n# its distribution will have a mean value 0 and standard deviation of 1.\n# In case of multivariate data, this is done feature-wise (in other words independently for each column of the data).\n# Given the distribution of the data, each value in the dataset will have the mean value subtracted, and then divided by the standard deviation of the whole dataset (or feature in the multivariate case).\n\n# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n# https://machinelearningmastery.com/standardscaler-and-minmaxscaler-transforms-in-python/\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nsteps = [('scalar', StandardScaler()),\n         ('SVM', SVC())]\npipeline = Pipeline(steps)\nparameters = {'SVM__C':[1, 10, 100],\n              'SVM__gamma':[0.1, 0.01]}\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state = 1)\ncv = GridSearchCV(pipeline,param_grid=parameters,cv=3)\ncv.fit(x_train,y_train)\n\ny_pred = cv.predict(x_test)\n\nprint(\"Accuracy: {}\".format(cv.score(x_test, y_test)))\nprint(\"Tuned Model Parameters: {}\".format(cv.best_params_))\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"3a8fede9eadd9bd0438f7cb50c0155491d3f776b","_cell_guid":"b343630f-75c8-4362-8cb0-858e71a2f4b0"},"cell_type":"markdown","source":"<a id=\"10\"></a> <br>\n## UNSUPERVISED LEARNING\n* **Unsupervised learning**: **It uses data that has  been unlabeled and uncover hidden patterns from unlabeled data**. Example, there are orthopedic patients data that do not have labels. You do not know which orthopedic patient is normal or abnormal.\n* As you know orthopedic patients data is labeled (supervised) data. It has target variables. In order to work on unsupervised learning, lets drop target variables and to visualize just consider *pelvic_radius* and *degree_spondylolisthesis*\n","execution_count":null},{"metadata":{"_uuid":"f4927a23d072a9416b57f0bd611d71411407a907","_cell_guid":"e492f7d0-5d05-4717-a149-e046d9435607"},"cell_type":"markdown","source":"<a id=\"11\"></a> <br>\n### K-MEANS\n* Lets try our first **unsupervised method **that is **KMeans Cluster**\n* **KMeans Cluster**: **The algorithm works iteratively to assign each data point to one of the K groups based on the features that are provided. Data points are clustered based on feature similarity**\n* KMeans(n_clusters = 2): n_clusters = 2 means that create 2 clusters","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> The k-nearest neighbors algorithm (KNN) is a supervised classification algorithm. It takes a bunch of labeled points and uses them to learn how to label other points\n\n> KNN (The k-nearest neighbors algorithm) represents a supervised classification algorithm that will give new data points accordingly to the k number or the closest data points, while k-means clustering is an unsupervised clustering algorithm that gathers and groups data into k number of clusters.\n\nhttps://becominghuman.ai/comprehending-k-means-and-knn-algorithms-c791be90883d#:~:text=The%20k%2Dnearest%20neighbors%20algorithm,how%20to%20label%20other%20points.&text=It%20is%20supervised%20because%20you,known%20classification%20of%20other%20points.\n\nhttps://pythonprogramminglanguage.com/how-is-the-k-nearest-neighbor-algorithm-different-from-k-means-clustering/#:~:text=KNN%20represents%20a%20supervised%20classification,into%20k%20number%20of%20clusters.","execution_count":null},{"metadata":{"_uuid":"53175af6bce4acd6f743250c240033db04921ad4","_cell_guid":"fa3dce86-b3cd-4a8f-b689-e5bc5f2fe551","trusted":true},"cell_type":"code","source":"# here data is labeled\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv('../input/column_2C_weka.csv')\nplt.scatter(data['pelvic_radius'],data['degree_spondylolisthesis'])\nplt.xlabel('pelvic_radius')\nplt.ylabel('degree_spondylolisthesis')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9550966d8637f2d5c61633d597cd3746fb294e7d","_cell_guid":"63a3245d-0056-46a0-ba65-a1f61d84441f","trusted":true},"cell_type":"code","source":"# KMeans Clustering\n# As you can see there is no labels in data2\n# here we are classifying the data based on 2 features\n\ndata2 = data.loc[:,['degree_spondylolisthesis','pelvic_radius']]\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters = 2)\nkmeans.fit(data2)\nlabels = kmeans.predict(data2)\nplt.scatter(data['pelvic_radius'],data['degree_spondylolisthesis'],c = labels)\nplt.xlabel('pelvic_radius')\nplt.ylabel('degree_spondylolisthesis')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data2.columns\n# n_clusters = list(data2.columns)\n# n_clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# len(n_clusters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # KMeans Clustering\n# # As you can see there is no labels in data2\n# # here we are classifying the data based on all features\n\n# data2 = data.loc[:,['pelvic_incidence', 'pelvic_tilt numeric', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis']]\n# from sklearn.cluster import KMeans\n# kmeans = KMeans(n_clusters = len(list(data2.columns)))\n# kmeans.fit(data2)\n# labels = kmeans.predict(data2)\n# plt.scatter(data['pelvic_radius'],data['degree_spondylolisthesis'],c = labels)\n# plt.xlabel('pelvic_radius')\n# plt.ylabel('degree_spondylolisthesis')\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb760dd8801ea5d0b7d98d1e22f784a9c999b900","_cell_guid":"fc405787-c073-4377-a4d8-0dcb83371888"},"cell_type":"markdown","source":"<a id=\"12\"></a> <br>\n### EVALUATING OF CLUSTERING\nWe cluster data in two groups. Okey well is that correct clustering? **In order to evaluate clustering we will use cross tabulation table**.\n* There are two clusters that are *0* and *1* \n* First class *0* includes 138 abnormal and 100 normal patients\n* Second class *1* includes 72 abnormal and 0 normal patiens\n*The majority of two clusters are abnormal patients.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* What is a cross tabulation table?\n\n> Cross tabulation is a method to quantitatively analyze the relationship between multiple variables. Also known as contingency tables or cross tabs, cross tabulation groups variables to understand the correlation between different variables. It also shows how correlations change from one variable grouping to another.\n\nhttps://humansofdata.atlan.com/2016/01/cross-tabulation-how-why/","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"labels","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"063968754033b4510dcb6f90245a1a9688d4c201","_cell_guid":"df624885-2e0e-474f-82cb-8a498049d8cc","trusted":true},"cell_type":"code","source":"df = pd.DataFrame({'labels':labels,\"class\":data['class']})\nct = pd.crosstab(df['labels'], df['class'])\nprint(ct)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03df9051e892dc064c706619e796b5c9128c0a68","_cell_guid":"f5c33609-4573-443a-82c5-5dfe9852c582"},"cell_type":"markdown","source":"The new question is that we know how many class data includes, but what if number of class is unknow in data. This is kind of like hyperparameter in KNN or regressions. \n* **inertia**: how spread out the clusters are distance from each sample\n* **lower inertia means more clusters**\n* What is the best number of clusters ?\n    *There are low inertia and not too many clusters trade off so we can choose **elbow**\n    \nThe KMeans algorithm clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares [ \\sum_{i=0}^{n}\\min_{\\mu_j \\in C}(||x_i - \\mu_j||^2) ]. This algorithm requires the number of clusters to be specified. It scales well to large number of samples and has been used across a large range of application areas in many different fields.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* What is inertia in clustering?\n\n> Inertia is the sum of squared error for each cluster. Therefore the smaller the inertia the denser the cluster(closer together all the points are)\n\nhttps://towardsdatascience.com/clustering-why-to-use-it-16d8e2fbafe\n\nhttp://scikit-learn.org/stable/modules/clustering.html#:~:text=2.-,K%2Dmeans,%2Dsquares%20(see%20below).&text=Inertia%20is%20not%20a%20normalized,better%20and%20zero%20is%20optimal.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nnp.empty(8)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c7e94ea35a4f535857aa2551ed646ff8c28c1ac","_cell_guid":"710c918b-481b-4f32-94f6-adecd992a97e","trusted":true},"cell_type":"code","source":"\n# inertia\ninertia_list = np.empty(8)\nfor i in range(1,8):\n    kmeans = KMeans(n_clusters=i)\n    kmeans.fit(data2)\n    inertia_list[i] = kmeans.inertia_\nplt.plot(range(0,8),inertia_list,'-o')\nplt.xlabel('Number of cluster')\nplt.ylabel('Inertia')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06031b6c13b5f700bb2e3d53081d990f66c5e11b","_cell_guid":"6449fdb8-7e0b-4432-9ea3-6d866d899704"},"cell_type":"markdown","source":"<a id=\"13\"></a> <br>\n### STANDARDIZATION\n* **Standardizaton is important for both supervised and unsupervised learning**\n* Do not forget standardization as **pre-processing**\n* As we already have visualized data so you got the idea. Now we can use all features for clustering.\n* We can use **pipeline** like supervised learning.","execution_count":null},{"metadata":{"_uuid":"97a7e3a3fa1722970ff34935470be85f2ec7977b","_cell_guid":"0ae27970-8319-45a7-ba7c-a8dd9d9b427e","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/column_2C_weka.csv')\ndata3 = data.drop('class',axis = 1)\ndata3.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* What is the difference between pipeline and make_pipeline in scikit?\n\na) Pipeline: Pipeline of transforms with a final estimator\n\nb) Make_pipeline: Construct a Pipeline from the given estimators. This is a shorthand for the Pipeline constructor.\n\nThe only difference is that make_pipeline generates names for steps automatically.\n\nhttps://stackoverflow.com/questions/40708077/what-is-the-difference-between-pipeline-and-make-pipeline-in-scikit","execution_count":null},{"metadata":{"_uuid":"6883975e95325db10ebed6703c372cbe4da29a48","_cell_guid":"65a46d3b-45b5-4bcd-8137-bdd5b6507ab0","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nscalar = StandardScaler()\nkmeans = KMeans(n_clusters = 2)\npipe = make_pipeline(scalar,kmeans)\npipe.fit(data3)\nlabels = pipe.predict(data3)\ndf = pd.DataFrame({'labels':labels,\"class\":data['class']})\nct = pd.crosstab(df['labels'],df['class'])\nprint(ct)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data3.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a602f3d3d47f3bcc8b62bf0ed3114a3ca4760a03","_cell_guid":"1a31b9e4-f152-4da1-a132-a6e3a8090401"},"cell_type":"markdown","source":"<a id=\"14\"></a> <br>\n### HIERARCHY\n* **vertical lines are clusters**\n* **height on dendogram**: **distance between merging cluster**\n* method= 'single' : closest points of clusters","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#  Introduction to Hierarchical Clustering\n\n1. Hierarchical clustering is another unsupervised learning algorithm that is used to group together the unlabeled data points having similar characteristics. Hierarchical clustering algorithms falls into following two categories.\n\n> > **Agglomerative hierarchical algorithms**  In agglomerative hierarchical algorithms, each data point is treated as a single cluster and then successively merge or agglomerate (bottom-up approach) the pairs of clusters. The hierarchy of the clusters is represented as a dendrogram or tree structure.\n\n> > **Divisive hierarchical algorithms**  On the other hand, in divisive hierarchical algorithms, all the data points are treated as one big cluster and the process of clustering involves dividing (Top-down approach) the one big cluster into various small clusters.\n\nhttps://towardsdatascience.com/hierarchical-clustering-and-its-applications-41c1ad4441a6\n\nhttps://www.tutorialspoint.com/machine_learning_with_python/machine_learning_with_python_clustering_algorithms_hierarchical.htm#:~:text=Hierarchical%20clustering%20is%20another%20unsupervised,falls%20into%20following%20two%20categories.&text=The%20hierarchy%20of%20the%20clusters,a%20dendrogram%20or%20tree%20structure.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data3.iloc[200:220,:]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2dc27b105fc443f442b9605237f45ba69d1e1f1","_cell_guid":"f04ec5bb-9f74-4036-9dfd-2649a3519d79","trusted":true},"cell_type":"code","source":"from scipy.cluster.hierarchy import linkage,dendrogram # Linkage creates a hierarchical cluster tree, using the Single Linkage algorithm.\n\nmerg = linkage(data3.iloc[200:220,:],method = 'single')\ndendrogram(merg, leaf_rotation = 90, leaf_font_size = 6)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **t-SNE ( tsne )** is an algorithm for dimensionality reduction that is well-suited to visualizing high-dimensional data. The name stands for t-distributed Stochastic Neighbor Embedding. The idea is to embed high-dimensional points in low dimensions in a way that respects similarities between points.\n\n\n* What is T SNE used for?\n\n> **What is t-SNE**? (t-SNE) t-Distributed Stochastic Neighbor Embedding is a non-linear dimensionality reduction algorithm used for exploring high-dimensional data. It maps multi-dimensional data to two or more dimensions suitable for human observation.\n\nhttps://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding\n\n> T-distributed Stochastic Neighbor Embedding (t-SNE) is a machine learning algorithm for visualization developed by Laurens van der Maaten and Geoffrey Hinton.[1] It is a nonlinear dimensionality reduction technique well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability.\n\n> The t-SNE algorithm comprises two main stages. First, t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects are assigned a higher probability while dissimilar points are assigned a lower probability. Second, t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the KullbackLeibler divergence (KL divergence) between the two distributions with respect to the locations of the points in the map. While the original algorithm uses the Euclidean distance between objects as the base of its similarity metric, this can be changed as appropriate.\n\n> t-SNE has been used for visualization in a wide range of applications, including computer security research,[2] music analysis,[3] cancer research,[4] bioinformatics,[5] and biomedical signal processing.[6] It is often used to visualize high-level representations learned by an artificial neural network.[7]\n\n> While t-SNE plots often seem to display clusters, the visual clusters can be influenced strongly by the chosen parameterization and therefore a good understanding of the parameters for t-SNE is necessary. Such \"clusters\" can be shown to even appear in non-clustered data,[8] and thus may be false findings. Interactive exploration may thus be necessary to choose parameters and validate results.[9][10] It has been demonstrated that t-SNE is often able to recover well-separated clusters, and with special parameter choices, approximates a simple form of spectral clustering.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/\n1. https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1\n1. https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/\n1. https://medium.com/analytics-vidhya/pca-vs-t-sne-17bcd882bf3d","execution_count":null},{"metadata":{"_uuid":"4166039422a3ed14b4ac779da29401378dfb4dc2","_cell_guid":"15b4f98e-0c9e-49de-b69f-f583524c19ac"},"cell_type":"markdown","source":"<a id=\"15\"></a> <br>\n### T - Distributed Stochastic Neighbor Embedding (T - SNE)\n * learning rate: 50-200 in normal\n * fit_transform: it is both fit and transform. t-sne has only have fit_transform\n * Varieties have same position relative to one another","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**What is manifold in deep learning?**\n> A manifold is an object of dimensionality d that is embedded in some higher dimensional space. Imagine a set of points on a sheet of paper. If we crinkle up the paper, the points are now in 3 dimensions. Many manifold learning algorithms seek to \"uncrinkle\" the sheet of paper to put the data back into 2 dimensions","execution_count":null},{"metadata":{"_uuid":"311a16a96b2ef3e9ca3f8e91db48f28837577291","_cell_guid":"b4ddf817-57ec-45eb-aa03-1ad385cd4f2e","trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\nmodel = TSNE(learning_rate=100)\ntransformed = model.fit_transform(data2)\nx = transformed[:,0]\ny = transformed[:,1]\nplt.scatter(x,y,c = color_list )\nplt.xlabel('pelvic_radius')\nplt.xlabel('degree_spondylolisthesis')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71d32fd64448612635a015c3a4e41871bddc337d","_cell_guid":"00555f09-d381-494e-8bb9-7afb768d912a"},"cell_type":"markdown","source":"<a id=\"16\"></a> <br>\n### PRINCIPLE COMPONENT ANALYSIS (PCA)\n* **Fundemental dimension reduction technique**\n* **first step is decorrelation**:\n    * 1 - rotates data samples to be aligned with axes\n    * 2 - shifts data samples so they have mean zero\n    * 3 - no information lost\n    * 4 - ** fit()** : learn how to shift samples\n    * 5 - **transform()**: apply the learned transformation. It can also be applied to test data\n* **Resulting PCA features are not linearly correlated**\n* **Principle components: directions of variance**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Principal component analysis (PCA)** is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called **principal components**.\n\nPrincipal Component Analysis (PCA) is a statistical procedure that uses an orthogonal transformation which converts a set of correlated variables to a set of uncorrelated variables. PCA is a most widely used tool in exploratory data analysis and in machine learning for predictive models\n\nPrincipal Component Analysis (PCA) is an unsupervised, non-parametric statistical technique primarily used for dimensionality reduction in machine learning.\n\nhttps://towardsdatascience.com/all-you-need-to-know-about-pca-technique-in-machine-learning-443b0c2be9a1\n\nhttps://towardsdatascience.com/a-complete-guide-to-principal-component-analysis-pca-in-machine-learning-664f34fc3e5a\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. https://www.geeksforgeeks.org/ml-principal-component-analysispca/#:~:text=Principal%20Component%20Analysis%20(PCA)%20is,machine%20learning%20for%20predictive%20models.\n1. https://medium.com/apprentice-journal/pca-application-in-machine-learning-4827c07a61db\n1. https://machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/\n1. https://machinelearningmastery.com/principal-components-analysis-for-dimensionality-reduction-in-python/\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data3.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"first step is decorrelation:\n1. rotates data samples to be aligned with axes\n1. shifts data samples so they have mean zero\n1. no information lost\n1. ** fit()** : learn how to shift samples\n1. transform(): apply the learned transformation. \nIt can also be applied to test data\nResulting PCA features are not linearly correlated","execution_count":null},{"metadata":{"_uuid":"04e33157c4c218d873892090649b169babfd03a8","_cell_guid":"93b827bf-616e-436f-a514-86fafbf44512","trusted":true},"cell_type":"code","source":"# PCA : step 1 \nfrom sklearn.decomposition import PCA\nmodel = PCA()\nmodel.fit(data3)\ntransformed = model.transform(data3)\nprint('Principle components: ',model.components_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.components_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"409100f13e8c3e7bea30dab77ad7ea56e2e338e8","_cell_guid":"fc2fdc3e-a5f6-4d67-979e-b9e9cd2e7198","trusted":true},"cell_type":"code","source":"# PCA variance\nscaler = StandardScaler()\npca = PCA()\npipeline = make_pipeline(scaler,pca)\npipeline.fit(data3)\n\nplt.bar(range(pca.n_components_), pca.explained_variance_)\nplt.xlabel('PCA feature')\nplt.ylabel('variance')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.components_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.explained_variance_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec0ad8f2341933207e26f9eaced8a45e16f2bacf","_cell_guid":"3b6f44f2-0add-4f82-a965-859215a9fea9"},"cell_type":"markdown","source":"* **Second step**: **intrinsic dimension**: number of features needed to approximate the data (essential idea behind dimension reduction)\n* PCA identifies intrinsic dimension when samples have any number of features\n* **intrinsic dimension = number of PCA features with significant variance**\n* In order to choose intrinsic dimension try all of them and find best accuracy\n* Also check intuitive way of PCA with this example: https://www.kaggle.com/kanncaa1/tutorial-pca-intuition-and-image-completion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"https://builtin.com/data-science/step-step-explanation-principal-component-analysis","execution_count":null},{"metadata":{"_uuid":"0f1768fd650fe0b85cdf21140c877471c8baf2bc","_cell_guid":"fa5ec66c-2338-4298-a6f8-364a0703ca1f","trusted":true},"cell_type":"code","source":"# apply PCA\npca = PCA(n_components = 2)\npca.fit(data3)\ntransformed = pca.transform(data3)\nx = transformed[:,0]\ny = transformed[:,1]\nplt.scatter(x,y,c = color_list)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformed","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c1a42c1de45ce53e04761b4b6c05840fddaee95","_cell_guid":"41762172-8d1f-47ef-b8ab-79fb3a2b0a19"},"cell_type":"markdown","source":"# CONCLUSION\nThis is the end of DATA SCIENCE tutorial. The first part is here:\n<br>  https://www.kaggle.com/kanncaa1/data-sciencetutorial-for-beginners/\n<br>**If you have any question or suggest, I will be happy to hear it.**","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}