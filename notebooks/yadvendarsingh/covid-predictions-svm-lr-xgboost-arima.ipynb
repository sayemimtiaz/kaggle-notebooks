{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nImage(\"../input/images/Images/Covid-19.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Context\n\n### Novel Coronavirus 2019 (nCoV-2019) is a virus which affects respiratory system and was first discovered in wuhan, China. Some early reports suggested that virus may have been transmitted from animal to person. As we know whole world has been shutdown  because of the widespread cases. At this time it's unclear how easily or sustainably this virus is spreading between people."},{"metadata":{},"cell_type":"markdown","source":"Two Datasets are used:\n\n1. Dataset obtained using ncov2019.live website\n2. Time series Data from https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'\n\n\n - For the purpose of this notebook i have saved the data after scrapping it from the first source as it was not working in the kaggle notebbook.\n - If you want to get latest model predictions than please uncomment the section stating (# Use this code in your notebook and it'll fetch data directly from ncov2019 website. For this notebook. I'm using a copy of the same). and comment the section stating (#Saved Data)\n \n - If you have any Queries please leave a comment"},{"metadata":{},"cell_type":"markdown","source":"#### Lets import all the dependencies for scrapping the website"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n# import requests\n# import bs4\n# from urllib.request import Request, urlopen\n# from urllib.request import urlopen as uReq\n# from bs4 import BeautifulSoup as soup\n# import pandas as pd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Dataset used"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.read_excel('../input/covid-predictions/covid_data/Data/meta/covid_data.xlsx')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Use this code in your notebook and it'll fetch data directly from ncov2019 website. For this notebook. I'm using a copy \n#of the same\n\n# # grabbing the url\n\n# url = \"https://ncov2019.live/\"\n# req = Request(url, headers={\"User-Agent\" : \"Mozilla/5.0\"})\n\n# webpage = urlopen(req).read()\n\n# #parsing it as lxml\n# pagesoup = soup(webpage,\"lxml\")\n\n\n#finding the relevant tags to scrap the data from website\n\n# website_name = pagesoup.find('a',class_ = \"navbar-brand\")\n# link = \"https://ncov2019.live/\"\n# Markdown('<strong>{}</strong>{}'.format(website_name.text,link))\n\n#some quick facts from the website\n\n# quickfacts = pagesoup.find('div', class_ = \"container--wrap bg-navy-4\")\n# Markdown('<strong align=\"center\">{}</strong>'.format(quickfacts))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Current Cases (WorldWide)\n### To know how bad the world has been affected lets get some information on current situation."},{"metadata":{},"cell_type":"markdown","source":"## World COVID-19 Stats"},{"metadata":{},"cell_type":"markdown","source":"#### We will scrap worldwide covid cases.\n1. We'll use pandas read.html which lets us read the webpage table without much of complexity.\n2. Convert the table into dataframe for further processing.\n3. In the header of the list generated you see a number \"1\", which was used in the original website as a filter for arranging data in ascending or descending order."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Use this code in your notebook and it'll fetch data directly from ncov2019 website. For this notebook. I'm using a copy \n#of the same\n\n# # grabbing latest worldwide data\n\n# url = \"https://ncov2019.live/data/world\"\n\n# r = requests.get(url)\n# df_list = pd.read_html(r.text)            #this parse all html tables from a webpage to alist\n# world_df = df_list[2]\n# world_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Saved Data\n\nworld_df = pd.read_csv('../input/covid-predictions/covid_data/Data/Covid-19/ncov2019_data.csv')\nworld_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Sorting the data on number of confirmed cases"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will now sort the countries based on total confirmed cases column\n\nworld_df = world_df.sort_values(\"Confirmed\" , ascending = False)\n\n\n\n#Lets get top 10 affected countries\n\n# world_df.head(10)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Lets see many coulmns are missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"world_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Lets import seaborn as well as matplotlib"},{"metadata":{"trusted":true},"cell_type":"code","source":"#We can also visualize the same using seaborn\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.heatmap(world_df.isnull())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We'll use plotly express for visualization.\n1. It generates graphs which are interactive and user friendly.\n2. We can use zoom in and zoom out feature for proper understanding to a specific part of graph."},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\n# import chart_studio.plotly as py\nimport plotly.graph_objs as go \nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plot number of confirmed cases."},{"metadata":{"trusted":true},"cell_type":"code","source":"init_notebook_mode (connected = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting world_df based on confirmed cases by country names.\n\nworld_fig = px.bar(world_df, x = 'Name' , y = 'Confirmed')\nworld_fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - We can zoom in the graph, thats the beauty of plotly."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets plot top 20 countries based on confirmed cases.\n\nworld_fig = px.bar(world_df.head(20), x = \"Name\" , y = 'Confirmed')\nworld_fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - Now we can see United states holds number 1 position. (cough cough \"we don't wear masks\" - americans)\n - Brazil and India comes at the second and third position surpassing Russia respectively."},{"metadata":{},"cell_type":"markdown","source":"#### Now we'll try to explore the world_df in more details.(based on number of Deceased People)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets see how many people have died with respect to countries. (For top 20 countries)\n\nworld_fig = px.bar(world_df.head(20), x = 'Name', y = 'Confirmed', color = \"Deceased\")\nworld_fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - Here the color of each bar corrosponds to how many people have died.\n - We cannot make out which country has most number of deceased people in a descending order."},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets grab the world_df based on deceased column.\nworld_df.sort_values('Deceased',ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - The column contains many unknown values.\n - We'll replace all the unknown values with zero.\n - Then we will arrange the column in descending order for visualization purpose."},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets replace unknown values to 0.\n\nworld_df['Deceased'].replace(\"Unknown\", 0,inplace=True)\nworld_df['Deceased'] = pd.to_numeric(world_df['Deceased'])          #convert column from type object to int64\nworld_df['Deceased']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now again lets grab world_df based on deceased column.\n\nworld_df.sort_values('Deceased',ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - Perfecto!. Now we can see that column has been cleared off all the \"Unknown\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets again try ro visualize world_df based on death poll for top 20 countries.\n\nworld_fig = px.bar(world_df.sort_values('Deceased', ascending=False).head(20), x = 'Name' , y = 'Deceased')\nworld_fig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Click on the link for more information.\n\n - United States tops the chart. <a href = \"https://www.sciencemag.org/news/2020/04/united-states-leads-coronavirus-cases-not-pandemic-response\" > If you want to know why United States leads in coronvirus cases, but not pandemic response</a>\n - Brazil also surpasses 100,000 deaths and becomes the one of the worst affected countries. <a href = \"https://www.ctvnews.ca/health/coronavirus/death-became-normal-brazil-surpasses-100-000-deaths-from-covid-19-1.5056757\" >'Death became normal': Brazil surpasses 100,000 deaths from COVID-19</a>\n - Mexico's death toll also reached 59.106k and many young people are dying of COVID-19 <a href = \"https://www.forbes.com/sites/nathanielparishflannery/2020/07/24/why-are-so-many-young-people-dying-of-covid-19-in-mexico-city/#148bc1f22792\">Why Are So Many Young People Dying Of Covid-19 In Mexico City?</a>\n - India has also reached 56k and there are many questions about India's rising COVID-19 infection <a href =\"https://www.bbc.com/news/world-asia-india-53018351\">Five key questions about India's rising Covid-19 infections</a>"},{"metadata":{},"cell_type":"markdown","source":"#### Lets visualize the death toll in relation to total confirmed case"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets visualize the death toll based on total confirmed case\n\nimport plotly.graph_objects as go\n\n\n# for grouped barplot using Deceased numbers per country and total number of cases per country.\n\nfig = go.Figure(data = [\ngo.Bar(\n    x = world_df['Name'],\n    y = world_df[\"Deceased\"].head(20),\n    name = \"Deceased\",\n    marker_color = \"indianred\"\n),\ngo.Bar(\n    x = world_df['Name'],\n    y = world_df['Confirmed'].head(20),\n    name = 'Confirmed',\n    marker_color = \"lightsalmon\"\n)\n])\n\n# Here we modify the tickangle of the xaxis, resulting in rotated labels.\nfig.update_layout(barmode='group')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - Here we can see the Death toll is very low as compared to confirmed cases, which is because most of the people recover from COVID-19. Early estimates predicted that the overall COVID-19 recovery rate is between 97% and 99.75%.\n - Mortality rate calculated  = 3.4% (802.318k/23.09665M)"},{"metadata":{},"cell_type":"markdown","source":"#### lets visualize the recovered cases based on total confirmed case"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets visualize the recovered case based in relation to total confirmed case\n\nimport plotly.graph_objects as go\n\n\n# for grouped barplot using recovered cases per country and total number of cases per country.\n\nfig = go.Figure(data = [\ngo.Bar(\n    x = world_df['Name'],\n    y = world_df[\"Recovered\"].head(20),\n    name = \"Recovered\",\n    marker_color = \"indianred\"\n),\ngo.Bar(\n    x = world_df['Name'],\n    y = world_df['Confirmed'].head(20),\n    name = 'Confirmed',\n    marker_color = \"lightsalmon\"\n)\n])\n\n# Here we modify the tickangle of the xaxis, resulting in rotated labels.\nfig.update_layout(barmode='group')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - Here we can see how many person recovered in relation to total cases registered.\n - Recovery rate  = 67% (15.4827M/23.09665M), this contradicts early predicted value of recovery rate which was 97%.\n - Recovery rate and mortality rate are based on how well a country is implementing the testing of its people. <a href = \"https://www.who.int/news-room/commentaries/detail/estimating-mortality-from-covid-19\">Estimating mortality from COVID-19</a>"},{"metadata":{},"cell_type":"markdown","source":"#### Lets see who has implemented testing vastly."},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace unknown values from the column\n\nworld_df['Tests'].replace(\"Unknown\", 0, inplace=True)\nworld_df['Tests'] = pd.to_numeric(world_df['Tests'])          #convert column from type object to int64\n\n\n\n#Now lets plot the data\n\nworld_fig = px.bar(world_df.sort_values('Tests', ascending=False).head(20), x = 'Name' , y = 'Tests')\nworld_fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - China on first position that was unexpected. I was expecting United States.\n - As you can see the countries who are vastly testing their people have a upper hand on curbing the spread of virus by implementing policies."},{"metadata":{},"cell_type":"markdown","source":"#### lets explore the Confirmed cases in relation to total population"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets visualize the confirmed case based in relation to total population\n\nimport plotly.graph_objects as go\n\n\n# for grouped barplot using confirmed cases per country and population per country.\n\nfig = go.Figure(data = [\ngo.Bar(\n    x = world_df['Name'],\n    y = world_df[\"Confirmed\"].head(20),\n    name = \"Confirmed\",\n    marker_color = \"indianred\"\n),\ngo.Bar(\n    x = world_df['Name'],\n    y = world_df['Population'].head(20),\n    name = 'Population',\n    marker_color = \"lightsalmon\"\n)\n])\n\n# Here we modify the tickangle of the xaxis, resulting in rotated labels.\nfig.update_layout(barmode='group')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - This graph shows a small percentage of people are affected by the novel coronavirus. <a href = \"https://www.canada.ca/en/public-health/services/publications/diseases-conditions/people-high-risk-for-severe-illness-covid-19.html\">\n    People who are at high risk for severe illness from COVID-19</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Mortality calculation\n\nworld_df['mortality'] = world_df[['Confirmed','Deceased']].apply(lambda x: (x['Deceased']*100/x['Confirmed']),axis=1 )\n\n#Recovery calculation\n\nworld_df['Recovered'] = pd.to_numeric(world_df['Recovered'],errors='coerce')\nworld_df['recovery'] = world_df[['Confirmed','Recovered']].apply(lambda x: (x['Recovered']*100/x['Confirmed']),axis=1 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def recovery_mortality_plot():\n    \n    name = ['recovery','mortality']\n    Value=[True,False]\n    \n    for i,j in zip(name,Value):\n        \n        world_fig = px.bar(world_df.sort_values(i, ascending=j).head(50), x = 'Name' , y = i)\n        world_fig.show()\n        \nrecovery_mortality_plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - **Martinique, Belgium, France** has lowest recovery rate among countries.\n - Yemen has highest mortality rate **~30%**, which is one of the highest in the world and five times the global average. <a href = \"https://www.bmj.com/content/370/bmj.m2997\">Covid-19: Deaths in Yemen are five times global average as healthcare collapses</a>"},{"metadata":{},"cell_type":"markdown","source":"#### Lets plot world data using Choropleth Map"},{"metadata":{"trusted":true},"cell_type":"code","source":"#something worng with the country names. plotly uses standard ISO-3_codes. Lets try to create a column for country codes\n\nprint(\"{} countries in the list.\". format(world_df['Name'].nunique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The country converter (coco) - a Python package for converting country names between different classifications schemes.\n<a href = \"https://pypi.org/project/country-converter/\">For more info please click here</a>."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install country_converter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import country_converter as coco","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a list and appending all the names from world_df column.\n\nNames = []\nfor i in range(1,215):\n    Names.append(world_df.iloc[i]['Name'][3:])\n\n# Insert Total at index 0. we left that because it doesn't contain any start in it.\n\nNames.insert(0,'TOTAL')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"standard_names = coco.convert(names= Names, to='ISO3')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"map_data = world_df[world_df['Name']!='TOTAL']\nprint(map_data.nunique())\nprint(len(map_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding the ISO3 code in a new world_df['Code'] column.\n\nmap_data = map_data[:215]\nmap_data['code'] = standard_names\n\nmap_data['code'] = map_data['code'].shift(-1)\n\n# Removing countries of which ISO3 code is not available\n\nchoropleth_data = map_data[map_data['code'] != \"NaN\"]\n# choropleth_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets again try to plot the data using choropleth dataframe.\n\n# For using choropleth first we have to make a dictionary\n\ndata = dict(\n        type = 'choropleth',\n        locations = choropleth_data['code'],\n        z = choropleth_data['Confirmed'],\n        text = choropleth_data['Deceased'],\n        marker = dict(line = dict(color = 'rgb(255,255,255)',width = 2)),\n        colorbar = {'title' : \"Confirmed Cases\"}\n        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now create a layout for the graph\n\nlayout = dict(\n    \n    title = 'World COVID-19 Stats',\n    width=1080,\n    height=900,\n    geo = dict(\n        showframe = False,\n        projection = {'type':'mercator'}\n    )\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finally we will pass both layout and data dictionary to generate the map.\nchoromap = go.Figure(data = [data],layout = layout)\nchoromap.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Canada COVID-19 Stats"},{"metadata":{},"cell_type":"markdown","source":"#### Lets get Latest Canada's information.\n1. We'll use the pandas read.html which lets us read the webpage table without much of complexity.\n2. We can also use the lsit to convert it to a dataframe.\n3. In the header of the list generated you see a number \"1\", which was used in the original website as a filter for arranging data in ascending or descending order."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use this code in your notebook and it'll fetch data directly from ncov2019 website. For this notebook. I'm using a copy \n#of the same\n\n# #grabbing latest canada specific data\n\n# url = \"https://ncov2019.live/data/canada\"\n\n# r = requests.get(url)\n# df_list = pd.read_html(r.text) # this parses all the tables in webpages to a list\n# canada_df = df_list[2]\n# canada_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Saved Data\n\ncanada_df = pd.read_csv('../input/covid-predictions/covid_data/Data/Covid-19/ncov2019_canadadata.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Canada COVID-19 Stats"},{"metadata":{},"cell_type":"markdown","source":"#### Lets visualize Canada's Data and see which province has been worst effected.\n1. We'll use the same above canada_df for visualization purpose.\n2. We are going to use this dataframe because it's the latest data and our script we'll update the data every time we run the cell based on the website mentioned above.\n3. I'm going to use plotly for visualization purpose as it generates graphs which are interactive and user friendly."},{"metadata":{"trusted":true},"cell_type":"code","source":"canada_fig = px.bar(canada_df.sort_values('Confirmed'),x='Name',y='Confirmed',color=\"Deceased\")\ncanada_fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - Quebec has maximum number of confirmed cases and twice as many deceased people than ontario. <a href = \"https://www.ctvnews.ca/health/coronavirus/quebec-leads-canada-in-coronavirus-deaths-so-why-is-it-starting-to-reopen-1.4928940\"> Quebec leads Canada in Coronavirus deaths</a>\n - In this article I also found one more interesting thing that Alberta has done more testing per capita, and along with good policies the death polls remains below 500.\n - There a some provinces where there were less to no cases, and no death has been reported, because quite a few people live there."},{"metadata":{},"cell_type":"markdown","source":"#### Lets see relation between total confirmed cases to recovered cases."},{"metadata":{"trusted":true},"cell_type":"code","source":"canada_fig = px.bar(canada_df.sort_values('Recovered'), x = 'Name', y = 'Recovered',color='Confirmed')\ncanada_fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Lets calculate recovery rate in Canada and Alberta specifically"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure(data = [\n    go.Bar(\n    x = canada_df['Name'],\n    y = canada_df['Recovered'],\n    name = \"Recovered\"\n    ),\n    \n    go.Bar(\n    x = canada_df['Name'],\n    y = canada_df['Confirmed'],\n    name = \"Confirmed\"\n    )\n])\n\nfig.update_layout(barmode = \"group\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - Recovery rate canada wide is 88% which is 21% higher than the worldwide recovery rate. This also brings in another factor the geographical location a patient is in and how is the healthcare system there.\n - Alberta's recovery rate is also 89% which is close to overall recovery rate.   "},{"metadata":{},"cell_type":"markdown","source":"#### lets calculate mortality rate."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure(data = [\n    go.Bar(\n    x = canada_df['Name'],\n    y = canada_df['Deceased'],\n    name = \"Deceased\"\n    ),\n    \n    go.Bar(\n    x = canada_df['Name'],\n    y = canada_df['Confirmed'],\n    name = \"Confirmed\"\n    )\n])\n\nfig.update_layout(barmode = \"group\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - Mortality rate of overall canada is 7% (9118/126.804k)\n - Mortality rate of Alberta is 1.8% which is quite astounding. Alberta is implementing policies very efficiently and because of that it has such a low mortality rate.\n - Highest mortality rate is of Quebec 8.9%.\n - Second highest mortality rate is of ontario 6.5%"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure(data = [\n    go.Bar(\n    x = canada_df['Name'],\n    y = canada_df['Recovered'],\n    name = \"Recovered\"\n    ),\n    \n    go.Bar(\n    x = canada_df['Name'],\n    y = canada_df['Deceased'],\n    name = \"Deceased\"\n    )\n])\n\nfig.update_layout(barmode = \"group\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# converting columns to int64 format from object dtype\n# canada_df['Deceased'].replace({'Unknown':0},inplace=True)\n# canada_df[['Deceased','Recovered']] = canada_df[['Deceased','Recovered']].apply(pd.to_numeric,errors='ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Mortality calculation\n\ncanada_df['mortality'] = canada_df[['Confirmed','Deceased']].apply(lambda x: (x['Deceased']*100/x['Confirmed']),axis=1 )\n\n#Recovery calculation\n\ncanada_df['Recovered'] = pd.to_numeric(canada_df['Recovered'],errors='coerce')\ncanada_df['recovery'] = canada_df[['Confirmed','Recovered']].apply(lambda x: (x['Recovered']*100/x['Confirmed']),axis=1 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def recovery_mortality_plot():\n    \n    name = ['recovery','mortality']\n    Value=[True,False]\n    \n    for i,j in zip(name,Value):\n        \n        canada_fig = px.bar(canada_df.sort_values(i, ascending=j).head(50), x = 'Name' , y = i)\n        canada_fig.show()\n        \nrecovery_mortality_plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- From these graphs we can see that overall recovery rate for canada is more than **~84%**. **Alberta** is very close with recovery rate of **~83%**.\n- **Manitoba** has very lowest recovery rate **~50%**. Highest recovery rate is in **PEI**, which can be attributed to low population.\n- Average mortality rate is close to **~4.5%**.\n- Highest mortality rate is observerd in **Quebec**. **Alberta** is in bottom **5** in terms of mortality rate."},{"metadata":{},"cell_type":"markdown","source":"## Model for predicting the number of confirmed cases."},{"metadata":{"trusted":true},"cell_type":"code","source":"# import confirmed cases data\n\nconfirmed_df = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv')\n\n#Getting all the dates\ncols = confirmed_df.keys()\nconfirmed = confirmed_df.loc[:4,cols[4]:cols[-1]]\n\ndates = confirmed.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"worldcases = []\n\nfor i in ((dates)):\n    \n    confirmed_sum = confirmed[i].sum()\n    \n    worldcases.append(confirmed_sum)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport random\nimport math\nimport time\nfrom sklearn.linear_model import LinearRegression, BayesianRidge\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport datetime","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Future Forecasting"},{"metadata":{"trusted":true},"cell_type":"code","source":"days_in_future = 10\nfuture_forcast = np.array([i for i in range(len(dates)+days_in_future)]).reshape(-1, 1)\nadjusted_dates = future_forcast[:-10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Convert integer into datetime for better visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"start = '1/20/2020'\nstart_date = datetime.datetime.strptime(start, '%m/%d/%Y')\nfuture_forcast_dates = []\nfor i in range(len(future_forcast)):\n    future_forcast_dates.append((start_date + datetime.timedelta(days=i)).strftime('%m/%d/%Y'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"days_from_1_20 = np.array([i for i in range(len(dates))]).reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_confirmed, X_test_confirmed, y_train_confirmed, y_test_confirmed = train_test_split(days_from_1_20[50:], worldcases[50:], test_size=0.15, shuffle=False) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Machine Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_confirmed = SVR(shrinking=True, kernel='poly',gamma=0.01,epsilon=1,degree=3,C=0.1)\nsvm_confirmed.fit(X_train_confirmed,y_train_confirmed)\nsvm_pred = svm_confirmed.predict(future_forcast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_test_pred = svm_confirmed.predict(X_test_confirmed)\nplt.figure(figsize=(20,15))\nplt.plot(y_test_confirmed)\nplt.plot(svm_test_pred)\nplt.legend(['Test Data', 'SVM Predictions'])\nprint('MAE:', mean_absolute_error(svm_test_pred, y_test_confirmed))\nprint('MSE:',mean_squared_error(svm_test_pred, y_test_confirmed))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Mean Absolute percentage error\nI prefer to use mean absolute percent error because it gives an simple percentage to communicate that shows how off the predictions are. MAPE is not included in Sklearn, so a custom feature must be used."},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_absolute_percentage_error(y_true, y_pred): \n    \"\"\"Calculates MAPE given y_true and y_pred\"\"\"\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean absolute percentage error of SVM is ',mean_absolute_percentage_error(y_test_confirmed,svm_test_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear Regression model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform our data for polynomial regression\npoly = PolynomialFeatures(degree=5)\npoly_X_train_confirmed = poly.fit_transform(X_train_confirmed)\npoly_X_test_confirmed = poly.fit_transform(X_test_confirmed)\npoly_future_forcast = poly.fit_transform(future_forcast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# polynomial regression\nlinear_model = LinearRegression(normalize=True, fit_intercept=False)\nlinear_model.fit(poly_X_train_confirmed, y_train_confirmed)\ntest_linear_pred = linear_model.predict(poly_X_test_confirmed)\nlinear_pred = linear_model.predict(poly_future_forcast)\nprint('MAE:', mean_absolute_error(test_linear_pred, y_test_confirmed))\nprint('MSE:',mean_squared_error(test_linear_pred, y_test_confirmed))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Mean Absolute percentage error\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean absolute percentage error of LR is ', mean_absolute_percentage_error(y_test_confirmed,test_linear_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,15))\nplt.plot(y_test_confirmed)\nplt.plot(test_linear_pred)\nplt.legend(['Test Data', 'Polynomial Regression Predictions'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confirmed_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transposing the row for time series analysis\n\nconfirmed_df = confirmed_df.T\nconfirmed_df = confirmed_df.rename(columns=confirmed_df.iloc[1])\n# confirmed_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confirmed_df = confirmed_df[4:]\n# confirmed_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confirmed_df['Total_cases'] = confirmed_df.sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# converting the index column to date\n\nconfirmed_df.reset_index(level=0,inplace=True)\n# confirmed_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confirmed_df['dates'] = pd.to_datetime(confirmed_df['index'])\n# confirmed_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"time_series_analysis_df = confirmed_df[['Total_cases','dates']]\n# time_series_analysis_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we will set the dates column as the index of the dataframe to allow us really explore the our data.\n\ntime_series_analysis_df = time_series_analysis_df.set_index('dates')\ntime_series_analysis_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import read_csv\nfrom matplotlib import pyplot\nimport xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nplt.style.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#save your file\n\n# path = r'C:\\Users\\yrsin\\Desktop\\Fall2020\\databasemanagement\\Homework\\CPSC-671\\covid_data\\Data\\Covid-19'\n# time_series_analysis_df.to_csv(path+'\\series.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load dataset\nseries = pd.read_csv('../input/covid-predictions/covid_data/Data/Covid-19/series.csv', header=0, index_col=0)\nvalues = series.values\n# plot dataset\npyplot.plot(values)\npyplot.title('Total Cases')\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - We are using the XGBoost model on the dataset when making one-step forecasts for the data from September month.\n - We will use previous 10 time steps as input to the model and default model hyperparameters, except we will change the loss to 'reg:sqarederror' and use 1,000 trees in the ensemble."},{"metadata":{"trusted":true},"cell_type":"code","source":"# forecast monthly births with xgboost\nfrom numpy import asarray\nfrom pandas import read_csv\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\nfrom matplotlib import pyplot\n\n# transform a time series dataset into a supervised learning dataset\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n\tn_vars = 1 if type(data) is list else data.shape[1]\n\tdf = DataFrame(data)\n\tcols = list()\n\t# input sequence (t-n, ... t-1)\n\tfor i in range(n_in, 0, -1):\n\t\tcols.append(df.shift(i))\n\t# forecast sequence (t, t+1, ... t+n)\n\tfor i in range(0, n_out):\n\t\tcols.append(df.shift(-i))\n\t# put it all together\n\tagg = concat(cols, axis=1)\n\t# drop rows with NaN values\n\tif dropnan:\n\t\tagg.dropna(inplace=True)\n\treturn agg.values\n\n# split a univariate dataset into train/test sets\ndef train_test_split(data, n_test):\n\treturn data[:-n_test, :], data[-n_test:, :]\n\n# fit an xgboost model and make a one step prediction\ndef xgboost_forecast(train, testX):\n\t# transform list into array\n\ttrain = asarray(train)\n\t# split into input and output columns\n\ttrainX, trainy = train[:, :-1], train[:, -1]\n\t# fit model\n\tmodel = XGBRegressor(objective='reg:squarederror', n_estimators=1000)\n\tmodel.fit(trainX, trainy)\n\t# make a one-step prediction\n\tyhat = model.predict(asarray([testX]))\n\treturn yhat[0]\n\n# walk-forward validation for univariate data\ndef walk_forward_validation(data, n_test):\n\tpredictions = list()\n\t# split dataset\n\ttrain, test = train_test_split(data, n_test)\n\t# seed history with training dataset\n\thistory = [x for x in train]\n\t# step over each time-step in the test set\n\tfor i in range(len(test)):\n\t\t# split test row into input and output columns\n\t\ttestX, testy = test[i, :-1], test[i, -1]\n\t\t# fit model on history and make a prediction\n\t\tyhat = xgboost_forecast(history, testX)\n\t\t# store forecast in list of predictions\n\t\tpredictions.append(yhat)\n\t\t# add actual observation to history for the next loop\n\t\thistory.append(test[i])\n\n\t\tprint('>expected=%.1f, predicted=%.1f' % (testy, yhat))\n\t# estimate prediction error\n\terror = mean_absolute_error(test[:, -1], predictions)\n\treturn error, test[:, -1], predictions\n\n# load the dataset\nseries = read_csv('../input/covid-predictions/covid_data/Data/Covid-19/series.csv', header=0, index_col=0)\nvalues = series.values\n# transform the time series data into supervised learning\ndata = series_to_supervised(values, n_in=10)\n# evaluate\nmae, y, yhat = walk_forward_validation(data, 71)\nprint('MAE: %.3f' % mae)\n# plot expected vs preducted\npyplot.plot(y, label='Expected')\npyplot.plot(yhat, label='Predicted')\npyplot.xlabel(\"Forecasting from September\")\npyplot.legend()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean absolute percentage error of XGBosst is',mean_absolute_percentage_error(y,yhat))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Persistance model for timeseries forecasting"},{"metadata":{},"cell_type":"markdown","source":"The persistence forecast is where the observation from the prior time step (t-1) is used to predict the observation at the current time step (t).\n\nWe can implement this by taking the last observation from the training data and history accumulated by walk-forward validation and using that to predict the current time step."},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import concat\nfrom pandas import DataFrame\nfrom pandas import Series\nfrom pandas import concat\nfrom pandas import read_csv\nfrom pandas import datetime\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom math import sqrt\nfrom matplotlib import pyplot\nimport numpy\nfrom pandas import datetime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = time_series_analysis_df.values\ntrain, test = X[0:230], X[230:]\nprint(train.shape, test.shape)\n\n# walk-forward validation\nhistory = [x for x in train]\npredictions = list()\nfor i in range(len(test)):\n\t# make prediction\n\tpredictions.append(history[-1])\n\t# observation\n\thistory.append(test[i])\n# report performance\nrmse = sqrt(mean_squared_error(test, predictions))\n\nprint('RMSE: %.3f' % rmse)\nprint(mean_absolute_percentage_error(test,predictions))\nfrom matplotlib import pyplot\n\n# line plot of observed vs predicted\npyplot.plot(test,label='Test')\npyplot.plot(predictions, label = 'Predictions')\nplt.legend(labels=('Test','Predictions'))\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### LSTM in Keras"},{"metadata":{},"cell_type":"markdown","source":"The Long Short-Term Memory network (LSTM) is a type of Recurrent Neural Network (RNN).\n\nA benefit of this type of network is that it can learn and remember over long sequences and does not rely on a pre-specified window lagged observation as input.\n\nIn Keras, this is referred to as stateful, and involves setting the “stateful” argument to “True” when defining an LSTM layer.\n\nBy default, an LSTM layer in Keras maintains state between data within one batch. A batch of data is a fixed-sized number of rows from the training dataset that defines how many patterns to process before updating the weights of the network. State in the LSTM layer between batches is cleared by default, therefore we must make the LSTM stateful. This gives us fine-grained control over when state of the LSTM layer is cleared, by calling the reset_states() function.\n\nThe LSTM layer expects input to be in a matrix with the dimensions: [samples, time steps, features].\n\nSamples: These are independent observations from the domain, typically rows of data.\nTime steps: These are separate time steps of a given variable for a given observation.\nFeatures: These are separate measures observed at the time of observation.\nWe have some flexibility in how the Total cases dataset is framed for the network. We will keep it simple and frame the problem as each time step in the original sequence is one separate sample, with one timestep and one feature.\n\nGiven that the training dataset is defined as X inputs and y outputs, it must be reshaped into the Samples/TimeSteps/Features format, for example:"},{"metadata":{},"cell_type":"markdown","source":"Batch Size: 1\nEpochs: 1500\nNeurons: 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"def parser(x):\n    return datetime.strptime(x,'%Y-%m-%d')\n\ndef timeseries_to_supervised(data, lag=1):\n    df = pd.DataFrame(data)\n    columns = [df.shift(i) for i in range(1, lag+1)]\n    columns.append(df)\n    df = concat(columns, axis=1)\n    df.fillna(0, inplace=True)\n    return df\n\ndef difference(dataset, interval=1):\n\tdiff = list()\n\tfor i in range(interval, len(dataset)):\n\t\tvalue = dataset[i] - dataset[i - interval]\n\t\tdiff.append(value)\n\treturn Series(diff)\n\ndef inverse_difference(history, yhat, interval=1):\n\treturn yhat + history[-interval]\n\ndef scale(train, test):\n\t# fit scaler\n\tscaler = MinMaxScaler(feature_range=(-1, 1))\n\tscaler = scaler.fit(train)\n\t# transform train\n\ttrain = train.reshape(train.shape[0], train.shape[1])\n\ttrain_scaled = scaler.transform(train)\n\t# transform test\n\ttest = test.reshape(test.shape[0], test.shape[1])\n\ttest_scaled = scaler.transform(test)\n\treturn scaler, train_scaled, test_scaled\n \n# inverse scaling for a forecasted value\ndef invert_scale(scaler, X, value):\n\tnew_row = [x for x in X] + [value]\n\tarray = numpy.array(new_row)\n\tarray = array.reshape(1, len(array))\n\tinverted = scaler.inverse_transform(array)\n\treturn inverted[0, -1]\n\n# fit an LSTM network to training data\ndef fit_lstm(train, batch_size, nb_epoch, neurons):\n\tX, y = train[:, 0:-1], train[:, -1]\n\tX = X.reshape(X.shape[0], 1, X.shape[1])\n\tmodel = Sequential()\n\tmodel.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n\tmodel.add(Dense(1))\n\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n\tfor i in range(nb_epoch):\n\t\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n\t\tmodel.reset_states()\n\treturn model\n \n# make a one-step forecast\ndef forecast_lstm(model, batch_size, X):\n\tX = X.reshape(1, 1, len(X))\n\tyhat = model.predict(X, batch_size=batch_size)\n\treturn yhat[0,0]\n\n\nseries = pd.read_csv('../input/covid-predictions/covid_data/Data/Covid-19/series.csv', header = 0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\nprint(series.head())\n\n\nraw_values = series.values\ndiff_values = difference(raw_values, 1)\n \n# transform data to be supervised learning\nsupervised = timeseries_to_supervised(diff_values, 1)\nsupervised_values = supervised.values\n \n# split data into train and test-sets\ntrain, test = supervised_values[0:230], supervised_values[230:]\n \n# transform the scale of the data\nscaler, train_scaled, test_scaled = scale(train, test)\n \n# fit the model\nlstm_model = fit_lstm(train_scaled, 1, 1500, 1)\n# forecast the entire training dataset to build up state for forecasting\ntrain_reshaped = train_scaled[:, 0].reshape(len(train_scaled), 1, 1)\nlstm_model.predict(train_reshaped, batch_size=1)\n \n# walk-forward validation on the test data\npredictions = list()\nfor i in range(len(test_scaled)):\n\t# make one-step forecast\n\tX, y = test_scaled[i, 0:-1], test_scaled[i, -1]\n\tyhat = forecast_lstm(lstm_model, 1, X)\n\t# invert scaling\n\tyhat = invert_scale(scaler, X, yhat)\n\t# invert differencing\n\tyhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n\t# store forecast\n\tpredictions.append(yhat)\n\texpected = raw_values[len(train) + i + 1]\n\tprint('day=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expected))\n \n# report performance\nrmse = sqrt(mean_squared_error(raw_values[231:], predictions))\nprint('Test RMSE: %.3f' % rmse)\n# line plot of observed vs predicted\npyplot.plot(raw_values[231:])\npyplot.plot(predictions)\npyplot.xlabel('Forecasting from September')\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean absolute percentage error of LSTM is ',mean_absolute_percentage_error(raw_values[231:], predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Additive model \n 1. This model is used when the time series level does not vary with the variations around the trend. Here, the time series components are simply added together using the formula:\n     - y(t) = Level(t) + Trend(t) + Seasonality(t) + Noise(t)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib\nimport statsmodels.api as sm\ndecomposition = sm.tsa.seasonal_decompose(time_series_analysis_df,model='additive')\nfir = decomposition.plot()\nmatplotlib.rcParams['figure.figsize']=[20.0,15.0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" -  Here we can see that trend is continously going up, **Total number of cases grew from 10 million in month of july to 40 million in the month of october.**\n - The increase in the number of the cases can be attributed to some of the severly affected country mentioned above in the discussion.\n - The sesonality shows us a sinusoidal trend which can be attributed to continous increasing trend in the number of confirmed cases.\n - we can see some noise components in later months of **august, september, and october** which can be attributed to poorly affected countries above mentioned."},{"metadata":{},"cell_type":"markdown","source":"#### Time Series Forecasting with Arima (Autoregressive Integrated Moving Average)\n\nWith the notation ARIMA(p, d, q), ARIMA models are denoted. The seasonality, pattern, and noise in the data account for these three parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p = d = q = range(0, 2)\npdq = list(itertools.product(p, d, q))\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\nprint('Examples of parameter combinations for Seasonal ARIMA...')\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Parameter Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use this for parameter selection\n\n# for param in pdq:\n#     for param_seasonal in seasonal_pdq:\n#         try:\n#             mod = sm.tsa.statespace.SARIMAX(time_series_analysis_df,\n#                                             order=param,\n#                                             seasonal_order=param_seasonal,\n#                                             enforce_stationarity=False,\n#                                             enforce_invertibility=False)\n#             results = mod.fit()\n#             print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n#         except:\n#             continue","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above output suggests that SARIMAX(1, 1, 1)x(1, 1, 1, 12) yields the lowest AIC value."},{"metadata":{},"cell_type":"markdown","source":"#### Fitting the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"mod = sm.tsa.statespace.SARIMAX(time_series_analysis_df,\n                                order=(1, 1, 1),\n                                seasonal_order=(1, 1, 1, 12),\n                                enforce_stationarity=False,\n                                enforce_invertibility=False)\nresults = mod.fit()\nprint(results.summary().tables[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.plot_diagnostics(figsize=(16, 8))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Validation of forecasts"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = results.get_prediction(start=pd.to_datetime('2020-09-01'), dynamic=False)\npred_ci = pred.conf_int()\nax = time_series_analysis_df.plot(label='observed')\npred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7))\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.2)\nax.set_xlabel('Date')\nax.set_ylabel('Number of cases')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_forecasted = pred.predicted_mean\n# print(y_forecasted)\n# time_series_analysis_df['Total_cases']['2020-09-01':]\ny_truth = time_series_analysis_df['Total_cases']['2020-09-01':]\nmse = ((y_forecasted - y_truth) ** 2).mean()\nprint('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The Root Mean Squared Error of our forecasts is {}'.format(round(np.sqrt(mse), 2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Mean Absolute percentage error"},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_absolute_percentage_error(y_truth,y_forecasted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_uc = results.get_forecast(steps=100)\npred_ci = pred_uc.conf_int()\nax = time_series_analysis_df['Total_cases'].plot(label='Total_cases', figsize=(14, 7))\npred_uc.predicted_mean.plot(ax=ax, label='Forecast')\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.25)\nax.set_title('COVID-19')\nax.set_xlabel('Date')\nax.set_ylabel('Total Cases')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - **Future Forecasting**\n - We used Arima model from stats to predict future values as mean absolute percentage error of ARIMA model is very low **~0.098%**\n - Our model predicts that in the month of **Jan2021** we will have around [60million, 90million] cases.\n - As we move further in the future the confidence interval of prediction drops because this model doesn't take into account various policies that have been implemented by countries to curb the spread the of the virus. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}