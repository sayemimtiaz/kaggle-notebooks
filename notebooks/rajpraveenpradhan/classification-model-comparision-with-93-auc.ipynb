{"cells":[{"metadata":{},"cell_type":"markdown","source":"### About this dataset:\n\n\n\nCardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worlwide. Heart failure is a common event caused by CVDs and this dataset contains 12 features that can be used to predict mortality by heart failure.\n\nMost cardiovascular diseases can be prevented by addressing behavioural risk factors such as tobacco use, unhealthy diet and obesity, physical inactivity and harmful use of alcohol using population-wide strategies.\n\nPeople with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help.\n\n### Task:\n    \nCreate a model to assess the likelihood of a death by heart failure event.\n\n### Steps Involved:\n1. [Reading and understanding the data](#1)\n2. [Exploratory Data Analysis (EDA)](#2)\n3. [Performing Train-Test Split](#3)\n4. [Handling data imbalance using SMOTE](#4)\n5. [Feature Engineering](#5)\n    - [Feature Selection](#5.1)\n    - [Feature Scaling](#5.2)\n6. [Model Building/Evaluation](#6)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n# 1. Reading and understanding the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing prerequisites\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport os\n\n%matplotlib inline\npd.set_option(\"display.max_rows\", None,\"display.max_columns\", None)\nwarnings.simplefilter(action='ignore')\nplt.style.use('seaborn')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing dataset\ndf = pd.read_csv('../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking numerical attributes stats, to identify if these is any abnormality\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking if there are any missing values\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since everything seems normal in the dataset, we can skip Data Formatting step and move directly to EDA."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a>\n# 2. Exploratory Data Analysis (EDA)"},{"metadata":{},"cell_type":"markdown","source":"## a) Univariate Analysis"},{"metadata":{},"cell_type":"markdown","source":"**Categorical Columns**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizing categorical features\ncat_features = [\"anaemia\",\"diabetes\",\"high_blood_pressure\",\"sex\",\"smoking\",\"DEATH_EVENT\"]\n\ni=0\nwhile i < 6:\n    fig = plt.figure(figsize=[8,3]) \n\n    plt.subplot(1,2,1)\n    sns.countplot(x=cat_features[i], data=df)\n    i += 1\n    \n    plt.subplot(1,2,2)\n    sns.countplot(x=cat_features[i], data=df)\n    i += 1\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Numerical Columns**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizing numerical columns\n\n# Extracting numerical columns using categorical columns\nnum_features = df.columns[~df.columns.isin(cat_features)].tolist()\nr = c = 0\nfig,ax = plt.subplots(4,2,figsize=(15,14))\n\nfor n,i in enumerate(num_features):\n    sns.boxplot(x=i, data=df,ax=ax[r,c])\n    c+=1\n    if (n+1)%2==0:\n        r+=1\n        c=0\nax[r,c].axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be observed from above boxplots 'creatinine_phosphokinase', 'platelets' & 'serum_creatinine' are having outliers which might affect the decision making of our model. There are various techniques using which one can treat outliers like capping the values or by removing the entire row. But for taking such decisions, one must have domain knowledge. So for now we will be keeping the them as it is.  "},{"metadata":{},"cell_type":"markdown","source":"## b) Bivariate Analysis"},{"metadata":{},"cell_type":"markdown","source":"**Categorical features vs target variable**"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features = [\"anaemia\",\"diabetes\",\"high_blood_pressure\",\"sex\",\"smoking\",\"DEATH_EVENT\"]\nr = c = 0\nfig,ax = plt.subplots(3,2,figsize=(10,12))\n\nfor n,i in enumerate(cat_features[:-1]):\n    temp = df.pivot_table(columns=df.DEATH_EVENT,index=df[i],aggfunc='count')['age']\n    temp.plot.bar(stacked=True,ax=ax[r,c])\n    ax[r,c].set_ylabel(\"count of observations\")\n    c+=1\n    if (n+1)%2==0:\n        r+=1\n        c=0\nax[r,c].axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- It seems that \"male\" patients have higher tendency get heart problems.\n- Above distribution suggests that smoking is not the root cause behind Cardiovascular diseases(CVDs).\n- There are fewer cases of CVDs patients that have high blood pressure. "},{"metadata":{},"cell_type":"markdown","source":"**Numerical features vs target variable**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extracting numerical columns using categorical columns\nnum_features = df.columns[~df.columns.isin(cat_features)].tolist()\nr = c = 0\nfig,ax = plt.subplots(4,2,figsize=(10,20))\n\nfor n,i in enumerate(num_features):\n    sns.boxplot(x='DEATH_EVENT',y=i, data=df,ax=ax[r,c])\n    c+=1\n    if (n+1)%2==0:\n        r+=1\n        c=0\nax[r,c].axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Generally patients who die due to CVDs are older then the patients who survive.\n- Concentation of 'serum_sodium' and 'ejection_fraction' are higher for patients who survive CVDs.\n- 'serum_creatinine' is lower for patients who survive CVDs."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a>\n# 3. Performing Train-Test split"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ny = df['DEATH_EVENT'] \nX = df.drop('DEATH_EVENT',axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\nprint(f'X_train order : {X_train.shape}')\nprint(f'y_train order : {y_train.shape}')\nprint(f'X_test order : {X_test.shape}')\nprint(f'y_test order :{y_test.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a>\n# 4. Handling data imbalance using SMOTE"},{"metadata":{},"cell_type":"markdown","source":"Imbalanced classification involves developing predictive models on classification datasets that have a severe class imbalance (eg. ratio of 90:10). These kind of senarios are usually common in Healthcare and BSFI domain. eg: In Credit risk analysis, where the credit worthiness of a potential borrowers is determined, chances of approving a bad loan are very low thus making the target variable very skewed.\n\nThe challenge of working with imbalanced datasets is that most machine learning techniques will ignore, and in turn have poor performance on, the minority class, although typically it is performance on the minority class that is most important.\n\nOne approach to addressing imbalanced datasets is to oversample the minority class. The simplest approach involves duplicating examples in the minority class, although these examples don’t add any new information to the model. Instead, new examples can be synthesized from the existing examples. This is a type of data augmentation for the minority class and is referred to as the Synthetic Minority Oversampling Technique, or SMOTE for short. <a href=\"https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/\">READ MORE...</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target distribution\nplt.figure(figsize=(3.5,3))\nsns.countplot(df['DEATH_EVENT'])\nplt.title(\"Target Class Distribution\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above plot shows that our dataset is moderately imbalanced. So, performing SMOTE analysis will not make any sense. Moreover since our dataset has very few training samples, performing SMOTE will reduce our model accuracy drastically.\n\nBut for the sake of understanding lets see what are the steps involved in SMOTE analysis."},{"metadata":{},"cell_type":"markdown","source":"**Creating Synthetic samples using SMOTE Analysis**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\nsmote_up = SMOTE(random_state = 42)\nX_train_res , y_train_res = smote_up.fit_sample(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from collections import Counter\n# Target distribution post SMOTE Analysis\nprint(\"Class distribution without SMOTE:\",Counter(y_train))\nprint(\"Class distribution with SMOTE:\",Counter(y_train_res))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that count of samples in class '1' have increased from '82' to '157' making the ratio of target classes 1:1."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a>\n# 5. Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5.1\"></a>\n## a) Feature Selection\n\nWe will be using RFE which is a wrapper based method for selecting best features."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\ndef top_feat(model,feat_cnt):\n    '''\n    This function provides the top \"n\" features for provided model using RFE.\n    '''\n    rfe = RFE(model, n_features_to_select=feat_cnt)\n    rfe.fit(X_train, y_train)\n    return X.columns[rfe.support_].to_list()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***There are other types of feature selection techniques which are mentioned below.***"},{"metadata":{},"cell_type":"markdown","source":"#### i) Filter Based Methods\n\nFeatures are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable. Filter methods are much faster compared to wrapper methods as they do not involve training the models. We need to take care of multicollinearity beforehand. Eg:\n- Pearson Correlation\n- Chi-Square test\n- ANOVA\n- LDA\n\n<img src=\"https://www.analyticsvidhya.com/wp-content/uploads/2016/11/FS1.png\"></img>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Saving 'DEATH_EVENT' corr. to a DataFrame\nFeature_Imp = df.corr()['DEATH_EVENT'].reset_index()\nFeature_Imp.drop(12,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting Pearsons corr. to its absolute value\nFeature_Imp.rename(columns={'DEATH_EVENT':'Peason_Corr'},inplace = True)\nFeature_Imp['Peason_Corr'] = Feature_Imp['Peason_Corr'].abs()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Feature_Imp.sort_values(by='Peason_Corr',ascending=False,ignore_index=True,inplace = True)\nFeature_Imp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ii) Wrapper Based Method"},{"metadata":{},"cell_type":"markdown","source":"Use a subset of features and train a model using them. Based on the inferences that we draw from the previous model, we decide to add or remove features from your subset. These methods are usually computationally very expensive. One example of this is RFE i.e. Recursive Feature Elimination which uses\"coef_\" or \"feature_importances_\" to select best features.\n\n- Recusive Feature Elimination (RFE)\n- Backward Elimination\n- Forward Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting top 6 features using RFE for Logestic Regression Model\nrfe = RFE(estimator=LogisticRegression(), n_features_to_select=6, verbose=1)\nrfe.fit(X_train, y_train)\nrfe_df = pd.DataFrame(list(zip(X.columns, rfe.support_, rfe.ranking_)),columns= ['index','RFE_Support','RFE_rank'])\nrfe_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Columns to be excluded for building Logestic Regression Model\nX.columns[~rfe.support_].to_list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Columns to be included for building Logestic Regression Model\nX.columns[rfe.support_].to_list()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### iii) Embedded Methods"},{"metadata":{},"cell_type":"markdown","source":"Embedded methods combine the qualities of filter and wrapper methods. It’s implemented by algorithms that have their own built-in feature selection methods. \neg:\n- Lasso regression performs L1 regularization\n- Ridge regression performs L2 regularization\n- feature_importances_ in Decision tree based models"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=500)\nrf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feat_imp = pd.DataFrame(list(zip(X_train.columns,rf.feature_importances_)),columns = ['index','RF_fe'])\ndf_feat_imp.sort_values(by='RF_fe', ascending=False, ignore_index=True, inplace=True)\ndf_feat_imp.plot.barh(x='index',y='RF_fe')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5.2\"></a>\n## b) Feature Scaling"},{"metadata":{},"cell_type":"markdown","source":"##### When to Scale ?\nRule of thumb I follow here is any algorithm that computes distance or assumes normality, scale your features!!!\n\nSome examples of algorithms where feature scaling matters are:\n\n- k-nearest neighbors with an Euclidean distance measure is sensitive to magnitudes and hence should be scaled for all features to weigh in equally.\n- Scaling is critical, while performing Principal Component Analysis(PCA). PCA tries to get the features with maximum variance and the variance is high for high magnitude features. This skews the PCA towards high magnitude features.\n- We can speed up gradient descent by scaling. This is because θ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.\n- Tree based models are not distance based models and can handle varying ranges of features. Hence, Scaling is not required while modelling trees.\n- Algorithms like Linear Discriminant Analysis(LDA), Naive Bayes are by design equipped to handle this and gives weights to the features accordingly. Performing a features scaling in these algorithms may not have much effect."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Performing Standard Scaler on X train and test\nfrom sklearn.preprocessing import StandardScaler\n\ndef standard_scaler():\n    scaler = StandardScaler()\n    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train),columns=X.columns)\n    X_test_scaled = pd.DataFrame(scaler.transform(X_test),columns=X.columns)\n    return X_train_scaled,X_test_scaled","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Model Creation/Evaluation\n\n1. Logestic Regression\n2. K Nearest Neighbour (KNN)\n3. Naive Bayers\n4. Support Vector machine (SVM)\n5. Decision Tree\n6. Ensembled Techniques\n    - Random Forest\n    - Gradient Boosting (GBM)\n    - XGboost\n    - CatBoost\n    - LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, plot_confusion_matrix, classification_report, plot_roc_curve\n\nAcc_list = []\n\ndef hf_model(model,model_name,features=\"\"):\n    # Feature Scaling\n    if model_name in ['GaussianNB','XGboost','RandomForest','GradientBoosting',\n                      'LGBMClassifier','CatBoostClassifier','DecisionTree']:\n        X_train_temp,X_test_temp = X_train,X_test\n    else:\n        X_train_temp,X_test_temp = standard_scaler()\n    \n    # Feature Selection\n    if model_name not in ['KNN','GaussianNB','SVM']:\n        X_train_temp = X_train_temp[features]\n        X_test_temp = X_test_temp[features]\n   \n    # Fitting train set\n    model.fit(X_train_temp,y_train)\n    \n    # Performing prediction on test set\n    y_test_pred = model.predict(X_test_temp)\n        \n    # Calculating Evaluation Metrics\n    s1 = accuracy_score(y_test,y_test_pred)\n    print(f\"{model_name} Success Rate :{round(s1*100,2)}%\\n\")\n    print(classification_report(y_test,y_test_pred))\n\n    \n    # Plotting Confusion Matrix and ROC curve\n    fig, ax = plt.subplots(1,2,figsize=(12,5))\n    plot_confusion_matrix(model,X_test_temp,y_test,cmap='Blues',ax=ax[0])\n    ax[0].set_title(\"Confusion Matrix\",fontsize=16)\n    ax[0].set_xticks([0,1], [\"Heart Not Failed\",\"Heart Fail\"])\n    ax[0].set_yticks([0,1], [\"Heart Not Failed\",\"Heart Fail\"])\n    ax[0].grid(False)\n    \n    plot_roc_curve(model,X_test_temp,y_test,ax=ax[1])\n    ax[1].set_title(\"ROC Curve\",fontsize=16)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### a) Logestic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(n_jobs=-1)\nfeatures = top_feat(lr,10)\nhf_model(lr,\"Logestic Regression\",features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### b) K Nearest Neighbour (KNN)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=7)\nhf_model(knn,\"KNN\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### c) Gaussian Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\nhf_model(gnb,\"GaussianNB\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### d) Support Vector machine (SVM)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nsv = SVC()\nhf_model(sv,\"SVM\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### e) Decision Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n\ndt = DecisionTreeClassifier()\n\n# criteria for splitting nodes\ncriterion = [\"gini\", \"entropy\"]\n# Maximum number of levels in a tree\nmax_depth = list(range(4,17,4))\n# The minimum number of samples required to be at a leaf node\nmin_samples_leaf=[1,2,5,7]\n# Minimum number of samples required to split an internal node\nmin_samples_split=list(range(2,9,2))\n\n# Hyperparameters dict\nparam_grid = {\"max_depth\":max_depth,\n              \"min_samples_split\":min_samples_split,\n              \"min_samples_leaf\":min_samples_leaf,\n              \"criterion\":criterion}\n\n# Performing RFE\nfeatures = top_feat(dt,10)\n\n# Using Stratified K-fold to train the model\nskf = StratifiedKFold(n_splits=5, shuffle = True, random_state = 0)\n\ndt_rs = RandomizedSearchCV(estimator = dt, param_distributions = param_grid,\n                           cv = skf.split(X_train[features],y_train))\n\nhf_model(dt_rs,\"DecisionTree\",features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### g) Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_jobs=-1)\n\n# Number of trees in Random forest\nn_estimators=list(range(400,1000,100))\n# Maximum number of levels in a tree\nmax_depth = list(range(4,13,4))\n# The minimum number of samples required to be at a leaf node\nmin_samples_leaf=[1,2,5,7]\n# Minimum number of samples required to split an internal node\nmin_samples_split=list(range(2,9,2))\n# Number of fearures to be considered at each split\nmax_features=['auto','sqrt']\n\n\n# Hyperparameters dict\nparam_grid = {\"n_estimators\":n_estimators,\n              \"max_depth\":max_depth,\n              \"min_samples_split\":min_samples_split,\n              \"min_samples_leaf\":min_samples_leaf,\n              \"max_features\":max_features}\n# Performing RFE\nfeatures = top_feat(rf,10)\n\n# Using Stratified K-fold to train the model\nskf = StratifiedKFold(n_splits=5, shuffle = True, random_state = 0)\n\nrf_rs = RandomizedSearchCV(estimator = rf, param_distributions = param_grid,\n                           cv = skf.split(X_train[features],y_train))\n\nhf_model(rf_rs,\"RandomForest\",features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### h) Gradient Boosting Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier()\n\n# Rate at which correcting is being made\nlearning_rate = [0.01, 0.1, 0.2,0.3]\n# Number of trees in Gradient boosting\nn_estimators=list(range(400,1000,100))\n# Maximum number of levels in a tree\nmax_depth=list(range(4,13,4))\n# Minimum number of samples required to split an internal node\nmin_samples_split=list(range(2,9,2))\n# Minimum number of samples required to be at a leaf node.\nmin_samples_leaf=[1,2,5,7]\n# Number of fearures to be considered at each split\nmax_features=['auto','sqrt']\n\n# Hyperparameters dict\nparam_grid = {\"learning_rate\":learning_rate,\n              \"n_estimators\":n_estimators,\n              \"max_depth\":max_depth,\n              \"min_samples_split\":min_samples_split,\n              \"min_samples_leaf\":min_samples_leaf,\n              \"max_features\":max_features}\n\n# Performing RFE\nfeatures = top_feat(gb,10)\n\n# Using Stratified K-fold to train the model\nskf = StratifiedKFold(n_splits=5, shuffle = True, random_state = 0)\n\ngb_rs = RandomizedSearchCV(estimator = gb, param_distributions = param_grid,\n                           cv = skf.split(X_train[features],y_train))\n\nhf_model(gb_rs,\"GradientBoosting\",features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_rs.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### i) XGBoost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(n_jobs=-1,learning_rate=0.01, n_estimators=1000, objective='binary:logistic')\n\n# Hyperparameters dict\nparam_grid = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }\n\n# Performing RFE\nfeatures = top_feat(xgb,10)\n\n# Using Stratified K-fold to train the model\nskf = StratifiedKFold(n_splits=5, shuffle = True, random_state = 0)\n\nxgb_rs = RandomizedSearchCV(estimator = xgb, param_distributions = param_grid,\n                           cv = skf.split(X_train[features],y_train))\n\nhf_model(xgb_rs,\"XGboost\",features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### j) LGBM Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\n\nlgb = LGBMClassifier(n_jobs=-1,learning_rate=0.01, n_estimators=1000)\nfeatures = top_feat(lgb,10)\nhf_model(lgb,\"LGBMClassifier\",features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### k) CatBoost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostClassifier\n\ncat = CatBoostClassifier(learning_rate=0.01, n_estimators=1000,verbose=0)\nfeatures = top_feat(cat,10)\nhf_model(cat,\"CatBoostClassifier\",features)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}