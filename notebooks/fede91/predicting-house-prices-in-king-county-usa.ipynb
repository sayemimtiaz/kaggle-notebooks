{"cells":[{"metadata":{},"cell_type":"markdown","source":"# House Prices Prediction\n##### Author: [Federico Sciuca](https://www.linkedin.com/in/federico-sciuca/)\n  \n  \n<img src=\"https://miro.medium.com/max/1400/0*tdIkLF-rCqIbGWkn\" title=\"Photo by Jamie Whiffen on Unsplash\" height=\"680\" width=\"680\">\n\nThis notebook is the second notebook I produce after my introduction to Machine Learning in Python.\nEvery comments and suggestions are welcome!\n\nBut let's start immidiately to code."},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"0\"></span> Contents Table"},{"metadata":{},"cell_type":"markdown","source":"1. [Overview](#1)\n1. [Importing Modules, Reading the Dataset and Defining an Evaluation Table](#2)\n1. [Explore the columns and deal with missing values](#3)\n    * [Handle Missing values: Mean and Frequence](#4)\n1. [Data Visualization](#5)\n1. [Correlation and Features selection](#6)\n1. [Model development](#7)\n    * [Split the dataset in train and test](#8)\n    * [Simple Linear Regression](#9)\n    * [Multiple Linear Regression - Top 5 Features](#10)\n    * [Multiple Linear Regression - All Features](#11)\n    * [Polynomial Regression - Top Feature](#12)\n    * [Multiple Polynomial Regression - Top 5 Features](#13)\n    * [Multiple Polynomial Regression - All Features](#14)\n1. [Regularization](#15)\n    * [Ridge Regression](#16)  \n        * [Ridge Regression - Best Feature](#17)\n        * [Ridge Regression - Top 5 Features](#18)\n        * [Ridge Regression - All Features](#19)\n    * [Lasso Regression](#20)\n        * [Lasso Regression - Best Feature](#21)\n        * [Lasso Regression - Top 5 Features](#22)\n        * [Lasso Regression - All Features](#23)\n1. [Decision Tree Regression](#24)\n    * [Decision Tree Regression - Best Feature](#25)\n    * [Decision Tree Regression - Top 5 Features](#26)\n    * [Decision Tree Regression - All Features](#27)\n1. [Multi-layer Perceptron Regressor](#28)\n1. [Evaluation Table](#29)\n1. [Conclusion](#30)"},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"1\"></span> Overview\n###### [Return Contents](#0)\n\nWelcome to my Kernel. In this kernal I'll *practise* with different **Regression Models** and I'll do my best to predict the house prices by using them.\n\nTo make the Kernal more readable, I'll explain and comment every model I'm going to use.\nMy previous studies are for the most related to the **Simple Linear regression, Multiple Linear Regression, Polynomial Regression.** \n\nIn order to explore and evaluate and learn different methods, I'll use this Kernal as an excuse to study more in-depth the **Scikit Learn Library**.\n"},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"2\"></span> Importing Modules, Reading the Dataset and Defining an Evaluation Table\n###### [Return Contents](#0)\n<hr>"},{"metadata":{},"cell_type":"markdown","source":"The first thing to do is to import all the libraries and data we are going to use to explore the dataset and develop our models.\n</br>\nI'll also define a DataFrame dedicated to the model evaluation. This DataFrame will be very helpful to summarize the models we are going to build in order to identify the best fit for our pourpuse."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno\nimport folium\n%matplotlib inline\n\n# Data and Statistics\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\n# Train and Test Preparation\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\n\n# Preprocessing\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\n\n# Models\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import linear_model\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Evaluation metrics\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import max_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.metrics import median_absolute_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_poisson_deviance\nfrom sklearn.metrics import mean_gamma_deviance\n\ndf = pd.read_csv(\"../input/housesalesprediction/kc_house_data.csv\")\n\nevaluation = pd.DataFrame({'Model': [],\n                           'Details':[],\n                           'Max Error':[],\n                           'Mean Absolute Error' : [],\n                           'Mean Squared Error' : [],\n                           'Mean Squared Log Error' : [],\n                           'Median Absolute Error' : [],\n                           'Mean Poisson Deviance' : [],\n                           'Mean Gamma Deviance': [],\n                           'Root Mean Squared Error (RMSE)':[],\n                           'R-squared (training)':[],\n                           'Adjusted R-squared (training)':[],\n                           'R-squared (test)':[],\n                           'Adjusted R-squared (test)':[],\n                           '12-Fold Cross Validation':[]})\ndef adjustedR2(r2,n,k):\n    return r2-(k-1)/(n-k)*(1-r2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have imported a few modules and libraries we are going to use in this analysis, it's time to import the csv file and explore it!"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The dataset has\", df.shape[0], \"rows and\", df.shape[1], \"features.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the data type we have:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'm not totally sure why some of the data are float instead of normal integer but let's explore more in-depth the columns and values.\n\n> **How does 2.25 bathrooms look like?**\n\nPS: This is not the question we want to answer throught this analysis\n\nLet's see if there are missing values."},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"3\"></span> Explore the columns and deal with missing values\n###### [Return Contents](#0)\n<hr>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot graphic of missing values\nmissingno.matrix(df, figsize = (30,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see from the previous visualization, there are no missing values in the dataset but let's see other ways to check it."},{"metadata":{},"cell_type":"markdown","source":"The fist method is using the isnull() function. This function convert the data in boolean. Taking the sum of them it simply show how many null value (isnull = True = 1) we have in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use isnull() function to convert the missing values in boolean and sum them.\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A similar way to do so is using a loop that print for each column in the dataset the exact number of isnull()=True and isnull()=False we have in each column."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's define a list of columns and the dataset in boolean version\ncol_list = df.columns.to_list()\ndf_isnull = df.isnull()\n\n# Now is time to create a loop that print the informations we are looking for.\nfor col in col_list:\n    print(col)\n    print(df_isnull[col].value_counts())\n    print(\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see all the columns present the same number of values and all of them are isnull()=False.\n</br>\n* **Can we say that there are no missing values?**\n* **Is the missing value always expressed as *NULL* or it can be expressed in other forms?**\n\n</br>\nConsidering that the missing value can be expressed in multiple ways and considering the data types, I'm going to use the pandas function .describe() to be sure there are not unusual maximum or minimum values in te columns such as **9999999** or **0** in some essential columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <span id=\"4\"></span> Handle Missing values: Mean and Frequence\n###### [Return Contents](#0)\n<hr>\n\nIs important to highlight that there are records that present **0** as values for the columns *\"bedrooms\"* and *\"bathrooms\"*.\n  \nIn my opinion is important to analyze these records and take in consideration to drop these rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_2 = df[(df['bedrooms'] == 0) | (df['bathrooms'] == 0)]\ndf_2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 16 rows that present **0** in at least one of the columns \"bedrooms\" and \"bathrooms\".  \n  \nThere are three main types of missing data:\n* **Missing completely at random (MCAR)**\n* **Missing at random (MAR)**\n* **Not missing at random (NMAR)**\n  \n\nConsidering that ***16*** rows out of ***21613*** is just the ***0.0742%*** of the dataset, I think it's better to drop these rows in order to have clean data to use to train the model.\n  \n**BUT**  \n  \nWhich other options do we have to deal with those cells?\nHere a list of options:\n1. Replace the missing values with the ***mean*** in the column\n2. Replace the missing values with the ***most frequent value*** in the column\n3. Imputation of the missing values using ***k-NN***"},{"metadata":{},"cell_type":"markdown","source":"Just for learning porpuse, let's analyze/implement the first two methods."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mean Calculation\nprint(\"The average number of bedrooms is:\" , df['bedrooms'].mean(axis=0))\nprint(\"The average number of bathrooms is\" , df['bathrooms'].mean(axis=0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are facing the same question I had before. How do 3.37 bedrooms look like? And 2.11 bathrooms?\n  \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most Frequent Value\nprint(\"The most frequent number of bedrooms is: \" , df['bedrooms'].value_counts().idxmax())\nprint(\"The most frequent number of bathrooms is\" , df['bathrooms'].value_counts().idxmax())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'm pretty satisfied about the results, even if the half bathroom is quite uncommon in Italy but is more reasonable then the 11% of a bathroom.\n  \nLet's replace the 0s in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most Frequent Value\nfreq_bed = df['bedrooms'].value_counts().idxmax()\nfreq_bath = df['bathrooms'].value_counts().idxmax()\n\n# Replace the values\ndf['bedrooms'].replace(0, freq_bed, inplace=True)\ndf['bathrooms'].replace(0, freq_bed, inplace=True)\n\n# Double check if there are other 0 values\n# df[(df['bedrooms'] == 0) | (df['bathrooms'] == 0)].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Number of bedrooms\n\nI noticed that in the dataset there is a record that shows ***33 bedrooms*** and ***1.75 bathrooms***.\nI use folium to identify the area where this house is located.\nComparing the size of the houses from the satelite and from the street map, I strongly believe that this record is inaccurate."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['bedrooms'] > 12]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = folium.Map(\n    location=[47.6878, -122.331], zoom_start=25, tiles=\"OpenStreetMap\", attr='Mapbox')\nm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So I decided to drop the row directly."},{"metadata":{},"cell_type":"markdown","source":"## Date format\n  \nAnother thing I believe is important to change is the date format. The date would give us insight about the price trend across the years after we have divided the records in clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['date'] =  pd.to_datetime(df['date'], format='%Y%m%dT%H%M%S%f')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the row where the 'badrooms' value is 33\ndf.drop(df[df['bedrooms'] == 33].index, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally let's reset the index!"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the dataset looks clean and ready to be visualized!"},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"5\"></span> Data Visualization\n###### [Return Contents](#0)"},{"metadata":{},"cell_type":"markdown","source":"This is the trickiest part for me. I had been a retoucher for the last two years and I use to dedicate a lot of attention to colours and how the graphic looks like in general.  \n  \nBeing at the start of my journey as Data Scientist I'm weak to build good visualizations using ***matplotlib*** and ***seaborn*** but I'll do my best!"},{"metadata":{},"cell_type":"markdown","source":"<hr>\nFirst of all I want to plot some bar chart to visualize the frequency the number of bedrooms and bathrooms occure in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's define a personal colour palette. This is something that I'm still working on in order to identify my graphic design as Data Scientist.\np_palette = ['#FCBB6D', \"#D8737F\", \"#AB6C82\", \"#685D79\", \"#475C7A\", \"#F18C8E\", \"#F0B7A4\", \"#F1D1B5\", \"#568EA6\", \"#305F72\"]\nd_palette = ['#568EA6']\n\n# Plot a bar chart with the number of bedrooms \nn_bedr = df['bedrooms'].unique()\nplt.figure(figsize = (12, 6))\nsns.barplot(x = n_bedr, y = df['bedrooms'].value_counts(), palette = p_palette, data = df)\nplt.xlabel(\"Number of bedrooms\", fontsize = 14)\nplt.ylabel(\"Count of Houses\", fontsize = 14)\nplt.title(\"Houses - Number of bedrooms distribution\", fontsize = 18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot a bar chart with the number of Bathrooms \n\n# Define a new DataFrame with the number of bathrooms and the frequency\nbath_dic = df['bathrooms'].value_counts()\nbath_df = bath_dic.to_frame()\nbath_df.reset_index(inplace=True)\nbath_df.rename(columns={'index': 'bathrooms', 'bathrooms': 'freq_b'}, inplace=True)\n\n# Plot the bar chart\nplt.figure(figsize = (12, 6))\n\nsns.barplot(x = bath_df['bathrooms'], y = bath_df['freq_b'], palette = p_palette, data = df)\nplt.xlabel(\"Number of bathrooms\", fontsize = 14)\nplt.ylabel(\"Count of Houses\", fontsize = 14)\nplt.title(\"Houses - Number of bathrooms distribution\", fontsize = 18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***How is the price distributed compared to the number of bedrooms?***  \n***How is the price distributed compared to the number of bathrooms?***"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's plot a pairplot to show the different distributions.\n# sns.pairplot(data=df, x_vars=df[['bedrooms', 'bathrooms']], y_vars = df['price'], kind='scatter')\n\nplt.figure(figsize=(26,6))\nsns.set_palette(d_palette)\n\n# First plot - Bathrooms - Price\nplt.subplot(1,2,1)\nsns.scatterplot(x=df['bathrooms'], y=df['price'], data=df, palette=p_palette)\nplt.xlabel('Bathrooms', fontsize=14)\nplt.ylabel('Price', fontsize=14)\nplt.title(\"Price Distribution by bathrooms\", fontsize=18)\n\n# Second plot Bedrooms - Price\nplt.subplot(1,2,2)\nsns.scatterplot(x=df['bedrooms'], y=df['price'], data=df, palette=p_palette)\nplt.xlabel('Bedrooms', fontsize=14)\nplt.ylabel('Price', fontsize=14)\nplt.title('Price Distribution by Bedrooms', fontsize=18)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group by number of bedrooms\ndf_mean_bed = df[['price','bedrooms']].groupby('bedrooms').mean()\n\n# Reset index\ndf_mean_bed.reset_index(inplace=True)\n\n# Calculate the price per bedroom\ndf_mean_bed['rate'] = df_mean_bed['price']/df_mean_bed['bedrooms']\n\n# Define the figure dimensions\nplt.figure(figsize=(26,6))\n\n# First Plot\nplt.subplot(1,2,1)\nsns.barplot(x = df_mean_bed['bedrooms'], y = df_mean_bed['price'], palette = p_palette, data = df_mean_bed)\nplt.xlabel('Number of bedrooms', fontsize = 14)\nplt.ylabel('Price', fontsize = 14)\nplt.title('Average price - bedrooms',fontsize = 18)\n\n# Second Plot\nplt.subplot(1,2,2)\nsns.barplot(x = df_mean_bed['bedrooms'], y = df_mean_bed['rate'], palette = p_palette, data = df_mean_bed)\nplt.xlabel('Number of bedrooms', fontsize = 14)\nplt.ylabel('Price/bedrooms', fontsize = 14)\nplt.title('Price/bedrooms rate',fontsize = 18)\n# ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group by bathrooms\ndf_mean_bath = df[['price','bathrooms']].groupby('bathrooms').mean()\n\n# Reset index\ndf_mean_bath.reset_index(inplace=True)\n\n# Calculate the price per bedrooms\ndf_mean_bath['rate'] = df_mean_bath['price']/df_mean_bath['bathrooms']\n\n# Define the figure dimensions\nplt.figure(figsize=(26,6))\n\n# Third Plot\nplt.subplot(1,2,1)\nsns.barplot(x = df_mean_bath['bathrooms'], y = df_mean_bath['price'], palette = p_palette, data = df_mean_bath)\nplt.xlabel('Number of bathrooms', fontsize = 14)\nplt.ylabel('Price', fontsize = 14)\nplt.title('Average price - bathrooms',fontsize = 18)\n\n# Fourth Plot\nplt.subplot(1,2,2)\nsns.barplot(x = df_mean_bath['bathrooms'], y = df_mean_bath['rate'], palette = p_palette, data = df_mean_bath)\nplt.xlabel('Number of bathrooms', fontsize = 14)\nplt.ylabel('Price/bathrooms', fontsize = 14)\nplt.title('Price/bathrooms rate',fontsize = 18)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I think is interesting to notice that the rate between price and number of bedrooms is descending.  \nThe rate between price and number of bathrooms, instead, is descending between 0 and 2 bathrooms, is almost constant to the minimum of the distribution between 2 and 3 bathrooms and then it is ascending. This behaviour of the market could indicate that to have more then 3 bathroom is considerder unessential and for this reason the price rise.\nWe can try to understand if this trend is verified calculating the price distribution considering bathrooms and bedrooms combined."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the dataset\ndf_bb_comb = df[['price', 'bedrooms', 'bathrooms']]\ndf_bb_comb.reset_index(drop=True, inplace=True)\n\n# Sum of bathrooms and bedrooms\ndf_bb_comb['bath_bed'] = df_bb_comb['bathrooms'] + df_bb_comb['bedrooms']\n# Price rate for number of bathrooms+bedrooms\ndf_bb_comb['bb_rate'] = round((df_bb_comb['price']/df_bb_comb['bath_bed']), 1)\n\n# Rate bathrooms/bedrooms\ndf_bb_comb['bath_bed_rate'] = round((df_bb_comb['bathrooms']/df_bb_comb['bedrooms']), 1)\n# Price rate for bathrooms/bedrooms rate\ndf_bb_comb['price_bath_bed_rate'] = round((df_bb_comb['price']/df_bb_comb['bath_bed_rate']), 1)\n\ndf_bb_comb.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(26,8))\n\nplt.subplot(2,1,1)\nsns.boxplot(x = df_bb_comb['bath_bed'], y = df_bb_comb['bb_rate'], palette = p_palette, data = df)\nplt.xlabel('Number of bathrooms and bedrooms', fontsize = 14)\nplt.ylabel('Price rate', fontsize = 14)\nplt.title('Price rate for bathrooms and bedrooms', fontsize=18)\nplt.subplots_adjust(hspace = 0.5)\n\nplt.subplot(2,1,2)\nsns.boxplot(x = df_bb_comb['bath_bed_rate'], y = df_bb_comb['price_bath_bed_rate'], palette = p_palette, data = df)\nplt.xlabel('Bathrooms/bedrooms rate', fontsize = 14)\nplt.ylabel('Price rate', fontsize = 14)\nplt.title('Price rate for bathrooms per bedrooms rate', fontsize=18)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.date.dt.year\ndf['age'] = df.date.dt.year - df['yr_built']\ndf[['date', 'yr_built', 'yr_renovated', 'age']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How does the age of the building influence the price?"},{"metadata":{"trusted":true},"cell_type":"code","source":"age_bins = [-2,10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 100000] \nlabels = ['10-', '11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100', '100+']\n\ndf['age_binned'] = pd.cut(df['age'], age_bins, labels=labels, include_lowest=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_age_binned = df.groupby(df.age_binned).mean()\ndf_age_binned.reset_index(inplace = True)\n\nplt.figure(figsize=(26,6))\n\nsns.barplot(x = df_age_binned['age_binned'], y = df_age_binned['price'], palette = p_palette, data = df_age_binned)\nplt.xlabel('Age of the building', fontsize = 14)\nplt.ylabel('Average price', fontsize = 14)\nplt.title('Average price per building age', fontsize=18)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We colud go forward to visualize and study the data but for now I'll stop at this point."},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"6\"></span> Correlation and Features Selection\n###### [Return Contents](#0)"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_name = ['price', 'bedrooms', 'bathrooms', 'sqft_living',\n       'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',\n       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',\n       'lat', 'long', 'sqft_living15', 'sqft_lot15']\n\ndf_stand = df[columns_name]\n\nscaler = StandardScaler()\nscaler.fit(df_stand)\ndf_stand = scaler.transform(df_stand)\nprint(scaler)\n\ndf_stand = pd.DataFrame(df_stand,columns = columns_name)\ndf_stand.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df_stand.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(18, 16))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot = True)\nplt.title('Heatmap of correlations', fontsize=18)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_palette(d_palette)\nh = df[columns_name].hist(bins=25,figsize=(26,26), xlabelsize='10', ylabelsize='10')\nsns.despine(left=True, bottom=True)\n[x.title.set_size(14) for x in h.ravel()];\n[x.yaxis.tick_left() for x in h.ravel()];","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p><b>Correlation</b>: a measure of the extent of interdependence between variables.</p>\n\n<p><b>Causation</b>: the relationship between cause and effect between two variables.</p>\n\n<p>It is important to know the difference between these two and that correlation does not imply causation. Determining correlation is much simpler  the determining causation as causation may require independent experimentation.</p>\n\n### Pearson Correlation\n<p>The Pearson Correlation measures the linear dependence between two variables X and Y.</p>\n<p>The resulting coefficient is a value between -1 and 1 inclusive, where:</p>\n<ul>\n    <li><b>1</b>: Total positive linear correlation.</li>\n    <li><b>0</b>: No linear correlation, the two variables most likely do not affect each other.</li>\n    <li><b>-1</b>: Total negative linear correlation.</li>\n</ul>\n\n<p>Pearson Correlation is the default method of the function \"corr\".  Like before we can calculate the Pearson Correlation of the of the 'int64' or 'float64'  variables.</p>\n\n<b>P-value</b>: \n<p>What is this P-value? The P-value is the probability value that the correlation between these two variables is statistically significant. Normally, we choose a significance level of 0.05, which means that we are 95% confident that the correlation between the variables is significant.</p>\n\nBy convention, when the\n<ul>\n    <li>p-value is $<$ 0.001: we say there is strong evidence that the correlation is significant.</li>\n    <li>the p-value is $<$ 0.05: there is moderate evidence that the correlation is significant.</li>\n    <li>the p-value is $<$ 0.1: there is weak evidence that the correlation is significant.</li>\n    <li>the p-value is $>$ 0.1: there is no evidence that the correlation is significant.</li>\n</ul>"},{"metadata":{},"cell_type":"markdown","source":"In order to verify if the correlation is statistically significant I'm going to code a loop that pass through all the columns we need to analyze and print the results."},{"metadata":{"trusted":true},"cell_type":"code","source":"pearson_an = pd.DataFrame(columns=['variable', 'pearson_coef', 'p_value'])\n\nfor col in columns_name[1:]:\n    pearson_coef, p_value = stats.pearsonr(df_stand[col], df_stand['price'])\n    #print(col)\n    #print('The Pearson coefficient is', pearson_coef, 'and the P_value is', p_value)\n    #print('')\n    to_append = pd.Series([col, pearson_coef, p_value], index = pearson_an.columns)\n    pearson_an = pearson_an.append(to_append, ignore_index=True)\n\npearson_an.sort_values(by='pearson_coef', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see there are some features that present high Pearson Coefficients and very low p_values.\nIn general all the p_values confirm that the correlation is statistically significant even if the correlation between the dependent and the independent variable is really low."},{"metadata":{},"cell_type":"markdown","source":"# <span id=\"7\"></span> Model Development\n###### [Return Contents](#0)\n<hr>"},{"metadata":{},"cell_type":"markdown","source":"## <span id=\"8\"></span> Split the dataset in train and test"},{"metadata":{},"cell_type":"markdown","source":"First of all I need to split the dataset and create the train and test.\nI'll use the train_test_split from Scikit-Learn library. I'll also use the random_state = 22 in all my tests."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train_test split using the original dataframe\ntrain_data,test_data = train_test_split(df, train_size = 0.8, random_state = 22)\n\n# Initialize a new LinearRegression model\nlr = linear_model.LinearRegression()\n\n# Identify the X_train and convert it to a Numpy Array\nX_train = np.array(train_data['sqft_living'], dtype=pd.Series).reshape(-1,1)\n\n# Identify the y_train and convert it to a Numpy Array\ny_train = np.array(train_data['price'], dtype=pd.Series)\n\n# Train the model on X_train and y_train\nlr.fit(X_train,y_train)\n\n# Define X_test and y_test\nX_test = np.array(test_data['sqft_living'], dtype=pd.Series).reshape(-1,1)\ny_test = np.array(test_data['price'], dtype=pd.Series)\n\n# Make a prediction on X_test\nYhat = lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\n## <span id=\"9\"></span> Simple Linear Regression\n###### [Return Contents](#0)"},{"metadata":{},"cell_type":"markdown","source":"It's finally time to do some model development!  \n  \nI want to start developing a Simple Linear Regression model using the independent variable ***'sqft_living'*** because it presents the highest correlation with the price."},{"metadata":{},"cell_type":"markdown","source":"Simple Linear Regression is a method to help us understand the relationship between two variables:\n  \n* The predictor/independent variable (X)\n* The response/dependent variable (that we want to predict)(Y)\n  \nThe result of Linear Regression is a linear function that predicts the response (dependent) variable as a function of the predictor (independent) variable.\n\n$$\n Y: Response \\ Variable\\\\\n X: Predictor \\ Variables\n$$\n\n**Linear function:**\n$$\nYhat = a + b  X\n$$\n\n* refers to the intercept of the regression line0, in other words: the value of Y when X is 0\n* refers to the slope of the regression line, in other words: the value with which Y changes when X increases by 1 unit  \n  \nBy convention in machine learning, you'll write the equation for a model slightly differently:\n\n$$\ny' = b + w_1 x_1\n$$\n\nwhere:\n\n* \\\\( y'\\\\) is the predicted label (a desired output).\n* \\\\(b\\\\) is the bias (the y-intercept), sometimes referred to as .\n* \\\\(w_1\\\\) is the weight of feature 1. Weight is the same concept as the \"slope\"  in the traditional equation of a line.\n* \\\\(x_1\\\\) is a feature (a known input).\nTo infer (predict) the temperature  for a new chirps-per-minute value , just substitute the  value into this model.\n\nAlthough this model uses only one feature, a more sophisticated model might rely on multiple features, each having a separate weight (, , etc.). For example, a model that relies on three features might look as follows:  \n  \n$$\ny' = b + w_1x_1 + w_2x_2 + w_3x_3\n$$"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluation Metrics\nmax_err = float(format(max_error(y_test, Yhat),'.3f'))\nmabserr = float(format(mean_absolute_error(y_test, Yhat),'.3f'))\nmsqerr = float(format(mean_squared_error(y_test, Yhat),'.3f'))\nmsqlogerr = float(format(mean_squared_log_error(y_test, Yhat),'.3f'))\nmedabserror = float(format(median_absolute_error(y_test, Yhat),'.3f'))\nmpoisdev = float(format(mean_poisson_deviance(y_test, Yhat),'.3f'))\nmgamdev = float(format(mean_gamma_deviance(y_test, Yhat),'.3f'))\nrmsesm = float(format(np.sqrt(mean_squared_error(y_test, Yhat)),'.3f'))\nrtrsm = float(format(lr.score(X_train, y_train),'.3f'))\nrtesm = float(format(lr.score(X_test, y_test),'.3f'))\ncv = float(format(cross_val_score(lr,df[['sqft_living']],df['price'],cv=12).mean(),'.3f'))\n\nprint (\"Average Price for Test Data: {:.3f}\".format(y_test.mean()))\nprint('Intercept: {}'.format(lr.intercept_))\nprint('Coefficient: {}'.format(lr.coef_))\n\nr = evaluation.shape[0]\n\nevaluation.loc[r] = ['Simple Linear Regression','Best Feature', max_err, mabserr, msqerr, msqlogerr, medabserror,mpoisdev, mgamdev, rmsesm,rtrsm,'-',rtesm,'-',cv]\nevaluation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nplt.scatter(X_test,y_test,color=\"DarkBlue\", label=\"Actual values\", alpha=.1)\nplt.plot(X_test,lr.predict(X_test),color='Coral', label=\"Predicted Regression Line\")\nplt.xlabel(\"Living Space (sqft)\", fontsize=15)\nplt.ylabel(\"Price ($)\", fontsize=15)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.legend()\n\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['top'].set_visible(False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\n## <span id=\"10\"></span> Multiple Linear Regression - Top 5 Features\n###### [Return Contents](#0)"},{"metadata":{},"cell_type":"markdown","source":"We want to predict the house price based on multiple variables.  \n  \nIf we want to use more variables in our model to predict house price, we can use **Multiple Linear Regression**.  \n  \nMultiple Linear Regression is very similar to Simple Linear Regression, but this method is used to explain the relationship between one continuous response (dependent) variable and **two or more** independent variables.  \n  \nMost of the real-world regression models involve multiple predictors. We will illustrate the structure by using four predictor variables, but these results can generalize to any integer:</p>\n\n$$\nY: Response \\ Variable\\\\\nX_1 :Predictor\\ Variable \\ 1\\\\\nX_2: Predictor\\ Variable \\ 2\\\\\nX_3: Predictor\\ Variable \\ 3\\\\\nX_4: Predictor\\ Variable \\ 4\\\\\n$$\n\n$$\na: intercept\\\\\nb_1 :coefficients \\ of\\ Variable \\ 1\\\\\nb_2: coefficients \\ of\\ Variable \\ 2\\\\\nb_3: coefficients \\ of\\ Variable \\ 3\\\\\nb_4: coefficients \\ of\\ Variable \\ 4\\\\\n$$\n\nThe equation is given by\n\n$$\nYhat = a + b_1 X_1 + b_2 X_2 + b_3 X_3 + b_4 X_4\n$$  \n  \nWe are going to use the top 5 independent variables selected by correlation:  \n\n|  Variable| Pearson Coefficient |\n|------|------|\n|sqft_living  | 0.702035   |\n|   grade  | 0.667434|\n|   sqft_above  | 0.605567|\n|   sqft_living15  | 0.585379|\n|   bathrooms  | 0.525138|"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We have train_data,test_data that include all the columns of our dataset.\ntop_5 = ['sqft_living', 'grade', 'sqft_above', 'sqft_living15', 'bathrooms']\n# Select X_train and X_test\nX_train = train_data[['sqft_living', 'grade', 'sqft_above', 'sqft_living15', 'bathrooms']]\nX_test = test_data[['sqft_living', 'grade', 'sqft_above', 'sqft_living15', 'bathrooms']]\n\n# Select y_train and y_test\ny_train = train_data[['price']]\ny_test = test_data[['price']]\n\n# Initialize a LinearRegression model and fit it with the train data\nmlr = linear_model.LinearRegression().fit(X_train, y_train)\n\n# Make a prediction\nYhat = mlr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluation Metrics\nmax_err = float(format(max_error(y_test, Yhat),'.3f'))\nmabserr = float(format(mean_absolute_error(y_test, Yhat),'.3f'))\nmsqerr = float(format(mean_squared_error(y_test, Yhat),'.3f'))\n#msqlogerr = float(format(mean_squared_log_error(y_test, Yhat),'.3f'))\nmedabserror = float(format(median_absolute_error(y_test, Yhat),'.3f'))\n#mpoisdev = float(format(mean_poisson_deviance(y_test, Yhat),'.3f'))\n#mgamdev = float(format(mean_gamma_deviance(y_test, Yhat),'.3f'))\nrmsesm = float(format(np.sqrt(mean_squared_error(y_test, Yhat)),'.3f'))\nrtrsm = float(format(mlr.score(train_data[['sqft_living', 'grade', 'sqft_above', 'sqft_living15', 'bathrooms']],train_data['price']),'.3f'))\nartrcm = float(format\n               (adjustedR2\n                (mlr.score\n                 (train_data[['sqft_living', 'grade', 'sqft_above', 'sqft_living15', 'bathrooms']],\n                  train_data['price']),train_data.shape[0],\n                 len(['sqft_living', 'grade', 'sqft_above', 'sqft_living15', 'bathrooms'])\n                ),'.3f')\n              )\nrtesm = float(format(mlr.score(X_test, y_test),'.3f'))\nartecm = float(format\n               (adjustedR2\n                (mlr.score\n                 (test_data[['sqft_living', 'grade', 'sqft_above', 'sqft_living15', 'bathrooms']],\n                  test_data['price']),\n                 test_data.shape[0],\n                 len(['sqft_living', 'grade', 'sqft_above', 'sqft_living15', 'bathrooms'])\n                ),'.3f')\n              )\ncv = float(format(cross_val_score(mlr,df[['sqft_living', 'grade', 'sqft_above', 'sqft_living15', 'bathrooms']],df['price'],cv=12).mean(),'.3f'))\n\nprint (\"Average Price for Test Data:\", y_test.mean())\nprint('Intercept: {}'.format(lr.intercept_))\nprint('Coefficient: {}'.format(lr.coef_))\n\nr = evaluation.shape[0]\n\nevaluation.loc[r] = ['Multiple Linear Regression','Top 5 Features by Pearson_coef', max_err, mabserr, msqerr, '-', medabserror,'-', '-', rmsesm,rtrsm,artrcm,rtesm,artecm,cv]\nevaluation.sort_values(by = '12-Fold Cross Validation', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Multiple Linear Regression works better then the Simple Linear Regrassion but there are rooms for improvement.  \n  \nWe can visualise the results plotting the distribution of Yhat and y_test.  \nThis, of course, is not a prove of accuracy but is still interesting to visualise."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (26,6))\n\nax1 = sns.distplot(y_test, label = 'Actual values', color = 'DarkBlue', hist=False, bins=50)\nsns.distplot(Yhat, color='Orange', label = 'Predicted values', hist=False, bins=50, ax=ax1)\nplt.xlabel('Price distribution', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.title('Yhat and y_test distribution comparison - Multiple Linear Regression - Top 5 Features', fontsize=18)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\n## <span id=\"11\"></span> Multiple Linear Regression - All Features\n###### [Return Contents](#0)"},{"metadata":{},"cell_type":"markdown","source":"Now, let's see how much the model can improve the prediction including all the features we have previously selected."},{"metadata":{"trusted":true},"cell_type":"code","source":"# columns_name\nall_features = columns_name[1:]\n\n# Define X_train and X_test\nX_train = train_data[all_features]\nX_test = test_data[all_features]\n\n# Define y_train and y_test\ny_train = train_data['price']\ny_test = test_data['price']\n\n# Initiate a LinearRegression Model and Train it\naflrm = linear_model.LinearRegression().fit(X_train, y_train)\n\n# Make a prediction\nYhat = aflrm.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluation Metrics\nmax_err = float(format(max_error(y_test, Yhat),'.3f'))\nmabserr = float(format(mean_absolute_error(y_test, Yhat),'.3f'))\nmsqerr = float(format(mean_squared_error(y_test, Yhat),'.3f'))\n#msqlogerr = float(format(mean_squared_log_error(y_test, Yhat),'.3f'))\nmedabserror = float(format(median_absolute_error(y_test, Yhat),'.3f'))\n#mpoisdev = float(format(mean_poisson_deviance(y_test, Yhat),'.3f'))\n#mgamdev = float(format(mean_gamma_deviance(y_test, Yhat),'.3f'))\nrmsesm = float(format(np.sqrt(mean_squared_error(y_test, Yhat)),'.3f'))\nrtrsm = float(format(aflrm.score(train_data[all_features],train_data['price']),'.3f'))\nartrcm = float(format\n               (adjustedR2\n                (aflrm.score\n                 (train_data[all_features],\n                  train_data['price']),train_data.shape[0],\n                 len(all_features)\n                ),'.3f')\n              )\nrtesm = float(format(aflrm.score(X_test, y_test),'.3f'))\nartecm = float(format\n               (adjustedR2\n                (aflrm.score\n                 (test_data[all_features],\n                  test_data['price']),\n                 test_data.shape[0],\n                 len(all_features)\n                ),'.3f')\n              )\ncv = float(format(cross_val_score(aflrm,df[all_features],df['price'],cv=12).mean(),'.3f'))\n\nprint (\"Average Price for Test Data:\", y_test.mean())\nprint('Intercept: {}'.format(lr.intercept_))\nprint('Coefficient: {}'.format(lr.coef_))\n\nr = evaluation.shape[0]\n\nevaluation.loc[r] = ['Multiple Linear Regression','All Features from Pearson_coef table', max_err, mabserr, msqerr, '-', medabserror,'-', '-', rmsesm,rtrsm,artrcm,rtesm,artecm,cv]\nevaluation.sort_values(by = '12-Fold Cross Validation', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, including all the valuable features we have in the dataset, we can increase the accurecy a lot."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (26,6))\n\nax1 = sns.distplot(y_test, label = 'Actual values', color = 'DarkBlue', hist=False, bins=50)\nsns.distplot(Yhat, color='Orange', label = 'Predicted values', hist=False, bins=50, ax=ax1)\nplt.xlabel('Price distribution', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.title('Yhat and y_test distribution comparison - Multiple Linear Regression - All Features', fontsize=18)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comparing the Yhat distribution, this time, we can see that "},{"metadata":{},"cell_type":"markdown","source":"<hr>\n## <span id=\"12\"></span> Polynomial Regression - Top Feature\n###### [Return Contents](#0)"},{"metadata":{},"cell_type":"markdown","source":"Polynomial regression is a particular case of the general linear regression model or multiple linear regression models.\n\nWe get non-linear relationships by squaring or setting higher-order terms of the predictor variables.\n\nThere are different orders of polynomial regression:  \n  \n**Quadratic - Second Order**\n  \n$$\nY' = a + b_1x + b_2x^2\n$$\n  \n**Cubic - 3rd order  **\n  \n$$\nY' = a + b_1x + b_2x^2 + b_3x^3\n$$  \n  \n**Higher order:  **\n$$\nY'= a + b_1x + b_2x^2 + b_3x^3 + b_4x^4...\n$$\n  \nWe saw earlier that a linear model did not provide the best fit while using sqft_living as the predictor variable. Let's see if we can try fitting a polynomial model to the data instead."},{"metadata":{},"cell_type":"markdown","source":"Let's start defining the X_train, y_train, X_test, y_test as usual and a pipeline to process the data, define the model we are going to use and finally to train the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define X_train, y_train, X_test, y_test\nX_train = train_data[['sqft_living']]\ny_train = train_data['price']\nX_test = test_data[['sqft_living']]\ny_test = test_data['price']\n\n# Define the pipeline input\nInput = [('standardscaler', StandardScaler()), ('polynomial', PolynomialFeatures(degree=2, include_bias=False)), ('model', linear_model.LinearRegression())]\n\n# Prepare the pipeline\npipe = Pipeline(Input)\n\n# Fit the pipeline\npipe.fit(X_train, y_train)\n\n# Make a prediction\nYhat = pipe.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluation Metrics\nmax_err = float(format(max_error(y_test, Yhat),'.3f'))\nmabserr = float(format(mean_absolute_error(y_test, Yhat),'.3f'))\nmsqerr = float(format(mean_squared_error(y_test, Yhat),'.3f'))\nmsqlogerr = float(format(mean_squared_log_error(y_test, Yhat),'.3f'))\nmedabserror = float(format(median_absolute_error(y_test, Yhat),'.3f'))\nmpoisdev = float(format(mean_poisson_deviance(y_test, Yhat),'.3f'))\nmgamdev = float(format(mean_gamma_deviance(y_test, Yhat),'.3f'))\nrmsesm = float(format(np.sqrt(mean_squared_error(y_test, Yhat)),'.3f'))\nrtrsm = float(format(pipe.score(train_data[['sqft_living']],train_data['price']),'.3f'))\nartrcm = float(format\n               (adjustedR2\n                (pipe.score\n                 (train_data[['sqft_living']],\n                  train_data['price']),train_data.shape[0],\n                 len(['sqft_living'])\n                ),'.3f')\n              )\nrtesm = float(format(pipe.score(X_test, y_test),'.3f'))\nartecm = float(format\n               (adjustedR2\n                (pipe.score\n                 (test_data[['sqft_living']],\n                  test_data['price']),\n                 test_data.shape[0],\n                 len(['sqft_living'])\n                ),'.3f')\n              )\ncv = float(format(cross_val_score(pipe,df[['sqft_living']],df['price'],cv=12).mean(),'.3f'))\n\nprint (\"Average Price for Test Data:\", y_test.mean())\nprint('Intercept: {}'.format(lr.intercept_))\nprint('Coefficient: {}'.format(lr.coef_))\n\nr = evaluation.shape[0]\n\nevaluation.loc[r] = ['Polynomial Regression','Best Feature', max_err, mabserr, msqerr, msqlogerr, medabserror,mpoisdev, mgamdev, rmsesm,rtrsm,artrcm,rtesm,artecm,cv]\nevaluation.sort_values(by = '12-Fold Cross Validation', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (26,6))\n\nax1 = sns.distplot(y_test, label = 'Actual values', color = 'DarkBlue', hist=False, bins=50)\nsns.distplot(Yhat, color='Orange', label = 'Predicted values', hist=False, bins=50, ax=ax1)\nplt.xlabel('Price distribution', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.title('Yhat and y_test distribution comparison - Polynomial Regression - Best Feature', fontsize=18)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\n## <span id=\"13\"></span>Multiple Polynomial Regression - Top 5 Features\n###### [Return Contents](#0)"},{"metadata":{},"cell_type":"markdown","source":"Let's train a Polynomial regression model using the top 5 features from the Pearson Coefficient."},{"metadata":{"trusted":true},"cell_type":"code","source":"# The top 5 features are stored into the top_5 list\n\n# Train and test split\nX_train = train_data[top_5]\ny_train = train_data['price']\nX_test = test_data[top_5]\ny_test = test_data['price']\n\n# Define the pipe's input\nInput = [('scale', StandardScaler()), ('polynomial', PolynomialFeatures(degree = 2, include_bias = False)), ('linearRegression', linear_model.LinearRegression())]\n\n# Define the pipe\npipe = Pipeline(Input)\n\n# Train the model\npipe.fit(X_train, y_train)\n\n# Make a prediction\nYhat = pipe.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluation Metrics\nmax_err = float(format(max_error(y_test, Yhat),'.3f'))\nmabserr = float(format(mean_absolute_error(y_test, Yhat),'.3f'))\nmsqerr = float(format(mean_squared_error(y_test, Yhat),'.3f'))\nmsqlogerr = float(format(mean_squared_log_error(y_test, Yhat),'.3f'))\nmedabserror = float(format(median_absolute_error(y_test, Yhat),'.3f'))\nmpoisdev = float(format(mean_poisson_deviance(y_test, Yhat),'.3f'))\nmgamdev = float(format(mean_gamma_deviance(y_test, Yhat),'.3f'))\nrmsesm = float(format(np.sqrt(mean_squared_error(y_test, Yhat)),'.3f'))\nrtrsm = float(format(pipe.score(train_data[top_5],train_data['price']),'.3f'))\nartrcm = float(format\n               (adjustedR2\n                (pipe.score\n                 (train_data[top_5],\n                  train_data['price']),train_data.shape[0],\n                 len(top_5)\n                ),'.3f')\n              )\nrtesm = float(format(pipe.score(X_test, y_test),'.3f'))\nartecm = float(format\n               (adjustedR2\n                (pipe.score\n                 (test_data[top_5],\n                  test_data['price']),\n                 test_data.shape[0],\n                 len(top_5)\n                ),'.3f')\n              )\ncv = float(format(cross_val_score(pipe,df[top_5],df['price'],cv=12).mean(),'.3f'))\n\nprint (\"Average Price for Test Data:\", y_test.mean())\nprint('Intercept: {}'.format(lr.intercept_))\nprint('Coefficient: {}'.format(lr.coef_))\n\nr = evaluation.shape[0]\n\nevaluation.loc[r] = ['Multivariate Polynomial Regression','Top 5 Features by Pearson_coef', max_err, mabserr, msqerr, msqlogerr, medabserror,mpoisdev, mgamdev, rmsesm,rtrsm,artrcm,rtesm,artecm,cv]\nevaluation.sort_values(by = '12-Fold Cross Validation', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (26,6))\n\nax1 = sns.distplot(y_test, label = 'Actual values', color = 'DarkBlue', hist=False, bins=50)\nsns.distplot(Yhat, color='Orange', label = 'Predicted values', hist=False, bins=50, ax=ax1)\nplt.xlabel('Price distribution', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.title('Yhat and y_test distribution comparison - Multivariate Polynomial Regression - Top 5 Features', fontsize=18)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Even if the Multiple Linear Regression is still a simple model, it still perform better then the other implementations."},{"metadata":{},"cell_type":"markdown","source":"<hr>\n## <span id=\"14\"></span>Multiple Polynomial Regression - All Features\n###### [Return Contents](#0)"},{"metadata":{},"cell_type":"markdown","source":"This will be pretty streight away because it follows exactly the same steps then the previous one but I'm going to train it using a higher number of features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# The variable that contain the list of features is all_features\n\n# X_train, y_train, X_test, y_test\nX_train = train_data[all_features]\ny_train = train_data['price']\nX_test = test_data[all_features]\ny_test = test_data['price']\n\n# Let's define the pipe's input\nInput = [('scaler', StandardScaler()), ('plynomial', PolynomialFeatures(degree=2, include_bias=False)), ('LinearRegression', linear_model.LinearRegression())]\n\n# Initialize the pipeline\npipe = Pipeline(Input)\n\n# Train the pipeline\npipe.fit(X_train, y_train)\n\n# Make a prediction\nYhat = pipe.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluation Metrics\nmax_err = float(format(max_error(y_test, Yhat),'.3f'))\nmabserr = float(format(mean_absolute_error(y_test, Yhat),'.3f'))\nmsqerr = float(format(mean_squared_error(y_test, Yhat),'.3f'))\n#msqlogerr = float(format(mean_squared_log_error(y_test, Yhat),'.3f'))\nmedabserror = float(format(median_absolute_error(y_test, Yhat),'.3f'))\n#mpoisdev = float(format(mean_poisson_deviance(y_test, Yhat),'.3f'))\n#mgamdev = float(format(mean_gamma_deviance(y_test, Yhat),'.3f'))\nrmsesm = float(format(np.sqrt(mean_squared_error(y_test, Yhat)),'.3f'))\nrtrsm = float(format(pipe.score(train_data[all_features],train_data['price']),'.3f'))\nartrcm = float(format\n               (adjustedR2\n                (pipe.score\n                 (train_data[all_features],\n                  train_data['price']),train_data.shape[0],\n                 len(all_features)\n                ),'.3f')\n              )\nrtesm = float(format(pipe.score(X_test, y_test),'.3f'))\nartecm = float(format\n               (adjustedR2\n                (pipe.score\n                 (test_data[all_features],\n                  test_data['price']),\n                 test_data.shape[0],\n                 len(all_features)\n                ),'.3f')\n              )\ncv = float(format(cross_val_score(pipe,df[all_features],df['price'],cv=12).mean(),'.3f'))\n\nprint (\"Average Price for Test Data:\", y_test.mean())\nprint('Intercept: {}'.format(lr.intercept_))\nprint('Coefficient: {}'.format(lr.coef_))\n\nr = evaluation.shape[0]\n\nevaluation.loc[r] = ['Multivariate Polynomial Regression','All Features from Pearson_coef', max_err, mabserr, msqerr, '-', medabserror,'-', '-', rmsesm,rtrsm,artrcm,rtesm,artecm,cv]\nevaluation.sort_values(by = '12-Fold Cross Validation', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (26,6))\n\nax1 = sns.distplot(y_test, label = 'Actual values', color = 'DarkBlue', hist=False, bins=50)\nsns.distplot(Yhat, color='Orange', label = 'Predicted values', hist=False, bins=50, ax=ax1)\nplt.xlabel('Price distribution', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.title('Yhat and y_test distribution comparison - Multivariate Polynomial Regression - All Features', fontsize=18)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we are talking!**  \n  \nThe Multivariate Polynomial Regression improved a lot the 5-Fold Cross Validation score.  \nAnother thing to notice is that the ***Root Mean Squared Error*** is becoming smaller and smaller"},{"metadata":{},"cell_type":"markdown","source":"<hr>\n# <span id=\"16\"></span>Regularization\n###### [Return Contents](#0)"},{"metadata":{},"cell_type":"markdown","source":"This is a form of regression, that constrains/ regularizes or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, so as to avoid the risk of overfitting.\nA simple relation for linear regression looks like this. Here Y represents the learned relation and  represents the coefficient estimates for different variables or predictors(X).\n\n$$\nY  _0 + _1X_1 + _2X_2 + + _pX_p\n$$\n\nThe fitting procedure involves a loss function, known as residual sum of squares or RSS. The coefficients are chosen, such that they minimize this loss function.\n\n$$\nRSS = \\sum_{i=1}^{n} \\left(y_i - _0 - \\sum_{j=1}^{p} _jx_{ij}\\right)^2\n$$\n\nNow, this will adjust the coefficients based on your training data. If there is noise in the training data, then the estimated coefficients wont generalize well to the future data. This is where regularization comes in and shrinks or regularizes these learned estimates towards zero."},{"metadata":{},"cell_type":"markdown","source":"<hr>\n## <span id=\"16\"></span>Ridge Regression\n> Explainations by Prashant Gupta - [Regularization in Machine Learning](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a)\n###### [Return Contents](#0)"},{"metadata":{},"cell_type":"markdown","source":"$$\nRSS = \\sum_{i=1}^{n} \\left(y_i - _0 - \\sum_{j=1}^{p} _jx_{ij}\\right)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2 = RSS + \\lambda \\sum_{j=1}^{p}\\beta_j^2\n$$\n\nAbove image shows ridge regression, where the ***RSS is modified by adding the shrinkage quantity***. Now, the coefficients are estimated by minimizing this function. Here, *** is the tuning parameter that decides how much we want to penalize the flexibility of our model.*** The increase in flexibility of a model is represented by increase in its coefficients, and if we want to minimize the above function, then these coefficients need to be small. This is how the Ridge regression technique prevents coefficients from rising too high. Also, notice that we shrink the estimated association of each variable with the response, except the intercept 0, This intercept is a measure of the mean value of the response when \\\\(x_{i1} = x_{i2} = ... x_{ip} = 0\\\\).  \n  \n*When  = 0, the penalty term has no eect*, and the estimates produced by ridge regression will be equal to least squares. However, as ***, the impact of the shrinkage penalty grows, and the ridge regression coecient estimates will approach zero.*** As can be seen, selecting a good value of  is critical. Cross validation comes in handy for this purpose. The coefficient estimates produced by this method are ***also known as the L2 norm.***  \n  \n***The coefficients that are produced by the standard least squares method are scale equivariant,*** i.e. if we multiply each input by c then the corresponding coefficients are scaled by a factor of 1/c. Therefore, regardless of how the predictor is scaled, the multiplication of predictor and coefficient ($x_j\\beta_j$) remains the same. ***However, this is not the case with ridge regression, and therefore, we need to standardize the predictors or bring the predictors to the same scale before performing ridge regression.*** The formula used to do this is given below.  \n  \n\n$$ \\tilde x_{ij} = \\frac{x_{ij}}{ \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_{ij} - \\bar x_{j} )^2} }  $$ \n  \n  \n  \n    \n### Advantages and Disadvantages Of Ridge Regression\n\n##### Advantages \n* Least squares regression doesnt differentiate important from less-important predictors in a model, so it includes all of them. This leads to overfitting a model and failure to find unique solutions. Ridge regression avoids these problems.\n* Ridge regression works in part because it doesnt require unbiased estimators; while least squares produce unbiased estimates; its variances can be so large that they may be wholly inaccurate.\n* Ridge regression adds just enough bias to make the estimates reasonably reliable approximations to true population values.\n* One important advantage of the ridge regression is that it still performs well, compared to the ordinary least square method in a situation where you have a large multivariate data with the number of predictors (p) larger than the number of observations (n).\n* The ridge estimator is especially good at improving the least-squares estimate when multicollinearity is present.  \n\n##### Disadvantages  \n* Firstly ridge regression includes all the predictors in the final model, unlike the stepwise regression methods which will generally select models that involve a reduced set of variables.\n* A ridge model does not perform feature selection. If a greater interpretation is necessary where we need to reduce the signal in our data to a smaller subset then a lasso model may be preferable.\n* Ridge regression shrinks the coefficients towards zero, but it will not set any of them exactly to zero. The lasso regression is an alternative that overcomes this drawback."},{"metadata":{},"cell_type":"markdown","source":"<hr>\n## <span id=\"17\"></span>Ridge Regression - Best Feature\n###### [Return Contents](#0)"},{"metadata":{},"cell_type":"markdown","source":"Taking in consideration the advantage of this algorithm, I'm going to verify how its accuracy change adding multiple features starting from the best one we have."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define train and test\nX_train = train_data[['sqft_living']]\ny_train = train_data['price']\nX_test = test_data[['sqft_living']]\ny_test = test_data['price']\n\n# Input pipeline\nInput = [('scaler', StandardScaler()), ('Ridge', linear_model.Ridge(alpha = 0.5, fit_intercept = True, random_state = 22))]\n\n# Initialize the Pipeline\npipe = Pipeline(Input)\n\n# Fit the pipeline\npipe.fit(X_train, y_train)\n\n# Make a prediction\nY_hat = pipe.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluation Metrics\nmax_err = float(format(max_error(y_test, Yhat),'.3f'))\nmabserr = float(format(mean_absolute_error(y_test, Yhat),'.3f'))\nmsqerr = float(format(mean_squared_error(y_test, Yhat),'.3f'))\n#msqlogerr = float(format(mean_squared_log_error(y_test, Yhat),'.3f'))\nmedabserror = float(format(median_absolute_error(y_test, Yhat),'.3f'))\n#mpoisdev = float(format(mean_poisson_deviance(y_test, Yhat),'.3f'))\n#mgamdev = float(format(mean_gamma_deviance(y_test, Yhat),'.3f'))\nrmsesm = float(format(np.sqrt(mean_squared_error(y_test, Yhat)),'.3f'))\n#rtrsm = float(format(pipe.score(train_data[all_features],train_data['price']),'.3f'))\nartrcm = float(format\n               (adjustedR2\n                (pipe.score\n                 (train_data[['sqft_living']],\n                  train_data['price']),train_data.shape[0],\n                 len(['sqft_living'])\n                ),'.3f')\n              )\nrtesm = float(format(pipe.score(X_test, y_test),'.3f'))\nartecm = float(format\n               (adjustedR2\n                (pipe.score\n                 (test_data[['sqft_living']],\n                  test_data['price']),\n                 test_data.shape[0],\n                 len(['sqft_living'])\n                ),'.3f')\n              )\ncv = float(format(cross_val_score(pipe,df[['sqft_living']],df['price'],cv=12).mean(),'.3f'))\n\nprint (\"Average Price for Test Data:\", y_test.mean())\nprint('Intercept: {}'.format(lr.intercept_))\nprint('Coefficient: {}'.format(lr.coef_))\n\nr = evaluation.shape[0]\n\nevaluation.loc[r] = ['Ridge Regression','Best Feature', max_err, mabserr, msqerr, '-', medabserror,'-', '-', rmsesm,rtrsm,artrcm,rtesm,artecm,cv]\nevaluation.sort_values(by = '12-Fold Cross Validation', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At the moment the Ridge Regression is the worst model in 12-Folds Cross Validation but I think is interesting to notice that:  \n  \n* The **Mean Absolute Error** is the same of the actual best model\n* The **R-squared (training)** is the same of the actual best model  \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (26,6))\n\nax1 = sns.distplot(y_test, label = 'Actual values', color = 'DarkBlue', hist=False, bins=50)\nsns.distplot(Yhat, color='Orange', label = 'Predicted values', hist=False, bins=50, ax=ax1)\nplt.xlabel('Price distribution', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.title('Yhat and y_test distribution comparison - Ridge Regression - Best Feature', fontsize=18)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\n## <span id=\"18\"></span>Ridge Regression - Top 5 Features\n###### [Return Contents](#0)"},{"metadata":{},"cell_type":"markdown","source":"Exactly how I did for the previous models analysed, I'm going to train the Ridge Regression Model with the top 5 features we selected using the Pearson Coefficient and the P value.  \n  \nThe top 5 features I'm about to use, are the following:\n* sqft_living\n* grade\n* sqft_above\n* sqft_living15\n* bathrooms"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define Train and test\nX_train = train_data[top_5]\ny_train = train_data['price']\nX_test = test_data[top_5]\ny_test = test_data['price']\n\n# Define the Input for the pipeline\nInput = [('scaler', StandardScaler()), ('Ridge_regression', linear_model.Ridge(alpha = 0.5, fit_intercept = True, random_state = 22))]\n\n# Initialize the pipeline\npipe = Pipeline(Input)\n\n# Train the Pipeline\npipe.fit(X_train, y_train)\n\n# Make a prediction\nYhat = pipe.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluation Metrics\nmax_err = float(format(max_error(y_test, Yhat),'.3f'))\nmabserr = float(format(mean_absolute_error(y_test, Yhat),'.3f'))\nmsqerr = float(format(mean_squared_error(y_test, Yhat),'.3f'))\n#msqlogerr = float(format(mean_squared_log_error(y_test, Yhat),'.3f'))\nmedabserror = float(format(median_absolute_error(y_test, Yhat),'.3f'))\n#mpoisdev = float(format(mean_poisson_deviance(y_test, Yhat),'.3f'))\n#mgamdev = float(format(mean_gamma_deviance(y_test, Yhat),'.3f'))\nrmsesm = float(format(np.sqrt(mean_squared_error(y_test, Yhat)),'.3f'))\nrtrsm = float(format(pipe.score(train_data[top_5],train_data['price']),'.3f'))\nartrcm = float(format\n               (adjustedR2\n                (pipe.score\n                 (train_data[top_5],\n                  train_data['price']),train_data.shape[0],\n                 len(top_5)\n                ),'.3f')\n              )\nrtesm = float(format(pipe.score(X_test, y_test),'.3f'))\nartecm = float(format\n               (adjustedR2\n                (pipe.score\n                 (test_data[top_5],\n                  test_data['price']),\n                 test_data.shape[0],\n                 len(top_5)\n                ),'.3f')\n              )\ncv = float(format(cross_val_score(pipe,df[top_5],df['price'],cv=12).mean(),'.3f'))\n\nprint (\"Average Price for Test Data:\", y_test.mean())\nprint('Intercept: {}'.format(lr.intercept_))\nprint('Coefficient: {}'.format(lr.coef_))\n\nr = evaluation.shape[0]\n\nevaluation.loc[r] = ['Ridge Regression','Top 5 Features by Pearson_coef', max_err, mabserr, msqerr, '-', medabserror,'-', '-', rmsesm,rtrsm,artrcm,rtesm,artecm,cv]\nevaluation.sort_values(by = '12-Fold Cross Validation', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's curious! Adding features reduce the R-squared achieved on the training data but improve the 5-Fold Cross Validation of the model!"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (26,6))\n\nax1 = sns.distplot(y_test, label = 'Actual values', color = 'DarkBlue', hist=False, bins=50)\nsns.distplot(Yhat, color='Orange', label = 'Predicted values', hist=False, bins=50, ax=ax1)\nplt.xlabel('Price distribution', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.title('Yhat and y_test distribution comparison - Ridge Regression - Top 5 Features', fontsize=18)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\n## <span id=\"19\"></span>Ridge Regression - All Features\n###### [Return Contents](#0)"},{"metadata":{},"cell_type":"markdown","source":"Now it's time to include all the features we have available and see what happend! I expect to have a signficant increase in 5-Fold Cross Validation score but I'm not confident if it ill be enough to perform better than the Multivariate Polynomial Regression I trained using the Top 5 Features.  \n  \n**Let's see!**  \n  \nThe features I'm going to use are the same we have into the Pearson table:  \n  \n  |  Variable| Pearson Coefficient |\n|------|------|\n|sqft_living  | 0.702035   |\n|   grade  | 0.667434|\n|   sqft_above  | 0.605567|\n|   sqft_living15  | 0.585379|\n|   bathrooms  | 0.525138|\n|   view  | 0.397299|\n|   sqft_basement  | 0.323812|\n|   bedrooms  | 0.316035|\n|   lat  | 0.306998|\n|   waterfront  | 0.266371|\n|   floors  | 0.256811|\n|   yr_renovated  | 0.126437|\n|   sqft_lot  | 0.089664|\n|   sqft_lot15  | 0.082451|\n|   yr_built  | 0.054023|\n|   condition  | 0.036336|\n|   long  | 0.021637|\n|   zipcode  | -0.053209|\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train and test\nX_train = train_data[all_features]\ny_train = train_data['price']\nX_test = test_data[all_features]\ny_test = test_data['price']\n\n# Define the Pipeline Input\nInput = [('scale', StandardScaler()), ('Ridge', linear_model.Ridge(alpha = 0.5, fit_intercept = True, random_state = 22))]\n\n# Initialize the Pipeline\npipe = Pipeline(Input)\n\n# Fit the model\npipe.fit(X_train, y_train)\n\n# Make a prediction\nYhat = pipe.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluation Metrics\nmax_err = float(format(max_error(y_test, Yhat),'.3f'))\nmabserr = float(format(mean_absolute_error(y_test, Yhat),'.3f'))\nmsqerr = float(format(mean_squared_error(y_test, Yhat),'.3f'))\n#msqlogerr = float(format(mean_squared_log_error(y_test, Yhat),'.3f'))\nmedabserror = float(format(median_absolute_error(y_test, Yhat),'.3f'))\n#mpoisdev = float(format(mean_poisson_deviance(y_test, Yhat),'.3f'))\n#mgamdev = float(format(mean_gamma_deviance(y_test, Yhat),'.3f'))\nrmsesm = float(format(np.sqrt(mean_squared_error(y_test, Yhat)),'.3f'))\nrtrsm = float(format(pipe.score(train_data[all_features],train_data['price']),'.3f'))\nartrcm = float(format\n               (adjustedR2\n                (pipe.score\n                 (train_data[all_features],\n                  train_data['price']),train_data.shape[0],\n                 len(all_features)\n                ),'.3f')\n              )\nrtesm = float(format(pipe.score(X_test, y_test),'.3f'))\nartecm = float(format\n               (adjustedR2\n                (pipe.score\n                 (test_data[all_features],\n                  test_data['price']),\n                 test_data.shape[0],\n                 len(all_features)\n                ),'.3f')\n              )\ncv = float(format(cross_val_score(pipe,df[top_5],df['price'],cv=12).mean(),'.3f'))\n\nprint (\"Average Price for Test Data:\", y_test.mean())\nprint('Intercept: {}'.format(lr.intercept_))\nprint('Coefficient: {}'.format(lr.coef_))\n\nr = evaluation.shape[0]\n\nevaluation.loc[r] = ['Ridge Regression','All Features from Pearson_coef', max_err, mabserr, msqerr, '-', medabserror,'-', '-', rmsesm,rtrsm,artrcm,rtesm,artecm,cv]\nevaluation.sort_values(by = '12-Fold Cross Validation', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is a bit surprising to be honest but let's analyze the results!  \n  \nOrdering the table by the 12-Fold Cross Validation with the values rounded to the third decimal rank the Ridge Regression Model that use all the features available, just above the Ridge Regression that use the Top 5 Features.\n\n**But**  \n  \n* The following metrics would rank this approach to the problem as second in the list:\n    * Mean Absolute Error\n    * Mean Squared Error\n    * Median Absolute Error\n    * The R-squared (training).\n    * Adjusted R-squared (training)\n    * R-squared (test)\n    * Adjusted R-suqared (test)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (26,6))\n\nax1 = sns.distplot(y_test, label = 'Actual values', color = 'DarkBlue', hist=False, bins=50)\nsns.distplot(Yhat, color='Orange', label = 'Predicted values', hist=False, bins=50, ax=ax1)\nplt.xlabel('Price distribution', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.title('Yhat and y_test distribution comparison - Ridge Regression - All Features', fontsize=18)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\n## <span id=\"20\"></span>Lasso Regression\n> Explainations by Prashant Gupta - [Regularization in Machine Learning](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a)\n###### [Return Contents](#0)"},{"metadata":{},"cell_type":"markdown","source":"$$ \\sum_{i=1}^{n} \\left(y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^{p}|\\beta_j| = RSS + \\lambda \\sum_{j=1}^{p}|\\beta_{j}| $$  \n \n \nLasso is another variation, in which the above function is minimized. Its clear that ***this variation differs from ridge regression only in penalizing the high coefficients.*** It uses \\\\(|\\beta_j|\\\\) (modulus)instead of squares of \\\\(\\beta\\\\), as its penalty. In statistics, this is ***known as the L1 norm.***  \n  \nLets take a look at above methods with a different perspective. *The ridge regression can be thought of as solving an equation, where summation of squares of coefficients is less than or equal to s. And the Lasso can be thought of as an equation where summation of modulus of coefficients is less than or equal to s.* Here, s is a constant that exists for each value of shrinkage factor \\\\(\\lambda\\\\). ***These equations are also referred to as constraint functions.***  \n  \n***Consider their are 2 parameters in a given problem.*** Then according to above formulation, the ***ridge regression is expressed by \\\\(\\beta_1^2 + \\beta_1^2 \\leq s \\\\).*** This implies that *ridge regression coefficients have the smallest RSS(loss function) for all points that lie within the circle given by \\\\(\\beta_1^2 + \\beta_1^2 \\leq s \\\\).*  \n  \nSimilarly, ***for lasso, the equation becomes, \\\\(|\\beta_1| + |\\beta_2| \\leq s \\\\).*** This implies that *lasso coefficients have the smallest RSS(loss function) for all points that lie within the diamond given by \\\\(|\\beta_1| + |\\beta_2| \\leq s \\\\).*  \n  \nThe image below describes these equations.\n\n<img src=\"https://miro.medium.com/max/1400/1*XC-8tHoMxrO3ogHKylRfRA.png\" title=\"Credit : An Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani\">\n> Credit : An Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani"},{"metadata":{},"cell_type":"markdown","source":"<hr>\n## <span id=\"21\"></span>Lasso Regression - Best Feature\n###### [Return Contents](#0)"},{"metadata":{},"cell_type":"markdown","source":"As usual I want to highlight how the model change in performance when I add multiple features starting from the best features we have, capable to explain the price."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train and test\nX_train = train_data[['sqft_living']]\ny_train = train_data['price']\nX_test = test_data[['sqft_living']]\ny_test = test_data['price']\n\n# Define the Input for the pipeline\nInput = [('scale', StandardScaler()), ('Lasso', linear_model.Lasso(alpha = 0.5, precompute = False, random_state = 22))]\n\n# Initialize the pipeline\npipe = Pipeline(Input)\n\n# Train the model\npipe.fit(X_train, y_train)\n\n# Make a prediction\nYhat = pipe.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluation Metrics\nmax_err = float(format(max_error(y_test, Yhat),'.3f'))\nmabserr = float(format(mean_absolute_error(y_test, Yhat),'.3f'))\nmsqerr = float(format(mean_squared_error(y_test, Yhat),'.3f'))\nmsqlogerr = float(format(mean_squared_log_error(y_test, Yhat),'.3f'))\nmedabserror = float(format(median_absolute_error(y_test, Yhat),'.3f'))\nmpoisdev = float(format(mean_poisson_deviance(y_test, Yhat),'.3f'))\nmgamdev = float(format(mean_gamma_deviance(y_test, Yhat),'.3f'))\nrmsesm = float(format(np.sqrt(mean_squared_error(y_test, Yhat)),'.3f'))\n#rtrsm = float(format(pipe.score(train_data[all_features],train_data['price']),'.3f'))\nartrcm = float(format\n               (adjustedR2\n                (pipe.score\n                 (train_data[['sqft_living']],\n                  train_data['price']),train_data.shape[0],\n                 len(['sqft_living'])\n                ),'.3f')\n              )\nrtesm = float(format(pipe.score(X_test, y_test),'.3f'))\nartecm = float(format\n               (adjustedR2\n                (pipe.score\n                 (test_data[['sqft_living']],\n                  test_data['price']),\n                 test_data.shape[0],\n                 len(['sqft_living'])\n                ),'.3f')\n              )\ncv = float(format(cross_val_score(pipe,df[['sqft_living']],df['price'],cv=12).mean(),'.3f'))\n\nprint (\"Average Price for Test Data:\", y_test.mean())\nprint('Intercept: {}'.format(lr.intercept_))\nprint('Coefficient: {}'.format(lr.coef_))\n\nr = evaluation.shape[0]\n\nevaluation.loc[r] = ['Lasso Regression','Best Feature', max_err, mabserr, msqerr, msqlogerr, medabserror,mpoisdev, mgamdev, rmsesm,rtrsm,artrcm,'-',artecm,cv]\nevaluation.sort_values(by = '12-Fold Cross Validation', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Lasso Regression Model trained on the Best Feature we have, achieves the same score of the Ridge Regression trained using the same independent variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (26,6))\n\nax1 = sns.distplot(y_test, label = 'Actual values', color = 'DarkBlue', hist=False, bins=50)\nsns.distplot(Yhat, color='Orange', label = 'Predicted values', hist=False, bins=50, ax=ax1)\nplt.xlabel('Price distribution', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.title('Yhat and y_test distribution comparison - Lasso Regression - Best Feature', fontsize=18)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\n## <span id=\"22\"></span>Lasso Regression - Top 5 Features\n###### [Return Contents](#0)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train and test\nX_train = train_data[top_5]\ny_train = train_data['price']\nX_test = test_data[top_5]\ny_test = test_data['price']\n\n# Define the Input for the pipeline\nInput = [('scale', StandardScaler()), ('Lasso', linear_model.Lasso(alpha = 0.5, precompute = False, random_state = 22))]\n\n# Initialize the pipeline\npipe = Pipeline(Input)\n\n# Train the model\npipe.fit(X_train, y_train)\n\n# Make a prediction\nYhat = pipe.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluation Metrics\nmax_err = float(format(max_error(y_test, Yhat),'.3f'))\nmabserr = float(format(mean_absolute_error(y_test, Yhat),'.3f'))\nmsqerr = float(format(mean_squared_error(y_test, Yhat),'.3f'))\n#msqlogerr = float(format(mean_squared_log_error(y_test, Yhat),'.3f'))#\nmedabserror = float(format(median_absolute_error(y_test, Yhat),'.3f'))\n#mpoisdev = float(format(mean_poisson_deviance(y_test, Yhat),'.3f'))#\n#mgamdev = float(format(mean_gamma_deviance(y_test, Yhat),'.3f'))#\nrmsesm = float(format(np.sqrt(mean_squared_error(y_test, Yhat)),'.3f'))\nrtrsm = float(format(pipe.score(train_data[top_5],train_data['price']),'.3f'))\nartrcm = float(format\n               (adjustedR2\n                (pipe.score\n                 (train_data[top_5],\n                  train_data['price']),train_data.shape[0],\n                 len(top_5)\n                ),'.3f')\n              )\nrtesm = float(format(pipe.score(X_test, y_test),'.3f'))\nartecm = float(format\n               (adjustedR2\n                (pipe.score\n                 (test_data[top_5],\n                  test_data['price']),\n                 test_data.shape[0],\n                 len(top_5)\n                ),'.3f')\n              )\ncv = float(format(cross_val_score(pipe,df[top_5],df['price'],cv=12).mean(),'.3f'))\n\nprint (\"Average Price for Test Data:\", y_test.mean())\nprint('Intercept: {}'.format(lr.intercept_))\nprint('Coefficient: {}'.format(lr.coef_))\n\nr = evaluation.shape[0]\n\nevaluation.loc[r] = ['Lasso Regression','Top 5 Features by Pearson_coef', max_err, mabserr, msqerr, '-', medabserror,'-', '-', rmsesm,rtrsm,artrcm,rtesm,artecm,cv]\nevaluation.sort_values(by = '12-Fold Cross Validation', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once again, the Lasso Regression Model and the Ridge Regression Model achieve the same results if trained on the same features available in this dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (26,6))\n\nax1 = sns.distplot(y_test, label = 'Actual values', color = 'DarkBlue', hist=False, bins=50)\nsns.distplot(Yhat, color='Orange', label = 'Predicted values', hist=False, bins=50, ax=ax1)\nplt.xlabel('Price distribution', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.title('Yhat and y_test distribution comparison - Lasso Regression - Top 5 Features', fontsize=18)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\n## <span id=\"23\"></span>Lasso Regression - All Features\n###### [Return Contents](#0)"},{"metadata":{},"cell_type":"markdown","source":"Training the Lasso Regression Model on all the features available required in this case to set the alpha = 1 and the maximum number of interaction to 50,000 to converge.  \n  \n* ***Alpha*** : Constant that multiplies the L1 term. Defaults to 1.0. <code> alpha = 0 </code> is equivalent to an ordinary least square, solved by the LinearRegression object. For numerical reasons, using <code> alpha = 0 </code> with the Lasso object is not advised. Given this, you should use the LinearRegression object."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train and test\nX_train = train_data[all_features]\ny_train = train_data['price']\nX_test = test_data[all_features]\ny_test = test_data['price']\n\n# Define the Input for the pipeline\nInput = [('scale', StandardScaler()), ('Lasso', linear_model.Lasso(alpha = 1, precompute = False, max_iter = 50000, random_state = 22))]\n\n# Initialize the pipeline\npipe = Pipeline(Input)\n\n# Train the model\npipe.fit(X_train, y_train)\n\n# Make a prediction\nYhat = pipe.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluation Metrics\nmax_err = float(format(max_error(y_test, Yhat),'.3f'))\nmabserr = float(format(mean_absolute_error(y_test, Yhat),'.3f'))\nmsqerr = float(format(mean_squared_error(y_test, Yhat),'.3f'))\n#msqlogerr = float(format(mean_squared_log_error(y_test, Yhat),'.3f'))\nmedabserror = float(format(median_absolute_error(y_test, Yhat),'.3f'))\n#mpoisdev = float(format(mean_poisson_deviance(y_test, Yhat),'.3f'))\n#mgamdev = float(format(mean_gamma_deviance(y_test, Yhat),'.3f'))\nrmsesm = float(format(np.sqrt(mean_squared_error(y_test, Yhat)),'.3f'))\nrtrsm = float(format(pipe.score(train_data[all_features],train_data['price']),'.3f'))\nartrcm = float(format\n               (adjustedR2\n                (pipe.score\n                 (train_data[all_features],\n                  train_data['price']),train_data.shape[0],\n                 len(all_features)\n                ),'.3f')\n              )\nrtesm = float(format(pipe.score(X_test, y_test),'.3f'))\nartecm = float(format\n               (adjustedR2\n                (pipe.score\n                 (test_data[all_features],\n                  test_data['price']),\n                 test_data.shape[0],\n                 len(all_features)\n                ),'.3f')\n              )\ncv = float(format(cross_val_score(pipe,df[top_5],df['price'],cv=12).mean(),'.3f'))\n\nprint (\"Average Price for Test Data:\", y_test.mean())\nprint('Intercept: {}'.format(lr.intercept_))\nprint('Coefficient: {}'.format(lr.coef_))\n\nr = evaluation.shape[0]\n\nevaluation.loc[r] = ['Lasso Regression','All Features from Pearson_coef', max_err, mabserr, msqerr, '-', medabserror,'-', '-', rmsesm,rtrsm,artrcm,rtesm,artecm,cv]\nevaluation.sort_values(by = '12-Fold Cross Validation', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can do the same consideration we did analysing the results of the Ridge Regression Model trained on all the available features.  \n  \n* The following metrics would rank this approach to the problem as second in the list:\n    * Mean Absolute Error\n    * Mean Squared Error\n    * Median Absolute Error\n    * Root Mean Squared Error\n    * The R-squared (training).\n    * Adjusted R-squared (training)\n    * R-squared (test)\n    * Adjusted R-suqared (test)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (26,6))\n\nax1 = sns.distplot(y_test, label = 'Actual values', color = 'DarkBlue', hist=False, bins=50)\nsns.distplot(Yhat, color='Orange', label = 'Predicted values', hist=False, bins=50, ax=ax1)\nplt.xlabel('Price distribution', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.title('Yhat and y_test distribution comparison - Lasso Regression - All Features', fontsize=18)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\n## <span id=\"24\"></span>Decision Tree Regression\n> Explaination by [Scikit-Learn](https://scikit-learn.org/stable/modules/tree.html#regression)\n###### [Return Contents](#0)"},{"metadata":{},"cell_type":"markdown","source":"***Decision Trees (DTs)*** are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.  \n  \nFor instance, in the example below, decision trees learn from data to approximate a sine curve with a set of if-then-else decision rules. The deeper the tree, the more complex the decision rules and the fitter the model.  \n\n<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_tree_regression_0011.png\" title=\"Decision Tree Regression\">  \n  \n  \n***Some advantages of decision trees are: *** \n\n* Simple to understand and to interpret. **Trees can be visualised.**\n\n* **Requires little data preparation**. Other techniques often require data normalisation, dummy variables need to be created and blank values to be removed. Note however that this module **does not support missing values.**\n\n* The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\n\n* **Able to handle both numerical and categorical data**. Other techniques are usually specialised in analysing datasets that have only one type of variable. See algorithms for more information.\n\n* **Able to handle multi-output problems.**\n\n* Uses a **white box model.** *If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic.* By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.\n\n* **Possible to validate a model using statistical tests**. That makes it possible to account for the reliability of the model.\n\n* Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\n  \n***The disadvantages of decision trees include:***\n  \n* Decision-tree learners **can create over-complex trees that do not generalise the data well**. **This is called overfitting**. Mechanisms such as pruning (not currently supported), setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n\n* Decision trees **can be unstable because small variations in the data might result in a completely different tree being generated**. *This problem is mitigated by using decision trees within an ensemble.*\n\n* *The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts.* Consequently, **practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node.** ***Such algorithms cannot guarantee to return the globally optimal decision tree.*** This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.\n\n* There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n\n* **Decision tree learners create biased trees if some classes dominate.** It is therefore recommended to balance the dataset prior to fitting with the decision tree."},{"metadata":{},"cell_type":"markdown","source":"<hr>\n## <span id=\"25\"></span>Decision Tree Regression - Best Feature\n###### [Return Contents](#0)"},{"metadata":{},"cell_type":"markdown","source":"Let's try to build a Decision Tree Regression Model and to interpret the results knowing that the overfitting is always around the corner!"},{"metadata":{},"cell_type":"markdown","source":"To build the Decision Trees Regressors I decided to iterate identify the best level of depth to use in order to achieve the best result possible.  \nI'm also going to plot a chart that explains the evolution of a few key scores used to evaluate the model.  \n  \nThe process individuate the best level of depth automatically and use it to train the final model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train and test.\nX_train = train_data[['sqft_living']]\ny_train = train_data['price']\nX_test = test_data[['sqft_living']]\ny_test = test_data['price']\n\n# The Decision Tree Regression Model doesn't need data normalisation.\n\ntree_depth = pd.DataFrame({'Model': [],\n                           'Depth':[],\n                           'Max Error':[],\n                           'Mean Absolute Error' : [],\n                           'Mean Squared Error' : [],\n                           'Mean Squared Log Error' : [],\n                           'Median Absolute Error' : [],\n                           'Mean Poisson Deviance' : [],\n                           'Mean Gamma Deviance': [],\n                           'Root Mean Squared Error (RMSE)':[],\n                           'R-squared (training)':[],\n                           'Adjusted R-squared (training)':[],\n                           'R-squared (test)':[],\n                           'Adjusted R-squared (test)':[],\n                           '12-Fold Cross Validation':[]})\n\n# Initialize the model\nfor depth in range(1,20):\n    tree = DecisionTreeRegressor(max_depth = depth)\n\n    # Train the model\n    tree.fit(X_train, y_train)\n\n    # Evaluation Metrics\n    max_err = float(format(max_error(y_test, Yhat),'.3f'))\n    mabserr = float(format(mean_absolute_error(y_test, Yhat),'.3f'))\n    msqerr = float(format(mean_squared_error(y_test, Yhat),'.3f'))\n    #msqlogerr = float(format(mean_squared_log_error(y_test, Yhat),'.3f'))\n    medabserror = float(format(median_absolute_error(y_test, Yhat),'.3f'))\n    #mpoisdev = float(format(mean_poisson_deviance(y_test, Yhat),'.3f'))\n    #mgamdev = float(format(mean_gamma_deviance(y_test, Yhat),'.3f'))\n    rmsesm = float(format(np.sqrt(mean_squared_error(y_test, Yhat)),'.3f'))\n    rtrsm = float(format(tree.score(train_data[['sqft_living']],train_data['price']),'.3f'))\n    artrcm = float(format\n                   (adjustedR2\n                    (tree.score\n                     (train_data[['sqft_living']],\n                      train_data['price']),train_data.shape[0],\n                     len(['sqft_living'])\n                    ),'.3f')\n                  )\n    rtesm = float(format(tree.score(X_test, y_test),'.3f'))\n    artecm = float(format\n                   (adjustedR2\n                    (tree.score\n                     (test_data[['sqft_living']],\n                      test_data['price']),\n                     test_data.shape[0],\n                     len(['sqft_living'])\n                    ),'.3f')\n                  )\n    cv = float(format(cross_val_score(tree,df[top_5],df['price'],cv=12).mean(),'.3f'))\n\n    r = tree_depth.shape[0]\n\n    tree_depth.loc[r] = ['Decision Tree Regression',depth, max_err, mabserr, msqerr, '-', medabserror,'-', '-', rmsesm,rtrsm,artrcm,rtesm,artecm,cv]\n\ntree_depth.sort_values(by = '12-Fold Cross Validation', ascending=False, inplace=True)\ntree_depth.reset_index(drop = True, inplace = True)\ntree_depth.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\n\nmax_depth = int(max(tree_depth['Depth']))\nbest_depth = tree_depth['Depth'][0]\nmax_cv_score = max(tree_depth['12-Fold Cross Validation'])\n\n\nax1 = sns.lineplot(x = tree_depth['Depth'], y = tree_depth['12-Fold Cross Validation'], color = 'Red', label=\"Cross Valudation\")\nsns.lineplot(x = tree_depth['Depth'], y = tree_depth['R-squared (test)'], label='R-squared (test)', color='Green')\nsns.lineplot(x = tree_depth['Depth'], y = tree_depth['R-squared (training)'], label='R-squared (training)', color=\"orange\")\n\nplt.xlabel('Max Depth Level', fontsize = 14)\nplt.ylabel('Evaluation Score', fontsize = 14)\nplt.title('Cross Validation Score per Depth Level', fontsize = 18)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train and test. Does it make sense to trtain a decision tree with one feature only?\nX_train = train_data[['sqft_living']]\ny_train = train_data['price']\nX_test = test_data[['sqft_living']]\ny_test = test_data['price']\n\n# The Decision Tree Regression Model doesn't need data normalisation.\n# Initialize the model\ntree = DecisionTreeRegressor(max_depth = best_depth)\n\n# Train the model\ntree.fit(X_train, y_train)\n\n# Make a prediction\ntree.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluation Metrics\nmax_err = float(format(max_error(y_test, Yhat),'.3f'))\nmabserr = float(format(mean_absolute_error(y_test, Yhat),'.3f'))\nmsqerr = float(format(mean_squared_error(y_test, Yhat),'.3f'))\n#msqlogerr = float(format(mean_squared_log_error(y_test, Yhat),'.3f'))\nmedabserror = float(format(median_absolute_error(y_test, Yhat),'.3f'))\n#mpoisdev = float(format(mean_poisson_deviance(y_test, Yhat),'.3f'))\n#mgamdev = float(format(mean_gamma_deviance(y_test, Yhat),'.3f'))\nrmsesm = float(format(np.sqrt(mean_squared_error(y_test, Yhat)),'.3f'))\nrtrsm = float(format(tree.score(train_data[['sqft_living']],train_data['price']),'.3f'))\nartrcm = float(format\n               (adjustedR2\n                (tree.score\n                 (train_data[['sqft_living']],\n                  train_data['price']),train_data.shape[0],\n                 len(['sqft_living'])\n                ),'.3f')\n              )\nrtesm = float(format(tree.score(X_test, y_test),'.3f'))\nartecm = float(format\n               (adjustedR2\n                (tree.score\n                 (test_data[['sqft_living']],\n                  test_data['price']),\n                 test_data.shape[0],\n                 len(['sqft_living'])\n                ),'.3f')\n              )\ncv = float(format(cross_val_score(tree,df[top_5],df['price'],cv=12).mean(),'.3f'))\n\nprint (\"Average Price for Test Data:\", y_test.mean())\nprint('Intercept: {}'.format(lr.intercept_))\nprint('Coefficient: {}'.format(lr.coef_))\n\nr = evaluation.shape[0]\n\nevaluation.loc[r] = ['Decision Tree Regression','Max Depth = {} Best Feature'.format(best_depth), max_err, mabserr, msqerr, '-', medabserror,'-', '-', rmsesm,rtrsm,artrcm,rtesm,artecm,cv]\nevaluation.sort_values(by = '12-Fold Cross Validation', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the results\nplt.figure(figsize=(16,6))\nplt.scatter(df[['sqft_living']], df[['price']],\n            color=\"DarkBlue\", label=\"Actual Values\", alpha=0.1)\nplt.plot(X_test, Yhat, color=\"Coral\",\n         label=\"Decision Tree\", linewidth=1)\n#plt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=5\", linewidth=2)\nplt.xlabel(\"data\")\nplt.ylabel(\"target\")\nplt.title(\"Decision Tree Regression\")\nplt.legend()\nplt.show()\n\n\n#plt.scatter(X_test,y_test,color=\"DarkBlue\", label=\"Actual values\", alpha=.1)\n#plt.plot(X_test,lr.predict(X_test),color='Coral', label=\"Predicted Regression Line\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\n## <span id=\"26\"></span>Decision Tree Regression - Top 5 Features\n###### [Return Contents](#0)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train and test.\nX_train = train_data[top_5]\ny_train = train_data['price']\nX_test = test_data[top_5]\ny_test = test_data['price']\n\n# The Decision Tree Regression Model doesn't need data normalisation.\n\ntree_depth = pd.DataFrame({'Model': [],\n                           'Depth':[],\n                           'Max Error':[],\n                           'Mean Absolute Error' : [],\n                           'Mean Squared Error' : [],\n                           'Mean Squared Log Error' : [],\n                           'Median Absolute Error' : [],\n                           'Mean Poisson Deviance' : [],\n                           'Mean Gamma Deviance': [],\n                           'Root Mean Squared Error (RMSE)':[],\n                           'R-squared (training)':[],\n                           'Adjusted R-squared (training)':[],\n                           'R-squared (test)':[],\n                           'Adjusted R-squared (test)':[],\n                           '12-Fold Cross Validation':[]})\n\n# Initialize the model\nfor depth in range(1,20):\n    tree = DecisionTreeRegressor(max_depth = depth)\n\n    # Train the model\n    tree.fit(X_train, y_train)\n\n    # Evaluation Metrics\n    max_err = float(format(max_error(y_test, Yhat),'.3f'))\n    mabserr = float(format(mean_absolute_error(y_test, Yhat),'.3f'))\n    msqerr = float(format(mean_squared_error(y_test, Yhat),'.3f'))\n    #msqlogerr = float(format(mean_squared_log_error(y_test, Yhat),'.3f'))\n    medabserror = float(format(median_absolute_error(y_test, Yhat),'.3f'))\n    #mpoisdev = float(format(mean_poisson_deviance(y_test, Yhat),'.3f'))\n    #mgamdev = float(format(mean_gamma_deviance(y_test, Yhat),'.3f'))\n    rmsesm = float(format(np.sqrt(mean_squared_error(y_test, Yhat)),'.3f'))\n    rtrsm = float(format(tree.score(train_data[top_5],train_data['price']),'.3f'))\n    artrcm = float(format\n                   (adjustedR2\n                    (tree.score\n                     (train_data[top_5],\n                      train_data['price']),train_data.shape[0],\n                     len(top_5)\n                    ),'.3f')\n                  )\n    rtesm = float(format(tree.score(X_test, y_test),'.3f'))\n    artecm = float(format\n                   (adjustedR2\n                    (tree.score\n                     (test_data[top_5],\n                      test_data['price']),\n                     test_data.shape[0],\n                     len(top_5)\n                    ),'.3f')\n                  )\n    cv = float(format(cross_val_score(tree,df[top_5],df['price'],cv=12).mean(),'.3f'))\n\n    r = tree_depth.shape[0]\n\n    tree_depth.loc[r] = ['Decision Tree Regression',depth, max_err, mabserr, msqerr, '-', medabserror,'-', '-', rmsesm,rtrsm,artrcm,rtesm,artecm,cv]\n\ntree_depth.sort_values(by = '12-Fold Cross Validation', ascending=False, inplace=True)\ntree_depth.reset_index(drop = True, inplace = True)\ntree_depth.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\n\nmax_depth = int(max(tree_depth['Depth']))\nbest_depth = tree_depth['Depth'][0]\nmax_cv_score = max(tree_depth['12-Fold Cross Validation'])\n\n\nax1 = sns.lineplot(x = tree_depth['Depth'], y = tree_depth['12-Fold Cross Validation'], color = 'Red', label=\"Cross Valudation\")\nsns.lineplot(x = tree_depth['Depth'], y = tree_depth['R-squared (test)'], label='R-squared (test)', color='Green')\nsns.lineplot(x = tree_depth['Depth'], y = tree_depth['R-squared (training)'], label='R-squared (training)', color=\"orange\")\n\nplt.xlabel('Max Depth Level', fontsize = 14)\nplt.ylabel('Evaluation Score', fontsize = 14)\nplt.title('Cross Validation Score per Depth Level', fontsize = 18)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train and test. Does it make sense to trtain a decision tree with one feature only?\nX_train = train_data[top_5]\ny_train = train_data['price']\nX_test = test_data[top_5]\ny_test = test_data['price']\n\n# The Decision Tree Regression Model doesn't need data normalisation.\n# Initialize the model\ntree = DecisionTreeRegressor(max_depth = best_depth)\n\n# Train the model\ntree.fit(X_train, y_train)\n\n# Make a prediction\ntree.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluation Metrics\nmax_err = float(format(max_error(y_test, Yhat),'.3f'))\nmabserr = float(format(mean_absolute_error(y_test, Yhat),'.3f'))\nmsqerr = float(format(mean_squared_error(y_test, Yhat),'.3f'))\n#msqlogerr = float(format(mean_squared_log_error(y_test, Yhat),'.3f'))\nmedabserror = float(format(median_absolute_error(y_test, Yhat),'.3f'))\n#mpoisdev = float(format(mean_poisson_deviance(y_test, Yhat),'.3f'))\n#mgamdev = float(format(mean_gamma_deviance(y_test, Yhat),'.3f'))\nrmsesm = float(format(np.sqrt(mean_squared_error(y_test, Yhat)),'.3f'))\nrtrsm = float(format(tree.score(train_data[top_5],train_data['price']),'.3f'))\nartrcm = float(format\n               (adjustedR2\n                (tree.score\n                 (train_data[top_5],\n                  train_data['price']),train_data.shape[0],\n                 len(top_5)\n                ),'.3f')\n              )\nrtesm = float(format(tree.score(X_test, y_test),'.3f'))\nartecm = float(format\n               (adjustedR2\n                (tree.score\n                 (test_data[top_5],\n                  test_data['price']),\n                 test_data.shape[0],\n                 len(top_5)\n                ),'.3f')\n              )\ncv = float(format(cross_val_score(tree,df[top_5],df['price'],cv=12).mean(),'.3f'))\n\nprint (\"Average Price for Test Data:\", y_test.mean())\nprint('Intercept: {}'.format(lr.intercept_))\nprint('Coefficient: {}'.format(lr.coef_))\n\nr = evaluation.shape[0]\n\nevaluation.loc[r] = ['Decision Tree Regression','Depth = {} - Top 5 Features by Pearson_coef'.format(best_depth), max_err, mabserr, msqerr, '-', medabserror,'-', '-', rmsesm,rtrsm,artrcm,rtesm,artecm,cv]\nevaluation.sort_values(by = '12-Fold Cross Validation', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (26,6))\n\nax1 = sns.distplot(y_test, label = 'Actual values', color = 'DarkBlue', hist=False, bins=50)\nsns.distplot(Yhat, color='Orange', label = 'Predicted values', hist=False, bins=50, ax=ax1)\nplt.xlabel('Price distribution', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.title('Yhat and y_test distribution comparison - Decision Tree Regression - Top 5 Features', fontsize=18)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\n## <span id=\"27\"></span>Decision Tree Regression - All Features\n###### [Return Contents](#0)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train and test.\nX_train = train_data[all_features]\ny_train = train_data['price']\nX_test = test_data[all_features]\ny_test = test_data['price']\n\n# The Decision Tree Regression Model doesn't need data normalisation.\n\ntree_depth = pd.DataFrame({'Model': [],\n                           'Depth':[],\n                           'Max Error':[],\n                           'Mean Absolute Error' : [],\n                           'Mean Squared Error' : [],\n                           'Mean Squared Log Error' : [],\n                           'Median Absolute Error' : [],\n                           'Mean Poisson Deviance' : [],\n                           'Mean Gamma Deviance': [],\n                           'Root Mean Squared Error (RMSE)':[],\n                           'R-squared (training)':[],\n                           'Adjusted R-squared (training)':[],\n                           'R-squared (test)':[],\n                           'Adjusted R-squared (test)':[],\n                           '12-Fold Cross Validation':[]})\n\n# Initialize the model\nfor depth in range(1,20):\n    tree = DecisionTreeRegressor(max_depth = depth)\n\n    # Train the model\n    tree.fit(X_train, y_train)\n\n    # Evaluation Metrics\n    max_err = float(format(max_error(y_test, Yhat),'.3f'))\n    mabserr = float(format(mean_absolute_error(y_test, Yhat),'.3f'))\n    msqerr = float(format(mean_squared_error(y_test, Yhat),'.3f'))\n    #msqlogerr = float(format(mean_squared_log_error(y_test, Yhat),'.3f'))\n    medabserror = float(format(median_absolute_error(y_test, Yhat),'.3f'))\n    #mpoisdev = float(format(mean_poisson_deviance(y_test, Yhat),'.3f'))\n    #mgamdev = float(format(mean_gamma_deviance(y_test, Yhat),'.3f'))\n    rmsesm = float(format(np.sqrt(mean_squared_error(y_test, Yhat)),'.3f'))\n    rtrsm = float(format(tree.score(train_data[all_features],train_data['price']),'.3f'))\n    artrcm = float(format\n                   (adjustedR2\n                    (tree.score\n                     (train_data[all_features],\n                      train_data['price']),train_data.shape[0],\n                     len(all_features)\n                    ),'.3f')\n                  )\n    rtesm = float(format(tree.score(X_test, y_test),'.3f'))\n    artecm = float(format\n                   (adjustedR2\n                    (tree.score\n                     (test_data[all_features],\n                      test_data['price']),\n                     test_data.shape[0],\n                     len(all_features)\n                    ),'.3f')\n                  )\n    cv = float(format(cross_val_score(tree,df[all_features],df['price'],cv=12).mean(),'.3f'))\n\n    r = tree_depth.shape[0]\n\n    tree_depth.loc[r] = ['Decision Tree Regression',depth, max_err, mabserr, msqerr, '-', medabserror,'-', '-', rmsesm,rtrsm,artrcm,rtesm,artecm,cv]\n\ntree_depth.sort_values(by = '12-Fold Cross Validation', ascending=False, inplace=True)\ntree_depth.reset_index(drop = True, inplace = True)\ntree_depth.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\n\nmax_depth = int(max(tree_depth['Depth']))\nbest_depth = tree_depth['Depth'][0]\nmax_cv_score = max(tree_depth['12-Fold Cross Validation'])\n\n\nax1 = sns.lineplot(x = tree_depth['Depth'], y = tree_depth['12-Fold Cross Validation'], color = 'Red', label=\"Cross Valudation\")\nsns.lineplot(x = tree_depth['Depth'], y = tree_depth['R-squared (test)'], label='R-squared (test)', color='Green')\nsns.lineplot(x = tree_depth['Depth'], y = tree_depth['R-squared (training)'], label='R-squared (training)', color=\"orange\")\n\nplt.xlabel('Max Depth Level', fontsize = 14)\nplt.ylabel('Evaluation Score', fontsize = 14)\nplt.title('Cross Validation Score per Depth Level', fontsize = 18)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train and test. Does it make sense to trtain a decision tree with one feature only?\nX_train = train_data[all_features]\ny_train = train_data['price']\nX_test = test_data[all_features]\ny_test = test_data['price']\n\n# The Decision Tree Regression Model doesn't need data normalisation.\n# Initialize the model\ntree = DecisionTreeRegressor(max_depth = best_depth)\n\n# Train the model\ntree.fit(X_train, y_train)\n\n# Make a prediction\ntree.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluation Metrics\nmax_err = float(format(max_error(y_test, Yhat),'.3f'))\nmabserr = float(format(mean_absolute_error(y_test, Yhat),'.3f'))\nmsqerr = float(format(mean_squared_error(y_test, Yhat),'.3f'))\n#msqlogerr = float(format(mean_squared_log_error(y_test, Yhat),'.3f'))\nmedabserror = float(format(median_absolute_error(y_test, Yhat),'.3f'))\n#mpoisdev = float(format(mean_poisson_deviance(y_test, Yhat),'.3f'))\n#mgamdev = float(format(mean_gamma_deviance(y_test, Yhat),'.3f'))\nrmsesm = float(format(np.sqrt(mean_squared_error(y_test, Yhat)),'.3f'))\nrtrsm = float(format(tree.score(train_data[all_features],train_data['price']),'.3f'))\nartrcm = float(format\n               (adjustedR2\n                (tree.score\n                 (train_data[all_features],\n                  train_data['price']),train_data.shape[0],\n                 len(all_features)\n                ),'.3f')\n              )\nrtesm = float(format(tree.score(X_test, y_test),'.3f'))\nartecm = float(format\n               (adjustedR2\n                (tree.score\n                 (test_data[all_features],\n                  test_data['price']),\n                 test_data.shape[0],\n                 len(all_features)\n                ),'.3f')\n              )\ncv = float(format(cross_val_score(tree,df[all_features],df['price'],cv=12).mean(),'.3f'))\n\nprint (\"Average Price for Test Data:\", y_test.mean())\nprint('Intercept: {}'.format(lr.intercept_))\nprint('Coefficient: {}'.format(lr.coef_))\n\nr = evaluation.shape[0]\n\nevaluation.loc[r] = ['Decision Tree Regression','Depth = {} - All Features from Pearson_coef'.format(best_depth), max_err, mabserr, msqerr, '-', medabserror,'-', '-', rmsesm,rtrsm,artrcm,rtesm,artecm,cv]\nevaluation.sort_values(by = '12-Fold Cross Validation', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (26,6))\n\nax1 = sns.distplot(y_test, label = 'Actual values', color = 'DarkBlue', hist=False, bins=50)\nsns.distplot(Yhat, color='Orange', label = 'Predicted values', hist=False, bins=50, ax=ax1)\nplt.xlabel('Price distribution', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.title('Yhat and y_test distribution comparison - Decision Tree Regression - All Features', fontsize=18)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\n## <span id=\"28\"></span>Multi-layer Perceptron Regressor\n###### [Return Contents](#0)"},{"metadata":{},"cell_type":"markdown","source":"The Multi-layer Perceptron Regressor is a class of feedforward Artificial Neural Network (ANN) that consist of at least 3 layers of nodes:\n * An input layer\n * A hidden layer\n * An output layer\n  \nExcept for the input nodes, each node is a neuron that uses a non linear activation function. This model optimizes the squared-loss using LBFGS or stochastic gradient descent."},{"metadata":{"trusted":true},"cell_type":"code","source":"# train and test\nX_train = train_data[all_features]\ny_train = train_data['price']\nX_test = test_data[all_features]\ny_test = test_data['price']\n\n# Define a pipeline\nInput = [('scaler', StandardScaler()), ('MLPR', MLPRegressor(activation = 'tanh',\n                                                            solver='sgd',\n                                                            learning_rate = 'adaptive',\n                                                            max_iter = 2000))]\npipe = Pipeline(Input)\n\n# Train the model\npipe.fit(X_train, y_train)\n\n# Make a prediction\nYhat = pipe.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluation Metrics\nmax_err = float(format(max_error(y_test, Yhat),'.3f'))\nmabserr = float(format(mean_absolute_error(y_test, Yhat),'.3f'))\nmsqerr = float(format(mean_squared_error(y_test, Yhat),'.3f'))\n#msqlogerr = float(format(mean_squared_log_error(y_test, Yhat),'.3f'))\nmedabserror = float(format(median_absolute_error(y_test, Yhat),'.3f'))\n#mpoisdev = float(format(mean_poisson_deviance(y_test, Yhat),'.3f'))\n#mgamdev = float(format(mean_gamma_deviance(y_test, Yhat),'.3f'))\nrmsesm = float(format(np.sqrt(mean_squared_error(y_test, Yhat)),'.3f'))\nrtrsm = float(format(pipe.score(train_data[all_features],train_data['price']),'.3f'))\nartrcm = float(format\n               (adjustedR2\n                (pipe.score\n                 (train_data[all_features],\n                  train_data['price']),train_data.shape[0],\n                 len(all_features)\n                ),'.3f')\n              )\nrtesm = float(format(pipe.score(X_test, y_test),'.3f'))\nartecm = float(format\n               (adjustedR2\n                (pipe.score\n                 (test_data[all_features],\n                  test_data['price']),\n                 test_data.shape[0],\n                 len(all_features)\n                ),'.3f')\n              )\ncv = float(format(cross_val_score(pipe,df[all_features],df['price'],cv=12).mean(),'.3f'))\n\nprint (\"Average Price for Test Data:\", y_test.mean())\nprint('Intercept: {}'.format(lr.intercept_))\nprint('Coefficient: {}'.format(lr.coef_))\n\nr = evaluation.shape[0]\n\nevaluation.loc[r] = ['Multi_layer Perceptron Regressor','All Features from Pearson_coef'.format(best_depth), max_err, mabserr, msqerr, '-', medabserror,'-', '-', rmsesm,rtrsm,artrcm,rtesm,artecm,cv]\nevaluation.sort_values(by = '12-Fold Cross Validation', ascending=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (26,6))\n\nax1 = sns.distplot(y_test, label = 'Actual values', color = 'DarkBlue', hist=False, bins=50)\nsns.distplot(Yhat, color='Orange', label = 'Predicted values', hist=False, bins=50, ax=ax1)\nplt.xlabel('Price distribution', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.title('Yhat and y_test distribution comparison - Decision Tree Regression - All Features', fontsize=18)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\n## <span id=\"29\"></span>Evauation Table\n###### [Return Contents](#0)"},{"metadata":{},"cell_type":"markdown","source":"It's finally time to select the best model!  \n  \nAs we can see from the table below, the Multivariate Polynomial Regression works pretty well considering not only the 12-Fold Cross Validation but also the other metrics we are considering in the table.  \n  \nThe second place go to the Multi-layer Perceptron Regressor in terms of 12-Fold Cross Validation score but I think is important to notice that the other metrics are not that good.  \nOne example of this is 3.93 times bigger than the Max Error recorded by the Multivariate Polynomial Regression.\n\nAnyway, I'm wondering what's the score the Multivariate Polynomial Regression can achieved after a bit of work on the parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation.sort_values(by = '12-Fold Cross Validation', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\n## <span id=\"30\"></span>Conclusion\n###### [Return Contents](#0)"},{"metadata":{},"cell_type":"markdown","source":"When we look at the evaluation table, 2nd degree polynomial with all the features is the best. \n  \nStudying other public Kernals on this dataset, I noticed that other Data Scientist used other method or they suggest for example a Polynomial Ridge Regression but my results with that model are pretty bad.\n\nAnyway!\n  \nI hope you enjoyed my Kernal and <font color=\"green\">if you liked it, please do not forget to UPVOTE </font>\n\nAnd ***Keep Coding!***\n\n<img src=\"https://media.giphy.com/media/PiQejEf31116URju4V/giphy.gif\">"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}