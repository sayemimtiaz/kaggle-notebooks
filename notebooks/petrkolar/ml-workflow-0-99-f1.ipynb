{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Company Bankrupcy Prediction\n\nhttps://www.kaggle.com/fedesoriano/company-bankruptcy-prediction\n\n### Table of contents\n\n1. Data overview\n2. Target variable analysis\n3. Features-target analysis\n4. Multicollinearity\n5. Data imbalance\n6. SVM\n7. Conclusion + links\n\n***note:*** I will not explain the basic statistical / machine learning concepts. Some links are provided in the last section.\n\n***important terminology note:***\n- I am going to use these terms interchangeably\n- *features = predictor variables = columns*\n- *target variables = categories = classes*\n- *datapoint = row of the dataset*","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import f1_score, make_scorer, accuracy_score, recall_score, precision_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils import class_weight\nfrom sklearn.feature_selection import RFE\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import plot_confusion_matrix\n\nfrom imblearn.over_sampling import SMOTE\n\nimport random\nimport itertools\n\nimport scipy\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:02:00.15864Z","iopub.execute_input":"2021-09-20T12:02:00.159154Z","iopub.status.idle":"2021-09-20T12:02:00.167473Z","shell.execute_reply.started":"2021-09-20T12:02:00.159079Z","shell.execute_reply":"2021-09-20T12:02:00.166597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Data overview","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/company-bankruptcy-prediction/data.csv\")\nprint(\"Number of datapoints is %.d\" %len(df))\ndf.head()\n#df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:02:00.169153Z","iopub.execute_input":"2021-09-20T12:02:00.170133Z","iopub.status.idle":"2021-09-20T12:02:00.370665Z","shell.execute_reply.started":"2021-09-20T12:02:00.170086Z","shell.execute_reply":"2021-09-20T12:02:00.369619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like the features are mostly numerical but I cannot see all 96 columns so I am going to look for categorical variables.","metadata":{}},{"cell_type":"code","source":"#looking for possibly categorical features\nFilter = []\n\nfor col in df.columns:\n    if df[col].nunique()<50: #quasi definition of categorical variable xd\n        Filter.append(True)\n    else:\n        Filter.append(False)\n\ncategorical_c = df.loc[:,Filter] #filtering columns in this case\n\nprint(df.loc[:,Filter])\n\ndf = df.drop([' Liability-Assets Flag',' Net Income Flag'], 1)\n\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:02:00.372057Z","iopub.execute_input":"2021-09-20T12:02:00.373544Z","iopub.status.idle":"2021-09-20T12:02:00.431661Z","shell.execute_reply.started":"2021-09-20T12:02:00.373408Z","shell.execute_reply":"2021-09-20T12:02:00.431117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It turns out that only two features are categorical (Liability-Assets Flag, Liability-Assets Flag) and they also happen to be useless because all datapoints have the same value. All other features are numerical.","metadata":{}},{"cell_type":"markdown","source":"## 2. Target variable analysis","metadata":{}},{"cell_type":"markdown","source":"The target is a dichotomous variable, I am going to have a look at the distribution of the two classes.","metadata":{}},{"cell_type":"code","source":"#analyzing Target variable (Class: 0 = Not Bankrupt, 1 = Bankrupt)\n\nprint(df[\"Bankrupt?\"].value_counts())\n\npercentage = df[\"Bankrupt?\"].value_counts()[1]/len(df)\nprint(\"Total percentage of bankrupted companies is %.1f\" %(percentage*100) + \" %.\")\n\nsns.countplot(df[\"Bankrupt?\"])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:02:00.433175Z","iopub.execute_input":"2021-09-20T12:02:00.433522Z","iopub.status.idle":"2021-09-20T12:02:00.551326Z","shell.execute_reply.started":"2021-09-20T12:02:00.433485Z","shell.execute_reply":"2021-09-20T12:02:00.550711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a huge imbalance between the two categories. It turns out that only 3.2% companies in this dataset bankrupted.","metadata":{}},{"cell_type":"markdown","source":"## 3. Variable-target analysis","metadata":{}},{"cell_type":"markdown","source":"Some companies bankrupted and some did not. I am not an economist and I sincerely know very little about the meaning of the predictor variables and bankrupcy. However, before proceeding with the analysis I would like to see at least a small evidence that the variables have effect on the bankrupcy.","metadata":{}},{"cell_type":"markdown","source":"### 3.1 Non-statistical test\nPlotting the relative difference between the means of the features for both categories (bankrupted and not bankrupted).","metadata":{}},{"cell_type":"code","source":"#Variables' effect on class\n\nfeatures = df.columns[1:] #from now on \"features\" are interchangable with \"columns\"\n\nX = df[features]\ny = df[\"Bankrupt?\"]\n\nX_0 = X.loc[y==0,:] #not bankrupted\nX_1 = X.loc[y==1,:] #bakrupted\n\nX_0_test = X_0.sample(n=220)\n\nsignificant_cols = [] #features that have \"very different\" means\ndifs=[] #differences between means\n\nfor col in X.columns:\n    relative_means_difference = (X_1[col].mean() - X_0_test[col].mean()) / X_0_test[col].mean() \n    difs.append([col,relative_means_difference])\n    if abs(relative_means_difference)>0.5: #tresnhold, at least 50% freater/smaller mean \n        significant_cols.append(col)\n\n\nsns.barplot(x=list(range(len(difs))),y=[e[1] for e in difs])\nplt.ylim((-1,5)) #this controls the size of the window displayed\nplt.xlabel(\"Features\")\nplt.ylabel(\"Relative difference between means\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:02:00.55264Z","iopub.execute_input":"2021-09-20T12:02:00.553108Z","iopub.status.idle":"2021-09-20T12:02:01.731338Z","shell.execute_reply.started":"2021-09-20T12:02:00.553069Z","shell.execute_reply":"2021-09-20T12:02:01.730567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are a few features with really big differences and overall around 20 features whose means are more than 50% apart in these two categories.","metadata":{}},{"cell_type":"markdown","source":"### 3.2 Monte Carlo Hypothesis Test\n\n#### HYPOTHESIS: There is a difference between bakrupted and not-bankrupted companies\n(Null hypothesis: There is no difference between the bankrupted and not-bankrupted companies.)\n\nI am going to generate 1000 samples, each containing 220 datapoints from <code>X</code> (- all datapoints) and obtain the sampling distribution of the sample mean for each feature. From the observed data (= the 220 datapoints of bankrupted companies) and sampling distribution I am going to determine the p-value.\n\n***p-value for each feature:*** percentage of sample means that are more extreme than the bankrupt companies mean","metadata":{}},{"cell_type":"code","source":"#MONTE CARLO HYPOTESIS TEST\n\nfrom statistics import mean\n\nsampling_distribution = {feature: [] for feature in features} #SAMPLING DISTRIBUTION OF SAMPLE MEANS for each feature\nbankrupt_means = {feature: X_1[feature].mean() for feature in features} #MEAN of each feature (observed data = bankrupt companies)\n\nfor i in range(1000): #sampling from the data 1000 times\n    X_sample = X.sample(n=220) #n same as the number of bankrupt companies,sampling from X\n    for feature in features:\n        s_mean = X_sample[feature].mean()\n        sampling_distribution[feature].append(s_mean)\n\npvalues = {feature: None for feature in features}\n\ndef get_p_value(sampling_distribution, observed):\n    l = abs(observed-mean(sampling_distribution)) #distance of observed from the sample mean\n    return sum(abs(sample_mean-mean(sampling_distribution))>l for sample_mean in sampling_distribution)/len(sampling_distribution) #the proportion of data more extreme than observed\n               \nfor feature in pvalues: #filling the pvalues dictionary\n    pvalues[feature] = get_p_value(sampling_distribution[feature],bankrupt_means[feature]) ","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:02:01.73243Z","iopub.execute_input":"2021-09-20T12:02:01.732798Z","iopub.status.idle":"2021-09-20T12:03:27.732481Z","shell.execute_reply.started":"2021-09-20T12:02:01.732752Z","shell.execute_reply":"2021-09-20T12:03:27.73186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of significantly different features: %d\" %sum(np.array(list(pvalues.values()))>0.05))\ndict(itertools.islice(pvalues.items(),10)) #look at the first 10 features and associated p-values","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:03:27.733756Z","iopub.execute_input":"2021-09-20T12:03:27.734138Z","iopub.status.idle":"2021-09-20T12:03:27.741998Z","shell.execute_reply.started":"2021-09-20T12:03:27.734096Z","shell.execute_reply":"2021-09-20T12:03:27.741044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting some features and their distribution of sample means + red line with the mean of the observed data (= data of bankrupt companies)\n\nfig, axes = plt.subplots(2,2, figsize=(15,8))\n\nsns.distplot(sampling_distribution[\" Operating Gross Margin\"], ax=axes[0,0],label=\"sampling distribution of the mean\")\naxes[0,0].axvline(x=bankrupt_means[\" Operating Gross Margin\"],label=\"observation - pvalue %.2f\"%pvalues[\" Operating Gross Margin\"],c=\"r\")\naxes[0,0].legend(loc='upper left')\n\nsns.distplot(sampling_distribution[\" Interest-bearing debt interest rate\"], ax=axes[0,1])\naxes[0,1].axvline(x=bankrupt_means[\" Interest-bearing debt interest rate\"],label=\"pvalue %.2f\"%pvalues[\" Interest-bearing debt interest rate\"],c=\"r\")\naxes[0,1].legend()\n\nsns.distplot(sampling_distribution[\" Inventory/Current Liability\"], ax=axes[1,0])\naxes[1,0].axvline(x=bankrupt_means[\" Inventory/Current Liability\"],label=\"pvalue %.2f\"%pvalues[\" Inventory/Current Liability\"],c=\"r\")\naxes[1,0].legend()\n\nsns.distplot(sampling_distribution[\" No-credit Interval\"], ax=axes[1,1])\naxes[1,1].axvline(x=bankrupt_means[\" No-credit Interval\"],label=\"pvalue %.2f\"%pvalues[\" No-credit Interval\"],c=\"r\")\naxes[1,1].legend()\n\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:03:27.74334Z","iopub.execute_input":"2021-09-20T12:03:27.743539Z","iopub.status.idle":"2021-09-20T12:03:28.91388Z","shell.execute_reply.started":"2021-09-20T12:03:27.743516Z","shell.execute_reply":"2021-09-20T12:03:28.912776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I only exaimined the variables independently while there are probably many dependencies between them so I am not going to draw conclusions or perform feature selection based on these p-values.","metadata":{}},{"cell_type":"markdown","source":"## 4. Multicollinearity\n\nI am going to find features with correlation coefficient greater than 0.9 and drop them.","metadata":{}},{"cell_type":"code","source":"#MULTICOLLINEARITY (CORRELATION BETWEEN PREDICTOR VARIABLES)\n\ncor_matrix = df.corr().abs()\ncor_matrix.style.background_gradient(sns.light_palette('red', as_cmap=True))","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:03:28.915187Z","iopub.execute_input":"2021-09-20T12:03:28.91548Z","iopub.status.idle":"2021-09-20T12:03:29.688942Z","shell.execute_reply.started":"2021-09-20T12:03:28.915442Z","shell.execute_reply":"2021-09-20T12:03:29.688199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dropping correlated data\n\nupper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(np.bool)) #upper triangle of the correlation matrix\n\ndropped_cols = set()\nfor feature in upper_tri.columns:\n    if any(upper_tri[feature] > 0.9): #more than 0.9 corr. coeficient -> dropped\n        dropped_cols.add(feature)\n\nprint(\"There are %d dropped columns\" %len(dropped_cols))\n\nX = X.drop(dropped_cols,axis=1)\nX.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:03:29.690552Z","iopub.execute_input":"2021-09-20T12:03:29.690919Z","iopub.status.idle":"2021-09-20T12:03:29.73483Z","shell.execute_reply.started":"2021-09-20T12:03:29.690889Z","shell.execute_reply":"2021-09-20T12:03:29.734227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PCA is a way to decorrelate and reduce the dimensionality of the data through the change of the basis. I am going to try if the method helps to decorrelate the data.","metadata":{}},{"cell_type":"code","source":"#PCA\n\nscaler = StandardScaler() \nX_for_pca = pd.DataFrame(data=scaler.fit_transform(X),index=X.index,columns=X.columns) #standardized dataset\n\nn_components = 10\n\npca = PCA(n_components=n_components)\nprincipal_components = pca.fit_transform(X_for_pca)\nX_pc = pd.DataFrame(data=principal_components, columns=['PC %d'%d for d in range(n_components)])\n\nprint(\"Explained variance by 10 components %.2f\" %sum(pca.explained_variance_ratio_))","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:03:29.735682Z","iopub.execute_input":"2021-09-20T12:03:29.736381Z","iopub.status.idle":"2021-09-20T12:03:29.8068Z","shell.execute_reply.started":"2021-09-20T12:03:29.736348Z","shell.execute_reply":"2021-09-20T12:03:29.805954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With 10 principal components the explained variance is still very low, so I do not find the PCA transformation useful for this data.","metadata":{}},{"cell_type":"markdown","source":"## 5. Data Imbalance\n\nThere is a huge imbalance between the data (only 3.2% companies from the dataset bankrupted). Before training a model I need to deal with this problem, otherwise the model would just predict every company to not bankrupt. \n\nI decided to try two ways:\n1. ***Introducing weights*** \\\nEvery datapoint from the minority class is considered \"more important\" than from the majority class, the weights for the two classes are inversely proportional to the number of datapoints in that class. Implemented within the SVM in next section.\n\n2. ***SMOTE*** \\\nThe Synthetic Minority Over-sampling TEchnique. \\\nCreates new synthetic datapoints using the k-nearest neighbor algorithm. \\\nWith this method I am going to obtain the dataset where the value counts for both categories are the same.","metadata":{}},{"cell_type":"code","source":"#DATA IMBALANCE\n#SMOTE \n\nsm = SMOTE(random_state=42)\n\nX_sm, y_sm = sm.fit_resample(X, y)\n\nprint('New balance of 1 and 0 classes (%):')\ny_sm.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:03:29.808265Z","iopub.execute_input":"2021-09-20T12:03:29.80878Z","iopub.status.idle":"2021-09-20T12:03:29.898016Z","shell.execute_reply.started":"2021-09-20T12:03:29.808739Z","shell.execute_reply":"2021-09-20T12:03:29.897202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. SVM \n\nI am going to train a SVM model. First with SMOTE-dataset, then without SMOTE data and lastly with SMOTE-dataset but reduced to 10% of the data.\n\nThe function <code>train_test_SVM(X,y)</code> has multiple steps:\n1. Splitting the data\n2. Assigning the weights\n3. Creating a <code>Pipeline</code>\n4. Using <code>GridSearchCV</code> to find the optimal hyperparameters \\\nTrain the model\n5. Score\n6. Confusion matrix","metadata":{}},{"cell_type":"markdown","source":"The SVM training takes quite long (around 4 minutes for me).\n- big amount of datapoints (perhaps too many for a SVM)\n- <code>GridSearchCV</code> using cross validation for different (C, gamma) combinations\n- training <code>'rbf'</code> kernel is slower than linear kernel","metadata":{}},{"cell_type":"code","source":"#SVM\n\ndef train_test_SVM(X,y):\n    \"\"\"Function finds the optimal hyperparameters of the SVM, plots the confusion matrix of test data, returns the model\"\"\"\n    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, stratify=y) #stratify adresses the unbalance only in the train test splitting\n    \n    sw_train = class_weight.compute_sample_weight(class_weight = 'balanced', y = y_train) #when balanced sw_train = [1.1.1...1]\n    \n    steps = [('scaler', StandardScaler()), ('SVM', SVC(cache_size=7000))]\n    pipeline = Pipeline(steps)\n    \n    #parameters' names must match the 'SVM' name in Pipeline followed by two underscores!\n    #standard SVM hyperparameters\n    param_grid = {\n    'SVM__C':[0.01,0.1,1,10],\n    'SVM__gamma':[0.1,0.01,0.001,0.0001],\n    'SVM__kernel':['rbf']\n    }\n    \n    f1 = make_scorer(f1_score , average='macro')\n    grid = GridSearchCV(pipeline,param_grid=param_grid, cv=5, scoring=f1, verbose=0) #verbose controls the training progression display!\n    grid.fit(X_train, y_train, SVM__sample_weight = sw_train)\n    \n    print(\"best parameters: \")\n    print(grid.best_params_)\n    \n    model = grid.best_estimator_\n    y_pred = model.predict(X_test)\n    \n    print(\"f1 score is %.2f \"%f1_score(y_test, y_pred))\n    print(\"Precision: %.2f\" %precision_score(y_test, y_pred))\n    print(\"Recall: %.2f\" %recall_score(y_test, y_pred))\n    print(\"Precision: %.2f\" %precision_score(y_test, y_pred))\n    plot_confusion_matrix(model,\n                         X_test,\n                         y_test,\n                         values_format='d')\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:03:29.899687Z","iopub.execute_input":"2021-09-20T12:03:29.900272Z","iopub.status.idle":"2021-09-20T12:03:29.915308Z","shell.execute_reply.started":"2021-09-20T12:03:29.900229Z","shell.execute_reply":"2021-09-20T12:03:29.913973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = train_test_SVM(X_sm,y_sm)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:03:29.91709Z","iopub.execute_input":"2021-09-20T12:03:29.917753Z","iopub.status.idle":"2021-09-20T12:11:43.062235Z","shell.execute_reply.started":"2021-09-20T12:03:29.917709Z","shell.execute_reply":"2021-09-20T12:11:43.061468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training and testing without SMOTE\n\ntrain_test_SVM(X,y)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:11:43.064047Z","iopub.execute_input":"2021-09-20T12:11:43.064712Z","iopub.status.idle":"2021-09-20T12:13:59.247267Z","shell.execute_reply.started":"2021-09-20T12:11:43.064649Z","shell.execute_reply":"2021-09-20T12:13:59.246753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Without SMOTE the performance is a way worse. The model is not \"meaningless\" as it would be without the weights, however I suppose the weights are simply just \"not enough\" for such a big imbalance. ","metadata":{}},{"cell_type":"code","source":"#Training and testing with 10% of the data\n\nXy_sm = pd.concat([X_sm,y_sm],axis=1)\nXy_reduced_1 = Xy_sm[Xy_sm[\"Bankrupt?\"]==1].sample(frac=0.1) #taking 10% of the datapoints \"Bankrupt?\" = 1\nXy_reduced_0 = Xy_sm[Xy_sm[\"Bankrupt?\"]==0].sample(frac=0.1) #taking 10% of the datapoints \"Bankrupt?\" = 0\nXy_reduced = pd.concat([Xy_reduced_1,Xy_reduced_0],axis=0) #the dataset is going to be shuffled in train_test_split\n\ny_reduced = Xy_reduced[\"Bankrupt?\"]\nX_reduced = Xy_reduced.drop(\"Bankrupt?\",axis=1)\n\ntrain_test_SVM(X_reduced,y_reduced)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T12:13:59.248388Z","iopub.execute_input":"2021-09-20T12:13:59.248844Z","iopub.status.idle":"2021-09-20T12:14:06.553263Z","shell.execute_reply.started":"2021-09-20T12:13:59.248797Z","shell.execute_reply":"2021-09-20T12:14:06.552723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SVM does not use all the data to make a decision boundary, that is why the model works quite good with only 10% data. And the training is much faster.","metadata":{}},{"cell_type":"markdown","source":"## Conclusion and links\n\nI preprocessed the data and trained an SVM estimator. I used a few widely used machine learning techniques along the way. However, there are more things that can be done to understand the data more. I would be interested in feature selection as well as applying other machine learning algorithms and observing the differences in their performance. \n\n### Sources: \n- PCA: https://www.youtube.com/watch?v=lrHboFMio7g&list=PLtV9G3anqB8B5s6FtBmqyc9VeZtlNp8GV&ab_channel=algomanic\n- Imbalanced Data: https://towardsdatascience.com/imbalanced-data-when-details-matter-16bd3ec7ef74\n- https://medium.com/@radecicdario\n- https://brilliant.org/","metadata":{}}]}