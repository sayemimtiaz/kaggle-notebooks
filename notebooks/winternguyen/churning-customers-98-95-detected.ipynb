{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\n+ A churn model is a mathematical representation of how churn impacts your business. Churn calculations are built on existing data (the number of customers who left your service during a given time period). A predictive churn model extrapolates on this data to show future potential churn rates.\n\n+ Churn (aka customer attrition) is a scourge on subscription businesses. When your revenue is based on recurring monthly or annual contracts, every customer who leaves puts a dent in your cash flow. High retention rates are vital for your survival. So what if we told you there was a way to predict, at least to some degree, how and when your customers will cancel?\n\n+ Building a predictive churn model helps you make proactive changes to your retention efforts that drive down churn rates. Understanding how churn impacts your current revenue goals and making predictions about how to manage those issues in the future also helps you stem the flow of churned customers. If you don’t take action against your churn now, any company growth you experience simply won’t be sustainable.\n\n+ Comprehensive customer profiles help you see what types of customers are canceling their accounts. Now it’s time to figure out how and why they’re churning. Ask yourself the following questions to learn more about the pain points in your product and customer experience that lead to a customer deciding to churn.","metadata":{}},{"cell_type":"markdown","source":"# What is customer churn?\n\n+ Customer churn (or customer attrition) is a tendency of customers to abandon a brand and stop being a paying client of a particular business. The percentage of customers that discontinue using a company’s products or services during a particular time period is called a customer churn (attrition) rate. One of the ways to calculate a churn rate is to divide the number of customers lost during a given time interval by the number of acquired customers, and then multiply that number by 100 percent. For example, if you got 150 customers and lost three last month, then your monthly churn rate is 2 percent.\n\n+ Churn rate is a health indicator for businesses whose customers are subscribers and paying for services on a recurring basis, thus, a customer stays open for more interesting or advantageous offers. Plus, each time their current commitment ends, customers have a chance to reconsider and choose not to continue with the company. Of course, some natural churn is inevitable, and the figure differs from industry to industry. But having a higher churn figure than that is a definite sign that a business is doing something wrong.”\n\n+ There are many things brands may do wrong, from complicated onboarding when customers aren’t given easy-to-understand information about product usage and its capabilities to poor communication, e.g. the lack of feedback or delayed answers to queries. Another situation: Longtime clients may feel unappreciated because they don’t get as many bonuses as the new ones.","metadata":{}},{"cell_type":"markdown","source":"# Step 1: Importing the Data and Libraries\n\nThe first step, as always, is to import the required libraries. Execute the following code to do so:","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/credit-card-customers/BankChurners.csv')","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LUCKY! NO need to handle with NaN or missing data.","metadata":{}},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Columns title is TO LONG, let's rename it.","metadata":{}},{"cell_type":"code","source":"old_names = data.columns\nnew_names = ['Clientnum', 'Attrition', 'Age', 'Gender', 'Dependent_count', 'Education', 'Marital_Status', 'Income', \n             'Card_Category', 'Months_on_book', 'Total_Relationship_Count', 'Months_Inactive', 'Contacts_Count', \n             'Credit_Limit', 'Total_Revolving_Bal','Avg_Open_To_Buy', 'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt',\n             'Total_Trans_Ct', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio','Naive_Bayes_1','Naive_Bayes_2']\ndata.rename(columns=dict(zip(old_names, new_names)), inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 2. Exploratory data analysis\n\nAfter preprocessing the data, it is analyzed through visual exploration to gather insights about the model that can be applied to the data, understand the diversity in the data and the range of every field. We use a bar chart, box plot, distribution graph, etc. to explore each feature varies and its relation with other features including the target feature.","metadata":{}},{"cell_type":"markdown","source":"Import basic Librairies.","metadata":{}},{"cell_type":"code","source":"import matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport seaborn as sns\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check to confirm if the Database is IMBALANCED? How?","metadata":{}},{"cell_type":"code","source":"#Churn vs. normal \ncounts = data.Attrition.value_counts()\nnormal = counts[0]\nChurn = counts[1]\nperc_normal = (normal/(normal+Churn))*100\nperc_Churn = (Churn/(normal+Churn))*100\nprint('There were {} non-Churn ({:.3f}%) and {} Churn ({:.3f}%).'.format(normal, perc_normal, Churn, perc_Churn))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Dataset is IMBALANCED, but NOT typical, since I expected for this type should be something like: 98:2 or 99:1, NOT such 84:16 :). But, it's OK, let's dicover some illustrations.","metadata":{}},{"cell_type":"code","source":"style.use('ggplot')\nsns.set_style('whitegrid')\nplt.subplots(figsize = (30,30))\n## Plotting heatmap. Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(data.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(data.corr(), cmap=sns.diverging_palette(20, 220, n=200), annot=True, mask=mask, center = 0, );\nplt.title(\"Heatmap of all the Features of Train data set\", fontsize = 25);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#visualizing the features whigh positive and negative correlation\nf, axes = plt.subplots(nrows=3, ncols=3, figsize=(25,15))\n\nf.suptitle('Features With High Negative Correlation', size=35)\nsns.boxplot(x=\"Attrition\", y=\"Total_Relationship_Count\", data=data, ax=axes[0,0])\nsns.boxplot(x=\"Attrition\", y=\"Total_Revolving_Bal\", data=data, ax=axes[0,1])\nsns.boxplot(x=\"Attrition\", y=\"Total_Amt_Chng_Q4_Q1\", data=data, ax=axes[0,2])\nsns.boxplot(x=\"Attrition\", y=\"Total_Trans_Amt\", data=data, ax=axes[1,0])\nsns.boxplot(x=\"Attrition\", y=\"Total_Trans_Ct\", data=data, ax=axes[1,1])\nsns.boxplot(x=\"Attrition\", y=\"Total_Ct_Chng_Q4_Q1\", data=data, ax=axes[1,2])\nsns.boxplot(x=\"Attrition\", y=\"Avg_Utilization_Ratio\", data=data, ax=axes[2,0])\nsns.boxplot(x=\"Attrition\", y=\"Months_Inactive\", data=data, ax=axes[2,1])\nsns.boxplot(x=\"Attrition\", y=\"Contacts_Count\", data=data, ax=axes[2,2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yprop = 'Age'\nxprop = 'Months_Inactive'\nh= 'Attrition'\npx.scatter(data, x=xprop, y=yprop, color=h, marginal_y=\"violin\", marginal_x=\"box\", trendline=\"ols\", template=\"simple_white\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yprop = 'Months_on_book'\nxprop = 'Dependent_count'\nh= 'Attrition'\npx.scatter(data, x=xprop, y=yprop, color=h, marginal_y=\"violin\", marginal_x=\"box\", trendline=\"ols\", template=\"simple_white\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.box(data, x='Gender',y='Avg_Utilization_Ratio', color='Attrition', notched=True)\nfig.update_layout(legend=dict(orientation=\"h\",yanchor=\"bottom\",y=1.02,xanchor=\"right\",x=1))\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yprop = 'Total_Amt_Chng_Q4_Q1'\nxprop = 'Total_Trans_Amt'\nh= 'Attrition'\npx.scatter(data, x=xprop, y=yprop, color=h, marginal_y=\"violin\", marginal_x=\"box\", trendline=\"ols\", template=\"simple_white\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nxprop = 'Gender'\nyprop = 'Total_Trans_Amt'\nsns.boxplot(data=data, x=xprop, y=yprop, hue='Attrition')\nplt.xlabel('{} range'.format(xprop), size=14)\nplt.ylabel('{}'.format(yprop), size=14)\nplt.title('Boxplot of {}'.format(yprop), size=20)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prop = 'Avg_Utilization_Ratio'\nxprop = 'Total_Trans_Ct'\nh= 'Attrition'\npx.scatter(data, x=xprop, y=yprop, color=h, marginal_y=\"violin\", marginal_x=\"box\", trendline=\"ols\", template=\"simple_white\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yprop = 'Total_Revolving_Bal'\nxprop = 'Credit_Limit'\nh= 'Attrition'\npx.scatter(data, x=xprop, y=yprop, color=h, marginal_y=\"violin\", marginal_x=\"box\", trendline=\"ols\", template=\"simple_white\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Overally, it's NOT EASY to conclude something from these illustrations, OR, the analysis is not deep ENOUGH !","metadata":{}},{"cell_type":"markdown","source":"But YES, features have not strong correlation with each other.","metadata":{}},{"cell_type":"markdown","source":"# Step 3: Converting Categorical Columns to Numeric Columns\n\nMachine learning algorithms work best with numerical data. However, in our dataset, we have some categorical columns. These columns contain data in textual format; we need to convert them to numeric columns.","metadata":{}},{"cell_type":"markdown","source":"Firsly, remove the Id column and rearrange the columns.","metadata":{}},{"cell_type":"code","source":"data = pd.DataFrame(data, columns = ['Age', 'Gender', 'Dependent_count','Education', 'Marital_Status', \n                                     'Income', 'Card_Category','Months_on_book', 'Total_Relationship_Count', \n                                     'Months_Inactive','Contacts_Count', 'Credit_Limit', 'Total_Revolving_Bal',\n                                     'Avg_Open_To_Buy', 'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt',\n                                     'Total_Trans_Ct', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio',\n                                     'Naive_Bayes_1', 'Naive_Bayes_2','Attrition'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's copy the Data and Str Map the NON OBJECT columns.","metadata":{}},{"cell_type":"code","source":"import copy\ndf_train=copy.deepcopy(data)\ncols=np.array(data.columns[data.dtypes != object])\nfor i in df_train.columns:\n    if i not in cols:\n        df_train[i]=df_train[i].map(str)\ndf_train.drop(columns=cols,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And then, coding the categorical parameters using LabelEncoder.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom collections import defaultdict\n\n# build dictionary function\ncols=np.array(data.columns[data.dtypes != object])\nd = defaultdict(LabelEncoder)\n\n# only for categorical columns apply dictionary by calling fit_transform \ndf_train = df_train.apply(lambda x: d[x.name].fit_transform(x))\ndf_train[cols] = data[cols]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And check the conversion...","metadata":{}},{"cell_type":"code","source":"df_train.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 4: Feature Selection\n\nOur data is now ready, and we can train our machine learning model. But first, we need to isolate the variable that we're predicting from the dataset.","metadata":{}},{"cell_type":"code","source":"df_train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The following columns/features can be split up in the following groups:       \n\n+ 'Clientium' : Unique identifier for the customer holding the account: we remove this column\n+ 'Attrition': Internal event (customer activity) variable - if the account is closed then 1 else 0: this is our target\n+ 'Age': Customer's Age in Years.\n+ 'Gender': M=Male, F=Female.\n+ 'Dependent_count': Number of dependents.\n+ 'Education': Educational Qualification of the account holder.\n+ 'Marital_Status': Married, Single, Divorced, Unknown.\n+ 'Income': Annual Income Category of the account holder\n+ 'Card_Category': Type of Card (Blue, Silver, Gold, Platinum).\n+ 'Months_on_book': Period of relationship with bank.\n+ 'Total_Relationship_Count': Total no. of products held by the customer.\n+ 'Months_Inactive': No. of Months in the last 12 months.\n+ 'Contacts_Count': No. of Contacts in the last 12 months.\n+ 'Credit_Limit': Credit Limit on the Credit Card.\n+ 'Total_Revolving_Bal': Total Revolving Balance on the Credit Card.\n+ 'Avg_Open_To_Buy': Open to Buy Credit Line (Average of last 12 months\n+ 'Total_Amt_Chng_Q4_Q1': Change in Transaction Amount (Q4 over Q1).\n+ 'Total_Trans_Amt': Total Transaction Amount (Last 12 months).\n+ 'Total_Trans_Ct': Total Transaction Count (Last 12 months).\n+ 'Total_Ct_Chng_Q4_Q1': Change in Transaction Count (Q4 over Q1).\n+ 'Avg_Utilization_Ratio': Average Card Utilization Ratio.\n\nAS NOTICED:\n\n+ Naive_Bayes: It was mentioned that all columns containing the \"N.B.\"-tag should be disregarded: we remove these 2 columns","metadata":{}},{"cell_type":"code","source":"df_train = pd.DataFrame(df_train, columns = ['Age', 'Gender', 'Dependent_count','Education', 'Marital_Status', 'Income', \n                                             'Card_Category','Months_on_book', 'Total_Relationship_Count', 'Months_Inactive',\n                                             'Contacts_Count', 'Credit_Limit', 'Total_Revolving_Bal','Avg_Open_To_Buy', \n                                             'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt','Total_Trans_Ct', 'Total_Ct_Chng_Q4_Q1',\n                                             'Avg_Utilization_Ratio','Attrition'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df_train.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find most important features relative to target Price\nprint(\"Find most important features relative to Attrition-target\")\ncorr = df_train.corr()\ncorr.sort_values([\"Attrition\"], ascending = False, inplace = True)\nprint(corr.Attrition)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"style.use('ggplot')\nsns.set_style('whitegrid')\nplt.subplots(figsize = (30,30))\n## Plotting heatmap. Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(df_train.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(df_train.corr(), cmap=sns.diverging_palette(20, 220, n=200), annot=True, mask=mask, center = 0, );\nplt.title(\"Heatmap of all the Features of Train data set\", fontsize = 25);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"OK, let's remove some columns with very low correlation: \"Avg_Open_To_Buy\"","metadata":{}},{"cell_type":"code","source":"features = ['Age', 'Gender', 'Dependent_count', 'Education','Marital_Status', 'Income', 'Card_Category','Months_on_book',\n            'Total_Relationship_Count','Months_Inactive','Contacts_Count', 'Credit_Limit', 'Total_Revolving_Bal','Total_Amt_Chng_Q4_Q1', \n            'Total_Trans_Amt','Total_Trans_Ct','Total_Ct_Chng_Q4_Q1','Avg_Utilization_Ratio','Attrition']\ndf_train = pd.DataFrame(df_train, columns = features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 5: Churn Prediction Model\n\nIn this section we present several regression models for the churn prediction, and compare their relative performances in terms of standard metrics such as precision, recall, specificity, and AUC. These performances also allow us to evaluate the effectiveness of the outlier removal, undersampling and ensemble techniques.","metadata":{}},{"cell_type":"markdown","source":"Now, import ML Librairie pakages","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nfrom imblearn.over_sampling import ADASYN\nfrom imblearn.over_sampling import SMOTE\nfrom lightgbm import LGBMClassifier\nfrom numpy import where\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn import preprocessing\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\nfrom sklearn.ensemble.forest import ExtraTreesClassifier\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, roc_curve\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler, Normalizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# A. Sampling\n\nAs observed, the number of churners is usually much smaller than the number of non-churners, leading to an imbalanced training set. This issue was also present in our analysis, we obsed the Churners rate is upto 16%, which might be NOT MUCH imbalanced. In particular, the most effective method was undersampling, which outperformed the sampling techniques that replicates the rare class, since the latter ones resulted in over-fitting the rare class, thus degrading the model performance significantly. In particular, the undersampling technique significantly improved the results only if combined with the outlier removal. The best proportion between classes was chosen so to optimize the classifier performance on the test set.\n\nWe define the Data for Input purpose as following:","metadata":{}},{"cell_type":"code","source":"def Definedata():\n    # define dataset\n    X=df_train.drop(columns=['Attrition']).values\n    y=df_train['Attrition'].values\n    return X, y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, X is our feature set; it contains all the columns except the one that we have to predict (Exited). The label set, y, contains only the Exited column. So we can later evaluate the performance of our machine learning model, let's also divide the data into a training and test set. The training set contains the data that will be used to train our machine learning model. The test set will be used to evaluate how good our model is. \n\n# We'll use 33% of the data for the test set and the remaining 67% for the training set.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\ndef Models_NO(models, graph):\n    \n    model = models\n    X, y = Definedata()\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 25)\n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    y_total = model.predict(X)\n    \n    if graph:\n        train_matrix = pd.crosstab(y_train, model.predict(X_train), rownames=['Actual'], colnames=['Predicted'])    \n        test_matrix = pd.crosstab(y_test, model.predict(X_test), rownames=['Actual'], colnames=['Predicted'])\n        matrix = pd.crosstab(y, model.predict(X), rownames=['Actual'], colnames=['Predicted'])\n    \n        f,(ax1,ax2,ax3) = plt.subplots(1,3,sharey=True, figsize=(15, 2))\n    \n        g1 = sns.heatmap(train_matrix, annot=True, fmt=\".1f\", cbar=False,annot_kws={\"size\": 18},ax=ax1)\n        g1.set_title(\"{}/train set\".format(model))\n        g1.set_ylabel('Total Churn = {}'.format(1- y_train.sum()), fontsize=14, rotation=90)\n        g1.set_xlabel('Accuracy for TrainSet: {}'.format(accuracy_score(model.predict(X_train), y_train)))\n        g1.set_xticklabels(['Churn','Not Churn'],fontsize=12)\n\n        g2 = sns.heatmap(test_matrix, annot=True, fmt=\".1f\",cbar=False,annot_kws={\"size\": 18},ax=ax2)\n        g2.set_title(\"{}/test set\".format(model))\n        g2.set_ylabel('Total Churn = {}'.format(1- y_test.sum()), fontsize=14, rotation=90)\n        g2.set_xlabel('Accuracy for TestSet: {}'.format(accuracy_score(y_pred, y_test)))\n        g2.set_xticklabels(['Churn','Not Churn'],fontsize=12)\n\n        g3 = sns.heatmap(matrix, annot=True, fmt=\".1f\",cbar=False,annot_kws={\"size\": 18},ax=ax3)\n        g3.set_title(\"{}/total set\".format(model))\n        g3.set_ylabel('Total Churn = {}'.format(1- y.sum()), fontsize=14, rotation=90)\n        g3.set_xlabel('Accuracy for TotalSet: {}'.format(accuracy_score(y_total, y)))\n        g3.set_xticklabels(['Churn','Not Churn'],fontsize=12)\n    \n        plt.show()\n        print (\"\")\n        print (\"Classification Report: \")\n        print (classification_report(y, y_total))\n    else:\n        print(\"\\t\\tError Table\")\n        print('Mean Absolute Error      : ', metrics.mean_absolute_error(y_test, (y_pred)))\n        print('Mean Squared  Error      : ', metrics.mean_squared_error(y_test, (y_pred) ))\n        print('Root Mean Squared  Error : ', np.sqrt(metrics.mean_squared_error(y_test, (y_pred) )))\n        print('Accuracy on Traing set   : ', model.score(X_train,y_train))\n        print('Accuracy on Testing set  : ', model.score(X_test,y_test))\n        print('AUC score                :', roc_auc_score(y, y_total)*100,'%')        \n    return y_total, y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# B. Machine Learning Algorithm Training\n\nNow, we'll use a machine learning algorithm that will identify patterns or trends in the training data. This step is known as algorithm training. We'll feed the features and correct output to the algorithm; based on that data, the algorithm will learn to find associations between the features and outputs. After training the algorithm, you'll be able to use it to make predictions on new data.\n\nThere are several machine learning algorithms that can be used to make such predictions. In this work, we'll try some ALGORITHMS to select one of the most powerful algorithms for classification problems.\n\nTo train this algorithm, we call the fit method and pass in the feature set (X) and the corresponding label set (y). You can then use the predict method to make predictions on the test set. Look at the following script:","metadata":{}},{"cell_type":"code","source":"y_predict, y_test = Models_NO(DecisionTreeRegressor(), True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We could confirm that the data is IMBALANCED and the regressor can not handle this data set. Let start the second step.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,auc,roc_curve\n\ny_predicted, y_actual = Models_NO(DecisionTreeRegressor(), False)\nfpr, tpr, thresholds = roc_curve(y_actual, y_predicted)\nroc_auc = auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b',label='AUC = %0.3f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.0])\nplt.ylim([-0.1,1.01])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# C. SMOTE sampling","metadata":{}},{"cell_type":"code","source":"def SMOTE():\n    # borderline-SMOTE for imbalanced dataset\n    from collections import Counter\n    from sklearn.model_selection import train_test_split\n    from sklearn.datasets import make_classification\n    from imblearn.over_sampling import SMOTE\n    from matplotlib import pyplot\n    from numpy import where\n    \n    X, y = Definedata()\n\n# summarize class distribution\n    counter = Counter(y)\n    print(counter)\n# transform the dataset\n    smt = SMOTE(random_state=0)\n    X, y = smt.fit_sample(X, y) \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=2)\n# summarize the new class distribution\n    counter = Counter(y)\n    print(counter)\n# scatter plot of examples by class label\n    for label, _ in counter.items():\n        row_ix = where(y == label)[0]\n        pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n    pyplot.legend()\n    pyplot.show()\n    return X_train, X_test, y_train, y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ADASYN():\n    from collections import Counter\n    from sklearn.model_selection import train_test_split\n    from imblearn.over_sampling import ADASYN\n    from matplotlib import pyplot\n    from numpy import where\n\n    X, y = Definedata()\n\n# summarize class distribution\n    counter = Counter(y)\n    print(counter)\n# transform the dataset\n    X, y = ADASYN().fit_resample(X, y)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=2)\n# summarize the new class distribution\n    counter = Counter(y)\n    print(counter)\n# scatter plot of examples by class label\n    for label, _ in counter.items():\n        row_ix = where(y == label)[0]\n        pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n    pyplot.legend()\n    pyplot.show()\n    return X_train, X_test, y_train, y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%time X_train1, X_test1, y_train1, y_test1 = SMOTE()\n%time X_train4, X_test4, y_train4, y_test4 = ADASYN()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Models(models, X_train, X_test, y_train, y_test, title, graph):\n    model = models\n    model.fit(X_train,y_train)\n    \n    X, y = Definedata()\n    train_matrix = pd.crosstab(y_train, model.predict(X_train), rownames=['Actual'], colnames=['Predicted'])    \n    test_matrix = pd.crosstab(y_test, model.predict(X_test), rownames=['Actual'], colnames=['Predicted'])\n    matrix = pd.crosstab(y, model.predict(X), rownames=['Actual'], colnames=['Predicted'])\n    \n    if graph:\n        f,(ax1,ax2,ax3) = plt.subplots(1,3,sharey=True, figsize=(15, 2))\n    \n        g1 = sns.heatmap(train_matrix, annot=True, fmt=\".1f\", cbar=False,annot_kws={\"size\": 18},ax=ax1)\n        g1.set_title(title)\n        g1.set_ylabel('Total Churn = {}'.format(y_train.sum()), fontsize=14, rotation=90)\n        g1.set_xlabel('Accuracy score (TrainSet): {}'.format(accuracy_score(model.predict(X_train), y_train)))\n        g1.set_xticklabels(['Churn','Not Churn'],fontsize=12)\n\n        g2 = sns.heatmap(test_matrix, annot=True, fmt=\".1f\",cbar=False,annot_kws={\"size\": 18},ax=ax2)\n        g2.set_title(title)\n        g2.set_ylabel('Total Churn = {}'.format(y_test.sum()), fontsize=14, rotation=90)\n        g2.set_xlabel('Accuracy score (TestSet): {}'.format(accuracy_score(model.predict(X_test), y_test)))\n        g2.set_xticklabels(['Churn','Not Churn'],fontsize=12)\n\n        g3 = sns.heatmap(matrix, annot=True, fmt=\".1f\",cbar=False,annot_kws={\"size\": 18},ax=ax3)\n        g3.set_title(title)\n        g3.set_ylabel('Total Churn = {}'.format(y.sum()), fontsize=14, rotation=90)\n        g3.set_xlabel('Accuracy score (Total): {}'.format(accuracy_score(model.predict(X), y)))\n        g3.set_xticklabels(['Churn','Not Churn'],fontsize=12)\n\n        plt.show()\n\n    print(\"\\t\\tError Table\")\n    print('Accuracy on Traing set   : ', model.score(X_train,y_train))\n    print('Accuracy on Testing set  : ', model.score(X_test,y_test))\n    print('Overall Accuracy_Score   :',accuracy_score(y, model.predict(X))*100,'%')\n    print('Recall ratio             :',metrics.recall_score(y, model.predict(X))*100,'%')\n    print('AUC score                :', roc_auc_score(y, model.predict(X))*100,'%')\n\n    return y, model.predict(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'KNeighborsClassifier/SMOTE'\n%time y_actual,y_predicted = Models(KNeighborsClassifier(n_neighbors=1),X_train1, X_test1, y_train1, y_test1, title, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'KNeighborsClassifier/ ADASYN'\n%time Models(KNeighborsClassifier(n_neighbors=1),X_train4, X_test4, y_train4, y_test4, title, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'DecisionTreeClassifier/SMOTE'\n%time y_actual,y_predicted = Models(DecisionTreeClassifier(max_depth=14),X_train1, X_test1, y_train1, y_test1, title, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'DecisionTreeClassifier/ADASYN'\n%time Models(DecisionTreeClassifier(max_depth=14),X_train4, X_test4, y_train4, y_test4, title, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'RandomForestClassifier/SMOTE'\n%time Models(RandomForestClassifier(),X_train1, X_test1, y_train1, y_test1, title, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'RandomForestClassifier/ADASYN'\n%time y_actual,y_predicted = Models(RandomForestClassifier(),X_train4, X_test4, y_train4, y_test4, title, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'GradientBoostingClassifier/SMOTE'\n%time Models(GradientBoostingClassifier(n_estimators=1500, learning_rate=1, max_features=10, max_depth=2, random_state=0),X_train1, X_test1, y_train1, y_test1, title, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'GradientBoostingClassifier/ADASYN'\n%time Models(GradientBoostingClassifier(n_estimators=500, learning_rate=1, max_features=2, max_depth=2, random_state=0),X_train4, X_test4, y_train4, y_test4, title, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'XGBClassifier/SMOTE'\n%time Models(XGBClassifier(colsample_bytree=0.9, learning_rate=0.2, max_depth=7),X_train1, X_test1, y_train1, y_test1, title, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'XGBClassifier/ADASYN'\n%time Models(XGBClassifier(colsample_bytree=0.9, learning_rate=0.2, max_depth=7),X_train4, X_test4, y_train4, y_test4, title, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'LGBMClassifier/SMOTE'\n%time Models(LGBMClassifier(max_depth=-1, random_state=20, silent=True, metric='None', n_jobs=5, n_estimators=1000),X_train1, X_test1, y_train1, y_test1, title, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'LGBMClassifier/ADASYN'\n%time Models(LGBMClassifier(),X_train4, X_test4, y_train4, y_test4, title, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'LGBMClassifier/ADASYN'\n%time Models(LGBMClassifier(max_depth=-1, random_state=310, silent=True, metric='None', n_jobs=15, n_estimators=2000),X_train4, X_test4, y_train4, y_test4, title, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'GradientBoostingClassifier/SMOTE'\n%time Models(GradientBoostingClassifier(),X_train1, X_test1, y_train1, y_test1, title, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'GradientBoostingClassifier/ADASYN'\n%time Models(GradientBoostingClassifier(),X_train4, X_test4, y_train4, y_test4, title, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'GradientBoostingClassifier/ADASYN'\n%time Models(GradientBoostingClassifier(learning_rate=0.005, n_estimators=1500,max_depth=20, min_samples_split=300, min_samples_leaf=60, subsample=0.85, random_state=10, max_features=7,warm_start=True),X_train4, X_test4, y_train4, y_test4, title, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'ExtraTreesClassifier/SMOTE'\n%time Models(ExtraTreesClassifier(),X_train1, X_test1, y_train1, y_test1, title, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'ExtraTreesClassifier/SMOTE'\n%time Models(ExtraTreesClassifier(),X_train1, X_test1, y_train1, y_test1, title, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'ExtraTreesClassifier/ADASYN'\n%time Models(ExtraTreesClassifier(),X_train4, X_test4, y_train4, y_test4, title, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'ExtraTreesClassifier/ADASYN'\n%time Models(ExtraTreesClassifier(n_estimators=305),X_train4, X_test4, y_train4, y_test4, title, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# For conclusion, ExtraTreesClassifier is the recommended model for this topic.\n+ Model | Training set | Testing set | Overal |\n+ KNeighborsClassifier: | 100% | 91.1% | 96.3% |\n+ DecisionTreeClassifier: | 99.6% | 95.3% | 97.6% | \n+ RandomForestClassifier: | 100% | 97.9% | 98.9% | \n+ GradientBoostingClassifier: | 100% | 98.4% | 99.1% |\n+ XGBClassifier: | 100% | 98.5% | 99.2% | \n+ LGBMClassifier: | 100% | 98.5% | 99.2% |\n+ GradientBoostingClassifier: | 99.7% | 98.2% | 98.7% |\n+ ExtraTreesClassifier: | 100% | 98.4% | 99.1% | ===> THIS IS THE BEST ONE","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,auc,roc_curve\n\ntitle = 'RandomForestClassifier using the SMOTE'\ny, ypred =  Models(RandomForestClassifier(),X_train1, X_test1, y_train1, y_test1, title, False)\n\nfpr, tpr, thresholds = roc_curve(y, ypred)\nroc_auc = auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b',label='AUC = %0.3f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.0])\nplt.ylim([-0.1,1.01])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=df_train.drop(columns=['Attrition'])\ny=df_train['Attrition'].values\n    \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 25)\n    \nmodel = ExtraTreesClassifier()\nmodel.fit(X_train,y_train)\nresultmymodel = permutation_importance(model, X_train, y_train, n_repeats=10,random_state=42, n_jobs=2)\nsorted_idx = resultmymodel.importances_mean.argsort()\n\nfig, ax = plt.subplots(figsize=(10,7))\nax.boxplot(resultmymodel.importances[sorted_idx].T,vert=False, labels=X_train.columns[sorted_idx])\nax.set_title(\"Permutation Importances (train set)\")\nfig.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X=df_train.drop(columns=['Attrition'])\n#y=df_train['Attrition'].values\n    \n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 25)\n    \n#model = ExtraTreesClassifier()\n#model.fit(X_test,y_test)\nresultmymodel = permutation_importance(model, X_test, y_test, n_repeats=10,random_state=42, n_jobs=2)\nsorted_idx = resultmymodel.importances_mean.argsort()\n\nfig, ax = plt.subplots(figsize=(10,7))\nax.boxplot(resultmymodel.importances[sorted_idx].T,vert=False, labels=X_test.columns[sorted_idx])\nax.set_title(\"Permutation Importances (test set)\")\nfig.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nCustomer churn prediction is crucial to the long-term financial stability of a company. In this Notebook, we successfully created a machine learning model that's able to predict customer churn with an accuracy higher than 98%.","metadata":{}}]}