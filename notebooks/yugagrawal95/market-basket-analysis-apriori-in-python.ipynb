{"cells":[{"metadata":{"_uuid":"346634847c3c5e74c326fa2e927de776d6a6588f"},"cell_type":"markdown","source":"**Market Basket Analysis using assocition rules - apriori technique in Two ways**"},{"metadata":{"_uuid":"d83854725434ebf80b808a6b883d635ac3171cd1"},"cell_type":"markdown","source":"Association rules analysis is a technique to uncover how items are associated to each other. There are three common ways to measure association.\n\nMeasure 1: Support. This says how popular an itemset is, it is number of times appear in total number of transaction. in other word we say frequency of item.\n    \nMeasure 2: Confidence. This says how likely item Y is purchased when item X is purchased, expressed as {X -> Y}. This is measured by the proportion of transactions with item X, in which item Y also appears. \n    \nMeasure 3: Lift.  it is ratio of expected confidance to observed confidance. it is described as confidance of Y when item X was already known(x/y) to the confidance of Y when X item is unknown. in other words confidance of Y w.r.t. x and confiadnce of Y without X (means both are independent to each other).\n\n**support = occurance of item / total no of transaction.**\n\n**confidance = support ( X Union Y) / support(X).**\n\n**lift = support (X Union Y)/ support(X) * support(Y) .**\n\nFor more info report this [link](https://www.kdnuggets.com/2016/04/association-rules-apriori-algorithm-tutorial.html)\n    \n"},{"metadata":{"trusted":true,"_uuid":"8eeae1ae1fc473fc8d1f07a67981772ce4ec9eb5"},"cell_type":"code","source":"#External package need to install\n!pip install apyori","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e54f39f9d95027f03de6a105807bf0c3b94a4d59"},"cell_type":"code","source":"#import all required packages..\nimport pandas as pd\nimport numpy as np\nfrom apyori import apriori","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9010389e1814455f6cab002ee973558d720168f6"},"cell_type":"code","source":"#loading market basket dataset..\n\ndf = pd.read_csv('../input/basket-optimisation/Market_Basket_Optimisation.csv',header=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"636f8cfe210199e6b2731309d2b7d9166a950930"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f3dbd2a0d3eca79e3cb5d685248222a07f3a93e"},"cell_type":"code","source":"#replacing empty value with 0.\ndf.fillna(0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"183803219cc64a347c36b435eff8ee2f757a8b33"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f4673e83dc22d65bcb5fc3bc752075d71c81891"},"cell_type":"code","source":"#for using aprori need to convert data in list format..\n# transaction = [['apple','almonds'],['apple'],['banana','apple']]....\ntransactions = []\nfor i in range(0,len(df)):\n    transactions.append([str(df.values[i,j]) for j in range(0,20) if str(df.values[i,j])!='0'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5866e48f904afd3b74b9cf8b8efe4506efd4e480"},"cell_type":"code","source":"transactions[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62151154f73de9ea3be1284f3ee9b804e665c1b1"},"cell_type":"code","source":"#Call apriori function which requires minimum support, confidance and lift, min length is combination of item default is 2\".\nrules = apriori(transactions,min_support=0.003,min_confidance=0.2,min_lift=3,min_length=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a31b87cb69a410c8db756f941053ce0d76848fe"},"cell_type":"code","source":"#it generates a set of rules in a generator file...\nrules","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ae063ad3f409626e1fed6b2432d159e6a3d7e7c","_kg_hide-output":true},"cell_type":"code","source":"# all rules need to be converted in a list..\nResults = list(rules)\nResults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6d1bdb35db052eac0fdce71a46a313773bddd82"},"cell_type":"code","source":"#convert result in a dataframe for further operation...\ndf_results = pd.DataFrame(Results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6df62c0506f46b7c5aba205dd12e59ad62c143df"},"cell_type":"code","source":"# as we see order statistics itself a list so need to be converted in proper format..\ndf_results.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf7711b724f2906a0c73cc29f683b5a5734b0c31"},"cell_type":"code","source":"#keep support in a separate data frame so we can use later.. \nsupport = df_results.support","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f48bb932b0266da190da81202ea953f97fd1d60"},"cell_type":"code","source":"'''\nconvert orderstatistic in a proper format.\norder statistic has lhs => rhs as well rhs => lhs we can choose any one for convience i choose first one which is 'df_results['ordered_statistics'][i][0]'\n''' \n\n#all four empty list which will contain lhs, rhs, confidance and lift respectively.\n\nfirst_values = []\nsecond_values = []\nthird_values = []\nfourth_value = []\n\n# loop number of rows time and append 1 by 1 value in a separate list.. first and second element was frozenset which need to be converted in list..\nfor i in range(df_results.shape[0]):\n    single_list = df_results['ordered_statistics'][i][0]\n    first_values.append(list(single_list[0]))\n    second_values.append(list(single_list[1]))\n    third_values.append(single_list[2])\n    fourth_value.append(single_list[3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6bce8d56076e72034cd2cea11f0f6b81658f38a9"},"cell_type":"code","source":"#convert all four list into dataframe for further operation..\nlhs = pd.DataFrame(first_values)\nrhs= pd.DataFrame(second_values)\nconfidance=pd.DataFrame(third_values,columns=['Confidance'])\nlift=pd.DataFrame(fourth_value,columns=['lift'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a46c51eb0ca2b82a4f8967947ec70e76f46c052","_kg_hide-output":true},"cell_type":"code","source":"#concat all list together in a single dataframe\ndf_final = pd.concat([lhs,rhs,support,confidance,lift], axis=1)\ndf_final","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88498a6a76fdefd003066a660005fc45d6f83956"},"cell_type":"code","source":"'''\n we have some of place only 1 item in lhs and some place 3 or more so we need to a proper represenation for User to understand. \n removing none with ' ' extra so when we combine three column in 1 then only 1 item will be there with spaces which is proper rather than none.\n example : coffee,none,none which converted to coffee, ,\n'''\ndf_final.fillna(value=' ', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63dddfe92400416e69c50d7fb1d9f0d628dba075"},"cell_type":"code","source":"#set column name\ndf_final.columns = ['lhs',1,2,'rhs','support','confidance','lift']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"263d8e70087aa19d9f7424ff33171aba499aa793"},"cell_type":"code","source":"#add all three column because those where the lhs itemset only\ndf_final['lhs'] = df_final['lhs']+str(\", \")+df_final[1]+str(\", \")+df_final[2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6927d65223d04325ceb67b18d7a5f3dd17970780"},"cell_type":"code","source":"#drop those 1,2 column because now we already appended to lhs column..\ndf_final.drop(columns=[1,2],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"979b380863e1d375732a2049b2a93b7d6c8869b7"},"cell_type":"code","source":"#this is final output.. you can sort based on the support lift and confidance..\ndf_final.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37fad95b716a1db144a722358f8319148af5b414"},"cell_type":"markdown","source":"> **Other way of doing Apriori in Python. **"},{"metadata":{"_uuid":"4b81a5948df0ec288269de14bfc42d5eed531f88"},"cell_type":"markdown","source":"Why we doing it in this way - \n\n1.  Limitation of first approach was need to converted data in a list fomat. when we see real life a store has many thousands of sku in that case it is computationally expensive.\n2. Apyori package is outdated. i mean there is no recent update from past few years.\n3. Results are coming in improper format which need to represent properly and that need computational operation to perform.\n4. mlxtend used two way based approach which generate frequent itemset and association rules over that. [-check here for more info ](https://towardsdatascience.com/association-rules-2-aa9a77241654)\n4. mlxtend are proper and has community support."},{"metadata":{"trusted":true,"_uuid":"bae69fde5cd7de2c715f98244818b697d7f65f67"},"cell_type":"code","source":"'''\nload apriori and association package from mlxtend. \nUsed different dataset because mlxtend need data in below format. \n\n             itemname  apple banana grapes\ntransaction  1            0    1     1\n             2            1    0     1  \n             3            1    0     0\n             4            0    1     0\n             \n we could have used above data as well but need to perform operation to bring in this format instead of that used seperate data only.            \n'''\n\n\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules\n\ndf1 = pd.read_csv('../input/ecommerce-data/data.csv', encoding=\"ISO-8859-1\")\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b869156690e357ab9d72778e3ba69552e6a4632f"},"cell_type":"code","source":"# data has many country choose any one for check..\ndf1.Country.value_counts().head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64e8bc19acc300699e2c4e08c956ca53531a141c"},"cell_type":"code","source":"#using only France country data for now can check for other as well..\ndf1 = df1[df1.Country == 'France']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"556fc5d1c636d5fe5446616f0b62afabe173fc02"},"cell_type":"code","source":"# some spaces are there in description need to remove else later operation it will create problem..\ndf1['Description'] = df1['Description'].str.strip()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d64fedb4166b8e83abf455789a57c950f36cf943"},"cell_type":"code","source":"#some of transaction quantity is negative which can not be possible remove that.\ndf1 = df1[df1.Quantity >0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12361137c1bc7bc5e559262cbe67528941cc23f5"},"cell_type":"code","source":"df1[df1.Country == 'France'].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f8ba588c00ed64a579e872abebd7c0b9f8088e8"},"cell_type":"code","source":"#convert data in format which it require converting using pivot table and Quantity sum as values. fill 0 if any nan values\n\nbasket = pd.pivot_table(data=df1,index='InvoiceNo',columns='Description',values='Quantity', \\\n                        aggfunc='sum',fill_value=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32adc08933fe8c3d961ec042304ee56f57660e35"},"cell_type":"code","source":"basket.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74c18efa837907b4a1a6d99ad7163812bb66c5f0"},"cell_type":"code","source":"#this to check correctness after binning it to 1 at below code..\nbasket['ALARM CLOCK BAKELIKE RED'].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b849345a1c6b4ba2c8af6ccd5b8259c52d93ca2e"},"cell_type":"code","source":"# we dont need quantity sum we need either has taken or not so if user has taken that item mark as 1 else he has not taken 0.\n\ndef convert_into_binary(x):\n    if x > 0:\n        return 1\n    else:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b65902345422094789cd85bbdeb2d2c8f22d4a58"},"cell_type":"code","source":"basket_sets = basket.applymap(convert_into_binary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a9568d732d9a1ee43921af8b1c1360dd11a82ff"},"cell_type":"code","source":"# above steps we can same item has quantity now converted to 1 or 0.\nbasket_sets['ALARM CLOCK BAKELIKE RED'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8913f5904549dbd50f24f9fbc9949daf59b29596"},"cell_type":"code","source":"#remove postage item as it is just a seal which almost all transaction contain. \nbasket_sets.drop(columns=['POSTAGE'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7eb9a2960368623c466713fcafc19389997a1b74"},"cell_type":"code","source":"#call apriori function and pass minimum support here we are passing 7%. means 7 times in total number of transaction that item was present.\nfrequent_itemsets = apriori(basket_sets, min_support=0.07, use_colnames=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"354f06be1525ac12a6c7f57f561b8cabd8487b5f"},"cell_type":"code","source":"#it will generate frequent itemsets using two step approch\nfrequent_itemsets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17c7b926a354831f08a23edfb29c4a0aee7a6a2c"},"cell_type":"code","source":"# we have association rules which need to put on frequent itemset. here we are setting based on lift and has minimum lift as 1\nrules_mlxtend = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\nrules_mlxtend.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0eb5af08f401c56026c61812b1c7b3a736065ce8"},"cell_type":"code","source":"# rules_mlxtend.rename(columns={'antecedents':'lhs','consequents':'rhs'})\n\n# as based business use case we can sort based on confidance and lift.\nrules_mlxtend[ (rules_mlxtend['lift'] >= 4) & (rules_mlxtend['confidence'] >= 0.8) ]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c860dfd582af14ae353f51b4f2569732fb71b07f"},"cell_type":"markdown","source":"Directed graph below is built for this rule and shown below. it will have always incoming and outcoming edges. Incoming edge(s) will represent antecedants and the stub (arrow) will be next to node."},{"metadata":{"trusted":true,"_uuid":"d5fe9cba9cc1aa71cb1d75a70303de386e2f4790"},"cell_type":"code","source":"#plotting output in a graph plot.\n\nimport networkx as nx\nimport matplotlib.pyplot as plt  \n\ndef draw_graph(rules, rules_to_show):\n    G1 = nx.DiGraph()\n    color_map=[]\n    N = 50\n    colors = np.random.rand(N)    \n    strs=['R0', 'R1', 'R2', 'R3', 'R4', 'R5', 'R6', 'R7', 'R8', 'R9', 'R10', 'R11']\n\n    for i in range(rules_to_show):\n        G1.add_nodes_from([\"R\"+str(i)])\n        for a in rules.iloc[i]['antecedents']:\n            G1.add_nodes_from([a])\n            G1.add_edge(a, \"R\"+str(i), color=colors[i] , weight = 2)\n        for c in rules.iloc[i]['consequents']:\n            G1.add_nodes_from([c])\n            G1.add_edge(\"R\"+str(i), c, color=colors[i],  weight=2)\n\n    for node in G1:\n        found_a_string = False\n        for item in strs: \n            if node==item:\n                found_a_string = True\n        if found_a_string:\n            color_map.append('yellow')\n        else:\n            color_map.append('green')       \n\n    edges = G1.edges()\n    colors = [G1[u][v]['color'] for u,v in edges]\n    weights = [G1[u][v]['weight'] for u,v in edges]\n\n    pos = nx.spring_layout(G1, k=16, scale=1)\n    nx.draw(G1, pos, edges=edges, node_color = color_map, edge_color=colors, width=weights, font_size=16, \n            with_labels=False)            \n\n    for p in pos:  # raise text positions\n        pos[p][1] += 0.07\n        nx.draw_networkx_labels(G1, pos)\n        plt.show()\n\ndraw_graph (rules_mlxtend, 10) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"559a6e1b2c0de51ece8bb4c317769d150f3afa2c"},"cell_type":"markdown","source":"**I hope it might helped you. your suggestions and feedback are always appreciated. Thumps up and let me know your view through comments.**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}