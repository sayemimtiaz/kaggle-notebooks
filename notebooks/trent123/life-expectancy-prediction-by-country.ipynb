{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"import pandas as pd \nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport math\nimport seaborn as sns\nimport sklearn\nfrom sklearn import linear_model\nfrom sklearn import preprocessing\nimport statsmodels.api as sm\nimport pylab \nimport scipy.stats as stats\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn import ensemble\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"df=pd.read_csv('LifeExpectancyData.csv')\npd.set_option('display.max_columns', None) \ndf.columns=['Country', 'Year', 'Status', 'Life Expectancy', 'Adult Mortality',\n       'Infant Deaths', 'Alcohol', 'Percent Expenditure', 'Hep B',\n       'Measles', 'BMI', 'U-5 Deaths', 'Polio', 'Total Expenditure',\n       'Diphtheria', 'HIVAIDS','GDP', 'Population', 'Thinness 10-19',\n       'Thinness 5-9', 'Income Composition', 'Schooling']\n#Canada and France are mislabeled as Developing\ndf[df['Country']=='France']['Status'].replace('Developing','Developed')\ndf[df['Country']=='Canada']['Status'].replace('Developing','Developed')\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Variables\nCountry- Country    \nYear- Year    \nStatus- Developed or Developing status    \nLife Expectancy- Age(years)   \nAdult Mortality- Adult Mortality Rates of both sexes(probability of dying between 15&60 years per 1000 population)         \nInfant Deaths- Number of Infant Deaths per 1000 population        \nAlcohol- Alcohol, recorded per capita (15+) consumption (in litres of pure alcohol)  \nPercent Expenditure- Expenditure on health as a percentage of Gross Domestic Product per capita(%)   \nHep B- Hepatitis B (HepB) immunization coverage among 1-year-olds(%)   \nMeasles- number of reported measles cases per 1000 population    \nBMI- Average Body Mass Index of entire population    \nU-5 Deaths- Number of under-five deaths per 1000 population    \nPolio- Polio(Pol3) immunization coverage among 1-year-olds(%)    \nTotal Expenditure- General government expenditure on health as a percentage of total government expenditure(%)    \nDiphtheria- Diphtheria tetanus toxoid and pertussis (DTP3) immunization coverage among 1-year-olds(%)      \nHIV/AIDS- Deaths per 1000 live births HIV/AIDS(0-4 years)    \nGDP- Gross Domestic Product per capita(in USD)   \nPopulation- Population \nThinness 10-19- Prevalence of thinness among children and adolescents for Age 10 to 19(%)    \nThinness 5-9- Prevalence of thinness among children for Age 5 to 9(%)    \nIncome Composition- Human Development Index in terms of income composition of resources(0-1)  \nSchooling- Number of years of Schooling  \n\nDataset found at:https://www.kaggle.com/kumarajarshi/life-expectancy-who"},{"metadata":{"trusted":false},"cell_type":"code","source":"import missingno as msno\nprint(msno.matrix(df))\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Population and GDP will be dropped due to discrepancies with the data. They would useful to have, but it is more likely to hinder the data due to poor reporting from there. The combination of the features will be a good litmus to meaure life expectancy.  "},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"#Drop Population and GDP\ndf=df.drop(['Population','GDP'],axis=1)\n\n#Replace Missing Values Associated with Country Feature Mean\nfor column in df.columns:\n    for i in range(len(df)): \n        country=df['Country'][i]\n        status=df['Country'][i]\n        if (df[column].isnull()[i]==True):\n            df[column][i]=df[df['Country']==country][column].mean() \n        else:\n             pass\n#Fill Unresolved Values by Status\ndf1=df[(df['Status']=='Developed')].fillna(df[(df['Status']=='Developed')].mean())\ndf2=df[(df['Status']=='Developing')].fillna(df[(df['Status']=='Developing')].mean())\ndf=df2.append(df1)\nprint(df.shape)\nprint(msno.matrix(df))\n\n\n###\n\n####","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Features with Outliers \nnumcol=['Life Expectancy', 'Adult Mortality',\n       'Infant Deaths', 'Alcohol', 'Percent Expenditure', 'Hep B',\n       'Measles', 'BMI', 'U-5 Deaths', 'Polio', 'Total Expenditure',\n       'Diphtheria', 'HIV/AIDS', 'Thinness 10-19', 'Thinness 5-9',\n       'Income Composition', 'Schooling']\nfor column in numcol:\n    if df[column].quantile(.9973)<df[column].max():\n        print(column)\n        print('99th Percentile',df[column].quantile(.9973))\n        print('Max',df[column].max())\n        print('Outliers Present in Column {}'.format(column))\n        print('')\n    elif df[column].quantile(0)>df[column].min():\n        print(column)\n        print('99th Percentile',df[column].quantile(.9973))\n        print('Min',df[column].min())\n        print('Outliers Present in Column {}'.format(column))\n        print('')\n    else:\n        pass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":false},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Life Expectancy \nsns.distplot(df['Life Expectancy'])\nplt.axvline(df['Life Expectancy'].mean(),0,.6,color='black')\nplt.axvline(df['Life Expectancy'].mean()+df['Life Expectancy'].std(),0,.45,color='black',linestyle='--')\nplt.axvline(df['Life Expectancy'].mean()-df['Life Expectancy'].std(),0,.45,color='black',linestyle='--')\nplt.axvline(df['Life Expectancy'].mean()+2*df['Life Expectancy'].std(),0,.30,color='black',linestyle='--')\nplt.axvline(df['Life Expectancy'].mean()-2*df['Life Expectancy'].std(),0,.30,color='black',linestyle='--')\nplt.axvline(df['Life Expectancy'].mean()-3*df['Life Expectancy'].std(),0,.15,color='black',linestyle='--')\nsns.set(rc={'figure.figsize':(10,10)})\nplt.show()\n\n#QQ plot   \nstats.probplot(df['Life Expectancy'], dist=\"norm\", plot=plt)\nplt.title('Life Expectancy QQ Plot')\nplt.show()\nprint(stats.shapiro(df['Life Expectancy']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The maximum value 89.0 is about 2.08 standard deviations away from the mean 69.2 while the minimum 36.3 is about 3.46 deviations away. The standard deviation for the whole sample is 9.50 years.Shapiro Wilk's p-value is more valid with over 5000 data points, but using the QQ plot and the Wilk statistic is trending close to normality."},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"#Life Expectancy \nsns.distplot(df[df['Status']=='Developed']['Life Expectancy'])\nsns.distplot(df[df['Status']=='Developing']['Life Expectancy'],color='y')\nlabels=['Developed','Developing']\nplt.legend(labels=labels,bbox_to_anchor=(1.02, 1), loc=2, borderaxespad=0.)\nsns.set(rc={'figure.figsize':(10,10)})\nplt.show()\n\n#QQ plot   \nstats.probplot(df[df['Status']=='Developed']['Life Expectancy'], dist=\"norm\", plot=plt)\nplt.title('Life Expectancy Developed Countries QQ Plot')\nprint(stats.shapiro(df['Life Expectancy']))\nplt.show()\n#QQ plot   \nstats.probplot(df[df['Status']=='Developing']['Life Expectancy'], dist=\"norm\", plot=plt)\nplt.title('Life Expectancy Developing Countries QQ Plot')\nprint(stats.shapiro(df['Life Expectancy']))\nplt.show()\n\ndf['Life Expectancy'].groupby(df['Status']).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"bins=[36,60,78,90]\nlabels=[3,2,1]\ndf['world']=pd.cut(df['Life Expectancy'],bins=bins,labels=labels)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"#Life Expectancy \nsns.distplot(df[df['world']==1]['Life Expectancy'])\nsns.distplot(df[df['world']==2]['Life Expectancy'],color='y')\nsns.distplot(df[df['world']==3]['Life Expectancy'],color='r')\n\nlabels=['1st World','2nd World','3rd World']\nplt.legend(labels=labels,bbox_to_anchor=(1.02, 1), loc=2, borderaxespad=0.)\nsns.set(rc={'figure.figsize':(10,10)})\nplt.show()\n\n#QQ plot   \nstats.probplot(df[df['world']==1]['Life Expectancy'], dist=\"norm\", plot=plt)\nplt.title('Life Expectancy 1st World Countries QQ Plot')\nprint(stats.shapiro(df['Life Expectancy']))\nplt.show()\n#QQ plot   \nstats.probplot(df[df['world']==2]['Life Expectancy'], dist=\"norm\", plot=plt)\nplt.title('Life Expectancy 2nd World Countries QQ Plot')\nprint(stats.shapiro(df['Life Expectancy']))\nplt.show()\n\nstats.probplot(df[df['world']==3]['Life Expectancy'], dist=\"norm\", plot=plt)\nplt.title('Life Expectancy 3rd World Countries QQ Plot')\nprint(stats.shapiro(df['Life Expectancy']))\nplt.show()\n\ndf['Life Expectancy'].groupby(df['world']).describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The cut off boundary age between developing and developed is 69.9. The countries will be further categorized into 3 catergories 1st, 2nd and 3rd world countries. 1st world countries are considered over 69.9 while 2nd world countries are over 60. The QQ plots broken up by status show better normality versus the overall histogram."},{"metadata":{"trusted":false},"cell_type":"code","source":"LEcountry=df.groupby(df['Country'])['Life Expectancy'].mean().sort_values(kind=\"quicksort\",ascending=False)\nsns.pointplot(y='Country',x='Life Expectancy',hue='Status',data=df,order=LEcountry.index,join=True)\nplt.title( 'Life Expectancy by Country')\nplt.axvline(78,0,10,color='g')\nplt.axvline(df['Life Expectancy'].mean()-df['Life Expectancy'].std(),0,10,color='r')\nsns.set(rc={'figure.figsize':(20,40)})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.countplot(df['Status'])\nprint('Developed or Developing Country Status')\nprint(df.Status.value_counts()/len(df.Status))\nsns.set(rc={'figure.figsize':(10,10)})\nplt.show()\n\nsns.countplot(df['world'])\nprint('1st,2nd,and 3rd World Countries')\nprint(((df.world.value_counts()/len(df.world))*193).round(0))\nsns.set(rc={'figure.figsize':(10,10)})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def LEfactorplot(column):\n    x=df[df['Status']=='Developed'][column]\n    y=df[df['Status']=='Developed']['Life Expectancy']\n    x1=df[df['Status']=='Developing'][column]\n    y1=df[df['Status']=='Developing']['Life Expectancy']\n    #Fit Lines\n    z1 =np.polyfit(x,y,1)\n    z2 =np.polyfit(x1,y1,1)\n    z1poly = np.poly1d(z1) \n    z2poly = np.poly1d(z2)\n\n    #Plot\n    plt.scatter(x,y,alpha=1)\n    plt.scatter(x1,y1,alpha=1)\n    plt.plot(x,z1poly(x),linewidth=7.0)\n    plt.plot(x1,z2poly(x1),linewidth=7.0,color='r')\n    labels=['Developed','Developing']\n    plt.legend(labels=labels,bbox_to_anchor=(1.02, 1), loc=2, borderaxespad=0.)\n    plt.ylabel('Life Expectancy')\n    plt.xlabel(column)\nLEfactorplot('Year')\nplt.xticks(np.arange(2000,2016,1))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"corrmat = df.corr()\nmask = np.zeros_like(corrmat, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\ntop_corr_features = corrmat.index\nplt.figure(figsize=(15,15))\n#plot heat map\nsns.heatmap(df[top_corr_features].corr(),annot=True,mask=mask)\nsol = (corrmat.where(np.triu(np.ones(corrmat.shape), k=1).astype(np.bool))\n                 .stack().sort_values(kind=\"quicksort\",ascending=False))\nLE=pd.Series(corrmat.unstack()[18:36]).sort_values(kind=\"quicksort\",ascending=False)\nprint('Correlation Values for the {} countries left after Data Cleaning:'.format(len(df['Country'].unique())))\nLE[1:18]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Top 10 Correlated Features Pairs:')\nprint(sol[0:10],'\\n')\nprint('Bottom 10 Correlated Features Pairs:')\nprint(sol[143:153])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"columns=['Alcohol','BMI','Hep B','Measles','Polio','Diphtheria','HIVAIDS','Thinness 10-19',\n         'Thinness 5-9','Adult Mortality','Infant Deaths','U-5 Deaths','Percent Expenditure'\n         ,'Total Expenditure','Income Composition','Schooling']\n\nfor column,i in zip(columns,range(len(columns))):\n    plt.subplot(4,4,i+1)\n    sns.distplot(df[column])  \n    plt.tight_layout()\n    sns.set(rc={'figure.figsize':(20,20)})","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"for column in df.columns:\n    if (column=='Country')or(column=='Status')or(column=='Life Expectancy')or(column=='world')or(column=='Year'):\n        pass\n    else:\n        LEfactorplot(column)\n        sns.set(rc={'figure.figsize':(10,10)})\n        sns.set(font_scale=1.5)\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Regression "},{"metadata":{"trusted":false},"cell_type":"code","source":"#Encode Country and Create copy of dataframe for regression \ndf_reg=df.copy()\nfrom sklearn.preprocessing import LabelEncoder\nlb_make = LabelEncoder()\ndf_reg[\"country_code\"] = lb_make.fit_transform(df_reg[\"Country\"])\n\n\n#Binarize Status\ndf_reg['Status']=np.where(df_reg['Status']=='Developing',0,1)\n\ndf_reg.columns=['Country', 'Year', 'Status', 'Life_Expectancy', 'Adult_Mortality',\n       'Infant_Deaths', 'Alcohol', 'Percent_Expenditure', 'Hep_B', 'Measles',\n       'BMI', 'U_5_Deaths', 'Polio', 'Total_Expenditure', 'Diphtheria',\n       'HIV_AIDS', 'Thinness_10_19', 'Thinness_5_9', 'Income_Composition',\n       'Schooling', 'world', 'country_code']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#remove outliers\nfor col in df_reg.columns:\n    if (col=='world')or (col=='Country'):\n        pass\n    else:\n        df_reg=df_reg[np.abs(df_reg[col]-df_reg[col].mean())<=(3*df_reg[col].std())]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Parameter "},{"metadata":{"trusted":false},"cell_type":"code","source":"#Developed Training Data\nX_developed_train=df_reg[(df_reg['Status']==1)&(df_reg['Year']<2011)].drop('Life_Expectancy',axis=1)\nY_developed_train=df_reg[(df_reg['Status']==1)&(df_reg['Year']<2011)]['Life_Expectancy']\n\n#Developed Testing Data\nX_developed_test=df_reg[(df_reg['Status']==1)&(df_reg['Year']>2011)].drop('Life_Expectancy',axis=1)\nY_developed_test=df_reg[(df_reg['Status']==1)&(df_reg['Year']>2011)]['Life_Expectancy']\n\n#Developing Training Data\nX_developing_train=df_reg[(df_reg['Status']==0)&(df_reg['Year']<2011)].drop('Life_Expectancy',axis=1)\nY_developing_train=df_reg[(df_reg['Status']==0)&(df_reg['Year']<2011)]['Life_Expectancy']\n\n#Developing Testing Data\nX_developing_test=df_reg[(df_reg['Status']==0)&(df_reg['Year']>2011)].drop('Life_Expectancy',axis=1)\nY_developing_test=df_reg[(df_reg['Status']==0)&(df_reg['Year']>2011)]['Life_Expectancy']\n\n#World=1 Training Data\nX_world1_train=df_reg[(df_reg['world']==1)&(df_reg['Year']<2011)].drop('Life_Expectancy',axis=1)\nY_world1_train=df_reg[(df_reg['world']==1)&(df_reg['Year']<2011)]['Life_Expectancy']\n\n#World=1 Testing Data\nX_world1_test=df_reg[(df_reg['world']==1)&(df_reg['Year']>2011)].drop('Life_Expectancy',axis=1)\nY_world1_test=df_reg[(df_reg['world']==1)&(df_reg['Year']>2011)]['Life_Expectancy']\n\n#World=2 Training Data\nX_world2_train=df_reg[(df_reg['world']==2)&(df_reg['Year']<2011)].drop('Life_Expectancy',axis=1)\nY_world2_train=df_reg[(df_reg['world']==2)&(df_reg['Year']<2011)]['Life_Expectancy']\n\n#World=2 Testing Data\nX_world2_test=df_reg[(df_reg['world']==2)&(df_reg['Year']>2011)].drop('Life_Expectancy',axis=1)\nY_world2_test=df_reg[(df_reg['world']==2)&(df_reg['Year']>2011)]['Life_Expectancy']\n\n#World=3 Training Data\nX_world3_train=df_reg[(df_reg['world']==3)&(df_reg['Year']<2011)].drop('Life_Expectancy',axis=1)\nY_world3_train=df_reg[(df_reg['world']==3)&(df_reg['Year']<2011)]['Life_Expectancy']\n\n#World=3 Testing Data\nX_world3_test=df_reg[(df_reg['world']==3)&(df_reg['Year']>2011)].drop('Life_Expectancy',axis=1)\nY_world3_test=df_reg[(df_reg['world']==3)&(df_reg['Year']>2011)]['Life_Expectancy']\n\n#Full Training Set\nX_train=df_reg[df_reg['Year']<2011].drop('Life_Expectancy',axis=1)\nY_train=df_reg[df_reg['Year']<2011]['Life_Expectancy']\n\n#Full Testing Set\nX_test=df_reg[df_reg['Year']>2011].drop('Life_Expectancy',axis=1)\nY_test=df_reg[df_reg['Year']>2011]['Life_Expectancy']\n\n#Full Set\nX=df_reg.drop('Life_Expectancy',axis=1)\nY=df_reg['Life_Expectancy']\n\n#Breakdown\nXlist=[X_developed_train,X_developed_test,X_developing_train,X_developing_test,\n       X_world1_train,X_world1_test,X_world2_train,X_world2_test,X_world3_train,X_world3_test,\n       X_train,X_test,X]\nYlist=[Y_developed_train,Y_developed_test,Y_developing_train,Y_developing_test,\n       Y_world1_train,Y_world1_test,Y_world2_train,Y_world2_test,Y_world3_train,Y_world3_test,\n       Y_train,Y_test,Y]\nxlist=['X_developed_train','X_developed_test','X_developing_train','X_developing_test',\n       'X_world1_train','X_world1_test','X_world2_train','X_world2_test','X_world3_train','X_world3_test',\n       'X_train','X_test','X']\nstatus=['Developed','Developed','Developing','Developing',\n                   '1st World','1st World','2nd World','2nd World','3rd World','3rd World',\n                   'Full Training','Full Testing','Full']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_reg['Life_Expectancy'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Thiel-sen Regression"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import TheilSenRegressor\nfrom sklearn.preprocessing import scale\nfrom pylab import rcParams\nfor x,y,i,z,s in zip(Xlist,Ylist,range(len(Xlist)),xlist,status):\n    x=x.drop(['Country','world','Status','Infant_Deaths','Thinness_10_19'],axis=1)\n    x=scale(x)\n    if i==0:\n        print('Thiel {}'.format(z))\n        print(z,x.shape)\n        #Model\n        theil = TheilSenRegressor(random_state=52).fit(x,y)\n        \n        #R2 \n        R=theil.score(x,y)\n        print('R^2 Score:{:0.4f}'.format(R))\n\n        #Predictions\n        Y_pred=theil.predict(x)\n        RMSE=mean_squared_error(y, Y_pred)**0.5\n        print('RMSE: {:0.3f}'.format(RMSE))\n        print('Minimum LE: {:0.1f}'.format(Y_pred.min()))\n        print('Maximum LE: {:0.1f}'.format(Y_pred.max()))\n        print('Average Predicted LE: {:0.1f}'.format(Y_pred.mean()))\n        print('LE Standard Deviation: {:0.3f}'.format(Y_pred.std()))\n        print('LE Variance: {:0.3f}'.format(Y_pred.std()**2))\n        \n        #plot\n        z1=np.polyfit(Y_pred,y,1)\n        z1poly = np.poly1d(z1) \n        plt.scatter(Y_pred,y,alpha=1)\n        plt.plot(Y_pred,z1poly(Y_pred),linewidth=7.0,color='r')\n        plt.title('Thiel {}'.format(z))\n        plt.xlabel('Y_pred')\n        plt.ylabel('Y')\n        rcParams['figure.figsize'] = 10, 10\n        plt.show()\n        \n        #Result DataFrame\n        results = pd.DataFrame()\n        results[\"Method\"]=['Thiel']\n        results['Set']=z\n        results['Status']=s\n        results['Datapoint Count']=x.shape[0]*x.shape[1]\n        results[\"RMSE\"] = RMSE.round(2)\n        results[\"R^2\"] = R.round(2)\n        results['LE Min']=Y_pred.min().round(1)\n        results['LE Max']=Y_pred.max().round(1)\n        results['Average LE']=Y_pred.mean().round(1)\n        results['LE Std']=Y_pred.std().round(2)\n        results['LE Var']=(Y_pred.std()**2).round(1)\n\n    else:\n        print('Thiel {}'.format(z))\n        print(z,x.shape)\n        #Model\n        theil = TheilSenRegressor(random_state=52).fit(x,y)\n        \n        #R2 \n        R=theil.score(x,y)\n        print('R^2 Score:{:0.4f}'.format(R))\n\n        #Predictions\n        Y_pred=theil.predict(x)\n        RMSE=mean_squared_error(y, Y_pred)**0.5\n        print('RMSE: {:0.3f}'.format(RMSE))\n        print('Minimum LE: {:0.1f}'.format(Y_pred.min()))\n        print('Maximum LE: {:0.1f}'.format(Y_pred.max()))\n        print('Average Predicted LE: {:0.1f}'.format(Y_pred.mean()))\n        print('LE Standard Deviation: {:0.3f}'.format(Y_pred.std()))\n        print('LE Variance: {:0.3f}'.format(Y_pred.std()**2))\n        \n        #plot\n        z1=np.polyfit(Y_pred,y,1)\n        z1poly = np.poly1d(z1) \n        plt.scatter(Y_pred,y,alpha=1)\n        plt.plot(Y_pred,z1poly(Y_pred),linewidth=7.0,color='r')\n        plt.title('Thiel {}'.format(z))\n        plt.xlabel('Y_pred')\n        plt.ylabel('Y')\n        rcParams['figure.figsize'] = 10, 10\n        plt.show()\n        \n        #Add to results\n        results.loc[i] = ['Thiel',z,s,x.shape[0]*x.shape[1]\n                          ,RMSE.round(3)\n                          ,R.round(4)\n                          ,Y_pred.min().round(1)\n                          ,Y_pred.max().round(1)\n                          ,Y_pred.mean().round(1)\n                          ,Y_pred.std().round(3)\n                          ,(Y_pred.std()**2).round(3)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ridge"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"for x,y,i,z,s in zip(Xlist,Ylist,range(len(Xlist)),xlist,status):\n    x=x.drop(['Country','world','Status'],axis=1)\n    x=scale(x)\n    print('Ridge {}'.format(z))\n    print(z,x.shape)\n    #Model\n    ridgeregr = linear_model.Ridge(alpha=10, fit_intercept=True,solver='auto',random_state=65)\n    ridge= ridgeregr.fit(x,y)\n\n    #R2 \n    R=ridge.score(x,y)\n    print('R^2 Score: {:0.4f}'.format(R))\n\n    #Predictions\n    Y_pred=ridge.predict(x)\n    RMSE=mean_squared_error(y, Y_pred)**0.5\n    print('RMSE: {:0.3f}'.format(RMSE))\n    print('Minimum LE: {:0.1f}'.format(Y_pred.min()))\n    print('Maximum LE: {:0.1f}'.format(Y_pred.max()))\n    print('Average Predicted LE: {:0.1f}'.format(Y_pred.mean()))\n    print('LE Standard Deviation: {:0.3f}'.format(Y_pred.std()))\n    print('LE Variance: {:0.3f}'.format(Y_pred.std()**2))\n    \n    #plot\n    z1 =np.polyfit(Y_pred,y,1)\n    z1poly = np.poly1d(z1) \n    plt.scatter(Y_pred,y,alpha=1)\n    plt.plot(Y_pred,z1poly(Y_pred),linewidth=7.0,color='r')\n    plt.title('Ridge {}'.format(z))\n    plt.xlabel('Y_pred')\n    plt.ylabel('Y')\n    plt.show()\n        \n    #Add to results\n    results.loc[i+13] = ['Ridge',z,s,x.shape[0]*x.shape[1]\n                          ,RMSE.round(3)\n                          ,R.round(4)\n                          ,Y_pred.min().round(1)\n                          ,Y_pred.max().round(1)\n                          ,Y_pred.mean().round(1)\n                          ,Y_pred.std().round(3)\n                          ,(Y_pred.std()**2).round(3)]        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gradient Boosting"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"cols=['Year','AdultMortality', 'Infant Deaths','Alcohol', 'PercentExpenditure', 'Hep B', 'Measles', 'BMI', 'U5Deaths',\n       'Polio', 'TotalExpenditure', 'Diphtheria', 'HIVAIDS', 'Thinness1019',\n       'Thinness59', 'IncomeComposition', 'Schooling','country_code']\nfeature_importances=pd.DataFrame(index=cols)\nfor x,y,i,z,s in zip(Xlist,Ylist,range(len(Xlist)),xlist,status):\n    x\n    y\n    x=x.drop(['Country','world','Status'],axis=1)\n    x=scale(x)\n    print('Random Forest {}'.format(z))\n    print(z,x.shape)\n    #Model\n    params = {'n_estimators':100,'max_depth': 3}\n    rf = ensemble.GradientBoostingRegressor(**params)\n    rfc= rf.fit(x,y)\n\n    #R2 \n    R=rfc.score(x,y)\n    print('R^2 Score: {:0.4f}'.format(R))\n\n    #Predictions\n    Y_pred=rf.predict(x)\n    RMSE=mean_squared_error(y, Y_pred)**0.5\n    print('RMSE: {:0.3f}'.format(RMSE))\n    print('Minimum LE: {:0.1f}'.format(Y_pred.min()))\n    print('Maximum LE: {:0.1f}'.format(Y_pred.max()))\n    print('Average Predicted LE: {:0.1f}'.format(Y_pred.mean()))\n    print('LE Standard Deviation: {:0.3f}'.format(Y_pred.std()))\n    print('LE Variance: {:0.3f}'.format(Y_pred.std()**2))\n        \n    #plot\n    z1 =np.polyfit(Y_pred,y,1)\n    z1poly = np.poly1d(z1) \n    plt.scatter(Y_pred,y,alpha=1)\n    plt.plot(Y_pred,z1poly(Y_pred),linewidth=7.0,color='r')\n    plt.title('Gradient Boosting {}'.format(z))\n    plt.xlabel('Y_pred')\n    plt.ylabel('Y')\n    plt.show()\n    \n    #Feature Importance\n    feature_importances[z]=(rfc.feature_importances_*100).round(2)\n    print('Top 5 Features\\n',feature_importances[z].nlargest(5).round(2),'\\n')\n        \n    #Add to results\n    results.loc[i+26] = ['Gradient Boosting',z,s,x.shape[0]*x.shape[1]\n                          ,RMSE.round(3)\n                          ,R.round(4)\n                          ,Y_pred.min().round(1)\n                          ,Y_pred.max().round(1)\n                          ,Y_pred.mean().round(1)\n                          ,Y_pred.std().round(3)\n                          ,(Y_pred.std()**2).round(3)] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"feature_importances","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"regsum=results.groupby(['Status','Method']).mean()\nregsum","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Theil is the worst performing model, but is runs better with more data. This should be antipicated due to it being a financial/economic model which it better suited for more data. Ridge is a mixed bag of results, but showed some promising results. \n\nGradient Boosting is the chosen model for regression due its consistent performance and ability to deal with all types and set of data. It captures the LE range,mean, and standard deviation and allows the user to see which features play a key factor in model performance. Given the lower amount of data for this model, it works well and quickly with more data and factors, it is likely to fall off. This is however very promising given the amount of cleaning present at the beginning. "},{"metadata":{},"cell_type":"markdown","source":"# Clustering "},{"metadata":{},"cell_type":"markdown","source":"# Methods\n\nK-Means and Mean-Shift will be compared with the following scores and the cluster percentages of the each method. \n\nK-Means Clustering works by assigning a number of clusters and matching each data point with the nearest centroid. The mean average is taken of all the points and the process is repeated until variance between the new and old clusters fall below a specified threshold value. It is a commonly run clustering technique.  \n\nMean-Shift Clustering is also an iterative process, but has no input on the number of clusters. It calculates the probability a data point falls within in n-dimesional space based on the features of the input file. The data points try to move up towards peaks in small steps. The peaks are determined by the bandwidth ranging from 0 to 1. A higher bandwidth means a smoother surface with defined peaks, while a lower bandwidth has more smaller peaks across a rougher surface. Once a data point reaches the top of the peak, it stops. After all the data points are settled, the clusters means are formed.  \n\n\nBoth clustering methods are pretty standard and reliable. The number of cluster chosen for k-means will be determined by the scores and running a loop to check the optimal value. The cluster labels will be used to look at the plot again as the hue. \n\n\n# Scores \n\nCalinski-Harabaz Index- additionally known as the Variance Ratio Criterion where a higher score means a better defined cluster. It compares the ratio of the between-clusters dispersion mean and the within-cluster dispersion. Scores are higher when dense and separated from other clusters. The score is normalized with respect to the others scores for comparison in one chart.    \nSource: Caliński, T., & Harabasz, J. (1974). “A dendrite method for cluster analysis”. Communications in   Statistics-theory and Methods 3: 1-27. doi:10.1080/03610926.2011.560741.  \n\nSilhouette Score- is the ratio of  difference between the mean nearest-cluster distance and mean intra-cluster distance over the maximum between both scores. The score ranges from -1 to +1. +1 indicates a highly dense cluster while scores close to 0 indicates overlapping clusters and -1 indicates incorrect clustering.   \ns=(b-a)/max(b,a)    \nmean intra-cluster distance (a)   \nmean nearest-cluster distance (b)    \nSource: Peter J. Rousseeuw (1987). “Silhouettes: a Graphical Aid to the Interpretation and Validation of Cluster Analysis”. Computational and Applied Mathematics 20: 53–65. doi:10.1016/0377-0427(87)90125-7.\n\nHomogeneity Score- Each cluster contains only members of a single class.  \n\nCompleteness Score-All members of a given class are assigned to the same cluster.   \n\nAdjusted Rand Score-The Rand Index compares how pairs of datapoints relate in the ground truth and in the post-clustering assignment.There are four possible types of pair relationships:  \na=Members of the same cluster in the ground truth match same cluster in the new solution.      \nb=Members of the same cluster in the ground truth match different clusters in the new solution.         \nc=Members of different clusters in the ground truth match the same cluster in the new solution.    \nd=Members of different clusters in the ground truth match different clusters in the new solution.     \nE(RI)- expected RI   \nRI=(a+c)/sum(a,b,c,d)  \nARI=(RI-E(RI)/(max(RI)-E(RI))  \n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Mean-Shift Clustering"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"df_cl=df_reg.copy()\nfrom sklearn.preprocessing import scale\nfrom sklearn import metrics\nfrom sklearn.cluster import MeanShift, estimate_bandwidth\nY = df_cl['world']\nX_unscaled = df_cl.drop(['Country','world','Status'],1)\nX=scale(X_unscaled)\n\n# Here we set the bandwidth. This function automatically derives a bandwidth\n# number based on an inspection of the distances among points in the data.\nbandwidth = estimate_bandwidth(X, quantile=0.2,n_samples=200)\n\n# Declare and fit the model.\nms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\nms.fit(X)\ny_pred=ms.predict(X)\n\n# Extract cluster assignments for each data point.\nlabels = ms.labels_\ndf_cl['MS Cluster']=labels\n\n# Coordinates of the cluster centers.\ncluster_centers = ms.cluster_centers_\n\n# Count our clusters.\nn_clusters_ = len(np.unique(labels))\n\nprint(\"Number of estimated clusters: {}\".format(n_clusters_))\nprint('Cal Harabaz Score: {}'.format(metrics.calinski_harabaz_score(X, ms.labels_)/1000))\nprint('Silhouette Score: {}'.format(metrics.silhouette_score(X, labels, metric='euclidean')))                            \nprint('Homogenity Score:',metrics.homogeneity_score(y_pred,Y))\nprint('Completeness Score:',metrics.completeness_score(y_pred,Y))\nprint('Adjusted Rand Score:',metrics.adjusted_rand_score(y_pred,Y))\n\nprint('Cluster Percentage')\n((df_cl['MS Cluster'].value_counts()/len(df_cl['MS Cluster'])).round(3))*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These scores are pretty poor in comparison to the K-Means while also not having a great percentage break down. However, 3 cluster is likely a good guess from K-Means initially.  "},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"LEcountry=df_cl.groupby(df_cl['Country'])['Life_Expectancy'].mean().sort_values(kind=\"quicksort\",ascending=False)\nsns.pointplot(y='Country',x='Life_Expectancy',hue='MS Cluster',data=df_cl,order=LEcountry.index,join=False)\nplt.title('Life Expectancy by Country')\nplt.axvline(78,0,10,color='g')\nplt.axvline(df_cl['Life_Expectancy'].mean()-df_cl['Life_Expectancy'].std(),0,10,color='r')\nsns.set(rc={'figure.figsize':(20,40)})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Poor clustering here, not much of a pattern or any clarity."},{"metadata":{},"cell_type":"markdown","source":"No added clarity, other than the other two clusters may represent more of the back end of countries in Life Expectancy Rank. "},{"metadata":{},"cell_type":"markdown","source":"# K-Means"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.cluster import KMeans\n#Scores\ncomplete = []\nhomogenity = []\nsilhouette=[]\ncalinski=[]\nadrs=[]\n\n#Cluster Range\nns = list(range(2,15))\n\n#Inputs and Ground Truth\nY = df_cl['world']\nX_unscaled = df_cl.drop(['Country','world','Status'],1)\nX=scale(X_unscaled)\n\nfor n in ns:\n    km=KMeans(n_clusters=n, random_state=42)\n    km.fit(X)\n    y_pred=km.predict(X)\n    cal=(metrics.calinski_harabaz_score(X, km.labels_)/1000)\n    calinski.append(cal) \n    sil=metrics.silhouette_score(X, km.labels_, metric='euclidean')\n    silhouette.append(sil)\n    comp = metrics.completeness_score(y_pred,Y)\n    complete.append(comp)\n    homog = metrics.homogeneity_score(y_pred,Y)\n    homogenity.append(homog)\n    ar=metrics.adjusted_rand_score(y_pred,Y)\n    adrs.append(ar)\n    \n#Plot\nplt.plot(ns,calinski)\nplt.plot(ns,silhouette)\nplt.plot(ns, complete)\nplt.plot(ns,homogenity)\nplt.plot(ns,adrs)\nplt.title('World Ground Truth')\nplt.xlabel('K Values')\nplt.ylabel('Score')\nplt.legend(['Calinski','Silhouette','Completeness', 'Homogeneity','ARI'],bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.1)\nplt.xticks(np.arange(2,15,1))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"k=3 represents the optimal number of clusters for K-means due to the majority of the scores starting to drop off after that point. Completeness continues to rise, but with more clusters that score should keep increasing due to more more classes and smaller groupings. "},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"Y = df_cl['world']\nX_unscaled = df_cl.drop(['Country','world','Status'],1)\nX=scale(X_unscaled)\nncluster=3\nkm=KMeans(n_clusters=ncluster, random_state=42)\nkm.fit(X)\ny_pred=km.predict(X)\n\n# Extract cluster assignments for each data point.\nlabels = km.labels_\ndf_cl['KM Cluster']=labels\n\n# Coordinates of the cluster centers.\ncluster_centers = km.cluster_centers_\n\nprint(\"Number of estimated clusters: {}\".format(ncluster))\nprint('Cal Harabaz Score: {}'.format(metrics.calinski_harabaz_score(X, km.labels_)/1000))\nprint('Silhouette Score: {}'.format(metrics.silhouette_score(X, labels, metric='euclidean')))  \nprint('Homogenity Score:',metrics.homogeneity_score(y_pred,Y))\nprint('Completeness Score:',metrics.completeness_score(y_pred,Y))\n\nprint('Cluster Percentage')\n((df_cl['KM Cluster'].value_counts()/len(df_cl['KM Cluster'])).round(3))*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(df_cl['Country'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The scores are more promising here. The silhouette score is low than needed, but some overlap is likely expected base on the dataset in how the countries are grouped. The percentage breakdown is much as well. "},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"LEcountry=df_cl.groupby(df_cl['Country'])['Life_Expectancy'].mean().sort_values(kind=\"quicksort\",ascending=False)\nsns.pointplot(y='Country',x='Life_Expectancy',hue='KM Cluster',data=df_cl,order=LEcountry.index,join=False)\nplt.title('Life Expectancy by Country')\nplt.axvline(78,0,10,color='g')\nplt.axvline(df_cl['Life_Expectancy'].mean()-df_cl['Life_Expectancy'].std(),0,10,color='r')\nsns.set(rc={'figure.figsize':(20,40)})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is some overlap, but the general trend seems to somewhat mirror the world binning. "},{"metadata":{"trusted":false},"cell_type":"code","source":"def Clusterplot(column):\n    if (column==\"world\") or (column=='Status'):\n        x=df_cl[df_cl['KM Cluster']==0][column]\n        y=df_cl[df_cl['KM Cluster']==0]['Life_Expectancy']\n        x1=df_cl[df_cl['KM Cluster']==1][column]\n        y1=df_cl[df_cl['KM Cluster']==1]['Life_Expectancy']\n        x2=df_cl[df_cl['KM Cluster']==2][column]\n        y2=df_cl[df_cl['KM Cluster']==2]['Life_Expectancy']\n        #Plot\n        plt.scatter(x,y,alpha=1,color='b')\n        plt.scatter(x1,y1,alpha=1,color='r')\n        plt.scatter(x2,y2,alpha=1,color='g')\n        labels=['0','1','2']\n        plt.legend(labels=labels,bbox_to_anchor=(1.02, 1), loc=2, borderaxespad=0.)\n        plt.ylabel('Life Expectancy')\n        plt.xlabel(column)\n   \n    else:\n        x=df_cl[df_cl['KM Cluster']==0][column]\n        y=df_cl[df_cl['KM Cluster']==0]['Life_Expectancy']\n        x1=df_cl[df_cl['KM Cluster']==1][column]\n        y1=df_cl[df_cl['KM Cluster']==1]['Life_Expectancy']\n        x2=df_cl[df_cl['KM Cluster']==2][column]\n        y2=df_cl[df_cl['KM Cluster']==2]['Life_Expectancy']\n        #Plot\n        plt.scatter(x,y,alpha=1,color='b')\n        plt.scatter(x1,y1,alpha=1,color='r')\n        plt.scatter(x2,y2,alpha=1,color='g')\n        #Fit Lines\n        z1 =np.polyfit(x,y,1)\n        z2 =np.polyfit(x1,y1,1)\n        z3=np.polyfit(x2,y2,1)\n        z1poly = np.poly1d(z1) \n        z2poly = np.poly1d(z2)\n        z3poly= np.poly1d(z3)\n\n        plt.plot(x,z1poly(x),linewidth=7.0,color='b')\n        plt.plot(x1,z2poly(x1),linewidth=7.0,color='r')\n        plt.plot(x2,z3poly(x2),linewidth=7.0,color='g')\n        labels=['0','1','2']\n        plt.legend(labels=labels,bbox_to_anchor=(1.02, 1), loc=2, borderaxespad=0.)\n        plt.ylabel('Life Expectancy')\n        plt.xlabel(column)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"cols=['Year','world','Status','Adult_Mortality','Infant_Deaths', 'Alcohol', 'Percent_Expenditure', 'Hep_B', 'Measles',\n       'BMI', 'U_5_Deaths', 'Polio', 'Total_Expenditure', 'Diphtheria',\n       'HIV_AIDS', 'Thinness_10_19', 'Thinness_5_9', 'Income_Composition',\n       'Schooling']\nfor column in cols:\n    Clusterplot(column)\n    sns.set(rc={'figure.figsize':(10,10)})\n    sns.set(font_scale=1.5)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Schooling, Polio, Year, and Income Composition features really showcase the world feature boundries. There is a clear division between groupings that could be considered in looking what particularly drives or declines Life Expectancy for a country based on the clustering presented. "},{"metadata":{},"cell_type":"markdown","source":"# MIXED EFFECT MODEL"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Adjust Year\nyears=[2001, 2000, 2015, 2014, 2013, 2012, 2011, 2010, 2009, 2008, 2007,\n       2006, 2005, 2004, 2003, 2002]\nyears.sort()\n\nfor year,i in zip(years,range(len(years))):\n    df_reg['Year'][df_reg['Year'] == year]=i+1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mixed Effect Models have the following assumptions:\n1. The features are related linearly to the outcome.\n2. The errors have constant variance: In other words, the model fits equally well for all values of the outcome and features within a level.\n3. The errors are independent: The fit of the model within a group(level 1) is uncorrelated with the fit of the model at between a group (level 2).\n4. The errors are normally distributed.\n5. Observations within a person/group are correlated with one another(high ICC).\n\nModels:     \n    \nRandom Intercepts-individuals within a group are allowed to have different intercepts while all keeping the same  slope. (No RE Added)\n\nRandom Slopes-individuals within a group are allowed to have different slopes while all keeping the same common interval.\n\nRandom Slopes and Intercepts-individuals within a group are allowed to have different slopes and intercepts.  \n\nICC Calculation  \nFirst, ICC will be calculated with Life Expectancy by itself to select the correct grouping. ICC represents the ratio of the between cluster variance over the total variance. The total variance is sum of the within-group and between groups unexpained by the fixed variable.   \n\ngroup var: between-group variance  \nresid: within-group variance  \nICC= group var/(group var +resid)  "},{"metadata":{},"cell_type":"markdown","source":"# Calculate ICC"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Function to calculate the intraclass correlation\ndef ICC(fittedmodel):\n    between_var= fittedmodel.cov_re.iloc[0,0]\n    resid=fittedmodel.scale\n    icc=between_var/(between_var+resid)\n    return icc","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"import statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.regression.mixed_linear_model import MixedLMParams\nfeatures=['Year','Country', 'Status', 'Adult_Mortality',\n       'Infant_Deaths', 'Alcohol', 'Percent_Expenditure', 'Hep_B', 'Measles',\n       'BMI', 'U_5_Deaths', 'Polio', 'Total_Expenditure', 'Diphtheria',\n       'HIV_AIDS', 'Thinness_10_19', 'Thinness_5_9', 'Income_Composition',\n       'Schooling', 'world']\nfor col,i in zip(features,range(len(features))):\n    if i==0:\n        # Model to use for calculating the ICC\n        print('Model {}'.format(col))\n        model = smf.mixedlm(\"Life_Expectancy ~ 1\",data=df_reg,groups=df_reg[col])\n        result = model.fit()\n        print(result.summary())\n        print('The Intraclass Correlation is: {:.3f}'.format(ICC(result)))\n        print('Group Var: {:.2f}\\n'.format(result.cov_re.iloc[0,0]))\n        \n        #DataFrame Columns\n        LMresults=pd.DataFrame(index=range(len(features)))\n        LMresults['Column']=col\n        LMresults['ICC']=ICC(result).round(3)\n        LMresults['Group Var']=result.cov_re.iloc[0,0].round(2)\n    else:\n        # Model to use for calculating the ICC\n        print('Model {}'.format(col))\n        model = smf.mixedlm(\"Life_Expectancy ~ 1\",data=df_reg,groups=df_reg[col])\n        result = model.fit()\n        print(result.summary())\n        print('The Intraclass Correlation is: {:.3f}'.format(ICC(result)))\n        print('Group Var: {:.2f}\\n'.format(result.cov_re.iloc[0,0]))\n        \n        #Add to DataFrame \n        LMresults.loc[i]=[col,ICC(result).round(3),result.cov_re.iloc[0,0].round(2)]     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"LMresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A low ICC score means that observations within groupings are not similiar than observations between the groupings themselves. A higher ICC indicates that there is a high similarity between observations within a group and a mixed effect model is appropriate. The Country model is very appropriate to have a mixed model performed. "},{"metadata":{},"cell_type":"markdown","source":"# Mixed Effect Model Comparison\n\nComparison of Random Intercept(RI), Random Slopes(RS), and Random Intercepts and Slopes Models(RIS):  \n\nFor Chosen Model: \nPlot Residual Distribution   \nCheck Variance with respect to Life Expectancy   \nCheck Variance with respect to Random Effect(if applicable)  "},{"metadata":{},"cell_type":"markdown","source":"# RI Model"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"cols=['Year', 'Status', 'Adult_Mortality','Infant_Deaths', 'Alcohol', 'Percent_Expenditure', 'Hep_B', 'Measles',\n       'BMI', 'U_5_Deaths', 'Polio', 'Total_Expenditure', 'Diphtheria',\n       'HIV_AIDS', 'Thinness_10_19', 'Thinness_5_9', 'Income_Composition',\n       'Schooling', 'world']\nre=['-','-','-','-','-','-','-','-','-','-','-','-','-','-','-','-','-','-','-']\n\nY=df_reg['Life_Expectancy']\nfor col,r,i in zip(cols,re,range(len(re))): \n    if i==0:\n        print('Running Random Intercepts')\n        model = smf.mixedlm(\"Life_Expectancy~{}\".format(col),data=df_reg,groups=df_reg['Country'])\n        riresult = model.fit()\n        print(riresult.summary())\n        print('FE: {}'.format(col))\n        print('The Intraclass Correlation is: {}'.format(ICC(riresult)))\n        print('The Likelihood is: {}'.format(riresult.llf))\n        print('RE Variance is {}'.format(r))\n        \n        #Predicted Values\n        Y_pred = riresult.fittedvalues\n        RMSE=mean_squared_error(Y, Y_pred)**0.5\n        var=Y_pred.std()**2\n        print('RMSE: {:0.3f}'.format(RMSE))\n        print('Minimum LE: {:0.1f}'.format(Y_pred.min()))\n        print('Maximum LE: {:0.1f}'.format(Y_pred.max()))\n        print('Average Predicted LE: {:0.1f}'.format(Y_pred.mean()))\n        print('LE Standard Deviation: {:0.3f}'.format(Y_pred.std()))\n        print('LE Variance: {:0.3f}'.format(var),'\\n')\n        \n        #DataFrame Columns\n        MXLMresults=pd.DataFrame(index=range(11))\n        MXLMresults['Model']='RI'\n        MXLMresults['Fixed Effect']=col\n        MXLMresults['Random Effect']=r\n        MXLMresults['RE VAR']=riresult.cov_re.iloc[0,0].round(2)     \n        MXLMresults['RMSE']=RMSE.round(3)\n        MXLMresults['ICC']=ICC(riresult).round(3)\n        MXLMresults['Likelihood']=riresult.llf.round(1)\n        MXLMresults['LE pred Min']=Y_pred.min().round(1)\n        MXLMresults['LE pred Max']=Y_pred.max().round(1)\n        MXLMresults['LE pred Mean']=Y_pred.mean().round(1)\n        MXLMresults['LE pred Std']=Y_pred.std().round(3)\n        MXLMresults['LE pred Var']=var.round(3)\n    else:  \n        print('Running Random Intercepts Model')\n        model = smf.mixedlm(\"Life_Expectancy~{}\".format(col),data=df_reg,groups=df_reg['Country'])\n        riresult = model.fit()\n        print(riresult.summary())\n        print('FE: {}'.format(col))\n        print('The Intraclass Correlation is: {}'.format(ICC(riresult)))\n        print('The Likelihood is: {}'.format(riresult.llf))\n        print('RE Variance is {}'.format(r))\n\n        #Predicted Values\n        Y_pred = riresult.fittedvalues\n        RMSE=mean_squared_error(Y, Y_pred)**0.5\n        var=Y_pred.std()**2\n        print('RMSE: {:0.3f}'.format(RMSE))\n        print('Minimum LE: {:0.1f}'.format(Y_pred.min()))\n        print('Maximum LE: {:0.1f}'.format(Y_pred.max()))\n        print('Average Predicted LE: {:0.1f}'.format(Y_pred.mean()))\n        print('LE Standard Deviation: {:0.3f}'.format(Y_pred.std()))\n        print('LE Variance: {:0.3f}'.format(var),'\\n')   \n   \n        #Add to results\n        MXLMresults.loc[i]=['RI',col,r,riresult.cov_re.iloc[0,0].round(2)\n                              ,RMSE.round(3),ICC(riresult).round(3),riresult.llf.round(1)\n                              ,Y_pred.min().round(1),Y_pred.max().round(1),Y_pred.mean().round(1),Y_pred.std().round(3),var.round(3)]         ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RS Model"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"cols=['Status','world','Year','Status','world','Status','world','Year','Status','world','Year','Status'\n      ,'world','Year','Status','world','Year']\nre=['Year','Year','Status','Schooling','Schooling','HIV_AIDS','HIV_AIDS','HIV_AIDS','Income_Composition','Income_Composition','Income_Composition'\n    ,'Alcohol','Alcohol','Alcohol','U_5_Deaths','U_5_Deaths','U_5_Deaths']\n\nY=df_reg['Life_Expectancy']\nfor col,r,i in zip(cols,re,range(len(re))): \n        print('Running Random Slopes Model')\n        model = smf.mixedlm(\"Life_Expectancy~{}\".format(col),data=df_reg,groups=df_reg['Country'],re_formula=\"~0+{}\".format(r))\n        rsresult = model.fit()\n        print(rsresult.summary())\n        print('FE: {}'.format(col))\n        print('The Intraclass Correlation is: {}'.format(ICC(rsresult)))\n        print('The Likelihood is: {}'.format(rsresult.llf))\n        print('{} Variance is {}'.format(r,rsresult.cov_re.iloc[0,0].round(2)))\n\n        #Predicted Values\n        Y_pred = rsresult.fittedvalues\n        RMSE=mean_squared_error(Y, Y_pred)**0.5\n        var=Y_pred.std()**2\n        print('RMSE: {:0.3f}'.format(RMSE))\n        print('Minimum LE: {:0.1f}'.format(Y_pred.min()))\n        print('Maximum LE: {:0.1f}'.format(Y_pred.max()))\n        print('Average Predicted LE: {:0.1f}'.format(Y_pred.mean()))\n        print('LE Standard Deviation: {:0.3f}'.format(Y_pred.std()))\n        print('LE Variance: {:0.3f}'.format(var),'\\n')   \n   \n        #Add to results\n        MXLMresults.loc[i+19]=['RS',col,r,rsresult.cov_re.iloc[0,0].round(2)\n                              ,RMSE.round(3),ICC(rsresult).round(3),rsresult.llf.round(1)\n                              ,Y_pred.min().round(1),Y_pred.max().round(1),Y_pred.mean().round(1),Y_pred.std().round(3),var.round(3)]         ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RIS Model"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"cols=['Status','world','Year','Status','world','Status','world','Year','Status','world','Year','Status'\n      ,'world','Year','Status','world','Year']\nre=['Year','Year','Status','Schooling','Schooling','HIV_AIDS','HIV_AIDS','HIV_AIDS','Income_Composition','Income_Composition','Income_Composition'\n    ,'Alcohol','Alcohol','Alcohol','U_5_Deaths','U_5_Deaths','U_5_Deaths']\n\nY=df_reg['Life_Expectancy']\nfor col,r,i in zip(cols,re,range(len(re))): \n    print('Running Random Slopes+Intercepts Model')\n    model = smf.mixedlm(\"Life_Expectancy~{}\".format(col),data=df_reg,groups=df_reg['Country'],re_formula=\"~{}\".format(r))\n    risresult = model.fit()\n    print(risresult.summary())\n    print('FE: {}'.format(col))\n    print('The Intraclass Correlation is: {}'.format(ICC(risresult)))\n    print('The Likelihood is: {}'.format(risresult.llf))\n    print('{} Variance is {}'.format(r,risresult.cov_re.iloc[1,1].round(2)))\n\n    #Predicted Values\n    Y_pred = risresult.fittedvalues\n    RMSE=mean_squared_error(Y, Y_pred)**0.5\n    var=Y_pred.std()**2\n    print('RMSE: {:0.3f}'.format(RMSE))\n    print('Minimum LE: {:0.1f}'.format(Y_pred.min()))\n    print('Maximum LE: {:0.1f}'.format(Y_pred.max()))\n    print('Average Predicted LE: {:0.1f}'.format(Y_pred.mean()))\n    print('LE Standard Deviation: {:0.3f}'.format(Y_pred.std()))\n    print('LE Variance: {:0.3f}'.format(var),'\\n')   \n   \n    #Add to results\n    MXLMresults.loc[i+36]=['RIS',col,r,risresult.cov_re.iloc[1,1].round(2)\n                              ,RMSE.round(3),ICC(risresult).round(3),risresult.llf.round(1)\n                              ,Y_pred.min().round(1),Y_pred.max().round(1),Y_pred.mean().round(1),Y_pred.std().round(3),var.round(3)]            \n      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"MXLMresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The RIS Models peform the best as whole in reducing RMSE and likelihood. Random Slopes perform the weakest as would be probably all the countries have a separate starting point for year naturally. In the case of RI and RIS models, it may not have the best RMSE, but it captures the overall LE interval better. RS and RI are chosen based on RMSE scores.\n\nRIS Model Chosen: Index 36   \nRS Model Chosen: Index 27    \nRI Model Chosen: Index 0   "},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"Y=df_reg['Life_Expectancy']\nprint('Running Random Intercepts Model')\nmodel = smf.mixedlm(\"Life_Expectancy~Year\",data=df_reg,groups=df_reg['Country'])\nriresult = model.fit()\nprint(riresult.summary())\nprint('The Intraclass Correlation is: {:.3f}'.format(ICC(riresult)),'\\n')\n\nprint('Running Random Slopes Model')\nmodel = smf.mixedlm(\"Life_Expectancy~Status\",data=df_reg,groups=df_reg['Country'],re_formula=\"~0+Income_Composition\")\nrsresult = model.fit()\nprint(rsresult.summary())\nprint('The Intraclass Correlation is: {:.3f}'.format(ICC(rsresult)),'\\n')\n\nprint('Running Random Slopes+Intercepts Model')\nmodel = smf.mixedlm(\"Life_Expectancy~Status\",data=df_reg,groups=df_reg['Country'],re_formula=\"~Year\")\nrisresult = model.fit()\nprint(risresult.summary())\nprint('The Intraclass Correlation is: {:.3f}'.format(ICC(risresult)),'\\n')\n\nfrom scipy.stats import chi2\n# Double Check Model Selection\ndef likelihood_ratio_test(bigmodel, smallmodel):\n    likelihoodratio=2*(bigmodel.llf-smallmodel.llf)\n    f=bigmodel.df_modelwc-smallmodel.df_modelwc\n    p=chi2.sf(likelihoodratio, f)\n    return p\n\nlrt=likelihood_ratio_test(risresult,rsresult)\nprint('The p-value for the likelihood ratio test of the random slope and random intercept/slope models is: {:.4f}'.format(lrt))\n\nlrt=likelihood_ratio_test(risresult,riresult)\nprint('The p-value for the likelihood ratio test of the random intercept and random intercept/slope models is: {:.4f}'.format(lrt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Use as 2nd hue\ndf_reg['Cluster']=df_cl['KM Cluster']","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"df_reg['residual']=risresult.resid\n\n# Are residuals normally distributed?\nplt.hist(df_reg['residual'])\nsns.set(rc={'figure.figsize':(10,10)})\nprint(stats.shapiro(df_reg['residual']))\nplt.show()\n\n# Is variance constant for all values of the outcome?\nsns.scatterplot(x='Life_Expectancy',y='residual',hue='Status',data=df_reg)\nplt.title('Residuals by raw LE values with cluster labels')\nsns.set(rc={'figure.figsize':(10,10)})\nplt.show()\n\n# Is variance constant for all values of the outcome?\nsns.scatterplot(x='Life_Expectancy',y='residual',hue='Cluster',data=df_reg,palette='Set2')\nplt.title('Residuals by raw LE values with cluster labels')\nsns.set(rc={'figure.figsize':(10,10)})\nplt.show()\n\n\n# Is variance constant for all values of the predictors?\nsns.scatterplot(x='Year',y='residual',hue='Status',data=df_reg)\nplt.title('Residuals by Year')\nsns.set(rc={'figure.figsize':(10,10)})\nplt.show()\n\n\n# Is variance constant for all values of the predictors?\nsns.scatterplot(x='Year',y='residual',hue='Cluster',data=df_reg,palette='Set2')\nplt.title('Residuals by Year with Cluster Labels')\nsns.set(rc={'figure.figsize':(10,10)})\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The residual distribution and plot seem to fall well distributed overall."},{"metadata":{"trusted":false},"cell_type":"code","source":"#plot out best with Cluster Labels\n#Predictions\nY_pred=rf.predict(scale(df_reg.drop(['Life_Expectancy','Country','world','Status','residual','Cluster'],1)))\nY=df_reg['Life_Expectancy']                 \n#plot\nz1 =np.polyfit(Y_pred,Y,1)\nz1poly = np.poly1d(z1) \nplt.scatter(Y_pred,Y,alpha=1,c=df_reg['Cluster'],cmap='Set2')\nplt.plot(Y_pred,z1poly(Y_pred),linewidth=7.0,color='r')\nplt.title('Gradient Boost {}'.format(z))\nplt.xlabel('Y_pred')\nplt.ylabel('Y')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"print(risresult.summary())\nY=df_reg['Life_Expectancy']\nY_pred = risresult.fittedvalues\nRMSE=mean_squared_error(Y, Y_pred)**0.5\nprint('RMSE: {:0.3f}'.format(RMSE))\nprint('Minimum LE: {:0.1f}'.format(Y_pred.min()))\nprint('Maximum LE: {:0.1f}'.format(Y_pred.max()))\nprint('Average Predicted LE: {:0.1f}'.format(Y_pred.mean()))\nprint('LE Standard Deviation: {:0.3f}'.format(Y_pred.std()))\nprint('LE Variance: {:0.3f}'.format(Y_pred.std()**2))\n        \n#plot\nz1 =np.polyfit(Y_pred,Y,1)\nz1poly = np.poly1d(z1)\nplt.scatter(Y_pred,Y,c=df_reg['Cluster'],alpha=1,cmap='Set2')\nplt.plot(Y_pred,z1poly(Y_pred),linewidth=7.0,color='r')\nplt.title('Mixed Effect RIS')\nplt.xlabel('Y_pred')\nplt.ylabel('Y')\nplt.show()            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_reg['Life_Expectancy'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Mixed Effect RIS Model with status as the fixed effect and year as the random effect outperforms all the regular regression and reduces the RMSE down to 1.370. \n\nOverall, the fit is good and it offers information versus your normal regression with a neat compact summary. Developed Countries have about 11.3 years higher in life expectancy versus developing countries. "},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"Top 5 Average Life Performance Predictors across all the model are Income Composition,Adult Mortality,HIV_AIDS, Schooling, and Thinness 5-9. Life Expectancy ranges are Developed(70-89), Developing (41-86), 1st World(78-88), 2nd World(65-78),3rd World(41-65). \n \n\nThe developed and developing status don’t fully cover the different categories of countries.Testing data was generally higher across all groupings due to the year raising LE inherently. In fact, there was a .3 increase every year in Life Expectancy from the models. \n\nDisease and hunger relief are an universal key to improving life expectancy. This can come in the form of just being smarter and preventing outbreaks by vacinating. This is helpful when traveling to other countries to keep it away from unknowing populations. \n\nAlcohol is negatively correlated with life expectancy for developed countries and 1st world countries while being positively correlated with 2nd, 3rd, and Developing Countries. In this case, it means that population is living longer, but alcohol consumption is not increasing life expectancy! \n\nMixed Models are effective, but  still need to keep improving! Some measures like covariance can be seen as less useful. R includes more information, but in a less summarized pattern. With time, this should improve. \n\n# Future Work\n\nLook at class within a particular country and see if these same factors are same in determining life expectancy for an individual.Upper, Middle, and Lower Class ratios could be a very interesting feature to add on to the this study. \n\nIncrease the dataset size with continuing UN and Global Data data. There are many variables that are not included in this model that could further describe how to particularly improve standards for Life Expectancy. Some new added features like population, GDP,  environmental, and etc in order to test and clarify country groupings. Mental Health and Life Expectancy Correlation also seems essential to look further this study!\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}