{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COVID-19 Open Research Dataset Challenge (CORD-19)\n# What has been published about ethical and social science considerations?\n\nhttps://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks?taskId=563\n\nSpecifically, we want to know what the literature reports about:\n\nEfforts to articulate and translate existing ethical principles and standards to salient issues in COVID-2019\nEfforts to embed ethics across all thematic areas, engage with novel ethical issues that arise and coordinate to minimize duplication of oversight\nEfforts to support sustained education, access, and capacity building in the area of ethics\nEfforts to establish a team at WHO that will be integrated within multidisciplinary research and operational platforms and that will connect with existing and expanded global networks of social sciences.\nEfforts to develop qualitative assessment frameworks to systematically collect information related to local barriers and enablers for the uptake and adherence to public health measures for prevention and control. This includes the rapid identification of the secondary impacts of these measures. (e.g. use of surgical masks, modification of health seeking behaviors for SRH, school closures)\nEfforts to identify how the burden of responding to the outbreak and implementing public health measures affects the physical and psychological health of those providing care for Covid-19 patients and identify the immediate needs that must be addressed.\nEfforts to identify the underlying drivers of fear, anxiety and stigma that fuel misinformation and rumor, particularly through social media.\n\n# Results and discussion\n\nThe following results were manually selected to highlight research and news articles that are relevant to information-sharing and collaboration:\n* [Panglobalism and pandemics: ecological and ethical concerns.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2259162/)\n* [Scientific and ethical basis for social-distancing interventions against COVID-19](https://doi.org/10.1016/s1473-3099%2820%2930190-0)\n* [Pandethics Summary This paper explains the ethical importance of infectious diseases, and reviews four major ethical issues associated with pandemic influenza: the obligation of individuals to avoid infecting others, healthcare workers' ‘duty to treat’, allocation of scarce resources, and coercive social distancing measures. ](https://doi.org/10.1016/j.puhe.2008.12.005)\n* [The Role of the Global Health Development/Eastern Mediterranean Public Health Network and the Eastern Mediterranean Field Epidemiology Training Programs in Preparedness for COVID-19](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7104707/)\n* [CIBT Education Group Provides Business Update Relating to the Coronavirus Outbreak.](https://www.globalbankingandfinance.com/category/news/cibt-education-group-provides-business-update-relating-to-the-coronavirus-outbreak/)\n* [Team Epi-Aid: Graduate Student Assistance with Urgent Public Health Response Team Epi-Aid provides graduate students with practical public health experience through participation in outbreak investigations and other applied projects with state and local health departments in North Carolina](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2569985/)\n* [Chapter 3 Measuring, Monitoring, and Evaluating the Health of a Population Abstract Public health depends on information derived from monitoring population health status to identify community health problems, and to diagnose and investigate health problems and hazards in the community.](https://doi.org/10.1016/b978-0-12-415766-8.00003-3)\n* [A review of instruments assessing public health preparedness.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1497752/)\n* [Ethical Obligations of Physicians Participating in Public Health Quarantine and Isolation Measures](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2099320/)\n* [Informed public against false rumor in the social media era: Focusing on social media dependency](https://doi.org/10.1016/j.tele.2017.12.017)\n* [U.K. fights coronavirus disinformation with rapid response team.](https://venturebeat.com/2020/03/30/u-k-fights-covid-19-disinformation-with-rapid-response-team/)\n\n\n# Highlights and suggestions for improvements\n\nThe advantages of this work includes:\n- Use of news content for greater information coverage. For example, the \"[U.K. fights coronavirus disinformation with rapid response team.](https://venturebeat.com/2020/03/30/u-k-fights-covid-19-disinformation-with-rapid-response-team/)\" resource was obtained from the news dataset. Another example includes \"[CIBT Education Group Provides Business Update Relating to the Coronavirus Outbreak.](https://www.globalbankingandfinance.com/category/news/cibt-education-group-provides-business-update-relating-to-the-coronavirus-outbreak/)\".\n- Accurate results\n- Data table fromatting with clickable URLs\n- Simple data pipeline to understand and re-use: Users can easily enter a query and find relevant documents by simply calling\n> q='equity considerations and problems of inequity'\n<br>\n> search(q, 'inequity')\n- Results are summarized as a WordCloud image\n\nSuggestions for improvements:\n- To speed up the process and minimize computation, only the title and part of the abstract / news content are used in the analysis. Analyzing full-text may reveal additional information.\n- Results can be fed into a document summarization algorithm so results can be more focussed even further\n- Further improvements in how data is presented with a more advanced user interface can be beneficial\n\n\n# Methodology\nData sources:\n- [CORD-19 Research Papers](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge)\n- [COVID-19 Public Media Dataset](https://www.kaggle.com/jannalipenkova/covid19-public-media-dataset)\n\nTo build the corpus, the title and the content from both data soucres are combined (only the first 3,000 characters are used). The task questions were used as search queries for relevant documents in the corpus. For each document in the corpus and search query pair, the word vectors are retrieved from the GoogleNews word embeddings and the cosine distance is calculated. The top documents that match each query are shown in this notebook together with a summary of results presented as a WordCloud image. Additional results are available as CSV files.\n\nI experimented with using the Word Mover's Distance measure as an alternative similarity measure to cosine distance however, it proved to be a lot slower and it was difficult to objectively decide if the results were better than cosine distance. I also tried using LDA (latent dirichlet allocation) to identify the major topics in the results but I decided to use WordClouds instead because it is more visually appealing and more keywords are highlighted.\n\n\nI hope that the findings from this notebook can help inform researchers and curious minds about the ongoing COVID-19 research. I want to thank the researchers, competition organizers, Kaggle and the dataset providers for making this work possible."},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport json\nimport requests\nimport io\nimport gc\nimport re\n\nimport logging\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n\n# from tqdm import tqdm\nfrom tqdm.notebook import tqdm\n\n\n# pandas settings\npd.set_option('display.max_colwidth', -1)\npd.set_option('display.max_rows', 1000)\nplt.rcParams['figure.figsize'] = [12, 8]\n\n\nfrom nltk import download\ndownload('stopwords')  # Download stopwords list.\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nfrom nltk import word_tokenize\ndownload('punkt')  # Download data for tokenizer.\n\nplt.rcParams['figure.figsize'] = [12, 8]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare data\n\nGather title and abstract from COVID19 articles and titles from news upto a maximum number of characters.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 3000   # 3000 chars\n\nresearch = pd.read_csv('/kaggle/input/CORD-19-research-challenge/metadata.csv')\nresearch['title_abstract'] = [str(research.loc[i,'title']) + ' ' + str(research.loc[i,'abstract']) for i in research.index ]\nresearch['source'] = 'research'\nresearch\n\nnews = pd.read_csv('/kaggle/input/covid19-public-media-dataset/covid19_articles.csv')\ndel news[\"Unnamed: 0\"]\nnews['source'] = 'news'\nnews['title_abstract'] = [ news.loc[i,'title'] + '. ' + news.loc[i,'content'][:(MAX_LEN-len(news.loc[i,'title']))] for i in news.index  ]\nnews\n\ndata = pd.concat([research[['title_abstract','source', 'url']], news[['title_abstract', 'source', 'url']]]).rename(columns={'title_abstract':'title'}).drop_duplicates().reset_index(drop=True)\n\nprint('News:',news.shape)\nprint('Research:',research.shape)\nprint('Combined data:',data.shape)\n\ndel research\ndel news\ngc.collect()\n\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Title similarity search and Topic Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gensim word embeddings\n# https://www.kaggle.com/raymishra/sentence-similarity-match\n# https://radimrehurek.com/gensim/models/fasttext.html\n# https://radimrehurek.com/gensim/models/keyedvectors.html\n\nimport gensim\nfrom gensim.models import Word2Vec\nfrom gensim.utils import simple_preprocess\n\nfrom gensim.models.keyedvectors import KeyedVectors\n\nfilepath = \"../input/gnewsvector/GoogleNews-vectors-negative300.bin\"\n\n\nfrom gensim.models import KeyedVectors\nwv_from_bin = KeyedVectors.load_word2vec_format(filepath, binary=True) \n\n#extracting words7 vectors from google news vector\nembeddings_index = {}\nfor word, vector in zip(wv_from_bin.vocab, wv_from_bin.vectors):\n    coefs = np.asarray(vector, dtype='float32')\n    embeddings_index[word] = coefs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# helper functions\n\ndef preprocess(doc):\n#     doc = re.sub(r'[\\W\\d]+',' ',doc)  # Remove numbers and punctuation.\n    doc = doc.lower()  # Lower the text.\n    doc = word_tokenize(doc)  # Split into words.\n    doc = [w for w in doc if not w in stop_words]  # Remove stopwords.\n    doc = [w for w in doc if w.isalpha()]  # Remove numbers and punctuation.\n    return doc\n\ndef avg_feature_vector(sentence, model, num_features):\n#     words = sentence.lower().split()\n#     words = preprocess(sentence)\n    words = simple_preprocess(sentence)\n    #feature vector is initialized as an empty array\n    feature_vec = np.zeros((num_features, ), dtype='float32')\n    n_words = 0\n    for word in words:\n        if word in embeddings_index.keys():\n            n_words += 1\n            feature_vec = np.add(feature_vec, model[word])\n    if (n_words > 0):\n        feature_vec = np.divide(feature_vec, n_words)\n    return feature_vec\n\nfrom scipy.spatial import distance\ndef calc_dist_cosine(s1, target, max_dist=0.5):\n    ret = []\n    for t in tqdm(target):\n        tv = avg_feature_vector(t,model= embeddings_index, num_features=300)\n        qv = avg_feature_vector(q,model= embeddings_index, num_features=300)\n        dist = distance.cosine(tv, qv)\n        if dist <= max_dist:\n            ret.append([dist, t])\n    df = pd.DataFrame(ret,columns=['dist','title']).reset_index(drop=True)\n    return pd.merge(df, data, on='title', how='left').sort_values(by='dist', ascending=True).reset_index(drop=True)\n\n\n# wv_from_bin.init_sims(replace=True)  # Normalizes the vectors in the word2vec class before calculating wmdistance\ndef calc_dist_wm(s1, target, max_dist=5.0):\n    \"\"\"\n    Word mover distance. Slower than cosine similarity.\n    https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html\n    \"\"\"\n    ret = []\n    for t in tqdm(target):\n#         print(t)\n        dist = wv_from_bin.wmdistance(preprocess(s1), preprocess(t))\n        if dist <= max_dist:\n            ret.append([dist, t])\n    df = pd.DataFrame(ret,columns=['dist','title']).reset_index(drop=True)\n    return pd.merge(df, data, on='title', how='left').sort_values(by='dist', ascending=True).reset_index(drop=True)\n       \ndef calc_dist(s1, target):\n    \"\"\"\n    Dist interface\n    \"\"\"\n    return calc_dist_cosine(s1, target)\n#     return calc_dist_wm(s1, target)\n\n# usage\n# s1_afv = avg_feature_vector('Why the second proforma does not coincide with the first, what has changed', model= embeddings_index, num_features=300 )\n# s2_afv = avg_feature_vector('Again came the proforma double.In the morning there was already a proforma with the same positions, but under a different number',model= embeddings_index, num_features=300)\n# cos = distance.cosine(s1_afv, s2_afv)\n# print(cos)\n# calc_dist_wm('Why the second proforma does not coincide with the first, what has changed', ['Again came the proforma double.In the morning there was already a proforma with the same positions, but under a different number'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LDA\n# https://www.kaggle.com/monsterspy/topic-modeling-with-lda\n# https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n\nfrom gensim.models import ldamodel\nimport gensim.corpora\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\nstop.update(['href','br'])\nfrom nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer(r'\\w+')\n\nnum_topics = 5\n\ndef train_lda(data_text):\n    train_ = []\n    for i in range(len(data_text)):\n        train_.append([word for word in tokenizer.tokenize(data_text[i].lower()) if word not in stop])\n\n    id2word = gensim.corpora.Dictionary(train_)\n    corpus = [id2word.doc2bow(text) for text in train_]    # Term Document Frequency\n    lda = ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=num_topics)\n    return lda\n\ndef get_lda_topics(model, num_topics, topn=5):\n    word_dict = {};\n    for i in range(num_topics):\n        words = model.show_topic(i, topn = topn);\n        word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words];\n    return pd.DataFrame(word_dict)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results\n\nThe topics include virus, cov, rna, protein, antibody, cells, infection which are research-based words in our data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# lda = train_lda(data.title.values.tolist())\n# lda_all_titles = get_lda_topics(lda, num_topics)\n# lda_all_titles","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_clickable(link):\n    # target _blank to open new window\n    return f'<a target=\"_blank\" href=\"{link}\">{link}</a>'\n# df.style.format({'url': make_clickable})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def search(q, out_prefix=\"result\"):\n    res = calc_dist(q, data.title)\n    res.to_csv(f'result_{out_prefix}.csv', index=False)\n\n    # second iteration using word distance\n#     res2 = calc_dist_wm(q, res.title)\n#     res2.to_csv(f'result_{out_prefix}_wmd.csv', index=False)\n\n#     lda = train_lda(res.title.values.tolist())\n#     lda_res = get_lda_topics(lda, num_topics)\n#     print(lda_res)\n    \n    topn = 20\n    wc = WordCloud(background_color='white', stopwords=stop_words).generate(' '.join(res.title.values.tolist()[:topn]).lower())\n    plt.imshow(wc)\n    plt.axis('off')\n\n    return res\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ethical and social science considerations"},{"metadata":{"trusted":true},"cell_type":"code","source":"q='ethical and social science considerations'\nres = search(q, 'ethics_considerations')\nres.head(20)[['title','url']].style.format({'url': make_clickable})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sustained education, access, and capacity building in the area of ethics"},{"metadata":{"trusted":true},"cell_type":"code","source":"q='sustained education, access, and capacity building in the area of ethics'\nres = search(q, 'sustained_ethics')\nres.head(20)[['title','url']].style.format({'url': make_clickable})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Qualitative assessment frameworks"},{"metadata":{"trusted":true},"cell_type":"code","source":"q='qualitative assessment frameworks and secondary impacts of public health measures for prevention and control'\nres = search(q, 'assessment')\nres.head(20)[['title','url']].style.format({'url': make_clickable})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Burden of responding to the outbreak and implementing public health measures affects the physical and psychological health of those providing care for Covid-19 patients"},{"metadata":{"trusted":true},"cell_type":"code","source":"q='burden of responding to the outbreak and implementing public health measures affects the physical and psychological health of those providing care for Covid-19 patients'\nres = search(q, 'burden')\nres.head(20)[['title','url']].style.format({'url': make_clickable})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Underlying drivers of fear, anxiety and stigma that fuel misinformation and rumor, particularly through social media."},{"metadata":{"trusted":true},"cell_type":"code","source":"q='underlying drivers of fear, anxiety and stigma that fuel misinformation and rumor, particularly through social media'\nres = search(q, 'misinformation')\nres.head(20)[['title','url']].style.format({'url': make_clickable})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ethical and social science considerations?\n\n* Efforts to articulate and translate existing ethical principles and standards to salient issues in COVID-2019\n* Efforts to embed ethics across all thematic areas, engage with novel ethical issues that arise and coordinate to minimize duplication of oversight\n* Efforts to support sustained education, access, and capacity building in the area of ethics\n* Efforts to establish a team at WHO that will be integrated within multidisciplinary research and operational platforms and that will connect with existing and expanded global networks of social sciences.\n* Efforts to develop qualitative assessment frameworks to systematically collect information related to local barriers and enablers for the uptake and adherence to public health measures for prevention and control. This includes the rapid identification of the secondary impacts of these measures. (e.g. use of surgical masks, modification of health seeking behaviors for SRH, school closures)\n* Efforts to identify how the burden of responding to the outbreak and implementing public health measures affects the physical and psychological health of those providing care for Covid-19 patients and identify the immediate needs that must be addressed.\n* Efforts to identify the underlying drivers of fear, anxiety and stigma that fuel misinformation and rumor, particularly through social media.\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}