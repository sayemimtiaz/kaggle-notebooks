{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. Context:\nThe American Stroke Association [reports](https://www.stroke.org/en/about-stroke), \"stroke is the no. 5 cause of death and a leading cause of disability in the United States\". In addition, according to [the Center for Disease Control and Prevention (CDC)](https://www.cdc.gov/stroke/conditions.htm), the factors that lead to an increased chance of stroke include high blood pressure, high cholesterol, high blood glucose level, etc. These conditions are further enhanced by an unhealthy diet, physical inactivity, and alcohol and tobacco use (including smoking). \n\nThe available dataset can be used to predict whether a patient is likely to get a stroke based on other features and behaviors like age, gender, smoking status, blood sugar level, etc.\n\n### Attribute Information\n\n1) id: unique identifier\n\n2) gender: \"Male\", \"Female\" or \"Other\"\n\n3) age: age of the patient\n\n4) hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n\n5) heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n\n6) ever_married: \"No\" or \"Yes\"\n\n7) work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n\n8) Residence_type: \"Rural\" or \"Urban\"\n\n9) avg_glucose_level: average glucose level in blood\n\n10) bmi: body mass index\n\n11) smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n\n12) stroke: 1 if the patient had a stroke or 0 if not\n\n**Note:** _\"Unknown\" in smoking_status means that the information is unavailable for this patient._\n\n\n# 2. Importing the necessary modules:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import the required modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.metrics import f1_score, confusion_matrix, roc_auc_score\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Data Loading and Exploration:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the data as a dataframe\ndata = pd.read_csv('/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.1 Data Exploration\n\nIn this section, we will do some data exploration using pandas, including looking at the missing values and the data types. Any inconsistencies need to be dealt with using data cleaning processes."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.dtypes)\nprint(\"===================================================================\")\nprint(data.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All data types seem to be correct. There are some missing values in the `bmi` column, but the others seem to be okay in that front. Most of the `object` type columns seem to be categorical with number of distinct values ranging from 2 to 5. Let's take a deeper look into the dataset, focusing on the categorical variables with less than or equal to 5 categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the column names and the number of unique values\n# if the number of unique values is less than or equal to 5\ncol_names = data.columns\ncol_nunique = []\ncol_unique = []\nfor column in data.columns:\n    col_nunique.append(data[column].nunique())\n    col_unique.append(data[column].unique())\n    \nfeatures = pd.DataFrame({'Feature':col_names, 'Number of Categories': col_nunique, 'Categories': col_unique})\nfeatures = features[features['Number of Categories']<=5]\nfeatures","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The `gender` column seems to have 3 values - male, female and 'other'. The 'other' category was probably due to lack of information. According to a [publication](https://pubmed.ncbi.nlm.nih.gov/11252851/) from the National Institutes of Health (NIH), \"the prevalence of stroke is higher among men up to the age of approximately 80 years, after which it becomes higher in women.\" This clearly indicates a dependence of stroke on gender. Therefore, the 'other' values in the gender column must be taken care of. This can either be by imputation or by removing the rows, depending on how many rows there are. We will deal with these categories in a later section, along with encoding the categorical variables for better classification.\n\nBut before that, let's check out the missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the missing values\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The column `bmi` has 201 missing values while none of the other columns have any. From the description of the dataset earlier, we saw that while the other data columns have values in the acceptable ranges, the column `bmi` seems off. The Body Mass Index (BMI) of regular humans ranges from 30.0 to 39.9 for \"obese\" body type. A bmi of 97.6 seems like an outlier data point. Let's take a closer look at this column."},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for all the data above normal ranges\ndata[data['bmi']>40]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There seem to be a lof of people (408 to be exact) with their BMI higher than 40. A person with a BMI of over 40 is considered to be \"morbidly obese\" according to a study by the [University of Rochester](https://www.urmc.rochester.edu/highland/bariatric-surgery-center/journey/morbid-obesity.aspx#:~:text=Normal%20BMI%20ranges%20from%2020,high%20blood%20pressure%20or%20diabetes.)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the outliers in the bmi column\n# Plot a boxplot\nsns.boxplot(data['bmi'], orient='v')\nplt.title('Distribution of BMI')\nplt.xlabel('BMI')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly, there are a lot of outliers in the dataset. The BMI, if above 40, is directly related to high blood sugar levels. Let's see if Pearson's correlation matches this fact."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find correlation between BMI greater than 40 and average glucose level.\ndata[data['bmi']>40][['bmi', 'avg_glucose_level']].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Pearson's correlation coefficient doesn't approve of a direct relation between `bmi` and `avg_glucose_level`, which is very odd. In fact, the coefficient suggests that they are almost unrelated (the coefficient is very close to 0). Let's look at the distribution of the BMI to see what values might be ignored (if we may)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot a bar graph for the BMI data to get a closer look\nplt.figure(figsize=(10,6))\nsns.countplot(data['bmi'])\nplt.locator_params(axis='x', nbins=10)\nplt.title('Count Distribution of BMI', fontsize=18)\nplt.xlabel('BMI', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a long tail in the data after the BMI of around 46.5. This can be considered the cutoff for the BMI. But before that, we will impute the mean BMI value for the missing values."},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering\n\nLet's impute the mean value of the BMI at all the missing data points.\n\n#### 1. Impute the missing values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Impute the missing values\ndata['bmi'].fillna(data['bmi'].mean(), inplace=True)\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2. Replace the 'Other' value in gender with the most frequent value"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replace 'other' with most frequent gender\ndf = data.copy()\ndf['gender'].replace('Other', data['gender'].value_counts().idxmax(), inplace=True)\ndf['gender'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3. Reduce the dataframe with bmi values less than 46.5"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Neglect the bmi values over 46.5\ndf = df[df['bmi']<=46.5]\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4. Drop the unnecessary columns:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('id', axis=1, inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5. Encode all the categorical columns:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select and transform the required columns\nle = LabelEncoder()\ncat_cols = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\ndf[cat_cols] = df[cat_cols].apply(le.fit_transform)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 6. Find the correlation coefficients between the columns:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the correlation matrix\nplt.figure(figsize=(12,8))\nsns.heatmap(df.corr(), annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perform the split into training and testing datasets\nX = df.drop('stroke', axis=1)\ny = df['stroke']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1. Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the model and Predict with Logistic Regression\nlr = LogisticRegression(random_state=0)\nlr.fit(X_train, y_train)\ny_pred_lr = lr.predict(X_test)\n\n# Print out the metrics for Logistic Regression\nprint(\"The F1-score of Logistic Regression is: {}\".format(round(f1_score(y_test, y_pred_lr),2)))\nprint(\"The ROC-AUC score of Logistic Regression is: {}\".format(roc_auc_score(y_test, y_pred_lr),2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2. Decision Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the model and predict with Decision Tree Classifier\ndt = DecisionTreeClassifier(random_state=0)\ndt.fit(X_train, y_train)\ny_pred_dt = dt.predict(X_test)\n\n# Print out the metrics for Decision Tree Classifier\nprint(\"The F1-score of Decision Tree Classifier is: {}\".format(round(f1_score(y_test, y_pred_dt),2)))\nprint(\"The ROC-AUC score of Decision Tree Classifier is: {}\".format(round(roc_auc_score(y_test, y_pred_dt),2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3. Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the model and predict with Random Forest Classifier\nrf = RandomForestClassifier(random_state=0)\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)\n\n# Print out the metrics for Random Forest Classifier\nprint(\"The F1-score of Random Forest Classifier is: {}\".format(round(f1_score(y_test, y_pred_rf),2)))\nprint(\"The ROC-AUC score of Random Forest Classifier is: {}\".format(round(roc_auc_score(y_test, y_pred_rf),2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the above classification models, it looks like the F1-score and the ROC/AUC score are really bad, except for the Decision Tree Classifier, where the numbers are slightly better. This can be because the ratio of \"negative\" to \"positive\" cases of strokes in the dataset is very, very high. Therefore, the model has high accuracy score in training the data but doesn't work very well with the testing dataset. Is there a way around this problem?\n\nThe low f1 scores and ROC-AUC scores are because of the mismatch in the number of target values. A popular method to deal with such a problem is resampling. The resampling can either be **undersampling**, which involves removing samples from the majority class and **oversampling**, where we add more examples from the minority class.\n\nEach sampling method has it's own disadvantages in addition to the obvious advantages. In undersampling, where we only take samples from the majority class, there can be loss of important data. During oversampling, there will be copies of the minority class, which can cause overfitting.\n\n**In our case, we will go with undersampling to see if our model improves.**\n\n# Resampling\n**1. UNDER-SAMPLING:** \nWe will use Python's imbalanced-learn module to under-sample our data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use RandomUnderSampler module\nrus = RandomUnderSampler(random_state=0)\nX_rus, y_rus = rus.fit_resample(X_train, y_train)\n\n# Use the resampled data to train the Decision Tree model:\ndt_rus = DecisionTreeClassifier(random_state=0)\ndt_rus.fit(X_rus, y_rus)\ny_pred_dt_rus = dt_rus.predict(X_test)\n\n# Print out the metrics for Decision Tree Classifier\nprint(\"The F1-score of Decision Tree Classifier is: {}\".format(round(f1_score(y_test, y_pred_dt_rus),2)))\nprint(\"The ROC-AUC score of Decision Tree Classifier is: {}\".format(round(roc_auc_score(y_test, y_pred_dt_rus),2)))\n\nprint(\"\\n========================================================================================================\\n\")\n\n# Use the resampled data to train the Logistic Regression model:\nlr_rus = LogisticRegression(random_state=0)\nlr_rus.fit(X_rus, y_rus)\ny_pred_lr_rus = lr_rus.predict(X_test)\n\n# Print out the metrics for Logistic Regression\nprint(\"The F1-score of Logistic Regression is: {}\".format(round(f1_score(y_test, y_pred_lr_rus),2)))\nprint(\"The ROC-AUC score of Logistic Regression is: {}\".format(round(roc_auc_score(y_test, y_pred_lr_rus),2)))\n\nprint(\"\\n========================================================================================================\\n\")\n\n# Check with Random Forest Classifier\nrf_rus = RandomForestClassifier(random_state=0)\nrf_rus.fit(X_rus, y_rus)\ny_pred_rf_rus = rf_rus.predict(X_test)\n\n# Print out the metrics for Random Forest Classifier\nprint(\"The F1-score of Random Forest Classifier is: {}\".format(round(f1_score(y_test, y_pred_rf_rus),2)))\nprint(\"The ROC-AUC score of Random Forest Classifier is: {}\".format(round(roc_auc_score(y_test, y_pred_rf_rus),2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2. OVER-SAMPLING:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use RandomOverSampler module\nros = RandomOverSampler(random_state=0)\nX_ros, y_ros = ros.fit_resample(X_train, y_train)\n\n# Use the resampled data to train the Decision Tree model:\ndt_ros = DecisionTreeClassifier(random_state=0)\ndt_ros.fit(X_ros, y_ros)\ny_pred_dt_ros = dt_ros.predict(X_test)\n\n# Print out the metrics for Decision Tree Classifier\nprint(\"The F1-score of Decision Tree Classifier is: {}\".format(round(f1_score(y_test, y_pred_dt_ros),2)))\nprint(\"The ROC-AUC score of Decision Tree Classifier is: {}\".format(round(roc_auc_score(y_test, y_pred_dt_ros),2)))\n\nprint(\"\\n==========================================================================================\\n\")\n\n# Check with Logistic Regression\nlr_ros = LogisticRegression(random_state=0)\nlr_ros.fit(X_ros, y_ros)\ny_pred_lr_ros = lr_ros.predict(X_test)\n\n# Print out the metrics for Logistic Regression\nprint(\"The F1-score of Logistic Regression is: {}\".format(round(f1_score(y_test, y_pred_lr_ros),2)))\nprint(\"The ROC-AUC score of Logistic Regression is: {}\".format(round(roc_auc_score(y_test, y_pred_lr_ros),2)))\n\nprint(\"\\n==========================================================================================\\n\")\n\n# Check with Random Forest Classifier\nrf_ros = RandomForestClassifier(random_state=0)\nrf_ros.fit(X_ros, y_ros)\ny_pred_rf_ros = rf_ros.predict(X_test)\n\n# Print out the metrics for Random Forest Classifier\n\nprint(\"The F1-score of Random Forest Classifier is: {}\".format(round(f1_score(y_test, y_pred_rf_ros),2)))\nprint(\"The ROC-AUC score of Random Forest Classifier is: {}\".format(round(roc_auc_score(y_test, y_pred_rf_ros),2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Modeling the data using Logistic Regression and under/over-sampling gives the best results for this dataset.** The values of F-1 score and ROC-AUC score from both undersampling and oversampling for Logistic Regression are really close to each other. It is up to the user to look at the benefits and drawbacks of each method and decide which sampling method they want to follow. \n\n\n**Feedback and suggestions are welcome.**\n\n# Thank you for reading."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}