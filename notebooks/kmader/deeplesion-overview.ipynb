{"cells":[{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cc79fa85a72f5c16da265ad3ca6f8d26573449dd"},"cell_type":"markdown","source":"# Overview\nThe notebook shows how to correctly load, process and interpret the information in the DeepLesion study. The notebook also previews some of the images overlayed with the bounding boxes and converts the bounding boxes into segmented regions to allow for the simple experiments to try and automatically detect and segment lesions. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"%matplotlib inline\nfrom glob import glob\nimport os, pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage.io import imread\nimport seaborn as sns\nfrom matplotlib.collections import PatchCollection\nfrom matplotlib.patches import Rectangle\n# make the necessary conversion\nread_hu = lambda x: imread(x).astype(np.float32)-32768\nbase_img_dir = '../input/minideeplesion/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3639d62fd4127952e9a71f0b4f9a7a8a78e50713","collapsed":true},"cell_type":"code","source":"patient_df = pd.read_csv('../input/DL_info.csv')\npatient_df['kaggle_path'] = patient_df.apply(lambda c_row: os.path.join(base_img_dir, \n                                                                        '{Patient_index:06d}_{Study_index:02d}_{Series_ID:02d}'.format(**c_row),\n                                                                        '{Key_slice_index:03d}.png'.format(**c_row)), 1)\npatient_df['Radius'] = patient_df['Lesion_diameters_Pixel_'].map(lambda x: float(x.split(', ')[0]))\nprint('Loaded', patient_df.shape[0], 'cases')\npatient_df.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"sns.pairplot(hue='Patient_gender', data=patient_df[['Patient_age', 'Patient_gender', 'Key_slice_index', 'Radius']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f6f2feb8e0d8536779f93989ad580741500e94e","collapsed":true},"cell_type":"code","source":"patient_df['exists'] = patient_df['kaggle_path'].map(os.path.exists)\npatient_df = patient_df[patient_df['exists']].drop('exists', 1)\n# extact the bounding boxes\npatient_df['bbox'] = patient_df['Bounding_boxes'].map(lambda x: np.reshape([float(y) for y in x.split(',')], (-1, 4)))\nprint('Found', patient_df.shape[0], 'patients with images')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"302157c6e4cacde63a358b810cd8d7cccfafa20a"},"cell_type":"markdown","source":"# Draw Image and  Bounding Box\nHere we use basic code to draw the image and the bounding box. We use the Lung window for the CT to make the views as consistent as possible"},{"metadata":{"trusted":true,"_uuid":"83889a0aa9cfb789115ca753c87414207ec2217e","collapsed":true},"cell_type":"code","source":"def create_boxes(in_row):\n    box_list = []\n    for (start_x, start_y, end_x, end_y) in in_row['bbox']:\n        box_list += [Rectangle((start_x, start_y), \n                         np.abs(end_x-start_x),\n                         np.abs(end_y-start_y)\n                         )]\n    return box_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39ac48de2d6892db5175dd1c614204df5612686c","collapsed":true},"cell_type":"code","source":"_, test_row = next(patient_df.sample(1, random_state=0).iterrows())\nfig, ax1 = plt.subplots(1, 1, figsize = (10, 10))\nc_img = read_hu(test_row['kaggle_path'])\nax1.imshow(c_img, vmin = -1200, vmax = 600, cmap = 'gray')\nax1.add_collection(PatchCollection(create_boxes(test_row), alpha = 0.25, facecolor = 'red'))\nax1.set_title('{Patient_age}-{Patient_gender}'.format(**test_row))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"efbf83094185fd3c3d836f4cad74306d58213255","collapsed":true},"cell_type":"code","source":"fig, m_axs = plt.subplots(3, 3, figsize = (20, 20))\nfor c_ax, (_, c_row) in zip(m_axs.flatten(), \n        patient_df.sample(50, random_state=0).iterrows()):\n    \n    c_img = read_hu(c_row['kaggle_path'])\n    c_ax.imshow(c_img, vmin = -1200, vmax = 600, cmap = 'gray')\n    c_ax.add_collection(PatchCollection(create_boxes(c_row), alpha = 0.25, facecolor = 'red'))\n    c_ax.set_title('{Patient_age}-{Patient_gender}'.format(**c_row))\n    c_ax.axis('off')\nfig.savefig('overview.png', dpi = 300)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38bd54ca2dda5a619161c350e4457c56ba848b20"},"cell_type":"markdown","source":"# Convert Bounding box to Segmentation\nSince there are lot of segmentation models we can try applying we show how the bounding box can be easily converted into a segmentation"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e7d737b6f407e78087e98d2549ef441d36f10d78"},"cell_type":"code","source":"def create_segmentation(in_img, in_row):\n    yy, xx = np.meshgrid(range(in_img.shape[0]),\n               range(in_img.shape[1]),\n               indexing='ij')\n    out_seg = np.zeros_like(in_img)\n    for (start_x, start_y, end_x, end_y) in in_row['bbox']:\n        c_seg = (xx<end_x) & (xx>start_x) & (yy<end_y) & (yy>start_y)\n        out_seg+=c_seg\n    return np.clip(out_seg, 0, 1).astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"435d5dc91c95f7302c0ecb20a06b91cbf60a88db","collapsed":true},"cell_type":"code","source":"from skimage.segmentation import mark_boundaries\napply_softwindow = lambda x: (255*plt.cm.gray(0.5*np.clip((x-50)/350, -1, 1)+0.5)[:, :, :3]).astype(np.uint8)\nfig, m_axs = plt.subplots(3, 2, figsize = (10, 15))\nfor (ax1, ax2), (_, c_row) in zip(m_axs, \n        patient_df.sample(50, random_state=0).iterrows()):\n    \n    c_img = read_hu(c_row['kaggle_path'])\n    ax1.imshow(c_img, vmin = -1200, vmax = 600, cmap = 'gray')\n    ax1.add_collection(PatchCollection(create_boxes(c_row), alpha = 0.25, facecolor = 'red'))\n    ax1.set_title('{Patient_age}-{Patient_gender}'.format(**c_row))\n    ax1.axis('off')\n    \n    c_segs = create_segmentation(c_img, c_row).astype(int)\n    ax2.imshow(mark_boundaries(image=apply_softwindow(c_img), \n                               label_img=c_segs,\n                               color=(0,1,0),\n                              mode='thick'))\n    ax2.set_title('Segmentation Map')\n    \nfig.savefig('over_withsegs.png', dpi = 300)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e530f8e1175038f286707c2c2b040333f6f0395b"},"cell_type":"markdown","source":"# Export Segmentations to HDF5"},{"metadata":{"trusted":true,"_uuid":"809672e57585b7779b3ecec4cb493eed39e402f6","collapsed":true},"cell_type":"code","source":"img_list = []\nseg_list = []\npath_list = []\nfrom tqdm import tqdm_notebook\nfor (_, c_row) in tqdm_notebook(patient_df.iterrows()):\n    c_img = read_hu(c_row['kaggle_path'])\n    img_list+=[c_img]\n    seg_list+=[create_segmentation(c_img, c_row).astype(bool)]\n    path_list+=[c_row['File_name']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a2021992882dffd7cc819b12f4f0389955f72e5d"},"cell_type":"code","source":"from skimage.transform import resize\ndef smart_stack(in_list, *args, **kwargs):\n    \"\"\"\n    Use the first element to determine the size for all the results and resize the ones that dont match\n    \"\"\"\n    base_shape = in_list[0].shape\n    return np.stack([x if x.shape==base_shape else resize(x, base_shape, preserve_range=True) for x in in_list], *args, **kwargs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"92bb43e41e283d3d6ba4b8aaafea24057ba39f1f"},"cell_type":"code","source":"# utility functions compied from https://github.com/4Quant/pyqae\ndef _dsum(carr,  # type: np.ndarray\n          cax  # type: int\n          ):\n    # type: (...) -> np.ndarray\n    \"\"\"\n    Sums the values along all other axes but the current\n    \"\"\"\n    return np.sum(carr, tuple(n for n in range(carr.ndim) if n is not cax))\n\ndef get_bbox(in_vol,\n             min_val=0):\n    # type: (np.ndarray, float) -> List[Tuple[int,int]]\n    \"\"\"\n    Calculate a bounding box around an image in every direction\n    \"\"\"\n    ax_slice = []\n    for i in range(in_vol.ndim):\n        c_dim_sum = _dsum(in_vol > min_val, i)\n        wh_idx = np.where(c_dim_sum)[0]\n        c_sl = sorted(wh_idx)\n        if len(wh_idx) == 0:\n            ax_slice += [(0, 0)]\n        else:\n            ax_slice += [(c_sl[0], c_sl[-1] + 1)]\n    return ax_slice\n\n\ndef apply_bbox(in_vol,  # type: np.ndarray\n               bbox_list,  # type: List[Tuple[int,int]]\n               pad_values=False,\n               padding_mode='edge'\n               ):\n    # type: (...) -> np.ndarray\n    \"\"\"\n    Apply a bounding box to an image\n    \"\"\"\n\n    if pad_values:\n        # TODO test padding\n        warnings.warn(\"Padded apply_bbox not fully tested yet\", RuntimeWarning)\n        n_pads = []  # type: List[Tuple[int,int]]\n        n_bbox = []  # type: List[Tuple[int,int]]\n        for dim_idx, ((a, b), dim_size) in enumerate(zip(bbox_list,\n                                                         in_vol.shape)):\n            a_pad = 0 if a >= 0 else -a\n            b_pad = 0 if b < dim_size else b - dim_size + 1\n            n_pads += [(a_pad, b_pad)]\n            n_bbox += [(a + a_pad, b + a_pad)]  # adjust the box\n\n        while len(n_pads)<len(in_vol.shape):\n            n_pads += [(0,0)]\n        # update the volume\n        in_vol = np.pad(in_vol, n_pads, mode=padding_mode)\n        # update the bounding box list\n        bbox_list = n_bbox\n\n    return in_vol.__getitem__([slice(a, b, 1) for (a, b) in bbox_list])\n\n\ndef autocrop(in_vol,  # type: np.ndarray\n             min_val  # type: double\n             ):\n    # type (...) -> np.ndarray\n    \"\"\"\n    Perform an autocrop on an image by keeping all the points above a value\n    \"\"\"\n    return apply_bbox(in_vol, get_bbox(in_vol,\n                                       min_val=min_val))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b734a1f13a67187492c8e4c2a03ff1bccc00244b"},"cell_type":"markdown","source":"## Show all the lesions in one view"},{"metadata":{"trusted":true,"_uuid":"541d53f16a2709379f6333bef7a63b3b956fed13","collapsed":true},"cell_type":"code","source":"from skimage.util.montage import montage2d as montage\nall_lesions = smart_stack([autocrop((img+1000)*seg,0)-1000 for img, seg in zip(img_list, seg_list)])\nfig, ax1 = plt.subplots(1, 1, figsize = (15, 15))\nax1.imshow(montage(all_lesions), cmap = 'bone', vmin = -500, vmax = 400)\nfig.savefig('montage.png', dpi = 300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f057901323520e995f2e3f78bd96d0464fee989","collapsed":true},"cell_type":"code","source":"import h5py\nwith h5py.File('deeplesion.h5', 'w') as h:\n    h.create_dataset('image', data=np.expand_dims(smart_stack(img_list, 0), -1), \n                     compression = 5)    \n    h.create_dataset('mask', data=np.expand_dims(smart_stack(seg_list, 0), -1).astype(bool), \n                     compression = 5)    \n    h.create_dataset('file_name', data=[x.encode('ascii') for x in path_list], \n                     compression = 0)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84f9823baab6d69d1bbbf3845e559c7f090ba305","collapsed":true},"cell_type":"code","source":"# check the file\n!ls -lh *.h5\nwith h5py.File('deeplesion.h5', 'r') as h:\n    for k in h.keys():\n        print(k, h[k].shape, h[k].dtype, h[k].size/1024**2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7d0cccd14f43db92f706a47362a2d0c60f27ec8a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}