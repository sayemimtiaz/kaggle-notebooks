{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Preamble\n\nThis notebook tests and compares the performance of four different machine learning models to classify text messages as either spam or not spam (binary classification). spaCy is the NLP library used in this notebook to illustrate the following concepts:\n\n- Lemmatizing (converting words to their \"base form\")\n- Removing stop words\n- Word vectors\n- Using word vectors as features in a machine learning model\n\nThe machine learning models used are the **neural network, decision tree, support vector machine, and k-nearest neighbours.** Although it's important, exploratory data analysis isn't included in this notebook. It is noted that the dataset is imbalanced and there are no instances of NaN. The dataset has two columns, *Message* (text to be classified) and *Category* (target label for the text).\n\n# Preprocessing - Lemmatizing, Removing stop words","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport spacy\nimport en_core_web_lg\n\ndata= pd.read_csv('../input/spam-text-message-classification/SPAM text message 20170820 - Data.csv', delimiter= ',')\nN,d= np.shape(data)\nprint(N,d)\n\nnlp= en_core_web_lg.load() #large starter model trained on web text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create new dataframe, same as original but every Messages entry will have only lemmatized words and stop words will be removed\ndata_clean= pd.DataFrame(columns=['Category', 'Message'])\n\nfor i in range(N):\n    doc= nlp(data.iloc[i,1]) #col 1 has the text to be preprocessed and classified\n    \n    clean_text= []\n    for tok in doc:\n        lex= nlp.vocab[tok.lemma] #consider lemmatized words only\n        if lex.is_stop == False: #don't include stop words\n            clean_text.append(tok.lemma_)\n    \n    add_row= pd.Series({'Category': data.iloc[i,0], 'Message': clean_text}) #for each entry in the new dataframe, keep the label the same\n    data_clean= data_clean.append(add_row, ignore_index= True) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Word Vectors\n\nWe've cleaned up the text a bit by lemmatizing the words in each of the messages and by removing stop words. Now we are going to represent each message numerically by converting them to a word vector. Word vectors are \"smarter\" than a bag of words representation because each word vector considers the meaning or context of the word; words that have similar meanings will have similar word vectors. Once all of the messages have been converted to word vectors, our data will be entirely numerical and we can proceed with using any machine learning model as we normally would.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#right now each message in data_clean is a list of strings. convert each message to be one string each\nfor i in range(N):\n    data_clean.iloc[i, 1]= \" \".join(data_clean.iloc[i, 1])\n\n#convert each message to a word vector\n#for each document, first get the word-level embeddings and then use the average of each word vector in the document as the document-level embedding\n#(even though there are multiple words in each message, we want a single vector representing all of the words in each message. taking the average of the vectors representing each of the words allows us to do this)\nmessage_vectors= np.array([nlp(j).vector for j in data_clean.iloc[:,1]])\nN,d= np.shape(message_vectors)\nprint(N,d)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that our number of messages is unchanged (5572) but each message is now represented by 300 features. The 300 dimension comes from the particular model we're using (*en_core_web_lg*) and may differ depending on the model used.\n\n# Results from Models\n\nOur text data is now entirely numerical. Since the dataset is relatively small, we'll use 10-fold cross validation to evaluate the performance of our machine learning models. For each model, precision and recall will be our evaluation metrics since the dataset is imbalanced. Despite the dataset being imbalanced, all examples will be equally weighted by the models making the predictions (i.e. examples belonging to the less frequent class won't be considered more important than the examples belonging to the more frequent class). The structure of the following code is as follows:\n\n1. Create objects that will standardize our features and create our 10 folds\n2. Create models (neural network, decision tree, support vector machine, k-nearest neighbors)\n3. Create a function that will create 10 folds, standardize our feature values, and train and evaluate a model's classification performance\n4. Run the function for each of the models created in step 2 and decide which model had the best performance","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# for each iteration of the 10 folds, first standardize all feature values to have a mean of 0 and unit variance\n# then train and evaluate the model's performance\n\nfrom sklearn.model_selection import StratifiedKFold #for cross validation train/test splits\nfrom sklearn.preprocessing import StandardScaler #for feature normalization\nfrom sklearn.neural_network import MLPClassifier \nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.svm import LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import precision_score, recall_score #model performance metrics\n\n\ndef eval_model(features, labels, model):\n    get_folds= StratifiedKFold(n_splits= 10, random_state= 2, shuffle= True)\n    scaler= StandardScaler()\n    \n    precisions= np.array([])\n    recalls= np.array([])\n\n    for train_idx, test_idx in get_folds.split(features, labels):\n        # get training and test data\n        x_train, x_test= features[train_idx], features[test_idx]\n        y_train, y_test= labels[train_idx], labels[test_idx]\n    \n        # scale training and test features\n        x_train_scaled= scaler.fit_transform(x_train)\n        x_test_scaled= scaler.transform(x_test) # no data leakage here! using the training data to transform the test data\n    \n        #fit model using training data\n        model.fit(x_train_scaled, y_train)\n    \n        #get predictions on the test set and return precision and recall for each of the 10 folds\n        predicts= model.predict(x_test_scaled)\n        precisions= np.append(precisions, precision_score(y_test, predicts, average= 'weighted'))\n        recalls= np.append(recalls, recall_score(y_test, predicts, average= 'weighted'))\n    \n    # return precision and recall for each of the 10 folds in the cross-validation\n    return precisions, recalls\n    \nlabels= data_clean.iloc[:,0] # message_vectors is the matrix of corresponding features\n\n\n#create 2-layer neural network. hyperparameter settings are arbitrary\nneural_net= MLPClassifier(hidden_layer_sizes= (50,), activation= 'relu', solver= 'adam', alpha= 0.001, max_iter= 250, shuffle= True, random_state= 2)\n\n#create decision tree. hyperparameter settings are arbitrary\nd_tree= DecisionTreeClassifier(random_state= 2)\n\n#create support vector machine. hyperparamter settings are arbitrary\nsvm= LinearSVC(dual= False, fit_intercept= False, random_state= 2, max_iter= 250)\n\n#create kNN model. hyperparameter settings are arbitrary\nknn= KNeighborsClassifier()\n\n\n#for each of the models, get the precision and recall for each of the 10 folds\nnn_precisions, nn_recalls= eval_model(message_vectors, labels, neural_net)\nprint('Neural net precisions and recalls:', '\\n', nn_precisions, '\\n', nn_recalls)\n\ntree_precisions, tree_recalls= eval_model(message_vectors, labels, d_tree)\nprint('Decision tree precisions and recalls:', '\\n', tree_precisions, '\\n', tree_recalls)\n\nsvm_precisions, svm_recalls= eval_model(message_vectors, labels, svm)\nprint('SVM precisions and recalls:', '\\n', svm_precisions, '\\n', svm_recalls)\n\nknn_precisions, knn_recalls= eval_model(message_vectors, labels, knn)\nprint('kNN precisions and recalls:', '\\n', knn_precisions, '\\n', knn_recalls)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualizing Model Performance\n\nSeeing the precisions and recalls like this can be a bit overwhelming and hard to summarize. Instead, we can visualize the precisions and recalls for each of the 10 folds for each of the models using box plots. By creating notched boxplots, we can visualize the confidence intervals about the median for each of the models for both precision and recall and decide which model had the best performance.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n#visualize precisions\nplt.figure()\nplt.boxplot([nn_precisions, tree_precisions, svm_precisions, knn_precisions], notch= True, labels= ['Neural net', 'Decision tree', 'SVM', 'k-Nearest neighbors'])\nplt.title('PRECISIONS for the 10 folds for each of the models')\nplt.ylim([0.9,1.0])\nplt.show()\n\n#visualize recalls\nplt.figure()\nplt.boxplot([nn_recalls, tree_recalls, svm_recalls, knn_recalls], notch= True, labels= ['Neural net', 'Decision tree', 'SVM', 'k-Nearest neighbors'])\nplt.title('RECALLS for the 10 folds for each of the models')\nplt.ylim([0.75, 1.0])\nplt.show()\n\n#get average precision and recall for all models\nprint('Average precisions: ')\nfor i in [nn_precisions, tree_precisions, svm_precisions, knn_precisions]:\n    print(np.mean(i))\n\nprint('\\n', 'Average recalls: ')\nfor i in [nn_recalls, tree_recalls, svm_recalls, knn_recalls]:\n    print(np.mean(i))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can see that the neural network had the best performance as it had the highest precision and recall. This model had an average precision of 98% and an average recall of 98%. The second-best model was the k-Nearest neighbors model followed by the decision tree followed by the support vector machine.** If there were overlap in the confidence intervals for any of the models, one may argue that the models with the overlapping confidence intervals performed equally well. This is not the case here - the ranking of the performance of the models is clear. \n\n# Afterthoughts\nAlthough exploratory data analysis wasn't included as part of this notebook, it would be interesting to compare word clouds of the messages before and after the text preprocessing (lemmatizing, removing stop words). This may be especially helpful if the machine learning models performed poorly because tokenizing the messages and preprocessing the text may have resulted in many nonsensical words - for example, when text is tokenized, words with apostrophes are split at the apostrophe into two words, so the word \"don't\" would become the words \"do\" \"nt\".\n\nIf you have any other thoughts or feedback, please leave below in the comments!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}