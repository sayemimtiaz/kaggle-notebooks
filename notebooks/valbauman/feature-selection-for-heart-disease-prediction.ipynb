{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Preamble\nThis notebook aims to illustrate some feature selection methods using the [Heart Failure Prediction dataset](https://www.kaggle.com/andrewmvd/heart-failure-clinical-data). This dataset has 12 numerical features for each example and the target for each feature is whether or not death occurred (binary classification). **Although ML models will be trained and evaluated in this notebook, the primary focus is feature selection methods.** The topics covered and coded are as follows:\n\n- Correlation plots\n- L1 regularization for feature selection\n- SelectKBest for feature selection\n- Random forests for feature selection\n\n*Note: some of this notebook and a [notebook](https://github.com/vvbauman/Feature-generation-selection/blob/master/Ad%20Click%20-%20Feature%20Generation%20and%20Selection.ipynb) I have published to GitHub use the same code and cover some of the same topics. I've also created a follow-up notebook that illustrates methods explaining feature importance using the same dataset and model developed in this notebook.*\n\n# Data Overview\nFirst we'll load the data, see the first few lines, and determine if there are any NaN values.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndata= pd.read_csv('../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv', delimiter= ',')\nprint(np.shape(data))\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#check for NaNs...\ndata.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This dataset has 299 examples, each characterized by 12 features. There are no NaN values anywhere in the dataset.\n\n# Correlation Plot\nWe can visualize the correlation between each feature value and the target to get an idea of how each feature is related to the target.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr= data.corr().iloc[-1,:].to_numpy().reshape(13,1)\nsns.heatmap(corr, yticklabels=data.columns, xticklabels= 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see there is a moderate positive correlation between the target and the features age and serum_creatinine. There is a moderate negative correlation between the target and the features ejection_fraction, serum_sodium, and time. When we try out our feature selection methods, let's see if these features (age, serum_creatinine, ejection_fraction, serum_sodium, and time) are the selected features. Before we get into the feature selection method, we'll split the data into train/validation sets (90/10 split). We'll use only the training set when doing our feature selection methods.\n\n# Feature Selection - L1 Regularization\nThis method involves training a linear model that uses an L1 penalty. All features are used to train this model and the L1 penalty causes the weight/contribution of unimportant features to be zero. We then extract the non-zeroed features and use them in our machine learning model. This method considers all features and how they collectively contribute to each prediction.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#declare the feature values and labels then split data into training/validation sets\nfrom sklearn.model_selection import train_test_split\n\nfeats= data.iloc[:,:-1]\nlabels= data.iloc[:,-1]\n\nx_train, x_devel, y_train, y_devel= train_test_split(feats, labels, test_size= 0.1, random_state= 20)\n\n#train linear model with L1 penalty\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\n\nlsvc= LinearSVC(C= 1.0, penalty= 'l1', dual= False).fit(x_train, y_train)\nsvc_mod= SelectFromModel(lsvc, prefit= True)\n\n#get non-zeroed features \nx_train_svc= svc_mod.transform(x_train) #training set w/non-zeroed features\nselected_feats_svc= pd.DataFrame(svc_mod.inverse_transform(x_train_svc), index= x_train.index, columns= x_train.columns)\nselected_cols_svc= selected_feats_svc.columns[selected_feats_svc.var() != 0]\n\n#get development set that has only the non-zeroed features\nx_devel_svc= x_devel[selected_cols_svc]\n\n#see which features were retained\nprint('Features retained: ', selected_cols_svc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the L1 regularization method for feature selection results in 11 of the 12 features being retained! Once we've tried all feature selection methods, we'll try them each in a machine learning model and compare their performance to a model that use all features.\n\n# Feature Selection - SelectKBest\nThis next feature selection method involves evaluating the linear relationship between each feature and the target. The top-k features with the strongest relationship with the target are identified and are used in the machine learning model. We will retain the top 5 features since this was the number of features with a moderate correlation with the target (as identified in the Correlation Plot section of this notebook). Unlike the L1 regularization method, this is a univariate method, meaning that each feature is considered individually/one-by-one.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, f_classif\nkbest_feats= SelectKBest(f_classif, k=5)\n\n#get top 5 best features\nx_train_kbest= kbest_feats.fit_transform(x_train, y_train)\nselected_feats_kbest= pd.DataFrame(kbest_feats.inverse_transform(x_train_kbest), index= x_train.index, columns= x_train.columns)\nselected_cols_kbest= selected_feats_kbest.columns[selected_feats_kbest.var() != 0]\n\n#get development set that has the top 5 features\nx_devel_kbest= x_devel[selected_cols_kbest]\n\n#see which features were retained\nprint('Features retained: ', selected_cols_kbest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that this feature selection method is equivalent to choosing the top-n features with the highest correlation with the target - we get the same features that we identified earlier that had the highest correlations with the target.\n\n# Feature Selection - Random Forest\nThe final feature selection method considered in this notebook is the random forest (an ensemble of decision trees). Decision trees naturally rank features by how well they distinguish classes. Features that best distinguish classes are evaluated at nodes at the start of a tree. Based on this, if we prune a tree at a certain node, we can get a subset of the most informative features.\n\nImplementing this feature selection method involves training a random forest and identifying the features that have an importance weight greater than some threshold. These features are the ones used in your machine learning model. We will use the median importance weight as our threshold, meaning that any features that have an importance weight greater than the median importance will be retained.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n#create and train a random forest\nforest= RandomForestClassifier(n_estimators= 1000, random_state= 20)\nforest.fit(x_train, y_train)\n\n#get the most important features\nforest_feats= SelectFromModel(forest, threshold= 'median')\nforest_feats.fit(x_train, y_train)\n\n#get training and development sets that have only the most important features\nx_train_forest= forest_feats.transform(x_train)\nx_devel_forest= forest_feats.transform(x_devel)\n\n#see which features were retained\nfor i in forest_feats.get_support(indices= True):\n    print(x_train.columns[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When we use a random forest as our feature selection method, 6 features are retained. Now that we've tried 3 different feature selection approaches, let's see which feature set gives us the best results for a machine learning model. The model we'll use is the Support Vector Machine with its default hyperparameter settings.\n\n# SVM to Decide Feature Set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import recall_score, precision_score\n\ndef eval_svm(train_feats, test_feats, train_labs, test_labs):\n    \"\"\"\n    INPUT: train_feats and test_feats are either 2D numpy arrays or pd dataframes with the feature values for the train/test sets \n    train_labs, test_labs are either 1D numpy arrays or pd series with the corresponding labels to the train/test features\n    \n    OUTPUT: classification_results is a string of results, incl. precision, recall, and f1-score for each class\n    \"\"\"\n    #scale features before using in SVM\n    scaler= StandardScaler()\n    train_feats_scale= scaler.fit_transform(train_feats)\n    test_feats_scale= scaler.transform(test_feats)\n    \n    svm= SVC()\n    svm.fit(train_feats_scale, train_labs)\n    \n    predicts= svm.predict(test_feats_scale)\n    \n    precision= precision_score(test_labs, predicts, average= None, zero_division= 0)\n    recall= recall_score(test_labs, predicts, average= None, zero_division= 0)\n    \n    return precision, recall\n\n#get performance of model that uses all features\nprec_allfeats, rec_allfeats= eval_svm(x_train, x_devel, y_train, y_devel)\n\n#get performance of model that uses features from L1 regularization\nprec_svc, rec_svc= eval_svm(x_train_svc, x_devel_svc, y_train, y_devel)\n\n#get performance of model that uses features from SelectKBest\nprec_kbest, rec_kbest= eval_svm(x_train_kbest, x_devel_kbest, y_train, y_devel)\n\n#get performance of model that uses features from random forest\nprec_forest, rec_forest= eval_svm(x_train_forest, x_devel_forest, y_train, y_devel)\n\nprint('SVM precision and recall, all features: ', prec_allfeats, rec_allfeats, '\\n'+'SVM precision and recall, L1 regularization features: ', prec_svc, rec_svc)\nprint('SVM precision and recall, top 5 features: ', prec_kbest, rec_kbest, '\\n'+'SVM precision and recall, random forest features: ', prec_forest, rec_forest, '\\n')   \nprint('Average recalls: ', np.mean(rec_allfeats), np.mean(rec_svc), np.mean(rec_kbest), np.mean(rec_forest))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To understand these results, we'll consider the first SVM that was trained using all of the features. The first two numbers are the precisions for the two classes: 0.9 is the precision for class 0 and 0.5 is the precision for class 1. The second two numbers are the recalls for the two classes: 0.78 is the recall for class 0 and 0.71 is the recall for class 1. \n\nBased on the average recall, the model that used all features is the best and should be retained as the final model. If the hospital wanted to reduce the number of features they measure for each patient but still be able to predict the occurrence of a death event, they can measure the features that were part of the top-5 feature set. The model that used the top 5 features achieved comparable prediction performance as the model that used all features (0.73 vs 0.75 average recall).\n\n# Conclusions and Next Steps\nThis notebook illustrated and provided sample code for 3 different approaches to feature selection: L1 regularization, SelectKBest, and random forest. When we tested a SVM model using the different feature sets that resulted from these feature selection approaches, the feature set that resulted in the best model performance was when we used all features. The model that resulted in the second-best performance was the model that used the feature set from the SelectKBest method. \n\n**I've created a follow-up notebook that illustrates different ways to describe and quantify feature importance. In this follow-up notebook, the SVM model that used the features from the SelectKBest method developed in this notebook will be used.** Questions and feedback are always welcome in the comments!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}