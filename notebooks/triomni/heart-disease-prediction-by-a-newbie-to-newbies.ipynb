{"cells":[{"metadata":{"trusted":false,"_uuid":"b9d8863957c0d17632d2b4edfd6bfd192093886c"},"cell_type":"code","source":"#@Rita","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2af8a815d952e5c2a9709d0a58ec6d7042c8851c"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74504bbf26957781aa97bc754bc87c1e631abe99"},"cell_type":"markdown","source":"# 1--> GET THE DATA"},{"metadata":{"trusted":true,"_uuid":"578c599b1e148dd97ad9d5910da9a9914e438b19"},"cell_type":"code","source":"df = pd.read_csv('../input/heart.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"995f3afd00dbb9ce713e080ace0c37dc3ea89829"},"cell_type":"markdown","source":"**AGE**: age in years.\n\n**SEX**: (1 = male; 0 = female).\n\n**CP**: chest pain type.\n+ 0, Typical angina: chest pain related to the decrease of blood supply to the heart.\n+ 1, Atypical angina: chest pain not related to the heart.\n+ 2, Non-anginal pain: esophageal spasms (not related to the heart).\n+ 3, Asymptomatic: chest pain not showing signs of disease.\n\n**TRESTBPS**: resting blood pressure (in mmHg on admission to the hospital).\n+ Anything above 130-140 is typically cause for concern.\n\n**CHOL**: serum cholestoral level in mg/dl.\n+ serum = LDL + HDL + 0.2*triglycerides.\n+ Above 200 is cause for concern.\n\n**FBS**: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false).\n+ fbs > 126 mg/dL signals diabetes.\n\n**RESTECG**: resting electrocardiographic results (0 - 2).\n+ 0, Nothing to note.\n+ 1, ST-T Wave abnormality. Signals non-normal heart beat â¤ï¸.\n+ 2, Possible or definite Left ventricular hypertrophy.\n\n**THALACH**: maximum heart rate achieved.\n+ Rate above 100 is cause for concern.\n\n**EXANG**: exercise induced angina (1 = yes; 0 = no).\n\n**OLDPEAK**: ST depression induced by exercise relative to rest.\n+ Looks at stress of heart during exercise.\n+ Unhealthy heart will stress more.\n\n**SLOPE**: the slope of the peak exercise ST segment.\n+ 0, Upsloping: better heart rate with exercise (uncommon).\n+ 1, Flatsloping: minimal change (typical healthy heart).\n+ 2, Downsloping: signs of unhealthy heart.\n\n**CA**: number of major vessels (0 - 3) colored by flourosopy.\n\n**THAL**: thalium stress test result. Sees how blood moves through the heart while exercising.\n+ 1, 3 = normal; \n+ 6 = fixed defect; Used to be defect, but now it is okay.\n+ 7 = reversable defect. Not proper blood movement when exercising.\n\n**TARGET**: heart disease (1 or 0)."},{"metadata":{"trusted":true,"_uuid":"afb11b98ea699083e4cddb7900238378622a2aba"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9771d7969e138fa0b1992dc2f0750abdcc1a0ccd"},"cell_type":"markdown","source":"# 2--> CLEAN THE DATA\n####  and make it easier to work with..."},{"metadata":{"_uuid":"93031f22a9d7f27f60689c369da7079c7f39668e"},"cell_type":"markdown","source":"3. Cleaning "},{"metadata":{"trusted":false,"_uuid":"acec752d5f20ff42934982dd5f91415d9538fe51"},"cell_type":"code","source":"# We must clean the NaN values, since we cannot train\n# our model with unknown values.\n# Luckily, there is nothing to clean.\ndf.info() , df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"edb8551c0cfc5ae26c50c23573c6dbf184956c05"},"cell_type":"code","source":"# We have no need to encode labels since \n# the categories are fine (integers).","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd8c3c23fadb8ee4b9e53f5468332cca9090d5d6"},"cell_type":"markdown","source":"# 3--> EXPLORE THE DATA\n#### and find the explanatory and the response variables..."},{"metadata":{"_uuid":"512643bb17463abf17bf90a5bf4b8ed4bd5a090a"},"cell_type":"markdown","source":"1. Identification of the problem:"},{"metadata":{"trusted":false,"_uuid":"700fc5a627901263d30528a8a184d0989b71364a"},"cell_type":"code","source":"# GOAL: presence of heart disease in a patient given the explanatory variables\n# which are age, sex, and so on.\n# Therefore, target is YES or NO (1/0) depending whether a patient has got the disease or not.\ndf.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8926eff6abc409f87028302899c283168c7c5d93"},"cell_type":"markdown","source":"2. Analysis of the problem:"},{"metadata":{"trusted":false,"_uuid":"a11543cffffc16f4aa308eaa499ee1bf6118cc6b"},"cell_type":"code","source":"# The best way to explore data is via plots\n# so we gonna plot some stuff","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c4c03e76d845e04e40ebd4ce2651ee18744425a0"},"cell_type":"code","source":"# First of all let's see how many zeros and ones do we have...\nnegative_target = len(df[df.target == 0])\npositive_target = len(df[df.target == 1])\nprint(\"Percentage of Patients that do not have Heart Disease: {:.3f}%\".format((negative_target / (len(df.target))*100)))\nprint(\"Percentage of Patients that have Heart Disease: {:.3f}%\".format((positive_target / (len(df.target))*100)))\nsns.countplot(x = \"target\", data = df, palette = \"pastel\")\nplt.xlabel(\"Target (0 = no, 1= yes)\")\nplt.ylabel(\"count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6d7e50ff7048fea7774e22d0fa8cf184b131d0f0"},"cell_type":"code","source":"# Males vs Females\nfemale_patient = len(df[df.sex == 0])\nmale_patient = len(df[df.sex == 1])\nprint(\"Percentage of Female Patients: {:.3f}%\".format((female_patient / (len(df.sex))*100)))\nprint(\"Percentage of Male Patients: {:.3f}%\".format((male_patient / (len(df.sex))*100)))\nsns.countplot(x = 'sex', data = df, palette = \"pastel\")\nplt.xlabel(\"Sex (0 = female, 1= male)\")\nplt.ylabel(\"count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fe341866f671e16c747e501bb931ac04132c77de"},"cell_type":"code","source":"for x in range (0, 4):\n    print(\"% of patients that show positivity with chest pain {}: {:.3f}%\".format(x, 100*(((df['cp'] == x) & df['target'] == 1).sum())/((df['target'] == 1).sum())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0c3b6e345fc197ebd40b43227b5d3e518e8620fe"},"cell_type":"code","source":"for x in range (0, 4):\n    print(\"% of patients that show negativity with chest pain {}: {:.3f}%\".format(x, 100*(((df['cp'] == x) & (df['target'] == 0)).sum())/((df['target'] == 0).sum())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0c730d3f6bcd58cfd92dcca8eeb65e81802c2776"},"cell_type":"code","source":"print(\"% of patients that show positivity with trestbps > 130: {:.3f}%\".format\n      (100*(((df['trestbps'] > 130) & df['target'] == 1).sum())\n       /((df['target'] == 1).sum())))\nprint(\"% of patients that show positivity with trestbps <= 130: {:.3f}%\".format\n      (100*(((df['trestbps'] <= 130) & df['target'] == 1).sum())\n       /((df['target'] == 1).sum())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"095fb7df12b3ceb7e463ad7d03bc9c417d25b21d"},"cell_type":"code","source":"print(\"% of patients that show positivity with chol > 200: {:.3f}%\".format\n      (100*(((df['chol'] > 200) & df['target'] == 1).sum())\n       /((df['target'] == 1).sum())))\nprint(\"% of patients that show positivity with chol <= 200: {:.3f}%\".format\n      (100*(((df['chol'] <= 200) & df['target'] == 1).sum())\n       /((df['target'] == 1).sum())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ca4f9bb833a86424e956e95a45f43ba02f3dc7b5"},"cell_type":"code","source":"for x in range (0, 2):\n    print(\"% of patients that show positivity with fbs {}: {:.3f}%\".format(x, 100*(((df['fbs'] == x) & df['target'] == 1).sum())/((df['target'] == 1).sum())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8eb8f21b5d7a35d0167f49a694ee1098e9959292"},"cell_type":"code","source":"for x in range (0, 3):\n    print(\"% of patients that show positivity with restecg {}: {:.3f}%\".format(x, 100*(((df['restecg'] == x) & df['target'] == 1).sum())/((df['target'] == 1).sum())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2971d65ed2f2c66fae2b8ab60b0e26c212b10e0b"},"cell_type":"code","source":"print(\"% of patients that show positivity with thalach > 100: {:.3f}%\".format\n      (100*(((df['thalach'] > 100) & df['target'] == 1).sum())\n       /((df['target'] == 1).sum())))\nprint(\"% of patients that show positivity with thalach <= 100: {:.3f}%\".format\n      (100*(((df['thalach'] <= 100) & df['target'] == 1).sum())\n       /((df['target'] == 1).sum())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"59af21bbc3c4afb1d1dfeb28a6aac941f76fdaa6"},"cell_type":"code","source":"for x in range (0, 2):\n    print(\"% of patients that show positivity with exang {}: {:.3f}%\".format(x, 100*(((df['exang'] == x) & df['target'] == 1).sum())/((df['target'] == 1).sum())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"64d883aa87747f08eb502df053c996a71818883f"},"cell_type":"code","source":"for x in range (0, 3):\n    print(\"% of patients that show positivity with slope {}: {:.3f}%\".format(x, 100*(((df['slope'] == x) & df['target'] == 1).sum())/((df['target'] == 1).sum())))\nprint(\"We can suppose that Slope = 2 is likely to sign Heart disease.\")   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6a5d7bcd489e16cedfa96c9fc0a12137269822b1"},"cell_type":"code","source":"for x in range (0, 3):\n    print(\"% of patients that show positivity with ca {}: {:.3f}%\".format(x, 100*(((df['ca'] == x) & df['target'] == 1).sum())/((df['target'] == 1).sum())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bfea6837b1558217b4841512f543450135e651aa"},"cell_type":"code","source":"# We have got more positive samples than negative,\n# And we have more females than males in the dataset...\n# but what is the proportion of positivity by sex?","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f199d56b08de5001dbd70a3849107a1f128dab86"},"cell_type":"code","source":"pd.crosstab(df.sex,df.target).plot(kind=\"bar\",figsize=(20,6), color=['#99ccff','#ffcc99'])\nplt.title('Heart Disease Frequency for Sex')\nplt.xlabel('Sex')\nplt.ylabel('Frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e2971e733c65b0a0747b1122237ef007c571fe7d"},"cell_type":"code","source":"# Intuitively, age must be an important factor:\nsns.set(rc={'figure.figsize':(11.7,8.27)})\nsns.countplot(x = 'age', data = df, palette = \"pastel\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8f513923c78612cc1a5457d33c8d28e2da0977ff"},"cell_type":"code","source":"pd.crosstab(df.age, df.target).plot(kind = \"bar\", figsize = (20,6), color=['#99ccff','#ffcc99'])\nplt.title('Heart Disease Frequency by Age')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a8bdb0aa7424b9c0d8582a573fc0da018e08eb1c"},"cell_type":"code","source":"# Now, let's dive into more medical categories: \n# What type of chest pain is likely to show a positive result?","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"69cd805e5fb0e57c691e36a5dfc179704b392888"},"cell_type":"code","source":"pd.crosstab(df.cp,df.target).plot(kind=\"bar\",figsize=(20,6), color=['#99ccff','#ffcc99'])\nplt.title('Heart Disease Frequency for Chest Pain Type')\nplt.xlabel('Chest Pain Type')\nplt.ylabel('Frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c7025ffb16922411e97322bc42c6ff98a4621190"},"cell_type":"code","source":"# Chest pain by age?\npd.crosstab(df.age, df.cp).plot(kind = \"bar\", figsize = (20,6), color=['#99ccff','#ffcc99', '#ffccff','#99ff99'])\nplt.title('Chest Pain type by Age')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2375d2728971572b9df549ce6a28403d4d1b2a78"},"cell_type":"code","source":"pd.crosstab(df.ca,df.target).plot(kind=\"bar\",figsize=(20,6))\nplt.title('Heart Disease Frequency for # of major vessels')\nplt.xlabel('Number of major vessels')\nplt.ylabel('Frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9ae31f69a10d1df98add24f165e3a46177374e57"},"cell_type":"code","source":"# The plot below can be used to determine the correlation between the different features of the dataset. \n# From the above set we can also find out the features which have the most and the least effect on the target \n# feature (whether the patient have heart diseases or not).","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0b9890764e856d1e0153135a30f8ada760e2066"},"cell_type":"markdown","source":"4. New features"},{"metadata":{"trusted":false,"_uuid":"334e2dae4e0e711bc9241dcce11fa84085f6c5cf"},"cell_type":"code","source":"sns.heatmap(df.corr(), cmap=\"YlGnBu\", annot=True)\n_ = plt.title('Correlation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e978fe9a2fb188877e719f7e602bc0b49def122a"},"cell_type":"code","source":"# target is highly correlated with CP, THALACH, EXANG, OLDPEAK.\n# Secondly with CA, SLOPE, THAL.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7af01686f5ff0f07d586a3cdd9621880ea626388"},"cell_type":"markdown","source":"# 4 --> TRAIN THE DATA\n#### and don't forget about CROSS VALIDATION"},{"metadata":{"_uuid":"beb87a82794041dd5a5e2d544cf8ac307a00ae1b"},"cell_type":"markdown","source":"8. Modelos de ML"},{"metadata":{"trusted":false,"_uuid":"abb897633276d33b54e4786df608f30ad32e658e"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b13ebf3a5aed3fb5b267b69c6cba4d83655f9b1b"},"cell_type":"code","source":"# It is a classification problem, and more precisely a BINARY CLASSIFICATION.\n# we divide into two subsets: the training data and the testing data\n# X are the explanatory variables, Y is the response variable ('target'):\nX = df.drop('target', axis=1)  # everything except target.\nY = df['target']               # only target.\n\n\n# Since the amount of data is not extremely large, we will use a small test_size (0.10-0.15).\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e6f4dbdd1236dcb890d87f53cedc3e4bb135c6d"},"cell_type":"markdown","source":"#### RANDOM FOREST"},{"metadata":{"trusted":false,"_uuid":"92159b1d88c3c6a9e5c0fb3ee54c8b23a0cc2a0f"},"cell_type":"code","source":"# Firstly, let's take Random Forest Classifier\nrfc = RandomForestClassifier(n_estimators=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7a66a572aa3ce52d67a3adc08bf823982a048939"},"cell_type":"code","source":"rfc.fit(X_train, Y_train)\nrfc.score(X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3384912014041cbc60021a79927623bb713c5758"},"cell_type":"code","source":"rfc_predictions = rfc.predict(X_test)\ntest_error = accuracy_score(rfc_predictions, Y_test)\n# test_error == rfc.score(X_test, Y_test)\ntest_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"391f0d7662374b958ee623eb9e424726c65994e1"},"cell_type":"code","source":"# Cross validation: see how it works in average:\ncross_validation_scores = cross_val_score(rfc, X, Y, cv=10)\nprint('Average accuracy for Random Forest: %0.4f' %cross_validation_scores.mean())\nprint(cross_validation_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1507d9d36f1de1c625ace0160537e78cea1126b9"},"cell_type":"code","source":"# no overfitting!\n# since test_error is lower than test_train (= cross_validation_scores.mean()).\n# Let's try a smaller test_size:\nrfc_2 = RandomForestClassifier(n_estimators=1000)\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.05) \nrfc_2.fit(X_train, Y_train)\nrfc_2.score(X_test, Y_test), cross_val_score(rfc_2, X, Y, cv=10).mean()\n# overfitting.","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f90323ae2a5de133ae9a1ea4471fd29e3ec8d857"},"cell_type":"code","source":"# Tuning of random forest classifier:\n# 1. n_estimators HIGH.  The more estimators you give it, the better it will do.\n# 2. max_features.  It may have a large impact on the behavior of the RF because it decides \n# how many features each tree in the RF considers at each split. Default is 'sqrt'.\ntuned_rfc = RandomForestClassifier(n_estimators = 5000, oob_score = True,\n                                   max_depth = None, max_features = 'sqrt')\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.05) \ntuned_rfc.fit(X_train, Y_train)\ntuned_rfc.score(X_test, Y_test), cross_val_score(tuned_rfc, X, Y, cv=10).mean()\n\n# I have tried max_features = 'log2' and it does not improve, so the best we can do in this\n# case is increasing n_estimators.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e86efffe747b4118960c0c3b55a400a703f2cc04"},"cell_type":"markdown","source":"#### KNN Model"},{"metadata":{"trusted":false,"_uuid":"245a62311d0400524d01186a349e7f825a78666c"},"cell_type":"code","source":"# KNN Model\nfrom sklearn.neighbors import KNeighborsClassifier\nfor i in range (1,10):\n    knn = KNeighborsClassifier(n_neighbors = i)  # n_neighbors means k\n    knn.fit(X_train, Y_train)\n    prediction = knn.predict(X_train)\n\n    print(\"{} NN Score: {:.4f}%\".format(i, knn.score(X_test, Y_test)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fd73cb5433bd25588693a6a2714a7caa34b13f50"},"cell_type":"code","source":"from sklearn.svm import SVC\nsvm = SVC(random_state = 1)\nsvm.fit(X_train, Y_train)\n\nprint(\"Test Accuracy of SVM Algorithm: {:.4f}%\".format(svm.score(X_test, Y_test)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b80d6e7ceda7687884879156e1a8718f63077713"},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train, Y_train)\nprint(\"Accuracy of Naive Bayes: {:.4f}%\".format(nb.score(X_train, Y_train)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8f4a3d40c17ded9ac252a8d296f11b9ec7036205"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train, Y_train)\nprint(\"Decision Tree Test Accuracy {:.4f}%\".format(dtc.score(X_train, Y_train)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"85527fdebab925219be1f9c7a8d5e3bacd1f27e3"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 1000, random_state = 1)\nrf.fit(X_train, Y_train)\nprint(\"Random Forest Algorithm Accuracy Score : {:.4f}%\".format(rf.score(X_train, Y_train)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bd431356af70b596b602b61f5b19409c0e0d955d"},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15)\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(probability=True,gamma='scale'),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators=100),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis(),\n    LogisticRegression(solver='lbfgs')]\n\nscores = []\nfor clf in classifiers:\n    clf.fit(X_train, Y_train)\n    cv_results = cross_validate(clf, X_test, Y_test, cv=5, return_train_score=True)\n    scores.append(np.mean(cv_results['test_score']))\n    \nsns.barplot(y=[n.__class__.__name__  for n in classifiers], x=scores, orient='h')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"be29147eb95c38f9a6ffa9817df9c9e80278ab02"},"cell_type":"code","source":"scores","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b79c46efbc44f50e94b93c429430bc86e157dc73"},"cell_type":"markdown","source":"11. SVC"},{"metadata":{"trusted":false,"_uuid":"6d60930963d7d8fd2e3218a389fa1467c3e11a5c"},"cell_type":"code","source":"# We observe that logistics regression is the best BUT! ðŸ™ˆ \n# If we use a LINEAR kernel in SVC it should work better!\nsvm = SVC(probability=True, kernel='linear',gamma='scale')\nsvm.fit(X_train, Y_train)\n\nprint(\"Test Accuracy of SVM Algorithm: {:.4f}%\".format(svm.score(X_test, Y_test)*100))\ncv_result = cross_val_score(svm, X, Y, cv=10)\nprint(cv_result.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2d3796f84924c1c6a2d58d9385965259aa392582"},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nX = df.drop('target', axis=1) # everything except target.\nY = df['target']               # only target.\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15)\nnb.fit(X_train, Y_train)\n\nprint(\"Accuracy of Naive Bayes: {:.4f}%\".format(nb.score(X_train, Y_train)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ad282fdfb851ee4c545008958892d2404b201649"},"cell_type":"code","source":"# Which SVC kernel is better?\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.05)\nsvm_linear = SVC(probability=True, kernel='linear',gamma='scale')\nsvm_linear.fit(X_train, Y_train)\n\nprint(\"Test Accuracy of SVM Algorithm: {:.4f}%\".format(svm_linear.score(X_test, Y_test)*100))\ncv_result_linear = cross_val_score(svm_linear, X, Y, cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"db9e9bdcf058919ede308dd0f3a8ee8cf0ac0971"},"cell_type":"code","source":"svm_poly = SVC(probability=True, kernel='poly',degree=3,gamma='scale')\nsvm_poly.fit(X_train, Y_train)\n\nprint(\"Test Accuracy of SVM Algorithm: {:.4f}%\".format(svm_poly.score(X_test, Y_test)*100))\ncv_result_poly = cross_val_score(svm_poly, X, Y, cv=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5b466318d66b1af34c008301cd19c666de5cb935"},"cell_type":"code","source":"svm_rbf = SVC(probability=True, kernel='rbf',gamma='scale')\nsvm_rbf.fit(X_train, Y_train)\n\nprint(\"Test Accuracy of SVM Algorithm: {:.4f}%\".format(svm_rbf.score(X_test, Y_test)*100))\ncv_result_rbf = cross_val_score(svm_rbf, X, Y, cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9ce64d870871e8cf1dae0ae272cb9e76e703e67d"},"cell_type":"code","source":"# order by\nprint(cv_result_linear.mean(), cv_result_poly.mean(), cv_result_rbf.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6c3862f0cf740083d03e0122fe534760f6e615a"},"cell_type":"markdown","source":"# 5 --> PREDICT THE DATA"},{"metadata":{"_uuid":"8295d328e9e4905b72d1b63971ed108f01ef41b8"},"cell_type":"markdown","source":"9. Analysis of obtained predictions"},{"metadata":{"trusted":false,"_uuid":"a4a0c06a21ca5c7873cb787a2cb1e5292f62c5ab"},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f500220fccd868f6ead58cc9bfe1b9a99b62ab7e"},"cell_type":"code","source":"Y_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"31678444aaad305f7d276a11b84977d2aa7f6600"},"cell_type":"code","source":"print(nb.predict((X_test.loc[[227]]).values.tolist()),\nnb.predict((X_test.loc[[269]]).values.tolist()),\nnb.predict((X_test.loc[[262]]).values.tolist()),\nnb.predict((X_test.loc[[300]]).values.tolist()),\nnb.predict((X_test.loc[[192]]).values.tolist()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7f7c01c0b3684ca0f900bdba4597640833553f0d"},"cell_type":"code","source":"# ðŸ˜®","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2209530a77ecd1f558f27e9d36fd46d86df8a1b4"},"cell_type":"markdown","source":"# 6 --> PCA"},{"metadata":{"trusted":false,"_uuid":"e04331e030b790bf3e6dd94109e1ef707c17cf5f"},"cell_type":"code","source":"pairs = sns.pairplot(df)\npairs","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a321948970d217660d90a404c155a7601386c097"},"cell_type":"code","source":"pairs.savefig('a')\n# age with chol.\n# trestbps with chol.\n# thalach with chol.","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"26c4463a0a49518aac793764bc54a2ea1e1dead6"},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"44ff08958836d28b6bd7a3c4629869cdde987a79"},"cell_type":"code","source":"df_pca = pca.fit_transform(df)\ny_variance = pca.explained_variance_ratio_\npd.DataFrame(pca.components_, columns=df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e58847b69813c1a2ee9598f4cbae4ed68a0935e9"},"cell_type":"code","source":"# See that 0 has 0.99 of CHOL\n# 1 has -0.97 of THALACH\n# 2 has 0.98 of TRESTBPS\n# recall:\nsns.heatmap(df.corr(), cmap=\"YlGnBu\", annot=True)\n# Possibly others had high values due to multicollinearity.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b61649ef4350a92180432083dd263ea432dcd9c"},"cell_type":"markdown","source":"5. Dataset analysis"},{"metadata":{"trusted":false,"_uuid":"205656adcadbda703df3d7fca5d0ea1e61002353"},"cell_type":"code","source":"sns.barplot(x=[i for i in range(len(y_variance))], y=y_variance)\nplt.title(\"PCA\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e568a234f0972a494492a7f97410d5f3c1b33f46"},"cell_type":"code","source":"X = df.drop('target', axis=1) \npca = PCA()\npca.fit(X)\ndf_pca = pca.transform(X)\nprint(\"original shape:   \", X.shape)\nprint(\"transformed shape:\", df_pca.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cd9aaae15729d5a3736d1b3e0cf07f54d8bd094b"},"cell_type":"code","source":"plt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');\n\n# we can observe the cumulative explained variance.","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"448b21d54d444efe17021a88dee6ba2b64934156"},"cell_type":"code","source":"np.cumsum(pca.explained_variance_ratio_)[2], np.cumsum(pca.explained_variance_ratio_)[3]\n# With 3 components it's more than enough, even with 2 we can explain the 98% of the\n# variability in the data! ðŸ˜®","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5fd307a593067481402550848d1ae3dcb59c3ce5"},"cell_type":"code","source":"sns.barplot(np.arange(0,14),pca.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97602105543348e37ce45456583282280ebff7ec"},"cell_type":"markdown","source":"7. Principal components' analysis"},{"metadata":{"trusted":false,"_uuid":"ec662108e3a5ddc56c703bb1c391223faf1e6c13"},"cell_type":"code","source":"from sklearn.datasets import make_blobs\nfrom sklearn import decomposition","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"43fd6f43ade48f451f0c470ebac6d94f0936147a"},"cell_type":"code","source":"# I decided to take 2 components:\npca = decomposition.PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\ndf_pca = pd.DataFrame(data = X_pca , \n        columns = ['PC1', 'PC2'])\ndf_pca['Cluster'] = Y\ndf_pca.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"15f3a81bf590d99cecd2e17babceb4de892d850b"},"cell_type":"code","source":"pca.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"230f619fdfa823940312505c5db1b2adf8ad8ab7"},"cell_type":"code","source":"pc_df = pd.DataFrame({'var': pca.explained_variance_ratio_,\n             'PC':['PC1','PC2']})\nsns.barplot(x='PC',y=\"var\", \n           data=pc_df, color=\"c\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"298ff8dd32ce920b0882352699c58036c20c3037"},"cell_type":"code","source":"sns.lmplot( x=\"PC1\", y=\"PC2\",\n  data=df_pca, \n  fit_reg=False, \n  hue='Cluster', # color by cluster\n  legend=True,\n  scatter_kws={\"s\": 80}) # specify the point size","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57d4b246ea473bde22680763617c4bbe8604f08d"},"cell_type":"markdown","source":"6. Feature selection"},{"metadata":{"trusted":false,"_uuid":"74b020ff42ffeef339a239373aa03a3e2e17194c"},"cell_type":"code","source":"from sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\n\nlsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, Y)\nmodel = SelectFromModel(lsvc, prefit=True)\nfeature_index = model.get_support()\nfeature_index # what features have an explicit influence","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e1c78b047147861f501a9e29e032e0bd582bb7be"},"cell_type":"code","source":"# We take them \nX.head()\nX_up = X[X.columns[feature_index]]\nX[X.columns[feature_index]].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7b60533a5f069093bf0a8bb0afa0e3b6cf973e07"},"cell_type":"code","source":"# First, let's see if our tuned RFC improves...\ntuned_rfc = RandomForestClassifier(n_estimators = 1000, oob_score = True,\n                                   max_depth = None, max_features = 'sqrt')\nX_train, X_test, Y_train, Y_test = train_test_split(X_up, Y, test_size=0.05) \ntuned_rfc.fit(X_train, Y_train)\ntuned_rfc.score(X_test, Y_test), cross_val_score(tuned_rfc, X_up, Y, cv=10).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bd5729ee769290b564fdaa307eb14af174ade929"},"cell_type":"code","source":"# Seems like not ðŸ˜¢","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"42f12c167c95e2b43b868ef785c01fa5a222e730"},"cell_type":"code","source":"# Let's try our best option SVC: \nX_train, X_test, Y_train, Y_test = train_test_split(X_up, Y, test_size=0.05) \n\nsvm = SVC(probability=True, kernel='linear',gamma='scale')\nsvm.fit(X_train, Y_train)\n\nprint(\"Test Accuracy of SVM Algorithm: {:.4f}%\".format(svm.score(X_test, Y_test)*100))\ncv_result = cross_val_score(svm, X, Y, cv=10)\nprint(cv_result.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3e22e7778b87f15d5f910ab1635a6730a1189527"},"cell_type":"code","source":"# Not better ðŸ˜‘ OK.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6355bb3dc2600d0f77bdb5d54770da8e6b80ff10"},"cell_type":"markdown","source":"10. Clustering"},{"metadata":{"trusted":false,"_uuid":"073174012dd1e2d58929cb59434b1602352fde41"},"cell_type":"code","source":"# First, let's see an example with 3 clusters:\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d5e94ca634a7aa1ba7c756b317905df453144d2c"},"cell_type":"code","source":"kmeans.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fd3913b4b7a2a0e1e626bb4b515a6cedab763fba"},"cell_type":"code","source":"kmeans.inertia_ # that's the cost, the lower - the better.","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a3b30ab147d7936ead4f4373299c008475fd88f1"},"cell_type":"code","source":"df['KMEANS'] = kmeans.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"34dafa61614e2bb4ac987b54ba7e9f186a81e530"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"927f613262b68c1ceb475ca21b01dafbc2d9fd4f"},"cell_type":"code","source":"plt.scatter(x=df_pca['PC1'], y=df_pca['PC2'], c=kmeans.labels_*2 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1f218e3d1697c213001b6df75d00e7c6b0f95bf1"},"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.metrics import pairwise_distances\nkmeans_cost = np.array([])\ncalinskis = np.array([])\nfor k in range(2,20):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(X)\n    kmeans_cost = np.append(kmeans_cost, kmeans.inertia_)\n    calinskis = np.append(calinskis, metrics.calinski_harabaz_score(pc, kmeans.labels_))\nplt.scatter(x=df_pca['PC1'], y=df_pca['PC2'], c=kmeans.labels_*2 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"31a63facccd6262137844174f0bec3fc84b2abdd"},"cell_type":"code","source":"x = np.arange(2,20)\ny = kmeans_cost\nsns.barplot(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ea131addf9d558d093e03b51094a91e7f56e2f42"},"cell_type":"code","source":"plt.scatter(x, calinskis), calinskis","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d8e80e0b3c59c836f657a4c879541f4e1664260f"},"cell_type":"code","source":"# We take the best k: where the cost does not vary much. See it with calinski\n# where do we have the lowest value? 11 groups:\nkmeans = KMeans(n_clusters=11)\nkmeans.fit(X)\nkmeans_cost = np.append(kmeans_cost, kmeans.inertia_)\nplt.scatter(x=df_pca['PC1'], y=df_pca['PC2'], c=kmeans.labels_*2 )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3110d94638116c1a5691f12dda428bd6a4e3d9d"},"cell_type":"markdown","source":"# Neural Network ðŸ•¸ðŸ•·"},{"metadata":{"_uuid":"732e5b6782887a30948f6ccbf528c8fb7b61f8eb"},"cell_type":"markdown","source":"12. Neural network"},{"metadata":{"trusted":false,"_uuid":"4961b634cecd666c34e9166abc262a2eec960680"},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.utils import to_categorical\nimport keras","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c1f3dab8ccd0d02f0899c24b154c48566aa4bb5e"},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.10)\n\naccuracies = []\nlosses = []\nfor i in range(0, 5):\n    model = Sequential()\n    model.add(Dense(5, input_dim=X_train.shape[1], activation='relu'))    \n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    # x_train and y_train are Numpy arrays --just like in the Scikit-Learn API.\n    model.fit(X_train, Y_train, epochs=400, batch_size=32, verbose=1)\n\n    loss, acc = model.evaluate(X_test, Y_test, batch_size=32)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b8ae14b4503a46e51e896eeca0a5742a5da0b2e9"},"cell_type":"code","source":"accuracies = []\nlosses = []\n# It is better for internal nodes to have linear and rectified activations.\n# In binary classification, the last level has to have just ONE node.\n# And it's better to have there a sigmoid-like activation to distinguish between the\n# 2 states (0 and 1).\ninternal = ['relu', 'tanh', 'linear']\nlast = ['sigmoid', 'tanh', 'softsign']\noptimizer = ['RMSprop', 'SGD', 'adam']\n\nfor internal_activation_func in range(len(internal)):\n    for last_activation_func in range(len(last)):\n        for j in range(0,5):\n            print(\"INTERNAL: \" + internal[internal_activation_func])\n            print(\"LAST: \" + last[last_activation_func])\n            \n            model = Sequential()\n            model.add(Dense(5, input_dim=X_train.shape[1], activation=internal[internal_activation_func]))    \n            model.add(Dense(1, activation=last[last_activation_func]))\n            model.compile(loss= 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n\n            # x_train and y_train are Numpy arrays --just like in the Scikit-Learn API.\n            model.fit(X_train, Y_train, epochs=400, batch_size=32, verbose=1)\n\n            loss, acc = model.evaluate(X_test, Y_test, batch_size=32)\n\n            accuracies.append(acc)\n            losses.append(loss)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c2a000648f0714889c20d0ed10f0b6937f56ae6c"},"cell_type":"code","source":"# After 20 minutes... ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8f726121c421e8a57d68dcbc96b0c2e7fdc13743"},"cell_type":"code","source":"print(\"NEURAL NETWORK 1:\")\nprint(\"\\tINTERNAL: \" + internal[0])\nprint(\"\\tLAST: \" + last[0])\nprint(\"\\tAccuracy: \" + str(np.mean(accuracies[0:5])))\n\nprint(\"NEURAL NETWORK 2:\")\nprint(\"\\tINTERNAL: \" + internal[0])\nprint(\"\\tLAST: \" + last[1])\nprint(\"\\tAccuracy: \" + str(np.mean(accuracies[5:10])))\n\nprint(\"NEURAL NETWORK 3:\")\nprint(\"\\tINTERNAL: \" + internal[0])\nprint(\"\\tLAST: \" + last[2])\nprint(\"\\tAccuracy: \" + str(np.mean(accuracies[10:15])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"47100c8dcd4d993bbfba0865cd4576b08808ca91"},"cell_type":"code","source":"print(\"NEURAL NETWORK 4:\")\nprint(\"\\tINTERNAL: \" + internal[1])\nprint(\"\\tLAST: \" + last[0])\nprint(\"\\tAccuracy: \" + str(np.mean(accuracies[15:20])))\n\nprint(\"NEURAL NETWORK 5:\")\nprint(\"\\tINTERNAL: \" + internal[1])\nprint(\"\\tLAST: \" + last[1])\nprint(\"\\tAccuracy: \" + str(np.mean(accuracies[20:25])))\n\nprint(\"NEURAL NETWORK 6:\")\nprint(\"\\tINTERNAL: \" + internal[1])\nprint(\"\\tLAST: \" + last[2])\nprint(\"\\tAccuracy: \" + str(np.mean(accuracies[25:30])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"99b0b6fdbb0f77237b5c7e9aa386211646cfc74b"},"cell_type":"code","source":"print(\"NEURAL NETWORK 7:\")\nprint(\"\\tINTERNAL: \" + internal[2])\nprint(\"\\tLAST: \" + last[0])\nprint(\"\\tAccuracy: \" + str(np.mean(accuracies[20:25])))\n\nprint(\"NEURAL NETWORK 8:\")\nprint(\"\\tINTERNAL: \" + internal[2])\nprint(\"\\tLAST: \" + last[1])\nprint(\"\\tAccuracy: \" + str(np.mean(accuracies[25:30])))\n\nprint(\"NEURAL NETWORK 9:\")\nprint(\"\\tINTERNAL: \" + internal[2])\nprint(\"\\tLAST: \" + last[2])\nprint(\"\\tAccuracy: \" + str(np.mean(accuracies[35:40])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c9171dc430fedc15425f1395713c062749dba40c"},"cell_type":"code","source":"# Horrible neural networks, bye ðŸ¤¯","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e4a9d575fe49127e64c5482af5acd7930410525d"},"cell_type":"code","source":"# Let's do a single neural network with several layers and with act functions\n# which have given the best results in the previous attempt, which are TANH and SIGMOID:","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1cb4c0504a65693b7e2ed7ba07a14bfd0c1d3535"},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.05)\naccuracies = []\nlosses = []\nfor j in range(0,5):\n    model = Sequential()\n    model.add(Dense(5, input_dim=X_train.shape[1], activation='tanh'))  \n    model.add(Dense(5, activation='tanh'))    \n    model.add(Dense(5, activation='tanh'))    \n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss= 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n\n    # x_train and y_train are Numpy arrays --just like in the Scikit-Learn API.\n    model.fit(X_train, Y_train, epochs=400, batch_size=32, verbose=1)\n\n    loss, acc = model.evaluate(X_test, Y_test, batch_size=32)\n\n    accuracies.append(acc)\n    losses.append(loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"56b90e658ec22de04e92133715b3160badbd1f0e"},"cell_type":"code","source":"np.mean(accuracies) # well...","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"12cd45889b35d040939c45e76926099f158e0804"},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.05)\naccuracies = []\nlosses = []\nfor j in range(0,5):\n    model = Sequential()\n    model.add(Dense(10, input_dim=X_train.shape[1], activation='tanh'))  \n    model.add(Dense(2, activation='sigmoid'))     \n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss= 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n\n    # x_train and y_train are Numpy arrays --just like in the Scikit-Learn API.\n    model.fit(X_train, Y_train, epochs=400, batch_size=32, verbose=1)\n\n    loss, acc = model.evaluate(X_test, Y_test, batch_size=32)\n\n    accuracies.append(acc)\n    losses.append(loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"03f79e4de81faeee0cb2bc631f6585543428d733"},"cell_type":"code","source":"np.mean(accuracies) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c4ba2647a32f76193196a046eeb8c9d550aa07fb"},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4b835a41f1626e9913f7b40c2b44eebd2af79b69"},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.05)\naccuracies = []\nlosses = []\nfor j in range(0,5):\n    model = Sequential()\n    model.add(Dense(1, input_dim=X_train.shape[1], activation='sigmoid'))  \n    model.compile(loss= 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n\n    # x_train and y_train are Numpy arrays --just like in the Scikit-Learn API.\n    model.fit(X_train, Y_train, epochs=400, batch_size=32, verbose=1)\n\n    loss, acc = model.evaluate(X_test, Y_test, batch_size=32)\n\n    accuracies.append(acc)\n    losses.append(loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e652e2f217d222df4df209f2681a3d6dbcbeaaf7"},"cell_type":"code","source":"# approx 0.5x","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8f9569a255ea268e61e129d2dbe6ec03436fb5d7"},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.05)\naccuracies = []\nlosses = []\nfor j in range(0,5):\n    model = Sequential()\n    model.add(Dense(1, input_dim=X_train.shape[1], activation='sigmoid'))  \n    model.compile(loss= 'binary_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n    \n\n    # x_train and y_train are Numpy arrays --just like in the Scikit-Learn API.\n    model.fit(X_train, Y_train, epochs=400, batch_size=32, verbose=1)\n\n    loss, acc = model.evaluate(X_test, Y_test, batch_size=32)\n\n    accuracies.append(acc)\n    losses.append(loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"250dcca5240b2e8daaef71a667f7d8dd5dc7b738"},"cell_type":"code","source":"np.mean(accuracies) #pff","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d23f12888bce50c689fb9308096262b402669d2e"},"cell_type":"code","source":"# that's all","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}