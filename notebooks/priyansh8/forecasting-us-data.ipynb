{"cells":[{"metadata":{"id":"p7JmaPsfpIex"},"cell_type":"markdown","source":"# **An analysis and forecasting  of the air pollution levels in India**\n\nIn this project we will take a look at 24 Indian cities air pollution levels over the years as well as forecast the air pollution levels for the next 1 year at the current rate of pollution for the entire country. We will also try to explain the trends, seasonality etc. from the data given.\nWe will be using AQI - Air quality Index, as our measure for the air pollution levels.\n\nThe data has been made publicly available by the Central Pollution Control Board: https://cpcb.nic.in/ which is the official portal of Government of India. They also have a real-time monitoring app: https://app.cpcbccr.com/AQI_India/ .\n\n\n\nThere will be two main parts to the project:\n\n1. To compare the various states on the level of pollution for the year 2019.\n\n2. To find trends, seasonality etc for the pollution levels of India as a whole as well as Delhi and forecast it to the future."},{"metadata":{"id":"qIfWSfuVpIdn"},"cell_type":"markdown","source":"## A brief introduction to the calculation of AQI\n\n<img style=\"float: center;\" src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcR8MwkjROMGpNIVRTeKgO_vIj2QU-J9MAIW8v6wf6yg6mWvPPWH&usqp=CAU.jpg\">\n\n1. The AQI calculation uses 7 measures: PM2.5(Particulate Matter 2.5-micrometer), PM10, SO2, NOx, NH3, CO and O3(ozone).\n\n2. For PM2.5, PM10, SO2, NOx and NH3 the average value in last 24-hrs is used with the condition of having at least 16 values.\n\n3. For CO and O3 the maximum value in last 8-hrs is used.\n\n4. Each measure is converted into a Sub-Index based on pre-defined groups.\n\n5. Sometimes measures are not available due to lack of measuring or lack of required data points.\n\n6. Final AQI is the maximum Sub-Index with the condition that at least one of PM2 and PM10 should be available and at least three out of the seven should be available.\n\n### How is AQI calculated?\n1. The Sub-indices for individual pollutants at a monitoring location are calculated using its\n24-hourly average concentration value (8-hourly in case of CO and O3) and health\nbreakpoint concentration range. The worst sub-index is the AQI for that location.\n2. All the eight pollutants may not be monitored at all the locations. Overall AQI is\ncalculated only if data are available for minimum three pollutants out of which one should\nnecessarily be either PM2.5 or PM10. Else, data are considered insufficient for calculating\nAQI. Similarly, a minimum of 16 hours’ data is considered necessary for calculating subindex.\n3. The sub-indices for monitored pollutants are calculated and disseminated, even if data are\ninadequate for determining AQI. The Individual pollutant-wise sub-index will provide air\nquality status for that pollutant.\n4. The web-based system is designed to provide AQI on real time basis. It is an automated\nsystem that captures data from continuous monitoring stations without human\nintervention, and displays AQI based on running average values (e.g. AQI at 6am on a\nday will incorporate data from 6am on previous day to the current day).\n5. For manual monitoring stations, an AQI calculator is developed wherein data can be fed\nmanually to get AQI value. \n\nLet us take a look at the ranges of AQI.\n\n<img src=\"https://i.imgur.com/XmnE0rT.png\" alt=\"\">\n\nNow we can proceed with our analysis.\n\n## Downloading the dataset and importing libraries to conduct analysis:\n"},{"metadata":{"id":"K5Nn_Be46iNb","trusted":true},"cell_type":"code","source":"# Importing necessary libraries to conduct our analysis\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n# Ignore harmless warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom IPython.display import HTML,display\n\nwarnings.filterwarnings(\"ignore\")\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"id":"MG8hBBUr6WAY","trusted":true},"cell_type":"code","source":"#Reading the dataset into object 'df' using pandas:\nimport datetime\ndf= pd.read_csv('../input/final-1/19-20_pred.csv',parse_dates=True)\n\nfor i in range(19542):\n    df['Date'][i] = datetime.datetime.strptime(df['Date'][i], \"%d-%m-%Y\").strftime(\"%Y-%m-%d\")\n\ndf['Date'] = pd.to_datetime(df['Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df.drop(df[df['City'] =='Portland'].index, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"id":"lj4vUaqXv4r-"},"cell_type":"markdown","source":"## Exploratory data analysis(EDA),Data Wrangling and Pre-processing:\nFirst, let us take a look at the first five rows of our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"-a7N0Kwb9p75","outputId":"92ad59e5-d9ca-4254-cdfc-f5f74230158a","trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"id":"DyJ9g-etU8l4"},"cell_type":"markdown","source":"Right off we can notice there are many missing values which can lead to incorrect predictions and inference.\n\nTaking a deeper look we notice that only the Delhi dataset is complete for the AQI column with the rest of the cities with incomplete data. This is unfortunately unrectifiable as official records of pollutant levels are only available as given above leaving a large amount of data missing.\n\nNext let us take a look at a summary of all the data:"},{"metadata":{"id":"20GbaS3h66iM","outputId":"1289a8c4-4213-4d33-8b99-d64e304abe01","trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"R2O8wMZh-HgR"},"cell_type":"markdown","source":"Above is a summary statistics of all the columns. The AQI as explained above is based on these columns and for our notebook we will only deal with AQI values across the states.\n\n\n\n\n#### Removing unused columns:\nHere we will keep the columns 'City', 'Date', 'AQI' and 'AQI_Bucket'.\n\n"},{"metadata":{"cellView":"both","id":"k6bT42657VFu","trusted":true},"cell_type":"code","source":"# df=df[['City','Date','PM2.5','O3', 'CO']]","execution_count":null,"outputs":[]},{"metadata":{"id":"23vOTXBSWkrs"},"cell_type":"markdown","source":"#### Modifying dataset for our needs:\nHere,we will tranform the data with the columns as the cities AQI so as to compare AQI between states. The table after transforming is given below."},{"metadata":{"id":"iAfWTeTRFWrg","trusted":true},"cell_type":"code","source":"cities=pd.unique(df['City'])\ncolumn1= cities+'_PM2.5'\n# column2=cities+'_O3'\n# columns=[*column1,*column2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(column1)","execution_count":null,"outputs":[]},{"metadata":{"id":"k7-ijVa671PE","trusted":true},"cell_type":"code","source":"final_df=pd.DataFrame(index=np.arange('2018-12-30','2020-05-30',dtype='datetime64[D]'),columns=column1)\n\nprint(final_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arr=dict()\nfor i in range(len(cities)):\n    arr[cities[i]] = 0\n    \n\nfor i in range(len(cities)):\n    for j in range(19542):\n        if(cities[i]==df['City'][j]):\n            arr[cities[i]]+=1\n            \n            \nprint(arr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for city,i in zip(cities,final_df.columns):\n    n=len(np.array(df[df['City']==city]['PM2.5']))\n#     print(n)\n    final_df[i][-n:]=np.array(df[df['City']==city]['PM2.5'])","execution_count":null,"outputs":[]},{"metadata":{"id":"9QoLsb8jkQqy"},"cell_type":"markdown","source":"Notice that the data is daily data. We will convert it into monthly data for our ease by averaging a months data."},{"metadata":{"id":"07V5y4PMtI7e","trusted":true},"cell_type":"code","source":"final_df=final_df.astype('float64')\nfinal_df=final_df.resample(rule='MS').mean()","execution_count":null,"outputs":[]},{"metadata":{"id":"jxkmi2ewBJHz","outputId":"b5f44220-9dc5-4e6c-d512-1c23bf6d0791","trusted":true},"cell_type":"code","source":"final_df.tail()\n# print(final_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"SXWhO6-1SNu5"},"cell_type":"markdown","source":"Next, we will add a column 'India_AQI' which gives us the average of all the cities data across a row. Note that this is not necessarily an accurate measure of AQI for India as a whole as only a small subset of all the cities are being used. Nevertheless,we can consider this as a reasonably representative measure of the AQI."},{"metadata":{"id":"4UXpyHXaSR-8","trusted":true},"cell_type":"code","source":"final_df['US_PM2.5']=final_df.mean(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"VuZQdl-KbjsG"},"cell_type":"markdown","source":"Let us take a quick look at the graph of India's AQI over the years."},{"metadata":{"id":"bA8mRJIuS1Qd","outputId":"57889e1d-2d79-433b-b47c-1256ebb8f931","trusted":true},"cell_type":"code","source":"ax=final_df[['US_PM2.5']].plot(figsize=(12,8),grid=True,lw=2,color='Red')\nax.autoscale(enable=True, axis='both', tight=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"10a7GlfCcSoW"},"cell_type":"markdown","source":"Straight away we can see patterns and trends over the years. There are two highly noticeable patterns. One is the general trend downwards. Over the past 5 years we can see the AQI reducing marginally. Note that this can be  a litte misleading, especially due to the 2015 data, as the dataset in the first few observations  only comprises of Delhi and Ahmedabad during which have relatively highly pollution compared to the rest of the cities which makes the initial portion of the graph highly exaggerated. Nevertheless we can see a general decline in pollution over the years. \n\nThe next pattern thats easily observable is the seasonal component which plays a big role in the pollution of the country. We will discuss further  in the 2nd part of our project.\nOne other important point to note is the affect of COVID-19 on India's pollution level. The pollution levels are drastically lower during the year 2020 for the same reason.\n\nWe can move on to comparing the AQI of the cities to find the most polluted city and the least.\nNote that we will be leaving the unavailable data as is and further modify if required."},{"metadata":{"id":"8IurUJjPgiqp"},"cell_type":"markdown","source":"## Air pollution by city for the year 2019\nOur aim of the section is to find level of pollution in the cities and compare them, we use the year 2019 as it is by far the most complete in terms of data and it is the most recent full year and hence rather apt to compare.\n\nWe will start with forming a table with the data from 2019."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_2019=final_df['2019-01-01':'2020-01-01']\n# print(df_2019.head())\n# print(df_2019.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"XSHOoNUsgibU","outputId":"e665eb1f-975c-43db-b06a-c18ebfbb8331","trusted":true},"cell_type":"code","source":"# df_2019=final_df\n# df_2019.head()\ndf_2019=df_2019.drop(['Brooklyn_PM2.5','Charlotte_PM2.5','Columbus_PM2.5','Detroit_PM2.5','Honolulu_PM2.5','Richmond_PM2.5','San Diego_PM2.5','Tallahassee_PM2.5','The Bronx_PM2.5'],axis=1)\n# for col in df_2019.columns:\n#     df_2019[col].fillna((df_2019[col].mean()), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"uEfwdWGZjZbH"},"cell_type":"markdown","source":"We can see that there seems to be still quite a few missing values. Let us take a look at the missing data."},{"metadata":{"id":"ceyWCtKkBKDh","outputId":"83a7cc71-a201-429a-a2dc-0482e2632c47","trusted":true},"cell_type":"code","source":"df_2019.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"4PatP3AcjjBt"},"cell_type":"markdown","source":"We can see that there are 3 cities whose data is missing in its entirety. We will remove these columns as they serve no purpose. There are few other columns with a few missing months of data. For our analysis we will keep them even though it might add to the inacuracy of our results."},{"metadata":{"id":"i1YWEKmMjlou","trusted":true},"cell_type":"code","source":"\n\n# print(df_2019.head)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"y0BFlBd2kTa8"},"cell_type":"markdown","source":"We will take the average of all the months for each city to find the AQI for the year 2019."},{"metadata":{"id":"O7bkbxf5jvc4","trusted":true},"cell_type":"code","source":"AQI_2019=df_2019.mean(axis=0)\nAQI_2019.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"zoM6e0m-KjqC"},"cell_type":"markdown","source":"Before looking at the means of the AQI values of the cities, we will take a look at the boxplots of the AQI values of the various cities."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,8))\nplt.xticks(rotation=90)\nbplot = sns.boxplot( data=df_2019,  width=0.75,palette=\"GnBu_d\")\nplt.ylabel('PM2.5');\nbplot.grid(True)","execution_count":null,"outputs":[]},{"metadata":{"id":"6_nqpgyIKqHp"},"cell_type":"markdown","source":"We can see that Ahmedebad has easily the highest values of AQI in the country, followed by Delhi lagging far behind. Let us take a look at the means of the values of AQI for further comparison."},{"metadata":{"id":"8L1pkcgzK_Gs","outputId":"c677ff5a-78e9-41b1-f10b-090ce05a3c10","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,8))\nplt.xticks(rotation=90)\nplt.ylabel('PM2.5')\nbplot=sns.barplot(AQI_2019.index, AQI_2019.values,palette=\"GnBu_d\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"6LBKfD-QQw8u"},"cell_type":"markdown","source":"We can see that Ahmedabad and Delhi are the most polluted whereas Shillong is the least followed by trivandrum. With this we end the comparison and move to the next section of forecasting the values of future AQI for the whole of India.\n\n## Analysing and forecasting of AQI values:\nWe will first take a look at the seasonal decompose of the AQI values of india."},{"metadata":{"id":"2kFq-3bbLduh","outputId":"fc173abd-0ddf-412a-897d-04259bb33fd9","trusted":true},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\nIndia_AQI=final_df['US_PM2.5']\n\nprint(India_AQI)\n# result=seasonal_decompose(India_AQI,model='multiplicative')\n# result.plot();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(India_AQI)","execution_count":null,"outputs":[]},{"metadata":{"id":"e56qziDcxsgz"},"cell_type":"markdown","source":"As we have discussed earlier ,there is a very clear seasonality, and  a less clearer trend. The trend is possibly  due to increasing restrictions on pollution by the govt and the last surge downward is clearly due to the recent Covid-19. \n\nHow about the seasonality, what causes the increase during certian months and a decline in others? Let us take a closer look during which months the pollution peaks."},{"metadata":{"id":"5nHsm7k7N-wa","outputId":"be7a632a-90a4-49cb-a5e2-24e190fac475","trusted":true},"cell_type":"code","source":"# from matplotlib import dates\n# ax=result.seasonal.plot(xlim=['2018-12-30','2020-05-15'],figsize=(20,8),lw=2)\n# ax.yaxis.grid(True)\n# ax.xaxis.grid(True)","execution_count":null,"outputs":[]},{"metadata":{"id":"BvnK8FFj9K8p"},"cell_type":"markdown","source":"We can see that there are two peaks largely, one during October and the during January. And the lowest amount of pollution is around july-September after which there is a sharp increase. \nSimilarly, there is a decrease from January to July. This spike in the winters is due to a combination of factors. One point of note is that North Indian states have a higher increase of pollution.\n\nThe spike is due to factors including Winter aversion(explained after),valley affect(explained after), seasonal factors such as dust storms,  crop fires, burning of solid fuels for heating, and firecracker-related pollution during Diwali, stubble burning etc.\n\n\n### Winter Aversion:\nIn summer, air in the planetary boundary layer (the lowest part of the atmosphere) is warmer and lighter, and rises upwards more easily. This carries pollutants away from the ground and mixes them with cleaner air in the upper layers of the atmosphere in a process called ‘vertical mixing’.  \n\nDuring winters the planetary boundary layer is thinner as the cooler air near the earth’s surface is dense. The cooler air is trapped under the warm air above that forms a kind of atmospheric ‘lid’. This phenomenon is called winter inversion. Since the vertical mixing of air happens only within this layer, the pollutants released lack enough space to disperse in the atmosphere.\nDuring summers, pollution levels decrease as the warmer air rises up freely, making the boundary layer thicker, and providing enough space for pollutants to disperse. The same thing happens during winter afternoons, when increased heat brings down pollution slightly.\n\nThe effects of inversion are stronger at night, which is why air quality levels drop overnight. This is also why experts ask people to refrain from early morning walks, as they could be exposed to much higher pollution levels at that time.\nIn cities closer to the coast, like Mumbai, the sea breeze and moisture help disperse pollution. However, the Indo-Gangetic plain, which includes Punjab, Delhi, UP, Bihar and West Bengal, is like a valley surrounded by the Himalayas and other mountain ranges. Polluted air settles in this land-locked valley and is unable to escape due to low wind speeds.\nIn major cities of this region, such as Delhi and Kanpur, high industrial and vehicular emissions coupled with biomass burning in surrounding areas cause more pollution that gets trapped due to this valley effect and inversion.\n\n Now that we have an explantion for the seasonal component as well as trend component let us try to predict future values of AQI based purely on previous values.\n\n"},{"metadata":{"id":"8DRWvFF9XdyK"},"cell_type":"markdown","source":"# Forecasting: \nWe will be using three methods for forecasting values of AQI for India, namely, RNN using LSTM\nIt is obviously overkill to be using these three methods however being new to time series I would personally like to explore all three options. Normally for such a small dataset RNN would not be recommended."},{"metadata":{"id":"oxOCTWfWYE92"},"cell_type":"markdown","source":"## SARIMA(Seasonal Autoregressive Integrated Moving Average)\nAutoregressive Integrated Moving Average, or ARIMA, is one of the most widely used forecasting methods for univariate time series data forecasting.Although the method can handle data with a trend, it does not support time series with a seasonal component.An extension to ARIMA that supports the direct modeling of the seasonal component of the series is called SARIMA."},{"metadata":{"id":"6MriRRMaOEK9","outputId":"4dea1242-5c1b-4b94-ad91-7e922ff45ef2","trusted":true},"cell_type":"code","source":"# Load specific forecasting tools\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n!pip install pmdarima;\nfrom pmdarima import auto_arima;                              # for determining ARIMA orders","execution_count":null,"outputs":[]},{"metadata":{"id":"6JkMQsIQZ3a6"},"cell_type":"markdown","source":"First, we run auto arima to find out the parameters of the model for us. We can manually do it,however, it is much easier for us let the notebook do the work for us."},{"metadata":{"id":"aM0pMKOLZfhS","outputId":"b97a3c43-024d-4d03-ef5e-1e4f16dfe80d","trusted":true},"cell_type":"code","source":"auto_arima(y=India_AQI,start_p=0,start_P=0,start_q=0,start_Q=0,seasonal=False, m=12).summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"-z7Jjk91ar3F"},"cell_type":"markdown","source":"We have found the optimal parameters for the SARIMAX model is (1,1,1)x(1,0,1,12).Note that the model is called SARIMAX, however, we do not have an external variable hence it reduces to SARIMA. The model selection criterion is AIC which is default.\n\nOur next step is to forecast using this model into the future. However, since we do not have information regarding future values, we will split the data into a training data and testing data and try to predict 1 year into the future. We will use the years 2015-2018(till june) as our train dataset and July-June the next year as our test dataset. The reason we exclude 2020 is due to the fact that 2020 is an outlier due to covid an we will not get an accurate figure for the prediction. We will also take a  look at the predicted values of 2020 for reference. Further, we will predict into the year 2021."},{"metadata":{"id":"nUyUe0yZaUF-","outputId":"03c5d84b-4085-47ed-d921-1002cd29a126","trusted":true},"cell_type":"code","source":"# len(India_AQI)","execution_count":null,"outputs":[]},{"metadata":{"id":"uxdzC4GCcrQt","trusted":true},"cell_type":"code","source":"#dividing into train and test:\n# train=India_AQI[:41]\n# test=India_AQI[42:54]","execution_count":null,"outputs":[]},{"metadata":{"id":"P3_UZaR2dBcq","outputId":"783d9b7a-c8cb-4f16-a465-66b90fba0b79","trusted":true},"cell_type":"code","source":"# Forming the model:\nmodel=SARIMAX(train,order=(1,1,1),seasonal_order=(1,0,1,12),)\nresults=model.fit()\nresults.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Fj81j6pzdjav"},"cell_type":"markdown","source":"We have fitted out model with the training data and the required parameters. Next we need to forecast the next 12 months AQI values."},{"metadata":{"id":"wZH2zXDedhX-","trusted":true},"cell_type":"code","source":"#Obtaining predicted values:\npredictions = results.predict(start=42, end=53, typ='levels')","execution_count":null,"outputs":[]},{"metadata":{"id":"IOpzaoxFeV31","outputId":"edc241ce-7257-4a45-d564-36370ef528f1","trusted":true},"cell_type":"code","source":"#Plotting predicted values against the true values:\npredictions.plot(legend=True)\ntest.plot(legend=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"8uyid8Xfoo8_"},"cell_type":"markdown","source":"We can see that the predicted values are fairly close to our actual values using SARIMA and hence is quite fascinating how looking at previous values gives us so much insight into future air pollution.However, there is a discrepency at the peak of the graph where our model has not been able to predict with a high accuracy. To obtain the value of error we will be using root mean square error(RMSE) for comparison between the models."},{"metadata":{"id":"YBSXhsLzeeAv","outputId":"56b3fea8-159e-4356-d4f8-e1cafa688fd3","trusted":true},"cell_type":"code","source":"# from sklearn.metrics import mean_squared_error\n# RMSE=np.sqrt(mean_squared_error(predictions,test))\n# print('RMSE = ',RMSE)\n# print('Mean AQI',test.mean())","execution_count":null,"outputs":[]},{"metadata":{"id":"0pjnU7klp8xL"},"cell_type":"markdown","source":"We have got an RMSE value of approximately 21, which is quite alright, we can approximately judge the scale of error by comparing with the mean values of AQI which is 177, so the error is approximately 1/9 of the actual values. \n\nNext we will try predicting the AQI values for the year 2019-2020(July-May)"},{"metadata":{"id":"22DRBZA4fkJM","outputId":"ba0d35a5-0197-4451-8ae8-075b9a5b46bc","trusted":true},"cell_type":"code","source":"#dividing into train and test:\n# train=India_AQI[:53]\n# test=India_AQI[54:]\n# # Forming the model:\n# model=SARIMAX(train,order=(1,1,1),seasonal_order=(1,0,1,12),)\n# results=model.fit()\n# results.summary()\n# #Obtaining predicted values:\n# predictions = results.predict(start=54, end=64, typ='levels').rename('Predictions')\n# #Plotting predicted values against the true values:\n# predictions.plot(legend=True)\n# test.plot(legend=True);","execution_count":null,"outputs":[]},{"metadata":{"id":"JK7kBTnvrtW2"},"cell_type":"markdown","source":"As expected, the predicted values are much higher than the actual value as we can see from the graphs. Let us take a look at the error value."},{"metadata":{"id":"oH13wmiorZSs","outputId":"3ab3aec0-1847-4f7a-cff1-5872188c7b2e","trusted":true},"cell_type":"code","source":"#Finding RMSE:\n# from sklearn.metrics import mean_squared_error\n# RMSE=np.sqrt(mean_squared_error(predictions,test))\n# print('RMSE = ',RMSE)\n# print('Mean AQI',test.mean())","execution_count":null,"outputs":[]},{"metadata":{"id":"btrmSkCn7J9S"},"cell_type":"markdown","source":"The error value is much higher than earlier for obvious reason and hence we can see that predicting for the year 2020 is not going to yeild accurate results due to the Covid-19.\n\nNext we will take a look into forecasting into the unknown, i.e. 2020-2021.\nThis poses a problem, as if we predict including 2020 data, we are bound to get an innacurate prediction for next year simply due to the fact that 2020 is an outlier.However, if we remove 2020 from our dataset and predict from 2019 till 2021 we are left with wrong predictions for sure and considering that covid-19 could have further lasting effects we will predict poorly. \nWe will choose to include 2020 as well for this predicition. We could compare the values next year.\n\n### Predicting into the future:"},{"metadata":{"id":"FC_SzZlErnOr","outputId":"9a0fb283-c8b7-41b4-eb7b-be2a396855cb","trusted":true},"cell_type":"code","source":"# Forming the model:\n# model=SARIMAX(India_AQI,order=(1,1,1),seasonal_order=(1,0,1,12))\n# results=model.fit()\n# results.summary()\n# #Obtaining predicted values:\n# predictions = results.predict(start=64, end=77, typ='levels').rename('Predictions')\n# #Plotting predicted values against the true values:\n# predictions.plot(legend=True)\n# India_AQI.plot(legend=True,figsize=(12,8),grid=True);","execution_count":null,"outputs":[]},{"metadata":{"id":"Kd9Y_Kl391Dz"},"cell_type":"markdown","source":"We can see the predictions plotted in continuation with 2020 and one thing we note is the highly optimistic prediction. That is purely due to the fact that 2020 is such an outlier, chances are, the pollution levels will follow the trend pre 2020 which would mean a bump in the AQI levels unless the country decides to keep the restrictions etc as is which is highly unlikely. We can always get a more accurate prediction skipping 2020."},{"metadata":{"id":"_yOlI97xNM8m"},"cell_type":"markdown","source":"Next we will take a look at root mean square error:"},{"metadata":{"id":"ED0iV5ZdU4LV"},"cell_type":"markdown","source":"##  Recurring Neural Networks(RNN):\nFor this last forecast we will be using RNN which is a type of Neural Network that is used for time based/frequency based/ memory based data like text data, speech, time series etc. We will be using a particular cell type LSTM(Long short term memory).LSTM networks are particularly meant to keep particular information for a longer term as compared to regular RNN's. As all Neural Networks, RNN's works best with a huge amount of data. RNN is a black box method, which means there is little transparency in the model and how it trains. \nAnother major disadvantage is the high complexity of hyperparameters.Hence RNN's should preferably used as last resort."},{"metadata":{"trusted":true},"cell_type":"code","source":"India_AQI = India_AQI.to_frame()","execution_count":null,"outputs":[]},{"metadata":{"id":"ET1aiz3VU6e6","trusted":true},"cell_type":"code","source":"# India_AQI=India_AQI.set_index('ds')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"India_AQI.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"RPyzdD9vZfiN"},"cell_type":"markdown","source":"#### Splitting into test/train:"},{"metadata":{"trusted":true},"cell_type":"code","source":"India_AQI.reset_index()\n# India_AQI.columns = ['Date','US_PM2.5']\n# print(type(India_AQI))","execution_count":null,"outputs":[]},{"metadata":{"id":"n5x8ksaTZYDV","trusted":true},"cell_type":"code","source":"train=India_AQI[:-5]\ntest=India_AQI[-5:]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train)\nprint(test)\nprint(type(train))\ndf = train\n# train.reshape(-1,1)\n# test.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"id":"Lf2ytr9WxUxx"},"cell_type":"markdown","source":"#### Scaling the data: \nFor this model we will be scaling the data to 0-1."},{"metadata":{"id":"3u1aULavU6NZ","outputId":"708fd479-602a-4e5b-9a47-a615ad218b96","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(train)","execution_count":null,"outputs":[]},{"metadata":{"id":"4SszZ9rJg7ba","trusted":true},"cell_type":"code","source":"scaled_train = scaler.transform(train)\nscaled_test = scaler.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"id":"uwkBwsjPyYh7"},"cell_type":"markdown","source":"We need to put the data in a particular format for Keras, the library used to implement RNN.n_input tells us how many values before the output value we need to consider to make a prediction. I have chosen 2 years. One year is also a reasonable value. However, since I want to predict into the future, I want the year before COVID-19  to be in my calculation too(Note that I can do this for my SARIMA as well).n_features is simply the number of values I want to predict.\n\n### Formatting the data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"SbDzkrwkg7Q_","trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import TimeseriesGenerator\nn_input = 10\nn_features = 1\ngenerator = TimeseriesGenerator(scaled_train, scaled_train, length=n_input, batch_size=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"7gvCMuSXg7Il","trusted":true},"cell_type":"code","source":"#To give an idea of what generator file holds:\nX,y = generator[0]","execution_count":null,"outputs":[]},{"metadata":{"id":"N5MMn27iideB","outputId":"de67a73c-d7c9-424a-ca8f-d4888ed91114","trusted":true},"cell_type":"code","source":"# We can see that the x array gives the list of values that we are going to predict y of:\nprint(f'Given the Array: \\n{X.flatten()}')\nprint(f'Predict this y: \\n {y}')","execution_count":null,"outputs":[]},{"metadata":{"id":"IbwVDDgrzWKA"},"cell_type":"markdown","source":"### Creating the model:"},{"metadata":{"id":"PcPOvcX-jLaw","trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.layers import LSTM\n# from keras","execution_count":null,"outputs":[]},{"metadata":{"id":"rKf_KpGIjRhR","outputId":"f953635f-cce2-4c3e-bab6-c7b236cad081","trusted":true},"cell_type":"code","source":"# defining the model(note that  I am using a very basic model here, a 2 layer model only):\nmodel = Sequential()\nmodel.add(LSTM(50,activation='relu', input_shape=(n_input, n_features)))\n# model.add(LSTM(50,return_sequences = True, activation='relu'))\n# model.add(LSTM(32, activation='relu'))\n# model.add(Dense(1))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse',metrics=['acc'])\n\nmodel.summary()\n\n# model = Sequential()\n# model.add(LSTM(200, activation='relu', input_shape=(n_input, n_features)))\n# model.add(Dropout(0.15))\n# model.add(Dense(1))\n# optimizer = keras.optimizers.Adam(learning_rate=0.001)\n# model.compile(optimizer=optimizer, loss='mse', metrics = ['acc'])\n# history = model.fit_generator(generator,epochs=100,verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"-7_bC0jRzdkE"},"cell_type":"markdown","source":"### Fitting the model:\nWe can define the number of epochs we want."},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')","execution_count":null,"outputs":[]},{"metadata":{"id":"MK32h4ULl8cP","outputId":"aa4b2af4-08f4-4b56-c8b7-117aab44e5d8","trusted":true},"cell_type":"code","source":"# Fitting the model with the generator object:\nmodel.fit_generator(generator,epochs=20,verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"id":"jdTT2sY9zz6M"},"cell_type":"markdown","source":"The plot below shows how the values of the loss reduces as each epoch gets over."},{"metadata":{"id":"D2uN1VEYl8NB","outputId":"2f2e66ad-64a8-4fe1-f2b7-12d945103857","trusted":true},"cell_type":"code","source":"loss_per_epoch = model.history.history['loss']\nplt.plot(range(len(loss_per_epoch)),loss_per_epoch)","execution_count":null,"outputs":[]},{"metadata":{"id":"ghI0XLY3z7Fe"},"cell_type":"markdown","source":"### Forming our predictions and putting them in the array test_predictions:"},{"metadata":{"id":"bjziAOLvmp-H","trusted":true},"cell_type":"code","source":"test_predictions = []\n\nfirst_eval_batch = scaled_train[-n_input:]\ncurrent_batch = first_eval_batch.reshape((1, n_input, n_features))\n\nfor i in range(len(test)):\n    \n    \n    current_pred = model.predict(current_batch)[0]\n    \n    \n    test_predictions.append(current_pred) \n    \n    \n    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"LNcwZK_SmrMw","trusted":true},"cell_type":"code","source":"true_predictions = scaler.inverse_transform(test_predictions)","execution_count":null,"outputs":[]},{"metadata":{"id":"htZINr5Dmwlv","trusted":true},"cell_type":"code","source":"test['Predictions'] = true_predictions","execution_count":null,"outputs":[]},{"metadata":{"id":"xVqZygZX0SMs"},"cell_type":"markdown","source":"### Plotting our predictions with the true values:"},{"metadata":{"id":"oR55tYdzm0of","outputId":"3cbe26f3-95a3-44da-e9d6-e94509b43872","trusted":true},"cell_type":"code","source":"test.plot(figsize=(12,8))\nplt.plot(true_predictions)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"e65eQ9fx0WSO"},"cell_type":"markdown","source":"The graph looks quite on point! Let us find the RMSE value for this model:"},{"metadata":{"id":"jtcmr9XRnhzg","outputId":"9eddfebf-7202-464e-e11a-35c414fe69e5","trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nRMSE=np.sqrt(mean_squared_error(test['US_PM2.5'],test['Predictions']))\nprint('RMSE = ',RMSE)\nprint('US_PM2.5=',India_AQI['US_PM2.5'].mean())","execution_count":null,"outputs":[]},{"metadata":{"id":"HDc9NOKd0cdg"},"cell_type":"markdown","source":"The RMSE value is lower than what we had predicted with the above two models even with our limited dataset."},{"metadata":{"id":"0HUCTOHP0nom"},"cell_type":"markdown","source":"### Forecasting into the future with RNN:\nWe will use the same model but with the entire dataset now and predict one year into the future."},{"metadata":{"id":"Cdws9MGi1tJJ","trusted":true},"cell_type":"code","source":"scaler.fit(India_AQI)\nscaled_India_AQI=scaler.transform(India_AQI)","execution_count":null,"outputs":[]},{"metadata":{"id":"8xfIqGtY1s_R","trusted":true},"cell_type":"code","source":"generator = TimeseriesGenerator(scaled_India_AQI, scaled_India_AQI, length=n_input, batch_size=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"6DIvi_lt0uFl","outputId":"17121b68-29fb-4956-ef18-dd0ef80fa696","trusted":true},"cell_type":"code","source":"model.fit_generator(generator,epochs=40, verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"id":"4BsG8Em20tqp","trusted":true},"cell_type":"code","source":"test_predictions = []\n\nfirst_eval_batch = scaled_India_AQI[-n_input:]\ncurrent_batch = first_eval_batch.reshape((1, n_input, n_features))\n\nfor i in range(len(test)):\n    \n    \n    current_pred = model.predict(current_batch)[0]\n    \n    \n    test_predictions.append(current_pred) \n    \n    \n    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"DrGpaJ3B0tlb","trusted":true},"cell_type":"code","source":"true_predictions = scaler.inverse_transform(test_predictions)","execution_count":null,"outputs":[]},{"metadata":{"id":"2SPMO9hZ3hS6","trusted":true},"cell_type":"code","source":"true_predictions=true_predictions.flatten()","execution_count":null,"outputs":[]},{"metadata":{"id":"A67ycaXo1_A_","trusted":true},"cell_type":"code","source":"true_preds=pd.DataFrame(true_predictions,columns=['Forecast'])\ntrue_preds=true_preds.set_index(pd.date_range('2020-09-01',periods=5,freq='MS'))","execution_count":null,"outputs":[]},{"metadata":{"id":"ofXEk6PyBAzJ"},"cell_type":"markdown","source":"Given below are the forecasted values:"},{"metadata":{"id":"gLQZtc7o61yk","outputId":"9cf2b027-77d0-43f6-ecdd-2379937681c5","trusted":true},"cell_type":"code","source":"true_preds","execution_count":null,"outputs":[]},{"metadata":{"id":"CsYMyt-mBEvO"},"cell_type":"markdown","source":"Next, we will take a look at the plot of the actual values followed by the predicted values:"},{"metadata":{"id":"ghwG_KkX6SpS","outputId":"2cfda75b-8ddf-4d86-9507-14460bd31183","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,8))\nplt.grid(True)\nplt.plot( true_preds['Forecast'])\nplt.plot( India_AQI['US_PM2.5'])","execution_count":null,"outputs":[]},{"metadata":{"id":"S_DQHOpEBX6-"},"cell_type":"markdown","source":"Again, like that with SARIMA, we can see that the prediction is highly optimistic due to COVID-19 which can possibly be better by removing the year 2020 and predicting two years in using data pre 2020. \n\nWith this we have come to the end of the forecasting section and the notebook overall."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train)\nprint(test)\ndf = train","execution_count":null,"outputs":[]},{"metadata":{"id":"2ueBJcQkWiGz","trusted":true},"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# from pandas.tseries.offsets import DateOffset\n# from sklearn.preprocessing import MinMaxScaler\n\n# import tensorflow as tf\n# from tensorflow import keras\n\n# from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import Dense\n# from tensorflow.keras.layers import LSTM\n# from tensorflow.keras.layers import Dropout\n# import warnings\n# warnings.filterwarnings(\"ignore\")\n\n# import chart_studio as py\n# import plotly.offline as pyoff\n# import plotly.graph_objs as go\n# pyoff.init_notebook_mode(connected=True)\n\n# # def parser(x):\n# #     return pd.datetime.strptime('190'+x, '%Y-%m')\n\n# # df = pd.read_csv('shampoo.csv', parse_dates=[0], index_col=0, date_parser=parser)\n# # df.tail()\n\n# # train = df\n\n# scaler = MinMaxScaler()\n# scaler.fit(train)\n# train = scaler.transform(train)\n\n# n_input = 12\n# n_features = 1\n# generator = TimeseriesGenerator(train, train, length=n_input, batch_size=6)\n\n# model = Sequential()\n# model.add(LSTM(200, activation='relu', input_shape=(n_input, n_features)))\n# model.add(Dropout(0.15))\n# model.add(Dense(1))\n\n# optimizer = keras.optimizers.Adam(learning_rate=0.001)\n# model.compile(optimizer=optimizer, loss='mse',metrics = ['acc'])\n\n# history = model.fit_generator(generator,epochs=100,verbose=1)\n\n# hist = pd.DataFrame(history.history)\n# hist['epoch'] = history.epoch\n\n# plot_data = [\n#     go.Scatter(\n#         x=hist['epoch'],\n#         y=hist['loss'],\n#         name='loss'\n#     )\n    \n# ]\n\n# plot_layout = go.Layout(\n#         title='Training loss'\n#     )\n# fig = go.Figure(data=plot_data, layout=plot_layout)\n# pyoff.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pred_list = []\n\n# batch = train[-n_input:].reshape((1, n_input, n_features))\n\n# for i in range(n_input):   \n#     pred_list.append(model.predict(batch)[0]) \n#     batch = np.append(batch[:,1:,:],[[pred_list[i]]],axis=1)\n\n\n# add_dates = [df.index[-1] + DateOffset(months=x) for x in range(0,13) ]\n# future_dates = pd.DataFrame(index=add_dates[1:],columns=df.columns)\n\n# df_predict = pd.DataFrame(scaler.inverse_transform(pred_list),\n#                           index=future_dates[-n_input:].index, columns=['Prediction'])\n\n# df_proj = pd.concat([df,df_predict], axis=1)\n\n# df_proj.tail(12)\n\n# plot_data = [\n#     go.Scatter(\n#         x=df_proj.index,\n#         y=df_proj['US_PM2.5'],\n#         name='actual'\n#     ),\n#     go.Scatter(\n#         x=df_proj.index,\n#         y=df_proj['Prediction'],\n#         name='prediction'\n#     )\n# ]\n\n# plot_layout = go.Layout(\n#         title='Shampoo sales prediction'\n#     )\n# fig = go.Figure(data=plot_data, layout=plot_layout)\n# pyoff.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pip install chart_studio","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from tensorflow import keras","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# conda install -c plotly chart-studio","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pip install chart-studio","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_predictions = []\n\n# first_eval_batch = scaled_train[-n_input:]\n# current_batch = first_eval_batch.reshape((1, n_input, n_features))\n\n# for i in range(len(test)):\n    \n    \n#     current_pred = model.predict(current_batch)[0]\n    \n    \n#     test_predictions.append(current_pred) \n    \n    \n#     current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}