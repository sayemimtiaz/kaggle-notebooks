{"cells":[{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"# Understanding Covid 19 with Topic Modeling and Sentence Embeddings"},{"metadata":{},"cell_type":"markdown","source":"#### Submission for COVID-19 Open Research Dataset Challenge (CORD-19)"},{"metadata":{},"cell_type":"markdown","source":"## Install and Load Packages"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# install scispacy\n!pip install scispacy\n!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz\n\n# install langdetect\n!pip install langdetect","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# enable widgets\n!jupyter nbextension enable --py --sys-prefix widgetsnbextension","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"# Helper packages.\nfrom IPython.core.display import display, HTML\nimport os\nimport pandas as pd\npd.set_option('max_colwidth', 1000)\npd.set_option('max_rows', 100)\nimport numpy as np\nnp.set_printoptions(threshold=10000)\nimport pickle\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport re\nimport json\nfrom tqdm.auto import tqdm\nimport textwrap\nimport importlib as imp\nfrom scipy.spatial.distance import cdist\nimport gc\n\n# Packages with tools for text processing.\n# if you have not downloaded stopwords, run the following line\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport nltk\nnltk.download('stopwords')\nimport scispacy\nimport spacy\n\n# Packages for working with text data.\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Packages for getting data ready for and building a LDA model\nimport gensim\nfrom gensim import corpora, models\nfrom gensim.models.coherencemodel import CoherenceModel\nfrom langdetect import detect\n\n# Package for FastText\nimport fasttext\n\n# Other plotting tools.\nimport pyLDAvis\nimport pyLDAvis.gensim\nfrom wordcloud import WordCloud\nfrom IPython.display import display, Markdown, Latex\nimport ipywidgets as widgets\n\n# Print current directory\nprint('Current directory is {}'.format(os.getcwd()))\n\n# Extend notebook to full width\ndisplay(HTML(\"<style>.container {width:100% !important; }</style>\"))\n\n# Check python version\nfrom platform import python_version\nprint('Current python version is {}'.format(python_version()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Set Directories"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_data_path = '/kaggle/input/CORD-19-research-challenge/'\nworking_data_path = '/kaggle/input/cov19-pickles/'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Set Global Variables"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"source_column = 'text' # if abstract, change to 'abstract'\nid_colname = 'cord_uid' # id in metadata for each article\nsplit_sentence_by = '(?<=\\\\.) ?(?![0-9a-z])' # sentence splitter","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"}},"cell_type":"markdown","source":"## Data Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"This `meta_full_text` is generated by the next 5 cells:\n1.     import `metadata`\n1.     parse publish time to datetime object\n1.     get full path to .pdf or .pmc\n1.     get full text\n1.     drop records with empty abstract and empty body text\n1.     check duplicated text: most likely due to publications on different journals in which case we keep the latest\n1.     check duplicated cord_uid: most likely due to publications on different journals in which case we keep the latest\n1.     drop redundant columns and save to pickle"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load from pickle (generated below)\nmeta_full_text = pickle.load(open(working_data_path + 'all_papers.pkl', 'rb'))","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"}},"cell_type":"markdown","source":"    # 1. import metadata\n    metadata = pd.read_csv(input_data_path + 'metadata.csv', encoding='utf-8').replace({np.nan: None})\n    print(metadata.shape)\n    print('\\nNumber of NA for each column: ')\n    metadata.isnull().sum(axis=0)"},{"metadata":{},"cell_type":"markdown","source":"    # 2. parse publish time to datetime object\n    def str2time(s):\n        try:\n            return datetime.strptime(s, '%Y-%m-%d')\n        except:\n            try:\n                return datetime.strptime(s, '%Y %B')\n            except:\n                try:\n                    return datetime.strptime(s, '%Y %b')\n                except:\n                    try:\n                        return datetime.strptime(s, '%Y %B %b')\n                    except:\n                        try:\n                            return datetime.strptime(s, '%Y %b %d') \n                        except:\n                            try:\n                                return datetime.strptime(s, '%Y')\n                            except:\n                                return pd.NaT\n                            return pd.NaT\n                        return pd.NaT\n                    return pd.NaT\n                return pd.NaT\n            return pd.NaT\n        return pd.NaT\n\n\n    metadata['full_text_file_path'] = None\n    for i in tqdm(metadata.index):\n        row = metadata.iloc[i,:]\n        full_text_file_path = []\n\n        # 3. get full path to .pdf or .pmc: prioritize pdf as source, if none, search for pmc\n        if row.pdf_json_files: \n            full_text_file_path.extend([path.strip() for path in row.pdf_json_files.split(';')])\n        else:\n            if row.pmc_json_files:\n                full_text_file_path.extend([path.strip() for path in row.pmc_json_files.split(';')])\n\n        if row.publish_time is None: row.publish_time = ''\n        publish_time = re.sub(' ([a-zA-Z]{3}-[a-zA-Z]{3})|(Spring)|(Summer)|(Autumn)|(Fall)|(Winter)','', row.publish_time).strip()\n        publish_time = str2time(publish_time)\n\n        metadata.loc[i,'publish_time'] = publish_time\n        metadata.loc[i,'full_text_file_path'] = full_text_file_path"},{"metadata":{"scrolled":true},"cell_type":"markdown","source":"    # 4. extract full text from JSON files\n    def get_paper_info(json_data):\n        return ' '.join([t['text'] for t in json_data['body_text']])\n\n    full_text = []\n    for r in tqdm(metadata.to_dict(orient='records')):\n        record = []\n        for p in r['full_text_file_path']:\n            with open(input_data_path + p, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n                record.append(get_paper_info(data))\n        full_text_ = '\\n'.join(np.unique(record)) if len(record) > 0 else None\n        full_text.append(full_text_)\n    metadata['full_text'] = full_text\n\n    # 5. drop records with empty abstract AND empty full text\n    meta_full_text = metadata\n    meta_full_text[source_column]= np.where(meta_full_text['full_text'].isnull(), meta_full_text['abstract'], meta_full_text['full_text'])\n    meta_full_text = meta_full_text.dropna(subset = [source_column]).reset_index(drop=True)\n\n    # 6. check duplicated text: most likely due to publications on different journals - in which case we keep the latest one\n    print('In total, {} of the rows have a duplicated {} column, and there are a total of {} duplicated {} entries.'.format(sum([len(g) for k, g in meta_full_text.groupby(source_column) if len(g) > 1]), source_column, len([1 for k, g in meta_full_text.groupby(source_column) if len(g) > 1]), source_column))\n    meta_full_text = meta_full_text.sort_values('publish_time', ascending=False).drop_duplicates(source_column)\n\n    # 7. check duplicated cord_uid: most likely due to publications on different journals - in which case we keep the latest one\n    print('In total, {} of the rows have a duplicated {} column, and there are a total of {} duplicated {} entries.'.format(sum([len(g) for k, g in meta_full_text.groupby(id_colname) if len(g) > 1]), id_colname, len([1 for k, g in meta_full_text.groupby(id_colname) if len(g) > 1]), id_colname))\n    meta_full_text = meta_full_text.sort_values('publish_time', ascending=False).drop_duplicates(id_colname)\n\n    # 8. remove metadata from memory to clear space\n    del metadata\n    gc.collect()"},{"metadata":{},"cell_type":"markdown","source":"    # 9. drop redundant columns and save to pickle\n    meta_full_text.drop(['sha', 'pmcid', 'pubmed_id', 's2_id', 'license', 'mag_id', 'arxiv_id', 'pdf_json_files', 'pmc_json_files', 'full_text_file_path', 'full_text'], inplace=True, axis=1)\n\n    print(meta_full_text.shape)\n    print(meta_full_text.columns)\n    print('number of unique cord_uid is {}'.format(len(meta_full_text.cord_uid.unique())))\n\n    pickle.dump(meta_full_text, open(working_data_path + 'all_papers.pkl', 'wb'))"},{"metadata":{},"cell_type":"markdown","source":"## Topic Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(source_column)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = meta_full_text[source_column]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first thing that we want to do is just to explore the themes of this corpus - figuring out what the main topics are there in this literature. We can use LDA to do that. Note that the `source_column` that we are using here is column `text` - so mostly full text and in some cases abstract when full text is not available. This should give us a comprehensive coverage of all tokens provided in the literature. \n\nHere are the steps for this data exploration:\n1.     build tokenizer to parse out valid tokens and get their count with `CountVectorizer`\n1.     find the optimal number of topics such that overall, tokens are assigned as few topics as possible and documents are assigned as few topics as possible\n1.     visualize topic representations for the optimal number of topics"},{"metadata":{},"cell_type":"markdown","source":"### 1. Parse out valid tokens and get their count"},{"metadata":{},"cell_type":"markdown","source":"`valid_tokens` and `X` are generated by the next 3 cells"},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_tokens = pickle.load(open(working_data_path + 'TM_valid_tokens.pkl', 'rb')) # valida tokens after parsing\nX = pickle.load(open(working_data_path + 'TM_X.pkl', 'rb')) # valid tokens with their count","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    # Load SpaCy for lemmatization\n    nlp_lg = spacy.load('en_core_sci_lg',disable=['tagger', 'parser', 'ner'])\n    nlp_lg.max_length = np.max([len(t) for t in corpus.values])\n\n\n    # Establish stop words\n\n    # default stop words\n    stop_words=stopwords.words('english')\n\n    # custom CORD19 stop words, mostly from Daniel Wolffram's submission \"Topic Modeling: Finding Related articles\"\n    cord_stopwords = ['doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', 'rights', 'reserved', \n                      'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', 'Elsevier', 'PMC', 'CZI','-PRON-',\n                      'abstract']\n    # all stop words\n    for word in tqdm(cord_stopwords):\n        if (word not in stop_words):\n            stop_words.append(word)\n        else:\n            continue\n\n    # update SpaCy stop words list\n    for w in tqdm(stop_words):\n        nlp_lg.vocab[w].is_stop = True\n\n    # Build tokenizer\n    def spacy_tokenizer(sentence):\n\n        # removes substrings before it's tokenized and stemmed\n        def removeParenthesesNumbers(v):\n            char_list_rm = ['[(]','[)]','[′·]']\n            char_list_rm_spc = [' no[nt]-',' non', ' low-', ' high-']\n            v = re.sub('|'.join(char_list_rm), '', v)\n            v = re.sub('|'.join(char_list_rm_spc), ' ', v)\n            return(v)\n\n        sentence = removeParenthesesNumbers(sentence)\n        tokenized_list = []\n        sentence_letters_only = re.sub('[^a-zA-Z]', '', sentence).strip()\n\n        if sentence_letters_only!=\"\":\n            lang = detect(sentence)\n\n            if lang=='en': # only focus on english literature\n                # define types of tokens that should be removed using regex\n                token_rm = ['(www.\\S+)','(-[1-9.])','([∼≈≥≤≦⩾⩽→μ]\\S+)','(\\S+=\\S+)','(http\\S+)']\n                tokenized_list = [word.lemma_ for word in nlp_lg(sentence) if not (word.like_num or word.is_stop or word.is_punct or word.is_space)]\n                tokenized_list = [word for word in tokenized_list if not re.search('|'.join(token_rm),word)]\n                tokenized_list = [word for word in tokenized_list if len(re.findall('[a-zA-Z]',word))>1]\n                tokenized_list = [word for word in tokenized_list if re.search('^[a-zA-Z0-9]',word)]\n        return tokenized_list"},{"metadata":{},"cell_type":"markdown","source":"    # Test tokenizer\n    sentence_test = '($2196.8)/case (in)fidelity μg μg/ml a=b2 www.website.org α-gal 2-len a.'\n    spacy_tokenizer(sentence_test)"},{"metadata":{},"cell_type":"markdown","source":"    # Initialize `CountVectorizer`. Remove common and sparse terms\n    vec = CountVectorizer(max_df = .8, min_df = .001, tokenizer = spacy_tokenizer)\n\n    # Transform the list of snippets into DTM.\n    X = vec.fit_transform(tqdm(corpus))\n\n    valid_tokens = vec.get_feature_names()\n\n    # pickle\n    pickle.dump(X, open(working_data_path + 'TM_X.pkl', 'wb'))\n    pickle.dump(valid_tokens, open(working_data_path + 'TM_valid_tokens.pkl', 'wb'))"},{"metadata":{},"cell_type":"markdown","source":"### 2. find the optimal number of topics"},{"metadata":{},"cell_type":"markdown","source":"We need `texts` (tokenized texts with repetition), `dictionary` (map from word IDs to words) and `bow_corpus` (count by word ID) before training LDA models, which is generated by the next 4 cells."},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(1)\ntexts = pickle.load(open(working_data_path + 'TM_texts.pkl', 'rb'))\n\ndictionary = gensim.corpora.Dictionary(texts)\nbow_corpus = pickle.load(open(working_data_path + 'TM_bow_corpus.pkl', 'rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bow_doc_1 = bow_corpus[0]\nprint(corpus[corpus.index[0]])\nfor i in tqdm(range(len(bow_doc_1))):\n    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_1[i][0], dictionary[bow_doc_1[i][0]],bow_doc_1[i][1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*code to generate `text`*"},{"metadata":{},"cell_type":"markdown","source":"    arr = X.toarray()\n    texts = []\n    for i in tqdm(range(arr.shape[0])):\n        text = []\n        for j in range(arr.shape[1]):\n            occurrence = arr[i,j]\n            if occurrence > 0:\n                text.extend([valid_tokens[j]] * occurrence)\n        texts.append(text)\n\n    pickle.dump(texts, open(working_data_path + 'TM_texts.pkl', 'wb'))"},{"metadata":{},"cell_type":"markdown","source":"*code to generate `dictionary` and `bow_corpus`*"},{"metadata":{},"cell_type":"markdown","source":"    dictionary = gensim.corpora.Dictionary(texts)\n    bow_corpus = [dictionary.doc2bow(doc) for doc in texts]\n    pickle.dump(bow_corpus, open(working_data_path + 'TM_bow_corpus.pkl', 'wb'))"},{"metadata":{},"cell_type":"markdown","source":"Before optimizing the model, we delete redundant variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"del X, valid_tokens\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we try to find the optimal model by trying various numbers of topics, from 10 to 20, and comparing their coherence scores `coherence_values`."},{"metadata":{"trusted":true},"cell_type":"code","source":"limit=20; start=10; step=1;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_list = pickle.load(open(working_data_path + 'TM_model_list.pkl', 'rb'))\ncoherence_values = pickle.load(open(working_data_path + 'TM_coherence_values.pkl', 'rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = range(start, limit, step)\ntopic_num = x[np.argmax(coherence_values)]\n\nplt.plot(x, coherence_values)\nplt.title(\"Optimal Number of Topics is \" + str(topic_num))\nplt.xlabel(\"Num Topics\")\nplt.ylabel(\"Coherence score\")\nplt.legend((\"coherence_values\"), loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"    def compute_coherence_values(dictionary, corpus, texts, limit, start = 2, step = 3):\n        coherence_values = []\n        model_list = []\n        for num_topics in tqdm(range(start, limit, step)):\n            model = gensim.models.LdaMulticore(corpus = corpus, id2word = dictionary, num_topics = num_topics, random_state = 1)\n            model_list.append(model)\n            coherencemodel = CoherenceModel(model = model, texts = texts, dictionary = dictionary, coherence = 'c_v')\n            coherence_values.append(coherencemodel.get_coherence())\n            print('Number of topics: {}, Coherence value: {}'.format(num_topics, coherencemodel.get_coherence()))\n\n        return model_list, coherence_values\n\n    model_list, coherence_values = (compute_coherence_values(dictionary = dictionary, \n                                                             corpus = bow_corpus,\n                                                             texts = texts, \n                                                             start = start, limit = limit, step = step))\n\n    pickle.dump(model_list, open(working_data_path + 'TM_model_list.pkl', 'wb'))\n    pickle.dump(coherence_values, open(working_data_path + 'TM_coherence_values.pkl', 'wb'))"},{"metadata":{"trusted":true},"cell_type":"code","source":"del model_list, coherence_values\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We get the model with the largest coherence score:"},{"metadata":{"trusted":true},"cell_type":"code","source":"lda_model = pickle.load(open(working_data_path+'TM_lda_model.pkl','rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"    lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics = topic_num, id2word = dictionary, workers = 4, passes = 2)\n    print(lda_model)\n    pickle.dump(lda_model, open(working_data_path+'TM_lda_model.pkl','wb'))"},{"metadata":{},"cell_type":"markdown","source":"### 3. visualize topic representations for the optimal number of topics"},{"metadata":{},"cell_type":"markdown","source":"Print Topics:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx, topic in lda_model.print_topics(-1):\n    print('Topic: {} Word: {}'.format(idx, topic))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualize topic modeling results:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import HTML\nHTML(filename=working_data_path + 'TM_lda_vis.html')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*code to generate visualization*"},{"metadata":{},"cell_type":"markdown","source":"    vis = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary)\n    pyLDAvis.display(vis)\n    pyLDAvis.save_html(vis, working_data_path + 'TM_lda_vis.html')"},{"metadata":{},"cell_type":"markdown","source":"Visualize wordclouds:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['#029386','#f97306','#ff796c','#cb416b','#fe01b1',\n        '#fd411e','#be03fd','#1fa774','#04d9ff','#c9643b',\n        '#7ebd01','#155084','#fd4659','#06b1c4','#8b88f8',\n        '#029386','#f97306']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topics = lda_model.show_topics(num_words=20,num_topics=topic_num,formatted=False)\ncloud = WordCloud(background_color='black',color_func=lambda *args, **kwargs: cols[i],prefer_horizontal=1.0, font_step=1, width=350,height=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make word clouds for all topics\nfig, axes = plt.subplots(3, 6, figsize=(25,10), sharex=True, sharey=True)\n\nfor i, ax in tqdm(enumerate(axes.flatten())):\n    if i < len(topics):\n        fig.add_subplot(ax)\n        topic_words = dict(topics[i][1])\n        cloud.generate_from_frequencies(topic_words, max_font_size=50)\n        plt.gca().imshow(cloud)\n        plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n        plt.gca().axis('off')\n    else:\n        ax.axis('off')\n\nplt.subplots_adjust(wspace=0, hspace=0)\nplt.axis('off')\nplt.margins(x=0, y=0)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"remove redundant variables before proceeding:"},{"metadata":{"trusted":true},"cell_type":"code","source":"del texts, dictionary, bow_corpus, lda_model\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"}},"cell_type":"markdown","source":"## Search Covid Literature using `fastText` embeddings"},{"metadata":{},"cell_type":"markdown","source":"In this last section, we uses `fastText` to generate word embeddings for tokens in the existing literature and calculate sentence embeddings for sentences in covid related literature. We define covid-related literature to be those that satisfy at least one of the conditions below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# covid earlist date\ncov_earliest_date = datetime.strptime('2019-12-01', \"%Y-%m-%d\")\n# covid key terms\ncov_key_terms = ['covid\\\\W19','covid19', 'covid', '2019\\\\Wncov', '2019ncov', 'ncov\\\\W2019','sars\\\\Wcov\\\\W2', 'sars\\\\Wcov2', '新型冠状病毒']\n# covid related terms\ncov_related_terms = '(novel|new)( beta| )coronavirus'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* the paper contains *covid key terms* anywhere in the paper\n* the paper contains *covid related terms* anywhere in the paper AND is published after *2019-12-01*\n* the paper is marked as *WHO #Covidence* AND, EITHER contains *related terms* OR is published after *2019-12-01*"},{"metadata":{},"cell_type":"markdown","source":"Our goal in this section is to build a search tool that uses sentence embeddings to rank sentences by their cosine similarity to that of a search term. We proceed as follows:\n1. Get covid-related literature and split them into sentences to apply word embeddings upon\n2. Train fastText model on the entire literature to get word embeddings\n3. Pre-calculate sentence embeddings using those word embeddings \n4. Build search tool to rank sentences by their cosine similarity to the query term"},{"metadata":{},"cell_type":"markdown","source":"I train the fastText model with `cbow` (instead of `skipgram`) and number of epochs equaling 3:"},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_m = 'cbow'\nselected_epoch = 3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Get covid-related literature and split them into sentences to apply word embeddings upon"},{"metadata":{},"cell_type":"markdown","source":"We generate word embeddings from sentences in **all text** - full text, or abstract if full text is not available."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(source_column)\n\nselected_text = 'raw_' + source_column\nmodel_name_suffix = selected_m + '_' + selected_text + '_epoch' + str(selected_epoch)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We calculate sentence embeddings for sentences in abstract, so all of our search results are sentences from abstract only. We can switch to `'text'` if we want to search all text. "},{"metadata":{"trusted":true},"cell_type":"code","source":"search_column = 'abstract' \n\nsearch_text = 'raw_' + search_column\nsearch_name_suffix = selected_m + '_' + search_text + '_epoch' + str(selected_epoch)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, we get covid-related literature:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_covid19(data):\n    cov_key_terms_mask = data[source_column].str.lower().str.contains('|'.join(cov_key_terms))\n    cov_related_terms_mask = data[source_column].str.lower().str.contains(cov_related_terms)\n\n    data['WHO_covidence'] = False\n    data.loc[~data['who_covidence_id'].isnull(), 'WHO_covidence'] = True\n\n    data['contain_key_terms'] = False\n    data.loc[cov_key_terms_mask,'contain_key_terms'] = True\n\n    data['contain_related_terms'] = False\n    data.loc[cov_related_terms_mask,'contain_related_terms'] = True\n\n    data['after_earliest_date'] = False\n    data.loc[data.publish_time>= cov_earliest_date,'after_earliest_date'] = True\n\n    covid19 = data[data.contain_key_terms | (data.contain_related_terms & data.after_earliest_date) | (data.WHO_covidence & (data.contain_related_terms | data.after_earliest_date))]\n    covid19.reset_index(drop=True, inplace=True)\n    print(\"There are a total number of {} papers satisfying the above definition\".format(len(covid19)))\n    return covid19","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"covid19 = get_covid19(meta_full_text)\nprint(covid19.shape)\ncovid19[:1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"remove redundant variables:"},{"metadata":{"trusted":true},"cell_type":"code","source":"del meta_full_text\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we split them into sentences, and for each sentence we generate a dictionary to lookup paper-level info. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sents_in_paper = pickle.load(open(working_data_path + 'fasttext_model_' + search_column + '_sents_in_paper.pkl', 'rb'))\npaper_lookup = pickle.load(open(working_data_path + 'fasttext_model_' + search_column + '_paper_lookup.pkl', 'rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"    def sentence_to_paper(df, id_colname, text_colname, topic_colname_prefix, split_sentence_by):\n        # link sentences to a paper: sents_in_paper\n        sents_in_paper = dict()\n        papers = [(paper[id_colname], paper[text_colname]) if paper[text_colname] is not None else (paper[id_colname], \"\") for paper in df.to_dict(orient='row')]\n        sents = [(paper[0], re.split(split_sentence_by, paper[1])) for paper in papers]\n        sent_order = 1\n        for pair in np.concatenate([list(zip(id, sent)) for id, sent in [([sent[0]]*len(sent[1]),sent[1]) for sent in sents]]):\n            sent = pair[1]\n            if sent not in sents_in_paper:\n                sents_in_paper[sent] = (pair[0], sent_order)\n                sent_order += 1\n\n        # lookup paper information: paper_lookup        \n        paper_lookup = dict()\n        for paper in df.to_dict(orient='records'):\n            id = str(paper[id_colname])\n            if id not in paper_lookup:\n                paper[topic_colname_prefix] = dict((k, paper[k]) for k in paper.keys() if k.startswith(topic_colname_prefix))\n                paper_lookup[id] = paper    \n\n        return sents_in_paper, paper_lookup\n\n\n    sents_in_paper, paper_lookup = sentence_to_paper(covid19, \n                                                     id_colname=id_colname, \n                                                     text_colname=search_column, \n                                                     topic_colname_prefix='topic', \n                                                     split_sentence_by=split_sentence_by)\n\n    pickle.dump(sents_in_paper, open(working_data_path + 'fasttext_model_' + search_column + '_sents_in_paper.pkl', 'wb'))\n    pickle.dump(paper_lookup, open(working_data_path + 'fasttext_model_' + search_column + '_paper_lookup.pkl', 'wb'))"},{"metadata":{},"cell_type":"markdown","source":"remove redundant variables:"},{"metadata":{"trusted":true},"cell_type":"code","source":"del covid19\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.Train fastText model on the entire literature to get word embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = fasttext.load_model(working_data_path + 'fasttext_model_' + model_name_suffix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"    # create file with individual sentence on each line\n    file = open(working_data_path + 'fasttext_model_' + source_column + '_by_sentence.txt', 'w', encoding='utf-8')\n    for txt in filter(None, corpus.values):\n        file.write('\\n'.join(re.split(split_sentence_by, txt)))\n    file.close()\n\n    # run model\n    model = fasttext.train_unsupervised(working_data_path + 'fasttext_model_' + source_column + '_by_sentence.txt', \n                                        model = selected_m, \n                                        epoch = selected_epoch)\n    model.save_model(working_data_path + 'fasttext_model_' + model_name_suffix)"},{"metadata":{"trusted":true},"cell_type":"code","source":"emb_len = len(model.get_output_matrix()[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Pre-calculate sentence embeddings using those word embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pickle.load(open(working_data_path + 'fasttext_model_' + search_name_suffix + '_X.pkl', 'rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"    # get sentence embeddings\n    X = pd.DataFrame(pd.np.empty((len(list(sents_in_paper.keys())), emb_len)))\n\n    i = 0\n    for sent in tqdm(list(sents_in_paper.keys())):\n        X.iloc[i] = model.get_sentence_vector(sent)\n        i+=1\n\n    pickle.dump(X, open(working_data_path + 'fasttext_model_' + search_name_suffix + '_X' + '.pkl', 'wb'))"},{"metadata":{},"cell_type":"markdown","source":"### 4. Build search tool to rank sentences by their cosine similarity to the query term"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create search\nclass Quicksearch:\n    def __init__(self, modl, emb_len, sentences, sentence_embeddings, paper_lookup):\n        self.modl = modl\n        self.emb_len = emb_len\n        self.sentences = sentences\n        self.sentence_embeddings = sentence_embeddings\n        self.paper_lookup = paper_lookup\n    def get_candidate_ranking(self, sent):\n        y = self.modl.get_sentence_vector(sent)\n        scores = cdist(self.sentence_embeddings,[y],'cosine').ravel()\n        return list(zip(list(self.sentences.keys()), scores))\n    def term(self, init, placeholder, description):\n        return widgets.Textarea(value=init, \n                                placeholder=placeholder, \n                                description=description, \n                                layout=widgets.Layout(width='90%', display='flex'))\n    def sort(self, init, options, description):\n        return widgets.Dropdown(options=options,\n                                  value=init,\n                                  description=description, \n                                  layout=widgets.Layout(width='90%', display='flex'))\n    def top(self, init, maxx, description):\n        return widgets.IntSlider(min=1, \n                                 max=maxx, \n                                 value=init, \n                                 description=description, \n                                 layout=widgets.Layout(width='90%', display='flex'))\n    def search(self, term, sort_by, show_top):\n        if term == '':\n            print('')\n        else:\n            term = term.lower()\n            sent_rank, paper_rank, final_result = [], dict(), []\n            \n            # get ranking for search results\n            ranked_sentences = sorted(self.get_candidate_ranking(term), key=lambda x:x[1])\n\n            # for each sentence, record content, rank, order in paper\n            # for each paper, record highest ranked sentence\n            for i, ranked_sentence in enumerate(ranked_sentences):\n                if i < show_top:\n                    sentence = ranked_sentence[0]\n                    score = ranked_sentence[1]\n\n                    r = dict()\n                    r['rank'] = i + 1\n                    r['sentence'] = sentence\n                    r['paper id'] = self.sentences[sentence][0]\n                    r['sentence_order'] = self.sentences[sentence][1]\n                    sent_rank.append(r)\n                    \n                    #record highest ranking sentence\n                    if self.sentences[sentence][0] not in paper_rank: \n                        paper_rank[self.sentences[sentence][0]] = i + 1\n    \n            # for each paper, lookup information on that paper\n            for key, group in pd.DataFrame(sent_rank).groupby('paper id'):\n                r = dict()\n                r['rank'] = paper_rank[key]\n                r['publish_time'] = self.paper_lookup[key]['publish_time']\n                r['title'] = self.paper_lookup[key]['title']\n                r['journal'] = self.paper_lookup[key]['journal']\n                r['url'] = self.paper_lookup[key]['url']\n                r['topic'] = self.paper_lookup[key]['topic']\n                r['sentences'] = [sent for sent, order in sorted(zip(group['sentence'].values, group['sentence_order'].values), key=lambda r: r[1])]\n                final_result.append(r)\n            final_result = pd.DataFrame(final_result)\n            \n            # print search results\n            if_ascend = False if sort_by == 'publish_time' else True\n            \n            print('Search Results for ' + '\\033[1m' + '\"' + term.upper() + '\\033[0m' + '\"\\n')\n            \n            for k,r in final_result.sort_values(by=[sort_by], ascending=if_ascend).iterrows():\n                r['url'] = \"\" if r['url'] is None else r['url']\n                r['journal'] = 'NA' if r['journal'] is None else r['journal']\n                r['publish_time'] = '' if pd.isnull(r['publish_time']) else datetime.strftime(r['publish_time'], '%Y-%m-%d')\n                r['sentences'] = '...'.join(r['sentences'])\n                \n                print('\\033[1m' + r['title'] + '\\033[0m')\n                print('\\033[1m' + 'Results: ' + '\\033[0m' + r['sentences'])\n                print('\\033[1m' + 'Publish Time: ' + '\\033[0m' + r['publish_time'])\n                print('\\033[1m' + 'Journal: ' + '\\033[0m' + r['journal'])\n                print('\\033[1m' + 'Link: ' + '\\033[0m' + r['url'])\n                print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"slideshow":{"slide_type":"fragment"},"trusted":true},"cell_type":"code","source":"# define quicksearch\nquicksearch = Quicksearch(model, emb_len, sents_in_paper, X, paper_lookup)\n\n# set up init options\ninit_show = 10\ninit_max = 100\ninit_sort = 'publish_time'\ninit_search = 'incubation period'\ninit_options = {'Most Recent': 'publish_time', 'Most Similar': 'rank'}\n\n# set up widget\nterm = quicksearch.term(init=init_search, placeholder='', description='Search: ')\nsort_by = quicksearch.sort(init=init_sort, options=init_options, description='Sort By: ')\nshow_top = quicksearch.top(init=init_show, maxx=init_max, description='Filter # of Sentences to Show: ')\nshow_top.style.handle_color='darkred'\nterm.style.description_width = '100px'\nsort_by.style.description_width = '100px'\nshow_top.style.description_width = '180px'\n\nsearch = widgets.interactive(quicksearch.search, \n                             term = term, \n                             sort_by = sort_by, \n                             show_top = show_top)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"display(search)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}