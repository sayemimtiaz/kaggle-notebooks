{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-19T13:42:28.55485Z","iopub.execute_input":"2021-08-19T13:42:28.555242Z","iopub.status.idle":"2021-08-19T13:42:28.575568Z","shell.execute_reply.started":"2021-08-19T13:42:28.55516Z","shell.execute_reply":"2021-08-19T13:42:28.574547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# install new spacy ver\n!pip install spacy==3.1.1\n!python -m spacy download en_core_web_sm\n\nimport spacy\nspacy.__version__","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:42:28.577522Z","iopub.execute_input":"2021-08-19T13:42:28.578258Z","iopub.status.idle":"2021-08-19T13:43:21.83172Z","shell.execute_reply.started":"2021-08-19T13:42:28.578215Z","shell.execute_reply":"2021-08-19T13:43:21.830619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# File cleaning\n\nFirst, let's open the list of papers and authors.","metadata":{}},{"cell_type":"code","source":"papers = pd.read_csv(\"/kaggle/input/nips-papers-1987-2019-updated/papers.csv\")\npapers.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:21.834005Z","iopub.execute_input":"2021-08-19T13:43:21.834446Z","iopub.status.idle":"2021-08-19T13:43:29.116221Z","shell.execute_reply.started":"2021-08-19T13:43:21.834397Z","shell.execute_reply":"2021-08-19T13:43:29.114972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"authors = pd.read_csv(\"/kaggle/input/nips-papers-1987-2019-updated/authors.csv\")\nauthors.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:29.117763Z","iopub.execute_input":"2021-08-19T13:43:29.118098Z","iopub.status.idle":"2021-08-19T13:43:29.177487Z","shell.execute_reply.started":"2021-08-19T13:43:29.118071Z","shell.execute_reply":"2021-08-19T13:43:29.176554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So far, some values are missing from both CSVs. The author dataset misses a couple of institutions and the abstracts are not all presents in the paper dataset.","metadata":{}},{"cell_type":"code","source":"# Check the NaNs\ndef show_na_proportion(df, name=\"df\"):\n    print(f\"Percentage of NaNs for {name}:\")\n    print((df.isna().sum() / df.shape[0]).mul(100).round(2))\n    return df.columns[df.isna().sum() > 0]\n\nna_col_papers = show_na_proportion(papers, name=\"papers\")\nna_col_authors = show_na_proportion(authors, name=\"authors\")","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:29.179024Z","iopub.execute_input":"2021-08-19T13:43:29.179441Z","iopub.status.idle":"2021-08-19T13:43:29.227313Z","shell.execute_reply.started":"2021-08-19T13:43:29.179397Z","shell.execute_reply":"2021-08-19T13:43:29.226034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The full text is present for almost every paper. However, 35% of abstracts are missing. This shouldn't impact the analysis as these are present inside the full text. We can thus omit this column.\n\nHowever, more than 40% of institutions are missing. We can assume the institution either isn't reported, or the paper isn't affiliated to any institution at all.","metadata":{}},{"cell_type":"code","source":"author_institution_unique_vals = authors.institution.unique()\nprint(f\"Number of unique institution values: {author_institution_unique_vals.size}\")\nprint(\"for values:\", author_institution_unique_vals)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:29.228509Z","iopub.execute_input":"2021-08-19T13:43:29.228822Z","iopub.status.idle":"2021-08-19T13:43:29.243984Z","shell.execute_reply.started":"2021-08-19T13:43:29.228789Z","shell.execute_reply":"2021-08-19T13:43:29.242799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's deal with those missing values...","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\nno_na_papers = SimpleImputer(strategy=\"constant\", fill_value=\"NO DATA\").fit_transform(papers.drop('abstract', axis=1))\nno_na_authors = SimpleImputer(strategy=\"constant\", fill_value=\"NO DATA\").fit_transform(authors)\n\nno_na_papers = pd.DataFrame(data=no_na_papers, columns=[col for col in papers.columns if col != 'abstract'])\nno_na_authors = pd.DataFrame(data=no_na_authors, columns=authors.columns)\n\nshow_na_proportion(no_na_papers, name=\"papers (no NaN)\")\nshow_na_proportion(no_na_authors, name=\"authors (no NaN)\")","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:29.246506Z","iopub.execute_input":"2021-08-19T13:43:29.246967Z","iopub.status.idle":"2021-08-19T13:43:29.538694Z","shell.execute_reply.started":"2021-08-19T13:43:29.246934Z","shell.execute_reply":"2021-08-19T13:43:29.537733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Author analysis\n\nOne might observe the repartition between\n- The papers and the institution\n- The authors and the institution\n- The number of papers per author\n\nThis analysis might help us identify the big shots of Deep Learning research.\n\nTo avoid any confusions between the names, we'll create a column titled `full_name`, where the first and last names are combined.","metadata":{}},{"cell_type":"code","source":"no_na_authors[\"full_name\"] = no_na_authors.apply(lambda row: row.first_name + \" \" + row.last_name, axis=1) \nno_na_authors.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:29.541743Z","iopub.execute_input":"2021-08-19T13:43:29.542144Z","iopub.status.idle":"2021-08-19T13:43:30.103061Z","shell.execute_reply.started":"2021-08-19T13:43:29.542101Z","shell.execute_reply":"2021-08-19T13:43:30.102054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can see the top 10 of the authors who contributed the most in NeurIPS articles:","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ntop_10_contributors = no_na_authors.full_name.value_counts().head(10)\n\nfig, ax = plt.subplots(figsize=(10,10))\nax.bar(top_10_contributors.index, top_10_contributors.values)\nax.set_xlabel(\"Authors\")\nax.set_ylabel(\"N° of contributions\")\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n         rotation_mode=\"anchor\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:30.105223Z","iopub.execute_input":"2021-08-19T13:43:30.1056Z","iopub.status.idle":"2021-08-19T13:43:30.34088Z","shell.execute_reply.started":"2021-08-19T13:43:30.105564Z","shell.execute_reply":"2021-08-19T13:43:30.339782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, Michael Jordan is by far the author who contributed the most. A quick research in the data shows he works for UC Berkeley among the publications he figures in.\n\nThe same goes for the rest of the top 10: they all work at prestigious schools and corporations like Cambridge, Montreal or Google.","metadata":{}},{"cell_type":"code","source":"top_10_auth_inst = set([inst for author in top_10_contributors.index for inst in no_na_authors[no_na_authors.full_name == author].institution.unique().tolist()])\ntop_10_auth_inst = list(top_10_auth_inst)\n\nprint(\"List of institutions from the top 10:\", top_10_auth_inst)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:30.342423Z","iopub.execute_input":"2021-08-19T13:43:30.342879Z","iopub.status.idle":"2021-08-19T13:43:30.383518Z","shell.execute_reply.started":"2021-08-19T13:43:30.342837Z","shell.execute_reply":"2021-08-19T13:43:30.382456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_10_institutions = no_na_authors.institution.value_counts().head(11)\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10,10))\n\nax1.bar([\"Not referenced\", \"Referenced\"],[top_10_institutions.values[0], top_10_institutions.values[1:].sum()])\nax1.set_xlabel(\"Institutions\")\nax1.set_ylabel(\"N° of authors\")\n\nax2.bar(top_10_institutions.index[1:], top_10_institutions.values[1:])\nax2.set_xlabel(\"Institutions\")\nax2.set_ylabel(\"N° of authors\")\nplt.setp(ax2.get_xticklabels(), rotation=45, ha=\"right\",\n         rotation_mode=\"anchor\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:30.385307Z","iopub.execute_input":"2021-08-19T13:43:30.385804Z","iopub.status.idle":"2021-08-19T13:43:30.658255Z","shell.execute_reply.started":"2021-08-19T13:43:30.385757Z","shell.execute_reply":"2021-08-19T13:43:30.657288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The top 10 of institutions seems to corroborate that fact.","metadata":{}},{"cell_type":"markdown","source":"# NeurIPS Papers\n\nEach paper so far has a title, a year of publication and a text content. The first thing we can notice so far is how fast interest for Deep Learning research evolves between yearly congresses:","metadata":{}},{"cell_type":"code","source":"papers_per_year = no_na_papers.year.value_counts()\n\nfig, ax = plt.subplots(figsize=(10, 10))\nax.scatter(papers_per_year.index, papers_per_year.values)\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"Number of publications\")\nplt.plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:30.661533Z","iopub.execute_input":"2021-08-19T13:43:30.66185Z","iopub.status.idle":"2021-08-19T13:43:30.859777Z","shell.execute_reply.started":"2021-08-19T13:43:30.661821Z","shell.execute_reply":"2021-08-19T13:43:30.858996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The graph reveals the interest for deep learning grows slightly at first, then rises fastly in the second part of the 10s, most likely because ML and data science generated a hype wave thanks to the biggest accomplishments from neural networks.\n\nOut of interest, we can pick the most recent article Michael Jordan wrote or contributed to:","metadata":{}},{"cell_type":"code","source":"first_article = no_na_authors[no_na_authors.full_name == \"Michael Jordan\"].join(no_na_papers.set_index('source_id'), \n                                                                                how='inner', on='source_id').sort_values('year', ascending=False).head(1)\nfirst_article","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:30.860831Z","iopub.execute_input":"2021-08-19T13:43:30.861218Z","iopub.status.idle":"2021-08-19T13:43:30.892388Z","shell.execute_reply.started":"2021-08-19T13:43:30.86119Z","shell.execute_reply":"2021-08-19T13:43:30.891711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There's several articles from 2019. Let's pick this one and see how the title and full text are. The text is quite long so let's limit it to 5000 characters.","metadata":{}},{"cell_type":"code","source":"title = first_article.title.values[0]\ntext = first_article.full_text.values[0]\n\nprint(title)\nprint(text[:2500], \"...\", text[-2500:],sep='\\n')","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:30.893311Z","iopub.execute_input":"2021-08-19T13:43:30.893738Z","iopub.status.idle":"2021-08-19T13:43:30.899815Z","shell.execute_reply.started":"2021-08-19T13:43:30.893709Z","shell.execute_reply":"2021-08-19T13:43:30.898528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to the description, the dataset was scraped from the HTML version, thus explaining why the text has more spaces than needed. On the other hand, it contains a plethora of informations:\n\n* The title\n* Details about the authors\n* The abstract\n* The different sections of an article, from the introduction to the conclusion\n* A list of references\n\nIn a scientific article, the title and the abstract are the most important parts of the discovery of a topic. An abstract must contain a brief summary of the article describing the problematic, the used method, the dataset, and a summary of the final results. Meanwhile, a title is a way to promote the article using **keywords**.","metadata":{}},{"cell_type":"code","source":"kw_sample = [word.lower().replace(\":\", \"\") for word in title.split() if word.lower() != \"of\"]\nkw_sample","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:30.901031Z","iopub.execute_input":"2021-08-19T13:43:30.901508Z","iopub.status.idle":"2021-08-19T13:43:30.913373Z","shell.execute_reply.started":"2021-08-19T13:43:30.901478Z","shell.execute_reply":"2021-08-19T13:43:30.912343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sorting keywords\n\nUsing just a word counter, we can encapsulate in a dataframe the number of relevant words per title. First we're going to determine the word quantity among these documents.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer()\ntitle_vectorized = vectorizer.fit_transform(no_na_papers.title)\n\nfeature_count = pd.DataFrame({\n    \"feat_name\": vectorizer.get_feature_names(), \n    \"feat_count\": title_vectorized.toarray().sum(axis=0).tolist()\n})\n\nfeature_count.sort_values(by='feat_count', ascending=False).head(20)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:30.914841Z","iopub.execute_input":"2021-08-19T13:43:30.915126Z","iopub.status.idle":"2021-08-19T13:43:31.303253Z","shell.execute_reply.started":"2021-08-19T13:43:30.9151Z","shell.execute_reply":"2021-08-19T13:43:31.302038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 10))\n\n# Only limit the plot to the nth entry\nn_entries = 75\n\nvals_to_display = feature_count.sort_values(by='feat_count', ascending=False).head(n_entries)\n\nax2.bar(x=vals_to_display.feat_name.values, height=vals_to_display.feat_count.values)\n\n# define a wordcloud\nwc = WordCloud(background_color=\"white\")\nwc.generate(\" \".join(vectorizer.get_feature_names()))\n\nax1.imshow(wc, interpolation=\"bilinear\")\nax1.axis(\"off\")\n\nplt.setp(ax2.get_xticklabels(), rotation=45, ha=\"right\",rotation_mode=\"anchor\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:31.304604Z","iopub.execute_input":"2021-08-19T13:43:31.304908Z","iopub.status.idle":"2021-08-19T13:43:32.727624Z","shell.execute_reply.started":"2021-08-19T13:43:31.30488Z","shell.execute_reply":"2021-08-19T13:43:32.726713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the most common terms come from either stop words or particular terms from Deep Learning. In the top 10, we can see, aside from stop words, that words like \"deep\", \"neural\" and \"networks\" often come by in the words. One can deduce, according to the thematic, that these can form the following words :\n\n* Neural Networks\n* Deep Neural Networks\n* Neural Networks model\n* etc...\n\nAs such, it's more appropriate to use a 2 or 3-gram count vectorizer.","metadata":{}},{"cell_type":"code","source":"feature_count = {\n    \"feat_name\": [],\n    \"feat_count\": [],\n    \"feat_n_gram\": []\n}\n\ntitle_sw = [\"for\", \"of\", \"and\", \"with\", \"in\", \"the\", \"to\", \"on\", \"from\", \"via\", \"by\", \"an\"]\n\nfor i_gram in range(1,4):\n    vectorizer = CountVectorizer(analyzer='word', ngram_range=(i_gram, i_gram), stop_words=title_sw)\n    title_vectorized = vectorizer.fit_transform(no_na_papers.title)\n\n    feature_count[\"feat_name\"].extend(vectorizer.get_feature_names())\n    feature_count[\"feat_count\"].extend(title_vectorized.toarray().sum(axis=0).tolist())\n    feature_count[\"feat_n_gram\"].extend([i_gram] * len(vectorizer.get_feature_names()))\n    \nfeature_count = pd.DataFrame(feature_count)\nfeature_count.sort_values(by='feat_count', ascending=False).head(20)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:32.728937Z","iopub.execute_input":"2021-08-19T13:43:32.729229Z","iopub.status.idle":"2021-08-19T13:43:36.08239Z","shell.execute_reply.started":"2021-08-19T13:43:32.729199Z","shell.execute_reply":"2021-08-19T13:43:36.081488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 10))\ntab_colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', \n              'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n\n# Only limit the plot to the nth entry\nn_entries = 25\n\nvals_to_display = feature_count.sort_values(by='feat_count', ascending=False).head(n_entries)\n\nax1.bar(x=vals_to_display.feat_name.values, height=vals_to_display.feat_count.values, \n        color=vals_to_display.feat_n_gram.apply(lambda i_gram: tab_colors[i_gram-1]).values)\n\nfeature_count.feat_n_gram.value_counts(ascending=True).plot.pie(ax=ax2)\n\nplt.setp(ax1.get_xticklabels(), rotation=45, ha=\"right\",rotation_mode=\"anchor\")\n\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:36.083609Z","iopub.execute_input":"2021-08-19T13:43:36.083924Z","iopub.status.idle":"2021-08-19T13:43:36.455762Z","shell.execute_reply.started":"2021-08-19T13:43:36.083897Z","shell.execute_reply":"2021-08-19T13:43:36.454945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code so far shows a higher proportion of 3 n-grams (more specific terms) and a more frequent proportion of single words terms. The trick with scientific articles is estimating the right number of n-gram words, as terms in research can get very specific, especially when it comes to presenting a variation of an already practiced method. Some titles can use acronyms but these are rare.","metadata":{}},{"cell_type":"code","source":"for i_gram in range(1,4):\n    top_50 = feature_count[feature_count.feat_n_gram == i_gram].sort_values(by='feat_count', ascending=False).head(50)\n    print(f\"Top 20 of the most common {i_gram}-gram words:\\n{', '.join(top_50.feat_name.tolist())}\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:36.456786Z","iopub.execute_input":"2021-08-19T13:43:36.457157Z","iopub.status.idle":"2021-08-19T13:43:36.485689Z","shell.execute_reply.started":"2021-08-19T13:43:36.457131Z","shell.execute_reply":"2021-08-19T13:43:36.484504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the top 50 words, we can deduce that a 1-gram research regroups common scientific terms like \"kernel\", \"variational\" or \"reinforcment\", without specifying exactly the kind of topic we would find, while an increasing number of n-grams would narrow down the topic. \n\nWe can see that selecting \"deep reinforcment learning\" would lead us to articles talking about Q-learning and it's variants, while \"reinforcement\" could not only mean \"reinforcment learningé, but imply that a DL model can be reinforced.\n\nWe can always check the diversity of 3-gram words per year, which would show a constant evolution of the Deep Learning field:","metadata":{}},{"cell_type":"code","source":"vectorizer = CountVectorizer(analyzer='word', ngram_range=(3, 3), stop_words=title_sw)\n\nX = vectorizer.fit_transform(no_na_papers.title)\n\ndiversity_papers = no_na_papers.loc[:, (\"title\", \"year\")]\n\ndiversity_papers[\"n_themes\"] = diversity_papers.index.values\ndiversity_papers[\"n_themes\"] = diversity_papers[\"n_themes\"].apply(lambda row: np.nonzero(X[row,:])[1].tolist())\n\ndiversity_papers.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:36.487116Z","iopub.execute_input":"2021-08-19T13:43:36.48749Z","iopub.status.idle":"2021-08-19T13:43:38.155643Z","shell.execute_reply.started":"2021-08-19T13:43:36.487445Z","shell.execute_reply":"2021-08-19T13:43:38.154692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yr_div_evolution = diversity_papers.groupby('year').sum().n_themes.apply(lambda row: len(list(set(row))))\nfig, ax = plt.subplots(figsize=(12, 6))\n\nyr_div_evolution.plot(ax=ax)\nax.set_title(\"Evolution of theme diversity through the years\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:38.158617Z","iopub.execute_input":"2021-08-19T13:43:38.159233Z","iopub.status.idle":"2021-08-19T13:43:38.390843Z","shell.execute_reply.started":"2021-08-19T13:43:38.159197Z","shell.execute_reply":"2021-08-19T13:43:38.389735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As expected, as research in Deep Learning goes, the further the knowledge of the field gets refined. Notice the exponential growth starting from 2015 onwards, when AI is becoming a hype trend.\n\nWe can also observe the increase and decrease in themes according to the year, when desired words started to appear. Let's try with \"graph neural networks\":","metadata":{}},{"cell_type":"code","source":"def trace_word_popularity(word):\n    is_word_in_list = np.array([feat==word for feat in vectorizer.get_feature_names()])\n    if not is_word_in_list.any():\n        raise Exception(\"Couldn't find the word you were looking for.\")\n        \n    if len(word.split()) != 3:\n        raise Exception(\"Must be a 3 word long token separated by spaces.\")\n        \n    word_idx = np.argwhere(is_word_in_list).item()\n    count_df = (diversity_papers.groupby('year').sum().n_themes\n                .apply(lambda row: np.array(row))\n                .apply(lambda row: row[row == word_idx].size))\n    fig, ax = plt.subplots(figsize=(12, 6))\n\n    count_df.plot(ax=ax)\n    ax.set_title(f\"Evolution of '{word}' through the years\")\n    plt.show()\n    \n\n\ntest_word = \"graph neural networks\"\ntrace_word_popularity(test_word)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:38.394946Z","iopub.execute_input":"2021-08-19T13:43:38.395254Z","iopub.status.idle":"2021-08-19T13:43:38.652094Z","shell.execute_reply.started":"2021-08-19T13:43:38.395225Z","shell.execute_reply":"2021-08-19T13:43:38.65101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nothing before 2016. Which makes sense since GNNs are a recent deep learning neural network.","metadata":{}},{"cell_type":"code","source":"from matplotlib import cm\nfrom matplotlib.colors import LinearSegmentedColormap, ListedColormap\n\ndef make_segmented_cmap(cmap_name, n_segments):\n    cmap = cm.get_cmap(cmap_name, n_segments)\n    if isinstance(cmap, LinearSegmentedColormap):\n        cmap = cmap(range(n_segments))\n    else:\n        cmap = cmap.colors\n    return cmap\n\ndef words_trend_3_gram(words, cmap_name='autumn'):\n    cmap = make_segmented_cmap(cmap_name, len(words))\n    fig, ax = plt.subplots(figsize=(12, 6))\n    for idx, word in enumerate(words):\n        is_word_in_list = np.array([feat==word for feat in vectorizer.get_feature_names()])\n        if not is_word_in_list.any():\n            raise Exception(\"Couldn't find the word you were looking for.\")\n\n        if len(word.split()) != 3:\n            raise Exception(\"Must be a 3 word long token separated by spaces.\")\n\n        word_idx = np.argwhere(is_word_in_list).item()\n        count_df = (diversity_papers.groupby('year').sum().n_themes\n                    .apply(lambda row: np.array(row))\n                    .apply(lambda row: row[row == word_idx].size))\n        \n        count_df.plot(ax=ax, color=cmap[idx], label=word)\n        \n    ax.set_title(f\"Evolution of {len(words)} terms through the years\")\n    ax.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\n    plt.show()\n\n# testing on the previous top_50:\nwords_trend_3_gram(top_50.feat_name.tolist()[:15], cmap_name='copper')","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:38.65367Z","iopub.execute_input":"2021-08-19T13:43:38.65395Z","iopub.status.idle":"2021-08-19T13:43:40.727601Z","shell.execute_reply.started":"2021-08-19T13:43:38.653922Z","shell.execute_reply":"2021-08-19T13:43:40.726395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the top 15 it's hard to deduce and compare individually how each term fares compared to each other. There doesn't seem to be a decline in a thematic or another. However, the most popular term, \"deep neural networks\", is on the rise. Mostly because it's the main thematic of NeurIPS papers...","metadata":{}},{"cell_type":"markdown","source":"# Analyzing paper contents\n\nLast but not least, we'll try and decipher the contents of the full papers within the CSV file. The first thing I'm interested in is seeing the character range between all these papers.","metadata":{}},{"cell_type":"code","source":"# Cleaning\npaper_length = no_na_papers.full_text.apply(lambda text: len(text))\n\nprint(f\"Mean number of characters: {paper_length.describe().loc['mean']:.2f}\")\nprint(f\"The number of characters goes from {paper_length.describe().loc['min']:.0f}\",\n      f\"to {paper_length.describe().loc['max']:.0f}\")\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,6))\n\nax1.boxplot(paper_length.values, 0, 'rs', 0)\nax1.set_title(\"Boxplot of full text length\")\n\nax2.boxplot(paper_length.values, 0, '', 0)\nax2.set_title(\"Boxplot of full text length\\n(No outliers)\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:40.729362Z","iopub.execute_input":"2021-08-19T13:43:40.729808Z","iopub.status.idle":"2021-08-19T13:43:40.953847Z","shell.execute_reply.started":"2021-08-19T13:43:40.729767Z","shell.execute_reply":"2021-08-19T13:43:40.952926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is very strange indeed! The shortest text is 1 character long, while the longest ones exceed 100k characters. If you remove the outliers as indicated by the boxplot, the average article is around 25-35k characters, which is consistent with the average Arxiv article.","metadata":{}},{"cell_type":"markdown","source":"## Filter outliers\n\nThe shortest articles are less than 10k characters. Let's analyze the top 10 of the shortest texts:","metadata":{}},{"cell_type":"code","source":"for i, row in paper_length.sort_values(ascending=True).head(10).iteritems():\n    print(f\"Text n°{i}, {row} characters\")\n    print(no_na_papers.full_text.iloc[i])","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:40.954984Z","iopub.execute_input":"2021-08-19T13:43:40.955266Z","iopub.status.idle":"2021-08-19T13:43:40.965332Z","shell.execute_reply.started":"2021-08-19T13:43:40.955238Z","shell.execute_reply":"2021-08-19T13:43:40.963737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the data is either missing or is outright saying gibberish. Probably as a result of the metadata missing or the request failing for a mysterious reason (eg. the hashes aren't registered). I won't display more for space reasons , but if you show a full text at random, you can see some of the shortest documents in the top 20 are indeed real documents that are just slightly shorter than the average article.\n\nAnother thing that I've noticed, especially if you open large articles, is the amount of whitespaces left by the text, which results in a disproportionnate number of characters. Try opening the largest article and see that it has a lot of whitespace in some of its parts. My guess is the metadata couldn't constrain $\\LaTeX$ expressions and tables. Figures could also be the result. \n\nIn some cases, it's most likely due to the paper's styling. Some papers put extra tabs and spaces to delimit author names and affiliations in example. Another explanation is extra space being left for the columns, although Arxiv papers hardly ever do this.","metadata":{}},{"cell_type":"code","source":"# Whitespace ratio\nimport re\n\ndef whitespace_ratio(text: str, verbose: bool = False) -> float:\n    \"\"\"\n    Takes a text and estimates the number of whitespaces over the\n    number of characters. Uses the common token `\\s` for estimation.\n    \"\"\"\n    len_text = len(text)\n    count_whitespaces = 0\n    \n    for match in re.finditer(r'\\s', text, re.MULTILINE):\n        count_whitespaces += (match.end() - match.start())\n        \n    if verbose:\n        print(f\"Total whitespaces: {count_whitespaces} / {len_text} ({count_whitespaces/len_text:.2%})\")\n        \n    return count_whitespaces / len_text\n\nlongest_paper = no_na_papers.full_text.iloc[paper_length.sort_values(ascending=True).tail(1).index.item()]\n\n_ = whitespace_ratio(longest_paper, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:40.966836Z","iopub.execute_input":"2021-08-19T13:43:40.967406Z","iopub.status.idle":"2021-08-19T13:43:41.37424Z","shell.execute_reply.started":"2021-08-19T13:43:40.967362Z","shell.execute_reply":"2021-08-19T13:43:41.373046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the largest paper so far is just blank space. Only 1.5% of the document is actually exploitable text, that is, if you don't count the equations.\n\n~~*Note: the following cell might take a while to run due to the iterative process. Statistics made with `tqdm` estimate 1:40h of calculations. The operation is using multiprocessing calculations to get the job done. If you want to reproduce the same stepw ith your CPU, you can use `os.cpu_count()` on your machine and change `n_cores`.*~~\n\n**ERRATUM:** The miscalculation might be due to an old variable being stuck. You can use apply but I'll let the multiprocess method just to show how you can clean up large walls of text in case you need more power.","metadata":{}},{"cell_type":"code","source":"from typing import Callable\nfrom multiprocessing import Pool\nfrom functools import partial\n\n# Function to parallelize the series\n# Many thanks to Rahul Agarwal for the tip\n# https://towardsdatascience.com/make-your-own-super-pandas-using-multiproc-1c04f41944a1\ndef parallelize_series(s: pd.Series, func: Callable, n_cores: int = 4) -> pd.Series:\n    \"\"\"\n    Uses Kaggle's CPU cores to divide the work and accelerate the process.\n    \"\"\"\n    s_split = np.array_split(s, n_cores)\n    pool = Pool(n_cores)\n    s = pd.concat(pool.map(func, s_split))\n    pool.close()\n    pool.join()\n    return s\n\n# Using this step because `Pool.map` doesn't picklize lambdas\ndef transform_series(s: pd.Series, f: Callable) -> pd.Series:\n    \"\"\"\n    Calls pandas Series' apply method.\n    \"\"\"\n    return s.apply(f)\n\n%time paper_ws_ratio = parallelize_series(no_na_papers.full_text, partial(transform_series, f=whitespace_ratio), n_cores = os.cpu_count())\n\n# Then we sort the values to show the results\npaper_ws_ratio.sort_values().tail(20)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:41.375463Z","iopub.execute_input":"2021-08-19T13:43:41.375748Z","iopub.status.idle":"2021-08-19T13:43:55.628009Z","shell.execute_reply.started":"2021-08-19T13:43:41.375721Z","shell.execute_reply":"2021-08-19T13:43:55.626679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see the correlation between the number of whitespaces and the number of characters. I expect a positive correlation as the longest papers are full of white spaces.","metadata":{}},{"cell_type":"code","source":"interm_result = pd.concat([paper_length.rename('length'), paper_ws_ratio.rename('ws_ratio')], axis=1).corr()\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 14))\n\nax1.scatter(paper_length.values, paper_ws_ratio.values, alpha=0.3)\nax1.plot([0, 1], [0, 1], transform=ax1.transAxes, color='red')\nax1.set_xscale('log')\nax1.set_yscale('log')\nax1.set_title(\"Plot paper length against whitespace ratio\")\nax1.set_xlabel(\"Paper length (characters)\")\nax1.set_ylabel(\"Whitespace ratio\")\n\nax2.imshow(interm_result, cmap='copper')\nax2.set_xticks(np.arange(interm_result.shape[1]))\nax2.set_yticks(np.arange(interm_result.shape[1]))\nax2.set_xticklabels(interm_result.columns.values)\nax2.set_yticklabels(interm_result.columns.values)\nfor i in range(interm_result.shape[1]):\n    for j in range(interm_result.shape[1]):\n        text = ax2.text(j, i, interm_result.iloc[i, j].round(4), fontsize=14,\n                       ha=\"center\", va=\"center\", color=\"w\")\nax2.set_title(\"Correlation matrix\")\n\nax3.boxplot(paper_length.values, 0, 'rs', 0)\nax3.set_title(\"Distribution of paper length\")\n\nax4.boxplot(paper_ws_ratio.values, 0, 'rs', 0)\nax4.set_title(\"Distribution of whitespace ratio\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:55.629516Z","iopub.execute_input":"2021-08-19T13:43:55.629892Z","iopub.status.idle":"2021-08-19T13:43:56.834047Z","shell.execute_reply.started":"2021-08-19T13:43:55.629855Z","shell.execute_reply":"2021-08-19T13:43:56.832793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The correlation coefficient doesn't show a linear tendency between the whitespace ratio and the paper length. Instead, it looks like the scatter plot reveals two clusters in the distribution where the link between paper length and whitespace ratio is above the norm. There's also a small cluster under the red line where the whitespace ratio is neglectible, indicating a compact paper. \n\nThe boxplots indicate the whitespace ratio is around 18% and spreads between 15 and 20%, which coincides with the diameter of the blob we see in the top-left graph.","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\npaper_ws = pd.concat([paper_length.rename('length'), paper_ws_ratio.rename('ws_ratio')], axis=1)\nkmeans = KMeans(n_clusters=2, random_state=1).fit(paper_ws.apply(np.log))\n\nfig, ax = plt.subplots(figsize=(10, 10))\npaper_ws['clusters'] = kmeans.labels_\nkmeans_cmap  = ['tab:blue', 'tab:orange']\n\nfor c in range(2):\n    ax.scatter(paper_ws[paper_ws.clusters == c].length, paper_ws[paper_ws.clusters == c].ws_ratio, \n               color=kmeans_cmap[c], alpha=0.25, label=f\"Label {c+1}\")\n\nax.scatter(np.exp(kmeans.cluster_centers_)[:,0], np.exp(kmeans.cluster_centers_)[:,1], marker='x', color='red')\n    \nax.plot([0, 1], [0, 1], transform=ax.transAxes, color='red')\nax.set_xscale('log')\nax.set_yscale('log')\nax.set_title(\"Plot paper length against whitespace ratio\")\nax.set_xlabel(\"Paper length (characters)\")\nax.set_ylabel(\"Whitespace ratio\")\n\nax.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:56.835731Z","iopub.execute_input":"2021-08-19T13:43:56.836143Z","iopub.status.idle":"2021-08-19T13:43:58.124447Z","shell.execute_reply.started":"2021-08-19T13:43:56.836107Z","shell.execute_reply":"2021-08-19T13:43:58.123489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compact_text = no_na_papers.iloc[paper_ws[paper_ws.clusters == 1].index].full_text.head(5)\n\nfor _, row in compact_text.iteritems():\n    print(row[:1000], '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:58.125705Z","iopub.execute_input":"2021-08-19T13:43:58.12597Z","iopub.status.idle":"2021-08-19T13:43:58.136522Z","shell.execute_reply.started":"2021-08-19T13:43:58.125944Z","shell.execute_reply":"2021-08-19T13:43:58.135437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Would you look at that! It seems not only some texts have exclusively white text, but others are at the opposite end of the spectrum: too compact. \n\nLuckily, the following cell estimates they're in the minority:","metadata":{}},{"cell_type":"code","source":"n_clusters, n_cluster_counts = np.unique(paper_ws.clusters, return_counts=True)\n\nfor cluster, c_count in zip(n_clusters, n_cluster_counts):\n    print(f\"Number of elements from cluster {cluster+1}: {c_count} ({c_count / no_na_papers.shape[0]:.2%})\")","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:58.137864Z","iopub.execute_input":"2021-08-19T13:43:58.138194Z","iopub.status.idle":"2021-08-19T13:43:58.147975Z","shell.execute_reply.started":"2021-08-19T13:43:58.138157Z","shell.execute_reply":"2021-08-19T13:43:58.147025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Clean whitespaces\n\nOnly 0.6% of these texts are the ones where the spaces are missing. Excluding them with the first few entries where the full text is unavailable won't damage the analysis. But what about the other category where there's too much spaces?\n\nWe have to find a way to replace the spaces without damaging the readability. Here's my strategy:\n\n* Replace tabs and spaces by a single space\n* Keep the newline characters `\\n` but always regroup them so we don't have a spaced out text\n\nSounds like a reasonable strategy.","metadata":{}},{"cell_type":"code","source":"print(\"Raw paper\")\nprint(longest_paper[:2000])\n\nprint(f\"\\n{'Replace whitespaces by spaces':=^120}\\n\")\nlp_no_spaces = re.sub(r\"[ \\t\\f\\r]+\", \" \", longest_paper)\nprint(lp_no_spaces[:2000])\n\nprint(f\"\\n{'Keep only one newline':=^120}\\n\")\nlp_clean = re.sub(r\"\\n+\", \"\\n\", lp_no_spaces)\nprint(lp_clean[:2000])\n\nprint(f\"\\n{'':=^120}\\n{len(longest_paper)} at the beginning, {len(lp_clean)} at the end!\")\nprint(f\"{(len(longest_paper) - len(lp_clean))/len(longest_paper):.2%} removed!\")","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:58.149194Z","iopub.execute_input":"2021-08-19T13:43:58.149495Z","iopub.status.idle":"2021-08-19T13:43:58.172334Z","shell.execute_reply.started":"2021-08-19T13:43:58.149456Z","shell.execute_reply":"2021-08-19T13:43:58.17157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There we go! The text looks much more clean so far. Let's try that cleaning method on the whole dataset.","metadata":{}},{"cell_type":"code","source":"def clean_whitespace(text: str) -> str:\n    \"\"\"\n    Removes the whitespace on the text\n    \"\"\"\n    txt_no_spaces = re.sub(r\"[ \\t\\f\\r]+\", \" \", text)\n    return re.sub(r\"\\n+\", \"\\n\", txt_no_spaces)\n\nfull_text_clean = no_na_papers.full_text.apply(clean_whitespace)\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16,6))\n\nax1.boxplot(full_text_clean.apply(lambda text: len(text)).values, 0, 'rs', 0)\nax1.set_title(\"Boxplot of full text length (clean)\")\n\nax2.boxplot(paper_length.values, 0, 'rs', 0)\nax2.set_title(\"Boxplot of full text length\")\n      \nax3.boxplot(full_text_clean.apply(lambda text: len(text)).values, 0, '', 0)\nax3.set_title(\"Boxplot of full text length (clean)\\nNo outliers\")\n\nax4.boxplot(paper_length.values, 0, '', 0)\nax4.set_title(\"Boxplot of full text length\\nNo outliers\")\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:43:58.173379Z","iopub.execute_input":"2021-08-19T13:43:58.173631Z","iopub.status.idle":"2021-08-19T13:44:23.791222Z","shell.execute_reply.started":"2021-08-19T13:43:58.173606Z","shell.execute_reply":"2021-08-19T13:44:23.790468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distribution hasn't really shifted but at least the worst outliers were readjusted.","metadata":{}},{"cell_type":"markdown","source":"## Detecting gibberish\n\nSo far we managed to clean the spaces and detect compact text. Now what about badly parsed text? Despite every article being read and understood with Python's `print()` function, there can be instances of badly parsed text, like the 10th shortest text including only numbers and punctuation.\n\nOne strategy would be to take any non-whitespace character, then evaluate the proportion between numbers and punctuation, all over the clean text.","metadata":{}},{"cell_type":"code","source":"from typing import Tuple\nimport string\n\ndata = {\n    \"total\": [],\n    \"n_digits\": [],\n    \"n_punct\": []\n}\n\ndef char_proportion(text: str) -> Tuple[int, int, int]:\n    \"\"\"\n    Returns a tuple with the following\n    - Number of non-space characters\n    - Number of digits\n    - Number of punctuation characters\n    \"\"\"\n    count_chrs = 0\n    count_digit = 0\n    count_punct = 0\n    \n    # Pattern for punctuation with Python's string module\n    punct_patt = re.compile(fr\"[{string.punctuation}]+\", re.M)\n    \n    for match in re.finditer(r'\\S+', text, re.MULTILINE):\n        count_chrs += (match.end() - match.start())\n        non_space_txt = match.group()\n        for dig_match in re.finditer(r'\\d+', non_space_txt, re.MULTILINE):\n            count_digit += (dig_match.end() - dig_match.start())\n        \n        for punct_match in punct_patt.finditer(non_space_txt):\n            count_punct += (punct_match.end() - punct_match.start())\n        \n        \n    return (count_chrs, count_digit, count_punct)\n\n\nfor _, row in full_text_clean.apply(char_proportion).iteritems():\n    tot, n_dig, n_pun = row\n    data[\"total\"].append(tot)\n    data[\"n_digits\"].append(n_dig)\n    data[\"n_punct\"].append(n_pun)\n\nchr_prop = pd.DataFrame(data)\nchr_prop['n_other'] = chr_prop.total - (chr_prop.n_digits + chr_prop.n_punct)\n\n\nfig, ax = plt.subplots(figsize=(10, 10))\nchr_prop[['n_digits', 'n_punct', 'n_other']].sum().plot.pie(ax=ax)\nax.set_ylabel(\"Proportion of digits and punctuation vs other characters\")\nplt.plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:44:23.792422Z","iopub.execute_input":"2021-08-19T13:44:23.792858Z","iopub.status.idle":"2021-08-19T13:46:55.782137Z","shell.execute_reply.started":"2021-08-19T13:44:23.792815Z","shell.execute_reply":"2021-08-19T13:46:55.781046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 6))\n\nax1.hist((chr_prop.n_digits / chr_prop.total).mul(100).values, bins=200)\nax1.set_title(\"Histogram of digit-character ratio\")\nax1.set_xlabel(\"Value (%)\")\nax1.set_ylabel(\"Intensity\")\nax1.set_xlim(0, 100)\n\nax2.hist((chr_prop.n_punct / chr_prop.total).mul(100).values, bins=200)\nax2.set_title(\"Histogram of punctuation-character ratio\")\nax2.set_xlabel(\"Value (%)\")\nax2.set_ylabel(\"Intensity\")\nax2.set_xlim(0, 100)\n\nax3.hist(((chr_prop.n_punct + chr_prop.n_digits) / chr_prop.total).mul(100).values, bins=200)\nax3.set_title(\"Histogram of letter vs not-letter ratio\")\nax3.set_xlabel(\"Value (%)\")\nax3.set_ylabel(\"Intensity\")\nax3.set_xlim(0, 100)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:46:55.783767Z","iopub.execute_input":"2021-08-19T13:46:55.784161Z","iopub.status.idle":"2021-08-19T13:46:57.397626Z","shell.execute_reply.started":"2021-08-19T13:46:55.784123Z","shell.execute_reply":"2021-08-19T13:46:57.396569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like the proportion is relatively reasonable (the distribution is centered on 8-10% for each). Which means most of the dataset is relatively clean. Counting the portion where digits and punctuations combined exceed 25% will only display 26 entries. Let's see hwo they look like.","metadata":{}},{"cell_type":"code","source":"chr_no_chr_prop = ((chr_prop.n_punct + chr_prop.n_digits) / chr_prop.total)\nfor _, txt in full_text_clean.iloc[chr_no_chr_prop[chr_no_chr_prop > 0.25].index].head(5).iteritems():\n    print(txt[:20], '\\n...\\n', txt[-20:], '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:46:57.399334Z","iopub.execute_input":"2021-08-19T13:46:57.399767Z","iopub.status.idle":"2021-08-19T13:46:57.411581Z","shell.execute_reply.started":"2021-08-19T13:46:57.399721Z","shell.execute_reply":"2021-08-19T13:46:57.410498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, some of this text is unintelligible. Using everything we applied so far, we can set up our full dtataset of cleaned full text with\n\n* Removing the shortest entries (total length under 1000 characters)\n* Removing the compact text (no space at all)\n* Removing the unintelligible entries (non-letter/letter ratio over 25%)","metadata":{}},{"cell_type":"code","source":"# Next delete the compact text and the first entries (<1500 characters)\nshortest_papers_idx = paper_length[paper_length < 1500].index\ncompact_text_idx = paper_ws[paper_ws.clusters == 1].index\ngibberish_idx = chr_no_chr_prop[chr_no_chr_prop > 0.25].index\n\nidx_to_drop = list(set(shortest_papers_idx.to_list() + compact_text_idx.to_list() + gibberish_idx.to_list()))\nidx_to_drop = pd.Index(idx_to_drop)\n\nfull_text_final = full_text_clean.drop(idx_to_drop, axis=0)\nprint(f\"{idx_to_drop.size} of {full_text_clean.shape[0]} entries removed\")","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:46:57.412937Z","iopub.execute_input":"2021-08-19T13:46:57.413264Z","iopub.status.idle":"2021-08-19T13:46:57.425147Z","shell.execute_reply.started":"2021-08-19T13:46:57.413233Z","shell.execute_reply":"2021-08-19T13:46:57.42405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like we only filtered a small amount of papers, which is good for an eventual study.","metadata":{}},{"cell_type":"markdown","source":"# spaCy visualization\n\nLast but not least, let's try to visualize how well the spaCy library fares over one of those texts. ","metadata":{}},{"cell_type":"code","source":"from spacy import displacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(full_text_final.sample(n=1).item())\ndisplacy.render(doc[:1500] , style=\"ent\", jupyter=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T13:51:58.55196Z","iopub.execute_input":"2021-08-19T13:51:58.552582Z","iopub.status.idle":"2021-08-19T13:52:00.308942Z","shell.execute_reply.started":"2021-08-19T13:51:58.552543Z","shell.execute_reply":"2021-08-19T13:52:00.307938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So far, spaCy's English module manages to find persons and orgs, but loses itself in-between equation residuals and number references. So far, rearranging equations from raw text is easier said than done, mostly because we don't know how they were originally written, and because we don't have the $\\TeX$ source code, which could've helped us generate a SVG and put aside as a figure.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}