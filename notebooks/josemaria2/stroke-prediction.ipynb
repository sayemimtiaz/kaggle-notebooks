{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Purpose of this analysis:\n\n#### The purpose of this analysis is to find the best model that better explains the relationshid between:\n   #### - Explanatory varialbes: lifestyle and health\n   #### - Dependent variable: suffered / not suffered a stroke\n\n#### In the first section I load basic libraries as well as de dataset of this exercise. In the second one, I perform an EDA in order to gain insight about the variables conforming the dataset. Finally, cross-validate some models in order to select the top performer."},{"metadata":{},"cell_type":"markdown","source":"## 1. Import of basic libraries and dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sbn\nsbn.set_style(\"dark\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv(r'/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. EDA"},{"metadata":{},"cell_type":"markdown","source":"### 1. Non graphical analysis:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### From the previous output we can see that there are some missing values for the variable bmi. Thus,such values will be replaced later on, after creating some variables that will be helpful for that purpose. \n\n#### Second, I check if there are duplicated rows. For that purpose, I use the following code that looks for them and keeps the first of all repeated elements (I use the unique() clause to make sure there isn't any after running the script)"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.duplicated(subset = None,keep = \"first\").unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Third, I use the .describe() function to explore the properties of the initial variables:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### From the description of the dataset, we can spot several facts: \n1. We can drop id variable since it only serves as record identifier.\n2. Hypertension, heart_disease are actually binary variables, but they are recorded in the dataset as integers (with values 0,1)\n3. We have to further analyse age, avg_glucose_level and bmi since there is an important difference between 75% percentile and maximum value. It could lead to extreme values.\n4. Age, avg_glucose_level and bmi are likely normally distributed variables.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.drop([\"id\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### After checking the numerical properties of the variables, I want to know the different values the categorical variables can adopt. For that purpose, I write an easy for-loop that skip numerical columns and shows the different options:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in dataset.columns:\n    if any(col in value for value in [\"id\", \"age\", \"avg_glucose_level\", \"bmi\"]):\n        pass   \n    else:\n        print(\"Column {}. Unique values: {}\".format(col, dataset[col].unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### From the previous output we can see the different values the non-numerical varialbes can adopt. \n#### Regarding smoking_status, there are two values that might have similar behaviour: formerly smoked and smokes. Nevertheless, this is only an ex-ante appreciation that should be further investigated.\n#### In addition, hypertension, heart_disease and ever_married are binaries, but their values differ. I will also standarize them."},{"metadata":{},"cell_type":"markdown","source":"### 2. Data cleaning:"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = dataset[\"stroke\"]\nX = dataset.drop([\"stroke\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X[\"hypertension\"] = [\"No\" if row == 0 else \"Yes\" for row in dataset[\"hypertension\"]]\nX[\"heart_disease\"] = [\"No\" if row == 0 else \"Yes\" for row in dataset[\"heart_disease\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now, we are going to fill NaNs values within bmi variable. For that purpose, I create some extra features in order to assign the mean value of the most similar subset.\n#### Regarding age classification, there are multiple segmentations. For this exercise, I stick with the standard of the World Health Organization:\n\n- 0-2 infant\n- 3-11 child\n- 12-17 teen\n- 18- 64 adult\n- 65+ senior adult\n\nSource: https://help.healthycities.org/hc/en-us/articles/219556208-How-are-the-different-age-groups-defined-"},{"metadata":{},"cell_type":"markdown","source":"#### Once I get a clear definition of the different age segments, I create the variable within the dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X['Age_segment'] = pd.cut(X['age'], bins = [0,2,11,17,64,float('Inf')], labels = ['Infant', 'Child', 'Teenager', 'Adult', 'Senior Adult'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### I will then repeat the same exercise with avg_glucose_level. I first plot its distribution to have an idea of its values and frequencies:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sbn.displot(dataset[\"avg_glucose_level\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Researching on the internet, I found this measurement system:\n\n- Normal: less than or equal 140\n- Prediabetes: 140 - 199\n- Diabetes: 200 or higher\n\n\nSource: https://www.diabetes.org/a1c/diagnosis\n\n#### As done with age, once we have a clear definition of the different glucose segments, I create the variable: "},{"metadata":{"trusted":true},"cell_type":"code","source":"X[\"Range_glucose\"] = pd.cut(X[\"avg_glucose_level\"], bins = [0,140, 199, 600], labels = [\"Normal\", \"Pre-diabetes\", \"Diabetes\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now that I have categorized both avg_glucose_level and age, I will fill nan values fo bmi column. I will set the empty values to the mean of the closest group they belong to. "},{"metadata":{"trusted":true},"cell_type":"code","source":"X[\"bmi\"] = X[\"bmi\"].fillna(X.groupby([\"Age_segment\"])[\"bmi\"].transform(\"mean\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Once we have filled nan values within bmi columns, and that the previous two variables have been categorized, I will do the same with bmi. I have done researched on the web and have found the way people is categorized according to their bmi values:\n\n* For adults: \n\n* <18.5: Underweight\n* 18.5 - 24.9: Normal weight\n* 25 - 29.9: Overweight\n\nSource: https://www.cdc.gov/obesity/adult/defining.html#:~:text=If%20your%20BMI%20is%20less,falls%20within%20the%20obesity%20range.\n\n\n* For infants, children and teenagers: \n\n* <= 2 %: Underweight \n* \">\"  2% and <= 91%: Normal weight \n* \">\" 92 %: Overweight\n\nSource: https://www.nhs.uk/live-well/healthy-weight/bmi-calculator/\n\n#### Now, It is time to categorize the variable according to the segmentation of both age and bmi:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X[\"Bmi_segment\"] = \"\"\nsegment_percentiles = (\"Infant\", \"Child\", \"Teenager\")\nnum_rows = X.shape[0]\nfor i in range(0, num_rows):\n    pivot_age_segment = X[\"Age_segment\"].iloc[i]\n    if any(pivot_age_segment in segment for segment in segment_percentiles):\n       \n        if X[\"bmi\"].iloc[i] <=  X[X[\"Age_segment\"] == pivot_age_segment][\"bmi\"].quantile(0.02):\n            X[\"Bmi_segment\"].iloc[i] = \"Underweight\"\n        elif (X[\"bmi\"].iloc[i] > X[X[\"Age_segment\"] == pivot_age_segment][\"bmi\"].quantile(0.02) and X[\"bmi\"].iloc[i] <= X[X[\"Age_segment\"] == pivot_age_segment][\"bmi\"].quantile(0.91)):\n            X[\"Bmi_segment\"].iloc[i] = \"Normal weight\"\n        else:\n            X[\"Bmi_segment\"].iloc[i] = \"Overweight\"\n    else:\n        if X[\"bmi\"].iloc[i] <= 18.5:\n            X[\"Bmi_segment\"].iloc[i] = \"Underweight\"\n        elif (X[\"bmi\"].iloc[i]> 18.5 and X[\"bmi\"].iloc[i]<= 24.9): \n            X[\"Bmi_segment\"].iloc[i] = \"Normal weight\"\n        else:\n            X[\"Bmi_segment\"].iloc[i] = \"Overweight\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### On the data cleaning stage, I have checked for duplicated rows, standarize binary values of different categorical variables and filled nan values on bmi column.\n#### Now, the dataset is ready to be graphically explored."},{"metadata":{},"cell_type":"markdown","source":"### 3. Graphical Analysis:"},{"metadata":{},"cell_type":"markdown","source":"#### I first separate numerical variables and check the correlation among them looking for uncorrelated or highly (positive or negative) correlations:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_num = pd.concat([X.select_dtypes(exclude = [\"object\", \"category\"]), y], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sbn.heatmap(data_num.corr(method = \"pearson\"), annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### From the previous heatmap we can see that there aren't neither uncorrelated nor highly correlated variables. \n#### I now plot their distributions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1,3,figsize = (15,5), squeeze = False)\nsbn.histplot(x = 'age',data = data_num, ax = axes[0,0])\nsbn.histplot(x = 'avg_glucose_level',data = data_num, ax = axes[0,1])\nsbn.histplot(x = 'bmi',data = data_num, ax = axes[0,2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### What I we can see from the previous plot is that: \n* bmi is highly concentrated in the range 20-40, with several extreme values.\n* age as large amount of people around 80. \n* Avg_glucose_level has two modes: between 70 and 100, aproximately and 190-220.\n\n#### In order to analyze the impact of these variables on stroke, I will use the segmented version: Age_segment, Range_Glucose and Bmi_segment."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_obj = pd.concat([X.select_dtypes(include = [\"object\", \"category\"]), y], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(3,3,figsize = (20,10), squeeze = False)\nsbn.barplot(x = 'gender', y = 'stroke',data = data_obj, orient = 'v', ax = axes[0,0])\nsbn.barplot(x = 'hypertension', y = 'stroke',data = data_obj, orient = 'v', ax = axes[0,1])\nsbn.barplot(x = 'heart_disease', y = 'stroke',data = data_obj, orient = 'v', ax = axes[0,2])\nsbn.barplot(x = 'ever_married', y = 'stroke',data = data_obj, orient = 'v', ax = axes[1,0])\nsbn.barplot(x = 'work_type', y = 'stroke',data = data_obj, orient = 'v', ax = axes[1,1])\nsbn.barplot(x = 'Residence_type', y = 'stroke',data = data_obj, orient = 'v', ax = axes[1,2])\nsbn.barplot(x = 'smoking_status', y = 'stroke',data = data_obj, orient = 'v', ax = axes[2,0])\nsbn.barplot(x = 'Bmi_segment', y = 'stroke',data = data_obj, orient = 'v', ax = axes[2,1])\nsbn.barplot(x = 'Range_glucose', y = 'stroke',data = data_obj, orient = 'v', ax = axes[2,2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sbn.barplot(x = 'Age_segment', y = 'stroke',data = data_obj, orient = 'v')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### As we can see in the previous charts, there is a higher concentration of strokes on those who: \n* suffer from hypertension and heart_disease.\n* are or have been married. \n* are self-employed workers.\n* suffer from overweight and / or diabetes.\n* are former smokers.\n* are senior adults.\n\n#### I also noticed that the variable gender has a value \"Other\". I first check that there is only one case and erase it."},{"metadata":{"trusted":true},"cell_type":"code","source":"index =X[X[\"gender\"] == \"Other\"].index\nindex","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.drop(X[X[\"gender\"] == \"Other\"].index)\ny = y.drop(y.iloc[index])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary of EDA: \n* 2 extra features have been created in order to fill nan values of Bmi variable.\n* There are extreme values on variable bmi.\n* There is neither highly correlated nor uncorrelated numerical variables.\n* Variables that appear to have a higher impact on stroke are: hypertension_yes, heart_disease_yes, ever_married_yes, work_type_self_employed, smoking_status_formerly_smoked, Bmi_segment_overweight and Range_glucose_diabetes."},{"metadata":{},"cell_type":"markdown","source":"## 3. Baseline Model"},{"metadata":{},"cell_type":"markdown","source":"### Import of libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.compose import ColumnTransformer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model creation"},{"metadata":{},"cell_type":"markdown","source":"#### Firstly, I drop the non-original features from the dataset: "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_init = X.drop([\"Age_segment\", \"Range_glucose\", \"Bmi_segment\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Then, I create the pipeline with transformation for both types of features: numerical (standarization) and categorical (dummyfication). Then, for each fold, the model is applied and scored with the F1-Score since the data is heavily unbalanced."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_idx = X_init.select_dtypes(include = ['object', 'category']).columns\nnum_idx = X_init.select_dtypes(exclude = ['object', 'category']).columns\ntrans_step = [('dummy', OneHotEncoder(drop = 'first'), cat_idx),('stand', StandardScaler(), num_idx)]\ncol_trans = ColumnTransformer(transformers =trans_step)\npipeline = Pipeline(steps = [('trans', col_trans), ('model', LogisticRegression())])\nkfold = StratifiedKFold(n_splits = 10)\ncv_score = cross_val_score(estimator = pipeline, X = X, y = y, cv = kfold, scoring = 'f1')\nprint(cv_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We see a quite poor result. I repeat execute the same pipeline, but with the new added features: Age_segment, Range_glucose and Bmi_segment."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_idx = X.select_dtypes(include = ['object', 'category']).columns\nnum_idx = X.select_dtypes(exclude = ['object', 'category']).columns\ntrans_step = [('dummy', OneHotEncoder(drop = 'first'), cat_idx),('stand', StandardScaler(), num_idx)]\ncol_trans = ColumnTransformer(transformers =trans_step)\npipeline = Pipeline(steps = [('trans', col_trans), ('model', LogisticRegression())])\nkfold = StratifiedKFold(n_splits = 10)\ncv_score = cross_val_score(estimator = pipeline, X = X, y = y, cv = kfold, scoring = 'f1')\nprint(cv_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We get multiples zeroes in the folds. But we can see how the maximum value for the model that uses the new variables almost doubles the performance. It means that the response to stroke is more related to segmentations rather than exact numerical values. "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## 4. Models comparison:"},{"metadata":{},"cell_type":"markdown","source":"#### Once the baseline has been stablished, I repeat the same pipeline with different classification algorithms. I firstly import them. Second, I create a dictionary with the different options and loop over it collecting the F1-score of each option:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_models = {}\ndict_models.update({\"KNN\": KNeighborsClassifier()})\ndict_models.update({\"SVC\": SVC()})\ndict_models.update({\"Random Forest\":RandomForestClassifier()})\ndict_models.update({\"Decission Tree\": DecisionTreeClassifier()})\ndict_models.update({\"Naive Bayes\": GaussianNB()})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_scores_models = {}\nfor item in dict_models.items():\n    cat_idx = X.select_dtypes(include = ['object', 'category']).columns\n    num_idx = X.select_dtypes(exclude = ['object', 'category']).columns\n    trans_step = [('dummy', OneHotEncoder(drop = 'first'), cat_idx),('stand', StandardScaler(), num_idx)]\n    col_trans = ColumnTransformer(transformers =trans_step)\n    pipeline = Pipeline(steps = [('trans', col_trans), ('model', item[1])])\n    kfold = StratifiedKFold(n_splits = 10)\n    cv_score = cross_val_score(estimator = pipeline, X = X, y = y, cv = kfold, scoring = 'f1')\n    max_scores_models.update({item[0]: round(cv_score.max(),3)})\nprint(max_scores_models)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We see how Decission Tree Classifier obtains a higher F1-score than the rest of Classification algorithms with a large difference.\n#### Now, I will train a neural network and see how it performs."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 5. Neural network"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nimport keras.backend as K","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### This dataset is quite unbalanced, meaning that it posses more information regarding people that does not suffered a stroke than people that actually did. For that reason, F1-score was used within the baseline model and the comparison of different algorithms. \n#### Now, with the neural network, I need to do the same in order to be able to compare it against the previous models. For that reason, I firstly need to declare the function of F1-score."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_f1(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    recall = true_positives / (possible_positives + K.epsilon())\n    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n    return f1_val","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Credits: https://aakashgoel12.medium.com/how-to-add-user-defined-function-get-f1-score-in-keras-metrics-3013f979ce0d\n"},{"metadata":{},"cell_type":"markdown","source":"#### We first prepare the dataset prior to using it for training the neural network."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_num = X.select_dtypes(exclude = [\"object\", \"category\"])\nss = StandardScaler()\nX_num = pd.DataFrame(data = ss.fit_transform(X_num), columns = X_num.columns)\nX_cat = X.select_dtypes(include = [\"object\", \"category\"])\nX_cat = pd.get_dummies(X_cat)\nX_cat = X_cat.drop([\"gender_Female\", \"hypertension_No\", \"heart_disease_No\", \"ever_married_No\", \"work_type_Govt_job\",\"Age_segment_Child\",\"Range_glucose_Normal\",\"Bmi_segment_Underweight\"], axis = 1)\nX_num.reset_index(drop = True, inplace = True)\nX_cat.reset_index(drop = True, inplace = True)\nX_trans = pd.concat([X_num, X_cat], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neural_network = Sequential()\nneural_network.add(Dense(units = X_trans.shape[1], input_dim = X_trans.shape[1], activation = \"relu\"))\nneural_network.add(Dense(units = 1, activation = \"sigmoid\"))\nneural_network.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [get_f1])\nneural_network.fit(x = X_trans, y = y, epochs = 200, batch_size = 15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### With a simple neural network of one layer of the same size of the input layer, we get a F1 Score in the range (0.02 and 0.09). \n#### I will create a second model with three intermediate layers and check if it produces an improvement in performance:"},{"metadata":{"trusted":true},"cell_type":"code","source":"neural_network = Sequential()\nneural_network.add(Dense(units = X_trans.shape[1], input_dim = X_trans.shape[1], activation = \"relu\"))\nneural_network.add(Dense(units = X_trans.shape[1], activation = \"relu\"))\nneural_network.add(Dense(units = X_trans.shape[1], activation = \"relu\"))\nneural_network.add(Dense(units = X_trans.shape[1], activation = \"relu\"))\nneural_network.add(Dense(units = 1, activation = \"sigmoid\"))\nneural_network.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [get_f1])\nneural_network.fit(x = X_trans, y = y, epochs = 200, batch_size = 15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### As we can see, inserting two extra layers improves the model significantly. Now, the neural network easily reaches the range 0.40-0.50.\n#### After proving with more layers and larger size of each one, the perfornace remains constant, meaning that we may have reached the limit for this exercise. \n\n#### We can state now that the neural network, among the tested ones, is the best model to predict the stroke variable."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}