{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nencoder=LabelEncoder()\nsns.set()\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df=pd.read_csv(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Head of dataset:\",df.head())\nprint('\\n')\nprint(\"Tail of dataset:\",df.tail())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------Null_value_analysis-----------------------\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#----------Count_plot_to_check_classes_are_imbalance_or_not-------------\nsns.countplot(df['label'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Male and Female data distribution is exact same. Which confirms that there is no such case of class imbalance.\nClass imbalance can create serious troubles for machine learning model as it treats minority class labels as noise and causes overfitting.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#-------------------Correlation_between_independent_variables--------------------\nplt.figure(figsize=(20,10))\ncorr_df=df.corr()\nsns.heatmap(corr_df,annot=True)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target=df['label']\nencoder.fit(target)\ntarget=encoder.transform(target)\ndf=df.drop('label',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------Split_dataset_into_train_and_test_set----------------\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(df,target,test_size=0.20,random_state=42)\nprint(X_train.shape,X_test.shape,y_train.shape,y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#----------Build_naive_model_with-all_featutres-------------------\nfrom sklearn.ensemble import RandomForestClassifier\nmodel=RandomForestClassifier(oob_score=True,random_state=41)\nprint(\"These are default model parameters:\\n\",model)\nmodel.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------Check_model_performance------------\npred=model.predict(X_test)\nfrom sklearn.metrics import classification_report,accuracy_score\nprint(\"The accuracy_score is:\",accuracy_score(y_test,pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-----------Calculate_classification_report----------------\nprint(\"The classification report is:\\n\")\nprint(classification_report(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Classification report show the model performance is very acceptable.\nThe precision,recall and f1_score is above 90%**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#--------------Calculate_ROC_curve-----------\nfrom sklearn.metrics import roc_curve,roc_auc_score\nfpr, tpr, thresholds = roc_curve(y_test, pred)\nplt.figure(figsize=(10,8))\nplt.plot(fpr,tpr,linewidth=5)\nplt.plot([0.0,0.2,0.4,0.6,0.8,1.0],[0.0,0.2,0.4,0.6,0.8,1.0],'--',color='r')\nplt.title('ROC_curve')\nplt.xlabel('FalsePositieRate')\nplt.ylabel('TruePositiveRate')\nplt.xticks","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The above ROC curve of model shows that model is accurate on test set. As larger area is under curve.\n* The curve is very far from digonal line."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusion_matrix_val=confusion_matrix(y_test,pred)\nprint(pd.DataFrame(confusion_matrix_val,columns=['Female','Male']))\n\n# 0 - Female, 1 - Male","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(pd.DataFrame(confusion_matrix_val),annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Looking at confusion matrix we can say that it is better at predicting female category from voice as compare to male category.\n* The number of mistakes made in prediction of female category is 4.\n* The number of mitakes made in prediction of male category is 15."},{"metadata":{"trusted":true},"cell_type":"code","source":"#----------------Calculate_feature_importance----------------\nfeature_imp_df=pd.DataFrame(model.feature_importances_,columns=['Feature_importance'],index=X_train.columns)\nfeature_imp_df=feature_imp_df.reset_index()\nfeature_imp_df=feature_imp_df.rename(columns={'index':'Feature_names'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-------------Arrange_all_feature_according_to_decreasing_order_of_feature_importance----------------\nfeature_imp_dec_sort=feature_imp_df.sort_values(by='Feature_importance',ascending=False)\nplt.figure(figsize=(20,5))\nplt.xticks(rotation=45)\nplt.bar('Feature_names','Feature_importance',data=feature_imp_dec_sort)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#--------------Calculate_cumulative_sum_of_all_features---------------------------\ncum_sum_df=pd.DataFrame([])\ncumsum_imp_features=list(np.cumsum(feature_imp_dec_sort['Feature_importance']))\n\ncum_sum_df['Feature_names'] = list(feature_imp_dec_sort['Feature_names'])\ncum_sum_df['Cum_feature_importance'] = cumsum_imp_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,8))\nplt.plot('Feature_names','Cum_feature_importance',data=cum_sum_df,linewidth=5)\nplt.axhline(y=0.9,linestyle='--')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Looking at the feature importance we can infer that 11 feature out of total 21 features are having considarable importance.\nCummulative feature importance shows that 9 out of total 21 features covers 90% of feature importance.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#----------Select_features_which_contribute_90%_of_feature_importance--------------\nselected_features=list(cum_sum_df[cum_sum_df['Cum_feature_importance']<=0.90]['Feature_names'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#----------Extract_selected_feature_data------------------\nX_train_updated=X_train[selected_features]\nX_test_updated=X_test[selected_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#----------Build_model_on_selected_features-----------------\nupdated_model=RandomForestClassifier(oob_score=True,random_state=41)\n#----------Train_model_on_selected_features-----------------\nupdated_model.fit(X_train_updated,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-------------Implement_updated_model-------------\nupdated_pred=updated_model.predict(X_test_updated)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------Check_model_performance_of_updated_model--------------\nfrom sklearn.metrics import classification_report,accuracy_score\nprint(\"The accuracy_score is:\",accuracy_score(y_test,updated_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-----------Calculate_classification_report----------------\nprint(\"The classification report is:\\n\")\nprint(classification_report(y_test,updated_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#--------------Calculate_ROC_curve-----------\nfrom sklearn.metrics import roc_curve,roc_auc_score\nfpr, tpr, thresholds = roc_curve(y_test, updated_pred)\nplt.figure(figsize=(10,8))\nplt.plot(fpr,tpr,linewidth=5)\nplt.plot([0.0,0.2,0.4,0.6,0.8,1.0],[0.0,0.2,0.4,0.6,0.8,1.0],'--',color='r')\nplt.title('ROC_curve')\nplt.xlabel('FalsePositieRate')\nplt.ylabel('TruePositiveRate')\nplt.xticks","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusion_matrix_val=confusion_matrix(y_test,updated_pred)\nprint(pd.DataFrame(confusion_matrix_val,columns=['Female','Male']))\n\n# 0 - Female, 1 - Male","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(pd.DataFrame(confusion_matrix_val),annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** After selecting feature which contributes 90% of feature importance, we can see that the corresponding accuracy is droped just 1%.\nThis is not very significant drop if we look at the number of features we reduced.\nCurrent models accuracy is just short compared to all feature model.\nThe complexity the current model is very less compared to all feature model.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}