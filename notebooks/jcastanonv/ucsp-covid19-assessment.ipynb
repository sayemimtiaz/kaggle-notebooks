{"cells":[{"metadata":{},"cell_type":"markdown","source":"<div class=\"knitr source\"><img src=\"https://seeklogo.com/images/U/universidad-catolica-san-pablo-ucsp-logo-5309049584-seeklogo.com.gif\" align = 'right', style = 'position:absolute; top:0; right:0'>\n    <h1><p style=\"text-align:left; font-size: 24px\"> Exploratory Data Analysis (EDA) for tweets of COVID 19</p></h1>\n    <h2><p style=\"text-align:left; font-size: 20px\"> Universidad Catolica San Pablo</p></h2>\n  \n</div>"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://images.indianexpress.com/2020/04/how-to-use-twitter-amid-covid-19-1.jpg\" align = 'center'>\n<p><i>Presented by: Joaquín Antonio Castañón Vilca</i></p>\n"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align:justify; font-size: 18px\">This notebook tries to make an Exploratory Data Analysis of all tweets that have been publishing during this pandemic situation.\nThe objective of this notebook is to get visualization and some insights based on existing features of the data collected in the database called \"covid19-tweets\", also I will try to make a clustering for the words and some geospatial visualization. All this is only for educational purpose, in as much as to cover a diploma assessment of UCSP. So let's start!</p>\n\n<p style=\"text-align:justify; font-size: 18px\">\nYou can visit this notebook in Kaggle: <a href='https://www.kaggle.com/jcastanonv/ucsp-covid19-assessment'>https://www.kaggle.com/jcastanonv/ucsp-covid19-assessment</a>\n    </p>\n"},{"metadata":{},"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#3E8FCE;\n           font-size:200%;\n           font-family:Verdana;\n           letter-spacing:1px\">\n\n<p style=\"padding: 40px;\n              color:white;\n          text-align:center\"> Navigation\n    <i class=\"fa fa-search icon\"></i>\n \n</p>\n</div>"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align:justify; font-size: 18px\">\n    <ul style=\"text-align:justify; font-size: 18px\">\n        <li>Dataset Overview</li>\n        <li>Data Visualization</li>\n        <li>Text analysis of tweets</li>\n        <li>Clustering of Sentiment</li>\n        <li>Geospacial Visualization</li>\n</ul>\n    </p>\n    "},{"metadata":{},"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#3E8FCE;\n           font-size:200%;\n           font-family:Verdana;\n           letter-spacing:1px\">\n\n\n<p style=\"padding: 40px;\n              color:white;\n          text-align:center\"> \n    Dataset Overview\n    <i class=\"fa fa-database icon\"></i>\n   \n \n</p>\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import all the libraries required to read csv and make some modifications in the datset\n\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfrom matplotlib.cm import ScalarMappable\nfrom matplotlib import rcParams\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom iso3166 import countries\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\n\n\nrcParams['figure.figsize'] = 10,7.5\nrcParams['figure.dpi'] = 80\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align:justify; font-size: 18px\">\n    In this section we gonna analyze first of all the numerical data. So let's take a look and quick check of the dataset which will be treated\n</p>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/covid19-tweets/covid19_tweets.csv')\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<p style=\"text-align:justify; font-size: 18px\">\n    As we saw, there are some NaN values; so let's view how many are.\n    </p>\n    "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's view the percentage of NaN in each column\n\nmiss_nan = pd.DataFrame()\nmiss_nan['column'] = df.columns\n\nmiss_nan['percent'] = [round(100* df[col].isnull().sum()/len(df), 2) for col in df.columns]\nmiss_nan = miss_nan.sort_values('percent', ascending = True)\nmiss_nan = miss_nan[miss_nan['percent']>0]\n\n\nsns.barplot(miss_nan['percent'], miss_nan['column'], palette = 'Blues')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align:justify; font-size: 18px\">\n    As we can see, the column \"hashtags\" have almost 30% of NaN values, and also we have \"use_lcoation\", and \"user_description\", this can be due some users didn't use the hashtag in their post and also some people don't have their profile complete, maybe doesn't use Twitter very often; however, the column \"source\" almost doesn't have NaN values. \n    </p>"},{"metadata":{},"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#3E8FCE;\n           font-size:200%;\n           font-family:Verdana;\n           letter-spacing:1px\">\n\n<p style=\"padding: 40px;\n              color:white;\n          text-align:center\"> Data Visualization\n    <i class=\"fa fa-bar-chart icon\"></i>\n \n</p>\n</div>"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align:justify; font-size: 18px\">\n    We gonna see the users by number of tweets\n    </p>\n    "},{"metadata":{"trusted":true},"cell_type":"code","source":"number_tweets = df['user_name'].value_counts().reset_index()\nnumber_tweets.columns = ['user_name', 'tweets']\n\n\nsns.barplot(x = \"tweets\", y = \"user_name\", data = number_tweets.head(30), palette = 'Blues_r')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align:justify; font-size: 18px\">\nThis shows the number of tweets about COVID19 was made, but this doesn't mean that this user has a big amount of followers, so now we gonna extract all accounts with a lot of followers and how many tweets about COVID19 have done.\n    </p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_users = df.sort_values('user_followers', ascending =  False).drop_duplicates(subset = 'user_name', keep = 'first')\ntop_users = top_users[['user_name', 'user_followers']]\ntop_users = pd.merge(top_users, number_tweets, 'inner')\n\n\n#Normalize the scale to make the color bar on the right of the bar plot\nnorm = plt.Normalize(top_users['tweets'].min(), top_users['tweets'].max())\nsm = plt.cm.ScalarMappable(cmap=\"Blues_r\", norm=norm)\nsm.set_array([])\n#Show the barplot with color bar\nax = sns.barplot(x=\"user_followers\", y = \"user_name\", data = top_users.head(20), hue = 'tweets', dodge = False, palette = 'Blues_r')\nax.get_legend().remove()\nax.figure.colorbar(sm)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align:justify; font-size: 18px\">\n    As we can see, CNN as much as National Geographic has a big amount of followers but this account doesn't have published much about COVID19, as the difference of China Xinhua News which had posted a lot of tweets about this topic, and CGTN and Hindustan Times which are Asian accounts; all these accounts don't have a lot of followers in comparison of CNN and others. Furthermore, we gonna see the geospatial information.\n    </p>\n    <p style=\"text-align:justify; font-size: 18px\">\nNow let's take a look at which is the major \"source\" or device which have used to publish tweets.\n    </p>\n    "},{"metadata":{"trusted":true},"cell_type":"code","source":"device = df['source'].value_counts().reset_index()\ndevice.columns = ['source', 'count']\ndevice['percent_tweets'] = round(device['count']/device['count'].sum()*100, 2)\n\n\nsns.barplot(x = \"percent_tweets\", y = \"source\", data = device.head(30), palette = 'Blues_r')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align:justify; font-size: 18px\">\nAs we can see in this graph, the sources or devices which were mostly used for tweeting, were \"Twitter Web App\", \"Twitter for Android\" and \"Twitter for iPhone\", these 3 sources represent approximately 74% of all the tweets posted; which correspond to the most traditional ways. Also, other sources were: \"Blood Donors India\", \"Zoho Social\", and others.\n    </p>\n<p style=\"text-align:justify; font-size: 18px\">\n    Now let's look, if this year the number of new users was increased.\n    </p>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['user_created'] = pd.to_datetime(df['user_created'])\nnew_users = df[['user_created', 'user_name']].drop_duplicates(subset = 'user_name', keep = 'first')\nnew_users['user_created']= new_users['user_created'].dt.year\ncount_year = new_users['user_created'].value_counts().reset_index()\ncount_year.columns = ['year', 'number']\ncount_year\n\n#sns.lineplot(x = 'year', y = 'number', data = count_year)\n#A first impression that we can see that some accounts were created in 1970 and obviously this is not real\ncount_year['year'] = count_year[count_year['year']>1990]\nsns.lineplot(x = 'year', y = 'number', data = count_year, marker = 'o')\nplt.xlabel('Year')\nplt.ylabel('Number of New Users')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align:justify; font-size: 18px\">\n    As we can see in this graph, the number of new accounts increased this year, we can also see that in the years between 2020 and 2009 there is a valley for approximately 10 years where people have not created many accounts in comparison to 2009, in fact, the number of new accounts per year decreased in this lap of time of 10 years; the sudden increase this year maybe is due for the pandemic and the lockdown in many countries over the world.\n    </p>\n<p style=\"text-align:justify; font-size: 18px\">\n    In the next section, we gonna find insights based on text data\n    </p>"},{"metadata":{},"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#3E8FCE;\n           font-size:200%;\n           font-family:Verdana;\n           letter-spacing:1px\">\n\n<p style=\"padding: 40px;\n              color:white;\n          text-align:center\"> Text analysis of tweets\n    <i class=\"fa fa-newspaper-o icon\"></i>\n \n</p>\n</div>"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align:justify; font-size: 18px\">\n    Now we gonna analyze the text information within hashtags that have been used in the tweets during this pandemic. As we know, in the before section, we have seen the column \"hashtag\" has a lot of NaN values, so the first step we gonna make converts these NaN values to something, and then we can extract some insights and create a wordcloud.\n    </p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['hashtags'] = df['hashtags'].fillna('[]')\ndf['hashtags_count'] = df['hashtags'].apply(lambda x: len(x.split(',')))\ndf.loc[df['hashtags'] == '[]', 'hashtags_count'] = 0\n\n# let's see the number of hashtags used by users\nhashtag_per_user = df[['user_name','hashtags_count']].sort_values('hashtags_count', ascending =  False).drop_duplicates(subset = 'user_name', keep = 'first')\nsns.barplot(x=\"hashtags_count\", y = \"user_name\", data = hashtag_per_user.head(30), palette = 'Blues_r')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hashtag_per_user.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align:justify; font-size: 18px\">\n    As we can see in this graph, the user called \"ROCAS THE PURPLEKING\" has used 17 different hashtags, in another hand, we can say, 75% of the users use 2 hashtags per tweet.\n    </p>\n<p style=\"text-align:justify; font-size: 18px\">\n        Now let's view which hashtag is the most used for the users within their tweets.\n    </p>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def hashtags_split(x):\n    return str(x).lower().replace('[','').replace(']','').replace(\"'\",'').replace(\" \", '').split(',')\n\nhashtag_tweets = df.copy()\nhashtag_tweets['hashtag'] = hashtag_tweets['hashtags'].apply(lambda row: hashtags_split(row))\nhashtag_tweets = hashtag_tweets.explode('hashtag')\nhashtag_tweets.loc[hashtag_tweets['hashtag'] == '', 'hashtag'] = 'No Hashtag'\nhashtag_tweets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hashtag_number = hashtag_tweets['hashtag'].value_counts().reset_index()\nhashtag_number.columns = ['hashtag', 'count']\n\nsns.barplot(x=\"count\", y = \"hashtag\", data = hashtag_number.head(10), palette = 'Blues_r')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align:justify; font-size: 18px\">\n    Now we gonna make a wordcloud of the principal words and topics posted in the different tweets.\n    </p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \"\".join(tweet for tweet in df['text'])\nstopwords = set(STOPWORDS)\nstopwords.update(['https', 't','co', 'many', 's'])\n\nwordcloud = WordCloud(stopwords=stopwords, background_color='white').generate(text)\n\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.title('Prevalent words for all tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In this part we gonna use another mask for the wordcloud and different colors\nimport os\nfrom PIL import Image\nfrom scipy.ndimage import gaussian_gradient_magnitude\nfrom wordcloud import ImageColorGenerator\n\nd = os.path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n# load image. This has been modified in gimp to be brighter and have more saturation.\ncovid_color = np.array(Image.open(os.path.join(d, \"../input/images/2019-nCoV-CDC-23312.jpg\")))\n# subsample by factor of 3. Very lossy but for a wordcloud we don't really care.\ncovid_color = covid_color[::3, ::3]\n\n# create mask  white is \"masked out\"\ncovid_mask = covid_color.copy()\ncovid_mask[covid_mask.sum(axis=2) == 0] = 255\n\nedges = np.mean([gaussian_gradient_magnitude(covid_color[:, :, i] / 255., 2) for i in range(3)], axis=0)\ncovid_mask[edges > .08] = 255\nhashtag = \" \".join(hashtag for hashtag in hashtag_tweets['hashtag'])\nstopwords = set(STOPWORDS)\nstopwords.update(['No', 'Hashtag'])\n\nwc = WordCloud(max_words=2000, mask=covid_mask, max_font_size=40, random_state=42, relative_scaling=0, stopwords=stopwords, background_color='white')\n\n# generate word cloud\nwc.generate(hashtag)\n\n\n\n# create coloring from image\nimage_colors = ImageColorGenerator(covid_color)\nwc.recolor(color_func=image_colors)\nplt.figure(figsize=(10, 10))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align:justify; font-size: 18px\">\n    As we can see the prevalent word for all tweets is \"COVID19\", followed by words like \"mask\", \"help\", \"people\" and \"pandemic\".\n    </p>\n<p style=\"text-align:justify; font-size: 18px\">\n    On another hand, in the case of hashtags, we can see all hashtags follow the same topic as text, with \"Covid19\" or \"coronavirus\" as prevalent words, followed with \"wearmask\" this can be because of the policy which almost every country have adopted about the use of a mask for prevention and \"coronovirusupdate\" maybe this keeps update the number of positive cases and deaths, also we can see hashtags like \"healthcare\", \"trump\", \"lockdown\" and \"socialdistancing\".\n    </p>"},{"metadata":{},"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#3E8FCE;\n           font-size:200%;\n           font-family:Verdana;\n           letter-spacing:1px\">\n\n<p style=\"padding: 40px;\n              color:white;\n          text-align:center\"> Clustering of Sentiment\n    <i class=\"fa fa-smile-o icon\"></i>\n \n</p>\n</div>"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align:justify; font-size: 18px\">\n    Now, let's figure it how can cluster the text of tweets in two differents groups or sentimental groups.\n    </p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"vec = TfidfVectorizer(stop_words = 'english')\nvec.fit(df['text'].values)\nfeatures = vec.transform(df['text'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters = 2, random_state = 0)\nkmeans.fit(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = kmeans.predict(features)\ndf['Cluster'] = res\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_cluster_1 = \" \".join(tweet for tweet in df[df['Cluster'] == 0]['text'])\nstopwords = set(STOPWORDS)\nstopwords.update(['https', 't','co', 'many', 's'])\n\nwordcloud_1 = WordCloud(max_words = 100, stopwords=stopwords, background_color='white').generate(text_cluster_1)\n\nplt.imshow(wordcloud_1)\nplt.axis('off')\nplt.title('Group of words for the cluster Nº0')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_cluster_2 = \" \".join(tweet for tweet in df[df['Cluster'] == 1]['text'])\nstopwords = set(STOPWORDS)\nstopwords.update(['https', 't','co', 'many', 's'])\n\nwordcloud_2 = WordCloud(max_words = 100, stopwords=stopwords, background_color='white').generate(text_cluster_2)\n\nplt.imshow(wordcloud_2)\nplt.axis('off')\nplt.title('Group of words for the cluster Nº1')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align:justify; font-size: 18px\">\n    In these two graphs, we can distinguish two sentimental groups, cluster 0 one is apparently is more negative because group all the texts with a topic about people who were positive in the test of COVID19, and the second group is more neutral information and talk about prevention, we can say is more optimistic.\n    </p>\n"},{"metadata":{},"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#3E8FCE;\n           font-size:200%;\n           font-family:Verdana;\n           letter-spacing:1px\">\n\n<p style=\"padding: 40px;\n              color:white;\n          text-align:center\"> Geospacial Visualization\n    <i class=\"fa fa-rocket icon\"></i>\n \n</p>\n</div>"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align:justify; font-size: 18px\">\n    Finally, we gonna see, first of all, the distribution of tweets about COVID19 all over the world, and next this information will be display in some geospatial graph.\n    </p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"location = df['user_location'].value_counts().reset_index()\nlocation.columns = ['user_location', 'count']\nlocation = location[location['user_location'] != 'NA']\nlocation = location.sort_values(['count'], ascending = False)\n\nsns.barplot(x=\"count\", y = \"user_location\", data = location.head(30), palette = 'Blues_r')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align:justify; font-size: 18px\">\n    As we can see, India and the United States are countries that contributed to publishing more tweets than other countries.\n    </p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install geopandas\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pycountry\n\ndef alpha3code(column):\n    CODE = []\n    for country in column:\n        try:\n            code = pycountry.countries.get(name=country).alpha_3\n            CODE.append(code)\n        except:\n            CODE.append('None')\n    return CODE\n\n\nlocation['CODE'] = alpha3code(location['user_location'])\nlocation.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import geopandas\nworld = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\nworld.columns = ['pop_est', 'continent', 'name', 'CODE', 'gdp_md_est', 'geometry']\nworld_merge = pd.merge(world, location, on='CODE')\n\nlocation_merge = pd.read_csv('../input/latlong/countries_latitude_longitude.csv')\nworld_merge = world_merge.merge(location_merge, on='name').sort_values(by='count', ascending=False).reset_index()\nworld_merge = world_merge[['user_location', 'count', 'latitude','longitude']]\nworld_merge.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import folium\nfrom folium import plugins\nfrom folium.plugins import HeatMap\n\nfolium_map = folium.Map(location=[50,0],\n                       zoom_start=3,\n                       tiles='CartoDB dark_matter')\n\nworld_merge['latitude']=world_merge['latitude'].fillna(0)\nworld_merge['longitude']=world_merge['longitude'].fillna(0)\n\nplugins.FastMarkerCluster(data=list(zip(world_merge['latitude'].values, world_merge['longitude'].values))).add_to(folium_map)\narr = world_merge[['latitude', 'longitude']].values\n\nHeatMap(arr, radius = 15).add_to(folium_map)\n\nfolium.LayerControl().add_to(folium_map)\nfolium_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<p style=\"text-align:justify; font-size: 18px\">\n    We have extracted some insights of the information collected in all tweets about COVID19, the next steps will be to aggregate some machine learning prediction, tune the clustering algorithm and finally optimize the extraction of latitude and longitude for each city or country. \n    </p>\n    \n<p style=\"text-align:justify; font-size: 18px\">\n    See You Soon!! :)\n    </p>"},{"metadata":{},"cell_type":"markdown","source":"    \n<p style=\"text-align:justify; font-size: 18px\">\n    References:\n        Kostiantyn Isaienkov (2020). Notebook\n    </p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}