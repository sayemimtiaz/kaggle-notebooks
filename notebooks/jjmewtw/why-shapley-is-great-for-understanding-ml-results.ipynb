{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 0.Introduction\n\n**The goal of this notebook is to discuss Shapley values and resulting SHAP**. This algorithm is the answer for the arguably most annoying shortcoming of machine elarning: 'interpretability'. I will discuss how we can use SHAP and provide some theoretical background. As the goal of the notebook is Shapley structure, I will not discuss granularly data preparation nor modeling.\n\n**Content:**\n1. Data preparation\n2. Model: Sequantial Model Assembling + Gradient Boosting\n3. Shapley values\n\nThe main sources of knowledge for this notebook are:\n1. [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/)\n2. [https://en.wikipedia.org/wiki/Shapley_value](https://en.wikipedia.org/wiki/Shapley_value)\n\n**For the readability I hide some lines of code, please uncover it if desired.**"},{"metadata":{},"cell_type":"markdown","source":"# 1.Data preparation\n\nThis is the regular step performed before modeling. I aim on loading the data, checking and cleaning. \n\n**1.1.Libraries**\n\nFirst I choose some libraries which can be useful:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nfrom numpy.random import seed\nimport pandas as pd \nimport matplotlib\nimport seaborn as sns\nimport holoviews as hv\nimport matplotlib.pyplot as plt\nimport mpl_toolkits.mplot3d.axes3d as p3\n\nimport nltk\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\nimport xgboost as xgb\n\nfrom keras import Sequential\nfrom keras.layers import Dense\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport os\nimport collections\n\nimport shap\n\nshap.initjs()\n\nRandomState = 123\nseed(RandomState)\ntf.random.set_seed(RandomState)\n\nprint('Libraries correctly loaded')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1.2.Data cleaning**\n\nSecond, include the data:"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_path = \"../input/us-airbnb-open-data/AB_US_2020.csv\"\nusecols=['id','name','latitude','longitude','room_type','price','minimum_nights','number_of_reviews',\n         'last_review','reviews_per_month','calculated_host_listings_count','availability_365','city']\n\ndf = pd.read_csv(df_path,usecols=usecols,index_col='id')\n\nprint('Number of rows: '+ format(df.shape[0]) +', number of features: '+ format(df.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"Missing_Percentage = (df.isnull().sum()).sum()/np.product(df.shape)*100\nprint(\"The number of missing entries before cleaning: \" + str(round(Missing_Percentage,5)) + \" %\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some corrections:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['name'] = df['name'].fillna('')\ndf['last_review'] = df['last_review'].fillna('01/01/00')\ndf['reviews_per_month'] = df['reviews_per_month'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data is cleaned. Let's define states' variable:"},{"metadata":{"trusted":true},"cell_type":"code","source":"states_dic = {'Asheville':'NC','Austin':'TX','Boston':'MA','Broward County':'FL','Cambridge':'MA','Chicago':'IL','Clark County':'NV','Columbus':'OH','Denver':'CO','Hawaii':'HI','Jersey City':'NJ',\n             'Los Angeles':'SC','Nashville':'TN','New Orleans':'MS','New York City':'NY','Oakland':'CA','Pacific Grove':'CA','Portland':'OR','Rhode Island':'RI','Salem':'MA','San Clara Country':'CA',\n             'Santa Cruz County':'CA','San Diego':'CA','San Francisco':'CA','San Mateo County':'CA','Seattle':'WA','Twin Cities MSA':'MN','Washington D.C.':'DC'}\n\ndf['state'] = df['city'].apply(lambda x : states_dic[x])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.Model: Sequantial Model Assembling + Gradient Boosting\n\nIn this chapter I use the approach from [this notebook](https://www.kaggle.com/thomaskonstantin/u-s-airbnb-analysis-and-price-prediction#notebook-container). Please upvote this guy if you like. There will be some differences with my approach:\n* I cap the extreme values for price applying max for quantile 0.95, but I don't remove another extreme level data as I find it relevant for analysis\n* The data is divded into train and test and results are produced on the basis of test\n\nAforementioned capping:"},{"metadata":{"trusted":true},"cell_type":"code","source":"upper_bound = 0.95\ndf.loc[df['price'] >= df['price'].quantile(upper_bound), ['price']] = df['price'].quantile(upper_bound)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_final = df.drop(columns= ['price'])\nTarget = df.price","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2.1.NLP cleaning**\n\nSome cleaning functions which I defined in my [another NLP notebook](https://www.kaggle.com/jjmewtw/total-bible-text-study-eda-cluster-bert-nlp). They offer complex cleaning offer:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"ps = PorterStemmer()\n\ndef lower_column_t(data):\n    values = data['name']\n    values = values.lower()\n    data['name'] = values\n    return data\n\ndef clean_interpunction(data):\n    values = data['name']\n    values = values.replace('.','')\n    values = values.replace(';','')\n    values = values.replace(':','')\n    values = values.replace(',','')\n    values = values.replace(\"'\",\"\")\n    values = values.replace('\"','')\n    values = values.replace('/',' ')\n    values = values.replace('-',' ')\n    values = values.replace('+',' ')\n    values = values.replace('#',' ')\n    values = values.replace('!','')\n    values = values.replace('(',' ')\n    values = values.replace(')',' ')\n    values = values.replace('*',' ')\n    values = values.replace('|',' ')\n    values = values.replace('&',' and ')\n    values = values.replace('@',' at ')\n    data['name'] = values\n    return data\n\ndef stem(a):\n    p = nltk.PorterStemmer()\n    b = []\n    for line in a:\n\n        split_line = line.split(' ')\n        length=len(split_line)\n        new_line = []\n\n        for word in range(length):\n            if word == 0:\n                new_line.append(str(p.stem(split_line[word])))\n            else:\n                new_line[0] = new_line[0] + ' ' + (str(p.stem(split_line[word])))\n\n        b.append(new_line[0])\n\n    return b\n\ndef lem(a):\n    p = nltk.WordNetLemmatizer()\n    b = []\n    for line in a:\n\n        split_line = line.split(' ')\n        length=len(split_line)\n        new_line = []\n\n        for word in range(length):\n            if word == 0:\n                new_line.append(str(p.lemmatize(split_line[word], pos=\"v\")))\n            else:\n                new_line[0] = new_line[0] + ' ' + (str(p.lemmatize(split_line[word], pos=\"v\")))\n\n        b.append(new_line[0])\n\n    return b\n\ndef tokenize(a):  \n    b = []\n    for line in a:\n        b.append(word_tokenize(line))\n                 \n    return b\n\ndef flatten(a):\n    b = []\n    for line in a:\n        b = ' '.join(line)\n    \n    return b\n\ndef count_words(a):\n    b=0\n    for line in a:\n        b = b + sum([i.strip(string.punctuation).isalpha() for i in line.split()])\n        \n    return b\n\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(' ') if token != '' if token not in sw]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [' '.join(ngram) for ngram in ngrams]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cleaning is applied on the variable 'name'. The results:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_final_prep = df_final\n\ndf_final_prep = df_final_prep.apply(lower_column_t, axis=1)\ndf_final_prep = df_final_prep.apply(clean_interpunction, axis=1)\ndf_final_prep['name']=stem(df_final_prep.name)\n\ndf_final_prep","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2.2.Sequantial Model Assembling**\n\nAs mentioned the data is divided into train and test:"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(df_final_prep,Target,test_size=0.2,random_state=RandomState)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Applying vocabulary counter on the train set, top ten words:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"vocab = collections.Counter(' '.join(x_train['name']).split(' '))\n\nvocab.most_common(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And the number of words in total vocabulary:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"MAX_LENGTH = max(x_train['name'].apply(lambda x: len(x)))\nVOCAB_SIZE = len(vocab.keys())\nVECTOR_SPACE = 100\nVOCAB_SIZE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Applying the sequential model as in the reference notebook, but on the basis of train data set keeping the same size of evaluation set."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"encoded_docs = [tf.keras.preprocessing.text.one_hot(d,VOCAB_SIZE) for d in x_train.name]\n\npadded_docs = tf.keras.preprocessing.sequence.pad_sequences(encoded_docs,maxlen=MAX_LENGTH,padding='post')\n\nn = 1000\n\npadded_docs_eval = padded_docs[0:n]\npadded_docs = padded_docs[n:]\nY = y_train[n:]\nY_eval = y_train[:n]\n\nFCNN_MODEL = Sequential([\n    tf.keras.layers.Embedding(VOCAB_SIZE,VECTOR_SPACE,input_length=MAX_LENGTH),\n    tf.keras.layers.Flatten(),\n    Dense(activation='relu',units=5),\n    Dense(activation='relu',units=1)\n])\n\nFCNN_MODEL.compile(optimizer='adam', loss='mse', metrics=['mae'])\n\ntf.keras.utils.plot_model(FCNN_MODEL,show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Applying the model with 4 epochs and 75 for batch size. This can be further calibrated in your own version of this notebook."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"history = FCNN_MODEL.fit(padded_docs, Y,validation_data=(padded_docs_eval,Y_eval), epochs=4, batch_size=75)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And the predictions for train data set:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"encoded_docs = [tf.keras.preprocessing.text.one_hot(d,VOCAB_SIZE) for d in x_train.name]\n\ndosc_prep = tf.keras.preprocessing.sequence.pad_sequences(encoded_docs,maxlen=MAX_LENGTH,padding='post')\n\npredictions = FCNN_MODEL.predict(dosc_prep)\npredictions = predictions.reshape(-1)\npredictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the enhanced train data set. You can see that dummy transformation was applied for state variable, and room type one. I kept all the nuemric variables. I think the most iinteresting factor here is variable 'Name predicted'. This is the value predicted by Sequential model only on the basis of string variable 'name'. I added it to the data set:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_train_2 = x_train\n\nFactorsToDrop = ['name','last_review','city']\n\ndf_train_2 = df_train_2.drop(columns = FactorsToDrop)\ndf_train_2 = pd.get_dummies(df_train_2)\ndf_train_2.insert(0, \"Actual value\", y_train, True)\ndf_train_2.insert(1, \"Name predicted\", predictions, True)\ndf_train_2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2.3.Gradient Boosting Machine**\n\nThe next idea is to define Gradient Boosting Regressors taking two different approaches: data set will contain 'Name predicted', second one will not. I will look how they perform between each other:"},{"metadata":{"trusted":true},"cell_type":"code","source":"RF_withName = xgb.XGBRegressor(random_state=RandomState)\nRF_withoutName = xgb.XGBRegressor(random_state=RandomState)\n\nRF_withName_fit = RF_withName.fit(df_train_2.drop(columns = 'Actual value'),df_train_2['Actual value'])\nRF_withoutName_fit = RF_withoutName.fit(df_train_2.drop(columns = ['Actual value','Name predicted']),df_train_2['Actual value'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The table with resulting predictions on the test data set:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"encoded_docs = [tf.keras.preprocessing.text.one_hot(d,VOCAB_SIZE) for d in x_test.name]\n\ndosc_prep = tf.keras.preprocessing.sequence.pad_sequences(encoded_docs,maxlen=MAX_LENGTH,padding='post')\n\npredictions = FCNN_MODEL.predict(dosc_prep)\npredictions = predictions.reshape(-1)\n\nx_test_2 = x_test\n\nFactorsToDrop = ['name','last_review','city']\n\nx_test_2 = x_test_2.drop(columns = FactorsToDrop)\nx_test_2 = pd.get_dummies(x_test_2)\nx_test_2.insert(0, \"Name predicted\", predictions, True)\n\nResults = pd.DataFrame({'Prediction only with Name':predictions,'Prediction RF with Name':RF_withName_fit.predict(x_test_2),'Prediction RF without Name':RF_withoutName_fit.predict(x_test_2.drop(columns = 'Name predicted'))})\nResults.insert(0, \"Actual value\", y_test.values, True)\nResults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2.4.Root Mean Square Error**\n\nLet's look at RMSE:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def f_rmse(predictions, targets):\n    return np.sqrt(np.mean((predictions-targets)**2))\n\nModel_Average_RMSE =     f_rmse(Results['Actual value'], y_train.mean())   \nOnlyName_RMSE =      f_rmse(Results['Actual value'].values, Results['Prediction only with Name'].values)    \nRF_withName_RMSE =      f_rmse(Results['Actual value'].values, Results['Prediction RF with Name'].values)    \nRF_withoutName_RMSE =      f_rmse(Results['Actual value'].values, Results['Prediction RF without Name'].values)  \n\nFinal = pd.DataFrame({'RMSE': [Model_Average_RMSE,OnlyName_RMSE,RF_withName_RMSE,RF_withoutName_RMSE],'Name': ['Model_Average','OnlyName','GBM_withName','GBM_withoutName']})\n\nplt.plot(Final['Name'],Final['RMSE'])\nplt.ylabel('RMSE results')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model average is the worst, no surprise but good quality check. If it was not a case, it would mean that our predictions are worse than bad. If we use only name factor, we can see already the improvement by approximately 30%, GBM without 'name' gives sligtly better performance than 'name' alone. The best is GBM with name, which leads to more than 40% improvement. This is quite okay, but as I said this model can be much imporved by calibration. Both Sequential and GBM as none of them was calibrated. You can try it by yourself."},{"metadata":{},"cell_type":"markdown","source":"# 3.Shapley values\n\nI will now focus on the central poit of this notebook - Shapley approach. First, if you wanted to find here some theoretical background, I am providing it just in a sec. In other case just jump to results.\n\n**3.1.Theoretical background**\n\nWhat are Shapley values? The Shapley value is a solution concept in cooperative game theory. To each cooperative game it assigns a unique distribution (among the players) of a total surplus generated by the coalition of all players. The Shapley value is characterized by a collection of desirable properties.\n\nLet's assume that coalitional games is defined:\n* set of players N\n* and function v which maps subsets of players to numbers: $v: 2^N \\rightarrow R$\n* coalition of players S (subset of N)\n\nThe use of function $v(S)$: called the worth of coalition S, describes the total expected sum of payoffs the members of S can obtain by cooperation. The Shapley value is one way to distribute the total gains to the players, assuming that they all collaborate.\n\nThis can be given by this complicated formula:\n\n$\\phi_i(v)=\\frac1{|N|!}\\sum_{S\\subseteq N\\setminus\\{i\\}}|S|!(|N|-|S|-1)!\\left(v(S\\cup\\{i\\})-v(S)\\right)$\n\nwhat can be better understood as:\n\n$\\phi_i(v) = \\frac{1}{number \\: of \\: players} \\sum_{coalitions \\: excluding \\: i} \\frac{marginal \\: contribution \\: of \\: i \\: to \\: coalition}{number \\: of \\: coalitions \\: excluding \\: i}$\n\nLet's break it down:\n\nFor example, we have apartment located in Texas, 50m2 with low number of reviews. Let's assume prediction equals to 100 US dollar. The average prediction for all apartment in the market is 120. We would like to understand how much features contributed such that we have this value.\n\nThis is a place where we can directly apply Shapley approach. The \"game\" is the prediction task for a single instance of the dataset. The \"gain\" is the actual prediction for this instance minus the average prediction for all instances. The \"players\" are the feature values of the instance that collaborate to receive the gain (= predict a certain value).\n\nAlgorithm will then gradually exclude given combinations, for example marking the driver of biggest differences the fact that flat is in Texas istead of New York. It can also say that the flat's size or low number of reviews is responsible. \n\nHow to calculate it in practice?\n\nThe Shapley value is the average marginal contribution of a feature value across all possible coalitions. [More granular description](https://christophm.github.io/interpretable-ml-book/shapley.html).\n\nWhat is SHAP?\n\nThe goal of SHAP is to explain the prediction of an instance x by computing the contribution of each feature to the prediction. The SHAP explanation method computes Shapley values from coalitional game theory. The feature values of a data instance act as players in a coalition. Shapley values tell us how to fairly distribute the \"payout\" (= the prediction) among the features. A player can be an individual feature value, e.g. for tabular data. "},{"metadata":{},"cell_type":"markdown","source":"**3.2.Results**\n\nI will look at some flats from our AiRbnb data set and analyze what is the opition of algorithm about them. To remind if you omitted chapter 1 and 2: we have two algorithms:\n* Sequantial Model Assembling only on 'name' + Gradient Boosting Machine on all data\n* Gradient Boosting Machine on all data\n\nFunctions from shap package are quite heavy computiation-wise. I choose just 5000 flats from the test set with given predictions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"shp_df = x_test_2.sample(n=5000, replace=False, weights=None, random_state=RandomState)\nshp_df_2 = shp_df.drop(columns = ['Name predicted'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, I define:\n* tree explainer (TreeSHAP): Lundberg proposed TreeSHAP in 2018, as a variant of SHAP for tree-based machine learning models such as decision trees, random forests and gradient boosted trees. TreeSHAP was introduced as a fast, model-specific alternative to KernelSHAP, but it turned out that it can produce unintuitive feature attributions.\n* Shap values given on the basis of this explainer"},{"metadata":{"trusted":true},"cell_type":"code","source":"explainer_withName = shap.TreeExplainer(RF_withName_fit, feature_dependence=\"feature_perturbation\")\nshap_values_withName = explainer_withName.shap_values(shp_df)\n\nexplainer_withoutName = shap.TreeExplainer(RF_withoutName_fit, feature_dependence=\"feature_perturbation\")\nshap_values_withoutName = explainer_withoutName.shap_values(shp_df_2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at some statistics from the test data set. It will be interesting to observe how chosen by me records perform aginst the general values."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"x_test_2[['Name predicted','latitude','longitude', 'minimum_nights','number_of_reviews','reviews_per_month','calculated_host_listings_count','availability_365']].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I choose the flat 505 from randomly drawn sample from the test set."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"i = 505\n\nid_505 = ((shp_df.iloc[i:(i+1)].reset_index()).id).values[0]\n\nprint(\"The index of this first guy: \" + format(id_505))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have the flat '20 minutes' from Manhattan. The room is in New Jersey. It is just a room, havig ittle number of reviews (avg is 34). But the flat is trending as it receives around 2.5 reviews monthly vs quantile 75% at level 1.62. The price is very low: 60 US dollar."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df.loc[id_505]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Second flat is located in Austin, Texas."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"j = 515\n\nid_515 = ((shp_df.iloc[j:(j+1)].reset_index()).id).values[0]\n\nprint(\"The index of the second guy: \" + format(id_515))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As mentioned in the title, it is 'new listing', called also 'peaceful retreat'. This is entire home/apartment. Not many reviews on the account, also not many new reviews. The price is high: 378 $."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df.loc[id_515]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Results for New Jersey guy using GBM with Name:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"shap.force_plot(explainer_withName.expected_value, shap_values_withName[i], features=shp_df.iloc[i], feature_names=shp_df.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Results for New Jersey guy using GBM wihout Name:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"shap.force_plot(explainer_withoutName.expected_value, shap_values_withoutName[i], features=shp_df_2.iloc[i], feature_names=shp_df_2.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The price estimation difference is only 5$. In the first algorithm 'name' factor plays great role loweirng the price by more than 50 dollar! It is interesting that title with 'Manhattan' led to lower price. Probably word 'room' lowers a lot. Indeed room type plays a big role in both estimations, but in first case it lowers, in second it increases the price. It seems to confirm my assumption then. In contrary, reviews per month lower the price for both of them. \n\nTime for Texas guy:\n\nFirst algorithm with name on the board. Multiple increasing factors. 'Name' leads to approx. 320 dollar increase, vast number:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"shap.force_plot(explainer_withName.expected_value, shap_values_withName[j], features=shp_df.iloc[j], feature_names=shp_df.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Value is significantly lower (110$ lower). Probably because of information from the name variable:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"shap.force_plot(explainer_withoutName.expected_value, shap_values_withoutName[j], features=shp_df_2.iloc[j], feature_names=shp_df_2.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at 10 different values for algorithm with name:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"i = 505\nj = 510\n\nshap.force_plot(explainer_withName.expected_value, shap_values_withName[i:j], features=shp_df.iloc[i:j], feature_names=shp_df.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And same 10 records for algorithm without name:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"i = 505\nj = 510\n\nshap.force_plot(explainer_withoutName.expected_value, shap_values_withoutName[i:j], features=shp_df_2.iloc[i:j], feature_names=shp_df_2.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another interesting graph is the SHAP value plot for all biggest factors. Far the most relevant is our 'name' factor: "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"shap.summary_plot(shap_values_withName, features=shp_df, feature_names=shp_df.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The SHAP value for two factors at once:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"shap.dependence_plot('Name predicted', shap_values_withName, shp_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"shap.dependence_plot('number_of_reviews', shap_values_withName, shp_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you think that this notebook helpd a bit in SHAPing your knowledge, please upvote it."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}