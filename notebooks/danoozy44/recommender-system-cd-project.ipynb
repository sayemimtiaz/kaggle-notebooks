{"cells":[{"metadata":{},"cell_type":"markdown","source":"# CS6109 - Compiler Design Project\n\n**Project Topic: Social Recommendations with multiple influence from Direct User Interactions**\n\nDone by:\n\n    Vivek Ramkumar, 2018103082\n    Kariketi Tharun Reddy, 201803034\n    G. R. Srikanth, 2018103603\n                                                    \n    Third Years studying B.E, Computer Science, at College of Engineering, Guindy\n    Chennai, Tamil Nadu, India"},{"metadata":{},"cell_type":"markdown","source":"# Importing Standard Libraries:"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"pip install advertools","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nfrom collections import OrderedDict\nimport gc\nimport random\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Obtaining Data:"},{"metadata":{},"cell_type":"markdown","source":"We are using three datasets of products, from which recommendations will be made.\n\n1. Groceries Dataset\n2. Flipkart Product Database\n3. Consumer Reviews of Amazon Products\n\nWhile extracting data from the respective databases, we need to ensure there are no duplicate items or NaN values.\n\nSo, we use OrderedDict.fromkeys() to remove all duplicate items. We then check whether any NaN values exist \nin the list of products. If there are NaN values, we promptly exclude them.\n\nFinally, we delete the database as it occupies a lot of space in the RAM.\ngc.collect() is a garbage collection function used to clear up any extra leftovers from the deletion.\n\nThis process is repeated for all the three databases."},{"metadata":{"trusted":true},"cell_type":"code","source":"groceries_db = pd.read_csv('../input/groceries/groceries - groceries.csv')\n\nnew_groceries_db = groceries_db.drop(columns = ['Item(s)'])\n\ntemp = []\n\nfor i in range(len(new_groceries_db.columns)):\n    for x in new_groceries_db.iloc[:, i]:\n        temp.append(x)\n            \ngroceries = [] \n\ngroceries = list(OrderedDict.fromkeys(temp))\n\ngroceries = [item for item in groceries if str(item) != 'nan']\n\ndel groceries_db, new_groceries_db\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"flipkart_db = pd.read_csv('../input/flipkart-products/flipkart_com-ecommerce_sample.csv')\n\ntemp = flipkart_db['product_name']\n\nflipkart_items = [] \n\nflipkart_items = list(OrderedDict.fromkeys(temp))\n\nflipkart_items = [item for item in flipkart_items if str(item) != 'nan']\n\ndel flipkart_db\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_amzn_prod_dbs(file_path):\n    \n    temp_db = pd.read_csv(file_path)\n\n    temp = temp_db['name']\n    \n    return temp\n    \ndef append_list_to_list(orig_list, new_list):\n    \n    for x in orig_list:\n        new_list.append(x)\n\ntemp_1 = read_amzn_prod_dbs('../input/consumer-reviews-of-amazon-products/1429_1.csv')\ntemp_2 = read_amzn_prod_dbs('../input/consumer-reviews-of-amazon-products/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv')\ntemp_3 = read_amzn_prod_dbs('../input/consumer-reviews-of-amazon-products/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv')\n\ntemp = []\n\nappend_list_to_list(temp_1, temp)\nappend_list_to_list(temp_2, temp)\nappend_list_to_list(temp_3, temp)\n\namazon_items = [] \n\namazon_items = list(OrderedDict.fromkeys(temp))\n\namazon_items = [item for item in amazon_items if str(item) != 'nan']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We combine all the obtained lists of products into a singular mega-list.\n    \nThis list will be referred to for recommending products to the users."},{"metadata":{"trusted":true},"cell_type":"code","source":"complete_list = []\n\nappend_list_to_list(flipkart_items, complete_list)\nappend_list_to_list(groceries, complete_list)\nappend_list_to_list(amazon_items, complete_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The total number of products available to recommend :\", len(complete_list))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The mega-list contains over 12,000 product names. This is a sizeable amount, enough to get different recommendations on every run.**"},{"metadata":{},"cell_type":"markdown","source":"# Lexical Analysis:"},{"metadata":{},"cell_type":"markdown","source":"The main focus of lexical analysis is to obtain tokens from the given text input.\n\nWe need to differentiate between entities (users, organizations, religion, etc.) and interests (garments, tech, etc.).\n\nThe technique we are going to employ is called POS tagging. (POS -> Part of Speech)\n    \nPOS tagging algorithms allocate tags to each token, such as \"NOUN\", \"VERB\", \"ADJ\", etc., based on the similarity to their respective tagged categories.\n\nWe will be using the Spacy library, which contains pre-trained NLP models and a huge dictionary of words.\n\nThe input is parsed sentence by sentence, from which nouns, verbs, users, and user types are obtained. The respective information is summarized in a pandas dataframe. You will see it in the 'Input and Output' section."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import nltk\nimport spacy\n\nfrom spacy import displacy\n\nfrom nltk.corpus import stopwords\n\nnltk.download('stopwords')\n\nfrom nltk.tokenize import word_tokenize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenizer(text, nouns, verbs, users, user_types):\n    \n    nlp = spacy.load(\"en_core_web_sm\")\n    doc = nlp(text)\n    \n    for token in doc:\n        if token.pos_ == 'NOUN':\n            nouns.append(str(token))\n\n        if token.pos_ == 'VERB':\n            verbs.append(str(token))\n\n    for entity in doc.ents:\n        users.append(entity.text)\n        user_types.append(entity.label_)\n    \n    nouns = [word for word in nouns if not word in stopwords.words()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Keep in mind the input is made to understand short conversations - especially one-liners.\n\nBasically, we separate the text blob by the '\\n', or newline symbol.\nThis is done by using the splitlines() function.\n\nIf any respective dialogue consists of two lines, the parser will be confused and allocate an extra user to the extra line.\n\nThis is done with consideration to social messaging applications, as users frequently communicate with short dialogues."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_tokens(text):\n    \n    conversation = text.splitlines()\n\n    token_summary = pd.DataFrame({'Users': [], 'Nouns': [], 'Verbs': [], 'User_Types': []})\n\n    for dialogue in conversation:\n    \n        nouns = []\n        verbs = []\n        users = []\n        user_types = []\n\n        tokenizer(dialogue, nouns, verbs, users, user_types)\n    \n        nouns = list(OrderedDict.fromkeys(nouns))\n        verbs = list(OrderedDict.fromkeys(verbs))\n    \n        token_summary = token_summary.append({'Users': users, 'Nouns': nouns, 'Verbs': verbs, 'User_Types': user_types}, ignore_index=True)\n    \n    return conversation, token_summary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Syntax Analysis:"},{"metadata":{},"cell_type":"markdown","source":"The main point of understanding the syntax of the sentence is to check the order by which tokens are parsed.\n\nFor example, seeing a \"not\" before a verb, say \"enjoying\", can imply a negative sentiment.\n\nSentiment analysis is an important tool in recommendation - it helps recommend products to those who want it.\n\nPeople who express negative sentiment towards a product will not recieve recommendations on it.\n    \nInstead, they will get recommendations based on what their friends like."},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\n\ndef get_sentiments(conversation):\n\n    sentiments = pd.DataFrame({'Dialogue': [], 'Compound': [], 'Negative': [], 'Neutral': [], 'Positive': []})\n\n    sid = SentimentIntensityAnalyzer()\n\n    for dialogue in conversation:\n    \n        ss = sid.polarity_scores(dialogue)\n    \n        sentiments = sentiments.append({'Dialogue': dialogue, 'Compound': ss['compound'], 'Negative': ss['neg'], 'Neutral': ss['neu'], 'Positive': ss['pos']}, ignore_index = True)\n        \n    return sentiments","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The NTLK Vader Sentiment Analyzer is an excellent tool for quickly analyzing the sentiment of a sentence.\n\nWe will obtain the polarity scores in each sentiment category, i.e \"Positive\", \"Negative\", \"Neutral\", and \"Compound\", for each user.\n\nA summary of the sentiments expressed by each user will be shown in the 'Input and Output' section."},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_dependency_tree(conversation):\n\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    for dialogue in conversation:\n    \n        doc = nlp(dialogue)\n\n        displacy.render(doc, style=\"dep\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Spacy library consists of one more feature - the ability to show dependency trees.\n\nDependency trees are useful in understanding the parsing process, in pursuit of finding the sentiments.\n\nYou can see the respective dependency trees for each user in the \"Input and Output\" section."},{"metadata":{},"cell_type":"markdown","source":"# Recommendation(Semantic):"},{"metadata":{},"cell_type":"markdown","source":"This is the recommendation section.\n\nWe will check the list of nouns from our summary of tokens to see what the interests of the users are.\n\nIf the particular noun is present in any product name, we will store that product name in a new list.\n\nThis new list is considered as our initial set of recommendations."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_init_rec(token_summary):\n\n    rec_1 = []\n\n    for i in range(len(token_summary['Nouns'])):\n    \n        sublist = []\n    \n        for x in token_summary['Nouns'][i]:\n        \n            for y in complete_list:\n            \n                if x.lower() in y.lower():\n                    sublist.append(y)\n        \n        rec_1.append(sublist)\n        \n    return rec_1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, our initial recommendations are quite a large number.\n\nWe will pick five products at random, to recommend to each user.\n\nOf course, we do not want to get the same products again in our random selection.\n\nThis is easily achieved by using random.sample(list, k), where k is the number of items selected. This function also ensures that there is no repetition in the selection of items."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_actual_rec(rec_1):\n\n    recommendations = []\n\n    for i in range(len(rec_1)):\n    \n        sublist = random.sample(rec_1[i], 5)\n    \n        recommendations.append(sublist)\n        \n    return recommendations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mixed_rec(rec, index):\n    \n    temp = []\n    \n    for i in range(len(rec)):\n        \n        if(i==index):\n            if(i == len(rec)-1):\n                temp.append(random.choice(rec[i-1]))\n            else:\n                temp.append(random.choice(rec[i+1]))\n        else:\n            temp.append(random.choice(rec[i]))\n        \n    return temp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As stated before, the sentiment of the user is considered as a factor in the recommendation process.\n\nThe compounded polarity score is considered here.\n\nIf a user's sentiment is negative, then we pick a random recommendation based on the interests of that user's friends.\n\nOtherwise, if it is positive or neutral, we can directly recommend based on their individual interest."},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_rec():\n\n    print(\"Recommendations are: \\n\")\n\n    i = 0\n\n    for u in token_summary['Users']:\n            \n        for x in u:\n            \n            if(sentiments['Compound'][i] > 0):     \n                print(x, '->', recommendations[i])\n            elif(sentiments['Compound'][i] < 0):\n                print(x, '->', mixed_rec(recommendations, i))\n            else:\n                print(x, '->', recommendations[i])\n        \n            print()\n            i = i + 1 ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Input and Output"},{"metadata":{},"cell_type":"markdown","source":"**This is the input text.**\n\n**Every time you want to get some new recommendations, tweak this and run the cells below it.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \"\"\" Rahul : I like to play on my tablet.\n Gokul : My dog does not enjoy using dog shampoo.\n Ankita : I love to wear a kurta.\n Badrinath : I frequently wear a shirt.\n Vijay : I am searching for a nice watch. \n 3 : I like to play cricket.\n John: This shirt is my favourite.\n Adam: Shall we buy a new watch?\n Oliver: I got a new smart phone.\n Vikram: I really like this mouse.\n Ramaswamy: I am planning to buy a watch.\n Sabu : Do you have the charger?\n Seetha: I am eager to see my new saree.\n Meenakshi: My parrot sat on the sofa.\n Parameshwar : I am waiting to get the laptop.\n Tom: These people need a good refrigerator.\n Sally: My mother wants a hat for the hot summers.\n Krishna: That is the song for keyboard.\n Mary: The cake is ready in the oven. \n Nicholas: I really do not like this chair. \n Sania: I love playing tennis. \n Sigmund: Do you have a discount on pet food?\n Rajan: It would be nice to get a new coffee mug.\n Regina: This new fan runs so smoothly!\n Nakatomi: One must always have a wooden table.\n Deepak: I love my dancing shoes.\n Watson: You should shine one side of the ball to spin.\n Maya: It's so chilly, I need a scarf.\n Shigeru: Back to the drawing board.\n Lara: Look at the cute brooch I found!\n Sawyer: This bike horn has been polished very nicely. \"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conversation, token_summary = get_tokens(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token_summary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiments = get_sentiments(conversation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiments","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Shown below are the dependency trees for each sentence.\n\nThe arrows infer dependencies between words. If an arrow points from 'a' to 'b', then it can be understood that 'b' depends on 'a'.\n\nThis is useful in seeing how the tokens are tagged, and how the sentence is syntactically analyzed."},{"metadata":{"trusted":true},"cell_type":"code","source":"display_dependency_tree(conversation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the final recommendation.\n\nYou can see the user's names followed by a list of recommendations based on their interests.\n    \nKeep in mind that for the most part the recommendations are accurate, but you may chance upon some outliers, due to the random nature of product selection."},{"metadata":{"trusted":true},"cell_type":"code","source":"rec_1 = create_init_rec(token_summary)\nrec_1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recommendations = get_actual_rec(rec_1)\nprint_rec()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**You have reached the end of this notebook. Thank you for reading!**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}