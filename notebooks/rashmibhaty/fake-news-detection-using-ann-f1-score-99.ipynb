{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Data Preparation:**\nData set is read. Is_True comumn is added, which will be used as dependent variable. \"title\" and \"text\" columns are combined to get maximum data.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndataset_fake = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/Fake.csv')\ndataset_true = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/True.csv')\n\ndataset_fake[\"Is_True\"]=0\ndataset_true[\"Is_True\"]=1\n\ndataset = pd.concat([dataset_fake,dataset_true]) #Merging the 2 datasets\n\ndataset[\"Full_Content\"] = dataset['title']+ \" \" + dataset['text']\n\ndataset.sample(2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data Cleaning :**\nData is cleaned to remove noise.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cleaning the texts\nimport re\nimport nltk\n#nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\n\n\ndef simplify_text(string):\n    review = re.sub('[^a-zA-Z]', ' ', string)\n    review = review.lower()\n    review = review.split()\n    ps = PorterStemmer()\n    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n    review = ' '.join(review)\n    return(string)\n    \n    \ndataset['Full_Content_New']=dataset['Full_Content'].apply(simplify_text)\nprint(\"Concat Done\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Feature Extraction : **\nUsing Bag of Words Model, important features are extacted.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Dataset Split : **\nData set is devided into training, validation and test sets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features=3000\n# Creating the Bag of Words model\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = features)\nX = cv.fit_transform(dataset['Full_Content_New'])\ny = dataset.iloc[:,4].values\n\n# Splitting the dataset into the Training set, Validation set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 0)\nX_test1, X_test2, y_test1, y_test2 = train_test_split(X_test, y_test, test_size = 0.50, random_state = 0)\n\nprint(\"Split done\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Neural Network Training and Prediction Step:\n\nHere neutal network with one hidden layer is used with binary_crossentropy as loss function.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\n\n# Initialising the ANN\nmodel = Sequential()\n\n# Adding the input layer and the first hidden layer\nmodel.add(Dense(features, kernel_initializer='uniform',activation = 'relu', input_dim = features))\n   \n# Adding the output layer\nmodel.add(Dense(1,kernel_initializer='uniform',activation = 'sigmoid'))\n\n# Compiling the ANN\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\nhistory = model.fit(X_train.toarray(), y_train, batch_size = 500, epochs = 12, validation_data=(X_test1.toarray(), y_test1))\n\n# Get training and validation loss histories\ntrain_loss = history.history['loss']\nvalidation_loss = history.history['val_loss']\n\n# Create count of the number of epochs\nepoch_count = range(1, len(train_loss) + 1)\n\nimport matplotlib.pyplot as plt\n# Visualize loss history\nplt.plot(epoch_count, train_loss, 'r--')\n#plt.plot(epoch_count, accuracy_val, 'b-')\nplt.plot(epoch_count, validation_loss, 'g--')\nplt.legend(['Training Loss', 'Validation Loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show();\n\n\n# Predicting the Test set results\ny_pred = model.predict(X_test2.toarray())\ny_pred = (y_pred > 0.5)\n\nprint(\"--------------------------------------------\")\nprint(\"Printing classification_report for Test Set\")    \nfrom sklearn.metrics import classification_report\nprint (classification_report(y_test2, y_pred))\nprint(\"--------------------------------------------\")\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion:\nIt is obseved that we get good performance for batch_size = 500, epochs = 12.\nWith these parameters we can see that F1 score using this model is >0.99 on test data.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}