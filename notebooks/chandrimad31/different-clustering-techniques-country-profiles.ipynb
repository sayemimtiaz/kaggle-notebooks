{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook, I tried to analyze country profiling data using various clustering techniques. The following four types of unsupervised techniques are used:\n* PCA Decomposition (Dimensionality Reduction)\n* K-Means Clustering (Centroid Based) Clustering\n* Hierarchical (Divisive and Agglomerative) Clustering\n* DBSCAN (Density Based) Clustering\n\n# Importing Data","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\ndata = pd.read_csv('../input/unsupervised-learning-on-country-data/Country-data.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the contents of data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What do the column headings mean? Let's check the data dictionary.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndata_dict = pd.read_csv('../input/unsupervised-learning-on-country-data/data-dictionary.csv')\ndata_dict.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analyzing Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data analysis baseline library\n!pip install dabl","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are using Data Analysis Baseline Library here. It will help us analyze the data with respect to the target column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import dabl\nimport warnings\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings('ignore')\nplt.style.use('ggplot')\nplt.rcParams['figure.figsize'] = (12, 6)\ndabl.plot(data, target_col = 'gdpp')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe very close positive correlation between \"Income\" and \"GDPP\". Also, \"Exports\", \"Imports\", \"Health\" have sort of positive correlation with \"GDPP\".\n\nHowever, we will now drop the column \"Country\" not because it is the only categorical (object type) parameter, but because it is not a deciding parameter to keep/not-keep a particular record within a cluster. In short, \"Country\" is a feature which is not required here for unsupervised learning.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exclude \"Country\" column\ndata = data.drop('country', axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use simple profile reporting where we can get an easy overview of variables, and we can explore interactions (pair-wise scatter plots), correlations (Pearson's, Spearman's, Kendall's, Phik), missing value information - all in one place. The output it produces is a bit long though, and we need to scroll down and toggle different tabs to view all the results, but the time you spend on it is worth it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%javascript\nIPython.OutputArea.prototype._should_scroll = function() {\n    return False;\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas_profiling as profile\nprofile.ProfileReport(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Gist of Overview:**\n* Average death of children under age 5 in every 100 people: 38.27\n* Average life expectancy: 70.56 (highly negatively skewed distribution)\n* Health has a perfectly symmetric distribution with mean 6.82 \n* Average exports of goods and services per capita: 41.11\n* Average imports of goods and services per capita: 46.89 (which is > avg. exports)\n* Average net income per person: 17144.69 (highly positively skewed distribution)\n* Average inflation: 7.78 (has a wide spread ranging from min -4.21 till +104)\n* Average GDP per capita: 12964.15 (highly negatively skewed distribution)\n\n**Gist of Interactions:**\n* Child Mortality has a perfect negative correlation with Life Expectancy\n* Total Fertility has somewhat positive correlation with Child Mortality\n* Exports and Imports have rough positive correlation\n* Income and GDPP have fairly positive correlation\n\n**Gist of Missing Values:**\n* There is no missing value in data\n\nWe will discuss correlation coefficients in detail later.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#More prominent correlation plot\nimport numpy as np\nimport seaborn as sns\ncorr = data.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(12, 12))\ncmap = sns.light_palette('black', as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=None, center=0,square=True, annot=True, linewidths=.5, cbar_kws={\"shrink\": .9})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights from Pearson's Correlation Coefficient Plot :**\n\n* Imports have high positive correlation with Exports (+0.74)\n* Income has fairly high positive correlation with Exports (+0.52)\n* Life Expectancy has fairly high positive correlation with Income (+0.61)\n* Total Fertility has very high positive correlation with Child Mortality (+0.85)\n* GDPP has very high positive correlation with Income (+0.90)\n* GDPP has fairly high positive correlation with Life Expectancy (+0.60)\n* Total Fertility has fairly high negative correlation with Life Expectancy (-0.76) - Well, I found this particular thing as an interesting insight but let's not forget **\"Correlation does not imply causation\"!**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# (1) Principal Component Analysis\n\nPrincipal Component Analysis (PCA) is a popular technique for deriving a set of low dimensional features from a large set of variables. Sometimes reduced dimensional set of features can represent distinct no. of groups with similar characteristics. Hence PCA can be an insightful clustering tool (or a preprocessing tool before applying clustering as well). We will standardize our data first and will use the scaled data for all clustering works in future.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\ndata_scaled=sc.fit_transform(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, I have used singular value decomposition solver \"auto\" to get the no. of principal components. You can also use solver \"randomized\" introducing a random state seed like \"0\" or \"12345\".","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npc = PCA(svd_solver='auto')\npc.fit(data_scaled)\nprint('Total no. of principal components =',pc.n_components_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Print Principal Components\nprint('Principal Component Matrix :\\n',pc.components_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us check the amount of variance explained by each principal component here. They will be arranged in decreasing order of their explained variance ratio.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#The amount of variance that each PC explains\nvar = pc.explained_variance_ratio_\nprint(var)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot explained variance ratio for each PC\nplt.bar([i for i, _ in enumerate(var)],var,color='black')\nplt.title('PCs and their Explained Variance Ratio', fontsize=15)\nplt.xlabel('Number of components',fontsize=12)\nplt.ylabel('Explained Variance Ratio',fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see, the variance explained by first and second principal components are nearly 46% and 17% respectively. We can now count cumulative variance explained by them. For convenience of observation, we are converting the figures to percentages here.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cumulative Variance explained by each PC\nimport numpy as np\ncum_var = np.cumsum(np.round(pc.explained_variance_ratio_, decimals=4)*100)\nprint(cum_var)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using these cumulative variance ratios for all PCs, we will now draw a scree plot. It is used to determine the number of principal components to keep in this principal component analysis.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scree Plot\nplt.plot(cum_var, marker='o')\nplt.title('Scree Plot: PCs and their Cumulative Explained Variance Ratio',fontsize=15)\nplt.xlabel('Number of components',fontsize=12)\nplt.ylabel('Cumulative Explained Variance Ratio',fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plot indicates the threshold of 90% is getting crossed at PC = 4. Ideally, we can keep 4 (or atmost 5) components here. Before PC = 5, the plot is following an upward trend. After crossing 5, it is almost steady. However, we have retailed all 9 PCs here to get the full data in results. And for visualization purpose in 2-D figure, we have plotted only PC1 vs PC2.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Principal Component Data Decomposition\ncolnames = list(data.columns)\npca_data = pd.DataFrame({ 'Features':colnames,'PC1':pc.components_[0],'PC2':pc.components_[1],'PC3':pc.components_[2],\n                          'PC4':pc.components_[3],'PC5':pc.components_[4], 'PC6':pc.components_[5], 'PC7':pc.components_[6], \n                          'PC8':pc.components_[7], 'PC9':pc.components_[8]})\npca_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualize 2 main PCs\nfig = plt.figure(figsize = (12,6))\nsns.scatterplot(pca_data.PC1, pca_data.PC2,hue=pca_data.Features,marker='o', s=500)\nplt.title('PC1 vs PC2',fontsize=15)\nplt.xlabel('Principal Component 1',fontsize=12)\nplt.ylabel('Principal Component 2',fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that 1st Principal Component (X-axis) is gravitated mainly towards features like: life expectancy, gdpp, income. 2nd Principal Component (Y-axis) is gravitated predominantly towards features like: imports, exports.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Export PCA results to file\npca_data.to_csv(\"PCA_results.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# (2) K-Means Clustering\n\nThis is the most popular method of clustering. It uses Euclidean distance between clusters in each iteration to decide a data point should belong to which cluster, and proceed accordingly. To decide how many no. of clusters to consider, we can employ several methods. The basic and most widely used method is **Elbow Curve**.\n\n**Method-1: Plotting Elbow Curve**\n\nIn this curve, wherever we observe a \"knee\" like bent, we can take that number as the ideal no. of clusters to consider in K-Means algorithm.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting Elbow Curve\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom sklearn import metrics\n\nmodel = KMeans()\nvisualizer = KElbowVisualizer(model, k=(1,10))\nvisualizer.fit(data_scaled)    \nvisualizer.poof()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, along Y-axis, \"distortion\" is defined as \"the sum of the squared differences between the observations and the corresponding centroid\". It is same as WCSS (Within-Cluster-Sum-of-Squares).\n\nLet's see the centroids of the clusters. Afterwards, we will fit our scaled data into a K-Means model having 3 clusters, and then label each data point (each record) to one of these 3 clusters.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting data into K-Means model with 3 clusters\nkm_3=KMeans(n_clusters=3,random_state=12345)\nkm_3.fit(data_scaled)\nprint(km_3.cluster_centers_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(km_3.labels_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see each record has got a label among 0,1,2. This label is each of their cluster_id i.e. in which cluster they belong to. We can count the records in each cluster now.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(km_3.labels_).value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see, the highest no. of records belong to the first cluster.\n\nNow, we are interested to check how good is our K-Means clustering model. Silhouette Coefficient is one such metric to check that. The **Silhouette Coefficient** is calculated using: \n* the mean intra-cluster distance ( a ) for each sample\n* the mean nearest-cluster distance ( b ) for each sample\n* The Silhouette Coefficient for a sample is (b - a) / max(a, b)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate Silhouette Coefficient for K=3\nfrom sklearn import metrics\nmetrics.silhouette_score(data_scaled, km_3.labels_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate SC for K=2 through K=10\nk_range = range(2, 10)\nscores = []\nfor k in k_range:\n    km = KMeans(n_clusters=k, random_state=12345)\n    km.fit(data_scaled)\n    scores.append(metrics.silhouette_score(data_scaled, km.labels_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can check **Silhouette Scores** for K-Means model with different no. of clusters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe the highest silhouette score with no. of clusters 3 and 4. However, from Elbow Curve, we got to see the \"knee\" like bent at no. of clusters 3. So we will do further analysis to choose the ideal no. of clusters between 3 and 4.\n\nFor further analysis, we will consider **Davies-Bouldin Score** apart from Silhouette Score. **Davies-Bouldin Score** is defined as the average similarity measure of each cluster with its most similar cluster, where similarity is the ratio of within-cluster distances to between-cluster distances. Thus, clusters which are farther apart and less dispersed will result in a better score.\n\nWe will also analyze **SSE (Sum of Squared Errors)**. SSE is the sum of the squared differences between each observation and its cluster's mean. It can be used as a measure of variation within a cluster. If all cases within a cluster are identical the SSE would then be equal to 0. The formula for SSE is: 1\n\n**Method-2: Plotting of SSE, Davies-Bouldin Scores, Silhouette Scores to Decide Ideal No. of Clusters**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import davies_bouldin_score, silhouette_score, silhouette_samples\nsse,db,slc = {}, {}, {}\nfor k in range(2, 10):\n    kmeans = KMeans(n_clusters=k, max_iter=1000,random_state=12345).fit(data_scaled)\n    if k == 4: labels = kmeans.labels_\n    clusters = kmeans.labels_\n    sse[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center\n    db[k] = davies_bouldin_score(data_scaled,clusters)\n    slc[k] = silhouette_score(data_scaled,clusters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting SSE\nplt.figure(figsize=(12,6))\nplt.plot(list(sse.keys()), list(sse.values()))\nplt.xlabel(\"Number of cluster\", fontsize=12)\nplt.ylabel(\"SSE (Sum of Squared Errors)\", fontsize=12)\nplt.title(\"Sum of Squared Errors vs No. of Clusters\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see \"knee\" like bent at both 3 and 4, still considering no. of clusters = 4 seems a better choice, because after 4, there is no further \"knee\" like bent observed. Still, we will analyse further to decide between 3 and 4.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting Davies-Bouldin Scores\nplt.figure(figsize=(12,6))\nplt.plot(list(db.keys()), list(db.values()))\nplt.xlabel(\"Number of cluster\", fontsize=12)\nplt.ylabel(\"Davies-Bouldin values\", fontsize=12)\nplt.title(\"Davies-Bouldin Scores vs No. of Clusters\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly, no. of clusters = 3 is the best choice here.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nplt.plot(list(slc.keys()), list(slc.values()))\nplt.xlabel(\"Number of cluster\", fontsize=12)\nplt.ylabel(\"Silhouette Score\", fontsize=12)\nplt.title(\"Silhouette Score vs No. of Clusters\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No. of clusters = 3 seems the best choice here as well. The silhouette score ranges from âˆ’1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. A score nearly 0.28 seems a good one.\n\n* **Silhouette Plots for Different No. of Clusters :**\nWe will now draw Silhouette Plots for different no. of clusters for getting more insights. Side by side, we will observe the shape of the clusters in 2-dimensional figure.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Silhouette Plots for Different No. of Clusters\nimport matplotlib.cm as cm\nimport numpy as np\nfor n_clusters in range(2, 10):\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 8)\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1 but here the range is from -0.2 till 1\n    ax1.set_xlim([-0.2, 1])\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette plots of individual clusters, to demarcate them clearly.\n    ax1.set_ylim([0, len(data_scaled) + (n_clusters + 1) * 10])\n    # Initialize the clusterer with n_clusters value and a random generator seed of 12345 for reproducibility.\n    clusterer = KMeans(n_clusters=n_clusters,max_iter=1000, random_state=12345)\n    cluster_labels = clusterer.fit_predict(data_scaled)\n    # The silhouette_score gives the average value for all the samples\n    # This gives a perspective into the density and separation of the formed clusters\n    silhouette_avg = silhouette_score(data_scaled, cluster_labels)\n    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(data_scaled, cluster_labels)\n    y_lower = 10\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to cluster i and sort them\n        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n        ith_cluster_silhouette_values.sort()\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n        color = cm.nipy_spectral(float(i) / n_clusters)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  \n\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels\n    ax1.set_xticks([-0.2, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n    # 2nd Plot showing the actual clusters formed\n    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n    ax2.scatter(data_scaled[:, 0], data_scaled[:, 1], marker='.', s=30, lw=0, alpha=0.7, c=colors, edgecolor='k')\n\n    # Labeling the clusters\n    centers = clusterer.cluster_centers_\n    # Draw white circles at cluster centers\n    ax2.scatter(centers[:, 0], centers[:, 1], marker='o', c=\"white\", alpha=1, s=200, edgecolor='k')\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50, edgecolor='k')\n\n    ax2.set_title(\"The visualization of the clustered data.\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\" % n_clusters), fontsize=14, fontweight='bold')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We get a very prominent silhouette plot for no. of clusters = 3. We recall the KMeans model we fitted earlier on our scaled country data with 3 clusters (km_3). We will use that model again to label each record of our data set with a particular cluster_id.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_labels = km_3.fit_predict(data_scaled)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we will see data with labels (cluster_ids labelled).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = km_3.labels_\ndata_df = pd.DataFrame(data)\ndata_df['KM_Clusters'] = preds\ndata_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will visualize 3 clusters now for various pairs of features. Initially, I chose the pairs randomly. Later, I chose the pairs including \"GDPP\", \"income\", \"inflation\" etc. important features. Since we are concerned about analyzing country profiles and \"GDPP\" is the main indicator to represent a country's status, we are concerned with that mainly.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualize clusters: Feature Pair-1\nimport matplotlib.pyplot as plt_1\nplt_1.rcParams['axes.facecolor'] = 'lightblue'\nplt_1.figure(figsize=(12,6))\nplt_1.scatter(data_scaled[:,0],data_scaled[:,1],c=cluster_labels) #child mortality vs exports\nplt_1.title(\"Child Mortality vs Exports (Visualize KMeans Clusters)\", fontsize=15)\nplt_1.xlabel(\"Child Mortality\", fontsize=12)\nplt_1.ylabel(\"Exports\", fontsize=12)\nplt_1.rcParams['axes.facecolor'] = 'lightblue'\nplt_1.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, we are more interested to dive deep into GDPP of the countries. So we can proceed with plotting \"Income\" vs \"GDPP\"; \"Inflation\" vs \"GDPP\" next.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualize clusters: Feature Pair-2\nplt_1.figure(figsize=(12,6))\nplt_1.scatter(data_scaled[:,4],data_scaled[:,8],c=cluster_labels) # income vs gdpp\nplt_1.title(\"Income vs GDPP (Visualize KMeans Clusters)\", fontsize=15)\nplt_1.xlabel(\"Income\", fontsize=12)\nplt_1.ylabel(\"GDPP\", fontsize=12)\nplt_1.rcParams['axes.facecolor'] = 'lightblue'\nplt_1.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualize clusters: Feature Pair-3\nplt_1.figure(figsize=(12,6))\nplt_1.scatter(data_scaled[:,5],data_scaled[:,8],c=cluster_labels) # inflation vs gdpp\nplt_1.title(\"Inflation vs GDPP (Visualize KMeans Clusters)\", fontsize=15)\nplt_1.xlabel(\"Inflation\", fontsize=12)\nplt_1.ylabel(\"GDPP\", fontsize=12)\nplt_1.rcParams['axes.facecolor'] = 'lightblue'\nplt_1.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Export KMeans results to file\ndata_df.to_csv(\"KMeans_results.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# (3) Hierarchical Clustering\n\nThere are two types of hierarchical clustering: **Divisive** and **Agglomerative**. In divisive (top-down) clustering method, all observations are assigned to a single cluster and then that cluster is partitioned to two least similar clusters, and then those two clusters are partitioned again to multiple clusters, and thus the process go on. In agglomerative (bottom-up), the opposite approach is followed. Here, the ideal no. of clusters is decided by **dendrogram**.\n\n* **Method-1: Dendrogram Plotting using Clustermap**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\ncmap = sns.cubehelix_palette(as_cmap=True, rot=-.3, light=1)\ng = sns.clustermap(data_scaled, cmap=cmap, linewidths=.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above dendrogram, we can consider 2 clusters at minimum or 6 clusters at maximum. We will again cross-check the dendrogram using **Ward's Method**. Ward's method is an alternative to single-link clustering. This algorithm works for finding a partition with small sum of squares (to minimise the within-cluster-variance).\n\n* **Method-2: Dendrogram Plotting using Ward's Method**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using the dendrogram to find the optimal number of clusters\nimport scipy.cluster.hierarchy as sch\n\nplt.rcParams['axes.facecolor'] = 'white'\nplt.rcParams['axes.grid'] = False\ndendrogram = sch.dendrogram(sch.linkage(data_scaled, method='ward'))\nplt.title(\"Dendrogram using Ward's Method\", fontsize=15)\nplt.xlabel('Clusters', fontsize=12)\nplt.ylabel('Euclidean distances', fontsize=12)\nplt.rcParams['axes.facecolor'] = 'white'\nplt.rcParams['axes.grid'] = False\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see 3 prominent clusters here (green, red, skyblue). We will now follow the similar process of labelling data with cluster ids, then visualize the 3 clusters using various feature pairs, and finally we will export the results to a .csv file.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\nhc = AgglomerativeClustering(n_clusters=3, affinity = 'euclidean', linkage = 'ward')\nY_hc = hc.fit_predict(data_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_hc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hcpreds = hc.labels_\ndata_hc_df = data.drop('KM_Clusters', axis=1)\ndata_hc_df['Hier_Clusters'] = hcpreds\ndata_hc_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualize clusters: Random Feature Pair-1 (income vs gdpp)\nimport matplotlib.pyplot as plt_2\nplt_2.rcParams['axes.facecolor'] = 'seagreen'\nplt_2.rcParams['axes.grid'] = True\nplt_2.figure(figsize=(12,6))\n#datahc_df = pd.DataFrame(data_hc_df)\nplt_2.scatter(data_hc_df['income'],data_hc_df['gdpp'],c=cluster_labels) \nplt_2.title('Income vs GDPP (Visualize Hierarchical Clusters)', fontsize=15)\nplt_2.xlabel(\"Income\", fontsize=12)\nplt_2.ylabel(\"GDPP\", fontsize=12)\nplt_2.rcParams['axes.facecolor'] = 'seagreen'\nplt_2.rcParams['axes.grid'] = True\nplt_2.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plot looks similar to what we had got earlier for the KMeans. Observing minutely both KMeans clustering's \"Income vs GDPP\" plot and hierarchical clustering's \"Income vs GDPP\" plot, we can detect the difference in assigning cluster labels for a few data points though.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualize clusters: Random Feature Pair-2 (inflation vs gdpp)\nplt_2.figure(figsize=(12,6))\nplt_2.scatter(data_hc_df['inflation'],data_hc_df['gdpp'],c=cluster_labels) \nplt_2.title('Inflation vs GDPP (Visualize Hierarchical Clusters)', fontsize=15)\nplt_2.xlabel(\"Inflation\", fontsize=12)\nplt_2.ylabel(\"GDPP\", fontsize=12)\nplt_2.rcParams['axes.facecolor'] = 'seagreen'\nplt_2.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For this pair, the plot looks similar to what we had got earlier for the KMeans. Also, observing minutely both KMeans clustering's \"Inflation vs GDPP\" plot and hierarchical clustering's \"Inflation vs GDPP\" plot, we find almost no difference in assigning cluster labels for all data points.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Export hierarchical clustering results\ndata_hc_df.to_csv('Hierchical_Results.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# (4) DBSCAN Clustering\n\nDBSCAN is an abbreviation of \"Density-based spatial clustering of applications with noise\". This algorithm groups together points that are close to each other based on a distance measurement (usually Euclidean distance) and a minimum number of points. It also marks noise as outliers (noise means the points which are in low-density regions).\n\n**I found an interesting result with DBSCAN when I used all features of country data. It gave me a single cluster.** I presume, that was very evident to happen because our data is almost evenly spread, so density wise, this algorithm could not bifurcate the datapoints into more than one cluster. Hence, I used only the features which have high correlation with \"GDPP\". I also kept \"Child Mortality\" and \"Total Fertility\" in my working dataset since they have polarizations - some data points have extremely high values, some have extremely low values (ref. to corresponding scatter plots in data profiling section in the beginning).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import DBSCAN\nimport sklearn.utils\nfrom sklearn.preprocessing import StandardScaler\n\nClus_dataSet = data[['child_mort','exports','health','imports','income','inflation','life_expec','total_fer','gdpp']]\nClus_dataSet = np.nan_to_num(Clus_dataSet)\nClus_dataSet = np.array(Clus_dataSet, dtype=np.float64)\nClus_dataSet = StandardScaler().fit_transform(Clus_dataSet)\n\n# Compute DBSCAN\ndb = DBSCAN(eps=1, min_samples=3).fit(Clus_dataSet)\ncore_samples_mask = np.zeros_like(db.labels_)\ncore_samples_mask[db.core_sample_indices_] = True\nlabels = db.labels_\n#data['Clus_Db']=labels\n\nrealClusterNum=len(set(labels)) - (1 if -1 in labels else 0)\nclusterNum = len(set(labels)) \n\n# A sample of clusters\nprint(data[['child_mort','exports','health','imports','income','inflation','life_expec','total_fer','gdpp']].head())\n\n# number of labels\nprint(\"number of labels: \", set(labels))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have got 7 clusters using density based clustering which is a distinct observation (7 is much higher than 3 which we got in all three different clustering algorithms we used earlier).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"db.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(db.labels_).value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can spot that 78.44% data points have been labelled to the 1st and 2nd clusters.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the cluster labels and sort by cluster\ndatacopy = data.copy()\ndatacopy = datacopy.drop('KM_Clusters', axis=1)\ndatacopy['DB_cluster'] = db.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# review the cluster centers\ndatacopy.groupby('DB_cluster').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualize clusters: Random Feature Pair-1 (income vs gdpp)\nimport matplotlib.pyplot as plt_3\nplt_3.rcParams['axes.facecolor'] = 'orange'\nplt_3.figure(figsize=(12,6))\nplt_3.scatter(datacopy['income'],datacopy['gdpp'],c=db.labels_) \nplt_3.title('Income vs GDPP (Visualize DBSCAN Clusters)', fontsize=15)\nplt_3.xlabel(\"Income\", fontsize=12)\nplt_3.ylabel(\"GDPP\", fontsize=12)\nplt_3.rcParams['axes.facecolor'] = 'orange'\nplt_3.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualize clusters: Random Feature Pair-2 (inflation vs gdpp)\nimport matplotlib.pyplot as plt_3\nplt_3.figure(figsize=(12,6))\nplt_3.scatter(datacopy['inflation'],datacopy['gdpp'],c=db.labels_) \nplt_3.title('Inflation vs GDPP (Visualize DBSCAN Clusters)', fontsize=15)\nplt_3.xlabel(\"Inflation\", fontsize=12)\nplt_3.ylabel(\"GDPP\", fontsize=12)\nplt_3.rcParams['axes.facecolor'] = 'orange'\nplt_3.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In both of the above plots, we can well observe the prevalence of two clusters (1st and 2nd) only.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data with labelled cluster_ids\ndatacopy.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Export DBSCAN results\ndatacopy.to_csv('DBSCAN_results.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}