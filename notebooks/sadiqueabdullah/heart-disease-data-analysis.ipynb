{"cells":[{"metadata":{},"cell_type":"markdown","source":"**INTRODUCTION:**\n\nThe reason behind this work was the say \" Learning by doing\"..So be it!! I am learning now. \n\nThis dataset being clean and small and relevant to everybody , was easy to analyze. By end of the analysis,the self expectation is to run anyone's report with 13 independent variables and get a sense if he/she is more likely to get heart attack. It may help in treating the right person and at the right time to minimize the damage. \n\nIn the following analysis  I tried to corelate the results with two different approaches.\n\n1)EDA with basic python libraries usage\n2)Analysing problem through Logistic regression Modeling"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nprint(os.listdir(\"../input\"))\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking top few rows of the data\nheart = pd.read_csv(\"../input/heart.csv\")\nheart.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the structure of the data set\n#There are 303 observations and 14 variables in the data set\n\nprint(heart.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(heart.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's sum the null values for each column\n\nheart.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This calculates the percent of missing values in eac column of the data set.\n#The results are 0 because there are no null values here.\n#I am doing this for practice purpose.\n\nround(100*(heart.isnull().sum()/len(heart.index)),2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding the statistical features of the data set using describe function.\n\nheart.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Density and histogram plots\n\nsns.boxplot(y= heart['age'])\nplt.title('Boxplot of Age')\nplt.show()\n\n#We can see that the median age here is around 55 years\n#Also most of the observations have age between 47-61 years","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating subplots\nplt.figure(figsize=(15, 12))\n\n#Subplot1 \nplt.subplot(2,2,1)\nplt.title('Distribution of Age')\nsns.distplot(heart['age'], rug = True)\n\n\n#Subplot2\nplt.subplot(2,2,2)\nplt.title('Distribution of trestbps')\nsns.distplot(heart['trestbps'], rug = True)\n\n\n#Subplot3\nplt.subplot(2,2,3)\nplt.title('Distribution of chol')\nsns.distplot(heart['chol'], rug = True)\n\n\n#Subplot4\nplt.subplot(2,2,4)\nplt.title('Distribution of thalach')\nsns.distplot(heart['thalach'], rug = True)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.figure(figsize=(10, 6))\n\n#scatter plot of age and trestbps\n#sns.jointplot('age','trestbps',heart)\n#plt.show()\n## found it as not relevant for this analysis##","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's dig deeper into the concentrated part, where trestbps is less than 150 and age is between 40-70\n\ndf1 = heart[(heart.trestbps <150) & (heart.trestbps > 100) & (heart.age <= 70) & (heart.age >=40)]\n\nsns.jointplot('age','trestbps',df1, kind=\"hex\", color = 'k')\nplt.show()\n\n#More concentration can be found in case of age between 55-58 and trestbps 130","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#scatter plot of age and trestbps\n#sns.jointplot('age','chol',heart)\n#plt.show()\n## found it as not relevant for this analysis##","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualizing the concentrated part for chol\n\ndf2 = heart[(heart.chol <=300)]\nsns.jointplot('age','chol',df2, kind = 'hex', color = 'k')\nplt.show()\n\n#More concentration can be found in case of age between 50-60 and chol 200-250","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#scatter plot of age and thalach\n\n#sns.jointplot('age','thalach', heart)\n#plt.show()\n\n#shows a decreasing pattern\n## found it as not relevant for this analysis##","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's create pairwise scatterplots\ndf3 = heart[['age', 'trestbps', 'chol', 'thalach', 'oldpeak']]\n\n\nplt.figure(figsize = (15,12))\nsns.pairplot(df3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlation between the variables\n\ncor = heart.corr()\nround(cor,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making heatmap to understand the correlation better\n\nplt.figure(figsize= (15,12))\n\nsns.heatmap(cor, cmap = 'YlGnBu', annot = True)\nplt.show()\n\n#We can see that there is a positive correlation between the target and cp, thalach and slope.\n#There is a negative correaltion between target and age, gender, oldpeak, thal etc. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heart.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['sex','cp','fbs','restecg','exang','slope','ca','thal','target']:\n    heart[col]= heart[col].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now let's look at some categorical data.\n\n#Gender\nsns.countplot(x='target', data = heart, hue = 'sex')\nplt.title('Number of Male & Female with heart disease')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Chest Pain type\n\nsns.countplot(x='target', hue= 'cp',data = heart)\nplt.title('Chest Pain Type')\nplt.show()\n\n#There are 4 types of chest pain(0,1,2,3) out of which cp = 2 is the most common type of chest pain in case of people who have heart disease.\n#Those who do not have a heart disease experience cp = 0 type of chest pain.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fbs (Fasting Blood Sugar > 120 mg/dl)\n\nsns.countplot(hue='fbs',x ='target',data = heart)\nplt.title('Fasting Blood Sugar > 120 mg/dl')\nplt.show()\n\n#Patients with heart disease doesn't necessarily have fbs >120 mg/dl.  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#restecg(resting electrocardiographic results (values 0,1,2))\n\nsns.countplot(x='restecg',hue ='target',data = heart)\nplt.title('resting electrocardiographic results')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#exang- Exercise induced angina (1 = yes; 0 = no)\n\nsns.countplot(hue='exang',x ='target',data = heart)\nplt.title('Exercise induced angina (1 = yes; 0 = no)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#the slope of the peak exercise ST segment\n\nsns.countplot(hue='slope',x ='target',data = heart)\nplt.title('Slope of the peak exercise ST segment')\nplt.show()\n\n#Most of the patients who have a heart disease have a slope = 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ca- number of major vessels (0-3) colored by flourosopy\n\nsns.countplot(hue='ca',x ='target',data = heart)\nplt.title('Number of major vessels (0-3) colored by flourosopy')\nplt.show()\n\n#Most of the patients who have a heart disease have a ca = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#thal- no explanation provided, but probably thalassemia (3 normal; 6 fixed defect; 7 reversable defect)\n\nsns.countplot(hue='thal',x ='target',data = heart)\nplt.title('thal')\nplt.show()\n\n#Most of the patients who have a heart disease have a thal = 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Target\n\nsns.countplot(x ='target',data = heart)\nplt.title('Have heart disease or not')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Lets Build a model to predict Heart attack through Logistic regreesion approach. Will use \"Multivariate Logistic Regression analysis\""},{"metadata":{"trusted":true},"cell_type":"code","source":"heart.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking no. of unique entries for columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\" Unique Entries for column 'heart' are \", heart['cp'].unique())\nprint(\" Unique Entries for column 'fbs' are \", heart['fbs'].unique())\nprint(\" Unique Entries for column 'restecg' are \", heart['restecg'].unique())\nprint(\" Unique Entries for column 'exang' are \", heart['exang'].unique())\nprint(\" Unique Entries for column 'oldpeak' are \", heart['oldpeak'].unique())\nprint(\" Unique Entries for column 'slope' are \", heart['slope'].unique())\nprint(\" Unique Entries for column 'ca' are \", heart['ca'].unique())\nprint(\" Unique Entries for column 'that' are \", heart['thal'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'exang' & 'fbs' cplumns have binary entries already, all others needs to be normalised for Logistic regression analysis. We will normalise by standard deviation method"},{"metadata":{"trusted":true},"cell_type":"code","source":"df= heart[['age','trestbps','thalach','cp','chol','restecg','oldpeak','slope','ca','thal']]\ndf= df.apply(pd.to_numeric)\nnormalized_df=(df-df.mean())/df.std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"drop the orginal coulmns 'cp','restecg','oldpeak','slope','ca','thal' from the data frame as we would like to replace it with the normalized valuew"},{"metadata":{"trusted":true},"cell_type":"code","source":"heart= heart.drop(['age','trestbps','thalach','cp','chol','restecg','oldpeak','slope','ca','thal'],1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets update heart data frame with normalized values"},{"metadata":{"trusted":true},"cell_type":"code","source":"heart= pd.concat([heart,normalized_df], axis=1)\nheart.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets Check the Heart attack rate in as is condition prior to model building"},{"metadata":{"trusted":true},"cell_type":"code","source":"Heart_attack_rate= (sum(heart['target'])/len(heart['target'].index))*100\nHeart_attack_rate #54.45% is the Heart attack rate","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking the columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"heart.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Modelling : Lets define the independent and Target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"X= heart.drop(['target'], axis=1)\ny= heart['target']\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm\nimport functools\n# importing Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nX_train, X_test, y_train, y_test= train_test_split(X, y, train_size=.7, random_state=10)\n# Will Create a generalised linear model GLM for begining the model building\nlogml = sm.GLM(y_train,(sm.add_constant(X_train.astype(float))), family= sm.families.Binomial())\n\nprint(logml.fit().summary())\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets Plot the Correlation matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.figure(figsize=(20,10))\nsns.heatmap(X_train.astype(float).corr(), annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature selection using RFE"},{"metadata":{},"cell_type":"markdown","source":"UDF for calculating vif (Variance inflation factor)value to drop independent outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef vif_cal(input_data, dependent_col):\n    vif_df = pd.DataFrame( columns = ['Var', 'Vif'])\n    x_vars=input_data.drop([dependent_col], axis=1)\n    xvar_names=x_vars.columns\n    for i in range(0,xvar_names.shape[0]):\n        y=x_vars[xvar_names[i]] \n        x=x_vars[xvar_names.drop(xvar_names[i])]\n        rsq=sm.OLS(y,x).fit().rsquared  \n        vif=round(1/(1-rsq),2)\n        vif_df.loc[i] = [xvar_names[i], vif]\n    return vif_df.sort_values(by = 'Vif', axis=0, ascending=False, inplace=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculating VIF value"},{"metadata":{"trusted":true},"cell_type":"code","source":"vif_cal(input_data= heart.astype(float), dependent_col='target')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets Run the Logistic rgression model and prdict the values"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nlogsk = LogisticRegression()\nlogsk.fit(X_train.astype(float), y_train)\ny_predict= logsk.predict_proba(X_test.astype(float)) \n# Converting y_pred to a dataframe which is an array\ny_pred_df = pd.DataFrame(y_predict)\n# Converting to column dataframe\ny_pred_1 = y_pred_df.iloc[:,[1]]\ny_pred_1.head() # below data shows the probability of heart attack","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df,y_pred_1], axis =1)\n# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 1 : 'Heart_attack_Prob'})\ny_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating new column 'predicted' with 1 if Churn_Prob>0.5 else 0\ny_pred_final['predicted'] = y_pred_final.Heart_attack_Prob.map( lambda x: 1 if x > 0.5 else 0)\ny_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets find out the optimum cut-off predicted probability through ROC curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n# Confusion matrix \nconfusion = metrics.confusion_matrix( y_pred_final.target, y_pred_final.predicted)\nconfusion","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It shows as 16 cases were predicted as missed as per confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's check the overall accuracy.\nmetrics.accuracy_score( y_pred_final.target, y_pred_final.predicted)# model accuracy is 78%","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = confusion[0,0] # true positive \nTN = confusion[1,1] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sensitivity = TP/float(TP+FN)\nprint(\"Sensitivity= \",sensitivity)\nspecificity= TN/float(TN+FP)\nprint(\"specificity= \", specificity)\nprint(\"Precision= \", TP/float(TP+FP))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting ROC curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\ndef draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(6, 4))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return fpr, tpr, thresholds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_roc(y_pred_final.target, y_pred_final.predicted)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Finding Optimal Cutoff Point**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)/10 for x in range(10)]\nfor i in numbers:\n    y_pred_final[i]= y_pred_final.Heart_attack_Prob.map( lambda x: 1 if x > i else 0)\ny_pred_final.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix( y_pred_final.target, y_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])/total1\n    sensi = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n    speci = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above plot of ROC curve, the optimum cut-off seems to be .7 around"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final['final_predicted'] = y_pred_final.Heart_attack_Prob.map( lambda x: 1 if x > 0.7 else 0)\ny_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the overall accuracy.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.accuracy_score( y_pred_final.target, y_pred_final.final_predicted)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Its 79% now , which was 78% earlier"},{"metadata":{},"cell_type":"markdown","source":"Lets check the confusion matrix again"},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.confusion_matrix( y_pred_final.target, y_pred_final.final_predicted )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above data shows as only 8 cases are predicted as missed now as per confusion matrix which was 16 earlier."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}