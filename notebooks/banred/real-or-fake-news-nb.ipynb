{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Real or Fake? Detecting Fake news."},{"metadata":{},"cell_type":"markdown","source":"Fake news is rife. It is misinformation with the aim to misinform and to spread false imformation to either harm or influence with little to no evidence. In this notebook, we will explore datasets from genuine news articles and from fake news articles through graphs and examples such as word frequencies, bi-grams/tri-grams and word clouds. Then apply a classify to these datasets to see how well a machine can distinguish the difference between the two classes. Finally, we will evaluate these models using statistical evaluations such as precision, recall, F1 score and ROC.\n\nAfterwards, the best model will be used to create a web application for users to input news articles and let the model predict whether to article is real or fake.\n\nDataset for this project from Kaggle. Click [Here](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset/notebooks)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import our visual libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\n# Import text clearning libraries\nimport re\nfrom bs4 import BeautifulSoup\nimport nltk\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import the data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Input full file path for True news dataset\nreal = pd.read_csv(\"/kaggle/input/fake-and-real-news-dataset/True.csv\")\nreal.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input full file path for Fake news dataset\nfake = pd.read_csv(\"/kaggle/input/fake-and-real-news-dataset/Fake.csv\")\nfake.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the number of entries there are for each dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Real news count: \" + str(len(real)))\nprint(\"Fake news count: \" + str(len(fake)))\nprint(\"Total available entries: \" + str(len(real) + len(fake)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge the datasets\nreal[\"label\"] = 1\nfake[\"label\"] = 0\n\nframe = [real, fake]\ndf = pd.concat(frame)\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check to see if any columns have missing values\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get basic information on dataset\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run this cell to avoid indexes of previous datasets from overlapping\ndf.reset_index(inplace = True)\ndf.drop(\"index\", axis=1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check an example of a fake news text:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"text\"][40000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can probably tell from reading this article that the writing style is a sensationalist piece designed to prey of the fears of the reader. Examples being the mention of graphic imagery and not written by a professional."},{"metadata":{},"cell_type":"markdown","source":"Example of geniune news text:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"text\"][10000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see the piece is well written by a professional writer, and the subject is on politics. What I have mentioned may be naive assumptions from just reading only two brief text. In the next section, we will be diving deep into the data to help better understand what differentiates genuine and fake news."},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"**Label Count**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 0 for fake\n# 1 for true\nsns.set(style=\"darkgrid\")\nsns.countplot(df[\"label\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"subject\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Subject count by Label**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Chart to show count of subject by label\nplt.figure(figsize=(12,9))\nsns.set(style=\"darkgrid\")\nsns.countplot(df[\"subject\"], hue=df[\"label\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the chart above, genuine news have the subjecst **politicsNews** and **worldnews**. Fake news will have the subjects **News, politics, Government News, left-news, US_News** and **Middle-east**."},{"metadata":{},"cell_type":"markdown","source":"**Publish data analysis**"},{"metadata":{"trusted":true},"cell_type":"code","source":"set(df[\"date\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Date contains http links and news article titles. Months may be shortened or only month."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter out dates with http links\nhttpremove = \"http\"\nfilter1 = df[\"date\"].str.contains(httpremove)\ndf_ = df[filter1]\ndf_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_[\"text\"][30775]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataset with wrong dates and meanliness text\ndf_ = df[df[\"date\"].apply(lambda x: len(x) > 20)]\ndf_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's okay to remove entries that are not considered news: all the text in this dataset contains http links and left-behind code."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Want to remove entries which are not dates\ndf = df[df[\"date\"].apply(lambda x: len(x) < 20)]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 44888 entries after removing non-dates\ndf[\"title\"].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_[\"date\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform dates to datetime\n# Use to_period('M') to get datetime to month\ndf_['date'] = pd.to_datetime(df_['date']).dt.to_period('M')\ndf_.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check count of articles by Year\n# Over half of the articles are from 2017\ndf_[\"date\"].apply(lambda x: (str(x)[:4])).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get number of articles by Year-Month\n# Change date type to string from datetime format\ndf_[\"date\"] = df_[\"date\"].apply(lambda x: (str(x)[:7]))\ndf_.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DataFrame of year of count by Year-Month\nyear_month = pd.DataFrame(df_[\"date\"].value_counts()).sort_index()\nyear_month.reset_index(inplace=True)\nyear_month[\"index\"] = year_month[\"index\"].astype(str)\nyear_month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count of articles by Month\nplt.figure(figsize=(12,9))\nplt.bar(year_month[\"index\"], year_month[\"date\"])\nplt.xticks(rotation=45)\nplt.xlabel(\"Year and Month\")\nplt.ylabel(\"Count\")\nplt.title(\"Count of articles by Month/Year\")\nplt.show","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Count by Month-Year**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot count of articles by Month-Year\ndf_1 = df_[df_[\"label\"]==1]\ndf_0 = df_[df_[\"label\"]==0]\ndf_1 = pd.DataFrame(df_1[\"date\"].value_counts()).sort_index()\ndf_1.rename(columns={\"date\": \"true\"}, inplace=True)\ndf_0 = pd.DataFrame(df_0[\"date\"].value_counts()).sort_index()\ndf_0.rename(columns={\"date\": \"false\"}, inplace=True)\n\nnew_df = df_1.join(df_0, how='outer')\nnew_df.reset_index(inplace=True)\nnew_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot Count of articles by Month\nplt.figure(figsize=(15,7))\nplt.plot(new_df[\"index\"], new_df[\"true\"], label=\"True\")\nplt.plot(new_df[\"index\"], new_df[\"false\"], color=\"red\", label=\"Fake\")\nplt.xticks(rotation=45)\nplt.legend(facecolor='white')\nplt.xlabel(\"Year and Month\")\nplt.ylabel(\"Count\")\nplt.title(\"Count of articles by Month/Year\")\nplt.show","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the distribution of news articles by Month-Year, we can see that fake news was published between 2015-03 to 2018-02, and true news published from 2016-01 to 2017-12. The majority of the real news articles were from August to November of 2017 with at least 2500-3000 each month, making up over half of the total real news dataset.\n\nThe bulk of fake news articles were collected between January 2016 to August 2017, with around 700-1000 each month."},{"metadata":{},"cell_type":"markdown","source":"**Length of text (by characters)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create side-by-side histograms of True and Fake news text\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(14,9))\n\ntrue_length = df[df[\"label\"]==1][\"text\"].str.len()\nax1.set_title(\"True text\")\nax1.hist(true_length, color=\"blue\")\n\nfake_length = df[df[\"label\"]==0][\"text\"].str.len()\nax2.set_title(\"Fake text\")\nax2.hist(fake_length, color=\"red\")\n\nfig.suptitle(\"Length of text (by characters)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both true and fake news text have different distributions: true news text will have mostly around 2500 characters, fake news text will have mostly 5000 characters in their pieces."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"title\"].count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Length of title (by characters)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create side-by-side histograms of True and Fake news text\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(14,9))\n\ntrue_length = df[df[\"label\"]==1][\"title\"].str.len()\nax1.set_title(\"True titles\")\nax1.hist(true_length, color=\"blue\")\n\nfake_length = df[df[\"label\"]==0][\"title\"].str.len()\nax2.set_title(\"Fake titles\")\nax2.hist(fake_length, color=\"red\")\n\nfig.suptitle(\"Length of title (by characters)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again the distributions for true and fake news are different. Going by the news titles, true news have mostly 60-80 characters in their titles, while fake news tend to be longer with 75-125 characters."},{"metadata":{},"cell_type":"markdown","source":"## Data preparation"},{"metadata":{},"cell_type":"markdown","source":"We need to perform data cleaning in order to use the wordcloud to the best possible usage. This involves reducing down words to lower case, removing stopwords such as \"a\" or \"as\", and removing hyperlinks to other sites. Let's merge the text and title columns together. Remove the other columns becasue we will only be concentrating on the text itself and not the subject matter or date released."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge text and title columns, remove title, subject and date.\ndf['text'] = df['title'] + \" \" + df['text']\ndel df['title']\ndel df['subject']\ndel df['date']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check if anything missing after cleaning\ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stopwords to remove from text will have little effect on the context of the text\n# Stopwords in list already in lower case\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nprint(stop_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use this to deal with apostophes and abbreviation\n# df_[\"news\"] = df_['news'].str.replace('[^\\w\\s]','')\ndef remove_apostrophe_abbrev(text):\n    return re.sub('[^\\w\\s]','', text)\n\n# Function to remove stopwords\ndef remove_stop_words(text):\n    clean_text = []\n    for word in text.split():\n        if word.strip().lower() not in stop_words:\n            clean_text.append(word)\n            \n    return \" \".join(clean_text)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Text example of stop words removed\nremove_stop_words(df[\"text\"][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Functions for cleaning text\n\n# Remove html tags, using regex is bad idea\ndef remove_html_tags(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\n# Remove links in text, http and https\n# s? in regex means or s (case sensitive)\ndef remove_links(text):\n    return re.sub('https?:\\/\\/\\S+', '', text)\n\n# Remove 's (possessive pronouns) from text\n# Two kinds of apostophes found\ndef remove_possessive_pronoun(text):\n    return re.sub(\"â€™s|'s\", '', text)\n\n# Remove between brackets and their contents\ndef remove_between_brackets(text):\n    return re.sub('\\([^]]*\\)', '', text)\n\n# Remove square brackets and their contents\ndef remove_between_square_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n\n# Remove curly brackets and their contents\n# Useful to remove any JavaScript scripts, though removed\n# through html as above\ndef remove_between_curly_brackets(text):\n    return re.sub('\\{[^]]*\\}', '', text)\n\ndef remove_n_space(text):\n    return re.sub('\\n', '', text)\n\ndef text_cleaner(text):\n    text = remove_html_tags(text)\n    text = remove_links(text)\n    text = remove_possessive_pronoun(text)\n    text = remove_apostrophe_abbrev(text)\n    text = remove_between_brackets(text)\n    text = remove_between_square_brackets(text)\n    text = remove_between_curly_brackets(text)\n    text = remove_n_space(text)\n    text = remove_stop_words(text)\n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply cleaning functions to text\ndf[\"text\"] = df[\"text\"].apply(text_cleaner)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word Cloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Word Cloud for true news**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# True news wordcloud\nplt.figure(figsize=(15,15))\nwordcloud = WordCloud(max_words = 1000 , width = 1600 , \n                      height = 800 , stopwords = STOPWORDS).generate(\" \".join(df[df[\"label\"] == 1].text))\n\nplt.axis(\"off\")\nplt.imshow(wordcloud)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Word Cloud for fake news**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fake news wordcloud\nplt.figure(figsize=(15,15))\nwordcloud = WordCloud(max_words = 1000 , width = 1600 , \n                      height = 800 , stopwords = STOPWORDS).generate(\" \".join(df[df[\"label\"] == 0].text))\n\nplt.axis(\"off\")\nplt.imshow(wordcloud)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can tell from both word clouds there is no distint words which can determine whether the text is more likely to be true or fake. Both seem include words associates with U.S. politics and U.S. Politicians.\n\nOkay, let's explore frequency of words in text."},{"metadata":{},"cell_type":"markdown","source":"**Frequent words**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get 25 most common words in True news\ntrue_corpus = pd.Series(\" \".join(df[df[\"label\"] == 1].text))[0].split()\n\ncounter = Counter(true_corpus)\ntrue_common = counter.most_common(25)\ntrue_common = dict(true_common)\ntrue_common","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# as a dataframe\ntrue_common_df = pd.DataFrame(true_common.items(), columns = [\"words\", \"count\"])\ntrue_common_df.set_index(\"words\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histogram of 25 true common words\nplt.figure(figsize=(12,9))\nplt.bar(true_common.keys(), true_common.values())\nplt.xticks(rotation=45)\nplt.xlabel(\"Common words\")\nplt.ylabel(\"Count\")\nplt.title(\"25 Most common words (True)\")\nplt.show","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get 25 most common words in Fake news\nfake_corpus = pd.Series(\" \".join(df[df[\"label\"] == 0].text))[0].split()\n\ncounter = Counter(fake_corpus)\nfake_common = counter.most_common(25)\nfake_common = dict(fake_common)\nfake_common","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# as a dataframe\nfake_common_df = pd.DataFrame(fake_common.items(), columns = [\"words\", \"count\"])\nfake_common_df.set_index(\"words\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histogram of 25 fake common words\nplt.figure(figsize=(12,9))\nplt.bar(fake_common.keys(), fake_common.values(), color=\"red\")\nplt.xticks(rotation=45)\nplt.xlabel(\"Common words\")\nplt.ylabel(\"Count\")\nplt.title(\"25 Most common words (Fake)\")\nplt.show","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So from these list of common words, the distribution of news leans towards U.S. politics and that there is no telling what is true news and what is fake news. This makes fake news all the more dangerous and harmful if taken at face value."},{"metadata":{},"cell_type":"markdown","source":"### Bigrams and Trigrams"},{"metadata":{},"cell_type":"markdown","source":"Let's look further into the text with bigrams and trigrams, and find common pair words and three-words."},{"metadata":{},"cell_type":"markdown","source":"**Bigrams**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.util import ngrams","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Bigram: True news"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find most common bigrams in True news\ntext = pd.Series(\" \".join(df[df[\"label\"] == 1].text))[0]\ntokenizer = nltk.RegexpTokenizer(r\"\\w+\")\ntoken = tokenizer.tokenize(text)\n\n# ngrams set to 2\ncounter = Counter(ngrams(token,2))\nmost_common = counter.most_common(25)\nmost_common = dict(most_common)\nmost_common","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# as a dataframe\ntrue_common_bi = pd.DataFrame(most_common.items(), columns = [\"bigram\", \"count\"])\ntrue_common_bi[\"bigram\"] = true_common_bi[\"bigram\"].apply(lambda x: \" \".join(x))\ntrue_common_bi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histogram of 25 common bigrams for True news\nplt.figure(figsize=(12,9))\nplt.bar(true_common_bi[\"bigram\"], true_common_bi[\"count\"]) # can do tuples\nplt.xticks(rotation=90)\nplt.xlabel(\"Common bigram\")\nplt.ylabel(\"Count\")\nplt.title(\"25 Most common bigram (True)\")\nplt.show","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Bigram: Fake news"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find most common bigrams in Fake news\ntext = pd.Series(\" \".join(df[df[\"label\"] == 0].text))[0]\ntokenizer = nltk.RegexpTokenizer(r\"\\w+\")\ntoken = tokenizer.tokenize(text)\n\n# ngrams set to 2\ncounter = Counter(ngrams(token,2))\nmost_common = counter.most_common(25)\nmost_common = dict(most_common)\nmost_common","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# as a dataframe\nfake_common_bi = pd.DataFrame(most_common.items(), columns = [\"bigram\", \"count\"])\nfake_common_bi[\"bigram\"] = fake_common_bi[\"bigram\"].apply(lambda x: \" \".join(x))\nfake_common_bi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histogram of 25 common bigrams for Fakefake news\nplt.figure(figsize=(12,9))\nplt.bar(fake_common_bi[\"bigram\"], fake_common_bi[\"count\"], color=\"red\") # can do tuples\nplt.xticks(rotation=90)\nplt.xlabel(\"Common bigram\")\nplt.ylabel(\"Count\")\nplt.title(\"25 Most common bigram (Fake)\")\nplt.show","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From these fake news bigrams, it's interesting to see news sources Fox News, realDonaldTrump and 21st Century appear. For real news, Reuters more frequent as a news source."},{"metadata":{},"cell_type":"markdown","source":"Trigrams"},{"metadata":{},"cell_type":"markdown","source":"Trigrams: True news"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find most common trigrams in True news\ntext = pd.Series(\" \".join(df[df[\"label\"] == 1].text))[0]\ntokenizer = nltk.RegexpTokenizer(r\"\\w+\")\ntoken = tokenizer.tokenize(text)\n\n# ngrams set to 3\ncounter = Counter(ngrams(token,3))\nmost_common = counter.most_common(25)\nmost_common = dict(most_common)\nmost_common","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# as a dataframe\ntrue_common_tri = pd.DataFrame(most_common.items(), columns = [\"trigram\", \"count\"])\ntrue_common_tri[\"trigram\"] = true_common_tri[\"trigram\"].apply(lambda x: \" \".join(x))\ntrue_common_tri","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histogram of 25 common trigrams for True news\nplt.figure(figsize=(12,9))\nplt.bar(true_common_tri[\"trigram\"], true_common_tri[\"count\"])\nplt.xticks(rotation=90)\nplt.xlabel(\"Common trigram\")\nplt.ylabel(\"Count\")\nplt.title(\"25 Most common trigram (True)\")\nplt.show","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Trigram: Fake News"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find most common trigrams in Fake news\ntext = pd.Series(\" \".join(df[df[\"label\"] == 0].text))[0]\ntokenizer = nltk.RegexpTokenizer(r\"\\w+\")\ntoken = tokenizer.tokenize(text)\n\n# ngrams set to 3\ncounter = Counter(ngrams(token,3))\nmost_common = counter.most_common(25)\nmost_common = dict(most_common)\nmost_common","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# as a dataframe\nfake_common_tri = pd.DataFrame(most_common.items(), columns = [\"trigram\", \"count\"])\nfake_common_tri[\"trigram\"] = fake_common_tri[\"trigram\"].apply(lambda x: \" \".join(x))\nfake_common_tri","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histogram of 25 common trigrams for True news\nplt.figure(figsize=(12,9))\nplt.bar(fake_common_tri[\"trigram\"], fake_common_tri[\"count\"], color=\"red\")\nplt.xticks(rotation=90)\nplt.xlabel(\"Common trigram\")\nplt.ylabel(\"Count\")\nplt.title(\"25 Most common trigram (Fake)\")\nplt.show","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lemmatization"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\n\n\nlemma = WordNetLemmatizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lemmatizer example\nprint(lemma.lemmatize(\"boys\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to perform lemmatization on text\ndef lemmatize_text(text):\n    tokenize_text = nltk.word_tokenize(text)\n    lemmatize_words = [lemma.lemmatize(word) for word in tokenize_text]\n    join_text = ' '.join(lemmatize_words)\n    \n    return join_text\n\n# Example sentence on function\nlemmatize_text(\"There once was a boy named Naruto who was possessed by a Nine-Tailed Demon Fox\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copy main df dataset and lemmatize the text\nlemmatized_df = df.copy()\nlemmatized_df[\"text\"] = lemmatized_df[\"text\"].apply(lemmatize_text)\nlemmatized_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatized_df[\"text\"][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"After must needed data cleaning and extensive exploratory analysis, let's move on to training the dataset on machine learning models to see how well it could predict whether the text is real or fake."},{"metadata":{},"cell_type":"markdown","source":"## Machine Learning"},{"metadata":{},"cell_type":"markdown","source":"### Training models"},{"metadata":{},"cell_type":"markdown","source":"We will begin training models and experiment with different vector methods, CountVectorizer and TF-IDF, with lemmatized text. We will use the following models:\n* Logistic Regression\n* Naive Bayes\n* Support Vector Machine\n* Random Forest\n* Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Machine Learning models to import\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix, classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random state at 42 for reproducibility\n# Go for 80:20 train:test set\ntest_size = 0.20\nX_train, X_test, y_train, y_test = train_test_split(lemmatized_df[\"text\"], lemmatized_df[\"label\"], test_size=test_size, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CountVectorizer**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit CountVectorizer to X_train and X_test datasets\ncv_train = CountVectorizer(max_features=10000).fit(X_train)\nX_vec_train = cv_train.transform(X_train)\nX_vec_test = cv_train.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_vec_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Logistic Regression (CountVectorizer)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training on Logistic Regression model\n# set LR parameter max_iter = 4000 to avoid error\nlr = LogisticRegression(max_iter = 4000)\nlr.fit(X_vec_train, y_train)\npredicted_value = lr.predict(X_vec_test)\nlr_accuracy_value = roc_auc_score(y_test, predicted_value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression Test ROC 99.68% on just lemmatized text\nprint(\"ROC: \" + str(lr_accuracy_value*100) + \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conmat = confusion_matrix(y_test, predicted_value)\nprint(conmat)\nprint(classification_report(y_test, predicted_value))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visual of confusion matrix of Logistic Regression\nfig = plt.subplot()\nsns.heatmap(conmat, annot=True, ax=fig)\nfig.set_ylabel('y_test')\nfig.set_xlabel('predicted values')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Multinomial Naive Bayes (CountVectorizer)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training on Naive Bayes model\n# Quick at training\nnb = MultinomialNB()\nnb.fit(X_vec_train, y_train)\npredicted_value = nb.predict(X_vec_test)\nnb_accuracy_value = roc_auc_score(y_test, predicted_value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Naive Bayes Training ROC 95.21% lemmatized text\nprint(\"ROC: \" + str(roc_auc_score(y_train, nb.predict(X_vec_train))*100) + \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Naive Bayes Test ROC 95.22% lemmatized text\nprint(\"ROC: \" + str(nb_accuracy_value*100) + \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conmat = confusion_matrix(y_test, predicted_value)\nprint(conmat)\nprint(classification_report(y_test, predicted_value))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visual of confusion matrix of Naive Bayes\nfig = plt.subplot()\nsns.heatmap(conmat, annot=True, ax=fig)\nfig.set_ylabel('y_test')\nfig.set_xlabel('predicted values')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Support Vector Machines (CountVectorizer)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training on Support Vector Machine\n# Very slow at training\n# time complexity O(no.features * no.of samples**2)\nsvm = SVC()\nsvm.fit(X_vec_train, y_train)\npredicted_value = svm.predict(X_vec_test)\nsvm_accuracy_value = roc_auc_score(y_test, predicted_value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SVM Training ROC 99.9% lemmatized text\n# svm predict on training set took very long time\nprint(\"ROC: \" + str(roc_auc_score(y_train, svm.predict(X_vec_train))*100) + \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SVM Test ROC 99.53% lemmatized text\nprint(\"ROC: \" + str(svm_accuracy_value*100) + \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conmat = confusion_matrix(y_test, predicted_value)\nprint(conmat)\nprint(classification_report(y_test, predicted_value))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visual of confusion matrix of SVM\nfig = plt.subplot()\nsns.heatmap(conmat, annot=True, ax=fig)\nfig.set_ylabel('y_test')\nfig.set_xlabel('predicted values')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Random Forest (CountVectorizer)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training on Random Forest\nrf = RandomForestClassifier()\nrf.fit(X_vec_train, y_train)\npredicted_value = rf.predict(X_vec_test)\nrf_accuracy_value = roc_auc_score(y_test, predicted_value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest Training ROC 100% lemmatized text\nprint(\"ROC: \" + str(roc_auc_score(y_train, rf.predict(X_vec_train))*100) + \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest Test ROC 99.67% lemmatized text\nprint(\"ROC: \" + str(rf_accuracy_value*100) + \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conmat = confusion_matrix(y_test, predicted_value)\nprint(conmat)\nprint(classification_report(y_test, predicted_value))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visual of confusion matrix of Random Forest\nfig = plt.subplot()\nsns.heatmap(conmat, annot=True, ax=fig)\nfig.set_ylabel('y_test')\nfig.set_xlabel('predicted values')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Gradient Boosting (CountVectorizer)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training on Gradient Boosting \ngbc = GradientBoostingClassifier()\ngbc.fit(X_vec_train, y_train)\npredicted_value = gbc.predict(X_vec_test)\ngbc_accuracy_value = roc_auc_score(y_test, predicted_value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gradient Boost Training ROC 99.64% lemmatized text\nprint(\"ROC: \" + str(roc_auc_score(y_train, gbc.predict(X_vec_train))*100) + \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gradient Boost Test ROC 99.5% lemmatized text\nprint(\"ROC: \" + str(gbc_accuracy_value*100) + \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conmat = confusion_matrix(y_test, predicted_value)\nprint(conmat)\nprint(classification_report(y_test, predicted_value))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visual of confusion matrix of Gradient Boosting\nfig = plt.subplot()\nsns.heatmap(conmat, annot=True, ax=fig)\nfig.set_ylabel('y_test')\nfig.set_xlabel('predicted values')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So it looks like most of the models did fairly well. On comparing training accuracy and test acccuracy difference, it seems the **Gradient Boosting** did pretty well. When training accuracy is 100%, then there may be a problem of the model overfitting, and this could lead to new news data being predicted incorrectly.\n\nNow lets save the Gradient Boosting file."},{"metadata":{},"cell_type":"markdown","source":"## Saving the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save model\nmodel_file = \"gbc.pkl\"\nwith open(model_file,mode='wb') as model_f:\n    pickle.dump(gbc,model_f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Open the model, print result for sanity check\nwith open(\"gbc.pkl\",mode='rb') as model_f:\n    model = pickle.load(model_f)\n    predict = model.predict(X_vec_test)\n    result = roc_auc_score(y_test, predict)\n    print(\"result:\",result*100, \"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**What have we learnt**\n* How to clean dataset by removing tags, punctuation, stopwords\n* How to use lemmatization to remove duplicated word meanings\n* Play with data to produce visualizations like countplots and wordcloud\n* Look for frequent words, sequence of words (bi-grams, tri-grams)\n* Train machine learning models by using CountVectorizer on text dataset\n* Evaluate model's training and test accuracy, classification report and confusion matrix\n* Save model as pkl file for reuse."},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}