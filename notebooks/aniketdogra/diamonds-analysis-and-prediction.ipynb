{"cells":[{"metadata":{"_uuid":"76a3f13cf081382449efc74e0950a70905919c0e"},"cell_type":"markdown","source":"# Diamond Price Modelling\n\n **What are diamonds ?**\n\n> Diamond is a solid form of the element carbon with its atoms arranged in a crystal structure called diamond cubic.\nThe most familiar uses of diamonds today are as gemstones used for adornment, and as industrial abrasives for cutting hard materials.\n\n\n\n **In this notebook, we will try to build a model to predict the prices of diamonds based on various features of diamond  like carat weight, cut quality ,etc.**\n \n*Dataset used in this notebook has been taken from [KAGGLE](https://www.kaggle.com/shivam2503/diamonds)*"},{"metadata":{"_uuid":"073188fc35c924f8382519b16354189c749b7ed0"},"cell_type":"markdown","source":"## TOPICS\n\n1. [**A Quick Look at the Dataset**](#link1)\n2. [**Exploring Correlation between Features**](#link2)\n3. [**Splitting Data into Test and Train Set**](#link3)\n4. [**Data Visualisation**](#link4)\n5. [**Preparing Data for ML algorithm**](#link5)\n6. [**Applying ML Algorithm on the Dataset**](#link6)\n7. [**Conclusion**](#link7)"},{"metadata":{"_uuid":"79ebb6c9e4acf3fa5e943f17c24cdb20df52cfb4"},"cell_type":"markdown","source":"<a id=\"link1\"></a>\n## A Quick Look at the Dataset"},{"metadata":{"_uuid":"4837af2d01ea60afde0c14858bb8f0872a119213"},"cell_type":"markdown","source":"### Importing the important libraries required for this project and getting the data from the dataset\n"},{"metadata":{"trusted":true,"_uuid":"e814e47fc42ce3bfc7871868f8c5d1667a124923"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport math\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nimport warnings\nwarnings.filterwarnings(action = \"ignore\")\n\n%matplotlib inline\ndiamonds = pd.read_csv(\"../input/diamonds.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"073dcba36b9cd00826a57fb2623736c52a62a7d8"},"cell_type":"markdown","source":"**Now let's take a look at our diamond dataset.**"},{"metadata":{"trusted":true,"_uuid":"2d4dacf6e5c8f0271b45c571bbb8958657aef241"},"cell_type":"code","source":"diamonds.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d29ef29cc458aa3e91e831cecfdd2111b993f002"},"cell_type":"code","source":"diamonds.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1f2c518a0f77c230f8e8012c9f6e295eec41d9a"},"cell_type":"markdown","source":"### Features of the Dataset\n\n- **Carat** weight of the diamond\n- **cut** Describe cut quality of the diamond. Quality in increasing order Fair, Good, Very Good, Premium, Ideal - - - **color** Color of the diamond, with D being the best and J the worst\n- **clarity** How obvious inclusions are within the diamond:(in order from best to worst, FL = flawless, I3= level 3 inclusions) FL,IF, VVS1, VVS2, VS1, VS2, SI1, SI2, I1, I2, I3\n- **depth** The height of a diamond, measured from the culet to the table, divided by its average girdle diameter\n- **table** The width of the diamond's table expressed as a percentage of its average diameter\n- **price** the price of the diamond\n- **x** length mm\n- **y** width mm\n- **z** depth mm"},{"metadata":{"trusted":true,"_uuid":"4325efc0d018cc21875b91015fa6c40a3224ae22"},"cell_type":"code","source":"diamonds[\"cut\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0c927f8ca0361d906169540d6bc52f4ea01522a"},"cell_type":"code","source":"diamonds[\"color\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcdd1924533cb3f082f3186127de56d9448cb0fb"},"cell_type":"code","source":"diamonds[\"clarity\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6588b6fa26fd260c058779fc8373645faca3ae1"},"cell_type":"markdown","source":"### Dropping the unnecessary column Unnamed: 0"},{"metadata":{"trusted":true,"_uuid":"8bc3b029f72480a68a47b097147f4b05f83af645"},"cell_type":"code","source":"# Price is of different data type and unnecessary column \"Unnamed\"\ndiamonds = diamonds.drop(\"Unnamed: 0\",axis = 1)\ndiamonds[\"price\"] = diamonds[\"price\"].astype(\"float64\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"53a866fa0c51c51ed394b947b64591177210a0ad"},"cell_type":"code","source":"diamonds.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e397cf1c6f9e31b64dfa5062bdaa7bdbc3d4162d"},"cell_type":"code","source":"diamonds.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a15c1018325059022eadd240c97ed35115a64153"},"cell_type":"markdown","source":"### Plotting Histogram to get an idea about the different features/attributes of the dataset"},{"metadata":{"trusted":true,"_uuid":"9a510096aa8523eb65de403772d8e9302cd9bba4"},"cell_type":"code","source":"diamonds.hist(bins = 50, figsize = (20,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74d040233c83296543b2338550f31cefcc97afc0"},"cell_type":"markdown","source":"<a id=\"link2\"></a>\n## Exploring Correlation between Features"},{"metadata":{"trusted":true,"_uuid":"7df4de0208ed4c42961548f72b0bf6f60bf28146"},"cell_type":"code","source":"corr_matrix = diamonds.corr()\n\nplt.subplots(figsize = (10,8))\nsns.heatmap(corr_matrix, annot = True, cmap = \"Blues\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7ed8313e7a9f07ac21d2315c35a9734b254355f"},"cell_type":"markdown","source":"### Conclusions\n - **x , y and z are correlated with the price.** \n - **Price of the diamond and carat weight of the diamond are highly correlated**\n - **Depth and Table are weakly correlated with the price of the diamond.**\n - **Carat is one of the main features to predict the price of a diamond.**"},{"metadata":{"_uuid":"eb96a56e969577cd6da770c3ad5cbaf3b09a3d9b"},"cell_type":"markdown","source":"<a id=\"link3\"></a>\n### Splitting Data into Test and Train Set\n\nIt is advisable to split the dataset into Test set (80%) and Train set (20%). The test set allows our model to make \npredictions on values which it has never seen before.\n\nBut taking random samples from our dataset can introduce significant **sampling bias**. Therefore, in order to avoid sampling bias, the data will be divide into different homogenous subgroups called strata. This is called **Stratified Sampling**. Since, we know that carat is the most important parameter to predict the price of the diamonds we will use it for Stratified sampling "},{"metadata":{"trusted":true,"_uuid":"01da5ac18f8faefcb9818710a3cd812814461fb7"},"cell_type":"code","source":"diamonds[\"carat\"].hist(bins = 50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc48e1543c8f07be73e8a40bf542cfcdc7113661"},"cell_type":"code","source":"diamonds[\"carat\"].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88f4e229f9be7064f6bcb31b24bd353c2d8dcae8"},"cell_type":"code","source":"diamonds[\"carat\"].min()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"078f7965a65fcdb22f47ce9eb61a837e65528e18"},"cell_type":"markdown","source":"Most of the carat value ranges from 0.3 to 1.2. So, we will divide the carat into 5 categories."},{"metadata":{"trusted":true,"_uuid":"f7ad8abfaf2228213211b88ce349c7e5fed0552d"},"cell_type":"code","source":"# Divide by 0.4 to limit the number of carat strata\n\ndiamonds[\"carat_cat\"] = np.ceil(diamonds[\"carat\"]/0.4)\n\n# Label those above 5 as 5\ndiamonds[\"carat_cat\"].where(diamonds[\"carat_cat\"] < 5, 5.0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45f12ce252ae54f8a9ff62c822d69b18701fb247"},"cell_type":"code","source":"diamonds[\"carat_cat\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2ee231979ad0d131098e122f3dcc460c90f00b8"},"cell_type":"code","source":"diamonds[\"carat_cat\"].hist()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d10e34839ffc0be29123a1a5ba9318ce629c3ed"},"cell_type":"markdown","source":"Now we will perform the stratified splitting of the dataset using sklearn's StratifiedShuffleSplit class"},{"metadata":{"trusted":true,"_uuid":"8a29e065e16d209bfcf88bbc59f266099ace3faf"},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index,test_index in split.split(diamonds,diamonds[\"carat_cat\"]):\n    strat_train_set = diamonds.loc[train_index]\n    strat_test_set = diamonds.loc[test_index]\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ccdf293352ecbc5831e564c9debff7008b671a49"},"cell_type":"code","source":"strat_test_set[\"carat_cat\"].value_counts() / len(strat_test_set)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91885782148f7e95b42d3bbd15e81bd490d01191"},"cell_type":"markdown","source":"We will now drop the carat category columns."},{"metadata":{"trusted":true,"_uuid":"dd85ec54ee046024f8c038624f43c8a4b334ede8"},"cell_type":"code","source":"for x in (strat_test_set, strat_train_set):\n    x.drop(\"carat_cat\", axis=1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08f7a59294a9b6d9626a60872f8bda1cc566538a"},"cell_type":"code","source":"strat_test_set.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b77fab7959a67124f9c2fe94d6094d8d1f70ef13"},"cell_type":"markdown","source":"Size of Test Set = 10788"},{"metadata":{"trusted":true,"_uuid":"49b1b118247e351d55b4f5c1bd921ac85dca32fc"},"cell_type":"code","source":"strat_train_set.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab490f8762470048f2223295bb73f7cb8f571572"},"cell_type":"markdown","source":"Size of Train Set = 43152"},{"metadata":{"_uuid":"47665655e127135b23bb3a1adc67811c50a1dcc3"},"cell_type":"markdown","source":"<a id=\"link4\"></a>\n## Data Visualisation \n\nWe will be using training set to plot varoius graphs to visualise and draw conclusions from the data."},{"metadata":{"trusted":true,"_uuid":"98aacb53eb9f00d0d395ed6bdc1dc8bb267d2663"},"cell_type":"code","source":"diamonds = strat_train_set.copy()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8412fd8dffe65faa36e69db9bd5b589453485fbe"},"cell_type":"markdown","source":"### Plotting scatterplot between price and carat"},{"metadata":{"trusted":true,"_uuid":"1810fe9e464e1cbda4ff913738cb2e1f32844408"},"cell_type":"code","source":"diamonds.plot(kind=\"scatter\", x=\"price\", y=\"carat\",alpha = 0.1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12f063f3b0394005cc1dcfb050b66f145674257b"},"cell_type":"markdown","source":"### Count plots of different categorical features of diamonds"},{"metadata":{"trusted":true,"_uuid":"69e4a7ab771807f16570418c9c9f91e0f3407119"},"cell_type":"code","source":"fig, ax = plt.subplots(3, figsize = (14,18))\nsns.countplot('cut',data = diamonds, ax=ax[0],palette=\"Spectral\")\nsns.countplot('clarity',data = diamonds, ax=ax[1],palette=\"deep\")\nsns.countplot('color',data = diamonds, ax=ax[2],palette=\"colorblind\")\nax[0].set_title(\"Diamond cut\")\nax[1].set_title(\"Diamond Clarity\")\nax[2].set_title(\"Diamond Color\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c417c3bc2dd67af02d7b659d247db6b694801f72"},"cell_type":"markdown","source":"### Comparison of carat with price based on diamond cut."},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"6164e2163f06655ca2e6294a40833c813661b859"},"cell_type":"code","source":"sns.pairplot(diamonds[[\"price\",\"carat\",\"cut\"]], markers = [\"o\",\"v\",\"s\",\"p\",\"d\"],hue=\"cut\", height=5)\nplt.show()\n\nf, ax = plt.subplots(2,figsize = (12,10))\nsns.barplot(x=\"cut\",y=\"price\",data = diamonds,ax=ax[0])\nsns.barplot(x=\"cut\",y=\"carat\",data = diamonds, ax=ax[1])\nax[0].set_title(\"Cut vs Price\")\nax[1].set_title(\"Cut vs Carat\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42969b9bbf3fdf94284ec8a609b69df4b9dd9b3c"},"cell_type":"markdown","source":"**Conclusion**\n- Fair cut diamonds weigh the most but are not the most expensive diamonds.\n- Premium cut diamonds are the most expensive diamonds.\n- Ideal cut diamonds weigh less and are cheapest diamonds.\n\nWe can see that price of diamond is dependent on the cut."},{"metadata":{"_uuid":"88c7c66b1a86ce292621318552dbed3b9b664524"},"cell_type":"markdown","source":"### Comparison of carat with price based on diamond color"},{"metadata":{"trusted":true,"_uuid":"1b22295565f85bf10761a3a2266d87c8f26aec8b"},"cell_type":"code","source":"sns.pairplot(diamonds[[\"price\",\"carat\",\"color\"]], hue=\"color\", height=5, palette=\"husl\")\nplt.show()\n\nf, ax = plt.subplots(2,figsize = (12,10))\nsns.barplot(x=\"color\",y=\"price\",data = diamonds,ax=ax[0])\nsns.barplot(x=\"color\",y=\"carat\",data = diamonds, ax=ax[1])\nax[0].set_title(\"Color vs Price\")\nax[1].set_title(\"Color vs Carat\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e23e48375996553136e1c7ab36d89a537a8731c"},"cell_type":"markdown","source":"**Conclusions**\n- J color diamonds are the most expensive and the heaviest diamonds.\n- The two plots are very similar.\n\nThus, it can be concluded that the heavier diamond is expensive, if only color is considered."},{"metadata":{"_uuid":"31373e9c9a8be241d727735d63be313700d2cfb8"},"cell_type":"markdown","source":"### Comparison of carat with price based on diamond clarity"},{"metadata":{"trusted":true,"_uuid":"4cf25c8771b68643b563e9d90184b57885aa222a"},"cell_type":"code","source":"sns.pairplot(diamonds[[\"price\",\"carat\",\"clarity\"]],hue=\"clarity\", height=5)\nplt.show()\n\nf, ax = plt.subplots(2,figsize = (12,10))\nsns.barplot(x=\"clarity\",y=\"price\",data = diamonds,ax=ax[0])\nsns.barplot(x=\"clarity\",y=\"carat\",data = diamonds, ax=ax[1])\nax[0].set_title(\"Clarity vs Price\")\nax[1].set_title(\"Clarity vs Carat\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9228e283a98417a3bbcf64c338a1aa42a4c002c"},"cell_type":"markdown","source":"### More plots to understand the realtion between cut,color and clarity with prices"},{"metadata":{"trusted":true,"_uuid":"31fdbcd6b716b1f414de8eeaedb06a240dd7118b"},"cell_type":"code","source":"fig, ax = plt.subplots(3, figsize = (14,18))\nsns.violinplot(x='cut',y='price',data = diamonds, ax=ax[0],palette=\"Spectral\")\nsns.violinplot(x='clarity',y='price',data = diamonds, ax=ax[1],palette=\"deep\")\nsns.violinplot(x='color',y='price',data = diamonds, ax=ax[2],palette=\"colorblind\")\nax[0].set_title(\"Cut vs Price\")\nax[1].set_title(\"Clarity vs Price\")\nax[2].set_title(\"Color vs Price \")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50c69a4f56095ca50c91d89961ee21aebaf59c78"},"cell_type":"code","source":"from pandas.plotting import scatter_matrix\n\nattributes = [\"depth\",\"table\",\"x\",\"y\",\"z\",\"price\"]\nscatter_matrix(diamonds[attributes], figsize=(12, 8))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76dd678ab641b07b1eb180c124aa6b027b2c7994"},"cell_type":"markdown","source":"<a id=\"link5\"></a>\n## Preparing data for the ML Algorithms\n"},{"metadata":{"trusted":true,"_uuid":"3e077ebb93e7204825133d97b43c57d9c692f787"},"cell_type":"code","source":"sample_incomplete_rows = diamonds[diamonds.isnull().any(axis=1)].head()\nsample_incomplete_rows","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"954e1ae0d747cebc9a8cda1147d35a7271146a25"},"cell_type":"code","source":"diamonds = strat_train_set.drop(\"price\", axis=1)\ndiamonds_label = strat_train_set[\"price\"].copy()\ndiamonds_only_num = diamonds.drop([\"cut\",\"clarity\",\"color\"],axis=1)\n\ndiamonds_only_num.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"831e54697a501e5c4c3504779fb4dcd98809145f"},"cell_type":"markdown","source":"\n### Feature Scaling\n\n\nMachine Learning algorithms donâ€™t perform well when the input numerical attributes have very different scales. Therefore, it is necessary to feature scale all the features of diamond dataset. There are two ways of doing feature scaling -min-max scaling and standardization. I will be using standardization as it is not affected by any outliers."},{"metadata":{"trusted":true,"_uuid":"1a74e46357794d7a8352ff8388c0ce437aefb804"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nstd_scaler = StandardScaler()\ndiamonds_scaled_num = std_scaler.fit_transform(diamonds_only_num)\n\ndiamonds_scaled_num","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"456c7f838eda7c98b7786e4d880b435450dfe74d"},"cell_type":"code","source":"pd.DataFrame(diamonds_scaled_num).head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36fc27df46a746e8bda81b3715fe0df20d543411"},"cell_type":"markdown","source":"### Encoding Categorical Attributes\n\nIn this dataset, we have three categorical attributes.ML algorithms work better with numbers.Thus, we will convert them into numbers using OneHotEncoder of scikit learn."},{"metadata":{"trusted":true,"_uuid":"7e2402e85e242d6f48b81b0fc73cc011cd6305e7"},"cell_type":"code","source":"diamonds_cat = diamonds[[\"cut\",\"color\",\"clarity\"]]\ndiamonds_cat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9d16e5d9b3b8113c561d4d52943a1d61400516c"},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\ncat_encoder = OneHotEncoder()\ndiamonds_cat_encoded = cat_encoder.fit_transform(diamonds_cat)\n\ndiamonds_cat_encoded.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c138a1681e31f796906e7348b41e48919ef3de1d"},"cell_type":"code","source":"cat_encoder.categories_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83e7e0f96ab274b67710978ea5ffd931b5e868e5"},"cell_type":"markdown","source":"### Transformation Pipeline\n\nWe have to perform feature scaling and label encoding on dataset before feeding it into ML algorithms. So, to simplify the process we will create a pipeline using ColumnTransformer which successively performs feature scaling and Label encoding.  "},{"metadata":{"trusted":true,"_uuid":"a7cb958e947173ef15979dc48f3ef652fbb54b1c"},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\n\nnum_attribs = list(diamonds_only_num)\ncat_attribs = [\"cut\",\"color\",\"clarity\"]\npipeline = ColumnTransformer([\n    (\"num\", StandardScaler(),num_attribs),\n    (\"cat\",OneHotEncoder(),cat_attribs),\n])\n\ndiamonds_prepared = pipeline.fit_transform(diamonds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2c55c2e4026009ebfcbff1be40ad4d01d762ac7"},"cell_type":"code","source":"diamonds_prepared","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ccbe02ee7ca788fdeb73e6a3a5f4b64353f5bec0"},"cell_type":"code","source":"pd.DataFrame(diamonds_prepared).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d047e3c6e965d633d050cc272127205ad57405da"},"cell_type":"code","source":"diamonds_prepared.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5cfee46bfc8cbe588046e8b2cebe2512c56b9d2e"},"cell_type":"markdown","source":"<a id=\"link6\"></a>\n## Applying ML Algorithms on the Dataset\n"},{"metadata":{"_uuid":"1d7ef85f9a42dfbd7574732881658d7468e4bbca"},"cell_type":"markdown","source":"Now, it is time to select a model, train it and evaluate its performance using test set.\nFirst of all we will import mean_squared_error and cross_val_score from sklearn to evaluate the models.\n\nWe will create one function that will run through each algorithm. We'll also have variables that hold results of the algorithms for future comparisons. RMSE and CV_scores are used to check the performance. The function will plot a graph to show how well our algorithm has predicted the data."},{"metadata":{"trusted":true,"_uuid":"8d59a9712b765f6335f27a3b6449cc02ee92d25c"},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom random import randint\n\nX_test = strat_test_set.drop(\"price\",axis=1)\ny_test = strat_test_set[\"price\"].copy()\n\nmodel_name = []\nrmse_train_scores = []\ncv_rmse_scores = []\naccuracy_models = []\nrmse_test_scores = []\n\ndef model_performance(modelname,model,diamonds = diamonds_prepared, diamonds_labels = diamonds_label,\n                      X_test = X_test,y_test = y_test,\n                      pipeline=pipeline, cv = True):\n    \n    model_name.append(modelname)\n    \n    model.fit(diamonds,diamonds_labels)\n    \n    predictions = model.predict(diamonds)\n    mse_train_score = mean_squared_error(diamonds_labels, predictions)\n    rmse_train_score = np.sqrt(mse_train_score)\n    cv_rmse = np.sqrt(-cross_val_score(model,diamonds,diamonds_labels,\n                                       scoring = \"neg_mean_squared_error\",cv=10))\n    cv_rmse_mean = cv_rmse.mean()\n    \n    print(\"RMSE_Train: %.4f\" %rmse_train_score)\n    rmse_train_scores.append(rmse_train_score)\n    print(\"CV_RMSE: %.4f\" %cv_rmse_mean)\n    cv_rmse_scores.append(cv_rmse_mean)\n    \n    \n    print(\"---------------------TEST-------------------\")\n    \n    X_test_prepared = pipeline.transform(X_test)\n    \n    test_predictions = model.predict(X_test_prepared)\n    mse_score = mean_squared_error(y_test,test_predictions)\n    rmse_score = np.sqrt(mse_score)\n    \n    print(\"RMSE_Test: %.4f\" %rmse_score)\n    rmse_test_scores.append(rmse_score)\n    \n    accuracy = (model.score(X_test_prepared,y_test)*100)\n    print(\"accuracy: \"+ str(accuracy) + \"%\")\n    accuracy_models.append(accuracy)\n    \n    start = randint(1, len(y_test))\n    some_data = X_test.iloc[start:start + 5]\n    some_labels = y_test.iloc[start:start + 5]\n    some_data_prepared = pipeline.transform(some_data)\n    print(\"Predictions:\", model.predict(some_data_prepared))\n    print(\"Labels:    :\", list(some_labels))\n    \n    \n    plt.scatter(y_test,test_predictions)\n    plt.xlabel(\"Actual\")\n    plt.ylabel(\"Predicted\")\n    x_lim = plt.xlim()\n    y_lim = plt.ylim()\n    plt.plot(x_lim, y_lim, \"go--\")\n    plt.show()\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da43f65d016c4d0bd158a85485b9a79b29b34a9f"},"cell_type":"markdown","source":"**Linear Regression**"},{"metadata":{"trusted":true,"_uuid":"4b1ee45007cb7d5433f6455ae0fb2f5b961a377c"},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression(normalize=True)\nmodel_performance(\"Linear Regression\",lin_reg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc2d5bc620681cefddae2d054b2e1ea7b9026add"},"cell_type":"markdown","source":"**Decision Tree Regression**"},{"metadata":{"trusted":true,"_uuid":"8b9cebbe631d224ae2c4bd247182cac7bde2b3be"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ndec_tree = DecisionTreeRegressor(random_state=42)\nmodel_performance(\"Decision Tree Regression\",dec_tree)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da3942b72938051f6ecdc4acfbc320cf30ab1d02"},"cell_type":"markdown","source":"**Random Forest Regression**"},{"metadata":{"trusted":true,"_uuid":"a2b173b4f79b24eac273fee6b513051a3ab46672"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor(n_estimators = 10, random_state = 42)\nmodel_performance(\"Random Forest Regression\",forest_reg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4fa472cec5ac99647a7edfeda41c669f07ad893"},"cell_type":"markdown","source":"**Ridge Regression**"},{"metadata":{"trusted":true,"_uuid":"d737bb039db3bb3bb462131a498ed089a1e1997a"},"cell_type":"code","source":"from sklearn.linear_model import Ridge\n\nridge_reg = Ridge(normalize = True)\nmodel_performance(\"Ridge Regression\",ridge_reg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab5f249077cc1628efe334b11d065035b98f40b3"},"cell_type":"markdown","source":"**Lasso Regression**"},{"metadata":{"trusted":true,"_uuid":"27d2ef4ae363f8c1637c513d449323488c979dd8"},"cell_type":"code","source":"from sklearn.linear_model import Lasso\n\nlasso_reg = Lasso(normalize = True)\nmodel_performance(\"Lasso Regression\",lasso_reg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"891175ef351e0bdefc8edcb194374239d7af0a50"},"cell_type":"markdown","source":"**Elastic Net Regression**"},{"metadata":{"trusted":true,"_uuid":"c3bbb830e0bbaa8fe496cdde34aa5fb43b0a389f"},"cell_type":"code","source":"from sklearn.linear_model import ElasticNet\n\nnet_reg = ElasticNet()\nmodel_performance(\"Elastic Net Regression\",net_reg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2218d72346cfd1cc9940ca18dc4968c984bdf72e"},"cell_type":"markdown","source":"**Ada Boost Regression**"},{"metadata":{"trusted":true,"_uuid":"3dc822b139fee7a40c0ca5c4ad6645957b5652b3"},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostRegressor\n\nada_reg = AdaBoostRegressor(n_estimators = 100)\nmodel_performance(\"Ada Boost Regression\",ada_reg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ebd651cc6d9e06dea373be21280bc971d75a28d9"},"cell_type":"markdown","source":"**Gradient Boosting Regression**"},{"metadata":{"trusted":true,"_uuid":"c3b23c7f01378a9975ddb493ae2da2ffe73419e1"},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\n\ngrad_reg = GradientBoostingRegressor(n_estimators = 100, learning_rate = 0.1,\n                                     max_depth = 1, random_state = 42, loss = 'ls')\nmodel_performance(\"Gradient Boosting Regression\",grad_reg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"683a60806a6b2eb16b24f3e604dc17433a0a2abb"},"cell_type":"markdown","source":"### Comparing the Accuracies of different Regression Models"},{"metadata":{"trusted":true,"_uuid":"e3f3479bb63ff8296d577a05c740f52dcca30a05"},"cell_type":"code","source":"compare_models = pd.DataFrame({\"Algorithms\" : model_name, \"Models RMSE\" : rmse_test_scores, \n                               \"CV RMSE Mean\" : cv_rmse_scores, \"Accuracy\" : accuracy_models})\ncompare_models.sort_values(by = \"Accuracy\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4bda727e2b1d06cf2c39bf571a535065613a0c21"},"cell_type":"code","source":"sns.pointplot(\"Accuracy\",\"Algorithms\",data=pd.DataFrame({'Algorithms':model_name,\"Accuracy\":accuracy_models}))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c0d4dffc6c0b5a0477ff7285bfac30872da3bbd"},"cell_type":"markdown","source":"<a id=\"link7\"></a>\n## Conclusion\n\n**Random Forest Regressor gives us the Highest accuracy.**\n\n**THANK YOU**"},{"metadata":{"trusted":true,"_uuid":"aa613efe8cd18120ad00d329b23138481f018b42"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":1}