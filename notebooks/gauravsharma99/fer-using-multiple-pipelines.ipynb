{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import cv2\nimport math\nimport joblib\nimport numpy as np\nimport pandas as pd\n\nimport scikitplot\nimport seaborn as sns\nfrom matplotlib import pyplot\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report\n\nimport tensorflow as tf\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Flatten, Dense, Conv2D, MaxPooling2D, Concatenate\nfrom tensorflow.keras.layers import Dropout, BatchNormalization\nfrom tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import plot_model\n\nfrom keras.utils import np_utils","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/fer2013/fer2013.csv')\nprint(df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.emotion.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"emotion_label_to_text = {0:'anger', 1:'disgust', 2:'fear', 3:'happiness', 4: 'sadness', 5: 'surprise', 6: 'neutral'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.emotion.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df.emotion)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`So majority classes belongs to 3:Happy, 4:Sad and 6:Neutral nd we are also intersted in these three classes only.`"},{"metadata":{"trusted":true},"cell_type":"code","source":"math.sqrt(len(df.pixels[0].split(' ')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = pyplot.figure(1, (14, 14))\n\nk = 0\nfor label in sorted(df.emotion.unique()):\n    for j in range(7):\n        px = df[df.emotion==label].pixels.iloc[k]\n        px = np.array(px.split(' ')).reshape(48, 48).astype('float32')\n\n        k += 1\n        ax = pyplot.subplot(7, 7, k)\n        ax.imshow(px, cmap='gray')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_title(emotion_label_to_text[label])\n        pyplot.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"INTERESTED_LABELS = [3, 4, 6]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[df.emotion.isin(INTERESTED_LABELS)]\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`Now I will make the data compatible for neural networks.`"},{"metadata":{"trusted":true},"cell_type":"code","source":"img_array = df.pixels.apply(lambda x: np.array(x.split(' ')).reshape(48, 48, 1).astype('float32'))\nimg_array = np.stack(img_array, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_array.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\nimg_labels = le.fit_transform(df.emotion)\nimg_labels = np_utils.to_categorical(img_labels)\nimg_labels.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\nprint(le_name_mapping)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"facial_landmarks = joblib.load(\"../input/fer2013-facial-landmarks/facial_landmarks.pkl\")\nfacial_landmarks.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"extracting HOG features, this may take some minutes...\")\n\nhog_feats = []\nfor img in img_array:\n    img = img.astype(\"uint8\")\n    img_ = cv2.resize(img, (64,128))\n    # img_ = cv2.equalizeHist(img_)\n\n    hog = cv2.HOGDescriptor()\n    hog_descr = hog.compute(img_)\n    hog_feats.append(hog_descr)\n\n\nhog_feats = np.array(hog_feats)\nprint(hog_feats.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`Splitting the data into training and validation set.`"},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtrain_img, Xvalid_img, Xtrain_fl, Xvalid_fl, Xtrain_hog, Xvalid_hog, y_train, y_valid = \\\ntrain_test_split(img_array, facial_landmarks, hog_feats, img_labels,\n                shuffle=True, stratify=img_labels, test_size=0.1,\n                random_state=42)\n\nprint(Xtrain_img.shape, Xtrain_fl.shape, Xtrain_hog.shape, y_train.shape)\nprint(Xvalid_img.shape, Xvalid_fl.shape, Xvalid_hog.shape, y_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_width = Xtrain_img.shape[1]\nimg_height = Xtrain_img.shape[2]\nimg_depth = Xtrain_img.shape[3]\nnum_classes = y_train.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalizing image arrays\nXtrain_img = Xtrain_img / 255.\nXvalid_img = Xvalid_img / 255.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dcnn_pipeline(input_shape):\n    model_in = Input(shape=input_shape, name=\"input_DCNN\")\n    \n    conv2d_1 = Conv2D(\n        filters=64,\n        kernel_size=(3,3),\n        activation='elu',\n        padding='same',\n        kernel_initializer='he_normal',\n        name='conv2d_1'\n    )(model_in)\n    batchnorm_1 = BatchNormalization(name='batchnorm_1')(conv2d_1)\n    conv2d_2 = Conv2D(\n        filters=64,\n        kernel_size=(3,3),\n        activation='elu',\n        padding='same',\n        kernel_initializer='he_normal',\n        name='conv2d_2'\n    )(batchnorm_1)\n    batchnorm_2 = BatchNormalization(name='batchnorm_2')(conv2d_2)\n    \n    maxpool2d_1 = MaxPooling2D(pool_size=(2,2), name='maxpool2d_1')(batchnorm_2)\n    dropout_1 = Dropout(0.4, name='dropout_1')(maxpool2d_1)\n\n    conv2d_3 = Conv2D(\n        filters=128,\n        kernel_size=(3,3),\n        activation='elu',\n        padding='same',\n        kernel_initializer='he_normal',\n        name='conv2d_3'\n    )(dropout_1)\n    batchnorm_3 = BatchNormalization(name='batchnorm_3')(conv2d_3)\n    conv2d_4 = Conv2D(\n        filters=128,\n        kernel_size=(3,3),\n        activation='elu',\n        padding='same',\n        kernel_initializer='he_normal',\n        name='conv2d_4'\n    )(batchnorm_3)\n    batchnorm_4 = BatchNormalization(name='batchnorm_4')(conv2d_4)\n    \n    maxpool2d_2 = MaxPooling2D(pool_size=(2,2), name='maxpool2d_2')(batchnorm_4)\n    dropout_2 = Dropout(0.4, name='dropout_2')(maxpool2d_2)\n\n    conv2d_5 = Conv2D(\n        filters=256,\n        kernel_size=(3,3),\n        activation='elu',\n        padding='same',\n        kernel_initializer='he_normal',\n        name='conv2d_5'\n    )(dropout_2)\n    batchnorm_5 = BatchNormalization(name='batchnorm_5')(conv2d_5)\n    conv2d_6 = Conv2D(\n        filters=256,\n        kernel_size=(3,3),\n        activation='elu',\n        padding='same',\n        kernel_initializer='he_normal',\n        name='conv2d_6'\n    )(batchnorm_5)\n    batchnorm_6 = BatchNormalization(name='batchnorm_6')(conv2d_6)\n    \n    maxpool2d_3 = MaxPooling2D(pool_size=(2,2), name='maxpool2d_3')(batchnorm_6)\n    dropout_3 = Dropout(0.4, name='dropout_3')(maxpool2d_3)\n\n    flatten = Flatten(name='flatten_dcnn')(dropout_3)\n        \n    dense_1 = Dense(\n        128,\n        activation='elu',\n        kernel_initializer='he_normal',\n        name='dense1_dcnn'\n    )(flatten)\n    batchnorm_7 = BatchNormalization(name='batchnorm_7')(dense_1)\n    \n    model_out = Dropout(0.6, name='dropout_4')(batchnorm_7)\n    \n    return model_in, model_out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def facial_landmarks_pipeline(input_shape):\n    model_in = Input(shape=input_shape, name=\"input_Facial_Landmarks\")\n    flatten = Flatten(name=\"flatten_fl\")(model_in)\n    dense1 = Dense(64, activation=\"relu\", name=\"dense1_fl\")(flatten)\n    model_out = Dropout(0.4, name='dropout1_fl')(dense1)\n    \n    return model_in, model_out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def facial_HOG_pipeline(input_shape):\n    model_in = Input(shape=input_shape, name=\"input_Facial_HOG\")\n    flatten = Flatten(name=\"flatten_hog\")(model_in)\n    dense1 = Dense(256, activation=\"relu\", name=\"dense1_hog\")(flatten)\n    model_out = Dropout(0.4, name='dropout1_hog')(dense1)\n        \n    return model_in, model_out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def merge_models(models_in: list, models_out: list, num_classes: int, show_summary=False):\n    \n    concated = Concatenate()(models_out)\n    dropout_1 = Dropout(0.4, name='dropout1_model')(concated)\n\n    dense1 = Dense(256, activation=\"relu\", name=\"dense1\")(dropout_1)\n    dropout_2 = Dropout(0.4, name='dropout2_model')(dense1)\n    out = Dense(num_classes, activation=\"softmax\", name=\"out_layer\")(dropout_2)\n\n    model = Model(inputs=models_in, outputs=out, name=\"FER_Model\")\n\n    if show_summary:\n        model.summary()\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dcnn_in, dcnn_out = dcnn_pipeline(input_shape=(48,48,1))\nfl_in, fl_out = facial_landmarks_pipeline(input_shape=(68,2))\nhog_in, hog_out = facial_HOG_pipeline(input_shape=(3780,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = merge_models(\n    models_in=[dcnn_in, hog_in],\n    models_out=[dcnn_out, hog_out],\n    num_classes=3\n)\nplot_model(model, show_shapes=True, show_layer_names=True, expand_nested=True, dpi=50, to_file='model_1.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stopping = EarlyStopping(\n    monitor='val_accuracy',\n    min_delta=0.0001,\n    patience=5,\n    verbose=1,\n    restore_best_weights=True,\n)\n\nlr_scheduler = ReduceLROnPlateau(\n    monitor='val_accuracy',\n    min_delta=0.00025,\n    factor=0.25,\n    patience=3,\n    min_lr=1e-6,\n    verbose=1,\n)\n\ncallbacks = [\n    early_stopping,\n    lr_scheduler,\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The below data generator is taken from [this](https://github.com/keras-team/keras/issues/3386) keras issue thread."},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_generator(Xtrain_img, Xtain_hog, ytrain, batch_size):\n    while True:\n        idx = np.random.permutation(Xtrain_img.shape[0])\n\n        datagen = ImageDataGenerator(\n            rotation_range=15,\n            width_shift_range=0.15,\n            height_shift_range=0.15,\n            shear_range=0.15,\n            zoom_range=0.15,\n            horizontal_flip=True,\n        )\n\n        batches = datagen.flow(Xtrain_img[idx], ytrain[idx], batch_size=batch_size, shuffle=False)\n        idx0 = 0\n        for batch in batches:\n            idx1 = idx0 + batch[0].shape[0]\n\n            yield [batch[0], Xtain_hog[idx[ idx0:idx1 ]]], batch[1]\n\n            idx0 = idx1\n            if idx1 >= Xtrain_img.shape[0]:\n                break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nepochs = 45\nlr = 0.001\noptim = optimizers.Adam(learning_rate=lr)\n\nmodel.compile(\n        loss='categorical_crossentropy',\n        optimizer=optim,\n        metrics=['accuracy']\n)\n\nhistory = model.fit(\n    data_generator(Xtrain_img, Xtrain_hog, y_train, batch_size=batch_size,),\n    validation_data=([Xvalid_img, Xvalid_hog], y_valid),\n    steps_per_epoch=len(Xtrain_img) / batch_size,\n    epochs=epochs,\n    callbacks=callbacks,\n    use_multiprocessing=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\nfig = pyplot.figure(0, (12, 4))\n\nax = pyplot.subplot(1, 2, 1)\nsns.lineplot(history.epoch, history.history['accuracy'], label='train')\nsns.lineplot(history.epoch, history.history['val_accuracy'], label='valid')\npyplot.title('Accuracy')\npyplot.tight_layout()\n\nax = pyplot.subplot(1, 2, 2)\nsns.lineplot(history.epoch, history.history['loss'], label='train')\nsns.lineplot(history.epoch, history.history['val_loss'], label='valid')\npyplot.title('Loss')\npyplot.tight_layout()\n\npyplot.savefig('epoch_history_multipipe_model.png')\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_yaml = model.to_yaml()\nwith open(\"model.yaml\", \"w\") as yaml_file:\n    yaml_file.write(model_yaml)\n    \nmodel.save(\"model.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yhat_valid = model.predict([Xvalid_img, Xvalid_hog, Xvalid_fl])\nyhat_valid = np.argmax(yhat_valid, axis=1)\n\nscikitplot.metrics.plot_confusion_matrix(np.argmax(y_valid, axis=1), yhat_valid, figsize=(7,7))\npyplot.savefig(\"confusion_matrix.png\")\n\nprint(f'total wrong validation predictions: {np.sum(np.argmax(y_valid, axis=1) != yhat_valid)}\\n\\n')\nprint(classification_report(np.argmax(y_valid, axis=1), yhat_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}