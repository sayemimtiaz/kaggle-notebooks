{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\n\nimport seaborn as sns\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# Read the data\nwine = pd.read_csv('../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\nwine.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:43:14.084419Z","iopub.execute_input":"2021-08-20T02:43:14.084773Z","iopub.status.idle":"2021-08-20T02:43:14.116951Z","shell.execute_reply.started":"2021-08-20T02:43:14.084743Z","shell.execute_reply":"2021-08-20T02:43:14.115966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wine.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:43:14.183353Z","iopub.execute_input":"2021-08-20T02:43:14.183715Z","iopub.status.idle":"2021-08-20T02:43:14.231769Z","shell.execute_reply.started":"2021-08-20T02:43:14.183683Z","shell.execute_reply":"2021-08-20T02:43:14.230732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wine.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:43:14.233583Z","iopub.execute_input":"2021-08-20T02:43:14.233917Z","iopub.status.idle":"2021-08-20T02:43:14.242062Z","shell.execute_reply.started":"2021-08-20T02:43:14.233874Z","shell.execute_reply":"2021-08-20T02:43:14.240995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot histogram of features\nimport matplotlib.pyplot as plt\nwine.hist(bins=50, figsize=(20,15))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:43:14.244207Z","iopub.execute_input":"2021-08-20T02:43:14.244675Z","iopub.status.idle":"2021-08-20T02:43:17.239867Z","shell.execute_reply.started":"2021-08-20T02:43:14.244616Z","shell.execute_reply":"2021-08-20T02:43:17.238784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Boxplot the continuous features\ncont_features = list(wine.columns)\n\nfig = make_subplots(\n    rows=3,\n    cols=4,\n    subplot_titles=cont_features\n)\n\nfig.append_trace(go.Box(y=wine[cont_features[0]]),1,1)\nfig.append_trace(go.Box(y=wine[cont_features[1]]),1,2)\nfig.append_trace(go.Box(y=wine[cont_features[2]]),1,3)\nfig.append_trace(go.Box(y=wine[cont_features[3]]),1,4)\nfig.append_trace(go.Box(y=wine[cont_features[4]]),2,1)\nfig.append_trace(go.Box(y=wine[cont_features[5]]),2,2)\nfig.append_trace(go.Box(y=wine[cont_features[6]]),2,3)\nfig.append_trace(go.Box(y=wine[cont_features[7]]),2,4)\nfig.append_trace(go.Box(y=wine[cont_features[8]]),3,1)\nfig.append_trace(go.Box(y=wine[cont_features[8]]),3,2)\nfig.append_trace(go.Box(y=wine[cont_features[8]]),3,3)\nfig.append_trace(go.Box(y=wine[cont_features[8]]),3,4)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:43:17.241818Z","iopub.execute_input":"2021-08-20T02:43:17.242231Z","iopub.status.idle":"2021-08-20T02:43:17.426911Z","shell.execute_reply.started":"2021-08-20T02:43:17.242186Z","shell.execute_reply":"2021-08-20T02:43:17.42581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = wine.corr()\ncorr.style.background_gradient(cmap='plasma').set_precision(2)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:43:17.429094Z","iopub.execute_input":"2021-08-20T02:43:17.429471Z","iopub.status.idle":"2021-08-20T02:43:17.484562Z","shell.execute_reply.started":"2021-08-20T02:43:17.429432Z","shell.execute_reply":"2021-08-20T02:43:17.483598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wine = wine[\n            (wine['volatile acidity'] <= wine['volatile acidity'].quantile(.99)) &\n            (wine['residual sugar'] <= wine['residual sugar'].quantile(.99)) &\n            (wine['chlorides'] <= wine['chlorides'].quantile(.99)) &\n            (wine['free sulfur dioxide'] <= wine['free sulfur dioxide'].quantile(.99)) &\n            (wine['total sulfur dioxide'] <= wine['total sulfur dioxide'].quantile(.99))\n    ]\n\n# tr1 = FunctionTransformer(lambda x: x[(wine1)])","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:43:17.485952Z","iopub.execute_input":"2021-08-20T02:43:17.486231Z","iopub.status.idle":"2021-08-20T02:43:17.500241Z","shell.execute_reply.started":"2021-08-20T02:43:17.486205Z","shell.execute_reply":"2021-08-20T02:43:17.499511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Break off validation set from training data\ny = wine.quality\nX = wine.drop(['quality'], axis=1, inplace=False)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    train_size=0.8, test_size=0.2,\n                                                    random_state=0)\n\n\n\n# Normalize data\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range = (0,1))\nscaler.fit(X_train)\nXtrain_norm = scaler.transform(X_train)\nXtest_norm = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:43:17.501804Z","iopub.execute_input":"2021-08-20T02:43:17.50208Z","iopub.status.idle":"2021-08-20T02:43:17.522439Z","shell.execute_reply.started":"2021-08-20T02:43:17.502053Z","shell.execute_reply":"2021-08-20T02:43:17.521469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic regression model (normalized & Lasso regularization)","metadata":{}},{"cell_type":"code","source":"# Find optimal regularization strength ('c')\nc_vals = [100, 10, 1, 0.1]\n\nfor c in c_vals:\n    logreg_model = LogisticRegression(C=c, penalty=\"l1\", solver='saga', max_iter=10000,\n                                      multi_class='ovr')\n    logreg_model.fit(Xtrain_norm, y_train)\n    accuracy = logreg_model.score(Xtrain_norm, y_train)\n    test_accuracy = logreg_model.score(Xtest_norm, y_test)\n\n    print(f\"c: {c}, accuracy: {accuracy}, test_accuracy: {test_accuracy}\")\n\n# Fit softmax model\nlogreg_model = LogisticRegression(C=10, penalty=\"l1\", solver=\"saga\", max_iter=10000, multi_class='multinomial')\\\n                                    .fit(Xtrain_norm, y_train)\npredictions = logreg_model.predict(Xtest_norm)\n\nprint(f\"\\n{(predictions == y_test).sum()}/{y_test.shape[0]} clasificados correctamente\\n\")\n\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(logreg_model, Xtrain_norm, y_train, cv=5)\n\nprint(\"Exactitud de cada particion:\", scores)\nprint(\"Exactitud Promedio:\", scores.mean())","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:43:17.523654Z","iopub.execute_input":"2021-08-20T02:43:17.523961Z","iopub.status.idle":"2021-08-20T02:43:25.087541Z","shell.execute_reply.started":"2021-08-20T02:43:17.523933Z","shell.execute_reply":"2021-08-20T02:43:25.086692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Simple linear regression (normalized)","metadata":{}},{"cell_type":"code","source":"Xtrain_norm_lrs = X_train[['alcohol']]\n\nlrs_model = LinearRegression().fit(Xtrain_norm_lrs, y_train)\n# score = lr_model.score(Xtrain_norm, y_train)\nscores = cross_val_score(lrs_model, Xtrain_norm_lrs, y_train, cv=5)\n\nprint(\"Exactitud de cada particion:\", scores)\nprint(\"Exactitud Promedio:\", scores.mean())","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:43:25.088699Z","iopub.execute_input":"2021-08-20T02:43:25.088976Z","iopub.status.idle":"2021-08-20T02:43:25.283071Z","shell.execute_reply.started":"2021-08-20T02:43:25.088949Z","shell.execute_reply":"2021-08-20T02:43:25.282321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Multiple linear regression (normalized)","metadata":{}},{"cell_type":"code","source":"lr_model = LinearRegression().fit(Xtrain_norm, y_train)\n# score = lr_model.score(Xtrain_norm, y_train)\nscores = cross_val_score(lr_model, Xtrain_norm, y_train, cv=5)\n\nprint(\"Exactitud de cada particion:\", scores)\nprint(\"Exactitud Promedio:\", scores.mean())","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:43:25.287419Z","iopub.execute_input":"2021-08-20T02:43:25.289106Z","iopub.status.idle":"2021-08-20T02:43:25.322439Z","shell.execute_reply.started":"2021-08-20T02:43:25.289068Z","shell.execute_reply":"2021-08-20T02:43:25.321697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Polynomial regression","metadata":{}},{"cell_type":"code","source":"# Fitting Polynomial Regression to the dataset\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_reg = PolynomialFeatures(degree=2)\nX_poly = poly_reg.fit_transform(X_train)\npol_model = LinearRegression().fit(X_poly, y_train)\n\nscores = cross_val_score(pol_model, X_poly, y_train, cv=5)\n\nprint(\"Exactitud de cada particion:\", scores)\nprint(\"Exactitud Promedio:\", scores.mean())","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:43:25.323933Z","iopub.execute_input":"2021-08-20T02:43:25.32434Z","iopub.status.idle":"2021-08-20T02:43:25.425738Z","shell.execute_reply.started":"2021-08-20T02:43:25.32431Z","shell.execute_reply":"2021-08-20T02:43:25.42467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Akaike Information Criteron","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom math import log\n\ndef calculate_aic(label, model, X, y):\n    print('Model: {}'.format(label))\n    \n    # numero de parametros\n    num_params = len(model.coef_) + 1\n    print('Number of parameters: %d' % (num_params))\n\n    # predicciones\n    yhat = model.predict(X)\n\n    # calcular el mean squared error\n    mse = mean_squared_error(y, yhat)\n    print('MSE: %.3f' % mse)\n    \n    n = len(y)\n    \n    '''calculate aic for linear regression'''\n    aic = n * log(mse) + 2 * num_params\n    return print('AIC: %.3f' % aic)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:43:25.427391Z","iopub.execute_input":"2021-08-20T02:43:25.428055Z","iopub.status.idle":"2021-08-20T02:43:25.504935Z","shell.execute_reply.started":"2021-08-20T02:43:25.428Z","shell.execute_reply":"2021-08-20T02:43:25.50379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import log_loss\n\ndef calculate_aic_log(label, logmodel, X, y):\n    print('Model: {}'.format(label))\n    \n    # numero de parametros\n    num_params = len(logmodel.coef_) + 1\n    print('Number of parameters: %d' % (num_params))\n\n    # predicciones\n    yhat = logmodel.predict_proba(X)\n\n    # calcular el mean squared error\n    logloss = log_loss(y, yhat)\n    print('Logloss: %.3f' % logloss)\n    \n    n = len(y)\n    \n    '''calculate aic for logistic regression'''\n    aic = (-2/n) * log(logloss) + (2 * (num_params/n))\n    return print('AIC: %.3f' % aic)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:43:25.506595Z","iopub.execute_input":"2021-08-20T02:43:25.5074Z","iopub.status.idle":"2021-08-20T02:43:25.517587Z","shell.execute_reply.started":"2021-08-20T02:43:25.507345Z","shell.execute_reply":"2021-08-20T02:43:25.516415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calculate_aic('Simple Linear Regression', lrs_model, Xtrain_norm_lrs, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:43:25.519342Z","iopub.execute_input":"2021-08-20T02:43:25.520082Z","iopub.status.idle":"2021-08-20T02:43:25.535325Z","shell.execute_reply.started":"2021-08-20T02:43:25.520031Z","shell.execute_reply":"2021-08-20T02:43:25.534085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calculate_aic('Multiple Linear Regression', lr_model, Xtrain_norm, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:43:25.536988Z","iopub.execute_input":"2021-08-20T02:43:25.537449Z","iopub.status.idle":"2021-08-20T02:43:25.545738Z","shell.execute_reply.started":"2021-08-20T02:43:25.537405Z","shell.execute_reply":"2021-08-20T02:43:25.544761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calculate_aic('Polynomial Linear Regression',pol_model, X_poly, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:43:25.547566Z","iopub.execute_input":"2021-08-20T02:43:25.54832Z","iopub.status.idle":"2021-08-20T02:43:25.559238Z","shell.execute_reply.started":"2021-08-20T02:43:25.54827Z","shell.execute_reply":"2021-08-20T02:43:25.558007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calculate_aic_log('Logistic Regression', logreg_model, Xtrain_norm, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:43:25.56096Z","iopub.execute_input":"2021-08-20T02:43:25.561455Z","iopub.status.idle":"2021-08-20T02:43:25.573693Z","shell.execute_reply.started":"2021-08-20T02:43:25.561411Z","shell.execute_reply":"2021-08-20T02:43:25.572582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bayesian Information Criterion","metadata":{}},{"cell_type":"code","source":"from math import log\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\ndef calculate_bic(label, model, X, y):\n    print('Model: {}'.format(label))\n\n    # numero de parametros\n    num_params = len(model.coef_) + 1\n    print('Number of parameters: %d' % (num_params))\n\n    # predicciones\n    yhat = model.predict(X)\n\n    # calcular el mean squared error\n    mse = mean_squared_error(y, yhat)\n    print('MSE: %.3f' % mse)\n\n    n = len(y)\n    \n    '''calculate bic for linear regression'''\n    bic = n * log(mse) + num_params * log(n)\n    return print('BIC: %.3f' % bic)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:43:25.575562Z","iopub.execute_input":"2021-08-20T02:43:25.576341Z","iopub.status.idle":"2021-08-20T02:43:25.587156Z","shell.execute_reply.started":"2021-08-20T02:43:25.576285Z","shell.execute_reply":"2021-08-20T02:43:25.585999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_bic_log(label, logmodel, X, y):\n    print('Model: {}'.format(label))\n\n    # numero de parametros\n    num_params = len(logmodel.coef_) + 1\n    print('Number of parameters: %d' % (num_params))\n\n    # predicciones\n    yhat = logmodel.predict_proba(X)\n\n    # calcular el mean squared error\n    logloss = log_loss(y, yhat)\n    print('Logloss: %.3f' % logloss)\n\n    n = len(y)\n    \n    '''calculate aic for logistic regression'''\n    bic = -2 * log(logloss) + log(n) * num_params\n    return print('BIC: %.3f' % bic)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:43:25.589022Z","iopub.execute_input":"2021-08-20T02:43:25.589795Z","iopub.status.idle":"2021-08-20T02:43:25.603834Z","shell.execute_reply.started":"2021-08-20T02:43:25.589741Z","shell.execute_reply":"2021-08-20T02:43:25.602696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calculate_bic('Simple Linear Regression', lrs_model, Xtrain_norm_lrs, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:43:25.605704Z","iopub.execute_input":"2021-08-20T02:43:25.606624Z","iopub.status.idle":"2021-08-20T02:43:25.61916Z","shell.execute_reply.started":"2021-08-20T02:43:25.606569Z","shell.execute_reply":"2021-08-20T02:43:25.618046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calculate_bic('Multiple Linear Regression', lr_model, Xtrain_norm, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:43:25.620978Z","iopub.execute_input":"2021-08-20T02:43:25.621747Z","iopub.status.idle":"2021-08-20T02:43:25.633916Z","shell.execute_reply.started":"2021-08-20T02:43:25.621698Z","shell.execute_reply":"2021-08-20T02:43:25.632775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calculate_bic('Polynomial Linear Regression',pol_model, X_poly, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:43:25.636082Z","iopub.execute_input":"2021-08-20T02:43:25.636825Z","iopub.status.idle":"2021-08-20T02:43:25.645205Z","shell.execute_reply.started":"2021-08-20T02:43:25.636769Z","shell.execute_reply":"2021-08-20T02:43:25.644015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calculate_bic_log('Logistic Regression', logreg_model, Xtrain_norm, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:43:25.647094Z","iopub.execute_input":"2021-08-20T02:43:25.647922Z","iopub.status.idle":"2021-08-20T02:43:25.65982Z","shell.execute_reply.started":"2021-08-20T02:43:25.647866Z","shell.execute_reply":"2021-08-20T02:43:25.658573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Selecci칩n de modelo","metadata":{}},{"cell_type":"code","source":"print('El modelo de regresi칩n polinomial es el que presenta menores valores para los criterios Akaike y bayesiano, lo que indica que es el modelo que presenta la mejor combinaci칩n de desempe침o y complejidad. Esto a pesar de que su exactitud es menor que otros modelos')\n","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:43:25.661798Z","iopub.execute_input":"2021-08-20T02:43:25.662655Z","iopub.status.idle":"2021-08-20T02:43:25.66873Z","shell.execute_reply.started":"2021-08-20T02:43:25.662586Z","shell.execute_reply":"2021-08-20T02:43:25.667701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}