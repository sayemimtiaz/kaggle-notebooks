{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\nThe data were collected from the Taiwan Economic Journal for the years 1999 to 2009. Company bankruptcy was defined based on the business regulations of the Taiwan Stock Exchange.\n# Purpose\nThe dataset has lots of features (96) so it's an excelent opportunity to put into practice dimensionality reduction techniques, EDA and finally a machine learning prediction model\n# Table of contents\n1. [Data Loading and Data Cleaning](#1.-Data-Loading-and-Data-Cleaning)\n2. [Model Based Feature Selection](#2.-Model-Based-Feature-Selection)\n3. [Descriptive Analysis](#3.-Descriptive-Analysis)\n4. [Data Analysis](#4.-Data-Analysis)\n5. [Predicting bankruptcy](#5.-Predicting-bankruptcy)\n6. [Conclusions](#6.-Conclusions)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true}},{"cell_type":"code","source":"# runtime\nimport timeit\n\n# Data manipulation\nimport pandas as pd\nimport numpy as np\n\n# Data visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# preprocessing\nfrom sklearn.feature_selection import RFE\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, Normalizer\n\n# Ml model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nnp.warnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Data Loading and Data Cleaning\nIn this step we are just going to see if we have any null's and see the shape of the dataset. Descriptive analytics wouldn't make sense since we are going to drop lot's of features in step number 2","metadata":{}},{"cell_type":"code","source":"bank = pd.read_csv('../input/company-bankruptcy-prediction/data.csv')\n\nprint(bank.isnull().values.any())\nprint(bank.shape)\n\nbank","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"great! let's start dropping features","metadata":{}},{"cell_type":"markdown","source":"# 2. Model Based Feature Selection\nModel based feature selection uses a supervised machine learning model to judge the importance of each feature, and keeps only the most important ones. For this case, we are going to use a random forest classifier, since it usually yields good results and because this is a classification task","metadata":{}},{"cell_type":"code","source":"# training set\nX = bank.iloc[:,1:].values\ny = bank.iloc[:,0].values.reshape(-1, 1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# determining optimal number of features\nn_features = [5, 10, 15, 20, 25, 30, 35, 40]\nfor i in n_features:\n    # Building the model based feature selection\n    select = RFE(RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=i)\n\n    select.fit(X_train, y_train)\n\n    mask = select.get_support()\n\n    X_train_rfe = select.transform(X_train)\n    X_test_rfe = select.transform(X_test)\n\n    score = RandomForestClassifier().fit(X_train_rfe, y_train).score(X_test_rfe, y_test)\n    \n    print(\"Test score: {:.3f}\".format(score), \" number of features: {}\".format(i))\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There's not so much difference between the scores with different features. We are going to work with 15 features since is a 'workable' number of features and has a good a score. Let's run the algorithm again and get the features","metadata":{}},{"cell_type":"code","source":"select = RFE(RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=15)\n\nselect.fit(X_train, y_train)\n\nmask = select.get_support()\n\nX_train_rfe = select.transform(X_train)\nX_test_rfe = select.transform(X_test)\n\nscore = RandomForestClassifier().fit(X_train_rfe, y_train).score(X_test_rfe, y_test)\n\nprint(\"Test score: {:.3f}\".format(score), \" number of features: {}\".format(15))\n\nfeatures = pd.DataFrame({'features':list(bank.iloc[:,1:].keys()), 'select':list(mask)})\nfeatures = list(features[features['select']==True]['features'])\nfeatures.append('Bankrupt?')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see the DataFrame we are going to work with it's stats","metadata":{}},{"cell_type":"code","source":"bank = bank[features]\nbank","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Descriptive Analysis\n\nNow that we have a more workable number of features, let's take a look at their stats","metadata":{}},{"cell_type":"markdown","source":"## 3.1. Target Variable","metadata":{}},{"cell_type":"code","source":"sns.countplot(data=bank, x='Bankrupt?', palette='bwr')\nplt.show()\n\nbank.groupby('Bankrupt?').size()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have highly unbalanced data, this is a problem since the machine learning algorithm could be making prediction based mainly on data majority, that's the reason we got so good reasults in section 2. In this case we are going and try oversampling the data: making synthetic data out of the smaller sample (1)","metadata":{}},{"cell_type":"markdown","source":"## 3.2. Features","metadata":{}},{"cell_type":"code","source":"bank.hist(figsize=(20,20), edgecolor='white')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the data is rich on outliers, and in some other the values are located in just one bin. Let's take a closer look at ' Non-industry income and expenditure/revenue'","metadata":{}},{"cell_type":"code","source":"bins = pd.cut(bank[' Non-industry income and expenditure/revenue'], bins=10)\nbins = pd.DataFrame(bins)\nbins.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lower = bank[' Non-industry income and expenditure/revenue'] >0.3025\nupper = bank[' Non-industry income and expenditure/revenue'] <0.3045\n\nclose = bank[lower & upper]\nprint('Rows with outliers: {}'.format(bank.shape[0]))\nprint('Rows withou outliers: {}'.format(close.shape[0]))\nprint('information lost = {} rows'.format(bank.shape[0]-close.shape[0]))\nclose[' Non-industry income and expenditure/revenue'].hist(edgecolor='white')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distribution does have a normal distribution but is highly influenced by the outliers. Therefore, when analysing the data, will be better to use the median as our analysis tool for central measures.\nAdditionally, When we are building our model, we could try and take this outliers out just to see if we can get a better result","metadata":{}},{"cell_type":"code","source":"display(bank.describe())\nbank.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3. Correlations","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(14,12))\n\nsns.heatmap(bank.corr(), vmin=-1, vmax=1, cmap=sns.diverging_palette(20, 220, as_cmap=True), annot=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have some interesting correlations. Let's inspect the top 3 and see if we can find any bankruptcy pattern","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1,3, figsize=(20, 6))\n\nsns.scatterplot(data=bank, x=' Net profit before tax/Paid-in capital', y=' Persistent EPS in the Last Four Seasons', hue='Bankrupt?', ax=ax[0])\nsns.scatterplot(data=bank, x=' Persistent EPS in the Last Four Seasons', y=' Net Value Per Share (A)', hue='Bankrupt?', ax=ax[1])\nsns.scatterplot(data=bank, x=\" Net Income to Stockholder's Equity\", y=' Borrowing dependency', hue='Bankrupt?', ax=ax[2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bank.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We start to see some patterns\n- companies with a low 'Net profit before tax/Paid-in capital', 'Persistent EPS in the Last Four Seasons' and 'Net Value Per Share (A)' tend to go bankrupt\n- 'Borrowing dependency' has bankrupt companies distributed through all it's range. But, around 0.4, are located the companies that do not go bankrupt. Having around 0.4 doesn't guarantee to be bankrupt safe since there are a lot of companies that went bankrupt with this index, but having a higher o lower index seems to be critical since there aren't any companies operating with this kind of index. Same goes to \"Net Income to Stockholder's Equity\" but around 0.8","metadata":{}},{"cell_type":"markdown","source":"## Descriptive Analysis Conclusions\n- We have highly unbalanced data. Therefore, we are going to try applying oversampling\n- Most of the features have outliers. Median will be a better analysis method and, also, taking some outliers out will be a good idea when building the model\n- companies with a low 'Net profit before tax/Paid-in capital', 'Persistent EPS in the Last Four Seasons' and 'Net Value Per Share (A)' tend to go bankrupt. **A KNN algorithm would yield good results since the clusters are so evident**\n- 0.4 'Borrowing dependency' is a good indicator to operate but doesn't completely safe you from bankruptcy\n- 0.8 \"Net Income to Stockholder's Equity\" is a good indicator to operate but doesn't completely safe you from bankruptcy","metadata":{}},{"cell_type":"markdown","source":"# 4. Data Analysis\nLet's compare the median of bankrupt and not bankrupt companies of each feature to further see if we can find a tendency","metadata":{}},{"cell_type":"code","source":"central = bank.groupby('Bankrupt?').median().reset_index()\nfeatures = list(central.keys()[1:])\n\nfig, ax = plt.subplots(5,3, figsize=(20,20))\n\nax = ax.ravel()\nposition = 0\n\nfor i in features:\n    sns.barplot(data=central, x='Bankrupt?', y=i, ax=ax[position], palette='bwr')\n    position += 1\n    \nplt.show()\ndisplay(central)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Analysis Conclusions\nLet's mention the most evident tendencies:\n\nCompanies with:\n- high \"Interest-bearing debt interest rate\" tend to go bankrupt (≈ 0.000499)\n- high \"Total debt/Total net worth\" tend to go bankrupt (≈ 0.015723)\n- high \"Fixed Assets Turnover Frequency\" tend to go bankrupt (≈ 0.001225)\n- low  \"Cash/Total Assets\" tend to go bankrupt (≈ 0.023755)\n- low \"Equity to Liability\" tend to go bankrupt (≈ 0.018662)\n\nAlso, These indicators should be enough to build a reliable model since the trend is very clear. Let's build our model","metadata":{}},{"cell_type":"markdown","source":"# 5. Predicting bankruptcy","metadata":{"trusted":true}},{"cell_type":"markdown","source":"## 5.1 KNN\nA KNN if we recall section 3 and 4 conclusions, an KNN algorithm with features 'Net profit before tax/Paid-in capital', 'Persistent EPS in the Last Four Seasons', \"Interest-bearing debt interest rate\", \"Total debt/Total net worth\", \"Fixed Assets Turnover Frequency\", \"Cash/Total Assets\" and \"Equity to Liability\" should do the work. let's go and try. \n\nWe also have to take into account that we are dealing with highly imbalanced data, so oversampling will be part of the preprocessing phase (part 2 next week).","metadata":{}},{"cell_type":"code","source":"model = ['Bankrupt?', ' Net profit before tax/Paid-in capital', ' Persistent EPS in the Last Four Seasons', \" Interest-bearing debt interest rate\", \" Total debt/Total net worth\", \" Fixed Assets Turnover Frequency\", \" Cash/Total Assets\", \" Equity to Liability\"]\nmodel = bank[model]\nX = model.iloc[:,1:].values\ny = model.iloc[:,0].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nbest_n = 0\nbest_training = 0\nbest_test = 0\n\nfor i in range(1, 20):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    \n    training = knn.score(X_train, y_train)\n    test = knn.score(X_test, y_test)\n    \n    if test > best_test:\n        best_n = i\n        best_training = training\n        best_test = test\n\nprint(\"best number of neighbors: {}\".format(best_n))\nprint(\"best training set score : {:.3f}\".format(best_training))\nprint(\"best test set score: {:.3f}\".format(best_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = timeit.default_timer()\n\nknn = KNeighborsClassifier(n_neighbors=7)\nknn.fit(X_train, y_train)\nprint(\"training set score : {:.2f}\".format(knn.score(X_train, y_train)))\nprint(\"test set score: {:.2f}\".format(knn.score(X_test, y_test)))\n\nstop = timeit.default_timer()\nprint('Time: ', stop - start)  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.2 Gradient Boosting Classifer\n\nHere we are going to first apply a more sophisticated classifier on our reduced data and then on the whole dataset. In the end, we compare the three models\n### 5.3 Gradient Boosting Classifer, reduced features","metadata":{}},{"cell_type":"code","source":"X = model.iloc[:,1:].values\ny = model.iloc[:,0].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nstart = timeit.default_timer()\ngbrt = GradientBoostingClassifier(n_estimators=100, random_state=42, max_depth=1).fit(X_train, y_train)\n\nprint(\"training set score : {:.2f}\".format(gbrt.score(X_train, y_train)))\nprint(\"test set score: {:.2f}\".format(gbrt.score(X_test, y_test)))\n\nstop = timeit.default_timer()\nprint('Time: ', stop - start)  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.4 Gradient Boosting Classifer, all features","metadata":{}},{"cell_type":"code","source":"bank = pd.read_csv('../input/company-bankruptcy-prediction/data.csv')\nX = bank.iloc[:,1:].values\ny = bank.iloc[:,0].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nstart = timeit.default_timer()\ngbrt = GradientBoostingClassifier(n_estimators=100, random_state=42, max_depth=1).fit(X_train, y_train)\n\nprint(\"training set score : {:.2f}\".format(gbrt.score(X_train, y_train)))\nprint(\"test set score: {:.2f}\".format(gbrt.score(X_test, y_test)))\n\nstop = timeit.default_timer()\nprint('Time: ', stop - start)  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Conclusions\n\n- We were able to build three models with a set accuracy of 0.97, while significantly reducing the number of feautures (just 7). This lead us to save running time (from 5.47 seconds to only 0.5)\n- With the reduced features, we were also able to describe how a company might go bankrupt or not, explaining the model better. The features conclusiones were:\n\nCompanies with:\n- high \"Interest-bearing debt interest rate\" tend to go bankrupt (≈ 0.000499)\n- high \"Total debt/Total net worth\" tend to go bankrupt (≈ 0.015723)\n- high \"Fixed Assets Turnover Frequency\" tend to go bankrupt (≈ 0.001225)\n- low  \"Cash/Total Assets\" tend to go bankrupt (≈ 0.023755)\n- low \"Equity to Liability\" tend to go bankrupt (≈ 0.018662)\n- companies with a low 'Net profit before tax/Paid-in capital', 'Persistent EPS in the Last Four Seasons' and 'Net Value Per Share (A)' tend to go bankrupt","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}