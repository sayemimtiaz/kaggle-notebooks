{"cells":[{"metadata":{},"cell_type":"markdown","source":"## HR Analytics dataset from Kaggle\nhttps://www.kaggle.com/vjchoudhary7/hr-analytics-case-study\n\n\nProblem Statement\nA large company named XYZ, employs, at any given point of time, around 4000 employees. However, every year, around 15% of its employees leave the company and need to be replaced with the talent pool available in the job market. The management believes that this level of attrition (employees leaving, either on their own or because they got fired) is bad for the company, because of the following reasons -\n\n- The former employeesâ€™ projects get delayed, which makes it difficult to meet timelines, resulting in a reputation loss among consumers and partners\n- A sizeable department has to be maintained, for the purposes of recruiting new talent\n- More often than not, the new employees have to be trained for the job and/or given time to acclimatise themselves to the company\n\nHence, the management has contracted an HR analytics firm to understand what factors they should focus on, in order to curb attrition. In other words, they want to know what changes they should make to their workplace, in order to get most of their employees to stay. Also, they want to know which of these variables is most important and needs to be addressed right away.\n\nSince you are one of the star analysts at the firm, this project has been given to you.\n\n<b><u>Goal of the case study</u></b><br>\n<b>You are required to model the probability of attrition using a logistic regression. The results thus obtained will be used by the management to understand what changes they should make to their workplace, in order to get most of their employees to stay.</b>\n\n"},{"metadata":{},"cell_type":"markdown","source":"### Import required Python packages\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Numerical libraries\nimport numpy as np   \n\n# to handle data in form of rows and columns \nimport pandas as pd    \n\n# importing ploting libraries\nimport matplotlib.pyplot as plt   \n\n#importing seaborn for statistical plots\nimport seaborn as sns\n\n# Import Logistic Regression machine learning library\nfrom sklearn.linear_model import LogisticRegression\n\n#Sklearn package's data splitting function which is based on random function\nfrom sklearn.model_selection import train_test_split\n\n\n# calculate accuracy measures and confusion matrix\nfrom sklearn import metrics\n\n# To scale the dimensions we need scale function which is part of sckikit preprocessing libraries\nfrom sklearn import preprocessing\n\n# To enable plotting graphs in Jupyter notebook\n%matplotlib inline ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read data files to Dataframes\ngen_df = pd.read_csv('../input/hr-analytics-case-study/general_data.csv')\neos_df = pd.read_csv('../input/hr-analytics-case-study/employee_survey_data.csv')\nmos_df = pd.read_csv('../input/hr-analytics-case-study/manager_survey_data.csv')\n\nprint('general data shape = ',gen_df.shape)\nprint('employee survey data shape = ',eos_df.shape)\nprint('manager survey data shape = ',mos_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge employee survey and manager survey data with general data using join() method\ngen_df=gen_df.join(eos_df,on='EmployeeID',rsuffix='_EOS')\ngen_df=gen_df.join(mos_df,on='EmployeeID',rsuffix='_MOS')\ngen_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gen_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check missing values count\nmissing_values=gen_df.columns[gen_df.isnull().any()]\ngen_df[missing_values].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace missing values with mean\ngen_df.fillna(gen_df.mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check again to confirm there are no more missing values\nmissing_values=gen_df.columns[gen_df.isnull().any()]\ngen_df[missing_values].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gen_df.describe(include='all').T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# There are columns like EmployeeCount, Over18, StandardHours that has only 1 value hence we would drop them \ngen_df.drop(['EmployeeCount','Over18','StandardHours'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's get overall number and percent of people left and stayed\nno=gen_df.Attrition.value_counts()['No']\nyes=gen_df.Attrition.value_counts()['Yes']\nprint('Attrition->No: Count=',no,' & Percentage=',((no/len(gen_df))*100).round(2))\nprint('Attrition->Yes: Count=',yes,' & Percentage=',((yes/len(gen_df))*100).round(2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets add Attrition Column to eos_df and find out relationship between Employee Survey Results and Attrition\neos_df=eos_df.join(gen_df['Attrition'],on=['EmployeeID'],rsuffix='_gen')\nmos_df=mos_df.join(gen_df['Attrition'],on=['EmployeeID'],rsuffix='_gen')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create new dataframe of Attrition from EmployeeSurvey\neos_yes_df=eos_df.loc[eos_df['Attrition']=='Yes']\n\n# create new dataframe of Attrition from ManagerSurvey\nmos_yes_df=mos_df.loc[mos_df['Attrition']=='Yes']\n\n# Create summary dataframe for ratings\nyes_summary=pd.DataFrame({'Ratings':[0,1,2,3,4]})\n\n# Create new columns from EmployeeSurvey and store count in summary dataframe\nyes_summary['EnvironmentSatisfaction']=eos_yes_df.groupby(['EnvironmentSatisfaction'])['Attrition'].count()\nyes_summary['JobSatisfaction']=eos_yes_df.groupby(['JobSatisfaction'])['Attrition'].count()\nyes_summary['WorkLifeBalance']=eos_yes_df.groupby(['WorkLifeBalance'])['Attrition'].count()\n\nyes_summary['JobInvolvement']=mos_yes_df.groupby(['JobInvolvement'])['Attrition'].count()\nyes_summary['PerformanceRating']=mos_yes_df.groupby(['PerformanceRating'])['Attrition'].count()\n\n# remove row with NA value\nyes_summary.fillna(0,inplace=True)\n\n# Replace Rating number to Text\nyes_summary.Ratings.replace({1:'Low',2:'Medium',3:'High',4:'Very High',},inplace=True)\n\nyes_summary.plot(kind='bar',x='Ratings',figsize=(20,10),title='Attrition related to Employee and Manager Survey')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observation\n\n### Above data suggests that People with High and Very High Employee Satisfaction and Performance Ratings have left an Organisation. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets look Age of people who have left Organisation\nattrition_age=gen_df.loc[gen_df['Attrition']=='Yes'].groupby('Age')['Attrition'].count()\\\n.plot(kind='line',figsize=(20,10),title='Age wise Attrition')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### High number of People between age of 25 and 35 have left an Organisation with max being at age of 29 and 31"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,7))\nsns.countplot(x='Attrition',data=gen_df,hue='JobLevel',ax=ax1)\nsns.countplot(x='Attrition',data=gen_df,hue='Gender',ax=ax2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Majority of People after Job Level 2 have left firm. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(gen_df[['Age','MonthlyIncome','DistanceFromHome','Attrition']],hue = 'Attrition')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=gen_df.drop(['Attrition','EmployeeID'],axis=1)\n#x=gen_df.drop(['Attrition','EmployeeID','BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus'],axis=1)\ny=gen_df['Attrition']\ny.replace({'Yes':1,'No':0},inplace=True)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# There are few fiels whose data type is Object indicating they have categorical values.\n# Attrition, BusinessTravel, Department, EducationField, Gender, JobRole, MaritalStatus\n\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\n\n\nx['BusinessTravel'] = label_encoder.fit_transform(x['BusinessTravel'].fillna('0'))    \nx['Department'] = label_encoder.fit_transform(x['Department'].fillna('0'))    \nx['EducationField'] = label_encoder.fit_transform(x['EducationField'].fillna('0'))    \nx['Gender'] = label_encoder.fit_transform(x['Gender'].fillna('0'))    \nx['JobRole'] = label_encoder.fit_transform(x['JobRole'].fillna('0'))    \nx['MaritalStatus'] = label_encoder.fit_transform(x['MaritalStatus'].fillna('0'))    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model 1 - Without Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.30,random_state=1)\ntype(x_train)\n\n\nmodel=LogisticRegression()\nmodel.fit(x_train,y_train)\ny_predict=model.predict(x_test)\nmodel_score=model.score(x_test,y_test)\nprint('Accuracy = ',model_score)\nprint(metrics.confusion_matrix(y_test,y_predict))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model 2 - Scale"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\n\nx_train_scaled = preprocessing.scale(x_train)\nx_test_scaled = preprocessing.scale(x_test)\n\nmodel=LogisticRegression()\nmodel.fit(x_train_scaled,y_train)\ny_predict=model.predict(x_test_scaled)\nmodel_score=model.score(x_test_scaled,y_test)\nprint('Accuracy = ',model_score)\nprint(metrics.confusion_matrix(y_test,y_predict))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model 3 - MinMaxScaler"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\n\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_train_scaled = min_max_scaler.fit_transform(x_train)\nx_test_scaled = min_max_scaler.fit_transform(x_test)\n\nmodel=LogisticRegression()\nmodel.fit(x_train_scaled,y_train)\ny_predict=model.predict(x_test_scaled)\nmodel_score=model.score(x_test_scaled,y_test)\nprint('Accuracy = ',model_score)\nprint(metrics.confusion_matrix(y_test,y_predict))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model 4 - Standard Scaler"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\n\n\nx_scaler = preprocessing.StandardScaler().fit(x_train)\nx_test_scaler = preprocessing.StandardScaler().fit(x_test)\n\nx_train_scaled=x_scaler.transform(x_train)\nx_test_scaled=x_scaler.transform(x_test)\n\nmodel=LogisticRegression()\nmodel.fit(x_train_scaled,y_train)\ny_predict=model.predict(x_test_scaled)\nmodel_score=model.score(x_test_scaled,y_test)\nprint('Accuracy = ',model_score)\nprint(metrics.confusion_matrix(y_test,y_predict))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model 6 - Max Absolute Scaler"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\n\nmax_abs_scaler = preprocessing.MaxAbsScaler()\nx_train_scaled = max_abs_scaler.fit_transform(x_train)\nx_test_scaled = max_abs_scaler.fit_transform(x_test)\n\nmodel=LogisticRegression()\nmodel.fit(x_train_scaled,y_train)\ny_predict=model.predict(x_test_scaled)\nmodel_score=model.score(x_test_scaled,y_test)\nprint('Accuracy = ',model_score)\nprint(metrics.confusion_matrix(y_test,y_predict))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model 6 - Quantile Transformer"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\n\nquantile_transformer = preprocessing.QuantileTransformer(random_state=0)\nx_train_scaled = quantile_transformer.fit_transform(x_train)\nx_test_scaled = quantile_transformer.fit_transform(x_test)\n\nmodel=LogisticRegression()\nmodel.fit(x_train_scaled,y_train)\ny_predict=model.predict(x_test_scaled)\nmodel_score=model.score(x_test_scaled,y_test)\nprint('Accuracy = ',model_score)\nprint(metrics.confusion_matrix(y_test,y_predict))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model 7 - Quantile Transformer with Normal Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\n\nquantile_transformer = preprocessing.QuantileTransformer(output_distribution='normal',random_state=0)\nx_train_scaled = quantile_transformer.fit_transform(x_train)\nx_test_scaled = quantile_transformer.fit_transform(x_test)\n\nmodel=LogisticRegression()\nmodel.fit(x_train_scaled,y_train)\ny_predict=model.predict(x_test_scaled)\nmodel_score=model.score(x_test_scaled,y_test)\nprint('Accuracy = ',model_score)\nprint(metrics.confusion_matrix(y_test,y_predict))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model 8 - Log Transformation using FunctionTransformer"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\n\nfrom sklearn.preprocessing import FunctionTransformer\n\ntransformer = FunctionTransformer(np.log1p, validate=True)\nx_train_scaled = transformer.transform(x_train)\nx_test_scaled = transformer.transform(x_test)\n\nmodel=LogisticRegression()\nmodel.fit(x_train_scaled,y_train)\ny_predict=model.predict(x_test_scaled)\nmodel_score=model.score(x_test_scaled,y_test)\nprint('Accuracy = ',model_score)\nprint(metrics.confusion_matrix(y_test,y_predict))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observation\n\n### Summary of Models\n\n<table>\n    <tr><th>Scaler</th><th>Accuracy</th><th>True Positives</th><th>True Negatives</th><th>False Positives (Type I Error)</th><th>False Negatives (Type II Error)</th></tr>\n    <tr><td>None</td><td>83.14</td><td>0</td><td>1100</td><td>0</td><td>223</td></tr>\n    <tr><td>Scale</td><td>83.82</td><td>12</td><td>1097</td><td>3</td><td>211</td></tr>\n    <tr><td>MinMaxScaler</td><td>83.67</td><td>10</td><td>1097</td><td>3</td><td>213</td></tr>\n    <tr><td>Standard Scaler</td><td>83.67</td><td>12</td><td>1097</td><td>5</td><td>211</td></tr>\n    <tr><td>MaxAbsScaler</td><td>83.9</td><td>10</td><td>1100</td><td>0</td><td>213</td></tr>\n    <tr style=\"background-color:#90ee90\"><td>QuantileTransformer</td><td>83.97</td><td>19</td><td>1092</td><td>8</td><td>204</td></tr>\n    <tr><td>QuantileTransformer - Normal Distribution</td><td>83.59</td><td>24</td><td>1082</td><td>18</td><td>199</td></tr>\n    <tr><td>FunctionTransformer</td><td>83.82</td><td>23</td><td>1086</td><td>14</td><td>200</td></tr>\n</table>\n\n### From above comparison table we can see that QuantileTransformer has better Accuracy and less Type 2 errors"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}