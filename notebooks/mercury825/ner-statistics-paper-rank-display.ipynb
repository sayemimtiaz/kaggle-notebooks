{"cells":[{"metadata":{},"cell_type":"markdown","source":"## The curent notebook is focus on COVID-19 related papers, this work including biomedical entities recognition, entity occurrence statistics, rank and display annotated paper by search topic and keywords, entity recognition model training.\n\n* Biomedical Entities Recognition and Display. \nThe biomedical entities mining is running on a customized biomedical dict. The entity recognition was built on Kindred (A Python biomedical entity recognition and relation extraction package that uses a supervised approach, the Kindred link is https://github.com/jakelever/kindred). Kindred relies on the Spacy toolkit (https://spacy.io/) for parsing, and then it uses a basic exact-string matching approach to find named terms. We save the Named entity recognition (NER) results in JSON format and render it in HTML format using displacy (https://explosion.ai/demos/displacy) modual from Spacy. \n\n    **In this new version, we added 4 entity types('DATE','GPE','CARDINAL','PERCENT') based on spacy model (\"en_core_web_sm\"), and we integrated entities recoginition results and solve conflicts from results based on biomedical dict.**\n    \n\n* Entity Occurrence Statistics. \nThen we produce simple entity occurrence statistics based on NER results. Because we focus on COVID-19 related papers, the entity occurrence is also the co-occurrence of entity and COVID-19. We sort and output TOP 10 co-occurrence terms and plot bar chart for each topic dict type. \n\n* Related Papers Rank and Display. \nSimple entity occurrence statistics may have biased results. Finding related papers and display of ranked paper result provides more confidence and intelligible results for CORD-19 challenge tasks. We first query papers by keywords (keywords can be empty list, that is, no filteration) and then count the occurrences of biomedical terms in one topic from dict. Render top papers' result in HTML table, highlight the topic terms and query keywords, sorted by irredudant terms' count. \n\n* Entity Recognition Model Training and Evaluating. \nThe NER results with exact-match-term annotations which can be used for NER model training. We implement the NER model training using Spacy.blank(\"en\") model. After train, we can use model to recognize biomedical entities and evaluate the prediction results.\n\n##  Dict libraries of Biomedical entities\nWe use a supervised approach, that we have defined 21 label types of the biomedical terms. The terms are defined by manual reading papers are taken from the literatures and from NCBI genomes and taxonomy database. We share this dict libraries at this link(https://www.kaggle.com/mercury825/customized-dict-for-cord19-entity-recognition). Then we add the dict libraries to input dict path (*/kaggle/input/customized-dict-for-cord19-entity-recognition/dict*), which are used to match CORD19 entities in order to focus on the known terms.\n\nThe custmoized dict_list is :\n* Medical_Dict:\n - disease.txt\n - symptoms.txt\n - diagnosis.txt\n - pathologic.txt\n - treatment.txt\n - drug.txt\n - organ.txt\n* Biology_Dict:\n - pathogen.txt\n - strain.txt\n - gene.txt\n - protein.txt\n - structure.txt\n - aa_mutation.txt\n - nucleotide_mutation.txt\n - host_human.txt\n - host_other.txt\n* COVID-19 behavior:\n - risk_factors.txt\n - location.txt\n - organization.txt\n - prevention.txt\n - transmission.txt\n\n## Pipeline description: \n1. Load all paper metadata (num of papers:51078)\n2. Find papers related to COVID-19 (num:3455)\n3. Supervised entity recognition, save as entity text file, format the results to JSON format and render results in HTML format (*.json and *.html )\n4. Merge all COVID-19 papers' NER results (num:3455) and extract 10% sample papers' NER results (num:346) for interactive preview (JSON and HTML). \n5. **(For Tasks) **Simple statistics and barplot of terms occurrences count based on NER results, output TOP occurrence terms from each dict type.\n6. **(For Tasks) **Find related papers by query biomedical topic and keywords (keywords can be empty). Render top paper results in HTML table, highlight the result terms, sorted by count of the irredudant terms. \n7. NER model training based on NER results, write NER model in directory\n8. evaluate NER model prediction results"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#!/usr/bin/env python\n# coding: utf8\nimport json\nimport pandas as pd\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. load all paper metadata"},{"metadata":{"trusted":true},"cell_type":"code","source":"def loadcorpus(title, abstract):\n    covid_paper_data=[]\n    if title is not '':\n        covid_paper_data.append(title)\n    if abstract is not '':\n        #clean the “abstract” which not belongs to the abstract mainbody text\n        if abstract.lower().startswith('abstract '):\n            abstract = abstract[9:]\n        elif abstract.lower().startswith('abstract'):\n            abstract = abstract[8:]\n        if abstract.lower().startswith('summary '):\n            abstract = abstract[8:]\n        elif abstract.lower().startswith('summary'):\n            abstract = abstract[7:]\n        elif abstract.lower().startswith('summary: '):\n            abstract = abstract[9:]\n        line = abstract.split('. ')\n        if line is not '':\n            covid_paper_data.extend(line)\n    return covid_paper_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"data_path = \"/kaggle/input/CORD-19-research-challenge\"\nmetadata_path = os.path.join(data_path,\"metadata.csv\")\nmeta_df = pd.read_csv(metadata_path, encoding='utf-8', dtype={\n    'cord_uid': str,\n    'doi': str,\n    'sha': str,\n    'title': str,\n    'abstract': str\n})\nprint(meta_df.shape) #up to latest release, total papers num: 51078\nmeta_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. find papers related to COVID-19\nusing key words to search texts of title and abstract in all papers"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"'''\nextract COVID-19 related papers\nkey words: COVID-19|Coronavirus disease 2019|Coronavirus Disease 2019|2019 novel coronavirus|2019 new coronavirus|SARS-CoV-2|SARS Coronavirus 2|Wuhan pneumonia|2019-nCoV\nWe will processe all covid19 related paper and output recognition results in working directory.\n'''\nmeta_df_title_abstract = meta_df[['cord_uid','doi','sha','title','abstract','publish_time']].fillna('') #process na\ns_covid_title = meta_df_title_abstract.title.str.contains('COVID-19|Coronavirus disease 2019|Coronavirus Disease 2019|2019 novel coronavirus|2019 new coronavirus|SARS-CoV-2|SARS Coronavirus 2|Wuhan pneumonia|2019-nCoV') #1780\ns_covid_abstract = meta_df_title_abstract.abstract.str.contains('COVID-19|Coronavirus disease 2019|Coronavirus Disease 2019|2019 novel coronavirus|2019 new coronavirus|SARS-CoV-2|SARS Coronavirus 2|Wuhan pneumonia|2019-nCoV') #1611\ncovid_bool = s_covid_title | s_covid_abstract  # the papers' title or abstract related to COVID-19 \ncovid_papers = meta_df_title_abstract[covid_bool]\ndel meta_df,covid_bool,meta_df_title_abstract\nprint(covid_papers.shape) #among all papers, the number of COVID-19 related papers: 3455\ncovid_papers.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"%%capture\n# all results display are too long\n# For result display preview, random sample 10% of covid19 related papers.\ncovid_papers_sample=covid_papers.sample(frac=0.1, replace=False, random_state=1) \nprint(covid_papers_sample.shape)\ncovid_papers_sample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"%%capture\n#A Python biomedical entity recognition and relation extraction package that uses a supervised approach\n!pip install kindred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. supervised entity recognition\nsupervised entity recognition is running on a customized biomedical dict using kindred package. We save raw output as entity text file, and then format the results to JSON format and render results in HTML format (.json and .html )"},{"metadata":{"trusted":true},"cell_type":"code","source":"#outdir: \"corpus-paper-anno\nannotations_tmp_dir = './corpus-paper-anno'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Customized dict directory declaration\ndict_list_dir = '/kaggle/input/customized-dict-for-cord19-entity-recognition/dict'\ndict_list = (',').join(os.listdir(dict_list_dir))\nprint(dict_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nWe use kindred package to implement entitiy recognize based on customized wordlists.\nDefine a class \"NewAnnotate\" to init dict libraries, parse paper corpus and implement entity recognition\nAdd new annotation types(\"DATE\", \"GPE\", \"CARDINAL\", \"PERCENT\") from spacy model entity prediction\n'''\nimport os\nimport kindred\nimport spacy\nnlp_core = spacy.load(\"en_core_web_sm\")\nnlp_need_tags = set(['DATE','GPE','CARDINAL','PERCENT'])\nimport bisect\n\nclass NewAnnotate(object):\n    def __init__(self, ner_out_dir, corpus_text, corpus_name, dictlists, dictlist_dir):\n        self.ner_out_dir = ner_out_dir\n        self.corpus_text = corpus_text\n        self.corpus_name = corpus_name\n        self.entityRecognizer = None\n        self.init_dict_recognizer(dictlists, dictlist_dir)\n    \n    #Init dict libraries for NER\n    def init_dict_recognizer(self, dictlists, dictlist_dir):\n        wordlistDict = {}\n        for wordlist in dictlists.split(','):\n            wordlistpath = os.path.join(dictlist_dir, wordlist)\n            assert os.path.isfile(wordlistpath), 'Unable to access file: %s' % wordlistpath\n            entityType = os.path.splitext(os.path.basename(wordlistpath))[0]\n            wordlistDict[entityType] = wordlistpath\n        wordlistLookup = kindred.EntityRecognizer.loadWordlists(wordlistDict, idColumn=0, termsColumn=0)\n        self.entityRecognizer = kindred.EntityRecognizer(wordlistLookup)\n    \n    def addparseWordsUsingSpacyTags(self, _ss, _oldents):\n        newents = _oldents\n        for nlp_doc in nlp_core.pipe([_ss], disable=[\"tagger\", \"parser\"]):\n            for _ent in nlp_doc.ents:\n                if _ent.label_ in nlp_need_tags:\n                    already_annotated = False\n                    for _oldent in _oldents:\n                        if (_oldent[0] >= _ent.start_char and _oldent[0] <= _ent.end_char) or (_oldent[1] >= _ent.start_char and _oldent[1] <= _ent.end_char) or (_ent.start_char >= _oldent[0] and _ent.start_char <= _oldent[1]) or (_ent.end_char >= _oldent[0] and _ent.end_char <= _oldent[1]):\n                            already_annotated = True\n                            break\n                    if not already_annotated:\n                        bisect.insort(newents, (_ent.start_char, _ent.end_char, _ent.label_, _ss[_ent.start_char: _ent.end_char]))\n                        \n        return newents\n    \n    #implement entity recognize\n    def entity_recognize(self):\n        parser = kindred.Parser()\n        print('begin entity annotate {}'.format(self.corpus_name))\n        sentenceCorpus = []\n        for ss_corpus_text in self.corpus_text.split(\". \"):\n            ss_corpus = kindred.Corpus(ss_corpus_text)\n            parser.parse(ss_corpus)\n            self.entityRecognizer.annotate(ss_corpus)\n            sentenceCorpus.append(ss_corpus)\n\n        if not os.path.exists(self.ner_out_dir):\n            os.makedirs(self.ner_out_dir)\n        entity_annotate_output_file = os.path.join(self.ner_out_dir, '{}.entity'.format(self.corpus_name))\n        file_out = open(entity_annotate_output_file, 'w', encoding='utf-8')\n        for ss_corpus in sentenceCorpus:\n            for _doc in ss_corpus.documents:\n                file_out.writelines('{}\\n'.format(_doc.text.strip().strip('.')))\n                _ents = []\n                for _entity in _doc.entities:\n                    for _pos in _entity.position:\n                        #file_out.writelines('{}\\t{} {} {}\\t{}\\n'.format(_entity.sourceEntityID, _entity.entityType, _pos[0], _pos[1], _entity.text))\n                        _ents.append((_pos[0], _pos[1], _entity.entityType, _entity.text))\n                newents = self.addparseWordsUsingSpacyTags(_doc.text.strip().strip('.'), _ents)\n                ei = 1\n                for _newent in newents:\n                    file_out.writelines('T{}\\t{} {} {}\\t{}\\n'.format(ei, _newent[2], _newent[0], _newent[1], _newent[3]))\n                    ei = ei + 1\n                file_out.writelines('\\n')\n        file_out.close()\n        print('end entity annotate {} file is {}'.format(self.corpus_name, entity_annotate_output_file))\n        return entity_annotate_output_file","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n    Define a class \"AnnoFormat\" to save the results in JSON format and render results in HTML format (.json and .html )\n    Add save and display of new annotation types(\"DATE\", \"GPE\", \"CARDINAL\", \"PERCENT\")\n'''\nimport json\nimport os\nimport re\nimport spacy\n\n# define the colors and options in HTML render\ncolors = {\"DIAGNOSIS\": \"#ef5050\", \"ORGANIZATION\": \"#F0E68C\", \"DISEASE\": \"#FF8888\", \"LOCATION\": \"#FFDAB9\",\n          \"PATHOLOGIC\": \"#FF00D4\",\"SYMPTOMS\": \"#D2691E\", \"TREATMENT\": \"#f172b2\", \"PATHOGEN\": \"#7FFFD4\",\n          \"GENE\": \"#33FFFF\",\"HOST_OTHER\": \"#00DD77\", \"HOST_HUMAN\": \"#1FBC9C\", \"PROTEIN\": \"#00BFFF\", \"STRUCTURE\": \"#87CEFA\",\n          \"ORGAN\":\"#FFFF77\", \"AA_MUTATION\":\"#801dae\", \"NUCLEOTIDE_MUTATION\":\"#cca4e3\", \"RISK_FACTORS\":\"#f9906f\", \"TRANSMISSION\":\"#b36d61\", \"DRUG\":\"#4b5cc4\", \"STRAIN\":\"#426666\", \n          \"DATE\":\"#FF7766\",\"GPE\":\"#FFDD55\",\"CARDINAL\":\"#FF8800\",\"PERCENT\":\"#FF5511\"\n          }\noptions = {\"ents\": [\"DIAGNOSIS\", \"ORGANIZATION\", \"DISEASE\", \"LOCATION\", \"PATHOLOGIC\", \"SYMPTOMS\", \"TREATMENT\",\n                    \"PATHOGEN\", \"GENE\",\"HOST_HUMAN\", \"HOST_OTHER\", \"PROTEIN\", \"STRUCTURE\",\n                    \"AA_MUTATION\", \"NUCLEOTIDE_MUTATION\", \"RISK_FACTORS\", \"TRANSMISSION\", \"DRUG\", \"STRAIN\", \"ORGAN\",\n                    \"DATE\", \"GPE\", \"CARDINAL\", \"PERCENT\"\n                    ], \"colors\": colors}\n\nclass AnnoFormat(object):\n    def __init__(self, corpus, corpus_format_dir, anno_file, publish_time):\n        self.anno_file=anno_file\n        self.corpus = corpus\n        self.format_outdir = corpus_format_dir\n        self.texts = \"\"\n        self.entitys = []\n        self.jsonformat = {}\n        self.entsjsonformat = {}\n        self.ents = []\n        self.publish_time = publish_time\n        \n    def erJsonFormat(self):\n        self.jsonformat[\"text\"]=self.texts\n        self.jsonformat[\"title\"]=self.corpus\n        self.jsonformat[\"publish_time\"]=self.publish_time\n        for _entity in self.entitys:\n            _entitylineitems=_entity.split(\"\\t\")\n            _entity_info = _entitylineitems[1].split(\" \")\n            _current_entity = {}\n            if _entity_info[0]==\"Title\" or _entity_info[0]==\"Paragrah\":\n                continue\n            else:\n                _current_entity[\"index\"] = _entitylineitems[0]\n                _current_entity[\"start\"] = int(_entity_info[1])\n                _current_entity[\"end\"] = int(_entity_info[2])\n                _current_entity[\"label\"] = _entity_info[0].upper()\n                self.ents.append(_current_entity)\n        self.jsonformat[\"ents\"]=self.ents\n        with open(os.path.join(self.format_outdir, self.corpus + \"_ents.json\"), 'w', encoding='utf-8') as jsonFile:\n            json.dump(self.jsonformat,jsonFile)\n\n    def erHtmlFormat(self):\n        renderex=[]\n        renderex.append(self.jsonformat)\n        html = spacy.displacy.render([self.jsonformat], style=\"ent\", manual=True, options=options, jupyter = False)\n        with open(os.path.join(self.format_outdir, self.corpus + \"_ents.html\"), 'w', encoding='utf-8') as htmlFile:\n            html_custom=html.replace('<span style=\"', '<span style=\"color:#666666;')\n            htmlFile.write(html_custom)\n\n    def erParseFormat(self):\n        with open(self.anno_file, encoding='utf-8')as f:\n            content = f.readlines()\n            p = re.compile('^(T\\d+\\t)*')  # match example: \"T1\tpathogen 38 51\tCoronaviruses\"\n            global_offset = 0\n            current_offset = 0\n            entity_index = 0\n            for _line in content:\n                _line=_line.strip()\n                if len(p.findall(_line)[0]) > 0: # is an entity line\n                    entity_index = entity_index + 1\n                    entityline_split = _line.split('\\t')\n                    Tnature_coordinate = entityline_split[1].split(' ')\n                    start = Tnature_coordinate[1]\n                    end = Tnature_coordinate[2]\n                    # adjust the coordinate and position offset\n                    entity = 'T' + str(entity_index) + '\\t' + str(Tnature_coordinate[0]) + ' ' + str(\n                        int(start) + global_offset) + ' ' + str(int(end) + global_offset) + '\\t' + entityline_split[2]\n                    self.entitys.append(entity)\n                else:  # is an char line\n                    if len(_line) > 0:  #is not empty char\n                        global_offset = global_offset + current_offset\n                        current_offset = 0\n                        self.texts = self.texts + _line+'. '\n                        current_offset = current_offset + len(_line) + 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# annotate an example paper corpus (take an paper which cord_uid is \"9lissxsy\" as an example)\n# make an \"9lissxsy_anno\" directory in the output dir(\"corpus-paper-anno\")\n# save as \"9lissxsy.entity\", \"9lissxsy_ents.json\", \"9lissxsy_ents.html\" under the \"9lissxsy_anno\" directory\n\n#example paper '9lissxsy'\n_corpus =\"9lissxsy\" # cord_uid\n_corpus_row = covid_papers.loc[covid_papers['cord_uid'] == _corpus]\n\n#init annotate\n_corpus_annotations_dir = os.path.join(annotations_tmp_dir, _corpus + \"_anno\")\n_corpus_text = ('. ').join(loadcorpus(_corpus_row['title'].values[0], _corpus_row['abstract'].values[0]))\nannotate = NewAnnotate(_corpus_annotations_dir, _corpus_text, _corpus, dict_list, dict_list_dir)\n\n#entity recognization\nannofile=annotate.entity_recognize()\n\n# entity format\nannoformat=AnnoFormat(_corpus, _corpus_annotations_dir, annofile, _corpus_row['publish_time'].values[0])\nannoformat.erParseFormat()\nannoformat.erJsonFormat()\nannoformat.erHtmlFormat() # entity html render\ndel annotate, annoformat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"'''\nThe raw entity recognitioin output in text file, show an example as follows:\n9lissxsy_anno/9lissxsy.entity\"\n'''\nimport os\nexample_file_path = os.path.join(annotations_tmp_dir, \"9lissxsy\"+\"_anno\")\nwith open(os.path.join(example_file_path,\"9lissxsy.entity\"), encoding = 'utf-8') as entity_example_file:\n    print(entity_example_file.read())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#%%capture\n'''\nThe JSON Format results, show an example as follows:\n9lissxsy_anno/9lissxsy_ents.json\"\n'''\nwith open(os.path.join(example_file_path,\"9lissxsy_ents.json\"), encoding = 'utf-8') as json_example_file:\n    json_ents = json.loads(json_example_file.read())\n    print(json.dumps(json_ents, indent=2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"'''\nThe rendered HTML Format results, show an example as follows:\n9lissxsy_anno/9lissxsy_ents.html\"\n'''\nfrom IPython.core.display import display, HTML\nwith open(os.path.join(example_file_path,\"9lissxsy_ents.html\"), encoding = 'utf-8') as html_example_file:\n    display(HTML(html_example_file.read()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"%%capture\n# Then annotate all covid19 related paper corpus and save each recognition result as *.entity, *_ents.json, *_ents.html\nfor row in covid_papers.itertuples():\n    _corpus = row.cord_uid\n    #init annotate\n    _corpus_annotations_dir = os.path.join(annotations_tmp_dir, _corpus + \"_anno\")\n    _corpus_text = ('. ').join(loadcorpus(row.title, row.abstract))\n    _corpus_publish_time = row.publish_time\n    annotate = NewAnnotate(_corpus_annotations_dir, _corpus_text, _corpus, dict_list, dict_list_dir)\n\n    #entity recognization\n    annofile=annotate.entity_recognize()\n\n    # entity format\n    annoformat=AnnoFormat(_corpus, _corpus_annotations_dir, annofile, _corpus_publish_time)\n    annoformat.erParseFormat()\n    annoformat.erJsonFormat()\n    annoformat.erHtmlFormat() # entity html render","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Merge results and Display\nmerge all COVID-19 papers' NER results in JSON and HTML format(num:3455) and extract 10% sample papers' NER results (num:346) for display (JSON and HTML). "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Output dir for merged results\nannotations_dir = './corpus-paper-annoformat'\nif not os.path.exists(annotations_dir):\n    os.makedirs(annotations_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define a function mergejsonents to merge all NER output results in json format for each paper\ndef mergejsonents(annotations_tmp_dir, jsonents_dir):\n    f = open(os.path.join(jsonents_dir,\"papers_anno.json\"),'w',encoding='utf-8')\n    f_sample = open(os.path.join(jsonents_dir,\"papers_anno_sample.json\"),'w',encoding='utf-8')\n    for parent, dirnames, filenames in os.walk(annotations_tmp_dir):\n        for filename in filenames:\n            if filename.endswith('_ents.json'):\n                file_path = os.path.join(parent, filename)\n                sf = open(file_path,encoding='utf-8')\n                ents_content = sf.read()\n                sf.close()\n                f.write(ents_content+\"\\n\")\n                cord_uid = filename.split('_ents.json')[0]\n                if cord_uid in covid_papers_sample['cord_uid'].values:\n                    f_sample.write(ents_content+\"\\n\")\n    f.close()\n    f_sample.close()\n\n# merge json results\nmergejsonents(annotations_tmp_dir, annotations_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#%%capture\n# For the sample papers, print NER results in pretty JSON style\njson_sample_file = open(os.path.join(annotations_dir,\"papers_anno_sample.json\"), encoding = 'utf-8')\nfor json_line in json_sample_file.readlines():\n    json_ents = json.loads(json_line)\n    #print(json.dumps(json_ents, indent=2))\n    print(json.dumps(json_ents))\njson_sample_file.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define a function mergehtmls to merge all NER output results in html format for each paper\ndef mergehtmls(annotations_tmp_dir, htmlrender_dir):\n    f= open(os.path.join(htmlrender_dir,\"papers_anno.html\"),'w',encoding='utf-8')\n    f_sample = open(os.path.join(htmlrender_dir,\"papers_anno_sample.html\"),'w',encoding='utf-8')\n    f.write('<?xml version=\"1.0\" encoding=\"utf-8\"?>')\n    f_sample.write('<?xml version=\"1.0\" encoding=\"utf-8\"?>')\n    for parent, dirnames, filenames in os.walk(annotations_tmp_dir):\n        for filename in filenames:\n            if filename.endswith('_ents.html'):\n                file_path = os.path.join(parent, filename)\n                sf = open(file_path,encoding='utf-8')\n                ents_html = sf.read()\n                f.write(ents_html)\n                f.write(\"<br /><br /><br />\")\n                cord_uid = filename.split('_ents.html')[0]\n                if cord_uid in covid_papers_sample['cord_uid'].values:\n                    f_sample.write(ents_html+\"\\n\")\nmergehtmls(annotations_tmp_dir, annotations_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#%%capture\n#For the sample papers, preview the NER results by IPython HTML Display \nfrom IPython.core.display import display, HTML\nwith open(os.path.join(annotations_dir,\"papers_anno_sample.html\"), encoding = 'utf-8') as html_sample_file:\n    display(HTML(html_sample_file.read()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. **(For Tasks) **Simple statistics and barplot of terms occurrences count based on NER results, output TOP occurrence terms from each dict type."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Init dict libraries for synonyms words combination\nsynonyms_dict = {}\nrepresentation_dict = {}\ndef synonyms_words_dict(dictlists, dictlist_dir):\n    wordlistDict = {}\n    for wordlist in dictlists.split(','):\n        wordlistpath = os.path.join(dictlist_dir, wordlist)\n        assert os.path.isfile(wordlistpath), 'Unable to access file: %s' % wordlistpath\n        entityType = os.path.splitext(os.path.basename(wordlistpath))[0]\n        wordlistDict[entityType] = wordlistpath\n\n        with open(wordlistpath, 'r', encoding='utf-8') as dict_file:\n            for _line in dict_file:\n                _line = _line.strip('\\n')\n                _words = _line.split('|')\n                representation_dict[_words[0].lower()] = _words[0]\n                for i in range(0, len(_words)):\n                    synonyms_dict[_words[i].lower()] = _words[0].lower()\n                    \n    #print(synonyms_dict['betacov/korea/kcdc03/2020'])\n\n#Terms Occurrence Count with synonyms combined\ndef statistics_group_by_synonyms_words(dict_list_dir):\n    dictLists = ''\n    entityTypes = []\n    data_stat_dict = {}\n\n    for root, subdirs, dict_files in os.walk(dict_list_dir):\n        dictLists = ','.join(dict_files)\n        #print(dict_files)\n        for dict_file in dict_files:\n            entityTypes.append(os.path.splitext(dict_file)[0].upper())\n\n    synonyms_words_dict(dictLists, dict_list_dir)\n\n    # statistics for terms occur\n    statistics = {}\n    f = open(os.path.join(annotations_dir, \"papers_anno.json\"), encoding='utf-8')\n\n    for json_str in f.readlines():\n        if json_str is not '':\n            myjson = json.loads(json_str)\n            text = myjson[\"text\"]\n            cord_uid = myjson[\"title\"]\n            ents = myjson[\"ents\"]\n            for ent in ents:\n                start_char = ent['start']\n                end_char = ent['end']\n                label_type = ent['label']\n                label_text = text[start_char:end_char]\n                if label_type in entityTypes:\n                    if label_type not in statistics:\n                        statistics[label_type] = {}\n\n                    _entity_name = synonyms_dict[label_text.lower()]\n                    if _entity_name not in statistics[label_type]:\n                        statistics[label_type][_entity_name] = 0\n                    statistics[label_type][_entity_name] += 1\n    f.close()\n\n    # sort by entity occurrence in each entity type\n    for label_type in statistics.keys():\n        a = statistics[label_type]\n        print(\"======{}: length is {}======\".format(label_type, len(a)))\n        a1 = sorted(a.items(), key=lambda x: x[1], reverse=True)\n        _dict = {}\n\n        for i in range(len(a1)):\n            _list = []\n            _list.append(representation_dict[a1[i][0]])\n            _list.append(a1[i][1])\n            _dict[_list[0]] = _list[1]\n\n            print(tuple(_list))\n            if i >= 9:  # print Top10 terms\n                break\n\n        data_stat_dict[label_type] = _dict\n\n    return data_stat_dict\nstatistics_combinesyns=statistics_group_by_synonyms_words(dict_list_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.pyplot import figure\nimport matplotlib.pyplot as plt\ndef plot_dict(topic, threshold, sort_values=False, barh=False, width=20, height=4, title=''):\n    filtered = dict(topic)\n    to_delete = []\n    for key in filtered:\n        if filtered[key] < threshold:\n            to_delete.append(key)\n    for key in to_delete:\n        del filtered[key]\n\n    if sort_values == False:\n        lists = sorted(filtered.items())\n    else:\n        if sort_values == True:\n            lists = sorted(filtered.items(), key=lambda item: item[1])\n        else:\n            lists = sorted(filtered.items(), key=sort_values)\n\n    x, y = zip(*lists)\n\n    fig = figure(num=None, figsize=(width, height))\n\n    if title != '':\n        fig.suptitle(title, fontsize=20)\n\n    if barh == True:\n        plt.barh(x, y)\n    else:\n        plt.bar(x, y)\n    plt.show()\n    # plt.savefig('{}.png'.format(title))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_topic = 'SYMPTOMS'\nplot_dict(statistics_combinesyns[_topic], threshold=50 ,sort_values=True,barh=True, width=20, title=_topic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_topic = 'ORGAN'\nplot_dict(statistics_combinesyns[_topic], threshold=5 ,sort_values=True,barh=True, width=20, title=_topic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_topic = 'DISEASE'\nplot_dict(statistics_combinesyns[_topic], threshold=50 ,sort_values=True,barh=True, width=20, title=_topic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_topic = 'PATHOGEN'\nplot_dict(statistics_combinesyns[_topic], threshold=20 ,sort_values=True,barh=True, width=20, title=_topic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_topic = 'RISK_FACTORS'\nplot_dict(statistics_combinesyns[_topic], threshold=20 ,sort_values=True,barh=True, width=20, title=_topic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_topic = 'TRANSMISSION'\nplot_dict(statistics_combinesyns[_topic], threshold=10 ,sort_values=True,barh=True, width=20, title=_topic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_topic = 'DRUG'\nplot_dict(statistics_combinesyns[_topic], threshold=5 ,sort_values=True,barh=True, width=20, title=_topic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_topic = 'TREATMENT'\nplot_dict(statistics_combinesyns[_topic], threshold=5 ,sort_values=True,barh=True, width=20, title=_topic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_topic = 'DIAGNOSIS'\nplot_dict(statistics_combinesyns[_topic], threshold=20 ,sort_values=True,barh=True, width=20, title=_topic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_topic = 'STRAIN'\nplot_dict(statistics_combinesyns[_topic], threshold=1 ,sort_values=True,barh=True, width=20, title=_topic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_topic = 'GENE'\nplot_dict(statistics_combinesyns[_topic], threshold=10 ,sort_values=True,barh=True, width=20, title=_topic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_topic = 'PROTEIN'\nplot_dict(statistics_combinesyns[_topic], threshold=10 ,sort_values=True,barh=True, width=20, title=_topic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_topic = 'HOST_OTHER'\nplot_dict(statistics_combinesyns[_topic], threshold=2 ,sort_values=True,barh=True, width=20, title=_topic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_topic = 'PREVENTION'\nplot_dict(statistics_combinesyns[_topic], threshold=5 ,sort_values=True, barh=True, width=20, title=_topic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_topic = 'LOCATION'\nplot_dict(statistics_combinesyns[_topic], threshold=2 ,sort_values=True, barh=True, width=20, title=_topic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_topic = 'STRUCTURE'\nplot_dict(statistics_combinesyns[_topic], threshold=2 ,sort_values=True, barh=True, width=20, title=_topic)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. **(For Tasks) **Find related papers by query biomedical topic and keyword. Render top paper results in HTML table, highlight the result terms, sorted by count of the irredudant terms. "},{"metadata":{},"cell_type":"markdown","source":"Simple entity occurrence statistics may have biased results.  Because in fact some terms' annotation may not related to the topic in context of paper. We need to filter papers' texts by keywords (keywords can be empty, that is, no filteration) at first. Then find dict terms annotations in each paper related to given dict type (which called topic)."},{"metadata":{"trusted":true},"cell_type":"code","source":"#define function filterbykeywords used to filter NER results.\n#We only retain entity recognition results of COVID-19 papers whose texts contain 'keyword'\n#return the filtered NER results in JSON format\ndef filterbykeywords(annojsonFile, keywords):\n    f = open(annojsonFile, encoding='utf-8')\n    filterjson = []\n    for json_str in f.readlines():\n        if json_str is not '':\n            if len(keywords)==0:\n                filterjson.append(json_str)\n            else:\n                myjson = json.loads(json_str)\n                text = myjson[\"text\"]\n                if any(_word in text for _word in keywords):\n                    filterjson.append(json_str)\n\n    f.close()\n    return filterjson\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Output paper rank result are rendered in HTML table, which is sorted by \"terms occurrences\"(descending) and \"publish_date\"(descending). In the table, the related annotations highlighted in 'red' font, the search keywords highlighted in 'blue bold' font."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Parse filtered NER json format, render HTML table dataframe\n#sort by columns of \"irredundant terms occurrence\"(first descending) and \"publish_time\"(second descending)\n#columns are ['cord_uid', 'related terms', 'irredundant terms occurrence', 'title+abstract', 'publish_time']\ndef correlated_articles(filterjson, topic):\n    correlatedResultDict = {}\n    \n    for json_str in filterjson:\n        if json_str is not '':\n            myjson = json.loads(json_str)\n            ents = myjson[\"ents\"]\n            cord_uid = myjson['title']\n            correlatedResultDict[cord_uid] = {}\n            correlatedResultDict[cord_uid][\"text\"] = myjson[\"text\"]\n            correlatedResultDict[cord_uid][\"publish_time\"] = myjson[\"publish_time\"]\n            correlatedResultDict[cord_uid][\"terms\"] = set()\n            correlatedResultDict[cord_uid][\"termsnum\"] = 0\n            correlatedResultDict[cord_uid][\"posList\"] = []\n            for ent in ents:\n                label_type = ent['label']\n                start_char = ent['start']\n                end_char = ent['end']\n                label_text = correlatedResultDict[cord_uid][\"text\"][start_char:end_char]\n                if topic.lower() == label_type.lower():\n                    correlatedResultDict[cord_uid][\"terms\"].add(representation_dict[synonyms_dict[label_text.lower()]])\n                    correlatedResultDict[cord_uid][\"termsnum\"]=len(correlatedResultDict[cord_uid][\"terms\"])\n                    correlatedResultDict[cord_uid][\"posList\"].append((start_char, end_char))\n\n    return correlatedResultDict\n\ndef getAnnotationHtml(text, posList, keywords):\n    _prePos = 0\n    annotationHtmlStr = ''\n    for _pos in posList:\n        annotationHtmlStr += text[_prePos:_pos[0]]\n        annotationHtmlStr += \" <font color='red'>\" + text[_pos[0]:_pos[1]] + \"</font> \"\n        _prePos = _pos[1]\n\n    annotationHtmlStr += text[_prePos:]\n    for _word in keywords:\n        if _word is not '':\n            annotationHtmlStr=annotationHtmlStr.replace(_word, \" <font color='blue' style='font-weight:bold'>\" + _word + \"</font> \")\n    return annotationHtmlStr\n\ndef get_answers(topic, keywords,top):\n    filterjson = filterbykeywords(os.path.join(annotations_dir, \"papers_anno.json\"), keywords)\n    print(\"All related papers number is: {}, we show top {} papers ordered by occurrences\".format(len(filterjson), top))\n    correlatedResultDict = correlated_articles(filterjson, topic)\n    temp_sort_result = sorted(correlatedResultDict.items(), key=lambda x: x[1][\"termsnum\"], reverse=True)\n    ranked_aswers = []\n    pandasData = []\n    for i in range(0,len(temp_sort_result)):\n        if i>top:\n            break\n        rowData = []\n        cord_uid = temp_sort_result[i][0]\n        #print(cord_uid)\n        annotationHtml = getAnnotationHtml(correlatedResultDict[cord_uid][\"text\"], correlatedResultDict[cord_uid][\"posList\"], keywords)\n        rowData += [cord_uid, ', '.join(correlatedResultDict[cord_uid][\"terms\"]), correlatedResultDict[cord_uid][\"termsnum\"], annotationHtml,  correlatedResultDict[cord_uid][\"publish_time\"]]\n        pandasData.append(rowData)\n    '''\n    for cord_uid in correlatedResultDict.keys():\n        rowData = []\n        annotationHtml = getAnnotationHtml(correlatedResultDict[cord_uid][\"text\"], correlatedResultDict[cord_uid][\"posList\"], keywords)\n        rowData += [cord_uid, ', '.join(correlatedResultDict[cord_uid][\"terms\"]), correlatedResultDict[cord_uid][\"termsnum\"], annotationHtml,  correlatedResultDict[cord_uid][\"publish_time\"]]\n        pandasData.append(rowData)\n    '''\n    #cord_uid, content, occurrence, related terms, publish_time\n    question_HTML = '<div style=\"font-family: Times New Roman; font-size: 28px; padding-bottom:28px\"><b>Topic</b>: ' + topic +'       <b>, Keywords</b>:'+(', ').join(keywords) + '</div>'\n    display(HTML(question_HTML))\n    df = pd.DataFrame(pandasData,\n                      columns=['cord_uid', 'related terms', 'irredundant terms occurrence', 'title+abstract', 'publish_time'])\n    df=df.sort_values([\"irredundant terms occurrence\", \"publish_time\"], ascending = (False, False))\n    df = df.reset_index(drop=True)\n    df_top10 = df.head(top)\n    del df\n    answers_HTML = df_top10.to_html(render_links=True, escape=False, justify ='left')#, formatters={'title+abstract': lambda x: '<b>' + x + '</b>'})\n    answers_HTML = answers_HTML.replace('<td>','<td style=\"text-align: center;\">')\n    #print(answers_HTML)\n    display(HTML(answers_HTML))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finding related papers and display of ranked paper result provides more confidence and intelligible results for CORD-19 challenge tasks."},{"metadata":{},"cell_type":"markdown","source":"\nFor example, a lot of papers may contain biomedical terms in the topic dict of \"Risk_Factors\", but risk factor is not the topic of paper, and the terms mentioned in the paper context are for other purpose. \n\nWe first query papers by keywords ('risk factor') and then count the occurrences of biomedical terms of the topic ('RISK_FACTORS') from dict. Render top papers' result in HTML table, highlight the topic terms and query keyword, sorted by irredudant terms' count. \n"},{"metadata":{},"cell_type":"markdown","source":"* Task1: What is known about transmission, incubation, and environmental stability?"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_answers(topic='TRANSMISSION', keywords=['transmission','incubation','environmental stability'], top=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Task2: What do we know about COVID-19 risk factors?"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_answers(topic='RISK_FACTORS', keywords=['risk factor'], top=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Task3: What do we know about virus genetics, origin, and evolution?"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_answers(topic='HOST_OTHER', keywords=['origin'], top=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_answers(topic='PATHOGEN', keywords=[], top=5)  #the keyword can be empty list [], if no keyword defined","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_answers(topic='PATHOGEN', keywords=['recombination'], top=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_answers(topic='PROTEIN', keywords=['mutation'], top=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Task5: What do we know about non-pharmaceutical interventions?"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_answers(topic='PREVENTION', keywords=['measure'], top=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Task6: What do we know about vaccines and therapeutics?"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_answers(topic='drug', keywords=['treatment'], top=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_answers(topic='treatment', keywords=['treatment'], top=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Task7: What do we know about diagnostics and surveillance?"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_answers(topic='DIAGNOSIS', keywords=['diagnosis'], top=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_answers(topic='SYMPTOMS', keywords=['symptom'], top=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Task8: What has been published about medical care?"},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%capture\nget_answers(topic='TREATMENT', keywords=['medical care'], top=5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_answers(topic='TREATMENT', keywords=['treatment'], top=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# 7. NER model training based on NER results, write NER model in directory"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load the customed train data from json output\n#loadjsonTrainData()\nfrom __future__ import unicode_literals, print_function\nimport random\nfrom pathlib import Path\nimport spacy\nfrom spacy.util import minibatch, compounding\n\nTRAIN_DATA = []\nTRAIN_LABELS = set()\n\ndef handleents(text, ents):\n    handleddata=[]\n    ss = text.split(\". \")\n    curr_text = \"\"\n    curr_len = 0\n    last_len = 0\n    curr_ents = []\n    for _s in ss:\n        if _s is not \"\":\n            curr_ents = []\n            curr_text = _s\n            curr_len = len(curr_text) + 2\n            for _ent in ents:\n                if _ent[\"end\"] <= (curr_len + last_len) and _ent[\"start\"] >= last_len:\n                    TRAIN_LABELS.add(_ent[\"label\"])\n                    curr_ents.append((_ent[\"start\"] - last_len, _ent[\"end\"] - last_len, _ent[\"label\"]))\n            curr_ents.sort()\n            last_len = last_len + curr_len\n            handleddata.append((curr_text, {\"entities\": curr_ents}))\n    return handleddata\n\ndef loadjsonTrainData(annotations_dir):\n    with open(os.path.join(annotations_dir, \"papers_anno_sample.json\"), encoding=\"utf-8\") as f:\n        for json_str in f.readlines():\n            if json_str is not '':\n                myjson = json.loads(json_str)\n                text = myjson[\"text\"]\n                cord_uid = myjson[\"title\"]\n                ents = myjson[\"ents\"]\n                handleddata = handleents(text, ents)\n                TRAIN_DATA.extend(handleddata)\n\nloadjsonTrainData(annotations_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrainNer(object):\n    def __init__(self, inmodel,outmodel,outmodel_dir,n_iter):\n        self.inmodel=inmodel\n        self.outmodel=outmodel\n        self.outmodel_dir = outmodel_dir\n        self.n_iter=n_iter\n\n    def trainNer(self, TRAIN_DATA, TRAIN_LABELS):\n        \n        #Set up the pipeline and entity recognizer, and train the new entity.\n        random.seed(0)\n        if self.inmodel is not None:\n            nlp = spacy.load(self.inmodel)  # load existing spaCy model\n            print(\"Loaded model '%s'\" % self.inmodel)\n        else:\n            nlp = spacy.blank(\"en\")  # create blank Language class\n            print(\"Created blank 'en' model\")\n\n        # Add entity recognizer to model if it's not in the pipeline\n        # nlp.create_pipe works for built-ins that are registered with spaCy\n        if \"ner\" not in nlp.pipe_names:\n            ner = nlp.create_pipe(\"ner\")\n            nlp.add_pipe(ner)\n        # otherwise, get it, so we can add labels to it\n        else:\n            ner = nlp.get_pipe(\"ner\")\n\n        # ner.add_label(LABEL)  # add new entity label to entity recognizer\n        for _label in TRAIN_LABELS:\n            if self.inmodel is not None:\n                ner.add_label(_label)\n            else:\n                ner.add_label(_label)\n\n        # _train_data may shuffle\n        for_traindata = TRAIN_DATA.copy()\n\n        if self.inmodel is None:\n            optimizer = nlp.begin_training()\n        else:\n            optimizer = nlp.resume_training()\n        move_names = list(ner.move_names)\n        print(move_names)\n        # get names of other pipes to disable them during training\n        pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n        other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n        with nlp.disable_pipes(*other_pipes):  # only train NER\n            sizes = compounding(1.0, 4.0, 1.001)\n            # batch up the examples using spaCy's minibatch\n            for itn in range(self.n_iter):\n                random.shuffle(for_traindata)\n                batches = minibatch(for_traindata, size=sizes)\n                losses = {}\n                for batch in batches:\n                    _texts, annotations = zip(*batch)\n                    nlp.update(_texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n                print(\"step {}, Losses {}\".format(itn, losses))\n\n        # save model to output directory\n        if self.outmodel_dir is not None:\n            output_dir = Path(self.outmodel_dir)\n            if not output_dir.exists():\n                output_dir.mkdir()\n            nlp.meta[\"name\"] = self.outmodel  # rename model\n            nlp.to_disk(output_dir)\n            print(\"Saved model to\", output_dir)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"trainner = TrainNer(None, \"en_train_web_sm\", \"nermodel\", 20)  # save the model name as \"en_train_web_sm\" and write to the ./nermodel dir\ntrainner.trainNer(TRAIN_DATA, TRAIN_LABELS) # Training based on the TRAIN_DATA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8. evaluate NER model prediction results"},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nfrom spacy.gold import GoldParse\nfrom spacy.scorer import Scorer\n\nclass EvaluateNer(object):\n    def __init__(self, nermodel_dir):\n        self.ner_model = spacy.load(nermodel_dir)\n\n    def evaluate(self, examples):\n        scorer = Scorer()\n        for input_, annot in examples:\n            doc_gold_text = self.ner_model.make_doc(input_)\n            gold = GoldParse(doc_gold_text, entities=annot[\"entities\"])\n            pred_value = self.ner_model(input_)\n            scorer.score(pred_value, gold)\n        return scorer.scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluatener = EvaluateNer(\"nermodel\") #nermodel\nscores = evaluatener.evaluate(TRAIN_DATA)\n#get the Precision, Recalling and F1 score\nprint(\"precision:{}\\nrecalling:{}\\nF1:{}\".format(scores['ents_p'],scores['ents_r'],scores['ents_f']))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}