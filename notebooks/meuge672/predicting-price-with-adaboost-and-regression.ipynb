{"cells":[{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"zwGsqkd7EKGg","trusted":true,"_uuid":"090bd6327da572be9b2726782b09136116fb9eb9"},"cell_type":"code","source":" ##THE LYBRARIES USED IN THIS NOTEBOOK\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n#To avoid FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom sklearn import metrics\nimport seaborn as sns\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.metrics import r2_score\n\n                               ","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"Zd6foEZaUpPh","_uuid":"96ea3607c5d964080a9e21129d464da777b06a13"},"cell_type":"markdown","source":"## We load the file into pandas to see which features the dataset has\n\nThis dataset contains house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015.. I start with this dataset as I followed the ones in the course from Washington University in Coursera. If you want to take a look to the contents from the course [Machine Learning Foundations](https://www.coursera.org/learn/ml-foundations)\nHere you can find a brief version of the features in the dataset:\n\n\n*   Price\n*   Bedrooms\n*   Bathrooms\n*   Sqft living\n*   Sqft_lot\n*   Floors\n\nand so on \n\n\n\n"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":612},"colab_type":"code","executionInfo":{"elapsed":3085,"status":"ok","timestamp":1530562359821,"user":{"displayName":"Mar√≠a Eugenia Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112432663657646180649"},"user_tz":180},"id":"g0WNJk0MUv6S","outputId":"037b2b8d-9cd1-4d8c-e150-51e09b81e6dc","trusted":true,"_uuid":"39b06b5db5d43faf15426e29f87bda63972a30d6"},"cell_type":"code","source":"dataset_link = '../input/kc_house_data.csv'\nhouses_df = pd.read_csv(dataset_link)\n\nminimum_y = houses_df['price'].min()\n# s refers to size\n#alpha --> 0.0 transparent through 1.0 opaque\nplt.figure(figsize=(10,10))\nplt.scatter(x = houses_df.sqft_living,y = houses_df.price/minimum_y,s = 1, alpha = 1)\nplt.xlabel(\"SQFT_LIVING\")\nplt.ylabel(\"PRICES\")\nplt.title(\"SEATTLE HOME PRICES\")\nplt.yscale('log')\n\n#Lets see what we have in the dataset\nhouses_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"hqB_bCssQqIZ","_uuid":"d72152ddfbe25cb3d7a2a9e8161cff6a9b3b4a7c"},"cell_type":"markdown","source":"### We use a log scale as it allows a large range of elements to be displayed without small values being compressed down into bottom of the graph. If you want to see how you appreciate the change between a normal and log scale, check out this question in [Stackoverflow](https://stats.stackexchange.com/questions/27951/when-are-log-scales-appropriate)"},{"metadata":{"colab_type":"text","id":"2N_EOTlfuy1G","_uuid":"0416fd54b6125580d7b2b24a4ab1ea82957d8043"},"cell_type":"markdown","source":"## Explore the data"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":337},"colab_type":"code","executionInfo":{"elapsed":1058,"status":"ok","timestamp":1530562360997,"user":{"displayName":"Mar√≠a Eugenia Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112432663657646180649"},"user_tz":180},"id":"esJUC9Obu11c","outputId":"c9f1930a-1d0b-4b54-f869-b461d7cb128f","trusted":true,"_uuid":"4d937bd691c4bc21680e9d0a749ca7c08752d120"},"cell_type":"code","source":"pd.set_option('display.float_format', lambda x: '%.3f' % x) #supress scientific notation\nhouses_df.describe().iloc[:,1:].drop(['yr_built','yr_renovated','zipcode'],axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"Fddi1sLsuIkf","_uuid":"3c2a8affe3deb3be0233030b299f6117916471ea"},"cell_type":"markdown","source":"The average sale price of a house in our dataset is close to $\\$$540,088  with  most of the values falling within $\\$$321,950 to $\\$$645,000 range.\n"},{"metadata":{"colab_type":"text","id":"jdn8WM1q5Lgd","_uuid":"395386d510e6e86bd784f2308da519ee8af19047"},"cell_type":"markdown","source":"## Pearson Correlation\n\nTo see how each variable is correlated to the other we are gonna use the Pearson Correlation Coeficient. This is a measure of the linear correlation between two variables X and Y. It has a value between +1 and ‚àí1, where 1 is total positive linear correlation, 0 is no linear correlation, and ‚àí1 is total negative linear correlation. \n![texto alternativo](https://www.examspm.com/wp-content/uploads/2016/07/Screen-Shot-2016-07-11-at-11.32.46-AM.png)\n\nFor any more information refer to [Pearson Correlation Coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)\n\n"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":479},"colab_type":"code","executionInfo":{"elapsed":1122,"status":"ok","timestamp":1530562362659,"user":{"displayName":"Mar√≠a Eugenia Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112432663657646180649"},"user_tz":180},"id":"FLt2N6la19zK","outputId":"66fef352-a947-473d-d7c6-edda26e2e862","trusted":true,"_uuid":"e14026a80f46053c1c801b2cf674377ff42e45b7"},"cell_type":"code","source":"correlation = houses_df.iloc[:,2:].corr(method='pearson')\ncorrelation.style.format(\"{:.2}\").background_gradient(cmap=plt.get_cmap('coolwarm'), axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":340},"colab_type":"code","executionInfo":{"elapsed":1047,"status":"ok","timestamp":1530562364229,"user":{"displayName":"Mar√≠a Eugenia Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112432663657646180649"},"user_tz":180},"id":"ZXhwnoVR7Uyo","outputId":"2fddaec1-3d27-4eba-a4fa-bc29b39c56be","trusted":true,"_uuid":"bd504b5b85be59823cdd5ad30016f239d2dbbc05"},"cell_type":"code","source":"correlation.price.sort_values(ascending=False)[1:]\n#we drop the first value as it is with itself.","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"xTI6BG1v7Z_d","_uuid":"95b09e4513a726f3fb5ff1e007edd0d4f0900074"},"cell_type":"markdown","source":"All the features are positively correlated with the House Price, except zipcode. The correlation of Price with Sqft_Living is the greatest, 0.702.\nA negative correlation between two variables means that one variable increases whenever the other decreases. We can see the biggest  minimum of every column right below. "},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":638},"colab_type":"code","executionInfo":{"elapsed":1049,"status":"ok","timestamp":1530562365739,"user":{"displayName":"Mar√≠a Eugenia Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112432663657646180649"},"user_tz":180},"id":"h4eCdB17TPvX","outputId":"9321fdc8-4bee-4fd2-8fb0-8e6cf0e420a7","trusted":true,"_uuid":"ea4653f4ba5c326be6bd473b211713285fa5fb98"},"cell_type":"code","source":"correlated_variables = correlation.idxmin()\ncorrelation_values = correlation.min().values\n\ncorrelation_dict = {'First Variable':correlated_variables.index, 'Second Variable':correlated_variables.values, 'Values':correlation_values}\npd.DataFrame(correlation_dict)\n","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"5Iu2_df-thnF","_uuid":"61f99c3eab267a8ad94a749839d278f77382bb5f"},"cell_type":"markdown","source":"## Data Visualization\n\n\nNow we are gonna pick the most interesting columns in this case, price,sqft_living,sqft_lot bedrooms, bathrooms and yr_built,to see how each other is correlated to one another. \n"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":1200},"colab_type":"code","executionInfo":{"elapsed":13606,"status":"ok","timestamp":1530562379904,"user":{"displayName":"Mar√≠a Eugenia Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112432663657646180649"},"user_tz":180},"id":"fFycKkD70Kbg","outputId":"628bae80-5110-41e1-e94d-51e4ac161da8","trusted":true,"_uuid":"4a248b84aecf31955e68bb131f95ee38faa9d71e","scrolled":true},"cell_type":"code","source":"sns.set(style = \"ticks\", color_codes=True)\ncorrelation_features = ['price','bedrooms','bathrooms','sqft_living','sqft_lot','yr_built']\nsns.set_style(\"darkgrid\")\nsns.pairplot(houses_df[correlation_features],diag_kind=\"kde\",dropna=True)\n#diag_kind:Use kernel density estimates for univariate plots:\n#kind:Fit linear regression models to the scatter plots\nplt.figure(figsize=(25,20))\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"PxWTLfpc2GpE","_uuid":"ea98db58933507886f55002b8ddc8eb70d5e00b7"},"cell_type":"markdown","source":"By looking to the scatter plots you can observe the next :\n\n$$Price \\rightarrow  Strong\\space Correlation = Sqft \\space living , \\space Very\\space Weak \\space Correlation= Year \\space built$$\n$$Bedrooms \\rightarrow  Moderate\\space Correlation = Sqft \\space living , \\space Very\\space Weak \\space Correlation= Year \\space built$$\n$$Bathrooms \\rightarrow  Strong\\space Correlation = Sqft \\space living , \\space Very\\space Weak \\space Correlation = Sqft \\space Lot$$\n$$Sqft \\space Living \\rightarrow  Strong\\space Correlation = Bathroom , \\space Very\\space Weak \\space Correlation = Sqft \\space Lot$$\n$$Sqft \\space Lot \\rightarrow  Very\\space Weak \\space Correlation = Sqft \\space living , \\space Very \\space Weak \\space Correlation  = Year \\space built$$\n$$Year \\space Built \\rightarrow  Moderate\\space Correlation = Bathrooms  , \\space Very\\space  Weak \\space Correlation = Sqft \\space Lot $$\n\nHere you can find the [intervals to classify the correlation ](http://www.statstutor.ac.uk/resources/uploaded/pearsons.pdf). \nI think the most difficult plot to analyze is the one from Year Built because they almost all look the same. Thats when it is useful if we calculate the Pearson coefficient along the plots."},{"metadata":{"colab_type":"text","id":"Id3P-4nc7tEE","_uuid":"af4424c1721ac81cd99227f50be2f0f2fbf6b288"},"cell_type":"markdown","source":""},{"metadata":{"colab_type":"text","id":"2FPclJnmUFOa","_uuid":"23b1eddbd4ae32674f13a577a71d15a42e1059d6"},"cell_type":"markdown","source":"### Lets divide the dataset into training and test"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"QsGC0U3cj1k5","trusted":true,"_uuid":"4863a02a4ce26908e4ff48bb4e24b8206f75c9bf"},"cell_type":"code","source":"dataset_train, dataset_test, price_train, price_test = train_test_split(houses_df,houses_df['price'],test_size=0.2,random_state=3)\n","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"_BaLl--DsxXx","_uuid":"7f9c49e2d9d0849e661488bceb6c90c9772eaadf"},"cell_type":"markdown","source":"## Building a Linear Regressor\n\nRegression is the process of estimating the relationship between input data and the continuous-valued output data. This data is usually in the form of real numbers, and our goal is to estimate the underlying function that governs the mapping from the input to the output.\n\n## Ordinary Least Squares\nThe first method we use is ordinary least squares and the idea behind is to find the best line that fits the data. \n<img src=\"https://bookdown.org/sbikienga/Intro_to_stat_book/images/chap7/ols.jpg\" width=\"525\" height=\"500\">\n\n \n\nThe error function or also called the loss function ( $\\epsilon_i$) is the difference between the observed values of y  (y <sub>i </sub>) and the predicted values of y (≈∑<sub>i </sub>). This term is called Residual sum of squares for more information [RSS](https://en.wikipedia.org/wiki/Residual_sum_of_squares)\n\nThe linear model is written as:\n\n$$y_i = a + bx_i +   \\epsilon_i $$\n\nThe ordinary least squares (OLS) seeks the coeeficient 'a' and 'b'. The goal is to find values of ùëé and ùëè that minimize the error. We redefine the error by the next formula\n\n$$\\epsilon(a,b)=\\sum_{i=1}^n (y_i‚àí≈∑ )^2= \\sum_{i=1}^n(y_i‚àí(a+bx_i))^2$$\n\nThis requires us to find the values of (ùëé, ùëè) such that the gradient of $\\epsilon$ with respect\nto our variables (which are ùëé and ùëè) vanishes; thus we require\n\n$$\\frac {‚àÇ\\epsilon}{‚àÇa}=0 $$     \n$$\\frac {‚àÇ\\epsilon}{‚àÇb}=0$$\n\nDifferentiating $\\epsilon(ùëé, ùëè)$ yields:\n\n$$\\frac {‚àÇ\\epsilon}{‚àÇa}= 2\\sum_{i=1}^n (y_i-a-bx_i)(-1)$$ \n$$\\frac {‚àÇ\\epsilon}{‚àÇb}= 2\\sum_{i=1}^n (y_i-a-bx_i)(-x_i)$$ \n\n\nTo solve this equations remember to use:\n$$ \\bar{X} =\\sum_{i=1}^n \\frac{1}{n} x_i $$\n\n\nSo we will end up with the following coefficients:\n\n$$a=\\bar{y}‚àíb\\bar{x}$$\n\n$$b=\\frac {\\sum_{i=1}^n (x_i‚àí\\bar{x})(y_i‚àí\\bar{y})}{\\sum_{i=1}^n(x_i‚àí\\bar{x})^2} $$\n\n**RMSE**  --> **Root Mean Square Error**\n\nIt indicates how close the observed data points are to the model's predicted values. Lower values of RMSE indicate better fit. RMSE is a good measure of how accurately the model predicts the response, and is the most important criterion for fit if the main purpose of the model is prediction.\n\n\n\n**The main advantages of using Least Squares are:**\n\n*   Applicability: There are hardly any applications where least squares doesn‚Äôt work\n*   Calculations are very fast\n*   Has no paramaters to tune\n*   Elemento de lista\n\n\n\n\n**Disadvantages:**\n - Sensitivity to outliers\n - Tendency to overfit data. If we have many features the learned hyphotesis may fit the training set very well but fail to generalize to new examples\n\n\n \n\n"},{"metadata":{"colab_type":"text","id":"aCDONmOpwX3-","_uuid":"c5e9474bdee9ac242003af47617888afcd740cfa"},"cell_type":"markdown","source":"## Let's train the model only taking into account one feature from the dataset, in this case we pick up Square feet living"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":480},"colab_type":"code","executionInfo":{"elapsed":1171,"status":"ok","timestamp":1530562382796,"user":{"displayName":"Mar√≠a Eugenia Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112432663657646180649"},"user_tz":180},"id":"xGpzV4BilKP-","outputId":"98f3f7de-5b1d-4b3e-e643-b0cdf3085f94","trusted":true,"_uuid":"c3c666e17dd484241e0514bf1de03d9a14a5baa2"},"cell_type":"code","source":"#Build the regression model using only sqft_living as a feature\n# Create linear regression object\nregression_ols = linear_model.LinearRegression()\n#We convert the column sqft_living to a numpy array to make it easier to work\nliving_train = np.asarray(dataset_train.sqft_living)\nliving_train = living_train.reshape(-1,1)\n#Train the model using the training sets\n#Here price is the \"target\" data in this model, the other features are the independet variables\nols_model = regression_ols.fit(living_train, price_train)\nliving_test = np.asarray(dataset_test.sqft_living)\nliving_test = living_test.reshape(-1,1)\n#We the trained dataset we make a prediction for the test dataset\nprediction_test_ols = ols_model.predict(living_test)\n\nprint ('Ordinary Least Squares')\n#Coefficient\nprint('Coefficient:',ols_model.coef_[0])\nprint ('Intercept', ols_model.intercept_)\n# Apply the model we created using the training data to the test data, and calculate the RSS.\nprint('RSS',((price_test - prediction_test_ols) **2).sum())\n# Calculate the RMSE ( Root Mean Squared Error)\nprint('RMSE', np.sqrt(metrics.mean_squared_error(price_test,prediction_test_ols)))\n#The model's performance on test set is:\nprint('The model\\'s performance is %.2f\\n'% ols_model.score(living_test, price_test))\n\n\n\nliving_test_sort = np.sort(living_test.reshape(-1))\nplt.scatter(living_test, price_test, color='blue', alpha=0.25,label='Real Price')\n#When you plot you have to sort the array, in this case square feet living , the one that belongs to the test, if you dont do this, the plot looks weird\nplt.plot(living_test_sort, ols_model.predict(living_test_sort.reshape(-1,1)),'r--',linewidth=3, label='Ordinary Least squares Regression')\n\n\nplt.xlabel('Price')\nplt.ylabel('Square_feet_living')\nplt.legend()\nplt.yscale('log')\nplt.figure(figsize=(15,10))\n\n\n\n\n\n#Blue dots are from the original data the red line is the prediction from the least squares \n\n\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":204},"colab_type":"code","executionInfo":{"elapsed":1121,"status":"ok","timestamp":1530562384651,"user":{"displayName":"Mar√≠a Eugenia Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112432663657646180649"},"user_tz":180},"id":"zZ_mJLamW9nv","outputId":"c29b1db7-dd1b-4103-fe34-87341260293c","trusted":true,"_uuid":"2c827655ebfad45afaf1ca05316c71e138a321f3"},"cell_type":"code","source":"actual_predicted_data_ols = pd.DataFrame({'Actual': price_test, 'Predicted': np.round(prediction_test_ols,decimals=3)})\nactual_predicted_data_ols.head()\n","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"0cnZPbxP81Td","_uuid":"0e4cb6e8fddf7131656e5bf503b52feec6672750"},"cell_type":"markdown","source":"## Lasso Regression\n\n\nIt's a shrinkage and variable selection method. LASSO is an acronym form Least Absolute selection and Shrinkage Operator. The Lasso imposes a constraint on the sum of the absolute values of the model parameters where the sum has a specified constant as an upper bound. This constraint causes regression coefficients for some variables to shrink towards zero. The shrinkage process identifies the variables most strongly associated with the response variable. The goal is to obtain the subset of predictors that minimized the prediction error. You should use this method when you hove more than two feautures at least.\n\n$$ Y= \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3 + b $$\n\n$ \\beta_1 , \\beta_2 , \\beta_3 $ are coefficients of regression  \n\n\n$ X_1, X_2, X_3$ are features \n\nThe Lasso method uses $L_1$ regularization. What is that? It's a way of avoiding overfitting. \n\n$$\\|X\\|_1 = \\sum_{i=1}^n|x_i|  $$\n\n$L_1$ norm is the sum of the absolute value of the coefficients.\n\nThe cost function in Lasso is the next formula:\n\n\n$$ \\epsilon = Error + Penalty$$\n\n$$ \\epsilon(a,b)=\\sum_{i=1}^n (y_i‚àí≈∑ )^2 + \\lambda\\sum_{j=1}^p |\\beta_j| $$\n\n\n $$ \\epsilon(a,b)=\\sum_{i=1}^n(y_i‚àí(\\sum_{j=1}^p x_{ij}\\beta_j))^2 +\\lambda\\sum_{j=1}^p|\\beta_j| $$\n \n###Tuning paramater $\\lambda $:\nIt is to control the strenght of the penalty. \n\n*   $\\lambda$ increases more coefficients are reduced to zero\n*   $\\lambda$ is zero then it's OLS Regression.\n*   $\\lambda \\rightarrow \\infty$ : we get $\\beta=0$ all coefficients are eliminated\n*   $\\lambda$ increases, bias increases.\n*  $\\lambda$ decreases, variance increases\n\n\nThe **bias** is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).The **variance** is an error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting). For more information learn about the  [Bias and Variance Tradeoff](https://en.wikipedia.org/wiki/Bias‚Äìvariance_tradeoff)\n\n\n\n\n**Advantages**\n- Greater prediction accuracy \n- Increase model interpretability. Reduce variance without a substantial increase in bias.\n- The regression coefficients for unimportant variables are reduced to zero and produces a simple model that selects only the most important predictors.\n\n\n**Disadvantages**\n \n* If coefficientes are correlated , Lasso arbitrarily chooses only  one of them.\n* Estimating p-values is not very straightforward\n\n"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":480},"colab_type":"code","executionInfo":{"elapsed":1914,"status":"ok","timestamp":1530562387120,"user":{"displayName":"Mar√≠a Eugenia Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112432663657646180649"},"user_tz":180},"id":"1cwysT9584ab","outputId":"ec6a2906-f5de-4386-9e54-4f843185477b","trusted":true,"_uuid":"396d0d8641f29d6d0031048bb17d2b3a45df3b2a"},"cell_type":"code","source":"regression_lasso = linear_model.Lasso(alpha=.1)\nlasso_model = regression_lasso.fit(living_train, price_train)\nprediction_test_lasso = lasso_model.predict(living_test)\n\nprint ('Lasso Regression')\n#Intercept\nprint ('Intercept', lasso_model.intercept_)\n# Coefficient\nprint('Coefficient:', lasso_model.coef_[0])\n# Apply the model we created using the training data to the test data, and calculate the RSS.\nprint('RSS',((price_test - prediction_test_lasso) **2).sum())\n# Calculate the RMSE (Root Mean Squared Error)\nprint('RMSE', np.sqrt(metrics.mean_squared_error(price_test,prediction_test_lasso)))\n# Coefficient of determination R^2 of the prediction\nprint('The model\\'s performance is %.2f\\n' % lasso_model.score(living_test, price_test))\n# Plot \nplt.scatter(living_test, price_test, color='green', alpha=0.25,label='Real Price')\nplt.plot(living_test_sort, lasso_model.predict(living_test_sort.reshape(-1,1)),'b--',linewidth=3, label='Lasso Regression')\nplt.xlabel('Price')\nplt.ylabel('square_feet_living')\nplt.legend()\nplt.yscale('log')\nplt.figure(figsize=(15,10))\n\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":204},"colab_type":"code","executionInfo":{"elapsed":1057,"status":"ok","timestamp":1530562388811,"user":{"displayName":"Mar√≠a Eugenia Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112432663657646180649"},"user_tz":180},"id":"ApQZQ78WqcKB","outputId":"206f8dbb-0f8f-400d-fa2f-dc5b05cc760e","trusted":true,"_uuid":"683907d64c9a103a887bd8137a8eb78dace6b0a0"},"cell_type":"code","source":"actual_predicted_data_lasso = pd.DataFrame({'Actual': price_test, 'Predicted': np.round(prediction_test_lasso,decimals=3)})\n\nactual_predicted_data_lasso.head()","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"6Ga2Il5NEybY","_uuid":"a426e519b71c48c17f44639de3d7606c8b704c82"},"cell_type":"markdown","source":"## Ridge Regression\n\nAims to avoid overfitting adding a cost to the RSS term of OLS. A tuning parameter $\\lambda$ controls the strength of the penalty.The $\\lambda$ parameter is a scalar that should be learned  using cross validation. The penalty uses the $L_2$ (euclidean lenght) of the coefficient vector \n\nThe Rdidge method uses $L_2$ regularization. What is that? It's a way of avoiding overfitting. \n\n$$\\|X\\|_2 =\\sum_{i=1}^n|x_i|^2$$\n\n$L_2$ norm is the sum of the squared value of the coefficients.\n\nThe cost function in Ridge is the next formula:\n\n\n$$ \\epsilon = Error + Penalty$$\n\n$$ \\epsilon(a,b)=\\sum_{i=1}^n (y_i‚àí≈∑ )^2 + \\lambda\\sum_{j=1}^p |\\beta_j|^2 $$\n\n\n $$ \\epsilon(a,b)=\\sum_{i=1}^n(y_i‚àí( \\sum_{j=1}^p x_{ij}\\beta_j))^2 +\\lambda\\sum_{j=1}^p|\\beta_j| ^2$$\n\n**Tuning parameter $\\lambda$**\n  - When Œª = 0, we get the linear regression estimate\n  - When $\\lambda\\rightarrow \\infty$,we get $ \\beta_{j} =0$\n  - For $\\lambda$  in between, we are balancing two ideas: fitting a linear model of y on X, and shrinking the coefficients\n\nAs Lasso regression $ \\rightarrow$ The bias increases as  $\\lambda$ (amount of shrinkage) increases. And the variance decreases as $\\lambda$ increases\nThe amount of shrinkage is controlled by $\\lambda$, the tuning parameter that multiplies the ridge penalty. Large Œª means more shrinkage, and so we get different coefficient estimates for different values of Œª. Choosing an appropriate value of Œª is important, and also difficult.\n\n\n**Advantages**\n   - Ridge regression performs particularly well when there is a subset of true coefficients that are small or even zero. \n   - Sparsity (Doesn't produce sparse results i.e. it does not shrink coefficients all the way to zero)\n   \n   \n ** Disadvantages** \n \n   - It doesn‚Äôt do as well when all of the true coefficients are moderately large\n"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":480},"colab_type":"code","executionInfo":{"elapsed":1186,"status":"ok","timestamp":1530562390520,"user":{"displayName":"Mar√≠a Eugenia Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112432663657646180649"},"user_tz":180},"id":"TViQeuzlE3Ac","outputId":"061678d2-0a91-4d47-d712-06cd3304350a","trusted":true,"_uuid":"7d8320e8aab4614a2df7234d3def6ed5c4601d53"},"cell_type":"code","source":"regression_ridge = linear_model.Ridge(alpha=[.1])\nridge_model = regression_ridge.fit(living_train, price_train)\nprediction_test_ridge = ridge_model.predict(living_test)\n\nprint ('Ridge Regression')\n#Intercept\nprint ('Intercept', ridge_model.intercept_)\n# Coeficient\nprint('Coefficient:', ridge_model.coef_[0])\n# Apply the model we created using the training data to the test data, and calculate the RSS.\nprint('RSS',((price_test - prediction_test_ridge) **2).sum())\n# Calculate the RMSE (Root Mean Squared Error)\nprint('RMSE', np.sqrt(metrics.mean_squared_error(price_test,prediction_test_ridge)))\n# Coefficient of determination R^2 of the prediction\nprint('The model\\'s performance is %.2f\\n' % ridge_model.score(living_test, price_test))\n# Plot \nplt.scatter(living_test, price_test, color='brown', alpha=0.25,label='Real Price')\nplt.plot(living_test_sort, ridge_model.predict(living_test_sort.reshape(-1,1)),'g--',linewidth=3, label='Ridge Regression')\nplt.xlabel('Price')\nplt.ylabel('square_feet_living')\nplt.legend()\nplt.yscale('log')\nplt.figure(figsize=(15,10))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":204},"colab_type":"code","executionInfo":{"elapsed":1051,"status":"ok","timestamp":1530562392165,"user":{"displayName":"Mar√≠a Eugenia Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112432663657646180649"},"user_tz":180},"id":"jCzBmAdMrQPq","outputId":"2c5bd1e5-4782-4099-b507-e4b8af0858d2","trusted":true,"_uuid":"178f451b6d591f10dbb2cedb83c4ea58b1933caf"},"cell_type":"code","source":"actual_predicted_data_ridge = pd.DataFrame({'Actual': price_test, 'Predicted': np.round(prediction_test_ridge,decimals=3)})\nactual_predicted_data_ridge.head()","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"4JfmB3SLMBuZ","_uuid":"18b560f1b9390154330b1164bcd31088689470b1"},"cell_type":"markdown","source":"## AdaBoost Algorithm\n\nAdaBoost stands for Adaptive Boosting. When we mention boosting we refer to  aggregate a set of weak classifiers into a strong classifier.  It is adaptive in the sense that classifiers that come in next for execution are adjusted according to those instances that were wrongly classified width the previous classifiers. You could say that by just focusing on the training data samples misclassified by the previous weak classifier, each weak classifier contributes its bit the best it can  to improve the overall classification rate.  AdaBoost calls the weak classifiers repeatedly, performing a series of  $t = 1,...,T$  classifiers. In each execution, \"weight\" calculated by incorrectly classified examples increases (or, alternatively, weights of each correctly classified examples decreases). New classifiers are constrained to focus on those examples that were incorrectly classified by previous classifiers. \n\n### Disadvantages\n\n- It is sensitive to noisy data and information that doesn't belong to the required set\n\n\n### Advantagess\n\n- in some situations, this algorithm may be less susceptible to memory input set in comparison to many other algorithms\n\n### Basic Idea \n\n1- Take lots of (possibly) weak predictors\n\n2- Weight them and add them up\n\n3- Get a stronger predictor\n\n\n\n**First** : Initialize the weight of each observation to $W_i =\\frac { 1}{N} .$ For $ t $ in 1 to T do the following.\n\n1- Using the weights, learn model $h_t(x_i) : X \\rightarrow [0,1] $\n\n2- Compute $ \\epsilon =\\sum_{i=1}^{n} w_i^t | y_i ‚àíh_t (x_i )|  $ as the error for $h_t$\n\n3- Let $ \\beta_{t} = \\frac{\\epsilon_{t}}{1 - \\epsilon_{t}} $ and update the weights  of each of the observations  as $ w_i ^{(t+1)} = w_i^{(t)}\\beta_{t}^{1-|y_i -h_t(x_i)|}  $ This scheme increases the weights of observations poorly  predicted by $h_t$ \n\n4- Normalize $ w^{t+1} $ so that they sum to one\n"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":429},"colab_type":"code","executionInfo":{"elapsed":19946,"status":"ok","timestamp":1530562412623,"user":{"displayName":"Mar√≠a Eugenia Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112432663657646180649"},"user_tz":180},"id":"FwRQywYvMFqR","outputId":"dd04e7c9-37d0-402a-d446-f962cfd5b33c","trusted":true,"_uuid":"4f05b286fa52f5ae6396d4be6d30ffd392f4c314"},"cell_type":"code","source":"#n_estimators: It controls the number of weak learners.\n#learning_rate:Controls the contribution of weak learners in the final combination. There is a trade-off between learning_rate and n_estimators.\n#base_estimators: It helps to specify different ML algorithm. By default sklearn uses decision tree\nadaboost_regressor = AdaBoostRegressor(n_estimators=1500, learning_rate = 0.001, loss='exponential')\nada_model = adaboost_regressor.fit(living_train, price_train)\nprediction_test_ada = ada_model.predict(living_test)\n# Apply the model we created using the training data to the test data, and calculate the RSS.\nprint('RSS',((price_test - prediction_test_ada) **2).sum())\n# Calculate the RMSE (Root Mean Squared Error)\nprint('RMSE', np.sqrt(metrics.mean_squared_error(price_test,prediction_test_ada)))\n#Coefficient of determination R^2 of the prediction\nprint('The model\\'s performance is %.2f\\n' % ada_model.score(living_test, price_test))\n# Plot \nplt.scatter(living_test, price_test, color='black', alpha=0.25,label='Real Price')\nplt.plot(living_test_sort, ada_model.predict(living_test_sort.reshape(-1,1)),'g--',linewidth=3, label='AdaBoost regressor')\nplt.xlabel('Price')\nplt.ylabel('square_feet_living')\nplt.legend()\nplt.yscale('log')\nplt.figure(figsize=(15,10))\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":204},"colab_type":"code","executionInfo":{"elapsed":1114,"status":"ok","timestamp":1530562413796,"user":{"displayName":"Mar√≠a Eugenia Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112432663657646180649"},"user_tz":180},"id":"8Zb2Mi3TIqeL","outputId":"47a56cc7-5a0b-41cb-f797-6c1f33a76c0b","trusted":false,"_uuid":"25bedc70fa9e41c085251814e0e603380828a690"},"cell_type":"code","source":"\nactual_predicted_data_ada = pd.DataFrame({'Actual': price_test, 'Predicted': np.round(prediction_test_ada,decimals=3)})\nactual_predicted_data_ada.head()\n","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"g5k5MLwZxNJt","_uuid":"af6999936193ddd90faa67727ea7f673a47c6441"},"cell_type":"markdown","source":"## Let's train the model adding more features\n\nIf we have too many features and we are not sure which ones might work the best, you can carry out a feature selection step through either PCA (Principal Components Analysis) or LDA (Linear Discriminant Analysis)\n\n\n\n\n## Polynomial Curve Fitting\n\nConsider the general form for a polynomial of order  N\n\n$$ \\hat y(x) = a_0 + a_1x^1 + a_2x^2 + ...... a_nx^n =  a_0 +\\sum_{i=1}^n  a_ix^i $$\n\nJust as was the case for linear regression, we ask, how can we pick the coefficients that best fits the curve to the data? We use the same idea:\nThe curve that gives minimum error between data  and the fit $\\hat y (x )$ is ‚Äòthe best‚Äô\n\n\n##¬∑ Error - Least squares approach\n\nAs we mentioned before the  error using the least squares approach is:\n$$ \\epsilon(x)= \\sum_{i=1}^n (y_i - \\hat y)^2 $$\n\n$$ \\epsilon(x)= \\sum_{i=1}^n (y_i - (a_0 + a_1x_i^1 +a_2x_i^2 + a_3x_i^3... ))^2 $$\n\nwhere 'i' is the current point and 'n' is the total number of points that we have\n\n$$ \\epsilon(x)=\\sum_{i=1}^n (y_i - (a_0 +\\sum_{j=1}^k  a_jx^j ))^2 $$\n\nTo  minimize this equation we proceed to take the derivative respect to each coefficient in order to find the best curve\n\n\n"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":136},"colab_type":"code","executionInfo":{"elapsed":1085,"status":"ok","timestamp":1530562415398,"user":{"displayName":"Mar√≠a Eugenia Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112432663657646180649"},"user_tz":180},"id":"GJ0xD9ZWxVxh","outputId":"fbf3898a-9b37-42a6-c8be-e66720efcd7f","trusted":true,"_uuid":"1123a97db89b83a1a5e0779e50673b6d47d679b7"},"cell_type":"code","source":"dataset_train, dataset_test = train_test_split(houses_df,test_size=0.2,random_state=3)\nprice_train= np.asarray(dataset_train.price).reshape(-1,1)\nmy_features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors','yr_built','zipcode']\ntrain_matrix = dataset_train.as_matrix(my_features)\nregr_with_more_features = linear_model.LinearRegression()\nmodel_least_squares = regr_with_more_features.fit(train_matrix, price_train)\n\n\nmatrix_test = dataset_test.as_matrix(my_features)\n\nprice_test_multiple_regression = np.asarray(dataset_test.price).reshape(-1,1)\n\nprediction_test_least_squares = model_least_squares.predict(matrix_test)\n\nprint ('Least Squares Means')\n#Coefficient\nprint('Coefficient:',model_least_squares.coef_[0])\n#Apply the model we created using the training data to the test data, and calculate the RSE.\nprint('RSS',((price_test_multiple_regression - prediction_test_least_squares) **2).sum())\n# Calculate the MSE\nprint('RMSE', np.sqrt(metrics.mean_squared_error(price_test_multiple_regression,prediction_test_least_squares)))\n# Coefficient of determination R^2 of the prediction\nprint('The model\\'s performance is %.2f\\n' % model_least_squares.score(matrix_test, price_test_multiple_regression))\n\n","execution_count":null,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":204},"colab_type":"code","executionInfo":{"elapsed":1242,"status":"ok","timestamp":1530562480887,"user":{"displayName":"Mar√≠a Eugenia Cruz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112432663657646180649"},"user_tz":180},"id":"CYATPLUmZl_b","outputId":"4aef72e7-7b89-4269-bd46-82fa9e91230b","trusted":true,"_uuid":"b2bb206a2a30cf9c638a240a7ebf7f3fa70c9ce1"},"cell_type":"code","source":"price_test_multiple_regression = price_test_multiple_regression.reshape(-1)\nprediction_test_least_squares  = prediction_test_least_squares.reshape(-1) \n\nactual_predicted_data_least_squares = pd.DataFrame({'Actual': price_test_multiple_regression, 'Predicted': np.round(prediction_test_least_squares,decimals=3)})\nactual_predicted_data_least_squares.head()","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"_XQ6MiBml0Wy","_uuid":"4b4b4fa6c891183441c4453e8d7e182061d6a7fa"},"cell_type":"markdown","source":" So this is the part that you ask yourself: How do I choose the best model that represents my data? In this particularly case we have to look at:\n\n-$RMSE$\n\n-$R^2$\n\n\nI will detail a little bit about the last one.\n$R^2$ It's the coefficient of determination. It explains how good is your model when compared to the baseline model.\nThe math formula is given by:\n\n$$R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} $$\n\nWhere:\n\n- $ \\bar{y} $ is the mean of the observed data:\n\n- $y_i$ represents the observed values\n\n  $$ SS_{tot} =\\sum_{i=1}^n (y_i - \\bar{y})^2 $$\n\n  $SS_{tot} $ quantifies how much the data points $y_i$ varies from their mean $ \\bar{y}$\n\n  $$ SS_{res} = \\sum_{i=1}^n (y_i - \\hat{y})^2 $$\n\n  $SSE_{res}$ quantifies how much the data points, $y_i$ varies around the estimated regression  $ \\hat{y} $\n\n\nIf this number is large, it is said, the regression gives a good fit . When is a large number? Well R goes from 0 to 1 for linear regressions.\n\n- $R^2$ = 1 indicates that the regression predictions perfectly fit the data.\n- $R^2$ = 0 indicates that the estimated regression line is perfectly horizontal\n\nSo... How do we interpret this coefficient?\n\n\"$R^2$ √ó100 percent of the variation in y is accounted for by the variation in predictor x.\"\n\nIf $R^2$=0.55 then it means 55% variations in House prices is accounted for by the variation in Square Feet Living..\n\nIf you want to read more about this, check the [Statistics Program from Pennsylvania State University ](https://onlinecourses.science.psu.edu/stat501/node/255/) \n\nAs for RMSE, the lowest the better, because it means that our prediction line is not varying that much from the actual values\nSo taking all this into account. Which are ours best models? According to $R^2$ and $RMSE$, our best pick is Linear regression with more features and then Adaboost algorithm. As for Linear regression if we take under consideration all the features probably we are going to have a better model.\nThats all for today, folks. I hope this comes useful to someone and don't hesitate if you have any doubt, or see an error. Drop a line!"},{"metadata":{"colab_type":"text","id":"ozyz_JEzqtYP","_uuid":"659c7573c802469ee0de07f733908405e16f46d5"},"cell_type":"markdown","source":"### References\n\n- [OLS Coefficients ](https://www.youtube.com/watch?v=EL-tayJzK7M)\n- [Linear Regression](https://bookdown.org/sbikienga/Intro_to_stat_book/introduction-to-simple-linear-regression.html)\n*   [Lasso and Ridge Regression](http://perso-etis.ensea.fr/tzompanaki/ML/regression.pdf)\n*   [Understanding Bias Variance tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html)\n*   [Ridge Regression](https://www.youtube.com/watch?v=5asL5Eq2x0A)\n*   [Curve fitting](https://www.essie.ufl.edu/~kgurl/Classes/Lect3421/Fall_01/NM5_curve_f01.pdf)\n*   [Adaboost](https://engineering.purdue.edu/kak/Tutorials/AdaBoost.pdf)\n*   [Others criterion](http://adataanalyst.com/machine-learning/guide-for-linear-regression-using-python-part-2/)\n\n"}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"default_view":{},"name":"House-Prices.ipynb","provenance":[],"version":"0.3.2","views":{}},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}