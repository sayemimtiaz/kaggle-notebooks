{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Extracting highly specific information on therapeutic studies and building a manually curated benchmark dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Summary\n\nWe developed a pipeline and a set of approaches to create the summary tables as specified in task 7 using the [CORD-19 Dataset](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) last updated on June 09. We identified 209 articles relevant to the question, '*What is the best method to combat the hypercoagulable state seen in COVID-19_.csv*', and 1,352 articles relevant to the question, '*What is the efficacy of novel therapeutics being tested currently_.csv*'. As the the [CORD-19 Dataset](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) still has many duplicated records, we excluded the duplicated ones and some irrelevant articles. At the end, we identified 174 articles (after excluding 2 irrelevant and 33 duplicated articles) and 1,095 articles (after excluding 52 irrelevant and 205 duplicated articles) for the two questions respectively, and extracted information from these articles to fill the summary tables following their specific formats. The summary tables are listed as follows, and can be downloaded from the following links:\n\n- [Summary table of '*What is the best method to combat the hypercoagulable state seen in COVID-19_.csv*'](https://www.kaggle.com/gdsttian/cord19round2task7?select=summary_table_hypercoagulability.csv)\n- [Summary table of '*What is the efficacy of novel therapeutics being tested currently_.csv*'](https://www.kaggle.com/gdsttian/cord19round2task7?select=summary_table_therapeutic_efficacy.csv)\n\nWe call the first table, hypercoagulable table, and the second table, therapeutics table.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\n\ntable_hypercoagulability = pd.read_csv(f\"/kaggle/input/cord19round2task7/summary_table_hypercoagulability.csv\", na_filter= False)\nprint(f\"Table 1: What is the best method to combat the hypercoagulable state seen in COVID-19_.csv\")\nprint(f\"{'Number of entries in the table:':20}{table_hypercoagulability.shape[0]:5}\")\ntable_hypercoagulability.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"table_therapeuticefficacy = pd.read_csv(f\"/kaggle/input/cord19round2task7/summary_table_therapeutic_efficacy.csv\", na_filter= False)\nprint(f\"Table 2: What is the efficacy of novel therapeutics being tested currently_.csv\")\nprint(f\"{'Number of entries in the table:':20}{table_therapeuticefficacy.shape[0]:5}\")\ntable_therapeuticefficacy.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We compared our summary tables with those curated by experts provided on Kaggle website. We retrieved 9 out of the 19 articles from the hypercoagulable table and 19 out of the 34 articles from the therapeutics table curated by experts. For the articles not retrieved in our results, we identified the reasons, which are listed in the following table. \n\n\n|Category|Table of Combating Hypercoagulable State|Table of Novel Therapeutics Efficacy|\n|:-|-|-|\n|Total articles in the sample table curated by experts|19|34|\n|Articles in our final result|9|19|\n|Articles not in our final result|10|15|\n|Articles without an abstract|5|5|\n|Articles without relevant keywords in title and abstract|0|6|\n|Articles without chemicals tagged|5|2|\n|Articles not in the CORD-19 dataset|0|2|","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nThe COVID-19 pandemic has caused nearly 8 million confirmed infected patients and more than 430 thousand deathes worldwide. In response to the pandemic, the [CORD-19 dataset](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) including scholarly articles related to COVID-19 was created for global research community to generate helpful insight for the ongoing combat against this infectious disease using state-of-the-art text mining, NLP and other AI technologies. The [CORD-19 competition](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) was organized by Kaggle as a call for actions to develop tools and information for answering scientific questions with high priority. [Round \\#2](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/discussion/150921) of the [CORD-19 challenge](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) asked participants to create summary tables with specific structures derived by expert curators.\n\nTo tackle the challenges, we have organized a collaborative team including scientists from [Insilicom Inc.](https://insilicom.com/) and the department of statistics of Florida State University. Insilicom specializes in providing innovative technologies to help scientists effectively use Big Data to accelerate their research and development efforts. It recently developed the [Biomedical Knowledge Discovery Engine (BioKDE)](https://biokde.com/), a deep-learning powered search engine for biomedical literature. \n\nOur information extraction pipeline consists of the following components. \nFirst, based on the [CORD-19 dataset](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge), we developed a dataset which is clean and annotated with different entities such as genes, disease, chemicals, etc. using [PubTator](https://www.ncbi.nlm.nih.gov/research/pubtator/index.html), [BeFree](http://ibi.imim.es/befree/) and [scispacy](https://allenai.github.io/scispacy/) annotated entities; Second, we used extended keywords to query articles relevant to a paticular topic; Third, we used synonyms to further increase the coverage of the retrieved relevant articles; Fourth, we used regular expressions to extract specific information for filling certain columns of the tables; Fifth, we parsed the relevant sentences to obtain typed dependency graphs, which were used to compute the shortest pathes between relevant keywords, such as chemical names and COVID-19 related terms. The shortest pathes are used to further curate the relevant sentences. They will also be used in the future for building predictive models for the corresponding information extraction tasks; Finally, we have manually verified the summary tables before submitting to obtain a manually curated dataset, which can be used in future studies as benchmark data. These manually curated data can also be used to build machine learning models. \n\nTask 7 of the challenge is to \"[create summary tables that address therapeutics, interventions, and clinical studies](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks?taskId=887)\". This task targets to answer two questions:\n\n1. What is the best method to combat the hypercoagulable state seen in COVID-19?\n2. What is the efficacy of novel therapeutics being tested currently? \n\nby creating two summary tables:\n\n1. What is the best method to combat the hypercoagulable state seen in COVID-19_.csv\n2. What is the efficacy of novel therapeutics being tested currently_.csv\n\nThis notebook was organized in the following structure:\n\n1. Developing a Cleaned Dataset with Annotated Entity Types\n   - Processing the [CORD-19 Dataset](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge)\n   - Entity Annotation\n   - Aggregation and Indexing of CORD-19 Articles\n2. Creating the Summary Tables\n   - Retrieving COVID-19 Related Articles\n   - Keywords for Retrieving Articles Related to the Summary Table\n   - Extracting relevant information\n   - Generating the Summary Tables","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Developing a Cleaned Dataset with Annotated Entity Types\n\n## Processing the CORD-19 Dataset\n\nTo process the [CORD-19 Dataset](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge), we verified the ids of doi, pmid and pmcid of each article and organized all the articles in a consistent format. For articles with pmids and/or pmcids, the pmids and pmcids will be used for getting entities annotations from [PubTator](https://www.ncbi.nlm.nih.gov/research/pubtator/index.html). Each article was stored in a JSON file after the above pre-processing. The codes for extracting ids and article pre-processing can be found as listed:\n- [code for getting ids](https://www.kaggle.com/gdsttian/preprocess-get-ids)\n- [code for article process](https://www.kaggle.com/gdsttian/preprocess-cord-data).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# !python preprocess_get_ids.py\n# !python preprocess_cord_data.py","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Entity Annotation\n\nEntity annotation is very helpful for extracting relevant information. [PubTator](https://www.ncbi.nlm.nih.gov/research/pubtator/) provides annotations of biomedical concepts in PubMed abstracts and PMC full-text articles. Using the [PubTator](https://www.ncbi.nlm.nih.gov/research/pubtator/) API, we acquired annotations for the articles in the [CORD-19 Dataset](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) when they are available. For those without pre-calculated annotations, we used the [PubTator](https://www.ncbi.nlm.nih.gov/research/pubtator/) web interface to retrieve the annotations. All [PubTator](https://www.ncbi.nlm.nih.gov/research/pubtator/) annotations were then parsed and organized in a consistent format for each article. The entities annotated by [PubTator](https://www.ncbi.nlm.nih.gov/research/pubtator/) include:\n\n- Genes\n- Diseases\n- Chemicals\n- Species\n- Mutation\n- Cellline\n\nBeside [PubTator](https://www.ncbi.nlm.nih.gov/research/pubtator/) annotations, we also used [BeFree](http://ibi.imim.es/befree/) and [scispacy](https://allenai.github.io/scispacy/) to annotate additional entities. [BeFree](http://ibi.imim.es/befree/) annotates entities of **genes** and **diseases** (a python package needs to be installed from the [BeFree repo](https://bitbucket.org/nmonath/befree/src/master/)). In order to use [BeFree](http://ibi.imim.es/befree/) in our pipeline, we modified a function of the package, which can be found [here](https://www.kaggle.com/gdsttian/befree-ner-covid19).\n\n[scispacy](https://allenai.github.io/scispacy/) includes different models for biomedical concept annotation, among which two were used in our pipeline. The two models and the entities annotated by each model are listed as follows:\n\n- en_ner_craft_md: genes, taxonomies, sequence ontologies, chemicals, gene ontologies and cellline\n- en_ner_jnlpba_md: DNA, cell type, cellline, RNA and protein\n\nAll annotations were combined into a final set of annotations. When annotations by different tools overlap with each other, we selected the annotations with the largest span. When different tools annotate entities at the same span, we gave priority to [PubTator](https://www.ncbi.nlm.nih.gov/research/pubtator/).\n\nAll codes for annotations are available through the following links:\n\n- [code for acquiring existing PubTator annotations](https://www.kaggle.com/gdsttian/entities-get-pubtator-annotation)\n- [code for posting titles abd abstracts to PubTator for annotations](https://www.kaggle.com/gdsttian/entities-post-tiabs-to-pubtator)\n- [code for retrieving completed title and abstract annotations from PubTator](https://www.kaggle.com/gdsttian/entities-retrieve-tiabs-from-pubtator)\n- [code for parsing PubTator annotations](https://www.kaggle.com/gdsttian/entities-process-pubtator-annotation)\n- [code for adding BeFree and scispacy annotations](https://www.kaggle.com/gdsttian/entities-additional-annotation)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install git+https://bitbucket.org/nmonath/befree.git\n# !python entities_get_pubtator_annotation.py\n# !python entities_post_tiabs_to_pubtator.py\n# !python entities_retrieve_tiabs_from_pubtator.py\n# !python entities_process_pubtator_annotation.py\n# !python entities_additional_annotation.py","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Aggregation and Indexing of CORD-19 Articles\n\nWith all the CORD-19 articles processed and the annotations combined, they were aggregated into a single JSON file. For each article, the entities identified were summarized and relations between entities were extracted if they co-occur in the same sentence. \n\nQuery of relevant articles plays an important role in creating the summary tables. In order to retrieve target articles, we created indices of articles by publication time and keywords in titles and abstracts. We used [spaCy](https://spacy.io/) for tokenization of titles and abstracts. Articles returned from the query were ranked by the counts of the keywords occuring in the articles.\n\nAll codes for data aggregation and indexing were given as follows:\n\n- [code for data aggregation](https://www.kaggle.com/gdsttian/data-aggregation)\n- [code for entities summary and relation building](https://www.kaggle.com/gdsttian/data-nodes-relations)\n- [code for index by time](https://www.kaggle.com/gdsttian/data-indexing-time)\n- [code for index by words](https://www.kaggle.com/gdsttian/data-indexing-word)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# !python data_aggregation.py\n# !python data_nodes_relations.py\n# !python data_indexing_time.py\n# !python data_indexing_word.py","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating the Summary Tables\n\nAfter the clean annotated dataset was created, we can start the query and information extraction process.\n\n## Configuration and Import of Python Packages and Tools\n\nAt the beginning, we define the data pathes and all the data needed during the process.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# data pathes\ndata_path = '/kaggle/input/cord-19-data-with-tagged-named-entities/data' # folder for system data\njson_path = '/kaggle/input/cord-19-data-with-tagged-named-entities/data/json_files/json_files' # path of final json files\nmapping_pnid = 'mapping_corduid2nid.json' # dictionary mapping cord_uid to numeric id for each paper\n\nindex_year = 'index_time_year.json' # dictionary of list of papers for each publish year\nindex_title = 'index_word_title.json' # dictionary of list of papers for each word in title\nindex_abstract = 'index_word_abstract.json' # dictionary of list of papers for each word in abstract\nword_counts = 'paper_word_counts.json' # word counts by paper\nindex_table = 'index_word_table.json'\npaper_tables = 'paper_tables.json'\n\nentity_lists = 'entity_lists.json' # entity checking lists including disease list, blacklist etc.\nentity_nodes = 'entity_nodes.json' # entities dictionary\nentity_relations = 'entity_relations.json' # entity relation dictionary\n\nmapping_sents = 'mapping_sents2nid.json' # mapping sent id to numeric id\nindex_sents = 'index_word_sents.json' # mapping word to a list of numeric sent id\nsentences = 'sentences.json' # dictionary of all sentences with unique id\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We import the python packages and the tools we developed for the process. These tools can be used for data loading, article query and display. Codes of the tools can be accessed by the following links:\n\n- [code for utility tools](https://www.kaggle.com/gdsttian/utils)\n- [code for search tools](https://www.kaggle.com/gdsttian/mining-search-tool)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# packages\nfrom utils import *\nfrom mining_search_tool import *\nimport os\ncsv_path = 'csv'\nos.makedirs(csv_path, exist_ok=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading the Data\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"papers = SearchPapers(data_path, json_path, mapping_pnid, index_year,\n                      index_title, index_abstract, word_counts, index_table, paper_tables,\n                      entity_lists, entity_nodes, entity_relations, index_sents, mapping_sents, sentences)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Retrieving COVID-19 Related Articles\n\nInformation for the summary tables need to be extracted from the articles relevant to COVID-19. These articles were queried using a list of keywords. We defined the list of extended keywords based on those used by the [PMC COVID-19 Initiative](https://www.ncbi.nlm.nih.gov/pmc/about/covid-19/) and manual reading of some relevant articles.\n\nAs most of the articles associated with COVID-19 were published in 2020, we limited the publication time to year 2020.\n\nThere are more than 35 thousand articles identified as relevant to COVID-19 in the [CORD-19 dataset](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) updated on June 9.\n\nOne of the articles was displayed as follows.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"covid19_names = \"\"\"covid-19, covid19, covid, sars-cov-2, sars-cov2, sarscov2,\n                   novel coronavirus, 2019-ncov, 2019ncov, wuhan coronavirus\n                \"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"papers_covid19 = papers.search_papers(covid19_names, section = None, publish_year = '2020')\nprint(f\"{'Total papers relevant to COVID-19:':20}{len(papers_covid19):6}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"papers.display_papers(papers_covid19[:1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Keywords for Retrieving Articles Related to the Summary Table\n\nBased on our preliminary research, we defined the lists of keywords for querying articles for the two summary tables as follows.\n\nThe keywords for querying articles for the summary table of '*What is the best method to combat the hypercoagulable state seen in COVID-19_.csv*' include the synonyms and phrases related to coagulation. We assume that any articles containing anyone of the keywords in title or abstract are relevant to the summary table.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"query_coagulation = \"\"\"coagulation, anticoagulant, decoagulant, anticoagulation,\n                       hypercoagulable, hypercoagulability, coagulopathy, vasoconstrictive,\n                       thromboprophylaxis, thrombosis, thrombotic, thromboembolism, thromboprophylaxis\n                    \"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The keywords for querying articles for the summary table, '*What is the efficacy of novel therapeutics being tested currently_.csv*', include two lists: a list of synonyms and phrases related to therapeutics and a list of synonyms and phrases related to efficacy. We assume that any articles containing anyone of the therapeutic keywords and anyone of the efficacy keywords in title or abstract are relevant to the summary table.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"query_therapy = \"\"\"therapy, therapeutic, therapeutics,inhibitor,\n                   inhibitors, medicine, medication, drug, drugs,\n                   treat, treatment, pharmaceutical, pharmaceutic\n                \"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query_effect = \"\"\"effect, effects, efficacy, effective, effectiveness, benifit, benifits\n               \"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extracting relevant information \n\nThe information we extracted for each table includes study type, therapeutic methods, sample size, severity, general outcome, primary endpoint and clinical improvement in addition to other information that can be extracted from the metadata of the articles such as article IDs, publication date, journal etc..\n\n### Study Types\n\nThe study types specified in the [CORD-19 dataset](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) includes systematic review and meta-analysis, prospective observational study, retrospective observational study, case series, expert review, editorial, ecological regression, and simulation. As there is no information of study type in the metadata of the articles, we classified the articles into only four types: systematic review, retrospective study, simulation and other studies. For each study type, we defined a list of keywords, whose presence in the abstracts determines the corresponding article type. When an article contains keywords of more than one study type, its study type was decided in the priority order of systematic review, retrospective study, simulation and other studies.\n\nThe lists of keywords for study type classification are given below. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sys_review = ['systematic review', 'meta-analysis',\n              'search: PubMed, PMC, Medline, Embase, Google Scholar, UpToDate, Web of Science',\n              'searched: PubMed, PMC, Medline, Embase, Google Scholar, UpToDate, Web of Science',\n              'in: PubMed, PMC, Medline, Embase, Google Scholar, UpToDate, Web of Science']\nretro_study = ['record review','retrospective', 'observational cohort', 'scoping review']\nsimulation = ['modelling','model','molecular docking','modeling','immunoinformatics', 'simulation', 'in silico', 'in vitro']","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def get_papers_by_study_type(study_type_list):\n    papers_for_the_type = set()\n    for phrase in study_type_list:\n        papers_by_phrase = papers.search_papers(phrase, section = 'abs', publish_year = '2020')\n        papers_for_the_type = papers_for_the_type.union(set(papers_by_phrase))\n    return papers_for_the_type\n\npapers_review = get_papers_by_study_type(sys_review).intersection(set(papers_covid19))\nprint(f\"{'Systematic Review: ':25}{len(papers_review):6}\")\n\npapers_retro = get_papers_by_study_type(retro_study).intersection(set(papers_covid19))\npapers_retro = papers_retro - (papers_retro & papers_review)\nprint(f\"{'Retrospective Study: ':25}{len(papers_retro):6}\")\n\npapers_simulation = get_papers_by_study_type(simulation).intersection(set(papers_covid19))\npapers_simulation = papers_simulation - (papers_simulation & (papers_retro | papers_review))\nprint(f\"{'Simulation: ':25}{len(papers_simulation):6}\")\n\npapers_others = set(papers_covid19) - (papers_review | papers_retro | papers_simulation)\nprint(f\"{'Other Studies: ':25}{len(papers_others):6}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Therapeutic Methods\n\nTherapeutic methods include the annotated chemicals contained in title and abstract of each article relevant to the according summary table. Due to false positive annotations of chemicals (i.e. proteins, molecule, etc.), we manually defined a list of entity names to exclude them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"drug_blocklist = ['/fio2', '04-MAY-2020', '2-o', '25-hydroxycholecalcifoerol', '25ohd', '3350',\n                  '3mtm', \"5'-tp\", '6-o', '80-82 oc', 'acid', 'adverse drug', 'alcohol',\n                  'alcohols', 'alkaline', 'amino acid-related', 'amp', 'amplify', 'androgen',\n                  'antagonist', 'asp355asn', 'atp', 'bi', 'biopharmaceutical', 'bipap',\n                  'bis(monoacylglycero)phosphate', 'bmp', 'cai', 'calcium channel', 'carbon dioxide',\n                  'cationic', 'chemical', 'chemical compounds', 'chemical space', 'chemicals', 'co',\n                  'co2', 'compound', 'compounds', 'copper', 'cov-2 poc', 'covalent', 'covalent fragments',\n                  'covid-19', 'creatinine', 'cs', 'ctpa', 'cu', 'cys141', 'd-d', 'd-dimer', 'daegu', 'dfu',\n                  'dic', 'dmec', 'drug molecules', 'drug products', 'effector', 'electron', 'electrophile',\n                  'electrophilic fragment', 'eosin', 'ethylene oxide', 'eto', 'exoflo', 'extracellularly',\n                  'fdp', 'fio2', 'food', 'food vacuole', 'foods', 'fumigants', 'gdp', 'glu166',\n                  'glucose', 'glycan', 'h2o2', 'h7gmu', 'heme', 'hemoglobin', 'hemoglobin molecule',\n                  'hepatocellular type', 'hfc%', 'hfno', 'hg', 'his', 'hormone', 'hormones',\n                  'hpgp', 'hs', 'hydrogen', 'hydrogen peroxide', 'immunomodulators', 'immunomodulatory',\n                  'in10', 'ingredients', 'inhibitor', 'inhibitors', 'iron', 'iso13485', 'ketone',\n                  'l506a-f', 'lipid bis(monoacylglycero) phosphate', 'lipid', 'low-dose', 'lu',\n                  'lysosomotropic drugs', 'magnesium', 'magnesium sulfate', 'mesh', 'metabolites', 'metal',\n                  'metal ions', 'metals', 'meteorological', 'mineral', 'molecular', 'molecular electrostatic',\n                  'molecular probes', 'molecule', 'molecules', 'mrna', 'mt039887', 'mt184913', 'mt259229',\n                  'n', 'n-95', 'n95', 'nacl', 'ncpp', 'nct04330638', 'nct04355364', 'nct04359654',\n                  'nitric oxide', 'nitrogen', 'niv', 'nmdd', 'no2', 'non-toxic', 'nps', 'nucleic acid',\n                  'nucleoside', 'nucleotide', 'nutrients', 'o2', 'organs', 'outbroke', 'oxygen',\n                  'oxygen heterocyclic compounds', 'oxygen partial', 'oxygen species', 'ozone', 'pao(2)/fio(2',\n                  'pao2', 'peptidomimetics', 'pergamum', 'pesticides', 'pharmaceuticals', 'pharmacies',\n                  'pharmacist', 'pharmacologically active substances', 'phd', 'phospholipid', 'phosphorus',\n                  'phytochemicals', 'pic', 'pico', 'pigmented', 'plant', 'pm', 'pollutants', 'ppe',\n                  'prodrug', 'progesterone', 'protein', 'proteins', 'quarantine', 'r +', 'radical', 'reagent',\n                  'renine', 'residue', 'residues', 'ribose', 's2', 'sanitizer', 'sao2', 'se', 'ser', 'silica',\n                  'silver', 'small-molecule inhibitors', 'sodium', 'spo(2)', 'spo2', 'srq-20', 'steroid',\n                  'steroids', 'substance', 'substances', 'supplements', 'therapeutic', 'therapeutic agents',\n                  'therapeutic anticoagulation', 'therapeutic drugs', 'thr27arg', 'topical', 'toxic',\n                  'trizol', 'ultraviolet', 'urea', 'urea nitrogen', 'vdi', 'vph', 'water', 'xenobiotics']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(drug_blocklist))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sample Size\n\nSample size information was extracted from abstract by regular expression. Types of sample size differ by study types, e.g. sample size of systematic reviews could be the number of articles being reviewed, while sample size of retrospective study or clinical trial could be the number of patients involved. As such, we defined two regex for extracting sample size information for different study types. Usually simulation articles do not have sample size information. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ss_patient = re.compile(r'(\\s)([0-9,]+)(\\s|\\s[^0-9,\\.\\s]+\\s|\\s[^0-9,\\.\\s]+\\s[^0-9,\\.\\s]+\\s)(patients|persons|cases|subjects|records)')\nss_review = re.compile(r'(\\s)([0-9,]+)(\\s|\\s[^0-9,\\.\\s]+\\s|\\s[^0-9,\\.\\s]+\\s[^0-9,\\.\\s]+\\s)(studies|papers|articles|publications|reports|records)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Severity\n\nInformation on severity was extracted by searching abstracts for occurences of a list of severity keywords defined as follows.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"severity_words = ['mild', 'moderate', 'severe', 'critical', 'icu', 'non-icu',\n                  'fatality','mortality','mortalities','death','deaths','dead','casualty']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### General Outcome\n\nWhen conclusion section is included in an abstract, it can be considered as the general outcome. Otherwise the last sentence containing the therapeutic methods or the last two sentences of the abstract are considered as general outcome.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Primary Endpoint and Clinical Improvement\n\nAny sentences in the abstract containing either the phrases of '*primary outcome*' or '*primary endpoint*' can be extracted as the primary endpoint. For clinical improvement, we defined a list of clinical improvement keywords, whose presence in an abstract indicates clinical improvement.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"primary_phrase = ['primary outcome', 'primary endpoint']\nimprove_phrase = ['improve', 'better', 'amend', 'ameliorate', 'meliorate']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generating the Summary Tables\n\nWith the approach and relevant keywords defined above, we were able to build the two summary tables as follows. In the process, we excluded the articles without abstract or without mention of any therapeutic methods.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Table 1:\n### What is the best method to combat the hypercoagulable state seen in COVID-19_.csv\n\nTotal of 224 articles were identified as relevant to the above question.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"papers_coagulation = papers.search_papers(query_coagulation, section = None, publish_year = '2020')\npapers_coagulation = list(set(papers_coagulation) & set(papers_covid19))\nprint(f\"{'Papers containing any hypercoagulability keywords: ':60}{len(papers_coagulation):6}\")\n\nselectpapers_coagulation = []\nfor paper in papers_coagulation:\n    if 'Chemical' in papers.entity_nodes[str(paper)]:\n        chemicals = [chem for chem in papers.entity_nodes[str(paper)]['Chemical'].keys() if chem.lower() not in drug_blocklist]\n        if len(chemicals) > 0:\n            selectpapers_coagulation.append(paper)\n\nprint(f\"{'Selected papers addressing hypercoagulability:':60}{len(selectpapers_coagulation):6}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"papers.display_papers(selectpapers_coagulation[:1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The top therapeutic methods mentioned in the relevant articles were listed as follows:","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"entity_stats = papers.get_entity_stats(selectpapers_coagulation)\ntherapeutics = {}\nfor k, v in entity_stats['Chemical'].items():\n    if k.lower() not in drug_blocklist:\n        if k.lower() not in therapeutics:\n            therapeutics[k.lower()] = v\n        else:\n            therapeutics[k.lower()] += v\nprint('-'*45)\nprint(f\"| {'Chemicals':32} | {'Counts':6} |\")\nprint('-'*45)\nfor i in sorted(therapeutics.items(), key = lambda x:x[1], reverse = True)[:15]:\n      print(f\"| {i[0]:32} | {i[1]:6} |\")\nprint('-'*45)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After excluding articles without abstracts or without mention of any therapeutic methods, 209 relevant articles were identified for extracting relevant information.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import csv\nfrom datetime import date\nfrom spacy.lang.en import English\nnlp = English()\nsentencizer = nlp.create_pipe(\"sentencizer\")\nnlp.add_pipe(sentencizer)\nfile_name = 'summary_table_hypercoagulability'\nwith open(f\"{csv_path}/{file_name}.csv\", 'w', encoding = 'utf-8') as fcsv:\n    csv_writer = csv.writer(fcsv)\n    csv_writer.writerow(['Date', 'Study', 'Study Link', 'Journal', 'Study Type',\n                         'Therapeutics', 'Sample Size', 'Severity', 'General Outcome',\n                         'Primary Endpoints', 'Clinical Improvement', 'Added On',\n                         'DOI', 'CORD_UID'])\n\n    for pid in selectpapers_coagulation:\n        file = json.load(open(f'{json_path}/{papers.nid2corduid[int(pid)]}.json', 'r', encoding = 'utf-8'))\n        abstract = file['abstract']['text']\n        if abstract == '': continue\n        doc = nlp(abstract)\n        sents_abs = list(doc.sents)\n        if len(sents_abs) == 1:\n            if 'copyright' in sents_abs[0].text:\n                continue\n        elif len(sents_abs) == 2:\n            if 'copyright' in sents_abs[0].text:\n                continue\n            elif 'copyright' in sents_abs[1].text:\n                sents_abs = sents_abs[0]\n        else:\n            if 'copyright' in sents_abs[-2].text:\n                sents_abs = sents_abs[:-2]\n            elif 'copyright' in sents_abs[-1].text:\n                sents_abs = sents_abs[:-1]\n        if len(sents_abs) == 0: continue\n        pub_date = file['publish_time']\n        study = file['title']['text']\n        study_link = file['url']\n        journal = file['journal']\n        #study type\n        if int(pid) in papers_review:\n            study_type = 'Systematic Review'\n        elif int(pid) in papers_retro:\n            study_type = 'Retrospective Study'\n        elif int(pid) in papers_simulation:\n            study_type = 'Simulation'\n        else:\n            study_type = 'Other'\n        # therapeutics\n        chemicals = list(set(chem.lower() for chem in papers.entity_nodes[str(pid)]['Chemical'].keys() if chem.lower() not in drug_blocklist))\n        therapeutics = ', '.join(chemicals)\n        # sample size\n        sample_size = ''\n        if study_type == 'Systematic Review':\n            matches = re.findall(ss_review, abstract)\n            for match in matches:\n                if match[1].isdigit() and int(match[1]) != 2019:\n                    sample_size = sample_size + ''.join(match[1:]) + '; '\n        elif study_type == 'Retrospective Study' or study_type == 'Other' :\n            matches = re.findall(ss_patient, abstract)\n            for match in matches:\n                if match[1].isdigit() and int(match[1]) != 2019:\n                    sample_size = sample_size + ''.join(match[1:]) + '; '\n        # severity\n        severity = []\n        for phrase in severity_words:\n            if phrase in abstract.lower():\n                severity.append(phrase)\n        severity = ', '.join(severity)\n        # general outcome\n        conclusion = ''\n        conclusion_match = re.search(r'(?<=\\s)(Conclusion[^,]?:\\s?)(.*)', abstract, flags = re.I)\n        if conclusion_match != None:\n            conclusion = conclusion_match[2].strip()\n        if conclusion != '':\n            gen_outcome = conclusion\n        else:\n            if len(sents_abs) <= 2:\n                gen_outcome = ' '.join(sent.text for sent in sents_abs)\n            else:\n                sents = []\n                num = len(sents_abs)\n                for sent_i, sent in enumerate(sents_abs):\n                    if any(chem.lower() in sent.text.lower() for chem in chemicals) and sent_i < num-2:\n                        sents.append(sent.text)\n                if len(sents) > 0:\n                    gen_outcome = sents[-1] + ' ' + sents_abs[-2].text + ' ' + sents_abs[-1].text\n                else:\n                    gen_outcome = sents_abs[-2].text + ' ' + sents_abs[-1].text\n        # primary endpoint\n        primary_endponit = ''\n        for sent in doc.sents:\n            if any(phrase.lower() in sent.text.lower() for phrase in primary_phrase):\n                primary_endponit = primary_endponit + sent.text + ' '\n        # clinical improvement\n        clinical_improvement = ''\n        if any(phrase.lower() in sent.text.lower() for phrase in improve_phrase):\n            clinical_improvement = 'Y'\n        # added on\n        added_on = date.today().strftime('%m/%d/%Y')\n        doi = file['doi']\n        cord_uid = file['cord_uid']\n            \n        csv_writer.writerow([pub_date, study, study_link, journal, study_type, therapeutics, sample_size,\n                             severity, gen_outcome, primary_endponit, clinical_improvement, added_on, doi, cord_uid])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ntable_hypercoagulability = pd.read_csv(f\"{csv_path}/{file_name}.csv\", na_filter= False)\nprint(f\"{'Total papers:':20}{table_hypercoagulability.shape[0]:5}\")\ntable_hypercoagulability.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Table 2:\n### What is the efficacy of novel therapeutics being tested currently_.csv\n\nTotal of 1,372 articles were filtered as associated with efficacy of novel therapeutics being tested currently.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"papers_therapy = papers.search_papers(query_therapy, section = None, publish_year = '2020')\npapers_therapy = list(set(papers_therapy) & set(papers_covid19))\nprint(f\"{'Papers containing any therapeutic keywords:':60}{len(papers_therapy):6}\")\n\npapers_effect = papers.search_papers(query_effect, section = None, publish_year = '2020')\npapers_effect = list(set(papers_effect) & set(papers_covid19))\nprint(f\"{'Papers containing any efficacy keywords:':60}{len(papers_effect):6}\")\n\npapers_therapyeffects = list(set(papers_therapy) & set(papers_effect))\nprint(f\"{'Papers containing both therapeutic and efficacy keywords:':60}{len(papers_therapyeffects):6}\")\n\nselectpapers_therapyeffects = []\nfor paper in papers_therapyeffects:\n    if 'Chemical' in papers.entity_nodes[str(paper)]:\n        chemicals = [chem for chem in papers.entity_nodes[str(paper)]['Chemical'].keys() if chem.lower() not in drug_blocklist]\n        if len(chemicals) > 0:\n            selectpapers_therapyeffects.append(paper)\n\nprint(f\"{'Selected papers addressing therapeutic efficacy:':60}{len(selectpapers_therapyeffects):6}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"papers.display_papers(selectpapers_therapyeffects[:1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The top therapeutic methods mentioned in the associated articles were listed as follows:","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"entity_stats = papers.get_entity_stats(selectpapers_therapyeffects)\ntherapeutics = {}\nfor k, v in entity_stats['Chemical'].items():\n    if k.lower() not in drug_blocklist:\n        if k.lower() not in therapeutics:\n            therapeutics[k.lower()] = v\n        else:\n            therapeutics[k.lower()] += v\nprint('-'*45)\nprint(f\"| {'Chemicals':32} | {'Counts':6} |\")\nprint('-'*45)\nfor i in sorted(therapeutics.items(), key = lambda x:x[1], reverse = True)[:15]:\n      print(f\"| {i[0]:32} | {i[1]:6} |\")\nprint('-'*45)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After the articles without abstract and the articles without mention of any therapeutic methods were excluded, there were 1,352 associated articles identified from which relevant information was extracted.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import csv\nfrom datetime import date\nfrom spacy.lang.en import English\nnlp = English()\nsentencizer = nlp.create_pipe(\"sentencizer\")\nnlp.add_pipe(sentencizer)\nfile_name = 'summary_table_therapeutic_efficacy'\nwith open(f\"{csv_path}/{file_name}.csv\", 'w', encoding = 'utf-8') as fcsv:\n    csv_writer = csv.writer(fcsv)\n    csv_writer.writerow(['Date', 'Study', 'Study Link', 'Journal', 'Study Type',\n                         'Therapeutics', 'Sample Size', 'Severity', 'General Outcome',\n                         'Primary Endpoints', 'Clinical Improvement', 'Added On',\n                         'DOI', 'CORD_UID'])\n\n    for pid in selectpapers_therapyeffects:\n        file = json.load(open(f'{json_path}/{papers.nid2corduid[int(pid)]}.json', 'r', encoding = 'utf-8'))\n        abstract = file['abstract']['text']\n        if abstract == '': continue\n        doc = nlp(abstract)\n        sents_abs = list(doc.sents)\n        if len(sents_abs) == 1:\n            if 'copyright' in sents_abs[0].text:\n                continue\n        elif len(sents_abs) == 2:\n            if 'copyright' in sents_abs[0].text:\n                continue\n            elif 'copyright' in sents_abs[1].text:\n                sents_abs = sents_abs[0]\n        else:\n            if 'copyright' in sents_abs[-2].text:\n                sents_abs = sents_abs[:-2]\n            elif 'copyright' in sents_abs[-1].text:\n                sents_abs = sents_abs[:-1]\n        if len(sents_abs) == 0: continue\n        pub_date = file['publish_time']\n        study = file['title']['text']\n        study_link = file['url']\n        journal = file['journal']\n        #study type\n        if int(pid) in papers_review:\n            study_type = 'Systematic Review'\n        elif int(pid) in papers_retro:\n            study_type = 'Retrospective Study'\n        elif int(pid) in papers_simulation:\n            study_type = 'Simulation'\n        else:\n            study_type = 'Other'\n        # therapeutics\n        chemicals = list(set(chem.lower() for chem in papers.entity_nodes[str(pid)]['Chemical'].keys() if chem.lower() not in drug_blocklist))\n        therapeutics = ', '.join(chemicals)\n        # find relevant information from abstract\n        abstract = file['abstract']['text']\n        doc = nlp(abstract)\n        # sample size\n        sample_size = ''\n        if study_type == 'Systematic Review':\n            matches = re.findall(ss_review, abstract)\n            for match in matches:\n                if match[1].isdigit() and int(match[1]) != 2019:\n                    sample_size = sample_size + ''.join(match[1:]) + '; '\n        elif study_type == 'Retrospective Study' or study_type == 'Other' :\n            matches = re.findall(ss_patient, abstract)\n            for match in matches:\n                if match[1].isdigit() and int(match[1]) != 2019:\n                    sample_size = sample_size + ''.join(match[1:]) + '; '\n        # severity\n        severity = []\n        for phrase in severity_words:\n            if phrase in abstract.lower():\n                severity.append(phrase)\n        severity = ', '.join(severity)\n        # general outcome\n        conclusion = ''\n        conclusion_match = re.search(r'(?<=\\s)(Conclusion[^,]?:\\s?)(.*)', abstract, flags = re.I)\n        if conclusion_match != None:\n            conclusion = conclusion_match[2].strip()\n        if conclusion != '':\n            gen_outcome = conclusion\n        else:\n            if len(sents_abs) <= 2:\n                gen_outcome = ' '.join(sent.text for sent in sents_abs)\n            else:\n                sents = []\n                num = len(sents_abs)\n                for sent_i, sent in enumerate(sents_abs):\n                    if any(chem.lower() in sent.text.lower() for chem in chemicals) and sent_i < num-2:\n                        sents.append(sent.text)\n                if len(sents) > 0:\n                    gen_outcome = sents[-1] + ' ' + sents_abs[-2].text + ' ' + sents_abs[-1].text\n                else:\n                    gen_outcome = sents_abs[-2].text + ' ' + sents_abs[-1].text\n        # primary endpoint\n        primary_endponit = ''\n        for sent in doc.sents:\n            if any(phrase.lower() in sent.text.lower() for phrase in primary_phrase):\n                primary_endponit = primary_endponit + sent.text + ' '\n        # clinical improvement\n        clinical_improvement = ''\n        if any(phrase.lower() in sent.text.lower() for phrase in improve_phrase):\n            clinical_improvement = 'Y'\n        # added on\n        added_on = date.today().strftime('%m/%d/%Y')\n        doi = file['doi']\n        cord_uid = file['cord_uid']\n            \n        csv_writer.writerow([pub_date, study, study_link, journal, study_type, therapeutics, sample_size,\n                             severity, gen_outcome, primary_endponit, clinical_improvement, added_on, doi, cord_uid])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ntable_therapeuticefficacy = pd.read_csv(f\"{csv_path}/{file_name}.csv\", na_filter= False)\nprint(f\"{'Total papers:':20}{table_therapeuticefficacy.shape[0]:5}\")\ntable_therapeuticefficacy.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}