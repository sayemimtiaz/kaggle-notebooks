{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1>Clustering</h1>\nIl clustering e' utile ai fini di raggruppare un insieme di oggetti, ove ogni singolo gruppo ha oggetti molto simili tra di loro.<br></br>Questa tecnica e' la forma piu' comune di <i>apprendimento non supervisionato</i>, composto da un insieme di oggetti, di cui non si conosce a priori la corretta classificazione.<br></br>\nEsiste anche una versione semi-supervisionata del clustering (<i>Semi-Supervised Clustering</i>), in cui una parte ristretta degli esempi a disposizione, possegono dei vincoli di appartenenza ai vari cluster. Abitualmente l'uso di questa informazione aggiuntiva ha come finalita' migliorare la classificazione.\n    "},{"metadata":{},"cell_type":"markdown","source":"<h2>Definizione della tecnica</h2>\n\nUn problema di clustering puo' venire come di seguito definito:\n* insieme di esempi X = {$x_1,...x_n$};\n* misura di similarita' tra le varie istanze;\n* numero desiderato di cluster K (puo' essere mancante).\n\nL'obiettivo e' quello di calcolare una funzione di assegnamento ($\\gamma$) tale che, per ogni oggetto in X, assegna questo oggetto a un solo valore di K:\n<center>$\\gamma$ : X $\\rightarrow$ {1,...,K}</center>\n\nLa funzione $\\gamma$ fa in modo che nessun assegnamento, nei cluster, risulti vuoto; e che non si abbia nessuna relazione di ordine trai  cluster, definiti solo da numeri simbolici. Inoltre definsce l'<i>hard-clustering</i>, perche' caratterizza un assegnamento deterministico.\n\nData pero' in questo modo, risulta evidente, che la definizione di cluster appare vaga e soggettiva (come si classifica un frutto, per forma o colore? In entrambi i casi il clustering risulta sensato). Tuttavia tale incertezza, viene sciolta, con la definizione della topologia di clusterizzazione e con la scelta dell'algortimo.\n\n- La topologia di clusterizzazione di un problema, e' definibile sulle seguenti decisioni:\n<ol> \n    <li> <b>Come rappresentare gli oggetti nel clustering </b></li>\n    Lo scopo che si persegue con il clustering, e' quello di dare una rappresentazione in un feature spaces, cercando di rappresentare i dati, in modo da\n    rispettare la conoscenza a priori in possesso.<br></br>\n    Fondamentale e' stabilire quale similarita' e quale distanza utilizzare. Per fare questo, si puo' decidere di impiegare un kernel, stabilendo pero' quale e' \n    il piu' adatto, sulla base della conoscenza (difatti esistono non solo diversi kernel per diversi oggetti, ma anche diversi kernel per \n    uno stesso oggetto).<br></br>\n    Ovviamente scelte differenti, su ognuno di questi campi, comporta la construzioni di cluster diversi.\n    <br></br>\n    <li> <b>Quanti cluster si va a considerare</b></li>\n    Considero il caso di avere cifre monoscritte, allora potrei decidere di usare 10 cluster, uno per ogni cifra, e analizzare come le immagini, delle cifre, si\n    distribuiscono al loro interno. Ma potrei anche decidere di non fissare K  a priori, e farmi guidare dai dati; in questo secondo caso mi sarebbe essenziale usare le\n    informazioni che ho dai dati, per riuscire a ottenere il numero ottimale di cluster da adoperare.<br></br>\n    In entrambe le scelte, e in generale per ogni problema di clusterizzazione, devo definire un numero di cluster che sia non banale; evitando di costruire cluster\n    troppo popolati o con solo 1 o 2 esempi, non in grado di darmi alcuna informazione sulla classificazione del dataset.\n</ol> \n\n- Ci sono 2 categorie di algoritmi di clustering:\n<ol>\n<li> <b>algoritmi di partizionamento</b></li> Che partono da una partizione random che incrementalmente viene migliorata;\n    <li> <b>algoritmi genarchici</b></li> Caratterizzati da  approcci bottom-up o agglomerativi, in cui all'inizio ogni esempio rappresenta un cluster, che successivamente vanno ad aggregarsi; top - down o divisivi, in cui tutti gli esempi rappresentano un unico cluster, successivamente diviso in base alla similarita'."},{"metadata":{},"cell_type":"markdown","source":"<h2>Funzione obiettivo e assegnamento</h2>\n\nIl problema del clustering si puo' formulare con una funzione obiettivo, che si presenta per cio' come un problema di ricerca/ottimizzazione, con lo scopo di minimizzazione dell'errore, di un dato di trovarsi assegnato a un certo cluster. Usualmente si usa la distanza.\n\n\nSvolgere una ricerca esaustiva su K cluster, significa che su n esempi ognuno va assegnato a uno dei K cluster. Tuttavia K$^n$ appare come una stima eccessiva. Difatti, il numero di assegnamenti possibili deve essere invariante al numero assegnato ai cluster. Cio' significa, che se ho i campioni 10 e 11 e sono entrambi assegnati all'cluster 3, e' la stessa cosa che fossero assegnati al cluster 2, l'importante e' solo mantenere l'immutabilita' delle relazioni e non la numerazione. Dunque si parla di K$^n$/K! possibilita' di assegnazione; che tuttavia rimane un numero molto grande, all'aumentare di n. Ecco che l'idea di una ricerca esaustiva per fare classificazione non viene usata; ecco che, come accennavo nel paragrafo precedente, per questo vengono impiegati algoritmi di partizionamento o gerarchici, che usano un approccio greedy per raffinare la scelta del cluster.\n"},{"metadata":{},"cell_type":"markdown","source":"<h2>Dataset `COVID-19 in Italiy`</h2>"},{"metadata":{},"cell_type":"markdown","source":"Di seguito implento dei cluster, con l'uso di algoritmi di partizionamento e genarchici. Il caso studio che ho deciso di utilizzare, sono i dati dei contagi delle regioni italiane, dall'inizio della pandemia COVID-19. Sono interessata al dataset sulle province; i dati sono aggiornati a dicembre 2020."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        df = os.path.join(dirname, filename)\n        print(df)\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Creazione del dataset</h3>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"covid_ita_df = pd.read_csv(df, usecols=[\"SNo\", \"Date\", \"RegionCode\",\"TotalPositiveCases\"], encoding='latin-1')\ncovid_ita_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = covid_ita_df.drop(\"Date\",axis=1).values # data set\ny = covid_ita_df[\"Date\"].values # features\n\nprint(\"Dimensione di X: \", X.shape)\nprint(\"Dimensione di y: \", y.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Algoritmi di partizionamento</h3>\n\n* <h3>K-MEANS</h3>"},{"metadata":{},"cell_type":"markdown","source":"K-MEANS e' l'algoritmo piu' usato, e parte dall'assunzione che gli esempi sono vettori a valori reali.\n\nPer ogni cluster, la funzione obiettivo cerca di minimizzare la media della distanza tra gli esempi e il centro del cluster\n<center>$\\mu(c) = \\frac{1}{|C|} \\sum_{x \\in C}x$</center>\nove C e' il centroide.\n\nI punti principali dell'algoritmo di K-MEANS sono i seguenti:\n1. *inizializzazione*: vengono generati K punti nello spazio delle istanze, e questi non sono altro che i centroidi dei cluster. Nella versione piu' semplice, dell'algoritmo, (random) vengono presi K esempi inizializzati a caso;\n2. *assegnazione*: a ogni esempio viene assengato il cluster del centroide piu' vicino;\n3. *ricalcolo*: viene ricalcolata la posizione dei K centroidi, come la media di ogni esempio compreso il vecchio centroide;\n4. *loop*: i passi (2) e (3) vengono ripetuti, fino alla stabilizzazione dei centroidi."},{"metadata":{},"cell_type":"markdown","source":"Per prima cosa ho dovuto predisporre i dati del dataset, in modo che fosse possibile applicare l'algoritmo di K-MEANS. Indispensabile, a tale fine, e' stato il rimpiazzo del campo `Date`, espresso  in stringa, con un valore numerico rappresentante il mese della rilevazione del campione."},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 0\nfor data in y:\n    if '-01-' in data:\n        y[i] = \"1\"       \n    elif '-02-' in data:\n        y[i] = \"2\"\n    elif '-03-' in data:\n        y[i] = \"3\"\n    elif '-04-' in data:\n        y[i] = \"4\"\n    elif '-05-' in data:\n        y[i] = \"5\"\n    elif '-06-' in data:\n        y[i] = \"6\"\n    elif '-07-' in data:\n        y[i] = \"7\"\n    elif '-08-' in data:\n        y[i] = \"8\"\n    elif '-09-' in data:\n        y[i] = \"9\"\n    elif '-10-' in data:\n        y[i] = \"10\"\n    elif '-11-' in data:\n        y[i] = \"11\"\n    else:\n        y[i] = \"12\"\n    i = i + 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dopodiche' ho svolto K-Means clustering.\n\nIl mio obiettivo e' stato svolgere date clustering, sul numero di positivi per regione, in modo da individuare un cluster per ogni mese, in cui c'e' stata rilevazione di dati.\n\nPer fare questo, nel metodo offerto da sklearn per K-MEANS, ho deciso:\n- di settare il numero di cluster a 11 (`n_clusters`);\n- di usare un numero casuale di osservazioni da utilizzare per individuare i centroidi (`init`), prediligendo per motivi di efficienza cosi random a k- means++;\n- il numero di volte in cui l'algoritmo viene eseguito (`n_init`),valutando come fosse meglio scegliere un valore piccolo (1) anziche' piu' grande. Tale scelta e' stata vincolta, ancora una volta, da motivi di efficienza. Molto probabilmente la funzione obiettivo ha molti minimi locali, che fanno si che partendo da punti di patenza diversi, si arrivi anche a cluster finali molto diversi; degradando l'accuratezza dei cluster individuati."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\n\n\nk_means = KMeans(n_clusters=11, init='random', n_init=1) \nk_means.fit(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Il campo `inertia` rappresenta la somma delle distanze al quadrato dei campioni dal centro del cluster pi√π vicino, che non e' altro che la funzione obiettivo."},{"metadata":{"trusted":true},"cell_type":"code","source":"# funzione obiettivo\nk_means.inertia_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Poi ho dovuto occuparmi della rappresentazione dei cluster veri e propri. Per fare questo ho ritenuto, fosse utile, assegnare a ogni mese un colore differente, in modo da rendere piu' evidente i limiti dell'applicazione di K-MEANS, sul set di dati `COVID-19 in Italy`. Difatti, come si vede dalla figura sottostante, i centroidi dei cluster appaiono maggiormente dove si ha un numero di casi a incidenza ridotta (in prossimita' dello 0), con valore di regione 11,12,13. Questo perche', il COVID-19, ha avuto una prima maggiore incidenza nelle regioni del Nord (classificate con valori da 1 a 8), e le restanti sono state soggette a un susseguirsi di casi positivi in prossimita' dello 0, almeno fino al mese di Marzo inoltrato. Questo fa si che lo 0 sia la numerazione piu' diffusa e incidente all'interno dei dati."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pylab as plt\n\nplt.rcParams['figure.figsize'] = [10, 10]\n\n# la creazione dell'arraydei colori, con riferimento alla classe di appartenenza\ny_color = [];\npos = 0\nfor c in y:\n    if c==\"1\":\n        y_color.append(\"white\")\n    elif c==\"2\":\n        y_color.append(\"silver\")\n    elif c==\"3\":\n        y_color.append(\"black\")\n    elif c==\"4\":\n        y_color.append(\"purple\")\n    elif c==\"5\":\n        y_color.append(\"yellow\")\n    elif c==\"6\":\n        y_color.append(\"gray\")\n    elif c==\"7\":\n        y_color.append(\"lime\")\n    elif c==\"8\":\n        y_color.append(\"navy\")\n    elif c==\"9\":\n        y_color.append(\"aqua\")\n    elif c==\"10\":\n        y_color.append(\"green\")\n    elif c==\"11\":\n        y_color.append(\"fuchsia\")\n    elif c==\"12\":\n        y_color.append(\"orange\")\n        \n    pos = pos + 1\n\n\n# 0 id\n# 1 regione\n# 2 contagi\nf0, f1 = 1,2\nplt.scatter(X[:,f0], X[:,f1], marker = 'o', c = y_color)\nplt.scatter(k_means.cluster_centers_[:,f0], k_means.cluster_centers_[:,f1], marker = '^', c = \"red\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nIn conclusione, alla discussione dell'immagine sopra, il gruppo di centroidi in posizione centrale, sta cercando di rappresentare i mesi di febbraio e quelli estivi, che hanno avuto un numero basso di casi, ove pero' la sparsita' dello 0, nelle regioni del Sud, incide male sulla formazione dei cluster. Invece i primi centroidi fanno riferimento, alle regioni del Nord, che hanno avuto un'incidenza maggiore di casi sia nei primi mesi della pandemia che durante l'ultimo trimestre dell'anno.\n\nI centroidi rispecchiano poco la classificazione dei dati, in base ai mesi, proprio per:\n- la presenza di un numero elevato di casi di positivita' a 0 per molte regioni;\n- i numeri di positivi hanno un andamento a picchi, che cresce molto rapidamente.\n\nTali caratteristiche incidono negativamente sull'accuratezza della classificazione con l'uso dei cluster, come appare evidente anche dal calcolo di RandIndex che ho posto di seguito.\n\nRandIndex e' un criterio esterno che permette di valutare la qualita' della classificazione in rapporto al \"ground truth\", che in questo caso, e' rappresentato da `y`. Lo scopo di questo indice e' valutare per ogni coppia di esempi, se sono stati correttamente distribuiti nel cluster. Nella situazione ottima si vorrebbe che ogni cluster corrispondesse a una classe del ground truth, con relazioni mantenute.\\\nIn sklearn RandIndex e' implementato dal metodo `adjusted_rand_score`."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\nmetrics.adjusted_rand_score(k_means.labels_, y)# RandIndex","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Algoritmi gerarchici</h3>\nQuesta classe di algoritmi e' utile quando il numero di cluster K non e' dato a priori.\n\nFondamentale per fare clustering, con algoritmi gerarchici, e' la costruzione del dendogramma.\n\n* <h3>Costruzione del dendogramma</h3>\nCostruire un dendogramma significa costruire una tassonomia dagli insiemi di esempi contenuti nel dataset.\n\nCi sono 4 diverse tecniche che permettono la costruzione di un dendogramma:\n* *single-link*:  in questo caso la similarita' tra due classi, e' intesa come la similarita' tra gli esempi piu' simili. Essendo che l'obiettivo e' fondere i cluster con una similarita' simile, questi sono quelli con una distanza tra gli esempi, di cluster diversi, inferiore.\n* *complete-link*: in questo caso la misura di similarita' coincide con i cluster piu' dissimili. Dunque vengono fusi i cluster con una distanza tra gli esempi, di cluser diversi, maggiore.\n* *average-link*: questo rappresenta una soluzione intermedia a \"single-link\" e \"complete-link\". Per tutte le coppie all'interno di due cluster la similarita' e' la similarita' media tra gli esempi dei due cluster.\n* *centroid*: in questo caso, per ogni cluster, viene calcolato un centroide, che rappresenta la media, e vengono uniti a due a due i cluster che hanno i centroidi piu' simili.\n\nOgnuna di queste tecniche presenta dei vantaggi e degli svantaggi, che ho potuto testare anche all'interno del dataset `Covid-19 in Italy`. \n\nPer prima cosa ho proceduto con la costruzione dei dendogrammi per ognuno delle 4 tecniche; dopodiche' ho proceduto a scegliere, in ognuno di essi, quale fosse il valore di soglia ottimale, che mi avrebbe permesso l'individuazione di 11 componeti connesse (i mesi di rilevazione dei dati covid). In fine, per avere la stima della qualita' dei cluster (le componenti connesse) che ho mappato, ho calcolato come gia' avevo fatto per la tecnica K-MEANS, il criterio RandIndex, con la chiamata al metodo `adjusted_rand_score`. Questo mi ha permesso anche un confronto tra le categorie di algoritmi gerarchici e di partizionamento."},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.cluster.hierarchy as h","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4> Single-link, Average-link e Centroid</h4>"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = [10, 8]\n\n# single-link\nimport sys\nsys.setrecursionlimit(10000)\nlink_matrix1= h.single(X)\nh.dendrogram(link_matrix1)\nplt.show()\n\n# average-link\nlink_matrix2= h.average(X)\nh.dendrogram(link_matrix2)\nplt.show()\n\n# centroid\nlink_matrix3= h.centroid(X)\nh.dendrogram(link_matrix3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# single-link\nfcluster1 = h.fcluster(link_matrix1, 2950, criterion = \"distance\")\n\n# average-link\nfcluster2 = h.fcluster(link_matrix2, 13000, criterion = \"distance\")\n\n# centroid\nfcluster3 = h.fcluster(link_matrix3, 12500, criterion = \"distance\")\n\n# single-link\nprint(\"single-link: \", metrics.adjusted_rand_score(fcluster1, y))# RandIndex\n# average-link\nprint(\"average-link: \", metrics.adjusted_rand_score(fcluster2, y))# RandIndex\n# centroid\nprint(\"centroid: \", metrics.adjusted_rand_score(fcluster3, y))# RandIndex","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4>Complete-link</h4>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# complete-link\nlink_matrix= h.complete(X)\nprint(link_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I metodi `single`, `complete`, `centroid` e `average` ritornano una linkage matrix, in cui:\n- ogni riga rappresenta un merge dell'algoritmo;\n- le prime due colonne, indicano rispettivamente gli indici di una coppia che l'algoritmo unisce (quando un cluster e' identificato da un numero superiore rispetto al numero di esempi, contenuti nel dataset, allora e' un cluster che e' gia' stato fuso con un altro);\n- la terza colonna, indica la distanza tra i due cluster;\n- la quarta colonna, il numero di esempi totali, contenuti all'interno della coppia di cluster in esame.\n\nNella matrice `link_matix`(dunque con riferimento al metodo `complete`), per esempio, la prima riga indica, usualmente, che vengono fusi i custer 0 e 1; la distanza tra i due e' 1; ed essendo che sia 0 che 1 sono inferiori a 40201, numero totale di esempi del dataset `COVID-19 in Italy`, sono entrambi cluster singleton, e di conseguenza con un numero di esempi complessivo pari a 2. Invece, nell'ultima riga della matrice, usualmente, si hanno cluster con numerazioni superiori al numero di elementi del dataset, come 80398 e 80399 (perche' entrambi gia' fusi in precedenza) ecco che, non essendo piu' composti da un singolo esempio, hanno un numero di elementi superiore a 2.\n\nOsservando le matrici linkage risultanti, posso dire che e' dimostrato il rispetto della monotonia fra le distanza, esclusivamente nei casi average, single-link e complete-link. Tale peculiarita' mi fa escludere centroid come strategia ottimale per la costruzione del dendogramma."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.rcParams['figure.figsize'] = [20, 8]\n\nh.dendrogram(link_matrix)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fcluster = h.fcluster(link_matrix, 26000, criterion = \"distance\")\nprint(\"complete-link: \", metrics.adjusted_rand_score(fcluster, y))# RandIndex","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ai fini di questo lavoro, io preferisco l'impiego della tecnica `complete-link`. Di norma e' migliore usare la tecnica `average` in quanto non e' sensibile ne' agli outliers (svantaggio della tecnica complete) ne' tende a fare cluster allungati (chaining effect, svantaggio della tecnica single), ne' le distanze della matrice linkage si presentano non monotone (svantaggio della tecnica `centroid`). Tuttavia il dataset in analisi non presenta valori anomali, ma situazioni di crescita-picco-decrescita; e complice il fatto di una qualita' leggermente superiore, ho deciso di prediligere la tecnica complete.\n\nC'e' da dire che, in rapporto alla tecnica K-MEAN, qui la qualita' della classificazione risulta nettamente inferiore. E questo mi fa escludere l'uso proficuo di algoritmi gerarchici per la clusterizzazione del dataset `COVID-19 in Italy`. Tale risultato lo riconduco alla natura stessa del dataset; difatti per ogni regione si crea una curva di contagi che e' difficile categorizzare per mesi. Obiettivo invece della mia clusterizzazione."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}