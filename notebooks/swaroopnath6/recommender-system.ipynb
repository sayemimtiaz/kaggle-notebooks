{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Recommender Systems\n\nThis notebook serves the following purposes - \n1. It acts as a tutorial for anyone wishing to get a deep dive into Recommendation algorithms.\n2. Contrats between various techniques that are popularly used for Recommender Systems.\n3. Briefly discuss on the various evaluation metrics for Recommender Systems.\n\nThe techniques that will be discussed are - \n1. Collaborative Filtering: User-User based.\n2. Collaborative Filtering: Item-Item based.\n3. Latent Factor Method: SVD.\n\nI will be using the Amazon Product Reviews dataset in order to perform this comparison."},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"# Imports\nimport sys\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport time\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.neighbors import BallTree","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dataset = pd.read_csv('../input/amazon-ratings/ratings_Beauty.csv')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"print(dataset.info())\ndataset.head(4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset has 4 columns - UserID, ProductID, Rating and the Timestamp of rating. For the initial systems, we would be ignoring the Timestamp data. From this dataset, we would be creating a User-Item-Rating matrix. In order to test it in acceptable time, we will be using only 25000 rows of the data."},{"metadata":{},"cell_type":"markdown","source":"The first technique that we will follow is Collaborative Filtering using User similarity. We will use two similarity metric - Euclidean distance and Cosine similarity. Let's then analyze how the two fare against each other."},{"metadata":{},"cell_type":"markdown","source":"## Memory Based Collaborative Filtering"},{"metadata":{"trusted":false},"cell_type":"code","source":"class Recommender:\n    def __init__(self, strategy = 'user', neighbours = 10):\n        self.strategy = strategy\n        self.num_neighbours = neighbours\n        if strategy is 'user':\n            self.columns = ['User_' + str(index) for index in range(1, self.num_neighbours + 1)]\n        elif strategy is 'item':\n            self.columns = ['Item_' + str(index) for index in range(1, self.num_neighbours + 1)]\n        \n    def fit(self, matrix):\n        if self.strategy is 'user':\n            # User - User based collaborative filtering\n            start_time = time.time()\n            self.user_item_matrix = matrix\n            self.mapper_indices = matrix.index\n            self.user_tree = BallTree(matrix, leaf_size = self.num_neighbours * 2)\n            time_taken = time.time() - start_time\n            print('Model built in {} seconds'.format(time_taken))\n            return self\n        \n        elif self.strategy is 'item':\n            # Item - Item based collaborative filtering\n            start_time = time.time()\n            matrix = matrix.T\n            self.item_user_matrix = matrix\n            self.mapper_indices = matrix.index\n            self.item_tree = BallTree(matrix, leaf_size = self.num_neighbours * 2)\n            time_taken = time.time() - start_time\n            print('Model built in {} seconds'.format(time_taken))\n            return self\n            \n                    \n    def predict(self, X_test):\n        if self.strategy is 'user':\n            y_pred = pd.Series(index = X_test.index)\n            \n            for index in tqdm(X_test.index, desc = 'Predicting Ratings'):\n                row = X_test.loc[index]\n                target_user = row['UserId']\n                target_product = row['ProductId']\n                \n                if target_user not in self.user_item_matrix.index:\n                    y_pred[index] = 0\n                    continue\n                \n                user_attributes = self.user_item_matrix.loc[target_user]\n                _, neighbour_indices = self.user_tree.query(user_attributes.values.reshape(1, -1), k = self.num_neighbours)\n                \n                rating = 0\n                for neighbour_index in neighbour_indices:\n                    user = self.mapper_indices[neighbour_index]\n                    if target_product in self.user_item_matrix.loc[user].index:\n                        rating += self.user_item_matrix.loc[user, target_product]\n                    else:\n                        rating += 0\n                avg_rating = rating/self.num_neighbours\n                y_pred.loc[index] = avg_rating\n                \n            return y_pred.values\n        \n        elif self.strategy is 'item':\n            y_pred = pd.Series(index = X_test.index)\n            \n            for index in tqdm(X_test.index, desc = 'Predicting Ratings'):\n                row = X_test.loc[index]\n                target_user = row['UserId']\n                target_product = row['ProductId']\n                \n                if target_product not in self.item_user_matrix.index:\n                    y_pred[index] = 0\n                    continue\n                \n                item_attributes = self.item_user_matrix.loc[target_product]\n                _, neighbour_indices = self.item_tree.query(item_attributes.values.reshape(1, -1), k = self.num_neighbours)\n                \n                rating = 0\n                for neighbour_index in neighbour_indices:\n                    product = self.mapper_indices[neighbour_index]\n                    if target_user in self.item_user_matrix.loc[product].index:\n                        rating += self.item_user_matrix.loc[product, target_user]\n                    else:\n                        rating += 0\n                avg_rating = rating/self.num_neighbours\n                y_pred.loc[index] = avg_rating\n                \n            return y_pred.values\n        \n    def recommend_items(self, id, num_recommendations = 10):\n        if self.strategy is 'user':\n            user_id = id\n            \n            if user_id not in self.user_item_matrix.index:\n                # New user - We will be looking at this case later on\n                return None\n            \n            user_attributes = self.user_item_matrix.loc[user_id]\n            distances, neighbour_indices = self.user_tree.query(user_attributes.values.reshape(1, -1), k = self.num_neighbours + 1)\n            distances = distances[0]\n            neighbour_indices = neighbour_indices[0]\n            \n            # We will be scoring each product by the user's distance from the target user and the \n            # rating given by the user to the item.\n            recommendations = pd.DataFrame(columns = ['ProductId', 'Recommendability'])\n            \n            for index, neighbour_index in enumerate(neighbour_indices):\n                user = self.mapper_indices[neighbour_index]\n                user_similarity = 1 - distances[index]\n                products_with_ratings = self.user_item_matrix.loc[user]\n                \n                for product_id in products_with_ratings.index:\n                    recommendability = user_similarity * products_with_ratings.loc[product_id]\n                    recommendation = {'ProductId': product_id, 'Recommendability': recommendability}\n                    recommendations = recommendations.append(recommendation, ignore_index = True)\n            \n            recommendations.sort_values(by = 'Recommendability', ascending = False, inplace = True)\n            recommendations = recommendations[~recommendations.duplicated('ProductId')]\n            \n            max_recommendations = min(num_recommendations, recommendations.shape[0])\n            return recommendations.iloc[:max_recommendations, :-1]\n        \n        elif self.strategy is 'item':\n            product_id = id\n            \n            if product_id not in self.item_user_matrix.index:\n                # New product - We will be looking at this case later on\n                return None\n            \n            product_attributes = self.item_user_matrix.loc[product_id]\n            distances, neighbour_indices = self.item_tree.query(product_attributes.values.reshape(1, -1), k = num_recommendations)\n            distances = distances[0]\n            neighbour_indices = neighbour_indices[0]\n            \n            recommendations = pd.DataFrame(columns = ['ProductId', 'Recommendability'])\n            \n            for index, neighbour_index in enumerate(neighbour_indices):\n                product_id = self.mapper_indices[neighbour_index]\n                product_similarity = 1 - distances[index]\n                \n                recommendation = {'ProductId': product_id, 'Recommendability': product_similarity}\n                recommendations = recommendations.append(recommendation, ignore_index = True)\n            \n            recommendations.sort_values(by = 'Recommendability', ascending = False, inplace = True)\n            \n            return recommendations.iloc[1:, :-1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will be checking the predicting ability of both __User - User__ & __Item - Item__ based Collaborative Filtering at first. For that I would take those products which are rated an appreciable number of times (atleast 500 ratings).\n\n#### Objectives - \n- Check the effect of replacing NaN values with mean, with mode, with median and with 0.\n- Check which method gives best result for newly added users/items, and how do you apply them!"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let's look at the ratings count of the products\ngb_product = dataset.groupby('ProductId').size()\ngb_product = gb_product.sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(range(1, gb_product.shape[0] + 1), gb_product.values)\nplt.show()\nhigh_rated_products = gb_product[gb_product >= 500]\nplt.plot(range(1, high_rated_products.shape[0] + 1), high_rated_products.values)\nprint(high_rated_products.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are a total of 174 products that are rated above 500 times. In order to see good results, we will focus on these products only."},{"metadata":{"trusted":false},"cell_type":"code","source":"data_complete = dataset.loc[dataset['ProductId'].isin(high_rated_products.index)]\ndata = data_complete.iloc[:, :-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data_train, data_test, _, _ = train_test_split(data, np.zeros(data.shape[0]), test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"user_item_matrix_raw = pd.pivot_table(data_train, index = 'UserId', \n                                  columns = 'ProductId', values = 'Rating', aggfunc = np.sum)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"print(user_item_matrix_raw.shape)\nuser_item_matrix_raw.head(4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The training data has ~1 lakh users and 174 products. Let's look at the sparsity of the User-Item matrix."},{"metadata":{"trusted":false},"cell_type":"code","source":"sparsity = np.isnan(user_item_matrix_raw.values).sum()/np.prod(user_item_matrix_raw.shape)\nprint('The sparsity of the matrix is: {}'.format(sparsity))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This means that only 0.067% of the matrix has values."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Filling the NaN values with mean of the column\nuser_item_matrix = user_item_matrix_raw.fillna(user_item_matrix_raw.mean())\nuser_item_matrix.head(4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will be following two ways to build the recommender system - Directly using the ratings given in the dataset, and Assuming that the ratings given can be broken down into chunks of interpretable sections. An example would be - \nRating = Baseline Rating + User-Product-Interaction.\n\nHave you ever noticed, users usually tend to give an average rating of 3 to almost every product they buy. Visualize this by imagining viewing a product with rating 1 - Would you call it as being a bit good, or, maybe the product is very bad, hence it has a __bad__ rating!\n\nThis baseline rating is a term which can be removed from each and every rating, to accurately understand the relationship between user and the product.\n\nAlong with these two strategies, we would also be using two metrics - Similarity index and Euclidean distance."},{"metadata":{"trusted":false},"cell_type":"code","source":"user_item_matrix_rating = user_item_matrix.apply(lambda row: row - 3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## User - User CF"},{"metadata":{"trusted":false},"cell_type":"code","source":"recommender = Recommender().fit(user_item_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"recommender_rating = Recommender().fit(user_item_matrix_rating)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One very important observation is that using a Ball-Tree structure reduces training time significantly. Using a brute for KNN would take a hell lot more time for training these many samples.\n\nThe brute force KNN that I was using previously took 7 minutes to train on just 3200 user ids, when it was run parallely on 6 CPU cores with a batch size of 100. While this version, using KD Tree, trains on a sample of 100,000 in just ~30 seconds."},{"metadata":{"trusted":false},"cell_type":"code","source":"X_test = data_test.iloc[:, :-1]\ny_test = data_test.iloc[:, -1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Predicting using the two recommender models for normal as well as overhead rating \ny_pred = recommender.predict(X_test)\ny_pred_rating = recommender_rating.predict(X_test)\n\ny_pred_rating += 3\n\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nrmse_rating = np.sqrt(mean_squared_error(y_test, y_pred_rating))\n\nprint('RMSE using first perscpective: {}\\nRMSE using second perspective: {}'.format(rmse, rmse_rating))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation - The root mean square in predictions drastically reduce with the hypothesis we assumed.\n\nThe predictions are also done in a very impressive time - ~2 minutes for ~30000 user ids"},{"metadata":{},"cell_type":"markdown","source":"Using the rating that is devoid of baseline rating does seem to give better results than the plain simple system. There are a very important deduction from these experimental results - \n\n- It is better to predict the overhead rating (by ignoring the baseline rating), and then add the baseline rating. This means that hypothesis: \"People usually tend to rate some product at a baseline level (3 here) unless the product experience with the user is very bad\" is a better start off point for a Recommender system."},{"metadata":{},"cell_type":"markdown","source":"The second perspective is definitely better in producing results. But how can it be improved? Can there be a third segment in the rating? Yes, definitely!\n\nIntuition - Have you experienced yourself rating drivers of a particular application (Uber, in my case) more generously/harshly day by day? This is the third aspect in rating - a general user experience with the application. In the context of movies, you can imagine this as you being a harsher critic day by day. This kind of interaction (user - service) can be modelled using the timestamp provided. We will look at such interactions closely in later part of this notebook.\n\nIn general - \n$$Rating = Baseline Rating + User Service Interaction Rating + User Product Interaction Rating$$"},{"metadata":{},"cell_type":"markdown","source":"## Item - Item CF"},{"metadata":{"trusted":false},"cell_type":"code","source":"recommender_item = Recommender(strategy = 'item').fit(user_item_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"recommender_item_rating = Recommender(strategy = 'item').fit(user_item_matrix_rating)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Predicting using the two recommender models for normal as well as overhead rating \ny_item_pred = recommender_item.predict(X_test)\ny_item_pred_rating = recommender_item_rating.predict(X_test)\n\ny_item_pred_rating += 3\n\nrmse = np.sqrt(mean_squared_error(y_test, y_item_pred))\nrmse_rating = np.sqrt(mean_squared_error(y_test, y_item_pred_rating))\n\nprint('RMSE using first perscpective: {}\\nRMSE using second perspective: {}'.format(rmse, rmse_rating))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just as the observation in User - User based CF, we can see that the RMSE figures drop when we assume the baseline rating hypothesis. However, one contrasting feature that can be found here is that there is some difference between Cosine and Euclidean metric."},{"metadata":{},"cell_type":"markdown","source":"Now that we have built models and seen how they perform in various scenarios, let's see how they recommend stuff!"},{"metadata":{},"cell_type":"markdown","source":"## Recommendations\n\nThis is the important and the most interesting part of building Recommender Systems - recommending products for the users. Let's see how the four models above perform!\n\nWe will be using the User ID - ABQAIIBTTEKVM, and Product ID - B004OHQR1Q in order to compare the results."},{"metadata":{},"cell_type":"markdown","source":"### User - User CF Recommendations"},{"metadata":{"trusted":false},"cell_type":"code","source":"recommender.recommend_items('ABQAIIBTTEKVM')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"recommender_rating.recommend_items('ABQAIIBTTEKVM')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Item - Item CF Recommendations"},{"metadata":{"trusted":false},"cell_type":"code","source":"recommender_item.recommend_items('B004OHQR1Q')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"recommender_item_rating.recommend_items('B004OHQR1Q')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Explanation of the results.\n\nThere are two aspects in this explanation - explaining the difference between the RMSE values for the two perspectives, and getting the same set of recommendations inspite of the difference is predicted ratings.\n\n1. The reason for observing the difference in RMSE is that a value of 0 is predicted for items which the user has not rated (Item - Item CF) or the similar users who have not rated the item (User - User CF). If we interpret this value of 0 that is predicted, it can be understood that it has two meanings in the two perspectives. In the first perspective, where the ratings are kept between 0 and 5, a rating of 0 means that the user has absolutely disliked the product (on a relative scale). However, in the latter perspective, a rating of 0 implies that user has an average view of the product. If you try to imagine the situation, it would be similar to - first perspective says that a user who has not seen this product will absolutely dislike it, and second perspective says that the user would give it a baseline rating.\n    Looking at it this way clearly shows that the perspectives are actually similar, if we rate 3 for a product unseen by the user in context.\n    \n2. The reason for outputting same recommendations for both the perspective is that - Euclidean distance (used for finding neighbours in KDTree or BallTree) is invariant under a Translation Transformation (deducting the baseline transformation is essentially a translation of the points by 3 units). Hence, the neighbour still remain the same, which leads to the same recommendations being generated [all the time]."},{"metadata":{},"cell_type":"markdown","source":"### Varying statistic for NaN values\n\nFrom the above explanation, it is pretty clear that using a baseline rating for unintroduced User-Item pair is quite beneficial. Hence, instead of using mean as the filler for 'NaN' values, it would be better to use the baseline rating. It would not introduce any unwanted bias.\n\nGoing forward, I would be using only the second perspective, as it is already established that the two are similar and just differ in ratings they output for unintroduced User-Item pair."},{"metadata":{"trusted":false},"cell_type":"code","source":"user_item_matrix_baseline = user_item_matrix_raw.fillna(3) - 3\nuser_item_matrix_baseline.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"recommender_baseline = Recommender().fit(user_item_matrix_baseline)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred_baseline = recommender_baseline.predict(X_test)\ny_pred_baseline += 3\n\nrmse_baseline = np.sqrt(mean_squared_error(y_test, y_pred_baseline))\nprint('RMSE using User-User CF: {}'.format(rmse_baseline))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"recommender_item_baseline = Recommender(strategy = 'item').fit(user_item_matrix_baseline)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred_item_baseline = recommender_item_baseline.predict(X_test)\ny_pred_item_baseline += 3\n\nrmse_item_baseline = np.sqrt(mean_squared_error(y_test, y_pred_baseline))\nprint('RMSE using Item-Item CF: {}'.format(rmse_item_baseline))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The observation is that RMSE value doesn't change! Using the baseline rating is much more logical than using the mean value. More strategies for dealing with missing values would be dealt with while discussing Latent Factor Methods."},{"metadata":{},"cell_type":"markdown","source":"## Model based Collaborative Filtering"},{"metadata":{},"cell_type":"markdown","source":"The most prominent type of model based collaborative filtering is Latent Matrix Factorization. Usually SVD is used to factorize the user-item matrix into two matrices of user-embeddings and embeddings-item.\n\nBut what do embeddings mean? Well these are latent features that are obtained after factorization. For a User-Movie dataset, you can assume embbedings to be - a. How sci-fi the movie is? b. how recent is the movie? etc.\n\nThese are not the actual factors that are generated, infact no one knows the interpretation of the embeddings - precisely why the word Latent is used.\n\nI will be using the Surprise library in order to implement the SVD to obtain the Latent Factor Matrices."},{"metadata":{"trusted":false},"cell_type":"code","source":"class ModelRecommender:\n    def __init__(self, strategy = 'SVD', latent_factors = 5, num_epochs = 10, reg_param = 0.01):\n        self.strategy = strategy\n        self.latent_factors = latent_factors = 5\n        self.num_epochs = 10\n        self.reg_param = reg_param\n        self.learning_rate = 0.0005\n        \n    def fit(self, matrix):\n        m, n = matrix.shape\n        self.P = pd.DataFrame(np.random.rand(m, self.latent_factors), index = matrix.index) # Users\n        self.Q = pd.DataFrame(np.random.rand(n, self.latent_factors), index = matrix.columns) # Products\n        \n        users = list(matrix.index)\n        products = list(matrix.columns)\n\n        for epoch in tqdm(range(self.num_epochs), desc = 'Epoch'):\n            for user, product in zip(users, products):\n                error = matrix.loc[user, product] - self.predictions(self.P.loc[user].values, self.Q.loc[product].values)\n                self.P.loc[user] += self.learning_rate * (error * self.Q.loc[product].values - self.reg_param * self.P.loc[user].values)\n                self.Q.loc[product] += self.learning_rate * (error * self.P.loc[user].values - self.reg_param * self.Q.loc[product].values)\n                \n    def predictions(self, P, Q):\n        return np.dot(P, Q.T)\n    \n    def predict(self, X_test):\n        y_pred = pd.Series(index = X_test.index)\n        \n        for index, row in X_test.iterrows():\n            user_id = row['UserId']\n            product_id = row['ProductId']\n            if user_id not in self.P.index:\n                y_pred.loc[index] = 0\n                continue\n            if product_id not in self.Q.index:\n                y_pred.loc[index] = 0\n                continue\n            pred = self.predictions(self.P.loc[user_id].values, self.Q.loc[product_id].values)\n            y_pred.loc[index] = pred\n        \n        return y_pred.values\n    \n    def recommend(self, user_id, num_recommendations = 10):\n        recommendations = pd.DataFrame(columns = ['ProductId', 'Recommendability'])\n        \n        for product_id in self.Q.index:\n            recommendability = self.predictions(self.P.loc[user_id].values, self.Q.loc[product_id].values)\n            recommendations = recommendations.append({'ProductId': product_id, 'Recommendability': recommendability}, ignore_index = True)\n            \n        recommendations.sort_values(by = 'Recommendability', ascending = False, inplace = True)\n        \n        max_recommendations = min(num_recommendations, self.Q.shape[0])\n        return recommendations.iloc[:max_recommendations, 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Filling NaN values with baseline rating\nuser_item_matrix = user_item_matrix_raw.fillna(3)\nuser_item_matrix.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"user_item_matrix_rating = user_item_matrix - 3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Normal Rating"},{"metadata":{"trusted":false},"cell_type":"code","source":"recommender = ModelRecommender()\nrecommender.fit(user_item_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_test = data_test.iloc[:, :-1]\ny_test = data_test.iloc[:, -1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred = recommender.predict(X_test)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\n\nprint('RMSE using normal rating method: {}'.format(rmse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Baseline Rating"},{"metadata":{"trusted":false},"cell_type":"code","source":"recommender_rating = ModelRecommender()\nrecommender_rating.fit(user_item_matrix_rating)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred_rating = recommender.predict(X_test)\ny_pred_rating += 3\nrmse_rating = np.sqrt(mean_squared_error(y_test, y_pred_rating))\n\nprint('RMSE using baseline rating method: {}'.format(rmse_rating))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a very slight increase in the performance of the model in terms of predictions. What is noticeable that it has achieved this much performance in almost negligible time, as compared to the previous methods.\n\nLet's look at the recommendations provided by the method."},{"metadata":{"trusted":false},"cell_type":"code","source":"recommender.recommend('ABQAIIBTTEKVM', 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"recommender_rating.recommend('ABQAIIBTTEKVM', 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we can see that not only the results are different for the two perspectives, but also different from the ones predicted by User - User and Item - Item based recommendations. \n\nAlthough, I can't comment on the qualitative aspect of the recommendation, however, I am inclined to say that these predictions would be better (Please pardon my biasness)."},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}