{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from __future__ import unicode_literals, print_function\n\nimport numpy as np \nimport pandas as pd \nimport json\nimport glob\nimport itertools\nimport logging\n\n\nimport plac\nimport random\nfrom pathlib import Path\nimport spacy\nfrom spacy.util import minibatch, compounding\nfrom tqdm import tqdm, tqdm_notebook\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\npd.options.mode.chained_assignment = None  # default='warn'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will use provided dataset for training and testing our ner model.\n# It is easily possible to replace current testing data with search \n# engine articles for each question and get summary tables. \n\n\npath = r'../input/CORD-19-research-challenge/Kaggle/target_tables/1_population/'\nall_files = glob.glob(path + \"/*.csv\")\n\ntemp_df = []\nfor filename in all_files:\n    df = pd.read_csv(filename, index_col=None, header=0)\n    temp_df.append(df)\n\ndf_all_provided_summary_tables = pd.concat(temp_df, axis=0, ignore_index=True)\n# this df_all_provided_summary_tables dataframe will be using to train and test our NER model. \n# this dataframe contains all the pre summary tables curated by experts ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import metadata file where all the covid19 research paper metadata stored\ndf_metadata = pd.read_csv('../input/CORD-19-research-challenge/metadata.csv',\n                          low_memory=False)\n# there are some rows which contains multiple entry for location of articles\n# we will remove those and keep first\ndf_metadata.pdf_json_files = df_metadata.pdf_json_files.apply(\n    lambda x: x.split(';')[0] if pd.notnull(x) else x)\ndf_metadata.pmc_json_files = df_metadata.pmc_json_files.apply(\n    lambda x: x.split(';')[0] if pd.notnull(x) else x)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n    try:\n        training_data = []\n        lines=[]\n        with open(dataturks_JSON_FilePath, 'r') as f:\n            lines = f.readlines()\n\n        for line in lines:\n            data = json.loads(line)\n            text = data['content']\n            entities = []\n            for annotation in data['annotation']:\n                #only a single point in text annotation.\n                point = annotation['points'][0]\n                labels = annotation['label']\n                # handle both list of labels or a single label.\n                if not isinstance(labels, list):\n                    labels = [labels]\n\n                for label in labels:\n                    #dataturks indices are both inclusive [start, end] but spacy is not [start, end)\n                    entities.append((point['start'], point['end'] + 1 ,label))\n\n\n            training_data.append((text, {\"entities\" : entities}))\n\n        return training_data\n    except Exception as e:\n        print(str(e))\n        logging.exception(\"Unable to process \" + dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n        return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_raw_articles_by_title(provided_titles, metadata,\n                articles_base_location='../input/CORD-19-research-challenge/'):\n    '''\n    get raw articles by title\n    \n    \n    provided_titles: string\n    metadata: dataframe\n    \n    return: datframe\n        'title', 'doi', 'publish_time', 'journal', 'url', 'abstract', 'body_text'\n    '''\n    methods = ['methods','method','statistical methods','materials',\n               'materials and methods','data collection','the study',\n               'study design','experimental design','objective',\n               'objectives','procedures','data collection and analysis',\n               'methodology','material and methods','the model',\n               'experimental procedures','main text']\n    \n    metadata_filtered = metadata.loc[metadata.title.isin(provided_titles)]\n    \n    # replace empty pdf_json_files column with pmc_json_files column value\n    metadata_filtered['pdf_json_files'] = \\\n    metadata_filtered.pdf_json_files.fillna(metadata_filtered.pmc_json_files)\n    # drop those rows that doesn't have location of articles \n    metadata_filtered = metadata_filtered.dropna(subset=['pdf_json_files'])\n    # create articles location for reading articles\n    metadata_filtered['articles_location'] = articles_base_location \\\n    + metadata_filtered['pdf_json_files']\n    \n    metadata_filtered['body_text'] = '' # create a column for articles body text\n    metadata_filtered['methods'] = ''\n    metadata_filtered['results'] = ''\n    # fill body_text column\n    for index, row in metadata_filtered.iterrows():\n        temp_body_text = ''\n        temp_methods = ''\n        temp_results = ''\n        with open(row['articles_location']) as file:\n            content = json.load(file)\n            for entry in content['body_text']:\n                temp_body_text = temp_body_text + entry['text']\n            # Methods\n            for entry in content['body_text']:\n                section_title = ''.join(\n                    x.lower() for x in entry['section'] \\\n                    if x.isalpha()) #remove numbers and spaces\n                if any(m in section_title for m in [''.join(\n                    x.lower() for x in m \\\n                    if x.isalpha()) for m in methods]) : \n                    temp_methods = temp_methods + entry['text']\n            # Results\n            results_synonyms = ['result', 'results']\n            for entry in content['body_text']:\n                section_title = ''.join(x.lower() for x in entry['section'] \\\n                                        if x.isalpha())\n                if any(r in section_title for r in results_synonyms) :\n                    temp_results = temp_results + entry['text']\n                    \n        metadata_filtered.at[index, 'body_text'] = temp_body_text\n        metadata_filtered.at[index, 'methods'] = temp_methods\n        metadata_filtered.at[index, 'results'] = temp_results\n        \n    metadata_filtered = metadata_filtered.rename(\n        columns={'title': 'Study', 'publish_time': 'Date'})\n    return metadata_filtered[['Study', 'doi', 'Date',\n                              'journal', 'url', 'abstract',\n                              'methods', 'results', 'body_text']]\n\n\n\n\ndef preprocess_articles(raw_articles_dataframe):\n    '''\n    clean abstract, body text for performance\n    \n    raw_articles_dataframe: dataframe\n        this dataframe should contain articles abstract,\n        methods, results, body_text.\n        ideal dataframe is the return of \n        get_raw_articles_by_title() function\n    \n    '''\n    raw_articles_dataframe['abstract'] = \\\n    raw_articles_dataframe['abstract']\\\n    .fillna(raw_articles_dataframe.body_text.str[:1500])\n    \n    raw_articles_dataframe['shorten_full_article_text'] = \\\n    raw_articles_dataframe['Study'] \\\n    + \"\\n\\n\" + raw_articles_dataframe['abstract'] \\\n    + \"\\n\\n\" + raw_articles_dataframe['methods'] \\\n    + \"\\n\\n\" + raw_articles_dataframe['results']\n    \n    \n    # remove (), [] and all text between baraces and normalize whitespace\n    raw_articles_dataframe['shorten_full_article_text'] = \\\n    raw_articles_dataframe['shorten_full_article_text']\\\n    .str.replace(r\"\\s*([\\(\\[]).*?([\\)\\]])\",\"\").str.strip()\n    \n    # remove all urls from text\n    raw_articles_dataframe['shorten_full_article_text'] = \\\n    raw_articles_dataframe['shorten_full_article_text']\\\n    .str.replace(r\"http\\S+|www.\\S+\",\"\").str.strip()\n    \n    # remove all single digit number\n    raw_articles_dataframe['shorten_full_article_text'] = \\\n    raw_articles_dataframe['shorten_full_article_text']\\\n    .str.replace(r\"(?<!\\d)[1-7]\\b\",\"\").str.strip()\n    \n    \n    \n    \n\n    \n    return raw_articles_dataframe\n\n\ndef generate_articles_for_annotation(processed_articles_dataframe):\n    '''\n    this function generate text for annotation\n    input is the dataframe that contains process full articles text\n    this function just make .txt file for each row of processed\n    dataframe in preprocess_articles() function\n    \n    note: hold some data from processed_articles_dataframe before providing\n    to this function and you can use it in later for testing the model\n    \n    '''\n    temp = pd.DataFrame(columns=['articles'])\n    temp['articles'] = processed_articles_dataframe['shorten_full_article_text']\n    temp = temp.dropna()\n    \n    temp = temp.reset_index(drop=True)\n    \n    file = './{}.txt'\n    for i, row in temp.iterrows():\n        with open(file.format(str(i)), 'w') as f:\n            f.write(str(row['articles']))\n            \n    return \"TEXT SAVE TO WORKING DIRECTORY\"\n\ndef training_ner_model(training_data, model=None, output_dir='./', n_iter=500):\n    TRAIN_DATA = training_data.copy()\n    print('Training started...')\n    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n    if model is not None:\n        nlp = spacy.load(model)  # load existing spaCy model\n        print(\"Loaded model '%s'\" % model)\n    else:\n        nlp = spacy.blank(\"en\")  # create blank Language class\n        print(\"Created blank 'en' model\")\n\n    # create the built-in pipeline components and add them to the pipeline\n    # nlp.create_pipe works for built-ins that are registered with spaCy\n    if \"ner\" not in nlp.pipe_names:\n        ner = nlp.create_pipe(\"ner\")\n        nlp.add_pipe(ner, last=True)\n    # otherwise, get it so we can add labels\n    else:\n        ner = nlp.get_pipe(\"ner\")\n\n    # add labels\n    for _, annotations in TRAIN_DATA:\n        for ent in annotations.get(\"entities\"):\n            ner.add_label(ent[2])\n\n    # get names of other pipes to disable them during training\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n    with nlp.disable_pipes(*other_pipes):  # only train NER\n        # reset and initialize the weights randomly â€“ but only if we're\n        # training a new model\n        if model is None:\n            nlp.begin_training()\n        for itn in tqdm_notebook(range(n_iter)):\n            random.shuffle(TRAIN_DATA)\n            losses = {}\n            # batch up the examples using spaCy's minibatch\n            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(\n                    texts,  # batch of texts\n                    annotations,  # batch of annotations\n                    drop=0.5,  # dropout - make it harder to memorise data\n                    losses=losses,\n                )\n    print('Training completed.')\n    return nlp\n\n\n# function for extracting data field value\ndef _ner_apply(text, ner_model):\n    # pass our text to spacy\n    # it will return us doc (spacy doc)\n    doc = ner_model(text)\n    # return list of tuples look like \n    # this [('four-year', 'EDUCATION_YEARS'), ('college or university', 'SCHOOL_TYPE')]\n    return [(ent.text, ent.label_) for ent in doc.ents]\n\ndef ner_extraction(model, processed_articles):\n    \n    \"\"\"\n    return full extracted dataset \n    \"\"\"\n    temp = processed_articles.copy()\n    # apply the function and store the result in a new column \n    temp['temp_entity'] = temp['shorten_full_article_text'].apply(lambda x: _ner_apply(x, ner_model=model))\n\n\n    # process our data field column and seperate each column and store their value in their column\n    flatter = sorted([list(x) + [idx] for idx, y in enumerate(temp['temp_entity']) \n                      for x in y], key = lambda x: x[1]) \n\n    # Find all of the values that will eventually go in each F column                \n    for key, group in itertools.groupby(flatter, lambda x: x[1]):\n        list_of_vals = [(val, idx) for val, _, idx in group]\n\n        # Add each value at the appropriate index and F column\n        for val, idx in list_of_vals:\n            temp.loc[idx, key] = val\n    return temp\n\n\n\n# get_raw_articles_by_title(df_all_provided_summary_tables.Study.tolist(), df_metadata)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"annotation_for_training = convert_dataturks_to_spacy('../input/covid19-annotation/Cord19_1_population_annotation.json')\nner_model = training_ner_model(annotation_for_training)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ner_model.to_disk('./task_1_ner_model')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}