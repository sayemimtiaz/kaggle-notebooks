{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Problem Statement\nA bike-sharing system is a service in which bikes are made available for shared use to individuals on a short term basis for a price or free. Many bike share systems allow people to borrow a bike from a \"dock\" which is usually computer-controlled wherein the user enters the payment information, and the system unlocks it. This bike can then be returned to another dock belonging to the same system.\n\n\nA US bike-sharing provider BoomBikes has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state. \n\n\nIn such an attempt, BoomBikes aspires to understand the demand for shared bikes among the people after this ongoing quarantine situation ends across the nation due to Covid-19. They have planned this to prepare themselves to cater to the people's needs once the situation gets better all around and stand out from other service providers and make huge profits.\n\n\nThey have contracted a consulting company to understand the factors on which the demand for these shared bikes depends. Specifically, they want to understand the factors affecting the demand for these shared bikes in the American market. The company wants to know:\n\nWhich variables are significant in predicting the demand for shared bikes.\nHow well those variables describe the bike demands\nBased on various meteorological surveys and people's styles, the service provider firm has gathered a large dataset on daily bike demands across the American market based on some factors. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Step 1: Reading and Understanding the Data\n\nLet's start with the following steps:\n\n1. Importing data using the pandas library\n2. Understanding the structure of the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Supress Warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the numpy and pandas package\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', 500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the given CSV file, and view some sample records\n\nbike = pd.read_csv('../input/us-boombike/day.csv')\nbike.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's inspect the various aspects of our dataframe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Determining the number of rows and columns\nbike.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#summary of all the numeric columns in the dataset\nbike.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Datatypes of each column\nbike.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking missing values\nbike.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No missing values in the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Rename the columns for better understanding\nbike.rename(columns = {'yr':'Year','mnth':'month','hum':'humidity','cnt':'count'}, inplace = True) \nbike.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Mapping variables season, month, weathersit, weekday\n\nbike['season']=bike.season.map({1: 'spring', 2: 'summer',3:'fall', 4:'winter' })\nbike['month']=bike.month.map({1:'Jan',2:'Feb',3:'Mar',4:'Apr',5:'May',6:'June',7:'July',8:'Aug',9:'Sep',10:'Oct',11:'Nov',12:'Dec'})\nbike['weathersit']=bike.weathersit.map({1: 'Clear',2:'Mist + Cloudy',3:'Light Snow',4:'Snow + Fog'})\nbike['weekday']=bike.weekday.map({0:'Sun',1:'Mon',2:'Tue',3:'Wed',4:'Thu',5:'Fri',6:'Sat'})\n\nbike.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2: Data Visualisation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Identify Continuous and Categorical Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# I can check the number of unique values is a column\n# If the number of unique values <=40: Categorical column\n# If the number of unique values in a columns> 50: Continuous\n\nbike.nunique().sort_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualising Numeric Variables\n\nLet's make a pairplot of all the numeric variables,  to visualise which variables are most correlated to the target variable 'count'.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pairplot for numeric variables\nsns.pairplot(bike, vars=[\"temp\", \"humidity\",'casual','windspeed','registered','atemp','count','instant'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By analysing all the plots above, we can see that there are some independent variables look positively correlated to the 'count' variable. \n-  Bike rentals are more correlated to temperature\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Visualising Categorical Variables\n\nAs you might have noticed, there are a few categorical variables as well. Let's make a boxplot for some of these variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"##Relationship between categorical and continuous variable\nplt.figure(figsize=(20, 12))\nplt.subplot(2,4,1)\nsns.boxplot(x = 'Year', y = 'count', data = bike)\nplt.subplot(2,4,2)\nsns.boxplot(x = 'holiday', y = 'count', data = bike)\nplt.subplot(2,4,3)\nsns.boxplot(x = 'workingday', y = 'count', data = bike)\nplt.subplot(2,4,4)\nsns.boxplot(x = 'month', y = 'count', data = bike)\nplt.subplot(2,4,5)\nsns.boxplot(x = 'weathersit', y = 'count', data = bike)\nplt.subplot(2,4,6)\nsns.boxplot(x = 'season', y = 'count', data = bike)\nplt.subplot(2,4,7)\nsns.boxplot(x = 'weekday', y = 'count', data = bike)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plots above shows the relationship between categorical variables and a Target variable.  \n- Bike Rentals are more during the Fall season and then in summer\n- Bike Rentals are more in the year 2019 compared to 2018\n- Bike Rentals are more in partly cloudy weather\n- Bike Rentals are more on Saturday,wednesday and thursday\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Lets understand the variables better**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 1: Season","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Barplot to see relation between season and count of bike rentals\nsns.barplot('season','count',data=bike,palette=\"rocket\",)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Bike Rentals are more during the Fall season and then in summer","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 2: Weathersit","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Relation between weather and count of bike rentals\nsns.barplot('weathersit','count',palette=\"muted\",data=bike)\nplt.show()\n           ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Bike Rentals are more in partly cloudy weather","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 3: Year","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### year (0: 2018, 1:2019)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Relation between Year and count of bike rentals\nsns.barplot('Year','count',data=bike)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Bike rentals are more in the year 2019 compared to 2018","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 4: Month","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Relation between month and \nplt.figure(figsize=(10,5))\nsns.barplot('month','count',hue='Year',data=bike,palette='Paired')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Bike Rentals are more in the year 2019 compared to 2018","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 5: Temperature","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#scatter plot for temperature to count\nsns.scatterplot(x='temp',y='count' ,data=bike)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Bike Rentals are observed at higher temperatures","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 6: Humidity","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x='humidity', y='count',data=bike)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Bike rentals more at high humidity","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Heatmap ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Heatmap to see correlation between variables\nplt.figure(figsize=(25, 12))\nsns.heatmap(bike.corr(), cmap='RdYlGn', annot = True)\nplt.title(\"Correlation between Variables\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As is visible from the pairplot and the heatmap, we can see temp, atemp, casual,registered,instant variables are correlated to 'count' variable\n- We can also see some other variables are also most correlated.\n- **Both the plots above helps to interpret the data well and identify the variables that can turn out to be useful in building the model**\n- **So yes we can consider a Linear Regression Model.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Drop the unnecessary variables from the dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We can see the dataset has some variables that are not required. \n**We can drop instant, dteday, casual, registered**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop unnecessary columns\nbike=bike.drop(['instant','dteday','casual', 'registered','atemp'], axis=1)\nbike.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check the datatypes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking datatypes of all the columns\nbike.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 3: Data Preparation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Dummy variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Get the dummy variables for month, season, weathersit, weekday and Let's drop the first column from  using 'drop_first = True'pd.get_dummies(bike.season,drop_first=True)\nmonths=pd.get_dummies(bike.month,drop_first=True)\nweekdays=pd.get_dummies(bike.weekday,drop_first=True)\nweather_sit=pd.get_dummies(bike.weathersit,drop_first=True)\nseasons=pd.get_dummies(bike.season,drop_first=True)\n\n#bike=pd.concat([seasons,bike], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add the results to the original bike dataframe\nbike=pd.concat([months,weekdays,weather_sit,seasons,bike],axis=1)\nbike.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop 'season','month','weekday','weathersit' as we have created the dummies for it\nbike.drop(['season','month','weekday','weathersit'], axis = 1, inplace = True)\nbike.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Number of rows and columns\nbike.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now lets check the correlation between variables again\n#Heatmap to see correlation between variables\nplt.figure(figsize=(25, 20))\nsns.heatmap(bike.corr(), cmap='YlGnBu', annot = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that temperature,Summer season,June to october months are in good correlation with the 'count' variable. And seem to have good influence on the number of bike rentals.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Step 4: Splitting the Data into Training and Testing Sets","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Before model building, you first need to perform the test-train split and scale the features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# We specify this so that the train and test data set always have the same rows, respectively\n#np.random.seed(0)\nbike_train, bike_test = train_test_split(bike, train_size = 0.7, random_state = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Rows and columns after split\nprint(bike_train.shape)\nprint(bike_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Rescaling the Features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"It is important to have all the variables on the same scale for the model to be easily interpretable. \nWe can use standardization or normalization so that the units of the coefficients obtained are all on the same scale. \n- **There are two common ways of rescaling:**\n\n- Min-Max scaling (Normalisation):Between 0 and 1\n- Standardisation :mean-0, sigma-1\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### Min-Max scaling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normalisation = (x-xmin)/(x max-x min)\n#Standardisation= (x-mu)/ sigma\n#import the library\nfrom sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Instantiate an object\nscaler = MinMaxScaler()\n\n#Create a list of numeric variables\nnum_vars=['temp','humidity','windspeed','count']\n\n#Fit on data\nbike_train[num_vars] = scaler.fit_transform(bike_train[num_vars])\nbike_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking numeric variables(min and max) after scaling\nbike_train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**All the numeric variables are now mapped between 0 and 1**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Multicollinearity: Assumes that predictors are not correlated with each other. If there is\n                       correlation among the predictors","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the correlation coefficients to see which variables are highly correlated after scaling\n#Little to no multicollinearity among predictors\n\nplt.figure(figsize=(25, 20))\nsns.heatmap(bike_train.corr(),cmap='YlOrRd',annot = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the heatmap we can see temp, year are correlated to the 'count' variable. Even in August, September months we can see the counts are little high.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Dividing into X and Y sets for the model building","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Divide the data into X and y\ny_train = bike_train.pop('count')\nX_train = bike_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 5: Building a linear model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\nWe will be using the **LinearRegression function from SciKit Learn** for its compatibility with RFE (which is a utility from sklearn)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### RFE","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Recursive Feature Elimination**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing RFE and LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running RFE with the output number of the variable equal to 15\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, 15)             # running RFE\nrfe = rfe.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#List of variables selected\nlist(zip(X_train.columns,rfe.support_,rfe.ranking_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Columns where RFE support is True\ncol = X_train.columns[rfe.support_]\ncol","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Columns where RFE support is False\nX_train.columns[~rfe.support_]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building model using statsmodel, for the detailed statistics","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating X_test dataframe with RFE selected variables\nX_train_rfe = X_train[col]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_rfe = sm.add_constant(X_train_rfe)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running the linear model \nlm = sm.OLS(y_train,X_train_rfe).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking VIF\n\nVariance Inflation Factor or VIF, gives a basic quantitative idea about how much the feature variables are correlated with each other. It is an extremely important parameter to test our linear model. The formula for calculating `VIF` is:\n\n### $ VIF_i = \\frac{1}{1 - {R_i}^2} $","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop the constant term B0\nX_train_rfe = X_train_rfe.drop(['const'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_rfe\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**January is insignificant in presence of other variables due to high p-value and low VIF; can be dropped**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop January\nX_train_new1 = X_train_rfe.drop([\"Jan\"], axis = 1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Rebuilding the model without 'Jan'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Build a model\nX_train_lm1 = sm.add_constant(X_train_new1)\nlm1 = sm.OLS(y_train,X_train_lm1).fit()\nprint(lm1.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop the constant term B0\nX_train_lm1 = X_train_lm1.drop(['const'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the VIFs for the new model\nvif = pd.DataFrame()\nX = X_train_new1\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**'humidity' variable can be dropped as its insignificant by looking at very high  VIF**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### Rebuilding the model without 'humidity'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop humidity\nX_train_new2 = X_train_lm1.drop([\"humidity\"], axis = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Build a model\nX_train_lm2 = sm.add_constant(X_train_new2)\nlm2 = sm.OLS(y_train,X_train_lm2).fit()\nprint(lm2.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop the constant\nX_train_lm2=X_train_lm2.drop(['const'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the VIFs for the new model\nvif = pd.DataFrame()\nX = X_train_new2\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Holiday variable seems to be insignificant, by looking at p value and low  VIF. We can drop it**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### Rebuliding the model without holiday","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop the holiday column\nX_train_new3=X_train_lm2.drop(['holiday'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding a constant variable \nX_train_lm3 = sm.add_constant(X_train_new3)\nlm3 = sm.OLS(y_train,X_train_lm3).fit()\nprint(lm3.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop constant \nX_train_lm3=X_train_lm3.drop(['const'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the VIFs for the new model\nvif = pd.DataFrame()\nX = X_train_new3\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Windspeed seems to be insignificant,by looking at  high VIF and negative correlation with count.  Lets drop it**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### Rebuilding the model without windspeed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop July\nX_train_new4= X_train_lm3.drop(['windspeed'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Build a model\nX_train_lm4=sm.add_constant(X_train_new4)\nlm4=sm.OLS(y_train,X_train_lm4).fit()\nprint(lm4.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop constant\nX_train_lm4= X_train_lm4.drop(['const'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the VIFs for the new model\nvif = pd.DataFrame()\nX =X_train_new4\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**July column can be dropped due to its p value and low VIF**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### Rebuilding the model without July","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop July\nX_train_new5=X_train_lm4.drop(['July'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Building a model\nX_train_lm5= sm.add_constant(X_train_new5)\nlm5=sm.OLS(y_train,X_train_lm5).fit()\nprint(lm5.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop the constant\nX_train_lm7=X_train_lm5.drop(['const'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the VIFs for the new model\nvif = pd.DataFrame()\nX = X_train_new5\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Yes! Now we can see we have our model.\n\n##### The p values represent the significance of the variables and VIF which represent how variables are correlated to each other. Based on these two parameters we decided which variable to drop.\n\n##### The VIFs and p-values both are within an acceptable range. So we go ahead and make our predictions using this model only.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**- The p-value for each term tests the null hypothesis that the coefficient is equal to zero (no effect). A low p-value (< 0.05) indicates that you can reject the null hypothesis.**\n\n**-  A rule of thumb commonly used in practice is if a VIF is > 10, you have high multicollinearity. In our case, with values less than 5, we are in good shape, and can proceed with our regression**\n\n**- R-squared measures the strength of the relationship between your model and the dependent variable on a convenient 0 â€“ 100% scale. And we have the R-square value of 0.826 or 82.6%**\n\n**- The adjusted R-squared adjusts for the number of terms in the model. And we got it around 0.82 or 82%**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### **Still lets check one more model by dropping temp keeping windspeed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop temp for the lm4 model \nX_train_new6=X_train_lm4.drop(['temp'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Building a model\nX_train_lm6= sm.add_constant(X_train_new6)\nlm6=sm.OLS(y_train,X_train_lm6).fit()\nprint(lm6.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Here we can see there is a huge drop on R-square and adjusted R-squared. So this wont be a good model.**\n### So our model is lm5 which is obtained by removing January, windspeed, holiday, July and humidity variables from the RFE support columns**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Step 6: Residual Analysis of the train data\n\nSo, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train_lm5=sm.add_constant(X_train_lm5)\n#X_train_lm5.columns\nX_train_lm5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y train predicted\ny_train_pred = lm5.predict(X_train_lm5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the required libraries for plots.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the histogram of the error terms\n\nfig = plt.figure()\nplt.figure(figsize=(14,7))\nsns.distplot((y_train - y_train_pred), bins = 20)\nplt.title('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)  # X-label\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can see Error terms are normally distributed**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Step 7: Making Predictions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Applying the scaling on the test sets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a list of numeric variables\nnum_vars=['temp','humidity','windspeed','count']\n\n#Fit on data\nbike_test[num_vars] = scaler.transform(bike_test[num_vars])\nbike_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Dividing into X_test and y_test\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dividing into X_test and y_test\ny_test = bike_test.pop('count')\nX_test = bike_test\nX_test.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Columns\nX_train_new5.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's use our model to make predictions.\n\n# Creating X_test_new dataframe by dropping variables from X_test\nX_test_new = X_test[X_train_new5.columns]\n\n# Adding a constant variable \nX_test_new1 = sm.add_constant(X_test_new)\nX_test_new1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making predictions\ny_pred = lm5.predict(X_test_new1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finding R-squared and Adjusted R-Squared for Test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluate R-square for test\nfrom sklearn.metrics import r2_score\nr2_score(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Adjusted R^2\n#adj r2=1-(1-R2)*(n-1)/(n-p-1)\n\n#n =sample size , p = number of independent variables\n\nAdj_r2=1-(1-0.8115083)*(11-1)/(11-1-1)\nprint(Adj_r2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 8: Model Evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting y_test and y_pred to understand the spread.\nfig = plt.figure()\nplt.figure(figsize=(15,8))\nplt.scatter(y_test,y_pred,color='blue')\nfig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16)                          # Y-label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Visualising the fit on the test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Regression plot\nplt.figure(figsize=(15,8))\nsns.regplot(x=y_test, y=y_pred, ci=68, fit_reg=True,scatter_kws={\"color\": \"blue\"}, line_kws={\"color\": \"red\"})\n\nplt.title('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16)                          # Y-label\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nWe can see that the equation of our best fitted line is:\n\n$ count=         0.4914 \\times temp+   0.0916   \\times September + 0.0645 \\times Saturday +0.0527 \\times summer + 0.0970 \\times winter + 0.2334 \\times Year + 0.0566 \\times working day   - 0.03041 \\times light snow - 0.0786 \\times mist cloudy -0.065 \\times spring $","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Final Result Comparison between Train model and Test: ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### - Train R^2 :  0.826\n\n### - Train Adjusted R^2 : 0.82\n\n### - Test R^2: 0.8115\n\n### - Test Adjusted R^2: 0.790564\n\n### - Difference in R^2 between train and test: 1.5%\n\n### - Difference in adjusted R^2 between Train and test: 3.15% which is less than 5%\n\n## Yes! Its a best model\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Interpretation:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### - We arrived at a very decent model for the  the demand for shared bikes with the significant variables\n\n### -  We can see that temperature variable is having the highest coefficient 0.4914, which means if the temperature increases by one unit the number of bike rentals increases by 0.4914 units.\n\n### Similary we can see coefficients of other variables in the equation for best fitted line.\n\n### We also see there are some variables with negative coefficients, A negative coefficient suggests that as the independent variable increases, the dependent variable tends to decrease. We have spring, mist cloudy , light snow variables with negative coefficient.  The coefficient value signifies how much the mean of the dependent variable changes given a one-unit shift in the independent variable while holding other variables in the model constant.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Business Goals:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### -  A US bike-sharing provider BoomBikes can focus more on Temperature\n\n### -  We can see demand for bikes was more in 2019 than 2018, so just focus as there is increase in 2019 and might be facing dips in their revenues due to the ongoing Corona pandemic and by the time it reduces the things will be better\n\n### - Can focus more  on Summer & Winter season, August, September month, Weekends, Working days as they have good influence on bike rentals.\n\n### - We can see spring season has negative coefficients and negatively correlated to bike rentals. So we can give some offers there to increase the demand\n\n### -  Now seeing to weathersit variable, we have got negative coefficients for Mist +cloudy and Lightsnow weather... And yes we can give offers","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}