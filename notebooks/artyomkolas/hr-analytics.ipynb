{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Who will move to a new job?"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### train data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"path = '../input/hr-analytics-job-change-of-data-scientists/aug_train.csv'\ntrain_data = pd.read_csv(path)\ntrain_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/hr-analytics-job-change-of-data-scientists/aug_test.csv'\ntest_data = pd.read_csv(path)\ntest_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### sample_submission.csv"},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/hr-analytics-job-change-of-data-scientists/sample_submission.csv'\ngender_submission_data = pd.read_csv(path)\ngender_submission_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data overview, cleaning and preprocessing"},{"metadata":{},"cell_type":"markdown","source":"### Contact for joint processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([train_data, test_data], ignore_index=True)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['target'].replace(to_replace=0,  value='no', inplace=True)\ndf['target'].replace(to_replace=1,  value='yes', inplace=True)\ndf['target'] = df['target'].astype('object')\ndf['target'].value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nplt.figure(figsize=(20, 30))\nsns.heatmap(df.isnull(), cbar=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def NaN_info(df):\n    global null_view\n    try:\n        null_view = df[[col for col in df.columns if df[col].isna().sum() > 0]].isna().sum().sort_values(ascending = True)\n        null_view = pd.DataFrame(null_view, columns=['NANs'])\n        null_view[['PERCENT']] = null_view.NANs.apply(lambda x: round((x/len(df))*100, 2))\n        null_view[['TYPE']] = df.dtypes\n    except:\n        return null_view\n    return null_view\n\nNaN_info(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-Processing and Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for el in list(df.columns):\n    print(f'======================= {el} =======================')\n    print(df[el].value_counts(dropna=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indexes = list(df.index)\nfor el in indexes:\n    city = df.loc[el, 'city']\n    city = city.split(sep='_')    \n    df.loc[el, 'city_num'] = int(city[1])\n    \ndf.city_num = df.city_num.astype('int64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['experience'].replace(to_replace='>20', value=np.NaN, inplace=True)\ndf['experience'].replace(to_replace='<1', value=0.5, inplace=True)\ndf['experience'] = df['experience'].astype('float64')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create feature with experience title"},{"metadata":{"trusted":true},"cell_type":"code","source":"def change(x):\n    if x < 1:\n        x = 'trainee'\n    elif x < 2:\n        x = 'junior'\n    elif x < 5:\n        x = 'middle'\n    elif x < 8:\n        x = 'senior'\n    elif x < 15:\n        x = 'master'\n    elif x > 14:\n        x = 'grandmaster'\n    else:\n        np.nan\n    return x\n\ndf['experience_cat'] = df['experience'].apply(change)\n\ndf[0:10][['experience','experience_cat']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"change={\n        '<10':10, \n        '10/49':50, \n        '50-99':100, \n        '100-500':500, \n        '500-999':1000, \n        '1000-4999':5000, \n        '5000-9999':10000, \n        '10000+':100000,\n        }\ndf['company_size_num'] = df['company_size'].map(change)\ndf[0:10][['company_size','company_size_num']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['last_new_job'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"change={\n        'never': 0,\n        '1': 1,\n        '2': 2,\n        '3': 3,\n        '4': 4,\n        '>4': 5,\n        }\ndf['last_new_job_num'] = df['last_new_job'].map(change)\ndf[0:10][['last_new_job','last_new_job_num']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# NaN prediction and imputation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def NaN_info(df):\n    global null_view\n    try:\n        null_view = df[[col for col in df.columns if df[col].isna().sum() > 0]].isna().sum().sort_values(ascending = True)\n        null_view = pd.DataFrame(null_view, columns=['NANs'])\n        null_view[['PERCENT']] = null_view.NANs.apply(lambda x: round((x/len(df))*100, 2))\n        null_view[['TYPE']] = df.dtypes\n    except:\n        return null_view\n    return null_view\n\nNaN_info(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def nan_predict(df,\n                skip_features_from_prediction_where_percent_missing_data_more_than = 100,\n                include_features_as_predictors_where_perc_missing_data_less_than = 50,\n                apply_fast_predictor_where_missing_data_less_than_percent = 100,\n                use_n_rows_for_train_not_more_than = 1000000000,    #  If your dataframe is large\n                randomizedSearchCV_iter_plus_perc_missing_data = 10,\n                n_estimators_parameter_for_LightGBM = 2000,\n                target_feature = None,   # For prediction at the end\n                ): \n    \n    import random\n    import pandas as pd\n    import numpy as np\n\n    # Disabling warnings\n    import sys\n    import warnings\n    if not sys.warnoptions:\n        warnings.simplefilter(\"ignore\")\n\n\n    from lightgbm import LGBMClassifier\n    from lightgbm import LGBMRegressor\n    from sklearn.model_selection import RandomizedSearchCV\n    from sklearn.model_selection import ShuffleSplit\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import mean_absolute_error\n    from sklearn.metrics import accuracy_score\n    from sklearn.metrics import f1_score\n    from sklearn.preprocessing import LabelEncoder\n    \n    \n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    %matplotlib inline\n    \n    \n\n    global counter_all_predicted_values\n    counter_all_predicted_values = 0\n    \n    global numeric_features\n    numeric_features = []\n    \n    global best_params\n    \n    \n    PARAMS  =  {'num_leaves': [12, 50, 120, 200, 300, 400, 500],   #np.arange(200, 600, step=100),\n                'max_depth': [4, 8, 12, 16],\n                'learning_rate': [0.001, 0.01, 0.1],\n                'n_estimators': [n_estimators_parameter_for_LightGBM],\n                'subsample': [0.1, 0.3, 0.5],\n                'feature_fraction': [0.1, 0.3, 0.5],\n                'bagging_fraction': [0.1, 0.3, 0.5],\n                'bagging_seed': np.arange(1, 3, step=1),\n                'lambda_l1': [0.2],\n                'lambda_l2': [0.1],\n                'min_child_samples': np.arange(2, 6, step=2),\n                'min_split_gain': [0.0001, 0.001]\n               }\n    \n    \n    CV = ShuffleSplit(n_splits=2, test_size=0.25, random_state=0)\n    \n    \n    \n\n    def NaN_info(df):\n        global null_view\n        try:\n            null_view = df[[col for col in df.columns if df[col].isna().sum() > 0]].isna().sum().sort_values(ascending = True)\n            null_view = pd.DataFrame(null_view, columns=['NANs'])\n            null_view[['PERCENT']] = null_view.NANs.apply(lambda x: round((x/len(df))*100, 2))\n            null_view[['TYPE']] = df.dtypes\n        except:\n            return null_view\n        return null_view\n    \n    \n    def numeric_features(df):\n        num_features = [feature for feature in df.columns if df[feature].dtype in ['int64', 'float64']]\n        return num_features\n    \n    \n    def integer_features(df):\n        global int_features\n        int_features = [feature for feature in df.columns if df[feature].dtype in ['int64']]\n        return int_features\n\n\n    def encoding(work_predictors, df):\n        feature_power = 0.5          # Skew handling\n        for j in work_predictors:\n            el_type = df[j].dtype\n            if el_type == 'object':\n                df[j].replace(np.nan, 'NoNoNo', inplace=True)\n                labelencoder = LabelEncoder()\n                df.loc[:, j] = labelencoder.fit_transform(df.loc[:, j])\n            else:\n                df[j] = df[j]**feature_power\n        return df, work_predictors\n\n\n    def hyperparms_tuning(CV, X_train, X_test, y_train, y_test, n_iter_for_RandomizedSearchCV, PARAMS, alg, scoring):\n        global best_params\n        global pred_test_lgb\n\n        lgbm = alg(random_state = 0)\n        lgbm_randomized = RandomizedSearchCV(estimator=lgbm, \n                                            param_distributions=PARAMS, \n                                            n_iter=n_iter_for_RandomizedSearchCV, \n                                            scoring=scoring, \n                                            cv=CV, \n                                            verbose=0,\n                                            n_jobs = -1)\n\n        lgbm_randomized.fit(X_train, y_train)\n        \n        best_params = lgbm_randomized.best_params_\n        pred_test_lgb = lgbm_randomized.predict(X_test)\n        return best_params, pred_test_lgb\n\n    \n    def predict_regressor(best_params, X, y, miss_df):\n        print('Best parameters:')\n        print(best_params)\n        print('')\n        global pred_miss\n        lgbm = LGBMRegressor(**best_params, n_jobs=-1, random_state=0)\n        lgbm.fit(X, y)\n        pred_miss = list(lgbm.predict(miss_df))\n        print('-------------------------------')\n        print(f\"The first 100 predicted missing values: \\n{pred_miss[:100]}\")\n        return pred_miss\n\n\n    def predict_classifier(best_params, X, y, miss_df):\n        print('Best parameters:')\n        print(best_params)\n        print('')\n        global pred_miss\n        lgbm = LGBMClassifier(**best_params, n_jobs=-1, random_state=0)\n        lgbm.fit(X, y)\n        pred_miss = list(lgbm.predict(miss_df))\n        print('-------------------------------')\n        print(f\"The first 100 predicted missing values: \\n{pred_miss[:100]}\")\n        return pred_miss\n    \n    \n    def imput_missing_value_to_main_df(df, miss_indeces, pred_miss, el):\n        counter = 0\n        for idx in miss_indeces:\n            df.loc[idx, el] = pred_miss[counter]\n            counter += 1\n        return df\n    \n    \n    \n    # Go)\n\n    plt.figure(figsize=(20, 5))\n    sns.heatmap(df.isnull(), cbar=False)\n    \n    \n    print(NaN_info(df))\n    print('\\n\\n\\n')\n    \n    all_features = list(df.columns)\n    df_indeces = list(df.index)\n    df.reset_index(drop=True, inplace=True)\n    \n    integer_features(df)\n\n    delete_miss_features = list(\n        (null_view.loc[null_view['PERCENT'] > skip_features_from_prediction_where_percent_missing_data_more_than]).index)\n    print(f'Exclude from the prediction, because missing data more than \\\n    {skip_features_from_prediction_where_percent_missing_data_more_than}% :\\n{delete_miss_features}')\n    print('')\n    all_miss_features = list(null_view.index)\n\n    for delete_feature in delete_miss_features:\n        all_miss_features.remove(delete_feature)\n        \n    \n    if target_feature in all_miss_features:  # moving target_feature to end of the prediction\n        all_miss_features.append(all_miss_features.pop(all_miss_features.index(target_feature)))\n        \n    \n    for el in all_miss_features:\n        print('\\n\\n\\n\\n')\n        \n        # select features as predictors\n        NaN_info(df)\n        lot_of_miss_features = list(\n            (null_view.loc[null_view['PERCENT'] > include_features_as_predictors_where_perc_missing_data_less_than]).index)\n        now_predictors = list(set(all_features)-set(lot_of_miss_features))\n        work_predictors = list(set(now_predictors) - set([el]))\n\n        \n        # missing data (data for prediction)\n        miss_indeces = list((df[pd.isnull(df[el])]).index)\n        miss_df = df.iloc[miss_indeces][:]\n        miss_df = miss_df[work_predictors]\n        encoding(work_predictors, df=miss_df)\n\n        \n        # data without NaN rows (X data for train, evaluation of model)\n        work_indeces = list(set(df_indeces) - set(miss_indeces))\n        if len(work_indeces) > use_n_rows_for_train_not_more_than:\n            randomlist = random.sample(range(0, len(work_indeces)), use_n_rows_for_train_not_more_than)\n            work_indeces = [work_indeces[i] for i in randomlist]\n        \n        work_df = df.iloc[work_indeces][:] \n        encoding(work_predictors, df=work_df)\n        X = work_df[work_predictors]\n        y = work_df[el]\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n\n        \n        # Info\n        feature_type = df[el].dtypes\n        percent_missing_data = null_view['PERCENT'][el]\n        print(f'Feature: {el},   type: {feature_type},   missing values: {percent_missing_data}%\\n')    \n        print(f'Shape for train dataframe: {(X.shape)}')\n        print(f'Unused features as predictors, because missing data more than {include_features_as_predictors_where_perc_missing_data_less_than}% :')\n        print(lot_of_miss_features)\n        print('')\n        \n        \n        # PREDICTIONS\n        if percent_missing_data < apply_fast_predictor_where_missing_data_less_than_percent:\n            \n            # FAST Predictions without tuning hyperparameters\n            \n            print('FAST prediction without tuning hyperparameters, because missing data less than 1%\\n')\n            best_params = {}\n            if feature_type == 'object' or feature_type == 'bool':\n                print('FAST CLASSIFIER:')\n                labelencoder = LabelEncoder()\n                y_train = labelencoder.fit_transform(y_train)\n                y_test = labelencoder.fit_transform(y_test)\n                lgbm = LGBMClassifier(n_jobs=-1, random_state=0)\n                lgbm.fit(X_train, y_train)\n                pred_test_lgb_FAST = lgbm.predict(X_test)\n                accuracy = accuracy_score(y_test, pred_test_lgb_FAST)\n                print('Evaluations:')\n                print(f'first 10 y_test: {y_test[:10]}')\n                print(f'first 10 y_pred: {pred_test_lgb_FAST[:10]}\\n')\n                f1 = f1_score(y_test, pred_test_lgb_FAST, average='weighted')\n                print(f'accuracy_score:      {accuracy}')\n                print(f'f1_score (weighted): {f1}')\n                \n                predict_classifier(best_params, X, y, miss_df)\n                counter_all_predicted_values += len(miss_indeces)\n                imput_missing_value_to_main_df(df, miss_indeces, pred_miss, el)\n\n            elif feature_type == 'float64' or feature_type == 'int64':\n                print('FAST REGRESSOR:')\n                \n                lgbm = LGBMRegressor(n_jobs=-1, random_state=0)\n                lgbm.fit(X_train, y_train)\n                pred_test_lgb_FAST = lgbm.predict(X_test)\n                MAE = mean_absolute_error(y_test,pred_test_lgb_FAST)\n                y_te = list(round(y_test[:10], 1))\n                y_pred = list(np.round(pred_test_lgb_FAST[:10], 1))\n                print('Evaluations:')\n                print(f'first 10 y_test: {y_te}')\n                print(f'first 10 y_pred: {y_pred}\\n')\n                print(f'mean_absolute_error: {MAE}')\n                print(f'mean for {el}: {df[el].mean()}')\n                \n                predict_regressor(best_params, X, y, miss_df)\n                counter_all_predicted_values += len(miss_indeces)\n                imput_missing_value_to_main_df(df, miss_indeces, pred_miss, el)\n\n            else:\n                print(f\"unprocessed feature: {el} - {feature_type} type\")\n                \n                  \n        else:\n            \n            # ADVANCED Predictions with tuning hyperparameters\n            \n            n_iter_for_RandomizedSearchCV = int(randomizedSearchCV_iter_plus_perc_missing_data + percent_missing_data * 1)\n            print(f'Iteration for RandomizedSearchCV: {n_iter_for_RandomizedSearchCV}\\n')\n            \n            if feature_type == 'object' or feature_type == 'bool':\n                print('ADVANCED CLASSIFIER:')\n                labelencoder = LabelEncoder()\n                y_train = labelencoder.fit_transform(y_train)\n                y_test = labelencoder.fit_transform(y_test)\n                hyperparms_tuning(CV, X_train, X_test, y_train, y_test, n_iter_for_RandomizedSearchCV, PARAMS, alg=LGBMClassifier, scoring='f1_weighted')\n                accuracy = accuracy_score(y_test, pred_test_lgb)\n                print('Evaluations:')\n                print(f'first 10 y_test: {y_test[:10]}')\n                print(f'first 10 y_pred: {pred_test_lgb[:10]}\\n')\n                f1 = f1_score(y_test, pred_test_lgb, average='weighted')\n                print(f'accuracy_score:      {accuracy}')\n                print(f'f1_score (weighted): {f1}')\n                \n                predict_classifier(best_params, X, y, miss_df)\n                counter_all_predicted_values += len(miss_indeces)\n                imput_missing_value_to_main_df(df, miss_indeces, pred_miss, el)\n\n            elif feature_type == 'float64' or feature_type == 'int64':\n                print('ADVANCED REGRESSOR:')\n                hyperparms_tuning(CV, X_train, X_test, y_train, y_test, n_iter_for_RandomizedSearchCV, PARAMS, alg=LGBMRegressor, scoring='neg_mean_squared_error')\n                MAE = mean_absolute_error(y_test,pred_test_lgb)\n                y_te = list(round(y_test[:10], 1))\n                y_pred = list(np.round(pred_test_lgb[:10], 1))\n                print('Evaluations:')\n                print(f'first 10 y_test: {y_te}')\n                print(f'first 10 y_pred: {y_pred}\\n')\n                print(f'mean_absolute_error: {MAE}')\n                print(f'mean for {el}: {df[el].mean()}')\n                \n                predict_regressor(best_params, X, y, miss_df)\n                counter_all_predicted_values += len(miss_indeces)\n                imput_missing_value_to_main_df(df, miss_indeces, pred_miss, el)\n\n            else:\n                print(f\"unprocessed feature: {el} - {feature_type} type\")\n        \n        plt.figure(figsize=(20, 5))\n        sns.heatmap(df.isnull(), cbar=False)\n\n        \n    for feature in int_features:\n        df[[feature]] = df[[feature]].astype('int64')\n        \n    df.index = df_indeces\n\n    print('\\n\\n\\n')\n    print(f'These features have not been processed, because missing data more than {skip_features_from_prediction_where_percent_missing_data_more_than}%')\n    print(NaN_info(df))\n    print('\\n\\n\\n')\n    print(f'{counter_all_predicted_values} values have been predicted and replaced')\n    print('\\n')\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nan_predict(df,\n            target_feature = 'target')     # For prediction at the end\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"int_features = ['training_hours', 'city_num', 'company_size_num', \n                'last_new_job_num', 'experience']\nfor feature in int_features:\n        df[[feature]] = df[[feature]].astype('int64')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"target_column = ['target']\npredictors = list(set(list(df.columns))-set(target_column))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(font_scale=1.5)\n\nfor el in predictors:\n    plot_data = df[['target', el]]\n    try:\n        g = sns.pairplot(plot_data, hue='target', palette='Set1', height=10, aspect=2)\n        \n        handles = g._legend_data.values()\n        labels = g._legend_data.keys()\n        g.fig.legend(handles=handles, labels=labels, loc='upper center', ncol=1)\n    except:\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data = df[['company_size', 'target']]\nplt.figure(figsize=(20,10))\nsns.countplot(x='company_size', hue='target', data=plot_data, palette='Set1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data = df[['major_discipline', 'target']]\nplt.figure(figsize=(20,20))\nsns.countplot(x='major_discipline', hue='target', data=plot_data, palette='Set1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data = df[['education_level', 'target']]\nplt.figure(figsize=(20,10))\nsns.countplot(x='education_level', hue='target', data=plot_data, palette='Set1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data = df[['enrolled_university', 'target']]\nplt.figure(figsize=(20,10))\nsns.countplot(x='enrolled_university', hue='target', data=plot_data, palette='Set1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data = df[['relevent_experience', 'target']]\nplt.figure(figsize=(20,10))\nsns.countplot(x='relevent_experience', hue='target', data=plot_data, palette='Set1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data = df[['last_new_job', 'target']]\nplt.figure(figsize=(20,10))\nsns.countplot(x='last_new_job', hue='target', data=plot_data, palette='Set1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data = df[['experience_cat', 'target']]\nplt.figure(figsize=(20,10))\nsns.countplot(x='experience_cat', hue='target', data=plot_data, palette='Set1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Permutation Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n\ndf_permutation = df.copy()\n\ntarget = ['target']\npredictors = list(set(list(df.columns)) - set(target))\n\n\ndef encoding(df, columns):\n    feature_power = 0.5          # Skew handling\n    for j in columns:\n        el_type = df[j].dtype\n        if el_type == 'object':\n            labelencoder = LabelEncoder()\n            df.loc[:, j] = labelencoder.fit_transform(df.loc[:, j])\n        else:\n            df[j] = df[j]**feature_power\n    return df, columns\n\nencoding(df_permutation, df_permutation.columns)\n\n\nX = df_permutation[predictors]\ny = df_permutation[target]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Permutation function"},{"metadata":{"trusted":true},"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\ndef permutation(X_train, X_test, y_train, y_test, alg):\n    model = alg(n_jobs=-1, random_state=0).fit(X_train, y_train)\n    perm = PermutationImportance(model, random_state=0).fit(X_test, y_test)\n    return eli5.show_weights(perm, feature_names = X_test.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Permutation Importance LGBMClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\npermutation(X_train, X_test, y_train, y_test, LGBMClassifier)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Permutation Importance LGBMRegressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMRegressor\npermutation(X_train, X_test, y_train, y_test, LGBMRegressor)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Permutation Importance RandomForestClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\npermutation(X_train, X_test, y_train, y_test, RandomForestClassifier)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Permutation Importance RandomForestRegressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\npermutation(X_train, X_test, y_train, y_test, RandomForestRegressor)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Partial Plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\nfrom pdpbox import pdp\nfrom lightgbm import LGBMClassifier\n\n\n\n\nmodel = LGBMClassifier(random_state=0).fit(X_train, y_train)\n\nfor feature in X_train.columns:\n    pdp_dist = pdp.pdp_isolate(model=model,\n                               dataset=X_test,\n                               model_features=X_test.columns, \n                               feature=feature)\n\n    pdp.pdp_plot(pdp_dist, feature)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_to_plot = ['target', 'city_development_index']\ninter1  =  pdp.pdp_interact(model=model, \n                            dataset=df_permutation, \n                            model_features=X_test.columns, \n                            features=features_to_plot)\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SHAP"},{"metadata":{"trusted":true},"cell_type":"code","source":"import shap\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n\nmy_model = RandomForestClassifier(n_estimators=30, random_state=1).fit(X_train, y_train)\n\ndef shap_force_plot(X_test, model, row):\n    data_for_prediction = X_test.iloc[row,:]\n    explainer = shap.TreeExplainer(my_model)\n    shap_values = explainer.shap_values(data_for_prediction)\n    shap.initjs()\n    return shap.force_plot(explainer.expected_value[0], shap_values[0], data_for_prediction)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap_force_plot(X_test, model, 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" "},{"metadata":{"trusted":true},"cell_type":"code","source":"shap_force_plot(X_test, model, 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap_force_plot(X_test, model, 1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap_force_plot(X_test, model, 2000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary Plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\n\n\nmodel = XGBClassifier(random_state=0).fit(X_train, y_train)\n\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_test)\n\nshap.summary_plot(shap_values, X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dependence Contribution Plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n\n\nmy_model = RandomForestClassifier(n_estimators=30, random_state=1).fit(X_train, y_train)\n\nexplainer = shap.TreeExplainer(my_model)\ndata_1 = pd.concat([X, y], axis=1)\ndata_1 = data_1.iloc[0:100,:]\nshap_values = explainer.shap_values(data_1)\nshap.dependence_plot('city_development_index', shap_values[1], data_1, interaction_index=\"target\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap_values = explainer.shap_values(data_1)\nshap.dependence_plot('city', shap_values[1], data_1, interaction_index=\"target\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap_values = explainer.shap_values(data_1)\nshap.dependence_plot('experience_cat', shap_values[1], data_1, interaction_index=\"target\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap_values = explainer.shap_values(data_1)\nshap.dependence_plot('experience', shap_values[1], data_1, interaction_index=\"target\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap_values = explainer.shap_values(data_1)\nshap.dependence_plot('company_size', shap_values[1], data_1, interaction_index=\"target\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['target'].replace(to_replace='no',   value=0.0, inplace=True)\ndf['target'].replace(to_replace='yes',  value=1.0, inplace=True)\ndf['target'] = df['target'].astype('float64')\ndf['target'].value_counts(dropna=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = df.loc[19158:, 'target']\npredictions = np.array(result)\npredictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'enrollee_id':test_data['enrollee_id'],'target':predictions})\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)\nprint('Finish')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}