{"cells":[{"metadata":{},"cell_type":"markdown","source":"## What is Keyword Exctraction?\n\nKeyword extraction is defined as the task that automatically identifies a set of the terms that best describe the subject of document. This is an important method in information retrieval (IR) systems: keywords simplify and speed up the search. Keyword extraction can be used to reduce the dimensionality of text for further text analysis (text classification ot topic modeling). [S.Art et al.](https://onlinelibrary.wiley.com/doi/abs/10.1002/smj.2699), for example, extracted keywords to measure patent similarity. Using keyword extraction, you can automatically index data, summarize a text, or generate tag clouds with the most representative keywords.\n\n## How to extract the keywords?\n\nAll keyword extraction algorithms include the following steps:\n\n- *Candidate generation*. Detection of possible candidate keywords from the text.\n- *Property calculation*. Computation of properties and statistics required for ranking.\n- *Ranking*. Computation of a score for each candidate keyword and sorting in descending order of all candidates. The top n candidates are finally selected as the n keywords representing the text.\n\n## Automatic Keyword extraction algorithms\n\n- Rapid Automatic Keyword Extraction (RAKE). Python implementations: [one](https://github.com/csurfer/rake-nltk), [two](https://github.com/zelandiya/RAKE-tutorial), [three](https://github.com/aneesha/RAKE)\n- TextRank. Python implementations [number one](https://pypi.org/project/summa/) and [number two](https://radimrehurek.com/gensim/summarization/keywords.html)\n- [Yet Another Keyword Extractor (Yake)](https://github.com/LIAAD/yake)\n\n\n## If you want to know more...\n- [Slobodan Beliga.](https://pdfs.semanticscholar.org/bdbf/25f3dcf63d38cdb527a9ffca269fa0b8046b.pdf) Keyword extraction: a review of methods and approache\n- [Kamil Bennani-Smires et al.](https://arxiv.org/pdf/1801.04470.pdf) Simple Unsupervised Keyphrase Extraction using Sentence Embeddings\n- [YanYing et al.](https://www.sciencedirect.com/science/article/pii/S1877050917303629) A Graph-based Approach of Automatic Keyphrase Extraction\n- [Martin Dostal and Karel Jezek](http://ceur-ws.org/Vol-706/poster13.pdf) Automatic Keyphrase Extraction based on NLP and Statistical Methods"},{"metadata":{},"cell_type":"markdown","source":"In this kernel we will apply different keyword extraction approaches to the NIPS Paper dataset. Fisrt of all, let's load and prepare the data."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load the dataset\ndf = pd.read_csv('/kaggle/input/nips-papers/papers.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"{} abstracts are missing\".format(df[df['abstract']=='Abstract Missing']['abstract'].count()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pprint\nsample = 941\npprint.pprint(\"TITLE:{}\".format(df['title'][sample]))\npprint.pprint(\"ABSTRACT:{}\".format(df['abstract'][sample]))\npprint.pprint(\"FULL TEXT:{}\".format(df['paper_text'][sample][:1000]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This dataset contains 7 columns: id, year, title, even_type, pdf_name, abstract and paper_text. We are mostly interested in the paper_text which include both title and abstract."},{"metadata":{},"cell_type":"markdown","source":"## Pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nstop_words = set(stopwords.words('english'))\n##Creating a list of custom stopwords\nnew_words = [\"fig\",\"figure\",\"image\",\"sample\",\"using\", \n             \"show\", \"result\", \"large\", \n             \"also\", \"one\", \"two\", \"three\", \n             \"four\", \"five\", \"seven\",\"eight\",\"nine\"]\nstop_words = list(stop_words.union(new_words))\n\ndef pre_process(text):\n    \n    # lowercase\n    text=text.lower()\n    \n    #remove tags\n    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n    \n    # remove special characters and digits\n    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n    \n    ##Convert to list from string\n    text = text.split()\n    \n    # remove stopwords\n    text = [word for word in text if word not in stop_words]\n\n    # remove words less than three letters\n    text = [word for word in text if len(word) >= 3]\n\n    # lemmatize\n    lmtzr = WordNetLemmatizer()\n    text = [lmtzr.lemmatize(word) for word in text]\n    \n    return ' '.join(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndocs = df['paper_text'].apply(lambda x:pre_process(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"docs[1][0:103]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.TF-IDF and Scikit-learn\n\nBased on the tutorial of [Kavita Ganesan](https://github.com/kavgan/nlp-in-practice/blob/master/tf-idf/Keyword%20Extraction%20with%20TF-IDF%20and%20SKlearn.ipynb)\n\nTF-IDF stands for Text Frequency Inverse Document Frequency. The importance of each word increases proportionally to the number of times a word appears in the document (Text Frequency - TF) but is offset by the frequency of the word in the corpus (Inverse Document Frequency - IDF). Using the tf-idf weighting scheme, the keywords are the words with the higherst TF-IDF score.\n\n### 1.1 CountVectorizer to create a vocabulary and generate word counts"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.feature_extraction.text import CountVectorizer\n#docs = docs.tolist()\n#create a vocabulary of words, \ncv=CountVectorizer(max_df=0.95,         # ignore words that appear in 95% of documents\n                   max_features=10000,  # the size of the vocabulary\n                   ngram_range=(1,3)    # vocabulary contains single words, bigrams, trigrams\n                  )\nword_count_vector=cv.fit_transform(docs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.2 TfidfTransformer to Compute Inverse Document Frequency (IDF)"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\ntfidf_transformer.fit(word_count_vector)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once we have our IDF computed, we are now ready to compute TF-IDF and extract the top keywords."},{"metadata":{"trusted":true},"cell_type":"code","source":"def sort_coo(coo_matrix):\n    tuples = zip(coo_matrix.col, coo_matrix.data)\n    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n\ndef extract_topn_from_vector(feature_names, sorted_items, topn=10):\n    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n    \n    #use only topn items from vector\n    sorted_items = sorted_items[:topn]\n\n    score_vals = []\n    feature_vals = []\n\n    for idx, score in sorted_items:\n        fname = feature_names[idx]\n        \n        #keep track of feature name and its corresponding score\n        score_vals.append(round(score, 3))\n        feature_vals.append(feature_names[idx])\n\n    #create a tuples of feature,score\n    #results = zip(feature_vals,score_vals)\n    results= {}\n    for idx in range(len(feature_vals)):\n        results[feature_vals[idx]]=score_vals[idx]\n    \n    return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get feature names\nfeature_names=cv.get_feature_names()\n\ndef get_keywords(idx, docs):\n\n    #generate tf-idf for the given document\n    tf_idf_vector=tfidf_transformer.transform(cv.transform([docs[idx]]))\n\n    #sort the tf-idf vectors by descending order of scores\n    sorted_items=sort_coo(tf_idf_vector.tocoo())\n\n    #extract only the top n; n here is 10\n    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n    \n    return keywords\n\ndef print_results(idx,keywords, df):\n    # now print the results\n    print(\"\\n=====Title=====\")\n    print(df['title'][idx])\n    print(\"\\n=====Abstract=====\")\n    print(df['abstract'][idx])\n    print(\"\\n===Keywords===\")\n    for k in keywords:\n        print(k,keywords[k])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx=941\nkeywords=get_keywords(idx, docs)\nprint_results(idx,keywords, df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am not happy with result. I would like to add a filter that will remove similar keywords, or short keywords inside of complex ones. For instance, non-negative matrix factorization meets us 5 time: non negative matrix, negative matrix, nmf, matrix factorization, matrix. Adding a 4-grams does not change the situation. Similar keywords appears due to the fact that TF-IDF does not take into account the context, the keywords importance comes only from their frequencies relationship. Thus, TF-IDF is a quick, intuitive, but not the best way to extract keywords from the text. Let's look at other ways."},{"metadata":{},"cell_type":"markdown","source":"## 2. Gensim implementation of TextRank summarization algorithm\n\nGensim is a free Python library designed to automatically extract semantic topics from documents. The gensim implementation is based on the popular TextRank algorithm. \n\n[Documentation](https://radimrehurek.com/gensim/summarization/keywords.html)\n\n[Tutorial](https://rare-technologies.com/text-summarization-with-gensim/)"},{"metadata":{},"cell_type":"markdown","source":"### 2.1 Small text"},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim\ntext = \"Non-negative matrix factorization (NMF) has previously been shown to \" + \\\n\"be a useful decomposition for multivariate data. Two different multiplicative \" + \\\n\"algorithms for NMF are analyzed. They differ only slightly in the \" + \\\n\"multiplicative factor used in the update rules. One algorithm can be shown to \" + \\\n\"minimize the conventional least squares error while the other minimizes the  \" + \\\n\"generalized Kullback-Leibler divergence. The monotonic convergence of both  \" + \\\n\"algorithms can be proven using an auxiliary function analogous to that used \" + \\\n\"for proving convergence of the Expectation-Maximization algorithm. The algorithms  \" + \\\n\"can also be interpreted as diagonally rescaled gradient descent, where the  \" + \\\n\"rescaling factor is optimally chosen to ensure convergence.\"\ngensim.summarization.keywords(text, \n         ratio=0.5,               # use 50% of original text\n         words=None,              # Number of returned words\n         split=True,              # Whether split keywords\n         scores=False,            # Whether score of keyword\n         pos_filter=('NN', 'JJ'), # Part of speech (nouns, adjectives etc.) filters\n         lemmatize=True,         # If True - lemmatize words\n         deacc=True)              # If True - remove accentuation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"SUMMARY: \", gensim.summarization.summarize(text,\n                                                  ratio = 0.5,\n                                                  split = True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2 Large text"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_keywords_gensim(idx, docs):\n    \n    keywords=gensim.summarization.keywords(docs[idx], \n                                  ratio=None, \n                                  words=10,         \n                                  split=True,             \n                                  scores=False,           \n                                  pos_filter=None, \n                                  lemmatize=True,         \n                                  deacc=True)              \n    \n    return keywords\n\ndef print_results_gensim(idx,keywords, df):\n    # now print the results\n    print(\"\\n=====Title=====\")\n    print(df['title'][idx])\n    print(\"\\n=====Abstract=====\")\n    print(df['abstract'][idx])\n    print(\"\\n===Keywords===\")\n    for k in keywords:\n        print(k)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx=941\nkeywords=get_keywords_gensim(idx, docs)\nprint_results_gensim(idx,keywords, df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The keywords highlight the main point , but still miss valuable information"},{"metadata":{},"cell_type":"markdown","source":"## 3. Python implementation of the Rapid Automatic Keyword Extraction algorithm (RAKE) using NLTK\n\n[Documentation](https://github.com/csurfer/rake-nltk)\n\n### Setup using pip"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install rake-nltk","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### or directly from the repository"},{"metadata":{"trusted":true},"cell_type":"code","source":"# !git clone https://github.com/csurfer/rake-nltk.git\n# !python rake-nltk/setup.py install","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.1 Small text"},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \"Non-negative matrix factorization (NMF) has previously been shown to \" + \\\n\"be a useful decomposition for multivariate data. Two different multiplicative \" + \\\n\"algorithms for NMF are analyzed. They differ only slightly in the \" + \\\n\"multiplicative factor used in the update rules. One algorithm can be shown to \" + \\\n\"minimize the conventional least squares error while the other minimizes the  \" + \\\n\"generalized Kullback-Leibler divergence. The monotonic convergence of both  \" + \\\n\"algorithms can be proven using an auxiliary function analogous to that used \" + \\\n\"for proving convergence of the Expectation-Maximization algorithm. The algorithms  \" + \\\n\"can also be interpreted as diagonally rescaled gradient descent, where the  \" + \\\n\"rescaling factor is optimally chosen to ensure convergence.\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from rake_nltk import Rake\nr = Rake()\nr.extract_keywords_from_text(text)\nr.get_ranked_phrases_with_scores()[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow! We see well interbretable machine learning terminology! But why diagonally rescaled gradient descent is more important than negative matrix factorization? "},{"metadata":{},"cell_type":"markdown","source":"### 3.2 Large Text"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_keywords_rake(idx, docs, n=10):\n    # Uses stopwords for english from NLTK, and all puntuation characters by default\n    r = Rake()\n    \n    # Extraction given the text.\n    r.extract_keywords_from_text(docs[idx][1000:2000])\n    \n    # To get keyword phrases ranked highest to lowest.\n    keywords = r.get_ranked_phrases()[0:n]\n    \n    return keywords\n\ndef print_results(idx,keywords, df):\n    # now print the results\n    print(\"\\n=====Title=====\")\n    print(df['title'][idx])\n    print(\"\\n=====Abstract=====\")\n    print(df['abstract'][idx])\n    print(\"\\n===Keywords===\")\n    for k in keywords:\n        print(k)\n\nidx=941\nkeywords = get_keywords_rake(idx, docs, n=10)\nprint_results(idx, keywords, df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Oups! Something goes wrong! Algorithm does not work for the preprocessed text without punctuations. Let's treat the raw text."},{"metadata":{"trusted":true},"cell_type":"code","source":"idx=941\nkeywords = get_keywords_rake(idx, df['paper_text'], n=10)\nprint_results(idx, keywords, df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Presented implementation works well on sentences, but it is not flexible enough for large text. However, those who are interested in RANK can expand the capabilities of this code to their needs. We will consider next options."},{"metadata":{},"cell_type":"markdown","source":"## 4. Yet Another Keyword Extractor (Yake)\n\n[Documentation](https://github.com/LIAAD/yake)"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install git+https://github.com/LIAAD/yake","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import yake\n\ndef get_keywords_yake(idx, docs):\n    y = yake.KeywordExtractor(lan='en',          # language\n                             n = 3,              # n-gram size\n                             dedupLim = 0.9,     # deduplicationthresold\n                             dedupFunc = 'seqm', #  deduplication algorithm\n                             windowsSize = 1,\n                             top = 10,           # number of keys\n                             features=None)           \n    \n    keywords = y.extract_keywords(text)\n    return keywords\n\nidx=941\nkeywords = get_keywords_yake(idx, docs[idx])\nprint_results(idx, keywords, df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Key phrases are repeated, and the text needs pre-processing to remove stop words"},{"metadata":{},"cell_type":"markdown","source":"## 5. Keyphrases extraction using pke\n\n`pke` an open source python-based keyphrase extraction toolkit. It provides an end-to-end keyphrase extraction pipeline in which each component can be easily modified or extended to develop new models.\n\n`pke` currently implements the following keyphrase extraction models:\n\n* Unsupervised models\n  * Statistical models\n    * TfIdf [[documentation](https://boudinfl.github.io/pke/build/html/unsupervised.html#tfidf)]\n    * KPMiner [[documentation](https://boudinfl.github.io/pke/build/html/unsupervised.html#kpminer), [article by (El-Beltagy and Rafea, 2010)](http://www.aclweb.org/anthology/S10-1041.pdf)]\n    * YAKE [[documentation](https://boudinfl.github.io/pke/build/html/unsupervised.html#yake), [article by (Campos et al., 2020)](https://doi.org/10.1016/j.ins.2019.09.013)]\n  * Graph-based models\n    * TextRank [[documentation](https://boudinfl.github.io/pke/build/html/unsupervised.html#textrank), [article by (Mihalcea and Tarau, 2004)](http://www.aclweb.org/anthology/W04-3252.pdf)]\n    * SingleRank  [[documentation](https://boudinfl.github.io/pke/build/html/unsupervised.html#singlerank), [article by (Wan and Xiao, 2008)](http://www.aclweb.org/anthology/C08-1122.pdf)]\n    * TopicRank [[documentation](https://boudinfl.github.io/pke/build/html/unsupervised.html#topicrank), [article by (Bougouin et al., 2013)](http://aclweb.org/anthology/I13-1062.pdf)]\n    * TopicalPageRank [[documentation](https://boudinfl.github.io/pke/build/html/unsupervised.html#topicalpagerank), [article by (Sterckx et al., 2015)](http://users.intec.ugent.be/cdvelder/papers/2015/sterckx2015wwwb.pdf)]\n    * PositionRank [[documentation](https://boudinfl.github.io/pke/build/html/unsupervised.html#positionrank), [article by (Florescu and Caragea, 2017)](http://www.aclweb.org/anthology/P17-1102.pdf)]\n    * MultipartiteRank [[documentation](https://boudinfl.github.io/pke/build/html/unsupervised.html#multipartiterank), [article by (Boudin, 2018)](https://arxiv.org/abs/1803.08721)]\n* Supervised models\n  * Feature-based models\n    * Kea [[documentation](https://boudinfl.github.io/pke/build/html/supervised.html#kea), [article by (Witten et al., 2005)](https://www.cs.waikato.ac.nz/ml/publications/2005/chap_Witten-et-al_Windows.pdf)]\n    * WINGNUS [[documentation](https://boudinfl.github.io/pke/build/html/supervised.html#wingnus), [article by (Nguyen and Luong, 2010)](http://www.aclweb.org/anthology/S10-1035.pdf)]\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install git+https://github.com/boudinfl/pke.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pke","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.1  SingleRank\n\nThis model is an extension of the TextRank model that uses the number of co-occurrences to weigh edges in the graph."},{"metadata":{"trusted":true},"cell_type":"code","source":"# define the set of valid Part-of-Speeches\npos = {'NOUN', 'PROPN', 'ADJ'}\n\n# 1. create a SingleRank extractor.\nextractor = pke.unsupervised.SingleRank()\n\n# 2. load the content of the document.\nextractor.load_document(input=text,\n                        language='en',\n                        normalization=None)\n\n# 3. select the longest sequences of nouns and adjectives as candidates.\nextractor.candidate_selection(pos=pos)\n\n# 4. weight the candidates using the sum of their word's scores that are\n#    computed using random walk. In the graph, nodes are words of\n#    certain part-of-speech (nouns and adjectives) that are connected if\n#    they occur in a window of 10 words.\nextractor.candidate_weighting(window=10,\n                              pos=pos)\n\n# 5. get the 10-highest scored candidates as keyphrases\nkeyphrases = extractor.get_n_best(n=10)\n\nidx = 941\n# now print the results\nprint(\"\\n=====Title=====\")\nprint(df['title'][idx])\nprint(\"\\n=====Abstract=====\")\nprint(df['abstract'][idx])\nprint(\"\\n===Keywords===\")\nfor k in keyphrases:\n    print(k[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Great job!","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2 TopicRank"},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nfrom nltk.corpus import stopwords\n\n# 1. create a TopicRank extractor.\nextractor = pke.unsupervised.TopicRank()\n\n# 2. load the content of the document.\nextractor.load_document(input=text)\n\n# 3. select the longest sequences of nouns and adjectives, that do\n#    not contain punctuation marks or stopwords as candidates.\npos = {'NOUN', 'PROPN', 'ADJ'}\nstoplist = list(string.punctuation)\nstoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\nstoplist += stopwords.words('english')\nextractor.candidate_selection(pos=pos, stoplist=stoplist)\n\n# 4. build topics by grouping candidates with HAC (average linkage,\n#    threshold of 1/4 of shared stems). Weight the topics using random\n#    walk, and select the first occuring candidate from each topic.\nextractor.candidate_weighting(threshold=0.74, method='average')\n\n# 5. get the 10-highest scored candidates as keyphrases\nkeyphrases = extractor.get_n_best(n=10)\n\nidx = 941\n# now print the results\nprint(\"\\n=====Title=====\")\nprint(df['title'][idx])\nprint(\"\\n=====Abstract=====\")\nprint(df['abstract'][idx])\nprint(\"\\n===Keywords===\")\nfor k in keyphrases:\n    print(k[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great job too! I love implementation of graph-based models by `pke` library."},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\n\n#### In this kernel, we looked at different algorithms of keywords extraction. My personal top is:\n 1. SingleRank by pke\n 2. TopicRank by pke\n 3. TextRank by gensim \n \nI am new to NLP and it was a great journey for me: I learned a lot when I wrote this kernel. Hope, you will find it usefull too. Thanks for reading!"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}