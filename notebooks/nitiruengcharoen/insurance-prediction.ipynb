{"cells":[{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://bradshiversinsurance.com/wp-content/uploads/sites/2194/2015/10/bgtest.jpg\" alt=\"Insurance\" width=\"1500\">\n"},{"metadata":{},"cell_type":"markdown","source":"# 1. Introduction\n\nInsurance company that has provided Health Insurance to its customers now they need your help in building a model to predict whether the policyholders (customers) from past year will also be interested in Vehicle Insurance provided by the company.\n\nThe strategy is to reach out to those customers and optimise its business model and revenue by create model in order to predict whether a customer would be interested in Vehicle Insurance from these information about demographics (gender, age, region code type), Vehicles (Vehicle Age, Damage), Policy (Premium, sourcing channel) etc."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom time import time\n\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import RandomizedSearchCV, cross_val_score, cross_val_predict, train_test_split, StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix,  roc_curve, auc, accuracy_score, precision_score, classification_report, roc_auc_score\n\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import plot_precision_recall_curve\n\nfrom sklearn.metrics import average_precision_score\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\n\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE, ADASYN\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Load Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/health-insurance-cross-sell-prediction/train.csv')\n\ntest = pd.read_csv('../input/health-insurance-cross-sell-prediction/test.csv')\n\ntrain.drop(columns = 'id', inplace =True)\ntest.drop(columns = 'id', inplace =True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test.shape)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check missing values\ndataset = pd.concat([train,test]).reset_index(drop=True)\n\nprint(dataset.isnull().sum())\n# sns.heatmap(dataset.isnull(), cbar=False, cmap='YlGnBu_r');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We combine train and test data and check missing value.\n* There are no missing (Response is targer variable, test dataset have no target variable)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # 3. Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"## 3.1 Univariate analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# dependent variable/Predicted variable/Target variable\nsns.countplot(x='Response', data=train, order = train['Response'].value_counts().index);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Target variable `Response` is imbalance data so in this case will be a problem like a bias in the training dataset can influence many machine learning algorithms, leading some to ignore the minority class entirely. This is a problem as it is typically the minority class on which predictions are most important."},{"metadata":{"trusted":true},"cell_type":"code","source":"col = ['Gender', 'Driving_License', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage']\n\nplt.figure(figsize=(18,4))\n\nfor i, col in enumerate(col):\n    plt.subplot(1,5, i+1)\n    sns.countplot(x=col, data=train, order = train[col].value_counts().index)\n    \nplt.tight_layout(pad=1);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(17,10))\n\ncol = [c for c in train.columns if c not in ['Gender', 'Driving_License', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage', 'id', 'Response']]\n\nfor i, col in enumerate(col):\n    plt.subplot(2,3, i+1)\n    sns.histplot(data=train, x=col, kde=True)\n    \nplt.tight_layout(pad=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2 Bivariate analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"col = ['Gender', 'Driving_License', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage']\n\nfor i, col in enumerate(col):\n    sns.catplot(x=col, col='Response', col_wrap=2,data=train, kind=\"count\", height=2.5, aspect=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot heat map\nfig, ax = plt.subplots(figsize=(12, 10))\ncmap = sns.diverging_palette(10, 240, n=9)\n\ntrain_corr = train.corr()\nsns.heatmap(train_corr, annot=True, fmt=\".2f\", linewidths=2, cmap=cmap, vmin=-1, vmax=1, cbar_kws={\"shrink\": .9}, square=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no independent variable is highly correlated that mean there no Multicollinearity Problem"},{"metadata":{"trusted":true},"cell_type":"code","source":"col = [c for c in train.columns if c not in ['Gender', 'Driving_License', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage', 'id', 'Response']]\n\nsns.pairplot(train[col]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Data Preprocessing"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# dataset = [train,test]\n\n# for df in dataset:\n    \n# #     df.replace(['Male', 'Female'], [1, 0], inplace=True)\n\n#     vehicle_age_map = {'< 1 Year':0, '1-2 Year':1, '> 2 Years':2}\n#     df['Vehicle_Age'] = df['Vehicle_Age'].map(vehicle_age_map).astype('int')\n\n# #     vehicle_damage_map = {'Yes':1, 'No':0}\n# #     df['Vehicle_Damage'].replace(vehicle_damage_map, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We consider these independent variables to \"String\" type then we will apply one-hot encoding to transform categorical data to numerical data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Change data type\ndataset = [train,test]\n\nfor df in dataset:\n    col = ['Gender', 'Driving_License', 'Previously_Insured', 'Vehicle_Damage']\n    for col in col:\n        df[col] = df[col].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import shuffle\n\ntrain = shuffle(train)\n\nX = train.iloc[:,:-1]\ny = train.iloc[:,-1]\n\nprint(Counter(y))\nprint(f'Ratio 0:1 = {Counter(y)[0] / Counter(y)[1]:.4f}%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Python scikit-learn provides a Pipeline utility to help automate machine learning workflows. The goal is to ensure that all of the steps in the pipeline are constrained to the data available for the evaluation.\n\n* Numerical dataset: we will scale data by using StandardScaler\n* Categorical dataset: we will transform data to numerical data by using OrdinalEncoder such as `Vehicle_Age` for ordinal data. OneHotEncoder for data without ordinal such as `Gender`, `Driving_License`, `Previously_Insured`, `Vehicle_Damage`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get categorical/numerical columns\ncat_col = [col for col in X.columns if X[col].dtype == 'object']\ncat_col.remove('Vehicle_Age')\n\nori_col = ['Vehicle_Age']\n\nnum_col = [col for col in X.columns if X[col].dtype != 'object']\n\n\n# Numerical preprocess missing value\nnumerical_transformer = make_pipeline(StandardScaler())\n\n# Categorical preprocess missing value\ncategorical_transformer = make_pipeline(OrdinalEncoder())\ncategorical_transformer2 = make_pipeline(OneHotEncoder(handle_unknown='error',sparse=False))\n                                        \n# Preprocess Numerical and Categorical variable\npreprocess = ColumnTransformer(transformers=[\n        ('num', numerical_transformer, num_col),\n        ('ordi', categorical_transformer, ori_col),\n        ('cat', categorical_transformer2, cat_col)])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocess.fit_transform(X)\n\nenc_cat_col = preprocess.named_transformers_['cat']['onehotencoder'].get_feature_names()\n\nlabels = np.concatenate([num_col, ori_col, enc_cat_col])\n\nX_transformed = pd.DataFrame(preprocess.fit_transform(X), columns=labels)\n\nX_transformed.rename(columns={'x0_Female':'Gender_Female',\n                             'x0_Male':'Gender_Male',\n                              'x1_0':'Driving_License_0',\n                              'x1_1':'Driving_License_1',\n                              'x2_0':'Previously_Insured_0',\n                              'x2_1':'Previously_Insured_1',\n                              'x3_No':'Vehicle_Damage_No',\n                              'x3_Yes':'Vehicle_Damage_Yes',}, inplace=True)\nX_transformed","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# # Get categorical/numerical columns\n# cat_col = [col for col in X.columns if X[col].dtype == 'object']\n\n# num_col = [col for col in X.columns if X[col].dtype != 'object']\n\n\n# # # Numerical preprocess missing value\n# # numerical_transformer = make_pipeline(StandardScaler())\n\n# # Categorical preprocess missing value\n# categorical_transformer = make_pipeline(OneHotEncoder(handle_unknown='error',sparse=False))\n                                        \n# # Preprocess Numerical and Categorical variable\n# preprocess = ColumnTransformer(transformers=[\n# #         ('num', numerical_transformer, num_col),\n#         ('cat', categorical_transformer, cat_col)])\n\n# preprocess.fit_transform(X)\n\n# enc_cat_col = preprocess.named_transformers_['cat']['onehotencoder'].get_feature_names()\n\n# labels = np.concatenate([num_col, enc_cat_col])\n\n# X_transformed = pd.DataFrame(preprocess.fit_transform(X), columns=enc_cat_col)\n# X_transformed\n# X_transformed.rename(columns={'x0_Female':'Gender_Female',\n#                              'x0_Male':'Gender_Male',\n#                               'x1_0':'Driving_License_0',\n#                               'x1_1':'Driving_License_1',\n#                               'x2_0':'Previously_Insured_0',\n#                               'x2_1':'Previously_Insured_1',\n#                               'x3_1-2 Year' : 'Vehicle_Age_1_2_Year', \n#                               'x3_< 1 Year' : 'Vehicle_Age_1_Year', \n#                               'x3_> 2 Years' : 'Vehicle_Age_2_Year', \n#                               'x4_No':'Vehicle_Damage_No',\n#                               'x4_Yes':'Vehicle_Damage_Yes',}, inplace =True)\n\n# X_transformed = pd.concat([X[num_col], X_transformed], axis =1)\n# X_transformed","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will split data to train set 80% and test set 20%"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split data\nX_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.20, random_state=0, stratify=y)\n\nprint('Train Dataset',Counter(y_train))\nprint(f'Ratio 0:1 = {Counter(y_train)[0] / Counter(y_train)[1]:.4f}%')\nprint('\\n')\nprint('Test Dataset',Counter(y_test))\nprint(f'Ratio 0:1 = {Counter(y_test)[0] / Counter(y_test)[1]:.4f}%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# skf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n\n# model = {'LogisticRegression': LogisticRegression(random_state=0),\n#         'RidgeClassifier' : RidgeClassifier(random_state=0),\n#         'LGBMClassifier' : LGBMClassifier(random_state=0),\n#         'KNeighborsClassifier' : KNeighborsClassifier(),\n#         'XGBClassifier' : XGBClassifier(random_state=0),\n#         'RandomForestClassifier': RandomForestClassifier(random_state=0)}\n\n# for name in model:\n#     start = time()\n#     score = cross_val_score(model[name], X_train, y_train, scoring = 'accuracy', cv = skf, n_jobs = -1)\n#     end = time()\n#     print(f'{name}\\naccuracy score: {score.mean():.5f} Â±{score.std():.4f} Time:{end-start:.1f} sec')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Baseline Original Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LGBMClassifier()\n\nmodel.fit(X_train, y_train)\n\ny_predict = model.predict(X_test)\n\n# confusion_matrix(y_test, y_predict)\n\ndata = pd.DataFrame({'test': y_test,'pred': y_predict})\nmatrix = pd.crosstab(data.test, data.pred, rownames=['Actual'], colnames=['Predicted'])\nsns.heatmap(matrix, annot=True, fmt='g')\nplt.title(f'Original Dataset')\nplt.show()\n\ny_score = model.predict_proba(X_test)[:,1]\nfpr, tpr, _ = roc_curve(y_test, y_score)\n\nplt.title('LogisticRegression ROC curve: CC Fraud')\nplt.xlabel('FPR (Precision)')\nplt.ylabel('TPR (Recall)')\n\nplt.plot(fpr,tpr, label='LogisticRegression')\nplt.legend(loc='lower right')\nplt.plot((0,1), ls='dashed',color='black')\nplt.show()\nprint ('Area under curve (AUC): ', auc(fpr,tpr)) #roc_auc_score(y_test, y_score)\n\nprint(f'precision : {precision_score(y_test, y_predict)}')\nprint('\\n')\nprint(classification_report(y_test, y_predict))\n\naverage_precision = average_precision_score(y_test, y_predict)\n\ndisp = plot_precision_recall_curve(model, X_test, y_test)\n\ndisp.ax_.set_title('Precision-Recall curve: Average precision score={0:0.2f}'.format(average_precision))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Resampling Training Dataset"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://miro.medium.com/max/2400/1*ENvt_PTaH5v4BXZfd-3pMA.png\" alt=\" resample\" width=\"700\">"},{"metadata":{},"cell_type":"markdown","source":"One approach to addressing the problem of class imbalance is to randomly resample the training dataset. \n\nThe two main approaches technique for rebalancing the class distribution for an imbalanced dataset, to randomly resampling an imbalanced dataset are\n\n\n## **6.1. Oversampling**\n\n* Random oversampling duplicates examples from the minority class in the training dataset and can result in overfitting for some models.\n\n\n## **6.2. Undersampling**\n\n* Random undersampling deletes examples from the majority class and can result in losing information invaluable to a model."},{"metadata":{},"cell_type":"markdown","source":"### 6.1.1 Naive random over-sampling\n\nThis makes them simple to implement and fast to execute, which is desirable for very large and complex datasets.\n\nBoth techniques can be used for two-class (binary) classification problems and multi-class classification problems with one or more majority or minority classes.\n\nGenerally, these naive methods can be effective, although that depends on the specifics of the dataset and models involved.\n\n\n*Importantly, the change to the class distribution is only applied to the training dataset. The intent is to influence the fit of the models. The resampling is not applied to the test or holdout dataset used to evaluate the performance of a model.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"ros = RandomOverSampler(random_state=0)\n\nX_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n\nprint(Counter(y_resampled))\nprint(f'Ratio 0:1 = {Counter(y_resampled)[0] / Counter(y_resampled)[1]:.4f}%')\n\nmodel = LGBMClassifier(random_state=0)\n\nmodel.fit(X_resampled, y_resampled)\n\ny_predict = model.predict(X_test)\n\n# confusion_matrix(y_test, y_predict)\n\ndata = pd.DataFrame({'test': y_test,'pred': y_predict})\nmatrix = pd.crosstab(data.test, data.pred, rownames=['Actual'], colnames=['Predicted'])\nsns.heatmap(matrix, annot=True, fmt='g')\nplt.title(f'Naive random over-sampling')\nplt.show()\n\n#plot_confusion_matrix(model, X_test, y_test);\n\ny_score = model.predict_proba(X_test)[:,1]\nfpr, tpr, _ = roc_curve(y_test, y_score)\n\nplt.title('LogisticRegression ROC curve: CC Fraud')\nplt.xlabel('FPR (Precision)')\nplt.ylabel('TPR (Recall)')\n\nplt.plot(fpr,tpr, label='LogisticRegression')\nplt.legend(loc='lower right')\nplt.plot((0,1), ls='dashed',color='black')\nplt.show()\nprint ('Area under curve (AUC): ', auc(fpr,tpr)) #roc_auc_score(y_test, y_score)\n\nprint(f'precision : {precision_score(y_test, y_predict)}')\nprint('\\n')\nprint(classification_report(y_test, y_predict))\n\naverage_precision = average_precision_score(y_test, y_predict)\n\ndisp = plot_precision_recall_curve(model, X_test, y_test)\n\ndisp.ax_.set_title('Precision-Recall curve: Average precision score={0:0.2f}'.format(average_precision))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://miro.medium.com/max/512/1*FcM03wUtW_dB2YGZXyVb7Q.png\" alt=\" resample\" width=\"600\">\n"},{"metadata":{},"cell_type":"markdown","source":"### 6.1.2 Over-sampling to SMOTE\n\nSynthetic Minority Over-sampling Technique (SMOTE). This method is considered a state-of-art technique and works well in various applications. This method generates synthetic data based on the feature space similarities between existing minority instances. In order to create a synthetic instance, it finds the K-nearest neighbors of each minority instance, randomly selects one of them, and then calculate linear interpolations to produce a new minority instance in the neighborhood."},{"metadata":{"trusted":true},"cell_type":"code","source":"sm = SMOTE(random_state=0)\n\nX_resampled, y_resampled = sm.fit_resample(X_train, y_train)\n\nprint(Counter(y_resampled))\nprint(f'Ratio 0:1 = {Counter(y_resampled)[0] / Counter(y_resampled)[1]:.4f}%')\n\nmodel = LGBMClassifier(random_state=0)\n\nmodel.fit(X_resampled, y_resampled)\n\ny_predict = model.predict(X_test)\n\n# confusion_matrix(y_test, y_predict)\n\ndata = pd.DataFrame({'test': y_test,'pred': y_predict})\nmatrix = pd.crosstab(data.test, data.pred, rownames=['Actual'], colnames=['Predicted'])\nsns.heatmap(matrix, annot=True, fmt='g')\nplt.title(f'SMOTE')\nplt.show()\n\n#plot_confusion_matrix(model, X_test, y_test);\n\ny_score = model.predict_proba(X_test)[:,1]\nfpr, tpr, _ = roc_curve(y_test, y_score)\n\nplt.title('LogisticRegression ROC curve: CC Fraud')\nplt.xlabel('FPR (Precision)')\nplt.ylabel('TPR (Recall)')\n\nplt.plot(fpr,tpr, label='LogisticRegression')\nplt.legend(loc='lower right')\nplt.plot((0,1), ls='dashed',color='black')\nplt.show()\nprint ('Area under curve (AUC): ', auc(fpr,tpr)) #roc_auc_score(y_test, y_score)\n\nprint(f'precision : {precision_score(y_test, y_predict)}')\nprint('\\n')\nprint(classification_report(y_test, y_predict))\n\naverage_precision = average_precision_score(y_test, y_predict)\n\ndisp = plot_precision_recall_curve(model, X_test, y_test)\n\ndisp.ax_.set_title('Precision-Recall curve: Average precision score={0:0.2f}'.format(average_precision))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.1.3 Over-sampling to ADASYN: Adaptive Synthetic Sampling\n\nADASYN generates samples of the minority class according to their density distributions. More synthetic data is generated for minority class samples that are harder to learn, compared to those minority samples that are easier to learn. It calculates the K-nearest neighbors of each minority instance, then gets the class ratio of the minority and majority instances to generate new samples. By repeating this process, it adaptively shifts the decision boundary to focus on those samples that are difficult to learn."},{"metadata":{"trusted":true},"cell_type":"code","source":"adasyn = ADASYN(random_state=0)\n\nX_resampled, y_resampled = adasyn.fit_resample(X_train, y_train)\n\nprint(Counter(y_resampled))\nprint(f'Ratio 0:1 = {Counter(y_resampled)[0] / Counter(y_resampled)[1]:.4f}%')\n\nmodel = LGBMClassifier(random_state=0)\n\nmodel.fit(X_resampled, y_resampled)\n\ny_predict = model.predict(X_test)\n\n# confusion_matrix(y_test, y_predict)\n\ndata = pd.DataFrame({'test': y_test,'pred': y_predict})\nmatrix = pd.crosstab(data.test, data.pred, rownames=['Actual'], colnames=['Predicted'])\nsns.heatmap(matrix, annot=True, fmt='g')\nplt.title(f'ADASYN')\nplt.show()\n\n#plot_confusion_matrix(model, X_test, y_test);\n\n\ny_score = model.predict_proba(X_test)[:,1]\nfpr, tpr, _ = roc_curve(y_test, y_score)\n\nplt.title('LogisticRegression ROC curve: CC Fraud')\nplt.xlabel('FPR (Precision)')\nplt.ylabel('TPR (Recall)')\n\nplt.plot(fpr,tpr, label='LogisticRegression')\nplt.legend(loc='lower right')\nplt.plot((0,1), ls='dashed',color='black')\nplt.show()\nprint ('Area under curve (AUC): ', auc(fpr,tpr)) #roc_auc_score(y_test, y_score)\n\nprint(f'precision : {precision_score(y_test, y_predict)}')\nprint('\\n')\nprint(classification_report(y_test, y_predict))\n\naverage_precision = average_precision_score(y_test, y_predict)\n\ndisp = plot_precision_recall_curve(model, X_test, y_test)\n\ndisp.ax_.set_title('Precision-Recall curve: Average precision score={0:0.2f}'.format(average_precision))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.2.1 Random under-sampling\nRandomUnderSampler is a fast and easy way to balance the data by randomly selecting a subset of data for the targeted classes:"},{"metadata":{"trusted":true},"cell_type":"code","source":"rus = RandomUnderSampler(random_state=0)\n\nX_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n\nprint(Counter(y_resampled))\nprint(f'Ratio 0:1 = {Counter(y_resampled)[0] / Counter(y_resampled)[1]:.4f}%')\n\nmodel = LGBMClassifier(random_state=0)\n\nmodel.fit(X_resampled, y_resampled)\n\ny_predict = model.predict(X_test)\n\n# confusion_matrix(y_test, y_predict)\n\ndata = pd.DataFrame({'test': y_test,'pred': y_predict})\nmatrix = pd.crosstab(data.test, data.pred, rownames=['Actual'], colnames=['Predicted'])\nsns.heatmap(matrix, annot=True, fmt='g')\nplt.title(f'RandomUnderSampler')\nplt.show()\n\n#plot_confusion_matrix(model, X_test, y_test);\n\n\ny_score = model.predict_proba(X_test)[:,1]\nfpr, tpr, _ = roc_curve(y_test, y_score)\n\nplt.title('LogisticRegression ROC curve: CC Fraud')\nplt.xlabel('FPR (Precision)')\nplt.ylabel('TPR (Recall)')\n\nplt.plot(fpr,tpr, label='LogisticRegression')\nplt.legend(loc='lower right')\nplt.plot((0,1), ls='dashed',color='black')\nplt.show()\nprint ('Area under curve (AUC): ', auc(fpr,tpr)) #roc_auc_score(y_test, y_score)\n\nprint(f'precision : {precision_score(y_test, y_predict)}')\nprint('\\n')\nprint(classification_report(y_test, y_predict))\n\naverage_precision = average_precision_score(y_test, y_predict)\n\ndisp = plot_precision_recall_curve(model, X_test, y_test)\n\ndisp.ax_.set_title('Precision-Recall curve: Average precision score={0:0.2f}'.format(average_precision))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use Naive random over-sampling technique to deal with imbalance data because in this case we need to focus the customer which might interested in Vehicle insurance. That is a reason why we chosse model which high recall (The actual data which is a relevant instances that were retrieved) and the overall F1 score is the highest compare to other over-sampling like SMOTE and ADASYN. We not use under sampling in this case because it result in losing information invaluable to a model. "},{"metadata":{},"cell_type":"markdown","source":"# 7. Model Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"# In this case we use RandomOverSampler technique to transfrom data\nros = RandomOverSampler(random_state=0)\n\nX_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n\nprint(Counter(y_resampled))\nprint(f'Ratio 0:1 = {Counter(y_resampled)[0] / Counter(y_resampled)[1]:.4f}%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n\nmodels = {'LogisticRegression': LogisticRegression(random_state=0),\n        'RidgeClassifier' : RidgeClassifier(random_state=0),\n        'LGBMClassifier' : LGBMClassifier(random_state=0),\n        'KNeighborsClassifier' : KNeighborsClassifier(),\n        'XGBClassifier' : XGBClassifier(random_state=0,eval_metric = 'auc'),\n        'RandomForestClassifier': RandomForestClassifier(random_state=0)}\n        \n\naccuracy = []\nprecision = []\nrecall = []\nf1 = []\nroc_auc = []\ntimes = []\n\nfor model_name in models:\n    \n    start = time()\n\n    models[model_name].fit(X_resampled, y_resampled)\n    \n    end = time()\n    \n    accuracy_ = cross_val_score(models[model_name], X_test, y_test, scoring = 'accuracy', cv = skf, n_jobs = -1)\n    precision_ = cross_val_score(models[model_name], X_test, y_test, scoring = 'precision', cv = skf, n_jobs = -1)\n    recall_ = cross_val_score(models[model_name], X_test, y_test, scoring = 'recall', cv = skf, n_jobs = -1)\n    f1_ = cross_val_score(models[model_name], X_test, y_test, scoring = 'f1', cv = skf, n_jobs = -1)\n    roc_auc_ = cross_val_score(models[model_name], X_test, y_test, scoring = 'roc_auc', cv = skf, n_jobs = -1)\n\n    accuracy.append(np.mean(accuracy_))\n    precision.append(np.mean(precision_))\n    recall.append(np.mean(recall_))\n    f1.append(np.mean(f1_))\n    roc_auc.append(np.mean(roc_auc_))\n    times.append(end-start)\n    \npd.concat([pd.DataFrame([models.keys()]).T.rename(columns = {0:'models'}),\n           pd.DataFrame({'accuracy':accuracy, 'precision':precision, 'recall':recall, 'f1':f1, 'roc_auc':roc_auc, 'times':times})],\n          axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# from scipy.stats import randint as sp_randint\n# from scipy.stats import uniform as sp_uniform\n# import time\n\n# #Set hypermeter search\n# fit_params={\"early_stopping_rounds\":30, \n#             \"eval_metric\" : 'auc', \n#             \"eval_set\" : [(X_test,y_test)],\n#             'eval_names': ['valid'],\n#             #'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_099)],\n#             'verbose': 100,\n#             'categorical_feature': 'auto'}\n\n\n# params ={'num_leaves': sp_randint(6, 50), \n#          'min_child_samples': sp_randint(100, 500), \n#          'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n#          'subsample': sp_uniform(loc=0.2, scale=0.8), \n#          'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n#          'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n#          'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n\n# n_HP_points_to_test = 100\n\n# clf = LGBMClassifier(max_depth=-1, random_state=0, silent=True, metric='None', n_jobs=-1, n_estimators=5000)\n\n# gs = RandomizedSearchCV(\n#     estimator = clf,\n#     param_distributions = params, \n#     n_iter = n_HP_points_to_test,\n#     scoring = 'roc_auc',\n#     cv = 3,\n#     refit = True,\n#     random_state = 314,\n#     verbose = False)\n\n# %time\n# gs.fit(X_train, y_train, **fit_params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8. Hyperparameter Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LGBMClassifier( \n    boosting_type=\"gbdt\",\n    is_unbalance=True, \n    random_state=10, \n    n_estimators=50,\n    num_leaves=30, \n    max_depth=8,\n    feature_fraction=0.5,  \n    bagging_fraction=0.8, \n    bagging_freq=15, \n    learning_rate=0.01,    \n)\n\nparams_opt = {'n_estimators':range(200, 600, 80),\n              'num_leaves':range(20,60,10)}\n\nrc = RandomizedSearchCV(estimator = model, \n    param_distributions = params_opt, \n    scoring='roc_auc',\n    n_jobs=4,\n    iid=False, \n    verbose=1,\n    cv=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\nrc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rc.best_estimator_, rc.best_params_, rc.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get model parameters at first\nclf_final = LGBMClassifier(**model.get_params())\n\n#set optimal parameters got from RandomizedSearchCV\nclf_final.set_params(**rc.best_params_)\n\n# --------  or second way use this --------\n# clf_final = LGBMClassifier(**rc.best_estimator_.get_params())\n# clf_final","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_final.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_ = cross_val_score(clf_final, X_test, y_test, scoring = 'accuracy', cv = skf, n_jobs = -1)\nprecision_ = cross_val_score(clf_final, X_test, y_test, scoring = 'precision', cv = skf, n_jobs = -1)\nrecall_ = cross_val_score(clf_final, X_test, y_test, scoring = 'recall', cv = skf, n_jobs = -1)\nf1_ = cross_val_score(clf_final, X_test, y_test, scoring = 'f1', cv = skf, n_jobs = -1)\nroc_auc_ = cross_val_score(clf_final, X_test, y_test, scoring = 'roc_auc', cv = skf, n_jobs = -1)\n\nprint(f'accuracy: {np.mean(accuracy_):.5f}')\nprint(f'precision: {np.mean(precision_):.5f}')\nprint(f'recall: {np.mean(recall_):.5f}')\nprint(f'f1: {np.mean(f1_):.5f}')\nprint(f'roc_auc: {np.mean(roc_auc_):.5f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importance = pd.DataFrame({'feature':X_train.columns, 'importance':clf_final.feature_importances_}).sort_values(by = 'importance',ascending=False)\n\nplt.figure(figsize=(10,8))\nsns.barplot(x=\"importance\", y=\"feature\", data=importance);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 9. Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocess.transform(test)\n\nenc_cat_col = preprocess.named_transformers_['cat']['onehotencoder'].get_feature_names()\n\nlabels = np.concatenate([num_col, ori_col, enc_cat_col])\n\ntest_transformed = pd.DataFrame(preprocess.fit_transform(test), columns=labels)\n\ntest_transformed.rename(columns={'x0_Female':'Gender_Female',\n                             'x0_Male':'Gender_Male',\n                              'x1_0':'Driving_License_0',\n                              'x1_1':'Driving_License_1',\n                              'x2_0':'Previously_Insured_0',\n                              'x2_1':'Previously_Insured_1',\n                              'x3_No':'Vehicle_Damage_No',\n                              'x3_Yes':'Vehicle_Damage_Yes',}, inplace=True)\ntest_transformed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Prediction = clf_final.predict(test_transformed)\nPrediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/health-insurance-cross-sell-prediction/sample_submission.csv')\n\nsubmission = pd.concat([sub.drop(columns = 'Response'), pd.DataFrame({'Prediction': Prediction})], axis=1)\n\n#save submission file to .csv\nsubmission.to_csv('vehicle_insurance_predicted.csv', index = False)\nsubmission","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}