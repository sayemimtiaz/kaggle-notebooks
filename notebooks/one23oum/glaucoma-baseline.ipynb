{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"class CFG:\n    debug=False\n    #height=256\n    #width=256\n    lr=1e-4\n    batch_size=16\n    epochs=5\n    seed=777\n    target_size=1\n    target_col = \"label\"\n    n_fold=4\nSIZE = 512","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import sys\n\nimport gc\nimport os\nimport random\nimport time\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom collections import defaultdict, Counter\n\nimport skimage.io\nimport cv2\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\n\nimport sklearn.metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold,GroupKFold\n\nfrom functools import partial\nfrom tqdm import tqdm\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nfrom torch.optim import Adam, SGD\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.models as models\n\nfrom albumentations import Compose, Normalize, HorizontalFlip, VerticalFlip,RandomGamma, RandomRotate90,GaussNoise,RGBShift,GaussianBlur\nfrom albumentations.pytorch import ToTensorV2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_torch(seed=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install efficientnet_pytorch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom efficientnet_pytorch import EfficientNet\nmodel = EfficientNet.from_name('efficientnet-b0')\nprint(model)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install geffnet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import geffnet\nmodel = geffnet.create_model('efficientnet_b0', pretrained=True)\nmodel.classifier=nn.Identity()\na =torch.randn((10,3,512,512))\nprint(model(a.float()).size())\nprint(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(torch.cuda.current_device())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#データセット作成\nimport glob\npath_list=glob.glob(\"/kaggle/input/glaucomadataset/Glaucoma/*\")\nlabel_list = np.ones(len(path_list))\nn_path_list=glob.glob(\"/kaggle/input/glaucomadataset/Non Glaucoma/*\")\nn_label_list = np.zeros(len(n_path_list))\npath_list.extend(n_path_list)\nlabels = np.concatenate([label_list, n_label_list])\nprint(len(path_list),labels.shape)\ndf = pd.DataFrame(columns =[\"file\",\"label\"])\ndf[\"file\"] = path_list\ndf[\"label\"] = labels\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.sample(frac=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df[\"label\"]==1][\"file\"].values[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"china = pd.read_csv(\"/kaggle/input/panda-efnetb2-180-weight/china_gla.csv\")\nchina[\"file\"] = [\"/kaggle/input/ocular-disease-recognition-odir5k/ODIR-5K/ODIR-5K/Training Images/{}\".format(china[\"filename\"].values[i]) for i in range(len(china))]\nchina[\"label\"] = china[\"Gla\"]\nchina.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##concat china_data\n\nchina_ = china.drop([\"Unnamed: 0\",\"Patient Age\",\"ID\",\"Patient Sex\"],axis=1)\nchina_1 = china_.head(300)\nchina_0 = china_.tail(300)\nprint(china_.head())\nchina_0[\"from_china\"]=1\nchina_1[\"from_china\"]=1\n#df[\"from_china\"]=0\ncat_df = pd.concat([china_1,china_0])\nprint(cat_df.shape)\ncat_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport cv2\n%matplotlib inline\nidx=0\nimage = cv2.imread(cat_df['file'].values[idx])\nplt.imshow(image)\nplt.show()\nimport math\ndef get_pad_width(im, new_shape, is_rgb=True):\n    pad_diff = new_shape - im.shape[0], new_shape - im.shape[1]\n    t, b = math.floor(pad_diff[0]/2), math.ceil(pad_diff[0]/2)\n    l, r = math.floor(pad_diff[1]/2), math.ceil(pad_diff[1]/2)\n    if is_rgb:\n        pad_width = ((t,b), (l,r), (0, 0))\n    else:\n        pad_width = ((t,b), (l,r))\n    return pad_width\ndef crop_object(img, thresh=10, maxval=200, square=False):\n    \"\"\"\n    Source: https://stackoverflow.com/questions/49577973/how-to-crop-the-biggest-object-in-image-with-python-opencv\n    \"\"\"\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)# convert to grayscale\n    #plt.imshow(gray,cmap=\"gray\")\n    #plt.show()#普通に白黒のがみえる\n    # threshold to get just the signature (INVERTED)\n    retval, thresh_gray = cv2.threshold(gray, thresh=thresh, maxval=maxval, type=cv2.THRESH_BINARY)\n    #plt.imshow(thresh_gray,cmap=\"gray\")\n    #plt.show()\n    contours, hierarchy = cv2.findContours(thresh_gray,cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n    #https://qiita.com/anyamaru/items/fd3d894966a98098376c\n    # Find object with the biggest bounding box\n    mx = (0,0,0,0)      # biggest bounding box so far\n    mx_area = 0\n    for cont in contours:\n        x,y,w,h = cv2.boundingRect(cont)\n        area = w*h\n        if area > mx_area:\n            mx = x,y,w,h\n            mx_area = area\n    x,y,w,h = mx#(0,0,0,0)なのはcontoursに何も入ってないから\n    crop = img[y:y+h, x:x+w]\n    if square:\n        pad_width = get_pad_width(crop, max(crop.shape))\n        crop = np.pad(crop, pad_width=pad_width, mode='constant', constant_values=255)\n    return crop\n\ncroped= crop_object(image)\nplt.imshow(croped)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrainDataset(Dataset):\n    def __init__(self, df,crop=True,transform1=None, transform2=None):\n        self.df = df\n        self.crop =crop\n        self.transform = transform1\n        self.transform_ = transform2\n        \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        file_path = self.df['file'].values[idx]\n        image = skimage.io.MultiImage(file_path)[0]\n        if self.crop:\n            image = crop_object(image)\n        image = cv2.resize(image,(SIZE,SIZE))\n        label_ = self.df[\"label\"].values[idx]\n        if self.transform:\n            image = self.transform(image=image)['image']\n        if self.transform_:\n            image = self.transform_(image=image)['image']\n\n        \n            \n        label = torch.tensor(label_).long()\n        #print(label_,type(label_),label,label.size())\n        \n        return image, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##train_test_split\nfrom sklearn.model_selection import train_test_split\ntrain, test = train_test_split(cat_df, test_size=0.3,stratify = cat_df[\"label\"], random_state=2020)\ntrain = train.reset_index(drop=True)\ntest = test.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##train_valid_split\nif CFG.debug:\n    folds = train.sample(n=200, random_state=CFG.seed).reset_index(drop=True).copy()\nelse:\n    folds = train.copy()\ntrain_labels = folds[\"label\"].values\nkf = StratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\nfor fold, (train_index, val_index) in enumerate(kf.split(folds.values, train_labels)):\n    print(\"num_train,val\",len(train_index),len(val_index),len(val_index)+len(train_index))\n    folds.loc[val_index, 'fold'] = int(fold)\n\nfolds['fold'] = folds['fold'].astype(int)\nfolds.to_csv('folds.csv', index=None)\nfolds.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn.metrics as metric\n\ndef auc(y,y_hat):\n    return metric.roc_auc_score(y,y_hat)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_transforms1(*, data):\n\n    #train,valid以外だったら怒る\n    \n    if data == 'train':\n        return Compose([\n            HorizontalFlip(p=0.5),\n            VerticalFlip(p=0.5),\n            #GaussNoise(p=0.5),\n            #RandomRotate90(p=0.5),\n            #RandomGamma(p=0.5),\n            #RandomAugMix(severity=3, width=3, alpha=1., p=0.5),\n            #GaussianBlur(p=0.5),\n            #GridMask(num_grid=3, p=0.3),\n            #Cutout(p=0.5),\n            Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n        ])\n    \n    elif data == 'valid':\n        return Compose([\n            Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            )\n        ])\n\ndef to_tensor(*args):\n\n        return Compose([\n            ToTensorV2()\n        ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = torchvision.models.resnet18(pretrained =True)\nbase_model.fc = nn.Linear(base_model.fc.in_features, 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#画像の確認\nimport matplotlib.pyplot as plt\n%matplotlib inline\ndataset = TrainDataset(train.reset_index(drop=True), \n                                 transform1=None,transform2=None)#get_transforms1(data='train')\ndata_loader = DataLoader(dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=8)\nx=0\nfor img,label in data_loader:\n    img = img.detach().numpy()[x]\n    print(img.shape,label.detach().numpy()[x])\n    plt.imshow(img)\n    plt.show()\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor img,label in data_loader:\n    img = img.detach().numpy()[0].transpose(1,2,0)\n    plt.imshow(img)\n    plt.show()\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.model = torchvision.models.resnet18(pretrained =False)\n        self.model.fc = nn.Linear(self.model.fc.in_features, 1)\n        \n        \n    def forward(self, x):\n        x = self.model(x)#ベースのモデルの流れに同じ\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class efenet_Model(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.model = geffnet.efficientnet_b0(pretrained=True, drop_rate=0.25)\n        self.model.classifier = nn.Linear(self.model.classifier.in_features, 1)\n        \n        \n    def forward(self, x):\n        x = self.model(x)#ベースのモデルの流れに同じ\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class extract_Model(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.model = geffnet.create_model('efficientnet_b0', pretrained=True)\n        self.model.classifier=nn.Identity()\n    \n        \n    def forward(self, x):\n        x = self.model(x)#ベースのモデルの流れに同じ\n        return x\n    \nclass TrainDataset(Dataset):\n    def __init__(self, df, transform1=None, transform2=None):\n        self.df = df\n        self.transform = transform1\n        self.transform_ = transform2\n        \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        file_path = self.df['file'].values[idx]\n        image = skimage.io.MultiImage(file_path)[0]\n        image = cv2.resize(image,(SIZE,SIZE))\n        label = self.df[\"label\"].values[idx]\n        if self.transform:\n            image = self.transform(image=image)['image']\n        if self.transform_:\n            image = self.transform_(image=image)['image']\n\n        \n            \n        label = torch.tensor(label).float()\n        \n        return image, label\n\ndataset = TrainDataset(folds.reset_index(drop=True), \n                                 transform1=None,transform2=to_tensor())\nloader = DataLoader(dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=4)\n##extract vecotr from images\ntk0 = tqdm(enumerate(loader), total=len(loader))\nembeds =[]\nmodel = extract_Model()\nmodel.to(device)\nfor i, (images, labels) in tk0:\n    images = images.to(device)\n    labels = labels.to(device)\n    with torch.no_grad():\n        embed = model(images.float())\n    embeds.append(embed.cpu().detach().numpy())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeds_ =np.concatenate(embeds)\nprint(embeds_.shape,len(folds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"china_dataset = TrainDataset(cat_df.reset_index(drop=True),transform1=None,transform2=to_tensor())\nchina_loader = DataLoader(china_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=4)\ntk1 = tqdm(enumerate(china_loader), total=len(china_loader))\nembeds_china =[]\nfor i, (images, labels) in tk1:\n    images = images.to(device)\n    labels = labels.to(device)\n    with torch.no_grad():\n        embed = model(images.float())\n    embeds_china.append(embed.cpu().detach().numpy())\nembeds_china_ =np.concatenate(embeds_china)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn import manifold\nimport matplotlib.pyplot as plt\n%matplotlib inline\nmds = manifold.MDS(n_components=2, dissimilarity=\"euclidean\", random_state=6)\nmds_result = mds.fit_transform(embeds_)\n#where_from_data = folds[\"from_china\"]\nwhich_Gla = folds[\"label\"]\n\n#plt.scatter(mds_result[:, 0], mds_result[:, 1], c=where_from_data)\n#plt.show()\nplt.scatter(mds_result[:, 0], mds_result[:, 1], c=which_Gla)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import umap\nembedding = umap.UMAP().fit_transform(embeds_)\n#plt.scatter(embedding[:, 0], embedding[:, 1], c=where_from_data)\n#plt.show()\nplt.scatter(embedding[:, 0], embedding[:, 1], c=which_Gla)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\ntsne_model = TSNE(n_components=2)\ntsne = tsne_model.fit_transform(embeds_)\n#plt.scatter(tsne[:, 0], tsne[:, 1], c=where_from_data)\n#plt.show()\nplt.scatter(tsne[:, 0], tsne[:, 1], c=which_Gla)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"glaucomadataset由来のデータは学習済みモデルに通したベクトルを次元圧縮すると割とはっきり疾患か否かが別れていることがわかる。"},{"metadata":{"trusted":true},"cell_type":"code","source":"which_Gla = cat_df[\"label\"]\nembedding_ = umap.UMAP().fit_transform(embeds_china_)\n#plt.scatter(embedding[:, 0], embedding[:, 1], c=where_from_data)\n#plt.show()\nplt.scatter(embedding_[:, 0], embedding_[:, 1], c=which_Gla)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne_model = TSNE(n_components=2)\ntsne = tsne_model.fit_transform(embeds_china_)\n#plt.scatter(tsne[:, 0], tsne[:, 1], c=where_from_data)\n#plt.show()\nplt.scatter(tsne[:, 0], tsne[:, 1], c=which_Gla)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mds_result_ = mds.fit_transform(embeds_china_)\nplt.scatter(mds_result_[:, 0], mds_result_[:, 1], c=which_Gla)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OScularのデータセット（以下中国産のデータセットと呼ぶ）は分離が難しそう。"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fn(fold):\n    print(f\"### fold: {fold} ###\")\n\n        \n    trn_idx = folds[folds['fold'] != fold].index\n    val_idx = folds[folds['fold'] == fold].index\n    train_dataset = TrainDataset(folds.loc[trn_idx].reset_index(drop=True), \n                                 transform1=None,transform2=to_tensor())#get_transforms1(data='train')\n    valid_dataset = TrainDataset(folds.loc[val_idx].reset_index(drop=True), \n                                 transform1=None,transform2=to_tensor())#get_transforms1(data='valid')\n    \n    \n    train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=4)\n    valid_loader = DataLoader(valid_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=4)\n    model = efenet_Model()\n    #model = Model()\n    \n    model.to(device)\n    \n    optimizer = Adam(model.parameters(), lr=CFG.lr, amsgrad=False)\n    #scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbose=True, eps=1e-6)\n    #scheduler = CosineAnnealingLR(optimizer, T_max=20, eta_min=0.001)\n    \n    criterion = nn.BCELoss()#weight = class_weight\n    best_score = -100\n    best_loss = np.inf\n    best_preds = None\n    \n    \n    for epoch in range(CFG.epochs):\n        \n        start_time = time.time()\n\n        model.train()\n        \"\"\"\n        if epoch <3:\n            for param in model.parameters():\n                param.requires_grad = False\"\"\"\n        avg_loss = 0.\n\n        optimizer.zero_grad()\n        tk0 = tqdm(enumerate(train_loader), total=len(train_loader))\n\n        for i, (images, labels) in tk0:\n            #if i ==0:\n                #print(images.size())\n\n            images = images.to(device)\n            labels = labels.to(device)\n            \n            y_preds = model(images.float())\n            y_preds = torch.sigmoid(y_preds.view(-1))\n            #if i ==0:\n                #print(y_preds.size(),labels.size())#同じ\n            loss = criterion(y_preds, labels)\n            #loss = criterion(y_preds.view, labels)\n            \n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            avg_loss += loss.item() / len(train_loader)\n        model.eval()\n        avg_val_loss = 0.\n        preds = []\n        valid_labels = []\n        tk1 = tqdm(enumerate(valid_loader), total=len(valid_loader))\n\n        for i, (images, labels) in tk1:\n            \n            images = images.to(device)\n            labels = labels.to(device)\n            \n            with torch.no_grad():\n                y_preds = model(images.float())\n                \n                y_preds = torch.sigmoid(y_preds.view(-1))\n            preds.append(y_preds.to('cpu').numpy())\n            valid_labels.append(labels.to('cpu').numpy())\n\n            loss = criterion(y_preds, labels)\n            avg_val_loss += loss.item() / len(valid_loader)\n        \n        #scheduler.step(avg_val_loss)\n            \n        preds = np.concatenate(preds)\n        #print(preds.shape)\n        valid_labels = np.concatenate(valid_labels)\n\n        score = auc(valid_labels,preds)\n\n        elapsed = time.time() - start_time\n        print(f'  Epoch {epoch+1} - avg_train_loss: {avg_loss:.6f}  avg_val_loss: {avg_val_loss:.6f}  time: {elapsed:.0f}s')\n        print(f'  Epoch {epoch+1} - AUC: {score}')\n        \n        if score>best_score:#aucのスコアが良かったら予測値を更新...best_epochをきめるため\n            best_score = score\n            best_preds = preds\n            print(\"====\",f'  Epoch {epoch+1} - Save Best Score: {best_score:.4f}',\"===\")\n            torch.save(model.state_dict(), f'fold{fold}_resnet18_baseline.pth')#各epochのモデルを保存。。。best_epoch終了時のモデルを推論に使用する？\n    \n    return best_preds, valid_labels,model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = []\nvalid_labels = []\nmodels =[]\nfor fold in range(CFG.n_fold):\n    _preds, _valid_labels,_model = train_fn(fold)\n    preds.append(_preds)\n    valid_labels.append(_valid_labels)\n    models.append(_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##\npreds_ = np.concatenate(preds)\nvalid_labels_ = np.concatenate(valid_labels)\n\nscore = auc(valid_labels_,preds_)\nimport datetime\n\ndt_now = datetime.datetime.now()\nprint(\"現在時刻\",dt_now)\nprint(\"=====AUC(CV)======\",score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.DataFrame()\ntrain_df[\"predict\"] = preds\ntrain_df[\"label\"] = valid_labels\ntrain_df[\"abs_pred-true\"] = np.abs(train_df[\"predict\"]-train_df[\"label\"])\n#train_df = train_df.sort_values('abs_pred-true', ascending=False)\ntrain_df.head(130)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\nfpr, tpr, thresholds = metrics.roc_curve(valid_labels_, preds_)\nimport matplotlib.pyplot as plt\nplt.plot(fpr, tpr, label='ROC curve (area = %.2f)'%score)\nplt.legend()\nplt.title('ROC curve')\nplt.xlabel('False Positive Rate(1-Specificity)')\nplt.ylabel('True Positive Rate(Recall)')\nplt.grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, df, transform1=None, transform2=None):\n        self.df = df\n        self.transform = transform1\n        self.transform_ = transform2\n        \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        file_path = self.df['file'].values[idx]\n        image = skimage.io.MultiImage(file_path)[0]\n        image = cv2.resize(image,(SIZE,SIZE))\n        label = self.df[\"label\"].values[idx]\n        if self.transform:\n            image = self.transform(image=image)['image']\n        if self.transform_:\n            image = self.transform_(image=image)['image']\n\n        \n        return image\n    \nclass baseline_model(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        #self.model = torchvision.models.resnet18(pretrained =False)\n        #self.model.fc = nn.Linear(self.model.fc.in_features, 1)\n        self.model = geffnet.efficientnet_b0(pretrained=False, drop_rate=0.25)\n        self.model.classifier = nn.Linear(self.model.classifier.in_features, 1)\n        \n        \n    def forward(self, x):\n        x = self.model(x)#ベースのモデルの流れに同じ\n        return x\ndef fix_model_state_dict(state_dict):\n    from collections import OrderedDict\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        name = k\n        if name.startswith('model.'):\n            name = name[6:]  # remove 'model.' of dataparallel\n        new_state_dict[name] = v\n    return new_state_dict\n\ndef inference(model, test_loader, device):\n    \n    model.to(device) \n    \n    probs = []\n\n    for i, images in tqdm(enumerate(test_loader), total=len(test_loader)):\n            \n        images = images.to(device)\n            \n        with torch.no_grad():\n            y_preds = model(images)\n            y_preds = torch.sigmoid(y_preds.view(-1))\n            \n        probs.append(y_preds.to('cpu').numpy())\n\n    probs = np.concatenate(probs)\n    \n    return probs\n\ndef submit():\n        print('run inference')\n        test_dataset = TestDataset(test, transform1=get_transforms1(data='valid'),transform2=to_tensor())\n        test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False)\n        probs = []\n        for fold in range(CFG.n_fold):\n            weights_path = \"/kaggle/working/fold{}_resnet18_baseline.pth\".format(fold)\n            model = baseline_model()\n            state_dict = torch.load(weights_path,map_location=device)\n            model.load_state_dict(state_dict)\n            _probs = inference(model, test_loader, device)\n            probs.append(_probs)\n        probs = np.mean(probs, axis=0)\n        return probs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['predict'] = submit()\nprint(test.head())\nscore = auc(test['label'].values[:],test['predict'])\nprint(\"=====AUC(inner_test)======\",score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"def submit():\n        print('run inference')\n        test_dataset = TestDataset(df, transform1=get_transforms1(data='valid'),transform2=to_tensor())\n        test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False)\n        probs = []\n        for fold in range(CFG.n_fold):\n            weights_path = \"/kaggle/working/fold{}_resnet18_baseline.pth\".format(fold)\n            model = baseline_model()\n            state_dict = torch.load(weights_path,map_location=device)\n            model.load_state_dict(state_dict)\n            _probs = inference(model, test_loader, device)\n            probs.append(_probs)\n        probs = np.mean(probs, axis=0)\n        return probs\n\ndf['predict'] = submit()\nprint(df.head())\nscore = auc(df['label'].values[:],df['predict'])\nprint(\"=====AUC(inner_test)======\",score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check test_df\npd.set_option('display.max_rows', 500)\ntest_df = test\ntest_df[\"abs_pred-true\"] = np.abs(test_df[\"predict\"]-test_df[\"label\"])\ntest_df = test_df.sort_values('abs_pred-true', ascending=False)\ntest_df.head(30)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mistake_file = test_df[\"file\"].values[0]\nprint(mistake_file)\nimage = skimage.io.MultiImage(mistake_file)[0]\nimage = cv2.resize(image,(SIZE,SIZE))\nplt.imshow(image)\nplt.show()\nprint(\"label:1,predict:\t0.150904\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mistake_file = test_df[\"file\"].values[1]\nprint(mistake_file)\nimage = skimage.io.MultiImage(mistake_file)[0]\nimage = cv2.resize(image,(SIZE,SIZE))\nplt.imshow(image)\nplt.show()\nprint(\"label:1,predict:0.306142\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"china = pd.read_csv(\"/kaggle/input/panda-efnetb2-180-weight/china_gla.csv\")\nchina.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfile_path = china['filename'].values[0]\nfile_path = \"/kaggle/input/ocular-disease-recognition-odir5k/ODIR-5K/Training Images/{}\".format(file_path)\nimage = cv2.imread(file_path)\nimage = cv2.resize(image,(SIZE,SIZE))\nplt.imshow(image)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = china.head(300)\nb = china.tail(300)\nchina = pd.concat([a,b])\nchina=china.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclass TestDataset_china(Dataset):\n    def __init__(self, df, transform1=None, transform2=None):\n        self.df = df\n        self.transform = transform1\n        self.transform_ = transform2\n        \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        file_path = self.df['filename'].values[idx]\n        file_path = \"/kaggle/input/ocular-disease-recognition-odir5k/ODIR-5K/Training Images/{}\".format(file_path)\n        image = cv2.imread(file_path)\n        image = cv2.resize(image,(SIZE,SIZE))\n        label = self.df[\"Gla\"].values[idx]\n        if self.transform:\n            image = self.transform(image=image)['image']\n        if self.transform_:\n            image = self.transform_(image=image)['image']\n\n        \n        return image\n    \nweights_path = \"/kaggle/working/fold{}_resnet18_baseline.pth\".format(0)\nmodel = baseline_model()\nstate_dict = torch.load(weights_path,map_location=device)\nmodel.load_state_dict(state_dict)\ntest_dataset = TestDataset_china(china, transform1=get_transforms1(data='valid'),transform2=to_tensor())\ntest_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False)\n_probs = inference(model,test_loader, device)\n\nscore = auc(china['Gla'].values[:],_probs)\nprint(\"=====AUC(china_single_fold)======\",score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"china['Gla'].values[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_probs[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter('ignore')\n\nclass TestDataset(Dataset):\n    def __init__(self, df, transform1=None, transform2=None):\n        self.df = df\n        self.transform = transform1\n        self.transform_ = transform2\n        \n    def __len__(self):\n            return len(self.df)\n\n    def __getitem__(self, idx):\n        file_path = self.df['file'].values[idx]\n        image = skimage.io.MultiImage(file_path)[0]\n        image = cv2.resize(image,(SIZE,SIZE))\n        original = image\n        label = self.df[\"label\"].values[idx]\n        if self.transform:\n            image = self.transform(image=image)['image']\n        if self.transform_:\n            image = self.transform_(image=image)['image']\n\n        \n        return image,original,label\nweights_path = \"/kaggle/working/fold{}_resnet18_baseline.pth\".format(0)\nmodel = baseline_model()\nstate_dict = torch.load(weights_path,map_location=device)\nmodel.load_state_dict(state_dict)\ndef getCAM(img,weight_fc,j=0):\n    m = torchvision.models.resnet18(pretrained =False)\n    m.fc = nn.Linear(m.fc.in_features, 1)\n    state_dict = torch.load(\"/kaggle/working/fold{}_resnet18_baseline.pth\".format(0),map_location=device)\n    m.load_state_dict(fix_model_state_dict(state_dict))\n    m = nn.Sequential(*list(m.children())[:-2])\n    m.to(device)\n    with torch.no_grad():\n        feature_conv = m(img).cpu().detach().numpy()\n    bs, nc, h, w = feature_conv.shape\n    #print(bs)\n    cam = weight_fc.dot(feature_conv[j,:, :, ].reshape((nc, h*w)))\n    cam = cam.reshape(h, w)\n    cam = cam - np.min(cam)\n    cam_img = cam / np.max(cam)\n    return cam_img\n\ncheck = train\n\ntest_dataset = TestDataset(check, transform1=None,transform2=to_tensor())\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\nmodel.to(device) \n    \nprobs = []\nfig = plt.figure(figsize=(20, 15))\n\nfc_params = list(model.model.fc.parameters())\nweight = np.squeeze(fc_params[0].cpu().data.numpy())\nimg_size =512\n\nfor i, (images,original,label) in tqdm(enumerate(test_loader), total=len(test_loader)):\n    images = images.to(device)\n    with torch.no_grad():\n        y_preds = model(images.float())\n        y_preds = torch.sigmoid(y_preds.view(-1))\n    if i <10:\n        cur_images = images.cpu().permute(0,2,3,1).detach().numpy()\n        for j in range(cur_images.shape[0]):\n            #print(\"{0}バッチ目、{1}枚目\".format(i,j))\n            print('Label:{0}, Predict:{1}'.format(label.view(-1)[j], y_preds[j]))\n            ax = fig.add_subplot(100, 200, i+1, xticks=[], yticks=[])\n            plt.imshow(cv2.cvtColor(cur_images[j], cv2.COLOR_BGR2RGB))\n            #ax.set_title('Label:{0}, Predict:{1}'.format(label.view(-1)[j], y_preds[j]), fontsize=14)\n            plt.show()\n            heatmap = getCAM(images.float(), weight,j=j)\n            ax = fig.add_subplot(100, 200, i+1, xticks=[], yticks=[])\n            plt.imshow(cv2.cvtColor(cur_images[j], cv2.COLOR_BGR2RGB))\n            plt.imshow(cv2.resize(heatmap, (img_size, img_size), interpolation=cv2.INTER_LINEAR), alpha=0.5, cmap='jet')\n            plt.show()\n            #if j==0:break\n        \n            \n    \n    probs.append(y_preds.to('cpu').numpy())\n\nprobs = np.concatenate(probs)\nprint(\"AUC\",auc(check['label'].values[:],probs))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrainDataset(Dataset):\n    def __init__(self, df, transform1=None, transform2=None):\n        self.df = df\n        self.transform = transform1\n        self.transform_ = transform2\n        \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        file_path = self.df['file'].values[idx]\n        image = skimage.io.MultiImage(file_path)[0]\n        image = cv2.resize(image,(SIZE,SIZE))\n        label = self.df[\"label\"].values[idx]\n        origin_img = image\n        if self.transform:\n            image = self.transform(image=image)['image']\n        if self.transform_:\n            image = self.transform_(image=image)['image']\n\n        \n            \n        label = torch.tensor(label).float()\n        \n        return image, label,origin_img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn.functional as F\nclass SaveFeatures():\n    \"\"\" Extract pretrained activations\"\"\"\n    features = None\n    def __init__(self, m):\n        self.hook = m.register_forward_hook(self.hook_fn)\n    def hook_fn(self, module, input, output):\n        self.features = ((output.cpu()).data).numpy()\n    def remove(self):\n        self.hook.remove()\n\n\ndef getCAM_(feature_conv, weight_fc, class_idx):\n    #Heatmap取得\n    print(\"feature_conv\",feature_conv)#None\n    _, nc, h, w = feature_conv.shape\n    cam = weight_fc.dot(feature_conv[0,:, :, ].reshape((nc, h*w)))\n    cam = cam.reshape(h, w)\n    cam = cam - np.min(cam)\n    cam_img = cam / np.max(cam)\n    return cam_img\n\n\ndef plotGradCAM(model, final_conv, fc_params, train_loader, \n                row=2, col=4, img_size=256, device='cuda', original=False):\n    for param in model.parameters():\n        param.requires_grad = False\n    model.to(device)\n    model.eval()\n    # save activated_features from conv\n    activated_features = SaveFeatures(final_conv)\n    # save weight from fc\n    weight = np.squeeze(fc_params[0].cpu().data.numpy())\n    # original images\n    if original:\n        fig = plt.figure(figsize=(20, 15))\n        for i, (img, target, org_img) in enumerate(train_loader):\n            if i ==0:\n                print(img.size())#bs,h,w,c\n            img = img.permute(0, 3, 1, 2)\n            if i ==0:\n                print(img.size())#bs,c,h,w\n            output = model(img.to(device))\n            \n            pred_idx = torch.sigmoid(output).to('cpu').numpy()\n            if i ==0:\n                print(\"元画像\",org_img.size())#1, 512, 512, 3\n            cur_images = org_img.numpy().transpose((0,1,2,3))\n            if i ==0:\n                print(\"cur_images\",cur_images.shape)#1, 512, 3, 512\n            ax = fig.add_subplot(row, col, i+1, xticks=[], yticks=[])\n            plt.imshow(cv2.cvtColor(cur_images[0], cv2.COLOR_BGR2RGB))\n            ax.set_title('Label:{0}, Predict:{1}'.format(target[0], pred_idx[0]), fontsize=14)\n            if i == row*col-1:\n                break\n        plt.show()\n    # heatmap images\n    fig = plt.figure(figsize=(20, 15))\n    for i, (img, target, _) in enumerate(train_loader):\n        img = img.permute(0, 3, 1, 2)#bs,c,h,w\n        if i ==0:\n            print(\"val,img\",img.size())#1, 3, 512, 512\n            #print(\"check_label\",target)\n        output = model(img.to(device).float())\n        pred_idx = torch.sigmoid(output).to('cpu').numpy()#0~1\n        if i ==0:\n            print(\"pred_idx\",pred_idx)\n        cur_images = img.cpu().numpy().transpose((0,2,3,1))\n        if i ==0:\n            print(\"val,cur_images\",cur_images.shape)#1, 3, 512, 512\n        #heatmap = getCAM(activated_features.features, weight, pred_idx)\n        heatmap = getCAM(img, weight)\n        ax = fig.add_subplot(row, col, i+1, xticks=[], yticks=[])\n        plt.imshow(cv2.cvtColor(cur_images[0], cv2.COLOR_BGR2RGB))\n        plt.imshow(cv2.resize(heatmap, (img_size, img_size), interpolation=cv2.INTER_LINEAR), alpha=1, cmap='jet')\n        ax.set_title('Label:{0}, Predict:{1}'.format(target[0], pred_idx[0]), fontsize=14)\n        if i == row*col-1:\n            break\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_loaders = []\n# we use fold=0 model for Grad-CAM\nfor fold in [0]:\n    \n    # idx\n    val_idx = folds[folds['fold'] == fold].index # check by val data\n    #val_idx = folds[folds['fold'] != fold].index # check by train data\n    \n    # prepare each label loader\n    for i in range(2):\n        valid_dataset = TrainDataset(folds.loc[val_idx][folds[CFG.target_col]==i].reset_index(drop=True),  \n                                     transform1=get_transforms1(data='valid'))\n        valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False)\n        class_loaders.append(valid_loader)\nprint(class_loaders[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nlabel = 1\nweights_path = \"/kaggle/working/fold{}_resnet18_baseline.pth\".format(0)\nmodel = baseline_model(weights_path)\nplotGradCAM(model, final_conv, fc_params, class_loaders[label], img_size=SIZE, device=device, original=True)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nlabel = 0\nweights_path = \"/kaggle/working/fold{}_resnet18_baseline.pth\".format(0)\nmodel = baseline_model(weights_path)\nplotGradCAM(model, final_conv, fc_params, class_loaders[label], img_size=SIZE, device=device, original=True)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}