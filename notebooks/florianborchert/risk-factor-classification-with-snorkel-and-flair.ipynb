{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Weakly supervised classification of risk factors\n\nIn order to more precisely analyze risk factors, we trained a binary sentence classifier to distinguish sentences that describe risk factors from those who do not. \n\nIn the absence of labelled data, we labelled sentences in a small set of abstracts manually with binary labels (`labelled_no_risk.tsv` and `labelled_prepared.tsv`).\nTo derive labels for the rest of data, we write noisy labeling functions based on heuristic rules and [scispaCy](https://allenai.github.io/scispacy/) NER models, which are denoised and turned into probabilistic labels using the [Snorkel](https://www.snorkel.org/) framework. The hand-labelled sentences are used as a development set for model tuning.\n\nThe resulting noisy labels are used to train a text classification model with [Flair](https://github.com/flairNLP/flair), which allows us to easily leverage contextualized word embeddings pre-trained on PubMed.\n\nWe present the **[results](#results)** using a pre-filtered dataframe containing only risk factor sentences (`covid_risk_sentences.tsv`) and using the already trained model (`final-model.pt`), in the beginning of the notebook, together with **[widgets](#widgets)** that allow you to browse the risk factors by UMLS categories.\n\nThe notebook also outputs **[CSV](#csv-output)** files of articles describing risk factors in the canonical format of https://www.kaggle.com/antgoldbloom/aipowered-literature-review-csvs, to facilitate the title / abstract screening phase of a systematic review on risk factors.\n\nThe model can easily be used to classify sentences in newly acquired scientific abstracts (see **[here](#flair-prediction)**), and we show how to create a new version of `covid_risk_sentences.tsv`.\n\nThe code to train **[everything from scratch](#train-scratch)**, to derive the programmatically labelled training dataset as well as the training of the **[generative label model with Snorkel](#snorkel)** and the **[classification model with Flair](#flair-training)** can be found at the end of the notebook.\n\n**Contributions**: A binary sentence classification model for risk factors, trained with weak supervision through noisy rules as well as the setup to train such a model with Snorkel and Flair.\n\n**Limitations**: We rely heavily on the UMLSLinker from scispaCy, which is currently in an alpha version and introduces some errors. Also, the dev set labels have not been reviewed by a medical experts. A larger and more represenative dev set would allow for better fine-tuning of both the label model and the classification model.\n\n**Acknowledgements**: We gratefully re-use some utility functions from https://www.kaggle.com/ajrwhite/covid19-tools for tagging the dataset and deriving heuristic labelling functions.\n\n**Note**: If you want to use the classifier to make predictions for large number of samples or train the classification model from scratch, you will likely want to do so using a GPU.\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"results\"></a>\n# Results"},{"metadata":{},"cell_type":"markdown","source":"## Prepare dependencies"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfrom tqdm import tqdm\nfrom pathlib import Path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We use scispaCy for UMLS entity recognition\n!pip install scispacy==0.2.4 https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_sm-0.2.4.tar.gz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"!pip install flair==0.4.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"custom_data_path = Path('/kaggle/input/covid-19-risk-classification/')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load prepared model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from flair.models import TextClassifier\nclf = TextClassifier.load(custom_data_path / 'final-model.pt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.predict('Risk is higher for men than women')[0].labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Displaying the pre-computed results"},{"metadata":{},"cell_type":"markdown","source":"We prepared a DataFrame than contains only sentences, which have been tagged as risk factors. We will explain how to contruct this dataset later on in the notebook."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Prepared results\ndf_covid_risk_sentences = pd.read_csv(custom_data_path / 'covid_risk_sentences.tsv', sep='\\t', index_col=['art_index', 'sent_index'])\ndf_covid_risk_sentences.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_colwidth', 0)\n\ndef get_risk_sentences(term):\n    res = df_covid_risk_sentences[df_covid_risk_sentences.canonical.str.lower() == term][['cui', 'sentence', 'title', 'cord_uid']]\n    return res.groupby(res.index).first()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For example, you may find that the literature mentions *chronic kidney diseases* as risk factors."},{"metadata":{"trusted":true},"cell_type":"code","source":"get_risk_sentences('chronic kidney diseases')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"... or certain populations or professional groups, such as *nurses*"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_risk_sentences('nurses')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"... risky behaviour, such as *alcohol consumption*\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_risk_sentences('alcohol consumption')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"... co-infections, e.g., with *bacteria*"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_risk_sentences('bacteria')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"csv-output\"></a>\n### CSV Output in canonical format\n\nPrepare output for https://www.kaggle.com/antgoldbloom/aipowered-literature-review-csvs"},{"metadata":{"trusted":true},"cell_type":"code","source":"os.mkdir('Risk Factor')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import regex as re\n\ncategories = ['comorbidity', 'population', 'behaviour', 'infection']\n\nfor category in categories:\n    folder = 'Risk Factor/' + category\n    if not os.path.exists(folder):\n        os.mkdir(folder)\n\n    df_risk = df_covid_risk_sentences[df_covid_risk_sentences.category == category]\n    values = df_risk.groupby('canonical').sentence.count().sort_values(ascending=False)\n\n    for factor in values.index:\n        result = pd.DataFrame(columns=['Date','Study','Study Link', 'Journal', 'Severe','Severe Significant','Severe Age Adjusted',\n                                       'Severe OR Calculated or Extracted','Fatality','Fatality Significant','Fatality Age Adjusted',\n                                       'Fatality OR Calculated or Extracted','Design','Sample','Study Population'])\n        result[['Date', 'Study', 'Study Link', 'Journal']] = df_risk[df_risk.canonical == factor].reset_index()[['publish_time', 'title', 'url', 'journal']].drop_duplicates()\n\n        result.to_csv(folder + '/' + re.sub('\\W', '_', factor) + '.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"widgets\"></a>\n### Examine risk factors by UMLS semantic type"},{"metadata":{},"cell_type":"markdown","source":"Using the widgets below, you are able to browse the term that occur in sentences tagged as describing risk factors. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import ipywidgets as widgets\nfrom IPython.display import display, Markdown, clear_output\n\ndef get_risk_factor_widget(df_risk):\n    buttons = []\n\n    output = widgets.Output(layout=widgets.Layout(width='50%'))\n\n    values = df_risk.groupby('canonical').sentence.count().sort_values(ascending=False)\n    \n    for i in values.index:\n        b = widgets.Button(\n            description='(' + str(values.loc[i]) + ') ' + i,\n            disabled=False,\n            tooltip='Click me',\n        )\n        b.item = i\n\n        def on_button_clicked(b):\n            vals = df_risk.loc[df_risk.canonical == b.item]\n            with output:\n                clear_output()\n                display(Markdown('# ' + b.item))\n                for i in vals.index.drop_duplicates().values:\n                    #display(vals)\n                    display(Markdown('** Article: ** ' + vals.loc[[i]].iloc[0].title))\n                    display(Markdown('** Sentence: ** ' + vals.loc[[i]].iloc[0].sentence))\n                    display(Markdown('** Terms: ** ' + ', '.join(vals.loc[[i]].term)))\n                    display(Markdown('---'))\n\n        b.on_click(on_button_clicked)\n        buttons.append(b)\n\n    return widgets.HBox(\n        [widgets.Box(buttons, layout=widgets.Layout(width='50%',display='inline-flex',flex_flow='row wrap')), output],\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Explore Populations at risk"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_risk_factor_widget(df_covid_risk_sentences[df_covid_risk_sentences.category == 'population'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Explore Comorbidities"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_risk_factor_widget(df_covid_risk_sentences[df_covid_risk_sentences.category == 'comorbidity'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Explore behaviour"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_risk_factor_widget(df_covid_risk_sentences[df_covid_risk_sentences.category == 'behaviour'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Explore other infections"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_risk_factor_widget(df_covid_risk_sentences[df_covid_risk_sentences.category == 'infection'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"flair-prediction\"></a>\n# Using the Flair model for sentence classification"},{"metadata":{},"cell_type":"markdown","source":"We will now explain how to use the classifier to filter the full CORD-19 dataset to only sentences describing risk factors in order to obtain a fresh `covid_risk_sentences.tsv`."},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"%%time\n\nimport en_core_sci_sm\nimport scispacy\nfrom scispacy.umls_linking import UmlsEntityLinker\nfrom scispacy.abbreviation import AbbreviationDetector\nfrom scispacy.candidate_generation import CandidateGenerator\n\nnlp = en_core_sci_sm.load()\n\nabbreviation_pipe = AbbreviationDetector(nlp)\nnlp.add_pipe(abbreviation_pipe)\n\ncandidate_generator = CandidateGenerator()\n\nlinker = UmlsEntityLinker(resolve_abbreviations=True, candidate_generator=candidate_generator)\n\nnlp.add_pipe(linker)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reading prepared labelled data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read sentences that do not contain a risk factor at all\ndf_sentence_no_risk = pd.read_csv(custom_data_path / 'labelled_no_risk.tsv', sep='\\t', index_col=0)\ndf_sentence_no_risk['label'] = 0\n# Read sentences with candidate risk factor terms\ndf_dev_sentence = pd.read_csv(custom_data_path / 'labelled_prepared.tsv', sep='\\t', index_col=0)\ndf_dev_sentence['label'] = df_dev_sentence['label'] == 'risk_factor' \ndf_dev_sentence = pd.concat([df_dev_sentence, df_sentence_no_risk], sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dev_sentence.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset contains annotations of risk factors on the term level. For the sake of this task, we consider only the sentence level. Classifying which term is the actual risk factor in a sentence would probably be more complicated."},{"metadata":{"trusted":true},"cell_type":"code","source":"term_attrs = ['term', 'cui', 'tui', 'type_name', 'category']\n\ndef flatten_sentence_df(sent_df):\n    def reduce(item):\n        if item.name == 'label':\n            return any(list(item))\n        if item.name in term_attrs:\n            return list(item)\n        return item.iloc[0]\n\n    cols = ['tui', 'term', 'cui', 'type_name', 'category', 'sentence']\n    if 'label' in sent_df.columns:\n        cols += ['label']\n    return sent_df.groupby(['art_index', 'sent_index'])[cols].aggregate(reduce)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dev = flatten_sentence_df(df_dev_sentence)\nY_dev = df_dev.label.values.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fraction of positive instances\nY_dev.shape[0], Y_dev.sum(), Y_dev.sum() / Y_dev.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Utility functions to get labels from flair predictions with a confidence threshold to control false positive and false negative rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(clf, sentences):\n    return clf.predict(sentences, embedding_storage_mode='none')\n\ndef apply_threshold(pred_clf, pos_threshold=0.5):\n    def get_pos_prob(p):\n        return p.labels[0].score if p.labels[0].value == '1' else (1 - p.labels[0].score)\n        \n    return np.array([(1 if get_pos_prob(p) >= pos_threshold else 0) for p in pred_clf])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get predictions on the dev set"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndev_pred_sent = predict(clf, list(df_dev.sentence.values))\ndev_pred = apply_threshold(dev_pred_sent, 0.5)\ndev_pred.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n\ndef print_metrics(y_true, y_pred):\n    print('Precision %.2f' % precision_score(y_true, y_pred))\n    print('Recall %.2f' % recall_score(y_true, y_pred))\n    print('F1-Score %.2f' % f1_score(y_true, y_pred))\n    print(confusion_matrix(y_true, y_pred)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_metrics(Y_dev, dev_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Utility functions for UMLS term extraction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# UMLS TUIs we care about\nroot_tuis = dict(\n    comorbidity = ['T046', 'T184'],\n    population = ['T096', 'T032'],\n    behaviour = ['T053'],\n    infection = ['T004', 'T005', 'T007']\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get UMLS subtypes as well\ndef expand_tree(semantic_type):\n    tuis = [semantic_type]\n    for c in semantic_type.children:\n        tuis += expand_tree(c)\n    return tuis\n\ntui_mapping = {}\nfor k, v in root_tuis.items():\n    for seed_tui in v:\n        seed_type = linker.umls.semantic_type_tree.type_id_to_node[seed_tui]\n        for semantic_type in expand_tree(seed_type):\n            tui = semantic_type.type_id\n            if tui in tui_mapping:\n                print('Error', tui, seed_tui)\n            tui_mapping[tui] = k, semantic_type","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_umls_match(spacy_entity, conf_threshold=0.8):\n    for umls_ent in spacy_entity._.umls_ents:\n        if umls_ent[1] < conf_threshold:\n            continue\n        umls_match = linker.umls.cui_to_entity[umls_ent[0]]\n        for tui in umls_match.types:\n            if tui in tui_mapping:\n                return umls_match, tui \n    return None, None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_sentence_term_df(df):\n    candidates = []\n\n    for i, article in tqdm(df.iterrows(), total=df.shape[0]):\n        doc = nlp(article.abstract)\n        for sid, s in enumerate(doc.sents):\n            for ent in s.ents:\n                umls_match, tui = get_umls_match(ent)\n                if umls_match:\n                    candidates.append({\n                        'art_index' : i,\n                        'sent_index' : sid,\n                        'term' : ent.text,\n                        'cui' : umls_match.concept_id,\n                        'canonical': umls_match.canonical_name,\n                        'span' : (ent.start, ent.end),\n                        'tui' : tui,\n                        'category' : tui_mapping[tui][0],\n                        'type_name' : tui_mapping[tui][1].full_name,\n                        'sentence' : s.text,\n                        #'spacy_entity' : ent,\n                        #'spacy_sentence' : s\n                    })\n\n    return pd.merge(pd.DataFrame(candidates), df, left_on='art_index', right_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Apply model to the full dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_path = Path('/kaggle/input/CORD-19-research-challenge')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(os.path.join(data_path / 'metadata.csv'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use these utilities to focus the risk factor analysis on articles concerning COVID-19"},{"metadata":{"trusted":true},"cell_type":"code","source":"%run /kaggle/usr/lib/covid19_tools/covid19_tools.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df, _ = count_and_tag(df, COVID19_SYNONYMS, 'disease_covid19')\nnovel_corona_filter = (abstract_title_filter(df, 'novel corona') &\n                       df.publish_time.str.startswith('2020', na=False))\nprint(f'novel corona (published 2020): {sum(novel_corona_filter)}')\ndf.loc[novel_corona_filter, 'tag_disease_covid19'] = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid_abstract = df[~df.abstract.isna() & df.tag_disease_covid19]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Extract UMLS terms, which are used to display the risk factors"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndf_covid_sentences = get_sentence_term_df(df_covid_abstract)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid_sentence_index = df_covid_sentences.groupby(['art_index', 'sent_index']).sentence.first()\ncovid_sentences = list(df_covid_sentence_index.values)\nlen(covid_sentences)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we apply the Flair model to all sentences.\n\n**WARNING:** This can take very long without GPU acceleration."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nsent_pred = predict(clf, covid_sentences)\nlabels = apply_threshold(sent_pred, 0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This will give you the filtered DataFrame we presented earlier\ndf_covid_risk_sentences = df_covid_sentences.set_index(['art_index', 'sent_index']).loc[df_covid_sentence_index.iloc[labels == 1].index]\ndf_covid_risk_sentences.to_csv('covid_risk_sentences.tsv', sep='\\t')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels.sum(), labels.shape[0], labels.sum() / labels.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"train-scratch\"></a>\n# Training the classification model from scratch with weak supervision\n\nHere we explain, how the model can be trained from scratch using the small labelled dataset introduced above + a large set of unlabelled sentences."},{"metadata":{},"cell_type":"markdown","source":"We will use Snorkel to derive programmatic labels"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install snorkel==0.9.5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use the prepared labelled dataset introduced above as a dev set and create a large, unlabelled training set in addition."},{"metadata":{"trusted":true},"cell_type":"code","source":"sars_synonyms = [r'\\bsars\\b',\n                 'severe acute respiratory syndrome']\nmers_synonyms = [r'\\bmers\\b',\n                 'middle east respiratory syndrome']\nards_synonyms = ['acute respiratory distress syndrome',\n                 r'\\bards\\b']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df, _ = count_and_tag(df, sars_synonyms + mers_synonyms + ards_synonyms,\n                      'disease_corona_general')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Construct all large, unlabelled training set with articles about Covid-19, SARS and MERS\ndf_train_articles = df[(df.tag_disease_covid19 | df.tag_disease_corona_general) & ~df.abstract.isna() & ~df.cord_uid.isin(df_dev_sentence['cord_uid'].unique())]\ndf_train_articles.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndf_train_sentence = get_sentence_term_df(df_train_articles)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = flatten_sentence_df(df_train_sentence)\ndf_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Desperately freeing up RAM by removing some unused objects"},{"metadata":{"trusted":true},"cell_type":"code","source":"del df_dev_sentence\ndel df_train_articles\ndel df_train_sentence\ndel df_covid_abstract\n\ndel abbreviation_pipe\ndel linker\ndel nlp\ndel candidate_generator\n\nimport gc; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"snorkel\"></a>\n### Create labelling functions with Snorkel"},{"metadata":{},"cell_type":"markdown","source":"We derive some heuristic rules for sentences including risk factors or for those who do not. Again we make heavy use of the utilities from https://www.kaggle.com/ajrwhite/covid19-tools"},{"metadata":{"trusted":true},"cell_type":"code","source":"from snorkel.labeling import labeling_function\n\nPOSITIVE = 1\nNEGATIVE = 0\nABSTAIN = -1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"risk_factor_synonyms = ['risk factor',\n                        'risk model',\n                        'risk by',\n                        'comorbidity',\n                        'comorbidities',\n                        'coexisting condition',\n                        'co existing condition',\n                        'clinical characteristics',\n                        'clinical features',\n                        'demographic characteristics',\n                        'demographic features',\n                        'behavioural characteristics',\n                        'behavioural features',\n                        'behavioral characteristics',\n                        'behavioral features',\n                        'predictive model',\n                        'prediction model',\n                        'univariate', # implies analysis of risk factors\n                        'multivariate', # implies analysis of risk factors\n                        'multivariable',\n                        'univariable',\n                        'odds ratio', # typically mentioned in model report\n                        'confidence interval', # typically mentioned in model report\n                        'logistic regression',\n                        'regression model',\n                        'factors predict',\n                        'factors which predict',\n                        'factors that predict',\n                        'factors associated with',\n                        'underlying disease',\n                        'underlying condition']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import regex as re\n\ndef zip_terms(item):\n    for _, row in pd.DataFrame(np.array([item[t] for t in term_attrs]).T, columns=term_attrs).iterrows():\n        yield row\n\ndef intersects(item_terms, terms):\n    return len(set(item_terms) & set(terms)) > 0\n\ndef find_in_sentence(sent, search_terms):\n    return [match for match in [re.search(term, sent) for term in search_terms] if match is not None]\n\ndef find_matches(x, search_terms):\n    return find_in_sentence(x.sentence.lower(), search_terms)\n\ndef count(x, search_terms):\n    return len(find_matches(x, search_terms))\n\ndef contains(x, search_terms):\n    return any(find_matches(x, search_terms))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RISK_TERMS = ['risk', 'susceptib', 'likel', 'higher', 'incr']\n\nGENERAL_PEOPLE_TERMS = [\n    'C0027361','C0027567', 'C1257890', 'C0687744', 'C0599755', 'C0337611', 'C2700280', 'C0679646']\n    \nDIABETES = ['C0011860', 'C0011849', 'C0011847']\nHYPERTENSION = ['C0020538']\nCARDIOVASCULAR = ['C0007222', 'C0497243', 'C0034072', 'C0010068']\nCANCER = ['C0006826', 'C0877578']\nHEALTH_PROBLEMS = ['C2963170']\nHOMELESSNESS = ['C0150041']\n\n# Population terms that are usually false positives\nPOPULATION_FP = ['nation', 'passenger', 'peer', \n                 'patient', 'differences', 'adult', \n                 'animal', 'species', 'human',\n                 'response', 'size', 'discharged', 'capabilit',\n                 'characteristics', 'heterogeneity', 'temperature', 'months', \n                 'abilities', 'clarity', 'viral', 'clinical', 'patients', \n                 'findings', 'HR',\n                 'problem', 'medication', \n                 'diagnosis', 'background',  'susceptib',\n                 'city', 'report', 'market', 'infection', 'capacit', 'interpretation']\n\nMEN = 'C0025266'\nWOMEN = 'C0043210'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@labeling_function()\ndef lf_no_population(x):\n    if 'population' in x.category:\n        return ABSTAIN\n    return NEGATIVE\n\n@labeling_function()\ndef lf_population_age_synonyms(x):\n    if contains(x, AGE_SYNONYMS):\n        return POSITIVE\n    return ABSTAIN\n\n@labeling_function()\ndef lf_population_sex_synonyms(x):\n    if count(x, SEX_SYNONYMS) >= 2 or intersects(x.cui, [MEN, WOMEN]):\n        return POSITIVE\n    return ABSTAIN\n\n@labeling_function()\ndef lf_population_terms(x):\n    for term in zip_terms(x):\n        if term.tui in ['T100', 'T032', 'T201']:\n            if (term.cui not in GENERAL_PEOPLE_TERMS) and not(len(find_in_sentence(term.term.lower(), POPULATION_FP)) > 0):\n                return POSITIVE\n    return ABSTAIN\n    \n@labeling_function()\ndef lf_known_comorbities(x):\n    for t in DIABETES + HYPERTENSION + CARDIOVASCULAR + CANCER + HEALTH_PROBLEMS + HOMELESSNESS:\n        if t in x.cui:\n            return POSITIVE\n    return ABSTAIN\n\n@labeling_function()\ndef lf_known_comorbities_synonmys(x):\n    if contains(x, DIABETES_SYNONYMS + HYPERTENSION_SYNONYMS + \n            CANCER_SYNONYMS + IMMUNITY_SYNONYMS + IMMUNODEFICIENCY_SYNONYMS + \n               ['underlying disease', 'cardiac injury', 'chronic diseases', 'homelessness', 'mental health (care|problems)']):\n        return POSITIVE\n    return ABSTAIN\n\nOUTCOME_TERMS = ['died', 'death', 'mortality']\n\ndef is_not_disease_fp(t):\n    return len(find_in_sentence(t.term, COVID19_SYNONYMS + \n                                ['infect', 'virus', 'death', 'outcome',\n                                 'diseas', 'epidemic', \n                                 'ARDS', 'acute respiratory distress syndrome',\n                                 'pneum', 'fever'])) == 0\n\n@labeling_function()\ndef lf_disease_1(x):\n    if not contains(x, ['\\d%', 'CI']):\n        return ABSTAIN\n    if len([t for t in zip_terms(x) if t.tui == 'T047' and is_not_disease_fp(t)]) > 1:\n        return POSITIVE\n    return ABSTAIN\n\n@labeling_function()\ndef lf_disease_0(x):\n    if not contains(x, ['\\d%', 'CI']):\n        return ABSTAIN\n    if len([t for t in zip_terms(x) if t.tui == 'T047' and is_not_disease_fp(t)]) > 0:\n        return POSITIVE\n    return ABSTAIN\n\n@labeling_function()\ndef lf_pregnancies(x):\n    if contains(x, ['mothers', 'natal', 'born']):\n        return POSITIVE\n    return ABSTAIN\n\n@labeling_function()\ndef lf_first_sentence(x):\n    if x.name[1] == 0:\n        return NEGATIVE\n    return ABSTAIN\n\n@labeling_function()\ndef lf_smoking_synonyms(x):\n    if contains(x, SMOKING_SYNONYMS):\n        return POSITIVE\n    return ABSTAIN\n\n@labeling_function()\ndef lf_bodyweigth_synonyms(x):\n    if contains(x, BODYWEIGHT_SYNONYMS):\n        return POSITIVE\n    return ABSTAIN\n\n@labeling_function()\ndef lf_old(x):\n    if contains(x, [r'\\sold', 'elderly']):\n        return POSITIVE\n    return ABSTAIN\n\n@labeling_function()\ndef lf_workers(x):\n    if 'T097' in x.tui and 'T047' in x.tui and any([t in x.sentence for t in OUTCOME_TERMS + risk_factor_synonyms + RISK_TERMS]):\n        return POSITIVE\n    return ABSTAIN\n\n@labeling_function()\ndef lf_short_sentences(x):\n    if len(x.sentence.split(' ')) < 6:\n        return NEGATIVE\n    return ABSTAIN\n\n@labeling_function()\ndef lf_hiv(x):\n    if 'HIV' in x.sentence:\n        return POSITIVE\n    return ABSTAIN\n\n@labeling_function()\ndef lf_symptoms(x):\n    if contains(x, ['symptom']):\n        return NEGATIVE\n    return ABSTAIN\n\n@labeling_function()\ndef lf_structure(x):\n    if contains(x, ['abstract', 'funding', 'design', 'objective', 'methods', 'importance', 'background', 'methods', 'collected']):\n        return NEGATIVE\n    return ABSTAIN\n\n@labeling_function()\ndef lf_ethnicity(x):\n    if contains(x, ['asian', 'white', 'ethnic']):\n        return POSITIVE\n    return ABSTAIN\n\n@labeling_function()\ndef lf_risk_specific(x):\n    if contains(x, ['more patients', \n                    '(associated|correlated) with higher',\n                    '(increased|higher) (morbidity|risk)',\n                    'more likely to',\n                    'worse outcome',\n                    'associated with \\w+ outcomes',\n                    'patients who died',\n                    'severity of COVID-19',\n                    'who (have become|were) infected',\n                    r'most of the \\w+ patients',\n                    'prone to',\n                    'predisposing factor',\n                    'association between',\n                    'contributes? to susceptibility'\n                    ]):\n        return POSITIVE\n    return ABSTAIN\n\n@labeling_function()\ndef lf_risk_unspecific(x):\n    if contains(x, risk_factor_synonyms):\n        return POSITIVE\n    return ABSTAIN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from snorkel.labeling import PandasLFApplier\n\nin_lfs = [\n    lf_risk_specific,\n    lf_risk_unspecific,\n    lf_population_terms,\n    lf_population_age_synonyms,\n    lf_population_sex_synonyms,\n    lf_disease_0,\n    lf_disease_1,\n    lf_known_comorbities,\n    lf_known_comorbities_synonmys,\n    lf_workers,\n    lf_old,\n    lf_pregnancies,\n    lf_hiv,\n    lf_smoking_synonyms,\n    lf_bodyweigth_synonyms,\n    lf_ethnicity,\n    lf_structure,\n    lf_no_population,\n    lf_short_sentences,\n    lf_symptoms,\n]\n\n# Catch all labelling function - we assume all completely unlabelled datapoints are negatives\ndef lf_all(in_lf):\n    @labeling_function()\n    def lf_combine(x):\n        for l in in_lf:\n            res = l(x)\n            if res == POSITIVE:\n                return ABSTAIN\n        return NEGATIVE\n    return lf_combine\n\nlfs = in_lfs + [lf_all(in_lfs)]\n\napplier = PandasLFApplier(lfs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"L_dev = applier.apply(df_dev)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Debugging the labelling functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"from snorkel.labeling import LFAnalysis\nLFAnalysis(L_dev, lfs).lf_summary(Y_dev).sort_values(['Emp. Acc.', 'Coverage'], ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apply the LFs to the complete training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nL_train = applier.apply(df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LFAnalysis(L_train, lfs).lf_summary().sort_values('Coverage', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train a generative label model that will account for conflicts and coverage of our LFs"},{"metadata":{"trusted":true},"cell_type":"code","source":"from snorkel.labeling.model import LabelModel\n\nlabel_model = LabelModel(cardinality=2, verbose=True)\nlabel_model.fit(L_train, Y_dev, n_epochs=5000, log_freq=500, seed=12345)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from snorkel.analysis import metric_score\nfrom snorkel.utils import probs_to_preds\n\nprobs_dev = label_model.predict_proba(L_dev)\npreds_dev = probs_to_preds(probs_dev)\n\nfor m in ['precision', 'recall', 'f1', 'roc_auc']:\n    print(\n        f\"Label model  {m} score: {metric_score(Y_dev, preds_dev, probs=probs_dev, metric=m)}\"\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Get the final labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"probs_train = label_model.predict_proba(L_train)\nY_train = probs_to_preds(probs_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Write the data to disk to use it with Flair"},{"metadata":{"trusted":true},"cell_type":"code","source":"ml_data = 'ml_data'\n\nif not os.path.exists(ml_data):\n    os.mkdir(ml_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def write_ml_data(df, labels, probs, fname):\n    pd.DataFrame(np.vstack([df['sentence'].values, labels, probs]).T).to_csv(os.path.join(ml_data, fname), index=False, sep='\\t', header=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"write_ml_data(df_train, Y_train, probs_train[:,1].ravel(), 'train.csv')\nwrite_ml_data(df_dev, Y_dev, Y_dev, 'dev.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"flair-training\"></a>\n## Train the classification model using Flair"},{"metadata":{"trusted":true},"cell_type":"code","source":"soft_label_map = {0 : 'text', 1 : '_', 2 : 'label'}\n#hard_label_map = {0 : 'text', 1 : 'label', 2 : '_'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from flair.datasets import CSVClassificationCorpus\n\nml_data = 'ml_data'\n\ncorpus = CSVClassificationCorpus(\n    data_folder=ml_data, \n    column_name_map=soft_label_map, \n    train_file='train.csv', \n    dev_file='dev.csv',\n    test_file='dev.csv',\n    skip_header=True,\n    in_memory=True,\n    delimiter='\\t',\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from flair.data import Dictionary\n\nlabel_dict = Dictionary(False)\nlabel_dict.add_item('0')\nlabel_dict.add_item('1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentRNNEmbeddings\nfrom flair.trainers import ModelTrainer\nfrom flair.models import TextClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We create a special TextClassifier that can deal with probabilistic labels, which are produced by Snorkel. In theory, we could round the Snorkel labels to 0 or 1, but this would lose some information. \n\n**Note:** The implementation is rather ad-hoc and can use some polishing. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport flair\nfrom typing import List, Union, Callable, Dict\nfrom flair.data import Dictionary, Sentence, Label, Token, space_tokenizer\n\nfrom torch.utils.data import DataLoader\nfrom flair.training_utils import (\n    convert_labels_to_one_hot,\n    Metric,\n    Result,\n    store_embeddings,\n)\n\n\nclass SoftTextClassifier(TextClassifier):\n    \n    def __init__(self, soft_loss, *args, **kwargs):\n        self.soft_loss = soft_loss\n        super().__init__(*args, **kwargs)\n    \n    def evaluate(\n        self,\n        data_loader: DataLoader,\n        out_path: Path = None,\n        embedding_storage_mode: str = \"none\",\n    ) -> (Result, float):\n\n        with torch.no_grad():\n            eval_loss = 0\n\n            metric = Metric(\"Evaluation\", beta=self.beta)\n\n            lines: List[str] = []\n            batch_count: int = 0\n            for batch in data_loader:\n\n                batch_count += 1\n\n                labels, loss = self.forward_labels_and_loss(batch)\n\n                eval_loss += loss\n\n                sentences_for_batch = [sent.to_plain_string() for sent in batch]\n                confidences_for_batch = [\n                    [label.score for label in sent_labels] for sent_labels in labels\n                ]\n                predictions_for_batch = [\n                    [label.value for label in sent_labels] for sent_labels in labels\n                ]\n                true_values_for_batch = [\n                    sentence.get_label_names() for sentence in batch\n                ]\n                available_labels = self.label_dictionary.get_items()\n\n                for sentence, confidence, prediction, true_value in zip(\n                    sentences_for_batch,\n                    confidences_for_batch,\n                    predictions_for_batch,\n                    true_values_for_batch,\n                ):\n                    eval_line = \"{}\\t{}\\t{}\\t{}\\n\".format(\n                        sentence, true_value, prediction, confidence\n                    )\n                    lines.append(eval_line)\n\n                for predictions_for_sentence, true_values_for_sentence in zip(\n                    predictions_for_batch, true_values_for_batch\n                ):\n\n                    y_pred = predictions_for_sentence[0]\n                    y_true = true_values_for_sentence[0]\n                    \n                    if y_true == '1':\n                        if y_pred == y_true:\n                            metric.add_tp('1')\n                        else:\n                            metric.add_fn('1')\n                    else:\n                        if y_pred == y_true:\n                            metric.add_tn('1')\n                        else:\n                            metric.add_fp('1')\n                            \n                    #import pdb; pdb.set_trace()\n                            \n                store_embeddings(batch, embedding_storage_mode)\n\n            eval_loss /= batch_count\n\n            detailed_result = (\n                f\"\\nMICRO_AVG: acc {metric.micro_avg_accuracy()} - f1-score {metric.micro_avg_f_score()}\"\n                f\"\\nMACRO_AVG: acc {metric.macro_avg_accuracy()} - f1-score {metric.macro_avg_f_score()}\"\n            )\n            for class_name in metric.get_classes():\n                detailed_result += (\n                    f\"\\n{class_name:<10} tp: {metric.get_tp(class_name)} - fp: {metric.get_fp(class_name)} - \"\n                    f\"fn: {metric.get_fn(class_name)} - tn: {metric.get_tn(class_name)} - precision: \"\n                    f\"{metric.precision(class_name):.4f} - recall: {metric.recall(class_name):.4f} - \"\n                    f\"accuracy: {metric.accuracy(class_name):.4f} - f1-score: \"\n                    f\"{metric.f_score(class_name):.4f}\"\n                )\n\n            result = Result(\n                main_score=metric.micro_avg_f_score(),\n                log_line=f\"{metric.precision()}\\t{metric.recall()}\\t{metric.micro_avg_f_score()}\",\n                log_header=\"PRECISION\\tRECALL\\tF1\",\n                detailed_results=detailed_result,\n            )\n\n            if out_path is not None:\n                with open(out_path, \"w\", encoding=\"utf-8\") as outfile:\n                    outfile.write(\"\".join(lines))\n\n            return result, eval_loss\n    \n    def custom_cross_entropy(self, i, target, size_average=True):\n        logsoftmax = torch.nn.LogSoftmax(dim=1)\n        if size_average:\n            return torch.mean(torch.sum(-target * logsoftmax(i), dim=1))\n        else:\n            return torch.sum(torch.sum(-target * logsoftmax(i), dim=1))\n\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n    \n    def _calculate_loss(self, scores, data_points):\n        if not self.soft_loss:\n            return super()._calculate_loss(scores, data_points)\n        \n        def get_prob_label(prob_str):\n            prob = float(prob_str)\n            return [1 - prob, prob]\n        \n        labels = torch.FloatTensor([get_prob_label(s.get_label_names()[0]) for s in data_points]).to(flair.device)\n        return self.custom_cross_entropy(scores, labels, size_average=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use embeddings pre-trained on PubMed"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"word_embeddings = [\n                    WordEmbeddings('glove'),\n                    FlairEmbeddings('pubmed-forward'),\n                    FlairEmbeddings('pubmed-backward'),\n                   ]\n\ndocument_embeddings: DocumentRNNEmbeddings = DocumentRNNEmbeddings(word_embeddings, hidden_size=128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = SoftTextClassifier(soft_loss=True, \n                                document_embeddings=document_embeddings, \n                                label_dictionary=label_dict)\n\ntrainer = ModelTrainer(classifier, corpus)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train the Flair model (this will take a very long time...). The resulting model will be saved in the folder 'tagger' and be used just as the provided `final-model.pt`."},{"metadata":{"trusted":true},"cell_type":"code","source":"#trainer.train('tagger',\n#              learning_rate=1e-01,\n#              mini_batch_size=32,\n#              anneal_factor=0.5,\n#              patience=10,\n#              max_epochs=10,\n#              embeddings_storage_mode='none')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}