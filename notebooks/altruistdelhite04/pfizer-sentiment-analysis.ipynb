{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install wordcloud\n","metadata":{"papermill":{"duration":8.872259,"end_time":"2021-03-19T06:41:33.468877","exception":false,"start_time":"2021-03-19T06:41:24.596618","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\n%matplotlib inline","metadata":{"papermill":{"duration":2.131234,"end_time":"2021-03-19T06:41:35.638804","exception":false,"start_time":"2021-03-19T06:41:33.50757","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/pfizer-vaccine-tweets/vaccination_tweets.csv').fillna('')","metadata":{"papermill":{"duration":0.222002,"end_time":"2021-03-19T06:41:35.897757","exception":false,"start_time":"2021-03-19T06:41:35.675755","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop([\"id\",\"user_created\"],axis=1,inplace=True)","metadata":{"papermill":{"duration":0.065612,"end_time":"2021-03-19T06:41:36.002135","exception":false,"start_time":"2021-03-19T06:41:35.936523","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(5)","metadata":{"papermill":{"duration":0.069144,"end_time":"2021-03-19T06:41:36.110354","exception":false,"start_time":"2021-03-19T06:41:36.04121","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install plotly","metadata":{"papermill":{"duration":6.790863,"end_time":"2021-03-19T06:41:42.939686","exception":false,"start_time":"2021-03-19T06:41:36.148823","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes\n","metadata":{"papermill":{"duration":0.051943,"end_time":"2021-03-19T06:41:43.030777","exception":false,"start_time":"2021-03-19T06:41:42.978834","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()\n","metadata":{"papermill":{"duration":0.07535,"end_time":"2021-03-19T06:41:43.145971","exception":false,"start_time":"2021-03-19T06:41:43.070621","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport plotly.express as px\npx.histogram(df, x=\"date\",  nbins=100,opacity=.5,title=\"Tweets by date\")\n","metadata":{"papermill":{"duration":2.638881,"end_time":"2021-03-19T06:41:45.825761","exception":false,"start_time":"2021-03-19T06:41:43.18688","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see the length of the tweets\nseq_length = [len(i) for i in df['text']]\n\npd.Series(seq_length).hist(bins = 25)","metadata":{"papermill":{"duration":0.318864,"end_time":"2021-03-19T06:41:46.189631","exception":false,"start_time":"2021-03-19T06:41:45.870767","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"num of words in text\"] = df[\"text\"].apply(lambda x: len(x))\nplt.figure(figsize=(10,7))\nsns.kdeplot(df[\"num of words in text\"],shade=True, color='m')\nplt.title(\"Distribution of words in text column\")\nplt.xlabel(\"Number of words\")\nplt.show()","metadata":{"papermill":{"duration":0.357063,"end_time":"2021-03-19T06:41:46.594241","exception":false,"start_time":"2021-03-19T06:41:46.237178","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dict_ = df['user_verified'].value_counts().to_dict()\ndict_['Verified'] = dict_.pop(True)\ndict_['Not-Verified'] = dict_.pop(False)\n\nplt.figure(figsize=(7,7))\nplt.pie(x=dict_.values(), labels=dict_.keys(), autopct='%1.1f%%', shadow=True, startangle=0, explode = [0.1, 0])\nplt.show()\n","metadata":{"papermill":{"duration":0.194195,"end_time":"2021-03-19T06:41:46.836525","exception":false,"start_time":"2021-03-19T06:41:46.64233","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MostUsedTweets = df.hashtags.value_counts().sort_values(ascending=False)[:5]\ncolors = ['lightcoral', 'lightskyblue', 'yellowgreen', 'pink', 'orange']\nexplode = (0.1, 0.2, 0.1, 0.1, 0.1) \n\n# Wedge properties \nwp = { 'linewidth' : 0.5, 'edgecolor' : \"red\" }\n\n# Creating autocpt arguments \ndef func(pct, allvalues): \n    absolute = int(pct / 100.*np.sum(allvalues)) \n    return \"{:.1f}%\\n({:d} g)\".format(pct, absolute) \n  \n# Creating the plot \nfig, ax = plt.subplots(figsize =(10, 7)) \nwedges, texts, autotexts = ax.pie(MostUsedTweets,  \n                                  autopct = lambda pct: func(pct, MostUsedTweets), \n                                  explode = explode,  \n                                  labels = MostUsedTweets.keys(), \n                                  shadow = True, \n                                  colors = colors, \n                                  startangle = 90, \n                                  wedgeprops = wp, \n                                  textprops = dict(color =\"black\")) \n  \n# Adding legend \nax.legend(wedges, MostUsedTweets.keys(), \n          title =\"Most used tweets\", \n          loc =\"center left\", \n          bbox_to_anchor =(1, 0, 0.5, 1)) \n\n\nplt.setp(autotexts, size=9, weight=\"bold\") \nax.set_title(\"Most used Hashtags\") \nplt.axis('equal')\nplt.show()","metadata":{"papermill":{"duration":0.325179,"end_time":"2021-03-19T06:41:47.210333","exception":false,"start_time":"2021-03-19T06:41:46.885154","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objects as go\ndf['tweet_date']=pd.to_datetime(df['date']).dt.date\ntweet_date=df['tweet_date'].value_counts().to_frame().reset_index().rename(columns={'index':'date','tweet_date':'count'})\ntweet_date['date']=pd.to_datetime(tweet_date['date'])\ntweet_date=tweet_date.sort_values('date',ascending=False)\n\nfig=go.Figure(go.Scatter(x=tweet_date['date'],\n                                y=tweet_date['count'],\n                               mode='markers+lines',\n                               name=\"Submissions\",\n                               marker_color='dodgerblue'))\n\nfig.update_layout(\n    title_text='Tweets per Day : ({} - {})'.format(df['tweet_date'].sort_values()[0].strftime(\"%d/%m/%Y\"),\n                                                       df['tweet_date'].sort_values().iloc[-1].strftime(\"%d/%m/%Y\")),template=\"plotly_dark\",\n    title_x=0.5)\n\nfig.show()","metadata":{"papermill":{"duration":0.171302,"end_time":"2021-03-19T06:41:47.435309","exception":false,"start_time":"2021-03-19T06:41:47.264007","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize=(18, 10))\n\nsns.distplot(df['hashtags'].dropna().apply(lambda x: len(x.split(','))).tolist(), kde=False, ax=ax[0], color='red')\nax[0].set_xlabel(\"Number of Hashtags\", weight='bold')\nax[0].set_ylabel('Number of Tweets', weight='bold')\n\nplt.show()","metadata":{"papermill":{"duration":0.466225,"end_time":"2021-03-19T06:41:47.953721","exception":false,"start_time":"2021-03-19T06:41:47.487496","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"date\"] = pd.to_datetime(df[\"date\"])\ndf[\"Month\"] = df[\"date\"].apply(lambda x : x.month)\ndf[\"day\"] = df[\"date\"].apply(lambda x : x.dayofweek)\ndmap = {0:'Mon',1:'Tue',2:'Wed',3:'Thu',4:'Fri',5:'Sat',6:'Sun'}\ndf[\"day\"] = df[\"day\"].map(dmap)\nplt.title(\"Day with maximun tweets\")\nsns.countplot(df[\"day\"])","metadata":{"papermill":{"duration":0.45452,"end_time":"2021-03-19T06:41:48.463547","exception":false,"start_time":"2021-03-19T06:41:48.009027","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\ny = df['is_retweet']\nfig, ax = plt.subplots(figsize=(7, 7))\ncount = Counter(y)\nax.pie(count.values(), labels=count.keys(), autopct=lambda p:f'{p:.2f}%')\nax.set_title('is_retweet?')\nplt.show()","metadata":{"papermill":{"duration":0.157852,"end_time":"2021-03-19T06:41:48.678091","exception":false,"start_time":"2021-03-19T06:41:48.520239","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5, 5))\nsns.countplot(x =\"user_verified\",data=df, palette=\"Set1\")\nplt.title(\" Verified VS Unverified Users\")\nplt.xticks([False,True],['Unverified','Verified'])\nplt.show()","metadata":{"papermill":{"duration":0.199484,"end_time":"2021-03-19T06:41:48.937008","exception":false,"start_time":"2021-03-19T06:41:48.737524","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_hashtags(x): \n    return str(x).replace('[', '').replace(']', '').split(',')\n\ndf = df.copy()\ndf['hashtag'] = df['hashtags'].apply(lambda row : split_hashtags(row))\ndf = df.explode('hashtag')\ndf['hashtag'] = df['hashtag'].astype(str).str.lower().str.replace(\"'\", '').str.replace(\" \", '')\ndf.loc[df['hashtag']=='', 'hashtag'] = 'NO HASHTAG'\n\n\nds = df['hashtag'].value_counts().reset_index()\nds.columns = ['hashtag', 'count']\nds = ds.sort_values(['count'],ascending=False)\nfig = sns.barplot(\n    x=ds.head(10)[\"count\"], \n    y=ds.head(10)['hashtag'], \n    orientation='horizontal', \n    #title='Top 20 hashtags', \n    #width=800, \n    #height=700\n).set_title('Top 10 hashtags')\n#fig.show()","metadata":{"papermill":{"duration":0.383385,"end_time":"2021-03-19T06:41:49.378136","exception":false,"start_time":"2021-03-19T06:41:48.994751","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import itertools\nfrom wordcloud import WordCloud\n\nlist_hashtags = df['hashtags'].dropna().str.lstrip('[').str.rstrip(']').str.replace(\"'\", \"\").str.split(', ').tolist()\nlist_hashtags = list(itertools.chain(*list_hashtags))\n\nplt.figure(figsize=(14,10))\nwordcloud = WordCloud(max_font_size=50, max_words=100,background_color=\"white\").generate(' '.join(list_hashtags))\nplt.imshow(wordcloud,interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","metadata":{"papermill":{"duration":0.778761,"end_time":"2021-03-19T06:41:50.214502","exception":false,"start_time":"2021-03-19T06:41:49.435741","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')","metadata":{"papermill":{"duration":0.229201,"end_time":"2021-03-19T06:41:50.510951","exception":false,"start_time":"2021-03-19T06:41:50.28175","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('punkt')\n","metadata":{"papermill":{"duration":0.160266,"end_time":"2021-03-19T06:41:50.741099","exception":false,"start_time":"2021-03-19T06:41:50.580833","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom wordcloud import WordCloud, STOPWORDS\n\nstopwords = set(STOPWORDS)\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='black',\n        stopwords=stopwords,\n        max_words=100,\n        max_font_size=40, \n        scale=5,\n        random_state=1\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(10,10))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","metadata":{"papermill":{"duration":0.086208,"end_time":"2021-03-19T06:41:50.899833","exception":false,"start_time":"2021-03-19T06:41:50.813625","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_wordcloud(df['text'], title = 'Prevalent words in tweets')","metadata":{"papermill":{"duration":0.79071,"end_time":"2021-03-19T06:41:51.762578","exception":false,"start_time":"2021-03-19T06:41:50.971868","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"india_df = df.loc[df.user_location==\"India\"]\nshow_wordcloud(india_df['text'], title = 'Prevalent words in tweets from India')","metadata":{"papermill":{"duration":0.751209,"end_time":"2021-03-19T06:41:52.587512","exception":false,"start_time":"2021-03-19T06:41:51.836303","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_list = df[\"text\"].to_list()\ntext = \"\"\nfor i in text_list:\n    text = text + i.split(\"https:\")[0]\n    \ntext = text.replace(\" \",\",\")\ntext = re.sub(\"[\\@\\#\\n\\.\\â€¦\\?\\\\\\'\\d\\)\\(\\%\\*]\", \",\", text)\ntext = re.sub(\",{2,}\", \",\", text)\n\ntext[:1000]","metadata":{"papermill":{"duration":0.278697,"end_time":"2021-03-19T06:41:52.943834","exception":false,"start_time":"2021-03-19T06:41:52.665137","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = text.split(',')\ntext[:10]\n","metadata":{"papermill":{"duration":0.110768,"end_time":"2021-03-19T06:41:53.133705","exception":false,"start_time":"2021-03-19T06:41:53.022937","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nfrom wordcloud import WordCloud\nimport nltk\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.probability import FreqDist\n\ndef stop_w(x):\n    new_s = []\n    for i in text:\n        if i.lower() not in stopwords.words(\"english\"):\n            new_s.append(i.lower())\n    return new_s\n\ntext = stop_w(text)\n\nplt.figure(figsize=(20,10))\nFreqDist(text).plot(30)","metadata":{"papermill":{"duration":28.255396,"end_time":"2021-03-19T06:42:21.469534","exception":false,"start_time":"2021-03-19T06:41:53.214138","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_count = pd.Series(text).value_counts()\nwc = WordCloud(width=1000, height=600, background_color=\"black\", random_state=0)\nplt.figure(figsize=(20,10),facecolor='w')\nplt.imshow(wc.generate_from_frequencies(text_count))\nplt.axis(\"off\")\nplt.show()","metadata":{"papermill":{"duration":1.88209,"end_time":"2021-03-19T06:42:23.431908","exception":false,"start_time":"2021-03-19T06:42:21.549818","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n%matplotlib inline\n\n\n","metadata":{"papermill":{"duration":0.106798,"end_time":"2021-03-19T06:42:23.633487","exception":false,"start_time":"2021-03-19T06:42:23.526689","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD,PCA\nfrom sklearn.feature_extraction.text import CountVectorizer\nnltk.download('vader_lexicon')\nfrom sklearn.cluster import KMeans\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport random\nplt.rc('figure',figsize=(17,13))\n\n","metadata":{"papermill":{"duration":0.352258,"end_time":"2021-03-19T06:42:24.079786","exception":false,"start_time":"2021-03-19T06:42:23.727528","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/pfizer-vaccine-tweets/vaccination_tweets.csv').fillna('')","metadata":{"papermill":{"duration":0.180818,"end_time":"2021-03-19T06:42:24.358593","exception":false,"start_time":"2021-03-19T06:42:24.177775","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = (set(stopwords.words('english')))\nsno = SnowballStemmer('english')\n\ndef remove_html_tags(sentence):\n    regex = re.compile(pattern='<.*?>')\n    clean_text = re.sub(regex, ' ', sentence)\n    return clean_text\n\ndef remove_punctuations(word):\n    cleaned_sentence = re.sub(pattern=r'[?|!|\\|\"|#|\\']', repl=r'', string=word)\n    cleaned_sentence = re.sub(pattern=r'[.|,|)|(|\\|/]', repl=r'', string=cleaned_sentence)\n    return cleaned_sentence\n\ndef get_preprocessed_data(data, feature, cleaned_feature):\n        \n        i = 0\n        final_string = []\n\n        sentences = data[feature].values\n        for sentence in sentences:\n            filtered_sentence = []\n            sentence = remove_html_tags(sentence)\n            for word in sentence.split():\n                for clean_word in remove_punctuations(word).split():\n                    if clean_word.isalpha() and len(clean_word) > 2:\n                        if clean_word.lower() not in stop_words:\n                            s = (sno.stem(clean_word.lower()))\n                            filtered_sentence.append(s)\n\n            string = \" \".join(filtered_sentence)\n            final_string.append(string)\n            i += 1\n        data[cleaned_feature] = final_string\n        return data\n\ndata = get_preprocessed_data(data, 'text', 'Tidy Tweet')\ndata = get_preprocessed_data(data, 'hashtags', 'Tidy hashtags')","metadata":{"papermill":{"duration":1.844732,"end_time":"2021-03-19T06:42:26.302556","exception":false,"start_time":"2021-03-19T06:42:24.457824","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentiment = SentimentIntensityAnalyzer()\ndef get_sentiment(data):\n    sentiment_list = []\n    for text in list(data['Tidy Tweet'].values):\n        if sentiment.polarity_scores(text)[\"compound\"] > 0:\n            sentiment_list.append(\"Positive\")\n        elif sentiment.polarity_scores(text)[\"compound\"] < 0:\n            sentiment_list.append(\"Negative\")\n        else:\n            sentiment_list.append(\"Neutral\")\n    return sentiment_list\n        \ndata['Sentiment'] = get_sentiment(data)\nsns.countplot(x=\"Sentiment\", data=data, palette=\"Set2\")\nprint(data.Sentiment.value_counts())","metadata":{"papermill":{"duration":1.896545,"end_time":"2021-03-19T06:42:28.299323","exception":false,"start_time":"2021-03-19T06:42:26.402778","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.099765,"end_time":"2021-03-19T06:42:28.496547","exception":false,"start_time":"2021-03-19T06:42:28.396782","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_word_cloud(sentiment):\n    stop_words = (set(stopwords.words('english')))\n    remove_words = ['vaccin', 'pfizerbiontech', 'coronavirus', 'pfizer', 'covid', 'covidvaccin', 'pfizervaccin']\n    stop_words = remove_words + list(stop_words)\n    plt.figure(figsize=[15,15])\n    clean_tweets= \"\".join(list(data[data['Sentiment']==sentiment]['Tidy Tweet'].values))\n    wordcloud = WordCloud(width=700,height=400, background_color='white',colormap='plasma', max_words=50, stopwords=stop_words, collocations=False).generate(clean_tweets)\n    plt.title(f\"Top 50 {sentiment} words used in tweets\", fontsize=20)\n    plt.imshow(wordcloud)\n    return plt.show()","metadata":{"papermill":{"duration":0.106535,"end_time":"2021-03-19T06:42:28.698747","exception":false,"start_time":"2021-03-19T06:42:28.592212","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['date'] = pd.to_datetime(data['date']).dt.date\nnegative_data = data[data['Sentiment']=='Negative'].reset_index()\npositive_data = data[data['Sentiment']=='Positive'].reset_index()\ngrouped_data_neg = negative_data.groupby('date')['Sentiment'].count().reset_index()\ngrouped_data_pos = positive_data.groupby('date')['Sentiment'].count().reset_index()\nmerged_data = pd.merge(grouped_data_neg, grouped_data_pos, left_on='date', right_on='date', suffixes=(' Negative', ' Positive'))\n\nmerged_data.plot(x='date', y=['Sentiment Negative', 'Sentiment Positive'], figsize=(14, 7), marker='o', xlabel='Date', ylabel='Count', title='Tweet count over a period of time')","metadata":{"papermill":{"duration":0.438822,"end_time":"2021-03-19T06:42:29.233615","exception":false,"start_time":"2021-03-19T06:42:28.794793","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndata.text = data.text.apply(lambda x:re.sub('@[^\\s]+','',x))\n\n#remove hashtags\ndata.text = data.text.apply(lambda x:re.sub(r'\\B#\\S+','',x))\n\n\n# Remove URLS\ndata.text = data.text.apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n\n# Remove all the special characters\ndata.text = data.text.apply(lambda x:' '.join(re.findall(r'\\w+', x)))\n\n#remove all single characters\ndata.text = data.text.apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))\n\n# Substituting multiple spaces with single space\ndata.text = data.text.apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))","metadata":{"papermill":{"duration":0.36589,"end_time":"2021-03-19T06:42:29.704505","exception":false,"start_time":"2021-03-19T06:42:29.338615","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sid = SIA()\ndata['sentiments']           = data['text'].apply(lambda x: sid.polarity_scores(' '.join(re.findall(r'\\w+',x.lower()))))\ndata['Positive Sentiment']   = data['sentiments'].apply(lambda x: x['pos']+1*(10**-6)) \ndata['Neutral Sentiment']    = data['sentiments'].apply(lambda x: x['neu']+1*(10**-6))\ndata['Negative Sentiment']   = data['sentiments'].apply(lambda x: x['neg']+1*(10**-6))\n\ndata.drop(columns=['sentiments'],inplace=True)","metadata":{"papermill":{"duration":1.68413,"end_time":"2021-03-19T06:42:31.491955","exception":false,"start_time":"2021-03-19T06:42:29.807825","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"papermill":{"duration":0.134057,"end_time":"2021-03-19T06:42:31.727124","exception":false,"start_time":"2021-03-19T06:42:31.593067","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Number of Words\ndata['Number_Of_Words'] = data.text.apply(lambda x:len(x.split(' ')))\n#Average Word Length\ndata['Mean_Word_Length'] = data.text.apply(lambda x:np.round(np.mean([len(w) for w in x.split(' ')]),2) )","metadata":{"papermill":{"duration":0.372951,"end_time":"2021-03-19T06:42:32.203563","exception":false,"start_time":"2021-03-19T06:42:31.830612","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(2,1,1)\nplt.title('Distriubtion Of Sentiments Across Our Tweets',fontsize=19,fontweight='bold')\nsns.kdeplot(data['Negative Sentiment'],bw=0.1)\nsns.kdeplot(data['Positive Sentiment'],bw=0.1)\nsns.kdeplot(data['Neutral Sentiment'],bw=0.1)\nplt.subplot(2,1,2)\nplt.title('CDF Of Sentiments Across Our Tweets',fontsize=19,fontweight='bold')\nsns.kdeplot(data['Negative Sentiment'],bw=0.1,cumulative=True)\nsns.kdeplot(data['Positive Sentiment'],bw=0.1,cumulative=True)\nsns.kdeplot(data['Neutral Sentiment'],bw=0.1,cumulative=True)\nplt.xlabel('Sentiment Value',fontsize=19)\nplt.show()","metadata":{"papermill":{"duration":0.922945,"end_time":"2021-03-19T06:42:33.226351","exception":false,"start_time":"2021-03-19T06:42:32.303406","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_word_cloud(sentiment='Positive')\n","metadata":{"papermill":{"duration":0.895596,"end_time":"2021-03-19T06:42:34.233782","exception":false,"start_time":"2021-03-19T06:42:33.338186","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_word_cloud(sentiment='Negative')\n","metadata":{"papermill":{"duration":0.824993,"end_time":"2021-03-19T06:42:35.17337","exception":false,"start_time":"2021-03-19T06:42:34.348377","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from plotly.subplots import make_subplots\n","metadata":{"papermill":{"duration":0.130019,"end_time":"2021-03-19T06:42:35.426674","exception":false,"start_time":"2021-03-19T06:42:35.296655","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nb_date_mean = data.groupby(by='date').mean().reset_index()\nb_date_std = data.groupby(by='date').std().reset_index()\n\n\nfig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Daily Average Positive Sentiment',  'Daily Average Negative Sentiment'))\n\nfig.add_trace(\n    go.Scatter(x=b_date_mean['date'], y=b_date_mean['Positive Sentiment'],name='Positive Sentiment Mean'),\n    row=1, col=1\n)\n\n    \n#positive mean\nfig.add_shape(type=\"line\",\n    x0=b_date_mean['date'].values[0], y0=b_date_mean['Positive Sentiment'].mean(), x1=b_date_mean['date'].values[-1], y1=b_date_mean['Positive Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean'\n)\n\nfig.add_annotation(x=b_date_mean['date'].values[3], y=b_date_mean['Positive Sentiment'].mean(),\n            text=r\"$\\mu : {:.2f}$\".format(b_date_mean['Positive Sentiment'].mean()),\n            showarrow=True,\n            arrowhead=3,\n            yshift=10)\n\n\n\nfig.add_trace(\n    go.Scatter(x=b_date_mean['date'], y=b_date_mean['Negative Sentiment'],name='Negative Sentiment Mean'),\n    row=2, col=1\n)\n\n#negative mean\nfig.add_shape(type=\"line\",\n    x0=b_date_mean['date'].values[0], y0=b_date_mean['Negative Sentiment'].mean(), x1=b_date_mean['date'].values[-1], y1=b_date_mean['Negative Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x2', \n        yref='y2'\n)\n\nfig.add_annotation(x=b_date_mean['date'].values[3], y=b_date_mean['Negative Sentiment'].mean(),\n            text=r\"$\\mu : {:.2f}$\".format(b_date_mean['Negative Sentiment'].mean()),\n            showarrow=True,\n            arrowhead=3,\n            yshift=10,\n            xref='x2', \n            yref='y2')\n\n\n\nfig.add_annotation(x=b_date_mean['date'].values[5], y=b_date_mean['Negative Sentiment'].mean()+0.01,\n            text=r\"Start Of Decline\",\n            showarrow=True,\n            arrowhead=6,\n            yshift=10,\n            xref='x2', \n            yref='y2')\n\nfig.add_annotation(x=b_date_mean['date'].values[15], y=.024,\n            text=r\"Start Of Incline\",\n            showarrow=True,\n            arrowhead=6,\n            yshift=10,\n            xref='x2', \n            yref='y2')\n\nfig['layout']['xaxis2']['title'] = 'Date'\nfig.update_layout(height=700, width=900, title_text=\"Sentiment Average Change With Time\")\nfig.show()","metadata":{"papermill":{"duration":0.251686,"end_time":"2021-03-19T06:42:35.800818","exception":false,"start_time":"2021-03-19T06:42:35.549132","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as ex","metadata":{"papermill":{"duration":0.130717,"end_time":"2021-03-19T06:42:36.053703","exception":false,"start_time":"2021-03-19T06:42:35.922986","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):    \n    text = str(text).lower()\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    \n    return text\ndata['text'] = data['text'].apply(lambda x:clean_text(x))","metadata":{"papermill":{"duration":0.185737,"end_time":"2021-03-19T06:42:36.362497","exception":false,"start_time":"2021-03-19T06:42:36.17676","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['text']\n","metadata":{"papermill":{"duration":0.13671,"end_time":"2021-03-19T06:42:36.621475","exception":false,"start_time":"2021-03-19T06:42:36.484765","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('wordnet')","metadata":{"papermill":{"duration":0.195072,"end_time":"2021-03-19T06:42:36.939675","exception":false,"start_time":"2021-03-19T06:42:36.744603","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.DataFrame()\ndf['text']=data['text']\ndef tokenization(text):\n    text = re.split('\\W+', text)\n    return text\n\ndf['tokenized'] = df['text'].apply(lambda x: tokenization(x.lower()))\nstopword = nltk.corpus.stopwords.words('english')\ndef remove_stopwords(text):\n    text = [word for word in text if word not in stopword]\n    return text\n    \ndf['No_stopwords'] = df['tokenized'].apply(lambda x: remove_stopwords(x))\n\nps = nltk.PorterStemmer()\n\ndef stemming1(text):\n    text = [ps.stem(word) for word in text]\n    return text\n\ndf['stemmed_porter'] = df['No_stopwords'].apply(lambda x: stemming1(x))\n\nfrom nltk.stem.snowball import SnowballStemmer\ns_stemmer = SnowballStemmer(language='english')\ndef stemming2(text):\n    text = [s_stemmer.stem(word) for word in text]\n    return text\ndf['stemmed_snowball'] = df['No_stopwords'].apply(lambda x: stemming2(x))\n\nwn = nltk.WordNetLemmatizer()\n\ndef lemmatizer(text):\n    text = [wn.lemmatize(word) for word in text]\n    return text\n\ndf['lemmatized'] = df['No_stopwords'].apply(lambda x: lemmatizer(x))","metadata":{"papermill":{"duration":5.69297,"end_time":"2021-03-19T06:42:42.764418","exception":false,"start_time":"2021-03-19T06:42:37.071448","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()\n","metadata":{"papermill":{"duration":0.149659,"end_time":"2021-03-19T06:42:43.04044","exception":false,"start_time":"2021-03-19T06:42:42.890781","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['text']=df['lemmatized']\ndata.head()","metadata":{"papermill":{"duration":0.157659,"end_time":"2021-03-19T06:42:43.324107","exception":false,"start_time":"2021-03-19T06:42:43.166448","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = data.groupby('Sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\ntemp.style.background_gradient(cmap='Greens')\n","metadata":{"papermill":{"duration":0.241654,"end_time":"2021-03-19T06:42:43.700189","exception":false,"start_time":"2021-03-19T06:42:43.458535","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nplt.figure(figsize=(12,6))\nsns.countplot(x='Sentiment',data=data)\nfig = go.Figure(go.Funnelarea(\n    text =temp.Sentiment,\n    values = temp.text,\n    title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of Sentiment Distribution\"}\n    ))\nfig.show()","metadata":{"papermill":{"duration":0.328419,"end_time":"2021-03-19T06:42:44.157126","exception":false,"start_time":"2021-03-19T06:42:43.828707","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import rcParams\n","metadata":{"papermill":{"duration":0.134666,"end_time":"2021-03-19T06:42:44.420822","exception":false,"start_time":"2021-03-19T06:42:44.286156","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2, ax3) = plt.subplots(3,1, figsize=(10, 16))\nsns.barplot(x=\"user_followers\", y=\"user_name\", orient=\"h\", ax=ax1, palette=[\"b\"],\n           data=data[(data.Sentiment== \"Positive\")]\\\n           .drop_duplicates(subset=[\"user_name\"])\\\n           .sort_values(by=[\"user_followers\"], ascending=False)[[\"user_name\", \"user_followers\"]][:10])\nax1.set_title('Top 10 Accounts with Highest Followers who tweet Positive')\nsns.barplot(x=\"user_followers\", y=\"user_name\", orient=\"h\", ax=ax2, palette=[\"g\"],\n           data=data[(data.Sentiment == \"Neutral\")]\n           .drop_duplicates(subset=[\"user_name\"])\\\n           .sort_values(by=[\"user_followers\"], ascending=False)[[\"user_name\", \"user_followers\"]][:10])\nax2.set_title('Top 10 Accounts with Highest Followers who tweet Neutral')\nsns.barplot(x=\"user_followers\", y=\"user_name\", orient=\"h\", ax=ax3, palette=[\"r\"],\n           data=data[(data.Sentiment == \"Negative\")]\n           .drop_duplicates(subset=[\"user_name\"])\\\n           .sort_values(by=[\"user_followers\"], ascending=False)[[\"user_name\", \"user_followers\"]][:10])\nax3.set_title('Top 10 Accounts with Highest Followers who tweet Negative')\n\nfig.show()","metadata":{"papermill":{"duration":0.991216,"end_time":"2021-03-19T06:42:45.539769","exception":false,"start_time":"2021-03-19T06:42:44.548553","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"date\"] = pd.to_datetime(data.date) \ntimeline = data.resample('D', on='date')[\"Sentiment\"].value_counts().unstack(1)\n\ntimeline.reset_index(inplace=True)\n\ntimeline = timeline.melt(\"date\", var_name='Sentiment',  value_name='vals')\n\nsns.set_style(\"whitegrid\")\nsns.lineplot(x=\"date\", y=\"vals\", hue=\"Sentiment\", data=timeline, palette=[\"r\", \"g\",\"b\"])\nplt.figure(figsize=(40,10))","metadata":{"papermill":{"duration":0.712578,"end_time":"2021-03-19T06:42:46.381111","exception":false,"start_time":"2021-03-19T06:42:45.668533","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Summary statistics of numerical features : \\n\", data.describe())\n","metadata":{"papermill":{"duration":0.186383,"end_time":"2021-03-19T06:42:46.705176","exception":false,"start_time":"2021-03-19T06:42:46.518793","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['year']         = pd.DatetimeIndex(data['date']).year\nimport datetime\nb_date_count = data.groupby(by='date').count().reset_index()\nb_date_count = b_date_count.rename(columns={'id':'Tweets Per Day'})\nfig = ex.line(b_date_count,x='date',y='Tweets Per Day')\n\n# fig.add_annotation(x=b_date_mean['date'].values[15], y=.024,\n#             text=r\"Start Of Incline\",\n#             showarrow=True,\n#             arrowhead=6,\n#             yshift=10)\n\n\nfig.add_shape(type=\"line\",\n    x0=b_date_count['date'].values[0], y0=b_date_count['Negative Sentiment'].mean(), x1=b_date_count['date'].values[-1], y1=b_date_count['Negative Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n)\n\nfig.update_traces(mode=\"markers+lines\")\nfig.update_layout(hovermode=\"x unified\")\n\n\n###annots\nb_date_count.date = pd.to_datetime(b_date_count.date)\nb_date_count_dt = b_date_count.set_index('date')\nfig.add_annotation(x=datetime.datetime(2021,2,19), y=b_date_count_dt.loc[pd.Timestamp('2021-02-19'),'year'],\n            text=r\"Israeli study finds Pfizer vaccine 85% effective after first shot\",\n            showarrow=True,\n            arrowhead=3,\n            yshift=5,bordercolor=\"#c7c7c7\")\n\nfig.add_annotation(x=datetime.datetime(2021,1,29), y=b_date_count_dt.loc[pd.Timestamp('2021-01-29'),'year'],\n            text=r\"vaccine found to be effective against variant discovered in U.K.\",\n            showarrow=True,\n            arrowhead=3,\n            yshift=5,ay=-160,bordercolor=\"#c7c7c7\")\nfig.add_annotation(x=datetime.datetime(2021,1,8), y=b_date_count_dt.loc[pd.Timestamp('2021-01-8'),'year'],\n            text=r\"Commission proposes to purchase up to 300 million additional doses of BioNTech-Pfizer vaccine\",\n            showarrow=True,\n            arrowhead=3,\n            yshift=5,ay=-30,bordercolor=\"#c7c7c7\")\n\nfig.add_annotation(x=datetime.datetime(2021,1,20), y=b_date_count_dt.loc[pd.Timestamp('2021-01-20'),'year'],\n            text=r\"The presidency of Joe Biden began\",\n            showarrow=True,\n            arrowhead=3,\n            yshift=3,ay=120,bordercolor=\"#c7c7c7\")\n\nfig.update_layout(title='<b> TREND ANALYSIS OF TWEETS WITH EVENTS ASSOCIATED TO THAT PARTICULAR DATE <b>',width=1200)\nfig.show()","metadata":{"papermill":{"duration":0.273015,"end_time":"2021-03-19T06:42:47.11422","exception":false,"start_time":"2021-03-19T06:42:46.841205","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.134946,"end_time":"2021-03-19T06:42:47.385718","exception":false,"start_time":"2021-03-19T06:42:47.250772","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}