{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# E-Commerce Shipping - Classification","metadata":{}},{"cell_type":"markdown","source":"## Contents:\n\n- Data Description & Cleaning\n- Exploratory Data Analysis (EDA)\n    * Categorical Features\n    * Numerical Features\n    * Target Column\n- Outliers\n    * Log Transformation\n    * Square Root Transformation\n    * Winsorization\n- Heatmap\n- One-Hot-Encoding\n- Scaling\n    * Normalization\n    * Standardization\n- Building Machine Learning Models\n    * Logistic Regression\n    * KNN\n    * Decision Trees\n    * Random Forest\n    * AdaBoost\n    * Gradient Boosting\n    * Extra Trees\n    * CatBoost\n    * Support Vector Machines\n    * XGBoost\n    * LightGBM\n- Hyperparameter Tuning\n    * Logistic Regression\n    * KNN\n    * Decision Trees\n    * Random Forest\n    * AdaBoost\n    * Gradient Boosting\n    * Extra Trees\n    * CatBoost\n    * Support Vector Machines\n    * XGBoost\n    * LightGBM\n- Best Parameters & Comparison\n- Classification with Artificial Neural Networks (ANNs)","metadata":{}},{"cell_type":"markdown","source":"## Context\nAn international e-commerce company based wants to discover key insights from their customer database. They want to use some of the most advanced machine learning techniques to study their customers. The company sells electronic products.\n\n## Columns\nThe dataset used for model building contained 10999 observations of 12 variables.\nThe data contains the following information:\n\n- **ID**: ID Number of Customers.\n- **Warehouse block**: The Company have big Warehouse which is divided in to block such as A,B,C,D,E.\n- **Mode of shipment**: The Company Ships the products in multiple way such as Ship, Flight and Road.\n- **Customer care calls**: The number of calls made from enquiry for enquiry of the shipment.\n- **Customer rating**: The company has rated from every customer. 1 is the lowest (Worst), 5 is the highest (Best).\n- **Cost of the product**: Cost of the Product in US Dollars.\n- **Prior purchases**: The Number of Prior Purchase.\n- **Product importance**: The company has categorized the product in the various parameter such as low, medium, high.\n- **Gender**: Male and Female.\n- **Discount offered**: Discount offered on that specific product.\n- **Weight in gms**: It is the weight in grams.\n- **Reached on time**: It is the target variable, where 1 Indicates that the product has NOT reached on time and 0 indicates it has reached on time.\n\n\\\n**Data Source:** https://www.kaggle.com/prachi13/customer-analytics","metadata":{}},{"cell_type":"markdown","source":"# Data Descrition & Cleaning","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nfrom scipy.stats.mstats import winsorize\n\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\nfrom sklearn.model_selection import GridSearchCV\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport time","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/customer-analytics/Train.csv')\n\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe(include='all')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop('ID', axis=1, inplace=True)\ndf.rename({'Reached.on.Time_Y.N':'Reached_on_Time'}, axis=1, inplace=True)\ndf['Reached_on_Time'].replace({1:'No', 0:'Yes'}, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Percentage of Null Values:\\n\")\nprint(df.isna().sum()*100/df.shape[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"# of Unique Values: \\n\")\nprint(df.nunique())","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Unique Values:\\n\")\nfor i in range(len(df.nunique())):\n    if df.nunique()[i] < 10:\n        print(\"- \", df.nunique().index[i], \": \", sorted(df.iloc[:, i].unique()), sep='')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Columns:\")\nfor column in df.columns:\n    print(\"- {}\".format(column))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"markdown","source":"## Categorical Features","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(18, 18))\n\nplt.subplot(3, 3, 1)\nsns.countplot(x='Warehouse_block', data=df)\nplt.title('Warehouse Block', fontsize=15)\n\nplt.subplot(3, 3, 2)\nsns.countplot(x='Mode_of_Shipment', data=df)\nplt.title('Mode of Shipment', fontsize=15)\n\nplt.subplot(3, 3, 3)\nsns.countplot(x='Customer_care_calls', data=df)\nplt.title('Customer Care Calls', fontsize=15)\n\nplt.subplot(3, 3, 4)\nsns.countplot(x='Customer_rating', data=df)\nplt.title('Customer Rating', fontsize=15)\n\nplt.subplot(3, 3, 5)\nsns.countplot(x='Prior_purchases', data=df)\nplt.title('Prior Purchases', fontsize=15)\n\nplt.subplot(3, 3, 6)\nsns.countplot(x='Product_importance', data=df)\nplt.title('Product Importance', fontsize=15)\n\nplt.subplot(3, 3, 7)\nsns.countplot(x='Gender', data=df)\nplt.title('Gender', fontsize=15)\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(18, 18))\n\nplt.subplot(3, 3, 1)\nsns.countplot(x='Warehouse_block', hue='Reached_on_Time', data=df)\nplt.title('Warehouse Block', fontsize=15)\n\nplt.subplot(3, 3, 2)\nsns.countplot(x='Mode_of_Shipment', hue='Reached_on_Time', data=df)\nplt.title('Mode of Shipment', fontsize=15)\n\nplt.subplot(3, 3, 3)\nsns.countplot(x='Customer_care_calls', hue='Reached_on_Time',  data=df)\nplt.title('Customer Care Calls', fontsize=15)\n\nplt.subplot(3, 3, 4)\nsns.countplot(x='Customer_rating', hue='Reached_on_Time',  data=df)\nplt.title('Customer Rating', fontsize=15)\n\nplt.subplot(3, 3, 5)\nsns.countplot(x='Prior_purchases', hue='Reached_on_Time',  data=df)\nplt.title('Prior Purchases', fontsize=15)\n\nplt.subplot(3, 3, 6)\nsns.countplot(x='Product_importance', hue='Reached_on_Time',  data=df)\nplt.title('Product Importance', fontsize=15)\n\nplt.subplot(3, 3, 7)\nsns.countplot(x='Gender', hue='Reached_on_Time',  data=df)\nplt.title('Gender', fontsize=15)\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Numeric Features","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 8))\n\nplt.subplot(2, 3, 1)\nplt.hist(df['Cost_of_the_Product'], bins=20)\nplt.title('Cost of the Product')\n\nplt.subplot(2, 3, 2)\nplt.hist(df['Discount_offered'], bins=20)\nplt.title('Discount Offered')\n\nplt.subplot(2, 3, 3)\nplt.hist(df['Weight_in_gms'], bins=20)\nplt.title('Weight in gms')\n\nplt.subplot(2, 3, 4)\nplt.boxplot(df['Cost_of_the_Product'])\nplt.title('Cost of the Product')\n\nplt.subplot(2, 3, 5)\nplt.boxplot(df['Discount_offered'])\nplt.title('Discount Offered')\n\nplt.subplot(2, 3, 6)\nplt.boxplot(df['Weight_in_gms'])\nplt.title('Weight in gms')\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(18, 12))\n\nplt.subplot(2, 3, 1)\nplt.scatter(df['Cost_of_the_Product'], df['Reached_on_Time'], s=5)\nplt.title(\"Cost of the Product vs Reached on Time\", fontsize=15)\n\nplt.subplot(2, 3, 2)\nplt.scatter(df['Discount_offered'], df['Reached_on_Time'], s=5)\nplt.title(\"Discount offered vs Reached on Time\", fontsize=15)\n\nplt.subplot(2, 3, 3)\nplt.scatter(df['Weight_in_gms'], df['Reached_on_Time'], s=5)\nplt.title(\"Weight in gms vs Reached On Time\", fontsize=15)\n\nplt.subplot(2, 3, 4)\nsns.violinplot(x='Reached_on_Time', y='Cost_of_the_Product', data=df)\nplt.title(\"Cost of the Product vs Reached on Time\", fontsize=15)\n\nplt.subplot(2, 3, 5)\nsns.violinplot(x='Reached_on_Time', y='Discount_offered', data=df)\nplt.title(\"Discount offered vs Reached on Time\", fontsize=15)\n\nplt.subplot(2, 3, 6)\nsns.violinplot(x='Reached_on_Time', y='Weight_in_gms', data=df)\nplt.title(\"Weight in gms vs Reached On Time\", fontsize=15)\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Target Column","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nsns.countplot(x='Reached_on_Time', data=df)\nplt.title('Reached on Time', fontsize=15)\n\nplt.subplot(1, 2, 2)\nplt.pie(df['Reached_on_Time'].value_counts(), labels=['No', 'Yes'], explode=[0.05, 0.05], autopct='%1.2f%%', shadow=True)\nplt.title('Reached on Time', fontsize=15)\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Outliers","metadata":{}},{"cell_type":"markdown","source":"According to the boxplots created earlier, the only column with outliers seems to be on \"**Discount_offered**\" column. But there seems to be a lot of outliers in this feature, because of the way boxplot defines outliers. Therefore, instead of directly removing or winsorizing those outliers, I will first apply **Log transformation** and **Square root transformation** to see which works better. Then, I will **winsorize** the remaining outliers.","metadata":{}},{"cell_type":"code","source":"def sum_outliers(X):\n    \"\"\"Outliers are calculated according to the matplotlib.pyplot's standards.\"\"\"\n    IQR = np.quantile(X, q=0.75) - np.quantile(X, q=0.25)\n    upper_whisker = np.quantile(X, q=0.75) + (IQR * 1.5)\n    lower_whisker = np.quantile(X, q=0.25) - (IQR * 1.5)\n    return (X > upper_whisker).sum() + (X < lower_whisker).sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(24, 12))\n\nplt.subplot(2, 4, 1)\nplt.boxplot(df['Discount_offered'])\nplt.title('Discount Offered')\n\nplt.subplot(2, 4, 2)\nplt.boxplot(np.log(df['Discount_offered']))\nplt.title('Discount Offered (Log Transformation)')\n\nplt.subplot(2, 4, 3)\nplt.boxplot(np.sqrt(df['Discount_offered']))\nplt.title('Discount Offered (Square Root Transformation)')\n\nplt.subplot(2, 4, 4)\nplt.boxplot(winsorize(np.log(df['Discount_offered']),limits=(0.15, 0.15)));\nplt.title('Discount Offered (Log Transformation & Winsorized)')\n\nplt.subplot(2, 4, 5)\nplt.hist(df['Discount_offered'], bins=20)\nplt.title('Discount Offered')\n\nplt.subplot(2, 4, 6)\nplt.hist(np.log(df['Discount_offered']), bins=20)\nplt.title('Discount Offered (Log Transformation)')\n\nplt.subplot(2, 4, 7)\nplt.hist(np.sqrt(df['Discount_offered']), bins=20)\nplt.title('Discount Offered (Square Root Transformation)')\n\nplt.subplot(2, 4, 8)\nplt.hist(winsorize(np.log(df['Discount_offered']),limits=(0.15, 0.15)), bins=20);\nplt.title('Discount Offered (Log Transformation & Winsorized)')\n\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Total number of observations: {}\".format(len(df['Discount_offered'])))\nprint(\"Number of outliers in 'Discount_offered': {}\".format(sum_outliers(df['Discount_offered'])))\nprint(\"Number of outliers in 'Discount_offered' (Log Transformation): {}\".format(sum_outliers(np.log(df['Discount_offered']))))\nprint(\"Number of outliers in 'Discount_offered' (Square Root Transformation): {}\".format(sum_outliers(np.sqrt(df['Discount_offered']))))\nprint(\"Number of outliers in 'Discount_offered' (Log Transformation & Winsorized): {}\".format(sum_outliers(winsorize(np.log(df['Discount_offered']),limits=(0.15, 0.15)))))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Discount_offered'] = np.array(winsorize(np.log(df['Discount_offered']),limits=(0.15, 0.15)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Heatmap","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 12))\nsns.heatmap(pd.get_dummies(df, drop_first=True).corr(), annot=True, fmt='.3f')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.get_dummies(df, drop_first=False).corr()['Reached_on_Time_Yes'].sort_values(ascending=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **Weight**, and **Cost** are _positively_, amount of **Discount** is _negatively_ correlated with the target variable.\n- Overall, there doesn't seem to be the problem of **multicollinearity**.","metadata":{}},{"cell_type":"markdown","source":"# One-Hot-Encoding","metadata":{}},{"cell_type":"code","source":"df = pd.get_dummies(df, drop_first=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.drop('Reached_on_Time_Yes', axis=1)\ny = df['Reached_on_Time_Yes']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scaling","metadata":{}},{"cell_type":"markdown","source":"## Normalization","metadata":{}},{"cell_type":"code","source":"normalizer = Normalizer()\nX_normalized = pd.DataFrame(normalizer.fit_transform(df.drop('Reached_on_Time_Yes', axis=1)), columns=df.columns[:-1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Standardization","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nX_scaled = pd.DataFrame(scaler.fit_transform(df.drop('Reached_on_Time_Yes', axis=1)), columns=df.columns[:-1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building Machine Learning Models\n\n- Logistic Regression\n- KNN\n- Decision Trees\n- Random Forest\n- AdaBoost\n- Gradient Boosting\n- Extra Trees\n- Cat Boost\n- Support Vector Machines\n- XGBoost\n- LightGBM","metadata":{}},{"cell_type":"code","source":"def fit_predict_score(Model, X_train, y_train, X_test, y_test):\n    \"\"\"Fit the model of your choice, predict for test data, and returns classification metrics.\"\"\"\n    model = Model\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    train_score = model.score(X_train, y_train)\n    test_score = model.score(X_test, y_test)\n    return train_score, test_score, precision_score(y_test, y_pred), recall_score(y_test, y_pred), f1_score(y_test, y_pred)\n\ndef model_comparison(X, y):\n    \"\"\"Creates a DataFrame comparing Logistic Regression, K-Nearest Neighbors, Decision Tree,\n    Random Forest, AdaBoost, Gradient Boosting, Extra Trees, CatBoost, Support Vector Machines,\n    XGBoost, and LightGBM.\"\"\"\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n    \n    lr_train_score, lr_test_score, lr_pr, lr_re, lr_f1 = fit_predict_score(LogisticRegression(), X_train, y_train, X_test, y_test)\n    knn_train_score, knn_test_score, knn_pr, knn_re, knn_f1 = fit_predict_score(KNeighborsClassifier(), X_train, y_train, X_test, y_test)\n    dtc_train_score, dtc_test_score, dtc_pr, dtc_re, dtc_f1 = fit_predict_score(DecisionTreeClassifier(), X_train, y_train, X_test, y_test)\n    rfc_train_score, rfc_test_score, rfc_pr, rfc_re, rfc_f1 = fit_predict_score(RandomForestClassifier(), X_train, y_train, X_test, y_test)\n    ada_train_score, ada_test_score, ada_pr, ada_re, ada_f1 = fit_predict_score(AdaBoostClassifier(), X_train, y_train, X_test, y_test)\n    gbc_train_score, gbc_test_score, gbc_pr, gbc_re, gbc_f1 = fit_predict_score(GradientBoostingClassifier(), X_train, y_train, X_test, y_test)\n    xtc_train_score, xtc_test_score, xtc_pr, xtc_re, xtc_f1 = fit_predict_score(ExtraTreesClassifier(), X_train, y_train, X_test, y_test)\n    cbc_train_score, cbc_test_score, cbc_pr, cbc_re, cbc_f1 = fit_predict_score(CatBoostClassifier(verbose=0), X_train, y_train, X_test, y_test)\n    svc_train_score, svc_test_score, svc_pr, svc_re, svc_f1 = fit_predict_score(SVC(), X_train, y_train, X_test, y_test)\n    xgbc_train_score, xgbc_test_score, xgbc_pr, xgbc_re, xgbc_f1 = fit_predict_score(XGBClassifier(verbosity=0), X_train, y_train, X_test, y_test)\n    lgbc_train_score, lgbc_test_score, lgbc_pr, lgbc_re, lgbc_f1 = fit_predict_score(LGBMClassifier(), X_train, y_train, X_test, y_test)\n    \n    models = ['Logistic Regression', 'K-Nearest Neighbors', 'Decision Tree', 'Random Forest', 'AdaBoost',\n              'Gradient Boosting', 'Extra Trees', 'CatBoost', 'Support Vector Machines', 'XGBoost', 'LightGBM']\n    train_score = [lr_train_score, knn_train_score, dtc_train_score, rfc_train_score, ada_train_score,\n                   gbc_train_score, xtc_train_score, cbc_train_score, svc_train_score, xgbc_train_score, lgbc_train_score]\n    test_score = [lr_test_score, knn_test_score, dtc_test_score, rfc_test_score, ada_test_score,\n                  gbc_test_score, xtc_test_score, cbc_test_score, svc_test_score, xgbc_test_score, lgbc_test_score]\n    precision = [lr_pr, knn_pr, dtc_pr, rfc_pr, ada_pr, gbc_pr, xtc_pr, cbc_pr, svc_pr, xgbc_pr, lgbc_pr]\n    recall = [lr_re, knn_re, dtc_re, rfc_re, ada_re, gbc_re, xtc_re, cbc_re, svc_re, xgbc_re, lgbc_re]\n    f1 = [lr_f1, knn_f1, dtc_f1, rfc_f1, ada_f1, gbc_f1, xtc_f1, cbc_f1, svc_f1, xgbc_f1, lgbc_f1]\n    \n    model_comparison = pd.DataFrame(data=[models, train_score, test_score, precision, recall, f1]).T.rename({0: 'Model',\n                                                                                                             1:'Training Score',\n                                                                                                             2: 'Test Score (Accuracy)',\n                                                                                                             3: 'Precision',\n                                                                                                             4: 'Recall',\n                                                                                                             5: 'F1 Score'\n                                                                                                            }, axis=1)\n    \n    return model_comparison","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Default DataFrame:\")\ndisplay(model_comparison(X, y))\nprint('-'*40)\nprint(\"\\nNormalized DataFrame:\")\ndisplay(model_comparison(X_normalized, y))\nprint('-'*40)\nprint(\"\\nStandardized DataFrame:\")\ndisplay(model_comparison(X_scaled, y))","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As expected, normalizing or standardization the data did not improve the performance of classification models significantly. I will proceed with the unscaled data.","metadata":{}},{"cell_type":"markdown","source":"# Hyperparameter Tuning","metadata":{}},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"code","source":"start = time.time()\n\nparams = {\"C\": [10 ** x for x in range (-5, 5, 1)],\n          \"penalty\": ['l1', 'l2']}\n\nlr_grid = GridSearchCV(estimator=LogisticRegression(),\n                       param_grid = params,\n                       cv = 5,\n                       verbose = 0)\n\nlr_grid.fit(X, y)\n\nend = time.time()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"GridSearchCV Runtime: {} minutes\".format(round((end - start) / 60, 2)))\nprint(\"Best Parameters : \", lr_grid.best_params_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## K-Nearest Neighbors","metadata":{}},{"cell_type":"code","source":"start = time.time()\n\nparams = {\n    \"n_neighbors\": [1, 3, 5, 10, 15, 30, 50],\n    \"weights\": ['uniform', 'distance'],\n    \"metric\": ['minkowski', 'euclidian', 'manhattan']\n}\n\nknn_grid = GridSearchCV(estimator=KNeighborsClassifier(),\n                        param_grid = params,\n                        cv = 5,\n                        verbose = 0)\n\nknn_grid.fit(X, y)\n\nend = time.time()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"GridSearchCV Runtime: {} minutes\".format(round((end - start) / 60, 2)))\nprint(\"Best Parameters : \", knn_grid.best_params_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decision Tree","metadata":{}},{"cell_type":"code","source":"start = time.time()\n\nparams = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [i for i in range(1, 10)],\n    'min_samples_split': [i for i in range(1, 10)],\n    'min_samples_leaf': [i for i in range(1, 5)]\n}\n\ndtc_grid = GridSearchCV(estimator = DecisionTreeClassifier(),\n                        param_grid = params,\n                        cv = 5,\n                        verbose = 0)\n\ndtc_grid.fit(X, y)\n\nend = time.time()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"GridSearchCV Runtime: {} minutes\".format(round((end - start) / 60, 2)))\nprint(\"Best Parameters : \", dtc_grid.best_params_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest","metadata":{}},{"cell_type":"code","source":"start = time.time()\n\nparams = {\n    'n_estimators': [100, 200, 300, 400, 500],\n    'max_depth' : [4, 5, 6, 7, 8, 10, 15],\n    'criterion' :['gini', 'entropy']\n}\n\nrfc_grid = GridSearchCV(estimator = RandomForestClassifier(),\n                        param_grid = params,\n                        cv = 5,\n                        verbose = 0)\n\nrfc_grid.fit(X, y)\n\nend = time.time()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"GridSearchCV Runtime: {} minutes\".format(round((end - start) / 60, 2)))\nprint(\"Best Parameters : \", rfc_grid.best_params_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## AdaBoost","metadata":{}},{"cell_type":"code","source":"start = time.time()\n\nparams = {\n    'n_estimators': [10, 50, 100, 200, 500],\n    'learning_rate': [0.1, 0.3, 0.5, 0.7]\n}\n\nada_grid = GridSearchCV(estimator = AdaBoostClassifier(),\n                        param_grid = params,\n                        cv = 5,\n                        verbose = 0)\n\nada_grid.fit(X, y)\n\nend = time.time()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"GridSearchCV Runtime: {} minutes\".format(round((end - start) / 60, 2)))\nprint(\"Best Parameters : \", ada_grid.best_params_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gradient Boosting","metadata":{}},{"cell_type":"code","source":"start = time.time()\n\nparams = {\n    'learning_rate': [0.1, 0.3, 0.5, 0.8, 1],\n    'max_depth': [1, 3, 5, 7, 10, 15, 25],\n    'subsample': [0.1, 0.3, 0.5, 0.8, 1],\n    'n_estimators' : [50, 100, 250, 500]\n}\n\ngbc_grid = GridSearchCV(estimator = GradientBoostingClassifier(),\n                        param_grid = params,\n                        cv = 3,\n                        verbose = 0)\n\ngbc_grid.fit(X, y)\n\nend = time.time()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"GridSearchCV Runtime: {} minutes\".format(round((end - start) / 60, 2)))\nprint(\"Best Parameters : \", gbc_grid.best_params_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extra Trees","metadata":{}},{"cell_type":"code","source":"start = time.time()\n\nparams = {\n    'n_estimators' : [50, 75, 100, 125, 150],\n    'max_depth': [i for i in range(1, 10, 2)],\n    'min_samples_leaf': [i for i in range(1, 10, 2)],\n    'min_samples_split': [i for i in range(1, 10, 2)]\n}\n\nxtc_grid = GridSearchCV(estimator = ExtraTreesClassifier(),\n                        param_grid = params,\n                        cv = 3,\n                        verbose = 0)\n\nxtc_grid.fit(X, y)\n\nend = time.time()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"GridSearchCV Runtime: {} minutes\".format(round((end - start) / 60, 2)))\nprint(\"Best Parameters : \", xtc_grid.best_params_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cat Boost","metadata":{}},{"cell_type":"code","source":"start = time.time()\n\nparams = {\n    'learning_rate': [0.03, 0.1, 0.5],\n    'depth': [4, 6, 10],\n    'l2_leaf_reg': [1, 3, 5, 7, 9]\n}\n\ncbc_grid = GridSearchCV(estimator = CatBoostClassifier(verbose = 0),\n                        param_grid = params,\n                        cv = 3,\n                        verbose = 0)\n\ncbc_grid.fit(X, y)\n\nend = time.time()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"GridSearchCV Runtime: {} minutes\".format(round((end - start) / 60, 2)))\nprint(\"Best Parameters : \", cbc_grid.best_params_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Support Vector Machines","metadata":{}},{"cell_type":"code","source":"start = time.time()\n\nparams = {'C': [10**i for i in range(1, 2)] + [round(0.1**i,5) for i in range(5)]}\n\nsvc_grid = GridSearchCV(estimator = SVC(),\n                        param_grid = params,                        \n                        cv = 5,\n                        verbose = 0)\n\nsvc_grid.fit(X, y)\n\nend = time.time()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"GridSearchCV Runtime: {} minutes\".format(round((end - start) / 60, 2)))\nprint(\"Best Parameters : \", svc_grid.best_params_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost","metadata":{}},{"cell_type":"code","source":"X_sample = X.sample(n=3000, random_state=42)\ny_sample = y[X_sample.index]\n\nstart = time.time()\n\nparams = {\n    'learning_rate': [0.1, 0.3, 0.5],\n    'max_depth': [1, 3, 5],\n    'min_child_weight': [1, 3, 5, 7, 9],\n    'subsample': [0.1, 0.3, 0.5, 0.8, 1],\n    'colsample_bytree': [0.1, 0.3, 0.5],\n    'n_estimators' : [100, 200, 300, 400, 500],\n    'objective': ['reg:squarederror']\n}\n\nxgbc_grid = GridSearchCV(estimator = XGBClassifier(),\n                         param_grid = params,\n                         cv = 3,\n                         verbose = 0)\n\nxgbc_grid.fit(X_sample, y_sample)\n\nend = time.time()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"GridSearchCV Runtime: {} minutes\".format(round((end - start) / 60, 2)))\nprint('Best Parameters: ', xgbc_grid.best_params_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LightGBM","metadata":{}},{"cell_type":"code","source":"start = time.time()\n\nparams = {\n    'learning_rate': [10 ** x for x in range (-5, 5, 1)],\n    'n_estimators': [x * 100 for x in range(1, 11)]\n}\n\nlgbc_grid = GridSearchCV(estimator = LGBMClassifier(),\n                        param_grid = params,                        \n                        cv = 3,\n                        verbose = 0)\n\nlgbc_grid.fit(X_normalized, y)\n\nend = time.time()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"GridSearchCV Runtime: {} minutes\".format(round((end - start) / 60, 2)))\nprint('Best Parameters: ', lgbc_grid.best_params_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Best Parameters & Comparison","metadata":{}},{"cell_type":"code","source":"print(\"Best Parameters (Logistic Regression): \", lr_grid.best_params_)\nprint(\"Best Parameters (K-Nearest Neighbors): \", knn_grid.best_params_)\nprint(\"Best Parameters (Decision Tree): \", dtc_grid.best_params_)\nprint(\"Best Parameters (Random Forest): \", rfc_grid.best_params_)\nprint(\"Best Parameters (AdaBoost): \", ada_grid.best_params_)\nprint(\"Best Parameters (Gradient Boosting): \", gbc_grid.best_params_)\nprint(\"Best Parameters (Extra Trees): \", xtc_grid.best_params_)\nprint(\"Best Parameters (CatBoost): \", cbc_grid.best_params_)\nprint(\"Best Parameters (SVC): \", svc_grid.best_params_)\nprint('Best Parameters (XGBoost):', xgbc_grid.best_params_)\nprint('Best Parameters (LightGBM): ', lgbc_grid.best_params_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nlr_train_score, lr_test_score, lr_pr, lr_re, lr_f1 = fit_predict_score(LogisticRegression(C=1, penalty='l2'), X_train, y_train, X_test, y_test)\nknn_train_score, knn_test_score, knn_pr, knn_re, knn_f1 = fit_predict_score(KNeighborsClassifier(metric='minkowski', n_neighbors=3, weights='uniform'), X_train, y_train, X_test, y_test)\ndtc_train_score, dtc_test_score, dtc_pr, dtc_re, dtc_f1 = fit_predict_score(DecisionTreeClassifier(criterion='gini', max_depth=2, min_samples_leaf=1, min_samples_split=2), X_train, y_train, X_test, y_test)\nrfc_train_score, rfc_test_score, rfc_pr, rfc_re, rfc_f1 = fit_predict_score(RandomForestClassifier(criterion='gini', max_depth=15, n_estimators=100), X_train, y_train, X_test, y_test)\nada_train_score, ada_test_score, ada_pr, ada_re, ada_f1 = fit_predict_score(AdaBoostClassifier(learning_rate=0.1, n_estimators=10), X_train, y_train, X_test, y_test)\ngbc_train_score, gbc_test_score, gbc_pr, gbc_re, gbc_f1 = fit_predict_score(GradientBoostingClassifier(learning_rate=0.8, max_depth=5, n_estimators=500, subsample=0.1), X_train, y_train, X_test, y_test)\nxtc_train_score, xtc_test_score, xtc_pr, xtc_re, xtc_f1 = fit_predict_score(ExtraTreesClassifier(max_depth=1, min_samples_leaf=7, min_samples_split=3, n_estimators=75), X_train, y_train, X_test, y_test)\ncbc_train_score, cbc_test_score, cbc_pr, cbc_re, cbc_f1 = fit_predict_score(CatBoostClassifier(verbose = 0, depth=6, l2_leaf_reg=3, learning_rate=0.5), X_train, y_train, X_test, y_test)\nsvc_train_score, svc_test_score, svc_pr, svc_re, svc_f1 = fit_predict_score(SVC(C=10), X_train, y_train, X_test, y_test)\nxgbc_train_score, xgbc_test_score, xgbc_pr, xgbc_re, xgbc_f1 = fit_predict_score(XGBClassifier(colsample_bytree=0.5, learning_rate=0.1, max_depth=1, min_child_weight=7, n_estimators=100, objective='reg:squarederror', subsample=0.5), X_train, y_train, X_test, y_test)\nlgbc_train_score, lgbc_test_score, lgbc_pr, lgbc_re, lgbc_f1 = fit_predict_score(LGBMClassifier(learning_rate=10000, n_estimators=100), X_train, y_train, X_test, y_test)\n\nmodels = ['Logistic Regression', 'K-Nearest Neighbors', 'Decision Tree', 'Random Forest', 'AdaBoost',\n          'Gradient Boosting', 'Extra Trees', 'CatBoost', 'Support Vector Machines', 'XGBoost', 'LightGBM']\ntrain_score = [lr_train_score, knn_train_score, dtc_train_score, rfc_train_score, ada_train_score,\n               gbc_train_score, xtc_train_score, cbc_train_score, svc_train_score, xgbc_train_score, lgbc_train_score]\ntest_score = [lr_test_score, knn_test_score, dtc_test_score, rfc_test_score, ada_test_score,\n              gbc_test_score, xtc_test_score, cbc_test_score, svc_test_score, xgbc_test_score, lgbc_test_score]\nprecision = [lr_pr, knn_pr, dtc_pr, rfc_pr, ada_pr, gbc_pr, xtc_pr, cbc_pr, svc_pr, xgbc_pr, lgbc_pr]\nrecall = [lr_re, knn_re, dtc_re, rfc_re, ada_re, gbc_re, xtc_re, cbc_re, svc_re, xgbc_re, lgbc_re]\nf1 = [lr_f1, knn_f1, dtc_f1, rfc_f1, ada_f1, gbc_f1, xtc_f1, cbc_f1, svc_f1, xgbc_f1, lgbc_f1]\n\ntuned_models = pd.DataFrame(data=[models, train_score, test_score, precision, recall, f1]).T.rename({0: 'Model',\n                                                                                                     1:'Training Score',\n                                                                                                     2: 'Test Score (Accuracy)',\n                                                                                                     3: 'Precision',\n                                                                                                     4: 'Recall',\n                                                                                                     5: 'F1 Score'\n                                                                                                        }, axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Default Parameters:\")\ndisplay(model_comparison(X, y))\nprint('-'*40)\nprint(\"\\nTuned Parameters:\")\ndisplay(tuned_models)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classification with Artificial Neural Networks (ANN)","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.33, random_state=42)\n\nprint(\"Shape of train set (X) :\", X_train.shape)\nprint(\"Shape of train set (y) :\", y_train.shape)\nprint(\"Shape of test set  (X) :\", X_test.shape)\nprint(\"Shape of test set  (y) :\", y_test.shape)\n\ninput_shape = X_train.shape[1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(16, activation='relu', input_shape = (input_shape,), name = \"Hidden_Layer_1\"))\nmodel.add(Dense(8, activation='relu', name = \"Hidden_Layer_2\"))\nmodel.add(Dense(4, activation='relu', name = \"Hidden_Layer_3\"))\nmodel.add(Dense(2, activation='relu', name = \"Hidden_Layer_4\"))\nmodel.add(Dense(1, activation='sigmoid', name = \"Output\"))\n\nmodel.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer ='adam',\n              loss='binary_crossentropy', \n              metrics =['accuracy'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train, y_train, epochs=100)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_test)\ny_pred = (y_pred>0.5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(y_test, y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_score = model.evaluate(X_train, y_train, verbose = 0)[1]\ntest_score = model.evaluate(X_test, y_test, verbose = 0)[1]\n\nprint(\"Training Score: {:.3f}\".format(train_score))\nprint(\"Test Score (Accuracy): {:.3f}\".format(test_score))\nprint(\"Precision: {:.3f}\".format(precision_score(y_test, y_pred)))\nprint(\"Recall: {:.3f}\".format(recall_score(y_test, y_pred)))\nprint(\"F1 Score: {:.3f}\".format(f1_score(y_test, y_pred)))","metadata":{},"execution_count":null,"outputs":[]}]}