{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predicting Customer Attrition using Machine Learning Techniques"},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"Customer attrition can have serious financial consequences for a business. Product and service offering has to be more innovative than that of competitors to entice customers to buy and remain loyal. Retaining satisfied customers is generally cheaper than acquiring new ones. \nTherefore, a customer retention strategy can be employed if we know which customer is likely to attrite.\n\nThis Notebook presents a solution to the problem of identifying churned customers using Machine Learning techniques.\nThe dataset is retrieved from https://www.kaggle.com/sakshigoyal7/credit-card-customers and consists of 10,000 bank customers. For each customer, the dataset includes the following variables:\n\n**Demographic**\n* Age\n* Gender\n\n**Socioeconomic**\n* Salary\n* Education level\n* Marital status\n\n**Business related**\n* Credit card category\n* Credit card limit\n* Duration of relationship with a customer\n* Total number of products held\n* Months inactive in the last 12 months\n* Number of contacts in the last 12 months\n* Total revolving balance on the credit card\n* Open to buy credit line (Average over the last 12 months)\n* Change in transaction amount (Q4 to Q1)\n* Total transaction amount in the last 12 months\n* Total transaction count in the last 12 months\n* Average card utilization ratio"},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Table of Contents\n\n\n* [1. Libraries](#libraries)\n* [2. Data Extraction and Filterig](#data)\n* [3. Data Analysis](#descrana)\n    * [3.1 Reading the Graphs](#info)\n    * [3.2 Analysis](#ana)\n    * [3.3 Correlations](#corr)\n* [4. Modeling](#model)"},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# 1. Libraries  <a class=\"anchor\" id=\"libraries\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.base import TransformerMixin, BaseEstimator\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE, BorderlineSMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n\nfrom category_encoders.leave_one_out import LeaveOneOutEncoder\n\npd.set_option('display.max_columns', None)\nplt.style.use('ggplot')\nwarnings.filterwarnings(\"ignore\")\n\nnp.random.seed(40)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Extraction and Filtering  <a class=\"anchor\" id=\"data\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the data\ndata = pd.read_csv('/kaggle/input/credit-card-customers/BankChurners.csv')\ndisplay(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div align=center>\n    The variables CLIENTNUM and the two Naive_Bayes_Classifiers are removed as they are not useful in this analysis.\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# delete irrelevant variables\nvariables_to_delete = ['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2',\n                       'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\n                       'CLIENTNUM']\n\ndata.drop(variables_to_delete, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# detecting missing data\nfig, ax = plt.subplots(figsize=(20, 6))\n\nax.set_title('Missing Values')\n\nsns.heatmap(data.isnull(),\n            yticklabels=False,\n            cbar=False,\n            cmap='magma',\n            ax=ax)\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div align=center>\nSince there are no white dots, which would otherwise indicate a missing value, there are no missing values in the dataset.   \n</div>\n"},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# 3. Data Analysis <a class=\"anchor\" id=\"descrana\"></a>"},{"metadata":{},"cell_type":"markdown","source":"<div align=center>\n    The focus of the analysis in this section is on inspecting the distributions of the attrited and existing customers.\n</div>"},{"metadata":{},"cell_type":"markdown","source":"## 3.1 Reading the graphs <a class=\"anchor\" id=\"info\"></a>"},{"metadata":{},"cell_type":"markdown","source":"<div align=center>\nThe graphs used in this analysis are explained below:\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot stacked barplot\nplt.hist([\n    data['Months_Inactive_12_mon'].loc[data['Attrition_Flag'] == 'Attrited Customer'],\n    data['Months_Inactive_12_mon'].loc[data['Attrition_Flag'] != 'Attrited Customer']], \n    bins=100, stacked=True, color=['tab:orange','tab:blue']\n)\nplt.xlabel('Months_Inactive_12_mon')\nplt.ylabel('Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Given the variable **Months_Inactive_12_mon**, with bins 0,1,...,6, the above graph represents a stacked bar plot.\nThe horizontal axis represents the bins. The vertical axis represents the number of individuals.\nThe height of an orange-colored bar represents the numberof attrited customers in a bin. The length of a blue bar represents the number of existing customers.  The total height of both bars combined represents the total number of individuals in a bin.\nNote that to retrieve the number of existing customers, we have to subtract the total number of individuals by the amount of attrited customers. \n\nTo illustrate: the total number of individuals belonging to category 3 is roughly 3800. The amount of attrited customers is roughly 800. Consequently, the amount of existing customers is around 3000."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# cut a continous feature into 100 bins\nd = pd.DataFrame(data[['Months_Inactive_12_mon', 'Attrition_Flag']].copy())\n\namount, edges = np.histogram(data['Months_Inactive_12_mon'], bins=100)\nedges[-1] += 1e-5\n\nzz = pd.cut(\n    data['Months_Inactive_12_mon'].loc[data['Attrition_Flag'] == 'Attrited Customer'], \n    edges, \n    right=False\n)\nzz = zz.value_counts().sort_index()\n\n# calculate proportion of attrited customers\npp = zz.values / amount\n\n# calculate variance of proportion\nstDev = np.sqrt(pp * (1 - pp) / amount)\n\n# determine the length of the confidence interval\nmm = (stDev * 1.96 * 2 < 0.12)\nmm2 = (stDev * 1.96 * 2  >= 0.12)\n\n# plot bins\nplt.bar(edges[:-1][mm], amount[mm] / amount[mm], width=np.diff(edges)[mm], color='tab:blue')\nplt.bar(edges[:-1][mm], zz.values[mm] / amount[mm], width=np.diff(edges)[mm], color='tab:orange')\n\n# plot faded bins\nplt.bar(edges[:-1][mm2], amount[mm2] / amount[mm2], width=np.diff(edges)[mm2], alpha=0.3, color='tab:blue')\nplt.bar(edges[:-1][mm2], zz.values[mm2] / amount[mm2], width=np.diff(edges)[mm2], alpha=0.3, color='tab:orange')\n\n# make fancy\ncolors = {\n    'Existing Customer':'tab:blue', \n    'Attrited Customer':'tab:orange'\n}         \nlabels = list(colors.keys())\nhandles = [plt.Rectangle((0,0),1,1, color=colors[label]) for label in labels]\nplt.legend(handles, labels)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above figure is another example of a stacked bar plot. In this plot, the vertical axis represents the proportion of attrited or existing customers.\n\nTo illustrate: the proportion of attrited customers in category 3 is around 0.21 (or 21%).\n\nNote that some bars are faded out. This is done to indicate that there are relatively few individuals in a bin.\nWhen there are only a few individuals observed, there is more uncertainty involved in assessing whether the proportion of attrited customers in that bin is consistent with that of the true population.\nA bar is faded out when the 95% confidence interval of the proportion of attrited customers ($p$) is greater than 0.12. Here, the confidence interval is derived using the normal approximation of the Bernoulli distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"d = pd.DataFrame(data[['Months_on_book', 'Attrition_Flag']].copy())\n        \nm = d.boxplot(by='Attrition_Flag', vert=False, figsize=(16, 4))\nm.set_xlabel('Months_on_book')\nm.set_title('')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above graph is a boxplot. The black circles indicate outliers. The vertical black bar at the far left of the line segment is the minimum value (excluding outliers). The first quantile is represented at the far left side of the box. The median is represented by the vertical blue line. The third quantile is represented at the far right side of the box. The maximum value (excluding outliers) and fourth quantile is represented by the vertical black bar at the far right of the line segment."},{"metadata":{},"cell_type":"markdown","source":"## 3.2 Analysis <a class=\"anchor\" id=\"ana\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"attrition_flag = data['Attrition_Flag'].value_counts()\nattrition_flag = attrition_flag / attrition_flag.sum()\nattrition_flag.plot(kind='bar', color='tab:blue', figsize=(9, 6))\nplt.ylabel('Proportion')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div align=center>\nThe proportion of attrited customers in the dataset is around 16%. This indicates that for every 100 new customers, we can expect that on average around 16 customers will attrite.\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"names = list(data.columns) \ncategorical_names = ['Gender', 'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category']\nnumerical_names = [c for c in names if c not in categorical_names + ['Attrition_Flag']]\n\ntarget = 1.0 * (data['Attrition_Flag'] != 'Existing Customer')\n    \nfor i, name in enumerate(names):\n    if name == 'Attrition_Flag':\n        continue\n    \n    fig = plt.figure(figsize=(16, 9))\n    \n    ax11 = fig.add_subplot(2,2,1)\n    ax11.set_ylabel('Count')\n    \n    ax21 = fig.add_subplot(2,2,2)\n    ax21.set_ylabel('Proportion')\n    \n    if name in categorical_names:\n        \n        # stacked bar categorical for counting\n        q = data.groupby([name, 'Attrition_Flag']).size().unstack()\n        q.plot(kind='bar', ax=ax11, stacked=True, color=['tab:orange','tab:blue'])\n        ax11.set_ylabel('Count')\n             \n        # stacked bar categorical proportion\n        ((q.T/q.sum(1).values).T).plot(kind='bar', ax=ax21, stacked=True, color=['tab:orange','tab:blue'])\n\n    else:        \n        # create stacked bar plot for counting (100 bins are used)\n        ax11.hist([data[name].loc[target == 1],data[name].loc[target == 0]],\n          bins=100, stacked=True, color=['tab:orange','tab:blue'])\n        \n        \n        # create the boxplot figure\n        d = pd.DataFrame(data[[name, 'Attrition_Flag']].copy())\n        ax2 = fig.add_subplot(2,1,2, sharex = ax11)\n        d.boxplot(by='Attrition_Flag', ax=ax2, vert=False)\n        ax2.set_xlabel(name)\n        ax2.set_title('')\n        \n        # divide a continous feature into 100 bins\n        amount, edges = np.histogram(data[name], bins=100)\n        edges[-1] += 1e-5\n        \n        zz = pd.cut(data[name].loc[target==1], edges, right=False).value_counts().sort_index()\n        \n        # calculate the proportion of attrited customer in each bi\n        pp = zz.values / amount\n        \n        # calculate the standard devation of the probabilities\n        stDev = np.sqrt(pp*(1 - pp) / amount)\n        \n        # determine length of confidence interval\n        mm = (stDev*1.96*2 < 0.12)\n        mm2 = (stDev*1.96*2  >= 0.12)\n        \n        # plot bins\n        ax21.bar(edges[:-1][mm], amount[mm]/ amount[mm], width=np.diff(edges)[mm], color='tab:blue')\n        ax21.bar(edges[:-1][mm], zz.values[mm]/amount[mm], width=np.diff(edges)[mm], color='tab:orange')\n        \n        # faded out bins\n        ax21.bar(edges[:-1][mm2], amount[mm2]/ amount[mm2], width=np.diff(edges)[mm2], alpha=0.3, color='tab:blue')\n        ax21.bar(edges[:-1][mm2], zz.values[mm2]/amount[mm2], width=np.diff(edges)[mm2], alpha=0.3, color='tab:orange')\n     \n        # make it fancy\n        colors = {'Existing Customer':'tab:blue', 'Attrited Customer':'tab:orange'}         \n        labels = list(colors.keys())\n        handles = [plt.Rectangle((0,0),1,1, color=colors[label]) for label in labels]\n        ax11.legend(handles, labels)\n        ax21.legend(handles, labels)\n        \n    # make it even more fancy\n    fig.suptitle(name, size=20) \n    ax11.set_xlabel(name)\n    ax21.set_xlabel(name)\n    fig.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Several observations can be made from the above figures:\n    \n* The distribution of most continuous features appears to have heavy tails (high skewness). This might distort our model's learning process.\n* In some bins there are relatively few observations. This adds to the uncertainty that our sample might not represent the attributes of the true population. Therefore, our model might have more trouble generalizing to unobserved data. A larger dataset might resolve this.\n* Some features appear to have multimodal distributions (Total_Trans_Ct, Total_Trans_Amt). This might indicate heterogeneity and the presence of subgroups. These features can be particularly interesting for modeling."},{"metadata":{},"cell_type":"markdown","source":"## 3.3 Correlations <a class=\"anchor\" id=\"corr\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(\n    data[numerical_names + ['Attrition_Flag']],\n    corner=True,\n    hue='Attrition_Flag',\n    kind='scatter',\n    palette={\n        'Attrited Customer': 'tab:orange',\n        'Existing Customer': 'tab:blue', \n    }\n)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the correlation figure above, we can induce that the correlation between Avg_Open_To_Buy and Credit_limit is close to 1. One of these features can be dropped since correlated features generally don't improve model performance.\nBy eyeballing I find the following interaction effects interesting enough to be included in the dataset:\n* Total_Trans_Ct x Total_Ct_Chng_Q4_Q1\n* Total_Trans_Amt x Total_Trans_Ct\n* Total_Amt_Chng_Q4_Q1 x Total_Ct_Chng_Q4_Q1\n* Total_Amt_Chng_Q4_Q1 x Total_Trans_Amt\n* Total_Revolving_Bal x Avg_Utilization_Ratio"},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# 4. Modeling <a class=\"anchor\" id=\"model\"></a>"},{"metadata":{},"cell_type":"markdown","source":"1. The numerical features will be log-transformed to reduce the skewness.\n2. The categorical features (Gender, Marital_Status, etc...) are transformed using Leave-One-Out Target Encoding (LOOE). The reasons for choosing LOOE over One-Hot Encoding are (1) the number of dimensions needed to encode the data is kept to a minimum (2) the encoded features represent numerical values between [0,1].\n\n3. The numerical features are then bounded between [0,1] using the min-max scaler. Hence, all features will be between [0,1].\n\n4. Borderline SMOTE is used to oversample the minority class (attrited customers).\n\n5. The majority class (existing customers) is undersampled such that the ratio between the classes is 1:1. Hence, our dataset will be balanced.\n\n6. Each model is evaluated using a stratified 10-fold cross-validation. The recall score is used as an evaluation metric. The model that has the highest mean test score is then selected as the final model."},{"metadata":{"trusted":true},"cell_type":"code","source":"y = (data['Attrition_Flag'] == 'Attrited Customer').astype(int)\nX = data.drop('Attrition_Flag', axis=1).copy()\nX.drop('Avg_Open_To_Buy', axis=1, inplace=True)\n\nX['Total_Trans_CtXTotal_Ct_Chng_Q4_Q1'] = X['Total_Trans_Ct'] * X['Total_Ct_Chng_Q4_Q1'] \nX['Total_Trans_AmtXTotal_Trans_Ct'] = X['Total_Trans_Amt'] * X['Total_Trans_Ct'] \nX['Total_Amt_Chng_Q4_Q1XTotal_Ct_Chng_Q4_Q1'] = X['Total_Amt_Chng_Q4_Q1'] * X['Total_Ct_Chng_Q4_Q1'] \nX['Total_Amt_Chng_Q4_Q1XTotal_Amt_Chng'] = X['Total_Amt_Chng_Q4_Q1'] * X['Total_Trans_Amt'] \nX['Total_Revolving_BalXAvg_Utilization_Ratio'] = X['Total_Revolving_Bal'] * X['Avg_Utilization_Ratio'] \n\nnumerical_names2 = [x for x in X.columns if x not in categorical_names]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the models to be evaluated\nmodels = {\n    'SVM': SVC(),\n    'KNN': KNeighborsClassifier(),\n    'SGD': SGDClassifier(),\n    'RF': RandomForestClassifier(),\n    'GB': GradientBoostingClassifier(),\n    'NN': MLPClassifier(),\n    'AB': AdaBoostClassifier()\n}\n\n# hyperparameters for each model\nparameters = {\n    'SVM': {\n        'clf__kernel': ['rbf', 'linear', 'sigmoid'],\n    },\n    'KNN': {\n        'clf__n_neighbors': [1, 2, 3, 4, 5, 10, 20, 40, 80],\n    },\n    'SGD': {\n        \n    },\n    'RF': {\n        'clf__n_estimators': [2000],\n        'clf__max_features': [X.shape[1]]\n    },\n    'GB': {\n        'clf__n_estimators': [2000],\n    },\n    'NN': {\n        'clf__hidden_layer_sizes': [100, 250, 500]\n    },\n    'AB': {\n        'clf__n_estimators': [50, 100, 500]\n    }\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a Leave-One-Out Encoder class that can be passed \n# to the pipeline\nclass LOOEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.transformer = LeaveOneOutEncoder()\n        \n    def fit(self, X, y):\n        self.transformer.fit(X, y)\n        return self\n    \n    def transform(self, X):\n        return self.transformer.transform(X)\n\n# create a log transform class that can be passed \n# to the pipeline\nclass LogEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        return np.log(X + 1)\n    \n    \n# combine the previous created classses\ncol_prepro = ColumnTransformer(\n                    transformers=[\n                        ('cat', LOOEncoder(), categorical_names),\n                        ('num', LogEncoder(), numerical_names2)\n                    ],\n                    remainder='passthrough'\n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model = None\nbest_mean_test_score = 0\n\n# evaluate each model\nfor model_name in models.keys():\n    model = models[model_name]\n    params = parameters[model_name]\n    \n    # the final pipeline\n    pipe = Pipeline(\n        steps=[\n            ('col_prepro', col_prepro),\n            ('scaler', MinMaxScaler()),\n            ('over', BorderlineSMOTE(\n                sampling_strategy=0.3\n                )\n            ),\n            ('under', RandomUnderSampler(\n                sampling_strategy=0.5\n                )\n            ),\n            ('clf', model)\n        ]\n    )\n    \n    # apply a gridsearch based on the pre-defined hyperparameters\n    gs_clf = GridSearchCV(\n        pipe, \n        params, \n        n_jobs=-1, \n        cv=StratifiedKFold(\n            n_splits=10, \n            shuffle=True\n        ), \n        scoring='recall'\n    )\n    \n    gs_clf.fit(X, y)\n    \n    # retrieve the results\n    d = pd.DataFrame(gs_clf.cv_results_)\n    best_submodel = d.loc[d['rank_test_score'] == 1]\n    \n    if best_submodel['mean_test_score'].values[0] > best_mean_test_score:\n        best_model = gs_clf.best_estimator_\n        best_mean_test_score = best_submodel['mean_test_score'].values[0]\n        \n    display(d.loc[d['rank_test_score'] <= 3])\n    \ny_pred = best_model.predict(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The highest mean test score of 0.923 is achieved with GradientBoosting. GradientBoosting also appears to be among the most robust models. Its standard deviation across the test sets is approximately 0.018, while most other models have a standard deviation of 0.025 or greater.\n\nThe below results are obtained from training the model on the entire dataset and predicting the corresponding attrited customers (in-sample predictions)."},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    # retrieve the feature importance\n    feat_importance = best_model.named_steps['clf'].feature_importances_\n    feat_importance = pd.DataFrame(feat_importance, \n                                   index = X.columns, \n                                   columns=['Feature Importance'])\n    feat_importance = feat_importance.sort_values(by='Feature Importance')\n    feat_importance.plot(kind='barh', figsize=(9, 7))\n    plt.show()\nexcept:\n    print('Feature importane could not be evaluated')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above figures visualizes the relative importance of each feature, based on the Gini Impurity Index. The higher the value, the better a predictor. The top 5 predictors are\n\n    (1) Total_Trans_Ct\n    (2) Total_Trans_Amt\n    (3) Total_Trans_Ct x Total_Ct_Chng_Q4_Q1\n    (4) Total_Revolving_Bal\n    (5) Total_Relationship_Count\n\nAs expected, the variables with a multimodal distribution are among the best predictors. The third best predictor is the interaction effect between Total_Trans_Ct and Total_Ct_Chng_Q4_Q1. The other interaction effects are less important predictors. For a follow-up study, it might be interesting to analyze customer segmentation."},{"metadata":{"trusted":true},"cell_type":"code","source":"def confusion_matrix2(y_pred, y, names):\n    multi_index = [np.array(['Actual', 'Actual', 'Actual']), \n                   np.array([names[0], names[1], 'Total'])]\n\n    multi_column = [np.array(['Predicted', 'Predicted', 'Predicted']), \n                    np.array([names[0], names[1], 'Total'])]\n\n    conf = np.zeros((3,3))\n    conf[:2, :2] = confusion_matrix(y, y_pred)\n    conf[-1,:] = conf.sum(0)\n    conf[:, -1] = conf.sum(1)\n\n    conf = pd.DataFrame(conf, \n                 index=multi_index, \n                 columns=multi_column)\n\n    conf.iloc[-1, -1] = ''\n\n    display(conf)\n\nconfusion_matrix2(y_pred, y, ['Existing Customer', 'Attrited Customer'])\nprint('Recall = {:.2f}'.format(recall_score(y, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The recall score of the in-sample predictions is 1. This is slighly higher than the mean test score of 0.923."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}