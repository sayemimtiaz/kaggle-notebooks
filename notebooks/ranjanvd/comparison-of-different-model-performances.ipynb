{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Importing required libraries "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing required libraries\n\n%matplotlib inline\n# import necessary libraries and specify that graphs should be plotted inline. \nfrom sklearn.datasets import load_iris\nfrom sklearn import tree\nfrom sklearn.model_selection import cross_val_score, train_test_split,GridSearchCV,KFold\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report,confusion_matrix,roc_curve, auc,matthews_corrcoef\nfrom matplotlib.legend_handler import HandlerLine2D\nfrom sklearn.preprocessing import Normalizer,MinMaxScaler\nimport scikitplot as skplt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\nimport numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading Data "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading the data set from sklearn. As we won't be using Id column, we can use data file from sklearn.\n# Further, both sklearn and the given data are same.\n\nfrom sklearn.datasets import load_breast_cancer\nwdbc = load_breast_cancer()\nprint(wdbc.DESCR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Explore the data set\nn_samples, n_features = wdbc.data.shape\n\n#print(type(wdbc))\n#print(wdbc.keys())\n\nprint ('The dimensions of the data set are', n_samples, 'by', n_features)\nprint('*'*75)\nprint('The classes are: ', wdbc.target_names)\nprint('*'*75)\nprint('The features in the data set are:', wdbc.feature_names)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Explore the data set\nprint('Data:',wdbc.data[:2])\nprint('*'*75)\nprint('Target:',wdbc.target[:2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"##************************************************************************************\n## \n## DECISION TREE\n## \n##************************************************************************************\ntrain_accuracies,test_accuracies = [],[]\nauc_train_results,auc_test_results = [],[]\n\n# Splitting data into train and test in 80-20 ratio.\nX_train, X_test, y_train, y_test = train_test_split(wdbc.data, wdbc.target, test_size=0.2)\n\nmax_depths = range(1,30) \nmin_leaf_size = range(1,30)\n\nfor depth in max_depths:\n    # Decision tree for varying depth\n    clf = tree.DecisionTreeClassifier(criterion=\"gini\", max_depth=depth)\n    \n    ## ******************************************************\n    # We can use AUC-ROC curve to select our hyperparameter.\n    # Here, ROC is a probability curve and AUC represents degree of separability.\n    # It tells how much model is capable of distinguishing between True and Flase output.\n    # Higher the AUC means the model is better at predicting 0s as 0s and 1s as 1s.\n    ## ******************************************************\n    \n    # auc score for training data\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, clf.fit(X_train, y_train).predict_proba(X_train)[:,1])\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    auc_train_results.append(roc_auc)\n    \n    # auc score for testing data\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, clf.fit(X_train, y_train).predict_proba(X_test)[:,1])\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    auc_test_results.append(roc_auc)\n    \n    # accuracy for testing data\n    test_accuracies.append(clf.fit(X_train, y_train).score(X_test, y_test))\n    \n    # accuracy for tarining data\n    train_accuracies.append(clf.fit(X_train, y_train).score(X_train, y_train))\n    \nmydict = {\n    'Train accuracies':train_accuracies,\\\n    'Test accuracies':test_accuracies,\\\n    'AUC train results':auc_train_results,\\\n    'AUC test results':auc_test_results  \n}\ndepths = pd.DataFrame(dict(mydict),index=max_depths)\n\ntrain_accuracies,test_accuracies = [],[]\nauc_train_results,auc_test_results = [],[]\n\nfor leaf in min_leaf_size:\n    # Decision tree for varying depth\n    clf = tree.DecisionTreeClassifier(criterion=\"gini\", min_samples_leaf=leaf)\n    \n    # auc score for training data\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, clf.fit(X_train, y_train).predict_proba(X_train)[:,1])\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    auc_train_results.append(roc_auc)\n    \n    # auc score for testing data\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, clf.fit(X_train, y_train).predict_proba(X_test)[:,1])\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    auc_test_results.append(roc_auc)\n    \n    # accuracy for testing data\n    test_accuracies.append(clf.fit(X_train, y_train).score(X_test, y_test))\n    \n    # accuracy for tarining data\n    train_accuracies.append(clf.fit(X_train, y_train).score(X_train, y_train))\n    \nmydict = {\n    'Train accuracies':train_accuracies,\\\n    'Test accuracies':test_accuracies,\\\n    'AUC train results':auc_train_results,\\\n    'AUC test results':auc_test_results\n}\nleafs = pd.DataFrame(dict(mydict),index=min_leaf_size)    \n# skplt.metrics.plot_cumulative_gain(y_test, predicted_probas)\ntrain_accuracies,test_accuracies = [],[]\nauc_train_results,auc_test_results = [],[]\n\nfor impurity in [.0001,.001,.01,.1,1,10,]:\n    # Decision tree for varying depth\n    clf = tree.DecisionTreeClassifier(criterion=\"gini\", min_impurity_decrease=impurity)\n    \n    # auc score for training data\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, clf.fit(X_train, y_train).predict_proba(X_train)[:,1])\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    auc_train_results.append(roc_auc)\n    \n    # auc score for testing data\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, clf.fit(X_train, y_train).predict_proba(X_test)[:,1])\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    auc_test_results.append(roc_auc)\n    \n    # accuracy for testing data\n    test_accuracies.append(clf.fit(X_train, y_train).score(X_test, y_test))\n    \n    # accuracy for tarining data\n    train_accuracies.append(clf.fit(X_train, y_train).score(X_train, y_train))\n    \nmydict = {\n    'Train accuracies':train_accuracies,\\\n    'Test accuracies':test_accuracies,\\\n    'AUC train results':auc_train_results,\\\n    'AUC test results':auc_test_results\n    \n}\nimpurity = pd.DataFrame(dict(mydict),index=[.0001,.001,.01,.1,1,10,])  \n\n## ***************************************************************************************\n\nfig = plt.figure(figsize=(15,8))\nax1 = fig.add_subplot(231)\nax3 = fig.add_subplot(232)\nax5 = fig.add_subplot(233)\nax2 = fig.add_subplot(234)\nax4 = fig.add_subplot(235)\nax6 = fig.add_subplot(236)\n\nax5.set_xscale('log')\nax6.set_xscale('log')\n\ndepths[['Train accuracies','Test accuracies']].plot(ax=ax1)\ndepths[['AUC train results','AUC test results']].plot(ax=ax2)\nleafs[['Train accuracies','Test accuracies']].plot(ax=ax3)\nleafs[['AUC train results','AUC test results']].plot(ax=ax4)\nimpurity[['Train accuracies','Test accuracies']].plot(ax=ax5)\nimpurity[['AUC train results','AUC test results']].plot(ax=ax6)\nax1.title.set_text('Accuracy vs Tree Depth')\nax2.title.set_text('AUC vs Tree Depth')\nax3.title.set_text('Accuracy vs Min leaf size')\nax4.title.set_text('AUC vs min leaf size')\nax5.title.set_text('Accuracy vs Min impurity decrease')\nax6.title.set_text('AUC vs vs Min impurity decrease')\nplt.show()\n\nprint('*'*100,'\\n')\nprint('Using GridSearchCV fidning the best model')\n# We use Grid search to optimize mutiple paramers:\nparameters = {'min_impurity_decrease':[.01,.1,0,1,10], 'max_depth':list(range(1,10)),'min_samples_leaf':[1,5,10,15,20]}\ndtree = tree.DecisionTreeClassifier()\nclf = GridSearchCV(dtree, parameters, cv=7,scoring='f1')\nclf.fit(wdbc.data, wdbc.target)\nprint('The best hyperparameters are:',clf.best_params_)\nprint('The best score is:',clf.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final Model\nclf = tree.DecisionTreeClassifier(criterion=\"gini\", max_depth=9,min_samples_leaf=5,min_impurity_decrease=0)\nclf = clf.fit(wdbc.data, wdbc.target)\nscores = cross_val_score(clf, wdbc.data, wdbc.target, cv=7,scoring='f1')\nprint(scores)\n# The mean score and the 95% confidence interval of the score estimate are hence given by:\nprint(\"F1 Score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n\nprint('*'*100)\n\nX_train, X_test, y_train, y_test = train_test_split(wdbc.data, wdbc.target, test_size=0.2,random_state = 1)\nclf = clf.fit(X_train, y_train)\npredictions = clf.predict(X_test)\nprobas = clf.predict_proba(X_test)\nprint(classification_report(y_test,predictions))\nprint('*'*100)\nprint('Confusion Matrix\\n',confusion_matrix(y_test, predictions))\nprint('*'*100)\nprint('Matthews Corrcoef',matthews_corrcoef(y_test,predictions))\nprint('*'*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ROC, Lift and Precision Recall Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"## ******************************************************\n# We can use AUC-ROC curve to select our hyperparameter.\n# Here, ROC is a probability curve and AUC represents degree of separability.\n# It tells how much model is capable of distinguishing between True and Flase output.\n# Higher the AUC means the model is better at predicting 0s as 0s and 1s as 1s.\n## ******************************************************\n    \nfig = plt.figure(figsize=(15,4))\nax1 = fig.add_subplot(131)\nax2 = fig.add_subplot(132)\nax3 = fig.add_subplot(133)\nskplt.metrics.plot_precision_recall_curve(y_test, probas,ax=ax1)\nskplt.metrics.plot_roc(y_test, probas,ax=ax2)\nskplt.metrics.plot_lift_curve(y_test, probas,ax=ax3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree Description and Conclusison\n\n**Model perform for several different parameter values**\n\nIn the decision tree, I tried to explore multiple hyperparameters calculating their individual and combined effect(cross producing using GridSearchCV). The hyperparameters which I selected are the depth of the tree, minimum sample size, and minimum impurity decrease. To access the hyperparameter performance I iterated over each hyperparameter and plotted the accuracy and area under the ROC curve (area under the true positive rate vs false positive rate curve)*(have described what is a ROC-AUC in the comments when used above)*. \n\nI split the data into a test and train with a 20-80 ratio. I used the training data to fit the model and then predicted the outcome. The test data and the prediction are used to plot the accuracy and AUC graphs. This process was repeated for the other two parameters. \n\nTo compare different model, we can refer to the polts which evaluates different model performances for there parameter values. \n\n**Overfitting and underfitting?**\n\nFrom the graphs, we can see that when the tree depth is less (i.e. 1 or 2) or the min sample size is high, we observe under-fitting. This makes sense as when the tree depth is less or min sample size is more, the tress will be short and will result in very poor predictions. Similarly, when the tree depth is high (i.e. 20-30) or the min sample size is less, we can observe over-fitting. Again, this is expected as the tree would have grown big that it perfectly fits the training data and fails to perform with testing data. To determine a sweet spot, we need high AUC and good accuracy. Hence, the maximum depth would be around 5, the min sample size would be around 10 and min impurity decrease would be close to .1. \n\nHowever, there is a possibility that the model might perform better if we tune multiple hyperparameters together. Hence, we cross product to generate the combination using GridsearchCV to find the model with a high f1 score. I chose f1 score over accuracy as our data is related to breast cancer and sensitivity is an important factor. If we imagined a cost matrix, we need to penalize a decision 'where a patient has cancer and we predict not cancer' higher over the other predictions. Hence, I chose f1 over accuracy to evaluate the best model. The best hyperparameters combination is {'max_depth': 8, 'min_impurity_decrease': 0, 'min_samples_leaf': 5}. \n\n**Goodness of model**\n\nFinally, we need to evaluate the goodness of our best model. We build a model with the best hyperparameters settings and test it with the 80-20 train and test split to obtain confusion matrix, classification error, precision, recall, f-measure, and MCC score. Further, to determine the mean accuracy we run cross-validation.\n\nWe obtain a mean accuracy as 0.94 with a standard deviation of +/- 0.06. Since the data is realted to a diagnostic of Breast cancer, it becomes important to have a good recall or sensitivity. From the classification report and confusion matrix, we obtain recall, precision and F1 score. The above model shows good perfomance with the cancer data and accuracy might not be the best indicator as mentioned above. Finally, we calculate the Matthews correlation coefficient score to understand a balanced view about true postive and false postive rate: 0.886.\n\n**ROC, Precision-Recall and Lift Curve**\n\nTo evaluate the goodness of the final established model, I plotted the ROC, Precision-Recall and lift curve for my best model. Both ROC and Precision-Recall show very close to ideal classification with an AUC of 0.95. The lift curve generated also has a lift ratio, i.e. the model performs well with respect to random guess."},{"metadata":{},"cell_type":"markdown","source":"## Logistic regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"##************************************************************************************\n## \n## Logistic regression\n## \n##************************************************************************************\n\nc_set = range(1,20)\n\n# Splitting data\nX_train, X_test, y_train, y_test = train_test_split(wdbc.data, wdbc.target, test_size=0.2,random_state=1)\n\ntrain_accuracies,test_accuracies = [],[]\nauc_train_results,auc_test_results = [],[]\nfor c in c_set:\n    # logit with varying c value (Inverse of regularization strength)\n    logreg = LogisticRegression(C=c, penalty='l1')\n    \n    # auc score for training data\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, clf.fit(X_train, y_train).predict_proba(X_train)[:,1])\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    auc_train_results.append(roc_auc)\n    \n    # auc score for testing data\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, clf.fit(X_train, y_train).predict_proba(X_test)[:,1])\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    auc_test_results.append(roc_auc)\n    \n    # accuracy for testing data\n    test_accuracies.append(clf.fit(X_train, y_train).score(X_test, y_test))\n    \n    # accuracy for tarining data\n    train_accuracies.append(clf.fit(X_train, y_train).score(X_train, y_train))\n    \nmydict = {\n    'Train accuracies':train_accuracies,\\\n    'Test accuracies':test_accuracies,\\\n    'AUC train results':auc_train_results,\\\n    'AUC test results':auc_test_results\n}\n\nprint('Logistic Regression for non scaled data')\nc_change_non_normal = pd.DataFrame(dict(mydict),index=c_set)  \nlogreg = LogisticRegression(C=5, penalty='l1')\nscores = cross_val_score(logreg, wdbc.data, wdbc.target, cv=7,scoring='f1')\nprint(scores)\n# The mean score and the 95% confidence interval of the score estimate are hence given by:\nprint(\"\\nF1 Score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n\n# ********************************************************************************************\n\n# Scaling the data\nscaler = MinMaxScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\ntrain_accuracies,test_accuracies = [],[]\nauc_train_results,auc_test_results = [],[]\nfor c in c_set:\n    # logit with varying c value (Inverse of regularization strength)\n    logreg = LogisticRegression(C=c, penalty='l2')\n    \n    # auc score for training data\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, clf.fit(X_train, y_train).predict_proba(X_train)[:,1])\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    auc_train_results.append(roc_auc)\n    \n    # auc score for testing data\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, clf.fit(X_train, y_train).predict_proba(X_test)[:,1])\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    auc_test_results.append(roc_auc)\n    \n    # accuracy for testing data\n    test_accuracies.append(clf.fit(X_train, y_train).score(X_test, y_test))\n    \n    # accuracy for tarining data\n    train_accuracies.append(clf.fit(X_train, y_train).score(X_train, y_train))\n    \nmydict = {\n    'Train accuracies':train_accuracies,\\\n    'Test accuracies':test_accuracies,\\\n    'AUC train results':auc_train_results,\\\n    'AUC test results':auc_test_results\n}\nc_change_normal = pd.DataFrame(dict(mydict),index=c_set)  \n\n# ********************************************************************************************\nprint('*'*100)\nprint('Logistic Regression for scaled data')\nlogreg = LogisticRegression(C=5, penalty='l2')\nscores = cross_val_score(logreg, wdbc.data, wdbc.target, cv=7,scoring='f1')\nprint(scores)\n# The mean score and the 95% confidence interval of the score estimate are hence given by:\nprint(\"\\nF1 Score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\nprint('*'*100)\n\ntrain_accuracies,test_accuracies = [],[]\nauc_train_results,auc_test_results = [],[]\nfor  p in ['l1','l2']:\n    # logit with varying c value (Inverse of regularization strength)\n    logreg = LogisticRegression(penalty=p)\n    \n    # auc score for training data\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, clf.fit(X_train, y_train).predict_proba(X_train)[:,1])\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    auc_train_results.append(roc_auc)\n    \n    # auc score for testing data\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, clf.fit(X_train, y_train).predict_proba(X_test)[:,1])\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    auc_test_results.append(roc_auc)\n    \n    # accuracy for testing data\n    test_accuracies.append(clf.fit(X_train, y_train).score(X_test, y_test))\n    \n    # accuracy for tarining data\n    train_accuracies.append(clf.fit(X_train, y_train).score(X_train, y_train))\n    \nmydict = {\n    'Train accuracies':train_accuracies,\\\n    'Test accuracies':test_accuracies,\\\n    'AUC train results':auc_train_results,\\\n    'AUC test results':auc_test_results,\n}\n\npenalties  = pd.DataFrame(dict(mydict),index=['l1','l2'])\n\n## ***************************************************************************************\nfig = plt.figure(figsize=(15,8))\nax1 = fig.add_subplot(231)\nax3 = fig.add_subplot(232)\nax5 = fig.add_subplot(233)\nax2 = fig.add_subplot(234)\nax4 = fig.add_subplot(235)\nax6 = fig.add_subplot(236)\n\nax5.set_xscale('log')\nax6.set_xscale('log')\n\nc_change_non_normal[['Train accuracies','Test accuracies']].plot(ax=ax1)\nc_change_non_normal[['AUC train results','AUC test results']].plot(ax=ax2)\nc_change_normal[['Train accuracies','Test accuracies']].plot(ax=ax3)\nc_change_normal[['AUC train results','AUC test results']].plot(ax=ax4)\npenalties[['Train accuracies','Test accuracies']].plot(ax=ax5,kind = 'barh')\npenalties[['AUC train results','AUC test results']].plot(ax=ax6,kind = 'barh')\nax1.title.set_text('Accuracy vs C(Inverse of regularization strength) Non Scaled Data')\nax2.title.set_text('AUC vs C(Inverse of regularization strength) Non Scaled Data')\nax3.title.set_text('Accuracy vs C(Inverse of regularization strength) Scaled Data')\nax4.title.set_text('AUC vs min C(Inverse of regularization strength) Scaled Data')\nax5.title.set_text('Accuracy vs Penality')\nax6.title.set_text('AUC vs vs Penality')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GRIDSEARCH\nprint('*'*100,'\\n')\nprint('Using GridSearchCV fidning the best model')\n# We use Grid search to optimize mutiple paramers:\nparameters = {'C':range(1,10), 'penalty':['l1','l2']}\nlogit = LogisticRegression()\nclf = GridSearchCV(logit, parameters, cv=7, scoring='f1')\nclf.fit(wdbc.data, wdbc.target)\nprint('The best hyperparameters are:',clf.best_params_)\nprint('The best score is:',clf.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting data\nX_train, X_test, y_train, y_test = train_test_split(wdbc.data, wdbc.target, test_size=0.2,random_state = 1)\n\n# Final Model - LOGIT\nclf = LogisticRegression(C=8, penalty='l1')\nscores = cross_val_score(logreg, wdbc.data, wdbc.target, cv=7,scoring='f1')\nprint(scores)\n# The mean score and the 95% confidence interval of the score estimate are hence given by:\nprint(\"\\nF1 Score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n\nprint('*'*100)\n\nX_train, X_test, y_train, y_test = train_test_split(wdbc.data, wdbc.target, test_size=0.2,random_state = 1)\nclf = clf.fit(X_train, y_train)\npredictions = clf.predict(X_test)\nprobas = clf.predict_proba(X_test)\nprint(classification_report(y_test,predictions))\nprint('*'*100)\nprint('Confusion Matrix\\n',confusion_matrix(y_test, predictions))\nprint('*'*100)\nprint('Matthews Corrcoef',matthews_corrcoef(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ROC, Lift and Precision Recall RECALL Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15,4))\nax1 = fig.add_subplot(131)\nax2 = fig.add_subplot(132)\nax3 = fig.add_subplot(133)\nskplt.metrics.plot_precision_recall_curve(y_test, probas,ax=ax1)\nskplt.metrics.plot_roc(y_test, probas,ax=ax2)\nskplt.metrics.plot_lift_curve(y_test, probas,ax=ax3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logit Description and Conclusion\n\n**Model perform for several different parameter values**\n\nIn the logistic regression, I tried to explore multiple hyperparameters calculating their individual and combined effect(cross producing using GridSearchCV). The hyperparameters which I selected are the C(Inverse of regularization strength) and penalty for scaled and unscaled data. To access the hyperparameter performance I iterated over each hyperparameter and plotted the accuracy and area under the ROC curve (area under the true positive rate vs false positive rate curve)*(have described what is a ROC-AUC in the comments when used above)*. \n\nI split the data into test and train with a 20-80 ratio. I used the training data to fit the model and then predict the outcome. The test data and the prediction are used to plot the accuracy and AUC graphs. This process was repeated for the other parameter. \n\n**Overfitting and when is underfitting?**\n\nFrom the graphs, we can see that when penalties are low we can observe under-fitting. As when the C value is smaller that is stronger regularization we see the accuracy and AUC curve to show lower values. Similarly, when we have higher C value which is lower regularization we can observe over-fitting. The classification would not perform well and hence we observe lower accuracy. Further, we explore the different penalties('l1', 'l2') which can be attributed to the model. Both the penalties try to bring the beta coefficients of the logit model toward zero. But the difference is that 'l1' penalizes the coefficients linearly and 'l2' penalizes the coefficients in a parabolic curve where outlier are more penalized. From the observation, we don't find a big difference between both the penalties.\n\nAgain, there might be a possibility that the model might be better if we tune multiple hyperparameters together. Hence, we cross product to generate the combination using GridsearchCV to find the model with high accuracy. I chose f1 score over accuracy as our data is related to breast cancer and sensitivity is an important factor. If we imagined a cost matrix, we need to penalize a decision where a patient has cancer and we predict not cancer higher over the other predictions. Hence, I chose f1 over accuracy to evaluate the best model. The best hyperparameters combination is: {'C': 8, 'penalty': 'l1'}\n\n**Goodness of model**\n\nFinally, we need to evaluate the goodness of our best model. We build a model with the best hyperparameters settings and test it with the 80-20 train and test split to obtain confusion matrix, classification error, precision, recall, f-measure, and MCC score. Further, to determine the mean accuracy we run cross-validation.\n\nOur logit has a high mean accuracy of 0.95 with a range +/- .05. We obtain Matthews Corrcoef 0.9 which is desirable. From the classification report and confusion matrix, we obtain recall, precision, and F score. For the above model accuracy might not be the best indicator as mentioned above. Hence, we considered the f1 score to evaluate the model.\n\n**ROC, Precision-Recall and Lift Curve**\n\nTo evaluate the goodness of the final established model, I plotted the ROC, Precision-Recall and lift curve for my best model. Both ROC and Precision-Recall show very close to ideal classification with an AUC of 0.99. The lift curve generated also has a lift ratio, i.e. the model performs well with respect to random guess."},{"metadata":{},"cell_type":"markdown","source":"## K-Nearest Neighbors"},{"metadata":{"trusted":true},"cell_type":"code","source":"##******************************************************\n## \n## KNN\n## \n##******************************************************\n\n# Optimize KNN classifier and detect (potential) over-fitting\n\nneighbors_values = range(1,30)\nscaler = MinMaxScaler()\n\nX_train, X_test, y_train, y_test = train_test_split(wdbc.data, wdbc.target, test_size=0.2)\n# Scaling the features to fit it into KNN\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\ncross_validation_score = []\nfor n in neighbors_values:\n    # KNN with varying n value\n    knn = KNeighborsClassifier(n_neighbors = n)\n    # Cross validation scrore\n    cross_validation_score.append(cross_val_score(knn, wdbc.data, wdbc.target, cv=7,scoring = 'f1').mean())\n\nmydict_ = {\n    'Cross validation score':cross_validation_score\n}\n\ncross_validation_score = []\nfor wt in [\"uniform\", \"distance\"]:\n    # KNN with varying n value\n    knn = KNeighborsClassifier(weights = wt)\n    # Cross validation scrore\n    cross_validation_score.append(cross_val_score(knn, wdbc.data, wdbc.target, cv=7,scoring = 'f1').mean())\n    \nmydict = {\n    'Cross validation score':cross_validation_score\n}\n\nneighbors = pd.DataFrame(dict(mydict_),index=neighbors_values)  \nweights = pd.DataFrame(dict(mydict),index=[\"uniform\", \"distance\"])  \n\nfig = plt.figure(figsize=(15,8))\nax1 = fig.add_subplot(221)\nax2 = fig.add_subplot(222)\n\n\nneighbors[['Cross validation score']].plot(ax=ax1)\nweights[['Cross validation score']].plot(kind='bar',ax=ax2)\nax1.title.set_text('F1 Score vs Neighbours')\nax2.title.set_text('AUC vs Weights')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GRIDSEARCH\nscaler = MinMaxScaler()\nscaler.fit(wdbc.data)\nx = scaler.transform(wdbc.data)\nprint('*'*100,'\\n')\nprint('Using GridSearchCV fidning the best model')\n# We use Grid search to optimize mutiple paramers:\nparameters = {'weights':[\"uniform\", \"distance\"], 'n_neighbors' : range(5,15)}\nknn = KNeighborsClassifier()\nclf = GridSearchCV(knn, parameters, cv=7,scoring='f1')\nclf.fit(x, wdbc.target)\nprint('The best hyperparameters are:',clf.best_params_)\nprint('The best score is:',clf.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final Model - KNN\nknn = KNeighborsClassifier(n_neighbors= 12, weights= 'distance')\nscores = cross_val_score(knn, wdbc.data, wdbc.target, cv=7,scoring='f1')\nprint(scores)\n# The mean score and the 95% confidence interval of the score estimate are hence given by:\nprint(\"\\nF1 Score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n\nX_train, X_test, y_train, y_test = train_test_split(wdbc.data, wdbc.target, test_size=0.2,random_state = 2)\n# Scaling the features to fit it into KNN\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\nknn = knn.fit(X_train, y_train)\npredictions = knn.predict(X_test)\nprobas = knn.predict_proba(X_test)\nprint('*'*100)\nprint(classification_report(y_test,predictions))\nprint('*'*100)\nprint('Confusion Matrix\\n',confusion_matrix(y_test, predictions))\nprint('*'*100)\nprint('Matthews Corrcoef',matthews_corrcoef(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ROC, Lift and Precision Recall RECALL Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15,4))\nax1 = fig.add_subplot(131)\nax2 = fig.add_subplot(132)\nax3 = fig.add_subplot(133)\nskplt.metrics.plot_precision_recall_curve(y_test, probas,ax=ax1)\nskplt.metrics.plot_roc(y_test, probas,ax=ax2)\nskplt.metrics.plot_lift_curve(y_test, probas,ax=ax3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### KNN Description and Conclusion\n\n**Model perform for several different parameter values**\n\nIn the KNN, I tried to explore multiple hyperparameters calculating their individual and combined effect(cross producing using GridSearchCV). The hyperparameters which I selected are the number of neighbors and weight of the distance calculation. To access the hyperparameter performance I iterated over each hyperparameter and plotted the cross-validation mean score. The scoring is based on the f1 parameter, as we are concerned about the recall and precision.\n\nTo compare different parameters, we can refer to the polts which evaluates different model performances for there parameter values. \n\n**Overfitting and when is underfitting?**\n\nFrom the graphs, we can see that when the knn has very fewer neighbors (i.e. 1 or 2), we observe over-fitting. This makes sense as when the neighbors are less the classifier will account for all trivial classification and will result in very poor predictions. Similarly, when the neighbors are more (i.e. 25-30), we can observe under-fitting. Again, this is expected as the knn would have generalized the sample more vaguely and fails to perform with testing data. To determine a sweet spot, we need a high F1 score. Hence, the neighbors would be around 12.\n\nHowever, there is a possibility that the model might perform better if we tune multiple hyperparameters together. Hence, we cross product to generate the combination using GridsearchCV to find the model with a high f1 score. I chose f1 score over accuracy as our data is related to breast cancer and sensitivity is an important factor. If we imagined a cost matrix, we need to penalize a decision where a patient has cancer and we predict not cancer higher over the other predictions. Hence, I chose f1 over accuracy to evaluate the best model. The best hyperparameters combination is {'n_neighbors': 12, 'weights': 'distance'}\n\n**Goodness of model**\n\nFinally, we need to evaluate the goodness of our best model. We build a model with the best hyperparameters settings and test it with the 80-20 train and test split to obtain confusion matrix, classification error, precision, recall, f-measure, and MCC score. Further, to determine the mean accuracy we run cross-validation.\n\nWe obtain a mean F1 score as 0.95 with a standard deviation of +/- 0.05. Since the data is related to a diagnostic of Breast cancer, it becomes important to have a good recall or sensitivity. From the classification report and confusion matrix, we obtain recall, precision. The above decision classifier performs well with the cancer data and accuracy might not be the best indicator as mentioned above. Finally, we calculate the Matthews correlation coefficient score to understand a balanced view about true positive and false positive rate: 0.907\n\n**ROC, Precision-Recall and Lift Curve**\n\nTo evaluate the goodness of the final established model, I plotted the ROC, Precision-Recall and lift curve for my best model. Both ROC and Precision-Recall show very close to ideal classification with an AUC of 0.99. The lift curve generated also has a lift ratio, i.e. the model performs well with respect to random guess."},{"metadata":{"trusted":true},"cell_type":"code","source":"##************************************************************************************\n## \n## SVM\n## \n##************************************************************************************\ntrain_accuracies,test_accuracies = [],[]\nauc_train_results,auc_test_results = [],[]\n\n# Splitting data into train and test in 80-20 ratio.\nX_train, X_test, y_train, y_test = train_test_split(wdbc.data, wdbc.target, test_size=0.2)\n\nscaler = MinMaxScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\nC = range(1,30) \ngamma = [.001,.01,.1,1,10,100]\n\nfor i in C:\n    # Decision tree for varying depth\n    clf = SVC(kernel=\"linear\", C=i,probability=True)\n    \n    ## ******************************************************\n    # We can use AUC-ROC curve to select our hyperparameter.\n    # Here, ROC is a probability curve and AUC represents degree of separability.\n    # It tells how much model is capable of distinguishing between True and Flase output.\n    # Higher the AUC means the model is better at predicting 0s as 0s and 1s as 1s.\n    ## ******************************************************\n    \n    # auc score for training data\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, clf.fit(X_train, y_train).predict_proba(X_train)[:,1])\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    auc_train_results.append(roc_auc)\n    \n    # auc score for testing data\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, clf.fit(X_train, y_train).predict_proba(X_test)[:,1])\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    auc_test_results.append(roc_auc)\n    \n    # accuracy for testing data\n    test_accuracies.append(clf.fit(X_train, y_train).score(X_test, y_test))\n    \n    # accuracy for tarining data\n    train_accuracies.append(clf.fit(X_train, y_train).score(X_train, y_train))\n    \nmydict = {\n    'Train accuracies':train_accuracies,\\\n    'Test accuracies':test_accuracies,\\\n    'AUC train results':auc_train_results,\\\n    'AUC test results':auc_test_results  \n}\ndepths = pd.DataFrame(dict(mydict),index=C)\n\ntrain_accuracies,test_accuracies = [],[]\nauc_train_results,auc_test_results = [],[]\n\nfor i in C:\n    # Decision tree for varying depth\n    clf = SVC(kernel=\"rbf\", C=i,probability=True)\n    \n    # auc score for training data\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, clf.fit(X_train, y_train).predict_proba(X_train)[:,1])\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    auc_train_results.append(roc_auc)\n    \n    # auc score for testing data\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, clf.fit(X_train, y_train).predict_proba(X_test)[:,1])\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    auc_test_results.append(roc_auc)\n    \n    # accuracy for testing data\n    test_accuracies.append(clf.fit(X_train, y_train).score(X_test, y_test))\n    \n    # accuracy for tarining data\n    train_accuracies.append(clf.fit(X_train, y_train).score(X_train, y_train))\n    \nmydict = {\n    'Train accuracies':train_accuracies,\\\n    'Test accuracies':test_accuracies,\\\n    'AUC train results':auc_train_results,\\\n    'AUC test results':auc_test_results\n}\nleafs = pd.DataFrame(dict(mydict),index=C)    \n# skplt.metrics.plot_cumulative_gain(y_test, predicted_probas)\ntrain_accuracies,test_accuracies = [],[]\nauc_train_results,auc_test_results = [],[]\n\nfor g in gamma:\n    # Decision tree for varying depth\n    clf = SVC(kernel=\"rbf\", gamma=g,probability=True)\n    \n    # auc score for training data\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, clf.fit(X_train, y_train).predict_proba(X_train)[:,1])\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    auc_train_results.append(roc_auc)\n    \n    # auc score for testing data\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, clf.fit(X_train, y_train).predict_proba(X_test)[:,1])\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    auc_test_results.append(roc_auc)\n    \n    # accuracy for testing data\n    test_accuracies.append(clf.fit(X_train, y_train).score(X_test, y_test))\n    \n    # accuracy for tarining data\n    train_accuracies.append(clf.fit(X_train, y_train).score(X_train, y_train))\n    \nmydict = {\n    'Train accuracies':train_accuracies,\\\n    'Test accuracies':test_accuracies,\\\n    'AUC train results':auc_train_results,\\\n    'AUC test results':auc_test_results\n    \n}\nimpurity = pd.DataFrame(dict(mydict),index=gamma)  \n\n## ***************************************************************************************\n\nfig = plt.figure(figsize=(15,8))\nax1 = fig.add_subplot(231)\nax3 = fig.add_subplot(232)\nax5 = fig.add_subplot(233)\nax2 = fig.add_subplot(234)\nax4 = fig.add_subplot(235)\nax6 = fig.add_subplot(236)\n\nax5.set_xscale('log')\nax6.set_xscale('log')\n\n\ndepths[['Train accuracies','Test accuracies']].plot(ax=ax1)\ndepths[['AUC train results','AUC test results']].plot(ax=ax2)\nleafs[['Train accuracies','Test accuracies']].plot(ax=ax3)\nleafs[['AUC train results','AUC test results']].plot(ax=ax4)\nimpurity[['Train accuracies','Test accuracies']].plot(ax=ax5)\nimpurity[['AUC train results','AUC test results']].plot(ax=ax6)\nax1.title.set_text('Accuracy vs C - Linear SVM')\nax2.title.set_text('AUC vs C - Linear SVM')\nax3.title.set_text('Accuracy vs C - Non Linear SVM')\nax4.title.set_text('AUC vs C - Non Linear SVM')\nax5.title.set_text('Accuracy vs Gamma - Non Linear SVM')\nax6.title.set_text('AUC vs Gamma - Non Linear SVM')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GRIDSEARCH\nscaler = MinMaxScaler()\nscaler.fit(wdbc.data)\nX = scaler.transform(wdbc.data)\n\nprint('*'*100,'\\n')\nprint('Using GridSearchCV fidning the best model for Linear SVC')\n# We use Grid search to optimize mutiple paramers:\nparameters = {'kernel':['linear','rbf'],'C':range(1,30)}\nclf = SVC()\nclf = GridSearchCV(clf, parameters, cv=7, scoring='f1')\nclf.fit(X, wdbc.target)\nprint('The best hyperparameters are:',clf.best_params_)\nprint('The best score is:',clf.best_score_)\n\n\nprint('*'*100,'\\n')\nprint('Using GridSearchCV fidning the best model for Non-Linear SVC')\nparameters = {'kernel':['rbf','sigmoid'],'C':range(1,30),'gamma':[.001,.01,.1,1,10,100]}\nclf = SVC()\nclf = GridSearchCV(clf, parameters, cv=7, scoring='f1')\nclf.fit(X, wdbc.target)\nprint('The best hyperparameters are:',clf.best_params_)\nprint('The best score is:',clf.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final Model\nclf = SVC(kernel=\"linear\", C=4,probability=True)\nclf = clf.fit(wdbc.data, wdbc.target)\nscores = cross_val_score(clf, X, wdbc.target, cv=7,scoring='f1')\nprint(scores)\n# The mean score and the 95% confidence interval of the score estimate are hence given by:\nprint(\"F1 Score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n\nprint('*'*100)\n\nX_train, X_test, y_train, y_test = train_test_split(wdbc.data, wdbc.target, test_size=0.2,random_state = 1)\n\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\nclf = clf.fit(X_train, y_train)\npredictions = clf.predict(X_test)\nprobas = clf.predict_proba(X_test)\nprint(classification_report(y_test,predictions))\nprint('*'*100)\nprint('Confusion Matrix\\n',confusion_matrix(y_test, predictions))\nprint('*'*100)\nprint('Matthews Corrcoef',matthews_corrcoef(y_test,predictions))\nprint('*'*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ROC, Lift and Precision Recall RECALL Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15,4))\nax1 = fig.add_subplot(131)\nax2 = fig.add_subplot(132)\nax3 = fig.add_subplot(133)\nskplt.metrics.plot_precision_recall_curve(y_test, probas,ax=ax1)\nskplt.metrics.plot_roc(y_test, probas,ax=ax2)\nskplt.metrics.plot_lift_curve(y_test, probas,ax=ax3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVC Description and Conclusion\n\n**Model perform for several different parameter values**\n\nIn the SVC, I tried to explore multiple hyperparameters calculating their individual and combined effect(cross producing using GridSearchCV). The hyperparameters which I selected are the number of neighbors and weight of the distance calculation. To access the hyperparameter performance I iterated over each hyperparameter and plotted the cross-validation mean score. The scoring is based on the f1 parameter, as we are concerned about the recall and precision.\n\nTo compare different parameters, we can refer to the polts which evaluates different model performances for there parameter values. \n\n**Overfitting and when is underfitting?**\n\nFrom the graphs, we can see that when the knn has very fewer neighbors (i.e. 1 or 2), we observe over-fitting. This makes sense as when the neighbors are less the classifier will account for all trivial classification and will result in very poor predictions. Similarly, when the neighbors are more (i.e. 25-30), we can observe under-fitting. Again, this is expected as the knn would have generalized the sample more vaguely and fails to perform with testing data. To determine a sweet spot, we need a high F1 score. Hence, the neighbors would be around 12.\n\nHowever, there is a possibility that the model might perform better if we tune multiple hyperparameters together. Hence, we cross product to generate the combination using GridsearchCV to find the model with a high f1 score. I chose f1 score over accuracy as our data is related to breast cancer and sensitivity is an important factor. If we imagined a cost matrix, we need to penalize a decision where a patient has cancer and we predict not cancer higher over the other predictions. Hence, I chose f1 over accuracy to evaluate the best model. The best hyperparameters combination is {'n_neighbors': 12, 'weights': 'distance'}\n\n**Goodness of model**\n\nFinally, we need to evaluate the goodness of our best model. We build a model with the best hyperparameters settings and test it with the 80-20 train and test split to obtain confusion matrix, classification error, precision, recall, f-measure, and MCC score. Further, to determine the mean accuracy we run cross-validation.\n\nWe obtain a mean F1 score as 0.95 with a standard deviation of +/- 0.05. Since the data is related to a diagnostic of Breast cancer, it becomes important to have a good recall or sensitivity. From the classification report and confusion matrix, we obtain recall, precision. The above decision classifier performs well with the cancer data and accuracy might not be the best indicator as mentioned above. Finally, we calculate the Matthews correlation coefficient score to understand a balanced view about true positive and false positive rate: 0.907\n\n**ROC, Precision-Recall and Lift Curve**\n\nTo evaluate the goodness of the final established model, I plotted the ROC, Precision-Recall and lift curve for my best model. Both ROC and Precision-Recall show very close to ideal classification with an AUC of 0.99. The lift curve generated also has a lift ratio, i.e. the model performs well with respect to random guess."},{"metadata":{},"cell_type":"markdown","source":"# Final Model Comparison"},{"metadata":{},"cell_type":"markdown","source":"\n|Models|Best F1 Score|Mean F1 Score|Nested F1 Score|Matthews Corrcoef|AUC of tuned model|\n|---|---|---|---|---|---|\n|Decision Tree|0.95|0.94|0.942|.88|.95|\n|Logit|0.97|0.96|0.960|0.905|.99|\n|KNN|0.9779|0.95|0.9739|0.907|.99|\n|SVM Linear|0.984|0.98|0.9805|0.94|.99|\n|SVM Non Linear|0.986|0.98|0.9825|0.94|.99|"},{"metadata":{},"cell_type":"markdown","source":"In conclusion, after comparing all the three models, and their F1 scores and MCC score we can say SVM is performing better, followed by Logit and KNN. Further, the AUC of all the models is quite good. SVM has a slightly better Precision-Recall curve."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}