{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Defining Functions for Importing the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import to_categorical\ndef dataset_to_tensor(data):\n    feature= np.array(data[['userAcceleration.x','userAcceleration.y','userAcceleration.z']])\n    label =  np.array(to_categorical(data[['act']]))\n    from keras.preprocessing.sequence import TimeseriesGenerator\n    data_gen = TimeseriesGenerator(feature, label,\n                               length=100,stride=100,sampling_rate=1,batch_size=len(data))\n    return data_gen[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom keras.preprocessing.sequence import TimeseriesGenerator\ndef get_ds_infos():\n    \"\"\"\n    Read the file includes data subject information.\n    \n    Data Columns:\n    0: code [1-24]\n    1: weight [kg]\n    2: height [cm]\n    3: age [years]\n    4: gender [0:Female, 1:Male]\n    \n    Returns:\n        A pandas DataFrame that contains inforamtion about data subjects' attributes \n    \"\"\" \n\n    dss = pd.read_csv(\"/kaggle/input/motionsense-dataset/data_subjects_info.csv\")\n    print(\"[INFO] -- Data subjects' information is imported.\")\n    \n    return dss\n\ndef set_data_types(data_types=[\"userAcceleration\"]):\n    \"\"\"\n    Select the sensors and the mode to shape the final dataset.\n    \n    Args:\n        data_types: A list of sensor data type from this list: [attitude, gravity, rotationRate, userAcceleration] \n\n    Returns:\n        It returns a list of columns to use for creating time-series from files.\n    \"\"\"\n    dt_list = []\n    for t in data_types:\n        if t != \"attitude\":\n            dt_list.append([t+\".x\",t+\".y\",t+\".z\"])\n        else:\n            dt_list.append([t+\".roll\", t+\".pitch\", t+\".yaw\"])\n\n    return dt_list\n\n\ndef creat_time_series(dt_list, act_labels, trial_codes, mode=\"mag\", labeled=True):\n    \"\"\"\n    Args:\n        dt_list: A list of columns that shows the type of data we want.\n        act_labels: list of activites\n        trial_codes: list of trials\n        mode: It can be \"raw\" which means you want raw data\n        for every dimention of each data type,\n        [attitude(roll, pitch, yaw); gravity(x, y, z); rotationRate(x, y, z); userAcceleration(x,y,z)].\n        or it can be \"mag\" which means you only want the magnitude for each data type: (x^2+y^2+z^2)^(1/2)\n        labeled: True, if we want a labeld dataset. False, if we only want sensor values.\n\n    Returns:\n        It returns a time-series of sensor data.\n    \n    \"\"\"\n    num_data_cols = len(dt_list) if mode == \"mag\" else len(dt_list*3)\n    \n    tensor_vals = np.empty([0,100,num_data_cols] ,dtype=object) #TOdo initialize Tuple Correctly\n    tensor_labels = np.empty([0,len(act_labels)])\n    print (tensor_vals.shape)\n    print(tensor_labels.shape)\n\n    \n    if labeled:\n        dataset = np.zeros((0,num_data_cols+7)) # \"7\" --> [act, code, weight, height, age, gender, trial] \n    else:\n        dataset = np.zeros((0,num_data_cols))\n        \n    ds_list = get_ds_infos()\n    \n    print(\"[INFO] -- Creating Time-Series\")\n    for sub_id in ds_list[\"code\"]:\n        for act_id, act in enumerate(act_labels):\n            for trial in trial_codes[act_id]:\n                fname = '/kaggle/input/motionsense-dataset/A_DeviceMotion_data/A_DeviceMotion_data/'+act+'_'+str(trial)+'/sub_'+str(int(sub_id))+'.csv'\n                raw_data = pd.read_csv(fname)\n                raw_data = raw_data.drop(['Unnamed: 0'], axis=1)\n                vals = np.zeros((len(raw_data), num_data_cols))\n                for x_id, axes in enumerate(dt_list):\n                    if mode == \"mag\":\n                        vals[:,x_id] = (raw_data[axes]**2).sum(axis=1)**0.5        \n                    else:\n                        vals[:,x_id*3:(x_id+1)*3] = raw_data[axes].values\n                    vals = vals[:,:num_data_cols]\n                if labeled:\n                    lbls = np.array([[act_id,\n                            sub_id-1,\n                            ds_list[\"weight\"][sub_id-1],\n                            ds_list[\"height\"][sub_id-1],\n                            ds_list[\"age\"][sub_id-1],\n                            ds_list[\"gender\"][sub_id-1],\n                            trial          \n                           ]]*len(raw_data))\n\n                    data_gen = TimeseriesGenerator(vals, np.full(len(vals),act_id),length=100,stride=100,sampling_rate=1,batch_size=len(vals)) \n                    vals = np.concatenate((vals, lbls), axis=1)\n\n                dataset = np.append(dataset,vals, axis=0)\n                tensor_vals= np.append(tensor_vals,data_gen[0][0],axis=0) \n                tensor_labels=np.append(tensor_labels,to_categorical(data_gen[0][1],num_classes=len(act_labels)),axis=0)\n                \n    cols = []\n    for axes in dt_list:\n        if mode == \"raw\":\n            cols += axes\n        else:\n            cols += [str(axes[0][:-2])]\n            \n    if labeled:\n        cols += [\"act\", \"id\", \"weight\", \"height\", \"age\", \"gender\", \"trial\"]\n    \n    dataset = pd.DataFrame(data=dataset, columns=cols)\n    return dataset,tensor_vals,tensor_labels\n#________________________________\n\n\nACT_LABELS = [\"dws\",\"ups\", \"wlk\", \"jog\", \"std\", \"sit\"]\nTRIAL_CODES = {\n    ACT_LABELS[0]:[1,2,11],\n    ACT_LABELS[1]:[3,4,12],\n    ACT_LABELS[2]:[8,15], #Removed Trail 7 to avoid imbalanced target classes\n    ACT_LABELS[3]:[9,16],\n    ACT_LABELS[4]:[6,14],\n    ACT_LABELS[5]:[5,13]\n}\n\n## Here we set parameter to build labeld time-series from dataset of \"(A)DeviceMotion_data\"\n## attitude(roll, pitch, yaw); gravity(x, y, z); rotationRate(x, y, z); userAcceleration(x,y,z)\nsdt = [\"userAcceleration\"]\nprint(\"[INFO] -- Selected sensor data types: \"+str(sdt))    \nact_labels = ACT_LABELS [0:4]\nprint(\"[INFO] -- Selected activites: \"+str(act_labels))    \ntrial_codes = [TRIAL_CODES[act] for act in act_labels]\ndt_list = set_data_types(sdt)\ndataset,tensor_vals,tensor_labels= creat_time_series(dt_list, act_labels, trial_codes, mode=\"raw\", labeled=True)\nprint(\"[INFO] -- Shape of time-Series dataset:\"+str(dataset.shape))    \ndataset['act'] = pd.Categorical(dataset['act'].astype(int))\ndataset['act'].cat.rename_categories(ACT_LABELS[0:4],inplace=True)\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tensor_vals = np.empty([0,100,3] ,dtype=object) #TOdo initialize Tuple Correctly\n#tensor_labels = np.empty([0,len(act_labels)])\n\n#for g in dataset.groupby(['act','id']).apply(lambda x : TimeseriesGenerator(x[['userAcceleration.x','userAcceleration.y','userAcceleration.z']].values.tolist(), np.full(len(x['act'].cat.codes.tolist()),x['act'].cat.codes.tolist()),length=100,stride=100,sampling_rate=1,batch_size=len(x['act'].cat.codes.tolist()))):\n   # data_gen = g\n   # tensor_vals= np.append(tensor_vals,data_gen[0][0],axis=0) \n   # tensor_labels=np.append(tensor_labels,to_categorical(data_gen[0][1],num_classes=len(act_labels)),axis=0)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Statistical Evaluation of the Dataset"},{"metadata":{},"cell_type":"markdown","source":"## Distribution of the target Classes among the number of Timeseries Datapoints"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=(30,10))\ndataset.groupby('act')['userAcceleration.x'].count().plot(kind='bar')\nplt.title('Number of Timeseries Datapoints by Actitity')\nplt.xlabel('Activity')\nplt.ylabel('Number of Datapoints')\nplt.show()\nplt.savefig('datapoints_by_label.svg')\n#print (dataset.dtypes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Value Distribution of the recorded Sensor Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[['userAcceleration.x','userAcceleration.y','userAcceleration.z']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[['userAcceleration.x','userAcceleration.y','userAcceleration.z']].boxplot()\nfig = plt.gcf()\nfig.set_size_inches(15, 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distribution of the Participants Parameters (age,height,weight)"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndataset[['weight','height','age']].boxplot()\nfig = plt.gcf()\nfig.set_size_inches(30, 15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.base import TransformerMixin\nfrom sklearn.preprocessing import StandardScaler\nclass NDStandardScaler(TransformerMixin):\n    def __init__(self, **kwargs):\n        self._scaler = StandardScaler(copy=True, **kwargs)\n        self._orig_shape = None\n\n    def fit(self, X, **kwargs):\n        X = np.array(X)\n        # Save the original shape to reshape the flattened X later\n        # back to its original shape\n        if len(X.shape) > 1:\n            self._orig_shape = X.shape[1:]\n        X = self._flatten(X)\n        self._scaler.fit(X, **kwargs)\n        return self\n\n    def transform(self, X, **kwargs):\n        X = np.array(X)\n        X = self._flatten(X)\n        X = self._scaler.transform(X, **kwargs)\n        X = self._reshape(X)\n        return X\n\n    def _flatten(self, X):\n        # Reshape X to <= 2 dimensions\n        if len(X.shape) > 2:\n            n_dims = np.prod(self._orig_shape)\n            X = X.reshape(-1, n_dims)\n        return X\n\n    def _reshape(self, X):\n        # Reshape X back to it's original shape\n        if len(X.shape) >= 2:\n            X = X.reshape(-1, *self._orig_shape)\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\n#print('Number of Batches: '+ str(len(tensor_data)))\n\n#features,labels = tensor_data\nscaler = NDStandardScaler()\nfeatures = scaler.fit_transform(tensor_vals)\nlabels = tensor_labels\n\nn_features = len(features[0][0])\nn_timesteps = len(features[0])\nn_outputs = len(labels[0])\n\nprint('Shape of Sequences within a Batch')\nprint(features.shape)\nprint('Shape of Labels within a Batch')\nprint(labels.shape)\n\nprint('Number of features :'+ str(n_features))\nprint('Number of timestamps: '+ str(n_timesteps))\nprint('Number of outputs: '+ str(n_outputs))\nprint('Number of samples:'+ str(len(features)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.bar(ACT_LABELS[0:4],np.bincount(np.argmax(labels,axis=1)))\nplt.title('Distribution of Activitys among Samples')\nplt.xlabel('Activity')\nplt.ylabel('Number of Samples')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split the data\nx_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.33, shuffle= True)\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.33, shuffle= True)\n\nscores = dict()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Try Naive ML Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_categorial(vector):\n    res =[]\n    for x in vector:\n        res.append(np.argmax(x))\n    return res\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.svm import LinearSVC\nnaive_model = LinearSVC()\nnaive_model.fit(x_train.reshape(len(x_train),300,order='F'),to_categorial(y_train))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nprediction = naive_model.predict(x_valid.reshape(len(x_valid),300,order='F'))\nf1 = metrics.f1_score(prediction,to_categorial(y_valid),average='weighted')\naccurancy = metrics.accuracy_score(prediction,to_categorial(y_valid))\nrecall = metrics.recall_score(prediction,to_categorial(y_valid),average='weighted')\nprecision = metrics.precision_score(prediction,to_categorial(y_valid),average='weighted')\nloss = metrics.hamming_loss(prediction,to_categorial(y_valid))\n#TODO Calculate Value Loss\nscores['navie']= {'loss':loss,'f1_m':f1,'accurancy':accurancy,'precision_m':precision,'recall_m':recall}\nprint (\"F1 Socre of Naive Model: \"+ str(f1) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preperation for Training of DNN with Keras\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\nimport matplotlib.pyplot as plt\n\ndef recall_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\n\ndef precision_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n\n\ndef train_model(model,name,training_data = (x_train,y_train),validation_data=(x_valid,y_valid),epochs=20,plot=False):\n    from keras.callbacks import ModelCheckpoint\n    model.compile(loss='categorical_crossentropy', optimizer='adam', weighted_metrics=[f1_m,'acc',precision_m, recall_m])\n \n    checkpointer = ModelCheckpoint(filepath='/kaggle/working/weights.best.'+name+'.hdf5', \n                               verbose=1, save_best_only=True)\n\n    history = model.fit(training_data[0], training_data[1], \n          validation_data=(validation_data[0], validation_data[1]),\n          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)\n    if plot:\n            # Plot training & validation accuracy values\n        plt.plot(history.history['acc'])\n        plt.plot(history.history['val_acc'])\n        plt.title('Model accuracy')\n        plt.ylabel('Accuracy')\n        plt.xlabel('Epoch')\n        plt.legend(['Train', 'Test'], loc='upper left')\n        plt.show()\n\n        # Plot training & validation loss values\n        plt.plot(history.history['loss'])\n        plt.plot(history.history['val_loss'])\n        plt.title('Model loss')\n        plt.ylabel('Loss')\n        plt.xlabel('Epoch')\n        plt.legend(['Train', 'Test'], loc='upper left')\n        plt.show()\n    \n\ndef evaluate_model(model,name,validation_data=(x_valid,y_valid)):\n    model.load_weights('/kaggle/working/weights.best.'+name+'.hdf5')\n    scores[name] = dict(zip (model.metrics_names,model.evaluate(validation_data[0],validation_data[1])))\n    f1 = scores[name]['f1_m']*100\n    print('Model: '+name+' test F1: %.4f%%' % f1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CNN Approach - Data Driven Approach"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Conv1D, MaxPooling1D,GlobalAveragePooling1D\nfrom keras.layers import Dropout, Flatten, Dense\nfrom keras.models import Sequential\n\nmodel_cnn = Sequential()\nmodel_cnn.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\nmodel_cnn.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\nmodel_cnn.add(Dropout(0.5))\nmodel_cnn.add(MaxPooling1D(pool_size=2))\nmodel_cnn.add(Flatten())\nmodel_cnn.add(Dense(100, activation='relu'))\nmodel_cnn.add(Dense(n_outputs, activation='softmax'))\n\nmodel_cnn.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_model(model_cnn,'cnn')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_model(model_cnn,'cnn')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RNN - An LSTM Approach"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import LSTM\nfrom keras.models import Sequential\n\nmodel_lstm = Sequential()\nmodel_lstm.add(LSTM(100, input_shape=(n_timesteps,n_features)))\nmodel_lstm.add(Dropout(0.5))\nmodel_lstm.add(Dense(100, activation='relu'))\nmodel_lstm.add(Dense(n_outputs, activation='softmax'))\nmodel_lstm.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_model(model_lstm,'lstm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_model(model_lstm,'lstm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Combining CNN and LSTM Architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D,GlobalAveragePooling1D\nfrom keras.layers import Dropout, Flatten, Dense,TimeDistributed,LSTM\n\n\nn_steps, n_length = 4, 25\nmodel_cnn_lstm = Sequential()\nmodel_cnn_lstm.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'), input_shape=(n_steps,n_length,n_features)))\nmodel_cnn_lstm.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\nmodel_cnn_lstm.add(TimeDistributed(Dropout(0.5)))\nmodel_cnn_lstm.add(TimeDistributed(MaxPooling1D(pool_size=2)))\nmodel_cnn_lstm.add(TimeDistributed(Flatten()))\nmodel_cnn_lstm.add(LSTM(100))\nmodel_cnn_lstm.add(Dropout(0.5))\nmodel_cnn_lstm.add(Dense(100, activation='relu'))\nmodel_cnn_lstm.add(Dense(n_outputs, activation='softmax'))\nmodel_cnn_lstm.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_cnn_lstm = x_train.reshape((x_train.shape[0], n_steps, n_length, n_features))\nx_valid_cnn_lstm = x_valid.reshape((x_valid.shape[0], n_steps, n_length, n_features))\nx_test_cnn_lstm = x_test.reshape((x_test.shape[0], n_steps, n_length, n_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_model(model_cnn_lstm,'cnn_lstm',training_data=(x_train_cnn_lstm,y_train),validation_data=(x_valid_cnn_lstm,y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_model(model_cnn_lstm,'cnn_lstm',validation_data=(x_valid_cnn_lstm,y_valid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint('%-15s'%('Model'),end='')\nfor key,value in scores['cnn'].items():\n    print ('%-15s'%(key),end='')\nprint('\\r\\n---------------------------------------------------------------------------------------------------',end='')\nfor key, value in scores.items():\n    print( '\\r\\n%-15s' %(key),end='')\n    for k, val in value.items():\n        print('%-15f'%(val),end='')\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Optimizing final Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D,GlobalAveragePooling1D\nfrom keras.layers import Dropout, Flatten, Dense,TimeDistributed,LSTM\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.wrappers.scikit_learn import KerasClassifier\nn_steps, n_length = 4, 25\n\ndef create_model(n_dense=100,n_filters=64,kernel_size=3,pool_size=2,n_steps=4,n_length=25):\n    \n    model = Sequential()\n    model.add(TimeDistributed(Conv1D(filters=n_filters, kernel_size=kernel_size, activation='relu'), input_shape=(n_steps,n_length,n_features)))\n    model.add(TimeDistributed(Conv1D(filters=n_filters, kernel_size=kernel_size, activation='relu')))\n    model.add(TimeDistributed(Dropout(0.5)))\n    model.add(TimeDistributed(MaxPooling1D(pool_size=pool_size)))\n    model.add(TimeDistributed(Flatten()))\n    model.add(LSTM(n_dense))\n    model.add(Dropout(0.5))\n    model.add(Dense(n_dense, activation='relu'))\n    model.add(Dense(n_outputs, activation='softmax'))\n    model.summary()\n    model.compile(loss='categorical_crossentropy', optimizer='adam', weighted_metrics=[f1_m,'acc',precision_m, recall_m])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define the grid search parameters\nparam_grid = {\n    #'n_dense': [100],\n    'n_filters': [32, 64],\n    'kernel_size': [3, 6,8],\n    'pool_size': [4,8],\n    #'n_dense':[25,100]\n    #'optimizer':['RMSprop', 'Adam', 'Adamax', 'sgd'],\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_cnn_lstm = x_train.reshape((x_train.shape[0], n_steps, n_length, n_features))\nx_valid_cnn_lstm = x_valid.reshape((x_valid.shape[0], n_steps, n_length, n_features))\nx_test_cnn_lstm = x_test.reshape((x_test.shape[0], n_steps, n_length, n_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fix random seed for reproducibility\nseed = 7\nnp.random.seed(seed)\nkears_estimator = KerasClassifier(build_fn=create_model,epochs=20, verbose=1)\n\ngrid = GridSearchCV(estimator=kears_estimator,   \n                    verbose=1,\n                    n_jobs=1,\n                    return_train_score=True,\n                    param_grid=param_grid,)\n\ngrid_result = grid.fit(x_train_cnn_lstm[0:1000],y_train[0:1000],validation_data=(x_valid_cnn_lstm,y_valid)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_df = pd.DataFrame(grid_result.cv_results_).sort_values(by='rank_test_score')\nresult_df =scores_df[['mean_test_score','std_test_score','param_kernel_size','param_n_filters','param_pool_size']]\nresult_df.columns = ['accurancy','loss','kernel_size','n_filter','pool_size']\nresult_df.head(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print('%10s %s %10s %10s %10s' % ('kernel_size','n_filters','pool_size'))\n    print('%10f %10f %10f %10f %10f '(mean,stddef,param['kernel_size'],param['pool_size']))\n    #print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create Final Model by setting improved Parameter Setup found by Gridsearch"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model = create_model(kernel_size=8,n_filters=64,pool_size=4)\ntrain_model(final_model,'final',training_data=(x_train_cnn_lstm,y_train),validation_data=(x_valid_cnn_lstm,y_valid),epochs=20,plot=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Evaluate Final Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_model(final_model,'final',validation_data=(x_valid_cnn_lstm,y_valid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Evaluate Final Model on the Testing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_model(final_model,'final',validation_data=(x_test_cnn_lstm,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores['final']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualize Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predicted = final_model.predict(x_test_cnn_lstm)\n\nx_visualize=[]\ny_vis_predicted=[]\ny_true = []\n\nclasses_tp = [0,1,2,3]#+[0,1,2,3]+[0,1,2,3]\nclasses_tn =  [0,1,2,3]\n\nfor el in enumerate(y_predicted):\n    i = el[0]\n    if (np.argmax(y_test[i]) != np.argmax(y_predicted[i]) and np.argmax(y_test[i]) in classes_tn):\n        y_vis_predicted.append(np.argmax(y_predicted[i]))\n        y_true.append(np.argmax(y_test[i]))\n        x_visualize.append(x_test[i])\n        classes_tn.remove(np.argmax(y_test[i]))\n    if (np.argmax(y_test[i]) == np.argmax(y_predicted[i]) and np.argmax(y_test[i]) in classes_tp):\n        y_vis_predicted.append(np.argmax(y_predicted[i]))\n        y_true.append(np.argmax(y_test[i]))\n        x_visualize.append(x_test[i])\n        classes_tp.remove(np.argmax(y_test[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pylab as pl\nimport matplotlib.gridspec as gridspec\n\ncolors =['red','blue', 'green', 'gray']\nlabels =ACT_LABELS[0:4] + ['false classified: '+l for l in ACT_LABELS[0:4]]\n# Create 2x2 sub plots\ngs = gridspec.GridSpec(3, 1,hspace=0.4)\n\npl.figure(figsize=(20,15))\n\nax1 = pl.subplot(gs[0, 0],title='userAcceleration.x',ylabel='value',xlabel='timestep') # row 0, col 0\nax2 = pl.subplot(gs[1, 0],title='userAcceleration.y',ylabel='value',xlabel='timestep') # row 0, col 1\nax3 = pl.subplot(gs[2, 0],title='userAcceleration.z',ylabel='value',xlabel='timestep') # row 1, span all columns\n\n\nfor ts in enumerate(x_visualize):\n    x,y,z = zip(*ts[1])\n    linestyle = '-'\n    if y_true[ts[0]] != y_vis_predicted[ts[0]]:\n        linestyle = '--'\n        label = 4+y_true[ts[0]]\n    else:\n        label = y_true[ts[0]]\n    ax1.plot(x,color=colors[y_true[ts[0]]],linestyle=linestyle,label=labels[label])\n    ax2.plot(y,color=colors[y_true[ts[0]]],linestyle=linestyle,label=labels[label])\n    ax3.plot(y,color=colors[y_true[ts[0]]],linestyle=linestyle,label=labels[label])\nax1.legend(bbox_to_anchor=(1.05, 0), loc='lower left', borderaxespad=0. )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> # Bayesian Optimization"},{"metadata":{"trusted":true},"cell_type":"code","source":"#imports we know we'll need\nimport skopt\n# !pip install scikit-optimize if  necessary\nfrom skopt import gbrt_minimize, gp_minimize\nfrom skopt.utils import use_named_args\nfrom skopt.space import Real, Categorical, Integer  \n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport tensorflow\nfrom tensorflow.python.keras import backend as K","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@use_named_args(dimensions=dimensions)\ndef fitness(n_dense,n_filters,kernel_size,pool_size):\n    n_steps=4\n    n_length=25\n    model = create_model(n_dense=n_dense,n_filters=n_filters,kernel_size=kernel_size,pool_size=pool_size,n_steps=n_steps,n_length=n_length)\n    \n    x_train_cnn_lstm = x_train.reshape((x_train.shape[0], n_steps, n_length, n_features))\n    x_valid_cnn_lstm = x_valid.reshape((x_valid.shape[0], n_steps, n_length, n_features))\n    #named blackbox becuase it represents the structure\n    blackbox = model.fit(x=x_train_cnn_lstm,\n                        y=y_train,\n                        epochs=3,\n                        batch_size=3,\n                        validation_data=(x_valid_cnn_lstm,y_valid),\n                        )\n    #return the validation accuracy for the last epoch.\n    accuracy = blackbox.history['val_acc'][-1]\n\n    # Print the classification accuracy.\n    print()\n    print(\"Accuracy: {0:.2%}\".format(accuracy))\n    print()\n\n\n    # Delete the Keras model with these hyper-parameters from memory.\n    del model\n    \n    # Clear the Keras session, otherwise it will keep adding new\n    # models to the same TensorFlow graph each time we create\n    # a model with a different set of hyper-parameters.\n    K.clear_session()\n    tensorflow.reset_default_graph()\n    \n    # the optimizer aims for the lowest score, so we return our negative accuracy\n    return -accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_dense = Integer(low=1, high=200,  name='n_dense')\nn_filters = Integer(low=1, high=128, name='n_filters')\nkernel_size = Integer(low=1, high=16, name='kernel_size')\npool_size = Integer(low=1, high=32, name='pool_size')\n\n\ndimensions = [n_dense,\n              n_filters,\n              kernel_size,\n              pool_size,\n             ]\ndefault_parameters = [100,64,3,4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kernel_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gp_result = gp_minimize(func=fitness,\n                        dimensions=dimensions,\n                            n_calls=12,\n                            noise= 0.01,\n                            n_jobs=-1,\n                            kappa = 5,\n                            x0=default_parameters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\nfrom hyperopt import Trials, STATUS_OK, tpe\nfrom keras.datasets import mnist\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.models import Sequential\nfrom keras.utils import np_utils\n#!pip install hyperas\nfrom hyperas import optim\nfrom hyperas.distributions import choice, uniform","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data():\n    return x_train,y_train,x_valid,y_valid\n\ndef new_model(x_train,y_train,x_valid,y_valid):\n    n_steps=4\n    n_length=25\n    model = create_model(n_dense={{choice([50, 100, 200])}},n_filters={{choice([16,32, 64,128])}},kernel_size={{choice([3,6, 8,16])}},pool_size={{choice([2,4, 8,16])}},n_steps=n_steps,n_length=n_length)\n    \n    x_train_cnn_lstm = x_train.reshape((x_train.shape[0], n_steps, n_length, n_features))\n    x_valid_cnn_lstm = x_valid.reshape((x_valid.shape[0], n_steps, n_length, n_features))\n    #named blackbox becuase it represents the structure\n    blackbox = model.fit(x=x_train_cnn_lstm,\n                        y=y_train,\n                        epochs=20,\n                        batch_size=20,\n                        validation_data=(x_valid_cnn_lstm,y_valid),\n                        )\n    #get the highest validation accuracy of the training epochs\n    validation_acc = np.amax(result.history['val_acc']) \n    print('Best validation acc of epoch:', validation_acc)\n    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    best_run, best_model = optim.minimize(model=new_model,\n                                          data=data,\n                                          algo=tpe.suggest,\n                                          max_evals=5,\n                                          notebook_name='kernel762afcce6b',\n                                          trials=Trials())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}