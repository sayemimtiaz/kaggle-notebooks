{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Our Data Preprocessing Classes","metadata":{"_uuid":"23862f72-d5fb-4ef7-803f-0c846d750c5f","_cell_guid":"c36b6eeb-d0d4-4cde-8605-65bda825b51a","trusted":true}},{"cell_type":"code","source":"%%writefile feature_extractor.py\n\nimport librosa\nimport scipy\n\n\nclass FeatureExtractor:\n    def __init__(self, audio, *, windowLength, overlap, sample_rate):\n        self.audio = audio\n        self.ffT_length = windowLength\n        self.window_length = windowLength\n        self.overlap = overlap\n        self.sample_rate = sample_rate\n        self.window = scipy.signal.hamming(self.window_length, sym=False)\n\n    def get_stft_spectrogram(self):\n        return librosa.stft(self.audio, n_fft=self.ffT_length, win_length=self.window_length, hop_length=self.overlap,\n                            window=self.window, center=True)\n\n    def get_audio_from_stft_spectrogram(self, stft_features):\n        return librosa.istft(stft_features, win_length=self.window_length, hop_length=self.overlap,\n                             window=self.window, center=True)\n\n    def get_mel_spectrogram(self):\n        return librosa.feature.melspectrogram(self.audio, sr=self.sample_rate, power=2.0, pad_mode='reflect',\n                                              n_fft=self.ffT_length, hop_length=self.overlap, center=True)\n\n    def get_audio_from_mel_spectrogram(self, M):\n        return librosa.feature.inverse.mel_to_audio(M, sr=self.sample_rate, n_fft=self.ffT_length,\n                                                    hop_length=self.overlap,\n                                                    win_length=self.window_length, window=self.window,\n                                                    center=True, pad_mode='reflect', power=2.0, n_iter=32, length=None)","metadata":{"_uuid":"2421e626-1585-4df0-9e4c-81dc3601a08a","_cell_guid":"ef652920-3387-4262-8655-20b3ff30b51d","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-09-23T22:47:17.999634Z","iopub.execute_input":"2021-09-23T22:47:18.000272Z","iopub.status.idle":"2021-09-23T22:47:18.006955Z","shell.execute_reply.started":"2021-09-23T22:47:18.000237Z","shell.execute_reply":"2021-09-23T22:47:18.006133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile dataset.py\n\nimport librosa\nimport numpy as np\nimport math\nfrom feature_extractor import FeatureExtractor\nfrom utils import prepare_input_features\nimport multiprocessing\nimport os\nfrom utils import get_tf_feature, read_audio\nimport tensorflow as tf\nfrom sklearn.preprocessing import StandardScaler\n\n\nnp.random.seed(999)\ntf.random.set_seed(999)\n\n\nclass Dataset:\n    def __init__(self, clean_filenames, noise_filenames, **config):\n        self.clean_filenames = clean_filenames\n        self.noise_filenames = noise_filenames\n        self.sample_rate = config['fs']\n        self.overlap = config['overlap']\n        self.window_length = config['windowLength']\n        self.audio_max_duration = config['audio_max_duration']\n\n    def _sample_noise_filename(self):\n        return np.random.choice(self.noise_filenames)\n\n    def _remove_silent_frames(self, audio):\n        trimed_audio = []\n        indices = librosa.effects.split(audio, hop_length=self.overlap, top_db=20)\n\n        for index in indices:\n            trimed_audio.extend(audio[index[0]: index[1]])\n        return np.array(trimed_audio)\n\n    def _phase_aware_scaling(self, clean_spectral_magnitude, clean_phase, noise_phase):\n        assert clean_phase.shape == noise_phase.shape, \"Shapes must match.\"\n        return clean_spectral_magnitude * np.cos(clean_phase - noise_phase)\n\n    def get_noisy_audio(self, *, filename):\n        return read_audio(filename, self.sample_rate)\n\n    def _audio_random_crop(self, audio, duration):\n        audio_duration_secs = librosa.core.get_duration(audio, self.sample_rate)\n\n        ## duration: length of the cropped audio in seconds\n        if duration >= audio_duration_secs:\n            # print(\"Passed duration greater than audio duration of: \", audio_duration_secs)\n            return audio\n\n        audio_duration_ms = math.floor(audio_duration_secs * self.sample_rate)\n        duration_ms = math.floor(duration * self.sample_rate)\n        idx = np.random.randint(0, audio_duration_ms - duration_ms)\n        return audio[idx: idx + duration_ms]\n\n    def _add_noise_to_clean_audio(self, clean_audio, noise_signal):\n        if len(clean_audio) >= len(noise_signal):\n            # print(\"The noisy signal is smaller than the clean audio input. Duplicating the noise.\")\n            while len(clean_audio) >= len(noise_signal):\n                noise_signal = np.append(noise_signal, noise_signal)\n\n        ## Extract a noise segment from a random location in the noise file\n        ind = np.random.randint(0, noise_signal.size - clean_audio.size)\n\n        noiseSegment = noise_signal[ind: ind + clean_audio.size]\n\n        speech_power = np.sum(clean_audio ** 2)\n        noise_power = np.sum(noiseSegment ** 2)\n        noisyAudio = clean_audio + np.sqrt(speech_power / noise_power) * noiseSegment\n        return noisyAudio\n\n    def parallel_audio_processing(self, clean_filename):\n\n        clean_audio, _ = read_audio(clean_filename, self.sample_rate)\n\n        # remove silent frame from clean audio\n        clean_audio = self._remove_silent_frames(clean_audio)\n\n        noise_filename = self._sample_noise_filename()\n\n        # read the noise filename\n        noise_audio, sr = read_audio(noise_filename, self.sample_rate)\n\n        # remove silent frame from noise audio\n        noise_audio = self._remove_silent_frames(noise_audio)\n\n        # sample random fixed-sized snippets of audio\n        clean_audio = self._audio_random_crop(clean_audio, duration=self.audio_max_duration)\n\n        # add noise to input image\n        noiseInput = self._add_noise_to_clean_audio(clean_audio, noise_audio)\n\n        # extract stft features from noisy audio\n        noisy_input_fe = FeatureExtractor(noiseInput, windowLength=self.window_length, overlap=self.overlap,\n                                          sample_rate=self.sample_rate)\n        noise_spectrogram = noisy_input_fe.get_stft_spectrogram()\n\n        # Or get the phase angle (in radians)\n        # noisy_stft_magnitude, noisy_stft_phase = librosa.magphase(noisy_stft_features)\n        noise_phase = np.angle(noise_spectrogram)\n\n        # get the magnitude of the spectral\n        noise_magnitude = np.abs(noise_spectrogram)\n\n        # extract stft features from clean audio\n        clean_audio_fe = FeatureExtractor(clean_audio, windowLength=self.window_length, overlap=self.overlap,\n                                          sample_rate=self.sample_rate)\n        clean_spectrogram = clean_audio_fe.get_stft_spectrogram()\n        # clean_spectrogram = cleanAudioFE.get_mel_spectrogram()\n\n        # get the clean phase\n        clean_phase = np.angle(clean_spectrogram)\n\n        # get the clean spectral magnitude\n        clean_magnitude = np.abs(clean_spectrogram)\n        # clean_magnitude = 2 * clean_magnitude / np.sum(scipy.signal.hamming(self.window_length, sym=False))\n\n        clean_magnitude = self._phase_aware_scaling(clean_magnitude, clean_phase, noise_phase)\n\n        scaler = StandardScaler(copy=False, with_mean=True, with_std=True)\n        noise_magnitude = scaler.fit_transform(noise_magnitude)\n        clean_magnitude = scaler.transform(clean_magnitude)\n\n        return noise_magnitude, clean_magnitude, noise_phase\n\n    def create_tf_record(self, *, prefix, subset_size, parallel=True):\n        counter = 0\n        p = multiprocessing.Pool(multiprocessing.cpu_count())\n\n        for i in range(0, len(self.clean_filenames), subset_size):\n\n            tfrecord_filename = '/kaggle/working/records/' + prefix + '_' + str(counter) + '.tfrecords'\n\n            if os.path.isfile(tfrecord_filename):\n                print(f\"Skipping {tfrecord_filename}\")\n                counter += 1\n                continue\n\n            writer = tf.io.TFRecordWriter(tfrecord_filename)\n            clean_filenames_sublist = self.clean_filenames[i:i + subset_size]\n\n            print(f\"Processing files from: {i} to {i + subset_size}\")\n            if parallel:\n                out = p.map(self.parallel_audio_processing, clean_filenames_sublist)\n            else:\n                out = [self.parallel_audio_processing(filename) for filename in clean_filenames_sublist]\n\n            for o in out:\n                noise_stft_magnitude = o[0]\n                clean_stft_magnitude = o[1]\n                noise_stft_phase = o[2]\n\n                noise_stft_mag_features = prepare_input_features(noise_stft_magnitude, numSegments=8, numFeatures=129)\n\n                noise_stft_mag_features = np.transpose(noise_stft_mag_features, (2, 0, 1))\n                clean_stft_magnitude = np.transpose(clean_stft_magnitude, (1, 0))\n                noise_stft_phase = np.transpose(noise_stft_phase, (1, 0))\n\n                noise_stft_mag_features = np.expand_dims(noise_stft_mag_features, axis=3)\n                clean_stft_magnitude = np.expand_dims(clean_stft_magnitude, axis=2)\n\n                for x_, y_, p_ in zip(noise_stft_mag_features, clean_stft_magnitude, noise_stft_phase):\n                    y_ = np.expand_dims(y_, 2)\n                    example = get_tf_feature(x_, y_, p_)\n                    writer.write(example.SerializeToString())\n\n            counter += 1\n            writer.close()","metadata":{"_uuid":"2513b632-0a02-4ef2-b00e-60d0929067fa","_cell_guid":"c05eaefa-9f7c-401a-a955-bdf337e846c5","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-09-23T22:47:18.189785Z","iopub.execute_input":"2021-09-23T22:47:18.190373Z","iopub.status.idle":"2021-09-23T22:47:18.200641Z","shell.execute_reply.started":"2021-09-23T22:47:18.190333Z","shell.execute_reply":"2021-09-23T22:47:18.199854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile mozilla_commonvoice.py\n\nimport pandas as pd\nimport numpy as np\nimport os\n\n# np.random.seed(999)\n\nclass MozillaCommonVoiceDataset:\n\n    def __init__(self, basepath, *, val_dataset_size):\n        self.basepath = basepath\n        self.val_dataset_size = val_dataset_size\n\n    def get_common_voice_filenames(self, subfolder=\"train\", dataframe_name='train.tsv'):\n        full_file_path = os.path.join(self.basepath, subfolder, dataframe_name)\n        print(\"full path\", full_file_path)\n\n        mozilla_metadata = pd.read_csv(full_file_path, sep='\\t')\n        clean_files = mozilla_metadata['path'].values\n        np.random.shuffle(clean_files)\n        print(\"Total number of training examples:\", len(clean_files))\n        return clean_files\n\n    def get_train_val_filenames(self):\n        clean_files = self.get_common_voice_filenames(subfolder=\"train\", dataframe_name='train.tsv')\n\n        # resolve full path\n        clean_files = [os.path.join(self.basepath, 'train', 'clips', filename+\".wav\") for filename in clean_files]\n\n        clean_files = clean_files[:-self.val_dataset_size]\n        clean_val_files = clean_files[-self.val_dataset_size:]\n        print(\"# of Training clean files:\", len(clean_files))\n        print(\"# of  Validation clean files:\", len(clean_val_files))\n        return clean_files, clean_val_files\n\n\n    def get_test_filenames(self):\n        clean_files = self.get_common_voice_filenames(subfolder=\"test\", dataframe_name='test.tsv')\n\n        # resolve full path\n        clean_files = [os.path.join(self.basepath, 'test', 'clips', filename+\".wav\") for filename in clean_files]\n\n        print(\"# of Testing clean files:\", len(clean_files))\n#         print(\"Clean Test Files: \", clean_files)\n        return clean_files","metadata":{"_uuid":"aee205a6-c225-409d-8eb1-fbd5d476e33a","_cell_guid":"4a695899-cdb0-46e8-8b69-45805273f062","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-09-23T22:47:18.234818Z","iopub.execute_input":"2021-09-23T22:47:18.23533Z","iopub.status.idle":"2021-09-23T22:47:18.240885Z","shell.execute_reply.started":"2021-09-23T22:47:18.2353Z","shell.execute_reply":"2021-09-23T22:47:18.240133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile urban_sound_8k.py\n\nimport pandas as pd\nimport numpy as np\nimport os\n\nnp.random.seed(999)\n\n\nclass UrbanSound8K:\n    def __init__(self, basepath, *, val_dataset_size, class_ids=None):\n        self.basepath = basepath\n        self.val_dataset_size = val_dataset_size\n        self.class_ids = class_ids\n\n    def _get_urban_sound_8K_filenames(self):\n        urbansound_metadata = pd.read_csv(os.path.join(self.basepath, 'UrbanSound8K.csv'))\n\n        # shuffle the dataframe\n        urbansound_metadata.reindex(np.random.permutation(urbansound_metadata.index))\n\n        return urbansound_metadata\n\n    def _get_filenames_by_class_id(self, metadata):\n\n        if self.class_ids is None:\n            self.class_ids = np.unique(metadata['classID'].values)\n            print(\"Number of classes:\", self.class_ids)\n\n        all_files = []\n        file_counter = 0\n        for c in self.class_ids:\n            per_class_files = metadata[metadata['classID'] == c][['slice_file_name', 'fold']].values\n            per_class_files = [os.path.join(self.basepath, 'fold' + str(file[1]), file[0]) for file in\n                               per_class_files]\n            print(\"Class c:\", str(c), 'has:', len(per_class_files), 'files')\n            file_counter += len(per_class_files)\n            all_files.extend(per_class_files)\n\n        assert len(all_files) == file_counter\n        return all_files\n\n    def get_train_val_filenames(self):\n        urbansound_metadata = self._get_urban_sound_8K_filenames()\n\n        # folds from 0 to 9 are used for training\n        urbansound_train = urbansound_metadata[urbansound_metadata.fold != 10]\n\n        urbansound_train_filenames = self._get_filenames_by_class_id(urbansound_train)\n        np.random.shuffle(urbansound_train_filenames)\n\n        # separate noise files for train/validation\n        urbansound_val = urbansound_train_filenames[-self.val_dataset_size:]\n        urbansound_train = urbansound_train_filenames[:-self.val_dataset_size]\n        print(\"Noise training:\", len(urbansound_train))\n        print(\"Noise validation:\", len(urbansound_val))\n\n        return urbansound_train, urbansound_val\n\n    def get_test_filenames(self):\n        urbansound_metadata = self._get_urban_sound_8K_filenames()\n\n        # fold 10 is used for testing only\n        urbansound_train = urbansound_metadata[urbansound_metadata.fold == 10]\n\n        urbansound_test_filenames = self._get_filenames_by_class_id(urbansound_train)\n        np.random.shuffle(urbansound_test_filenames)\n\n        print(\"# of Noise testing files:\", len(urbansound_test_filenames))\n        return urbansound_test_filenames","metadata":{"_uuid":"2eff8f80-40d5-4d73-b376-85dc85e45bdd","_cell_guid":"537b42b9-6936-4389-9cfe-77b983002265","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-09-23T22:47:18.374266Z","iopub.execute_input":"2021-09-23T22:47:18.37449Z","iopub.status.idle":"2021-09-23T22:47:18.380776Z","shell.execute_reply.started":"2021-09-23T22:47:18.374467Z","shell.execute_reply":"2021-09-23T22:47:18.379963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Using our Data Preprocessing library","metadata":{"_uuid":"917be118-ff50-42e4-b09b-bebbe1df1cdf","_cell_guid":"16abf0e5-8807-4e0d-9a1e-f1b58c01af27","trusted":true}},{"cell_type":"code","source":"%%writefile create_dataset.py\n\nfrom mozilla_commonvoice import MozillaCommonVoiceDataset\nfrom urban_sound_8k import UrbanSound8K\n# from urban_sound_8K import UrbanSound8K\nfrom dataset import Dataset\nimport warnings\n\nwarnings.filterwarnings(action='ignore')\n\nmozilla_basepath = \"../input/commonvoice2/commonvoice\"\nurbansound_basepath = \"../input/urbansound8k\"\n\nmcv = MozillaCommonVoiceDataset(mozilla_basepath, val_dataset_size=1000)\nclean_train_filenames, clean_val_filenames = mcv.get_train_val_filenames()\n\nus8K = UrbanSound8K(urbansound_basepath, val_dataset_size=200)\nnoise_train_filenames, noise_val_filenames = us8K.get_train_val_filenames()\nprint(\"clean train:\", clean_train_filenames[:10])\nprint(\"noise train:\", noise_train_filenames[:10])\n\nwindowLength = 256\nconfig = {'windowLength': windowLength,\n          'overlap': round(0.25 * windowLength),\n          'fs': 8000,\n          'audio_max_duration': 0.8}\n\nval_dataset = Dataset(clean_val_filenames, noise_val_filenames, **config)\nval_dataset.create_tf_record(prefix='val', subset_size=200)\n\ntrain_dataset = Dataset(clean_train_filenames, noise_train_filenames, **config)\ntrain_dataset.create_tf_record(prefix='train', subset_size=400)\n\n## Create Test Set\nclean_test_filenames = mcv.get_test_filenames()\n\nnoise_test_filenames = us8K.get_test_filenames()\nnoise_test_filenames = noise_test_filenames\n\ntest_dataset = Dataset(clean_test_filenames, noise_test_filenames, **config)\ntest_dataset.create_tf_record(prefix='test', subset_size=1, parallel=False)","metadata":{"_uuid":"e98999c1-bef8-45fa-9c67-19aa6a178f75","_cell_guid":"81239a61-35ca-40e9-b12f-4cc48135a3e4","execution":{"iopub.status.busy":"2021-09-23T22:47:18.455958Z","iopub.execute_input":"2021-09-23T22:47:18.456416Z","iopub.status.idle":"2021-09-23T22:47:18.461867Z","shell.execute_reply.started":"2021-09-23T22:47:18.456387Z","shell.execute_reply":"2021-09-23T22:47:18.460965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %run create_dataset.py","metadata":{"_uuid":"cdf99037-a624-4bb2-a340-7e9a0de3aad8","_cell_guid":"173e30b9-255c-438b-bc48-c124374b4c8a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-23T22:47:18.471828Z","iopub.execute_input":"2021-09-23T22:47:18.472291Z","iopub.status.idle":"2021-09-23T22:47:18.476016Z","shell.execute_reply.started":"2021-09-23T22:47:18.472261Z","shell.execute_reply":"2021-09-23T22:47:18.475109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile utils.py\n\nimport numpy as np\nimport pickle\nimport librosa\n# import sounddevice as sd\nfrom pydub import AudioSegment\nimport IPython\nimport tensorflow as tf\n\n\ndef inverse_stft_transform(stft_features, window_length, overlap):\n    return librosa.istft(stft_features, win_length=window_length, hop_length=overlap)\n\n\ndef revert_features_to_audio(features, phase, window_length, overlap, cleanMean=None, cleanStd=None):\n    # scale the outpus back to the original range\n    if cleanMean and cleanStd:\n        features = cleanStd * features + cleanMean\n\n    phase = np.transpose(phase, (1, 0))\n    features = np.squeeze(features)\n    features = features * np.exp(1j * phase)  # that fixes the abs() ope previously done\n\n    features = np.transpose(features, (1, 0))\n    return inverse_stft_transform(features, window_length=window_length, overlap=overlap)\n\n\ndef play(audio, sample_rate):\n    # ipd.display(ipd.Audio(data=audio, rate=sample_rate))  # load a local WAV file\n    IPython.display.Audio(data=audio, rate=sample_rate)\n#     sd.play(audio, sample_rate, blocking=True)\n\n\ndef add_noise_to_clean_audio(clean_audio, noise_signal):\n    if len(clean_audio) >= len(noise_signal):\n        # print(\"The noisy signal is smaller than the clean audio input. Duplicating the noise.\")\n        while len(clean_audio) >= len(noise_signal):\n            noise_signal = np.append(noise_signal, noise_signal)\n\n    ## Extract a noise segment from a random location in the noise file\n    ind = np.random.randint(0, noise_signal.size - clean_audio.size)\n\n    noiseSegment = noise_signal[ind: ind + clean_audio.size]\n\n    speech_power = np.sum(clean_audio ** 2)\n    noise_power = np.sum(noiseSegment ** 2)\n    noisyAudio = clean_audio + np.sqrt(speech_power / noise_power) * noiseSegment\n    return noisyAudio\n\ndef read_audio(filepath, sample_rate, normalize=True):\n    audio, sr = librosa.load(filepath, sr=sample_rate)\n    if normalize is True:\n        div_fac = 1 / np.max(np.abs(audio)) / 3.0\n        audio = audio * div_fac\n        # audio = librosa.util.normalize(audio)\n    return audio, sr\n\n\ndef prepare_input_features(stft_features, numSegments, numFeatures):\n    noisySTFT = np.concatenate([stft_features[:, 0:numSegments - 1], stft_features], axis=1)\n    stftSegments = np.zeros((numFeatures, numSegments, noisySTFT.shape[1] - numSegments + 1))\n\n    for index in range(noisySTFT.shape[1] - numSegments + 1):\n        stftSegments[:, :, index] = noisySTFT[:, index:index + numSegments]\n    return stftSegments\n\n\ndef get_input_features(predictorsList):\n    predictors = []\n    for noisy_stft_mag_features in predictorsList:\n        # For CNN, the input feature consisted of 8 consecutive noisy\n        # STFT magnitude vectors of size: 129 Ã— 8,\n        # TODO: duration: 100ms\n        inputFeatures = prepare_input_features(noisy_stft_mag_features)\n        # print(\"inputFeatures.shape\", inputFeatures.shape)\n        predictors.append(inputFeatures)\n\n    return predictors\n\n\ndef _bytes_feature(value):\n    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n    if isinstance(value, type(tf.constant(0))):\n        value = value.numpy()  # BytesList won't unpack a string from an EagerTensor.\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef _float_feature(value):\n    \"\"\"Returns a float_list from a float / double.\"\"\"\n    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n\n\ndef _int64_feature(value):\n    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef get_tf_feature(noise_stft_mag_features, clean_stft_magnitude, noise_stft_phase):\n    noise_stft_mag_features = noise_stft_mag_features.astype(np.float32).tostring()\n    clean_stft_magnitude = clean_stft_magnitude.astype(np.float32).tostring()\n    noise_stft_phase = noise_stft_phase.astype(np.float32).tostring()\n\n    example = tf.train.Example(features=tf.train.Features(feature={\n        'noise_stft_phase': _bytes_feature(noise_stft_phase),\n        'noise_stft_mag_features': _bytes_feature(noise_stft_mag_features),\n        'clean_stft_magnitude': _bytes_feature(clean_stft_magnitude)}))\n    return example","metadata":{"_uuid":"f69810c8-a985-47fc-aa67-83150cd35833","_cell_guid":"76866e18-6424-425b-9d2a-ad86253cb6b3","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-09-23T22:47:18.587016Z","iopub.execute_input":"2021-09-23T22:47:18.587217Z","iopub.status.idle":"2021-09-23T22:47:18.594338Z","shell.execute_reply.started":"2021-09-23T22:47:18.587194Z","shell.execute_reply":"2021-09-23T22:47:18.593535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile test_tf_record.py\n\nimport tensorflow as tf\nimport numpy as np\nfrom utils import play\nfrom data_processing.feature_extractor import FeatureExtractor\n\ntrain_tfrecords_filenames = '../kaggle/working/records/test_0.tfrecords'\n\ndef tf_record_parser(record):\n    keys_to_features = {\n        \"noise_stft_phase\": tf.io.FixedLenFeature((), tf.string, default_value=\"\"),\n        'noise_stft_mag_features': tf.io.FixedLenFeature([], tf.string),\n        \"clean_stft_magnitude\": tf.io.FixedLenFeature((), tf.string)\n    }\n\n    features = tf.io.parse_single_example(record, keys_to_features)\n\n    noise_stft_mag_features = tf.io.decode_raw(features['noise_stft_mag_features'], tf.float32)\n    clean_stft_magnitude = tf.io.decode_raw(features['clean_stft_magnitude'], tf.float32)\n    noise_stft_phase = tf.io.decode_raw(features['noise_stft_phase'], tf.float32)\n\n    n_features = 129\n    # reshape input and annotation images\n    noise_stft_mag_features = tf.reshape(noise_stft_mag_features, (n_features, 8, 1), name=\"noise_stft_mag_features\")\n    clean_stft_magnitude = tf.reshape(clean_stft_magnitude, (n_features, 1, 1), name=\"clean_stft_magnitude\")\n    noise_stft_phase = tf.reshape(noise_stft_phase, (n_features,), name=\"noise_stft_phase\")\n\n    return noise_stft_mag_features, clean_stft_magnitude, noise_stft_phase\n\ntrain_dataset = tf.data.TFRecordDataset([train_tfrecords_filenames])\ntrain_dataset = train_dataset.map(tf_record_parser)\ntrain_dataset = train_dataset.repeat(1)\ntrain_dataset = train_dataset.batch(1000)\ntrain_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n\nwindow_length=256\noverlap=64\nsr = 16000\n\nfeature_extractor = FeatureExtractor(None, windowLength=window_length, overlap=overlap, sample_rate=sr)\n\n\ndef revert_features_to_audio(features, phase, cleanMean=None, cleanStd=None):\n    # scale the outpus back to the original range\n    if cleanMean and cleanStd:\n        features = cleanStd * features + cleanMean\n\n    phase = np.transpose(phase, (1, 0))\n    features = np.squeeze(features)\n\n    # features = librosa.db_to_amplitude(features)\n    # features = librosa.db_to_power(features)\n    features = features * np.exp(1j * phase)  # that fixes the abs() ope previously done\n\n    features = np.transpose(features, (1, 0))\n    return feature_extractor.get_audio_from_stft_spectrogram(features)\n\nfor pred, target, phase in train_dataset:\n\n    # pred = np.transpose(pred, (1, 0))\n    # target = np.transpose(target, (1, 0))\n    print(\"Min:\", np.min(pred), \"Max:\", np.max(pred))\n    print(\"Min:\", np.min(target), \"Max:\", np.max(target))\n    print(\"Min:\", np.min(phase), \"Max:\", np.max(phase))\n\n    phase = np.transpose(phase.numpy(), (1, 0))\n    print(\"Pred:\", pred.shape)\n    print(\"Phase:\", phase.shape)\n    print(\"target:\", target.shape)\n    audio = revert_features_to_audio(target.numpy(), phase)\n    break\n\nprint(\"Audio length:\", len(audio))\nplay(audio, sample_rate=16000)\n\n# Min: -0.5883574 Max: 10.728247\n# Min: -4.8901606 Max: 7.3664904\n# Min: -3.1415927 Max: 3.1415927\n# Phase: (129, 201)\n# target: (201, 129, 1, 1)\n# Audio length: 12800","metadata":{"_uuid":"0756df33-2631-4fd9-8b67-fe6f957c0f24","_cell_guid":"ee19985b-822b-40a3-81e1-a7db777d9e88","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-09-23T22:47:18.627142Z","iopub.execute_input":"2021-09-23T22:47:18.627324Z","iopub.status.idle":"2021-09-23T22:47:18.633126Z","shell.execute_reply.started":"2021-09-23T22:47:18.627302Z","shell.execute_reply":"2021-09-23T22:47:18.632423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Keras Rectified Adam Optimizer","metadata":{"_uuid":"76b973cc-cc5d-4544-a5bf-adbf4b8cbf69","_cell_guid":"2b7a9b60-da91-48e9-87da-d2f5ee6fd24e","trusted":true}},{"cell_type":"code","source":"!pip install keras-rectified-adam","metadata":{"_uuid":"16d6024a-273d-4272-9892-3c80e0191f9e","_cell_guid":"3fd36b8a-6274-4cfa-9f16-955608fe9f30","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-23T22:47:18.641654Z","iopub.execute_input":"2021-09-23T22:47:18.64203Z","iopub.status.idle":"2021-09-23T22:47:29.63699Z","shell.execute_reply.started":"2021-09-23T22:47:18.641997Z","shell.execute_reply":"2021-09-23T22:47:29.636156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ['TF_KERAS'] = '1'\n!echo $TF_KERAS","metadata":{"_uuid":"b9c21f68-d4a9-49ba-b22b-3e0f90987663","_cell_guid":"da1a0fee-54cb-4673-b9ca-5d5dfd42a9c9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-23T22:47:29.640673Z","iopub.execute_input":"2021-09-23T22:47:29.640904Z","iopub.status.idle":"2021-09-23T22:47:30.316793Z","shell.execute_reply.started":"2021-09-23T22:47:29.640876Z","shell.execute_reply":"2021-09-23T22:47:30.315916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %tensorflow_version 2.x\nimport tensorflow as tf\ndevice_name = tf.test.gpu_device_name()\nif device_name != '/device:GPU:0':\n  raise SystemError('GPU device not found')\nprint('Found GPU at: {}'.format(device_name))","metadata":{"_uuid":"52a43fd5-c4e9-48b5-838f-8f38ca1c0164","_cell_guid":"fa2459c9-ddda-43bd-be8c-e91803a117d6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-23T22:47:30.318418Z","iopub.execute_input":"2021-09-23T22:47:30.31894Z","iopub.status.idle":"2021-09-23T22:47:36.514629Z","shell.execute_reply.started":"2021-09-23T22:47:30.318899Z","shell.execute_reply":"2021-09-23T22:47:36.511689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Import Libraries","metadata":{"_uuid":"f5c65a12-a78d-4e22-9912-dd9195a400eb","_cell_guid":"c01df142-854b-4b67-978b-90482c7d97f3","trusted":true}},{"cell_type":"code","source":"import librosa\nimport pandas as pd\nimport os\nimport datetime\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nimport IPython.display as ipd\nimport librosa.display\nimport scipy\nimport glob\nimport numpy as np\nimport math\nimport warnings\nimport pickle\nfrom sklearn.utils import shuffle\n\n# Load the TensorBoard notebook extension.\n%load_ext tensorboard","metadata":{"_uuid":"1834cbbf-2d3a-4700-b1d3-94b113ffa462","_cell_guid":"a1b5c854-e653-4e48-890e-21cf492014f3","execution":{"iopub.status.busy":"2021-09-23T22:47:36.516006Z","iopub.execute_input":"2021-09-23T22:47:36.516274Z","iopub.status.idle":"2021-09-23T22:47:38.206943Z","shell.execute_reply.started":"2021-09-23T22:47:36.516237Z","shell.execute_reply":"2021-09-23T22:47:38.206081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_radam import RAdam","metadata":{"_uuid":"fe89641a-91cb-4fe2-b761-fbdb86454840","_cell_guid":"2a024c8b-3458-405f-823d-87ffccdb0ab3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-23T22:47:38.209529Z","iopub.execute_input":"2021-09-23T22:47:38.209842Z","iopub.status.idle":"2021-09-23T22:47:38.214689Z","shell.execute_reply.started":"2021-09-23T22:47:38.209805Z","shell.execute_reply":"2021-09-23T22:47:38.214023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.python.client import device_lib\ndevice_lib.list_local_devices()","metadata":{"_uuid":"d2105d2b-3d8d-4fb9-ae69-2f62af022935","_cell_guid":"d0f272f6-6f4c-4f37-bd1e-a2ca440797d5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-23T22:47:38.215862Z","iopub.execute_input":"2021-09-23T22:47:38.216722Z","iopub.status.idle":"2021-09-23T22:47:38.23673Z","shell.execute_reply.started":"2021-09-23T22:47:38.216559Z","shell.execute_reply":"2021-09-23T22:47:38.235941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.random.set_seed(999)\nnp.random.seed(999)","metadata":{"_uuid":"27984da8-06d8-424a-9e21-b48b804c69c6","_cell_guid":"beb1fa90-f6a1-4d69-bb58-19c1d6ffe058","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-23T22:47:38.238138Z","iopub.execute_input":"2021-09-23T22:47:38.238403Z","iopub.status.idle":"2021-09-23T22:47:38.243241Z","shell.execute_reply.started":"2021-09-23T22:47:38.238367Z","shell.execute_reply":"2021-09-23T22:47:38.242443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pydub import AudioSegment\nimport IPython\n\nIPython.display.Audio(\"../input/commonvoice2/commonvoice/train/clips/common_voice_en_10153.wav\")","metadata":{"_uuid":"43779c80-2d59-474a-b96b-7766faf8b190","_cell_guid":"689eac7e-8847-4040-8f95-d2de3db252dd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-23T22:47:38.245121Z","iopub.execute_input":"2021-09-23T22:47:38.245459Z","iopub.status.idle":"2021-09-23T22:47:38.308575Z","shell.execute_reply.started":"2021-09-23T22:47:38.245425Z","shell.execute_reply":"2021-09-23T22:47:38.307537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%run feature_extractor.py\n%run utils.py\n%run dataset.py\n%run mozilla_commonvoice.py\n%run urban_sound_8k.py","metadata":{"_uuid":"e54eef40-335d-4c84-aef5-1ae1a7b130e9","_cell_guid":"2815b953-5c84-49f8-beb3-1972ca1ccdc4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-23T22:47:38.309419Z","iopub.execute_input":"2021-09-23T22:47:38.30963Z","iopub.status.idle":"2021-09-23T22:47:38.329546Z","shell.execute_reply.started":"2021-09-23T22:47:38.309603Z","shell.execute_reply":"2021-09-23T22:47:38.328871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.mkdir(\"/kaggle/working/records\")","metadata":{"_uuid":"9351a72c-554a-4287-a238-b44152b2ec21","_cell_guid":"a3964f2a-0238-4d95-b1e6-e5acc4418cbe","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-23T22:47:38.330898Z","iopub.execute_input":"2021-09-23T22:47:38.33134Z","iopub.status.idle":"2021-09-23T22:47:38.335138Z","shell.execute_reply.started":"2021-09-23T22:47:38.331306Z","shell.execute_reply":"2021-09-23T22:47:38.33443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%run create_dataset.py","metadata":{"_uuid":"8f231f35-a6bd-4150-aaf8-a7cdb2f31b58","_cell_guid":"408ad024-221b-42ef-811e-fc78b71b1197","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-23T22:47:38.33657Z","iopub.execute_input":"2021-09-23T22:47:38.337194Z","iopub.status.idle":"2021-09-23T23:02:26.619614Z","shell.execute_reply.started":"2021-09-23T22:47:38.337035Z","shell.execute_reply":"2021-09-23T23:02:26.618708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tfrecords_filenames = glob.glob('/kaggle/working/records/train_*')\nnp.random.shuffle(train_tfrecords_filenames)\ntrain_tfrecords_filenames = list(train_tfrecords_filenames)\nprint(train_tfrecords_filenames)\nval_tfrecords_filenames = glob.glob('/kaggle/working/records/val_*')\n# glob.glob(\"/kaggle/working/records/val_*\")","metadata":{"_uuid":"20437682-6960-4fff-a470-759da181537e","_cell_guid":"c4e9cbd4-2666-45d9-a10a-dd53347f809a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-23T23:02:26.621645Z","iopub.execute_input":"2021-09-23T23:02:26.623563Z","iopub.status.idle":"2021-09-23T23:02:26.633544Z","shell.execute_reply.started":"2021-09-23T23:02:26.623519Z","shell.execute_reply":"2021-09-23T23:02:26.632391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Variable declarations","metadata":{"_uuid":"466e6fe1-7a05-49dd-bbe8-770176d00230","_cell_guid":"edecb894-aa62-4b02-bc41-68bc215ade5b","trusted":true}},{"cell_type":"code","source":"windowLength = 256\noverlap      = round(0.25 * windowLength) # overlap of 75%\nffTLength    = windowLength\ninputFs      = 48e3\nfs           = 8e3\nnumFeatures  = ffTLength//2 + 1\nnumSegments  = 8\nprint(\"windowLength:\",windowLength)\nprint(\"overlap:\",overlap)\nprint(\"ffTLength:\",ffTLength)\nprint(\"inputFs:\",inputFs)\nprint(\"fs:\",fs)\nprint(\"numFeatures:\",numFeatures)\nprint(\"numSegments:\",numSegments)","metadata":{"_uuid":"58413833-b9a8-4cb2-a7de-7eb077146f97","_cell_guid":"2ba229ef-7d07-4125-b4b0-ed567fce0b79","execution":{"iopub.status.busy":"2021-09-23T23:02:26.63539Z","iopub.execute_input":"2021-09-23T23:02:26.635694Z","iopub.status.idle":"2021-09-23T23:02:26.647826Z","shell.execute_reply.started":"2021-09-23T23:02:26.635639Z","shell.execute_reply":"2021-09-23T23:02:26.646822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(mozilla_basepath)\nprint(urbansound_basepath)","metadata":{"_uuid":"3c298e0d-c23e-46dc-a7a8-34eee86a783e","_cell_guid":"5da27597-91e1-40f4-b573-64a64ad5f587","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-23T23:02:26.654256Z","iopub.execute_input":"2021-09-23T23:02:26.654481Z","iopub.status.idle":"2021-09-23T23:02:26.659463Z","shell.execute_reply.started":"2021-09-23T23:02:26.654455Z","shell.execute_reply":"2021-09-23T23:02:26.658722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Prepare Input features","metadata":{"_uuid":"9636733f-00b6-4825-ae52-27bcc6db91ee","_cell_guid":"c3bdcf41-555e-4c63-b53a-4a04eb7a7a15","trusted":true}},{"cell_type":"code","source":"def tf_record_parser(record):\n    keys_to_features = {\n        \"noise_stft_phase\": tf.io.FixedLenFeature((), tf.string, default_value=\"\"),\n        'noise_stft_mag_features': tf.io.FixedLenFeature([], tf.string),\n        \"clean_stft_magnitude\": tf.io.FixedLenFeature((), tf.string)\n    }\n\n    features = tf.io.parse_single_example(record, keys_to_features)\n\n    noise_stft_mag_features = tf.io.decode_raw(features['noise_stft_mag_features'], tf.float32)\n    clean_stft_magnitude = tf.io.decode_raw(features['clean_stft_magnitude'], tf.float32)\n    noise_stft_phase = tf.io.decode_raw(features['noise_stft_phase'], tf.float32)\n\n    # reshape input and annotation images\n    noise_stft_mag_features = tf.reshape(noise_stft_mag_features, (129, 8, 1), name=\"noise_stft_mag_features\")\n    clean_stft_magnitude = tf.reshape(clean_stft_magnitude, (129, 1, 1), name=\"clean_stft_magnitude\")\n    noise_stft_phase = tf.reshape(noise_stft_phase, (129,), name=\"noise_stft_phase\")\n\n    return noise_stft_mag_features, clean_stft_magnitude","metadata":{"_uuid":"49738c66-2360-487e-a6c7-8399c3df29db","_cell_guid":"9b478e13-08be-4251-8752-4f48f8f1066d","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-09-23T23:02:26.660758Z","iopub.execute_input":"2021-09-23T23:02:26.661628Z","iopub.status.idle":"2021-09-23T23:02:26.672426Z","shell.execute_reply.started":"2021-09-23T23:02:26.661587Z","shell.execute_reply":"2021-09-23T23:02:26.671743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Create tf.Data.Dataset","metadata":{"_uuid":"f35543b9-ecca-42e6-b5fe-e192cd800c3a","_cell_guid":"10b9144d-bef0-4ff5-9912-cb60e93a77f9","trusted":true}},{"cell_type":"code","source":"train_dataset = tf.data.TFRecordDataset([train_tfrecords_filenames])\ntrain_dataset = train_dataset.map(tf_record_parser)\ntrain_dataset = train_dataset.shuffle(8192)\ntrain_dataset = train_dataset.repeat()\ntrain_dataset = train_dataset.batch(512+256)\ntrain_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)","metadata":{"_uuid":"0bc8a442-7d4a-43b6-ab78-de8fda37d4f2","_cell_guid":"47f61b02-ceb1-4d58-9a65-1e8b2f0ed3d9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-23T23:02:26.673609Z","iopub.execute_input":"2021-09-23T23:02:26.673909Z","iopub.status.idle":"2021-09-23T23:02:27.203262Z","shell.execute_reply.started":"2021-09-23T23:02:26.673869Z","shell.execute_reply":"2021-09-23T23:02:27.202525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = tf.data.TFRecordDataset([val_tfrecords_filenames])\ntest_dataset = test_dataset.map(tf_record_parser)\ntest_dataset = test_dataset.repeat(1)\ntest_dataset = test_dataset.batch(512)","metadata":{"_uuid":"0abdf2bc-5742-4ada-9ca1-f4fb7ed997fe","_cell_guid":"c27bef01-f373-46c1-b590-25684769f16a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-23T23:02:27.205792Z","iopub.execute_input":"2021-09-23T23:02:27.206268Z","iopub.status.idle":"2021-09-23T23:02:27.231107Z","shell.execute_reply.started":"2021-09-23T23:02:27.206227Z","shell.execute_reply":"2021-09-23T23:02:27.230461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Model Training","metadata":{"_uuid":"759646cc-3233-47e9-a147-4a4420669e23","_cell_guid":"3cbe268e-9d2d-464c-aea4-6fedc5282aca","trusted":true}},{"cell_type":"code","source":"from tensorflow.keras.layers import Conv2D, Input, LeakyReLU, Flatten, Dense, Reshape, Conv2DTranspose, BatchNormalization, Activation\nfrom tensorflow.keras import Model, Sequential","metadata":{"_uuid":"b9f94818-1595-414a-8ae0-a132377a40b7","_cell_guid":"b986bfe3-2235-49ee-bced-e7644bfc1b9d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-23T23:02:27.232373Z","iopub.execute_input":"2021-09-23T23:02:27.232638Z","iopub.status.idle":"2021-09-23T23:02:27.239335Z","shell.execute_reply.started":"2021-09-23T23:02:27.232604Z","shell.execute_reply":"2021-09-23T23:02:27.238644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def conv_block(x, filters, kernel_size, strides, padding='same', use_bn=True):\n    x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, use_bias=False,\n              kernel_regularizer=tf.keras.regularizers.l2(0.0006))(x)\n    x = Activation('relu')(x)\n    if use_bn:\n        x = BatchNormalization()(x)\n        return x","metadata":{"_uuid":"b778f9d0-c33e-4923-b73b-b1df35f52dc3","_cell_guid":"a50143c9-18c5-4266-8c03-3ba43f51ce5b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-23T23:02:27.240796Z","iopub.execute_input":"2021-09-23T23:02:27.241088Z","iopub.status.idle":"2021-09-23T23:02:27.247626Z","shell.execute_reply.started":"2021-09-23T23:02:27.241054Z","shell.execute_reply":"2021-09-23T23:02:27.246975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def full_pre_activation_block(x, filters, kernel_size, strides, padding='same', use_bn=True):\n    shortcut = x\n    in_channels = x.shape[-1]\n\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same')(x)\n\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = Conv2D(filters=in_channels, kernel_size=kernel_size, strides=strides, padding='same')(x)\n\n    return shortcut + x","metadata":{"_uuid":"a6a039fd-6ec7-4c7e-9afd-bb7339511b56","_cell_guid":"7746732f-0ed3-49f8-9f2d-67399444ec21","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-23T23:02:27.248883Z","iopub.execute_input":"2021-09-23T23:02:27.249326Z","iopub.status.idle":"2021-09-23T23:02:27.259465Z","shell.execute_reply.started":"2021-09-23T23:02:27.249287Z","shell.execute_reply":"2021-09-23T23:02:27.258782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(l2_strength):\n    inputs = Input(shape=[numFeatures, numSegments, 1])\n    x = inputs\n\n    # -----\n    x = tf.keras.layers.ZeroPadding2D(((4, 4), (0, 0)))(x)\n    x = Conv2D(filters=18, kernel_size=[9, 8], strides=[1, 1], padding='valid', use_bias=False,\n               kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n\n    skip0 = Conv2D(filters=30, kernel_size=[5, 1], strides=[1, 1], padding='same', use_bias=False,\n                   kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n    x = Activation('relu')(skip0)\n    x = BatchNormalization()(x)\n\n    x = Conv2D(filters=8, kernel_size=[9, 1], strides=[1, 1], padding='same', use_bias=False,\n               kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n\n    # -----\n    x = Conv2D(filters=18, kernel_size=[9, 1], strides=[1, 1], padding='same', use_bias=False,\n               kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n\n    skip1 = Conv2D(filters=30, kernel_size=[5, 1], strides=[1, 1], padding='same', use_bias=False,\n                   kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n    x = Activation('relu')(skip1)\n    x = BatchNormalization()(x)\n\n    x = Conv2D(filters=8, kernel_size=[9, 1], strides=[1, 1], padding='same', use_bias=False,\n               kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n\n    # ----\n    x = Conv2D(filters=18, kernel_size=[9, 1], strides=[1, 1], padding='same', use_bias=False,\n               kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n\n    x = Conv2D(filters=30, kernel_size=[5, 1], strides=[1, 1], padding='same', use_bias=False,\n               kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n\n    x = Conv2D(filters=8, kernel_size=[9, 1], strides=[1, 1], padding='same', use_bias=False,\n               kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n\n    # ----\n    x = Conv2D(filters=18, kernel_size=[9, 1], strides=[1, 1], padding='same', use_bias=False,\n               kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n\n    x = Conv2D(filters=30, kernel_size=[5, 1], strides=[1, 1], padding='same', use_bias=False,\n               kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n    x = x + skip1\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n\n    x = Conv2D(filters=8, kernel_size=[9, 1], strides=[1, 1], padding='same', use_bias=False,\n               kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n\n    # ----\n    x = Conv2D(filters=18, kernel_size=[9, 1], strides=[1, 1], padding='same', use_bias=False,\n               kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n\n    x = Conv2D(filters=30, kernel_size=[5, 1], strides=[1, 1], padding='same', use_bias=False,\n               kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n    x = x + skip0\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n\n    x = Conv2D(filters=8, kernel_size=[9, 1], strides=[1, 1], padding='same', use_bias=False,\n               kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n\n    # ----\n    x = tf.keras.layers.SpatialDropout2D(0.2)(x)\n    x = Conv2D(filters=1, kernel_size=[129, 1],\n               strides=[1, 1], padding='same')(x)\n\n    model = Model(inputs=inputs, outputs=x)\n\n    optimizer = tf.keras.optimizers.Adam(3e-4)\n    # optimizer = RAdam(total_steps=10000, warmup_proportion=0.1, min_lr=3e-4)\n\n    model.compile(optimizer=optimizer, loss='mse',\n                  metrics=[tf.keras.metrics.RootMeanSquaredError('rmse')])\n    return model","metadata":{"_uuid":"69bf0d46-5af4-4aa1-9868-1b7109358fd6","_cell_guid":"7a0362ee-21ef-4784-ac66-dc5ac3e5aa81","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-09-23T23:02:27.260793Z","iopub.execute_input":"2021-09-23T23:02:27.261058Z","iopub.status.idle":"2021-09-23T23:02:27.289178Z","shell.execute_reply.started":"2021-09-23T23:02:27.261023Z","shell.execute_reply":"2021-09-23T23:02:27.28839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build_model(l2_strength=0.0)\nmodel.summary()","metadata":{"_uuid":"a865b72e-59e4-4c3f-9a35-6e64d7635c60","_cell_guid":"59c8f9c7-9580-4f4c-a5dc-0db9346f1bda","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-23T23:02:27.290407Z","iopub.execute_input":"2021-09-23T23:02:27.290684Z","iopub.status.idle":"2021-09-23T23:02:27.848836Z","shell.execute_reply.started":"2021-09-23T23:02:27.29065Z","shell.execute_reply":"2021-09-23T23:02:27.848129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model, show_shapes=True, dpi=64)","metadata":{"_uuid":"e9970575-ef67-4c6c-b32f-c5cce01dfbd6","_cell_guid":"4cafa508-06ab-4fa5-a4da-4d7d827fb078","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-23T23:02:27.850074Z","iopub.execute_input":"2021-09-23T23:02:27.850327Z","iopub.status.idle":"2021-09-23T23:02:28.846108Z","shell.execute_reply.started":"2021-09-23T23:02:27.850293Z","shell.execute_reply":"2021-09-23T23:02:28.845409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.load_weights('drive/My Drive/datasets/dataset_v2/denoiser_cnn_log_mel_generator.h5')\nbaseline_val_loss = model.evaluate(test_dataset)[0]\nprint(baseline_val_loss)","metadata":{"_uuid":"84ad7372-ff99-47d7-96ba-103a52d5f871","_cell_guid":"dccb622d-d9d3-4fe4-bc08-4653c2a510d4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-23T23:02:28.847508Z","iopub.execute_input":"2021-09-23T23:02:28.848305Z","iopub.status.idle":"2021-09-23T23:02:40.647158Z","shell.execute_reply.started":"2021-09-23T23:02:28.848261Z","shell.execute_reply":"2021-09-23T23:02:40.646395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def l2_norm(vector):\n    return np.square(vector)\n\ndef SDR(denoised, cleaned, eps=1e-7): # Signal to Distortion Ratio\n    a = l2_norm(denoised)\n    b = l2_norm(denoised - cleaned)\n    a_b = a / b\n    return np.mean(10 * np.log10(a_b + eps))","metadata":{"_uuid":"1e302b33-894e-49a3-af87-6f9de50779be","_cell_guid":"3829eea7-5467-4015-8ce3-546a200239db","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-23T23:02:40.648913Z","iopub.execute_input":"2021-09-23T23:02:40.649177Z","iopub.status.idle":"2021-09-23T23:02:40.654394Z","shell.execute_reply.started":"2021-09-23T23:02:40.649141Z","shell.execute_reply":"2021-09-23T23:02:40.653661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True, baseline=baseline_val_loss, mode=\"auto\")\n\nlogdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\ntensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, update_freq='batch')\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath='/kaggle/working/speech_denoiser.h5', \n                                                         monitor='val_loss', save_best_only=True)\n\nmodel.fit(train_dataset,\n         steps_per_epoch=600,\n         validation_data=test_dataset,\n         epochs=999,\n         callbacks=[early_stopping_callback, tensorboard_callback, checkpoint_callback]\n        )","metadata":{"_uuid":"430724f1-8cd9-4df4-a7e7-ad7d459afe62","_cell_guid":"72936efd-88af-408b-bb1f-d09d571b4884","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-23T23:02:40.65578Z","iopub.execute_input":"2021-09-23T23:02:40.656237Z","iopub.status.idle":"2021-09-24T00:34:46.050863Z","shell.execute_reply.started":"2021-09-23T23:02:40.656202Z","shell.execute_reply":"2021-09-24T00:34:46.045991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Testing","metadata":{"_uuid":"1ad45dda-5657-4b64-91c5-331cc74819a6","_cell_guid":"12935dbc-0851-4918-9374-abf4635c9a75","trusted":true}},{"cell_type":"code","source":"import librosa\nimport pandas as pd\nimport os\nimport datetime\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nimport IPython.display as ipd\nimport librosa.display\nimport scipy\nimport glob\nimport numpy as np\nimport math\nimport warnings\nimport pickle\nfrom sklearn.utils import shuffle\n\n# Load the TensorBoard notebook extension.\n%load_ext tensorboard\n\nmozilla_basepath = '../input/commonvoice2/commonvoice'\nUrbanSound8K_basepath = '../input/urbansound8k'","metadata":{"_uuid":"e993290f-ba0d-45d7-a8c2-ea298247fe58","_cell_guid":"e19d3f2c-8591-4501-b1bd-be40e9e2f41a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-24T00:34:46.052086Z","iopub.execute_input":"2021-09-24T00:34:46.052334Z","iopub.status.idle":"2021-09-24T00:34:46.071105Z","shell.execute_reply.started":"2021-09-24T00:34:46.052302Z","shell.execute_reply":"2021-09-24T00:34:46.070439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_audio(filepath, sample_rate, normalize=True):\n    # print(f\"Reading: {filepath}\").\n    audio, sr = librosa.load(filepath, sr=sample_rate)\n    if normalize:\n        div_fac = 1 / np.max(np.abs(audio)) / 3.0\n        audio = audio * div_fac\n    return audio, sr\n        \ndef add_noise_to_clean_audio(clean_audio, noise_signal):\n    if len(clean_audio) >= len(noise_signal):\n        # print(\"The noisy signal is smaller than the clean audio input. Duplicating the noise.\")\n        while len(clean_audio) >= len(noise_signal):\n            noise_signal = np.append(noise_signal, noise_signal)\n\n    ## Extract a noise segment from a random location in the noise file\n    ind = np.random.randint(0, noise_signal.size - clean_audio.size)\n\n    noiseSegment = noise_signal[ind: ind + clean_audio.size]\n\n    speech_power = np.sum(clean_audio ** 2)\n    noise_power = np.sum(noiseSegment ** 2)\n    noisyAudio = clean_audio + np.sqrt(speech_power / noise_power) * noiseSegment\n    return noisyAudio\n\ndef play(audio, sample_rate):\n    ipd.display(ipd.Audio(data=audio, rate=sample_rate))  # load a local WAV file","metadata":{"_uuid":"39bca0a3-c81a-49a1-a199-58bb4862c2e2","_cell_guid":"7a19b06d-fa51-41a7-a822-4fd4bf1fa360","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-09-24T00:34:46.072464Z","iopub.execute_input":"2021-09-24T00:34:46.0732Z","iopub.status.idle":"2021-09-24T00:34:46.089837Z","shell.execute_reply.started":"2021-09-24T00:34:46.073163Z","shell.execute_reply":"2021-09-24T00:34:46.088322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cleanAudio, sr = read_audio(os.path.join(mozilla_basepath, 'test', 'clips', 'common_voice_en_100738.wav'), sample_rate=fs)\nprint(\"Min:\", np.min(cleanAudio),\"Max:\",np.max(cleanAudio))\nipd.Audio(data=cleanAudio, rate=sr) # load a local WAV file","metadata":{"_uuid":"7815079c-d4b1-404d-84c7-43e02ade5c5b","_cell_guid":"85e596b0-08c6-4915-81c7-d4558b9f2e82","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-24T00:34:46.094112Z","iopub.execute_input":"2021-09-24T00:34:46.094408Z","iopub.status.idle":"2021-09-24T00:34:46.414845Z","shell.execute_reply.started":"2021-09-24T00:34:46.094371Z","shell.execute_reply":"2021-09-24T00:34:46.414122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"noiseAudio, sr = read_audio(os.path.join(UrbanSound8K_basepath, 'fold6', '108638-9-0-0.wav'), sample_rate=fs)\nprint(\"Min:\", np.min(noiseAudio),\"Max:\",np.max(noiseAudio))\nipd.Audio(data=noiseAudio, rate=sr) # load a local WAV file","metadata":{"_uuid":"e212ae66-d44b-4289-b86b-40501dcd2572","_cell_guid":"5ac95b48-07d2-423e-8a04-6607113fb035","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-24T00:34:46.41655Z","iopub.execute_input":"2021-09-24T00:34:46.417418Z","iopub.status.idle":"2021-09-24T00:34:46.708358Z","shell.execute_reply.started":"2021-09-24T00:34:46.417379Z","shell.execute_reply":"2021-09-24T00:34:46.707571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cleanAudioFeatureExtractor = FeatureExtractor(cleanAudio, windowLength=windowLength, overlap=overlap, sample_rate=sr)\nstft_features = cleanAudioFeatureExtractor.get_stft_spectrogram()\nstft_features = np.abs(stft_features)\nprint(\"Min:\", np.min(stft_features),\"Max:\",np.max(stft_features))","metadata":{"_uuid":"6b7e26ad-5086-4157-87f0-ff3a0a04f8a7","_cell_guid":"047d34f3-9132-437c-a018-c726f3554e12","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-24T00:34:46.711945Z","iopub.execute_input":"2021-09-24T00:34:46.712217Z","iopub.status.idle":"2021-09-24T00:34:46.728921Z","shell.execute_reply.started":"2021-09-24T00:34:46.71218Z","shell.execute_reply":"2021-09-24T00:34:46.727125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"noisyAudio = add_noise_to_clean_audio(cleanAudio, noiseAudio)\nipd.Audio(data=noisyAudio, rate=fs) # load a local WAV file","metadata":{"_uuid":"f3f5119c-c355-41c2-9afd-32f717795e97","_cell_guid":"126f6802-4053-4ee2-934c-4407ef262df3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-24T00:34:46.733171Z","iopub.execute_input":"2021-09-24T00:34:46.737846Z","iopub.status.idle":"2021-09-24T00:34:46.763516Z","shell.execute_reply.started":"2021-09-24T00:34:46.737804Z","shell.execute_reply":"2021-09-24T00:34:46.76268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_input_features(stft_features):\n    # Phase Aware Scaling: To avoid extreme differences (more than\n    # 45 degree) between the noisy and clean phase, the clean spectral magnitude was encoded as similar to [21]:\n    noisySTFT = np.concatenate([stft_features[:,0:numSegments-1], stft_features], axis=1)\n    stftSegments = np.zeros((numFeatures, numSegments , noisySTFT.shape[1] - numSegments + 1))\n\n    for index in range(noisySTFT.shape[1] - numSegments + 1):\n        stftSegments[:,:,index] = noisySTFT[:,index:index + numSegments]\n    return stftSegments","metadata":{"_uuid":"ae86bfb6-2ca7-4fc4-8424-144c94ecaf43","_cell_guid":"92fb3cbb-4ea0-4d89-b6ec-d6f86929f1f1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-24T00:34:46.765309Z","iopub.execute_input":"2021-09-24T00:34:46.767471Z","iopub.status.idle":"2021-09-24T00:34:46.790186Z","shell.execute_reply.started":"2021-09-24T00:34:46.767433Z","shell.execute_reply":"2021-09-24T00:34:46.789178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"noiseAudioFeatureExtractor = FeatureExtractor(noisyAudio, windowLength=windowLength, overlap=overlap, sample_rate=sr)\nnoise_stft_features = noiseAudioFeatureExtractor.get_stft_spectrogram()\n\n# Paper: Besides, spectral phase was not used in the training phase.\n# At reconstruction, noisy spectral phase was used instead to\n# perform in- verse STFT and recover human speech.\nnoisyPhase = np.angle(noise_stft_features)\nprint(noisyPhase.shape)\nnoise_stft_features = np.abs(noise_stft_features)\n\nmean = np.mean(noise_stft_features)\nstd = np.std(noise_stft_features)\nnoise_stft_features = (noise_stft_features - mean) / std","metadata":{"_uuid":"057e24f5-50a2-4444-92e0-14877ff7bbaf","_cell_guid":"5bbf6be8-f9eb-4a53-9a7e-6b8aa772432e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-24T00:34:46.791585Z","iopub.execute_input":"2021-09-24T00:34:46.792041Z","iopub.status.idle":"2021-09-24T00:34:46.811931Z","shell.execute_reply.started":"2021-09-24T00:34:46.79196Z","shell.execute_reply":"2021-09-24T00:34:46.811036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictors = prepare_input_features(noise_stft_features)","metadata":{"_uuid":"85021b46-633e-471c-8a94-bd5c9800cf40","_cell_guid":"b5a0802b-604d-45cd-8034-7550f33a5898","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-24T00:34:46.813916Z","iopub.execute_input":"2021-09-24T00:34:46.814359Z","iopub.status.idle":"2021-09-24T00:34:46.822899Z","shell.execute_reply.started":"2021-09-24T00:34:46.814322Z","shell.execute_reply":"2021-09-24T00:34:46.822103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictors = np.reshape(predictors, (predictors.shape[0], predictors.shape[1], 1, predictors.shape[2]))\npredictors = np.transpose(predictors, (3, 0, 1, 2)).astype(np.float32)\nprint('predictors.shape:', predictors.shape)","metadata":{"_uuid":"4014dd3e-9e62-443d-a3fe-bc771ba7c7c7","_cell_guid":"cd7091a9-9203-4f91-b4cf-e8cfbcd314ff","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-24T00:34:46.824511Z","iopub.execute_input":"2021-09-24T00:34:46.824971Z","iopub.status.idle":"2021-09-24T00:34:46.831402Z","shell.execute_reply.started":"2021-09-24T00:34:46.824934Z","shell.execute_reply":"2021-09-24T00:34:46.830393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = model.loa\nSTFTFullyConvolutional = model.predict(predictors)\nprint(STFTFullyConvolutional.shape)","metadata":{"_uuid":"150871f1-39d9-4f4a-8002-17e82527fb60","_cell_guid":"aa4d80e4-530b-40b2-a9a4-413acf639c94","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-24T00:34:46.833044Z","iopub.execute_input":"2021-09-24T00:34:46.833328Z","iopub.status.idle":"2021-09-24T00:34:47.198245Z","shell.execute_reply.started":"2021-09-24T00:34:46.833295Z","shell.execute_reply":"2021-09-24T00:34:47.197514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def revert_features_to_audio(features, phase, cleanMean=None, cleanStd=None):\n    # scale the outpus back to the original range\n    if cleanMean and cleanStd:\n        features = cleanStd * features + cleanMean\n\n    phase = np.transpose(phase, (1, 0))\n    features = np.squeeze(features)\n\n    # features = librosa.db_to_power(features)\n    features = features * np.exp(1j * phase)  # that fixes the abs() ope previously done\n\n    features = np.transpose(features, (1, 0))\n    return noiseAudioFeatureExtractor.get_audio_from_stft_spectrogram(features)","metadata":{"_uuid":"c847a08e-a759-43f0-b105-66b666df4a35","_cell_guid":"10b1796d-241a-460d-9452-d0a1550df858","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-24T00:34:47.201239Z","iopub.execute_input":"2021-09-24T00:34:47.201462Z","iopub.status.idle":"2021-09-24T00:34:47.209254Z","shell.execute_reply.started":"2021-09-24T00:34:47.201437Z","shell.execute_reply":"2021-09-24T00:34:47.208463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"denoisedAudioFullyConvolutional = revert_features_to_audio(STFTFullyConvolutional, noisyPhase, mean, std)\nprint(\"Min:\", np.min(denoisedAudioFullyConvolutional),\"Max:\",np.max(denoisedAudioFullyConvolutional))\nipd.Audio(data=denoisedAudioFullyConvolutional, rate=fs) # load a local WAV file","metadata":{"_uuid":"72aebb6c-0fc9-49dc-b093-1c303cd1cd44","_cell_guid":"6db19594-f40c-4b64-b763-c76d0654795b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-24T00:34:47.212118Z","iopub.execute_input":"2021-09-24T00:34:47.212373Z","iopub.status.idle":"2021-09-24T00:34:47.68342Z","shell.execute_reply.started":"2021-09-24T00:34:47.212339Z","shell.execute_reply":"2021-09-24T00:34:47.682728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2, ax3) = plt.subplots(3, 1, sharey=True)\n\nax1.plot(cleanAudio)\nax1.set_title(\"Clean Audio\")\n\nax2.plot(noisyAudio)\nax2.set_title(\"Noisy Audio\")\n\nax3.plot(denoisedAudioFullyConvolutional)\nax3.set_title(\"Denoised Audio\")","metadata":{"_uuid":"6c8ac939-e139-4e9d-b331-ea72e93b1d01","_cell_guid":"b20c5f0a-60b6-4231-b0dc-366a73ebc0fb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-24T00:34:47.684899Z","iopub.execute_input":"2021-09-24T00:34:47.685364Z","iopub.status.idle":"2021-09-24T00:34:48.185404Z","shell.execute_reply.started":"2021-09-24T00:34:47.685322Z","shell.execute_reply":"2021-09-24T00:34:48.184752Z"},"trusted":true},"execution_count":null,"outputs":[]}]}