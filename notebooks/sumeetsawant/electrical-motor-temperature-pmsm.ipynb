{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import YouTubeVideo","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Context\n\nThe data set comprises several sensor data collected from a permanent magnet synchronous motor (PMSM) deployed on a test bench. The PMSM represents a german OEM's prototype model. Test bench measurements were collected by the LEA department at Paderborn University.This data set is mildly anonymized.<br>\n\n# Working of PMSM \n\n\nThe below embedded vedio shows the working of the PMSM motor . Basically the motor consists of two parts the stator (meaning stationary) and rotor ( which rotates) . A alternating current is passed through the stator which generates a rotating magnetic field and a DC current is passed through the rotor which generates a stationary magnetic field . When the opposite poles of the two magnetic field unite the stator's rotating magnetic field drives the rotor's stationary magnetic field.<br> \n\n\n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nYouTubeVideo('Vk2jDXxZIhs', width=800, height=500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# How the data was collected \n\nAll recordings are sampled at 2 Hz. The data set consists of multiple measurement sessions, which can be distinguished from each other by column \"profile_id\". A measurement session can be between one and six hours long.\n\nThe motor is excited by hand-designed driving cycles denoting a reference motor speed and a reference torque.\nCurrents in d/q-coordinates (columns \"id\" and iq\") and voltages in d/q-coordinates (columns \"ud\" and \"uq\") are a result of a standard control strategy trying to follow the reference speed and torque.\nColumns \"motor_speed\" and \"torque\" are the resulting quantities achieved by that strategy, derived from set currents and voltages.\n\nMost driving cycles denote random walks in the speed-torque-plane in order to imitate real world driving cycles to a more accurate degree than constant excitations and ramp-ups and -downs would."},{"metadata":{},"cell_type":"markdown","source":"# Aim of this notebook \n\nMy aim with this notebook was to predict the stator winding and rotor winding temperatures and select the best model out of various ones avaliable. \n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n%matplotlib inline \n\n# Sklearn imports \nfrom sklearn.linear_model import Lasso,Ridge, LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn import neighbors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv('/kaggle/input/electric-motor-temperature/pmsm_temperature_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(20)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()\n\n# No Nulls values are detected ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data['profile_id'].value_counts().sort_index()\n\n#Un commnet to see the values under each profile_id ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8));\n\nsns.barplot(x=data.groupby('profile_id').agg('count').sort_values(by='pm').index,\n            y=data.groupby('profile_id').agg('count').sort_values(by='pm')['pm'],\n            order=data.groupby('profile_id').agg('count').sort_values(by='pm').index,\n           orient='v',color='orange');\n\nplt.title('Profile_id vs data points');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above graph we see that number of test points in each profile_id. This basically represents how long the test was run \n\nWe see that max number of points are there in 65 ,6 and 20 "},{"metadata":{},"cell_type":"markdown","source":"# Spliting into Train and Test set \n\nGiven from desription we know that profile id 65 and 72 should be used in test set "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test=data[(data['profile_id']==65) | (data['profile_id']==72)]\nX_train=data[(data['profile_id']!=65) | (data['profile_id']!=72)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this dataset we need to predict the stator and rotor temperature given by \n\n__Target features__:<br>\n\n\nstator_yoke<br>\nstator_winding<br>\nstator_tooth<br>\npm<br>\n\n\nRest all feature exluding torque as it is difficult to measure and profile_id as it is a label can be us for prediction \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let see how the other features are colrelated with each other \n\ndata.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns=['ambient', 'coolant', 'u_d', 'u_q', 'motor_speed', 'i_d',\n       'i_q', 'pm', 'stator_yoke', 'stator_tooth', 'stator_winding','torque']\n\ncorr=data[columns].corr()\n\nplt.figure(figsize=(12,10))\nsns.heatmap(corr,annot=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Points to note \n\nWe see that the rotor temperature(pm) is corelated well with other features . Rotor temperature is also well corelated with \nstator temperarture so we can use predict rotor temperature first and then use that as an feature to predict stator temps\n\nAlso we see that there is a great corelation among various starter temperature so we can also decided if we want to predict one of the startor components only and assume the other around same temperature range . See the graph below \n\nAlso we see that stator tooth is most of the time in between stator yoke and stator winding\n\n\nAlso torque is in well agreement with i_q"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Change the profile ID number \n\nplt.figure(figsize=(20,5))\ndata[data['profile_id'] == 80]['stator_yoke'].plot(label = 'stator yoke')\ndata[data['profile_id'] == 80]['stator_tooth'].plot(label = 'stator tooth')\ndata[data['profile_id'] == 80]['stator_winding'].plot(label = 'stator winding')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As part of dependent variables we have 4 temperature to predict .But out of the 4 temperatures required to predict 3 of stator usually follow the same profile and also are in the close vicinity of each other. Thus I feel if a model which can accurately model one of the three stator temperature should work for us. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above we see that the min max and mean for all variables are with same value range hence I do not feel the need to \nnormalize the data \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=X_train[['ambient','coolant','u_d','u_q','motor_speed','torque','i_d','i_q']]\ny=X_train['pm']\n\ny_test_rotor=X_test['pm']\nX_test_rotor=X_test[['ambient','coolant','u_d','u_q','motor_speed','torque','i_d','i_q']]\n\n\n#from sklearn.model_selection import train_test_split\n\n\n#X_train,X_val,y_train,y_val=train_test_split(X,y,shuffle=True,random_state=7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Establishing a baseline using Linear Regression "},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nlr=LinearRegression(normalize=False)\n\nkfold=model_selection.KFold(n_splits=10,shuffle=True,random_state=101)\n\nlr_scores=[]\n\nlr_scores=cross_val_score(lr,X,y,cv=kfold,n_jobs=-1,scoring='neg_root_mean_squared_error')\n\n    \nprint('RMSE for Linear Regression is {0:.3f} and variance of MSE is {1:.8f}'.format(-1*np.mean(lr_scores),np.var(lr_scores)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. ## Optimizing Ridge Regression "},{"metadata":{"trusted":true},"cell_type":"code","source":"\nridge=Ridge()\nparam_grid={\n    'alpha':[0.001,0.01,1,5,10,20],\n    'normalize' : [True,False],\n    'fit_intercept':[True,False]\n}\n\nkfold=model_selection.KFold(n_splits=10,shuffle=True,random_state=101)\n\nmodel=model_selection.RandomizedSearchCV(\n                        estimator=ridge,\n                        param_distributions=param_grid,\n                        n_iter=20,\n                        scoring='neg_root_mean_squared_error',\n                        cv=kfold,\n                        refit=True,\n                        verbose=5,\n                        n_jobs=-1\n                        )\n\nmodel.fit(X,y)\n\nprint('Best Scorer{}'.format(model.best_score_))\n\nprint('/n')\n\nprint('Best Parameters{}'.format(model.best_params_))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Implementing the best fit Ridge model from above "},{"metadata":{"trusted":true},"cell_type":"code","source":"# implementing Ridge Regression \n\nridge=Ridge(alpha=5,fit_intercept=True,normalize=False)\n\nkfold=model_selection.KFold(n_splits=10,shuffle=True,random_state=101)\n\nridge_scores=[]\n\nridge_scores=cross_val_score(ridge,X,y,cv=kfold,n_jobs=-1,scoring='neg_root_mean_squared_error')\n\n    \nprint('RMSE for Ridge Regression is {0:.5f} and variance of MSE is {1:.8f}'.format(-1*np.mean(ridge_scores),np.var(ridge_scores)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Optimizing Lasso Regression "},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso=Lasso()\n\nparam_grid={\n    'alpha':[0.001,0.01,1,5,10,20],\n    'normalize' : [True,False],\n    'fit_intercept':[True,False]\n}\n\nkfold=model_selection.KFold(n_splits=10,shuffle=True,random_state=101)\n\nmodel=model_selection.RandomizedSearchCV(\n                        estimator=lasso,\n                        param_distributions=param_grid,\n                        n_iter=20,\n                        scoring='neg_mean_squared_error',\n                        cv=kfold,\n                        refit=True,\n                        verbose=5,\n                        n_jobs=-1\n                        )\n\nmodel.fit(X,y)\n\nprint('Best Scorer{}'.format(model.best_score_))\n\nprint('/n')\n\nprint('Best Parameters{}'.format(model.best_params_))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Implementing the Best Lasso Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":" \n\nlasso=Lasso(alpha=0.001,fit_intercept=True,normalize=False)\n\nkfold=model_selection.KFold(n_splits=10,shuffle=True,random_state=101)\n\nlasso_scores=[]\n\nlasso_scores=cross_val_score(lasso,X,y,cv=kfold,n_jobs=-1,scoring='neg_root_mean_squared_error')\n\n    \nprint('RMSE for Lasso Regression is {0:.5f} and variance of MSE is {1:.8f}'.format(-1*np.mean(lasso_scores),np.var(lasso_scores)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Most Linear Regression model gives us RMSE value close by so this can serve as good baseline model "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Linear model with polynomial features \n\nI feel polynomial features should be good as this will create additional features . Also limitting the degree of polynomial to 2 as since usually Power generated within an electrical component has an effect on the temperature of the component and power is usually a product of 2 terms. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nlasso_pipeline=Pipeline([\n    ('poly',PolynomialFeatures(2)),\n    ('scalar',StandardScaler())\n])\n\n\n\nX_poly=lasso_pipeline.fit_transform(X)\n\n# implementing Lasso Regression \n\nlasso=Lasso(alpha=0.001,fit_intercept=True,normalize=False)\n\nkfold=model_selection.KFold(n_splits=10,shuffle=True,random_state=101)\n\nlasso_scores=[]\n\nlasso_scores=cross_val_score(lasso,X_poly,y,cv=kfold,n_jobs=-1,scoring='neg_root_mean_squared_error')\n\n    \nprint('RMSE for Lasso Regression is {0:.5f} and variance of MSE is {1:.8f}'.format(-1*np.mean(lasso_scores),np.var(lasso_scores)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## KNN Regressor "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import neighbors\n\nknn=neighbors.KNeighborsRegressor()\n\nparam_grid={\n    'n_neighbors':np.arange(1,10),\n    'weights':['uniform', 'distance']   \n    \n}\n\nkfold=model_selection.KFold(n_splits=5,shuffle=True,random_state=101)\n\nmodel=model_selection.GridSearchCV(\n                        estimator=knn,\n                        param_grid=param_grid,\n                        scoring='neg_root_mean_squared_error',\n                        cv=kfold,\n                        refit=True,\n                        verbose=5,\n                        n_jobs=-1\n                        )\nmodel.fit(X,y)\n\nprint('Best Scorer{}'.format(model.best_score_))\n\nprint('/n')\n\nprint('Best Parameters{}'.format(model.best_params_))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Implementing the best KNN model "},{"metadata":{"trusted":true},"cell_type":"code","source":"knn=neighbors.KNeighborsRegressor(n_neighbors=3,weights='distance')\n\nkfold=model_selection.KFold(n_splits=10,shuffle=True,random_state=101)\n\nknn_scores=[]\n\nknn_scores=cross_val_score(knn,X,y,cv=kfold,n_jobs=-1,scoring='neg_root_mean_squared_error')\n\n    \nprint('RMSE for KNN Regression is {0:.5f} and variance of MSE is {1:.8f}'.format(-1*np.mean(knn_scores),np.var(knn_scores)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As the KNN is so far the best one using KNN with sklearn BaggingClassifier class ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.ensemble import BaggingRegressor\n#from sklearn import neighbors\n\n#knn_bag=BaggingRegressor(neighbors.KNeighborsRegressor(n_neighbors=3),\n                          #max_samples=1.0, \n                          #max_features=1.0,\n                         # oob_score=True,\n                          #bootstrap=True)\n\n#param_dist={\n#    'n_estimators':np.arange(10,120,10)\n#}\n\n#model=model_selection.GridSearchCV(\n#                        estimator=knn_bag,\n #                       param_grid=param_dist,\n#            \n #                       scoring='neg_mean_squared_error',\n#                        cv=5,\n  #                      refit=True,\n   #                     verbose=5,\n     #                   n_jobs=-1\n    #                    )\n\n\n#model.fit(X,y)\n\n#print('Best Scorer{}'.format(model.best_score_))\n\n#print('/n')\n\n#print('Best Parameters{}'.format(model.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random forest Regressor \n\n#from sklearn import ensemble\n\n#random_forest=ensemble.RandomForestRegressor(criterion='mse',random_state=101,n_jobs=-1)\n\n#param_distribution={\n#    'n_estimators':np.arange(100,500,50),\n#    'max_depth':[2,4,6,8,'None'], \n#    'min_samples_split':np.arange(2,20,2), \n#    'warm_start':[False,True]\n#}\n\n\n#kfold=model_selection.KFold(n_splits=10,shuffle=True,random_state=101)\n\n#model=model_selection.RandomizedSearchCV(\n #                       estimator=random_forest,\n #                       param_distributions=param_distribution,\n #                       n_iter=80,\n #                       scoring='neg_mean_squared_error',\n #                       cv=kfold,\n                       # refit=True,\n                       # verbose=5,\n                       # n_jobs=-1\n                       # )\n\n#model.fit(X,y)\n\n#print('Best Scorer{}'.format(model.best_score_))\n\n#print('/n')\n\n#print('Best Parameters{}'.format(model.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Working on XGboost \nimport xgboost as xgb \n\ndata_dmatrix = xgb.DMatrix(data=X,label=y)\n\nparams = {\"objective\":\"reg:squarederror\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n                'max_depth': 5, 'alpha': 10}\n\ncv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,\n                    num_boost_round=1500,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True, seed=123)\n\ncv_results.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Implementing pipeline \n#from sklearn.pipeline import Pipeline\n#from sklearn.ensemble import VotingRegressor\n#import xgboost as xgb \n\n\n#votingregressor = VotingRegressor([('lasso',Lasso(alpha=0.001,fit_intercept=True,normalize=False)), \n  #                    ('knn', neighbors.KNeighborsRegressor(n_neighbors=3,weights='distance')),\n  #                    ('xgb',xgb.XGBRegressor(objective='reg:squarederror',colsample_bytree=0.3,learning_rate=0.1,max_depth=5,alpha=10,n_rounds=1500))\n    #                  ])\n\n\n#vot_pipeline=Pipeline([\n  #  ('poly',PolynomialFeatures(2)),\n  #  ('scalar',StandardScaler()),\n  #  ('voting',votingregressor)\n#])\n\n#kfold=model_selection.KFold(n_splits=10,shuffle=True,random_state=101)\n\n#vot_scores=[]\n\n#vot_scores=cross_val_score(vot_pipeline,X,y,cv=kfold,n_jobs=-1,scoring='neg_root_mean_squared_error')\n\n#-1*np.mean(vot_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final Working Model for Predicting for Predicting Rotor Temperature "},{"metadata":{"trusted":true},"cell_type":"code","source":"KNN_rotor=neighbors.KNeighborsRegressor(n_neighbors=3,weights='distance')\n\n\nKNN_rotor.fit(X,y)\n\ny_rotor_predict=KNN_rotor.predict(X_test_rotor)\n\nprint(\"The RMSE for rotor temperature is {}\".format(metrics.mean_squared_error(y_test_rotor,y_rotor_predict)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Implementing Model for Stator tooth \nthe other stators temperature should be around this ballpark temperature\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_stator_train=X_train[['ambient','coolant','u_d','u_q','motor_speed','torque','i_d','i_q']]\ny_stator_train=X_train['stator_tooth']\n\ny_stator_test=X_test['stator_tooth']\nX_stator_test=X_test[['ambient','coolant','u_d','u_q','motor_speed','torque','i_d','i_q']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import neighbors\n\nknn=neighbors.KNeighborsRegressor()\n\nparam_grid={\n    'n_neighbors':np.arange(1,15),\n    'weights':['uniform', 'distance']   \n    \n}\n\nkfold=model_selection.KFold(n_splits=5,shuffle=True,random_state=101)\n\nstator_model=model_selection.GridSearchCV(\n                        estimator=knn,\n                        param_grid=param_grid,\n                        scoring='neg_root_mean_squared_error',\n                        cv=kfold,\n                        refit=True,\n                        verbose=5,\n                        n_jobs=-1\n                        )\nstator_model.fit(X_stator_train,y_stator_train)\n\nprint('Best Scorer{}'.format(stator_model.best_score_))\n\nprint('/n')\n\nprint('Best Parameters{}'.format(stator_model.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"KNN_stator=neighbors.KNeighborsRegressor(n_neighbors=3,weights='distance')\n\n\nKNN_stator.fit(X_stator_train,y_stator_train)\n\ny_stator_predict=KNN_stator.predict(X_stator_test)\n\nprint(\"The RMSE for rotor temperature is {}\".format(metrics.mean_squared_error(y_stator_test,y_stator_predict)))\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}