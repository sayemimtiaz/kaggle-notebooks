{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Convolutional Neural Network (CNN)\n\n<font color=\"blue\">**Content:** </font>\n* <font color=\"blue\">Learning:</font>\n    * [Convolutional Layer](#1)\n    * [Non-linearity](#2)\n    * [Pooling Layer](#3)\n    * [Flattening Layer](#4)\n    * [Fully-Connected Layer](#5)\n* <font color=\"blue\">Practice:</font>\n    * [Loading the Data Set](#6)\n    * [Normalization, Reshape and Label Encoding](#7)\n    * [Train Test Split](#8)\n\n---\n\nWhen you upload an image to Facebook, you ask \"Do you want to tag the X person?\" makes a proposition. So did you wonder how you knew that person?\nOr have you ever thought how Google's image search algorithm works?\nThere is a neural network behind all this. To be more precise, we are talking about the Convolutional Neural Network (ConvNet or CNN). . While CNN may seem like a strange mixture of biology and computer science, this is a very effective mechanism used for image recognition.\n\n![](https://miro.medium.com/max/2800/0*XQ8zWS1TO-QgU6sB.jpg)\n\nCnn use unique features that make a plane a plane or a snake to distinguish the images given. In fact, this process is also unconscious in our brains.\nFor example, when we look at an airplane picture, we can identify the airplane by separating the features such as two wings, engines, windows. Cnn does the same, but previously they detect low-level properties such as curves and edges and create them up to more abstract concepts.\n\nTo achieve the functionality we talked about, Cnn processes the image with various layers. Let's give an overview of these layers and their objectives:\n- **Convolutional Layer: ** Used to detect features \n- **Non-Linearity Layer: ** Introducing non-linearity to the system\n- **Pooling (Downsampling) Layer: ** Reduces weight count and checks fitness\n- **Flattening Layer: ** Prepares data for Classical Neural Network\n- **Fully-Connected Layer: ** Standard Neural Network used in classification\n\n![](https://adeshpande3.github.io/assets/Cover.png)\n\nBasically, Cnn uses the standard Neural Network to solve the classification problem, but uses other layers to identify information and identify some features.\nLet's dive into the details of each layer and its functions.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Convolutional Layer <a id=\"1\"></a>\nThis layer is the main building block of CNN. Responsible for perceiving the features of the picture. This layer applies some filters to the image to remove low and high level features in the image. For example, this filter may be a filter to detect edges. These filters are usually multi-dimensional and contain pixel values. (5x5x3) represent the height and width of 5 matrices, the depth of 3 matrices.\n\n<img src=\"https://lh3.googleusercontent.com/proxy/JMtwv40-TRsA0RZU0z07y8ICXS6QNHETJsty61fBKvg98tIxYPMhYhUHIStO4mem8r3kB0R3W9P0HMj7Oi8iBvh5422oRxJWqq2-Kj_tsCAT3DAagjLER7uGdd4-S_-RwbnKxnw9UEg\" width=\"150\" height=\"150\"/>\n\nFor example, only 1 channel will be processed for simplicity.\nLet's assume that the picture is a 5 × 5 dimension and consists of 1's and 0's. Let's create our filter in 3 × 3 dimensions.\n\n<img src=\"https://lh3.googleusercontent.com/dhMlStbHduqMth-g5mXnI4D2uRY4ScR9IVKxJJ9uZ0mSzaoWfLgM8u11YSPraHwZAAnhsw=s69\" width=\"75\" height=\"75\"/>\n\nNow let's take a look at how the filter is applied:\n\n<img src=\"https://eg.bucknell.edu/~cld028/courses/379-FA19/images/ConvEx.gif\" width=\"400\" height=\"400\"/>\n\nFirst, the filter is positioned in the upper left corner of the image. Here, the indices between the two matrices (image and filter) are multiplied by each other and all the results are summed, then the result is stored in the output matrix. Then move this filter to the right by 1 pixel (also known as “step”) and repeat the process. After the 1st line is over, the 2nd line is passed and the transactions are repeated. The output matrix is created after all operations are finished. The reason why the output matrix is 3 × 3 is because the 3 × 3 filter in the 5 × 5 matrix moves 3 times horizontally and vertically.\n\nIf the image was 6 × 4 and the filter was 3 × 3, the output matrix would be 4 × 2.\n\nSo what does the output matrix tell us? This matrix is often called Feature Map. Shows the location of the image in the property represented by the filter. In short, we detect our properties by moving the filter over the image and using the simple matrix product.\nUsually, multiple filters are used to detect multiple features, that is, a Cnn network has multiple Convolutional layers. Take a look at the animation below, here is a little more visually explained:\n\n<img src=\"https://www.apsl.net/media/apslweb/images/giphy.original.gif\" width=\"400\" height=\"215\"/>\n\nWhen we apply the first filter, we create a Feature Map and identify a feature type. Next, we create a second Feature Map that uses a second filter and detects another feature type.\nAs we can see in the example above, these filters can be simple, but they can become complicated if you want to extract some complex features in the image. You can browse the animation below to see more complex filters.\n\n<img src=\"https://cdn-images-1.medium.com/max/640/1*S1WhBMR7wLN4DViZTljQ5w.gif\" width=\"550\" height=\"200\"/>\n\nAnother thing we mentioned earlier, but not explained in detail, is stride.\n\nThis term is often used in conjunction with the term padding. Stride controls how the filter evolves around the input image. In the example above, Stride was 1 pixel, but it could be larger. This affects the size of the output of the Feature Map.\n\nIn the early stages of Cnn, we need to protect as much information as possible for other Convolutional Layers when applying the first filters. Here padding is used for this reason. You may have noticed that the Feature Map is smaller than the original input image. Therefore, Padding will add zero values to this map to maintain the size of the image (as in the image below):\n\n<img src=\"https://cdn-images-1.medium.com/max/1000/0*wD3fHOCzj9cWXt_6\" width=\"260\" height=\"250\"/>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Non-linearity  <a id=\"2\"></a>\nNon-Linearity layer usually comes after all Convolutional layers. So why is the linearity in the image a problem? The problem is that the Neural Network acts as a single perception, since all layers can be a linear function, so the result can be calculated as a linear combination of outputs. This layer is called Activation Layer because one of the activation functions is used. In the past, nonlinear functions such as sigmoid and tahn have been used, but this function is now being used as the Rectifier (ReLu) function gives the best result on the speed of Neural Network training.\n> **ReLu Function f(x) = max(0, x)**\n\nWhen the ReLu function is applied to the Feature Map, a result like below is produced.\n\n<img src=\"https://miro.medium.com/max/1240/0*AgDOXRGswiJSQ9mB\" width=\"650\" height=\"220\"/>\n\nBlack values ​​in the Feature Map are negative. After relu function is applied, black values are removed and 0 is replaced.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Pooling Layer  <a id=\"3\"></a>\nThis layer is a layer that is frequently added between consecutive convolutional layers in CovNet. The task of this layer is to reduce the shift size of the impression and the parameters and number of calculations within the network. In this way, incompatibility in the network is checked. There are many Pooling operations, but the most popular is max pooling. There are also average pooling, and L2-norm pooling algorithms working on the same principle.\nLet's go by explaining this process over the figures. First of all, let's create a 2×2 filter. You can see this filter on the picture below (4x4). As you can see in the picture, the filter gets the largest number in the area it covers. In this way, it uses smaller outputs that contain enough information for the neural network to make the right decision.\n\n<img src=\"https://www.oreilly.com/radar/wp-content/uploads/sites/3/2019/06/Figure_4-87c227113cdd0b73d842267404d4aa00.gif\" width=\"500\" height=\"300\"/>\n\n\n# Flattening Layer <a id=\"4\"></a>\nThe task of this layer is simply to prepare the data at the entrance of the last and most important layer, Fully Connected Layer. Generally, neural networks receive input data from a one-dimensional array. The data in this neural network is the one-dimensional array of matrixes from the Convolutional and Pooling layer.\n\n<img src=\"https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/73_blog_image_1.png\" width=\"900\" height=\"450\"/>\n\n\n# Fully-Connected Layer  <a id=\"5\"></a>\nThis layer is the last and most important layer of ConvNet. It takes the data from the flattening process and performs the learning process via the neural network.\n\n---","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading the Data Set <a id=\"6\"></a>\n* In this part we load and visualize the data.\n","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/sign-language-mnist/sign_mnist_train.csv\")\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({\n    'X': ['Train Shape','Different number of labels','Different number of labels (Sum)' ],\n    'Y': [train_df.shape, train_df.label.unique(), len(train_df.label.unique())],\n})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/sign-language-mnist/sign_mnist_test.csv\")\nprint(\"Test Shape: \", test_df.shape)\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_circle = plt.Circle( (0,0), 0.7, color='white')\nplt.pie([len(train_df),len(test_df)], labels=[\"Train\",\"Test\"], colors=['green','skyblue'])\np = plt.gcf()\np.gca().add_artist(my_circle)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I synchronize the numbers it represents to a variable in the form of an array\nY_train = train_df['label']\nY_test = test_df['label']\n\nX_train = train_df.drop([\"label\"],axis=1)\nX_test = test_df.drop([\"label\"],axis=1)\n\ndel train_df['label']\ndel test_df['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,7))\ng = sns.countplot(Y_train, palette=\"icefire\")\nplt.title(\"Number of digit classes\")\nY_train.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(2,4)\nf.set_size_inches(8,8)\n\nk = 0\nfor i in range(2):\n    for j in range(4):\n        img = X_train.iloc[k].to_numpy()\n        img = img.reshape((28,28))\n        ax[i,j].set_xlabel(chr(Y_train[k] + 65))\n        ax[i,j].imshow(img,cmap='gray')\n        k += 1\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Normalization, Reshape and Label Encoding <a id=\"7\"></a>\n* **Normalization**\n    * We perform a grayscale normalization to reduce the effect of illumination's differences.\n    * If we perform normalization, CNN works faster.\n* **Reshape**\n    * Train and test images (28 x 28)\n    * We reshape all data to 28x28x1 3D matrices.\n    * Keras needs an extra dimension in the end which correspond to channels. Our images are gray scaled so it use only one channel.\n* **Label Encoding**\n    * Encode labels to one hot vectors\n        * 2 => [0,0,1,0,0,0,0,0,0,0]\n        * 4 => [0,0,0,0,1,0,0,0,0,0]\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalize the data\nX_train = X_train/255.0\nX_test = X_test/255.0\nprint(\"X_train shape: \",X_train.shape)\nprint(\"X_test shape: \",X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshape\nX_train = X_train.values.reshape(-1,28,28,1)\nX_test = X_test.values.reshape(-1,28,28,1)\nprint(\"X_train shape: \",X_train.shape)\nprint(\"X_test shape: \",X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# label encoding\nfrom sklearn.preprocessing import LabelBinarizer\nlabel_binrizer = LabelBinarizer()\nY_train = label_binrizer.fit_transform(Y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Test Split <a id=\"8\"></a>\n* We split the data into train and test sets.\n* test size is 15%\n* train size is 85%","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.15, random_state=42)\n\nprint(\"x_train shape\",X_train.shape)\nprint(\"x_test shape\",X_val.shape)\nprint(\"y_train shape\",Y_train.shape)\nprint(\"y_test shape\",Y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(X_train[10][:,:,0],cmap=\"gray\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters=75 , kernel_size=(3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (28,28,1)))\nmodel.add(MaxPool2D(pool_size=(2,2), strides = 2 , padding = 'same'))\n\nmodel.add(Conv2D(filters=50, kernel_size=(3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides = 2 , padding = 'same'))\n\nmodel.add(Conv2D(filters=25, kernel_size=(3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(MaxPool2D(pool_size=(2,2) , strides = 2 , padding = 'same'))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(units = 512 , activation = 'relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(units = 24 , activation = 'softmax'))\nmodel.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = Adam(lr=0.003, beta_1=0.9, beta_2=0.999)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 25  # for better result increase the epochs\nbatch_size = 200","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data augmentation\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # dimesion reduction\n        rotation_range=15,  # randomly rotate images in the range 15 degrees\n        zoom_range = 0.5, # Randomly zoom image 5%\n        width_shift_range=0.15,  # randomly shift images horizontally 15%\n        height_shift_range=0.15,  # randomly shift images vertically 15%\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\ndatagen.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),epochs = epochs, validation_data = (X_val,Y_val), steps_per_epoch=X_train.shape[0] // batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### fit() & fit_genarator()\n* In keras, fit() is much similar to sklearn's fit method, where you pass array of features as x values and target as y values. You pass your whole dataset at once in fit method. Also, use it if you can load whole data into your memory (small dataset).\n* In fit_generator(), you don't pass the x and y directly, instead they come from a generator. As it is written in keras documentation, generator is used when you want to avoid duplicate data when using multiprocessing. This is for practical purpose, when you have large dataset.\n\n### Dropout\n* It prevents over learning.\n* It increases diversity.\n\n### Epoch & Batch & Number of Iteration\n* **one epoch: ** one forward pass and one backward pass of all the training examples\n* **batch size:** the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need.\n* **number of iterations:** number of passes, each pass using [batch size] number of examples. To be clear, one pass = one forward pass + one backward pass (we do not count the forward pass and backward pass as two different passes).\n\nExample: if you have 1000 training examples, and your batch size is 500, then it will take 2 iterations to complete 1 epoch.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nfig.set_size_inches(12,4)\n\nax[0].plot(history.history['accuracy'])\nax[0].plot(history.history['val_accuracy'])\nax[0].set_title('Training Accuracy vs Validation Accuracy')\nax[0].set_ylabel('Accuracy')\nax[0].set_xlabel('Epoch')\nax[0].legend(['Train', 'Validation'], loc='upper left')\n\nax[1].plot(history.history['loss'])\nax[1].plot(history.history['val_loss'])\nax[1].set_title('Training Loss vs Validation Loss')\nax[1].set_ylabel('Loss')\nax[1].set_xlabel('Epoch')\nax[1].legend(['Train', 'Validation'], loc='upper left')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n# Predict the values from the validation dataset\nY_pred = model.predict(X_val)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(Y_val,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nf,ax = plt.subplots(figsize=(16, 12))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=sns.cubehelix_palette(8),fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}