{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hello ðŸ™Œ, welcome to my notebook. In this notebook we will try to make Retail Analytics, Exploratory Data Analysis (EDA) and also develop and evaluate model to predict Sales. Feel free if you have any question or suggestion! Thank you!"},{"metadata":{},"cell_type":"markdown","source":"# Feature Defenition"},{"metadata":{},"cell_type":"markdown","source":"- Context: One challenge of modeling retail data is the need to make decisions based on limited history. Holidays and select major events come once a year, and so does the chance to see how strategic decisions impacted the bottom line. In addition, markdowns are known to affect sales â€“ the challenge is to predict which departments will be affected and to what extent.\n\n- Content: You are provided with historical sales data for 45 stores located in different regions - each store contains a number of departments. The company also runs several promotional markdown events throughout the year. These markdowns precede prominent holidays, the four largest of which are the Super Bowl, Labor Day, Thanksgiving, and Christmas. The weeks including these holidays are weighted five times higher in the evaluation than non-holiday weeks.\n\n1. Stores: Anonymized information about the 45 stores, indicating the type and size of store\n\n2. Features: Contains additional data related to the store, department, and regional activity for the given dates.\n\n    - Store - the store number\n    - Date - the week\n    - Temperature - average temperature in the region\n    - Fuel_Price - cost of fuel in the region\n    - MarkDown1-5 - anonymized data related to promotional markdowns. MarkDown data is only available after Nov 2011, and is not available for  all stores all the time. Any missing value is marked with an NA\n    - CPI - the consumer price index\n    - Unemployment - the unemployment rate\n    - IsHoliday - whether the week is a special holiday week\n\n\n3. Sales: Historical sales data, which covers to 2010-02-05 to 2012-11-01. Within this tab you will find the following fields:\n\n    - Store - the store number\n    - Dept - the department number\n    - Date - the week\n    - Weekly_Sales -  sales for the given department in the given store\n    - IsHoliday - whether the week is a special holiday week\n    \n\n- The Task\n\n    1. Predict the department-wide sales for each store for the following year\n    2. Model the effects of markdowns on holiday weeks\n    3. Provide recommended actions based on the insights drawn, with prioritization placed on largest business impact"},{"metadata":{},"cell_type":"markdown","source":"# Import Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', None)\n\ndata1 = pd.read_csv('../input/retaildataset/stores data-set.csv')\ndata2 = pd.read_csv('../input/retaildataset/Features data set.csv')\ndata3 = pd.read_csv('../input/retaildataset/sales data-set.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data3.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- First we import all the data available\n- And then take a preview about the data\n- For the first section, i want to take a deeper look about first data which is Store Data"},{"metadata":{},"cell_type":"markdown","source":"# Stores Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Nunique Columns'''\n\ndef nunique_counts(data):\n   for i in data.columns:\n       count = data[i].nunique()\n       print(i, \": \", count)\n    \nnunique_counts(data1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1['Type'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Missing Value Chart'''\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(5, 5))\ndata1.isnull().mean(axis=0).plot.barh()\nplt.title(\"Ratio of missing values per columns\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Checking Duplicate'''\n\nprint('Dupplicate entries: {}'.format(data1.duplicated().sum()))\n# data1.drop_duplicates(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.offline as py \npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go \nimport plotly.tools as tools\nimport warnings\nfrom collections import Counter \nimport plotly.express as px\n\nlabels = data1['Store'].tolist()\nvalues = data1['Size'].tolist()\n\nfig = px.pie(data1, values=values, names=labels)\nfig.update_traces(textposition='inside')\nfig.update_layout(uniformtext_minsize=12, uniformtext_mode='hide')\nfig['layout'].update(height=500, width=700, title='Store by Size')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"custom_aggregation = {}\ncustom_aggregation[\"Size\"] = \"mean\"\ndata0 = data1.groupby(\"Type\").agg(custom_aggregation)\ndata0.columns = [\"Size\"]\ndata0['Type'] = data0.index\n\nfig = px.bar(data0, x='Type', y=\"Size\", color=\"Type\")\nfig['layout'].update(height=400, width=550, title='Size of Type Store')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"custom_aggregation = {}\ncustom_aggregation[\"Store\"] = \"count\"\ndata0 = data1.groupby(\"Type\").agg(custom_aggregation)\ndata0.columns = [\"Store Count\"]\ndata0['Type'] = data0.index\n\nfig = px.bar(data0, x='Type', y=\"Store Count\", color=\"Type\")\nfig['layout'].update(height=400, width=550, title='Nb. of Store Based on Type Store')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- From Store by Size graph, we can see that the majority of the store have similar size around(1,77% - 3,75%)\n- Typa A store have store count more that other type store (B and C), and also have the largest size"},{"metadata":{},"cell_type":"markdown","source":"# Features Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nunique_counts(data2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2['IsHoliday'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 5))\ndata2.isnull().mean(axis=0).plot.barh()\nplt.title(\"Ratio of missing values per columns\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2.isnull().sum().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2[data2.isnull().T.any().T].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2['Date'] = pd.to_datetime(data2['Date']) #datetime format\ndata3['Date'] = pd.to_datetime(data3['Date']) #datetime format\ndata2['Year'] = data2['Date'].dt.year # getting year\ndata2['Month'] = data2['Date'].dt.month # getting month\ndata2['Day'] = data2['Date'].dt.day # getting day","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Equidistance Checking'''\n\ndata0 = data2.sort_values(by='Date')\ndata0['Time_Interval'] = data0.Date - data0.Date.shift(1)\ndata0[['Date', 'Time_Interval']].head()\n\nprint(f\"{data0['Time_Interval'].value_counts().sort_values(ascending=False)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2['Date'].min(), data2['Date'].max()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- After checking missing values, the most feature which have missing values are Markdown 1-5. Also Unemploymnet and CPI\n- For now I'll leave the data as it is\n- And for the Date, i convert it to datetimeformat. And also generate Year, Month and Day for further modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from plotly.subplots import make_subplots\n\nfig = make_subplots(rows=1, \n                    cols=1)\n\ncustom_aggregation = {}\ncustom_aggregation[\"CPI\"] = \"mean\"\ndata2 = data2.set_index(pd.DatetimeIndex(data2['Date']))\ndata0 = data2.resample('M').agg(custom_aggregation)\ndata0.columns = [\"CPI\"]\ndata0['Date'] = data0.index\n\nx = data0['Date'].tolist()\ny = data0['CPI'].tolist()\ny_ = data0['CPI'].interpolate().tolist()\n\nfig.add_trace(go.Scatter(x=x, y=y,name='CPI'), 1, 1)\nfig.add_trace(go.Scatter(x=x, y=y_,mode='markers',name='Interpolate CPI'), 1, 1)\n\nfig['layout'].update(height=500, width=900, title='Monthly Consumer Price Index (Average)')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=1, \n                    cols=1)\n\ncustom_aggregation = {}\ncustom_aggregation[\"Unemployment\"] = \"mean\"\ndata2 = data2.set_index(pd.DatetimeIndex(data2['Date']))\ndata0 = data2.resample('M').agg(custom_aggregation)\ndata0.columns = [\"Unemployment\"]\ndata0['Date'] = data0.index\n\nx = data0['Date'].tolist()\ny = data0['Unemployment'].tolist()\ny_ = data0['Unemployment'].interpolate().tolist()\n\nfig.add_trace(go.Scatter(x=x, y=y,name='Unemployment',line=dict(color='firebrick', width=2)), 1, 1)\nfig.add_trace(go.Scatter(x=x, y=y_,mode='markers',name='Interpolate Unemployment'), 1, 1)\n\nfig['layout'].update(height=500, width=900, title='Monthly Unemployment Index (Average)')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=1, \n                    cols=1)\n\ncustom_aggregation = {}\ncustom_aggregation[\"Temperature\"] = \"mean\"\ndata2 = data2.set_index(pd.DatetimeIndex(data2['Date']))\ndata0 = data2.resample('M').agg(custom_aggregation)\ndata0.columns = [\"Temperature\"]\ndata0['Date'] = data0.index\n\nx = data0['Date'].tolist()\ny = data0['Temperature'].tolist()\ny_ = data0['Temperature'].interpolate().tolist()\n\nfig.add_trace(go.Scatter(x=x, y=y,name='Temperature',line=dict(color='green', width=2)), 1, 1)\nfig.add_trace(go.Scatter(x=x, y=y_,mode='markers',name='Interpolate Temperature'), 1, 1)\n\nfig['layout'].update(height=500, width=900, title='Monthly Temperature (Average)')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=1, \n                    cols=1)\n\ncustom_aggregation = {}\ncustom_aggregation[\"Fuel_Price\"] = \"mean\"\ndata2 = data2.set_index(pd.DatetimeIndex(data2['Date']))\ndata0 = data2.resample('M').agg(custom_aggregation)\ndata0.columns = [\"Fuel_Price\"]\ndata0['Date'] = data0.index\n\nx = data0['Date'].tolist()\ny = data0['Fuel_Price'].tolist()\ny_ = data0['Fuel_Price'].interpolate().tolist()\n\nfig.add_trace(go.Scatter(x=x, y=y,name='Fuel_Price',line=dict(color='pink', width=2)), 1, 1)\nfig.add_trace(go.Scatter(x=x, y=y_,mode='markers',name='Interpolate Fuel_Price'), 1, 1)\n\nfig['layout'].update(height=500, width=900, title='Monthly Fuel_Price (Average)')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The idea of what i'm doing is: i try to resample the date to monthly basis to make visualization much clear\n- We can see that the original data contain Nan data, and i use interpolate to complement the visualization.\n- I prefer to use interpolate compared to mean, median etc\n- As we can see, monthly Consumer Price Index (CPI) is increasing every month.\n\n'Consumer Price Index (CPI) is a measure that examines the weighted average of prices of a basket of consumer goods and services, such as transportation, food, and medical care. It is calculated by taking price changes for each item in the predetermined basket of goods and averaging them.' (www.investopedia.com)\n\n- For the unemployment, is decrease every month.\n- Temperatures vary each year, usually reaching the highest peak in May - August and the lowest in Nov - Feb.\n- For fuel prices experienced a significant increase in January 2011 and did not experience a decline thereafter. In fact, the price tends to increase every month."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=2, \n                    cols=1)\n\ncustom_aggregation = {}\ncustom_aggregation[\"MarkDown1\"] = \"mean\"\ndata2 = data2.set_index(pd.DatetimeIndex(data2['Date']))\ndata0 = data2.resample('M').agg(custom_aggregation)\ndata0.columns = [\"MarkDown1\"]\ndata0['Date'] = data0.index\n\nx = data0['Date'].tolist()\ny = data0['MarkDown1'].tolist()\ny_ = data0['MarkDown1'].interpolate().tolist()\n\nfig.add_trace(go.Scatter(x=x, y=y,name='MarkDown1',line=dict(color='darkorchid', width=2)), 1, 1)\nfig.add_trace(go.Scatter(x=x, y=y_,mode='markers',name='Interpolate MarkDown1'), 1, 1)\n\n#-----------------------------------------------------------------------------------------------------\n\ndata3_0 = data3.loc[data3['Weekly_Sales'] > 0]\ncustom_aggregation = {}\ncustom_aggregation[\"Weekly_Sales\"] = \"mean\"\ndata3_0 = data3_0.set_index(pd.DatetimeIndex(data3_0['Date']))\ndata0 = data3_0.resample('M').agg(custom_aggregation)\ndata0.columns = [\"Monthly Profit Sales\"]\ndata0['Date_'] = data0.index\n\nx = data0['Date_'].tolist()\ny = data0['Monthly Profit Sales'].tolist()\n\nfig.add_trace(go.Scatter(x=x, y=y,name='Profit Sales',line=dict(color='darkgreen', width=2)), 1, 1)\n\n#-----------------------------------------------------------------------------------------------------\n\n# Crosstabbing\ndata2 = data2.reset_index(drop=True)\nfeature = [\"IsHoliday\", 'Date']\ndata0 = pd.crosstab(data2[feature[0]], data2[feature[1]]).T\ndata0['Date_'] = data0.index\ndata0.columns = [\"Not Holiday\", 'Is Holiday', 'Date_']\n\n# Is Holiday\nfeature = [\"Is Holiday\", 'Date_']\ndata_a = pd.crosstab(data0[feature[0]], data0[feature[1]]).T\ndata_a['Date_'] = data_a.index\ndata_a.columns = [\"No\", 'Yes', 'Date_']\n\ncustom_aggregation = {}\ncustom_aggregation[\"No\"] = \"sum\"\ncustom_aggregation[\"Yes\"] = \"sum\"\ndata_a = data_a.resample('M').agg(custom_aggregation)\ndata_a['Date_'] = data_a.index\n\nfig.add_trace(go.Bar(\n            x = data_a['Date_'],\n            y = data_a['Yes'],\n            name='Is Holiday'), 2, 1)\n\nfig.add_trace(go.Bar(\n            x = data_a['Date_'],\n            y = data_a['No'],\n            name='Not Holiday'), 2, 1)\n\nfig['layout'].update(height=700, width=900, title='Monthly MarkDown1 (Average)')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=2, \n                    cols=1)\n\ncustom_aggregation = {}\ncustom_aggregation[\"MarkDown2\"] = \"mean\"\ndata2 = data2.set_index(pd.DatetimeIndex(data2['Date']))\ndata0 = data2.resample('M').agg(custom_aggregation)\ndata0.columns = [\"MarkDown2\"]\ndata0['Date'] = data0.index\n\nx = data0['Date'].tolist()\ny = data0['MarkDown2'].tolist()\ny_ = data0['MarkDown2'].interpolate().tolist()\n\nfig.add_trace(go.Scatter(x=x, y=y,name='MarkDown2',line=dict(color='turquoise', width=2)), 1, 1)\nfig.add_trace(go.Scatter(x=x, y=y_,mode='markers',name='Interpolate MarkDown2'), 1, 1)\n\n#-----------------------------------------------------------------------------------------------------\n\ndata3_0 = data3.loc[data3['Weekly_Sales'] > 0]\ncustom_aggregation = {}\ncustom_aggregation[\"Weekly_Sales\"] = \"mean\"\ndata3_0 = data3_0.set_index(pd.DatetimeIndex(data3_0['Date']))\ndata0 = data3_0.resample('M').agg(custom_aggregation)\ndata0.columns = [\"Monthly Profit Sales\"]\ndata0['Date_'] = data0.index\n\nx = data0['Date_'].tolist()\ny = data0['Monthly Profit Sales'].tolist()\n\nfig.add_trace(go.Scatter(x=x, y=y,name='Profit Sales',line=dict(color='darkgreen', width=2)), 1, 1)\n\n\n#-----------------------------------------------------------------------------------------------------\n\n# Crosstabbing\ndata2 = data2.reset_index(drop=True)\nfeature = [\"IsHoliday\", 'Date']\ndata0 = pd.crosstab(data2[feature[0]], data2[feature[1]]).T\ndata0['Date_'] = data0.index\ndata0.columns = [\"Not Holiday\", 'Is Holiday', 'Date_']\n\n# Is Holiday\nfeature = [\"Is Holiday\", 'Date_']\ndata_a = pd.crosstab(data0[feature[0]], data0[feature[1]]).T\ndata_a['Date_'] = data_a.index\ndata_a.columns = [\"No\", 'Yes', 'Date_']\n\ncustom_aggregation = {}\ncustom_aggregation[\"No\"] = \"sum\"\ncustom_aggregation[\"Yes\"] = \"sum\"\ndata_a = data_a.resample('M').agg(custom_aggregation)\ndata_a['Date_'] = data_a.index\n\nfig.add_trace(go.Bar(\n            x = data_a['Date_'],\n            y = data_a['Yes'],\n            name='Is Holiday'), 2, 1)\n\nfig.add_trace(go.Bar(\n            x = data_a['Date_'],\n            y = data_a['No'],\n            name='Not Holiday'), 2, 1)\n\nfig['layout'].update(height=700, width=900, title='Monthly MarkDown2 (Average)')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=2, \n                    cols=1)\n\ncustom_aggregation = {}\ncustom_aggregation[\"MarkDown3\"] = \"mean\"\ndata2 = data2.set_index(pd.DatetimeIndex(data2['Date']))\ndata0 = data2.resample('M').agg(custom_aggregation)\ndata0.columns = [\"MarkDown3\"]\ndata0['Date'] = data0.index\n\nx = data0['Date'].tolist()\ny = data0['MarkDown3'].tolist()\ny_ = data0['MarkDown3'].interpolate().tolist()\n\nfig.add_trace(go.Scatter(x=x, y=y,name='MarkDown3',line=dict(color='peru', width=2)), 1, 1)\nfig.add_trace(go.Scatter(x=x, y=y_,mode='markers',name='Interpolate MarkDown3'), 1, 1)\n\n#-----------------------------------------------------------------------------------------------------\n\ndata3_0 = data3.loc[data3['Weekly_Sales'] > 0]\ncustom_aggregation = {}\ncustom_aggregation[\"Weekly_Sales\"] = \"mean\"\ndata3_0 = data3_0.set_index(pd.DatetimeIndex(data3_0['Date']))\ndata0 = data3_0.resample('M').agg(custom_aggregation)\ndata0.columns = [\"Monthly Profit Sales\"]\ndata0['Date_'] = data0.index\n\nx = data0['Date_'].tolist()\ny = data0['Monthly Profit Sales'].tolist()\n\nfig.add_trace(go.Scatter(x=x, y=y,name='Profit Sales',line=dict(color='darkgreen', width=2)), 1, 1)\n\n\n#-----------------------------------------------------------------------------------------------------\n\n# Crosstabbing\ndata2 = data2.reset_index(drop=True)\nfeature = [\"IsHoliday\", 'Date']\ndata0 = pd.crosstab(data2[feature[0]], data2[feature[1]]).T\ndata0['Date_'] = data0.index\ndata0.columns = [\"Not Holiday\", 'Is Holiday', 'Date_']\n\n# Is Holiday\nfeature = [\"Is Holiday\", 'Date_']\ndata_a = pd.crosstab(data0[feature[0]], data0[feature[1]]).T\ndata_a['Date_'] = data_a.index\ndata_a.columns = [\"No\", 'Yes', 'Date_']\n\ncustom_aggregation = {}\ncustom_aggregation[\"No\"] = \"sum\"\ncustom_aggregation[\"Yes\"] = \"sum\"\ndata_a = data_a.resample('M').agg(custom_aggregation)\ndata_a['Date_'] = data_a.index\n\nfig.add_trace(go.Bar(\n            x = data_a['Date_'],\n            y = data_a['Yes'],\n            name='Is Holiday'), 2, 1)\n\nfig.add_trace(go.Bar(\n            x = data_a['Date_'],\n            y = data_a['No'],\n            name='Not Holiday'), 2, 1)\n\nfig['layout'].update(height=700, width=900, title='Monthly MarkDown3 (Average)')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=2, \n                    cols=1)\n\ncustom_aggregation = {}\ncustom_aggregation[\"MarkDown4\"] = \"mean\"\ndata2 = data2.set_index(pd.DatetimeIndex(data2['Date']))\ndata0 = data2.resample('M').agg(custom_aggregation)\ndata0.columns = [\"MarkDown4\"]\ndata0['Date'] = data0.index\n\nx = data0['Date'].tolist()\ny = data0['MarkDown4'].tolist()\ny_ = data0['MarkDown4'].interpolate().tolist()\n\nfig.add_trace(go.Scatter(x=x, y=y,name='MarkDown4',line=dict(color='rosybrown', width=2)), 1, 1)\nfig.add_trace(go.Scatter(x=x, y=y_,mode='markers',name='Interpolate MarkDown4'), 1, 1)\n\n#-----------------------------------------------------------------------------------------------------\n\ndata3_0 = data3.loc[data3['Weekly_Sales'] > 0]\ncustom_aggregation = {}\ncustom_aggregation[\"Weekly_Sales\"] = \"mean\"\ndata3_0 = data3_0.set_index(pd.DatetimeIndex(data3_0['Date']))\ndata0 = data3_0.resample('M').agg(custom_aggregation)\ndata0.columns = [\"Monthly Profit Sales\"]\ndata0['Date_'] = data0.index\n\nx = data0['Date_'].tolist()\ny = data0['Monthly Profit Sales'].tolist()\n\nfig.add_trace(go.Scatter(x=x, y=y,name='Profit Sales',line=dict(color='darkgreen', width=2)), 1, 1)\n\n\n#-----------------------------------------------------------------------------------------------------\n\n# Crosstabbing\ndata2 = data2.reset_index(drop=True)\nfeature = [\"IsHoliday\", 'Date']\ndata0 = pd.crosstab(data2[feature[0]], data2[feature[1]]).T\ndata0['Date_'] = data0.index\ndata0.columns = [\"Not Holiday\", 'Is Holiday', 'Date_']\n\n# Is Holiday\nfeature = [\"Is Holiday\", 'Date_']\ndata_a = pd.crosstab(data0[feature[0]], data0[feature[1]]).T\ndata_a['Date_'] = data_a.index\ndata_a.columns = [\"No\", 'Yes', 'Date_']\n\ncustom_aggregation = {}\ncustom_aggregation[\"No\"] = \"sum\"\ncustom_aggregation[\"Yes\"] = \"sum\"\ndata_a = data_a.resample('M').agg(custom_aggregation)\ndata_a['Date_'] = data_a.index\n\nfig.add_trace(go.Bar(\n            x = data_a['Date_'],\n            y = data_a['Yes'],\n            name='Is Holiday'), 2, 1)\n\nfig.add_trace(go.Bar(\n            x = data_a['Date_'],\n            y = data_a['No'],\n            name='Not Holiday'), 2, 1)\n\nfig['layout'].update(height=700, width=900, title='Monthly MarkDown4 (Average)')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=2, \n                    cols=1)\n\ncustom_aggregation = {}\ncustom_aggregation[\"MarkDown5\"] = \"mean\"\ndata2 = data2.set_index(pd.DatetimeIndex(data2['Date']))\ndata0 = data2.resample('M').agg(custom_aggregation)\ndata0.columns = [\"MarkDown5\"]\ndata0['Date'] = data0.index\n\nx = data0['Date'].tolist()\ny = data0['MarkDown5'].tolist()\ny_ = data0['MarkDown5'].interpolate().tolist()\n\nfig.add_trace(go.Scatter(x=x, y=y,name='MarkDown5',line=dict(color='navy', width=2)), 1, 1)\nfig.add_trace(go.Scatter(x=x, y=y_,mode='markers',name='Interpolate MarkDown5'), 1, 1)\n\n#-----------------------------------------------------------------------------------------------------\n\ndata3_0 = data3.loc[data3['Weekly_Sales'] > 0]\ncustom_aggregation = {}\ncustom_aggregation[\"Weekly_Sales\"] = \"mean\"\ndata3_0 = data3_0.set_index(pd.DatetimeIndex(data3_0['Date']))\ndata0 = data3_0.resample('M').agg(custom_aggregation)\ndata0.columns = [\"Monthly Profit Sales\"]\ndata0['Date_'] = data0.index\n\nx = data0['Date_'].tolist()\ny = data0['Monthly Profit Sales'].tolist()\n\nfig.add_trace(go.Scatter(x=x, y=y,name='Profit Sales',line=dict(color='darkgreen', width=2)), 1, 1)\n\n#-----------------------------------------------------------------------------------------------------\n\n# Crosstabbing\ndata2 = data2.reset_index(drop=True)\nfeature = [\"IsHoliday\", 'Date']\ndata0 = pd.crosstab(data2[feature[0]], data2[feature[1]]).T\ndata0['Date_'] = data0.index\ndata0.columns = [\"Not Holiday\", 'Is Holiday', 'Date_']\n\n# Is Holiday\nfeature = [\"Is Holiday\", 'Date_']\ndata_a = pd.crosstab(data0[feature[0]], data0[feature[1]]).T\ndata_a['Date_'] = data_a.index\ndata_a.columns = [\"No\", 'Yes', 'Date_']\n\ncustom_aggregation = {}\ncustom_aggregation[\"No\"] = \"sum\"\ncustom_aggregation[\"Yes\"] = \"sum\"\ndata_a = data_a.resample('M').agg(custom_aggregation)\ndata_a['Date_'] = data_a.index\n\nfig.add_trace(go.Bar(\n            x = data_a['Date_'],\n            y = data_a['Yes'],\n            name='Is Holiday'), 2, 1)\n\nfig.add_trace(go.Bar(\n            x = data_a['Date_'],\n            y = data_a['No'],\n            name='Not Holiday'), 2, 1)\n\nfig['layout'].update(height=700, width=900, title='Monthly MarkDown5 (Average)')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- I counted the total week which is special week and visualize it in purple bar chart below the markdown chart\n- And also plot the sales which overlay the markdown so that we can understand the relatiobship between sales, markdown and special week\n- I make different visualization in every markdown\n1. Markdown 1: The highest amount of markdown is in February - March 2012, but in that time there is no special week. Also the sales tends to stagnate. In Oct, Nov, Dec have 1 specal week but there is no markdown on that time.\n2. Markdown 2: On Nov - Dec 2011 there are lots of special week, and the markdown also high. The sales experienced a significant increase at that time.\n3. Markdown 3: On this type of Markdown, there are only 2 significant peaks which is in November 2011 and November 2012. In that month there is also a special week. But sales only increased significantly in November 2011.\n4. Markdown 4: The highest markdown given is in Februari - March 2012 but not followed with occurrence of special week\n5. Markdown 5: The amount of markdown in this type is lower than the others"},{"metadata":{},"cell_type":"markdown","source":"# Sales Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data3.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data3.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data3.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nunique_counts(data3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 5))\ndata3.isnull().mean(axis=0).plot.barh()\nplt.title(\"Ratio of missing values per columns\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Checking Duplicate'''\n\nprint('Dupplicate entries: {}'.format(data3.duplicated().sum()))\n# data3.drop_duplicates(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=1, \n                    cols=1)\n\ndata3_0 = data3.loc[data3['Weekly_Sales'] < 0]\ncustom_aggregation = {}\ncustom_aggregation[\"Weekly_Sales\"] = \"mean\"\ndata3_0 = data3_0.set_index(pd.DatetimeIndex(data3_0['Date']))\ndata0 = data3_0.resample('M').agg(custom_aggregation)\ndata0.columns = [\"Monthly Lose Sales\"]\ndata0['Date_'] = data0.index\n\nx = data0['Date_'].tolist()\ny = data0['Monthly Lose Sales'].tolist()\n\nfig.add_trace(go.Scatter(x=x, y=y,name='Lose Sales',line=dict(color='red', width=2)), 1, 1)\n\nfig['layout'].update(height=500, width=900, title='Overall Monthly Lose Sales')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data3_0 = data3.loc[data3['Weekly_Sales'] < 0]\ndata3_0 = data3_0.rename(columns={'Date':'Date_'}, inplace=False)\n\ncustom_aggregation = {}\ncustom_aggregation[\"Weekly_Sales\"] = \"mean\"\ndata0 = data3_0.groupby(\"Store\").agg(custom_aggregation)\ndata0.columns = [\"Lose Sales\"]\ndata0['Store'] = data0.index\ndata0['Lose Sales'] = abs(data0['Lose Sales'])\n\nStore = data0['Store'].tolist()\nSales = data0['Lose Sales'].tolist()\n\nfig = px.pie(data0, values=Sales, names=Store)\nfig.update_traces(textposition='inside')\nfig.update_layout(uniformtext_minsize=12, uniformtext_mode='hide')\nfig['layout'].update(height=500, width=700, title='Lose Sales by Store')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data3_0 = data3.loc[data3['Weekly_Sales'] < 0]\ndata3_0 = data3_0.rename(columns={'Date':'Date_'}, inplace=False)\n\ncustom_aggregation = {}\ncustom_aggregation[\"Weekly_Sales\"] = \"mean\"\ndata0 = data3_0.groupby(\"Dept\").agg(custom_aggregation)\ndata0.columns = [\"Lose Sales\"]\ndata0['Dept'] = data0.index\ndata0['Lose Sales'] = abs(data0['Lose Sales'])\n\nDept = data0['Dept'].tolist()\nSales = data0['Lose Sales'].tolist()\n\nfig = px.pie(data0, values=Sales, names=Dept)\nfig.update_traces(textposition='inside')\nfig.update_layout(uniformtext_minsize=12, uniformtext_mode='hide')\nfig['layout'].update(height=500, width=700, title='Lose Sales by Dept')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=1, \n                    cols=1)\n\ndata3_0 = data3.loc[data3['Weekly_Sales'] > 0]\ncustom_aggregation = {}\ncustom_aggregation[\"Weekly_Sales\"] = \"mean\"\ndata3_0 = data3_0.set_index(pd.DatetimeIndex(data3_0['Date']))\ndata0 = data3_0.resample('M').agg(custom_aggregation)\ndata0.columns = [\"Monthly Profit Sales\"]\ndata0['Date_'] = data0.index\n\nx = data0['Date_'].tolist()\ny = data0['Monthly Profit Sales'].tolist()\n\nfig.add_trace(go.Scatter(x=x, y=y,name='Profit Sales',line=dict(color='darkgreen', width=2)), 1, 1)\n\nfig['layout'].update(height=500, width=900, title='Overall Monthly Profit Sales')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data3_0 = data3.loc[data3['Weekly_Sales'] > 0]\ndata3_0 = data3_0.rename(columns={'Date':'Date_'}, inplace=False)\n\ncustom_aggregation = {}\ncustom_aggregation[\"Weekly_Sales\"] = \"mean\"\ndata0 = data3_0.groupby(\"Store\").agg(custom_aggregation)\ndata0.columns = [\"Profit Sales\"]\ndata0['Store'] = data0.index\n\nStore = data0['Store'].tolist()\nSales = data0['Profit Sales'].tolist()\n\nfig = px.pie(data0, values=Sales, names=Store)\nfig.update_traces(textposition='inside')\nfig.update_layout(uniformtext_minsize=12, uniformtext_mode='hide')\nfig['layout'].update(height=500, width=700, title='Profit Sales by Store')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data3_0 = data3.loc[data3['Weekly_Sales'] > 0]\ndata3_0 = data3_0.rename(columns={'Date':'Date_'}, inplace=False)\n\ncustom_aggregation = {}\ncustom_aggregation[\"Weekly_Sales\"] = \"mean\"\ndata0 = data3_0.groupby(\"Dept\").agg(custom_aggregation)\ndata0.columns = [\"Profit Sales\"]\ndata0['Dept'] = data0.index\n\nDept = data0['Dept'].tolist()\nSales = data0['Profit Sales'].tolist()\n\nfig = px.pie(data0, values=Sales, names=Dept)\nfig.update_traces(textposition='inside')\nfig.update_layout(uniformtext_minsize=12, uniformtext_mode='hide')\nfig['layout'].update(height=500, width=700, title='Profit Sales by Dept')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=1, \n                    cols=1)\n\n# Bad performance store\ndata3_0 = data3.loc[data3['Store'] == 28]\ncustom_aggregation = {}\ncustom_aggregation[\"Weekly_Sales\"] = \"mean\"\ndata3_0 = data3_0.set_index(pd.DatetimeIndex(data3_0['Date']))\ndata0 = data3_0.resample('M').agg(custom_aggregation)\ndata0.columns = [\"Monthly Profit Sales\"]\ndata0['Date_'] = data0.index\n\nx = data0['Date_'].tolist()\ny = data0['Monthly Profit Sales'].tolist()\n\n# Good performance store\ndata3_0 = data3.loc[data3['Store'] == 20]\ncustom_aggregation = {}\ncustom_aggregation[\"Weekly_Sales\"] = \"mean\"\ndata3_0 = data3_0.set_index(pd.DatetimeIndex(data3_0['Date']))\ndata0 = data3_0.resample('M').agg(custom_aggregation)\ndata0.columns = [\"Monthly Profit Sales\"]\ndata0['Date_'] = data0.index\n\nx_ = data0['Date_'].tolist()\ny_ = data0['Monthly Profit Sales'].tolist()\n\n\nfig.add_trace(go.Scatter(x=x, y=y,name='Store 28 (Bad Perf.)',line=dict(color='red', width=2)), 1, 1)\nfig.add_trace(go.Scatter(x=x_, y=y_,name='Store 20 (Good Perf.)',line=dict(color='darkgreen', width=2)), 1, 1)\n\nfig['layout'].update(height=500, width=900, title='Compared Good and Bad Store Performance based on Monthly Profit Sales')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=1, \n                    cols=1)\n\n# Bad performance store\ndata3_0 = data3.loc[data3['Dept'] == 32]\ncustom_aggregation = {}\ncustom_aggregation[\"Weekly_Sales\"] = \"mean\"\ndata3_0 = data3_0.set_index(pd.DatetimeIndex(data3_0['Date']))\ndata0 = data3_0.resample('M').agg(custom_aggregation)\ndata0.columns = [\"Monthly Profit Sales\"]\ndata0['Date_'] = data0.index\n\nx = data0['Date_'].tolist()\ny = data0['Monthly Profit Sales'].tolist()\n\n# Good performance store\ndata3_0 = data3.loc[data3['Dept'] == 92]\ncustom_aggregation = {}\ncustom_aggregation[\"Weekly_Sales\"] = \"mean\"\ndata3_0 = data3_0.set_index(pd.DatetimeIndex(data3_0['Date']))\ndata0 = data3_0.resample('M').agg(custom_aggregation)\ndata0.columns = [\"Monthly Profit Sales\"]\ndata0['Date_'] = data0.index\n\nx_ = data0['Date_'].tolist()\ny_ = data0['Monthly Profit Sales'].tolist()\n\n\nfig.add_trace(go.Scatter(x=x, y=y,name='Dept 32 (Bad Perf.)',line=dict(color='red', width=2)), 1, 1)\nfig.add_trace(go.Scatter(x=x_, y=y_,name='Dept 92 (Good Perf.)',line=dict(color='darkgreen', width=2)), 1, 1)\n\nfig['layout'].update(height=500, width=900, title='Compared Good and Bad Dept Performance based on Monthly Profit Sales')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- I seperate the lose sales (negative) and the profit sales (positive). This is so that we can clearly see in what month the biggest loss occurred, because when we do aggregation using the mean there will be bias in the visualization. \n- I also find out what store and dept both giving big loses or profit\n- After it i also compare both store and dept which giving the highest proft and the lowest profit.\n- For the monthly loses, we can see that Feb, Aug, Sep which is usually the most significant sales decrease occurs.\n- The worst performance is Store number 28 (-16.8%), and Dept number 32 (-19.35%)\n- The best performance is Store number 20 (+4.25%), and Dept number 92 (+6.61%)\n- For Store number 28 have size 3.52% and Store number 20 3.48%. We can see that the size of the two store is not too different, but the difference in sales about 10K. It means that, the management shoult take aware of this thing, why Store number 28 hav very bad performance, while the operational costs incurred are not much different from those of the best performing stores.\n- And for Dept number 32 and 92 have very significant different in sales, which is about 50K\n- Need further inspection about Dept number 32, because it give very low profit in the business."},{"metadata":{},"cell_type":"markdown","source":"# Predicting Sales"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = data1.merge(data2).merge(data3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 5))\nall_data.isnull().mean(axis=0).plot.barh()\nplt.title(\"Ratio of missing values per columns\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['Date'].min(), all_data['Date'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testing = all_data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testing.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testing.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = testing.columns.values.tolist()\n\nfor col in features:\n    testing[col] = testing[col].interpolate()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testing.isnull().sum().sort_values(ascending=False) #missing values culomns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testing.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Checking Duplicate'''\n\nprint('Dupplicate entries: {}'.format(testing.duplicated().sum()))\n# data1.drop_duplicates(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testing = testing.drop('Date', axis=1, inplace=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nunique_counts(testing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Label Encoding With Label'''\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nle.fit(testing['Type'])\n\ntesting['Type'] = le.transform(testing['Type'])\n\nl = [i for i in range(3)]\ndict(zip(list(le.classes_), l))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le.fit(testing['IsHoliday'])\n\ntesting['IsHoliday'] = le.transform(testing['IsHoliday'])\n\nl = [i for i in range(2)]\ndict(zip(list(le.classes_), l))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Min Max Scaler'''\n\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef normalize_col(data):\n    for col in testing.columns.values.tolist():\n        scaler = MinMaxScaler()\n        data[col] = scaler.fit_transform(data[col].values.reshape(-1,1))\n    return data\n\ntesting = normalize_col(testing)\ntesting.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testing.corr()['Weekly_Sales'].sort_values(ascending=False)[:8]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The next step is modelling, from this case it categorized as Regression problem. And i will try using Logistic Regression\n- I combine all the data\n- And the for the data we will use from 2010-01-10 until 2012-12-10\n- For the missing values we know that Markdown feature have so mach missing values. Instead to drop it, or fill it with 0 i prefer to chose interpolate\n- After interpolating the feature, again checking for the missing values\n- All the feature which already interpolate still have missing values, and we will drop all the rows\n- And then trying to label encoding categorical data, and using min max scaler to make data tight\n- For the correlation, we can see that Size, Dpet, and MarkDown strongly correlated with our target feature. Later we will use this selected feature for modelling\n- After all the pre-processing step, we will start to modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation = pd.DataFrame({'Model': [],\n                           'Details':[],\n                           'Root Mean Squared Error (RMSE)':[],\n                           'R-squared (training)':[],\n                           'Adjusted R-squared (training)':[],\n                           'R-squared (test)':[],\n                           'Adjusted R-squared (test)':[],\n                           '5-Fold Cross Validation':[]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def adjustedR2(r2,n,k):\n    return r2-(k-1)/(n-k)*(1-r2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''LINIER REGRESION'''\n#with selected feature\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\ntrain_data,test_data = train_test_split(testing,train_size = 0.8,random_state=3)\n\nfeatures = ['Size','Dept', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\nlr1 = LinearRegression()\nlr1.fit(train_data[features],train_data['Weekly_Sales'])\n\npred1 = lr1.predict(test_data[features])\nrmsecm1 = float(format(np.sqrt(metrics.mean_squared_error(test_data['Weekly_Sales'],pred1)),'.3f'))\nrtrcm1 = float(format(lr1.score(train_data[features],train_data['Weekly_Sales']),'.3f'))\nartrcm1 = float(format(adjustedR2(lr1.score(train_data[features],train_data['Weekly_Sales']),train_data.shape[0],len(features)),'.3f'))\nrtecm1 = float(format(lr1.score(test_data[features],test_data['Weekly_Sales']),'.3f'))\nartecm1 = float(format(adjustedR2(lr1.score(test_data[features],test_data['Weekly_Sales']),test_data.shape[0],len(features)),'.3f'))\ncv1 = float(format(cross_val_score(lr1,testing[features],testing['Weekly_Sales'],cv=5).mean(),'.3f'))\n\nr = evaluation.shape[0]\nevaluation.loc[r] = ['Linear Regression','Selected Features',rmsecm1,rtrcm1,artrcm1,rtecm1,artecm1,cv1]\nevaluation.sort_values(by = '5-Fold Cross Validation', ascending=False)\nevaluation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''LINIER REGRESION'''\n#with all feature\n\nfeatures = testing.drop(['Weekly_Sales','Type', 'Size', 'Dept'], axis=1).columns.values.tolist()\nlr2 = LinearRegression()\nlr2.fit(train_data[features],train_data['Weekly_Sales'])\n\npred2 = lr2.predict(test_data[features])\nrmsecm2 = float(format(np.sqrt(metrics.mean_squared_error(test_data['Weekly_Sales'],pred2)),'.3f'))\nrtrcm2 = float(format(lr2.score(train_data[features],train_data['Weekly_Sales']),'.3f'))\nartrcm2 = float(format(adjustedR2(lr2.score(train_data[features],train_data['Weekly_Sales']),train_data.shape[0],len(features)),'.3f'))\nrtecm2 = float(format(lr2.score(test_data[features],test_data['Weekly_Sales']),'.3f'))\nartecm2 = float(format(adjustedR2(lr2.score(test_data[features],test_data['Weekly_Sales']),test_data.shape[0],len(features)),'.3f'))\ncv2 = float(format(cross_val_score(lr2,testing[features],testing['Weekly_Sales'],cv=5).mean(),'.3f'))\n\nr = evaluation.shape[0]\nevaluation.loc[r] = ['Linear Regression','All Features',rmsecm2,rtrcm2,artrcm2,rtecm2,artecm2,cv2]\nevaluation.sort_values(by = '5-Fold Cross Validation', ascending=False)\nevaluation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DISPLAY MODELLING\nimport seaborn as sns\n\ndef model_performance_sc_plot(predictions, labels, title):\n    min_val = max(max(predictions), max(labels))\n    max_val = min(min(predictions), min(labels))\n    performance_df = pd.DataFrame({\"Label\":labels})\n    performance_df[\"Prediction\"] = predictions\n    sns.jointplot(y=\"Label\", x=\"Prediction\", data=performance_df, kind=\"reg\", height=7)\n    plt.plot([min_val, max_val], [min_val, max_val], 'm--')\n    plt.title(title, fontsize=9)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LR With Selected Features\nmodel_performance_sc_plot(pred1, test_data['Weekly_Sales'], 'Validation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LR With All Features\nmodel_performance_sc_plot(pred2, test_data['Weekly_Sales'], 'Validation')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- For the first, i will create dataframe to input the result\n- I split the train and test 80 : 20\n- After all the modelling and calculate the parameter to check the model, i input it in evaluation dataframe\n- And then try to make function, to visualize our result\n- From the joint jointplot we can see the predicted and actual label distributin\n- I prefer to use logistic regression model which input all features compare to only selected features. Its because after checking the joint plot we can see that with selected feature, model tent to have larger predicted values (not similar to actual label)."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature = data2.columns.values.tolist()\n\nfor col in feature:\n    data2[col] = data2[col].interpolate()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2['Date'].min(), data2['Date'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2 = data2.loc[data2['Date'] >= '2012-12-10 00:00:00']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2['Sales Prediction'] = lr2.predict(data2[features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=1, \n                    cols=1)\n\ncustom_aggregation = {}\ncustom_aggregation[\"Sales Prediction\"] = \"mean\"\ndata2 = data2.set_index(pd.DatetimeIndex(data2['Date']))\ndata0 = data2.resample('M').agg(custom_aggregation)\ndata0.columns = [\"Monthly Profit Sales Prediction\"]\ndata0['Date_'] = data0.index\n\nx = data0['Date_'].tolist()\ny = data0['Monthly Profit Sales Prediction'].tolist()\ny_ = data0['Monthly Profit Sales Prediction'].interpolate().tolist()\n\nfig.add_trace(go.Scatter(x=x, y=y,name='Avg. Sales',line=dict(color='black', width=2)), 1, 1)\nfig.add_trace(go.Scatter(x=x, y=y_,name=' ',line=dict(color='black', width=2)), 1, 1)\n#--------------------------------------------------------------------------------------------------------\n\ncustom_aggregation = {}\ncustom_aggregation[\"Sales Prediction\"] = \"min\"\ndata2 = data2.set_index(pd.DatetimeIndex(data2['Date']))\ndata0 = data2.resample('M').agg(custom_aggregation)\ndata0.columns = [\"Monthly Profit Sales Prediction\"]\ndata0['Date_'] = data0.index\n\nx_min = data0['Date_'].tolist()\ny_min = data0['Monthly Profit Sales Prediction'].tolist()\ny_min_ = data0['Monthly Profit Sales Prediction'].interpolate().tolist()\n\n\nfig.add_trace(go.Scatter(x=x_min, y=y_min,name='Pesimistic Sales',line=dict(color='red', width=2)), 1, 1)\nfig.add_trace(go.Scatter(x=x_min, y=y_min_,name=' ',line=dict(color='red', width=2)), 1, 1)\n#--------------------------------------------------------------------------------------------------------\n\ncustom_aggregation = {}\ncustom_aggregation[\"Sales Prediction\"] = \"max\"\ndata2 = data2.set_index(pd.DatetimeIndex(data2['Date']))\ndata0 = data2.resample('M').agg(custom_aggregation)\ndata0.columns = [\"Monthly Profit Sales Prediction\"]\ndata0['Date_'] = data0.index\n\nx_max = data0['Date_'].tolist()\ny_max = data0['Monthly Profit Sales Prediction'].tolist()\ny_max_ = data0['Monthly Profit Sales Prediction'].interpolate().tolist()\n\nfig.add_trace(go.Scatter(x=x_max, y=y_max,name='Optimistic Sales',line=dict(color='darkgreen', width=2)), 1, 1)\nfig.add_trace(go.Scatter(x=x_max, y=y_max_,name=' ',line=dict(color='darkgreen', width=2)), 1, 1)\n#--------------------------------------------------------------------------------------------------------\n\n\nfig['layout'].update(height=500, width=900, title='Monthly Sales Prediction')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- After selectin the best model, i preparing the data which we will predict (data2)\n- We will range the data between 2012-12-10 untill 2013-12-07\n- After predicting the sales, the we will make time frame visualization about our prediction\n- Here i make monthly sales with 3 type, Average Sales (Mean weekly sales in one month), Pesimistic Sales (The minumum weekly sales in one month), and the the Optimistic Sales (The maximum weekly sales in one month)\n- The sales in Feb 2013 tend to increase before it goes down. And then start increased again until its highest peak on August 2013\n- Finally for answering our question, Did Sales Increase on Christmas? Yes and No! Because on real complete data we can see highest monthly sales peak are on December each year. But in our model it tend to decrease when December 2013. It may because our interpolate Markdown predict lows values after the lastest knowing Markdown. It may be in accurate for some reason."},{"metadata":{},"cell_type":"markdown","source":"Finish, don't forget to upvote. Thank You!:)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}